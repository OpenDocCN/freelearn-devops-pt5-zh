- en: '*Chapter 9*: Observability on Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：Kubernetes 上的可观察性'
- en: This chapter dives into capabilities that are highly recommended to implement
    when running Kubernetes in production. First, we discuss observability in the
    context of distributed systems such as Kubernetes. Then, we look at the built-in
    Kubernetes observability stack and what functionality it implements. Finally,
    we learn how to supplement the built-in observability tooling with additional
    observability, monitoring, logging, and metrics infrastructure from the ecosystem.
    The skills you learn in this chapter will help you deploy observability tools
    to your Kubernetes cluster and enable you to understand how your cluster (and
    applications running on it) are functioning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了在生产环境中运行 Kubernetes 时，强烈推荐实施的功能。首先，我们讨论在 Kubernetes 等分布式系统中的可观察性。然后，我们查看
    Kubernetes 内置的可观察性堆栈及其实现的功能。最后，我们学习如何通过额外的可观察性、监控、日志记录和度量基础设施来补充内置的可观察性工具。您将在本章中学到的技能将帮助您将可观察性工具部署到
    Kubernetes 集群中，并使您能够了解您的集群（以及在其上运行的应用程序）如何运作。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding observability on Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 上的可观察性
- en: Using default observability tooling – metrics, logging, and the dashboard
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用默认的可观察性工具 – 度量、日志记录和仪表盘
- en: Implementing the best of the ecosystem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现生态系统中的最佳实践
- en: To start, we will learn the out-of-the-box tools and processes that Kubernetes
    provides for observability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解 Kubernetes 提供的开箱即用的可观察性工具和流程。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the kubectl tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持 `kubectl` 命令行工具的计算机，并且需要一个正常工作的 Kubernetes 集群。请参阅 [*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与
    Kubernetes 通信*，了解如何快速启动 Kubernetes，以及如何安装 kubectl 工具的说明。
- en: 'The code used in this chapter can be found in the book''s GitHub repository:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在本书的 GitHub 仓库中找到：
- en: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9)'
- en: Understanding observability on Kubernetes
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 上的可观察性
- en: No production system is complete without a way to monitor it. In software, we
    define observability as the ability to, at any point in time, understand how our
    system is performing (and, in the best case, why). Observability grants significant
    benefits in security, performance, and operational capacity. By knowing how your
    system is responding at the VM, container, and application level, you can tune
    it to perform efficiently, react quickly to events, and more easily troubleshoot
    bugs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 没有监控机制的生产系统是无法完整运行的。在软件中，我们将可观察性定义为在任何时刻，了解我们的系统如何运行（并且在最佳情况下，了解原因）。可观察性在安全性、性能和操作能力方面提供了显著的优势。通过了解系统在虚拟机、容器和应用层的响应情况，您可以对其进行调优，以提高性能、快速响应事件，并更容易排除故障。
- en: For instance, let's take a scenario where your application is running extremely
    slowly. In order to find the bottleneck, you may look at the application code
    itself, the resource specifications of the Pod, the number of Pods in the deployment,
    the memory and CPU usage at the Pod level or Node level, and externalities such
    as a MySQL database running outside your cluster.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您的应用程序运行得非常慢。为了找出瓶颈，您可能需要查看应用程序代码、Pod 的资源规格、部署中的 Pod 数量、Pod 层或节点层的内存和 CPU
    使用情况，以及像运行在集群外的 MySQL 数据库等外部因素。
- en: By adding observability tooling, you would be able to diagnose many of these
    variables and figure out what issues may be contributing to your application slowdown.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加可观察性工具，您将能够诊断许多变量，并找出可能导致应用程序性能下降的问题。
- en: 'Kubernetes, as a production-ready container orchestration system, gives us
    some default tools to monitor our applications. For the purposes of this chapter,
    we will separate observability into four ideas: metrics, logs, traces, and alerts.
    Let''s look at each of them:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes作为一个生产就绪的容器编排系统，提供了一些默认工具来监控我们的应用程序。为了本章的目的，我们将可观察性分为四个概念：指标、日志、追踪和警报。让我们逐一了解这些概念：
- en: '**Metrics** here represents the ability to see numerical representations of
    the system''s current state, with specific attention paid to CPU, memory, network,
    disk space, and more. These numbers allow us to judge the gap in current state
    with the system''s maximum capacity and ensure that the system remains available
    to users.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**在这里代表着能够看到系统当前状态的数字化表现，特别关注CPU、内存、网络、磁盘空间等方面。这些数字帮助我们判断当前状态与系统最大容量之间的差距，并确保系统能够保持对用户可用。'
- en: '**Logs** refers to the practice of collecting text logs from applications and
    systems. Logs will likely be a combination of Kubernetes control plane logs and
    logs from your application Pods themselves. Logs can help us diagnose the availability
    of the Kubernetes system, but they also can help with triaging application bugs.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志**是指从应用程序和系统中收集文本日志的做法。日志可能是Kubernetes控制平面日志和应用程序Pod日志的结合。日志有助于我们诊断Kubernetes系统的可用性，也有助于处理应用程序错误。'
- en: '**Traces** refers to collecting distributed traces. Traces are an observability
    pattern that delivers end-to-end visibility of a chain of requests – which can
    be HTTP requests or otherwise. This topic is especially important in a distributed
    cloud-native setting where microservices are used. If you have many microservices
    and they call each other, it can be difficult to find bottlenecks or issues when
    many services are involved in a single end-to-end request. Traces allow you to
    view requests broken down by each leg of a service-to-service call.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追踪**是指收集分布式追踪。追踪是一种可观察性模式，能够提供请求链的端到端可视化——这些请求可以是HTTP请求，也可以是其他类型的请求。这个话题在使用微服务的分布式云原生环境中特别重要。如果你有很多微服务，并且它们互相调用，当多个服务参与到单一的端到端请求时，找出瓶颈或问题可能会变得困难。追踪使你能够查看每个服务间调用的每一个环节的请求。'
- en: '**Alerts** correspond to the practice of setting automated touch points when
    certain events happen. Alerts can be set on both *metrics* and *logs*, and delivered
    through a host of mediums, from text messages to emails to third-party applications
    and everything in between.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报**是指在某些事件发生时设置自动触发点的做法。警报可以设置在*指标*和*日志*上，并通过多种媒介传递，从短信到电子邮件，再到第三方应用程序，涵盖了各种方式。'
- en: Between these four aspects of observability, we should be able to understand
    the health of our cluster. However, it is possible to configure many different
    possible data points for metrics, logs, and even alerting. Therefore, knowing
    what to look for is important. The next section will discuss the most important
    observability areas for Kubernetes cluster and application health.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四个可观察性方面之间，我们应该能够理解集群的健康状况。然而，针对指标、日志甚至警报，配置了许多不同的数据点。因此，知道应该关注哪些内容非常重要。下一节将讨论Kubernetes集群和应用程序健康的最重要的可观察性领域。
- en: Understanding what matters for Kubernetes cluster and application health
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Kubernetes集群和应用程序健康状态的重要因素
- en: Among the vast number of possible metrics and logs that Kubernetes or third-party
    observability solutions for Kubernetes can provide, we can narrow down some of
    the ones that are most likely to cause major issues with your cluster. You should
    keep these pieces front and center in whichever observability solution you end
    up using. First, let's look at the connection between CPU usage and cluster health.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes或第三方可观察性解决方案为Kubernetes提供的众多指标和日志中，我们可以缩小范围，找出最可能对集群造成重大问题的一些指标。你应该将这些要点放在最终使用的任何可观察性解决方案的核心位置。首先，让我们来看一下CPU使用率与集群健康之间的关系。
- en: Node CPU usage
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点CPU使用率
- en: The state of CPU usage across the Nodes in your Kubernetes cluster is a very
    important metric to keep an eye on across your observability solution. We've discussed
    in previous chapters how Pods can define resource requests and limits for CPU
    usage. However, it is still possible for Nodes to oversubscribe their CPU usage
    when the limits are set higher than the maximum CPU capacity of the cluster. Additionally,
    the master Nodes that run our control plane can also encounter CPU capacity issues.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群中各节点的 CPU 使用情况是一个非常重要的指标，应该在你的可观察性解决方案中时刻关注。我们在前面的章节中讨论了 Pods 如何定义
    CPU 使用的资源请求和限制。然而，当限制设置高于集群的最大 CPU 容量时，节点仍然有可能会过度分配 CPU 使用情况。此外，运行我们控制平面的主节点也可能遇到
    CPU 容量问题。
- en: Worker Nodes with maxed-out CPUs may perform poorly or throttle workloads running
    on Pods. This can easily occur if no limits are set on Pods – or if a Node's total
    Pod resource limits are greater than its max capacity, even if its total resource
    requests are lower. Master Nodes with capped-out CPUs may hurt the performance
    of the scheduler, kube-apiserver, or any of the other control plane components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 满载的工作节点可能会表现不佳，或限制在 Pods 上运行的工作负载。如果没有为 Pods 设置限制，或者一个节点的总 Pod 资源限制超过了其最大容量，即使其总资源请求较低，也很容易发生这种情况。CPU
    满载的主节点可能会影响调度器、kube-apiserver 或任何其他控制平面组件的性能。
- en: In general, CPU usage across worker and master Nodes should be visible in your
    observability solution. This is best done via a combination of metrics (for instance
    on a charting solution such as Grafana, which you'll learn about later in this
    chapter) – and alerts for high CPU usage across the nodes in your cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，工作节点和主节点的 CPU 使用情况应该在你的可观察性解决方案中可见。最好通过结合使用指标（例如在图表解决方案如 Grafana 上，你将在本章稍后学习到）以及集群中各节点的高
    CPU 使用率警报来实现。
- en: Memory usage is also an extremely important metric to keep track of, similar
    to with CPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 内存使用情况也是一个极其重要的指标，类似于 CPU。
- en: Node memory usage
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点内存使用情况
- en: As with CPU usage, memory usage is an extremely important metric to observe
    across your cluster. Memory usage can be oversubscribed using Pod Resource Limits
    – and many of the same issues as with CPU usage can apply for both the master
    and worker Nodes in the cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CPU 使用情况类似，内存使用情况是观察集群内情况时非常重要的一个指标。内存使用情况可以通过 Pod 资源限制被过度分配——并且许多与 CPU 使用情况相同的问题可能适用于集群中的主节点和工作节点。
- en: Again, a combination of alerting and metrics is important for visibility into
    cluster memory usage. We will learn some tools for this later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，警报和指标的结合对于集群内存使用情况的可见性至关重要。我们将在本章稍后学习一些工具来实现这一点。
- en: For the next major observability piece, we will look not at metrics but at logs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个主要的可观察性内容，我们将关注日志，而不是指标。
- en: Control plane logging
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制平面日志
- en: The components of the Kubernetes control plane, when running, output logs that
    can be used to get an in-depth view of cluster operations. These logs can also
    significantly help with troubleshooting, as we'll see in [*Chapter 10*](B14790_10_Final_PG_ePub.xhtml#_idTextAnchor230),
    *Troubleshooting Kubernetes*. Logs for the Kubernetes API server, controller manager,
    scheduler, kube proxy, and kubelet can all be very useful for certain troubleshooting
    or observability reasons.
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面组件在运行时会输出日志，这些日志可以用于深入了解集群操作。这些日志也能显著帮助故障排除，正如我们在[*第 10 章*](B14790_10_Final_PG_ePub.xhtml#_idTextAnchor230)《故障排除
    Kubernetes》中所看到的那样。Kubernetes API 服务器、控制器管理器、调度器、kube proxy 和 kubelet 的日志，对于某些故障排除或可观察性原因来说，都非常有用。
- en: Application logging
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序日志
- en: Application logging can also be incorporated into an observability stack for
    Kubernetes – being able to view application logs along with other metrics can
    be very helpful to operators.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序日志也可以被集成到 Kubernetes 的可观察性栈中——能够查看应用程序日志和其他指标一起，能够帮助运维人员非常有效。
- en: Application performance metrics
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序性能指标
- en: As with application logging, application performance metrics and monitoring
    are highly relevant to the performance of your applications on Kubernetes. Memory
    usage and CPU profiling at the application level can be a valuable piece of the
    observability stack.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与应用程序日志一样，应用程序性能指标和监控与 Kubernetes 上应用程序的性能高度相关。在应用程序层面上的内存使用情况和 CPU 分析可以成为可观察性栈中的一个有价值的组成部分。
- en: Generally, Kubernetes provides the data infrastructure for application monitoring
    and logging but stays away from providing higher-level functionality such as charting
    and searching. With this in mind, let's review the tools that Kubernetes gives
    us by default to address these concerns.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，Kubernetes 提供了应用监控和日志记录的数据基础设施，但避免提供如图表和搜索等更高级的功能。考虑到这一点，让我们回顾一下 Kubernetes
    默认提供给我们的工具，以解决这些问题。
- en: Using default observability tooling
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用默认的可观察性工具
- en: Kubernetes provides observability tooling even without adding any third-party
    solutions. These native Kubernetes tools form the basis of many of the more robust
    solutions, so they are important to discuss. Since observability includes metrics,
    logs, traces, and alerts, we will discuss each in turn, focusing first on the
    Kubernetes-native solutions. First, let's discuss metrics.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了可观察性工具，即使没有添加任何第三方解决方案。这些原生的 Kubernetes 工具是许多更强大解决方案的基础，因此它们非常值得讨论。由于可观察性包括度量、日志、跟踪和警报，我们将依次讨论每个方面，首先聚焦于
    Kubernetes 原生解决方案。首先，让我们讨论度量。
- en: Metrics on Kubernetes
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 的度量
- en: A lot of information about your applications can be gained by simply running
    `kubectl describe pod`. We can see information about our Pod's spec, what state
    it is in, and key issues preventing its functionality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单运行`kubectl describe pod`，你可以获取大量关于应用的信息。我们可以看到关于 Pod 的规格、其所处的状态，以及影响其功能的关键问题。
- en: 'Let''s assume we are having some trouble with our application. Specifically,
    the Pod is not starting. To investigate, we run `kubectl describe pod`. As a reminder
    on kubectl aliases mentioned in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, `kubectl describe pod` is the same as `kubectl
    describe pods`. Here is an example output from the `describe pod` command – we''ve
    stripped out everything apart from the `Events` information:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的应用程序遇到了一些问题。具体来说，Pod 没有启动。为了进行调查，我们运行了 `kubectl describe pod`。提醒一下，[*第
    1 章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)中提到的 kubectl 别名，`kubectl
    describe pod` 和 `kubectl describe pods` 是等效的。以下是 `describe pod` 命令的示例输出——我们已去掉除
    `Events`（事件）信息之外的所有内容：
- en: '![Figure 9.1 – Describe Pod Events output](img/B14790_09_001_new.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 描述 Pod 事件输出](img/B14790_09_001_new.jpg)'
- en: Figure 9.1 – Describe Pod Events output
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 描述 Pod 事件输出
- en: As you can see, this Pod is not being scheduled because our Nodes are all out
    of memory! That would be a good thing to investigate further.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Pod 无法调度是因为我们的节点内存已满！这应该是进一步调查的一个重要问题。
- en: 'Let''s keep going. By running `kubectl describe nodes`, we can learn a lot
    about our Kubernetes Nodes. Some of this information can be very relevant to how
    our system is performing. Here''s another example output, this time from the `kubectl
    describe nodes` command. Rather than putting the entire output here, which can
    be quite lengthy, let''s zero in on two important sections – `Conditions` and
    `Allocated resources`. First, let''s review the `Conditions` section:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续前进。通过运行 `kubectl describe nodes`，我们可以了解很多关于 Kubernetes 节点的信息。部分信息对于我们的系统性能非常相关。这里是另一个示例输出，这次来自
    `kubectl describe nodes` 命令。为了避免输出过长，我们将聚焦于两个重要部分——`Conditions`（条件）和 `Allocated
    resources`（分配的资源）。首先，让我们回顾一下 `Conditions`（条件）部分：
- en: '![Figure 9.2 – Describe Node Conditions output](img/B14790_09_002_new.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 描述节点条件输出](img/B14790_09_002_new.jpg)'
- en: Figure 9.2 – Describe Node Conditions output
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 描述节点条件输出
- en: As you can see, we have included the `Conditions` block of the `kubectl describe
    nodes` command output. It's a great place to look for any issues. As we can see
    here, our Node is actually experiencing issues. Our `MemoryPressure` condition
    is true, and the `Kubelet` has insufficient memory. No wonder our Pods won't schedule!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经包含了 `kubectl describe nodes` 命令输出中的 `Conditions`（条件）块。这是查看任何问题的好地方。正如我们在这里看到的，我们的节点确实存在问题。我们的
    `MemoryPressure`（内存压力）条件为真，且 `Kubelet` 内存不足。难怪我们的 Pods 无法调度！
- en: 'Next, check out the `Allocated resources` block:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，查看 `Allocated resources`（分配的资源）部分：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we're seeing some metrics! It looks like our Pods are requesting too much
    memory, leading to our Node and Pod issues. As you can tell from this output,
    Kubernetes is already collecting metrics data about our Nodes, by default. Without
    that data, the scheduler would not be able to do its job properly, since maintaining
    Pod resources requests with Node capacity is one of its most important functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到了些指标！看起来我们的 Pods 请求了过多的内存，导致了节点和 Pod 的问题。从这个输出中可以看出，Kubernetes 已经在默认情况下收集了有关节点的指标数据。如果没有这些数据，调度器就无法正常工作，因为维护
    Pod 资源请求与节点容量之间的平衡是它最重要的功能之一。
- en: However, by default, these metrics are not surfaced to the user. They are in
    fact being collected by each Node's `Kubelet` and delivered to the scheduler for
    it to do its job. Thankfully, we can easily get these metrics by deploying Metrics
    Server to our cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，默认情况下，这些指标并不会显示给用户。它们实际上是由每个节点的 `Kubelet` 收集并传递给调度器，以便调度器可以执行其工作。幸运的是，我们可以通过将
    Metrics Server 部署到集群中，轻松获取这些指标。
- en: Metrics Server is an officially supported Kubernetes application that collects
    metrics information and surfaces it on an API endpoint for use. Metrics Server
    is in fact required to make the Horizontal Pod Autoscaler work, but it is not
    always included by default, depending on the Kubernetes distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Metrics Server 是一个官方支持的 Kubernetes 应用程序，用于收集指标信息并将其通过 API 端点提供给使用。实际上，Metrics
    Server 是 Horizontal Pod Autoscaler 正常工作的必要条件，但根据 Kubernetes 发行版的不同，它并不总是默认包含在内。
- en: 'Deploying Metrics Server is very quick. As of the writing of this book, the
    newest version can be installed using the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 Metrics Server 非常快速。截至本书写作时，可以使用以下命令安装最新版本：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Full documentation on how to use Metrics Server can be found at [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用 Metrics Server 的完整文档可以在 [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)
    找到。
- en: Once Metrics Server is running, we can use a brand-new Kubernetes command. The
    `kubectl top` command can be used with either Pods or Nodes to see granular information
    about how much memory and CPU capacity is in use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Metrics Server 启动，我们就可以使用一个全新的 Kubernetes 命令。`kubectl top` 命令可以与 Pods 或 Nodes
    一起使用，以查看内存和 CPU 使用量的详细信息。
- en: 'Let''s take a look at some example usage. Run `kubectl top nodes` to see Node-level
    metrics. Here''s the output of the command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些示例用法。运行 `kubectl top nodes` 查看节点级别的指标。以下是该命令的输出：
- en: '![Figure 9.3 – Node Metrics output](img/B14790_09_003_new.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 节点指标输出](img/B14790_09_003_new.jpg)'
- en: Figure 9.3 – Node Metrics output
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 节点指标输出
- en: As you can see, we are able to see both absolute and relative CPU and memory
    usage.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们可以看到 CPU 和内存使用情况的绝对值和相对值。
- en: Important note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: CPU cores are measured in `millcpu` or `millicores`. 1,000 `millicores` is equivalent
    to one virtual CPU. Memory is measured in bytes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 核心的度量单位是 `millcpu` 或 `millicores`。1,000 `millicores` 等于一个虚拟 CPU。内存以字节为单位进行度量。
- en: Next, let's take a look at the `kubectl top pods` command. Run it with the `–namespace
    kube-system` flag to see Pods in the `kube-system` namespace.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看一下 `kubectl top pods` 命令。使用 `–namespace kube-system` 标志运行它，可以查看 `kube-system`
    命名空间中的 Pods。
- en: 'To do this, we run the following command:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们运行以下命令：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And we get the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到以下输出：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, this command uses the same absolute units as `kubectl top nodes`
    – millicores and bytes. There are no relative percentages when looking at Pod-level
    metrics.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个命令使用了与 `kubectl top nodes` 相同的绝对单位——毫核心和字节。在查看 Pod 层级的指标时，没有相对百分比。
- en: Next, we'll look at how Kubernetes handles logging.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看 Kubernetes 如何处理日志。
- en: Logging on Kubernetes
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 上的日志
- en: We can split up logging on Kubernetes into two areas – *application logs* and
    *control plane logs*. Let's start with control plane logs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 Kubernetes 的日志分为两个领域——*应用程序日志* 和 *控制平面日志*。首先让我们看一下控制平面日志。
- en: Control plane logs
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制平面日志
- en: 'Control plane logs refers to the logs created by the Kubernetes control plane
    components, such as the scheduler, API server, and others. For a vanilla Kubernetes
    install, control plane logs can be found on the Nodes themselves and require direct
    access to the Nodes in order to see. For clusters with components set up to use
    `systemd`, logs are found using the `journalctl` CLI tool (refer to the following
    link for more information: [https://manpages.debian.org/stretch/systemd/journalctl.1.en.html](https://manpages.debian.org/stretch/systemd/journalctl.1.en.html)
    ).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面日志指的是由 Kubernetes 控制平面组件（如调度器、API 服务器等）创建的日志。对于标准 Kubernetes 安装，控制平面日志可以在节点上找到，并且需要直接访问节点才能查看。对于配置为使用
    `systemd` 的集群，可以使用 `journalctl` CLI 工具来查看日志（有关更多信息，请参阅以下链接：[https://manpages.debian.org/stretch/systemd/journalctl.1.en.html](https://manpages.debian.org/stretch/systemd/journalctl.1.en.html)）。
- en: 'On master Nodes, you can find logs in the following locations on the filesystem:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在主节点上，你可以在文件系统的以下位置找到日志：
- en: At `/var/log/kube-scheduler.log`, you can find the Kubernetes scheduler logs.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `/var/log/kube-scheduler.log`，你可以找到 Kubernetes 调度器的日志。
- en: At `/var/log/kube-controller-manager.log`, you can find the controller manager
    logs (for instance, to see scaling events).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `/var/log/kube-controller-manager.log`，你可以找到控制器管理器的日志（例如，查看扩缩容事件）。
- en: At `/var/log/kube-apiserver.log`, you can find the Kubernetes API server logs.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `/var/log/kube-apiserver.log`，你可以找到 Kubernetes API 服务器的日志。
- en: 'On worker Nodes, logs are available in two locations on the filesystem:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作节点上，日志可以在文件系统的两个位置找到：
- en: At `/var/log/kubelet.log`, you can find the kubelet logs.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `/var/log/kubelet.log`，你可以找到 kubelet 的日志。
- en: At `/var/log/kube-proxy.log`, you can find the kube proxy logs.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `/var/log/kube-proxy.log`，你可以找到 kube proxy 的日志。
- en: Although, generally, cluster health is influenced by the health of the Kubernetes
    master and worker Node components, it is of course also important to keep track
    of your application's logs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一般来说，集群的健康状况受到 Kubernetes 主节点和工作节点组件健康状况的影响，但当然也需要关注你的应用日志。
- en: Application logs
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用日志
- en: It's very easy to find application logs on Kubernetes. Before we explain how
    it works, let's look at an example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上查找应用日志非常简单。在我们解释如何运作之前，先来看一个例子。
- en: 'To check logs for a specific Pod, you can use the `kubectl logs <pod_name>`
    command. The output of the command will display any text written to the container''s
    `stdout` or `stderr`. If a Pod has multiple containers, you must include the container
    name in the command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看特定 Pod 的日志，你可以使用 `kubectl logs <pod_name>` 命令。该命令的输出会显示写入容器 `stdout` 或 `stderr`
    的任何文本。如果一个 Pod 有多个容器，你必须在命令中包含容器的名称：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Under the hood, Kubernetes handles Pod logs by using the container engine's
    logging driver. Typically, any logs to `stdout` or `stderr` are persisted to each
    Node's disk in the `/var/logs` folder. Depending on the Kubernetes distribution,
    log rotations may be set up to prevent overuse of Node disk space by logs. In
    addition, Kubernetes components such as the scheduler, kubelet, and kube-apiserver
    also persist logs to Node disk space, usually within the `/var/logs` folder. It
    is important to note how limited this default logging capability is – a robust
    observability stack for Kubernetes would certainly include a third-party solution
    for log forwarding, as we'll see shortly.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从底层来看，Kubernetes 通过使用容器引擎的日志驱动程序来处理 Pod 日志。通常，任何写入 `stdout` 或 `stderr` 的日志都会被保存到每个节点的磁盘中的
    `/var/logs` 文件夹。根据 Kubernetes 发行版的不同，日志轮转可能已设置，以防止日志占用节点磁盘空间过多。此外，Kubernetes 组件（如调度器、kubelet
    和 kube-apiserver）也会将日志保存到节点磁盘空间，通常是在 `/var/logs` 文件夹中。需要注意的是，这种默认的日志记录能力非常有限——一个强大的可观察性栈肯定会包括第三方的日志转发解决方案，正如我们接下来将看到的那样。
- en: Next, for general Kubernetes observability, we can use Kubernetes Dashboard.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了进行 Kubernetes 的整体可观察性，我们可以使用 Kubernetes Dashboard。
- en: Installing Kubernetes Dashboard
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Kubernetes Dashboard
- en: Kubernetes Dashboard provides all of the functionality of kubectl – including
    viewing logs and editing resources – in a GUI. It's very easy to get the dashboard
    set up – let's see how.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Dashboard 提供了 kubectl 的所有功能——包括查看日志和编辑资源——通过图形用户界面（GUI）实现。设置仪表板非常简单——我们来看看如何操作。
- en: The dashboard can be installed in a single `kubectl apply` command. For customizations,
    check out the Kubernetes Dashboard GitHub page at [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板可以通过一个 `kubectl apply` 命令来安装。若要进行自定义，请访问 Kubernetes Dashboard 的 GitHub 页面：[https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)。
- en: 'To install a version of Kubernetes Dashboard, run the following `kubectl` command,
    replacing the `<VERSION>` tag with your desired version, based on the version
    of Kubernetes you are using (again, check the Dashboard GitHub page for version
    compatibility):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Kubernetes仪表盘的版本，请运行以下`kubectl`命令，将`<VERSION>`标签替换为你想要的版本，基于你使用的Kubernetes版本（再次检查仪表盘的GitHub页面以确保版本兼容性）：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In our case, as of the writing of this book, we will use v2.0.4 – the final
    command looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就本书撰写时而言，我们将使用v2.0.4版本——最终命令如下所示：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once Kubernetes Dashboard has been installed, there are a few methods to access
    it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了Kubernetes仪表盘，就有几种方法可以访问它。
- en: Important note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is not usually recommended to use Ingress or a public load balancer service,
    because Kubernetes Dashboard allows users to update cluster objects. If for some
    reason your login methods for the dashboard are compromised or easy to figure
    out, you could be looking at a large security risk.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通常不建议使用Ingress或公共负载均衡器服务，因为Kubernetes仪表盘允许用户更新集群对象。如果由于某些原因你的仪表盘登录方法被泄露或容易猜到，可能会面临巨大的安全风险。
- en: With that in mind, we can use either `kubectl port-forward` or `kubectl proxy`
    in order to view our dashboard from our local machine.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个前提，我们可以使用`kubectl port-forward`或`kubectl proxy`来从本地计算机查看我们的仪表盘。
- en: For this example, we will use the `kubectl proxy` command, because we haven't
    used it in an example yet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`kubectl proxy`命令，因为我们还没有在之前的例子中使用过它。
- en: The `kubectl proxy` command, unlike the `kubectl port-forward` command, requires
    only one command to proxy to every service running on your cluster. It does this
    by proxying the Kubernetes API directly to a port on your local machine, which
    is by default `8081`. For a full discussion of the `Kubectl proxy` command, check
    the docs at [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与`kubectl port-forward`命令不同，`kubectl proxy`命令只需要一个命令来代理到集群中运行的所有服务。它通过将Kubernetes
    API直接代理到本地计算机的端口（默认为`8081`）来实现这一点。有关`Kubectl proxy`命令的详细讨论，请参阅文档：[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy)。
- en: 'In order to access a specific Kubernetes service using `kubectl proxy`, you
    just need to have the right path. The path to access Kubernetes Dashboard after
    running `kubectl proxy` will be the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用`kubectl proxy`访问特定的Kubernetes服务，你只需要拥有正确的路径。运行`kubectl proxy`后访问Kubernetes仪表盘的路径将如下所示：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the `kubectl proxy` path we put in our browser is on localhost
    port `8001`, and mentions the namespace (`kubernetes-dashboard`), the service
    name and selector (`https:kubernetes-dashboard`), and a proxy path.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在浏览器中输入的`kubectl proxy`路径是本地`8001`端口，并提到命名空间（`kubernetes-dashboard`）、服务名称和选择器（`https:kubernetes-dashboard`）以及代理路径。
- en: 'Let''s put our Kubernetes Dashboard URL in a browser and see the result:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将Kubernetes仪表盘的URL输入浏览器并查看结果：
- en: '![Figure 9.4 – Kubernetes Dashboard login](img/B14790_09_004_new.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – Kubernetes 仪表盘登录](img/B14790_09_004_new.jpg)'
- en: Figure 9.4 – Kubernetes Dashboard login
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – Kubernetes 仪表盘登录
- en: When we deploy and access Kubernetes Dashboard, we are met with a login screen.
    We can either create a Service Account (or use our own) to log in to the dashboard,
    or simply link our local `Kubeconfig` file. By logging in to Kubernetes Dashboard
    with a specific Service Account's token, the dashboard user will inherit that
    Service Account's permissions. This allows you to specify what type of actions
    a user will be able to take using Kubernetes Dashboard – for instance, read-only
    permissions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署并访问Kubernetes仪表盘时，首先会看到一个登录界面。我们可以创建一个服务账户（或使用我们自己的账户）来登录仪表盘，或者直接链接本地的`Kubeconfig`文件。通过使用特定服务账户的令牌登录Kubernetes仪表盘，仪表盘用户将继承该服务账户的权限。这使得我们能够指定用户在使用Kubernetes仪表盘时可以执行的操作类型——例如，只读权限。
- en: 'Let''s go ahead and create a brand-new Service Account for our Kubernetes Dashboard.
    You could customize this Service Account and limit its permissions, but for now
    we will give it admin permissions. To do this, follow these steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个全新的服务账户用于Kubernetes仪表盘。你可以自定义这个服务账户并限制它的权限，但现在我们将给予它管理员权限。为此，请按照以下步骤操作：
- en: 'We can create a Service Account imperatively using the following Kubectl command:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下Kubectl命令以命令式方式创建服务账户：
- en: '[PRE8]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output, confirming the creation of our Service
    Account:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出，确认我们已创建服务账户：
- en: '[PRE9]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we need to link our Service Account to a ClusterRole. You could also use
    a Role, but we want our dashboard user to be able to access all namespaces. To
    link a Service Account to the `cluster-admin` default ClusterRole using a single
    command, we can run the following:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要将服务帐户与ClusterRole关联。你也可以使用Role，但我们希望仪表盘用户能够访问所有命名空间。为了将服务帐户与`cluster-admin`默认ClusterRole通过单个命令进行关联，我们可以运行以下命令：
- en: '[PRE10]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This command will result in the following output:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令将生成以下输出：
- en: '[PRE11]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After this command is run, we should be able to log in to our dashboard! First,
    we just need to find the token that we will use to log in. A Service Account''s
    token is stored as a Kubernetes secret, so let''s see what it looks like. Run
    the following command to see which secret our token is stored in:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此命令后，我们应该能够登录仪表盘！首先，我们只需要找到用于登录的token。服务帐户的token作为Kubernetes密钥存储，因此我们来看看它是什么样的。运行以下命令查看我们的token存储在哪个密钥中：
- en: '[PRE12]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the output, you should see a secret that looks like the following:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在输出中，你应该能看到一个类似以下内容的密钥：
- en: '[PRE13]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, to get our token for signing in to the dashboard, we only need to describe
    the secret contents using the following:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了获取我们的token以登录仪表盘，我们只需要使用以下命令描述密钥内容：
- en: '[PRE14]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting output will look like the following:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果输出将如下所示：
- en: '[PRE15]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To log in to the dashboard, copy the string next to `token`, copy it into the
    token input on the Kubernetes Dashboard login screen, and click **Sign In**. You
    should be greeted with the Kubernetes Dashboard overview page!
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要登录仪表盘，复制`token`旁边的字符串，将其粘贴到Kubernetes仪表盘登录页面的token输入框中，然后点击**登录**。你应该会看到Kubernetes仪表盘的概览页面！
- en: Go ahead and click around the dashboard – you should be able to see all the
    same resources you would be able to using kubectl, and you can filter by namespace
    in the left-hand sidebar. For instance, here's a view of the **Namespaces** page:![Figure
    9.5 – Kubernetes Dashboard detail](img/B14790_09_005_new.jpg)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续点击仪表盘，你应该能够看到所有与使用kubectl相同的资源，并且可以在左侧边栏按命名空间进行筛选。例如，这里是**命名空间**页面的视图：![图
    9.5 – Kubernetes 仪表盘详情](img/B14790_09_005_new.jpg)
- en: Figure 9.5 – Kubernetes Dashboard detail
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.5 – Kubernetes 仪表盘详情
- en: You can also click on individual resources, and even edit those resources using
    the dashboard as long as the Service Account you used to log in has the proper
    permissions.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以点击单独的资源，甚至使用仪表盘编辑这些资源，只要你用于登录的服务帐户具有适当的权限。
- en: 'Here''s a view of editing a Deployment resource from the deployment detail
    page:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是从部署详情页面编辑部署资源的视图：
- en: '![Figure 9.6 – Kubernetes Dashboard edit view](img/B14790_09_006_new.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – Kubernetes 仪表盘编辑视图](img/B14790_09_006_new.jpg)'
- en: Figure 9.6 – Kubernetes Dashboard edit view
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – Kubernetes 仪表盘编辑视图
- en: Kubernetes Dashboard also lets you view Pod logs and dive into many other resource
    types in your cluster. To understand the full capabilities of the dashboard, check
    the docs at the previously mentioned GitHub page.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes仪表盘还允许你查看Pod日志，并深入了解集群中的许多其他资源类型。要了解仪表盘的全部功能，请查看前面提到的GitHub页面上的文档。
- en: Finally, to round out our discussion of default observability on Kubernetes,
    let's take a look at alerting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了完整讨论Kubernetes的默认可观察性，让我们来看一下警报功能。
- en: Alerts and traces on Kubernetes
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes上的警报和跟踪
- en: Unfortunately, the last two pieces of the observability puzzle – *alerts* and
    *traces* – are not yet native pieces of functionality on Kubernetes. In order
    to create this type of functionality, let's move on to our next section – incorporating
    open source tooling from the Kubernetes ecosystem.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，可观察性谜题的最后两部分——*警报*和*跟踪*——目前在Kubernetes中还不是原生功能。为了创建这类功能，让我们进入下一个部分——整合Kubernetes生态系统中的开源工具。
- en: Enhancing Kubernetes observability using the best of the ecosystem
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生态系统中的最佳工具增强Kubernetes的可观察性
- en: As we've discussed, though Kubernetes provides the basis for powerful visibility
    functionality, it is generally up to the community and vendor ecosystem to create
    higher-level tooling for metrics, logging, traces, and alerting. For the purposes
    of this book, we will focus on fully open source, self-hosted solutions. Since
    many of these solutions fulfill multiple visibility pillars between metrics, logs,
    traces, and alerting, instead of categorizing solutions into each visibility pillar
    during our review, we will review each solution separately.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，虽然 Kubernetes 提供了强大的可视化功能基础，但通常由社区和供应商生态系统创建更高级的度量、日志、跟踪和警报工具。对于本书的目的，我们将重点关注完全开源、自托管的解决方案。由于许多这样的解决方案跨越多个可视化支柱（包括度量、日志、跟踪和警报），因此在回顾过程中，我们不会根据每个可视化支柱来分类这些解决方案，而是将每个解决方案分别回顾。
- en: 'Let''s start with an often-used combination of technologies for metrics and
    alerts: **Prometheus** and **Grafana**.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个常用的度量和警报技术组合开始：**Prometheus** 和 **Grafana**。
- en: Introducing Prometheus and Grafana
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Prometheus 和 Grafana
- en: Prometheus and Grafana are a typical combination of visibility technologies
    on Kubernetes. Prometheus is a time series database, query layer, and alerting
    system with many integrations, while Grafana is a sophisticated graphing and visualization
    layer that integrates with Prometheus. We'll walk you through the installation
    and usage of these tools, starting with Prometheus.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 和 Grafana 是 Kubernetes 上典型的可视化技术组合。Prometheus 是一个时间序列数据库、查询层和警报系统，拥有许多集成，而
    Grafana 是一个复杂的图表和可视化层，能与 Prometheus 集成。我们将带你了解如何安装和使用这些工具，首先从 Prometheus 开始。
- en: Installing Prometheus and Grafana
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Prometheus 和 Grafana
- en: There are many ways to install Prometheus on Kubernetes, but most use Deployments
    in order to scale the service. For our purposes, we will be using the `kube-prometheus`
    project ([https://github.com/coreos/kube-prometheus](https://github.com/coreos/kube-prometheus)).
    This project includes an `operator` as well as several **custom resource definitions**
    (**CRDs**). It will also automatically install Grafana for us!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上安装 Prometheus 有多种方式，但大多数都使用 Deployments 来扩展服务。为了我们的目的，我们将使用 `kube-prometheus`
    项目（[https://github.com/coreos/kube-prometheus](https://github.com/coreos/kube-prometheus)）。该项目包括一个
    `operator` 和几个 **自定义资源定义**（**CRDs**）。它还会自动为我们安装 Grafana！
- en: An operator is essentially an application controller on Kubernetes (deployed
    like other applications in a Pod) that happens to make commands to the Kubernetes
    API in order to correctly run or operate its application.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Operator 本质上是 Kubernetes 上的一个应用控制器（像其他应用一样部署在 Pod 中），它通过 Kubernetes API 发出命令来正确运行或操作其应用程序。
- en: A CRD, on the other hand, allows us to model custom functionality inside of
    the Kubernetes API. We'll learn a lot more about operators and CRDs in [*Chapter
    13*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289), *Extending Kubernetes with
    CRDs*, but for now just think of operators as a way to create *smart deployments*
    where the application can control itself properly and spin up other Pods and Deployments
    as necessary – and think of CRDs as a way to use Kubernetes to store application-specific
    concerns.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: CRD 允许我们在 Kubernetes API 内建模自定义功能。我们将在[*第 13 章*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289)《使用
    CRD 扩展 Kubernetes》中学到更多关于 operators 和 CRDs 的内容，但现在只需将 operators 理解为创建 *智能部署* 的一种方式，其中应用程序能够自我管理并根据需要启动其他
    Pods 和 Deployments，而 CRD 则是使用 Kubernetes 存储特定应用程序相关问题的一种方式。
- en: 'To install Prometheus, first we need to download a release, which may be different
    depending on the newest version of Prometheus or your intended version of Kubernetes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Prometheus，首先我们需要下载一个发行版，具体版本可能会根据 Prometheus 的最新版本或你计划使用的 Kubernetes 版本有所不同：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, unzip the file using any tool. First, we're going to need to install the
    CRDs. In general, most Kubernetes tooling installation instructions will have
    you create the CRDs on Kubernetes first, since any additional setup that uses
    the CRD will fail if the underlying CRD has not already been created on Kubernetes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用任何工具解压文件。首先，我们需要安装 CRDs。通常，大多数 Kubernetes 工具的安装说明会要求你先在 Kubernetes 上创建
    CRDs，因为任何使用 CRD 的额外配置都会在底层 CRD 尚未创建时失败。
- en: 'Let''s install them using the following command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令进行安装：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ll need to wait a few seconds while the CRDs are created. This command
    will also create a `monitoring` namespace for our resources to live in. Once everything
    is ready, let''s spin up the rest of the Prometheus and Grafana resources using
    the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要等待几秒钟，直到 CRD（自定义资源定义）被创建。此命令还将为我们的资源创建一个`monitoring`命名空间。一旦一切准备就绪，让我们使用以下命令启动其余的
    Prometheus 和 Grafana 资源：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s talk about what this command will actually create. The entire stack
    consists of the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个命令实际会创建什么。整个堆栈由以下组件组成：
- en: '**Prometheus Deployment**: Pods of the Prometheus application'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus Deployment**：Prometheus 应用程序的 Pods'
- en: '**Prometheus Operator**: Controls and operates the Prometheus app Pods'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus Operator**：控制和操作 Prometheus 应用程序 Pods'
- en: '**Alertmanager Deployment**: A Prometheus component to specify and trigger
    alerts'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alertmanager Deployment**：Prometheus 组件，用于指定和触发警报'
- en: '**Grafana**: A powerful visualization dashboard'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grafana**：一个强大的可视化仪表盘'
- en: '**Kube-state-metrics agent**: Generates metrics from the Kubernetes API state'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kube-state-metrics agent**：从 Kubernetes API 状态生成指标'
- en: '**Prometheus Node Exporter**: Exports Node hardware- and OS-level metrics to
    Prometheus'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus Node Exporter**：将节点硬件和操作系统级别的指标导出到 Prometheus'
- en: '**Prometheus Adapter for Kubernetes Metrics**: Adapter for Kubernetes Resource
    Metrics API and Custom Metrics API for ingest into Prometheus'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus Adapter for Kubernetes Metrics**：用于 Prometheus 吸收 Kubernetes 资源指标
    API 和自定义指标 API 的适配器'
- en: Together, all these components will provide sophisticated visibility into our
    cluster, from the command plane down to the application containers themselves.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件一起将提供对我们集群的深度可视化，从命令平面到应用程序容器本身。
- en: Once the stack has been created (check by using the `kubectl get po -n monitoring`
    command), we can start using our components. Let's dive into usage, starting with
    plain Prometheus.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦堆栈创建完成（可以通过使用 `kubectl get po -n monitoring` 命令检查），我们就可以开始使用这些组件。让我们从普通的 Prometheus
    开始使用。
- en: Using Prometheus
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Prometheus
- en: Though the real power of Prometheus is in its data store, query, and alert layer,
    it does provide a simple UI to developers. As you'll see later, Grafana provides
    many more features and customizations, but it is worth it to get acquainted with
    the Prometheus UI.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Prometheus 的真正强大之处在于它的数据存储、查询和警报层，但它确实为开发人员提供了一个简单的 UI。如你稍后将看到的，Grafana 提供了更多的功能和定制，但值得熟悉
    Prometheus UI。
- en: By default, `kube-prometheus` will only create ClusterIP services for Prometheus,
    Grafana, and Alertmanager. It's up to us to expose them outside the cluster. For
    the purposes of this tutorial, we're simply going to port forward the service
    to our local machine. For production, you may want to use Ingress to route requests
    to the three services.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`kube-prometheus` 只会为 Prometheus、Grafana 和 Alertmanager 创建 ClusterIP 服务。我们需要自己将它们暴露到集群外部。为了本教程的目的，我们将仅仅通过端口转发将服务暴露到本地机器。对于生产环境，你可能希望使用
    Ingress 来路由请求到这三项服务。
- en: 'In order to `port-forward` to the Prometheus UI service, use the `port-forward`
    kubectl command:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了`port-forward`到 Prometheus UI 服务，使用 `port-forward` kubectl 命令：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We need to use port `9090` for the Prometheus UI. Access the service on your
    machine at `http://localhost:3000`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用端口 `9090` 来访问 Prometheus UI。你可以通过 `http://localhost:3000` 在本地机器上访问该服务。
- en: 'You should see something like the following screenshot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到类似下面的截图：
- en: '![Figure 9.7 – Prometheus UI](img/B14790_09_007_new.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – Prometheus UI](img/B14790_09_007_new.jpg)'
- en: Figure 9.7 – Prometheus UI
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – Prometheus UI
- en: As you can see, the Prometheus UI has a **Graph** page, which is what you can
    see in *Figure 9.4*. It also has its own UI for seeing configured alerts – but
    it doesn't allow you to create alerts via the UI. Grafana and Alertmanager will
    help us for that task.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Prometheus UI 有一个 **Graph** 页面，这就是你在 *图 9.4* 中看到的内容。它还有自己的 UI 用于查看已配置的警报——但它不允许你通过
    UI 创建警报。Grafana 和 Alertmanager 将帮助我们完成这项任务。
- en: 'To perform a query, navigate to the `PromQL` – we won''t present it fully to
    you in this book, but the Prometheus docs are a great way to learn. You can refer
    to it using the following link: [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行查询，请导航到 `PromQL`——我们在本书中不会完全介绍它，但 Prometheus 文档是学习的好途径。你可以通过以下链接参考：[https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)。
- en: 'To show how this works, let''s enter a basic query, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何运作，让我们输入一个基本的查询，如下所示：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This query will list the total number of HTTP requests made to the kubelet
    on each Node, for each request category, as shown in the following screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询将列出每个Node上对kubelet发出的HTTP请求总数，按每个请求类别分类，如下图所示：
- en: '![Figure 9.8 – HTTP requests query](img/B14790_09_008_new.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – HTTP请求查询](img/B14790_09_008_new.jpg)'
- en: Figure 9.8 – HTTP requests query
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – HTTP请求查询
- en: 'You can also see the requests in graph form by clicking the **Graph** tab next
    to **Table** as shown in the following screenshot:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过点击**图形**标签查看请求的图形形式，位置在**表格**旁边，如下图所示：
- en: '![Figure 9.9 – HTTP requests query – graph view](img/B14790_09_009_new.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – HTTP请求查询 – 图形视图](img/B14790_09_009_new.jpg)'
- en: Figure 9.9 – HTTP requests query – graph view
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – HTTP请求查询 – 图形视图
- en: This provides a time series graph view of the data from the preceding screenshot.
    As you can see, the graphing capability is fairly simple.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了前面截图中数据的时间序列图表视图。如你所见，图表功能相当简单。
- en: Prometheus also provides an **Alerts** tab for configuring Prometheus alerts.
    Typically, these alerts are configured via code instead of using the **Alerts**
    tab UI, so we will skip that page in our review. For more information, you can
    check the official Prometheus documentation at [https://prometheus.io/docs/alerting/latest/overview/](https://prometheus.io/docs/alerting/latest/overview/).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus还提供了一个**警报**标签，用于配置Prometheus警报。通常，这些警报是通过代码配置的，而不是使用**警报**标签的UI，因此我们将跳过这一页面。如果你想了解更多信息，可以查阅官方Prometheus文档：[https://prometheus.io/docs/alerting/latest/overview/](https://prometheus.io/docs/alerting/latest/overview/)。
- en: Let's move on to Grafana, where we can extend Prometheus powerful data tooling
    with visualizations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进，进入Grafana，在那里我们可以通过可视化扩展Prometheus强大的数据工具。
- en: Using Grafana
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Grafana
- en: Grafana provides powerful tools for visualizing metrics, with many supported
    charting types that can update in real time. We can connect Grafana to Prometheus
    in order to see our cluster metrics charted on the Grafana UI.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana提供了强大的可视化工具，可以实时更新多种支持的图表类型。我们可以将Grafana连接到Prometheus，以便在Grafana UI上查看我们的集群指标图表。
- en: 'To get started with Grafana, do the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Grafana，请执行以下操作：
- en: 'We will end our current port forwarding (*CTRL* + *C* will do the trick) and
    set up a new port forward listener to the Grafana UI:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将结束当前的端口转发（*CTRL* + *C*即可完成），并设置一个新的端口转发监听器，指向Grafana UI：
- en: '[PRE21]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Again, navigate to `localhost:3000` to see the Grafana UI. You should be able
    to log in with `admin` and `admin`, at which point you should be able to change
    the initial password as shown in the following screenshot:![Figure 9.10 – Grafana
    Change Password screen](img/B14790_09_010_new.jpg)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次访问`localhost:3000`以查看Grafana UI。你应该能够使用`admin`和`admin`登录，此时你可以更改初始密码，如下图所示：![图9.10
    – Grafana修改密码屏幕](img/B14790_09_010_new.jpg)
- en: Figure 9.10 – Grafana Change Password screen
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.10 – Grafana修改密码屏幕
- en: Upon login, you will see the following screen. Grafana does not come preconfigured
    with any dashboards, but we can add them easily by clicking the **+** sign as
    shown in the following screenshot:![Figure 9.11 – Grafana main page](img/B14790_09_011_new.jpg)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，你将看到以下屏幕。Grafana默认没有配置任何仪表板，但我们可以通过点击屏幕截图中显示的**+**号轻松添加它们：![图9.11 – Grafana主页面](img/B14790_09_011_new.jpg)
- en: Figure 9.11 – Grafana main page
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.11 – Grafana主页面
- en: Each Grafana dashboard includes one or more graphs for different sets of metrics.
    To add a preconfigured dashboard (instead of creating one yourself), click the
    plus sign (**+**) on the left-hand menu bar and click **Import**. You should see
    a page like the following screenshot:![Figure 9.12 – Grafana Dashboard Import](img/B14790_09_012_new.jpg)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个Grafana仪表板包含一个或多个用于不同指标集的图表。要添加一个预配置的仪表板（而不是自己创建一个），请点击左侧菜单栏中的加号（**+**），然后点击**导入**。你应该会看到如下截图的页面：![图9.12
    – Grafana仪表板导入](img/B14790_09_012_new.jpg)
- en: Figure 9.12 – Grafana Dashboard Import
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.12 – Grafana仪表板导入
- en: We can add a dashboard via this page either using the JSON configuration or
    by pasting in a public dashboard ID.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过此页面添加仪表板，既可以使用JSON配置，也可以粘贴公共仪表板ID。
- en: 'You can find public dashboards and their associated IDs at [https://grafana.com/grafana/dashboards/315](https://grafana.com/grafana/dashboards/315).
    Dashboard #315 is a great starter dashboard for Kubernetes – let''s add it to
    the textbox labeled **Grafana.com Dashboard** and click **Load**.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在[https://grafana.com/grafana/dashboards/315](https://grafana.com/grafana/dashboards/315)找到公共仪表板及其相关ID。仪表板#315是一个很好的Kubernetes入门仪表板—将其添加到标记为**Grafana.com仪表板**的文本框中，然后点击**加载**。
- en: 'Then, on the next page, select the **Prometheus** data source from the **Prometheus**
    option dropdown, which is used to pick between multiple data sources if available.
    Click **Import**, and the dashboard should be loaded, which will look like the
    following screenshot:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在下一页中，从**Prometheus**下拉菜单中选择**Prometheus**数据源，如果有多个数据源可用，可以通过该菜单进行选择。点击**导入**，仪表板应加载完成，显示如下截图：
- en: '![Figure 9.13 – Grafana dashboard](img/B14790_09_013_new.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – Grafana仪表板](img/B14790_09_013_new.jpg)'
- en: Figure 9.13 – Grafana dashboard
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – Grafana仪表板
- en: This particular Grafana dashboard provides a good high-level overview of network,
    memory, CPU, and filesystem utilization across the cluster, and it is broken down
    per Pod and container. It is configured with real-time graphs for **Network I/O
    pressure**, **Cluster memory usage**, **Cluster CPU usage**, and **Cluster filesystem
    usage** – though this last option may not be enabled depending on how you have
    installed Prometheus.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Grafana仪表板提供了关于网络、内存、CPU和文件系统利用率的整体概览，并按Pod和容器进行拆分。它配置了实时图表，显示**网络I/O压力**、**集群内存使用**、**集群CPU使用**和**集群文件系统使用**——不过，最后一个选项可能不会启用，这取决于你安装Prometheus的方式。
- en: Finally, let's look at the Alertmanager UI.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看Alertmanager UI。
- en: Using Alertmanager
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Alertmanager
- en: 'Alertmanager is an open source solution for managing alerts generated from
    Prometheus alerts. We installed Alertmanager previously as part of our stack –
    let''s take a look at what it can do:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager是一个开源解决方案，用于管理由Prometheus警报生成的警报。我们之前已经作为栈的一部分安装了Alertmanager——让我们看看它能做些什么：
- en: 'First, let''s `port-forward` the Alertmanager service using the following command:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用以下命令对Alertmanager服务进行`port-forward`：
- en: '[PRE22]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As usual, navigate to `localhost:3000` to see the UI as shown in the following
    screenshot. It looks similar to the Prometheus UI:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如常，访问`localhost:3000`查看UI，界面如下图所示。它与Prometheus UI类似：
- en: '![Figure 9.14 – Alertmanager UI](img/B14790_09_014_new.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – Alertmanager UI](img/B14790_09_014_new.jpg)'
- en: Figure 9.14 – Alertmanager UI
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – Alertmanager UI
- en: Alertmanager works together with Prometheus alerts. You can use the Prometheus
    server to specify alert rules, and then use Alertmanager to group similar alerts
    into single notifications, perform deduplications, and create *silences*, which
    are essentially a way to mute alerts if they match specific rules.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager与Prometheus警报配合使用。你可以使用Prometheus服务器来指定警报规则，然后使用Alertmanager将相似的警报归为一组，进行去重，并创建*静默*，这实际上是当警报符合特定规则时，静音警报的一种方式。
- en: Next, we will review a popular logging stack for Kubernetes – Elasticsearch,
    FluentD, and Kibana.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾一个流行的Kubernetes日志栈——Elasticsearch、FluentD和Kibana。
- en: Implementing the EFK stack on Kubernetes
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Kubernetes上实现EFK栈
- en: Similar to the popular ELK stack (Elasticsearch, Logstash, and Kibana), the
    EFK stack swaps out Logstash for the FluentD log forwarder, which is well supported
    on Kubernetes. Implementing this stack is easy and allows us to get started with
    log aggregation and search functionalities using purely open source tooling on
    Kubernetes.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于流行的ELK栈（Elasticsearch、Logstash和Kibana），EFK栈将Logstash替换为FluentD日志转发器，这在Kubernetes上得到了很好的支持。实现此栈非常简单，能够让我们在Kubernetes上使用纯开源工具快速启动日志聚合和搜索功能。
- en: Installing the EFK stack
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装EFK栈
- en: 'There are many ways to install the EFK Stack on Kubernetes, but the Kubernetes
    GitHub repository itself has some supported YAML, so let''s just use that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上安装EFK栈的方法有很多，但Kubernetes的GitHub存储库本身提供了一些支持的YAML配置，因此我们就使用这些：
- en: 'First, clone or download the Kubernetes repository using the following command:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令克隆或下载Kubernetes存储库：
- en: '[PRE23]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The manifests are located in the `kubernetes/cluster/addons` folder, specifically
    under `fluentd-elasticsearch`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清单文件位于`kubernetes/cluster/addons`文件夹中，具体在`fluentd-elasticsearch`下：
- en: '[PRE24]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For a production workload, we would likely make some changes to these manifests
    in order to properly customize the configuration for our cluster, but for the
    purposes of this tutorial we will leave everything as default. Let's start the
    process of bootstrapping our EFK stack.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于生产工作负载，我们可能需要对这些清单文件进行一些更改，以便根据我们的集群需求进行定制配置，但在本教程中，我们将保留所有默认设置。让我们开始启动EFK栈的过程。
- en: 'First, let''s create the Elasticsearch cluster itself. This runs as a StatefulSet
    on Kubernetes, and also provides a Service. To create the cluster, we need to
    run two `kubectl` commands:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建Elasticsearch集群。该集群作为Kubernetes上的StatefulSet运行，并提供一个服务。要创建集群，我们需要运行两个`kubectl`命令：
- en: '[PRE25]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Important note
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: A word of warning for the Elasticsearch StatefulSet – by default, the resource
    request for each Pod is 3 GB of memory, so if none of your Nodes have that available,
    you will not be able to deploy it as configured by default.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于Elasticsearch StatefulSet的一个警告——默认情况下，每个Pod的资源请求是3 GB内存，因此如果你的Node没有足够的内存，你将无法按默认配置部署它。
- en: Next, let's deploy the FluentD logging agents. These will run as a DaemonSet
    – one per Node – and forward logs from the Nodes to Elasticsearch. We also need
    to create the ConfigMap YAML, which contains the base FluentD agent configuration.
    This can be further customized to add things such as log filters and new sources.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们部署FluentD日志代理。这些代理将作为DaemonSet运行——每个Node一个——并将日志从Node转发到Elasticsearch。我们还需要创建ConfigMap
    YAML文件，其中包含FluentD代理的基础配置。此配置可以进一步自定义，添加例如日志过滤器和新数据源等内容。
- en: 'To install the DaemonSet for the agents and their configuration, run the following
    two `kubectl` commands:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装代理和其配置的DaemonSet，运行以下两个`kubectl`命令：
- en: '[PRE26]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now that we''ve created the ConfigMap and the FluentD DaemonSet, we can create
    our Kibana application, which is a GUI for interacting with Elasticsearch. This
    piece runs as a Deployment, with a Service. To deploy Kibana to our cluster, run
    the final two `kubectl` commands:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了ConfigMap和FluentD DaemonSet，可以创建Kibana应用程序，它是与Elasticsearch交互的GUI。这个组件作为Deployment运行，带有Service。要将Kibana部署到我们的集群中，运行最后两个`kubectl`命令：
- en: '[PRE27]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once everything has been initiated, which may take several minutes, we can
    access the Kibana UI in the same way that we did Prometheus and Grafana. To check
    the status of the resources we just created, we can run the following:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有内容初始化完成，这可能需要几分钟，我们可以像访问Prometheus和Grafana一样访问Kibana用户界面。要检查我们刚刚创建的资源状态，可以运行以下命令：
- en: '[PRE28]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once all Pods for FluentD, Elasticsearch, and Kibana are in the `addons` folder
    for more information.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有FluentD、Elasticsearch和Kibana的Pods都放在`addons`文件夹中，更多信息请参考。
- en: 'Once we''ve confirmed that our components are working properly, let''s use
    the `port-forward` command to access the Kibana UI. By the way, our EFK stack
    pieces will live in the `kube-system` namespace – so our command needs to reflect
    that. So, let''s use the following command:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦确认我们的组件正常工作，我们可以使用`port-forward`命令来访问Kibana用户界面。顺便提一下，我们的EFK堆栈组件将位于`kube-system`命名空间中——所以我们的命令需要反映这一点。那么，让我们使用以下命令：
- en: '[PRE29]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's check out the Kibana UI at `localhost:8080`. It should look something
    like the following, depending on your exact version and configuration:![Figure
    9.15 – Basic Kibana UI](img/B14790_09_015_new.jpg)
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在`localhost:8080`上查看Kibana用户界面。它应该看起来像下面的样子，具体取决于你的版本和配置：![图9.15 – 基本的Kibana用户界面](img/B14790_09_015_new.jpg)
- en: Figure 9.15 – Basic Kibana UI
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.15 – 基本的Kibana用户界面
- en: Kibana provides several different features for searching and visualizing logs,
    metrics, and more. The most important section of the dashboard for our purposes
    is **Logging**, since we are using Kibana solely as a log search UI in our example.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kibana提供了几种不同的功能，用于搜索和可视化日志、度量数据等。对于我们的目的来说，仪表板中最重要的部分是**日志**，因为我们在示例中仅将Kibana用作日志搜索UI。
- en: However, Kibana has many other functions, some of which are comparable to Grafana.
    For instance, it includes a full visualization engine, **application performance
    monitoring** (**APM**) capabilities, and Timelion, an expression engine for time
    series data very similar to what is found in Prometheus's PromQL. Kibana's metrics
    functionality is similar to Prometheus and Grafana.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，Kibana还有许多其他功能，其中一些与Grafana类似。例如，它包含一个完整的可视化引擎、**应用性能监控**（**APM**）功能和Timelion，这是一个类似于Prometheus的PromQL的时间序列数据表达引擎。Kibana的度量功能与Prometheus和Grafana相似。
- en: In order to get Kibana working, we will first need to specify an index pattern.
    To do this, click on the **Visualize** button, then click **Add an Index Pattern**.
    Select an option from the list of patterns and choose the index with the current
    date on it, then create the index pattern.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了让Kibana正常工作，我们首先需要指定一个索引模式。为此，点击**可视化**按钮，然后点击**添加索引模式**。从模式列表中选择一个选项，并选择当前日期的索引，然后创建索引模式。
- en: 'Now that we''re set up, the `h`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了，`h`：
- en: '![Figure 9.16 – Discover UI](img/B14790_09_016_new.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – Discover用户界面](img/B14790_09_016_new.jpg)'
- en: Figure 9.16 – Discover UI
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – Discover用户界面
- en: When Kibana cannot find any results, it gives you a handy set of possible solutions
    including query examples, as you can see in *Figure 9.13*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kibana无法找到任何结果时，它会提供一组便捷的解决方案，包括查询示例，正如你在*图9.13*中看到的那样。
- en: 'Now that you know how to create search queries, you can create visualizations
    from queries on the **Visualize** page. These can be chosen from a selection of
    visualization types including graphs, charts, and more, and then customized with
    specific queries as shown in the following screenshot:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何创建搜索查询后，你可以在**Visualize**页面根据查询创建可视化。这些可视化可以从多种可视化类型中选择，包括图形、图表等，并可以根据特定查询进行定制，如下图所示：
- en: '![Figure 9.17 – New visualization](img/B14790_09_017_new.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.17 – 新的可视化](img/B14790_09_017_new.jpg)'
- en: Figure 9.17 – New visualization
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – 新的可视化
- en: Next, these visualizations can be combined into dashboards. This works similarly
    to Grafana where multiple visualizations can be added to a dashboard, which can
    then be saved and reused.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这些可视化可以组合成仪表盘。这类似于Grafana，多个可视化可以添加到一个仪表盘中，然后保存并重复使用。
- en: 'You can also use the search bar to further filter your dashboard visualizations
    – pretty nifty! The following screenshot shows how a dashboard can be tied to
    a specific query:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用搜索栏进一步过滤仪表盘的可视化内容——非常方便！下图展示了如何将仪表盘与特定查询关联：
- en: '![Figure 9.18 – Dashboard UI](img/B14790_09_018_new.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – 仪表盘 UI](img/B14790_09_018_new.jpg)'
- en: Figure 9.18 – Dashboard UI
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 仪表盘 UI
- en: As you can see, a dashboard can be created for a specific query using the **Add**
    button.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，可以使用**添加**按钮为特定查询创建仪表盘。
- en: 'Next, Kibana provides a tool called *Timelion*, which is a time series visualization
    synthesis tool. Essentially, it allows you to combine separate data sources into
    a single visualization. Timelion is very powerful, but a full discussion of its
    feature set is outside the scope of this book. The following screenshot shows
    the Timelion UI – you may notice some similarities to Grafana, as these two sets
    of tools offer very similar capabilities:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Kibana提供了一个名为*Timelion*的工具，这是一个时间序列可视化合成工具。它本质上允许你将不同的数据源合并成一个单一的可视化图表。Timelion功能强大，但对其特性的全面讨论超出了本书的范围。下图展示了Timelion
    UI——你可能会注意到它与Grafana有一些相似之处，因为这两套工具提供了非常相似的功能：
- en: '![Figure 9.19 – Timelion UI](img/B14790_09_019_new.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – Timelion UI](img/B14790_09_019_new.jpg)'
- en: Figure 9.19 – Timelion UI
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – Timelion UI
- en: As you can see, in Timelion a query can be used to drive a real-time updating
    graph, just like in Grafana.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在Timelion中，查询可以用来驱动实时更新的图表，和Grafana一样。
- en: Additionally, though less relevant to this book, Kibana provides APM functionality,
    which requires some further setup, especially with Kubernetes. In this book we
    lean on Prometheus for this type of information while using the EFK stack to search
    logs from our applications.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管与本书关系较少，Kibana提供了APM功能，这需要进一步的配置，特别是在Kubernetes环境下。在本书中，我们依赖Prometheus来获取此类信息，同时使用EFK堆栈来搜索我们应用程序的日志。
- en: Now that we've covered Prometheus and Grafana for metrics and alerting, and
    the EFK stack for logging, only one piece of the observability puzzle is left.
    To solve this, we will use another excellent piece of open source software – Jaeger.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经介绍了用于度量和告警的Prometheus与Grafana，以及用于日志记录的EFK堆栈，只有可观察性拼图的最后一块还未讲解。为了解决这个问题，我们将使用另一个出色的开源软件——Jaeger。
- en: Implementing distributed tracing with Jaeger
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Jaeger实现分布式追踪
- en: Jaeger is an open source distributed tracing solution compatible with Kubernetes.
    Jaeger implements the OpenTracing specification, which is a set of standards for
    defining distributed traces.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger是一个开源的分布式追踪解决方案，兼容Kubernetes。Jaeger实现了OpenTracing规范，这是定义分布式追踪的一组标准。
- en: Jaeger exposes a UI for viewing traces and integrates with Prometheus. The official
    Jaeger documentation can be found at [https://www.jaegertracing.io/docs/](https://www.jaegertracing.io/docs/).
    Always check the docs for new information, since things may have changed since
    the publishing of this book.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger提供了一个用于查看追踪信息的UI，并与Prometheus集成。Jaeger的官方文档可以在[https://www.jaegertracing.io/docs/](https://www.jaegertracing.io/docs/)找到。由于自本书出版以来可能已有变化，建议始终查看文档以获取最新信息。
- en: Installing Jaeger using the Jaeger Operator
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Jaeger Operator安装Jaeger
- en: To install Jaeger, we are going to use the Jaeger Operator, which is the first
    operator that we've come across in this book. An *operator* in Kubernetes is simply
    a pattern for creating custom application controllers that speak Kubernetes's
    language. This means that instead of having to deploy all the various Kubernetes
    resources for an application, you can deploy a single Pod (or usually, single
    Deployment) and that application will talk to Kubernetes and spin up all the other
    required resources for you. It can even go further and self-operate the application,
    making resource changes when necessary. Operators can be highly complex, but they
    make it easier for us as end users to deploy commercial or open source software
    on our Kubernetes clusters.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装Jaeger，我们将使用Jaeger操作员，这是我们在本书中遇到的第一个操作员。在Kubernetes中，*操作员*只是创建自定义应用程序控制器的模式，这些控制器能与Kubernetes语言进行交互。这意味着，您不必部署应用程序所需的各种Kubernetes资源，而是可以部署一个单一的Pod（或通常是单一的Deployment），然后该应用程序将与Kubernetes进行通信，并为您启动所有其他所需的资源。操作员甚至可以进一步自我操作应用程序，在必要时进行资源更改。操作员可以非常复杂，但它们让我们作为终端用户更容易在Kubernetes集群上部署商业或开源软件。
- en: To get started with the Jaeger Operator, we need to create a few initial resources
    for Jaeger, and then the operator will do the rest. A prerequisite for this installation
    of Jaeger is that the `nginx-ingress` controller is installed on our cluster,
    since that is how we will access the Jaeger UI.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Jaeger操作员，我们需要为Jaeger创建一些初始资源，之后操作员将自动完成其余部分。安装Jaeger的先决条件是集群中必须安装`nginx-ingress`控制器，因为我们将通过它来访问Jaeger
    UI。
- en: 'First, we need to create a namespace for Jaeger to live in. We can get this
    via the `kubectl create namespace` command:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为Jaeger创建一个命名空间。我们可以通过`kubectl create namespace`命令来完成：
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that our namespace is created, we need to create some **CRDs** that Jaeger
    and the operator will use. We will discuss CRDs in depth in our chapter on extending
    Kubernetes, but for now, think of them as a way to co-opt the Kubernetes API to
    build custom functionality for applications. Using the following steps, let''s
    install Jaeger:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的命名空间已经创建，我们需要创建一些**CRDs**供Jaeger和操作员使用。我们将在后续关于扩展Kubernetes的章节中详细讨论CRDs，但现在可以将它们看作是利用Kubernetes
    API为应用程序构建自定义功能的一种方式。通过以下步骤，让我们安装Jaeger：
- en: 'To create the Jaeger CRDs, run the following command:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建Jaeger CRDs，请运行以下命令：
- en: '[PRE31]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With our CRDs created, the operator needs a few Roles and Bindings to be created
    in order to do its work.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们创建了CRDs之后，操作员需要创建一些角色和绑定才能开始工作。
- en: 'We want Jaeger to have cluster-wide permission in our cluster, so we will create
    some optional ClusterRoles and ClusterRoleBindings as well. To accomplish this,
    we run the following commands:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望Jaeger在集群中具有集群范围的权限，因此我们还将创建一些可选的ClusterRoles和ClusterRoleBindings。为此，我们运行以下命令：
- en: '[PRE32]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we finally have all the pieces necessary for our operator to work. Let''s
    install the operator with one last `kubectl` command:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们终于拥有了操作员所需的所有组件。让我们通过最后一个`kubectl`命令安装操作员：
- en: '[PRE33]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, check to see if the operator is running, using the following command:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下命令检查操作员是否正在运行：
- en: '[PRE34]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If the operator is running correctly, you will see something similar to the
    following output, with one available Pod for the deployment:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作员运行正常，你将看到类似以下的输出，并且会有一个可用的Pod用于部署：
- en: '![Figure 9.20 – Jaeger Operator Pod output](img/B14790_09_020_new.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20 – Jaeger 操作员 Pod 输出](img/B14790_09_020_new.jpg)'
- en: Figure 9.20 – Jaeger Operator Pod output
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 – Jaeger 操作员 Pod 输出
- en: We now have our Jaeger Operator up and running – but Jaeger itself isn't running.
    Why is this the case? Jaeger is a highly complex system and can run in different
    configurations, and the operator makes it easier to deploy these configurations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的Jaeger操作员已经启动并运行，但Jaeger本身还没有运行。这是为什么呢？Jaeger是一个非常复杂的系统，可以在不同的配置下运行，而操作员使得部署这些配置变得更加容易。
- en: The Jaeger Operator uses a CRD called `Jaeger` to read a configuration for your
    Jaeger instance, at which time the operator will deploy all the necessary Pods
    and other resources on Kubernetes.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger操作员使用一个名为`Jaeger`的CRD来读取Jaeger实例的配置，在此过程中，操作员将部署所有必要的Pod和其他Kubernetes资源。
- en: 'Jaeger can run in three main configurations: *AllInOne*, *Production*, and
    *Streaming*. A full discussion of these configurations is outside the scope of
    this book (check the Jaeger docs link shared previously), but we will be using
    the AllInOne configuration. This configuration combines the Jaeger UI, Collector,
    Agent, and Ingestor into a single Pod, without any persistent storage included.
    This is perfect for demo purposes – to see production-ready configurations, check
    the Jaeger docs.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 可以以三种主要配置运行：*AllInOne*、*Production* 和 *Streaming*。关于这些配置的详细讨论超出了本书的范围（请查看之前分享的
    Jaeger 文档链接），但我们将使用 AllInOne 配置。该配置将 Jaeger UI、Collector、Agent 和 Ingestor 合并到一个
    Pod 中，并不包括任何持久化存储。这非常适合演示用途 – 若要查看适用于生产环境的配置，请查看 Jaeger 文档。
- en: 'In order to create our Jaeger deployment, we need to tell the Jaeger Operator
    about our chosen configuration. We do that using the CRD that we created earlier
    – the Jaeger CRD. Create a new file for this CRD instance:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的 Jaeger 部署，我们需要将我们选择的配置告诉 Jaeger Operator。我们使用之前创建的 CRD – Jaeger CRD
    来实现这一点。为此 CRD 实例创建一个新文件：
- en: Jaeger-allinone.yaml
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger-allinone.yaml
- en: '[PRE35]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We are just using a small subset of the possible Jaeger type configurations
    – again, check the docs for the full story.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用了 Jaeger 配置类型中的一小部分 – 详细信息请查阅文档。
- en: 'Now, we can create our Jaeger instance by running the following:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行以下命令来创建我们的 Jaeger 实例：
- en: '[PRE36]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This command creates an instance of the Jaeger CRD we installed previously.
    At this point, the Jaeger Operator should realize that the CRD has been created.
    In less than a minute, our actual Jaeger Pod should be running. We can check for
    it by listing all the Pods in the observability namespace, with the following
    command:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建了我们之前安装的 Jaeger CRD 实例。在这一点上，Jaeger Operator 应该已经意识到 CRD 已被创建。不到一分钟，我们的实际
    Jaeger Pod 应该会启动。我们可以使用以下命令，通过列出可观察性命名空间中的所有 Pod 来检查它：
- en: '[PRE37]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As an output, you should see the newly created Jaeger Pod for our all-in-one
    instance:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应显示我们为全功能实例创建的新的 Jaeger Pod：
- en: '[PRE38]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The Jaeger Operator creates an Ingress record when we also have an Ingress controller
    running on our cluster. This means that we can simply list our Ingress entries
    using kubectl to see where to access the Jaeger UI.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在集群中运行 Ingress 控制器时，Jaeger Operator 会创建一个 Ingress 记录。这意味着我们可以简单地使用 kubectl
    列出我们的 Ingress 条目，以查看如何访问 Jaeger UI。
- en: 'You can list ingresses using this command:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令列出 ingress：
- en: '[PRE39]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output should show your new Ingress for the Jaeger UI as shown:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应显示您的新 Jaeger UI Ingress，如下所示：
- en: '![Figure 9.21 – Jaeger UI Service output](img/B14790_09_021_new.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.21 – Jaeger UI 服务输出](img/B14790_09_021_new.jpg)'
- en: Figure 9.21 – Jaeger UI Service output
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 – Jaeger UI 服务输出
- en: 'Now you can navigate to the address listed in your cluster''s Ingress record
    to see the Jaeger UI. It should look like the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以导航到集群的 Ingress 记录中列出的地址以查看 Jaeger UI。它应显示如下：
- en: '![Figure 9.22 – Jaeger UI](img/B14790_09_022_new.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – Jaeger UI](img/B14790_09_022_new.jpg)'
- en: Figure 9.22 – Jaeger UI
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – Jaeger UI
- en: As you can see, the Jaeger UI is pretty simple. There are three tabs at the
    top – **Search**, **Compare**, and **System Architecture**. We will focus on the
    **Search** tab, but for more information about the other two, check the Jaeger
    docs at [https://www.jaegertracing.io](https://www.jaegertracing.io).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Jaeger UI 非常简单。顶部有三个标签页 – **搜索**、**比较**和**系统架构**。我们将重点讨论**搜索**标签页，但有关其他两个标签页的更多信息，请查看
    Jaeger 文档：[https://www.jaegertracing.io](https://www.jaegertracing.io)。
- en: The Jaeger **Search** page lets us search for traces based on many inputs. We
    can search based on which Service is included in the trace, or based on tags,
    duration, or more. However, right now there's nothing in our Jaeger system.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 的**搜索**页面允许我们根据多种输入条件搜索 trace。我们可以基于 trace 中包含的 Service、标签、持续时间等进行搜索。然而，目前我们的
    Jaeger 系统中还没有任何数据。
- en: The reason for this is that even though we have Jaeger up and running, our apps
    still need to be configured to send traces to Jaeger. This usually needs to be
    done at the code or framework level and is out of the scope of this book. If you
    want to play around with Jaeger's tracing capabilities, a sample app is available
    to install – see the Jaeger docs page at [https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod](https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是，即使我们已经启动并运行了 Jaeger，我们的应用仍然需要配置为将追踪发送到 Jaeger。这通常需要在代码或框架层面完成，超出了本书的范围。如果你想体验
    Jaeger 的追踪功能，可以安装一个示例应用——请参见 Jaeger 文档页面 [https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod](https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod)。
- en: 'With services sending traces to Jaeger, it is possible to see traces. A trace
    in Jaeger looks like the following. We''ve cropped out some of the later parts
    of the trace for readability, but this should give you a good idea of what a trace
    can look like:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过服务将追踪信息发送到 Jaeger，可以看到追踪。Jaeger 中的追踪如下所示。我们已剪裁了追踪的后部分以提高可读性，但这应该能给你一个追踪的良好概念：
- en: '![Figure 9.23 – Trace view in Jaeger](img/B14790_09_023_new.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.23 – Jaeger 中的追踪视图](img/B14790_09_023_new.jpg)'
- en: Figure 9.23 – Trace view in Jaeger
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – Jaeger 中的追踪视图
- en: As you can see, the Jaeger UI view for a trace splits up service traces into
    constituent parts. Each service-to-service call, as well as any specific calls
    within the services themselves, have their own line in the trace. The horizontal
    bar chart you see moves from left to right with time, and each individual call
    in the trace has its own line. In this trace, you can see we have HTTP calls,
    SQL calls, as well as some Redis statements.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Jaeger UI 中的追踪视图将服务追踪分解为各个组成部分。每个服务到服务的调用，以及服务内部的任何特定调用，都在追踪中有独立的行。你看到的横向条形图是随着时间从左到右移动的，每个追踪中的独立调用都有自己的行。在这个追踪中，你可以看到我们有
    HTTP 调用、SQL 调用，以及一些 Redis 语句。
- en: You should be able to see how Jaeger and tracing in general can help developers
    make sense of a web of service-to-service calls and can help find bottlenecks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到 Jaeger 和追踪功能如何帮助开发者理解服务到服务调用的网络，并帮助找到瓶颈。
- en: With that review of Jaeger, we have a fully open source solution to every problem
    in the observability bucket. However, that does not mean that there is no use
    case where a commercial solution makes sense – in many cases it does.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对 Jaeger 的回顾，我们拥有一个完全开源的解决方案来应对可观察性领域的所有问题。然而，这并不意味着在某些情况下商业解决方案不合适——在许多情况下，商业解决方案确实有意义。
- en: Third-party tooling
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三方工具
- en: In addition to many open source libraries, there are many commercially available
    products for metrics, logging, and alerting on Kubernetes. Some of these can be
    much more powerful than the open source options.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 除了许多开源库外，还有许多商业化的产品用于 Kubernetes 上的度量、日志记录和告警。其中一些产品的功能可能比开源选项更强大。
- en: Generally, most tooling in metrics and logging will require you to provision
    resources on your cluster to forward metrics and logs to your service of choice.
    In the examples we've used in this chapter, these services are running in the
    cluster, though in commercial products these can often be separate SaaS applications
    where you log on to analyze your logs and see your metrics. For instance, with
    the EFK stack we provisioned in this chapter, you can pay Elastic for a hosted
    solution where the Elasticsearch and Kibana pieces of the solution would be hosted
    on Elastic's infrastructure, reducing complexity in the solution. There are also
    many other solutions in this space, from vendors including Sumo Logic, Logz.io,
    New Relic, DataDog, and AppDynamics.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，度量和日志工具需要你在集群中配置资源，以便将度量和日志转发到你选择的服务中。在本章中我们使用的示例中，这些服务运行在集群中，尽管在商业产品中，它们通常是分开的
    SaaS 应用，你可以登录分析日志并查看度量。例如，在本章中我们配置的 EFK 堆栈，你可以付费使用 Elastic 提供的托管解决方案，其中 Elasticsearch
    和 Kibana 组件将托管在 Elastic 的基础设施上，从而减少解决方案的复杂性。这个领域还有许多其他解决方案，来自包括 Sumo Logic、Logz.io、New
    Relic、DataDog 和 AppDynamics 等供应商。
- en: For a production environment, it is common to use separate compute (either a
    separate cluster, service, or SaaS tool) to perform log and metric analytics.
    This ensures that the cluster running your actual software can be dedicated to
    the application alone, and any costly log searching or querying functionality
    can be handled separately. It also means that if our application cluster goes
    down, we can still view logs and metrics up until the point of the failure.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，通常会使用独立的计算资源（无论是独立的集群、服务还是SaaS工具）来进行日志和指标分析。这可以确保运行实际软件的集群可以专门用于应用程序，而任何昂贵的日志搜索或查询功能都可以单独处理。这样，如果我们的应用程序集群发生故障，我们仍然可以查看故障发生之前的日志和指标。
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about observability on Kubernetes. We first learned
    about the four major tenets of observability: metrics, logging, traces, and alerts.
    Then we discovered how Kubernetes itself provides tooling for observability, including
    how it manages logs and resource metrics and how to deploy Kubernetes Dashboard.
    Finally, we learned how to implement and use some key open source tools to provide
    visualization, searching, and alerting for the four pillars. This knowledge will
    help you build robust observability infrastructure for your future Kubernetes
    clusters and help you decide what is most important to observe in your cluster.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了Kubernetes中的可观测性。我们首先学习了可观测性的四个主要支柱：指标、日志、追踪和警报。然后我们发现Kubernetes本身提供了可观测性工具，包括如何管理日志和资源指标以及如何部署Kubernetes
    Dashboard。最后，我们学习了如何实施和使用一些关键的开源工具来提供这四个支柱的可视化、搜索和警报功能。这些知识将帮助你为未来的Kubernetes集群构建强大的可观测性基础设施，并帮助你决定在集群中最需要观察的内容。
- en: In the next chapter, we will use what we learned about observability to help
    us troubleshoot applications on Kubernetes.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将利用我们学到的可观测性知识来帮助我们排查Kubernetes上的应用程序问题。
- en: Questions
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Explain the difference between metrics and logs.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释指标和日志之间的区别。
- en: Why would you use Grafana instead of simply using the Prometheus UI?
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么你会选择使用Grafana，而不是仅仅使用Prometheus的UI？
- en: When running an EFK stack in production (so as to keep as much compute off the
    production app cluster as possible), which piece(s) of the stack would run on
    the production app cluster? And which piece(s) would run off the cluster?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产环境中运行EFK堆栈时（为了尽可能将计算负载从生产应用集群中分离出来），堆栈的哪些组件会运行在生产应用集群上？哪些组件会在集群外运行？
- en: Further reading
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In-depth review of Kibana Timelion: [https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html](https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana Timelion的深入回顾：[https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html](https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html)
