- en: '*Chapter 14*: Service Meshes and Serverless'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：服务网格与无服务器架构'
- en: This chapter discusses advanced Kubernetes patterns. First, it details the in-vogue
    service mesh pattern, where observability and service-to-service discovery are
    handled by a sidecar proxy, as well as a guide to setting up Istio, a popular
    service mesh. Lastly, it describes the serverless pattern and how it can be applied
    in Kubernetes. The major case study in this chapter will include setting up Istio
    for an example application and service discovery, along with Istio ingress gateways.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了高级 Kubernetes 模式。首先，它详细介绍了当前流行的服务网格模式，其中可观察性和服务到服务的发现由边车代理处理，并提供了设置流行服务网格
    Istio 的指南。最后，它描述了无服务器模式及其如何在 Kubernetes 中应用。本章的主要案例研究将包括为示例应用程序设置 Istio 和服务发现，以及
    Istio 入口网关的配置。
- en: Let's start with a discussion of the sidecar proxy, which builds the foundation
    of service-to-service connectivity for service meshes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从边车代理的讨论开始，它为服务网格提供了服务到服务连接的基础。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Using sidecar proxies
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用边车代理
- en: Adding a service mesh to Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向 Kubernetes 添加服务网格
- en: Implementing serverless on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上实现无服务器架构
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool, along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持 `kubectl` 命令行工具的计算机，并且需要一个正在运行的 Kubernetes 集群。请参阅 [*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与
    Kubernetes 通信*，了解如何快速启动并运行 Kubernetes，并查看如何安装 `kubectl` 工具的说明。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在本书的 GitHub 仓库中找到，网址是 [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14)。
- en: Using sidecar proxies
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用边车代理
- en: As we mentioned earlier in this book, a sidecar is a pattern where a Pod contains
    another container in addition to the actual application container to be run. This
    additional "extra" container is the sidecar. Sidecars can be used for a number
    of different reasons. Some of the most popular uses for sidecars are monitoring,
    logging, and proxying.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如本书前面提到的，边车是一种模式，在这种模式中，一个 Pod 除了运行实际应用容器外，还包含另一个容器。这个额外的容器就是边车。边车有多种用途，其中最常见的用途包括监控、日志记录和代理。
- en: For logging, a sidecar container can fetch application logs from the application
    container (since they can share volumes and communicate on localhost), before
    sending the logs to a centralized logging stack, or parsing them for the purpose
    of alerting. It's a similar story for monitoring, where the sidecar Pod can track
    and send metrics about the application Pod.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日志记录，边车容器可以从应用容器中获取日志（因为它们可以共享卷并在本地主机上进行通信），然后将日志发送到集中式日志堆栈，或解析日志以便进行警报。监控也是类似的情况，边车
    Pod 可以跟踪并发送关于应用 Pod 的度量数据。
- en: With a sidecar proxy, when requests come into the Pod, they first go to the
    proxy container, which then routes requests (after logging or performing other
    filtering) to the application container. Similarly, when requests leave the application
    container, they first go to the proxy, which can provide routing out of the Pod.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用边车代理时，当请求进入 Pod 时，它们首先会进入代理容器，代理容器会在记录日志或进行其他过滤后，将请求路由到应用容器。同样，当请求离开应用容器时，它们首先会进入代理，代理可以提供从
    Pod 外部的路由。
- en: Normally, proxy sidecars such as NGINX only provide proxying for requests coming
    into a Pod. However, in the service mesh pattern, both requests coming into and
    leaving the Pod go through the proxy, which provides the foundation for the service
    mesh pattern itself.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，像 NGINX 这样的代理边车仅为进入 Pod 的请求提供代理服务。然而，在服务网格模式中，进入和离开 Pod 的请求都必须经过代理，这为服务网格模式本身提供了基础。
- en: 'Refer to the following diagram to see how a sidecar proxy can interact with
    an application container:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图表，了解边车代理如何与应用容器进行交互：
- en: '![Figure 14.1 – Proxy sidecar](img/B14790_14_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – 代理边车](img/B14790_14_001.jpg)'
- en: Figure 14.1 – Proxy sidecar
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – 代理边车
- en: As you can see, the sidecar proxy is in charge of routing requests to and from
    the application container in the Pod, allowing for functionality such as service
    routing, logging, and filtering.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，边车代理负责将请求路由到Pod中的应用容器并返回，支持诸如服务路由、日志记录和过滤等功能。
- en: The sidecar proxy pattern is an alternative to a DaemonSet-based proxy, where
    a proxy Pod on each node handles proxying to other Pods on that node. The Kubernetes
    proxy itself is similar to a DaemonSet pattern. Using a sidecar proxy can provide
    more flexibility than using a DaemonSet proxy, at the expense of performance efficiency,
    since many extra containers need to be run.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 边车代理模式是基于DaemonSet的代理的一种替代方案，其中每个节点上的代理Pod负责将请求代理到该节点上的其他Pod。Kubernetes代理本身类似于DaemonSet模式。使用边车代理比使用DaemonSet代理提供了更多的灵活性，但以性能效率为代价，因为需要运行许多额外的容器。
- en: 'Some popular proxy options for Kubernetes include the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的Kubernetes代理选项包括：
- en: '*NGINX*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NGINX*'
- en: '*HAProxy*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HAProxy*'
- en: '*Envoy*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Envoy*'
- en: While NGINX and HAProxy are more traditional proxies, Envoy was built specifically
    for a distributed, cloud-native environment. For this reason, Envoy forms the
    core of popular service meshes and API gateways built for Kubernetes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然NGINX和HAProxy是更传统的代理，Envoy则是专门为分布式云原生环境构建的。因此，Envoy成为了构建Kubernetes服务网格和API网关的核心。
- en: Before we get to Envoy, let's discuss the installation of other proxies as sidecars.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论Envoy之前，先讨论其他代理作为边车的安装。
- en: Using NGINX as a sidecar reverse proxy
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NGINX作为边车反向代理
- en: Before we specify how NGINX can be used as a sidecar proxy, it is relevant to
    note that in an upcoming Kubernetes release, the sidecar will be a Kubernetes
    resource type that will allow easy injection of sidecar containers to large numbers
    of Pods. Currently however, sidecar containers must be specified at the Pod or
    controller (ReplicaSet, Deployment, and others) level.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们指定NGINX如何作为边车代理使用之前，值得注意的是，在即将发布的Kubernetes版本中，边车将成为Kubernetes资源类型，允许将边车容器轻松注入到大量Pod中。然而，目前，边车容器必须在Pod或控制器（ReplicaSet、Deployment等）级别指定。
- en: Let's take a look at how we can configure NGINX as a sidecar, with the following
    Deployment YAML, which we will not create just yet. This process is a bit more
    manual than using the NGINX Ingress Controller.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何将NGINX配置为边车，下面的Deployment YAML我们暂时还不创建。这个过程比使用NGINX Ingress Controller稍微更手动一些。
- en: 'We''ve split the YAML into two parts for space reasons and trimmed some of
    the fat, but you can see it in its entirety in the code repository. Let''s start
    with the containers spec for our deployment:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们将YAML分成了两部分，并去掉了一些冗余部分，但你可以在代码库中看到完整内容。我们从部署的容器规格开始：
- en: 'Nginx-sidecar.yaml:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx-sidecar.yaml：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, we specify two containers, both our main app container, `myapp`,
    and the `nginx` sidecar, where we inject some configuration via volume mounts,
    as well as some TLS certificates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们指定了两个容器，一个是我们的主应用容器`myapp`，另一个是`nginx`边车容器，在这里我们通过卷挂载注入了一些配置，以及一些TLS证书。
- en: 'Next, let''s look at the `volumes` spec in the same file, where we inject some
    certs (from a secret) and `config` (from a `ConfigMap`):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下同一文件中的`volumes`规格，在这里我们注入了一些证书（来自secret）和`config`（来自`ConfigMap`）：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we need both a cert and a secret key.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们需要一个证书和一个密钥。
- en: 'Next, we need to create the NGINX configuration using `ConfigMap`. The NGINX
    configuration looks like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用`ConfigMap`创建NGINX配置。NGINX配置如下所示：
- en: 'nginx.conf:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: nginx.conf：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, we have some basic NGINX configuration. Importantly, we have
    the `proxy_pass` field, which proxies requests to a port on `127.0.0.1`, or localhost.
    Since containers in a Pod can share localhost ports, this acts as our sidecar
    proxy. We won't review all the other lines for the purposes of this book, but
    check the NGINX docs for more information about what each line means ([https://nginx.org/en/docs/](https://nginx.org/en/docs/)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有一些基本的NGINX配置。重要的是，我们有`proxy_pass`字段，它将请求代理到`127.0.0.1`或本地主机的某个端口。由于Pod中的容器可以共享本地主机端口，这就充当了我们的边车代理。为了本书的目的，我们不再逐一审查其他行，但你可以查看NGINX文档，了解每一行的含义（[https://nginx.org/en/docs/](https://nginx.org/en/docs/)）。
- en: 'Now, let''s create the `ConfigMap` from this file. Use the following command
    to imperatively create the `ConfigMap`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们根据这个文件创建`ConfigMap`。使用以下命令直接创建`ConfigMap`：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will result in the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, let's make our certificates for TLS in NGINX, and embed them in a Kubernetes
    secret. You will need the CFSSL (CloudFlare's PKI/TLS open source toolkit) library
    installed to follow these instructions, but you can use any other method to create
    your cert.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为 NGINX 创建 TLS 证书，并将它们嵌入到 Kubernetes 密钥中。你需要安装 CFSSL（CloudFlare 的 PKI/TLS
    开源工具包）库来执行这些步骤，但你也可以使用其他任何方法来创建证书。
- en: 'First, we need to create the **Certificate Authority** (**CA**). Start with
    the JSON configuration for the CA:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建 **证书授权中心**（**CA**）。从 CA 的 JSON 配置开始：
- en: 'nginxca.json:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: nginxca.json：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, use CFSSL to create the CA certificate:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 CFSSL 创建 CA 证书：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will require the CA config:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要 CA 配置：
- en: 'Nginxca-config.json:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Nginxca-config.json：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And we''ll also need a cert request config:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个证书请求配置：
- en: 'Nginxcarequest.json:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Nginxcarequest.json：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can actually make our certs! Use the following command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实际生成我们的证书了！使用以下命令：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As the final step for our cert secrets, create the Kubernetes secret from the
    certificate files'' output by means of the last `cfssl` command:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们证书机密的最后一步，使用最后一个 `cfssl` 命令，从证书文件的输出创建 Kubernetes 密钥：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can finally create our deployment:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以创建我们的部署了：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This produces the following output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In order to check the NGINX proxy functionality, let''s create a service to
    direct to our deployment:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查 NGINX 代理功能，让我们创建一个服务来指向我们的部署：
- en: 'Nginx-sidecar-service.yaml:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx-sidecar-service.yaml：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, accessing any node of the cluster using `https` should result in a working
    HTTPS connection! However, since our cert is self-signed, browsers will display
    an *insecure* message.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 `https` 访问集群的任何节点应该能够建立有效的 HTTPS 连接！然而，由于我们的证书是自签名的，浏览器会显示 *不安全* 的提示信息。
- en: Now that you've seen how NGINX can be used as a sidecar proxy with Kubernetes,
    let's move on to a more modern, cloud-native proxy sidecar – Envoy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何将 NGINX 用作 Kubernetes 的侧车代理，让我们继续探索一个更现代的云原生代理侧车——Envoy。
- en: Using Envoy as a sidecar proxy
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Envoy 作为侧车代理
- en: Envoy is a modern proxy built for cloud-native environments. In the Istio service
    mesh, which we'll review later in this chapter, Envoy acts as both a reverse and
    forward proxy. Before we get to Istio, however, let's try our hand at deploying
    Envoy as a proxy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 是为云原生环境构建的现代代理。在本章后面我们将回顾的 Istio 服务网格中，Envoy 既充当反向代理，也充当正向代理。然而，在我们讨论
    Istio 之前，让我们尝试将 Envoy 部署为代理。
- en: We will tell Envoy where to route various requests using routes, listeners,
    clusters, and endpoints. This functionality is what forms the core of Istio, which
    we will review later in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过路由、监听器、集群和端点告诉 Envoy 如何路由各种请求。这一功能构成了 Istio 的核心，我们将在本章后面回顾它。
- en: Let's go through each of the Envoy configuration pieces to see how it all works.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解每个 Envoy 配置项，看看它是如何工作的。
- en: Envoy listeners
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy 监听器
- en: Envoy allows the configuration of one or more listeners. With each listener,
    we specify a port for Envoy to listen on, as well as any filters we want to apply
    to the listener.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 允许配置一个或多个监听器。对于每个监听器，我们指定 Envoy 监听的端口，以及我们希望应用于监听器的任何过滤器。
- en: Filters can provide complex functionality, including caching, authorization,
    **Cross-Origin Resource Sharing** (**CORS**) configuration, and more. Envoy supports
    the chaining of multiple filters together.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器可以提供复杂的功能，包括缓存、授权、**跨源资源共享**（**CORS**）配置等。Envoy 支持多个过滤器的链式配置。
- en: Envoy routes
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy 路由
- en: Certain filters have route configuration, which specifies domains from which
    requests should be accepted, route matching, and forwarding rules.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 某些过滤器具有路由配置，它指定了应接受请求的域、路由匹配和转发规则。
- en: Envoy clusters
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy 集群
- en: A Cluster in Envoy represents a logical service where requests can be routed
    to based-on routes in listeners. A cluster likely contains more than one possible
    IP address in a cloud-native setting, so it supports load balancing configurations
    such as *round robin*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Envoy 中，集群表示一个逻辑服务，请求可以根据监听器中的路由路由到该服务。集群可能包含多个 IP 地址，尤其在云原生环境中，因此它支持如 *轮询*
    等负载均衡配置。
- en: Envoy endpoints
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy 端点
- en: Finally, endpoints are specified within a cluster as one logical instance of
    a service. Envoy supports fetching a list of endpoints from an API (this is essentially
    what happens in the Istio service mesh) and load balancing between them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，端点被指定为集群内的服务逻辑实例。Envoy 支持从 API 获取端点列表（这本质上是 Istio 服务网格中的操作），并在它们之间进行负载均衡。
- en: In a production Envoy deployment on Kubernetes, it is likely that some form
    of dynamic, API-driven Envoy configuration is going to be used. This feature of
    Envoy is called xDS, and is used by Istio. Additionally, there are other open
    source products and solutions that use Envoy along with xDS, including the Ambassador
    API gateway.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上的生产环境部署中，可能会使用某种形式的动态、API 驱动的 Envoy 配置。这个功能叫做 xDS，并且被 Istio 所使用。此外，还有其他开源产品和解决方案也使用
    Envoy 配合 xDS，包括 Ambassador API 网关。
- en: For the purposes of this book, we will look at some static (non-dynamic) Envoy
    configuration; that way, we can pick apart each piece of the config, and you'll
    have a good idea of how everything works when we review Istio.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目的下，我们将查看一些静态（非动态）Envoy 配置；这样我们可以逐个分析配置的每一部分，并且当我们回顾 Istio 时，你将对一切工作原理有清晰的了解。
- en: 'Let''s now dive into an Envoy configuration for a setup where a single Pod
    needs to be able to route requests to two services, *Service 1* and *Service 2*.
    The setup looks like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解一个 Envoy 配置，其中单个 Pod 需要能够将请求路由到两个服务，*Service 1* 和 *Service 2*。该配置如下所示：
- en: '![Figure 14.2 – Outbound envoy proxy](img/B14790_14_002.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2 – 出站 Envoy 代理](img/B14790_14_002.jpg)'
- en: Figure 14.2 – Outbound envoy proxy
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 – 出站 Envoy 代理
- en: As you can see, the Envoy sidecar in our application Pod will have configurations
    to route to two upstream services, *Service 1* and *Service 2*. Both services
    have two possible endpoints.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，应用 Pod 中的 Envoy sidecar 将有配置来路由到两个上游服务，*Service 1* 和 *Service 2*。这两个服务各自有两个可能的端点。
- en: In a dynamic setting with Envoy xDS, the Pod IPs for the endpoints would be
    loaded from the API, but for the purposes of our review, we will show the static
    Pod IPs in the endpoints. We will completely ignore Kubernetes Services and instead
    directly access Pod IPs in a round robin configuration. In a service mesh scenario,
    Envoy would also be deployed on all of the destination Pods, but we'll keep it
    simple for now.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个动态设置中，Envoy xDS 会从 API 加载端点的 Pod IP，但为了方便我们的审查，我们将在端点中展示静态的 Pod IP。我们将完全忽略
    Kubernetes 服务，而是直接在轮询配置中访问 Pod IP。在服务网格场景中，Envoy 也会部署在所有目标 Pod 上，但现在我们保持简单。
- en: Now, let's look at how this network map is configured in an envoy configuration
    YAML (which you can find in its entirety in the code repository). This is, of
    course, very different from a Kubernetes resource YAML – we will get to that part
    later. The entire configuration has a lot of YAML involved, so let's take it piece
    by piece.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个网络映射是如何在 Envoy 配置 YAML 中配置的（你可以在代码库中找到完整的配置）。这当然与 Kubernetes 资源 YAML
    非常不同——我们稍后会讲到这部分。整个配置包含大量的 YAML 内容，所以我们将逐步解析它。
- en: Understanding Envoy configuration files
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Envoy 配置文件
- en: 'First off, let''s look at the first few lines of our config—some basic information
    about our Envoy setup:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下配置的前几行——一些关于我们 Envoy 设置的基本信息：
- en: 'Envoy-configuration.yaml:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy-configuration.yaml：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see, we specify a port and address for Envoy''s `admin`. As with
    the following configuration, we are running Envoy as a sidecar so the address
    will always be local – `0.0.0.0`. Next, we start our list of listeners with an
    HTTPS listener:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们为 Envoy 的 `admin` 指定了一个端口和地址。与以下配置一样，我们将 Envoy 作为 sidecar 运行，因此地址将始终是本地的——`0.0.0.0`。接下来，我们从一个
    HTTPS 监听器开始列出监听器：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, for each Envoy listener, we have a local address and port for
    the listener (this listener is an HTTPS listener). Then, we have a list of filters
    – though in this case, we only have one. Each envoy filter type has slightly different
    configuration, and we won''t review it line by line (check the Envoy docs for
    more information at [https://www.envoyproxy.io/docs](https://www.envoyproxy.io/docs)),
    but this particular filter matches two routes, `/service/1` and `/service/2`,
    and routes them to two envoy clusters. Still under our first HTTPS listener section
    of the YAML, we have the TLS configuration, including certs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于每个 Envoy 监听器，我们为监听器指定了一个本地地址和端口（此监听器是一个 HTTPS 监听器）。然后，我们列出了一个过滤器列表——不过在这种情况下，我们只有一个。每个
    Envoy 过滤器类型的配置稍有不同，我们不会逐行审查（请查阅 Envoy 文档获取更多信息：[https://www.envoyproxy.io/docs](https://www.envoyproxy.io/docs)），但这个特定的过滤器匹配两个路由，`/service/1`
    和 `/service/2`，并将它们路由到两个 Envoy 集群。仍然在 YAML 中的第一个 HTTPS 监听器部分，我们有 TLS 配置，包括证书：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, this configuration passes in a `private_key` and a `certificate_chain`.
    Next, we have our second and final listener, an HTTP listener:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个配置传递了一个 `private_key` 和一个 `certificate_chain`。接下来，我们有第二个也是最后一个监听器，一个
    HTTP 监听器：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see, this configuration is quite similar to that of our HTTPS listener,
    except that it listens on a different port, and does not include certificate information.
    Next, we move into our cluster configuration. In our case, we have two clusters,
    one for `service1` and one for `service2`. First off, `service1`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个配置与我们的 HTTPS 监听器配置非常相似，只是它监听的是不同的端口，并且没有包含证书信息。接下来，我们进入集群配置。在我们的案例中，我们有两个集群，一个用于
    `service1`，另一个用于 `service2`。首先是 `service1`：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And next, `Service 2`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `Service 2`：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For each of these clusters, we specify where requests should be routed, and
    to which port. For instance, for our first cluster, requests are routed to `http://service1:5000`.
    We also specify a load balancing policy (in this case, round robin) and a timeout
    for the connections. Now that we have our Envoy configuration, we can go ahead
    and create our Kubernetes Pod and inject our sidecar along with the envoy configuration.
    We''ll also split this file into two since it is a bit too big to understand as
    is:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个集群，我们指定请求应该路由到哪里，以及路由到哪个端口。例如，对于我们的第一个集群，请求将路由到 `http://service1:5000`。我们还指定了一个负载均衡策略（在本例中是轮询）以及连接的超时。现在我们有了
    Envoy 配置，可以继续创建我们的 Kubernetes Pod，并将我们的 sidecar 与 Envoy 配置注入。由于文件太大，我们将把它拆分成两个文件：
- en: 'Envoy-sidecar-deployment.yaml:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'Envoy-sidecar-deployment.yaml:'
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As you can see, this is a typical deployment YAML. In this case, we actually
    have two containers. First off is the Envoy proxy container (or sidecar). It listens
    on two ports. Next up, moving further down the YAML, we have a volume mount for
    that first container (to hold the Envoy config) as well as a start command and
    arguments:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这是一个典型的部署 YAML 文件。在这个文件中，实际上我们有两个容器。第一个是 Envoy 代理容器（或 sidecar）。它监听两个端口。接下来，在
    YAML 文件的更下方，我们有第一个容器的卷挂载（用于保存 Envoy 配置），以及启动命令和参数：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we have our second container in the Pod, which is an application container:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在 Pod 中有第二个容器，这是一个应用容器：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you can see, this application responds on port `5000`. Lastly, we also have
    our Pod-level volume definition to match the Envoy config volume mounted in the
    Envoy container. Before we create our deployment, we need to create a `ConfigMap`
    with our Envoy configuration. We can do this using the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个应用在 `5000` 端口上响应。最后，我们还定义了 Pod 级别的卷，以匹配 Envoy 容器中挂载的 Envoy 配置卷。在创建部署之前，我们需要创建一个包含
    Envoy 配置的 `ConfigMap`。我们可以使用以下命令来完成：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will result in the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can create our deployment with the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令创建我们的部署：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will result in the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we need our downstream services, `service1` and `service2`. For this
    purpose, we will continue to use the `http-responder` open source container image,
    which will respond on port `5000`. The deployment and service specs can be found
    in the code repository, and we can create them using the following commands:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要我们的下游服务，`service1` 和 `service2`。为此，我们将继续使用开源的 `http-responder` 容器镜像，它将在
    `5000` 端口上响应。部署和服务规格可以在代码仓库中找到，我们可以使用以下命令来创建它们：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we can test our Envoy configuration! From our `my-service` container,
    we can make a request to localhost on port `8080`, with the `/service1` path.
    This should direct to one of our `service1` Pod IPs. To make this request we use
    the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以测试我们的 Envoy 配置！在我们的 `my-service` 容器中，我们可以向本地主机的 `8080` 端口发送请求，路径为 `/service1`。这应该会指向我们
    `service1` Pod 的一个 IP 地址。为了发出这个请求，我们使用以下命令：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ve set up out services to echo their names on a `curl` request. Look at
    the following output of our `curl` command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置好服务，让它们在 `curl` 请求时回显它们的名称。看看下面我们 `curl` 命令的输出：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we've looked at how Envoy works with a static configuration, let's
    move on to a dynamic service mesh based on Envoy – Istio.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Envoy 如何与静态配置一起工作，接下来让我们来看看基于 Envoy 的动态服务网格——Istio。
- en: Adding a service mesh to Kubernetes
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向 Kubernetes 添加服务网格
- en: A *service mesh* pattern is a logical extension of the sidecar proxy. By attaching
    sidecar proxies to every Pod, a service mesh can control functionality for service-to-service
    requests, such as advanced routing rules, retries, and timeouts. In addition,
    by having every request pass through a proxy, service meshes can implement mutual
    TLS encryption between services for added security and can give administrators
    incredible observability into requests in their cluster.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务网格*模式是sidecar代理的逻辑扩展。通过将sidecar代理附加到每个Pod，服务网格可以控制服务间请求的功能，如高级路由规则、重试和超时。此外，通过让每个请求都经过代理，服务网格可以实现服务间的双向TLS加密，增强安全性，并为管理员提供对集群中请求的极高可观测性。'
- en: 'There are several service mesh projects that support Kubernetes. The most popular
    are as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个服务网格项目支持Kubernetes。最受欢迎的如下：
- en: '*Istio*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Istio*'
- en: '*Linkerd*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Linkerd*'
- en: '*Kuma*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kuma*'
- en: '*Consul*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Consul*'
- en: Each of these service meshes has different takes on the service mesh pattern.
    *Istio* is likely the single most popular and comprehensive solution, but is also
    quite complex. *Linkerd* is also a mature project, but is easier to configure
    (though it uses its own proxy instead of Envoy). *Consul* is an option that supports
    Envoy in addition to other providers, and not just on Kubernetes. Finally, *Kuma*
    is an Envoy-based option that is also growing in popularity.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务网格对服务网格模式有不同的看法。*Istio*可能是最受欢迎和最全面的解决方案，但它也相当复杂。*Linkerd*也是一个成熟的项目，但配置更简单（尽管它使用自己的代理，而不是Envoy）。*Consul*是一个支持Envoy及其他提供商的选项，不仅仅在Kubernetes上可用。最后，*Kuma*是一个基于Envoy的选项，且在不断增长的流行度中。
- en: Exploring all the options is beyond the scope of this book, so we will stick
    with Istio, as it is often considered the default solution. That said, all of
    these meshes have strengths and weaknesses, and it is worth looking at each one
    when planning to adopt the service mesh.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 探索所有选项超出了本书的范围，因此我们将坚持使用Istio，因为它通常被认为是默认的解决方案。尽管如此，这些网格都有优缺点，计划采用服务网格时值得逐一了解。
- en: Setting up Istio on Kubernetes
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Kubernetes上设置Istio
- en: Although Istio can be installed with Helm, the Helm installation option is no
    longer the officially supported installation method.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Istio可以通过Helm安装，但Helm安装选项不再是官方支持的安装方法。
- en: 'Instead, we use the `Istioctl` CLI tool to install Istio with configuration
    onto our clusters. This configuration can be completely customized, but for the
    purposes of this book, we will just use the "demo" configuration:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用`Istioctl` CLI工具将Istio及其配置安装到我们的集群中。此配置可以完全自定义，但为了本书的目的，我们将使用“demo”配置：
- en: 'The first step to installing Istio on a cluster is to install the Istio CLI
    tool. We can do this with the following command, which installs the newest version
    of the CLI tool:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群上安装Istio的第一步是安装Istio CLI工具。我们可以使用以下命令来安装CLI工具的最新版本：
- en: '[PRE30]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we''ll want to add the CLI tool to our path for ease of use:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将CLI工具添加到路径中，以方便使用：
- en: '[PRE31]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, let's install Istio! Istio configurations are called *profiles* and, as
    mentioned previously, they can be completely customized using a YAML file.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们安装Istio！Istio的配置被称为*配置文件*，正如前面提到的，它们可以通过YAML文件完全自定义。
- en: 'For this demonstration, we''ll use the inbuilt `demo` profile with Istio, which
    provides some basic setup. Install profile using the following command:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用Istio的内置`demo`配置文件，它提供了一些基本的设置。使用以下命令安装配置文件：
- en: '[PRE32]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will result in the following output:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 14.3 – Istioctl profile installation output](img/B14790_14_003.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图14.3 – Istioctl配置文件安装输出](img/B14790_14_003.jpg)'
- en: Figure 14.3 – Istioctl profile installation output
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14.3 – Istioctl配置文件安装输出
- en: Since the sidecar resource has not been released yet as of Kubernetes 1.19,
    Istio will itself inject Envoy proxies into any namespace that is labeled with
    `istio-injection=enabled`.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于sidecar资源在Kubernetes 1.19版本中尚未发布，Istio将自动将Envoy代理注入到任何带有`istio-injection=enabled`标签的命名空间中。
- en: 'To label any namespace with this, run the following command:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要为任何命名空间添加标签，请运行以下命令：
- en: '[PRE33]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: To test easily, label the `default` namespace with the preceding `label` command.
    Once the Istio components come up, any Pods in that namespace will automatically
    be injected with the Envoy sidecar, just like we created manually in the previous
    section.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便测试，使用之前的`label`命令为`default`命名空间添加标签。一旦Istio组件启动，该命名空间中的任何Pod将自动注入Envoy sidecar，就像我们在上一节中手动创建的那样。
- en: 'In order to remove Istio from the cluster, run the following command:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了从集群中移除Istio，请运行以下命令：
- en: '[PRE34]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This should result in a confirmation message telling you that Istio has been
    removed.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会显示一条确认消息，告诉你Istio已经被移除。
- en: 'Now, let''s deploy a little something to test our new mesh with! We will deploy
    three different application services, each with a deployment and a service resource:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们部署一些内容来测试我们新的网格！我们将部署三个不同的应用服务，每个服务都有一个部署和一个服务资源：
- en: a. Service Frontend
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 服务前端
- en: b. Service Backend A
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 服务后端A
- en: c. Service Backend B
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 服务后端B
- en: 'Here''s the Deployment for *Service Frontend*:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是*服务前端*的部署配置：
- en: '[PRE35]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And here''s the Service for *Service Frontend*:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是*服务前端*的服务配置：
- en: '[PRE36]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The YAML for Service Backends A and B will be the same as *Service Frontend*,
    apart from swapping the names, image names, and selector labels.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 服务后端A和B的YAML与*服务前端*相同，除了交换名字、镜像名称和选择器标签。
- en: Now that we have a couple of services to route to (and between), let's start
    setting up some Istio resources!
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了几个服务来路由（和在其间路由），让我们开始设置一些Istio资源！
- en: 'First thing''s first, we need a `Gateway` resource. In this case, we are not
    using the NGINX Ingress Controller, but that''s fine because Istio provides a
    `Gateway` resource that can be used for ingress and egress. Here''s what an Istio
    `Gateway` definition looks like:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一件事，我们需要一个`Gateway`资源。在这个例子中，我们并没有使用NGINX Ingress Controller，但这没关系，因为Istio提供了一个`Gateway`资源，可以用于入口和出口。以下是一个Istio
    `Gateway`定义的样子：
- en: '[PRE37]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: These `Gateway` definitions look pretty similar to ingress records. We have
    `name`, and `selector`, which Istio uses to decide which Istio Ingress Controller
    to use. Next, we have one or more servers, which are essentially ingress points
    on our gateway. In this case, we do not restrict the host, and we accept requests
    on port `80`.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些`Gateway`定义看起来与入口记录非常相似。我们有`name`和`selector`，Istio使用这些来决定使用哪个Istio Ingress
    Controller。接下来，我们有一个或多个服务器，它们本质上是网关上的入口点。在这种情况下，我们不限制主机，并且接受在`80`端口上的请求。
- en: 'Now that we have a gateway for getting requests into our cluster, we can start
    setting up some routes. We do this in Istio using `VirtualService`. `VirtualService`
    in Istio is a set of routes that should be followed when requests to a particular
    hostname are made. In addition, we can use a wildcard host to make global rules
    for requests from anywhere in the mesh. Let''s take a look at an example `VirtualService`
    configuration:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个网关来接收请求进入集群，我们可以开始设置一些路由了。我们在Istio中通过`VirtualService`来实现。Istio中的`VirtualService`是一组路由规则，当向特定主机名发出请求时，这些路由应该被遵循。此外，我们可以使用通配符主机来为来自网格中任何地方的请求制定全局规则。让我们看一个`VirtualService`配置的例子：
- en: '[PRE38]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this `VirtualService`, we route requests to any host to our entry point at
    *Service Frontend* if it matches one of our `uri` prefixes. In this case, we are
    matching on the prefix, but you can use exact matching as well by swapping out
    `prefix` with `exact` in the URI matcher.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个`VirtualService`中，我们将请求路由到任何主机，如果它与我们的`uri`前缀之一匹配，则指向我们的入口点*服务前端*。在这种情况下，我们按前缀进行匹配，但你也可以通过将`prefix`替换为`exact`在URI匹配器中使用精确匹配。
- en: So, now we have a setup fairly similar to what we would expect with an NGINX
    Ingress, with entry into the cluster dictated by a route match.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，现在我们有了一个设置，和NGINX Ingress相似，集群的入口由路由匹配来决定。
- en: 'However, what''s that `v1` in our route? This actually represents a version
    of our *Frontend Service*. Let''s go ahead and specify this version using a new
    resource type – the Istio `DestinationRule`. Here''s what a `DestinationRule`
    config looks like:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但是，路由中的`v1`代表什么？这实际上表示我们的*前端服务*的一个版本。让我们通过一个新的资源类型来指定这个版本——Istio `DestinationRule`。以下是一个`DestinationRule`配置的样子：
- en: '[PRE39]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you can see, we specify two different versions of our frontend service in
    Istio, each looking at a label selector. From our previous Deployment and Service,
    you see that our current frontend service version is `v2`, but we could be running
    both in parallel! By specifying our `v2` version in the ingress virtual service,
    we tell Istio to route all requests to `v2` of the service. In addition, we have
    our `v1` version also configured, which is referenced in the previous `VirtualService`.
    This hard rule is only one possible way to route requests to different subsets
    in Istio.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，我们在 Istio 中指定了前端服务的两个不同版本，每个版本都通过标签选择器来识别。从我们之前的部署和服务来看，我们当前的前端服务版本是`v2`，但我们也可以并行运行两个版本！通过在入口虚拟服务中指定`v2`版本，我们告诉
    Istio 将所有请求路由到该版本的服务。此外，我们的`v1`版本也已配置，并在之前的`VirtualService`中进行了引用。这条硬规则是路由请求到
    Istio 中不同子集的一个可能方式。
- en: Now, we've managed to route traffic into our cluster via a gateway, and to a
    virtual service subset based on a destination rule. At this point, we are effectively
    "inside" our service mesh!
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们已经通过网关成功地将流量路由到我们的集群，并根据目标规则路由到虚拟服务子集。此时，我们实际上已经“进入”了我们的服务网格！
- en: 'Now, from our *Service Frontend*, we want to be able to route to *Service Backend
    A* and *Service Backend B*. How do we do this? More virtual services is the answer!
    Let''s take a look at a virtual service for *Backend Service A*:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，从我们的*服务前端*，我们希望能够路由到*服务后端 A*和*服务后端 B*。我们该如何做到这一点？更多的虚拟服务是答案！让我们来看一下*后端服务
    A*的虚拟服务：
- en: '[PRE40]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, this `VirtualService` routes to a `v1` subset for our service,
    `service-backend-a`. We'll also need another `VirtualService` for `service-backend-b`,
    which we won't include in full (but looks nearly identical). To see the full YAML,
    check the code repository for `istio-virtual-service-3.yaml`.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，这个`VirtualService`将路由到我们服务`service-backend-a`的`v1`子集。我们还需要为`service-backend-b`创建另一个`VirtualService`，尽管我们不会将其全部包括在内（但它几乎完全相同）。要查看完整的
    YAML 文件，请查看代码库中的`istio-virtual-service-3.yaml`。
- en: 'Once our virtual services are ready, we require some destination rules! The
    `DestinationRule` for *Backend Service A* is as follows:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们的虚拟服务准备好，我们就需要一些目标规则！*后端服务 A*的`DestinationRule`如下所示：
- en: 'Istio-destination-rule-2.yaml:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Istio-destination-rule-2.yaml：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: And the `DestinationRule` for *Backend Service B* is similar, just with different
    subsets. We won't include the code, but check `istio-destination-rule-3.yaml`
    in the code repository for the exact specifications.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*后端服务 B*的`DestinationRule`也很相似，只是子集不同。我们不会包括代码，但可以在代码库中的`istio-destination-rule-3.yaml`查看具体规范。
- en: 'These destination rules and virtual services add up to make the following routing
    diagram:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些目标规则和虚拟服务加起来形成了以下路由图：
- en: '![Figure 14.4 – Istio routing diagram](img/B14790_14_004.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4 – Istio 路由图](img/B14790_14_004.jpg)'
- en: Figure 14.4 – Istio routing diagram
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – Istio 路由图
- en: As you can see, requests from *Frontend Service* Pods can route to *Backend
    Service A version 1* or *Backend Service B version 3*, and each backend service
    can route to the other as well. These requests to Backend Service A or B additionally
    engage one of the most valuable features of Istio – mutual (two-way) TLS. In this
    setup, TLS security is maintained between any two points in the mesh, and this
    all happens automatically!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，来自*前端服务*的请求可以路由到*后端服务 A 版本 1*或*后端服务 B 版本 3*，并且每个后端服务也可以路由到另一个服务。这些请求到后端服务
    A 或 B 还启用了 Istio 的一项最有价值的功能——双向 TLS。在此设置中，TLS 安全在网格中的任何两点之间保持，且这一切都会自动进行！
- en: Next, let's take a look at using serverless patterns with Kubernetes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看一下如何在 Kubernetes 中使用无服务器模式。
- en: Implementing serverless on Kubernetes
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上实现无服务器
- en: 'Serverless patterns on cloud providers have quickly been gaining in popularity.
    Serverless architectures consist of compute that can automatically scale up and
    down, even scaling all the way to zero (where zero compute capacity is being used
    to serve a function or other application). **Function-as-a-Service** (**FaaS**)
    is an extension of the serverless pattern, where function code is the only input,
    and the serverless system takes care of routing requests to compute and scale
    as necessary. AWS Lambda, Azure Functions, and Google Cloud Run are some of the
    more popular FaaS/serverless options officially supported by cloud providers.
    Kubernetes also has many different serverless frameworks and libraries that can
    be used to run serverless, scale-to-zero workloads as well as FaaS on Kubernetes.
    Some of the most popular ones are as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商上的无服务器模式迅速变得流行起来。无服务器架构由可以自动扩展的计算资源组成，甚至可以扩展到零（即没有计算资源被用来提供函数或其他应用程序）。**功能即服务**（**FaaS**）是无服务器模式的扩展，其中函数代码是唯一的输入，而无服务器系统负责将请求路由到计算资源并根据需要进行扩展。AWS
    Lambda、Azure Functions 和 Google Cloud Run 是一些云服务提供商官方支持的较为流行的 FaaS/无服务器选项。Kubernetes
    也有许多不同的无服务器框架和库，可以用于运行无服务器、自动扩展到零的工作负载以及在 Kubernetes 上运行 FaaS。以下是一些最受欢迎的选项：
- en: '*Knative*'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Knative*'
- en: '*Kubeless*'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubeless*'
- en: '*OpenFaaS*'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenFaaS*'
- en: '*Fission*'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fission*'
- en: 'A full discussion of all serverless options on Kubernetes is beyond the scope
    of this book, so we''ll focus on two different ones, which aim to serve two vastly
    different use cases: *OpenFaaS* and *Knative*.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本书无法涵盖所有 Kubernetes 上的无服务器选项，因此我们将重点讨论两个不同的选项，它们旨在服务于两种截然不同的用例：*OpenFaaS* 和
    *Knative*。
- en: While Knative is highly extensible and customizable, it uses multiple coupled
    components that add complexity. This means that some added configuration is necessary
    to get started with an FaaS solution, since functions are just one of many other
    patterns that Knative supports. OpenFaaS, on the other hand, makes getting up
    and running with serverless and FaaS on Kubernetes extremely easy. Both technologies
    are valuable for different reasons.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Knative 具有高度的可扩展性和自定义性，但它使用了多个耦合的组件，这增加了复杂性。这意味着，为了开始使用 FaaS 解决方案，必须进行一些额外的配置，因为函数只是
    Knative 支持的众多模式之一。另一方面，OpenFaaS 使得在 Kubernetes 上启动和运行无服务器和 FaaS 非常容易。这两种技术在不同的方面各有其价值。
- en: For this chapter's tutorial, we will look at Knative, one of the most popular
    serverless frameworks, and one that also supports FaaS via its eventing feature.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的教程中，我们将探讨 Knative，作为最流行的无服务器框架之一，它也通过事件功能支持 FaaS。
- en: Using Knative for FaaS on Kubernetes
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Knative 在 Kubernetes 上实现 FaaS
- en: As mentioned previously, Knative is a modular set of building blocks for serverless
    patterns on Kubernetes. For this reason, it requires a bit of configuration before
    we can get to the actual functions. Knative can also be installed with Istio,
    which it uses as a substrate for routing and scaling serverless applications.
    Other non-Istio routing options are also available.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Knative 是一套用于 Kubernetes 上无服务器模式的模块化构建块。因此，在实际使用功能之前，需要进行一些配置。Knative 也可以与
    Istio 一起安装，Istio 被用作无服务器应用程序路由和扩展的基础层。也提供了其他非 Istio 的路由选项。
- en: 'To use Knative for FaaS, we will need to install both *Knative Serving* and
    *Knative Eventing*. While Knative Serving will allow us to run our serverless
    workloads, Knative Eventing will provide the pathway to make FaaS requests to
    these scale-to-zero workloads. Let''s accomplish this by following these steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 FaaS 中使用 Knative，我们需要安装 *Knative Serving* 和 *Knative Eventing*。虽然 Knative
    Serving 使我们能够运行无服务器工作负载，但 Knative Eventing 将提供一个途径，以便向这些自动扩展到零的工作负载发出 FaaS 请求。让我们按照以下步骤来实现这一目标：
- en: 'First, let''s install the Knative Serving components. We will begin by installing
    the CRDs:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装 Knative Serving 组件。我们将从安装 CRDs 开始：
- en: '[PRE42]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we can install the serving components themselves:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以安装服务组件本身：
- en: '[PRE43]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'At this point, we''ll need to install a networking/routing layer for Knative
    to use. Let''s use Istio:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，我们需要为 Knative 安装一个网络/路由层。让我们使用 Istio：
- en: '[PRE44]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We''ll need the gateway IP address from Istio. Depending on where you''re running
    this (in other words, AWS or locally), this value may differ. Pull it using the
    following command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要从 Istio 获取网关 IP 地址。根据你的运行环境（即 AWS 或本地），这个值可能有所不同。使用以下命令获取它：
- en: '[PRE45]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Knative requires a specific DNS setup for enabling the serving component. The
    easiest way to do this in a cloud setting is to use `xip.io` "Magic DNS," though
    this will not work for Minikube-based clusters. If you're running one of these
    (or just want to see all the options available), check out the Knative docs at
    [https://knative.dev/docs/install/any-kubernetes-cluster/](https://knative.dev/docs/install/any-kubernetes-cluster/).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Knative 需要特定的 DNS 设置来启用服务组件。在云环境中，最简单的方法是使用 `xip.io` 的“魔术 DNS”，不过这在基于 Minikube
    的集群中无法使用。如果您正在运行这些集群（或者只是想查看所有可用选项），请查看 Knative 文档：[https://knative.dev/docs/install/any-kubernetes-cluster/](https://knative.dev/docs/install/any-kubernetes-cluster/)。
- en: 'To set up Magic DNS, use the following command:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要设置魔术 DNS，请使用以下命令：
- en: '[PRE46]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now that we''ve installed Knative Serving, let''s install Knative Eventing
    to deliver our FaaS requests. First, we''ll need more CRDs. Install them using
    the following command:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 Knative Serving，让我们安装 Knative Eventing 来传递我们的 FaaS 请求。首先，我们需要更多的 CRD。请使用以下命令安装它们：
- en: '[PRE47]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, install the eventing components just like we did with serving:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，像我们安装服务组件一样安装事件组件：
- en: '[PRE48]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: At this point, we need to add a queue/messaging layer for our eventing system
    to use. Did we mention that Knative supports lots of modular components?
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此时，我们需要为我们的事件系统添加一个队列/消息层。我们提到过 Knative 支持许多模块化组件吗？
- en: Important note
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: To make things easy, let's just use the basic in-memory messaging layer, but
    it's good to know all the options available to you. As regards modular options
    for messaging channels, check the docs at [https://knative.dev/docs/eventing/channels/channels-crds/](https://knative.dev/docs/eventing/channels/channels-crds/).
    For event source options, you can look at [https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/).
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了简化操作，让我们使用基本的内存消息层，但了解所有可用的选项还是很有帮助的。关于消息通道的模块化选项，请查看[https://knative.dev/docs/eventing/channels/channels-crds/](https://knative.dev/docs/eventing/channels/channels-crds/)中的文档。对于事件源选项，您可以查看[https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/)。
- en: 'To install the `in-memory` messaging layer, use the following command:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装`in-memory`消息层，请使用以下命令：
- en: '[PRE49]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Thought we were done? Nope! One last thing. We need to install a broker, which
    will take events from the messaging layer and get them processed in the right
    place. Let''s use the default broker layer, the MT-Channel broker layer. You can
    install it using the following command:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以为我们已经完成了？不！还有最后一件事。我们需要安装一个代理，它将从消息层获取事件，并将其处理到正确的位置。让我们使用默认的代理层，即 MT-Channel
    代理层。您可以使用以下命令安装它：
- en: '[PRE50]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With that, we are finally done. We have installed an end-to-end FaaS implementation
    via Knative. As you can tell, this was not an easy task. What makes Knative amazing
    is the same thing that makes it a pain – it offers so many different modular options
    and configurations that even when selecting the most basic options for each step,
    we've still taken a lot of time to explain the install. There are other options
    available, such as OpenFaaS, which are a bit easier to get up and running with,
    and we'll look into that in the next section! On the Knative side, however, now
    that we have our setup finally ready, we can add in our FaaS.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！我们通过 Knative 安装了端到端的 FaaS 实现。正如您所见，这并不是一件简单的事。Knative 的强大之处，正是它令人头痛的地方——它提供了如此多的模块化选项和配置，即使选择每一步的最基本选项，我们仍然花了大量时间来解释安装过程。还有其他选项，例如
    OpenFaaS，它们更容易启动运行，我们将在下一节中探讨！不过，在 Knative 方面，既然我们的设置终于准备好了，我们可以开始添加 FaaS 了。
- en: Implementing an FaaS pattern in Knative
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Knative 中实现 FaaS 模式
- en: 'Now that we have Knative set up, we can use it to implement an FaaS pattern
    where events will trigger some code running in Knative through a trigger. To set
    up a simple FaaS, we will require three things:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了 Knative，可以使用它来实现 FaaS 模式，其中事件将通过触发器触发在 Knative 中运行的代码。要设置一个简单的 FaaS，我们需要三样东西：
- en: A broker to route our events from an entry point
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代理，用于将我们的事件从入口点路由
- en: A consumer service to actually process our events
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个消费者服务，实际处理我们的事件
- en: A trigger definition that specifies when to route events to the consumer for
    processing
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个触发器定义，指定何时将事件路由到消费者进行处理
- en: 'First thing''s first, our broker needs to be created. This is simple and similar
    to creating an ingress record or gateway. Our `broker` YAML looks like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事，我们需要创建代理。这很简单，类似于创建入口记录或网关。我们的 `broker` YAML 文件如下所示：
- en: 'Knative-broker.yaml:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Knative-broker.yaml：
- en: '[PRE51]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Next, we can create a consumer service. This component is really just our application
    that is going to process events – our function itself! Rather than showing you
    even more YAML than you've already seen, let's assume our consumer service is
    just a regular old Kubernetes Service called `service-consumer`, which routes
    to a four-replica deployment of Pods running our application.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个消费者服务。这个组件实际上就是我们的应用程序，用来处理事件——即我们的函数本身！为了避免展示更多 YAML 文件内容，假设我们的消费者服务只是一个名为
    `service-consumer` 的普通 Kubernetes 服务，它将请求路由到一个包含四个副本的 Pod 部署，这些 Pod 运行我们的应用程序。
- en: 'Finally, we''re going to need a trigger. This determines how and which events
    will be routed from the broker. The YAML for a trigger looks like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个触发器。它决定了如何以及哪些事件会从代理中路由过来。触发器的 YAML 文件内容如下：
- en: 'Knative-trigger.yaml:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Knative-trigger.yaml：
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In this YAML, we create a `Trigger` rule that any event that comes through our
    broker, `my-broker`, and has a type of `myeventtype`, will automatically be routed
    to our consumer, `service-consumer`. For full documentation on trigger filters
    in Knative, check out the docs at [https://knative.dev/development/eventing/triggers/](https://knative.dev/development/eventing/triggers/).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 YAML 文件中，我们创建了一个 `Trigger` 规则，任何通过我们的代理 `my-broker` 且类型为 `myeventtype` 的事件，将会自动路由到我们的消费者
    `service-consumer`。有关 Knative 中触发器过滤器的完整文档，请查阅 [https://knative.dev/development/eventing/triggers/](https://knative.dev/development/eventing/triggers/)。
- en: 'So, how do we create some events? First, check the broker URL using the following
    command:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何创建一些事件呢？首先，使用以下命令检查代理 URL：
- en: '[PRE53]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This should result in the following output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成以下输出：
- en: '[PRE54]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can now finally test our FaaS solution. Let''s spin up a quick Pod from
    which we can make requests to our trigger:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以测试我们的 FaaS 解决方案了。让我们启动一个快速的 Pod，从中可以向我们的触发器发出请求：
- en: '[PRE55]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now, from inside this Pod, we can go ahead and test our trigger, using `curl`.
    The request we need to make needs to have a `Ce-Type` header that equals `myeventtype`,
    since this is what our trigger requires. Knative uses headers in the form `Ce-Id`,
    `Ce-Type`, as shown in the following code block, to do the routing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个 Pod 内部，我们可以使用 `curl` 测试我们的触发器。我们需要发出的请求需要有一个 `Ce-Type` 头，其值为 `myeventtype`，因为这是我们的触发器所要求的。Knative
    使用 `Ce-Id`、`Ce-Type` 等头部，如下代码块所示，来进行路由。
- en: 'The `curl` request will look like the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 请求将如下所示：'
- en: '[PRE56]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As you can see, we are sending a `curl` `http` request to the broker URL. Additionally,
    we are passing some special headers along with the HTTP request. Importantly,
    we are passing `type=myeventtype`, which our filter on our trigger requires in
    order to send the request for processing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们正在向代理 URL 发送一个 `curl` `http` 请求。此外，我们还随 HTTP 请求传递了一些特殊头部。重要的是，我们传递了 `type=myeventtype`，这是我们的触发器过滤器要求的，以便发送请求进行处理。
- en: 'In this example, our consumer service echoes back the payload key of the body
    JSON, along with a `200` HTTP response, so running this `curl` request gives us
    the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的消费者服务会回显消息体 JSON 的 payload 键，并返回 `200` HTTP 响应，因此运行这个 `curl` 请求会得到如下结果：
- en: '[PRE57]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Success! We have tested our FaaS and it returns what we are expecting. From
    here, our solution will scale up and down to zero along with the number of events,
    and, as with everything Knative, there are many more customizations and configuration
    options to tailor our solution precisely to what we need.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们已经测试了我们的 FaaS，它返回了我们期望的结果。从这里开始，我们的解决方案会根据事件数量自动扩展或缩减到零，正如 Knative 的一切一样，仍然有许多自定义和配置选项，可以将我们的解决方案精确调整到我们所需的状态。
- en: Next up, we'll look at the same pattern with OpenFaaS instead of Knative in
    order to highlight the differences between the two approaches.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看在 OpenFaaS 中使用相同的模式，而不是 Knative，以便突出两种方法之间的区别。
- en: Using OpenFaaS for FaaS on Kubernetes
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上使用 OpenFaaS 进行 FaaS
- en: Now that we've discussed getting started with Knative, let's do the same with
    OpenFaaS. First, to install OpenFaaS itself, we are going to use the Helm charts
    from the `faas-netes` repository, found at [https://github.com/openfaas/faas-netes](https://github.com/openfaas/faas-netes).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了如何开始使用 Knative，那么让我们也用 OpenFaaS 来做同样的事情。首先，安装 OpenFaaS 本身，我们将使用来自 `faas-netes`
    仓库的 Helm charts，地址是 [https://github.com/openfaas/faas-netes](https://github.com/openfaas/faas-netes)。
- en: Installing OpenFaaS components with Helm
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Helm 安装 OpenFaaS 组件
- en: 'First, we will create two namespaces to hold our OpenFaaS components:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建两个命名空间来存放我们的 OpenFaaS 组件：
- en: '`openfaas` to hold the actual service components of OpenFaas'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas` 用于存放 OpenFaaS 的实际服务组件'
- en: '`openfaas-fn` to hold our deployed functions'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas-fn` 用于存放我们部署的函数'
- en: 'We can add these two namespaces using a nifty YAML file from the `faas-netes`
    repository using the following command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令，通过`faas-netes`仓库中的一个巧妙的YAML文件来添加这两个命名空间：
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we need to add the `faas-netes` `Helm` `repository` with the following
    Helm command:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过以下Helm命令将`faas-netes` `Helm` `仓库`添加到配置中：
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Finally, we actually deploy OpenFaaS!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实际部署OpenFaaS！
- en: 'The Helm chart for OpenFaaS at the preceding `faas-netes` repository has several
    possible variables, but we will use the following configuration to ensure that
    an initial set of authentication credentials are created, and that ingress records
    are deployed:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 上述`faas-netes`仓库中的OpenFaaS Helm图表包含多个可能的变量，但我们将使用以下配置，确保创建初始的身份验证凭证，并部署入口记录：
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, that our OpenFaaS infrastructure has been deployed to our cluster, we''ll
    want to fetch the credentials that were generated as part of the Helm install.
    The Helm chart will create these as part of a hook and store them in a secret,
    so we can get them by running the following command:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的OpenFaaS基础设施已经部署到集群中，我们接下来需要获取在Helm安装过程中生成的凭证。Helm图表会作为钩子创建这些凭证，并将它们存储在一个secret中，因此我们可以通过运行以下命令来获取它们：
- en: '[PRE61]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: That is all the Kubernetes setup we require!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所需要的所有Kubernetes配置！
- en: Moving on, let's install the OpenFaas CLI, which will make it extremely easy
    to manage our OpenFaas functions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们安装OpenFaaS CLI，这将使我们管理OpenFaaS函数变得极其简单。
- en: Installing the OpenFaaS CLI and deploying functions
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装OpenFaaS CLI并部署函数
- en: 'To install the OpenFaaS CLI, we can use the following command (for Windows,
    check the preceding OpenFaaS documents):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装OpenFaaS CLI，我们可以使用以下命令（对于Windows，请参阅前面的OpenFaaS文档）：
- en: '[PRE62]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, we can get started with building and deploying some functions. This is
    easiest to do via the CLI.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始构建和部署一些函数。通过CLI来做这件事是最简单的。
- en: When building and deploying functions for OpenFaaS, the OpenFaaS CLI provides
    an easy way to generate boilerplates, and build and deploy functions for specific
    languages. It does this via "templates," and supports various flavors of Node,
    Python, and more. For a full list of the template types, check the templates repository
    at [https://github.com/openfaas/templates](https://github.com/openfaas/templates).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在为OpenFaaS构建和部署函数时，OpenFaaS CLI提供了一种简单的方式来生成样板文件，并为特定语言构建和部署函数。它通过“模板”来实现，并支持Node、Python等多种语言的版本。有关模板类型的完整列表，请访问[https://github.com/openfaas/templates](https://github.com/openfaas/templates)。
- en: 'The templates created using the OpenFaaS CLI are similar to what you would
    expect from a hosted serverless platform such as AWS Lambda. Let''s create a brand-new
    Node.js function using the following command:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenFaaS CLI创建的模板与您从托管的无服务器平台（如AWS Lambda）中预期的非常相似。让我们使用以下命令创建一个全新的Node.js函数：
- en: '[PRE63]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This results in the following output:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出如下：
- en: '[PRE64]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: As you can see, the `new` command generates a folder, and within it some boilerplate
    for the function code itself, and an OpenFaaS YAML file.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`new`命令会生成一个文件夹，并在其中创建一些函数代码的样板文件，以及一个OpenFaaS YAML文件。
- en: 'The OpenFaaS YAML file will appear as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS YAML文件将如下所示：
- en: 'My-function.yml:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: My-function.yml：
- en: '[PRE65]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The actual function code (inside the `my-function` folder) consists of a function
    file – `handler.js` – and a dependencies manifest, `package.json`. For other languages,
    these files will be different, and we won''t delve into the specifics of dependencies
    in Node. However, we will edit the `handler.js` file to return some text. This
    is what the edited file looks like:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的函数代码（位于`my-function`文件夹内）由一个函数文件`handler.js`和一个依赖清单文件`package.json`组成。对于其他语言，这些文件会有所不同，我们不会深入探讨Node中的依赖关系。但我们将编辑`handler.js`文件以返回一些文本。编辑后的文件如下所示：
- en: 'Handler.js:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Handler.js：
- en: '[PRE66]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This JavaScript code will return a JSON response with our text.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这段JavaScript代码将返回一个包含我们文本的JSON响应。
- en: 'Now that we have our function and handler, we can move on to building and deploying
    our function. The OpenFaaS CLI makes it simple to build the function, which we
    can do with the following command:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了函数和处理器，我们可以继续构建和部署我们的函数。OpenFaaS CLI让构建函数变得非常简单，我们可以使用以下命令来完成：
- en: '[PRE67]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The output of this command is long, but when it is complete, we will have a
    new container image built locally with our function handler and dependencies embedded!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的输出较长，但完成时，我们将拥有一个本地构建的新容器镜像，其中包含我们的函数处理器和嵌入的依赖项！
- en: 'Next, we push our container image to a container repository as we would for
    any other container. The OpenFaaS CLI has a neat wrapper command for this, which
    will push the image to Docker Hub or an alternate container image repository:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将容器镜像推送到容器仓库，就像我们为任何其他容器做的那样。OpenFaaS CLI 提供了一个非常方便的包装命令，可以将镜像推送到 Docker
    Hub 或其他容器镜像仓库：
- en: '[PRE68]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, we can deploy our function to OpenFaaS. Once again, this is made easy
    by the CLI. Deploy it using the following command:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的函数部署到 OpenFaaS。再次提醒，CLI 使得这一过程变得非常简单。使用以下命令进行部署：
- en: '[PRE69]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Everything is now set up for us to test our function, deployed on OpenFaaS!
    We used an ingress setting when deploying OpenFaaS so requests can go through
    that ingress. However, our generated YAML file from our new function is set to
    make requests on `localhost:8080` for development purposes. We could edit that
    file to the correct `URL` for our ingress gateway (refer to the docs at [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/)
    for how to do that), but instead, let's just do a shortcut to get OpenFaaS open
    on our localhost.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都已设置好，我们可以测试部署在 OpenFaaS 上的函数！我们在部署 OpenFaaS 时使用了 ingress 设置，以便请求能够通过该 ingress。不过，我们的新函数生成的
    YAML 文件是设置为在 `localhost:8080` 上进行请求的，目的是用于开发。我们可以编辑该文件，修改为 ingress 网关的正确 `URL`（可以参考文档
    [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/)
    来了解如何操作），但我们可以使用一个捷径，让 OpenFaaS 在本地打开。
- en: 'Let''s use a `kubectl port-forward` command to open our OpenFaaS service on
    localhost port `8080`. We can do this as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `kubectl port-forward` 命令将 OpenFaaS 服务映射到本地的 `8080` 端口。操作方法如下：
- en: '[PRE70]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, let''s add our previously generated auth credentials to the OpenFaaS CLI,
    as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将之前生成的身份验证凭证添加到 OpenFaaS CLI，方法如下：
- en: '[PRE71]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, all we need to do in order to test our function is to run the following
    command:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试我们的函数，我们只需运行以下命令：
- en: '[PRE72]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This results in the following output:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE73]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: As you can see, we've successfully received our intended response!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经成功收到了预期的响应！
- en: 'Finally, if we want to delete this specific function, we can do so with the
    following command, similar to how we would use `kubectl delete -f`:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们想删除这个特定的函数，可以使用以下命令，类似于使用 `kubectl delete -f`：
- en: '[PRE74]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: And that's it! Our function has been removed.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们的函数已经被删除。
- en: Summary
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about service mesh and serverless patterns on Kubernetes.
    In order to set the stage for these, we first discussed running sidecar proxies
    on Kubernetes, specifically with the Envoy proxy.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 Kubernetes 上的服务网格和无服务器模式。为了铺垫这些内容，我们首先讨论了如何在 Kubernetes 上运行 sidecar
    代理，特别是使用 Envoy 代理。
- en: Then, we moved on to service mesh, and learned how to install and configure
    the Istio service mesh for service-to-service routing with mutual TLS.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们转向了服务网格，学习了如何安装和配置 Istio 服务网格，以便进行服务到服务的路由，并实现双向 TLS。
- en: Finally, we moved on to serverless patterns on Kubernetes, where you learned
    how to configure and install Knative, and an alternative, OpenFaaS, for serverless
    eventing, and FaaS on Kubernetes.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们转向了 Kubernetes 上的无服务器模式，在这里你学习了如何配置和安装 Knative，以及另一个无服务器事件和 FaaS 解决方案 OpenFaaS。
- en: The skills you used in this chapter will help you to build service mesh and
    serverless patterns on Kubernetes, setting you up for fully automated service-to-service
    discovery and FaaS eventing.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你所使用的技能将帮助你在 Kubernetes 上构建服务网格和无服务器模式，帮助你实现完全自动化的服务到服务发现和 FaaS 事件处理。
- en: In the next (and final) chapter, we'll discuss running stateful applications
    on Kubernetes.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章（也是最后一章），我们将讨论如何在 Kubernetes 上运行有状态的应用程序。
- en: Questions
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between static and dynamic Envoy configurations?
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态和动态 Envoy 配置有什么区别？
- en: What are the four major pieces of Envoy configuration?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Envoy 配置的四个主要部分是什么？
- en: What are some of the downsides to Knative, and how does OpenFaaS compare?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Knative 的一些缺点是什么？OpenFaaS 又是如何对比的？
- en: Further reading
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'CNCF landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CNCF landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
- en: 'Official Kubernetes forums: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方 Kubernetes 论坛：[https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)
