- en: Publishing Applications in Docker Enterprise
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker 企业版中发布应用程序
- en: The previous chapter helped us to understand Docker Enterprise's control plane
    components. Docker UCP deploys Docker Swarm and Kubernetes clusters over the same
    nodes. Both orchestrators share host components and devices. Each orchestrator
    will manage its own hardware resources. Information such as available memory and
    CPU is not shared between orchestrators. Therefore, we have to take care if we
    use both on a host simultaneously.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章帮助我们理解了 Docker 企业版的控制平面组件。Docker UCP 在相同节点上部署 Docker Swarm 和 Kubernetes 集群。这两个编排器共享主机组件和设备。每个编排器将管理自己的硬件资源。可用内存和
    CPU 等信息不会在编排器之间共享。因此，如果我们在同一主机上同时使用这两者时，必须小心。
- en: But what about publishing applications deployed on them? We learned how to publish
    applications on Docker Swarm and Kubernetes, but working on enterprise environments
    must be secure. In this chapter, we will learn how to publish applications on
    Docker Enterprise environments using either UCP-provided or community tools.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何发布部署在它们上的应用程序呢？我们已经学习了如何在 Docker Swarm 和 Kubernetes 上发布应用程序，但在企业环境中工作必须是安全的。本章将学习如何在
    Docker 企业环境中发布应用程序，使用的是 UCP 提供的工具或社区工具。
- en: This chapter will show us the main publishing resources and features provided
    by UCP for Docker Swarm and Kubernetes. These components will help us to publish
    only front services, thereby ensuring an application's security. We will learn
    about ingress controllers, which is the preferred solution for publishing applications
    in Kubernetes, and Interlock, an enterprise-ready solution provided by UCP to
    publish applications in Docker Swarm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向我们展示 UCP 为 Docker Swarm 和 Kubernetes 提供的主要发布资源和功能。这些组件将帮助我们只发布前端服务，从而确保应用程序的安全性。我们将了解
    ingress 控制器，这是发布 Kubernetes 应用程序的首选解决方案，以及 Interlock，这是 UCP 提供的一个企业级解决方案，用于在 Docker
    Swarm 中发布应用程序。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding publishing concepts and components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解发布概念和组件
- en: Deep diving into your application's logic
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入分析你的应用程序逻辑
- en: Ingress controllers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 控制器
- en: Interlock
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Interlock
- en: Chapter labs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 章节实验
- en: We will begin this chapter by reviewing some of the concepts learned in connection
    with Docker Swarm and Kubernetes deployments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从回顾与 Docker Swarm 和 Kubernetes 部署相关的一些概念开始。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the code for this chapter in the GitHub repository: [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 仓库中找到本章的代码：[https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git)
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，观看代码演示：
- en: '"[https://bit.ly/2EHobBy](https://bit.ly/2EHobBy)"'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"[https://bit.ly/2EHobBy](https://bit.ly/2EHobBy)"'
- en: Understanding publishing concepts and components
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解发布概念和组件
- en: '[Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml), *Orchestration Using
    Docker Swarm*, showed us how applications work when deployed on top of a Docker
    Swarm cluster.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml)，*使用 Docker Swarm 进行编排*，向我们展示了当应用程序部署在
    Docker Swarm 集群上时的工作原理。'
- en: We will use service objects to deploy applications in Docker Swarm. Internal
    communication between services is always allowed if they run in the same network.
    Therefore, we will deploy an application's components in the same network and
    they will interact with other published applications. If two applications have
    to interact, they should share the network or be published.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用服务对象在 Docker Swarm 中部署应用程序。如果它们运行在同一网络中，服务之间的内部通信始终是允许的。因此，我们将把应用程序的组件部署在同一网络中，它们将与其他已发布的应用程序进行交互。如果两个应用程序必须相互作用，它们应该共享网络或被发布。
- en: Publishing applications is easy; we will just specify the ports that should
    be listening on the host where the process is running. However, we learned that
    Docker Swarm will publish an application's ports on all cluster hosts, and Router
    Mesh will route internal traffic to reach an appropriate service's tasks. Let's
    go back to these topics relating to containers and services before reviewing multi-service
    applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 发布应用程序很简单；我们只需指定主机上应该监听的端口。然而，我们了解到 Docker Swarm 会在所有集群主机上发布应用程序的端口，而路由器网格（Router
    Mesh）会将内部流量路由到合适的服务任务。在回顾多服务应用程序之前，我们先回顾一下与容器和服务相关的这些主题。
- en: 'We have different options to publish container applications, as we learned
    in [Chapter 4](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml), *Container Persistency
    and Networking*. To make processes visible out of a container''s isolated network
    namespace, we will use different network strategies:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有不同的选项来发布容器应用程序，正如我们在[第4章](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml)中所学到的，*容器持久性与网络*。为了使进程能够从容器的隔离网络命名空间中可见，我们将使用不同的网络策略：
- en: '**Bridge networking**: This is the default option. A container''s processes
    will be exposed using the host''s **Network Address Translation** (**NAT**) features.
    Therefore, a container''s process listening on a port will be attached to a host''s
    port. NAT rules will be applied either to Linux or Microsoft Windows containers.
    This allows us to execute more than one container''s instances using different
    hosts'' ports.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桥接网络**：这是默认选项。容器的进程将通过主机的**网络地址转换**（**NAT**）功能进行暴露。因此，监听某个端口的容器进程将与主机的端口绑定。NAT规则将应用于Linux或Microsoft
    Windows容器。这使我们能够使用不同主机的端口执行多个容器实例。'
- en: 'We will publish container processes using the `--publish` or `-p` (or even
    `--publish-all` or `-P` to publish all image-declared exposed ports) option with
    the optional Docker host''s IP address and port alongside the published port and
    protocol (TCP/UDP): `docker container run -p [HOST_IP:HOST_PORT:]<CONTAINER_PORT>[/PROTOCOL]`.
    By default, all the host''s IP addresses and random ports within the `32768`-`65000`
    range will be used.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`--publish`或`-p`选项（甚至使用`--publish-all`或`-P`发布所有镜像声明的暴露端口）来发布容器进程，同时指定可选的Docker主机IP地址和端口，以及发布的端口和协议（TCP/UDP）：`docker
    container run -p [HOST_IP:HOST_PORT:]<CONTAINER_PORT>[/PROTOCOL]`。默认情况下，将使用主机的所有IP地址和`32768`到`65000`范围内的随机端口。
- en: '**The host''s network namespace**: In this situation, we will use the host''s
    network namespace. Processes will be directly available, listening on the host''s
    ports. No port translation will be used between the container and the host. Since
    the process port is attached directly, only one container''s instance is allowed
    per host. We will use `docker container run --net=host` to associate a new container
    with the host''s network namespace.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机的网络命名空间**：在这种情况下，我们将使用主机的网络命名空间。进程将直接可用，监听主机的端口。容器与主机之间不会使用端口转换。由于进程端口是直接绑定的，因此每个主机只允许一个容器实例。我们将使用`docker
    container run --net=host`将新容器与主机的网络命名空间关联。'
- en: '**MacVLAN**: This is a special case where a container will use its own namespace,
    but it will be available at the host''s network level. This allows us to attach
    VLANs (Virtual LANs) directly to containers and make them visible within an actual
    network. Containers will receive their own MACs; hence, services will be available
    in the network as if they were nodes.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MacVLAN**：这是一个特殊情况，容器将使用自己的命名空间，但它将在主机的网络级别可用。这使我们能够将VLAN（虚拟局域网）直接附加到容器上，并使它们在实际网络中可见。容器将获得自己的MAC地址，因此服务将在网络中像节点一样可用。'
- en: These were the basic options. We will use external DNS to announce how they
    will be reached. We can also deploy containers on customized bridge networks.
    Custom networks have their own DNS namespace and containers will reach one another
    within the same network through their names or aliases. Services won't be published
    for other services running in the same network. We will just publish them for
    other networks or user access. In these cases, we will use NAT (common bridge
    networking), a host's namespace, or MacVLAN.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是基本选项。我们将使用外部DNS来宣布如何访问这些服务。我们还可以在自定义桥接网络上部署容器。自定义网络具有自己的DNS命名空间，容器将在同一网络内通过其名称或别名互相访问。服务不会对同一网络中运行的其他服务进行发布。我们只会将它们发布到其他网络或供用户访问。在这些情况下，我们将使用NAT（常见的桥接网络）、主机命名空间或MacVLAN。
- en: 'These will work on standalone hosts, but things will change if we distribute
    our workloads cluster-wide. We will now introduce the Kubernetes network model.
    This model must cover these situations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置适用于独立主机，但如果我们将工作负载分布到整个集群中，情况将发生变化。现在我们将介绍Kubernetes网络模型。该模型必须涵盖以下情况：
- en: Pods running on a node should be able to communicate with others running on
    other hosts without NAT.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行在节点上的Pods应该能够与其他主机上运行的Pods进行通信，而无需使用NAT。
- en: System components (kubelet and control plane daemons) should be able to communicate
    with pods running on a host.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统组件（kubelet和控制平面守护进程）应该能够与运行在主机上的Pods进行通信。
- en: Pods running in the host network of a node can communicate with all pods running
    on other hosts without NAT.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行在节点主机网络中的Pods可以与其他主机上运行的所有Pods进行通信，而无需使用NAT。
- en: As we have learned, all containers within a pod share its IP address and all
    pods run on a flat network. We do not have network segmentation in Kubernetes,
    so we need other tools to isolate them. We will use network policies to implement
    firewall-like or network ACL-like rules. These rules are also applied to publishing
    services (ingress traffic).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，Pod中的所有容器共享一个IP地址，且所有Pod都运行在一个扁平化网络中。Kubernetes中没有网络分割，因此我们需要其他工具来进行隔离。我们将使用网络策略来实现类似防火墙或网络ACL的规则。这些规则同样适用于发布服务（入口流量）。
- en: Docker's network model is based on the **Container Network Model** (**CNM**)
    standard, while Kubernetes' network model is implemented using the **Container
    Network Interface** (**CNI**) model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Docker的网络模型基于**容器网络模型**（**CNM**）标准，而Kubernetes的网络模型则是使用**容器网络接口**（**CNI**）模型来实现的。
- en: Docker's CNM manages **Internet Protocol Address Management** (**IPAM**) and
    network plugins. IPAM will be used to manage address pools and containers' IP
    addresses, while network plugins will be responsible for managing networks on
    each Docker Engine. CNM is implemented on Docker Engine via its `libnetwork` library,
    although we can add third-party plugins to replace this built-in Docker driver.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Docker的CNM管理**互联网协议地址管理**（**IPAM**）和网络插件。IPAM将用于管理地址池和容器的IP地址，而网络插件则负责管理每个Docker引擎上的网络。CNM通过Docker引擎的`libnetwork`库来实现，尽管我们可以添加第三方插件来替代这个内置的Docker驱动程序。
- en: On the other hand, CNI modes expose an interface for managing a container's
    network. CNI will assign IP addresses to pods, although we can also add external
    IPAM interfaces, describing its behavior using JSON format. These describe how
    any CNI plugin must provide cluster and standalone networking when we add third-party
    plugins. As mentioned previously, Docker Enterprise's default CNI plugin is Calico.
    It provides cluster networking and security features using IP in IP encapsulation
    (although it also provides VXLAN mode).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，CNI模式暴露了一个接口来管理容器的网络。CNI将为Pod分配IP地址，尽管我们也可以添加外部IPAM接口，并使用JSON格式描述其行为。这些描述了当我们添加第三方插件时，任何CNI插件必须提供集群和独立网络的功能。如前所述，Docker企业版的默认CNI插件是Calico。它使用IP内嵌（IP
    in IP）封装提供集群网络和安全功能（尽管它也提供VXLAN模式）。
- en: Let's move on. Docker Engine provides all the networking features for hosts,
    and Kubernetes will also provide cluster-wide networking using CNI. Docker Swarm
    includes cluster-wide networking out of the box using VXLAN. An overlay network
    driver creates a distributed network between all hosts using their bridge network
    interfaces. We will just initialize a Docker Swarm cluster and no additional operations
    will be required. An ingress overlay network and a `docker_gwbridge` bridge network
    will be created. The former will manage control and data traffic related to Swarm
    services, while `docker_gwbridge` will be used to interconnect Docker hosts within
    Docker Swarm overlay networks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们继续。Docker引擎提供了所有主机所需的网络功能，而Kubernetes也将使用CNI提供集群范围的网络。Docker Swarm内置了使用VXLAN的集群级别网络功能。一个覆盖网络驱动程序通过主机的桥接网络接口在所有主机之间创建分布式网络。我们只需要初始化一个Docker
    Swarm集群，之后无需进行额外操作。将会创建一个入口覆盖网络和一个`docker_gwbridge`桥接网络。前者将管理与Swarm服务相关的控制和数据流量，而`docker_gwbridge`则用于在Docker
    Swarm覆盖网络中互连Docker主机。
- en: We improved cluster's security by encrypting overlay networks, but we will expect
    some overhead and a minor negative impact on performance. As demonstrated in standalone
    networking and containers sharing networks, all services connected to the same
    overlay network will be able to talk to one another, even if we have not published
    any ports. Ports that should be accessible outside of a service's network must
    be explicitly published using `-p [ HOSTS_PORT:]<CONTAINER_PORT>[/PROTOCOL]`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过加密覆盖网络提高了集群的安全性，但这也意味着我们会遇到一定的开销，并对性能造成轻微的负面影响。如在独立网络和容器共享网络中所展示的那样，所有连接到同一覆盖网络的服务将能够互相通信，即使我们没有发布任何端口。必须显式地使用`-p
    [ HOSTS_PORT:]<CONTAINER_PORT>[/PROTOCOL]`来发布应对外部服务网络可访问的端口。
- en: There is a long format for publishing a service's ports. Although you have to
    write more, it is clearer. We will write `-p published=<HOSTS_PORT>,target=<CONTAINER_PORT>,protocol=<PROTOCOL>`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 发布服务端口的格式较长，虽然需要写更多内容，但它更清晰。我们将写作`-p published=<HOSTS_PORT>,target=<CONTAINER_PORT>,protocol=<PROTOCOL>`。
- en: Publishing services within Docker Swarm will expose a defined service's port
    on all hosts in the cluster. This feature is **Router Mesh**. All hosts will publish
    this service even if they do not really run any service's processes. Docker will
    guide traffic to a service's tasks within the cluster using an internal ingress
    overlay network.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker Swarm中发布服务将在集群中的所有主机上公开定义服务的端口。这个特性是**路由器网格**。所有主机将发布这个服务，即使它们实际上并不运行任何服务进程。Docker将通过内部入口覆盖网络将流量引导到集群中的服务任务。
- en: Remember that all services received a virtual IP address. This IP address will
    be fixed during a service's lifetime. Each service is composed of tasks associated
    with the containers. Docker will run as many tasks, and thus containers, as are
    required for this service to work. Each task will run just one container, with
    its IP address. As containers can run everywhere in the cluster and they are ephemeral
    (between different hosts), they will receive different IP addresses. A service's
    IP addresses are fixed and will create a DNS entry in Docker Swarm's embedded
    DNS. Therefore, all services within an overlay network will be reachable and known
    by their names (and aliases).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，所有服务都会获得一个虚拟IP地址。这个IP地址在服务的生命周期内是固定的。每个服务由与容器关联的任务组成。Docker会运行尽可能多的任务，因此容器，以便这个服务可以工作。每个任务将只运行一个容器，具有其自己的IP地址。由于容器可以在集群中的任何地方运行，并且它们是临时的（在不同主机之间），它们将获得不同的IP地址。服务的IP地址是固定的，并将在Docker
    Swarm的内嵌DNS中创建DNS条目。因此，所有在覆盖网络内的服务都可以通过它们的名称（和别名）被访问和识别。
- en: A similar approach is present in Kubernetes. In this case, services are just
    a grouping of pods. Pods will get different dynamic IP addresses because resilience
    will manage their life cycle, creating new ones if they die. But services will
    always have a fixed IP address during their life. This is also true for Docker
    Swarm. Therefore, we will publish services and internal routing and load balancing
    will guide traffic to pods or tasks' containers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中也有类似的方法。在这种情况下，服务只是一组Pod。Pod将获得不同的动态IP地址，因为弹性将管理它们的生命周期，如果它们死掉了，会创建新的。但是服务在其生命周期内始终有一个固定的IP地址。这对于Docker
    Swarm也是如此。因此，我们将发布服务，内部路由和负载均衡将引导流量到Pod或任务的容器。
- en: Both orchestrators will allow us to bypass these default behaviors, but we are
    not going to dive deep into these ideas because we have covered them in [Chapter
    8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml), *Orchestration Using Docker Swarm*,
    and [Chapter 9](abcbf266-c469-4d84-ad4f-abd321a64b53.xhtml), *Orchestration Using
    Kubernetes*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个编排器都允许我们绕过这些默认行为，但我们不会深入探讨这些想法，因为我们已经在[第8章](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml)，*使用Docker
    Swarm进行编排*，和[第9章](abcbf266-c469-4d84-ad4f-abd321a64b53.xhtml)，*使用Kubernetes进行编排*中进行了覆盖。
- en: Now that we have a basic understanding, we can introduce ingress controllers.
    These are pieces of software that will allow us to publish fewer ports within
    our cluster. They will help us to ensure security by default access, publishing
    fewer ports and only specific application routes. Ingress controllers will provide
    reverse proxy with load balancing capacities to help us publish an application's
    backends running as services inside a container's infrastructure. We will use
    internal networking instead of publishing an application's services. We will just
    publish the ingress controller and all the application's traffic will become internal
    from this endpoint.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对此有了基本的理解，我们可以介绍入口控制器。这些是一些软件组件，它们将允许我们在集群内发布较少的端口。它们将帮助我们通过默认访问控制来确保安全性，只发布较少的端口和特定的应用程序路由。入口控制器将提供反向代理和负载均衡功能，以帮助我们发布作为容器基础设施内服务运行的应用程序后端。我们将使用内部网络而不是发布应用程序的服务。我们只会发布入口控制器，所有应用程序的流量将从此端点变为内部流量。
- en: The ingress controller concept can be applied to both Kubernetes and Docker
    Swarm. Kubernetes has special resources for this to work, but Docker Swarm has
    nothing already prepared. In this case, we will have to use external applications.
    Docker Enterprise does provide an out-of-the-box solution for Docker Swarm services.
    Interlock integrates the ingress controller features described but applied to
    Docker Swarm's behavior.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 入口控制器的概念可以应用于Kubernetes和Docker Swarm。Kubernetes有专门的资源来实现这一点，但Docker Swarm并没有已经准备好的内容。在这种情况下，我们将不得不使用外部应用程序。Docker
    Enterprise确实为Docker Swarm服务提供了一个即用即走的解决方案。Interlock集成了描述的入口控制器功能，但应用于Docker Swarm的行为。
- en: In the next section, we will talk a little about application logic and expected
    behavior on container platforms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要讨论应用程序逻辑以及容器平台上预期的行为。
- en: Understanding an application's logic
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解应用程序的逻辑
- en: We have reviewed how publishing will work for our application's components,
    but should they all be published? The short answer is probably no. Imagine a three-layer
    application. We will have a middle layer for some kind of backend that will consume
    a database and should be accessed through a frontend. In a legacy data center,
    this layered application will probably run each service on a separate node. These
    nodes will run on different subnets to isolate accesses between them with firewalls.
    This architecture is quite common. Backend components will be in the middle, between
    the database and the frontend. The frontend should not access the database. In
    fact, the database should only be accessible from the backend component. Therefore,
    should we publish the database component service? The frontend component will
    access the backend, but do we have to publish the backend component? No, but the
    frontend should be able to access the backend service. Users and other applications
    will use frontend components to consume our application. Therefore, only frontend
    components should be published. This guarantees security by using a container's
    features instead of firewalls and subnets, but the final outcome is the same.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经回顾了如何为应用程序的组件进行发布，但它们都应该被发布吗？简短的回答可能是否定的。假设我们有一个三层应用程序。我们会有一个中间层作为某种后端，它会消耗一个数据库，并且应该通过前端来访问。在传统的数据中心中，这种分层应用程序可能会将每个服务运行在不同的节点上。这些节点会在不同的子网中运行，以通过防火墙隔离它们之间的访问。这种架构相当常见。后端组件位于中间层，处于数据库和前端之间。前端不应该直接访问数据库。实际上，数据库应该仅能从后端组件访问。那么，我们是否应该发布数据库组件服务？前端组件将访问后端，但我们是否必须发布后端组件？不，但前端应该能够访问后端服务。用户和其他应用程序将使用前端组件来消费我们的应用程序。因此，应该只发布前端组件。这通过使用容器的功能而不是防火墙和子网来保证安全性，但最终结果是一样的。
- en: 'Docker Swarm allows us to implement multi-networking applications using overlay
    custom networks. These will allow us to interconnect components of applications
    from different applications sharing some networks. This can become complex if
    many services from different applications have to consume one service. This many-to-one
    networking behavior may not work correctly in your environment. To avoid this
    complexity, you have two options:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 允许我们使用覆盖自定义网络来实现多网络应用程序。这些网络可以使我们将来自不同应用程序的组件互联，且共享某些网络。如果来自不同应用程序的许多服务需要访问一个服务，那么这可能会变得复杂。这种多对一的网络行为可能在你的环境中无法正常工作。为了避免这种复杂性，你有两个选择：
- en: Use flat networks, either moving to Kubernetes or defining large overlay subnets.
    The first option is better in this case as Kubernetes provides network policies
    to improve flat network security. Large networks in Docker Swarm do not provide
    any security for their components. It is up to you to improve it with external
    tools.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扁平网络，可以迁移到 Kubernetes 或定义大型覆盖子网。在这种情况下，第一个选项更好，因为 Kubernetes 提供了网络策略来增强扁平网络的安全性。而
    Docker Swarm 中的大型网络则不会为其组件提供任何安全性。你需要使用外部工具来提高安全性。
- en: Publish this common service and allow other applications to consume it as if
    they were cluster-external. We will use DNS entries for our service and other
    applications will know how to access it. We will use load balancers and/or API
    managers to improve availability and security. These external components are beyond
    the scope of this book, but they will provide non-container-based application
    behavior.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布这个常见的服务并允许其他应用程序像访问集群外部一样访问它。我们将使用 DNS 记录来为服务命名，其他应用程序将知道如何访问它。我们将使用负载均衡器和/或
    API 管理器来提高可用性和安全性。这些外部组件超出了本书的范围，但它们将提供非容器化应用程序的行为。
- en: Now that we understand how applications can be deployed and published, we will
    introduce the concept of ingress controllers and their components before getting
    into Docker Enterprise's Interlock.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了应用程序如何部署和发布，在介绍 Docker Enterprise 的 Interlock 之前，我们将先介绍 ingress 控制器及其组件的概念。
- en: Publishing applications in Kubernetes using ingress controllers
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ingress 控制器在 Kubernetes 中发布应用程序
- en: As mentioned previously, ingress controllers are special Kubernetes components
    that are deployed to publish applications and services.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，ingress 控制器是特殊的 Kubernetes 组件，部署后用于发布应用程序和服务。
- en: Ingress resources will define rules and routes required to expose HTTP and HTTPS
    deployed services.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 入口资源将定义暴露 HTTP 和 HTTPS 部署服务所需的规则和路由。
- en: An ingress controller will complete this equation as a reverse proxy, adding
    load-balancing capabilities. These features can be arranged by an external edge
    router or a cluster-deployed software proxy. Any of these will manage traffic
    using dynamic configurations built using ingress resource rules.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 入口控制器将作为反向代理完成这个过程，增加负载均衡功能。这些功能可以通过外部边缘路由器或集群内部部署的软件代理来配置。任何这些都将使用动态配置来管理流量，动态配置是基于入口资源规则构建的。
- en: We can also use ingress for TCP and UDP raw services. This will depend on which
    ingress reverse proxy has been deployed. It is customary to publish an application's
    services using protocols other than HTTP and HTTPS. In this case, we can use either
    Router Mesh on Docker Swarm or NodePort/LoadBalancer on Kubernetes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用入口控制器为 TCP 和 UDP 原始服务提供发布。这将取决于已部署的哪个入口反向代理。通常，发布一个应用的服务时，使用的是 HTTP 和
    HTTPS 以外的协议。在这种情况下，我们可以使用 Docker Swarm 上的 Router Mesh 或 Kubernetes 上的 NodePort/LoadBalancer。
- en: 'An ingress resource may look like the following YAML file:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个入口资源可能看起来像下面的 YAML 文件：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Ingress rules contain an optional host key used to associate this resource with
    a proxied host header for inbound traffic. All subsequent rules will be applied
    to this host.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 入口规则包含一个可选的 host 键，用于将此资源与传入流量的代理主机头关联。所有后续规则将应用于此主机。
- en: It will also contain a list of paths, associated with different services, defined
    as proxied backends. All requests matching the host and path keys will be redirected
    to listed backends. Deployed services and ports will define each backend for an
    application.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它还将包含一组路径列表，每个路径与不同的服务相关联，定义为代理后的后端。所有匹配主机和路径键的请求将被重定向到列出的后端。部署的服务和端口将为每个应用定义后端。
- en: We will define a default backend to route any request not matching any ingress
    resource's rules.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个默认的后端，用于路由任何不匹配入口资源规则的请求。
- en: As mentioned, ingress controllers will deploy ingress rules on different proxy
    services. We will either use existing external hardware or software load balancers
    or we will deploy these components within the cluster. As these pieces are interchangeable,
    different deployments will provide different behaviors, although ingress resource
    configurations will be similar. These deployments should be published, but backend
    services do not require direct external access. Ingress controller pieces will
    manage routes and rules required to access services.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，入口控制器将部署不同代理服务上的入口规则。我们将使用现有的外部硬件或软件负载均衡器，或者将这些组件部署在集群内。由于这些组件是可互换的，不同的部署将提供不同的行为，尽管入口资源配置将是相似的。这些部署应该被发布，但后端服务不需要直接外部访问。入口控制器组件将管理访问服务所需的路由和规则。
- en: Ingress controllers will be published using any of this chapter's described
    methods, although we will usually use NodePort- and LoadBalancer-type services.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 入口控制器将使用本章描述的任何方法发布，尽管我们通常会使用 NodePort 类型和 LoadBalancer 类型的服务。
- en: We can deploy multiple ingress controllers on any Kubernetes cluster. This is
    important because we can improve isolation on multi-tenant environments using
    specific ingress controllers for each customer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在任何 Kubernetes 集群上部署多个入口控制器。这一点非常重要，因为我们可以通过为每个客户使用特定的入口控制器，改善多租户环境中的隔离性。
- en: 'We have described a layer 7 routing architecture for Kubernetes. The following
    diagram shows an example of ingress controller deployment. An external load balancer
    will route a user''s requests to the ingress controller. This component will review
    ingress resource tables and route traffic to the appropriate internal service''s
    ClusterIP. Then, Kubernetes will manage internal service-to-pod communications
    to ensure that a user''s requests reach the service''s associated pods:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了一种适用于 Kubernetes 的第 7 层路由架构。下图展示了一个入口控制器部署的示例。外部负载均衡器将用户的请求路由到入口控制器。该组件将审查入口资源表并将流量路由到适当的内部服务的
    ClusterIP。然后，Kubernetes 将管理内部服务与 Pod 之间的通信，确保用户的请求能够到达服务关联的 Pod：
- en: '![](img/0bbf197b-2648-4bc0-94e9-cc86c2add1dc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bbf197b-2648-4bc0-94e9-cc86c2add1dc.png)'
- en: In the next section, we will learn how Docker Enterprise deploys this publishing
    logic for Docker Swarm services.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习 Docker Enterprise 如何为 Docker Swarm 服务部署此发布逻辑。
- en: Using Interlock to publish applications deployed in Docker Swarm
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Interlock 发布在 Docker Swarm 中部署的应用
- en: Interlock is based on the ingress controller's logic described previously. Docker
    Swarm architecture is different. Its differences are even more pronounced when
    we talk about Kubernetes and Docker Swarm networking implementations. Kubernetes
    provides a flat network architecture, as we have seen. Multiple networks within
    the cluster will add additional security features, but also more complexity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock 基于之前描述的入口控制器的逻辑。Docker Swarm 架构不同，当我们谈到 Kubernetes 和 Docker Swarm
    的网络实现时，它们的差异更加明显。Kubernetes 提供了一个扁平化的网络架构，正如我们所看到的那样。集群中的多个网络将增加额外的安全功能，但也会带来更多的复杂性。
- en: Interlock substitutes the previous Docker Enterprise's router mesh L7 routing
    implementation. Router mesh was available in previous UCP releases. Interlock
    appeared in the 2.0 release of Docker Enterprise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock 替代了以前 Docker Enterprise 的路由网格 L7 路由实现。路由网格在以前的 UCP 版本中可用。Interlock
    出现在 Docker Enterprise 的 2.0 版本中。
- en: Interlock will integrate Docker Swarm and Docker Remote API features to isolate
    and configure dynamically an application proxy such as NGINX or HA-Proxy using
    extensions. Interlock will use Docker Swarm's well-known objects, such as configs
    and secrets, to manage proxy required configurations. We will be able to manage
    TLS tunnels and integrate rolling updates (and rollbacks) and zero-downtime reconfigurations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock 将集成 Docker Swarm 和 Docker Remote API 功能，以使用扩展动态地隔离和配置应用代理（如 NGINX
    或 HA-Proxy）。Interlock 将利用 Docker Swarm 的著名对象，如配置和机密，来管理代理所需的配置。我们将能够管理 TLS 隧道，并集成滚动更新（和回滚）以及零停机时间的重新配置。
- en: 'Interlock''s logic is distributed in three main services:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock 的逻辑分布在三个主要服务中：
- en: The **Interlock service** is the main process. It will interact with the Docker
    Remote API to monitor Docker Swarm events. This service will create all the configurations
    required by a proxy to route requests to an application's endpoints, including
    headers, routes, and backends. It will also manage extensions and proxy services.
    The Interlock service will be consumed via its gRPC API. Other Interlock services
    and extensions will access Interlock's API to get their prepared configurations.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Interlock 服务** 是主进程。它将与 Docker Remote API 交互，以监控 Docker Swarm 事件。此服务将创建代理路由请求到应用程序端点所需的所有配置，包括头部、路由和后端。它还将管理扩展和代理服务。Interlock
    服务将通过其 gRPC API 被使用。其他 Interlock 服务和扩展将通过访问 Interlock 的 API 来获取它们准备好的配置。'
- en: The **Interlock-extension** service will query Interlock's API for the configurations
    created upstream. Extensions will use this pre-configuration to prepare real configurations
    for the extension-associated proxy. For proxy services such as NGINX or HA-Proxy,
    deployed within the cluster, the Interlock-extension service will create its configurations
    and then these will be sent to the Interlock service via its API. The Interlock
    service will then create a config object within the Docker Swarm cluster for the
    deployed proxy services.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Interlock-extension** 服务将查询 Interlock 的 API 以获取上游创建的配置。扩展将使用此预配置来准备与扩展相关联的代理的真实配置。对于部署在集群中的代理服务，如
    NGINX 或 HA-Proxy，Interlock-extension 服务将创建其配置，然后通过 API 将这些配置发送给 Interlock 服务。接着，Interlock
    服务将在 Docker Swarm 集群中为部署的代理服务创建一个配置对象。'
- en: The **Interlock-proxy** is the proxy service. It will use configurations stored
    in config objects to route and manage HTTP and HTTPS requests.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Interlock-proxy** 是代理服务。它将使用存储在配置对象中的配置来路由和管理 HTTP 和 HTTPS 请求。'
- en: Docker Enterprise deploys NGINX as the Interlock-proxy. Docker Swarm cluster
    changes affecting published services will be updated dynamically.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Enterprise 将 NGINX 部署为 Interlock-proxy。影响已发布服务的 Docker Swarm 集群变更将动态更新。
- en: Interlock allows DevOps groups to implement **Blue-Green** and **Canary** service
    deployment. These will help DevOps to deploy application upgrades without impacting
    access on the part of users.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock 允许 DevOps 团队实现 **蓝绿部署** 和 **金丝雀部署** 服务部署。这些部署方式将帮助 DevOps 在不影响用户访问的情况下进行应用程序升级。
- en: 'The following diagram shows a basic Interlock schema. As mentioned, Interlock
    looks like an ingress controller. The following schema represents common applications''
    traffic. User requests will be forwarded by the external load balancer to the
    Interlock proxy instances. This component will review its rules and forward requests
    to the configured service''s IP address. Then, Docker Swarm will use internal
    routing and load balancing to forward requests to the service''s tasks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个基本的 Interlock 架构。如前所述，Interlock 看起来像一个入口控制器。以下架构表示常见应用程序的流量。用户请求将由外部负载均衡器转发到
    Interlock 代理实例。该组件将检查其规则，并将请求转发到配置的服务 IP 地址。然后，Docker Swarm 将使用内部路由和负载均衡将请求转发到服务的任务：
- en: '![](img/2212ed99-bd4e-4df3-abbb-268953b64c86.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2212ed99-bd4e-4df3-abbb-268953b64c86.png)'
- en: 'Interlock''s layer 7 routing supports the following features:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock 的第7层路由支持以下功能：
- en: Since Interlock services run as Docker Swarm services, high availability based
    on resilience is granted.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 Interlock 服务作为 Docker Swarm 服务运行，因此提供基于弹性的高可用性。
- en: Interlock interacts with the Docker API, hence, dynamic and automatic configuration
    is provided.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Interlock 与 Docker API 交互，因此提供动态和自动配置。
- en: 'Automatic configuration: Interlock uses the Docker API for configuration. You
    do not have to manually update or restart anything to make services available.
    UCP monitors your services and automatically reconfigures proxy services.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动配置：Interlock 使用 Docker API 进行配置。你不需要手动更新或重启任何服务即可使服务可用。UCP 会监控你的服务并自动重新配置代理服务。
- en: We can scale a proxy service up and down because it is deployed as a separate
    component.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以对代理服务进行上下扩展，因为它作为一个独立的组件进行部署。
- en: Interlock provides TLS tunneling, either for TLS termination or TCP passthrough.
    Certificates will be stored using Docker Swarm's secret objects.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Interlock 提供 TLS 隧道服务，可以用于 TLS 终止或 TCP 透传。证书将通过 Docker Swarm 的机密对象进行存储。
- en: Interlock supports request routing by context or paths.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Interlock 支持根据上下文或路径进行请求路由。
- en: We can deploy multiple extensions and proxy configurations simultaneously to
    isolate accesses on multi-tenant or multi-region environments.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以同时部署多个扩展和代理配置，以便在多租户或多区域环境中隔离访问。
- en: Interlock-proxy and Interlock-extension services' instances run on worker nodes.
    This will improve security, isolating the control plane from publishing services.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Interlock-proxy 和 Interlock-extension 服务的实例运行在工作节点上。这将提高安全性，将控制平面与发布服务隔离开来。
- en: We can use host mode networking to bypass default routing mesh services' behavior
    for the Interlock-proxy service. This will improve network performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用主机模式网络来绕过默认的路由网格服务行为，以优化 Interlock-proxy 服务的网络性能。
- en: 'Publishing services using Interlock are based on label customization. We will
    require at least the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Interlock 发布服务是基于标签自定义的。我们至少需要以下内容：
- en: '`com.docker.lb.hosts`: This label will manage the host header, hence the service''s
    published name.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`com.docker.lb.hosts`：此标签将管理主机头，因此也会管理服务的发布名称。'
- en: '`com.docker.lb.port`: The internal service''s port is also required and associated
    using this label. Remember that this port should not be published.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`com.docker.lb.port`：内部服务的端口也是必需的，并且通过此标签进行关联。记住，该端口不应被公开。'
- en: '`com.docker.lb.network`: This defines which network the Interlock-proxy service
    should attach to in order to be able to communicate with the defined service.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`com.docker.lb.network`：此标签定义了 Interlock-proxy 服务应连接到哪个网络，以便能够与定义的服务进行通信。'
- en: 'Other labels will allow us to modify configured-proxy behavior and features.
    This is a list of some other important labels:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其他标签将允许我们修改已配置的代理行为和功能。以下是一些其他重要标签的列表：
- en: '| **Labels** | **Description** |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **标签** | **描述** |'
- en: '| `com.docker.lb.ssl_cert` and `com.docker.lb.ssl_key` | These keys allow us
    to integrate the backend''s certificate and key. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `com.docker.lb.ssl_cert` 和 `com.docker.lb.ssl_key` | 这些密钥允许我们集成后端的证书和密钥。
    |'
- en: '| `com.docker.lb.sticky_session_cookie` | We will set a cookie to allow sticky
    sessions to define a service instance''s backends. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `com.docker.lb.sticky_session_cookie` | 我们将设置一个 cookie，以允许粘性会话定义服务实例的后端。
    |'
- en: '| `com.docker.lb.backend_mode` | This stipulates how requests reach different
    backends (it defaults to `vip`, which is also the default mode for Docker Swarm
    services). |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `com.docker.lb.backend_mode` | 这规定了请求如何到达不同的后端（默认为 `vip`，这是 Docker Swarm
    服务的默认模式）。 |'
- en: '| `com.docker.lb.ssl_passthrough` | We can close tunnels on application backends,
    thereby enabling SSL passthrough. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `com.docker.lb.ssl_passthrough` | 我们可以关闭应用后端的隧道，从而启用 SSL 透传。 |'
- en: '| `com.docker.lb.redirects` | This key allows us to redirect requests to different
    FQDNs using host header definitions. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `com.docker.lb.redirects` | 这个键允许我们通过主机头定义将请求重定向到不同的 FQDN。 |'
- en: You can review all the available labels in Docker Enterprise's documentation
    ([https://docs.docker.com/ee/ucp/interlock/usage/labels-reference](https://docs.docker.com/ee/ucp/interlock/usage/labels-reference)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看 Docker Enterprise 文档中所有可用的标签([https://docs.docker.com/ee/ucp/interlock/usage/labels-reference](https://docs.docker.com/ee/ucp/interlock/usage/labels-reference))。
- en: If a service is isolated on just one network, we don't need to add `com.docker.lb.network`,
    but it will be required if it is combined with `com.docker.lb.ssl_passthrough`.
    If we publish services using stacks, we will use the stack's name.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务仅隔离在一个网络中，我们不需要添加 `com.docker.lb.network`，但如果它与 `com.docker.lb.ssl_passthrough`
    配合使用，则需要。如果我们使用堆栈发布服务，我们将使用堆栈的名称。
- en: There are many options and configurations available for Interlock's described
    components. We will be allowed to change the proxy's default port, the Docker
    API socket, and the polling interval, among other things. Extensions will have
    many features and configurations depending on external load balancing integrations.
    We recommend that you review all the available keys and configurations in Docker
    Enterprise's documentation ([https://docs.docker.com/ee/ucp/interlock/config](https://docs.docker.com/ee/ucp/interlock/config)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Interlock 所描述的组件，有许多可用的选项和配置。我们将能够更改代理的默认端口、Docker API 套接字和轮询间隔等。扩展功能将有许多特性和配置，具体取决于外部负载均衡集成。我们建议你查看
    Docker Enterprise 文档中所有可用的键和配置([https://docs.docker.com/ee/ucp/interlock/config](https://docs.docker.com/ee/ucp/interlock/config))。
- en: We recommend reviewing this link, [https://success.docker.com/article/how-to-troubleshoot-layer-7-loadbalancing](https://success.docker.com/article/how-to-troubleshoot-layer-7-loadbalancing),
    to get some interesting tips regarding the troubleshooting of Interlock-related
    issues.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议查看这个链接，[https://success.docker.com/article/how-to-troubleshoot-layer-7-loadbalancing](https://success.docker.com/article/how-to-troubleshoot-layer-7-loadbalancing)，以获取有关排查
    Interlock 相关问题的一些有趣的技巧。
- en: In the next chapter, we will introduce Docker Trusted Registry. This tool provides
    a secure image store, integrating image signing features and vulnerability scanning.
    These features, among others, provide a production-ready image store solution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍 Docker Trusted Registry。这个工具提供了一个安全的镜像存储，集成了镜像签名功能和漏洞扫描功能。这些功能，等等，提供了一个生产就绪的镜像存储解决方案。
- en: Reviewing Interlock usage
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾 Interlock 的使用
- en: We will now review some examples of Interlock usage.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将回顾一些 Interlock 使用的示例。
- en: 'We will need to enable Interlock in Docker Enterprise. It is disabled by default
    and is part of the Admin Settings section. We can change the default ports (`8080`
    for HTTP and `8443` for secure access using HTTPS), as shown in the following
    screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 Docker Enterprise 中启用 Interlock。它默认是禁用的，并且是管理员设置部分的一部分。我们可以更改默认端口（HTTP
    的 `8080` 和使用 HTTPS 的安全访问端口 `8443`），如下所示的截图：
- en: '![](img/f588779d-7176-4fe4-b4ab-a1150ce60a0e.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f588779d-7176-4fe4-b4ab-a1150ce60a0e.jpg)'
- en: 'Once enabled, Interlock''s services are created, which we can verify by using
    the admin''s UCP bundle and executing `docker service ls`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 启用后，Interlock 的服务会被创建，我们可以通过使用管理员的 UCP 包并执行 `docker service ls` 来验证：
- en: '[PRE1]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is important to observe that, by default, Interlock-proxy will not be isolated
    on worker nodes if there are not enough nodes to run the required number of instances.
    We can change this behavior by using simple location constraints ([https://docs.docker.com/ee/ucp/interlock/deploy/production](https://docs.docker.com/ee/ucp/interlock/deploy/production)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，默认情况下，如果没有足够的节点来运行所需数量的实例，Interlock-proxy 将不会在工作节点上隔离。我们可以通过使用简单的位置约束来改变这一行为（[https://docs.docker.com/ee/ucp/interlock/deploy/production](https://docs.docker.com/ee/ucp/interlock/deploy/production)）。
- en: 'For this example, we will use the `colors` application again. We used this
    simple application in [Chapter 5](1c86479c-e4f5-4508-9eca-d29bb3dbaf4b.xhtml),
    *Deploying Multi-Container Applications*. This is a simple `docker-compose` file
    prepared to deploy a `colors` service. We will use a random color, leaving the
    `COLORS` variable empty. We will create a `colors-stack.yml` file with the following
    content:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将再次使用 `colors` 应用程序。我们在[第5章](1c86479c-e4f5-4508-9eca-d29bb3dbaf4b.xhtml)，*部署多容器应用程序*中使用了这个简单的应用程序。这是一个简单的
    `docker-compose` 文件，用于部署 `colors` 服务。我们将使用一个随机颜色，并将 `COLORS` 变量留空。我们将创建一个名为 `colors-stack.yml`
    的文件，内容如下：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will connect to Docker Enterprise with a valid user using their bundle.
    For this lab, we will use the `admin` user that we created during installation.
    We will download the user''s `ucp` bundle using any of the procedures described
    in [Chapter 11](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml), *Universal Control
    Plane*. Once downloaded and unzipped, we will just load UCP''s environment using
    `source env.sh`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用有效的用户及其捆绑包连接到Docker Enterprise。对于本实验，我们将使用在安装过程中创建的`admin`用户。我们将按照[第11章](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml)中描述的任何流程下载该用户的`ucp`捆绑包，*Universal
    Control Plane*。下载并解压后，我们只需使用`source env.sh`加载UCP环境：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the UCP environment is loaded, we will use the book''s Git repository
    ([https://github.com/frjaraur/dca-book-code.git](https://github.com/frjaraur/dca-book-code.git)).
    Interlock''s labs can be found under the `interlock-lab` directory. We will deploy
    the `colors` stack using `docker stack deploy -c colors-stack.yml lab`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 加载完UCP环境后，我们将使用本书的Git仓库（[https://github.com/frjaraur/dca-book-code.git](https://github.com/frjaraur/dca-book-code.git)）。Interlock的实验位于`interlock-lab`目录下。我们将使用`docker
    stack deploy -c colors-stack.yml lab`部署`colors`堆栈：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will review how `colors` instances are distributed within the cluster by
    using `docker stack ps`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`docker stack ps`查看`colors`实例在集群中的分布情况：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We enabled Interlock on UCP''s Admin Settings section. We used the default
    port, so we should access our deployed service on the `8080` port (because we
    are using HTTP in this lab). Notice that we have not used any `port` key in the
    `docker-compose` file. We have not published any service''s port. Let''s check
    whether Interlock is working by specifying the required host header, `colors.lab.local`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在UCP的管理员设置部分启用了Interlock。我们使用了默认端口，因此应该可以在`8080`端口访问我们部署的服务（因为我们在本实验中使用的是HTTP）。请注意，我们没有在`docker-compose`文件中使用任何`port`键，也没有发布任何服务的端口。让我们通过指定所需的主机头`colors.lab.local`来检查Interlock是否正常工作：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output may change and we will launch some requests to ensure that we get
    different backends (we deployed three instances). If we do not specify any host
    header, a default one will be used. If none was configured (default behavior),
    we will get a proxy error. As we are using NGINX (default), we will get a `503`
    error:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能会有所不同，我们将发起一些请求，确保得到不同的后端（我们部署了三个实例）。如果没有指定任何主机头，将使用默认值。如果没有配置（默认行为），我们将得到一个代理错误。由于我们使用的是NGINX（默认），我们将得到一个`503`错误：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can change the default Interlock''s backend using the special label `com.docker.lb.default_backend:
    "true"`, associated with one of our services. This will act as a default site
    when headers don''t match any configured service.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以使用特殊标签`com.docker.lb.default_backend: "true"`来更改默认的Interlock后端，并将其与我们的某个服务关联。当请求的头信息与任何已配置的服务不匹配时，这将作为默认站点。'
- en: 'Let''s remove this lab before continuing. We will use `docker stack rm`. We
    will probably get an error because stacks will now have to be removed carefully:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先移除这个实验。我们将使用`docker stack rm`。由于堆栈现在需要小心移除，我们可能会遇到一个错误：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This error is normal. The Interlock-proxy component is attached to our application''s
    network, hence it cannot be removed. Interlock will refresh configurations every
    few seconds (Docker API polls will be launched every 3 seconds and, after these
    intervals, Interlock will manage the required changes). If we just wait a few
    seconds and launch the removal command again, it will delete the stack''s remaining
    components (network):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误是正常的。Interlock-proxy组件已连接到我们的应用网络，因此无法移除。Interlock会每隔几秒刷新一次配置（Docker API会每3秒轮询一次，经过这些间隔后，Interlock将管理所需的更改）。如果我们等待几秒钟并再次执行移除命令，它将删除堆栈剩余的组件（网络）：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We will now test a simple redirection using the `com.docker.lb.redirects` key.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用`com.docker.lb.redirects`键测试一个简单的重定向。
- en: Simple application redirection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的应用重定向
- en: 'In this example, we will review how we can redirect requests from one service
    to another. This can be interesting when we want to migrate users from an old
    application to a newer release, at application level. We are not talking about
    an image upgrade in this case. We will simply create a new overlay network using
    `docker network create`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将查看如何将请求从一个服务重定向到另一个服务。当我们希望将用户从旧应用程序迁移到新版本时，这可能非常有用。这里我们讨论的不是镜像升级，而是简单地使用`docker
    network create`创建一个新的覆盖网络：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now create a simple web server application service (the smallest NGINX
    image, `nginx:alpine`). Notice that we will add to host headers inside the `com.docker.lb.hosts`
    label. We have also added `com.docker.lb.redirects` to ensure that all requests
    sent to `http://old.lab.local` will be redirected to `http://new.lab.local`. This
    is how this service definition will appear:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个简单的 Web 服务器应用程序服务（最小的 NGINX 镜像，`nginx:alpine`）。请注意，我们将在`com.docker.lb.hosts`标签中添加主机头。我们还添加了`com.docker.lb.redirects`，以确保所有发送到`http://old.lab.local`的请求将被重定向到`http://new.lab.local`。服务定义如下所示：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we test access to one of our UCP nodes on port `8080`, using `old.lab.local`
    as the host header, we will be redirected to `http://new.lab.local`. We added
    `-L` to the `curl` command to allow the required redirection:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们测试对其中一个 UCP 节点的访问，端口为`8080`，并使用`old.lab.local`作为主机头，系统会将我们重定向到`http://new.lab.local`。我们在`curl`命令中添加了`-L`选项，以允许所需的重定向：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that `new.lab.local` was a dummy FQDN, hence we cannot resolve it, but
    the test request was forwarded to this new application site.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`new.lab.local`是一个虚拟的 FQDN，因此我们无法解析它，但测试请求已转发到这个新的应用程序站点。
- en: We will now deploy an example service that is protected using TLS certificates.
    Interlock will manage its certificates and access will be secure.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将部署一个使用 TLS 证书保护的示例服务。Interlock 将管理其证书，并确保访问是安全的。
- en: Publishing a service securely using Interlock with TLS
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TLS 通过 Interlock 安全发布服务
- en: In this example, we will deploy a service that should be published securely
    using TLS. We can create tunnels from users directly to our service, configuring
    Interlock as a transparent proxy, or we can allow Interlock to manage tunnels.
    In this case, a service can be deployed using HTTP, but HTTPS will be required
    from the user's perspective. Users will interact with the Interlock-proxy component
    before reaching the defined service's backends.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将部署一个应该使用 TLS 安全发布的服务。我们可以直接为用户创建通道到我们的服务，将 Interlock 配置为透明代理，或者允许 Interlock
    管理通道。在这种情况下，服务可以通过 HTTP 部署，但从用户的角度来看，HTTPS 将是必需的。用户将首先与 Interlock-proxy 组件交互，然后才能访问定义的服务后端。
- en: 'For this example, we will use the `colors` application again with random configuration.
    We will use the `colors-stack-https.yml` file with the following content:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将再次使用`colors`应用程序，配以随机配置。我们将使用`colors-stack-https.yml`文件，内容如下：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We will create a sample key and an associated certificate and these will be
    integrated inside Interlock's configuration automatically.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个示例密钥和一个关联的证书，这些将自动集成到 Interlock 的配置中。
- en: It is always relevant to review Interlock's component logs using Docker service
    logs; for example, we will detect configuration errors using `docker service logs
    ucp-interlock`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 始终建议使用 Docker 服务日志查看 Interlock 组件的日志；例如，我们可以使用`docker service logs ucp-interlock`检测配置错误。
- en: 'We will use `openssl` to create a certificate that is valid for 365 days:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`openssl`创建一个有效期为365天的证书：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once these keys and certificates are created, we will connect to Docker Enterprise
    using the `admin` user again. Although the admin''s environment will probably
    already be loaded (if you are following these labs one by one), we will load the
    `ucp` environment using `source env.sh`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些密钥和证书创建完成，我们将再次使用`admin`用户连接到 Docker Enterprise。尽管管理员环境可能已经加载（如果你按顺序跟随这些实验），我们仍将使用`source
    env.sh`加载`ucp`环境：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the UCP environment is loaded, we will use this book''s example `colors-stack-ssl.yaml`
    file. We will deploy the `colors` stack with HTTPS using `docker stack deploy
    -c colors-stack-https.yml lab`. This directory also contains a prepared certificate
    and key:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 UCP 环境加载完成，我们将使用本书的示例`colors-stack-ssl.yaml`文件。我们将通过`docker stack deploy
    -c colors-stack-https.yml lab`部署带有 HTTPS 的`colors`堆栈。该目录还包含了准备好的证书和密钥：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will review how `colors` instances are distributed within the cluster using
    `docker stack ps`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过`docker stack ps`查看`colors`实例在集群中的分布情况：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We enabled Interlock on UCP's Admin Settings section. We used the default port,
    hence we should access our deployed service on the `8443` port (because we are
    using HTTPS). Notice that we have not used any `port` key on the `docker-compose`
    file. We have not published any service's port.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 UCP 的管理员设置部分启用了 Interlock。我们使用了默认端口，因此我们应通过`8443`端口访问我们部署的服务（因为我们使用的是 HTTPS）。请注意，我们在`docker-compose`文件中没有使用任何`port`键。我们没有发布任何服务的端口。
- en: 'We can review Interlock''s proxy configuration by reading the associated `com.docker.interlock.proxy.<ID>`
    configuration object. We can use `docker config inspect` and filter its output.
    First, we will obtain the current `ucp-interlock-proxy` configuration object:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过读取相关的 `com.docker.interlock.proxy.<ID>` 配置对象来查看 Interlock 的代理配置。我们可以使用
    `docker config inspect` 并过滤其输出。首先，我们将获取当前的 `ucp-interlock-proxy` 配置对象：
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we will just inspect this object:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将检查这个对象：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Inspecting the Interlock-proxy configuration can be very useful when it comes
    to troubleshooting Interlock issues. Try to include one service or stack at a
    time. This will avoid the mixing of configurations and help us to follow incorrect
    configuration issues.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Interlock 代理配置在排查 Interlock 问题时非常有用。尽量一次检查一个服务或堆栈。这样可以避免配置混合，帮助我们追踪错误配置问题。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered Docker Enterprise's publishing features. We learned different
    publishing strategies for Docker Swarm and Kubernetes and how these tools can
    be integrated inside Docker Enterprise.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Docker Enterprise 的发布功能。我们学习了 Docker Swarm 和 Kubernetes 的不同发布策略，以及如何将这些工具集成到
    Docker Enterprise 中。
- en: We have seen how these methods also improve an application's security by isolating
    different layers and allowing us to publish only frontend and requisite services.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到这些方法如何通过隔离不同层次并允许我们仅发布前端和必要的服务来提高应用程序的安全性。
- en: The next chapter will teach us how Docker Enterprise implements a fully secure
    and production-ready image store solution.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将教我们 Docker Enterprise 如何实现一个完全安全、适用于生产的镜像存储解决方案。
- en: Questions
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which labels are required to publish a service using Interlock?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Interlock 发布服务需要哪些标签？
- en: a) `com.docker.lb.backend_mode`
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: a) `com.docker.lb.backend_mode`
- en: b) `com.docker.lb.port`
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: b) `com.docker.lb.port`
- en: c) `com.docker.lb.hosts`
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: c) `com.docker.lb.hosts`
- en: d) `com.docker.lb.network`
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: d) `com.docker.lb.network`
- en: Which one of these processes is not part of Interlock?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪一项不是 Interlock 过程的一部分？
- en: a) `ucp-interlock`
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: a) `ucp-interlock`
- en: b) `ucp-interlock-controller`
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: b) `ucp-interlock-controller`
- en: c) `ucp-interlock-extension`
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: c) `ucp-interlock-extension`
- en: d) `ucp-interlock-proxy`
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: d) `ucp-interlock-proxy`
- en: Where do Interlock processes run within Docker Enterprise nodes?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Interlock 进程在 Docker 企业节点中运行的位置是哪里？
- en: a) `ucp-interlock` runs on Docker Swarm's leader.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: a) `ucp-interlock` 在 Docker Swarm 的领导者节点上运行。
- en: b) `ucp-interlock-extension` runs on any manager.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: b) `ucp-interlock-extension` 在任何管理节点上运行。
- en: c) `ucp-interlock-proxy` runs only on workers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: c) `ucp-interlock-proxy` 仅在工作节点上运行。
- en: d) None of the above answers are correct.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: d) 以上答案均不正确。
- en: Which features does Interlock support?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Interlock 支持哪些功能？
- en: a) SSL/TLS endpoint management
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: a) SSL/TLS 端点管理
- en: b) Transparent proxy or SSL/TLS passthrough
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: b) 透明代理或 SSL/TLS 透传
- en: c) Dynamic configuration using the Docker API
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: c) 使用 Docker API 进行动态配置
- en: d) TCP/UDP publishing
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: d) TCP/UDP 发布
- en: Which of the following statements regarding the publishing of applications on
    container-orchestrated environments are true?
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下关于在容器编排环境中发布应用程序的哪些陈述是正确的？
- en: a) Ingress controllers and Interlock have a common logic using reverse proxy
    services for publishing applications.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: a) Ingress 控制器和 Interlock 采用相同的逻辑，通过反向代理服务发布应用程序。
- en: b) Ingress controllers help us to publish applications securely by exposing
    only required services.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: b) Ingress 控制器通过仅暴露必要的服务，帮助我们安全地发布应用程序。
- en: c) Interlock requires access to an application's front service networks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: c) Interlock 需要访问应用程序的前端服务网络。
- en: d) None of these premises are true.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: d) 这些前提条件都不正确。
- en: Further reading
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following links for more information regarding the topics covered
    in this chapter:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章所涉及主题的更多信息，请参考以下链接：
- en: 'Docker Interlock documentation: [https://docs.docker.com/ee/ucp/interlock/](https://docs.docker.com/ee/ucp/interlock/)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Interlock 文档：[https://docs.docker.com/ee/ucp/interlock/](https://docs.docker.com/ee/ucp/interlock/)
- en: 'Universal Control Plane Service Discovery and Load Balancing for Swarm: [https://success.docker.com/article/ucp-service-discovery-swarm](https://success.docker.com/article/ucp-service-discovery-swarm)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 的通用控制平面服务发现与负载均衡：[https://success.docker.com/article/ucp-service-discovery-swarm](https://success.docker.com/article/ucp-service-discovery-swarm)
- en: 'Universal Control Plane Service Discovery and Load Balancing for Kubernetes:
    [https://success.docker.com/article/ucp-service-discovery-k8s](https://success.docker.com/article/ucp-service-discovery-k8s)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 的通用控制平面服务发现与负载均衡：[https://success.docker.com/article/ucp-service-discovery-k8s](https://success.docker.com/article/ucp-service-discovery-k8s)
