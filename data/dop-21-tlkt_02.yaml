- en: Setting Up and Operating a Swarm Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置和操作Swarm集群
- en: '*Organizations which design systems … are constrained to produce designs which
    are copies of the communication structures of these organizations'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*设计系统的组织……受到这些组织的通信结构的限制，只能生产这些组织通信结构的复制品。*'
- en: –M.Conway*
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: –M. Conway*
- en: Many will tell you that they have a *scalable system*. After all, scaling is
    easy. Buy a server, install WebLogic (or whichever other monster application server
    you're using) and deploy your applications. Then wait for a few weeks until you
    discover that everything is so *fast* that you can click a button, have some coffee,
    and, by the time you get back to your desk, the result will be waiting for you.
    What do you do? You scale. You buy a few more servers, install your monster application
    servers and deploy your monster applications on top of them. Which part of the
    system was the bottleneck? Nobody knows. Why did you duplicate everything? Because
    you must. And then some more time passes, and you continue scaling until you run
    out of money and, simultaneously, people working for you go crazy. Today we do
    not approach scaling like that. Today we understand that scaling is about many
    other things. It's about elasticity. It's about being able to quickly and easily
    scale and de-scale depending on variations in your traffic and growth of your
    business, and that you should not go bankrupt during the process. It's about the
    need of almost every company to scale their business without thinking that IT
    department is a liability. It's about getting rid of those monsters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人会告诉你他们拥有一个*可扩展的系统*。毕竟，扩展很简单。买一台服务器，安装WebLogic（或者你使用的其他大型应用服务器），然后部署应用程序。接着等上几周，直到你发现一切都变得如此*快速*，以至于你可以点击一个按钮，喝杯咖啡，等你回到桌前，结果已经在那里等着你。你会怎么做？你扩展。你再买几台服务器，安装你那些庞大的应用服务器，然后在它们上面部署你的庞大应用程序。系统的瓶颈在哪个部分？没人知道。为什么要重复所有的东西？因为你不得不这么做。然后时间过去了，你继续扩展，直到钱花光了，同时，你的员工也都快疯了。今天我们不再像那样进行扩展。今天我们明白，扩展涉及到很多其他的事情。它关乎弹性，关乎根据流量的变化和业务的增长，能够快速、轻松地进行扩展和收缩，而且在这个过程中不至于破产。它关乎几乎每家公司都需要扩展他们的业务，而不会认为IT部门是一项负担。它关乎摆脱那些庞然大物。
- en: '**A note to The DevOps 2.0 Toolkit readers**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**《DevOps 2.0工具集》读者须知**'
- en: The text that follows is identical to the one published in *The DevOps 2.0 Toolkit*.
    If it is still fresh in your mind, feel free to jump to the section *Docker Swarm
    Mode* of this chapter. You'll see that a lot has changed. One of those changes
    is that the old Swarm running as a separate container is deprecated for *Swarm
    Mode*. There are many other new things we'll discover along the way.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容与*《DevOps 2.0工具集》*中发布的文本完全相同。如果这部分内容你已经记得很清楚，可以直接跳到本章的*Docker Swarm模式*部分。你会发现很多东西已经发生了变化。其中之一就是，旧版的Swarm作为独立容器运行的方式已经被废弃，取而代之的是*Swarm模式*。在接下来的过程中，我们还会发现许多其他新内容。
- en: Scalability
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Let us, for a moment take a step back and discuss why we want to scale applications.
    The main reason is *high availability*. Why do we want high availability? We want
    it because we want our business to be available under any load. The bigger the
    load, the better (unless you are under DDoS). It means that our business is booming.
    With high availability our users are happy. We all want speed, and many of us
    simply leave the site if it takes too long to load. We want to avoid having outages
    because every minute our business is not operational can be translated into a
    money loss. What would you do if an online store is not available? Probably go
    to another. Maybe not the first time, maybe not the second, but, sooner or later,
    you would get fed up and switch it for another. We are used to everything being
    fast and responsive, and there are so many alternatives that we do not think twice
    before trying something else. And if that something else turns up to be better.
    One man's loss is another man's gain. Do we solve all our problems with scalability?
    Not even close. Many other factors decide the availability of our applications.
    However, scalability is an important part of it, and it happens to be the subject
    of this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微退后一步，讨论一下为什么我们要扩展应用程序。主要原因是*高可用性*。为什么我们需要高可用性？因为我们希望我们的业务在任何负载下都能保持可用。负载越大越好（除非你遭遇了DDoS攻击）。这意味着我们的业务在蓬勃发展。拥有高可用性，我们的用户会很满意。我们都希望速度快，而且很多人如果网站加载太慢就会直接离开。我们希望避免宕机，因为每一分钟我们的业务无法运行都可能导致经济损失。如果一家在线商店无法访问，你会怎么做？大概会去另一个商店吧。也许不是第一次，也许不是第二次，但迟早你会受够了，换个商店。我们已经习惯了一切都很快速且响应迅速，而且有那么多替代选择，以至于我们在尝试其他东西之前不会多想。如果那个“其他”东西更好呢？一个人的损失可能是另一个人的收益。我们是否能通过扩展性解决所有问题？远远不够。还有许多其他因素决定了我们应用程序的可用性。然而，扩展性是其中的重要部分，而它恰好是本章的主题。
- en: What is scalability? It is a property of a system that indicates its ability
    to handle increased load in a graceful manner or its potential to be enlarged
    as demand increases. It is the capacity to accept increased volume or traffic.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是扩展性？它是系统的一种属性，表示系统优雅地处理增加负载的能力，或在需求增加时扩展的潜力。它是接受增加的流量或工作负载的能力。
- en: The truth is that the way we design our applications dictates the scaling options
    available. Applications will not scale well if they are not designed to scale.
    That is not to say that an application not designed for scaling cannot scale.
    Everything can scale, but not everything can scale well.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们设计应用程序的方式决定了可用的扩展选项。如果应用程序没有设计为可扩展，它们将无法很好地扩展。这并不是说没有为扩展设计的应用程序就不能扩展。一切都可以扩展，但并不是所有的扩展都能做得好。
- en: Commonly observed scenario is as follows.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的场景如下。
- en: We start with a simple architecture, sometimes with load balancer sometimes
    without, setup a few application servers and one database. Everything is great,
    complexity is low, and we can develop new features very fast. The cost of operations
    is low, income is high (considering that we just started), and everyone is happy
    and motivated.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的架构开始，有时有负载均衡器，有时没有，设置几个应用服务器和一个数据库。一切都很好，复杂性较低，我们可以非常快速地开发新功能。运营成本低，收入高（考虑到我们刚刚起步），大家都很高兴并充满动力。
- en: Business is growing, and the traffic is increasing. Things are beginning to
    fail, and performance is dropping. Firewalls are added, additional load balancers
    are set up, the database is scaled, more application servers are added and so
    on. Things are still relatively straightforward. We are faced with new challenges,
    but we can overcome obstacles in time. Even though the complexity is increasing,
    we can still handle it with relative ease. In other words, what we're doing is
    still, more or less, the same but bigger. Business is doing well, but it is still
    relatively small.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 业务在增长，流量在增加。问题开始出现，性能下降。防火墙被添加，额外的负载均衡器被设置，数据库被扩展，增加了更多的应用服务器，等等。事情仍然相对简单。我们面临着新的挑战，但可以及时克服障碍。尽管复杂性在增加，但我们仍然能够相对轻松地处理它。换句话说，我们正在做的事情基本上还是一样的，只是规模变大了。业务做得很好，但仍然相对较小。
- en: And then it happens. The big thing you've been waiting for. Maybe one of the
    marketing campaigns hit the spot. Maybe there was an adverse change in your competition.
    Maybe that last feature was indeed a killer one. No matter the reasons, business
    got a big boost. After a short period of happiness due to this change, your pain
    increases tenfold. Adding more databases does not seem to be enough. Multiplying
    application servers does not appear to fulfill the needs. You start adding caching
    and what not. You start getting the feeling that every time you multiply something,
    benefits are not equally big. Costs increase, and you are still not able to meet
    the demand. Database replications are too slow. New application servers do not
    make such a big difference anymore. Operational costs are increasing faster than
    you expected. The situation hurts the business and the team. You are starting
    to realize that the architecture you were so proud of cannot fulfill this increase
    in load. You cannot split it. You cannot scale things that hurt the most. You
    cannot start over. All you can do is continue multiplying with ever decreasing
    benefits of such actions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它发生了。你一直在等待的大事。也许某个营销活动得到了很好的反响。也许你的竞争对手发生了不利的变化。也许那个最后推出的功能确实非常关键。不管原因是什么，业务得到了大幅提升。在经历了这段由变化带来的短暂幸福之后，你的痛苦增加了十倍。增加更多的数据库似乎不够。扩展应用服务器似乎也不能满足需求。你开始加入缓存等等。你开始有一种感觉，每次你扩展某样东西时，收益并没有成比例地增加。成本在增加，但你仍然无法满足需求。数据库复制太慢。新的应用服务器已不再带来那么大的差异。运营成本的增长速度超出了你的预期。这个局面正在伤害业务和团队。你开始意识到，你曾经为之自豪的架构无法应对这种负载增长。你无法将它拆分。你无法扩展最痛苦的部分。你无法从头开始。你能做的只是继续扩展，但每次扩展的收益都在递减。
- en: The situation described above is quite common. What was good at the beginning,
    is not necessarily right when the demand increases. We need to balance the need
    for **You ain't gonna need it** (**YAGNI**) principle and the longer term vision.
    We cannot start with the system optimized for large companies because it is too
    expensive and does not provide enough benefits when business is small. On the
    other hand, we cannot lose the focus from one of the primary objectives of any
    business. We cannot not think about scaling from the very first day. Designing
    scalable architecture does not mean that we need to start with a cluster of a
    hundred servers. It does not mean that we have to develop something big and complex
    from the start. It means that we should start small, but in the way that, when
    it becomes big, it is easy to scale. While microservices are not the only way
    to accomplish that goal, they are indeed a good way to approach this problem.
    The cost is not in development but operations. If operations are automated, that
    cost can be absorbed quickly and does not need to represent a massive investment.
    As you already saw (and will continue seeing throughout the rest of the book),
    there are excellent open source tools at our disposal. The best part of automation
    is that the investment tends to have lower maintenance cost than when things are
    done manually.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上述情况是相当常见的。一开始有效的方案，在需求增加时不一定还是对的。我们需要平衡**你不需要的东西（YAGNI）**原则和长期愿景。我们不能一开始就用针对大公司的系统来优化，因为它成本过高，并且在业务小的时候无法带来足够的收益。另一方面，我们也不能忽视任何业务的首要目标之一。从第一天起，我们就不能不考虑扩展性。设计可扩展的架构并不意味着我们必须从一开始就部署百台服务器的集群。也不意味着我们要从一开始就开发庞大复杂的系统。它的意思是，我们应该从小做起，但要确保当系统变大时，能够轻松扩展。虽然微服务并不是唯一能够实现这一目标的方式，但它们的确是一个很好的解决方案。成本不在于开发，而是在于运营。如果运营是自动化的，这个成本可以很快被吸收，并且不需要大量的投资。正如你已经看到的（并将在本书的其余部分继续看到），我们有许多优秀的开源工具可以使用。自动化的最大好处是，投资的维护成本往往低于手动操作时的成本。
- en: We already discussed microservices and automation of their deployments on a
    tiny scale. Now it's time to convert this small scale to something bigger. Before
    we jump into practical parts, let us explore what are some of the different ways
    one might approach scaling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了微服务以及在小规模上的自动化部署。现在是时候将这种小规模转变为更大的规模了。在我们跳入实际部分之前，让我们探索一下人们可以采用哪些不同的方式来扩展。
- en: We are often limited by our design and choosing the way applications are constructed
    limits our choices severely. Although there are many different ways to scale,
    the most common one is called *Axis Scaling*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常受到设计的限制，选择应用程序的构建方式严重限制了我们的选择。尽管有许多不同的扩展方式，最常见的一种叫做*轴扩展*。
- en: Axis scaling
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轴扩展
- en: 'Axis scaling can be best represented through three dimensions of a cube; *X-Axis*,
    *Y-Axis*, and *Z-Axis*. Each of those dimensions describes a type of scaling:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 轴扩展最好的表示方式是通过立方体的三个维度：*X轴*、*Y轴*和*Z轴*。每个维度描述了一种扩展类型：
- en: 'X-Axis: Horizontal duplication'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X轴：水平复制
- en: 'Y-Axis: Functional decomposition'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y轴：功能分解
- en: 'Z-Axis: Data partitioning'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z轴：数据分区
- en: '![](img/scale-cube.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/scale-cube.png)'
- en: 'Figure 2-1: Scale cube'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-1：扩展立方体
- en: Let's go through the Axes, one at a time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解这些轴。
- en: X-axis scaling
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: X轴扩展
- en: In a nutshell, *x-axis* scaling is accomplished by running multiple instances
    of an application or a service. In most cases, there is a load balancer on top
    that makes sure that the traffic is shared among all those instances. The biggest
    advantage of *x-axis* scaling is simplicity. All we have to do is deploy the same
    application on multiple servers. For that reason, this is the most commonly used
    type of scaling. However, it comes with its set of disadvantages when applied
    to monolithic applications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*x轴*扩展是通过运行多个应用实例或服务实例来实现的。在大多数情况下，会有一个负载均衡器在上面，确保流量均匀分配到所有实例上。*x轴*扩展的最大优点是简便性。我们只需要将相同的应用部署到多个服务器上。因此，这是最常用的扩展方式。然而，当应用是单体应用时，这种扩展方式也有一系列缺点。
- en: Having a large application usually requires a big cache that demands heavy usage
    of memory. When such an application is multiplied, everything is multiplied with
    it, including the cache. Another, often more important, problem is an inappropriate
    usage of resources. Performance issues are almost never related to the whole application.
    Not all modules are equally affected, and, yet, we multiply everything. That means
    that even though we could be better off by scaling only part of the application
    that requires such an action, we scale everything. Nevertheless, *x- axis* scaling
    is important no matter the architecture. The major difference is the effect that
    such a scaling has.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个大型应用通常需要一个大缓存，这就要求大量使用内存。当这样的应用被复制时，一切都会被复制，包括缓存。另一个，通常更重要的问题是资源的不当使用。性能问题几乎从不与整个应用相关。并不是所有模块都受到同等影响，但我们却将它们全部复制。这意味着，尽管我们可以通过仅扩展需要的部分应用来获得更好的效果，但我们却扩展了整个应用。尽管如此，*x轴*扩展在任何架构中都是重要的。主要的区别在于这种扩展的影响。
- en: '![](img/monolith-scaling.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/monolith-scaling.png)'
- en: 'Figure 2-2: Monolithic application scaled inside a cluster'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-2：在集群中扩展的单体应用
- en: By using microservices, we are not removing the need for *x-axis* scaling but
    making sure that due to their architecture such scaling has more effect than with
    alternative and more traditional approaches to architecture. With microservices,
    we have the option to fine-tune scaling. We can have many instances of services
    that suffer a lot under heavy load and only a few instances of those that are
    used less often or require fewer resources. On top of that, since they are small,
    we might never reach a limit of a service. A small service in a big server would
    need to receive a truly massive amount of traffic before the need for scaling
    arises. Scaling microservices is more often related to fault tolerance than performance
    problems. We want to have multiple copies running so that, if one of them dies,
    the others can take over until recovery is performed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用微服务，我们并不是去除对*x轴*扩展的需求，而是确保由于它们的架构，使得这种扩展比传统架构方法更有效。在微服务架构中，我们有更多的选择来微调扩展。我们可以为承受重负载的服务部署多个实例，而对于使用较少或需要较少资源的服务则只部署少数实例。除此之外，由于微服务体积小，我们可能永远不会达到某个服务的限制。在大服务器上运行一个小服务，只有当流量达到极高的水平时，才会需要扩展。微服务的扩展更多地与容错能力相关，而非性能问题。我们希望有多个副本在运行，这样当某个副本宕机时，其他副本可以接管，直到恢复完成。
- en: Y-axis scaling
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Y轴扩展
- en: Y-axis scaling is all about decomposition of an application into smaller services.
    Even though there are different ways to accomplish this decomposition, microservices
    are probably the best approach we can take. When they are combined with immutability
    and self-sufficiency, there is indeed no better alternative (at least from the
    prism of y-axis scaling). Unlike x-axis scaling, the y-axis is not accomplished
    by running multiple instances of the same application but by having multiple different
    services distributed across the cluster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Y轴扩展完全是关于将应用程序分解成更小的服务。尽管有多种方法可以完成这种分解，但微服务可能是我们可以采取的最佳方法。当它们与不可变性和自给自足结合时，确实没有比这更好的替代方案（至少从Y轴扩展的角度来看）。与X轴扩展不同，Y轴扩展不是通过运行多个相同应用程序的实例来实现的，而是通过将多个不同的服务分布到集群中。
- en: Z-axis scaling
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Z轴扩展
- en: Z-axis scaling is rarely applied to applications or services. Its primary and
    most common usage is among databases. The idea behind this type of scaling is
    to distribute data among multiple servers thus reducing the amount of work that
    each of them needs to perform. Data is partitioned and distributed so that each
    server needs to deal only with a subset of the data. This type of separation is
    often called **sharding**, and there are many databases specially designed for
    this purpose. Benefits of z-axis scaling are most noticeable in I/O and cache
    and memory utilization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Z轴扩展很少应用于应用程序或服务。它的主要和最常见的用途是在数据库中。此类扩展背后的理念是将数据分布到多个服务器上，从而减少每个服务器需要执行的工作量。数据被分区并分配，使得每个服务器只需要处理数据的一个子集。这种分离通常被称为**分片**，并且有许多数据库专门为此目的而设计。Z轴扩展的好处在于I/O、缓存和内存利用率上最为明显。
- en: Clustering
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群
- en: A server cluster consists of a set of connected servers that work together and
    can be seen as a single system. They are usually connected to a fast **Local Area
    Network **(**LAN**). The significant difference between a cluster and a group
    of servers is that the cluster acts as a single system trying to provide high
    availability, load balancing, and parallel processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器集群由一组连接的服务器组成，这些服务器共同工作，可以看作是一个单一系统。它们通常通过高速**局域网**(**LAN**)连接。集群与服务器组的显著区别在于，集群作为一个单一系统，旨在提供高可用性、负载均衡和并行处理。
- en: 'If we deploy applications, or services, to individually managed servers and
    treat them as separate units, the utilization of resources is sub-optimum. We
    cannot know in advance which group of services should be deployed to a server
    and utilize resources to their maximum. Moreover, resource usage tends to fluctuate.
    While, in the morning, some service might require a lot of memory, during the
    afternoon that usage might be lower. Having predefined servers prevents us from
    having elasticity that would balance that usage in the best possible way. Even
    if such a high level of dynamism is not required, predefined servers tend to create
    problems when something goes wrong, resulting in manual actions to redeploy the
    affected services to a healthy node:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将应用程序或服务部署到单独管理的服务器上，并将它们视为独立单元，那么资源的利用率将处于次优状态。我们无法预先知道哪些服务组应该部署到某台服务器上，并将资源利用到最大。此外，资源使用往往会波动。比如，早上某些服务可能需要大量内存，而下午它们的使用量可能会降低。预定义的服务器限制了我们在最优方式下平衡资源使用的灵活性。即便不需要如此高的动态性，预定义的服务器也往往在出现问题时造成麻烦，导致需要人工干预，将受影响的服务重新部署到健康的节点上：
- en: '![](img/servers.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/servers.png)'
- en: 'Figure 2-3: Cluster with containers deployed to predefined servers'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-3：部署到预定义服务器上的容器集群
- en: Real clustering is accomplished when we stop thinking in terms of individual
    servers and start thinking of a cluster; of all servers as one big entity. That
    can be better explained if we drop to a bit lower level. When we deploy an application,
    we tend to specify how much memory or CPU it might need. However, we do not decide
    which memory slots our application will use nor which CPUs it should utilize.
    For example, we don’t specify that some application should use CPUs 4, 5 and 7\.
    That would be inefficient and potentially dangerous. We only decide that three
    CPUs are required. The same approach should be taken on a higher level. We should
    not care where an application or a service will be deployed but what it needs.
    We should be able to define that the service has certain requirements and tell
    some tool to deploy it to whichever server in our cluster, as long as it fulfills
    the needs we have. The best (if not the only) way to accomplish that is to consider
    the whole cluster as one entity.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实现真正的集群是当我们不再只关注单独的服务器，而是开始将所有服务器视为一个大实体。要更好地理解这一点，我们可以稍微从更低的层次来解释。当我们部署应用时，我们通常会指定它可能需要多少内存或CPU。然而，我们并不会决定应用使用哪个内存插槽或应使用哪些CPU。例如，我们不会指定某个应用必须使用CPU
    4、5和7。这既低效又可能带来风险。我们只会决定需要三个CPU。我们应该在更高层次上采用同样的方法。我们不应关心某个应用或服务将部署到哪里，而应关心它需要什么。我们应该能够定义该服务的某些需求，并告诉某个工具将它部署到集群中的任何服务器，只要它满足我们的需求。实现这一目标的最佳（如果不是唯一的）方法是将整个集群视为一个实体。
- en: We can increase or decrease the capacity of a cluster by adding or removing
    servers but, no matter what we do, it should still be a single entity. We define
    a strategy and let our services be deployed somewhere inside the cluster. Those
    using cloud providers like **Amazon Web Services **(**AWS**), Microsoft Azure
    and **Google Compute Engine **(**GCE**) are already accustomed to this approach,
    even though they might not be aware of it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加或移除服务器来增加或减少集群的容量，但无论做什么，它仍然应该是一个单一的实体。我们定义策略，然后让我们的服务在集群的某个地方部署。那些使用云服务提供商，如**Amazon
    Web Services**（**AWS**）、Microsoft Azure和**Google Compute Engine**（**GCE**）的人，已经习惯了这种方法，尽管他们可能并没有意识到这一点。
- en: 'Throughout the rest of this chapter, we''ll explore ways to create our cluster
    and explore tools that can help us with that objective. The fact that we''ll be
    simulating the cluster locally does not mean that the same strategies cannot be
    applied to public or private clouds and data centers. Quite the opposite:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将探索创建集群的方法，并探讨可以帮助我们实现这一目标的工具。我们将在本地模拟集群，但这并不意味着相同的策略无法应用于公共或私有云和数据中心。恰恰相反：
- en: '![](img/cluster.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cluster.png)'
- en: 'Figure 2-4: Cluster with containers deployed to servers based on a predefined
    strategy'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-4：基于预定义策略将容器部署到服务器的集群
- en: Docker Swarm Mode
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm模式
- en: '*Docker Engine v1.12* was released in July 2016\. It is the most significant
    version since *v1.9*. Back then, we got Docker networking that, finally, made
    containers ready for use in clusters. With *v1.12*, Docker is reinventing itself
    with a whole new approach to cluster orchestration. Say goodbye to Swarm as a
    separate container that depends on an external data registry and welcome the *new
    Docker Swarm* or *Swarm Mode*. Everything you''ll need to manage your cluster
    is now incorporated into Docker Engine. Swarm is there. Service discovery is there.
    Improved networking is there. That does not mean that we do not need additional
    tools. We do. The major difference is that Docker Engine now incorporates all
    the "essential" (not to say minimal) tools we need.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*Docker Engine v1.12* 于2016年7月发布。它是自*v1.9*以来最重要的版本。那时，我们获得了Docker网络功能，最终使得容器可以在集群中使用。通过*v1.12*，Docker以全新的方式重新定义了集群编排。告别依赖外部数据注册表的Swarm容器，迎接*全新的Docker
    Swarm*或*Swarm模式*。现在，你管理集群所需的一切都已集成进Docker Engine。Swarm已经在其中。服务发现已经在其中。改进的网络功能也已在其中。这并不意味着我们不需要额外的工具。我们确实需要。不过，主要的区别在于，Docker
    Engine现在集成了我们所需的所有“基本”工具（甚至可以说是最小化的工具）。'
- en: The old Swarm (before *Docker v1.12*) used *fire-and-forget principle*. We would
    send a command to Swarm master, and it would execute that command. For example,
    if we would send it something like `docker-compose scale go-demo=5`, the old Swarm
    would evaluate the current state of the cluster, discover that, for example, only
    one instance is currently running, and decide that it should run four more. Once
    such a decision is made, the old Swarm would send commands to Docker Engines.
    As a result, we would have five containers running inside the cluster. For all
    that to work, we were required to set up Swarm agents (as separate containers)
    on all the nodes that form the cluster and hook them into one of the supported
    data registries (Consul, etcd, and Zookeeper).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 旧版Swarm（在*Docker v1.12*之前）采用了*一发即忘原则*。我们会向Swarm主节点发送命令，它就会执行该命令。例如，如果我们发送类似`docker-compose
    scale go-demo=5`的命令，旧版Swarm会评估当前集群的状态，发现例如只有一个实例在运行，然后决定应该再启动四个实例。一旦做出这个决定，旧版Swarm就会向Docker
    Engines发送命令。结果，我们会在集群中运行五个容器。为了让这一切正常工作，我们需要在组成集群的所有节点上设置Swarm代理（作为独立容器），并将它们接入到其中一个支持的数据注册中心（如Consul、etcd或Zookeeper）。
- en: 'The problem was that Swarm was executing commands we send it. It was not maintaining
    the desired state. We were, effectively, telling it what we want to happen (example:
    scale up), not the state we wanted (make sure that five instances are running).
    Later on, the old Swarm got the feature that would reschedule containers from
    failed nodes. However, that feature had a few problems that prevented it from
    being a reliable solution (example: failed containers were not removed from the
    overlay network).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，Swarm执行我们发送的命令，但它并没有维持所需的状态。我们实际上是在告诉它我们希望发生什么（例如：扩容），而不是我们期望的状态（确保有五个实例在运行）。后来，旧版Swarm获得了从故障节点重新调度容器的功能。然而，该功能存在一些问题，导致它不能成为一个可靠的解决方案（例如：故障容器没有从覆盖网络中移除）。
- en: Now we got a brand new Swarm. It is part of Docker Engine (no need to run it
    as separate containers), it has incorporated service discovery (no need to set
    up Consul or whatever is your data registry of choice), it is designed from the
    ground up to accept and maintain the desired state, and so on. It is a truly major
    change in how we deal with cluster orchestration.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了全新的Swarm。它是Docker Engine的一部分（无需将其作为独立容器运行），并且集成了服务发现功能（无需设置Consul或任何你选择的数据注册中心），它从底层开始设计，能够接受并维持所需的状态，等等。这是我们处理集群编排方式的真正重大变化。
- en: 'In the past, I was inclined towards the old Swarm more than Kubernetes. However,
    that inclination was only slight. There were pros and cons for using either solution.
    Kubernetes had a few features Swarm was missing (example: the concept of the desired
    state), the old Swarm shined with its simplicity and low usage of resources. With
    the new Swarm (the one that comes with *v1.12*), I have no more doubts which one
    to use. *The new Swarm is often a better choice than Kubernetes*. It is part of
    Docker Engine, so the whole setup is a single command that tells an engine to
    join the cluster. The new networking works like a charm. The bundle that can be
    used to define services can be created from Docker Compose files, so there is
    no need to maintain two sets of configurations (Docker Compose for development
    and a different one for orchestration). Most importantly, the new Docker Swarm
    continues being simple to use. From the very beginning, Docker community pledged
    that they are committed to simplicity and, with this release, they, once again,
    proved that to be true.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，我倾向于使用旧版Swarm而非Kubernetes。然而，这种倾向只是稍微的。两者各有优缺点。Kubernetes有一些Swarm所缺少的功能（例如：所需状态的概念），而旧版Swarm则因其简洁性和低资源使用而脱颖而出。随着新版本Swarm（即*v1.12*版本）的推出，我再也没有疑问该使用哪个了。*新版Swarm往往比Kubernetes更优选择*。它是Docker
    Engine的一部分，因此整个设置只需一个命令来指示引擎加入集群。新的网络功能也表现得非常出色。可以用Docker Compose文件创建的捆绑包来定义服务，因此无需维护两套配置（开发时使用Docker
    Compose，编排时使用另一套配置）。最重要的是，新版Docker Swarm继续保持简便易用。从一开始，Docker社区就承诺致力于简洁性，而通过这个发布，他们再次证明了这一点。
- en: And that's not all. The new release comes with a lot of other features that
    are not directly related with Swarm. However, this book is dedicated to cluster
    management. Therefore, I'll focus on Swarm and leave the rest for one of the next
    books or a blog article.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不是全部。新版本还带来了许多与Swarm直接无关的其他功能。然而，本书专注于集群管理。因此，我将专注于Swarm，将其他内容留待下本书或博客文章中讨论。
- en: Since I believe that code explains things better than words, we'll start with
    a demo of some of the new features introduced in *version 1.12*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我认为代码比文字更能解释问题，我们将从展示 *1.12 版本* 中的一些新特性开始。
- en: Setting up a Swarm cluster
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Swarm 集群
- en: 'We''ll continue using Docker Machine since it provides a very convenient way
    to simulate a cluster on a laptop. Three servers should be enough to demonstrate
    some of the key features of a Swarm cluster:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用 Docker Machine，因为它提供了一种非常方便的方式在笔记本上模拟集群。三个服务器应该足够展示 Swarm 集群的一些关键特性：
- en: All the commands from this chapter are available in the `02-docker-swarm.sh` ([https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896](https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896))
    Gist
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 `02-docker-swarm.sh` 文件中找到 ([https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896](https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896))
    Gist
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At this moment, we have three nodes. Please note that those servers are not
    running anything but Docker Engine.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有三台节点。请注意，这些服务器除了 Docker 引擎外没有运行任何其他服务。
- en: 'We can see the status of the nodes by executing the following `ls` command:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行以下 `ls` 命令来查看节点的状态：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows (ERROR column removed for brievity):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（错误列已去除以简化内容）：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/nodes.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/nodes.png)'
- en: 'Figure 2-5: Machines running Docker Engines'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-5：运行 Docker 引擎的机器
- en: With the machines up and running we can proceed and set up the Swarm cluster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器启动并运行后，我们可以继续设置 Swarm 集群。
- en: 'The cluster setup consists of two types of commands. We need to start by initializing
    the first node which will be our manager. Refer to the following illustration:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 集群设置包含两种类型的命令。我们需要先初始化第一个节点，这将是我们的管理节点。请参考以下插图：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first command set environment variables so that the local Docker Engine
    is pointing to the `node-1`. The second initialized Swarm on that machine.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令设置了环境变量，使得本地的 Docker 引擎指向 `node-1`。第二个命令在该机器上初始化了 Swarm。
- en: We specified only one argument with the `swarm init` command. The `--advertise-addr`
    is the address that this node will expose to other nodes for internal communication.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在 `swarm init` 命令中指定了一个参数。`--advertise-addr` 是该节点将向其他节点公开的地址，用于内部通信。
- en: 'The output of the `swarm init` command is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`swarm init` 命令的输出如下：'
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that the node is now a manager and we've got the commands we can
    use to join other nodes to the cluster. As a way to increase security, a new node
    can be added to the cluster only if it contains the token generated when Swarm
    was initialized. The token was printed as a result of the `docker swarm init`
    command. You can copy and paste the code from the output or use the `join-token`
    command. We'll use the latter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到该节点现在是管理节点，并且我们得到了可以用来将其他节点加入集群的命令。为了提高安全性，只有包含在初始化 Swarm 时生成的令牌的新节点才能加入集群。该令牌是在执行
    `docker swarm init` 命令时作为输出结果打印出来的。您可以从输出中复制并粘贴代码，或者使用 `join-token` 命令。我们将使用后者。
- en: Right now, our Swarm cluster consists of only one VM. We'll add the other two
    nodes to the cluster. But, before we do that, let us discuss the difference between
    a *manager* and a *worker*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的 Swarm 集群仅由一个虚拟机组成。我们将把另外两个节点添加到集群中。但是，在此之前，让我们讨论一下 *manager* 和 *worker*
    之间的区别。
- en: A Swarm manager continuously monitors the cluster state and reconciles any differences
    between the actual state and your expressed desired state. For example, if you
    set up a service to run ten replicas of a container, and a worker machine hosting
    two of those replicas crashes, the manager will create two new replicas to replace
    the ones that failed. Swarm manager assigns new replicas to workers that are running
    and available. A manager has all the capabilities of a worker.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 管理器持续监控集群的状态，并使实际状态与您设定的期望状态保持一致。例如，如果您设置了一个服务来运行十个容器副本，而托管其中两个副本的工作节点崩溃了，管理器将创建两个新的副本来替代失败的副本。Swarm
    管理器将新副本分配给正在运行并可用的工作节点。管理节点具有所有工作节点的功能。
- en: We can get a token required for adding additional nodes to the cluster by executing
    the `swarm join-token` command.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行 `swarm join-token` 命令来获取添加额外节点到集群所需的令牌。
- en: 'The command to obtain a token for adding a manager is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 获取添加管理节点令牌的命令如下：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Similarly, to get a token for adding a worker, we would run the command that
    follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，要获取添加工作节点的令牌，我们将执行以下命令：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In both cases, we'd get a long hashed string.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们都会得到一个长的哈希字符串。
- en: 'The output of the worker token is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点令牌的输出如下：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Please note that this token was generated on my machine and, in your case, it
    will be different.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此令牌是在我的机器上生成的，您的令牌将会有所不同。
- en: 'Let''s put the token into an environment variable and add the other two nodes
    as workers:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将令牌放入环境变量中，并将另外两个节点添加为工作节点：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that have the token inside a variable, we can issue the command that follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在令牌已存储在变量中，我们可以执行以下命令：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The command we just ran iterates over nodes two and three and executes the
    `swarm join command`. We set the token, the advertise address, and the address
    of our manager. As a result, the two machines joined the cluster as workers. We
    can confirm that by sending the `node ls` command to the manager node `node-1`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚运行的命令会遍历节点二和节点三并执行 `swarm join` 命令。我们设置了令牌、广告地址和管理节点的地址。结果，这两台机器作为工作节点加入了集群。我们可以通过向管理节点
    `node-1` 发送 `node ls` 命令来确认这一点：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the `node ls` command is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`node ls` 命令的输出如下：'
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The asterisk tells us which node we are currently using. The `MANAGER STATUS`
    indicates that the `node-1` is the *leader:*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 星号告诉我们当前使用的是哪个节点。`MANAGER STATUS` 显示 `node-1` 是*领导者*：
- en: '![](img/swarm-nodes.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes.png)'
- en: 'Figure 2-6: Docker Swarm cluster with three nodes'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-6：包含三个节点的 Docker Swarm 集群
- en: In a production environment, we would probably set more than one node to be
    a manager and, thus, avoid deployment downtime if one of them fails. For the purpose
    of this demo, having one manager should suffice.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们可能会设置多个节点为管理节点，从而避免其中一个节点故障时导致部署停机。为了演示的目的，设置一个管理节点应该足够。
- en: Deploying services to the Swarm cluster
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将服务部署到 Swarm 集群
- en: 'Before we deploy a demo service, we should create a new network so that all
    containers that constitute the service can communicate with each other no matter
    on which nodes they are deployed:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署示例服务之前，我们应创建一个新网络，以便所有构成该服务的容器无论部署在哪个节点上，都能相互通信：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next chapter will explore networking in more details. Right now, we'll discuss
    and do only the absolute minimum required for an efficient deployment of services
    inside a Swarm cluster.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将更详细地探讨网络。现在，我们只讨论并完成在 Swarm 集群内高效部署服务所需的最低限度内容。
- en: 'We can check the status of all networks with the command that follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下命令检查所有网络的状态：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the `network ls` command is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`network ls` 命令的输出如下：'
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, we have two networks that have the `swarm` scope. The one named
    `ingress` was created by default when we set up the cluster. The second `go-demo` was
    created with the `network create` command. We'll assign all containers that constitute
    the `go-demo` service to that network.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有两个 `swarm` 范围的网络。一个名为 `ingress`，是我们设置集群时默认创建的。第二个 `go-demo` 是通过 `network
    create` 命令创建的。我们将把所有构成 `go-demo` 服务的容器分配到该网络。
- en: The next chapter will go deep into the Swarm networking. For now, it is important
    to understand that all services that belong to the same network can speak with
    each other freely.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将深入探讨 Swarm 网络。现在，重要的是要理解所有属于同一网络的服务可以自由地相互通信。
- en: The *go-demo* application requires two containers. Data will be stored in MongoDB.
    The back-end that uses that `DB` is defined as `vfarcic/go-demo` container.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*go-demo* 应用程序需要两个容器。数据将存储在 MongoDB 中。使用该 `DB` 的后端定义为 `vfarcic/go-demo` 容器。'
- en: Let's start by deploying the `mongo` container somewhere within the cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在集群中的某个地方部署 `mongo` 容器。
- en: 'Usually, we''d use constraints to specify the requirements for the container
    (example: HD type, the amount of memory and CPU, and so on). We''ll skip that,
    for now, and tell Swarm to deploy it anywhere within the cluster:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会使用约束条件来指定容器的要求（例如：硬盘类型、内存和 CPU 的数量等）。目前我们跳过这些，直接告诉 Swarm 在集群中的任何地方部署它：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Please note that we haven't specified the port `Mongo` listens to `27017`. That
    means that the database will not be accesible to anyone but other services that
    belong to the same `go-demo` network .
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有指定 `Mongo` 所监听的端口 `27017`。这意味着该数据库将不会对除属于同一 `go-demo` 网络的其他服务外的任何人可访问。
- en: As you can see, the way we use service create is similar to the Docker `run`
    command you are, probably, already used to.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用 `service create` 的方式与您可能已经熟悉的 Docker `run` 命令类似。
- en: 'We can list all the running services:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以列出所有正在运行的服务：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Depending on how much time passed between `service create` and `service ls`
    commands, you'll see the value of the `REPLICAS` column being zero or one. Immediately
    after creating the service, the value should be `*0/1*`, meaning that zero replicas
    are running, and the objective is to have one. Once the `mongo` image is pulled,
    and the container is running, the value should change to `*1/1*`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`service create`和`service ls`命令之间经过的时间，你会看到`REPLICAS`列的值是零或一。服务创建后，值应该是`*0/1*`，意味着没有副本在运行，而目标是运行一个副本。一旦拉取了`mongo`镜像并且容器启动，值应该会变为`*1/1*`。
- en: 'The final output of the `service ls` command should be as follows (IDs are
    removed for brevity):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ls`命令的最终输出应该如下所示（为了简洁，已移除ID）：'
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we need more information about the `go-demo-db` service, we can run the
    service inspect command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要更多关于`go-demo-db`服务的信息，我们可以运行`service inspect`命令：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that the database is running, we can deploy the `go-demo` container:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据库已经运行，我们可以部署`go-demo`容器：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There's nothing new about that command. The service will be attached to the
    `go-demo` network. The environment variable `DB` is an internal requirement of
    the `go-demo` service that tells the code the address of the database.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令没有什么新鲜的内容。服务将会连接到`go-demo`网络。环境变量`DB`是`go-demo`服务的内部要求，告知代码数据库的地址。
- en: 'At this point, we have two containers (`mongo` and `go-demo`) running inside
    the cluster and communicating with each other through the `go-demo` network. Please
    note that none of them is *yet* accessible from outside the network. At this point,
    your users do not have access to the service API. We''ll discuss this in more
    details soon. Until then, I''ll give you only a hint: *you need a reverse proxy*
    capable of utilizing the new Swarm networking.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经在集群中运行了两个容器（`mongo`和`go-demo`），并通过`go-demo`网络相互通信。请注意，它们目前*还*无法从网络外部访问。此时，用户无法访问服务API。我们稍后会更详细地讨论这个问题。在此之前，我只给你一个提示：*你需要一个反向代理*，它能够利用新的Swarm网络功能。
- en: 'Let''s run the `service ls` command one more time:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再运行一次`service ls`命令：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result, after the `go-demo` service is pulled to the destination node,
    should be as follows (IDs are removed for brevity):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，在`go-demo`服务被拉取到目标节点后，应该如下所示（为了简洁，已移除ID）：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see, both services are running as a single replica:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两个服务都以单个副本的形式运行：
- en: '![](img/swarm-nodes-single-container.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes-single-container.png)'
- en: 'Figure 2-8: Docker Swarm cluster containers communicating through the go-demo
    SDN'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-8：Docker Swarm集群中的容器通过go-demo SDN通信
- en: What happens if we want to scale one of the containers? How do we scale our
    services?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要扩展其中一个容器会发生什么？我们如何扩展我们的服务？
- en: Scaling services
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展服务
- en: We should always run at least two instances of any given service. That way they
    can share the load and, if one of them fails, there will be no downtime. We'll
    explore Swarm's failover capability soon and leave load balancing for the next
    chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该始终运行至少两个实例的服务。这样，它们可以共享负载，而且如果其中一个服务失败，就不会出现停机。我们很快会探讨Swarm的故障切换功能，并将负载均衡留到下一章。
- en: 'We can, for example, tell Swarm that we want to run five replicas of then `go-demo`
    service:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，我们可以告诉Swarm，我们希望运行五个`go-demo`服务的副本：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With the `service scale` command, we scheduled five replicas. Swarm will make
    sure that five instances of `go-demo` are running somewhere inside the cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`service scale`命令，我们安排了五个副本。Swarm会确保在集群的某个地方运行五个`go-demo`实例。
- en: 'We can confirm that, indeed, five replicas are running through the, already
    familiar, `service ls` command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过已熟悉的`service ls`命令确认，确实有五个副本在运行：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已移除ID）：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, five out of five `REPLICAS` of the `go-demo` service are running.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，五个`go-demo`服务的副本中有五个在运行。
- en: 'The `service ps` command provides more detailed information about a single
    service:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps`命令提供关于单个服务的更详细信息：'
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows (IDs and ERROR PORTs columns are removed for brevity):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已移除ID和ERROR PORT列）：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can see that the `go-demo` service is running five instances distributed
    across the three nodes. Since they all belong to the same **go-demo SDN**, they
    can communicate with each other no matter where they run inside the cluster. At
    the same time, none of them is accessible from outside:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`go-demo` 服务正在运行五个实例，这些实例分布在三个节点上。由于它们都属于同一个 **go-demo SDN**，无论它们在集群中运行在哪个位置，都可以相互通信。同时，它们都无法从外部访问：
- en: '![](img/swarm-nodes-replicas-1.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes-replicas-1.png)'
- en: 'Figure 2-9: Docker Swarm cluster with go-demo service scaled to five replicas'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-9：Docker Swarm 集群，其中 go-demo 服务已扩展至五个副本
- en: What happens if one of the containers is stopped or if the entire node fails?
    After all, processes and nodes do fail sooner or later. Nothing is perfect, and
    we need to be prepared for such situations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其中一个容器停止运行，或者整个节点出现故障，会发生什么呢？毕竟，进程和节点迟早会发生故障。没有什么是完美的，我们需要为这种情况做好准备。
- en: Failover
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容错
- en: Fortunately, failover strategies are part of Docker Swarm. Remember, when we
    execute a `service` command, we are not telling Swarm what to do but the state
    we desire. In turn, Swarm will do its best to maintain the specified state no
    matter what happens.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，容错策略是 Docker Swarm 的一部分。记住，当我们执行 `service` 命令时，我们并不是在告诉 Swarm 应该做什么，而是在告诉它我们希望的状态。反过来，Swarm
    将尽力保持指定的状态，不管发生什么。
- en: 'To test a failure scenario, we''ll destroy one of the nodes:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试故障场景，我们将销毁其中一个节点：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Swarm needs a bit of time until it detects that the node is down. Once it does,
    it will reschedule containers. We can monitor the situation through `service ps
    command`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 需要一些时间才能检测到节点已宕机。一旦它检测到，便会重新调度容器。我们可以通过 `service ps` 命令来监控情况：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output (after rescheduling) is as follows (`ID` is removed for brevity):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（在重新调度后）如下所示（`ID` 被省略以简化展示）：
- en: '![](img/Capture.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Capture.png)'
- en: As you can see, after a short period, Swarm rescheduled containers among healthy
    nodes (`node-1` and `node-2`) and changed the state of those that were running
    on the failed node to `Shutdown`. If your output still shows that some instances
    are running on the `node-3`, please wait for a few moments and repeat the `service
    ps command`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，经过短暂的时间，Swarm 在健康节点（`node-1` 和 `node-2`）之间重新调度了容器，并将那些在故障节点上运行的容器状态更改为
    `Shutdown`。如果你的输出仍显示某些实例在 `node-3` 上运行，请稍等片刻并重新执行 `service ps` 命令。
- en: What now?
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: That concludes the exploration of basic concepts of the new Swarm features we
    got with *Docker v1.12+*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 Docker v1.12+ 新 Swarm 特性基本概念的探索。
- en: Is this everything there is to know to run a Swarm cluster successfully? Not
    even close! What we explored by now is only the beginning. There are quite a few
    questions waiting to be answered. How do we expose our services to the public?
    How do we deploy new releases without downtime? I'll try to give answers to those
    and quite a few other questions in the chapters that follow. The next one will
    be dedicated to the exploration of the ways we can expose our services to the
    public. We'll try to integrate a proxy with a Swarm cluster. To do that, we need
    to dive deeper into Swarm networking.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是运行一个 Swarm 集群所需了解的一切吗？远远不够！到目前为止，我们探讨的只是一个开始。还有很多问题等待解答。我们如何将服务暴露给公众？我们如何在没有停机时间的情况下部署新版本？我将在接下来的章节中尝试回答这些问题，以及其他一些问题。下一章将专门探讨我们如何将服务暴露给公众。我们将尝试将代理与
    Swarm 集群集成。为此，我们需要深入了解 Swarm 网络。
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是休息的时间，在进入下一章之前。像之前一样，我们将销毁所创建的机器并重新开始：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
