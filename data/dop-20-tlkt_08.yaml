- en: Chapter 8. Service Discovery – The Key to Distributed Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 服务发现 – 分布式服务的关键
- en: '|   | *It does not take much strength to do things, but it requires a great
    deal of strength to decide what to do.* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *做事不需要太多力量，但决定做什么需要巨大的力量。* |   |'
- en: '|   | --*Elbert Hubbard* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*埃尔伯特·哈伯德* |'
- en: 'The more services we have, the bigger the chance for a conflict to occur if
    we are using predefined ports. After all, there can be no two services listening
    on the same port. Managing an accurate list of all the ports used by, let''s say,
    a hundred services is a challenge in itself. Add to that list the databases those
    services need and the number grows even more. For that reason, we should deploy
    services without specifying ports and letting Docker assign random ones for us.
    The only problem is that we need to discover the port number and let others know
    about it:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的服务越多，如果使用预定义端口，发生冲突的可能性就越大。毕竟，不能有两个服务监听同一个端口。管理一个包含例如一百个服务的所有端口的准确列表本身就是一个挑战。如果再加上这些服务所需要的数据库，数量会更多。因此，我们应该部署服务时不指定端口，而是让
    Docker 为我们分配随机端口。唯一的问题是，我们需要发现端口号并让其他人知道它：
- en: '![Service Discovery – The Key to Distributed Services](img/B05848_08_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现 – 分布式服务的关键](img/B05848_08_01.jpg)'
- en: Figure 8-1 – Single node with services deployed as Docker containers
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-1 – 单个节点上的服务作为 Docker 容器部署
- en: 'Things will get even more complicated later on when we start working on a distributed
    system with services deployed into one of the multiple servers. We can choose
    to define in advance which service goes to which server, but that would cause
    a lot of problems. We should try to utilize server resources as best we can, and
    that is hardly possible if we define in advance where to deploy each service.
    Another problem is that automatic scaling of services would be difficult at best,
    and not to mention automatic recuperation from, let''s say, server failure. On
    the other hand, if we deploy services to the server that has, for example, least
    number of containers running, we need to add the IP to the list of data needed
    to be discovered and stored somewhere:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始在分布式系统中工作时，情况会变得更加复杂，这时服务会部署到多个服务器中的某一台上。我们可以选择提前定义哪个服务部署到哪个服务器，但那会带来许多问题。我们应该尽可能地利用服务器资源，而如果提前定义每个服务的部署位置，这几乎是不可能的。另一个问题是，服务的自动扩展将变得困难，尤其是当我们需要自动从比如服务器故障中恢复时。另一方面，如果我们将服务部署到例如容器最少的服务器上，我们需要将
    IP 地址添加到需要被发现和存储的数据列表中：
- en: '![Service Discovery – The Key to Distributed Services](img/B05848_08_02.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现 – 分布式服务的关键](img/B05848_08_02.jpg)'
- en: Figure 8-2 – Multiple nodes with services deployed as Docker containers
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-2 – 多个节点上的服务作为 Docker 容器部署
- en: There are many other examples of cases when we need to store and retrieve (discover)
    some information related to the services we are working with.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他例子，在这些情况下，我们需要存储和检索（发现）与我们正在使用的服务相关的一些信息。
- en: 'To be able to locate our services, we need at least the following two processes
    to be available for us:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够定位我们的服务，我们至少需要以下两个过程可用：
- en: '**Service registration** process that will store, as a minimum, the host and
    the port service is running on.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**服务注册**过程，它至少会存储服务运行所在的主机和端口。'
- en: '**Service discovery** process that will allow others to be able to discover
    the information we stored during the registration process:![Service Discovery
    – The Key to Distributed Services](img/B05848_08_03.jpg)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**服务发现**过程，它允许其他人发现我们在注册过程中存储的信息：![服务发现 – 分布式服务的关键](img/B05848_08_03.jpg)'
- en: Figure 8-3 – Service registration and discovery
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8-3 – 服务注册与发现
- en: Besides those processes, we need to consider several other aspects. Should we
    unregister the service if it stops working and deploy/register a new instance?
    What happens when there are multiple copies of the same service? How do we balance
    the load among them? What happens if a server goes down? Those and many other
    questions are tightly related to the registration and discovery processes and
    will be the subject of the next chapters. For now, we'll limit the scope only
    to the *service discovery* (the common name that envelops both processes mentioned
    above) and the tools we might use for such a task. Most of them feature highly
    available distributed key/value storage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些过程外，我们还需要考虑其他几个方面。如果服务停止工作，我们是否应该注销该服务并部署/注册一个新的实例？当同一个服务有多个副本时，应该怎么办？我们如何在它们之间进行负载均衡？如果某台服务器出现故障，会发生什么？这些以及其他许多问题与注册和发现过程紧密相关，并将是接下来的章节内容。目前，我们将范围限定在*服务发现*（这个术语涵盖了上述两个过程）和我们可能用于此任务的工具上。大多数工具都具有高可用的分布式键值存储。
- en: Service Registry
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务注册中心
- en: The goal of the service registry is simple. Provide capabilities to store service
    information, be fast, persistent, fault-tolerant, and so on. In its essence, service
    registry is a database with a very limited scope. While other databases might
    need to deal with a vast amount of data, service registry expects a relatively
    small data load. Due to the nature of the task, it should expose some API so that
    those in need of it's data can access it easily.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 服务注册中心的目标很简单。提供存储服务信息的能力，要求快速、持久、容错等。从本质上讲，服务注册中心是一个范围非常有限的数据库。虽然其他数据库可能需要处理大量数据，但服务注册中心预期的负载较小。由于任务的性质，它应该暴露一些API，以便需要其数据的人可以轻松访问。
- en: There's not much more to be said (until we start evaluating different tools)
    so we'll move on to service registration.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有更多需要说明的内容（直到我们开始评估不同的工具），所以我们将继续讨论服务注册。
- en: Service Registration
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务注册
- en: Microservices tend to be very dynamic. They are created and destroyed, deployed
    to one server and then moved to another. They are always changing and evolving.
    Whenever there is any change in service properties, information about those changes
    needs to be stored in some database (we'll call it *service registry* or simply
    *registry*). The logic behind service registration is simple even though the implementation
    of that logic might become complicated. Whenever a service is deployed, its data
    (IP and port as a minimum) should be stored in the service registry. Things are
    a bit more complicated when a service is destroyed or stopped. If that is a result
    of a purposeful action, service data should be removed from the registry. However,
    there are cases when service is stopped due to a failure and in such a situation
    we might choose to do additional actions meant to restore the correct functioning
    of that service. We'll speak about such a situation in more details when we reach
    the self-healing chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务通常非常动态。它们会被创建和销毁，部署到一台服务器上，然后移动到另一台服务器上。它们总是在变化和发展。当服务属性发生任何变化时，这些变化的信息需要存储在某个数据库中（我们称之为*服务注册中心*，或简称*注册中心*）。服务注册的逻辑很简单，尽管这种逻辑的实现可能会变得复杂。每当一个服务被部署时，它的数据（至少包括IP和端口）应该存储在服务注册中心。当一个服务被销毁或停止时，事情就变得有些复杂。如果这是由于人为操作所导致，服务数据应该从注册中心中删除。然而，也有一些情况是服务由于故障停止的，在这种情况下，我们可能会选择采取额外的措施来恢复该服务的正常运行。我们将在自愈章节中详细讨论这种情况。
- en: There are quite a few ways service registration can be performed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 服务注册可以通过多种方式进行。
- en: Self-Registration
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注册
- en: '*Self-registration* is a common way to register service information. When a
    service is deployed it notifies the registry about its existence and sends its
    data. Since each service needs to be capable of sending its data to the registry,
    this can be considered an anti-pattern. By using this approach, we are breaking
    *single concern* and *bounded context* principles that we are trying to enforce
    inside our microservices. We''d need to add the registration code to each service
    and, therefore, increase the development complexity. More importantly, that would
    couple services to a specific registry service. Once their number increases, modifying
    all of them to, for example, change the registry would be a very cumbersome work.
    Besides, that was one of the reasons we moved away from monolithic applications;
    freedom to modify any service without affecting the whole system. The alternative
    would be to create a library that would do that for us and include it in each
    service. However, this approach would severally limit our ability to create entirely
    self-sufficient microservices. We''d increase their dependency on external resources
    (in this case the registration library).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*自注册*是一种常见的注册服务信息的方式。当一个服务被部署时，它会通知注册中心其存在并发送数据。由于每个服务都需要能够将数据发送到注册中心，这可以视为一种反模式。通过使用这种方式，我们破坏了我们在微服务中试图强制执行的*单一职责*和*有界上下文*原则。我们需要在每个服务中添加注册代码，从而增加了开发复杂度。更重要的是，这会将服务与特定的注册服务耦合。一旦服务数量增加，修改它们所有的代码，例如更改注册中心，将变得非常繁琐。而且，这也是我们摆脱单体应用程序的原因之一；即可以自由地修改任何服务而不影响整个系统。另一种选择是创建一个库来为我们完成注册，并将其包含在每个服务中。然而，这种方法会严重限制我们创建完全自给自足微服务的能力。我们将增加它们对外部资源（在本例中是注册库）的依赖。'
- en: 'De-registration is, even more, problematic and can quickly become quite complicated
    with the self-registration concept. When a service is stopped purposely, it should
    be relatively easy to remove its data from the registry. However, services are
    not always stopped on purpose. They might fail in unexpected ways and the process
    they''re running in might stop. In such a case it might be difficult (if not impossible)
    to always be able to de-register the service from itself:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注销甚至更加复杂，尤其是在自注册的概念下。当一个服务被故意停止时，它应该相对容易地将其数据从注册中心移除。然而，服务并非总是故意停止。它们可能会以意外的方式失败，或者运行它们的进程可能会停止。在这种情况下，可能很难（如果不是不可能的话）总是能够从服务本身注销它：
- en: '![Self-Registration](img/B05848_08_04.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![自注册](img/B05848_08_04.jpg)'
- en: Figure 8-4 – Self-registration
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-4 – 自注册
- en: While self-registration might be common, it is not an optimum nor productive
    way to perform this type of operations. We should look at alternative approaches.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自注册可能很常见，但这并不是一种最优或高效的操作方式。我们应该考虑其他替代方法。
- en: Registration Service
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注册服务
- en: 'Registration service or third party registration is a process that manages
    registration and de-registration of all services. The service is in charge of
    checking which microservices are running and should update the registry accordingly.
    A similar process is applied when services are stopped. The registration service
    should detect the absence of a microservice and remove its data from the registry.
    As an additional function, it can notify some other process of the absence of
    the microservice that would, in turn, perform some corrective actions like re-deployment
    of the absent microservice, email notifications, and so on. We''ll call this registration
    and de-registration process *service registrator* or simply *registrator* (actually,
    as you''ll soon see, there is a product with the same name):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注册服务或第三方注册是一个管理所有服务注册和注销的过程。该服务负责检查哪些微服务正在运行，并相应地更新注册中心。当服务停止时，类似的过程也会应用。注册服务应当检测到某个微服务的缺失，并将其数据从注册中心删除。作为附加功能，它可以通知其他进程该微服务的缺失，进而执行一些纠正措施，比如重新部署缺失的微服务、发送电子邮件通知等。我们将称这个注册和注销过程为*服务注册器*，或者简称*注册器*（实际上，正如你很快会看到的，这个名字已经有一个相同名称的产品了）：
- en: '![Registration Service](img/B05848_08_05.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![注册服务](img/B05848_08_05.jpg)'
- en: Figure 8-5 – Registration service
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-5 – 注册服务
- en: A separate registration service is a much better option than self-registration.
    It tends to be more reliable and, at the same time, does not introduce unnecessary
    coupling inside our microservices code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单独的注册服务比自注册要好得多。它通常更加可靠，同时也不会在我们的微服务代码中引入不必要的耦合。
- en: Since we established what will be the underlying logic behind the services registration
    process, it is time to discuss the discovery.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了服务注册过程的基本逻辑，现在是时候讨论服务发现了。
- en: Service Discovery
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: Service discovery is the opposite of service registration. When a client wants
    to access a service (the client might also be another service), it must know,
    as a minimum, where that service is. One approach we can take is self-discovery.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现是服务注册的对立面。当一个客户端想要访问一个服务时（客户端也可能是另一个服务），它至少需要知道该服务的位置。我们可以采取的一种方法是自我发现。
- en: Self-Discovery
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自我发现
- en: Self-discovery uses the same principles as self-registration. Every client or
    a service that wants to access other services would need to consult the registry.
    Unlike self-registration that posed problems mostly related to our internal ways
    to connect services, self-discovery might be used by clients and services outside
    our control. One example would be a front-end running in user browsers. That front-end
    might need to send requests to many separate back-end services running on different
    ports or even different IPs. The fact that we do have the information stored in
    the registry does not mean that others can, should, or know how to use it. Self-discovery
    can be effectively used only for the communication between internal services.
    Even such a limited scope poses a lot of additional problems many of which are
    the same as those created by self-registration. Due to what we know by now, this
    option should be discarded.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自我发现使用与自我注册相同的原理。每个客户端或想要访问其他服务的服务，都需要查阅注册表。与主要与我们内部连接服务的方式相关的问题的自我注册不同，自我发现可能被我们无法控制的客户端和服务使用。一个例子是运行在用户浏览器中的前端。该前端可能需要向许多独立的后端服务发送请求，这些服务运行在不同的端口甚至不同的
    IP 上。我们将信息存储在注册表中并不意味着其他人能够、应该或知道如何使用它。自我发现只能有效地用于内部服务之间的通信。即使如此有限的范围也带来了许多额外的问题，其中许多问题与自我注册所产生的问题相同。根据我们目前所了解的情况，这个选项应该被放弃。
- en: Proxy Service
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理服务
- en: Proxy services have been around for a while and proved their worth many times
    over. The next chapter will explore them in more depth so we'll go through them
    only briefly. The idea is that each service should be accessible through one or
    more fixed addresses. For example, the list of books from our `books-ms` service
    should be available only through the `[DOMAIN]/api/v1/books` address. Notice that
    there is no IP, port nor any other deployment-specific detail. Since there will
    be no service with that exact address, something will have to detect such a request
    and redirect it to the IP and port of the actual service. Proxy services tend
    to be the best type of tools that can fulfill this task.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 代理服务已经存在了一段时间，并且多次证明了它们的价值。下一章将更深入地探讨它们，因此我们这里只做简要介绍。其基本思想是每个服务都应该通过一个或多个固定地址进行访问。例如，我们的
    `books-ms` 服务的书籍列表应该仅通过 `[DOMAIN]/api/v1/books` 地址访问。注意，这里没有 IP、端口或任何其他与部署相关的细节。由于没有服务会有这个精确的地址，因此必须有某种机制来检测此类请求并将其重定向到实际服务的
    IP 和端口。代理服务往往是能够完成这一任务的最佳工具类型。
- en: Now that we have a general, and hopefully clear, idea of what we're trying to
    accomplish, let's take a look at some of the tools that can help us out.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个大致的、并且希望是清晰的目标，来了解我们想要完成的事情，让我们看一下可以帮助我们的工具。
- en: Service Discovery Tools
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现工具
- en: The primary objective of *service discovery tools* is to help services find
    and talk to one another. To perform their duty, they need to know where each service
    is. The concept is not new, and many tools existed long before Docker was born.
    However, containers brought the need for such tools to a whole new level.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务发现工具*的主要目标是帮助服务彼此查找并进行通信。为了履行其职责，它们需要知道每个服务的位置。这个概念并不新颖，许多工具在 Docker 诞生之前就已经存在。然而，容器技术使得这种工具的需求达到了全新的高度。'
- en: The basic idea behind *service discovery* is for each new instance of a service
    (or application) to be able to identify its current environment and store that
    information. Storage itself is performed in a registry usually in key/value format.
    Since the discovery is often used in distributed system, registry needs to be
    scalable, fault-tolerant and distributed among all nodes in the cluster. The primary
    usage of such a storage is to provide, as a minimum, IP and port of a service
    to all interested parties that might need to communicate with it. This data is
    often extended with other types of information.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务发现*的基本思想是每个新实例的服务（或应用程序）能够识别其当前环境并存储该信息。存储本身通常是在注册中心进行，通常采用键/值格式。由于服务发现常用于分布式系统，注册中心需要具备可扩展性、容错性，并在集群中的所有节点之间分布。这类存储的主要用途是至少向所有需要与其通信的相关方提供服务的IP和端口。此数据通常会扩展为其他类型的信息。'
- en: Discovery tools tend to provide some API that can be used by a service to register
    itself as well as by others to find the information about that service.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现工具通常会提供一些API，服务可以通过这些API注册自身，其他服务也可以通过这些API查询该服务的信息。
- en: Let's say that we have two services. One is a provider, and the other one is
    its consumer. Once we deploy the provider, we need to store its information in
    the *service registry* of choice. Later on, when the consumer tries to access
    the provider, it would first query the registry and call the provider using the
    IP and port obtained from the registry. To decouple the consumer from a particular
    implementation of the registry, we often employ some *proxy service*. That way
    the consumer would always request information from the fixed address that would
    reside inside the proxy that, in turn, would use the discovery service to find
    out the provider information and redirect the request. Actually, in many cases,
    there is no need for the proxy to query the service registry if there is a process
    that updates its configuration every time data in the registry changes. We'll
    go through *reverse proxy* later on in the book. For now, it is important to understand
    that the flow that is based on three actors; consumer, proxy, and provider.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个服务，一个是提供者，另一个是它的消费者。一旦我们部署了提供者，我们需要将其信息存储在选定的*服务注册中心*中。稍后，当消费者尝试访问提供者时，它首先会查询注册中心，并使用从注册中心获得的IP和端口来调用提供者。为了将消费者与特定实现的注册中心解耦，我们通常会使用一些*代理服务*。这样，消费者始终会从固定地址请求信息，该地址会驻留在代理中，代理则会使用发现服务来获取提供者的信息并重定向请求。实际上，在许多情况下，如果有一个进程每次注册中心数据变化时都会更新配置，代理就无需查询服务注册中心了。我们将在本书后面介绍*反向代理*。现在，重要的是要理解，这一流程是基于三个角色的：消费者、代理和提供者。
- en: What we are looking for in the service discovery tools is data. As a minimum,
    we should be able to find out where the service is, whether it is healthy and
    available, and what is its configuration. Since we are building a distributed
    system with multiple servers, the tool needs to be robust, and failure of one
    node should not jeopardize data. Also, each of the nodes should have the same
    data replica. Further on, we want to be able to start services in any order, be
    able to destroy them, or to replace them with newer versions. We should also be
    able to reconfigure our services and see the data change accordingly.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在服务发现工具中寻找的是数据。至少，我们应该能够找出服务的位置，它是否健康且可用，以及它的配置是什么。由于我们正在构建一个分布式系统，涉及多个服务器，工具需要足够强大，单个节点的故障不应影响数据。同时，每个节点都应该有相同的数据副本。进一步说，我们希望能够按任意顺序启动服务，能够销毁它们，或用新版替换它们。我们还应该能够重新配置服务，并看到数据相应地变化。
- en: Let's take a look at a few of the tools we can use to accomplish the goals we
    set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下可以用来实现我们目标的几种工具。
- en: Manual Configuration
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动配置
- en: Most of the services are still managed manually. We decide in advance where
    to deploy the service, what is its configuration and hope beyond reason that it
    will continue working properly until the end of days. Such approach is not easily
    scalable. Deploying a second instance of the service means that we need to start
    the manual process all over. We have to bring up a new server or find out which
    one has low utilization of resources, create a new set of configurations and deploy
    it. The situation is even more complicated in the case of, let's say, a hardware
    failure since the reaction time is usually slow when things are managed manually.
    Visibility is another sore point. We know what the static configuration is. After
    all, we prepared it in advance. However, most of the services have a lot of information
    generated dynamically. That information is not easily visible. There is no single
    location we can consult when we are in need of that data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数服务仍然是手动管理的。我们提前决定服务的部署位置、配置，并寄希望于它能够一直正常工作直到天荒地老。这种方法不容易扩展。部署服务的第二个实例意味着我们需要重新开始手动过程。我们必须启动一台新服务器，或者找出哪一台服务器的资源利用率低，创建一套新的配置并部署它。假设出现硬件故障，情况就更复杂了，因为当一切都需要手动管理时，反应时间通常会很慢。可见性也是另一个痛点。我们知道静态配置是什么，毕竟是我们提前准备好的。然而，大多数服务都有大量动态生成的信息。这些信息并不容易查看。在需要这些数据时，我们没有一个可以咨询的统一位置。
- en: Reaction time is inevitably slow, failure resilience questionable at best and
    monitoring difficult to manage due to a lot of manually handled moving parts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 反应时间不可避免地较慢，容错性充其量可疑，且由于许多部分需要手动操作，监控也难以管理。
- en: While there was an excuse to do this job manually in the past or when the number
    of services and/or servers is small, with the emergence of service discovery tools,
    this excuse quickly evaporated.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然过去或者在服务和/或服务器数量较少时，手动完成这些工作还有借口，但随着服务发现工具的出现，这个借口迅速消失了。
- en: Zookeeper
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Zookeeper
- en: Zookeeper is one of the oldest projects of this type. It originated out of the
    Hadoop world, where it was built to help the maintenance of various components
    in a Hadoop cluster. It is mature, reliable and used by many big companies (YouTube,
    eBay, Yahoo, and so on). The format of the data it stores is similar to the organization
    of the file system. If run on a server cluster, Zookeeper will share the state
    of the configuration across all of the nodes. Each cluster elects a leader and
    clients can connect to any of the servers to retrieve data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper 是这种类型中最古老的项目之一。它起源于 Hadoop 世界，最初是为了帮助维护 Hadoop 集群中的各种组件而构建的。它成熟、可靠，许多大公司（如
    YouTube、eBay、Yahoo 等）都在使用它。它存储的数据格式类似于文件系统的组织方式。如果在服务器集群中运行，Zookeeper 将在所有节点之间共享配置的状态。每个集群会选举一个领导者，客户端可以连接到任何服务器以检索数据。
- en: The main advantages Zookeeper brings to the table is its maturity, robustness,
    and feature richness. However, it comes with its set of disadvantages, with Java
    and complexity being main culprits. While Java is great for many use cases, it
    is massive for this type of work. Zookeeper's usage of Java, together with a considerable
    number of dependencies, makes Zookeeper much more resource hungry that its competition.
    On top of those problems, Zookeeper is complex. Maintaining it requires considerably
    more knowledge than we should expect from an application of this type. That is
    the part where feature richness converts itself from an advantage to a liability.
    The more features an application has, the bigger the chances that we won't need
    all of them. Thus, we end up paying the price in the form of complexity for something
    we do not fully need.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper 带来的主要优点是它的成熟性、稳健性和功能丰富性。然而，它也有一系列的缺点，Java 和复杂性是主要的原因。虽然 Java 对许多使用场景来说很优秀，但对于这种类型的工作来说，它的体积庞大。Zookeeper
    使用 Java，加上大量的依赖项，使得它比竞争对手更消耗资源。除了这些问题，Zookeeper 也很复杂。维护它需要的知识远远超过我们对这种类型应用的预期。这正是功能丰富性从优势转化为负担的地方。一个应用拥有的功能越多，我们就越有可能用不到其中的一些功能。于是，我们最终为不完全需要的功能付出了复杂性这个代价。
- en: Zookeeper paved the way that others followed with considerable improvements.
    "Big players" are using it because there were no better alternatives at the time.
    Today, Zookeeper shows its age, and we are better off with alternatives.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper 为后来的其他工具铺平了道路，并做出了显著的改进。那些“巨头”之所以使用它，是因为当时没有更好的替代品。今天，Zookeeper 显露出它的老化，我们有更好的替代方案。
- en: We'll skip Zookeeper examples and skip straight into better options.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过Zookeeper示例，直接进入更好的选项。
- en: etcd
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: etcd
- en: etcd is a key/value store accessible through HTTP. It is distributed and features
    hierarchical configuration system that can be used to build service discovery.
    It is very easy to deploy, setup and use, provides reliable data persistence,
    it's secure and with excellent documentation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: etcd是一个通过HTTP访问的键/值存储系统。它是分布式的，具有层次化配置系统，可以用于构建服务发现。它非常容易部署、设置和使用，提供可靠的数据持久性，安全且文档完善。
- en: etcd is a better option than Zookeeper due to its simplicity. However, it needs
    to be combined with a few third-party tools before it can serve service discovery
    objectives.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性，etcd比Zookeeper更具优势。然而，它需要与一些第三方工具结合，才能实现服务发现的目标。
- en: Setting Up etcd
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置etcd
- en: Let us set up the *etcd*. First, we should create the first node in the cluster
    (*serv-disc-01*) together with the, already familiar, *cd* VM.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置*etcd*。首先，我们应该创建集群中的第一个节点（*serv-disc-01*），并且使用之前熟悉的*cd*虚拟机。
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the cluster node *serv-disc-01* up and running, we can install `etcd` and
    `etcdctl` (etcd command line client).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群节点*serv-disc-01*启动并运行后，我们可以安装`etcd`和`etcdctl`（etcd命令行客户端）。
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We downloaded, uncompressed and moved the executables to `/usr/local/bin` so
    that they are easily accessible. Then, we removed unneeded files and, finally,
    run the `etcd` with output redirected to `/tmp/etcd.log`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载、解压并将可执行文件移动到`/usr/local/bin`，使其易于访问。然后，我们删除了不需要的文件，最后将`etcd`运行，并将输出重定向到`/tmp/etcd.log`。
- en: Let's see what we can do with etcd.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们能用etcd做些什么。
- en: 'Basic operations are `set` and `get`. Please note that we can set a key/value
    inside a directory:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基本操作是`set`和`get`。请注意，我们可以在目录中设置键/值：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first command put the key `port` with the value `1234` into the directory
    `myService`. The second did the same with the key `ip`, and the last two commands
    were used to output values of those two keys.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令将键`port`和值`1234`放入目录`myService`中。第二个命令将键`ip`也放入其中，最后两个命令用于输出这两个键的值。
- en: 'We can also list all the keys in the specified directory or delete a key with
    its value:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以列出指定目录中的所有键，或删除某个键及其值：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The last command output only the `/myService/ip` value since previous command
    removed the port.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个命令只输出了`/myService/ip`的值，因为前一个命令已删除了端口。
- en: 'Besides `etcdctl`, we can also run all commands through HTTP API. Before we
    try it out, let''s install `jq` so that we can see the formatted output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`etcdctl`，我们还可以通过HTTP API运行所有命令。在尝试之前，我们先安装`jq`，以便查看格式化后的输出：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can, for example, put a value into `etcd` through its HTTP API and retrieve
    it through a `GET` request.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们可以通过HTTP API将一个值放入`etcd`，并通过`GET`请求检索它。
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `jq ''.''` is not required, but I tend to use it often to format JSON.
    The output should be similar to the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`jq ''.''`不是必须的，但我经常使用它来格式化JSON。输出应该类似于以下内容：'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: HTTP API is especially useful when we need to query etcd remotely. In most,
    I prefer the `etcdctl`, when running ad-hoc commands while HTTP is a preferred
    way to interact with `etcd` through some code.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP API在我们需要远程查询etcd时特别有用。大多数时候，我更喜欢使用`etcdctl`，当执行临时命令时，而HTTP则是通过一些代码与`etcd`交互的首选方式。
- en: 'Now that we''ve seen (briefly) how etcd works on a single server, let us try
    it inside a cluster. The cluster setup requires a few additional arguments to
    be passed to `etcd`. Let''s say that we''ll have a cluster of three nodes with
    IPs `10.100.197.201` (`serv-disc-01`), `10.100.197.202` (`serv-disc-02`) and `10.100.197.203`
    (`serv-disc-03`). The etcd command that should be run on the first server would
    be the following (please don''t run it yet):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经简要了解了etcd在单个服务器上的工作原理，让我们在集群中试试看。集群的设置需要传递一些额外的参数给`etcd`。假设我们有一个包含三个节点的集群，IP地址分别是`10.100.197.201`（`serv-disc-01`）、`10.100.197.202`（`serv-disc-02`）和`10.100.197.203`（`serv-disc-03`）。在第一台服务器上运行的etcd命令如下（请暂时不要运行）：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I extracted parts that would change from one server (or a cluster) to another
    into variables so that you can see them clearly. We won't go into details of what
    each argument means. You can find more information in the [https://coreos.com/etcd/docs/latest/clustering.html](https://coreos.com/etcd/docs/latest/clustering.html).
    Suffice to say that we specified the IP and the name of the server where this
    command should run as well as the list of all the servers in the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我将会把从一个服务器（或集群）到另一个服务器可能会改变的部分提取成变量，这样你可以清楚地看到它们。我们不会深入讨论每个参数的具体含义。你可以在[https://coreos.com/etcd/docs/latest/clustering.html](https://coreos.com/etcd/docs/latest/clustering.html)找到更多信息。可以简单地说，我们指定了这个命令应该在哪个服务器上运行的IP地址和名称，以及集群中所有服务器的列表。
- en: 'Before we start working on the `etcd` deployment to the cluster, let us kill
    the currently running instance and create the rest of servers (there should be
    three in total):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始在集群中部署`etcd`之前，让我们先终止当前正在运行的实例，并创建其余的服务器（总共有三个）：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Doing the same set of tasks manually across multiple servers is tedious and
    error prone. Since we already worked with Ansible, we can use it to set up etcd
    across the cluster. This should be a fairly easy task since we already have all
    the commands, and all we have to do is translate those we already run into the
    Ansible format. We can create the `etcd` role and add it to the playbook with
    the same name. The role is fairly simple. It copies the executables to the `/usr/local/bin`
    directory and runs etcd with the cluster arguments (the very long command we examined
    above). Let us take a look at it before running the playbook.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个服务器上手动执行相同的任务集既繁琐又容易出错。由于我们已经使用过Ansible，我们可以用它在集群中设置etcd。这应该是一个相对简单的任务，因为我们已经有了所有的命令，我们所要做的就是将已经运行的命令转换为Ansible格式。我们可以创建`etcd`角色，并将其添加到具有相同名称的playbook中。该角色相当简单，它将可执行文件复制到`/usr/local/bin`目录，并使用集群参数运行etcd（我们上面分析过的非常长的命令）。在运行playbook之前，让我们先来看一下它。
- en: The first task in the `roles/etcd/tasks/main.yml` is as follows.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`roles/etcd/tasks/main.yml`中的第一个任务如下：'
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The name is purely descriptive and followed with the copy module. Then, we
    are specifying few of the module options. The copy option `src` indicates the
    name of the local file we want to copy and is relative to the `files` directory
    inside the role. The second copy option (`dest`) is the destination path on the
    remote server. Finally, we are setting the mode to be `755`. The user that runs
    with roles will have `read/write/execute` permissions, and those belonging to
    the same group and everyone else will be assigned `read/execute` permissions.
    Next is the `with_items` declaration that allows us to use a list of values. In
    this case, the values are specified in the `roles/etcd/defaults/main.yml` file
    and are as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名称纯粹是描述性的，后面跟着的是复制模块。接下来，我们指定了一些模块选项。复制选项`src`表示我们要复制的本地文件的名称，它相对于角色内部的`files`目录。第二个复制选项（`dest`）是远程服务器上的目标路径。最后，我们将模式设置为`755`。运行角色的用户将拥有`读/写/执行`权限，而属于同一组和其他所有人的用户将被赋予`读/执行`权限。接下来是`with_items`声明，它允许我们使用一个值的列表。在这种情况下，值是在`roles/etcd/defaults/main.yml`文件中指定的，内容如下：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Externalizing variables is a good way to keep things that might change in the
    future separated from the tasks. If, for example, we are to copy another file
    through this role, we'd add it here and avoid even opening the tasks file. The
    task that uses the `files` variable will iterate for each value in the list and,
    in this case, run twice; once for `etcd` and the second time for `etcdctl`. Values
    from variables are represented with the variable key surrounded with `{{` and
    `}}` and use the Jinja2 format. Finally, we set `etcd` to be the tag associated
    with this task. Tags can be used to filter tasks when running playbooks and are
    very handy when we want to run only a subset of them or when we want to exclude
    something.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 外部化变量是将可能在未来发生变化的内容与任务分离开来的好方法。例如，如果我们需要通过这个角色复制另一个文件，我们可以在这里添加它，而无需打开任务文件。使用`files`变量的任务将对列表中的每个值进行迭代，在这种情况下，它将运行两次；第一次是针对`etcd`，第二次是针对`etcdctl`。变量的值通过变量键被`{{`和`}}`包围，并使用Jinja2格式表示。最后，我们将`etcd`设置为与该任务相关联的标签。标签可以在运行playbook时用来过滤任务，非常方便，当我们只想运行其中一部分任务或想排除某些任务时，标签特别有用。
- en: 'The second task is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个任务如下：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Shell module is often the last resort since it does not work with states. In
    most cases, commands run as through shell will not check whether something is
    in the correct state or not and run every time we execute Ansible playbook. However,
    etcd always runs only a single instance and there is no risk that multiple executions
    of this command will produce multiple instances. we have a lot of arguments and
    all those that might change are put as variables. Some of them, like ansible_hostname,
    are discovered by Ansible. Others were defined by us and placed in the `roles/etcd/defaults/main.yml`.
    With all the tasks defined, we can take a look at the playbook `etcd.yml`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Shell 模块通常是最后的手段，因为它不与状态一起工作。在大多数情况下，通过 shell 运行的命令不会检查某些东西是否处于正确的状态，而每次执行 Ansible
    playbook 时都会运行这些命令。然而，etcd 总是只运行单个实例，多个执行该命令不会导致多个实例的产生。我们有很多参数，其中可能会改变的参数都作为变量放置。像
    ansible_hostname 这样的参数是由 Ansible 自动发现的，其他的参数则是我们定义的，并且放在了 `roles/etcd/defaults/main.yml`
    文件中。定义好所有任务后，我们可以看看 playbook `etcd.yml`：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When this playbook is run, Ansible will configure all the servers defined in
    an inventory, use `vagrant` as the remote user, run commands as `sudo` and execute
    the `common` and `etcd` roles.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行这个 playbook 时，Ansible 会配置所有在清单中定义的服务器，使用 `vagrant` 作为远程用户，使用 `sudo` 执行命令，并执行
    `common` 和 `etcd` 角色。
- en: 'Let us take a look at the `hosts/serv-disc` file. It is our inventory that
    contains the list of all hosts we''re using:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `hosts/serv-disc` 文件。它是我们的清单，包含了我们正在使用的所有主机列表：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, you can a different way to define hosts. The second line is
    Ansible's way of saying that all addresses between `10.100.194.201` and `10.100.194.203`
    should be used. In total, we have three IPs specified for this purpose.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，您可以使用另一种方式来定义主机。第二行是 Ansible 的方式，表示应使用 `10.100.194.201` 和 `10.100.194.203`
    之间的所有地址。总的来说，我们为此目的指定了三个 IP 地址。
- en: 'Let''s run the `etcd` playbook and see it in action:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行 `etcd` playbook，看看它是如何工作的：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can check whether etcd cluster was correctly set by putting a value through
    one server and getting it from the another:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个服务器发送一个值，并从另一个服务器获取它，来检查 etcd 集群是否正确设置：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of those commands should be similar to the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令的输出应该类似于以下内容：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We sent the HTTP PUT request to the `serv-disc-01` server (`10.100.197.201`)
    and retrieved the stored value through the HTTP GET request from the `serv-disc-03`
    (`10.100.197.203`) node. In other words, data set through any of the servers in
    the cluster is available in all of them. Isn't that neat?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向 `serv-disc-01` 服务器（`10.100.197.201`）发送了 HTTP PUT 请求，并通过 HTTP GET 请求从 `serv-disc-03`（`10.100.197.203`）节点获取了存储的值。换句话说，通过集群中任何服务器设置的数据都可以在所有服务器中访问。是不是很棒？
- en: Our cluster (after we deploy few containers), would look as presented in the
    Figure 8-6.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的集群（在我们部署几个容器之后）将如图 8-6 所示。
- en: '![Setting Up etcd](img/B05848_08_06.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![设置 etcd](img/B05848_08_06.jpg)'
- en: Figure 8-6 – Multiple nodes with Docker containers and etcd
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-6 – 多节点 Docker 容器与 etcd
- en: Now that we have a place to store the information related to our services, we
    need a tool that will send that information to etcd automatically. After all,
    why would we put data to etcd manually if that can be done automatically? Even
    if we would want to put the information manually to etcd, we often don't know
    what that information is. Remember, services might be deployed to a server with
    least containers running and it might have a random port assigned. Ideally, that
    tool should monitor Docker on all nodes and update etcd whenever a new container
    is run, or an existing one is stopped. One of the tools that can help us with
    this goal is *Registrator*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个存储与服务相关信息的位置，我们需要一个工具来自动将这些信息发送到 etcd。毕竟，如果可以自动完成，为什么我们要手动将数据放入 etcd
    呢？即使我们想手动将信息放入 etcd，我们通常也不知道这些信息是什么。记住，服务可能会部署到一个容器较少的服务器上，并且该服务器可能会分配一个随机端口。理想情况下，这个工具应该监控所有节点上的
    Docker，并在运行新容器或停止现有容器时更新 etcd。可以帮助我们实现这个目标的工具之一是 *Registrator*。
- en: Setting Up Registrator
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Registrator
- en: Registrator automatically registers and deregisters services by inspecting containers
    as they are brought online or stopped. It currently supports **etcd**, **Consul**
    and **SkyDNS 2**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Registrator 通过检查容器的状态，自动注册和注销服务，容器上线或停止时会触发这一过程。它当前支持 **etcd**、**Consul** 和
    **SkyDNS 2**。
- en: 'Setting up Registrator with etcd registry is easy. We can simply run the Docker
    container as follows (please do not run it yourself):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 设置带有 etcd 注册表的 Registrator 很简单。我们可以按照以下方式运行 Docker 容器（请不要自己运行）：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With this command we are sharing `/var/run/docker.sock` as Docker volume. Registrator
    will monitor and intercept Docker events and, depending on the event type, put
    or remove service information to/from etcd. With the `-h` argument we are specifying
    the hostname. Finally, we are passing two arguments to Registrator. The first
    one is the `-ip` and represents the IP of the host and the second one is the protocol
    (`etcd`), the IP (`serv-disc-01`) and the port (`2379`) of the registration service.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令，我们将 `/var/run/docker.sock` 作为 Docker 卷共享。Registrator 会监控并拦截 Docker 事件，根据事件类型，将服务信息添加到或从
    etcd 中移除。通过 `-h` 参数，我们指定了主机名。最后，我们传递了两个参数给 Registrator。第一个是 `-ip`，表示主机的 IP，第二个是协议（`etcd`）、IP（`serv-disc-01`）和注册服务的端口（`2379`）。
- en: Before we proceed, let's create a new Ansible role called *registrator* and
    deploy it to all nodes inside the cluster. The `roles/registrator/tasks/main.yml`
    file is as follows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们创建一个新的 Ansible 角色，命名为 *registrator*，并将其部署到集群中的所有节点。`roles/registrator/tasks/main.yml`
    文件如下。
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This Ansible role is equivalent to the manual command we saw earlier. Please
    note that we changed the hard-coded `etcd` protocol with a variable. That way
    we can reuse this role with other registries as well. Keep in mind that having
    quotes is not mandatory in Ansible except when value starts with `{{` as in the
    case of the `hos` `tname` value.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Ansible 角色相当于我们之前看到的手动命令。请注意，我们将硬编码的 `etcd` 协议替换为一个变量。这样，我们就可以将这个角色与其他注册表一起重用。请记住，除非值以
    `{{` 开头（如 `hos` `tname` 的值），否则在 Ansible 中不强制要求使用引号。
- en: Let's take a look at the `registrator-etcd.yml` playbook.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `registrator-etcd.yml` playbook。
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Most of the playbook is similar to those we used before except the `vars` key.
    In this case, we're using it to define the Registrator protocol as `etcd` and
    port of the registry as `2379`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 playbook 大部分内容与我们之前使用过的类似，唯一不同的是 `vars` 键。在这种情况下，我们用它来定义 Registrator 协议为
    `etcd`，并将注册表的端口设置为 `2379`。
- en: With everything in place, we can run the playbook.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪后，我们可以运行 playbook。
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once the playbook is finished executing, Registrator will be running on all
    three nodes of our cluster.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 playbook 执行完毕，Registrator 将在我们集群中的所有三个节点上运行。
- en: 'Let''s give Registrator a spin and run one container inside one of the three
    cluster nodes:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一下 Registrator，并在三个集群节点中的一个上运行一个容器：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We exported the `DOCKER_HOST` variable so that Docker commands are sent to the
    cluster node 2 (`serv-disc-02`) and run the `nginx` container exposing port `1234`.
    We'll use `nginx` later on, and there will be plenty of opportunities to get familiar
    with it. For now, we are not interested in what nginx does, but that Registrator
    detected it and stored the information in etcd. In this case, we put a few environment
    variables (`SERVICE_NAME` and `SERVICE_ID`) that Registrator can use to identify
    better the service.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导出了 `DOCKER_HOST` 变量，以便 Docker 命令发送到集群节点 2（`serv-disc-02`）并运行 `nginx` 容器，暴露端口
    `1234`。我们稍后会使用 `nginx`，并且会有很多机会熟悉它。目前，我们不关心 nginx 的具体功能，而是 Registrator 是否检测到它并将信息存储在
    etcd 中。在这种情况下，我们设置了一些环境变量（`SERVICE_NAME` 和 `SERVICE_ID`），供 Registrator 用来更好地识别服务。
- en: Let us take a look at Registrator's log.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下 Registrator 的日志。
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output should be similar to the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can see that Registrator detected nginx container with the ID `5cf7dd974939`.
    We can also see that it ignored the port `443`. The `nginx` container internally
    exposes ports 80 and `443`. However, we exposed only `80` to the outside world,
    so Registrator decided to ignore the port `443`. After all, why would we store
    the information about the port not accessible to anyone?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Registrator 检测到了 ID 为 `5cf7dd974939` 的 nginx 容器。我们还可以看到，它忽略了端口 `443`。`nginx`
    容器内部暴露了端口 80 和 `443`，但是我们只向外界暴露了 `80` 端口，因此 Registrator 决定忽略端口 `443`。毕竟，为什么要存储一个对任何人都不可访问的端口信息呢？
- en: 'Now, let us take a look at data stored in etcd:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下存储在 etcd 中的数据：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of the last command is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个命令的输出如下：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first command listed all keys at the root, the second listed all those inside
    `nginx-80` and the last one retrieved the final value. Registrator stored values
    in the format `/` that matches environment variables we used when running the
    container. Please note that in case more that one port is defined for a service,
    Registrator adds it as a suffix (e.g. nginx-`80`). The value that Registrator
    put corresponds with the IP of the host where the container is running and the
    port that we exposed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令列出了根目录下的所有键，第二个列出了`nginx-80`中的所有键，最后一个命令检索了最终值。`Registrator`以与我们运行容器时使用的环境变量相匹配的`/`格式存储值。请注意，如果为某个服务定义了多个端口，`Registrator`会将其作为后缀添加（例如：nginx-`80`）。`Registrator`存储的值与容器运行所在主机的IP地址和我们暴露的端口相对应。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that even though the container is run on the node 2, we queried
    etcd running on the node 1\. It was yet another demonstration that data is replicated
    across all nodes etcd is running on.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管容器运行在节点2上，但我们查询的是运行在节点1上的`etcd`。这再次展示了数据是如何在所有运行`etcd`的节点之间复制的。
- en: What happens when we remove the container?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们删除容器时会发生什么？
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output of Registrator logs should be similar to the following:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`Registrator`日志的输出应与以下内容类似：'
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Registrator detected that we removed the container and sent a request to etcd
    to remove corresponding values. We can confirm that with the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`Registrator`检测到我们删除了容器，并向`etcd`发送请求以删除相应的值。我们可以通过以下命令确认这一点：'
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The service with the ID `nginx/nginx` disappeared.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 服务ID为`nginx/nginx`的服务消失了。
- en: Registrator combined with `etcd` is a powerful, yet simple, combination that
    will allow us to practice many advanced techniques. Whenever we bring up a container,
    data will be stored in etcd and propagated to all nodes in the cluster. What we'll
    do with that information will be the subject of the next chapter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`Registrator` 与 `etcd` 结合是一个强大而简单的组合，允许我们练习许多高级技术。每当我们启动一个容器时，数据将存储在`etcd`中，并传播到集群中的所有节点。我们将如何使用这些信息将是下一章的主题。'
- en: '![Setting Up Registrator](img/B05848_08_07.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Registrator](img/B05848_08_07.jpg)'
- en: Figure 8-7 – Multiple nodes with Docker containers, etcd and Registrator
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-7 – 拥有多个节点的Docker容器、etcd和Registrator
- en: There is one more piece of the puzzle missing. We need a way to create configuration
    files with data stored in `etcd` as well as run some commands when those files
    are created.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个缺失的部分。我们需要一种方式，用于创建包含存储在`etcd`中的数据的配置文件，并在这些文件创建时运行某些命令。
- en: Setting Up confd
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 confd
- en: The confd is a lightweight tool that can be used to maintain configuration files.
    The most common usage of the tool is keeping configuration files up-to-date using
    data stored in `etcd`, `consul`, and few other data registries. It can also be
    used to reload applications when configuration files change. In other words, we
    can use it as a way to reconfigure services with the information stored in etcd
    (or few other registries).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: confd 是一个轻量级的工具，可以用来维护配置文件。该工具最常见的用途是使用存储在`etcd`、`consul`以及其他少数数据注册表中的数据来保持配置文件的最新状态。它还可以在配置文件更改时重新加载应用程序。换句话说，我们可以利用它作为重新配置服务的一种方式，依靠存储在`etcd`（或其他少数注册表）中的信息。
- en: 'Installing `confd` is straightforward. The commands are as follows (please
    don''t run them yet):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`confd`非常简单。命令如下（请勿立即运行）：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In order for `confd` to work, we need a configuration file located in the `/etc/confd/conf.d/`
    directory and a template in the `/etc/confd/templates`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让`confd`正常工作，我们需要在`/etc/confd/conf.d/`目录下放置一个配置文件，并在`/etc/confd/templates`目录中放置一个模板。
- en: 'Example configuration file is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 示例配置文件如下：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As a minimum, we need to specify template source, destination file, and keys
    that will be fetched from the registry.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，我们需要指定模板源、目标文件以及将从注册表中提取的键。
- en: 'Templates use GoLang text templates format. An example template is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 模板使用GoLang文本模板格式。一个示例模板如下：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: When this template is processed, it will substitute `{{getv "/nginx/nginx"}}`
    with the value from the registry.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理此模板时，它会用注册表中的值替换`{{getv "/nginx/nginx"}}`。
- en: 'Finally, `confd` can be run in two modes. In the Daemon mode, it polls a registry
    and updates destination configuration whenever relevant values change. The `onetime`
    mode is run once. An example of the `onetime` mode is as follows (please do not
    run it yet):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`confd`可以以两种模式运行。在守护进程模式下，它会轮询注册表，并在相关值变化时更新目标配置文件。`onetime`模式则只运行一次。`onetime`模式的示例如下（请勿立即运行）：
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This command would run in the `onetime` mode, would use `etcd` as the backend
    running on the specified node. When executed, destination configuration would
    be updated with values from the `etcd` registry.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将在 `onetime` 模式下运行，并将 `etcd` 作为后台在指定节点上运行。执行时，目标配置将使用来自 `etcd` 注册表的值进行更新。
- en: Now that we know basics of how confd works, let's take a look at the Ansible
    role `confd` that will make sure that it is installed on all servers in the cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 confd 的基本工作原理，接下来让我们看看 Ansible 角色 `confd`，它将确保在集群中的所有服务器上安装该工具。
- en: 'The `roles/confd/tasks/main.yml` file is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`roles/confd/tasks/main.yml` 文件内容如下：'
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This Ansible role is even simpler than the one we created for `etcd` since
    we are not even running the binary. It makes sure that directories are created
    and that files are copied to the destination servers. Since there are multiple
    directories and files involved, we defined them as variables in the `roles/confd/defaults/main.yml`
    file:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Ansible 角色比我们为 `etcd` 创建的角色还要简单，因为我们甚至没有运行二进制文件。它确保了目录的创建，并将文件复制到目标服务器。由于涉及多个目录和文件，我们将它们定义为
    `roles/confd/defaults/main.yml` 文件中的变量：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We defined directories where we'll put configurations and templates. We also
    defined files that need to be copied; one binary, one configuration, and one template
    file that we'll use to try out confd.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了存放配置和模板的目录。我们还定义了需要复制的文件：一个二进制文件、一个配置文件，以及一个模板文件，我们将使用它来尝试 confd。
- en: 'Finally, we need `confd.yml` file that will act as the Ansible playbook:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个 `confd.yml` 文件，它将作为 Ansible 的 playbook：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: There's nothing new to discuss since this file is almost the same the other
    playbooks we worked with.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该文件几乎与我们之前处理的其他 playbooks 相同，因此没有新内容需要讨论。
- en: 'With everything set up, we can deploy confd to all the cluster servers:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一切设置好后，我们可以将 confd 部署到所有集群服务器上：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With `confd` installed on all nodes in the cluster, we can try it out.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中的所有节点上安装了 `confd` 后，我们可以开始尝试了。
- en: 'Let''s run the nginx container again so that Registrator can put some data
    to etcd:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新启动 nginx 容器，这样 Registrator 就能将一些数据放入 etcd 中：
- en: '[PRE38]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We run the nginx container on the `serv-disc-01` node and exposed the port
    `4321`. Since *Registrator* is already running on that server, it put data to
    `etcd`. Finally, we run the local instance of `confd` that checked all its configuration
    files and compared keys with those stored in etcd. Since `nginx/nginx` key has
    been changed in etcd, it processed the template and updated the destination config.
    That can be seen from the output that should be similar to the following (timestamp
    has been removed for brevity):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `serv-disc-01` 节点上运行了 nginx 容器，并暴露了端口 `4321`。由于 *Registrator* 已经在该服务器上运行，它将数据放入了
    `etcd` 中。最后，我们运行了本地的 `confd` 实例，它检查了所有的配置文件，并将其中的键与存储在 etcd 中的键进行比较。由于 `nginx/nginx`
    键在 etcd 中发生了变化，它处理了模板并更新了目标配置。输出结果应该类似于以下内容（为了简洁，已去掉时间戳）：
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It found that the `/tmp/example.conf` is out of sync and updated it. Let us
    confirm that:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 它发现 `/tmp/example.conf` 与实际不符并进行了更新。让我们确认一下：
- en: '[PRE40]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE41]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If any of the changes in templates or `etcd` data is updated, running `confd`
    will make sure that all destination configurations are updated accordingly:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模板或 `etcd` 数据中的任何更改已更新，运行 `confd` 将确保所有目标配置也会相应更新：
- en: '![Setting Up confd](img/B05848_08_08.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![设置 confd](img/B05848_08_08.jpg)'
- en: Figure 8-8 – Multiple nodes with Docker containers, etcd, Registrator and confd
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-8 – 带有 Docker 容器、etcd、Registrator 和 confd 的多个节点
- en: Combining etcd, Registrator, and confd
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合 etcd、Registrator 和 confd
- en: When etcd, Registrator, and confd are combined, we get a simple yet powerful
    way to automate all our service discovery and configuration needs. That will come
    in handy when we start working on more advanced deployment strategies. The combination
    also demonstrates the effectiveness of having the right mix of small tools. Those
    three do what we need them to do. Less than this and we would not be able to accomplish
    the goals set in front of us. If, on the other hand, they were designed with bigger
    scope in mind, we would introduce unnecessary complexity and overhead on server
    resources and maintenance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当 etcd、Registrator 和 confd 结合使用时，我们就能得到一种简单而强大的方式来自动化所有的服务发现和配置需求。这在我们开始处理更高级的部署策略时将大有帮助。这个组合还展示了拥有正确的小工具组合的有效性。这三者完成了我们需要它们完成的工作。如果少了其中任何一个，我们将无法完成预定的目标。相反，如果它们是以更大范围的目标设计的，我们将引入不必要的复杂性，增加服务器资源和维护的负担。
- en: Before we make the final verdict, let's take a look at another combination of
    tools with similar goals. After all, we should never settle for some solution
    without investigating alternatives.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做出最终决定之前，让我们看一下另一个目标相似的工具组合。毕竟，我们永远不应该在没有调查其他选择的情况下就决定某个解决方案。
- en: Consul
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Consul
- en: Consul is strongly consistent datastore that uses gossip to form dynamic clusters.
    It features hierarchical key/value store that can be used not only to store data
    but also to register watches that can be used for a variety of tasks, from sending
    notifications about data changes, to running health checks and custom commands
    depending on their output.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Consul 是一个强一致性的数据库，它使用 gossip 协议形成动态集群。它具有层级化的键/值存储，不仅可以用于存储数据，还可以注册监控，用于各种任务，例如发送数据变化的通知、运行健康检查和根据输出运行自定义命令。
- en: Unlike Zookeeper and etcd, Consul implements service discovery system embedded,
    so there is no need to build your own or use a third-party one. This discovery
    includes, among other things, health checks of nodes and services running on top
    of them.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Zookeeper 和 etcd 不同，Consul 实现了内嵌的服务发现系统，因此无需自己构建或使用第三方的。此发现系统包括，除了其他功能外，还能检查节点及其上运行的服务的健康状态。
- en: ZooKeeper and etcd provide only a primitive K/V store and require that application
    developers build their own system to provide service discovery. Consul, on the
    other hand, provides a built-in framework for service discovery. Clients only
    need to register services and perform discovery using the DNS or HTTP interface.
    The other two tools require either a hand-made solution or the usage of third-party
    tools.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper 和 etcd 仅提供一个原始的 K/V 存储，并要求应用开发者构建自己的系统来提供服务发现。另一方面，Consul 提供了一个内建的服务发现框架。客户端只需注册服务，并通过
    DNS 或 HTTP 接口进行发现。另两个工具则需要手动解决方案或使用第三方工具。
- en: Consul offers out of the box native support for multiple data centers and the
    gossip system that works not only with nodes in the same cluster but across data
    centers as well.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Consul 提供开箱即用的本地多数据中心支持，并且其 gossip 系统不仅能与同一集群中的节点协作，还能跨数据中心工作。
- en: Consul has another nice feature that distinguishes it from the others. Not only
    that it can be used to discover information about deployed services and nodes
    they reside on, but it also provides easy to extend health checks through HTTP
    and TCP requests, TTLs (time-to-live), custom scripts and even Docker commands.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Consul 还有另一个使其与其他工具不同的优点。它不仅可以用来发现有关已部署服务和节点的信息，还提供了通过 HTTP 和 TCP 请求、TTL（生存时间）、自定义脚本甚至
    Docker 命令的健康检查，且易于扩展。
- en: Setting Up Consul
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Consul
- en: 'As before, we''ll start by exploring manual installation commands and, later
    on, automate them with Ansible. We''ll configure it on the `cd` node as an exercise:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，我们首先通过手动安装命令来开始，然后用 Ansible 自动化它们。我们将在 `cd` 节点上进行配置作为练习：
- en: '[PRE42]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We started by installing `unzip` since it is not included in default Ubuntu
    distribution. Then we downloaded the Consul ZIP, unpacked it, moved it to the
    `/usr/local/bin` directory, removed the ZIP file since we won't need it anymore
    and, finally, created few directories. Consul will place its information to the
    `data` directory and configuration files into `config`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装了 `unzip`，因为它不包含在默认的 Ubuntu 发行版中。然后我们下载了 Consul 的 ZIP 文件，解压后将其移动到 `/usr/local/bin`
    目录，删除了 ZIP 文件，因为我们不再需要它，最后创建了一些目录。Consul 会将其信息存放在 `data` 目录中，配置文件则放在 `config`
    中。
- en: 'Next we can run `consul`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们可以运行 `consul`：
- en: '[PRE43]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Running Consul was very straight forward. We specified that it should run the
    `agent` as a `server` and that there will be only one server instance `(-bootstrap-expect
    1`). That is followed by locations of key directories; `ui`, `data` and `config`.
    Then we specified the name of the `node`, address it will `bind` to and which
    `client` can connect to it (`0.0.0.0` refers to all). Finally, we redirected the
    output and made sure that it's running in the background (`&`).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Consul 非常直接。我们指定它应该作为 `server` 运行 `agent`，并且只会有一个服务器实例 `(-bootstrap-expect
    1)`。接着是关键目录的路径：`ui`、`data` 和 `config`。然后我们指定了 `node` 的名称、它将 `bind` 的地址以及可以连接到它的
    `client`（`0.0.0.0` 表示所有地址）。最后，我们将输出重定向并确保它在后台运行（`&`）。
- en: Let's verify that Consul started correctly.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证 Consul 是否正确启动。
- en: '[PRE44]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The output of the log file should be similar to the following (timestamps are
    removed for brevity).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 日志文件的输出应该类似于以下内容（为了简洁，省略了时间戳）。
- en: '[PRE45]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We can see that the Consul agent we run in server mode elected itself as the
    leader (which is to be expected since it's the only one).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，作为服务器模式运行的 Consul 代理自选为领导者（这也在预期之中，因为它是唯一的一个）。
- en: With Consul up and running, let's see how we can put some data into it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 启动并运行 Consul 后，让我们看看如何向其添加一些数据。
- en: '[PRE46]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The first command created the `msg1` key with the value `this is a test`. The
    second had nested the key `msg2` into a parent key `messages`. Finally, the last
    command added the `flag` with the value `1234`. Flags can be used to store version
    number or any other information that can be expressed as an integer.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令创建了`msg1`键，值为`this is a test`。第二个命令将`msg2`键嵌套到父键`messages`中。最后，最后一个命令添加了`flag`，值为`1234`。标志可以用来存储版本号或任何其他可以表示为整数的信息。
- en: 'Let''s take a look how to retrieve the information we just stored:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何检索我们刚刚存储的信息：
- en: '[PRE47]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output of the command is as follows (order is not guaranteed):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下（顺序无法保证）：
- en: '[PRE48]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Since we used the `recurse` query, keys were returned from the root recursively.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了`recurse`查询，键值是从根递归返回的。
- en: Here we can see all the keys we inserted. However, the value is base64 encoded.
    Consul can store more than text and, in fact, it stores everything as binary under
    the hood. Since not everything can be represented as text, you can store anything
    in Consul's K/V, but there are size limitations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到我们插入的所有键。但是，值是经过 base64 编码的。Consul 不仅能存储文本，实际上它在幕后存储所有内容为二进制数据。由于并非所有内容都可以表示为文本，因此你可以将任何内容存储在
    Consul 的键值对中，但有大小限制。
- en: 'We can also retrieve a single key:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检索单个键：
- en: '[PRE49]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is the same as before but limited to the key `msg1`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与之前相同，但仅限于键`msg1`：
- en: '[PRE50]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we can request only the value:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以只请求值：
- en: '[PRE51]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This time, we put the `raw` query parameter and the result is only the value
    of the requested key:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们添加了`raw`查询参数，结果仅返回请求的键的值：
- en: '[PRE52]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'As you might have guessed, Consul keys can easily be deleted. The command to,
    for example, delete the `messages/msg2` key is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，Consul 的键可以轻松地被删除。例如，删除`messages/msg2`键的命令如下：
- en: '[PRE53]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can also delete recursively:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以递归删除：
- en: '[PRE54]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The Consul agent we deployed was set up to be the server. However, most agents
    do not need to run in the server mode. Depending on the number of nodes, we might
    opt for three Consul agents running in the server mode and many non-server agents
    joining it. If, on the other hand, the number of nodes is indeed big, we might
    increase the number of agents running in the server mode to five. If only one
    server is running, there will be data loss in case of its failure. In our case,
    since the cluster consists of only three nodes and this is a demo environment,
    one Consul agent running in the server mode is more than enough.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署的 Consul 代理被设置为服务器模式。然而，大多数代理不需要在服务器模式下运行。根据节点数量，我们可以选择三个 Consul 代理运行在服务器模式下，其他非服务器代理加入它。另一方面，如果节点数非常多，我们可能会将运行在服务器模式下的代理数量增加到五个。如果只运行一个服务器，若其失败则会导致数据丢失。在我们的情况下，由于集群仅由三个节点组成，而且这是一个演示环境，一个运行在服务器模式下的
    Consul 代理就足够了。
- en: 'The command to run an agent on the `serv-disc-02` node and make it join the
    cluster is as follows (please don''t run it yet):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在`serv-disc-02`节点上运行代理并使其加入集群的命令如下（请不要立即运行）：
- en: '[PRE55]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The only difference we did when compared with the previous execution is the
    removal of arguments `-server` and `-bootstrap-expect 1`. However, running Consul
    in one of the cluster servers is not enough. We need to join it with the Consul
    agent running on the other server. The command to accomplish that is as follows
    (please don't run it yet).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的执行相比，唯一的区别是移除了`-server`和`-bootstrap-expect 1`参数。然而，在集群服务器上运行 Consul 还不够，我们需要将其与运行在其他服务器上的
    Consul 代理连接起来。完成这项工作的命令如下（请不要立即运行）。
- en: '[PRE56]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The effect of running this command is that agents of both servers would be clustered
    and data synchronized between them. If we continued adding Consul agents to other
    servers and joining them, the effect would be an increased number of cluster nodes
    registered in Consul. There is no need to join more than one agent since Consul
    uses a gossip protocol to manage membership and broadcast messages to the cluster.
    That is one of the useful improvements when compared to `etcd` that requires us
    to specify the list of all servers in the cluster. Managing such a list tends
    to be more complicated when the number of servers increases. With the gossip protocol,
    Consul is capable of discovering nodes in the cluster without us telling it where
    they are.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此命令的效果是两个服务器的代理会聚集在一起，数据在它们之间同步。如果我们继续向其他服务器添加Consul代理并让它们加入，效果将是Consul中注册的集群节点数增加。无需加入多个代理，因为Consul使用gossip协议来管理成员身份并广播消息到集群。这是与`etcd`相比的一个有用改进，`etcd`要求我们指定集群中所有服务器的列表。而当服务器数量增加时，管理这样的列表会变得更加复杂。通过gossip协议，Consul能够在不告知它节点位置的情况下自动发现集群中的节点。
- en: 'With Consul basics covered, let''s see how we can automate its configuration
    across all servers in the cluster. Since we are already committed to Ansible,
    we''ll create a new role for Consul. While the configuration we''re about to explore
    is very similar to those we did by now, there are few new details we have not
    yet seen.tasks from the Ansible role `roles/consul/tasks/main.yml` are as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握Consul的基础知识后，让我们看看如何在集群中的所有服务器上自动化配置它。由于我们已经使用Ansible，我们将为Consul创建一个新的角色。虽然我们即将探索的配置与之前的配置非常相似，但也有一些我们之前没有看到的新细节。Ansible角色`roles/consul/tasks/main.yml`中的任务如下：
- en: '[PRE57]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We started by creating directories and copying files. Both tasks use variables
    array specified in the `with_items` tag.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建了目录并复制了文件。这两个任务都使用在`with_items`标签中指定的变量数组。
- en: 'Let''s take a look at those variables. They are defined in the `roles/consul/defaults/main.yml`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些变量。它们在`roles/consul/defaults/main.yml`文件中定义：
- en: '[PRE58]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Even though we could specify all those variables inside the `roles/consul/tasks/main.yml`
    file, having them separated allows us to change their values more easily. In this
    case, have a simple list of directories and the list of files in JSON format with
    source, destination and mode.tinue with the tasks in the `roles/consul/tasks/main.yml`.
    The third one is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们可以在`roles/consul/tasks/main.yml`文件中指定所有这些变量，将它们分开存放可以让我们更容易地更改它们的值。在这种情况下，我们有一个简单的目录列表和一个以JSON格式表示的文件列表，其中包括源、目标和模式。然后继续执行`roles/consul/tasks/main.yml`中的任务。第三个任务如下：
- en: '[PRE59]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Since Consul makes sure that there is only one process running at the time,
    there is no danger running this task multiple times. It is equivalent to the command
    we run manually with an addition of a few variables.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Consul确保在任何时候只有一个进程在运行，因此多次执行此任务并不会造成危险。它相当于我们手动执行的命令，只是多了几个变量。
- en: 'If you remember the manual execution of Consul, one node should run Consul
    in the server node and the rest should join at least one node so that Consul can
    gossip that information to the whole cluster. We defined those differences as
    the (`consul_extra`) variable. Unlike those we used before that are defined in
    `roles/consul/defaults/main.y` `ml` file inside the role, `consul_extra` is defined
    in the `hosts/serv-disc` inventory file. Let''s take a look at it:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得手动执行Consul的过程，应该有一个节点在服务器节点上运行Consul，其余节点至少加入一个节点，以便Consul能够将这些信息传播到整个集群中。我们将这些差异定义为(`consul_extra`)变量。与之前定义的变量不同，这些变量定义在`roles/consul/defaults/main.yml`文件内，而`consul_extra`是在`hosts/serv-disc`清单文件中定义的。我们来看看它：
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We defined variables to the right of the server IPs. In this case, the `.201`
    is acting as a server. The rest is defining the `consul_server_ip` variables that
    we'll discuss very soon.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在服务器IP的右侧定义了变量。在这种情况下，`.201`作为服务器使用。其余部分定义了我们稍后将讨论的`consul_server_ip`变量。
- en: 'Let''s jump into the fourth (and last) task defined in the `roles/consul/tasks/main.yml`
    file:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳到`roles/consul/tasks/main.yml`文件中定义的第四个（也是最后一个）任务：
- en: '[PRE61]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This task makes sure that every Consul agent, except the one running in the
    server mode, joins the cluster. The task runs the same command like the one we
    executed manually, with the addition of the `consul_server_ip` variable that has
    a double usage. The first usage is to provide value for the `shell` command. The
    second usage is to decide whether this task is run at all. We accomplished that
    using the `when: consu` `l_server_i` `p is defined` definition:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '这个任务确保除了在服务器模式下运行的 Consul 代理外，其他所有 Consul 代理都加入了集群。该任务执行与我们手动执行的相同命令，并添加了 `consul_server_ip`
    变量，该变量有双重用途。第一个用途是为 `shell` 命令提供值。第二个用途是决定是否运行此任务。我们通过使用 `when: consul_server_ip
    is defined` 定义实现了这一点：'
- en: 'Finally, we have the `consul.yml` playbook, that is as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 `consul.yml` playbook，内容如下：
- en: '[PRE62]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: There's not much to say about it since it follows the same structure as the
    playbooks we used before.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于它没有太多要说的，因为它遵循了我们之前使用的 playbook 的相同结构。
- en: Now that we have the playbook, let us execute it and take a look at Consul nodes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 playbook，让我们执行它并查看 Consul 节点。
- en: '[PRE63]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can confirm whether Consul is indeed running on all nodes by sending the
    *nodes* request to one of its agents:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向其中一个代理发送 *nodes* 请求来确认 Consul 是否确实在所有节点上运行：
- en: '[PRE64]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The output of the command is as follows.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下。
- en: '[PRE65]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: All three nodes in the cluster are now running Consul. With that out of the
    way, we can move back to Registrator and see how it behaves when combined with
    Consul.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的所有三个节点现在都在运行 Consul。现在可以回到 Registrator，看看它与 Consul 配合使用时的表现。
- en: '![Setting Up Consul](img/B05848_08_09.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Consul](img/B05848_08_09.jpg)'
- en: Figure 8-9 – Multiple nodes with Docker containers and Consul
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-9 – 使用 Docker 容器和 Consul 的多个节点
- en: Setting Up Registrator
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Registrator
- en: 'Registrator has two Consul protocols. We''ll take a look at `consulkv` first
    since its results should be very similar to those obtained with the etcd protocol:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Registrator 有两种 Consul 协议。我们先看一下 `consulkv`，因为它的结果应该与使用 etcd 协议时获得的结果非常相似：
- en: '[PRE66]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let''s take a look at the Registrator log and check whether everything seems
    to be working correctly:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 Registrator 日志，检查一切是否正常工作：
- en: '[PRE67]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output should be similar to the following (timestamps were removed for
    brevity):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容（为了简洁，已移除时间戳）：
- en: '[PRE68]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The result is the same as when we run Registrator with the etcd protocol. It
    found the nginx container running (the one that we started previously while practicing
    `etcd`) and published the exposed port `4321` to Consul. We can confirm that by
    querying Consul:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与我们使用 etcd 协议运行 Registrator 时相同。它找到了运行中的 nginx 容器（就是我们之前在练习 `etcd` 时启动的那个），并将公开的端口
    `4321` 发布到 Consul。我们可以通过查询 Consul 来确认这一点：
- en: '[PRE69]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As expected, the output is the IP and the port exposed through the nginx container:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，输出是通过 nginx 容器暴露的 IP 和端口：
- en: '[PRE70]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: However, Registrator has another protocol called `consul` (the one we just used
    is `consulkv`) that utilizes Consul's format for storing service information.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Registrator 还有另一种协议叫做 `consul`（我们刚刚使用的是 `consulkv`），它利用 Consul 存储服务信息的格式。
- en: '[PRE71]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Let''s see what information Registrator sent to Consul this time:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这次 Registrator 向 Consul 发送了什么信息：
- en: '[PRE72]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This time, the data is a bit more complete yet still in a very simple format:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，数据更完整一些，但仍然保持非常简单的格式：
- en: '[PRE73]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Besides the IP and the port that is normally stored with `etcd` or `consulkv`
    protocols, this time, we got more information. We know the node the service is
    running on, service ID and the name. We can do even better than that with few
    additional environment variables. Let''s bring up another nginx container and
    see the data stored in Consul:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通常与 `etcd` 或 `consulkv` 协议一起存储的 IP 和端口外，这次我们得到了更多的信息。我们知道服务运行的节点、服务 ID 和名称。通过添加几个额外的环境变量，我们甚至可以做得更好。让我们再启动一个
    nginx 容器，看看 Consul 中存储的数据：
- en: '[PRE74]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The output of the last command is as follows.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个命令的输出如下。
- en: '[PRE75]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The second container (`nginx2`) was registered and, this time, Consul got tags
    that we might find useful later on. Since both containers are listed under the
    same name Consul considers them to be two instances of the same service.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个容器（`nginx2`）已注册，这次，Consul 获取了我们可能稍后会用到的标签。由于这两个容器都列在同一名称下，Consul 将它们视为同一服务的两个实例。
- en: 'Now that we know how Registrator works in conjunction with Consul, let''s configure
    it in all nodes of the cluster. The good news is that the role is already created,
    and we set the protocol to be defined with the variable `protocol`. We also put
    the name of the container as the `registrator_name` variable so that we can bring
    the Registrator container with the consul protocol without getting in conflict
    with the etcd one we configured earlier:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了 Registrator 如何与 Consul 配合工作，让我们在集群中的所有节点上配置它。好消息是角色已经创建，并且我们设置了通过 `protocol`
    变量定义协议。我们还将容器的名称作为 `registrator_name` 变量，以便能够通过 Consul 协议启动 Registrator 容器，而不会与之前配置的
    etcd 容器发生冲突：
- en: The playbook `registrator.yml` is as follows.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: playbook `registrator.yml` 如下所示。
- en: '[PRE76]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The `registrator-etcd.yml` has the `registrator_protocol` variable set to `etcd`
    and `registrator_port` to `2379`. We didn''t need it in this case since we already
    had default values set to `consul` and `8500` in the `roles/registrator/defaults/main.yml`
    file. On the other hand, we did overwrite the default value of the `registrator_name`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`registrator-etcd.yml` 文件中的 `registrator_protocol` 变量设置为 `etcd`，`registrator_port`
    设置为 `2379`。在本例中我们不需要它，因为我们已经在 `roles/registrator/defaults/main.yml` 文件中设置了默认值为
    `consul` 和 `8500`。另一方面，我们确实覆盖了 `registrator_name` 的默认值：'
- en: 'With everything ready, we can run the playbook:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪后，我们可以运行 playbook：
- en: '[PRE77]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Once the execution of this playbook is finished, Registrator with the consul
    protocol will be configured on all nodes in the cluster:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦此 playbook 执行完成，使用 Consul 协议的 Registrator 将在集群中的所有节点上配置完成：
- en: '![Setting Up Registrator](img/B05848_08_10.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Registrator](img/B05848_08_10.jpg)'
- en: Figure 8-10 – Multiple nodes with Docker containers, Consul and Registrator
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-10 – 多个节点与 Docker 容器、Consul 和 Registrator
- en: How about templating? Should we use confd or something else?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 模板化怎么样？我们应该使用 confd 还是其他工具？
- en: Setting Up Consul Template
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Consul Template
- en: We can use confd with Consul in the same way as we used it with etcd. However,
    Consul has its own templating service with features more in line with what Consul
    offers.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像在 etcd 中使用 confd 一样在 Consul 中使用它。然而，Consul 有自己独特的模板服务，其功能更符合 Consul 提供的功能。
- en: Consul Template is a very convenient way to create files with values obtained
    from Consul. As a bonus, it can also run arbitrary commands after the files have
    been updated. Just as confd, Consul Template also uses Go Template format.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Consul Template 是一种非常方便的方式，可以使用从 Consul 获得的值创建文件。作为额外的好处，它还可以在文件更新后运行任意命令。与
    confd 一样，Consul Template 也使用 Go Template 格式。
- en: By now, you're probably accustomed to the routine. First we'll try Consul Template
    manually. As with all other tools, we set up in this chapter, installation consists
    of downloading the release, unpacking it and making sure that the executable is
    in the system path.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经习惯了这个流程。首先我们将手动尝试 Consul Template。与本章中设置的所有其他工具一样，安装过程包括下载发布版本、解压缩并确保可执行文件位于系统路径中。
- en: '[PRE78]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'With Consul Template available on the node, we should create one template:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点上安装了 Consul Template 后，我们应该创建一个模板：
- en: '[PRE79]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: When this template is processed, it will iterate (`range`) over all services
    with the name `nginx-80`. Each iteration will produce the text with service `Address`
    and `Port`. Template has been created as `/tmp/nginx.ctmpl`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个模板被处理时，它将对所有名为 `nginx-80` 的服务进行迭代（`range`）。每次迭代都会生成包含服务 `Address` 和 `Port`
    的文本。模板已创建为 `/tmp/nginx.ctmpl`。
- en: 'Before we run the Consul Template, let''s take another look at what we have
    stored in Consul for the nginx services:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 Consul Template 之前，让我们再看看我们在 Consul 中存储的 nginx 服务信息：
- en: '[PRE80]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE81]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We have two nginx services up and running and registered in Consul. Let''s
    see the result of applying the template we created:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个 nginx 服务已经启动并在 Consul 中注册。让我们看看应用我们创建的模板后的结果：
- en: '[PRE82]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The result of the second command is as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个命令的结果如下：
- en: '[PRE83]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The Consul Template command we executed found both services and generated the
    output in the format we specified. We specified that it should run only once.
    The alternative is to run it in daemon mode. In such a case, it would monitor
    the registry for changes and apply them to specified configuration files.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的 Consul Template 命令找到了两个服务，并以我们指定的格式生成了输出。我们指定它应该只运行一次。另一种方式是以守护进程模式运行。在这种情况下，它将监视注册表的变化，并将这些变化应用到指定的配置文件中。
- en: We will go into details of how Consul Template works later on when we start
    using it in our deployment pipeline. Until then, please consult [https://www.consul.io/docs/](https://www.consul.io/docs/)
    yourself. For now, it is important to understand that it can obtain any information
    stored in Consul and apply it to the template we specify. Besides creating the
    file, it can also run custom commands. That will come in handy with `reverse proxy`,
    that is the subject of our next chapter.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 等到我们在部署流水线中开始使用Consul模板时，我们将详细介绍它的工作原理。在那之前，请自行查阅[https://www.consul.io/docs/](https://www.consul.io/docs/)。目前重要的是要理解，它可以获取存储在Consul中的任何信息，并将其应用到我们指定的模板中。除了创建文件之外，它还可以运行自定义命令。这将在我们下一章节讨论的反向代理中非常有用。
- en: We didn't try Consul Template applied to Consul's key/value format. In that
    combination, there is no significant difference when compared to confd.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有尝试将Consul模板应用于Consul的键/值格式。在这种组合中，与confd相比并没有显著的区别。
- en: The major downside Consul Template has is its tight coupling with Consul. Unlike
    confd that can be used with many different registries, Consul Template is created
    as a templating engine tightly integrated with Consul. That is, at the same time,
    an advantage, since it understands Consul's service format. If you choose to use
    Consul, Consul Template is a great fit.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Consul模板的主要缺点是与Consul的紧密耦合。与可以与许多不同的注册中心一起使用的confd不同，Consul模板被创建为与Consul紧密集成的模板引擎。这一点既是优势，因为它理解Consul的服务格式。如果选择使用Consul，Consul模板非常合适。
- en: Before we move on to the next subject, let's create Consul Template role and
    configure it on all nodes. The `roles/consul-template/tasks/main.yml` file is
    as follows.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续下一个主题之前，让我们创建Consul模板角色，并在所有节点上配置它。`roles/consul-template/tasks/main.yml`文件如下所示。
- en: '[PRE84]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'There''s nothing exciting with this role. It''s probably the simplest one we
    did by now. The same holds true for the `consul-template.yml` playbook:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这个角色并没有什么特别的。到目前为止，这可能是我们做过的最简单的一个角色。对于`consul-template.yml`playbook也是一样：
- en: '[PRE85]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'And, finally, we can configure it on all nodes:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在所有节点上进行配置：
- en: '[PRE86]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The end result is very similar to the etcd/Registrator combination with the
    difference in data format sent to Consul:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果与etcd/Registrator组合非常相似，唯一的区别在于发送到Consul的数据格式：
- en: '![Setting Up Consul Template](img/B05848_08_11.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![设置Consul模板](img/B05848_08_11.jpg)'
- en: Figure 8-11 – Multiple nodes with Docker containers, Consul, Registrator and
    Consul Template
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-11 – 使用Docker容器、Consul、Registrator和Consul模板的多节点
- en: Up to this point, we covered Consul's features that are, somewhat, similar to
    the etcd/registrator/confd combination. It's time to take a look at the characteristics
    that make Consul indeed stand up from the crowd.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了Consul的功能，这些功能在某种程度上类似于etcd/registrator/confd组合。现在是时候看看使Consul确实脱颖而出的特性了。
- en: Consul Health Checks, Web UI, and Data Centers
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Consul健康检查、Web UI和数据中心
- en: Monitoring health of cluster nodes and services is as important as testing and
    deployment itself. While we should aim towards having stable environments that
    never fail, we should also acknowledge that unexpected failures happen and be
    prepared to act accordingly. We can, for example, monitor memory usage and, if
    it reaches a certain threshold, move some services to a different node in the
    cluster. That would be an example of preventive actions performed before the "disaster"
    would happen. On the other hand, not all potential failures can be detected in
    time for us to act on time. A single service can fail. A whole node can stop working
    due to a hardware failure. In such cases, we should be prepared to act as fast
    as possible by, for example, replacing a node with a new one and moving failed
    services. We won't go into details how Consul can help us in this task since there
    is a whole chapter dedicated to `self-healing systems` and Consul will play a
    major role in it. For now, suffice to say that Consul has a simple, elegant and,
    yet, powerful way to perform health checks that can help us define what actions
    should be performed when health thresholds are reached.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 监控集群节点和服务的健康状况和测试、部署本身一样重要。虽然我们应该努力实现稳定的环境，避免故障，但也应该承认意外的故障会发生，并做好相应的准备。例如，我们可以监控内存使用情况，如果达到某个阈值，可以将一些服务迁移到集群中的其他节点。这是发生“灾难”前采取的预防性措施。另一方面，并不是所有潜在的故障都能及时检测到，以便我们及时采取行动。一项服务可能会失败，一个节点可能因硬件故障而停止工作。在这种情况下，我们应该准备好尽快采取行动，例如，通过更换一个新节点并迁移失败的服务。我们不会详细讨论
    Consul 如何帮助我们完成这项任务，因为有一整章专门讲解 `自愈系统`，而 Consul 在其中将发挥重要作用。现在，足以说 Consul 具有一种简单、优雅且强大的健康检查方式，可以帮助我们在健康阈值达到时确定应该采取的措施。
- en: If you googled `etcd ui` or `etcd dashboard` you probably saw that there are
    a few solutions available, and you might be asking why we haven't presented them.
    The reason is simple; etcd is a key/value store and not much more. Having a UI
    to present data is not of much use since we can easily obtain it through the etcdctl.
    That does not mean that etcd UI is of no use but that it does not make much difference
    due to its limited scope.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您谷歌过 `etcd ui` 或 `etcd dashboard`，您可能看到了一些现有的解决方案，您可能会问为什么我们没有展示它们。原因很简单：etcd
    只是一个键/值存储，没什么更多的功能。拥有一个 UI 来展示数据并没有太大用处，因为我们可以通过 etcdctl 容易地获取数据。这并不意味着 etcd UI
    没有用处，而是由于其功能范围有限，它并不会带来太大的区别。
- en: Consul is much more than a simple key/value store. As we've already seen, besides
    storing key/value pairs, it has a notion of a service together with data that
    belong to it. It can also perform health checks, thus becoming a good candidate
    for a dashboard that can be used to see the status of our nodes and services running
    on top of them. Finally, it understands the concept of multiple data centers.
    All those features combined, let us see the need for a dashboard in a different
    light.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Consul 不仅仅是一个简单的键/值存储。正如我们已经看到的，除了存储键/值对，它还包含与之关联的服务和数据。它还可以执行健康检查，因此成为一个理想的仪表板，可以用来查看我们节点和在其上运行的服务的状态。最后，它理解多个数据中心的概念。所有这些功能结合起来，让我们以不同的角度看待仪表板的需求。
- en: With the Consul Web UI, we can view all services and nodes, monitor health checks
    and their statuses, read and set key/value data as well as switch from one data
    center to another. To see it in action, please open `http://10.100.194.201:8500/ui`
    in your favorite browser. You'll see items in the top menu that correspond to
    the steps we performed earlier through the API.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Consul Web UI，我们可以查看所有服务和节点，监控健康检查及其状态，读取和设置键/值数据，并在不同数据中心之间切换。要查看实际效果，请在您喜欢的浏览器中打开
    `http://10.100.194.201:8500/ui`。您将看到顶部菜单中的项目，这些项目对应我们之前通过 API 执行的步骤。
- en: 'The `Services` menu item lists all the services we registered. There''s not
    much at the moment since only Consul server, Docker UI and two instances of the
    nginx service are up and running. We can filter them by name or status and see
    details by clicking on one of the registered services:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`Services` 菜单项列出了我们注册的所有服务。目前没有多少，因为只有 Consul 服务器、Docker UI 和两个 nginx 服务实例正在运行。我们可以通过名称或状态进行过滤，并通过点击其中一个注册的服务来查看详细信息：'
- en: '![Consul Health Checks, Web UI, and Data Centers](img/B05848_08_12.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 健康检查、Web UI 和数据中心](img/B05848_08_12.jpg)'
- en: Figure 8-12 – Consul Web UI services
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-12 – Consul Web UI 服务
- en: 'Nodes show us the list of all nodes belonging to the selected data center.
    In our case, we have three nodes. The first one has three registered services:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 节点显示我们所选数据中心中所有节点的列表。在我们的例子中，我们有三个节点。第一个节点有三个注册服务：
- en: '![Consul Health Checks, Web UI, and Data Centers](img/B05848_08_13.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 健康检查、Web UI 和数据中心](img/B05848_08_13.jpg)'
- en: Figure 8-13 – Consul Web UI nodes
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-13 – Consul Web UI 节点
- en: The `Key/Value` screen can be used to both display and modify data. In it, you
    can see data put to Consul by the Registrator instance set to use `consulkv` as
    the protocol. Please feel free to add data yourself and see how they are visualized
    in the UI. Besides working with Consul key/value data with the API we used before,
    you can also manage them through the UI.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '`Key/Value` 屏幕可用于显示和修改数据。在该屏幕上，您可以看到由 Registrator 实例（已设置为使用 `consulkv` 协议）放入
    Consul 的数据。请随意添加数据，并查看它们如何在 UI 中进行可视化。除了通过我们之前使用的 API 操作 Consul 键值数据外，您还可以通过 UI
    管理这些数据。'
- en: '![Consul Health Checks, Web UI, and Data Centers](img/B05848_08_14.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 健康检查、Web UI 和数据中心](img/B05848_08_14.jpg)'
- en: Figure 8-14 – Consul Web UI key/value
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-14 – Consul Web UI 键值
- en: Note
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that Consul allows us to group nodes into data centers. We haven't
    used this feature since we are running only three nodes. When nodes in the cluster
    start increasing, splitting them into data centers is often a good idea and Consul
    helps us to visualize them through its UI.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Consul 允许我们将节点分组到数据中心中。由于我们只有三个节点，因此并未使用此功能。当集群中的节点开始增加时，将它们分割到不同的数据中心通常是一个好主意，且
    Consul 帮助我们通过其 UI 进行可视化。
- en: Combining Consul, Registrator, Template, Health Checks and WEB UI
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 Consul、Registrator、Template、Health Checks 和 Web UI 结合使用
- en: Consul, together with the tools we explored, is in many cases a better solution
    than what etcd offers. It was designed with services architecture and discovery
    in mind. It is simple, yet powerful. It provides a complete solution without sacrificing
    simplicity and, in many cases, it is the best tool for service discovery and health
    checking needs (at least among those we evaluated).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们探索的工具一起，Consul 在许多情况下比 etcd 提供的解决方案更好。它是专为服务架构和发现设计的。它简单却强大，提供了一个完整的解决方案而没有牺牲简洁性，在许多情况下，它是服务发现和健康检查需求的最佳工具（至少在我们评估的工具中是这样）。
- en: Service Discovery Tools Compared
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现工具对比
- en: All of the tools are based on similar principles and architecture. They run
    on nodes, require a quorum to operate and are strongly consistent. They all provide
    some form of key/value storage.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工具都基于相似的原则和架构。它们运行在节点上，需要法定人数才能操作，并且具有强一致性。它们都提供某种形式的键值存储。
- en: Zookeeper is the oldest of the three, and the age shows in its complexity, utilization
    of resources and goals it's trying to accomplish. It was designed in a different
    age than the rest of the tools we evaluated (even though it's not much older).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper 是三者中最古老的，它的复杂性、资源利用和试图实现的目标都显现出它的年代感。它的设计时代与我们评估的其他工具不同（尽管它并不比其他工具早多少）。
- en: '`etcd` with Registrator and `confd` is a very simple, yet very powerful combination
    that can solve most, if not all, of our service discovery needs. It showcases
    the power we can obtain when we combine simple and very specific tools. Each of
    them performs a very specific task, communicates through well-established API
    and is capable of working with relative autonomy. They are `microservices` both
    in their architectural as well as their functional approach.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd` 与 Registrator 和 `confd` 的组合是一个非常简单，但又非常强大的组合，可以解决我们大多数（如果不是所有）服务发现的需求。它展示了当我们将简单且非常具体的工具组合时可以获得的强大功能。每个工具都执行一个非常具体的任务，通过成熟的
    API 进行通信，并能够相对独立地工作。它们在架构和功能上都是 `微服务`。'
- en: What distinguishes `Consul` is the support for multiple data centers and health
    checking without the usage of third-party tools. That does not mean that the usage
    of third-party tools is wrong. Actually, throughout this book we are trying to
    combine different tools by choosing those that are performing better than others
    without introducing unnecessary features overhead. The best results are obtained
    when we use right tools for the job. If the tool does more than the job we require,
    its efficiency drops. On the other hand, a tool that doesn't do what we need it
    to do is useless. Consul strikes the right balance. It does very few things, and
    it does them well.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`Consul`的独特之处在于它支持多个数据中心和健康检查，而无需使用第三方工具。这并不意味着使用第三方工具是错误的。事实上，在本书中，我们正尝试通过选择那些性能优于其他工具且不引入不必要的功能开销的工具，来将不同工具结合使用。当我们使用适合工作的工具时，可以获得最佳的结果。如果工具做得比我们要求的更多，它的效率会下降。另一方面，不能满足我们需求的工具则毫无用处。Consul在这方面达到了平衡。它做的事情很少，而且做得很好。'
- en: The way Consul uses the gossip protocol to propagate knowledge about the cluster
    makes it easier to set up than etcd, especially in the case of a big data center.
    The ability to store data as a service makes it more complete and useful than
    key/value storage used in etcd (even though Consul has that option as well). While
    we could accomplish the same by inserting multiple keys in etcd, Consul's service
    achieves a more compact result that often requires a single query to retrieve
    all the data related to the service. On top of that, Registrator has quite a good
    implementation of the Consul protocol making the two an excellent combination,
    especially when Consul Template is added to this mixture. Consul's Web UI is like
    a cherry on top of a cake and provides a good way to visualize your services and
    their health.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Consul通过使用gossip协议传播集群信息，使其比etcd更易于设置，尤其是在大数据中心的情况下。将数据作为服务存储的能力使其比etcd中使用的键值存储更加完整和实用（尽管Consul也有这个选项）。虽然我们可以通过在etcd中插入多个键来实现相同的效果，但Consul的服务实现了更紧凑的结果，通常只需要一次查询即可检索与该服务相关的所有数据。更重要的是，Registrator对Consul协议有很好的实现，使得两者成为一个出色的组合，尤其是在将Consul
    Template加入其中时。Consul的Web UI就像蛋糕上的樱桃，为你提供了一个很好的方式来可视化你的服务及其健康状态。
- en: I can't say that Consul is a clear winner. Instead, it has a slight edge when
    compared with etcd. Service discovery as a concept, as well as the tools we can
    use, is so new that we can expect a lot of changes in this field. By the time
    you read this book, it's likely that new tools will come, or those we evaluated
    will change enough that some of the exercises we did will become obsolete. Have
    an open mind and try to take bits of advice from this chapter with a grain of
    salt. The logic we employed is solid and is not likely to change anytime soon.
    The same can not be said for tools. They are bound to evolve rapidly soon.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我不能说Consul是明显的赢家。相反，与etcd相比，它有一些优势。作为一个概念，服务发现以及我们可以使用的工具是如此新颖，以至于我们可以预期这个领域会发生很多变化。当你读到这本书时，可能会有新的工具出现，或者我们评估的工具发生足够的变化，以至于我们做的一些练习会变得过时。保持开放的心态，尝试以适当的批判态度接受本章的一些建议。我们使用的逻辑是坚实的，不太可能很快改变，但工具则不一定如此。它们很可能会迅速发展。
- en: We are left with one more subject before we can get back to our deployment procedure.
    The integration step will require that we go through `reverse proxy`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们回到部署过程之前，还剩下一个主题。集成步骤将要求我们了解`反向代理`。
- en: Before we move on, let's destroy the virtual machines we created for the purpose
    of service discovery practice and free some resources for the next chapter.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，先销毁我们为服务发现实践创建的虚拟机，并为下一章释放一些资源。
- en: '[PRE87]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
