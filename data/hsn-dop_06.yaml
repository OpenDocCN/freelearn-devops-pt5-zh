- en: Building Big Data Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建大数据应用程序
- en: In this chapter, we will learn to build big data applications, analyze a traditional
    end-to end data workflow life cycle, and on similar lines build a big data application
    step by step. We will cover the big data process--discovery, ingestion, visualization,
    and governance. The emphasis will be on the Spark platform and data science prediction
    models. DevOps applications to various phases of big data will be explored in
    the subsequent chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习构建大数据应用程序，分析传统端到端数据工作流生命周期，并按类似的线路逐步构建大数据应用程序。我们将涵盖大数据流程--发现、摄入、可视化和治理。重点将放在Spark平台和数据科学预测模型上。随后章节将探讨DevOps应用于大数据各个阶段。
- en: Traditional data platforms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统数据平台
- en: Big data platform core principles
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据平台核心原则
- en: 'Big data life cycle:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据生命周期：
- en: Data discovery
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据发现
- en: Data quality
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量
- en: Data ingestion
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄入
- en: Data analytics
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析
- en: Spark platform
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark平台
- en: Data visualization
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Data governance
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据治理
- en: Building enterprise applications
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建企业应用程序
- en: Data science--prediction models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学--预测模型
- en: Traditional enterprise architecture
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统企业架构
- en: Traditionally, an **enterprise data warehouse **(**EDW**) system is considered
    as a core component of the business intelligence environment. Data warehouse systems
    are central repositories built by integrating data from multiple disparate source
    systems, used for data analysis and reporting the needs of the enterprise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，**企业数据仓库**（**EDW**）系统被视为商业智能环境的核心组件。数据仓库系统是通过集成来自多个不同源系统的数据构建的中央存储库，用于满足企业的数据分析和报告需求。
- en: 'Let''s review the end-to end data life cycle components of the traditional
    system:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾传统系统端到端数据生命周期组件：
- en: The **data discovery** phase is where the source systems are explored and analyzed
    for relevant data and data structures. If the analyzed data is valid, correct,
    and usable, it is ingested into the data warehouse system. For example, if we
    need customer ID information, we should be connecting and extracting data from
    the correct columns and tables.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据发现**阶段是探索和分析源系统中相关数据和数据结构的阶段。如果分析的数据有效、正确且可用，则将其摄入数据仓库系统。例如，如果我们需要客户ID信息，则应连接并从正确的列和表中提取数据。'
- en: '**Data quality **ensures that the ingested data is acceptable and usable. A
    simple example is name formats of the first name and last name convention, which
    should be adhered to and, as appropriate, corrected for a few records.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**确保摄入的数据是可接受和可用的。一个简单的例子是名字格式的第一个名和姓氏惯例，应该遵守，并根据需要为一些记录进行纠正。'
- en: '**Data transformation** is the phase where data manipulation rules as per business
    logic are applied in tune with business needs. For example, every employee''s
    yearly salary is computed from multiple systems and saved in the system.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**是根据业务逻辑应用数据操作规则的阶段。例如，每个员工的年薪从多个系统计算并保存在系统中。'
- en: '**Extract**, **Transform**, and **Load** (**ETL**) is the common term for consolidated
    reference for all the preceding phases together (data discovery, data quality,
    and data transformation) as a cycle.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取**、**转换**和**加载**（**ETL**）是所有前述阶段（数据发现、数据质量和数据转换）的集合参考常用术语。'
- en: '**Data staging **is the landing area on your systems for data collected from
    source systems.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据暂存**是您系统中从源系统收集数据的着陆区域。'
- en: '**Data lineage **traces the origin and credibility of the data ingested into
    the system to ensure only authentic, trusted, and authorized data is inducted
    into the system.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据血缘**追溯数据摄入到系统中的来源和可信度，以确保只有真实、可信和授权的数据被引入系统。'
- en: '**Metadata **is data about data. For example, a sales receipt is the origin
    of the record with transaction details, from where the requisite data from our
    computations is extracted. Details such as store ID, sales amount, item ID, date
    of transaction, and so on, are extracted from a sales receipt.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据**是关于数据的数据。例如，销售收据是记录的起源，包含交易详情，从中提取我们计算所需数据的位置。从销售收据中提取商店ID、销售金额、商品ID、交易日期等详细信息。'
- en: '**Data warehouse** is the storage layer, into which the transformed data is
    loaded as consolidated copy. It is time-variant, consistent, and read-intensive.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据仓库**是存储层，将转换后的数据加载为汇总副本。它具有时间变体、一致性和读取密集性。'
- en: '**Data marts** are data serving repositories specializing in some category,
    such as customer data, product data, employee data, and so on.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集市**是专门用于某一类别的数据服务仓库，如客户数据、产品数据、员工数据等。'
- en: '**Data analytics **is the common term for an analysis to address all the business
    needs. Its building queries address business demands, such as how many customers
    were added last month, or what products are selling above targets this week.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析**是指为满足所有业务需求而进行的分析。其构建的查询解决业务需求，例如上个月新增了多少客户，或者本周哪些产品的销售超过了目标。'
- en: '**Semantic layer**--its business interface builds queries on the database with
    business intelligence tools, hiding complex data tables from business users.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义层**——其业务接口使用商业智能工具在数据库上构建查询，隐藏复杂的数据表格，使业务用户无需接触。'
- en: '**Reporting**--reports are for business use, such as all products sold last
    month in a state.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**报告**——报告是供业务使用的，例如展示上个月在某州销售的所有产品。'
- en: The **dashboard **provides a consolidated quick view of important key performance
    indicators. An analogy is a car dashboard with speed, battery, petrol reserve,
    and so on.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仪表盘**提供重要关键绩效指标的快速整合视图。一个类比是汽车仪表盘，显示速度、电池、电量剩余等信息。'
- en: '**Data visualization**--finding key performance business trends solely based
    on Excel reports can be a daunting task. Presenting them in a visual form is quite
    appealing, such as representing them as charts, histograms, and pie diagrams.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化**——仅仅根据Excel报告找到关键的业务表现趋势可能是一项艰巨的任务。将其以可视化形式呈现非常具有吸引力，例如将其表示为图表、直方图和饼图。'
- en: Principles to build big data enterprise applications
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建大数据企业应用的原则
- en: Big data platforms and applications manage, integrate, analyze, and secure analytics
    on many data types both within the enterprise as well as in external data. They
    integrate multiple data sources in real time, taking into account volume, velocity,
    and variety. The platform can be built as a repository of an enterprise knowledge
    base with the organization's collective data assets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据平台和应用程序管理、集成、分析和保护对多种数据类型的分析，不仅涵盖企业内部数据，还包括外部数据。它们实时集成多个数据源，考虑到数据的体积、速度和种类。该平台可以构建为企业知识库的存储库，存储组织的集体数据资产。
- en: 'Some of the salient features for building these platforms are discussed and
    as we see DevOps is very appropriate and instruments to enhance value at every
    stage, like versioning systems for building algorithms, data models, scalable
    reproducible platforms with virtual machines as seen in previous chapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些构建这些平台的显著特点已经讨论过，正如我们所见，DevOps非常适合，并且是增强每个阶段价值的工具，比如用于构建算法、数据模型的版本控制系统，使用虚拟机构建可扩展的可复现平台，正如前一章所见：
- en: '**Flexible data modeling**: Big data systems integrate many different forms
    of data from multiple data sources. Rather than a pre-defined schema of rigid
    rows and columns, the schema is to be defined on the fly and data modeled to reflect
    how information is to be assimilated. To reflect real-world entities, it is also
    flexibly specified as a graph of objects with relationships.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的数据建模**：大数据系统集成了来自多个数据源的多种不同形式的数据。与其采用预定义的固定行列结构，数据架构应动态定义，并且数据建模需反映如何同化信息。为了反映现实世界中的实体，它也可以灵活地指定为具有关系的对象图。'
- en: '**Knowledge management**: Version controlled knowledge base with accumulated
    insights of an organization can be leveraged as an enterprise asset.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识管理**：带有版本控制的知识库，包含组织积累的洞察力，可以作为企业资产加以利用。'
- en: '**Privacy and security controls**: The platform is designed for data lineage,
    multi-level security, and audit compliance. Every object integrated into the platform
    is traced to its original data source, where access restrictions are in place
    including authorization and authentication.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私和安全控制**：该平台设计用于数据血缘、多级安全和审计合规。平台中集成的每个对象都可以追溯到其原始数据源，并且设置了访问限制，包括授权和认证。'
- en: '**Algorithms for data processing**: These are massive datasets to be compiled
    and analyzed with built-in machine learning algorithms to augment the human user''s
    ability to make sense of large-scale data by identifying patterns in the data.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理算法**：这些是大量的数据集，需要使用内置的机器学习算法进行编译和分析，以增强人工用户通过识别数据模式理解大规模数据的能力。'
- en: '**Scalable platforms**: These platforms handle petabyte-scale data through
    a combination of a scalable architecture with federated data storage to hold large
    types of unstructured data, such as documents, emails, audio, video, images, and
    so on. These platforms are designed as open platforms extendable at every layer
    of the stack. The provision for efficient data discovery, data lineage, and elastic
    search tools should be considered.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展平台**：这些平台通过可扩展的架构与联合数据存储相结合，处理PB级别的数据，以存储大量非结构化数据，如文档、电子邮件、音频、视频、图像等。这些平台被设计为开放平台，可以在堆栈的每一层进行扩展。需要考虑高效的数据发现、数据血缘关系和弹性搜索工具的提供。'
- en: '**Collaboration**: This platform enables multiple users, within and across
    organizations, to seamlessly, securely collaborate to analyze the same data concurrently,
    from low-level data integration, importing pipeline customizations, to building
    custom user interfaces. Data that has been integrated can be accessed as objects
    via APIs or can be exported for other frameworks and tools'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协作**：该平台使多个用户能够在组织内外无缝、安全地协作，实时分析相同的数据，从低级数据集成、导入管道定制到构建自定义用户界面。已集成的数据可以通过API作为对象访问，或导出供其他框架和工具使用。'
- en: '**Building models on models**: Simple models can serve as building blocks for
    more complex models, building out sophisticated analyses to be streamlined as
    a modular process. Models can be built using various in-built rich reusable libraries
    of statistical and mathematical operators.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在模型上构建模型**：简单的模型可以作为更复杂模型的构建模块，构建精细的分析并将其流线化为模块化过程。可以使用内置的丰富可重用统计和数学操作库来构建模型。'
- en: '**Data visualization**: This is an interactive user interface to provide a
    seamless holistic view of all the integrated data of interest in the form of rich
    visualizations, such as tables, scatter plots, and charts. These visualizations
    in real-time are up to date with the source data so that users always see the
    most accurate and current information at any given time.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化**：这是一个互动用户界面，提供一个无缝的整体视图，展示所有集成数据的丰富可视化形式，如表格、散点图和图表。这些可视化图表实时与源数据保持同步，使得用户始终能够在任何给定时刻查看最准确和最新的信息。'
- en: Big data systems life cycle
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据系统生命周期
- en: 'Big data systems are built in accordance with the data life cycle model, which
    can be broadly categorized in the following stages:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据系统是根据数据生命周期模型构建的，通常可以分为以下几个阶段：
- en: Data discovery
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据发现
- en: Data quality
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量
- en: Ingesting data into the system
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据导入系统
- en: Persisting the data in storage
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据持久化存储
- en: Analytics on the data
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行分析
- en: Data governance
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据治理
- en: Visualizing the results
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化结果
- en: We will study them in detail next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来详细学习它们。
- en: Data discovery into the system
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据导入系统
- en: Data discovery, like in the traditional process, ingests raw data from multiple
    source systems; however, the data will be divergent in volume, variety, and velocity
    when it comes to transforming it into business insights. Leveraging the power
    of big data, the data discovery process enables data wrangling and data enrichment
    facilitates combining datasets to recreate new perspectives and interactive visual
    analytics. An interactive data catalog facilitates guided search capabilities
    and enables us to thoroughly analyze and understand the data quality. A matured
    and robust data discovery process ensures possible data correlations; it lets
    users define attribute-based rules, relationships between data sources, harmonize
    data sources, and create enriched data based on a data mart.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发现，像传统过程一样，从多个源系统中摄取原始数据；然而，在将其转化为商业洞察时，数据的体积、种类和速度将会有所不同。通过利用大数据的力量，数据发现过程使得数据清洗和数据丰富成为可能，促进将数据集组合以重建新的视角和互动的可视化分析。一个互动的数据目录有助于引导搜索功能，帮助我们彻底分析和理解数据质量。成熟而强大的数据发现过程确保可能的数据关联，它让用户定义基于属性的规则、数据源之间的关系，协调数据源并基于数据集市创建丰富的数据。
- en: To expand the boundaries of traditional business intelligence systems, harnessing
    the potential of big data is the key for business success. It helps to unlock
    the insights from new sources of information effectively. Organizations have the
    potential to access a wealth of knowledge to be tapped appropriately, with the
    proliferation of new sources of digital information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拓展传统商业智能系统的边界，充分利用大数据的潜力是企业成功的关键。这有助于有效地解锁来自新信息源的洞察力。随着新型数字信息源的不断涌现，组织有潜力访问到丰富的知识，并能得当挖掘这些知识。
- en: Data discovery with big data tools and technology facilitates deep exploration
    for visibility into business performance with a big variety of data across the
    organization and even beyond. The business can explore new dimensions, transforming
    the way that business analytic systems are built and used more efficiently. Data
    discovery with any combination of data sources allows rapid, intuitive exploration
    and analysis of information; it enables deeper insight into the business, and
    the opportunity for greater efficiency. It builds new relations redefining roles
    between business and IT, adding new roles, new leadership, and revised means of
    data governance as well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大数据工具和技术进行数据发现，有助于深入探索组织内外的数据，从而清晰地看到商业表现。企业可以探索新的维度，改变商业分析系统的构建和使用方式，使其更高效。通过结合各种数据源，数据发现允许快速、直观地探索和分析信息；它为业务提供了更深层次的洞察力，并带来了更高效率的机会。它建立了重新定义商业与IT之间角色的新关系，同时增设了新的角色、新的领导力，并对数据治理的方式进行了修订。
- en: Many organizations have updated business intelligence decision systems to improve
    business performance based on existing data and systems to understand and monitor.
    These days, the vast majority of data growth is from systems beyond the reach
    of traditional BI environments, such as websites, social media, content management
    systems, emails, documents, sensor data, external databases, and so on. Hence
    the need to adopt to new age data discovery tools, also to consider that this
    diverse and changing data is growing exponentially. Data types to be dealt by
    the discover phase are varied, ranging from structured database tables to semi-structured
    forms containing a mix of numbers and free-form text to wholly unstructured documents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织已经更新了商业智能决策系统，以基于现有数据和系统来提升商业表现，帮助理解和监控数据。如今，绝大多数数据增长来源于传统商业智能环境无法触及的系统，例如网站、社交媒体、内容管理系统、电子邮件、文档、传感器数据、外部数据库等。因此，需要采用新时代的数据发现工具，同时也要考虑到这些多样化和变化的数据正在呈指数级增长。在发现阶段需要处理的数据类型多种多样，从结构化的数据库表格，到包含数字和自由格式文本的半结构化形式，再到完全非结构化的文档。
- en: The biggest challenge along with the volume of data is the variety rather and
    its uncertain value. An internet-savvy business culture and the impact of consumer
    interaction with enterprise business software with mobile and web applications
    have created an urgent need to quickly explore relevant information to discover
    new insight for business prospects and decisions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量和种类的多样性，以及其不确定的价值，是最大的挑战之一。互联网文化的普及和消费者与企业商业软件的互动，特别是通过移动和网页应用程序，催生了迅速探索相关信息的迫切需求，以发现新的业务洞察力并做出决策。
- en: '**Datafication** is the process of quantifying data from all types of sources,
    in all types of formats. Datafication allows information to be collected, tabulated,
    and analyzed, so that the potential uses of the information are limited only by
    the ingenuity of the skilled business user. The data''s true value is like an
    iceberg floating in the ocean. Only a tiny part of it is visible at first sight,
    while much of it is hidden beneath the surface. Innovative companies that understand
    this can extract that hidden value and reap potentially huge benefits.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据化**是将所有类型来源、所有类型格式的数据量化的过程。数据化使得信息能够被收集、整理和分析，从而使信息的潜在用途仅受技能熟练的商业用户的创造力限制。数据的真正价值就像浮在海洋中的冰山。最初，只有一小部分是可见的，而大部分则隐藏在水面下。那些理解这一点的创新公司能够提取隐藏的价值，获得潜在的巨大收益。'
- en: 'Enterprise-class data discovery systems enable rapid, intuitive exploration
    and analysis of data from a wide combination of structured and unstructured sources.
    They enable organizations to channel their existing investments to extend business
    analytics capabilities to new combinations of a greater variety of sources, including
    social media, websites, content systems, e-mail, and database text, providing
    a new level of visibility into data and business processes, saving time and cost
    and leading to better business decisions. Some of the advantages of this approach
    are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 企业级数据发现系统能够快速、直观地探索和分析来自多种结构化和非结构化来源的数据。它们使组织能够利用现有投资，将商业分析能力扩展到新的、多样化的来源组合，包括社交媒体、网站、内容系统、电子邮件和数据库文本，为数据和业务流程提供新的可见性，节省时间和成本，并帮助做出更好的商业决策。此方法的一些优势包括：
- en: You can gain deeper insight into the business by enabling users with increased
    insight and visibility to find the data they want to analyze.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为用户提供更深入的见解和可见性，帮助他们找到自己想要分析的数据，从而深入了解业务。
- en: Near real-time data and content can be delivered by access to fresher information,
    helping people make decisions based on the most current information.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过访问更新的信息，可以实现近实时的数据和内容传递，帮助人们根据最当前的信息做出决策。
- en: Increased reuse of assets. You are able to reuse information assets and eliminate
    the costs of re-creating these assets.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加资产的重用。你可以重用信息资产，并消除重新创建这些资产的成本。
- en: Ease of use for BI professionals to develop and deliver analytic consumer-style
    applications for business professionals, leading to higher adoption rates, lower
    training costs, and faster time to value.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为BI专业人员提供开发和交付适合商业专业人员使用的分析型消费级应用的便利，进而提高采用率、降低培训成本并加快价值实现的速度。
- en: Data discovery stages
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据发现阶段
- en: 'The data discovery process involves stages such as prototyping, visualization,
    bridging, replication, and transformation. These concepts applied in tandem in
    the context of data discovery provide value out of big data:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发现过程涉及原型设计、可视化、桥接、复制和转化等阶段。将这些概念在数据发现的背景下并行应用，可以从大数据中获得价值：
- en: '**Prototyping**: Generally, in any business context, projects are always under
    pressure and constrained to deliver under-limited resources of time and budgets.
    For a complex project, prototyping is an effective way of making progress when
    challenges are pressing, demanding, seem difficult, and are under time constraints.
    Prototyping is about acting before you''ve got the answers, about taking chances
    in the absence of a proven formula or a known way of solving a problem. Prototyping
    lets us test a hypothesis and explore alternative approaches, building in small
    blocks to get the big picture. DevOps expedites the process of prototyping to
    make it a continuous cycle of development'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原型设计**：通常，在任何商业环境中，项目总是面临压力，并受限于时间和预算等资源。在复杂项目中，原型设计是当面临紧迫、苛刻、看似困难且时间有限的挑战时，推动进展的有效方式。原型设计是在没有找到答案之前采取行动，在没有经过验证的公式或已知解决问题的方法的情况下冒险。原型设计让我们能够测试假设并探索替代方法，通过构建小模块来获取整体视图。DevOps加快了原型设计过程，使其成为一个持续的开发循环。'
- en: With data discovery, people are empowered to wander around in the data, try
    new combinations of sources, filter and refine the analysis, find previously hidden
    patterns, or jump to another experimental thread if the first one yields no insight.
    One of the key challenges is synchronizing data schema changes at the source systems
    with staging and development systems in real time; the DevOps methodology and
    process can be adopted to address this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发现让人们能够在数据中自由探索，尝试新的数据源组合、筛选和精炼分析，发现先前隐藏的模式，或者如果第一次尝试没有得到有价值的见解，可以跳转到另一个实验线程。一个关键的挑战是如何实时同步源系统的数据模式变更与暂存和开发系统；可以采用DevOps方法和流程来解决这一问题。
- en: Experimentation through data discovery comprises three main tasks--asking new
    questions, seeing new patterns, and adding new data. These steps comprise a continuous
    process which is iterative and which flows in any direction, depending upon what
    the user sees or theorizes at any given moment. DevOps integrated with the data
    discovery phase makes it an automated process from source systems identification
    to ingestion of data to staging systems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据发现的实验包括三个主要任务——提出新问题、观察新模式和添加新数据。这些步骤构成了一个连续的过程，是迭代的，并且会根据用户在任何给定时刻所看到或假设的内容流动到任何方向。与数据发现阶段集成的DevOps，使得这一过程从源系统识别到数据摄取，再到暂存系统的过程自动化。
- en: '**Visualization**: Data visualization helps data analysts gain immediate feedback
    on their hypotheses, as graphics are more convenient instruments for realizing
    patterns in quantitative information. Viewing the results of empirical business
    analysis in real time enables data analysts to determine what refined or further
    search could lead to a deeper understanding of a performance gap or market opportunity.
    Effective data discovery is augmented by quick visualization of analysis and results
    to aid both experienced data analysts and non-technical business users. Data visualization
    techniques such as drag and drop dashboards, quickly produced charts, easy to
    use wizards, and consumer-style navigation, accelerate understanding of patterns
    of data, and its behavior attributes quickly.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化**：数据可视化帮助数据分析师快速反馈他们的假设，因为图形化更便于在定量信息中发现模式。实时查看经验性业务分析的结果，使数据分析师能够确定哪些细化或进一步的搜索可能导致对绩效差距或市场机会的更深理解。通过快速可视化分析结果来增强有效的数据发现，帮助经验丰富的数据分析师以及非技术性的业务用户。数据可视化技术，如拖放式仪表板、快速生成的图表、易用的向导以及类似消费者风格的导航，加速了对数据模式及其行为特征的理解。'
- en: '**Bridging**: Business and IT can have a more effective working relationship
    through the data discovery process. In a traditional setup, business and IT perform
    separate activities as a provider and user in the data discovery phase. This enables
    them to work together as a team to combine efforts to jointly explore and learn.
    IT analysts help their business partners, showing them how to add data sources,
    refine searches, and explore new questions, stepping through the capabilities
    of the software. They can work with the business directly on how to build discovery
    applications in real time, and how to become self-sufficient in analysis of the
    possibilities. Conventional extracting requirements, writing complex specs, and
    going through a lengthy development and deployment process are tedious in terms
    of time and effort. IT gains huge efficiencies bypassing much of the SDLC process
    and by empowering business users via self-service data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桥接**：通过数据发现过程，业务与IT之间可以建立更加高效的合作关系。在传统环境中，业务和IT分别扮演提供者和用户的角色，进行数据发现阶段的独立活动。这使得他们可以作为一个团队合作，汇聚力量共同探索和学习。IT分析师帮助业务合作伙伴，向他们展示如何添加数据源、完善搜索，并探索新的问题，逐步了解软件的功能。他们还可以直接与业务方合作，如何实时构建发现应用程序，并如何在分析可能性时变得自给自足。传统的提取需求、编写复杂规范，以及经过漫长的开发和部署过程，通常在时间和精力上都非常繁琐。IT通过绕过大部分SDLC流程并赋能业务用户自助服务数据，从而大大提高了效率。'
- en: '**Systematizing**: As data exploration becomes more versed with deep insight
    skills, in some cases a certain path of investigation could be repeatable a few
    times. It could also yield valuable insights beyond a limited one-time scenario
    to more effectively grasp a part of the business question to continually repeat
    and track the outcome. Once the business queries and responses are well established
    by the metrics and dimensions that are used to describe key business processes,
    they can be considered to roll the analysis and metrics into the organization''s
    enterprise BI system. Business intelligence platforms excel in allowing users
    to perform standardized queries for well-known questions on standard datasets.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统化**：随着数据探索与深度洞察技能的不断提升，在某些情况下，某一条调查路径可能会重复几次。这也可能带来超越单次情景的宝贵见解，更有效地抓住业务问题的一部分，并不断重复和追踪结果。一旦通过用于描述关键业务流程的度量标准和维度将业务查询和响应建立起来，就可以考虑将分析和度量整合到组织的企业BI系统中。商业智能平台通过让用户对标准数据集执行标准化查询来回答已知问题，从而表现出色。'
- en: 'Data discovery and business intelligence systems complement each other, with
    data discovery excelling in unknown questions and BI focusing on systematized
    analysis, as depicted in the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发现与商业智能系统互为补充，数据发现擅长解决未知问题，而BI则专注于系统化分析，如下图所示：
- en: '![](img/e727252e-fd20-4acf-a1fb-3f486db0a40d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e727252e-fd20-4acf-a1fb-3f486db0a40d.png)'
- en: DevOps will standardize the process of data models, dashboards, and visualization
    reports can be maintained into the repository and automated testing and deployment
    from development systems to QA and production as continuous integration and deployment
    models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps 将标准化数据模型、仪表板和可视化报告的维护过程，并将其自动化测试和部署，从开发系统到 QA 和生产环境，形成持续集成和部署模型。
- en: '**Transformation**: The data discovery process can help explore an unlimited
    number of new avenues to address business problems and find new opportunities
    previously hidden in the data. Uncovering these new dimensions through a process
    of experimentation uncovers valuable new insights, paving the way for transformation.
    However, for standard known areas, they will continue using existing BI systems,
    and use data discovery to explore ways to give insights for new questions and
    problems. Thus data discovery is proving its value in deploying an easy exploration
    of diverse data to uncover insights that drive dramatic increases in revenue and
    productivity while improving the alignment and relationship of business and IT.
    It enables a transformation in the world of business analytics.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转型**：数据发现过程能够帮助探索无限多的新途径，解决业务问题并发现之前隐藏在数据中的新机会。通过实验过程揭示这些新维度，能够发现有价值的新洞察，为转型铺平道路。然而，对于标准已知领域，企业仍将继续使用现有的BI系统，并利用数据发现探索为新问题和问题提供洞察的方法。因此，数据发现通过易于探索多样数据的方式，展现了其价值，帮助揭示推动收入和生产力显著增长的洞察，同时改善了业务与IT的对齐和关系。它推动了商业分析世界的转型。'
- en: There are several tools in big data discovery, such as Apache PIG, Oracle Big
    Data Discovery, Zoomdata, Exasol, Revolution, GridGain, and so on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据发现领域有多种工具，如 Apache PIG、Oracle Big Data Discovery、Zoomdata、Exasol、Revolution、GridGain
    等。
- en: Traditional BI tools such as Tableau, QlikView, Microstrategy, Informatica,
    and so on, are also offering extensive data discovery features for big data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的商业智能（BI）工具，如 Tableau、QlikView、Microstrategy、Informatica 等，也在为大数据提供广泛的数据发现功能。
- en: Data quality
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量
- en: 'One of the biggest challenges is ensuring data quality and accuracy, which
    is easier said than done. DevOps will augment the data quality process to ensure
    the data quality cycle from scripts to automation of the entire validation process.
    Let''s look at some of the challenges:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个最大的挑战是确保数据质量和准确性，这说起来容易做起来难。DevOps 将增强数据质量过程，确保从脚本到整个验证过程的自动化循环。我们来看看一些挑战：
- en: '**Data variety**: The data coming from diverse data sources such as mobile
    devices, web technologies, sensor data, social media, and so on brings with it
    multiple complex data types and data structures, increasing the difficulty of
    data integration. These data sources produce data types such as unstructured data
    in the form of documents, video, audio, and so on, and semi-structured data like
    software package, modules, spreadsheets, financial reports, and structured data.
    The valuable insights gained depend on the data collected, stored, and verified.
    They come from so many divergent sources and rely on the effectiveness of the
    integration process. When working with data-intensive and sensitive industries
    such as life sciences, this process has to be foolproof.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据多样性**：来自移动设备、Web 技术、传感器数据、社交媒体等各种数据源的数据，带来了多种复杂的数据类型和数据结构，增加了数据集成的难度。这些数据源生成了例如文档、视频、音频等形式的非结构化数据，以及软件包、模块、电子表格、财务报告等形式的半结构化数据和结构化数据。所获得的有价值洞察依赖于收集、存储和验证的数据。由于这些数据来自如此多的不同来源，且依赖于集成过程的有效性，因此至关重要。在处理生命科学等数据密集型且敏感的行业时，这一过程必须是万无一失的。'
- en: '**Complexity of data**: The data becomes complex, accounting for multiple attributes
    such as raw data from a variety of different sources directly from consumers,
    salespeople, operations, and other sources within the organization. The timeliness
    of the data is crucial; you need to gather the required data in real time or deal
    with the data needs in real time, otherwise the data can become stale, outdated,
    and invalid. Processing analysis based on the data will produce useless or misleading
    information and conclusions, misleading the decision-making systems.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据复杂性**：数据变得复杂，涉及多个属性，例如来自不同来源的原始数据，这些来源包括消费者、销售人员、运营部门以及组织内的其他来源。数据的时效性至关重要；你需要实时收集所需数据，或实时处理数据需求，否则数据可能会变得陈旧、过时和无效。基于这些数据的处理分析将产生无用或误导性的信息和结论，从而误导决策系统。'
- en: '**Ensuring data security**: There are so many technologies that contribute
    to multiple channels of communications across applications of mobile, web, and
    ERP, which adds to the complexity of maintaining data security. New age technologies
    such as cloud, big data, and mobile are expanding their reach very quickly and
    becoming popular. However, data security needs and challenges are more complex
    than before.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保数据安全**：有许多技术有助于通过移动、网络和 ERP 等应用程序跨多个渠道进行通信，这增加了维持数据安全的复杂性。云、大数据和移动等新兴技术正迅速扩展其应用范围并变得越来越流行。然而，数据安全的需求和挑战比以往更加复杂。'
- en: 'General guidelines to ensure data quality are the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据质量的一般指南如下：
- en: '**Data availability**: The data intended for consumption to the big data systems
    should be appropriately available for the intended use. It should be timely data
    with either real-time streaming or batch mode. The data should be accessible with
    API interfaces available and also the data should be authorized for use.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可用性**：大数据系统所需的数据应根据预期用途适当提供。数据应是及时的，可以通过实时流式传输或批处理模式获取。数据应通过可用的 API 接口进行访问，并且数据应被授权用于使用。'
- en: '**Data appropriateness**: The data should be credible; otherwise, the purpose
    is defeated. The data lineage process traces the origin of the data. The data
    definitions should be appropriate and acceptable on freshness and regular updates
    for the data. The data documentation and metadata should be audited for correctness
    within an acceptable range of values.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据适当性**：数据应具有可信度，否则目标将无法实现。数据溯源过程追溯数据的来源。数据定义应适当，并且在数据的新鲜度和定期更新方面应是可以接受的。数据文档和元数据应在可接受的值范围内经过审计以确保其正确性。'
- en: '**Data accuracy**: The data should be reliable; the data value should represent
    the true state of the source information and the data should not be ambiguous
    and should be a single version of a fact. Data should be consistent and verifiable
    during the intended time as per the time stamp. The data should be complete and
    auditable.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准确性**：数据应可靠；数据值应真实反映源信息的状态，数据不应含糊不清，应是事实的单一版本。数据应在预定时间内保持一致并可验证。数据应完整并且可审计。'
- en: '**Data integrity**: The data format should be clear and meet the set criteria
    of consistency with the structure and content. Its integrity should be intact.
    The data should be relevant and match the required purpose, and be complete in
    all aspects and fit to use for its intended purpose.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据完整性**：数据格式应清晰，并符合一致性的结构和内容标准。数据的完整性应保持完好。数据应相关并符合所需的用途，且在各方面完整，适合用于其预定目的。'
- en: '**Presentation quality**: The data content and format for the data should be
    clear and understandable. The data description, classification, and coding content
    should be easy to understand and meet the stated objectives and specifications.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**呈现质量**：数据的内容和格式应清晰易懂。数据描述、分类和编码内容应易于理解，并符合既定目标和规范。'
- en: There are many big data open tools providing multiple features for data quality,
    such as Talend, AB Initio, Data Manager, Datamartist, and iManage Data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多开源大数据工具提供多种数据质量特性，例如 Talend、AB Initio、Data Manager、Datamartist 和 iManage
    Data。
- en: In the data ingestion process, raw data is added to the system from multiple
    data sources. The process can be complex, depending on the format and quality
    of the data from the source systems and the state of the data to be processed
    from the desired target state for consumption. There are multiple ways and means
    of ingesting data into big data systems, depending on the type of big data ingested.
    To prepare raw data for the system's use, some level of analysis, sorting, and
    labeling usually takes place during the ingestion process. Extending conventionally
    from the legacy data warehousing processes consists of extracting, transforming,
    and loading, referred to as the ETL process. Some of the same concepts apply to
    data entering the big data system as well.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄取过程中，原始数据来自多个数据源并被加入系统。根据源系统的数据格式和质量，以及目标数据处理状态的需求，这一过程可能会非常复杂。数据摄取到大数据系统的方式多种多样，具体取决于摄取的数据类型。为了准备原始数据供系统使用，通常在摄取过程中需要进行某些程度的分析、排序和标记。从传统的遗留数据仓库过程扩展出来的，包含提取、转换和加载（ETL过程）。一些相同的概念也适用于进入大数据系统的数据。
- en: The process of data ingestion involves massaging the input data for proper formatting,
    categorizing, and labeling. The data structure should adhere to predefined standards
    by eliminating unwanted data. The data thus captured from multiple source systems
    in large volumes is used for further processing. It is stored in a data lake in
    raw format
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取过程涉及对输入数据进行格式化、分类和标记的处理。数据结构应遵循预定义标准，通过剔除不需要的数据来确保符合要求。因此，从多个源系统捕获的大量数据将用于进一步处理。它以原始格式存储在数据湖中。
- en: Batch processing
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理
- en: Batch jobs, as the name indicates, are dataset jobs which are non-time sensitive
    and processed in batches in large datasets. The processing of batch jobs can happen
    in multiple modes, as described next.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理作业，顾名思义，是那些数据集作业，这些作业对时间不敏感，并且在大型数据集中以批量方式处理。批处理作业的处理可以以多种模式进行，下面将描述这些模式。
- en: RDBMS to NoSQL
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从RDBMS到NoSQL
- en: 'Most of the legacy data stored in RDBMS can be imported into NoSQL databases
    on HDFS using Sqoop, DevOps can aid for baseline of the Sqoop scripts, automating
    the process of importing and exporting the data, scalability of the storage and
    computing systems, test automation of the data integrity, and automated deployment..
    The data migration process is described here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在关系型数据库（RDBMS）中的大部分遗留数据可以使用Sqoop导入到HDFS上的NoSQL数据库中，DevOps可以为Sqoop脚本的基准提供支持，自动化数据导入和导出的过程，存储和计算系统的可扩展性，数据完整性的测试自动化，以及自动部署。数据迁移过程如下所述：
- en: Sqoop is a command-line interface application for transferring data between
    relational databases and Hadoop. It supports incremental loads of a single table
    or a free form SQL query as well as saved jobs which can be run multiple times
    to import updates made to a database since the last import. Imports can also be
    used to populate tables in Hive or HBase.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sqoop是一个命令行界面应用程序，用于在关系型数据库和Hadoop之间传输数据。它支持增量加载单个表或自由格式的SQL查询，以及可以多次运行的保存作业，以导入自上次导入以来对数据库所做的更新。导入还可以用于填充Hive或HBase中的表。
- en: Oozie can be used to schedule and create flows for importing/exporting data.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oozie可以用于调度和创建数据导入/导出的流程。
- en: Full as well as incremental imports can be configured in Sqoop/Oozie
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Sqoop/Oozie中可以配置全量和增量导入。
- en: We can directly import and create Hive tables, but if we build a layered architecture
    it is suggested to import to the staging.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以直接导入并创建Hive表，但如果构建分层架构，建议将数据导入到暂存区。
- en: For example--`sqoop import -connect jdbc:mysql://:/ -username -password --table
    --target-dir`.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如--`sqoop import -connect jdbc:mysql://:/ -username -password --table --target-dir`。
- en: '![](img/a9a8a246-8299-4065-84ef-8292381e98a2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9a8a246-8299-4065-84ef-8292381e98a2.png)'
- en: Flume
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flume
- en: For other batch sources, we can deploy Flume. It will watch for new source files
    and push the data to HDFS.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他批处理数据源，我们可以部署Flume。它会监控新的源文件并将数据推送到HDFS。
- en: Flume is effective and reliable for moving log data in large volumes to a distributed
    system, collecting and aggregating it in accordance with business demand
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flume在将大量日志数据传输到分布式系统、收集并根据业务需求汇总数据方面非常有效和可靠。
- en: The architecture is simple and flexible, matching the incoming streaming data
    flow needs
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构简单且灵活，能够满足传入流数据流的需求。
- en: Flume is robust, fault-tolerant, customizable, with advanced features such as
    failover and a recovery mechanism, and so on
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flume具有强大的容错性和可定制性，拥有高级功能，如故障转移和恢复机制等。
- en: A simple extensible data model supports online analytic applications
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单且可扩展的数据模型支持在线分析应用程序
- en: '![](img/0ed49622-6fcc-4849-ae46-48f9851634ce.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ed49622-6fcc-4849-ae46-48f9851634ce.png)'
- en: Stream processing
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理
- en: Stream processing, as the name indicates, is the computing of real-time data
    which is time-sensitive, usually with high-velocity metrics. Analytics are usually
    performed on the streaming data while it is being ingested for quality checks,
    and so on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理，顾名思义，是对实时数据的计算，这些数据通常具有时效性，且通常带有高速度的度量。分析通常在流数据被摄取时进行，以便进行质量检查等。
- en: Real-time
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时
- en: If data is available in streams such as application logs, program output (like
    web scrapper), sensors, geo-location, or social media, this can be collected using
    Kafka on a real-time basis.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据以流的形式存在，例如应用日志、程序输出（如网页抓取器）、传感器、地理位置或社交媒体，这些可以通过Kafka实时收集。
- en: Kafka is an open-source message broker project that aims to provide a unified,
    high-throughput, low-latency platform for handling real-time data feeds. It is,
    in essence, *a massively scalable pub/sub message queue architected as a distributed
    transaction log, making it highly valuable for enterprise infrastructures to process
    streaming data*.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka是一个开源消息代理项目，旨在提供一个统一的、高吞吐量、低延迟的平台，用于处理实时数据流。它本质上是*一个大规模可扩展的发布/订阅消息队列，构建为分布式事务日志，使其在企业基础设施中处理流数据时具有极高的价值*。
- en: Kafka can be easily scaled to support more data sources and growing data volumes.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka可以轻松扩展，以支持更多的数据源和不断增长的数据量。
- en: Kafka also supports direct connectivity to Spark.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka还支持与Spark的直接连接。
- en: There is one topic per incoming data source and one topic per consumer group.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输入数据源对应一个主题，每个消费者组对应一个主题。
- en: The number of partitions per topic will depend on the data size.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主题的分区数量将取决于数据的大小。
- en: '![](img/b08b30a6-0cd8-4657-89e0-09effbfb6389.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b08b30a6-0cd8-4657-89e0-09effbfb6389.png)'
- en: Apart from Sqoop and Kafka, Some other specialized data ingestion tools for
    importing and aggregating both server and application logs are Apache Flume and
    Apache Chukwa. **Gobblin** also offers a data ingestion framework to aggregate
    and normalize date.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Sqoop和Kafka，还有一些其他专门的数据摄取工具，用于导入和聚合服务器及应用日志，这些工具包括Apache Flume和Apache Chukwa。**Gobblin**也提供了一个数据摄取框架，用于聚合和规范化数据。
- en: Lambda architecture
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda架构
- en: 'The lambda architecture is effective when both batch and streaming data are
    ingested to systems at the same time, as shown in the following diagram:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当批处理和流数据同时导入系统时，Lambda架构非常有效，如下图所示：
- en: '![](img/ddfe9eef-df2d-4862-b1c2-479da1a168fe.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ddfe9eef-df2d-4862-b1c2-479da1a168fe.png)'
- en: The data storage layer
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据存储层
- en: 'Persisting big data has multiple storage options, such as Data Lakes and data
    warehouse; cloud technologies for data storage greatly augment the needs of big
    data storage systems, scalable to terabytes and petabytes through simple storage
    devices and virtual machines. DevOps is an effective solution for managing scalable
    data storage with infrastructure as code discussed in detail in this book:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化大数据有多种存储选项，例如数据湖和数据仓库；云技术对数据存储的支持大大增强了大数据存储系统的需求，这些系统能够通过简单的存储设备和虚拟机扩展至TB和PB级别。本书详细讨论了DevOps作为管理可扩展数据存储的有效解决方案，使用基础设施即代码。
- en: '**Data Lake**: Data Lake is synonymous with a water lake where water is stored
    and consumed by many people. A Data Lake is the repository for the collection
    of raw data. The data collected could be unstructured and frequently changing,
    so, initially the data is pooled into the large repository to be consumed by users
    as per their scheduled needs.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据湖**：数据湖与水湖同义，水湖储存水并供许多人使用。数据湖是原始数据的存储库。收集到的数据可能是非结构化的，并且经常变化，因此，最初这些数据会被集中存储在大存储库中，以便用户根据其预定需求进行使用。'
- en: '**Data warehouse**: A data warehouse is an ordered repository for large volumes
    of data to be used for analysis and reporting. The data in a data warehouse is
    typically being cleaned, is well-ordered, and is integrated with other sources.
    It is generally more prominent with conventional systems.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据仓库**：数据仓库是一个有序的数据存储库，用于存储大量数据以供分析和报告使用。数据仓库中的数据通常是经过清洗的、组织良好的，并且与其他数据源进行了集成。它在传统系统中通常更为显著。'
- en: The ingestion process ensures the incoming data is processed as per business
    needs to be persisted reliably in the storage disk. The ingestion process can
    be complex, depending on the volume and variety of the data from the source systems.
    The availability of the distributed storage system is accomplished by Apache Hadoop's
    HDFS filesystem. With HDFS, large quantities of raw data are written to multiple
    nodes simultaneously with redundancy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 摄取过程确保进入的数据根据业务需求被处理，并可靠地持久化到存储磁盘。摄取过程可能很复杂，取决于来自源系统的数据的量和多样性。分布式存储系统的可用性通过
    Apache Hadoop 的 HDFS 文件系统实现。通过 HDFS，大量原始数据可以同时写入多个节点，并具备冗余性。
- en: Ceph and GlusterFS are other filesystems offering all these capabilities.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph 和 GlusterFS 是提供所有这些功能的其他文件系统。
- en: Distributed databases such as NoSQL are well placed to import data for more
    structured access. They have features such as fault tolerance, and they have the
    capability to ingest heterogeneous data formats. Based on the organization's business
    needs, appropriate databases can be selected from a variety of available choices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据库，如 NoSQL，适合导入数据以便进行更结构化的访问。它们具备容错功能，并能够摄取异构数据格式。根据组织的业务需求，可以从多种可用选择中选择适当的数据库。
- en: Data storage - best practices for better organization and effectiveness
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据存储 - 为了更好的组织和效率的最佳实践
- en: '![](img/fdae2dc1-b9d9-4e7d-bcea-62398e3a1785.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdae2dc1-b9d9-4e7d-bcea-62398e3a1785.png)'
- en: Landing
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 着陆
- en: A landing zone is where in the initial data lands from different source systems
    in the as it is state to the storage system.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 着陆区是初始数据从不同源系统以原始状态进入存储系统的地方。
- en: The landing zone is where incoming data is stored
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 着陆区是存储进入数据的地方
- en: All input validation should be done here
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的输入验证应该在此进行
- en: The folder structure can be `<source>/<type of data>/<yyyymmddhhisss>`
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件夹结构可以是`<source>/<type of data>/<yyyymmddhhisss>`
- en: The archive mechanism should be applied as well (day/week/month)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该同时应用归档机制（按天/周/月）
- en: Access should be restricted to only processing users and not end users
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问应仅限于处理用户，而非最终用户
- en: Raw
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始
- en: Once the data lands in the landing zone, it undergoes sanity checks as to its
    appropriateness, format, and quality; upon satisfactory compliance, it is stored
    as raw data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据进入着陆区，就会进行适当性、格式和质量的完整性检查；在符合要求的情况下，它会以原始数据的形式存储。
- en: This is where the raw data is stored in its original format
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是存储原始数据的地方，数据保持其原始格式
- en: Validated input from the landing layer is stored here
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自着陆层的经过验证的输入数据存储在这里
- en: The directory structure is managed by the ingestion framework
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目录结构由摄取框架管理
- en: Only selected super-users and system users will have access to this data
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有选定的超级用户和系统用户能够访问这些数据
- en: Snappy/LZO compression should be applied
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应应用 Snappy/LZO 压缩
- en: A data classifier should be applied (hot, cold) and set to the archive policy
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该应用数据分类器（热数据、冷数据），并设置归档策略
- en: The folder structure can be `<base dir>/<system type>/<dataset Source Name>/<Source
    Type>`
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件夹结构可以是`<base dir>/<system type>/<dataset Source Name>/<Source Type>`
- en: Work
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作
- en: The work area stores identified clean data to be used for business purposes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 工作区存储已识别的清洁数据，用于业务目的
- en: This is the temporary working area and cleanup up should take place after related
    jobs unless the jobs require data for debugging
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是临时工作区，清理应该在相关任务完成后进行，除非任务需要数据用于调试
- en: The checkpoint location for Spark streaming will be located here as well
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 流式处理的检查点位置也会位于这里
- en: Gold
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黄金
- en: Gold data is the valuable data that is the transformed, partitioned, and classified
    as master data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 黄金数据是经过转换、分区，并分类为主数据的有价值数据。
- en: This is the location that stores transformed data
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是存储转换后数据的位置
- en: Partitioning is important in this layer and should be done based on the most
    frequently accessed columns
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一层，分区是非常重要的，应该根据最常访问的列来进行分区
- en: Classification should be done for very hot, hot, cold, and very cold data
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应对非常热、热、冷和非常冷的数据进行分类
- en: Very cold data should be archived to blob storage (or any other cheap storage)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常冷的数据应该归档到 Blob 存储（或任何其他便宜的存储）
- en: The folder structure could be `<base dir>/<system type>/<dataset Source Name>/<Source
    Type>/<Job ID>`
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件夹结构可以是`<base dir>/<system type>/<dataset Source Name>/<Source Type>/<Job ID>`
- en: Quarantine
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隔离
- en: This is the place where unwanted and stale data, which is of no active immediate
    use, is saved.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是保存不需要的、过时的数据的地方，这些数据当前没有活跃的即时用途。
- en: All rejected files from the various steps will be stored here, such as ingestion,
    transformation, as well as validation exception records
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自不同步骤的所有被拒绝的文件将存储在这里，如摄取、转换以及验证异常记录
- en: Classification should be done for very hot, hot, cold, and very cold data
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应对非常热、热、冷和非常冷的数据进行分类
- en: Very cold data should be archived to blob storage (or any other cheap storage)
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常冷的数据应该归档到 blob 存储（或任何其他廉价存储）
- en: Business
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务
- en: Business data is the master data for a client's particular details, such as
    address, product preference, and so on.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 业务数据是客户特定详情的主数据，如地址、产品偏好等。
- en: This layer will have application/client-specific data
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一层将包含应用程序/客户端特定的数据
- en: All data must be store in parquet format since all data at this stage will be
    structured
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数据必须以 parquet 格式存储，因为此阶段的所有数据将是结构化的
- en: Outgoing
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出
- en: Outgoing data is what is to be shared with external entities, such as suppliers,
    vendors, or third-party APIs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数据是将要与外部实体共享的数据，例如供应商、卖方或第三方 API。
- en: This is the location from where data can be shared with the external application
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是可以与外部应用程序共享数据的位置
- en: Files need to be archived once they are copied
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件在复制后需要被归档
- en: Clients can have temporary or permanent access
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端可以拥有临时或永久访问权限
- en: A good easy-to-use backup and disaster recovery solution provides integrated
    data sync between Hadoop clusters. It enables data protection by replicating data
    stored in HDFS platforms and across data centers.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的易用备份和灾难恢复解决方案提供了 Hadoop 集群之间的集成数据同步。它通过复制存储在 HDFS 平台中的数据并跨数据中心进行同步，从而实现数据保护。
- en: Computing and analyzing data
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算和分析数据
- en: The data made available from the preceding processes can be analyzed to unearth
    the actual information value. The computational layer performs diverse functions
    where data is often processed iteratively with a combination of tools. The different
    types of insight needed for business needs are extracted by tailored practices
    that vary from organization to organization.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的过程生成的数据可以被分析，以发掘实际的信息价值。计算层执行各种功能，其中数据通常通过组合工具进行迭代处理。为了满足不同的业务需求，提取的各种洞察力通过量身定制的做法得以实现，而这些做法因组织而异。
- en: 'Batch processing is one method of computing over a large dataset where data
    is ingested in batch mode either daily or hourly. Apache Hadoop''s MapReduce is
    the most prominent and powerful batch processing engine and it is known as a distributed
    Map Reduce algorithm; it adopts the following strategy and is most useful when
    dealing with very large datasets that require quite a bit of computation:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理是对大数据集进行计算的一种方法，其中数据以批处理模式进行摄取，可以是每日或每小时。Apache Hadoop 的 MapReduce 是最突出和强大的批处理引擎，它被称为分布式
    MapReduce 算法；它采用以下策略，在处理需要大量计算的非常大数据集时最为有效：
- en: '**Splitting**: In this process, the work is divided into smaller pieces'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拆分**：在这个过程中，工作被划分为更小的部分'
- en: '**Mapping**: This is the process of scheduling each piece on an individual
    machine'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**映射**：这是将每个数据块调度到单独机器上的过程'
- en: '**Shuffling**: Reshuffling data based on the intermediate results'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洗牌**：基于中间结果重新洗牌数据'
- en: '**Reducing**: Processing each group of output data'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少**：处理每组输出数据'
- en: '**Assembling**: The final result is assembled together'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组装**：最终结果被组合在一起'
- en: 'The MapReduce framework forms the compute node while the HDFS filesystem forms
    the data node. Typically, in the Hadoop ecosystem architecture, both the data
    node and compute node perform similar roles. The delegation tasks of the MapReduce
    component are performed by two daemons, the **Job Tracker** and **Task Tracker**,
    Their activities are shown in the following diagram:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 框架形成了计算节点，而 HDFS 文件系统则形成了数据节点。通常，在 Hadoop 生态系统架构中，数据节点和计算节点执行相似的角色。MapReduce
    组件的委托任务由两个守护进程执行，**Job Tracker** 和 **Task Tracker**，它们的活动如下图所示：
- en: '![](img/3a3405a6-ad22-4820-b3f5-fba4b8975a6b.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a3405a6-ad22-4820-b3f5-fba4b8975a6b.png)'
- en: Data processing in batch, real-time, and stream processing is discussed next.
    DevOps is integral to big data systems for high volume data processing from source
    system discovery to the scalability of infrastructure to support storage needs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来讨论的是批处理、实时处理和流处理中的数据处理。DevOps 是大数据系统中不可或缺的一部分，涉及从源系统发现到基础设施的可扩展性，以支持存储需求的高容量数据处理。
- en: '**Batch processing**: It is very efficient in processing high volume data.The
    data is ingested to the system, processed, and then results are produced in batches.
    The computational power of the system is designed based on the size of the data
    being processed. The systems are configured to run automatically without manual
    intervention. The system can scale very quickly to accommodate the entire dataset
    for computational analyses of the huge volume of data files. Based on the volume
    of data processed and the computational power of the system defined, the output
    timelines can vary significantly.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理**：在处理大量数据时非常高效。数据被导入系统、处理，然后以批次形式生成结果。系统的计算能力是根据处理的数据大小设计的。系统配置为无需人工干预即可自动运行。该系统可以快速扩展，以适应大量数据文件的计算分析。根据处理的数据量和定义的系统计算能力，输出时间线可能会有显著变化。'
- en: '**Real-time/stream processing**: Though batch processing is a good choice for
    certain types of data and computation, other workloads require a more low-latency
    turnaround. There are real-time systems which are required to respond in real-time
    as they process information and make the analytics or visualizations readily available
    to the business while assimilating the new information continuously. These systems
    are called stream processing, which operates on a continuous stream of data composed
    of individual items. These systems, real-time or stream processing systems, utilize
    the real-time processing capability of in-memory engines; computational analytics
    are performed in the cluster''s memory to avoid having to write back to disk as
    in traditional disk-based persistent systems. Real-time processing is best suited
    for analyzing smaller chunks of data that are changing or being added to the system
    rapidly.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时/流处理**：虽然批处理适用于某些类型的数据和计算，但其他工作负载则需要更低延迟的响应。有一些实时系统需要在处理信息时做出实时响应，并在持续吸收新信息的同时，使分析或可视化结果迅速呈现给业务方。这些系统称为流处理系统，它们基于连续的数据流进行工作，这些数据流由独立的数据项组成。实时处理系统或流处理系统，利用内存引擎的实时处理能力；计算分析是在集群的内存中执行的，从而避免像传统的基于磁盘的持久系统那样必须写回磁盘。实时处理最适用于分析较小的数据块，这些数据正在快速变化或被添加到系统中。'
- en: 'There are many platforms and tools to achieve real-time processing, such as
    Apache Storm, Apache Flink, and Apache Spark. Each of them is designed as different
    ways of achieving real-time or near real-time processing. Apart from these listed
    computational frameworks, there are many other means of analyzing data or performing
    computations within a big data ecosystem. These tools frequently plug into the
    aforementioned frameworks and provide additional interfaces for interacting with
    the underlying layers. We have already discussed in the previous chapter their
    applicability to different scenarios and the best application for any individual
    problem, but we will recap a few of the tools here again:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多平台和工具可以实现实时处理，如 Apache Storm、Apache Flink 和 Apache Spark。每个平台都设计为实现实时或接近实时处理的不同方式。除了这些列出的计算框架外，还有许多其他手段可以在大数据生态系统中分析数据或执行计算。这些工具常常与前述框架集成，并提供额外的接口与底层层次进行交互。我们在前一章已经讨论了它们在不同场景中的适用性以及每个问题的最佳应用，但我们将在此再次回顾一些工具：
- en: Apache Hive provides a data warehouse interface for Hadoop
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hive 提供了 Hadoop 的数据仓库接口
- en: Apache Pig provides a high-level querying interface
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Pig 提供了一个高级查询接口
- en: Apache Drill, Apache Impala, Apache Spark SQL, Presto, and so on, provide SQL-like
    interactions with data
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Drill、Apache Impala、Apache Spark SQL、Presto 等，提供类似 SQL 的数据交互
- en: R and Python are popular choices for simple analytics programming
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 和 Python 是常见的简单分析编程语言选择
- en: Apache SystemML, Apache Mahout, and Apache Spark's MLlib, provide building prediction
    models for machine learning libraries
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache SystemML、Apache Mahout 和 Apache Spark 的 MLlib 提供机器学习库的预测模型构建功能
- en: Apache Spark analytic platform
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 分析平台
- en: Apache Spark is a next-generation in-memory open-source platform, combining
    batch, streaming, and interactive analytics under one umbrella. Spark facilitates
    ease of use, providing the ability to quickly write applications with built-in
    operators and APIs along with faster performance and implementation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个下一代内存中开源平台，集成了批处理、流处理和交互式分析。Spark 提供了易于使用的功能，能够快速编写应用程序，并具有内置的操作符和
    API，此外还提供更快的性能和实现。
- en: Spark provides a faster and more general data processing platform and runs programs
    up to 100x faster in memory, or 10x faster on disk, than Hadoop, with lightning-fast
    cluster computing. The Spark framework is built on top of Hadoop clusters to process
    data from structured system such as Hive and stream data from Flume and Kafka.
    It has many advanced features and supports a variety of languages, including Java,
    Python, and Scala. It has extensive features for analytics, out-of-the-box algorithms,
    machine learning, interactive queries, and complex function analytics.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一个更快、更通用的数据处理平台，内存中运行的程序比Hadoop快最多100倍，磁盘上运行的程序比Hadoop快最多10倍，具备极速的集群计算能力。Spark框架建立在Hadoop集群之上，用于处理来自结构化系统（如Hive）的数据，并从Flume和Kafka流式传输数据。它具有许多先进功能，并支持多种编程语言，包括Java、Python和Scala。它拥有广泛的分析功能、开箱即用的算法、机器学习、交互式查询和复杂功能分析。
- en: 'Spark''s popularity is due to many advantages, including:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的流行度源于其许多优点，包括：
- en: Spark is a subset of the Hadoop ecosystem. It integrates well with the reliable
    and secure storage platform, HDFS of the ecosystem. It is compatible with other
    data sources, such as Amazon S3, Hive, HBase, and Cassandra. It can run on clusters
    managed by Hadoop YARN or Apache Mesos, and can also run as a standalone.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark是Hadoop生态系统的一个子集。它与生态系统中的可靠且安全的存储平台HDFS良好集成，且与其他数据源兼容，如Amazon S3、Hive、HBase和Cassandra。它可以在Hadoop
    YARN或Apache Mesos管理的集群上运行，也可以作为独立模式运行。
- en: Spark is a fast technology to enable large-scale data processing. This framework
    provides Java-, Scala-, and Python-based high-level APIs with a rich set of data
    stores for stream processing and machine learning. Though primary APIs are for
    Scala, Java, and Python, languages such as R are also supported.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark是一项用于大规模数据处理的快速技术。该框架提供基于Java、Scala和Python的高级API，并为流处理和机器学习提供丰富的数据存储。虽然主要API是为Scala、Java和Python设计的，但也支持R等语言。
- en: Spark ensures parallel processing for data, very well integrated with Hadoop/HDFS
    for data storage, and to support variety of filesystems and databases.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark确保数据的并行处理，并与Hadoop/HDFS进行良好的集成以进行数据存储，同时支持多种文件系统和数据库。
- en: Spark's machine learning capabilities are proved to be excellent solution for
    stream processing. With Spark REPL writing code is easier and quicker with inbuilt
    high-level (80) operators . Spark's **Read Evaluate Print Loop** (**REPL**) is
    interactive (out-of-the box) shell is a modified version of the interactive scala
    REPL. With REPL, no need to compile and execute the code. User expressions are
    evaluated and REPL will display the results of the expression. The *Read* takes
    expression as an input and parses and stores in memory as an internal data structure.
    *Eval* traverses the data structure, evaluates the called functions. *Print* displays
    the results with print ability. *Loop* iterates going back to read state to terminate
    the loop on exit. REPL expedites the turnaround time also supports ad hoc data
    query analysis.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的机器学习能力被证明是流处理的优秀解决方案。通过Spark REPL，编写代码更容易、更快捷，内置了高级（80个）操作符。Spark的**读取评估打印循环**（**REPL**）是交互式的（开箱即用）Shell，是Scala交互式REPL的修改版。使用REPL时，无需编译和执行代码。用户的表达式会被评估，并且REPL将显示该表达式的结果。*读取*将表达式作为输入，解析并以内部数据结构存储在内存中。*评估*遍历数据结构，评估调用的函数。*打印*显示结果，并具有打印功能。*循环*迭代返回读取状态，在退出时终止循环。REPL加快了周转时间，并支持临时数据查询分析。
- en: Spark is more nimble and well suited for big data analytics. Continuous micro-batch
    processing with integrated advanced analytics is based on its own streaming API,
    which is developer-friendly.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark更加灵活，特别适合大数据分析。其基于自身流处理API的持续微批处理和集成的先进分析功能对开发者友好。
- en: Spark is more efficient compared to MapReduce. It is 100 times faster than MapReduce
    for the same process.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与MapReduce相比，Spark的效率更高。对于相同的处理过程，它比MapReduce快100倍。
- en: 'The popular batch job processing engine for Hadoop, MapReduce poses a significant
    challenge due to its high-latency to the batch-mode response and it is difficult
    to maintain due to inherent inefficiencies associated with its architecture design
    and code. Spark''s main component is the Spark Core Engine. It is complemented
    by a set of powerful, higher-level libraries that can be seamlessly used in the
    same application:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop的流行批处理作业引擎MapReduce，由于其批处理模式响应的高延迟，面临着巨大的挑战，且由于其架构设计和代码的固有低效性，维护起来非常困难。Spark的主要组件是Spark
    Core Engine，它还配有一套强大的、更高级的库，这些库可以无缝地在同一应用程序中使用：
- en: Spark SQL is a powerful query language with inbuilt DataFrames
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL 是一个强大的查询语言，内置 DataFrames。
- en: Spark Streaming engine is for data streaming
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming 引擎用于数据流处理
- en: Spark MLlib for machine learning along with machine learning pipelines models
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark MLlib 用于机器学习以及机器学习管道模型
- en: GraphX with GraphFrames stores relationship between entities as a graphical
    representation.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX 与 GraphFrames 将实体之间的关系存储为图形表示。
- en: '![](img/a6de56f0-97e8-40bf-adde-a96b9aa8c862.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6de56f0-97e8-40bf-adde-a96b9aa8c862.png)'
- en: Spark Core Engine
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 核心引擎
- en: 'The base engine for Spark Core to perform large-scale parallel and distributed
    data processing is the Spark Core Engine. It performs the following functions:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 核心引擎是 Spark 进行大规模并行和分布式数据处理的基础引擎。它执行以下功能：
- en: Fault-tolerant and recovery-based memory management
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错和基于恢复的内存管理
- en: Cluster job scheduling, distributing, and monitoring
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群作业调度、分发和监控
- en: Storage device systems interfacing
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储设备系统接口
- en: Spark is built and based on an immutable, fault-tolerant, distributed collection
    of objects called **Resilient Distributed Dataset** (**RDD**) that can be operated
    on in parallel. Objects are created through loading an external dataset or distributing
    from the internal driver program. The operations performed on objects through
    RDD are transformations.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是基于不可变的、容错的、分布式的对象集合构建的，称为**弹性分布式数据集**（**RDD**），可以并行操作。对象通过加载外部数据集或从内部驱动程序分发来创建。通过
    RDD 执行的操作是转换。
- en: Transformation operations include map, filter, join, and union, and are performed
    on RDD and yield a new result.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 转换操作包括 map、filter、join 和 union，执行在 RDD 上并产生新的结果。
- en: Action operations include reduce, count, first and they return a value after
    running a computation on RDD.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 动作操作包括 reduce、count、first，它们在对 RDD 进行计算后返回一个值。
- en: The Spark Engine is designed efficiently. The transformations are actually computed
    when an action is called and the result is returned to the driver program. Transformations
    are lazy and do not compute their results right away; however, they remember the
    task to be performed and the dataset (for example, a file) to perform the operation.
    The transformed RDD can be recomputed for the next transformation. The advantage
    is avoiding unnecessary changes and computations in the process. All this is achieved
    by the in-memory engine where the data is persisted and cached for the RDD objects.
    Spark will keep the elements around on the cluster-cached memory for much faster
    access the next time you query it.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 引擎设计高效。转换操作实际上是在调用动作时计算，并将结果返回给驱动程序。转换是惰性操作，不会立即计算结果；然而，它们会记住要执行的任务和数据集（例如文件），以执行操作。转换后的
    RDD 可以为下一个转换重新计算。其优点是避免了不必要的变更和计算。所有这些都通过内存引擎实现，数据会在 RDD 对象上持久化并缓存。Spark 会将元素保存在集群缓存内存中，以便下次查询时能够更快地访问。
- en: Spark SQL
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: The Spark component, Spark SQL, supports querying data either through SQL or
    through the Hive query language. Spark SQL is integrated with the Spark stack
    providing support for various data sources. It allows you to weave SQL queries
    with code transformations, making it a very powerful tool.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 组件 Spark SQL 支持通过 SQL 或 Hive 查询语言查询数据。Spark SQL 与 Spark 堆栈集成，提供对各种数据源的支持。它允许你将
    SQL 查询与代码转换结合，使其成为一个非常强大的工具。
- en: Spark Streaming
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Spark Streaming is a powerful functionality built on in-memory technology. The
    Spark Streaming API is compatible with the Spark Core Engine. It facilitates both
    batch and streaming data to process real-time streaming data from systems such
    as web server log files, Twitter-based social media data, and so on. Spark interfaces
    with other tools, such as Kafka, for various messaging queues.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是一个强大的基于内存技术的功能。Spark Streaming API 与 Spark 核心引擎兼容。它支持批处理和流式数据处理，能够实时处理来自
    Web 服务器日志文件、基于 Twitter 的社交媒体数据等系统的数据。Spark 还与其他工具（如 Kafka）接口，用于各种消息队列。
- en: Spark Streaming receives the input data from upstream systems such as Apache
    Flume, Kafka, and so on, and divides the data into batches to process them with
    the Spark Engine and generate a final stream of results in batches to store them
    on HDFS/S3, and so on.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 从上游系统接收输入数据，如 Apache Flume、Kafka 等，并将数据划分为批次，通过 Spark 引擎处理这些批次数据，并生成最终的结果流批次，将其存储在
    HDFS/S3 等位置。
- en: '![](img/f28d00a5-e3c5-4e7b-b5b2-a3b0c1d8b977.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f28d00a5-e3c5-4e7b-b5b2-a3b0c1d8b977.png)'
- en: MLlib is a set of library functions that provides various algorithms of machine
    learning, such as classification, regression, clustering, and collaborative filtering.
    Apache Mahout (a machine learning library for Hadoop) is integrated into Spark
    MLlib. A few algorithms such as linear regression or k-means clustering also work
    with streaming data designed to scale out on a cluster.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 是一组库函数，提供各种机器学习算法，如分类、回归、聚类和协同过滤。Apache Mahout（一个用于 Hadoop 的机器学习库）已经集成到
    Spark MLlib 中。一些算法，如线性回归或 K-means 聚类，也支持流数据，并设计为在集群上扩展。
- en: GraphX provides ETL functionality, exploratory analysis, and iterative graph
    computations. It provides a library for manipulating graphs and performing graph-parallel
    operations on common graph algorithms such as **page rank**.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX 提供了 ETL 功能、探索性分析和迭代图计算。它提供了一个用于操作图形和执行图并行操作的库，支持常见图算法，如**PageRank**。
- en: Spark is ideal to simplify challenging and compute-intensive task for real-time
    data processing of high volumes of streaming or archived data, both structured
    and unstructured, seamlessly integrating relevant complex capabilities, such as
    machine learning and graph algorithms.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 非常适合简化复杂且计算密集型的任务，用于高容量实时数据处理，无论是流数据还是归档数据，结构化和非结构化数据都能轻松处理，同时无缝集成相关复杂功能，如机器学习和图算法。
- en: Some challenges include operational complexity and the high skills required
    to develop and manage applications. Spark performs well with Hadoop to take advantage
    of Hadoop's HDFS. Performance tuning of both systems is imperative; otherwise,
    Spark's nuances can lead to out-of-memory error issues and memory lag, if jobs
    are not tuned well.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一些挑战包括操作复杂性和开发管理应用所需的高技能水平。Spark 与 Hadoop 配合良好，能充分利用 Hadoop 的 HDFS。两个系统的性能调优至关重要，否则，Spark
    的细微差别可能会导致内存溢出错误和内存滞后问题，特别是在作业未经过良好调优时。
- en: Visualization with big data systems
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据系统中的可视化
- en: 'We are well versed with the saying: *garbage in, garbage out*. Identifying
    and recognizing trends, variations, and changes in data over time is often more
    important and data visualization is an inevitable step. Due to the complexity
    of information being processed in big data systems, visualizing data is one of
    the most important and useful ways to spot trends and create meaningful insights
    from a large number of data points.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都熟知“**垃圾进，垃圾出**”这句话。识别和了解数据随着时间的变化、趋势、波动和变化通常比单纯的数据分析更为重要，而数据可视化则是不可避免的步骤。由于大数据系统处理的信息复杂性，数据可视化成为发现趋势并从大量数据点中提取有意义洞察的最重要和最有用的方法之一。
- en: 'We will discuss some real-time processing tools here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里讨论一些实时处理工具：
- en: '**Prometheus**: To visualize application and server metrics in real-time processing,
    data streams as a time-series database and visualizes that information. The health
    of the systems is gauged by the frequent data changes and large variations in
    the metrics typically indicate significant KPIs.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus**：用于实时处理应用和服务器指标，数据流作为时序数据库，进行可视化。系统的健康状况通过频繁的数据变化来衡量，指标的大幅波动通常表示重要的关键绩效指标（KPI）。'
- en: '**Elastic Stack**: It is popular for visualizing big data systems to visually
    interface with the results of calculations or raw metrics. It is also known as
    the ELK stack, composed of Logstash for data collection, Elasticsearch for indexing
    data, and Kibana for visualization.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elastic Stack**：它在大数据系统可视化中非常流行，用于以视觉方式展示计算结果或原始指标。它也被称为 ELK Stack，由 Logstash（用于数据收集）、Elasticsearch（用于数据索引）和
    Kibana（用于可视化）组成。'
- en: '**SILK**: It is a similar stack achieved by using Apache Solr for indexing
    and a Kibana fork called **Banana for visualization**.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SILK**：它是一个类似的堆栈，通过使用 Apache Solr 进行索引和一个名为 **Banana** 的 Kibana 分支来实现可视化。'
- en: '**Jupyter Notebook** and **Apache Zeppelin** offer visualization interfaces
    for interactive exploration and visualization of data in a format conducive to
    sharing, presenting, or collaborating. This technology is typically used for interactive
    data science work and is termed a data *notebook*.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook** 和 **Apache Zeppelin** 提供了交互式数据探索和可视化的界面，数据呈现格式便于共享、展示或协作。这项技术通常用于交互式数据科学工作，并被称为数据
    *笔记本*。'
- en: Data governance
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据治理
- en: Data governance is the process of classifying enterprise data and providing
    the right access and privileges for appropriate roles and personnel. It refers
    to the overall process of managing the availability, usability, integrity, and
    security of the data assets in an organization. A matured data governance model
    is a defined set of strategies, and includes a governing group and a well-orchestrated
    plan to execute those procedures.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理是对企业数据进行分类，并为适当的角色和人员提供正确的访问权限和特权的过程。它指的是管理组织中数据资产的可用性、可用性、完整性和安全性的整体过程。成熟的数据治理模型是一套明确定义的策略，包含一个治理小组以及一个协调有序的计划来执行这些程序。
- en: 'Adopting an open source software development approach for platform/product
    development will ensure many advantages, such as the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 采用开源软件开发方法来进行平台/产品开发将带来许多优势，例如：
- en: Seamless collaboration across different diverse technology teams spread across
    the organization
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组织内部跨不同技术团队的无缝协作
- en: Leverage and re-use of existing software and knowledge assets across the organization
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组织内 leverage 和重用现有的软件和知识资产
- en: It ensures region, client, and country-specific needs are addressed
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它确保解决地区、客户和国家特定需求
- en: This approach for software development helps a governance mechanism in place
    to ensuring avoid duplication of work and enable transparency following a common
    standard framework.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这种软件开发方法有助于建立治理机制，以确保避免重复工作并按照统一的标准框架实现透明化。
- en: 'Data governance should define the minimum necessary rules instead of being
    an overhead. It should also balance across:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理应定义必要的最小规则，而不是成为负担。它还应在以下方面进行平衡：
- en: Rules versus public (free for all)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则与公众（人人可用）
- en: People versus process
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人员与流程
- en: Empowerment versus directing
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 授权与指引
- en: 'Open source governance operates on three pillars:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 开源治理基于三个支柱：
- en: Transparency
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 透明度
- en: Set governance parameters
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置治理参数
- en: Faster delivery of every team and every member
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快地交付每个团队和每个成员的成果
- en: 'Open source communities that practice transparency, encourage active participation,
    and recognize the contributions of all constituents are more likely to thrive,
    iterate, and strengthen their prospects. The guiding principles of governance
    in the open source community development model can be better demonstrated by describing
    the seven pillars illustrated in the following diagram:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 实践透明度、鼓励积极参与并认可所有成员贡献的开源社区，更有可能繁荣、迭代并增强前景。开源社区开发模型中的治理原则可以通过描述以下图示中的七个支柱来更好地展示：
- en: '![](img/530f5272-4684-40bf-8606-8a2ca5c1d53f.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/530f5272-4684-40bf-8606-8a2ca5c1d53f.png)'
- en: People and collaboration in accordance with DevOps core concept
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据DevOps核心概念的人员与协作
- en: 'Technology is driven by people; therefore, the success of technology depends
    on a few attributes--it should be quite easily adoptable for people, it should
    be flexible, easy to learn, and convenient to collaborate with. We will discuss
    them here:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 技术由人驱动，因此技术的成功依赖于几个属性——它应该易于被人们采纳，应该具有灵活性，易于学习，并且便于协作。我们将在这里讨论这些：
- en: Establishing ownership for the process example code, submission, and review
    process
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确立过程示例代码、提交和审查过程的所有权
- en: A common dashboard to check the status of any component, changes made, review
    status, testing, impacted components, and so on
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通用的仪表盘，用于查看任何组件的状态、已做更改、审查状态、测试、受影响的组件等
- en: Adopt forums for discussions for resolution of queries than e-mail, Yammer Groups,
    and so on
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用论坛讨论而非电子邮件、Yammer 小组等，来解决问题和解答疑问
- en: Regular (weekly, bi-weekly) all-hands deployment to discuss changes in components
    and roadmaps
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期（每周、每两周）全员部署会议，讨论组件变化和路线图
- en: Environment management
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境管理
- en: Management of environments in information technology is a complex and critical
    function.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 信息技术中环境管理是一个复杂且至关重要的职能。
- en: Parallel environments for development to be maintained, for example, current
    programs and other production fixes
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护开发的并行环境，例如当前程序和其他生产修复
- en: Define access restriction for components to interact with the database
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义组件与数据库交互的访问限制
- en: Define allocation of resources (disk space, threads/mappers, and so on) for
    each component/program
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义每个组件/程序的资源分配（磁盘空间、线程/映射器等）
- en: Documentation
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档
- en: Documentation of the ongoing work is important for the shell life of the project,
    features upgrades, and also to support and maintain as an operations manual.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对正在进行的工作进行文档记录，对于项目的生命周期、功能升级以及作为操作手册的支持和维护都非常重要
- en: Comprehensive documentation of the core components, business/assembly line usage,
    governance, and so on. It is important for collaboration across teams, new members,
    and so on.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心组件、业务/装配线使用、治理等的全面文档。这对于跨团队合作、新成员的加入等非常重要
- en: 'The artifacts documents to help each team member could be revised regularly
    to add more granular details:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于帮助每个团队成员的文档可以定期修订，添加更细化的细节：
- en: Master architecture reference document--created in TOGAF suggested format to
    speak in a common language
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主架构参考文档——按照TOGAF建议的格式创建，以便使用统一的语言交流
- en: Developer guides
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者指南
- en: Governance guide
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 治理指南
- en: Deployment guide
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署指南
- en: Architecture board
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构委员会
- en: The architecture board has the responsibility for the long-term enterprise and
    for ensuring the architecture meets the business goals such as service-oriented
    architecture, usage of components meeting the security guidelines, open source
    tools usage percentage as a roadmap, and so on.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 架构委员会负责企业的长期规划，并确保架构符合业务目标，例如面向服务的架构、使用符合安全指南的组件、开放源代码工具使用比例作为路线图等。
- en: The **Architecture Review Board** (**ARB**) is the authority for defining and
    participating in Tollgate (or milestone) planning
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构评审委员会** (**ARB**) 是定义和参与门控（或里程碑）规划的权威机构'
- en: Define the Tollgate, the milestone for every program's high-level design
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义每个程序高层设计的门控里程碑
- en: Budget the re-factoring effort in every program and approve this in Tollgate
    meetings
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预算每个程序的重构工作量，并在门控会议中批准
- en: Centralize decision making for design approvals
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中化设计审批决策
- en: Review checklists used for Tollgate and reviews
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于门控和评审的审查清单
- en: Development and build best practices
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发和构建最佳实践
- en: Development best practices are the adoption of coding standards, adequate documentation,
    peer reviews, quality of the code, and so on, to ensure high quality of the code
    and performance. Build is a complex task with many interfaces. Adherence to proper
    guidelines will make it robust and well functioning as per the organization's
    needs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 开发最佳实践包括采用编码标准、适当的文档、同行评审、代码质量等，以确保代码和性能的高质量。构建是一个复杂的任务，涉及许多接口。遵循适当的指南将使其根据组织需求更为健壮和高效。
- en: Automated builds with automated testing processes
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化构建与自动化测试流程
- en: Peer review of source code based on sample
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于示例的源代码同行评审
- en: Common IDE, code review tools (PMD, check style, and so on), build tools (Hudson
    and Maven)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用IDE、代码审查工具（PMD、check style等）、构建工具（Hudson和Maven）
- en: Standard build promotion procedures and schedules
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化的构建推广程序和时间表
- en: Publish environment stability on a weekly basis through continuous integration
    and testing
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每周通过持续集成和测试发布环境稳定性
- en: Define a test bed and regression suite to execute impacted modules
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义测试环境和回归套件以执行受影响的模块
- en: Version control
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 版本控制
- en: Version control will ensure code changes are well tracked for traceability and
    accountability.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制将确保代码变更得到良好的跟踪，便于可追溯性和问责
- en: 'All of the organization teams using an enterprise standard tool for version
    control is the ideal. Governance in versioning has the following attributes:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 所有组织团队使用企业标准工具进行版本控制是理想状态。版本控制治理具有以下特点：
- en: Automated email for every check-in to a controlled group of supervisors mailing
    list
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次检查提交时，自动向受控的主管邮件列表发送电子邮件
- en: Define contributors for every program component and restrict access to the components
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义每个程序组件的贡献者，并限制对组件的访问
- en: Periodic audit of check-ins and approval from a component owner
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期审计检查提交，并获得组件所有者的批准
- en: Release management
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发布管理
- en: A mature release management process is a strong asset for the organization to
    deliver timely dependable products to its customers with quality.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成熟的发布管理过程是组织向客户及时交付高质量可靠产品的重要资产
- en: Centralized release management
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中化发布管理
- en: Common priority defined across programs and features
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨程序和功能定义的共同优先级
- en: Employ microservices/incremental deployment--architecture for independent deployments.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微服务/增量部署——支持独立部署的架构
- en: Building enterprise applications with Spark
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark构建企业应用程序
- en: For enterprise applications to be successful, it is very important that you
    carefully define the data access, processing, and governance framework.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业应用程序的成功来说，仔细定义数据访问、处理和治理框架是非常重要的。
- en: Client-services presentation tier
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端服务展示层
- en: 'This graphical user interface will be backed by a set of APIs to help on-board
    new users. Some features that can be supported are as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图形用户界面将由一组API支持，帮助新用户入驻。支持的功能如下：
- en: Manage client data sources, file formats, delivery frequency, validation rules,
    join conditions (if multiple datasets are present), and so on.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理客户端数据源、文件格式、交付频率、验证规则、联接条件（如果有多个数据集）等
- en: Validate and transform datasets
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证和转换数据集
- en: Manage access to datasets
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理数据集访问权限
- en: Additional data delivery requirements from Eureka
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eureka的附加数据传输要求
- en: Data catalog services
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据目录服务
- en: 'This graphical user interface will be backed by a set of APIs to provide data-related
    services. Some of the features that can be supported are:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图形用户界面将由一组API支持，提供与数据相关的服务。支持的功能如下：
- en: Search for any dataset/data in the data lake in a fashion similar to Google
    Search
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Google搜索的方式，在数据湖中搜索任何数据集/数据
- en: Browse (preview with pagination) search results
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览（带分页的预览）搜索结果
- en: Display the lineage and data profile of the selected data set
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示所选数据集的血统和数据概况
- en: Workflow catalog
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作流目录
- en: 'This graphical user interface will let you define workflows for an application
    and schedule runs. The main functionalities include:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图形用户界面将允许你为应用程序定义工作流并安排执行。主要功能包括：
- en: Create workflows for an application and schedule them
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为应用程序创建工作流并安排执行
- en: Display the execution status of workflows, the time taken, as well as the datasets
    involved along with the lineage
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示工作流的执行状态、所用时间，以及涉及的的数据集和血统
- en: Ability to restart the process in case of a failure or from any given point
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在失败或从任何给定点恢复时，能够重新启动处理过程
- en: Configure status updates, notifications, and alerts
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置状态更新、通知和警报
- en: Usage and tracking
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用和跟踪
- en: This graphical user interface will be in use for Sentry and Navigator to track
    the usage of datasets across the cluster. The main functionalities include
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图形用户界面将用于Sentry和Navigator，跟踪整个集群中数据集的使用情况。其主要功能包括
- en: '**Valid usage tracking of datasets**: How many times a dataset was accessed
    and by who'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集的有效使用跟踪**：数据集被访问了多少次，以及由谁访问'
- en: '**Invalid usage tracking**: Who tried to access a dataset they did not have
    access to'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无效使用跟踪**：谁尝试访问他们没有权限的数据集'
- en: Process tracking in terms of which user is running what process and what resources
    are being consumed
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理跟踪，了解哪些用户在运行什么处理过程以及消耗了哪些资源
- en: Need to define dashboards based on requirements from the operations team
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要根据运营团队的要求定义仪表板
- en: Security catalog
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全目录
- en: 'This graphical user interface will be backed by REST APIs to configure access
    control for user groups. The features included are as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图形用户界面将由REST API支持，用于配置用户组的访问控制。包括的功能如下：
- en: Manage users and groups
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理用户和用户组
- en: Manage user access to various datasets across the cluster and applications that
    can run in the data lake
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理用户对集群中各种数据集和能够在数据湖中运行的应用程序的访问
- en: Processing framework
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理框架
- en: This is where all transformations and processing on the data will take place.
    Processing can be batch as well as real-time with support for various frameworks
    such as Spark and MapReduce, along with querying engines such as Hive and Impala.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是所有数据转换和处理的发生地。处理可以是批量的，也可以是实时的，支持各种框架，如Spark和MapReduce，以及查询引擎，如Hive和Impala。
- en: Ingestion services
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取服务
- en: Data can be ingested from various sources, ranging from batch to real-time streams
    using tools such as Sqoop, Flume, Kafka, and SFTP.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以通过多种来源进行摄取，从批处理到实时流，使用如Sqoop、Flume、Kafka和SFTP等工具。
- en: Ingestion will be metadata-driven
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取将由元数据驱动
- en: In order to ingest new data sources, new code is not required as long as it
    makes use of the supported ingestion methods, such as Kafka, Flume, Sqoop, and
    so on
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了摄取新的数据源，只要使用支持的摄取方法（如Kafka、Flume、Sqoop等），就不需要新代码
- en: We should be able to register datasets and start cataloging them into the data
    catalog as soon as they are ingested
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该能够注册数据集，并在数据摄取后立即将它们开始进行目录化
- en: 'The ingested data can be used in multiple forms: stored, persisted into a device,
    published to external vendors. It can be accessed by other third-party programs
    through APIs.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 获取的数据可以以多种形式使用：存储、持久化到设备、发布到外部供应商。它可以通过 API 被其他第三方程序访问。
- en: '**Storage layers**: In order to maintain data integrity and isolation, we can
    spread our data in HDFS across multiple layers so that each layer defines a certain
    stage between ingesting raw data and generating insights.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储层**：为了保持数据完整性和隔离性，我们可以将数据分布在多个 HDFS 层中，以便每个层定义从获取原始数据到生成洞察之间的某个阶段。'
- en: '**Publishing**: This service will be used to publish data to external users
    and subscribers, as well as applications, as an outward push from the data lake.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布**：该服务将用于将数据发布到外部用户和订阅者，以及应用程序，作为数据湖的外部推送。'
- en: '**Bulk API**: This service will be used to download data from the system using
    an asynchronous API. Users will make requests for data retrieval and they will
    be notified when the data set is ready. Delivery of the data can be provided in
    multiple ways:'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量 API**：该服务将用于通过异步 API 从系统中下载数据。用户将发出数据检索请求，并在数据集准备好时收到通知。数据的交付可以通过多种方式提供：'
- en: Download link.
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载链接。
- en: Push to SFTP location.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推送到 SFTP 位置。
- en: HDFS location for internal users (same cluster or another cluster).
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部用户的 HDFS 位置（同一集群或另一个集群）。
- en: API contracts need to defined.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要定义 API 合同。
- en: '**Data Access API**: This API is similar to the Bulk API, but will only support
    small datasets such as a credit score to be synchronized frequently.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问 API**：该 API 类似于批量 API，但仅支持频繁同步的小型数据集，例如信用评分。'
- en: '**Notebooks**: These can include interfaces such as Apache Zeppelin or the
    Hue data science workbench, which will give a GUI interface for users to access
    datasets in the cluster and query them using Impala, Spark, R, and so on.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：这些可以包括诸如 Apache Zeppelin 或 Hue 数据科学工作台等界面，为用户提供图形界面，以便访问集群中的数据集并使用
    Impala、Spark、R 等进行查询。'
- en: Data science
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学
- en: Data science as a field has many dimensions and applications. As we all are
    familiar with science by formulating reusable and established formulas, we understand
    the features, behavior patterns, and meaningful sights. In a similar way from
    relevant data too through engineering and statistical methods, we understand the
    behavior patterns and meaningful sights. Thus, it's also viewed as data plus science,
    the science of data or data science.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学作为一个领域有许多维度和应用。正如我们都熟悉通过制定可重用和已建立的公式来理解科学，我们理解特征、行为模式和有意义的洞察。同样，通过工程和统计方法从相关数据中，我们也能理解行为模式和有意义的洞察。因此，它也被视为数据加科学，数据科学的科学。
- en: 'Data science has been in use for decades across industries. Many algorithms
    have been developed and are in use across the industries, including:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学在各行各业已经使用了几十年。许多算法已经开发并在各行业中使用，包括：
- en: K-means clustering
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类
- en: Association rule mining
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: Linear regression
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Naïve Bayesian classifiers
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天真贝叶斯分类器
- en: Decision trees
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Time series analysis
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列分析
- en: Text analytics
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析
- en: Big data processing
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据处理
- en: Visual work flows
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化工作流
- en: Apriori
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apriori
- en: Neural networks
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Combinations of the preceding algorithms are used to solve popular business
    problems such as the following, and new business opportunities are surfacing continuously:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 前述算法的组合用于解决流行的商业问题，例如以下问题，并且新的商业机会不断涌现：
- en: '| Cross-selling | Find relationship among customer characteristicsMatch campaigns
    to potential customers |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 交叉销售 | 查找客户特征之间的关系将营销活动与潜在客户匹配 |'
- en: '| Product yield analysis | Classify product defects |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 产品产量分析 | 分类产品缺陷 |'
- en: '| Direct marketing | Classify customersMatch campaigns to potential customers
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 直接营销 | 分类客户将营销活动与潜在客户匹配 |'
- en: '| Churn analysis | Predict the tendency of churningWin back customers |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 流失分析 | 预测流失趋势争取挽回客户 |'
- en: '| Cross-selling | Find relationship of products in transactions |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 交叉销售 | 查找交易中产品的关系 |'
- en: '| Segmentation analysis | Classify customersMatch campaigns to potential customers
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 分割分析 | 分类客户将营销活动与潜在客户匹配 |'
- en: '| Inventory analysis | Find relationship of products in transactionsMake replenishment
    decision |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 库存分析 | 查找交易中产品的关系做出补货决策 |'
- en: '| Product-mix-analysis | Classify customersMatch campaigns to potential customersEstimate
    the revenue of product mix |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 产品组合分析 | 分类客户将营销活动与潜在客户匹配估算产品组合的收入 |'
- en: '| Fraud detection | Classify customersDetect unusual activities |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 欺诈检测 | 分类客户检测异常活动 |'
- en: '| Credit-rating | Classify customersCredit rating customers |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 信用评级 | 分类客户信用评级客户 |'
- en: 'For example, in data classification itself we can implement the following three
    models to get a refined and accurate model:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在数据分类中，我们可以实施以下三种模型，以获得一个精炼且准确的模型：
- en: '**Random forests**: Random forests or random decision forests are an ensemble
    learning method for classification, regression, and other tasks. They operate
    by constructing a multitude of decision trees at training time and outputting
    the class that is the mode of the classes (classification) or mean prediction
    (regression) of the individual trees. Random decision forests correct for decision
    trees'' habit of overfitting to their training set.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**：随机森林或随机决策森林是一种集成学习方法，用于分类、回归和其他任务。它们通过在训练时构建多个决策树，并输出类别的众数（分类）或个别树的平均预测值（回归）。随机决策森林可以纠正决策树过拟合训练集的问题。'
- en: Please check the following link: [https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下链接：[https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)。
- en: '**Naive Bayes**: Naive Bayes classifiers are a family of simple probabilistic
    classifiers based on applying Bayes'' theorem with strong independence assumptions
    between the features. Naive Bayes classifiers are highly scalable, requiring a
    number of parameters to be linear in the number of variables (features/predictors)
    in a learning problem.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**：朴素贝叶斯分类器是一类简单的概率分类器，基于在特征之间做出强独立假设的贝叶斯定理。朴素贝叶斯分类器具有很高的可扩展性，要求学习问题中的变量（特征/预测变量）数目与参数数量之间呈线性关系。'
- en: '**Support vector machine**: Support vector machines are supervised learning
    models with associated learning algorithms that analyze data used for classification
    and regression analysis. Given a set of training examples, each marked as belonging
    to one of two categories. An SVM training algorithm builds a model that assigns
    new examples into one category or the other, making it a non-probabilistic binary
    linear classifier.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**：支持向量机是有监督的学习模型，配有相关的学习算法，用于分类和回归分析。给定一组标记为属于两个类别之一的训练样本，SVM训练算法构建一个模型，将新样本分配到其中一个类别，从而成为一种非概率的二分类线性分类器。'
- en: 'The following are the steps towards building a prediction model to solve a
    business problem:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是构建预测模型以解决业务问题的步骤：
- en: Defining the tangible business goal is the most important criteria. The business
    value and purpose of the data science problem agreement by different stakeholders
    is the most important step.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义具体的业务目标是最重要的标准。不同利益相关方对数据科学问题协议的业务价值和目的达成一致是最关键的步骤。
- en: Sponsorship and buy-in from key stakeholders is crucial to the success of the
    project.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自关键利益相关方的资助和支持对于项目的成功至关重要。
- en: Collaboration among data engineers and data scientists is critical; otherwise,
    working in silos will not lead to project success.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师和数据科学家之间的协作至关重要；否则，孤立作业将无法实现项目的成功。
- en: A data lake is a repository that gathers useful data from different source systems
    of appropriate, valid, meaningful, useful, and historical data required for the
    business problem. For building a data lake, capacity planning for the initial
    data and growth considerations for the future should be taken into account.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖是一个汇集来自不同源系统的有用数据的存储库，这些数据是为了解决业务问题所需的适当、有效、有意义、有用且历史的数据。建立数据湖时，应考虑初始数据的容量规划以及未来增长的考虑。
- en: Cleanse data as per quality norms to ensure only valid and appropriate data
    is used and no dirty or stale data enters the system.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据质量标准清理数据，以确保仅使用有效和适当的数据，避免脏数据或过时数据进入系统。
- en: Feature engineering is core engineering of the data science project to extract
    meaningful insights (features) from the raw data.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程是数据科学项目的核心工程，旨在从原始数据中提取有意义的见解（特征）。
- en: Feature selection is to eliminate irrelevant, redundant, or highly correlated
    features.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择是消除不相关、冗余或高度相关的特征。
- en: Test the prediction models by following the proper validation methods, such
    as K-Fold Cross Validation or 70:30 models.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过遵循适当的验证方法（例如K折交叉验证或70:30模型）来测试预测模型。
- en: Establish the model results. As with any project, the repetition of results
    accuracy validates the model's effectiveness.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定模型结果。与任何项目一样，结果准确性的重复验证模型的有效性。
- en: Optimize the model by continuous improvement with new data iteratively, and
    by fine-tuning.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用新数据进行持续迭代改进，并进行微调来优化模型。
- en: 'Many popular statistical modeling tools are on the market, such as:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上有许多受欢迎的统计建模工具，例如：
- en: SPSS modeler
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SPSS建模工具
- en: KNIME
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNIME
- en: Microsoft Revolution Analytics
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Revolution Analytics
- en: RapidMiner
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RapidMiner
- en: SAP Predictive Analytics
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAP预测分析
- en: SAS Enterprise Miner
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAS Enterprise Miner
- en: Oracle Advanced Analytics (Oracle Data Miner, Oracle R Advanced Analytics)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle高级分析（Oracle数据挖掘，Oracle R高级分析）
- en: 'A few popular open source tools offer all the functionality on a par with commercial
    established statistical packages:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的开源工具提供与商业统计软件包相当的全部功能：
- en: R
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R
- en: Python
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: Scala
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala
- en: MatLab
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MatLab
- en: Julia
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia
- en: The recent surge in low-cost technology availability such as Hadoop Eco systems,
    cloud computing, big data and open source tools has led to large-scale adoption
    by every industry from small enterprises to large giants.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 近期低成本技术（如Hadoop生态系统、云计算、大数据和开源工具）的普及，已促使从小型企业到大型巨头的各行各业大规模采用。
- en: Approach to data science
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学方法
- en: 'The approach to data science solutions involves the following staged approach:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学解决方案的方法包括以下几个阶段：
- en: '![](img/064bb679-aa95-48ab-b565-0e068db1a8f9.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/064bb679-aa95-48ab-b565-0e068db1a8f9.png)'
- en: Knowledge mining (discovery) in datasets is an interactive and iterative process
    involving several steps for identifying valid, useful, and understandable patterns
    in data.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的知识挖掘（发现）是一个交互式和迭代的过程，涉及多个步骤，用于识别数据中的有效、有用和可理解的模式。
- en: '**Data processing**: Data cleansing pre-transforms the raw data into an easy
    and convenient format for usage; a few related tasks are:'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理**：数据清洗将原始数据预处理成易于使用的格式；相关任务包括：'
- en: Sampling, that is, selecting representative subsets from a large population
    of data
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽样，即从大量数据中选择具有代表性的子集
- en: Remove noise
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除噪声
- en: Missing data handling for incomplete rows
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不完整行进行缺失数据处理
- en: Normalization of data
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据标准化
- en: 'Feature extraction: data useful in a particular context is extracted'
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取：提取在特定上下文中有用的数据
- en: '**Data transformation**: Ensure usability of data by missing data treatment
    methods like:'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：通过缺失数据处理方法确保数据的可用性，例如：'
- en: Use only rows with relevant data
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用包含相关数据的行
- en: 'Substitution of values:'
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值替代：
- en: Mean value of particular attribute is used
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定属性的均值
- en: Regression substitution with historical value from similar case
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相似案例的历史值进行回归替代
- en: Matching imputation, similar attribute correlation case is used
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用匹配插补、相似属性关联的案例
- en: Maximum likelihood, EM, and so on
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大似然法、EM算法等
- en: '**Data mining**: It is automating the searching patterns in the data by methods
    like:'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据挖掘**：通过方法如：'
- en: Association rules
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联规则
- en: Sequence and path analysis
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列和路径分析
- en: 'Clustering analysis:'
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类分析：
- en: Partitioning a data set into clusters or subsets based on some common attributes
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一些共同属性将数据集划分为聚类或子集
- en: 'Classification methods:'
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类方法：
- en: Division of samples into classes
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将样本划分为类别
- en: Trained set is used previous labelled data
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用经过训练的集合作为之前标注过的数据
- en: Regression models
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型
- en: Prediction of new values based on past data by inference
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于过去数据的推断预测新值
- en: Compute new values for dependent variable values based on few other measured
    attributes
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于少量其他测量属性计算因变量的新值
- en: Visualization patterns
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化模式
- en: 'Classification is similar to clustering but requires classes to be defined
    ahead of time:'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类类似于聚类，但需要提前定义类别：
- en: Classification with classifier based on input label is returned
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于输入标签的分类器返回的分类结果
- en: Probabilistic classification is where classifier returns probable values to
    assign them to a class
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率分类是指分类器返回可能的值，将它们分配到一个类别中。
- en: Specific criteria like data more than 90% to avoid costly mistakes
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用特定标准，如数据超过90%，以避免成本高昂的错误
- en: Assign objects to class based on probability limits (greater than 40%, and so
    on)
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据概率限制（如超过40%）将对象分配到类别中
- en: Regression and forecasting
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归与预测
- en: 'Data table statistical correlation:'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据表统计相关性：
- en: Data distribution in functional forms with prior assumptions
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于先验假设的功能形式中的数据分布
- en: Machine learning-based algorithms
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于机器学习的算法
- en: 'Curve fitting:'
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合：
- en: A well defined and known function underlying the data is explored
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据背后定义良好且已知的函数
- en: Theory- and expertise based
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论和专业知识为基础
- en: 'Machine learning:'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习：
- en: 'Supervised:'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习：
- en: Both input and desired results are part of training data
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和期望结果都作为训练数据的一部分
- en: Model training process inputs are based on correct known target and results
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练过程的输入基于已知的正确目标和结果
- en: Proper training, validation, and test set construction are crucial
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当的训练、验证和测试集构建至关重要
- en: Fast and accurate results
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速且准确的结果
- en: Ability to generalize, new data should produce correct results without prior
    knowledge of target
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化能力，新的数据应能在没有目标先验知识的情况下得出正确结果
- en: Generalization means the ability to produce valid outputs for inputs not available
    during training
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化意味着能够为训练过程中未出现的输入产生有效的输出
- en: 'Unsupervised:'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习：
- en: Correct results are not provided to the model during training
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中不会提供正确的结果给模型
- en: Input data clustered to classes based on their statistical properties alone
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据根据其统计特性仅按类别进行聚类
- en: Significance of cluster and label
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类和标签的重要性
- en: Even small number of objects which are representative of desired classes and
    labels can be applied
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是少量的代表性对象，也可以应用于所需的类别和标签
- en: Curve fitting
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合
- en: Is by using proper subsets and early stopping
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用适当的子集和早期停止实现
- en: Based on data learning and not just underlying function
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据学习，而不仅仅是底层函数
- en: Data used in training should perform well with new data
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的数据在面对新数据时应表现良好
- en: '![](img/4c3da999-84f6-4ed8-8796-31c7f7e62f69.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c3da999-84f6-4ed8-8796-31c7f7e62f69.png)'
- en: 'Data sets:'
  id: totrans-459
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集：
- en: '**Training set**: Used for learning where target value is known.'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：用于学习，其中目标值是已知的。'
- en: '**Validation set**: Used to tune classifier architecture to estimate error.'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：用于调优分类器架构，以估计误差。'
- en: '**Test set**: Performance of classifier is assessed only; it''s never used
    in the training process. Test set error should provide unbiased generalization
    error estimate.'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：仅评估分类器的性能；在训练过程中从未使用。测试集的误差应提供无偏的泛化误差估计。'
- en: '![](img/312fc9c9-cc15-44a2-92a9-c97b076e211f.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](img/312fc9c9-cc15-44a2-92a9-c97b076e211f.png)'
- en: '**Data selection**: Garbage In , Garbage Out, underlying model representation
    should be training, validation, and test data.'
  id: totrans-464
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据选择**：垃圾进，垃圾出，底层模型表示应包括训练、验证和测试数据。'
- en: '**Unbalanced datasets**:'
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平衡数据集**：'
- en: Network minimizes overall error so the proportion of types of data in the set
    is critical
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络最小化总体误差，因此数据集中各类型数据的比例至关重要
- en: Loss matrix inclusion
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失矩阵包含
- en: An even representation of different cases is the best approach to interpret
    networks decision
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同情况的均衡表示是解读网络决策的最佳方法
- en: 'Learning process:'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程：
- en: 'Back propagation:'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播：
- en: Output values compared with target value to compute the predefined error function
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出值与目标值进行比较，以计算预定义的误差函数
- en: Error is fed back into the network
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误差反馈到网络中
- en: Using these inputs the algorithm adjusts the weights of each connection to reduce
    the value of error function.
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些输入，算法调整每个连接的权重，以减少误差函数的值。
- en: The network will converge after repeating the process for longer training cycles
    for sufficient numbers
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络将在经过足够次数的训练周期后收敛
- en: '![](img/9cf39fb0-b1cc-4dec-9ef3-fedfac318dd2.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cf39fb0-b1cc-4dec-9ef3-fedfac318dd2.png)'
- en: 'Results:'
  id: totrans-476
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果：
- en: 'Confusion matrix: The prediction results on X axis are compared to target values
    on Y. Rows represent the true classes and columns predicted classes.'
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵：X轴上的预测结果与Y轴上的目标值进行比较。行表示真实类别，列表示预测类别。
- en: '![](img/2695048c-bfd3-44fe-8db7-1698019e41c6.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2695048c-bfd3-44fe-8db7-1698019e41c6.png)'
- en: Completeness and contamination
  id: totrans-479
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整性和污染
- en: 'Performances are rated as the following criteria for the classifiers, for example
    between two classes:'
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器的性能根据以下标准进行评估，例如在两个类别之间：
- en: '**Completeness**: The percentage of objects of class A correctly classified'
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：正确分类的A类对象的百分比'
- en: '**Contamination**: The percentage of objects of class A incorrectly classified
    as objects belonging to class B'
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**污染**：A类对象被错误分类为B类对象的百分比'
- en: '**Classification rate**: The overall percentage of objects correctly classified'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类率**：正确分类的对象的整体百分比'
- en: Supervised models
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习模型
- en: Neural networks
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Multi layer perceptron
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器
- en: Decision trees
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Neural network
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: It is a structured flow consisting of the input layer of neurons and the output
    layer of neurons with one or more hidden layers in the middle.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个结构化的流程，由神经元的输入层和输出层组成，中间有一个或多个隐藏层。
- en: Neurons are well connected with adjacent layers though being in different topologies,
    they are connection by a choice of activation function, the **weights** are assigned
    as function values associated with the connections of various types and architectures.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元通过选择激活函数与相邻层之间建立了良好的连接，尽管它们位于不同的拓扑结构中，**权重**是根据各种类型和架构的连接的函数值分配的。
- en: '![](img/eefb53d0-7d3f-46a3-ac3a-09156bcc0cd0.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eefb53d0-7d3f-46a3-ac3a-09156bcc0cd0.png)'
- en: '**Artificial neural networks** this is inspired by the biological nervous system
    process. An artificial neural network is an information-processing mechanism.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**：受生物神经系统过程的启发。人工神经网络是一种信息处理机制。'
- en: Highly interconnected large number of simple processing neuron elements working
    together to address specific problems.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 大量高度互联的简单处理神经元元素协同工作，解决特定问题。
- en: 'A simple artificial neuron:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的人工神经元：
- en: '![](img/c9c38d79-06f3-41bd-934f-098d854172b8.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9c38d79-06f3-41bd-934f-098d854172b8.png)'
- en: A node or unit is a basic computational element that receives input from other
    units or external source.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 节点或单元是一个基本的计算元素，接收来自其他单元或外部源的输入。
- en: To model synaptic learning, each input is considered with associated weight
    *w*.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟突触学习，每个输入都与相关的权重 *w* 一起考虑。
- en: 'The weighted sum of its inputs is computed as a function by the unit:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 该单元的输入加权和通过一个函数进行计算：
- en: '![](img/3e295094-0714-43c9-9891-a83e78188e96.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e295094-0714-43c9-9891-a83e78188e96.png)'
- en: 'The different types of neural network are:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的不同类型有：
- en: '**Feedforward**: **Adaptive Linear Neuron** (**ADALINE**), RBF, single layer
    perception'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈**：**自适应线性神经元** (**ADALINE**)，RBF，单层感知器'
- en: '**Self-organised**: SOM (Kohonen Maps)'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自组织**：SOM（科霍宁映射）'
- en: '**Recurrent**: Simple recurrent network, Hopfield network'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归**：简单递归网络，霍普菲尔德网络'
- en: '**Stochastic**: Boltzmann machines, RBM'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机**：玻尔兹曼机，RBM'
- en: '**Modular**: **Associative Neural Networks** (**ASNN**), committee of machines'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**：**关联神经网络** (**ASNN**)，机器委员会'
- en: '**Others**: NeuroFuzzy, Cascades, PPS, GTM, Spiking (SNN), Instantaneously
    Trained'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他**：神经模糊，级联，PPS，GTM，脉冲（SNN），即时训练'
- en: '![](img/0daf9047-0a3b-41d3-8981-1e5e6f53247c.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0daf9047-0a3b-41d3-8981-1e5e6f53247c.png)'
- en: Multi layer perceptron
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: This is a most popular supervised model consisting of multiple layers of computational
    units inter connected in a feed-forward way usually. Each neuron in one layer
    is connected to subsequent layer neurons through direct connections.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最流行的监督模型，通常由多个计算单元层组成，单元层之间通过前馈方式互联。每一层的神经元通过直接连接与后续层的神经元相连。
- en: Decision tree
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: This is a classification method with a set of simple rules; they are non-parametric
    without the need for any assumptions on distribution of the variables in each
    class.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种分类方法，基于一组简单的规则；它们是非参数的，不需要对每个类中变量的分布做任何假设。
- en: 'As example decision tree is depicted in following:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，决策树如下所示：
- en: '![](img/395ad59d-c432-4a07-a086-1788017f9062.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![](img/395ad59d-c432-4a07-a086-1788017f9062.png)'
- en: Unsupervised models
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督模型
- en: Clusters
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Distances
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离
- en: Normalization
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化
- en: K-means
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值
- en: Self-organizing maps
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自组织映射
- en: Clusters
  id: totrans-520
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Clusters are hierarchical, it finds successive clusters using previously assigned
    clusters with bottom up (agglomerative) or top-down (divisive) and partitional
    type cluster depicted below as right and left side respectively.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是层次化的，它使用先前分配的聚类来查找连续的聚类，采用自下而上（凝聚性）或自上而下（分裂性）以及分区类型的聚类，如下图右侧和左侧所示。
- en: Distances
  id: totrans-522
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离
- en: To determine the similarity between two clusters and the shape of clusters.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 用于确定两个聚类之间的相似性和聚类的形状。
- en: Normalization
  id: totrans-524
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: "**VAR**: For a transformed set of data points for each attribute, the mean\
    \ is \Preduced to zero; this is by subtracting the mean of each attribute from\
    \ the \Pvalues of the attributes and dividing the result by the standard deviation\
    \ of the attribute."
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VAR**：对于每个属性的变换数据集，均值被减少到零；这是通过从每个属性的值中减去该属性的均值，并将结果除以该属性的标准差来实现的。'
- en: "**RANGE (Min‐Max\P Normalization)**: It subtracts the minimum value of an attribute\
    \ from each value of the attribute and then divides the difference by \Pthe range\
    \ of the attribute. The advantage is preserving all relationship in the\Pdata\
    \ precisely, without adding any bias."
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围（最小-最大归一化）**：它从每个属性的值中减去该属性的最小值，然后将差值除以属性的范围。其优点是准确地保持数据中的所有关系，不引入任何偏差。'
- en: '**SOFTMAX**: It is a way of reducing the influence  of extreme values or outliers
    in the data without removing them from the dataset. It is useful when you have
    outlier data that you wish to include in the dataset while still preserving the
    significance of data within a standard deviation of the mean.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：它是一种减少数据中极端值或离群值影响的方法，而无需将它们从数据集中删除。当你有离群值数据且希望将其包含在数据集中，同时仍然保留在均值标准差内的数据的显著性时，这种方法非常有用。'
- en: K-means
  id: totrans-528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means
- en: K-means is popular model as it's fast and simple; however it does not yield
    the same result with every run.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是流行的模型，因为它快速且简单；然而，每次运行的结果可能不同。
- en: Partitions data into K clusters based on their features
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据特征将数据划分为K个簇。
- en: Each cluster is represented by its centroid, the center of the cluster points
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个簇由其质心表示，质心是簇中点的中心。
- en: Each point is assigned to the nearest cluster
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个点被分配到最近的簇。
- en: The goal is to minimize intra-cluster variance or the sum of squares of distances
    between data and the corresponding cluster centroid
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是最小化簇内方差或数据与对应簇质心之间距离的平方和。
- en: Computes the mean point--centroid
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算均值点——质心。
- en: A new partition is built by associating each point with the nearest centroid.
    Computes the mean point or centroid of each set.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个点与最近的质心关联来构建新的划分。计算每个数据集的均值点或质心。
- en: The recent surge of low-cost technology availability such as Hadoop eco systems,
    cloud computing, big data, and open source tools has led to large-scale adoption
    by every industry from small to large giants. Data science penetration is also
    witnessed across every industry with eco system being available.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，低成本技术的涌现，如Hadoop生态系统、云计算、大数据和开源工具，已导致从小型企业到大型巨头的广泛采用。数据科学的渗透也遍及各行各业，生态系统得到了普及。
- en: Summary
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered the key concepts of building big data applications,
    covering the tools for data discovery, quality, and ingestion. We discussed the
    Spark Stream in-memory engine and its versatility; data science models for various
    industry solutions were also discussed.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了构建大数据应用程序的关键概念，涵盖了数据发现、质量和摄取的工具。我们讨论了Spark Stream内存引擎及其多功能性；还讨论了适用于各种行业解决方案的数据科学模型。
