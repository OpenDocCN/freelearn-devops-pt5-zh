- en: Orchestration Using Kubernetes
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 进行编排
- en: This chapter is dedicated to the most widely used container orchestrator today—Kubernetes.
    In 2018, Kubernetes was adopted by 51% of container users as their main orchestrator.
    Kubernetes adoption has increased in recent years, and it is now at the core of
    most **Container-as-a-S****ervice** (**CaaS**) platforms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章专门介绍当今最广泛使用的容器编排工具——Kubernetes。2018年，51%的容器用户选择将 Kubernetes 作为他们的主要编排工具。近年来，Kubernetes
    的采用率不断增长，现在它已成为大多数 **容器即服务**（**CaaS**）平台的核心。
- en: Cloud providers have followed the expansion of Kubernetes, and most of them
    (including Amazon, Google, and Azure) now provide their own **Kubernetes-as-a-Service**
    (**KaaS**) platforms where users do not have to take care of Kubernetes' administrative
    tasks. These services are designed for simplicity and availability on cloud platforms.
    Users just run their workloads on them and the cloud providers manage complicated
    maintenance tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商已经跟随 Kubernetes 的扩展，且大多数提供商（包括 Amazon、Google 和 Azure）现在都提供自有的 **Kubernetes
    即服务**（**KaaS**）平台，用户无需处理 Kubernetes 的管理任务。这些服务旨在简化操作并保证在云平台上的可用性。用户只需在这些平台上运行其工作负载，而云服务提供商则负责复杂的维护任务。
- en: In this chapter, we will learn how Kubernetes works and what features it provides.
    We'll review what is required to deploy a Kubernetes cluster with high availability.
    We will then learn about Kubernetes objects, such as pods and services, among
    others. Networking is key to distributing workloads within a cluster; we will
    learn how Kubernetes networking works and how it provides service discovery and
    load balancing. Finally, we will review some of the special security features
    provided by Kubernetes to manage cluster authentication and authorization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解 Kubernetes 的工作原理以及它提供的功能。我们将回顾部署一个高可用性 Kubernetes 集群所需的内容。接下来，我们将学习
    Kubernetes 对象，如 pods 和服务等。网络是将工作负载分配到集群中的关键，我们将学习 Kubernetes 网络是如何工作的，以及它如何提供服务发现和负载均衡。最后，我们将回顾
    Kubernetes 提供的一些特殊安全功能，用于管理集群的认证和授权。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Deploying Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 Kubernetes
- en: High availability with Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 实现高可用性
- en: Pods, services, and other Kubernetes resources
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods、服务和其他 Kubernetes 资源
- en: Deploying orchestrated resources
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署编排资源
- en: Kubernetes networking
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Publishing applications
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布应用程序
- en: Kubernetes is not part of the Docker Certified Associate exam yet, but it probably
    will be in the next release as Docker Enterprise comes with a fully compatible
    Kubernetes platform deployed on top of the Docker Swarm orchestrator. Docker Enterprise
    is the only container platform that provides both orchestrators at the same time.
    We will learn about Docker Enterprise's components and features in the third section
    of this book, with a chapter dedicated to each component.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 目前还不包括在 Docker Certified Associate 考试中，但它可能会在下一个版本中加入，因为 Docker Enterprise
    包含了一个完全兼容的 Kubernetes 平台，并且部署在 Docker Swarm 编排工具之上。Docker Enterprise 是唯一同时提供这两种编排工具的容器平台。我们将在本书的第三部分学习
    Docker Enterprise 的组件和功能，每个组件都有独立的章节。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will learn about the features of the Docker Swarm orchestrator.
    We also provide some labs at the end of the chapter to help you to understand
    and learn about the concepts that we will cover. These labs can be run on your
    laptop or PC using the provided Vagrant *Kubernetes environment* or any already-deployed
    Docker Swarm cluster by yourself. You can view additional information in this
    book's GitHub code repository, which is available at [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解 Docker Swarm 编排工具的功能。我们还将在章末提供一些实验室，帮助你理解和学习我们将要覆盖的概念。这些实验可以在你的笔记本电脑或个人电脑上运行，使用提供的
    Vagrant *Kubernetes 环境* 或自行部署的任何 Docker Swarm 集群。你可以在本书的 GitHub 代码库中查看更多信息，地址为
    [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git)。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，观看代码演示：
- en: '"[https://bit.ly/3gzAnS3](https://bit.ly/3gzAnS3)"'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '"[https://bit.ly/3gzAnS3](https://bit.ly/3gzAnS3)"'
- en: Deploying Kubernetes using Docker Engine
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker 引擎部署 Kubernetes
- en: Kubernetes has many features and is more complex than Docker Swarm. It provides
    additional features not available on Docker Swarm without having to modify our
    application code. Docker Swarm is more aligned with microservices logic, while
    Kubernetes is closer to the virtual machine application's **lift and shift** approach
    (move application as is to a new infrastructure). This is because the Kubernetes
    pod object can be compared to virtual machines (with application processes running
    as containers inside a pod).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes具有许多功能，比Docker Swarm更为复杂。它提供了Docker Swarm没有的额外功能，且不需要修改应用程序代码。Docker
    Swarm更符合微服务逻辑，而Kubernetes更接近虚拟机应用程序的**提升与迁移**方法（将应用程序原样迁移到新的基础设施）。这是因为Kubernetes的pod对象可以与虚拟机进行比较（其中应用程序进程作为容器在pod内部运行）。
- en: Before we begin discussing Kubernetes architecture, let's review some of the
    concepts that we've learned about orchestration.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始讨论Kubernetes架构之前，让我们回顾一下我们已学到的一些编排概念。
- en: Orchestration should provide all that's required for deploying a solution to
    execute, manage, and publish applications based on the containers distributed
    on a pool of nodes. Therefore, it should provide a control plane to ensure cluster
    availability, a scheduler for deploying applications, and a network plane to interconnect
    distributed applications. It should also provide features for publishing cluster-distributed
    applications. Application health will also be managed by the orchestrator. As
    a result, if one application component dies, a new one will be deployed to ensure
    the application's health.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 编排应该提供部署解决方案所需的所有功能，用于执行、管理和发布基于容器的分布式应用程序。因此，它应该提供一个控制平面，以确保集群的可用性，一个调度器来部署应用程序，以及一个网络平面来互联分布式应用程序。它还应提供发布集群分布式应用程序的功能。应用程序的健康状况也将由编排器进行管理。因此，如果某个应用程序组件出现故障，编排器会部署一个新的组件，以确保应用程序的健康。
- en: Kubernetes provides all of these features, and so does Docker Swarm too. However,
    Kubernetes has many more features, is extensible, and has a bigger community behind
    the project. Docker also adopted Kubernetes in its Docker Enterprise 2.0 release.
    It is the only platform that supports Docker Swarm and Kubernetes on the same
    infrastructure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了所有这些功能，Docker Swarm也提供这些功能。然而，Kubernetes有更多的功能，具有可扩展性，并且有一个更大的社区支持该项目。Docker也在其Docker
    Enterprise 2.0版本中采纳了Kubernetes。它是唯一一个在同一基础设施上同时支持Docker Swarm和Kubernetes的平台。
- en: Kubernetes provides more container density because it is able to run more than
    one container at once for each application component. It also provides autoscale
    features and other advanced scheduling features.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了更高的容器密度，因为它能够为每个应用程序组件同时运行多个容器。它还提供了自动扩展功能以及其他高级调度功能。
- en: Because Kubernetes is a big community project, some of its components have also
    been decoupled on different projects to provide faster deployment. The main open
    source project is hosted by the **Cloud Native Computing Foundation** (**CNCF**).
    Kubernetes releases a new version every 6 months—imagine updating old legacy applications
    in production every 6 months. As previously mentioned, it is not easy to follow
    this application life cycle for many other products, but Kubernetes provides a
    methodology to upgrade to new software releases easily.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kubernetes是一个大型社区项目，它的某些组件也被解耦到不同的项目中，以便提供更快的部署速度。这个主要的开源项目由**云原生计算基金会**（**CNCF**）托管。Kubernetes每6个月发布一个新版本——想象一下每6个月就要在生产环境中更新旧的遗留应用程序。正如前面提到的，许多其他产品很难跟上这种应用生命周期，但Kubernetes提供了一种方法，使得升级到新软件版本变得更加容易。
- en: Kubernetes' architectural model is based on the usual orchestration components.
    We deploy master nodes to execute management tasks and worker nodes (also known
    as minions) to run application workloads. We also deploy an `etcd` key-value database
    to store all of the cluster object data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的架构模型基于常见的编排组件。我们部署主节点来执行管理任务，工作节点（也称为从节点）来运行应用程序负载。我们还部署了一个`etcd`键值数据库来存储所有集群对象的数据。
- en: 'Let''s introduce the Kubernetes components. Masters and workers run different
    processes, and their number may vary depending on the functionalities provided
    by each role. Most of these components could be installed as either system services
    or containers. Here is a list of Kubernetes cluster components:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来介绍一下Kubernetes的组件。主节点和工作节点运行不同的过程，它们的数量可能会根据每个角色所提供的功能而有所不同。这些组件中的大多数可以作为系统服务或容器进行安装。以下是Kubernetes集群组件的列表：
- en: '`kube-apiserver`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`'
- en: '`kube-scheduler`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`'
- en: '`kube-controller-manager`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`'
- en: '`etcd`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`'
- en: '`` `kubelet` ``'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `kubelet` ``'
- en: '`kube-proxy`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy`'
- en: Container runtime
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时
- en: Note that this list is very different from what we learned about Docker Swarm,
    where everything was built-in. Let's review each component's features and properties.
    Remember, this is not a Kubernetes book—we will only learn the basics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个列表与我们在 Docker Swarm 中学到的内容有很大不同，在 Docker Swarm 中，一切都是内建的。让我们回顾一下每个组件的特性和属性。记住，这不是一本
    Kubernetes 的书——我们只会学习基础知识。
- en: 'We will run dedicated master nodes to provide an isolated cluster control plane.
    The following components will run on these nodes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行专用的主节点来提供隔离的集群控制平面。以下组件将在这些节点上运行：
- en: '`kube-apiserver`: This is the Kubernetes core, and it exposes the Kubernetes
    API via HTTP (HTTPS if we use TLS certificates). We will connect to this component
    in order to deploy and manage applications.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`：这是 Kubernetes 的核心，它通过 HTTP（如果使用 TLS 证书，则为 HTTPS）暴露 Kubernetes
    API。我们将连接到该组件，以便部署和管理应用程序。'
- en: '`kube-scheduler`: When we deploy an application''s components, the scheduler
    will decide where to run each one if no node-specific location has been defined.
    To decide where to run deployed workloads, it will review workload properties,
    such as specific resources, limits, architecture requirements, affinities, or
    constraints.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`：当我们部署应用程序的组件时，如果没有定义特定节点位置，调度器将决定每个组件运行的位置。为了决定在哪里运行已部署的工作负载，它将检查工作负载的属性，如特定资源、限制、架构要求、亲和性或约束。'
- en: '`kube-controller-manager`: This component will manage controllers, which are
    processes that are always watching for a cluster object''s state changes. This,
    for example, will manage the node''s and workload''s states to ensure the desired
    number of instances are running.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`：该组件将管理控制器，控制器是始终监视集群对象状态变化的过程。例如，它将管理节点和工作负载的状态，以确保所需数量的实例正在运行。'
- en: '`etcd`: This is the key-value store for all Kubernetes objects'' information
    and states. Some production environments will run `etcd` out of the master nodes''
    infrastructure to avoid performance issues and to improve components'' high availability.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`：这是所有 Kubernetes 对象信息和状态的键值存储。某些生产环境将会将 `etcd` 部署在主节点基础设施之外，以避免性能问题并提高组件的高可用性。'
- en: 'Worker processes, on the other hand, can run on any node. As we learned with
    Docker Swarm, we can decide to run application workloads on worker and master
    nodes. These are the required components for compute nodes:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，工作进程可以在任何节点上运行。正如我们在 Docker Swarm 中学到的，我们可以决定在工作节点和主节点上运行应用程序工作负载。这些是计算节点所需的组件：
- en: '`kubelet`: This is the core Kubernetes agent component. It will run on any
    cluster node that is able to execute application workloads. This process will
    also ensure that node-assigned Kubernetes workloads are running and are healthy
    (it will only manage pods created within Kubernetes).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubelet`：这是 Kubernetes 的核心代理组件。它将在能够执行应用程序工作负载的任何集群节点上运行。该过程还将确保分配给节点的 Kubernetes
    工作负载正在运行且处于健康状态（它只会管理在 Kubernetes 内创建的 Pods）。'
- en: We are talking about scheduling containers or workloads on a Kubernetes cluster.
    The fact is that we will schedule pods, which are Kubernetes-specific objects.
    Kubernetes will run pods; it will never run standalone containers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论在 Kubernetes 集群上调度容器或工作负载。实际上，我们将调度的是 Pods，Pods 是 Kubernetes 特有的对象。Kubernetes
    会运行 Pods；它永远不会运行独立的容器。
- en: '`kube-proxy`: This component will manage the workload''s network interactions
    using operating system packet filtering and routing features. `kube-proxy` should
    run on any worker node (that is, nodes that run workloads).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy`：该组件将使用操作系统的数据包过滤和路由功能管理工作负载的网络交互。`kube-proxy` 应该在任何工作节点上运行（即运行工作负载的节点）。'
- en: Earlier, we mentioned the container runtime as one of the Kubernetes cluster's
    components. In fact, it is a requirement because Kubernetes itself does not provide
    one. We will use Docker Engine as it is the most widely used engine, and we have
    already discussed it in previous chapters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前面我们提到过容器运行时作为 Kubernetes 集群的一个组件。事实上，它是一个必要的要求，因为 Kubernetes 本身并不提供容器运行时。我们将使用
    Docker 引擎，因为它是最广泛使用的引擎，而且我们已经在前面的章节中讨论过它。
- en: 'The following workflow represents all Kubernetes components distributed on
    five nodes (notice that the master has worker components too and that `etcd` is
    also deployed out of it):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下工作流展示了所有 Kubernetes 组件在五个节点上的分布（注意主节点上也有工作组件，并且 `etcd` 也部署在主节点之外）：
- en: '![](img/698fa4f7-056a-4506-a8aa-de77fe895290.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/698fa4f7-056a-4506-a8aa-de77fe895290.jpg)'
- en: As discussed in [Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml), *Orchestration
    Using Docker Swarm*, external load balancers will provide L4 and L7 routing on
    replicated services. In this case, cluster management components do not use router
    mesh-like services. We will provide high availability for core components using
    replicated processes on different nodes. A virtual IP address will be required
    and we will also use **Fully Qualified Domain Name** (**FQDN**) names for **Transport
    Layer Security** (**TLS**) certificates. This will ensure secure communications
    and access to and from Kubernetes components.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第 8 章](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml)中讨论的，*使用 Docker Swarm 进行编排*，外部负载均衡器将为复制服务提供
    L4 和 L7 路由。在这种情况下，集群管理组件不会使用类似路由器网状服务。我们将通过在不同节点上运行复制进程来为核心组件提供高可用性。我们将需要一个虚拟
    IP 地址，并且还会使用**完全限定域名**（**FQDN**）作为**传输层安全性**（**TLS**）证书。这将确保 Kubernetes 组件之间的安全通信和访问。
- en: 'The following diagram shows the TLS certificates that will be created to ensure
    secure communication between components:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了为确保组件之间的安全通信而创建的 TLS 证书：
- en: '![](img/86153b0f-f81f-4726-a0d3-52bf4519857b.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86153b0f-f81f-4726-a0d3-52bf4519857b.jpg)'
- en: We will use the `kubectl` command line to interact with the Kubernetes cluster,
    and we will always connect to the `kube-apiserver` processes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`kubectl`命令行与 Kubernetes 集群进行交互，并始终连接到`kube-apiserver`进程。
- en: In the next section, we will learn how to implement high-availability Kubernetes
    cluster environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何实现高可用性 Kubernetes 集群环境。
- en: Deploying a Kubernetes cluster with high availability
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署具有高可用性的 Kubernetes 集群
- en: Docker Swarm was easy to implement. To provide high availability, we simply
    changed the node roles to accomplish the required odd number of managers. In Kubernetes,
    this is not so easy; roles cannot be changed, and, usually, administrators do
    not change the initial number of master nodes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 容易实现。为了提供高可用性，我们只需更改节点角色来实现所需的奇数个管理节点。在 Kubernetes 中，这并非如此简单；角色不能更改，通常管理员不会更改主节点的初始数量。
- en: Therefore, installing a Kubernetes cluster with high-availability components
    requires some planning. The good thing here is that Docker Enterprise will deploy
    the cluster for you (since the 2.0 release). We will review this method in [Chapter
    11](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml), *Universal Control Plane*, as
    **Universal Control Plane** (**UCP**) will deploy Kubernetes on top of Docker
    Swarm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，安装具有高可用性组件的 Kubernetes 集群需要一些规划。幸运的是，Docker Enterprise 会为你部署集群（自 2.0 版本起）。我们将在[第
    11 章](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml)中回顾这种方法，因为**统一控制平面**（**UCP**）将基于
    Docker Swarm 部署 Kubernetes。
- en: 'To provide high availability, we will deploy an odd number of control plane
    components. It is usual to deploy `etcd` on three additional nodes. In this scenario,
    nodes would be neither masters nor workers because `etcd` will be deployed out
    of the Kubernetes nodes. We will require access to this external `etcd` from the
    master nodes only. Therefore, in this situation, we will run a cluster of eight
    nodes: three nodes will run `etcd`, three masters nodes will run all of the other
    control plane components (cluster management), and there will be at least two
    workers to provide redundancy if one of them dies. This is appropriate for many
    Kubernetes environments. We isolate `etcd` from the control plane components to
    provide better management performance.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供高可用性，我们将部署奇数个控制平面组件。通常会在另外三个节点上部署`etcd`。在这种情况下，这些节点既不是主节点也不是工作节点，因为`etcd`将部署在
    Kubernetes 节点之外。我们只需要从主节点访问这些外部的`etcd`。因此，在这种情况下，我们将运行一个由八个节点组成的集群：三个节点运行`etcd`，三个主节点运行所有其他控制平面组件（集群管理），并且至少有两个工作节点以提供冗余，以防其中一个宕机。这种配置适用于许多
    Kubernetes 环境。我们将`etcd`与控制平面组件隔离，以提供更好的管理性能。
- en: We can deploy `etcd` on master nodes. This is similar to what we learned about
    Docker Swarm. We can have *pure masters—*running only management components—and
    worker nodes for workloads.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在主节点上部署`etcd`。这与我们在 Docker Swarm 中学到的类似。我们可以拥有*纯主节点*—只运行管理组件—以及用于处理工作负载的工作节点。
- en: Installing Kubernetes is not easy, and there are many software vendors that
    have developed their own KaaS platforms to provide different methods of installation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Kubernetes 并不容易，许多软件供应商已经开发了自己的 KaaS 平台，提供不同的安装方法。
- en: For high availability we will run distributed copies of `etcd`. In this scenario,
    `kube-apiserver` will connect to a list of nodes instead of just one `etcd` node.
    The `kube-apiserver`, `kube-scheduler`, and `kube-controller-manager` processes
    will run duplicated on different master nodes (one instance on each master node).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高可用性，我们将运行 `etcd` 的分布式副本。在这种情况下，`kube-apiserver` 将连接到一组节点，而不是仅连接到一个 `etcd`
    节点。`kube-apiserver`、`kube-scheduler` 和 `kube-controller-manager` 进程将在不同的主节点上运行多个副本（每个主节点上一个实例）。
- en: We will use `kube-apiserver` to manage the cluster. The Kubernetes client will
    connect to this server process using the HTTP/HTTPS protocol. We will use an external
    load balancer to distribute traffic between different replicas running on the
    master nodes. Kubernetes works with the Raft algorithm because `etcd` uses it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `kube-apiserver` 来管理集群。Kubernetes 客户端将通过 HTTP/HTTPS 协议连接到此服务器进程。我们将使用外部负载均衡器在主节点上的不同副本之间分配流量。Kubernetes
    使用 Raft 算法，因为 `etcd` 使用了该算法。
- en: Applications deployed in the cluster will have high availability based on resilience
    by default (just like in Docker Swarm clusters). Once an application is deployed
    with all of its components, if one of them fails, `kube-controller-manager` will
    run a new one. There are different controllers processes, for different deployments
    that are responsible for executing applications based on replicas, on all nodes
    at the same time, and other specific execution situations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在集群中的应用程序将基于弹性默认具有高可用性（就像在 Docker Swarm 集群中一样）。一旦应用程序及其所有组件被部署，如果其中一个组件失败，`kube-controller-manager`
    将启动一个新的实例。有不同的控制器进程，负责根据副本在所有节点上同时执行应用程序以及其他特定的执行情况。
- en: In the next section, we will introduce the pod concept, which is key to understanding
    the differences between Kubernetes and Docker Swarm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍 pod 概念，这是理解 Kubernetes 和 Docker Swarm 之间区别的关键。
- en: Pods, services, and other Kubernetes resources
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pods、服务和其他 Kubernetes 资源
- en: The pod concept is key to understanding Kubernetes. A pod is a group of containers
    that run together. It is very simple. All of these containers share a network
    namespace and storage. It is like a small logical host because we run many processes
    together, sharing the same IP addresses and volumes. The isolation methods that
    we learned about in [Chapter 1](c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml), *Modern
    Infrastructures and Applications with Docker*, are applicable here.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: pod 概念是理解 Kubernetes 的关键。一个 pod 是一组一起运行的容器。它非常简单。所有这些容器共享一个网络命名空间和存储。它就像一个小的逻辑主机，因为我们在一起运行多个进程，共享相同的
    IP 地址和卷。我们在[第 1 章](c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml)《现代基础设施与应用程序使用 Docker》中学到的隔离方法，在这里同样适用，*现代基础设施和应用程序使用
    Docker*。
- en: Pods
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pods
- en: Pods are the smallest scheduling unit in Kubernetes environments. Containers
    within a pod will share the same IP address and can find each other using `localhost`.
    Therefore, assigned ports must be unique within pods. We cannot reuse ports for
    other containers and inter-process communication because processes will run as
    if they were executed on the same logical host. A pod's life relies on the healthiness
    of a container.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是 Kubernetes 环境中最小的调度单元。一个 pod 内的容器将共享相同的 IP 地址，并且可以通过 `localhost` 相互发现。因此，分配的端口在
    pod 内必须是唯一的。我们不能为其他容器和进程间通信重复使用端口，因为进程将像在同一个逻辑主机上执行一样运行。一个 pod 的生命周期依赖于其容器的健康状态。
- en: Pods can be used to integrate full application stacks, but it is true that they
    are usually used with a few containers. In fact, microservices rely on small functionalities;
    therefore, we will run just one container per node. As pods are the smallest Kubernetes
    scheduling unit, we scale pods up and down, not containers. Therefore, complete
    stacks will be replicated if many grouped application components are executed
    together within a pod.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 可以用于集成完整的应用程序堆栈，但它们通常用于少量容器。事实上，微服务依赖于小的功能模块；因此，我们每个节点上只运行一个容器。由于 pod 是
    Kubernetes 中最小的调度单元，因此我们扩展的是 pod，而不是容器。因此，如果许多应用组件在同一个 pod 内一起执行，则完整的堆栈将被复制。
- en: On the other hand, pods allow us, for example, to execute a container in order
    to initialize some special features or properties for another container. Remember
    the *Deploying using Docker Stacks* section from [Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml),
    *Orchestration Using Docker Swarm*? In that lab, we launched a PostgreSQL database
    and we added an initialization script to create a specific database. We can do
    this on Kubernetes using the initial containers within a pod.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Pods 允许我们执行容器，例如为了初始化另一个容器的某些特殊功能或属性。还记得 [第 8 章](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml)
    中的 *使用 Docker Stacks 部署* 部分吗？在那个实验中，我们启动了一个 PostgreSQL 数据库，并添加了一个初始化脚本来创建一个特定的数据库。我们可以在
    Kubernetes 上使用 Pod 中的初始容器来完成这个操作。
- en: Terminating and removing pods will depend on how much time it will take to stop
    or delete all of the containers running within a pod.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 终止和移除 Pod 将取决于停止或删除 Pod 内所有容器所需的时间。
- en: 'The following diagram represents a pod with some containers inside, sharing
    the same IP address and volume, among other features (we will be able to apply
    a special security context to all containers within a pod):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表示一个包含多个容器的 Pod，共享相同的 IP 地址和卷等特性（我们可以为 Pod 中的所有容器应用一个特殊的安全上下文）：
- en: '![](img/f23d1b15-275f-4766-8466-05b44fb71f75.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f23d1b15-275f-4766-8466-05b44fb71f75.jpg)'
- en: Let's now review the service resources on Kubernetes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下 Kubernetes 上的服务资源。
- en: Services
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务
- en: Services have a different meaning in Kubernetes. Services are abstract objects
    for the cluster; we do not schedule services in Kubernetes. They define a logical
    set of pods that work together to serve an application component. We can also
    associate a service with an external resource (endpoint). This service will be
    used inside a cluster like any other, but with external IP addresses and ports,
    for example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 服务在 Kubernetes 中有不同的含义。服务是集群的抽象对象；我们在 Kubernetes 中不会调度服务。它们定义了一组协同工作的 Pod，用于提供应用程序组件。我们还可以将服务与外部资源（端点）关联。这个服务将像集群内的其他服务一样使用，但例如，可以使用外部
    IP 地址和端口。
- en: 'We also use services to publish applications inside and outside a Kubernetes
    cluster. For these purposes, there are different types of services. All of them,
    except headless services, provide internal load balancing between all pod replicas
    for a common service:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用服务来发布应用程序，既可以在 Kubernetes 集群内，也可以在外部发布。为此，存在不同类型的服务。除了无头服务外，所有这些服务都提供 Pod
    副本之间的内部负载均衡，供公共服务使用：
- en: '**Headless**: We use headless services to interface with non-Kubernetes service
    discovery solutions. No virtual IP will be allocated. There will be no load balancing
    or proxy to reach the service''s pods. This behavior is similar to Docker Swarm''s
    DNSRR mode.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Headless**：我们使用无头服务与非 Kubernetes 服务发现解决方案进行交互。不会分配虚拟 IP。也不会有负载均衡或代理来访问服务的
    Pod。这种行为类似于 Docker Swarm 的 DNSRR 模式。'
- en: '**ClusterIP**: This is the default service type. Kubernetes will provide an
    internal virtual IP address chosen from a configurable pool. This will allow only
    internal cluster objects to reach the defined service.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterIP**：这是默认的服务类型。Kubernetes 将提供一个从可配置池中选择的内部虚拟 IP 地址。这样只有集群内部的对象才能访问定义的服务。'
- en: '**NodePort**: NodePort services also receive a virtual IP (ClusterIP), but
    exposed services'' ports will be available on all cluster nodes. Kubernetes will
    route requests to the service''s ClusterIP address, no matter which node received
    them. Therefore, the service''s defined port will be available on `<ANY_CLUSTER_NODE>:<NODEPORT_PORT>`.
    This effectively reminds us of the routing mesh''s behavior on Docker Swarm. In
    this case, we need to add some cluster nodes to external load balancers to reach
    the defined and exposed service''s ports.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodePort**：NodePort 服务也会接收一个虚拟 IP（ClusterIP），但是暴露的服务端口将在所有集群节点上可用。Kubernetes
    将请求路由到服务的 ClusterIP 地址，无论请求到达哪个节点。因此，服务的定义端口将在 `<ANY_CLUSTER_NODE>:<NODEPORT_PORT>`
    上可用。这实际上让我们想起了 Docker Swarm 上的路由网格行为。在这种情况下，我们需要将一些集群节点添加到外部负载均衡器中，以便访问定义并暴露的服务端口。'
- en: '**LoadBalancer**: This service type is available only in a cloud provider''s
    Kubernetes deployment. We expose a service externally using automatically created
    (using the cloud provider''s API integration) load balancers. It uses both a ClusterIP
    virtual IP for internal routing and a NodePort concept for reaching service-defined
    ports from load balancers.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoadBalancer**：此服务类型仅在云服务提供商的 Kubernetes 部署中可用。我们使用自动创建的（通过云服务提供商的 API 集成）负载均衡器将服务暴露到外部。它同时使用
    ClusterIP 虚拟 IP 进行内部路由，并使用 NodePort 概念从负载均衡器访问服务定义的端口。'
- en: '**ExternalName**: This is not very common nowadays because it relies on DNS
    CNAME records and is a new implementation. It is used to add external services,
    out of the Kubernetes cluster. External services will be reachable by their names
    as if they were running inside'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ExternalName**：这种方式现在不太常见，因为它依赖于DNS CNAME记录，并且是新的实现。它用于添加外部服务，位于Kubernetes集群之外。外部服务将通过其名称进行访问，就像它们在集群内部运行一样。'
- en: Kubernetes cluster.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kubernetes集群。
- en: 'The following schema represents the NodePort service type''s usual configuration.
    In this example, the service is reachable on port `7000` from an external load
    balancer, while pods are reachable internally on port `5000`. All traffic will
    be internally load balanced between all of the service''s pod endpoints:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下架构表示NodePort服务类型的常见配置。在此示例中，服务可以通过外部负载均衡器在`7000`端口访问，而Pod则可以在`5000`端口内部访问。所有流量将在服务的所有Pod端点之间进行负载均衡：
- en: '![](img/d68704cd-fb33-4e9a-95f6-0d1afacbed24.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d68704cd-fb33-4e9a-95f6-0d1afacbed24.jpg)'
- en: There are many other resources in Kubernetes. We will take a quick look at some
    of them before going into how we deploy applications on Kubernetes clusters in
    depth.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中还有许多其他资源。我们将在深入了解如何在Kubernetes集群上部署应用之前，快速浏览其中的一些资源。
- en: ConfigMaps and secrets
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置映射和机密
- en: We learned how to distribute the required application information cluster-wide
    with Docker Swarm. Kubernetes also provides solutions for this. We will use ConfigMaps,
    instead of Docker Swarm config objects, and secrets.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何使用Docker Swarm在集群范围内分发所需的应用信息。Kubernetes也提供了类似的解决方案。我们将使用配置映射，代替Docker
    Swarm的配置对象，以及机密。
- en: In both cases, we can use either files or standard input (using the `--from-literal`
    option) to create these resources. The literal option will allow us to create
    these objects using the command line instead of a YAML file.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们可以使用文件或标准输入（使用`--from-literal`选项）来创建这些资源。字面量选项允许我们通过命令行创建这些对象，而不是使用YAML文件。
- en: The Kubernetes `kubectl` command line provides two different approaches to create
    cluster resources/objects (imperative and declarative). We will use either command-line
    generators or resource files, usually in YAML format. The first method is usually
    known as imperative, but is not available for all kinds of resources, and using
    files is known as declarative. This will apply to all Kubernetes resources; therefore,
    we will be able to use either `kubectl create pod` with arguments or `kubectl
    create -f <POD_DEFINITION_FILE_IN_YAML_FORMAT>`. We can export a previously generated
    command-line object into YAML format easily to allow resource reproducibility,
    to save its definition somewhere safe.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的`kubectl`命令行提供了两种不同的方法来创建集群资源/对象（命令式和声明式）。我们将使用命令行生成器或资源文件，通常是YAML格式。第一种方法通常被称为命令式，但并非所有资源都可以使用这种方法，使用文件的方法被称为声明式。这适用于所有Kubernetes资源；因此，我们可以使用带参数的`kubectl
    create pod`，或者使用`kubectl create -f <POD_DEFINITION_FILE_IN_YAML_FORMAT>`。我们可以轻松地将之前生成的命令行对象导出为YAML格式，以便实现资源的可重现性，并将其定义保存在某个安全的地方。
- en: ConfigMaps and secrets allow us to decouple configurations from image content
    without using unsecured runtime-visible variables or local files shared on some
    nodes. We will use secrets for sensitive data, while ConfigMaps will be used for
    common configurations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 配置映射和机密允许我们将配置从镜像内容中解耦，而无需使用不安全的运行时可见变量或某些节点上共享的本地文件。我们将使用机密来处理敏感数据，而配置映射则用于常见的配置。
- en: Namespaces
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名空间
- en: Namespaces can be understood as scopes based on names. They allow us to isolate
    resources between them. The names of resources are unique within each namespace.
    Resources can only be within one namespace; therefore, we can divide access to
    them using namespaces.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间可以理解为基于名称的作用域。它们允许我们在不同命名空间之间隔离资源。资源的名称在每个命名空间内是唯一的。资源只能存在于一个命名空间内；因此，我们可以使用命名空间来划分对资源的访问。
- en: One of the simplest uses for namespaces is to limit user access and the usage
    of Kubernetes' objects and resources' quotas. Based on namespaces, we will allow
    a specific set of host resources for users. For example, different groups of users
    or teams will have their own resources and a quota that will limit their environment's
    behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间的最简单用途之一是限制用户访问和Kubernetes对象及资源配额的使用。基于命名空间，我们将为用户提供一组特定的主机资源。例如，不同的用户组或团队将拥有自己的资源和配额，限制他们环境中的行为。
- en: Persistent volumes
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久卷
- en: We learned about volumes in [Chapter 4](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml),
    *Container Persistency and Networking*. In Kubernetes, volumes are attached to
    pods, not containers; therefore, volumes will follow a pod's life cycle.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 4 章](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml)《*容器持久性和网络*》中了解了卷。在 Kubernetes
    中，卷是附加到 pod 的，而不是容器；因此，卷将跟随 pod 的生命周期。
- en: 'There are many volume types in Kubernetes and we can mix them inside pods.
    Volumes are available to any container running within a pod. There are volumes
    specially designed for cloud providers and storage solutions that are available
    in most data centers. Let''s review a couple of interesting, commonly used volumes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有许多不同类型的卷，我们可以在 pods 内部混合使用它们。卷对运行在 pod 中的任何容器都可用。有些卷是专为云服务提供商设计的，还有些卷是大多数数据中心中可用的存储解决方案。让我们回顾一些有趣的、常用的卷：
- en: '`emptyDir`: This volume is created when a pod is assigned to a node and is
    removed with the pod. It starts off empty and is usually used to share information
    between containers running within a pod.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emptyDir`: 这个卷在 pod 被分配到节点时创建，并随着 pod 被删除。它一开始是空的，通常用于在同一个 pod 内的容器之间共享信息。'
- en: '`hostPath`: We have already used this type of volume on Docker. These volumes
    allow us to mount a file or directory from the host into pods.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hostPath`: 我们已经在 Docker 中使用过这种类型的卷。这些卷允许我们将主机上的文件或目录挂载到 pods 中。'
- en: Each volume type has its own special options to enable its unique features.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每种卷类型都有自己独特的选项来启用其独特功能。
- en: These volumes are designed to be used within pods, but they are not prepared
    for Kubernetes clustering and storing permanent data. For these situations, we
    use **Persistent Volumes** (**PVs**).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些卷设计用于在 pods 内部使用，但它们并没有为 Kubernetes 集群和存储永久数据做好准备。对于这些情况，我们使用**持久化卷**（**PVs**）。
- en: PVs allow us to abstract how storage is provided. It doesn't matter how storage
    hosts arrive in the cluster; we only care about how to use them. A PV is provisioned
    by an administrator, for example, and users are allowed to use it. PVs are Kubernetes
    resources; hence, we can associate them with namespaces and they have their own
    life cycle. They are pod-independent.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: PV 使我们能够抽象存储是如何提供的。无论存储主机是如何进入集群的，我们只关心如何使用它们。PV 由管理员预配置，例如，用户可以使用它。PV 是 Kubernetes
    资源，因此，我们可以将它们与命名空间关联，并且它们有自己的生命周期。它们是与 pod 独立的。
- en: PVs are requested by **Persistent Volume Claims** (**PVCs**). Therefore, PVCs
    consume defined PVs. This is the way to associate a pod with a PV.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: PV 是通过**持久化卷声明**（**PVCs**）请求的。因此，PVC 消耗已定义的 PV。这是将 pod 与 PV 关联起来的方式。
- en: Therefore, PVCs allow users to consume storage. We can designate storage according
    to internal properties, such as speed, how it is provided on the hosts, and more,
    and allow dynamic provisioning using **storage classes**. With these objects,
    we describe all of the storage solutions available in the cluster with their properties
    as profiles and Kubernetes prepares the persistent storage to be used.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PVC 允许用户使用存储。我们可以根据存储的内部属性（如速度、主机上的提供方式等）来指定存储，并通过**存储类**实现动态配置。通过这些对象，我们描述集群中所有可用存储解决方案的属性作为配置文件，Kubernetes
    会为使用这些存储做好持久化存储准备。
- en: It is important to know that we can decide the behavior of the PV data once
    pods die. The **retail reclaim** policy describes what to do with volumes and
    their content once pods no longer use them. Therefore, we will choose between
    deleting the volume, retaining the volume and its content, and recycling it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 需要知道的是，我们可以决定在 pod 死亡后 PV 数据的行为。**回收策略**描述了在 pod 不再使用卷及其内容时应该怎么做。因此，我们可以选择删除卷、保留卷及其内容，或回收它。
- en: We can say that PVs are Kubernetes cluster resources designated for application
    persistent storage and PVCs are the requests to use them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，PV（持久化卷）是 Kubernetes 集群资源，用于应用程序持久化存储，而 PVC（持久化卷声明）是请求使用这些资源的方式。
- en: Storage classes are a new feature that allow administrators to integrate dynamic
    provisions into our cluster. This helps us to provide storage without having to
    manually configure each volume. We will just define profiles and features for
    storage and the provisioners will give the best solution for the required volume.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类是一项新功能，允许管理员将动态配置集成到我们的集群中。这帮助我们在不需要手动配置每个卷的情况下提供存储。我们只需定义存储的配置文件和特性，存储提供者就会为所需的卷提供最佳解决方案。
- en: In the next section, we will learn how to deploy workloads on Kubernetes clusters.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何在 Kubernetes 集群上部署工作负载。
- en: Deploying orchestrated resources
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署编排资源
- en: Deploying workloads in Kubernetes is easy. We will use `kubectl` to specify
    the resources to be created and interact with `kube-apiserver`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署工作负载很简单。我们将使用 `kubectl` 来指定要创建的资源，并与 `kube-apiserver` 进行交互。
- en: As mentioned earlier, we can use the command line to either use built-in generators
    or YAML files. Depending on the Kubernetes API version, some options may not be
    available, but we will assume Kubernetes 1.11 or higher.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用命令行来使用内置生成器或 YAML 文件。根据 Kubernetes API 的版本，某些选项可能不可用，但我们假设 Kubernetes
    版本为 1.11 或更高。
- en: In this chapter, all examples use Kubernetes 1.14 because it is the version
    available on the current Docker Enterprise release, 3.0, at the time of writing
    this book.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，所有示例使用 Kubernetes 1.14，因为这是在编写本书时，当前 Docker Enterprise 3.0 版本上可用的版本。
- en: Let's start by creating a simple pod. We will review both options—imperative,
    using the command-line, and declarative, using YAML manifests.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从创建一个简单的 Pod 开始。我们将回顾两种方式——命令行的命令式方式和使用 YAML 清单的声明式方式。
- en: 'Using the pod generator, we will run the `kubectl run --generator=run-pod/v1`
    command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pod 生成器，我们将运行 `kubectl run --generator=run-pod/v1` 命令：
- en: '[PRE0]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using a YAML definition file, we will describe all of the required properties
    of the pod:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 YAML 定义文件，我们将描述 Pod 所需的所有属性：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To deploy this `.yaml` definition file, we will just run `kubectl create -f
    <YAML_DEFINITION_FILE>`. This will create all of the defined resources in the
    file on the specified namespace. Because we are not using an argument to specify
    a namespace, they will be created on the user-defined one. In our case, we are
    using the `default` namespace by default.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署这个 `.yaml` 定义文件，我们只需运行 `kubectl create -f <YAML_DEFINITION_FILE>`。这将会在指定的命名空间中创建文件中定义的所有资源。由于我们没有使用参数来指定命名空间，它们将在用户定义的命名空间中创建。在我们的例子中，默认使用的是
    `default` 命名空间。
- en: We can define the namespace either on each YAML file or by using a command-line
    argument. The latter will overwrite the YAML definition.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在每个 YAML 文件中定义命名空间，或者通过命令行参数来定义。后者会覆盖 YAML 定义。
- en: Both examples will create the same pod, with one container inside, running an
    `nginx:alpine` image.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 两个示例都会创建相同的 Pod，Pod 内部有一个容器，运行 `nginx:alpine` 镜像。
- en: Take care when using the `args` and `command` definitions on Kubernetes. These
    keys differ from the definitions we used for Docker containers or images. Kubernetes'
    `command` will represent `ENTRYPOINT`, while `args` will represent the container/image
    `CMD` definition.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中使用 `args` 和 `command` 定义时需要小心。这些键与我们用于 Docker 容器或镜像的定义不同。Kubernetes
    的 `command` 表示 `ENTRYPOINT`，而 `args` 表示容器/镜像的 `CMD` 定义。
- en: 'We can kill this pod by simply removing it using `kubectl delete`. To get a
    list of pods running within a namespace, we will use `kubectl get pods`. If the
    namespace is omitted on the `kubectl` execution, the user-assigned namespace will
    be used:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地删除它来终止这个 Pod，命令是 `kubectl delete`。要获取在命名空间中运行的 Pod 列表，我们将使用 `kubectl
    get pods`。如果在 `kubectl` 执行时省略了命名空间，将使用用户指定的命名空间：
- en: '[PRE2]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But this just created a simple pod; we cannot create more NGINX replicas with
    this kind of resource. To use replicas, we will use ReplicaSets instead of single
    pods.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是创建了一个简单的 Pod；我们无法使用这种资源创建更多的 NGINX 副本。要使用副本，我们将使用 ReplicaSets，而不是单个 Pod。
- en: We will set up a pod template section and pod selectors to identify which deployed
    pods belong to this `ReplicaSet` resource within a new YAML file. This will help
    the controller to watch the pods' health.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置一个 Pod 模板部分和 Pod 选择器，以识别哪些已部署的 Pod 属于这个 `ReplicaSet` 资源，并将其写入新的 YAML 文件。这将帮助控制器监控
    Pod 的健康状态。
- en: 'Here, to the previous pod definition, we add a `template` section and a `selector`
    key with labels:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，针对之前的 Pod 定义，我们添加了一个 `template` 部分和一个带有标签的 `selector` 键：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Therefore, we created three replicas using the same pod definition as we did
    earlier. This pod''s definition was used as a template for all of the replicas.
    We can review all of the resources deployed using `kubectl get all`. In the following
    command, we filter the results to retrieve only resources with the `example` label
    and the `myfirstrs` value:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用与之前相同的 Pod 定义创建了三个副本。这个 Pod 的定义被用作所有副本的模板。我们可以使用 `kubectl get all` 来查看所有已部署的资源。在以下命令中，我们过滤结果，只获取带有
    `example` 标签和 `myfirstrs` 值的资源：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each replica will have the same prefix name, but its own ID will be part of
    the name. This uniquely identifies the resource in the Kubernetes cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个副本将具有相同的前缀名称，但其 ID 将成为名称的一部分。这使得该资源在 Kubernetes 集群中唯一标识。
- en: We are using `kubectl get all -l <KEY=VALUE>` to filter all of the resources
    we labeled with the `example` key and the `myfirstrs` value.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 `kubectl get all -l <KEY=VALUE>` 来过滤所有我们用 `example` 键和 `myfirstrs` 值标记的资源。
- en: 'We can use `DaemonSet` to deploy a replica on each node in the cluster, just
    as we did with Docker Swarm''s global services:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`DaemonSet`在集群的每个节点上部署副本，就像我们在 Docker Swarm 的全局服务中所做的那样：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now review the pod distribution again using `kubectl get all`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以再次使用 `kubectl get all` 回顾 pod 的分布情况。
- en: 'Notice that we added the container''s resource limits and resource requests.
    The `limits` key allows us to specify resource limits for each container. On the
    other hand, `requests` informs the scheduler about the minimal resources required
    to run this component. A pod will not be able to run on a node if there are not
    enough resources to achieve the requested CPU, memory, and more. If any containers
    exceed their limits, they will be terminated:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们添加了容器的资源限制和资源请求。`limits` 键允许我们为每个容器指定资源限制。另一方面，`requests` 向调度器提供有关运行此组件所需最小资源的信息。如果没有足够的资源满足请求的
    CPU、内存等要求，pod 将无法在节点上运行。如果任何容器超出了它们的限制，它们将被终止：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `Deployment` resource is a higher-level concept, as it manages `ReplicaSet`
    and allows us to issue application component updates. It is recommended that you
    use `Deployment` instead of `ReplicaSet`. We will again use the `template` and
    `select` sections:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`Deployment` 资源是一个更高级的概念，因为它管理 `ReplicaSet` 并允许我们发布应用程序组件的更新。推荐使用`Deployment`而不是`ReplicaSet`。我们将再次使用`template`和`select`部分：'
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Therefore, the deployment will run three replicas of `nginx:alpine`, distributed
    again on cluster nodes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，部署将运行三个`nginx:alpine`的副本，这些副本将再次分布在集群节点上：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that replicas are only running on some nodes. This is because we have
    some taintson the other nodes (some Kubernetes deployments avoid workloads on
    master nodes by default). Taints and tolerations help us to allow the scheduling
    of pods on only specific nodes. In this example, the master node will not run
    a workload, although it also has a worker role (it runs the Kubernetes worker
    processes that we learned about, `kubelet` and `kube-proxy`). These features remind
    us of Docker Swarm's node availability concepts. In fact, we can also execute
    `kubectl cordon <NODE>` to set a node as non-schedulable.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，副本仅在某些节点上运行。这是因为其他节点上存在一些污点（默认情况下，某些 Kubernetes 部署会避免在主节点上运行工作负载）。污点和容忍帮助我们只允许在特定节点上调度
    pods。在此示例中，主节点将不会运行工作负载，尽管它也有一个工作节点角色（它运行我们学习过的 Kubernetes 工作进程，即 `kubelet` 和
    `kube-proxy`）。这些特性让我们想起了 Docker Swarm 的节点可用性概念。实际上，我们也可以执行 `kubectl cordon <NODE>`
    将节点设置为不可调度。
- en: 'This chapter is a brief introduction to the main concepts of Kubernetes. We
    highly recommend that you view the Kubernetes documentation for further information:
    [https://kubernetes.io](https://kubernetes.io).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是 Kubernetes 主要概念的简要介绍。我们强烈建议您查看 Kubernetes 文档以获取更多信息：[https://kubernetes.io](https://kubernetes.io)。
- en: We can set replication based on a pod's performance and limits. This is known
    as **autoscaling**, and it is an interesting feature that is not available in
    Docker Swarm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据 pod 的性能和限制设置副本。这称为**自动扩缩**，它是 Docker Swarm 中没有的一个有趣特性。
- en: When an application's replicated components require persistence, we use another
    kind of resource. StatefulSets guarantee the order and uniqueness of pods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序的副本组件需要持久性时，我们使用另一种资源类型。StatefulSets 保证 pods 的顺序和唯一性。
- en: Now that we know how to deploy applications, let's review how Kubernetes manages
    and deploys a network locally with components distributed on different nodes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何部署应用程序，让我们回顾一下 Kubernetes 如何本地管理和部署网络，并将组件分布在不同节点上。
- en: Kubernetes networking
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: 'Kubernetes, like any other orchestrator, provides local and distributed networking.
    There are a few important communication assumptions that Kubernetes has to accomplish:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和其他编排工具一样，提供本地和分布式网络。Kubernetes 要完成几个重要的通信假设：
- en: Container-to-container communication
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器到容器的通信
- en: Pod-to-pod communication
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 到 Pod 的通信
- en: Pod-to-service communication
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 到 Service 的通信
- en: User access and communication between external or internal applications
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户访问和外部或内部应用程序之间的通信
- en: Container-to-container communication is easy because we learned that containers
    within a pod share the same IP and network namespace.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 容器到容器的通信很简单，因为我们了解到，同一 pod 内的容器共享相同的 IP 和网络命名空间。
- en: We know that each pod gets its own IP address. Therefore, Kubernetes needs to
    provide routing and accessibility to and from pods running on different hosts.
    Following the Docker concepts that we learned about in [Chapter 4](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml),
    *Container Persistency and Networking*, Kubernetes also uses bridge networking
    for pods running on the same host. Therefore, all pods running on a host will
    be able to talk with each other using bridge networking.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道每个 pod 都有自己的 IP 地址。因此，Kubernetes 需要提供路由功能，以便从不同主机上运行的 pod 之间进行访问和通信。按照我们在
    [第 4 章](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml)《*容器持久性与网络*》中学到的 Docker 概念，Kubernetes
    也使用桥接网络为在同一主机上运行的 pod 提供服务。因此，所有在同一主机上运行的 pod 都能通过桥接网络相互通信。
- en: Remember how Docker allowed us to deploy different bridge networks on a single
    host? This way, we were able to isolate applications on a host using different
    networks. Using this local concept, overlaying networks on a Docker Swarm cluster
    also deployed bridged interfaces. And these interfaces will be connected using
    tunnels created between hosts using VXLAN. Isolation was something simple on Docker
    standalone hosts and Docker Swarm. Docker Engine had to manage all of the backstage
    magic to make this work with firewall rules and routing, but overlay networking
    is available out of the box.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得 Docker 如何让我们在单一主机上部署不同的桥接网络吗？通过这种方式，我们可以使用不同的网络在主机上隔离应用程序。利用这一本地概念，覆盖网络在
    Docker Swarm 集群上也会部署桥接接口。这些接口将通过在主机之间创建的 VXLAN 隧道进行连接。在 Docker 独立主机和 Docker Swarm
    上，隔离变得简单。Docker 引擎必须管理所有后台的操作，使其与防火墙规则和路由配合工作，但覆盖网络是开箱即用的。
- en: Kubernetes provides a simpler approach. All pods run on the same network; hence,
    every pod will see other pods within the same host. In fact, we can go further—pods
    are locally accessible from hosts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了一种更简单的方法。所有 pod 都运行在同一网络中，因此每个 pod 都能看到同一主机上的其他 pod。实际上，我们可以更进一步——pod
    可以从主机上本地访问。
- en: 'Let''s consider this concept with a couple of pods. We will run `example-webserver`
    and `example-nettools` at the same time, executing simple `nginx:alpine` and `frjaraur/nettools:minimal`
    (this is a small alpine image with some helpful network tools) pods. First, we
    will create a deployment for `example-webserver` using `kubectl create deployment`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用几个 pod 来考虑这个概念。我们将同时运行 `example-webserver` 和 `example-nettools`，执行简单的 `nginx:alpine`
    和 `frjaraur/nettools:minimal`（这是一个带有一些有用网络工具的小 alpine 镜像）pod。首先，我们将使用 `kubectl
    create deployment` 为 `example-webserver` 创建一个部署：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We review the pod''s IP address using `kubectl get pods`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `kubectl get pods` 查看 pod 的 IP 地址：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we said, `localhost` communications to the pod will work. Let''s try a simple
    `ping` command from the host to the pod''s IP address:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，`localhost` 与 pod 之间的通信是有效的。让我们从主机向 pod 的 IP 地址尝试一个简单的 `ping` 命令：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Additionally, we can also have access to its running `nginx` process. Let''s
    try `curl` using the pod''s IP again, but this time, we will use port `80`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以访问其正在运行的 `nginx` 进程。让我们再次尝试使用 pod 的 IP 地址进行 `curl`，但这次我们将使用端口 `80`：
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Therefore, the host can communicate with all of the pods running on top of Docker
    Engine.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，主机可以与所有在 Docker 引擎上运行的 pod 进行通信。
- en: 'We can get a pod''s IP address using `jsonpath`, to format the pod''s information
    output, which is very interesting when we have hundreds of pods: `kubectl get
    pod example-webserver -o jsonpath=''{.status.podIP}''`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `jsonpath` 获取 pod 的 IP 地址，以格式化 pod 的信息输出，当我们有数百个 pod 时，这非常有用：`kubectl
    get pod example-webserver -o jsonpath='{.status.podIP}'`。
- en: 'Let''s execute an interactive pod with the aforementioned `frjaraur/nettools:minimal`image.
    We will use `kubectl run --generator=run-pod/v1` to execute this new pod. Notice
    that we added `-ti -- sh` to run an interactive shell within this pod. From this
    pod, we will run `curl` again, connecting to the `example-webserver` pod''s IP
    address:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行一个交互式 pod，使用前述的 `frjaraur/nettools:minimal` 镜像。我们将使用 `kubectl run --generator=run-pod/v1`
    来执行这个新的 pod。注意，我们添加了 `-ti -- sh` 来在这个 pod 中运行一个交互式 shell。从这个 pod 中，我们将再次运行 `curl`，连接到
    `example-webserver` pod 的 IP 地址：
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have successfully accessed the deployed `example-webserver` pod using `ping`
    and `curl`, sending some requests to its `nginx` running process. It is clear
    that both containers can see each other.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地使用 `ping` 和 `curl` 访问了部署的 `example-webserver` pod，并向其运行的 `nginx` 进程发送了一些请求。很明显，两个容器可以相互看到。
- en: 'There is something even more interesting in this example: we have not reviewed
    where these pods are running. In fact, they are running on different hosts, as
    we can read from the `kubectl get pods -o wide` command''s output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，还有一个更有趣的地方：我们还没有查看这些 pod 是在哪些主机上运行的。事实上，它们运行在不同的主机上，正如我们从 `kubectl get
    pods -o wide` 命令的输出中可以看到的那样：
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Networking between hosts is controlled by another component that will allow
    these distributed communications. In this case, this component is Calico, which
    is a **container network interface** (**CNI**) applied to this Kubernetes cluster.
    The Kubernetes network model provides a flat network (all pods are distributed
    on the same network), and data plane networking is based on interchangeable plugins.
    We will use the plugin that best affords all of the required features in our environment.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 主机之间的网络通信由另一个组件控制，这个组件将允许这些分布式通信。在此情况下，这个组件是 Calico，它是应用于此 Kubernetes 集群的 **容器网络接口**
    (**CNI**) 。Kubernetes 的网络模型提供了一个平面网络（所有 pod 都分布在同一网络上），而数据平面网络基于可互换的插件。我们将使用最适合我们环境所需特性的插件。
- en: There are other CNI implementations apart from Calico, such as Flannel, Weave,
    Romana, Cillium, and more. Each one provides its own features and host-to-host
    implementations. For example, Calico uses **Border Gateway Protocol** (**BGP**)
    to route real container IP addresses inside the cluster. Once a CNI is deployed,
    all of the container IP addresses will be managed by its implementation. They
    are usually deployed at the beginning of a Kubernetes cluster implementation.
    Calico allows us to implement network policies, which are very important to ensure
    security in this flat network where every pod sees other pods.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Calico 之外，还有其他 CNI 实现，如 Flannel、Weave、Romana、Cilium 等。每个实现都提供自己的特性和主机到主机的实现。例如，Calico
    使用 **边界网关协议** (**BGP**) 来路由集群内的真实容器 IP 地址。一旦部署了 CNI，所有容器 IP 地址将由其实现进行管理。它们通常在
    Kubernetes 集群实现的开始阶段进行部署。Calico 允许我们实施网络策略，这对于确保安全非常重要，因为在这个平面网络中，每个 pod 都能看到其他
    pod。
- en: We have not looked at any service networking yet, which is also important here.
    If a pod dies, a new IP will be allocated, hence access will be lost on the previous
    IP address; that is why we use services. Remember, services are logical groupings
    of pods, usually with a virtual IP address. This IP address will be assigned from
    another pool of IP addresses (the service IP addresses pool). Pods and services
    do not share the same IP address pool. A service's IP will not change when new
    pods are recreated.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有查看任何服务网络设置，这在这里也很重要。如果一个 pod 死掉了，新的 IP 地址将会分配，从而导致之前的 IP 地址无法访问；这就是我们使用服务的原因。记住，服务是
    pod 的逻辑分组，通常具有虚拟 IP 地址。这个 IP 地址将从另一个 IP 地址池中分配（即服务 IP 地址池）。Pod 和服务不会共享同一个 IP 地址池。当新的
    pod 被重新创建时，服务的 IP 地址不会改变。
- en: Service discovery
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'Let''s create a service associated with the currently deployed `example-webserver`
    deployment. We''ll use `kubectl expose`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个与当前已部署的 `example-webserver` 部署相关联的服务。我们将使用 `kubectl expose`：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We could have done this using either `kubectl create service` (imperative format)
    or a YAML definition file (declarative format). We used `kubectl expose` because
    it''s simpler to quickly publish any kind or resource. We can review a service''s
    IP addresses using `kubectl get services`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用 `kubectl create service`（命令式格式）或 YAML 定义文件（声明式格式）来完成这项工作。我们使用了 `kubectl
    expose`，因为它更简单，可以快速发布任何类型的资源。我们可以使用 `kubectl get services` 查看服务的 IP 地址：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Remember that we define the services associated with pods using selectors.
    In this case, the service will group all pods with the `app` label and the `example-webserver`
    value. This label was automatically created because we created `Deployment`. As
    a result, all pods grouped for this service will be accessible on the `10.98.107.31`
    IP address and the internal TCP port `80`. We defined which pod''s port will be
    associated with this service—in both cases, we set port `80`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们通过选择器定义与 pod 相关联的服务。在此情况下，服务将把所有带有 `app` 标签和 `example-webserver` 值的 pod
    归为一组。这个标签是自动创建的，因为我们创建了 `Deployment`。因此，所有为此服务分组的 pod 都可以通过 `10.98.107.31` IP
    地址和内部 TCP 端口 `80` 进行访问。我们定义了哪个 pod 的端口会与此服务关联——在这两种情况下，我们设置了端口 `80`：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It is accessible, as expected. Kubernetes' internal network has published this
    service on the defined ClusterIP address.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 它按预期可访问。Kubernetes 的内部网络已将此服务发布到定义的 ClusterIP 地址。
- en: Because we created this service as `NodePort`, a random port has been associated
    with the service. In this case, it is port `30951`. As a result, requests will
    be routed to the application's pods within the cluster when we reach the cluster
    nodes' IP addresses in the randomly chosen port.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们创建了一个 `NodePort` 服务，所以该服务已与一个随机端口关联。在这种情况下，它是端口 `30951`。因此，当我们通过随机选择的端口访问集群节点的
    IP 地址时，请求将被路由到集群内应用程序的 pod。
- en: '`NodePort` ports are assigned randomly by default, but we can set them manually
    in the range between `30000` and `32767`.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort` 端口默认是随机分配的，但我们可以手动设置它们，范围在 `30000` 到 `32767` 之间。'
- en: 'Let''s verify this feature. We will send some requests to the port that is
    listening on cluster nodes. In this example, we''ll use the `curl` command on
    the local `0.0.0.0` IP address and port `30951` on various nodes:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证这个功能。我们将向集群节点上监听的端口发送一些请求。在这个例子中，我们将使用 `curl` 命令通过本地 `0.0.0.0` IP 地址和端口
    `30951` 在不同的节点上进行测试：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Communication between pods happens even if they are not running on the same
    node. The following output shows that pods are not running in either `node1` or
    `node3`. The application''s pod is running on `node2`. The internal routing works:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 pod 没有运行在同一节点上，它们之间也可以进行通信。以下输出显示 pod 并没有运行在 `node1` 或 `node3` 上，应用程序的 pod
    运行在 `node2` 上，内部路由正常工作：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'There is something more interesting, though—services create a DNS entry with
    their names following this pattern:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些更有趣的事情——服务会创建一个以其名称为基础的 DNS 记录，格式如下：
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In our example, we have not used a namespace or a domain. The service resolution
    will be simple: `example-webserver.default.svc.cluster.local`. This resolution
    is only available in the Kubernetes cluster by default. Therefore, we can test
    this resolution by executing a pod with the `host` or `nslookup` tools. We will
    attach our terminal interactively to the running `example-nettools`pod using `kubectl
    attach` and run `host` and `curl` to test the DNS resolution:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们没有使用命名空间或域名。服务解析将是简单的：`example-webserver.default.svc.cluster.local`。这种解析默认只有在
    Kubernetes 集群内可用。因此，我们可以通过执行一个 pod 并使用 `host` 或 `nslookup` 工具来测试此解析。我们将通过 `kubectl
    attach` 交互式连接到运行中的 `example-nettools` pod，并运行 `host` 和 `curl` 来测试 DNS 解析：
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have confirmed that the service has a DNS entry that is reachable by any
    other Kubernetes cluster resource. We have also published the service using `NodePort`,
    so it is accessible on any node IP address. We could have an external load balancer
    routing requests to this deployed service on any cluster node's IP address and
    a chosen (or manually set) port. This port will be fixed for this service until
    it is removed.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确认该服务具有一个可以被任何其他 Kubernetes 集群资源访问的 DNS 记录。我们还通过 `NodePort` 发布了该服务，因此它可以通过任何节点的
    IP 地址访问。我们可以通过外部负载均衡器将请求路由到集群中任一节点的 IP 地址以及选择的（或手动设置的）端口。此端口将在该服务存在期间保持固定，直到它被移除。
- en: Notice that we used `kubectl attach example-nettools -c example-nettools -i
    -t` to reconnect to a running pod left in the background.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用 `kubectl attach example-nettools -c example-nettools -i -t` 重新连接到一个在后台运行的
    pod。
- en: In the next section, we will learn how scaling will change the described behavior.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习扩容如何改变描述的行为。
- en: Load balancing
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'If we now scale up to three replicas, without changing anything on the deployed
    service, we will add load balancing features. Let''s scale up using `kubectl scale`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在将副本数扩展到三个，而不更改任何已部署的服务，我们将添加负载均衡功能。让我们使用 `kubectl scale` 来扩容：
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now we will have three running instances or pods for the `example-webserver`
    deployment.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将有三个运行中的实例或 pod 来支持 `example-webserver` 部署。
- en: 'Notice that we have scaled from the command line using the resource''s type
    and its name: `kubectl scale --replicas=<NUMBER_OF_REPLICAS> <RESOURCE_TYPE>/<NAME>`.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经通过命令行使用资源类型和名称来进行扩缩容：`kubectl scale --replicas=<NUMBER_OF_REPLICAS>
    <RESOURCE_TYPE>/<NAME>`。
- en: 'We can review deployment pods using `kubectl get pods` with the associated
    label:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `kubectl get pods` 和相关标签来查看部署的 pod：
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we now test the service''s access again, we will reach each one of the three
    replicas. We execute the next simple loop to reach the service''s backend pods
    five times:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在再次测试服务的访问，我们将访问到三个副本中的每一个。我们执行下一个简单的循环，以便五次访问服务的后端 pod：
- en: '[PRE24]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we review one of the deployed pod''s logs using `kubectl logs`, we will
    notice that not all requests were logged. Although we made more than two requests
    using the service''s IP address, we just logged a few:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `kubectl logs` 查看部署的 pod 日志，我们会发现并不是所有请求都被记录。尽管我们使用服务的 IP 地址发出了超过两个请求，但日志中只记录了几个请求：
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Only one-third of the requests are logged on each pod; therefore, the internal
    load balancer is distributing the traffic between all available applications'
    pods. Internal load balancing is deployed by default between all pods associated
    with a service.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 每个pod仅记录了每个请求的三分之一；因此，内部负载均衡器在所有可用应用程序的pod之间分发流量。内部负载均衡默认在与服务关联的所有pod之间部署。
- en: As we have seen, Kubernetes provides flat networks for pods and services, simplifying
    networking and internal application accessibility. On the other hand, it is insecure
    because any pod can reach any other pods or services. In the next section, we
    will learn how to avoid this situation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Kubernetes为pod和服务提供了扁平网络，简化了网络和内部应用的可访问性。另一方面，由于任何pod都可以访问任何其他pod或服务，因此它是不安全的。在接下来的部分，我们将学习如何避免这种情况。
- en: Network policies
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络策略
- en: Network policies define rules to allow communication between groups of pods
    and other components. Using labels, we apply specific rules to matching pods for
    ingress and egress traffic on defined ports. These rules can be set using IP ranges,
    namespaces, or even other labels to include or exclude resources.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略定义了允许组内pod和其他组件通信的规则。使用标签，我们可以为匹配的pod应用特定规则，用于定义端口上的入口和出口流量。这些规则可以使用IP范围、命名空间，甚至其他标签来包含或排除资源。
- en: Network policies are applied using network plugins; therefore, the CNI deployed
    on our cluster must support them. For example, Calico supports `NetworkPolicy`
    resources.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略是通过网络插件应用的；因此，我们集群上部署的CNI必须支持它们。例如，Calico支持`NetworkPolicy`资源。
- en: We will be able to define default rules to all pods in the cluster, isolating
    all internet traffic, for example, or a defined group of hosts.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将能够为集群中所有pod定义默认规则，隔离所有Internet流量，例如，或一组定义的主机。
- en: 'This YAML file represents an example of a `NetworkPolicy` resource applying
    ingress and egress traffic rules:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这个YAML文件代表了一个应用入口和出口流量规则的`NetworkPolicy`资源示例：
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this example, we will apply defined ingress and egress rules to all pods
    including the `tier` label with the `database` value.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将为包括具有`database`值的`tier`标签的所有pod应用定义的入口和出口规则。
- en: The ingress rule allows traffic from any pod on the same namespace with the
    `tier` label and the `frontend`value. All IP addresses in subnet `172.17.10.0/24`
    will also be allowed to access defined `database` pods.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 入口规则允许来自同一命名空间中具有`tier`标签和`frontend`值的任何pod的流量。也将允许访问定义的`database` pods上的子网`172.17.10.0/24`中的所有IP地址。
- en: The egress rule allows traffic from defined `database` pods to port `5978` on
    all IP addresses on subnet `10.0.0.0/24`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 出口规则允许从定义的`database` pods到子网`10.0.0.0/24`上所有IP地址的端口`5978`的流量。
- en: 'If we do not apply a `NetworkPolicy` resource to a namespace, all traffic is
    allowed. We can change this behavior using `podSelector: {}`. This will match
    all pods in the namespace. For example, to disallow all egress traffic, we can
    use the following `NetworkPolicy` YAML definition:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们没有在命名空间中应用`NetworkPolicy`资源，那么所有流量都是允许的。我们可以使用`podSelector: {}`来改变这种行为。这将匹配命名空间中的所有pod。例如，为了禁止所有出口流量，我们可以使用以下`NetworkPolicy`
    YAML定义：'
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So, we have learned that we can ensure security even on a Kubernetes flat network
    with `NetworkPolicy` resources. Let's review the ingress resources.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学到，即使在Kubernetes的扁平网络上，我们也可以通过`NetworkPolicy`资源确保安全。让我们来回顾一下入口资源。
- en: Publishing applications
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发布应用程序
- en: Ingress resources help us to publish applications deployed on Kubernetes clusters.
    They work very well with HTTP and HTTPS services, providing many features for
    distributing and managing traffic between services. This traffic will be located
    on the OSI model's transport and application layers; they are also known as layers
    4 and 7, respectively. It also works with raw TCP and UDP services; however, in
    these cases, traffic will be load balanced at layer 4 only.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress资源帮助我们在Kubernetes集群上发布部署的应用程序。它们与HTTP和HTTPS服务非常配合，提供许多功能来在服务之间分发和管理流量。这些流量将位于OSI模型的传输和应用层，它们也被称为第4层和第7层。它还与原始TCP和UDP服务一起工作；但在这些情况下，流量将仅在第4层进行负载平衡。
- en: 'These resources route traffic from outside the cluster to services running
    within the cluster. Ingress resources require the existence of a special service
    called an **ingress controller**. These services will load balance or route traffic
    using rules created by ingress resources. Therefore, publishing an application
    using this feature requires two components:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源将流量从集群外部路由到集群内运行的服务。Ingress 资源需要一个名为 **ingress 控制器** 的特殊服务。这些服务将使用 ingress
    资源创建的规则进行负载均衡或路由流量。因此，使用此功能发布应用程序需要两个组件：
- en: '**Ingress resource**: The rules to apply to incoming traffic'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingress 资源**：应用于传入流量的规则。'
- en: '**Ingress controller**: The load balancer that will automatically convert or
    translate ingress rules to load balance configurations'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingress 控制器**：一个负载均衡器，它会自动将 ingress 规则转换或翻译为负载均衡配置。'
- en: 'A combination of both objects provides the dynamic publishing of applications.
    If one application''s pod dies, a new one will be created and the service and
    ingress controller will automatically route all traffic to the new one. This will
    also isolate services from external networks. We will publish one single endpoint
    instead of the `NodePort` or `LoadBalancer` service types for all services, which
    will consume many nodes'' ports or cloud IP addresses. This endpoint is the load
    balancer that will use the ingress controller and ingress resource rules to route
    traffic internally to deployed services:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 两者的结合提供了应用程序的动态发布。如果某个应用程序的 pod 死亡，会自动创建一个新的 pod，服务和 ingress 控制器将自动将所有流量路由到新的
    pod。这也将服务与外部网络隔离。我们将只发布一个端点，而不是为所有服务使用 `NodePort` 或 `LoadBalancer` 服务类型，这样可以节省许多节点的端口或云
    IP 地址。这个端点就是负载均衡器，它将使用 ingress 控制器和 ingress 资源规则，将流量内部路由到已部署的服务：
- en: '![](img/64584d31-c831-4590-a961-cc5877468a3a.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64584d31-c831-4590-a961-cc5877468a3a.jpg)'
- en: 'This chapter''s labs show us an interesting load balancing example using **NGINX
    Ingress Controller**. Let''s review a quick example YAML configuration file:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实验向我们展示了一个有趣的负载均衡示例，使用了 **NGINX Ingress 控制器**。让我们快速回顾一下示例的 YAML 配置文件：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This example outlines the rules that should be applied to route requests to
    a specific `example.local`-specific host header. Any request containing `/example1`
    in its URL will be guided to `example-webserver`, while `another-service` will
    receive requests containing the `/example2` path in its URL. Notice that we have
    used the internal service's ports; therefore, no additional service exposure is
    required. One ingress controller endpoint will redirect traffic to the `example-webserver`
    and `another-service` services. This saves up the host's ports (and/or IP addresses
    on the cloud providers because the `LoadBalancer` service type uses one published
    public IP address per service).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例概述了应应用的规则，以便将请求路由到特定的 `example.local` 主机头。任何 URL 中包含 `/example1` 的请求将被引导到
    `example-webserver`，而包含 `/example2` 路径的请求则会转发到 `another-service`。注意，我们使用了内部服务的端口，因此不需要额外的服务曝光。一个
    ingress 控制器端点将会把流量重定向到 `example-webserver` 和 `another-service` 服务。这节省了主机的端口（以及云提供商上的
    IP 地址，因为 `LoadBalancer` 服务类型每个服务使用一个公开的 IP 地址）。
- en: We can provide as many ingress controllers as needed. In fact, in multi-tenant
    environments, we usually deploy more than just one to isolate publishing planes
    between different tenants.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据需要提供多个 ingress 控制器。实际上，在多租户环境中，我们通常会部署不止一个控制器，以便在不同租户之间隔离发布平面。
- en: This brief look at publishing applications on Kubernetes has finished this review
    of the main Kubernetes networking features. Let's now move on to Kubernetes security
    properties.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这一简要的 Kubernetes 应用程序发布介绍，已完成对 Kubernetes 主要网络功能的回顾。接下来，我们将转向 Kubernetes 的安全特性。
- en: Kubernetes security components and features
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的安全组件和特性
- en: Kubernetes provides mechanisms to authenticate and authorize access to its API.
    This allows us to apply different levels of privileges for users or roles within
    a cluster. This prevents unauthorized access to some core resources, such as scheduling
    or nodes in the cluster.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了认证和授权访问其 API 的机制。这使我们能够为集群内的用户或角色应用不同级别的权限。这可以防止未经授权的访问某些核心资源，如调度或集群中的节点。
- en: Once users are allowed to use cluster resources, we use namespaces to isolate
    their own resources from other users. This works even in multi-tenant environments
    where a higher level of security is required.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦用户被允许使用集群资源，我们使用命名空间将他们的资源与其他用户的资源隔离开来。这在多租户环境中也能有效工作，尤其是在需要更高安全性的场景下。
- en: Kubernetes works with the very elaborate **Role-Based Access Control** (**RBAC**)
    environment, which provides a great level of granularity to allow specific actions
    on some resources while other actions are denied.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes与非常精细的**基于角色的访问控制**（**RBAC**）环境配合使用，提供了很高的粒度，允许对某些资源执行特定操作，同时拒绝其他操作。
- en: We manage the `Role` and `ClusterRole` resources to describe permissions for
    different resources. We use `Role` to define permissions within namespaces and
    `ClusterRole` for permissions on cluster-wide resources. Rules are supplied using
    some defined verbs, such as `list`, `get`, `update`, and more, and these verbs
    are applied to resources (or even specific resource names). The `RoleBinding`
    and `ClusterRoleBinding` resources grant permissions defined in roles to users
    or sets of users.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管理`Role`和`ClusterRole`资源，以描述不同资源的权限。我们使用`Role`来定义命名空间内的权限，使用`ClusterRole`来定义集群范围内的资源权限。规则通过一些已定义的动词提供，例如`list`、`get`、`update`等，这些动词作用于资源（甚至特定的资源名称）。`RoleBinding`和`ClusterRoleBinding`资源将角色中定义的权限授予用户或用户集合。
- en: 'Kubernetes also provides the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还提供以下功能：
- en: Service accounts to identify processes within pods to other resources
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务账户用于标识pod内的进程对其他资源的访问
- en: Pod security policies to control the special behaviors of pods, such as privileged
    containers, host namespaces, restrictions on running containers with root users,
    or enabling read-only root filesystems on containers, among other features
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod安全策略控制pod的特殊行为，例如特权容器、主机命名空间、限制以root用户运行容器，或启用容器的只读根文件系统等功能。
- en: Admission controllers to intercept API requests, allowing us to validate or
    modify them to ensure image freshness and security, forcing the creation of pods
    to always pull from registries, to set the default storage, to deny the execution
    of processes within privileged containers, or to specify the default host resource
    limit ranges if none are declared, among other security features
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审批控制器拦截API请求，允许我们验证或修改请求，以确保镜像的新鲜度和安全性，强制创建的pod始终从注册表拉取，设置默认存储，禁止在特权容器中执行进程，或在未声明时指定默认的主机资源限制范围等其他安全功能。
- en: It is very important in production environments to limit a host's resource usage
    because non-limited pods can consume all of their resources by default.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中限制主机的资源使用非常重要，因为未限制的pod默认可以消耗所有资源。
- en: Kubernetes provides many features to ensure cluster security at all levels.
    It is up to you to use them because most of them are not applied by default. We
    will learn more about the roles and grants applied to resources in [Chapter 11](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml),
    *Universal Control Plane*, because many of these configurations are integrated
    into Docker Enterprise.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了许多功能，确保集群在各个层级的安全。是否使用这些功能由你决定，因为大多数功能默认情况下不会启用。我们将在[第11章](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml)中学习更多关于角色和权限应用于资源的内容，*通用控制平面*，因为许多这些配置已经集成到Docker
    Enterprise中。
- en: We are not going to go deeper into this topic because Kubernetes is not part
    of the current Docker Certified Associate curriculum, and this is just a quick
    introduction.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨这个话题，因为Kubernetes不是当前Docker认证助理课程的一部分，这只是一个简要的介绍。
- en: It is recommended that you take a closer look at Kubernetes' security features
    because it has many more compared to Docker Swarm. On the other hand, it is true
    that Docker Enterprise provides many of these features to Docker Swarm.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 建议你更仔细地查看Kubernetes的安全功能，因为它比Docker Swarm有更多的安全特性。另一方面，确实Docker Enterprise为Docker
    Swarm提供了许多这些功能。
- en: Comparing Docker Swarm and Kubernetes side by side
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Docker Swarm和Kubernetes进行并排比较
- en: 'In this section, we will compare the Docker Swarm and Kubernetes features side
    by side to get a good idea of how they solve common problems. We have discussed
    these concepts in both this chapter and in [Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml),
    *Orchestration Using Docker Swarm.* They have common approaches to many problems:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对比Docker Swarm和Kubernetes的功能，以便更好地理解它们如何解决常见问题。我们在本章和[第8章](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml)，*使用Docker
    Swarm进行编排*中都讨论了这些概念。它们在许多问题上有共同的解决方法：
- en: '| **Parameters** | **Docker Swarm** | **Kubernetes** |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **Docker Swarm** | **Kubernetes** |'
- en: '| High-availability solution | Provides high availability for core components.
    | Provides high availability for core components. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 高可用性解决方案 | 为核心组件提供高可用性。 | 为核心组件提供高可用性。 |'
- en: '| Resilience | All services run with resilience based on the state definition.
    | All resources based on replication controllers will provide resilience (`ReplicaSet`,
    `DaemonSet`, `Deployment`, and `StatefulSet`) based on the state definition. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 弹性 | 所有服务根据状态定义以弹性方式运行。 | 基于复制控制器的所有资源将根据状态定义提供弹性（`ReplicaSet`、`DaemonSet`、`Deployment`
    和 `StatefulSet`）。 |'
- en: '| Infrastructure as code | The Docker Compose file format will allow us to
    deploy stacks. | We will use YAML to format resource files. These will allow us
    to deploy workloads using a declarative format. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 基础设施即代码 | Docker Compose 文件格式将允许我们部署堆栈。 | 我们将使用 YAML 格式化资源文件，这将允许我们使用声明性格式部署工作负载。
    |'
- en: '| Dynamic distribution | Application components and their replicas will be
    automatically distributed cluster-wide, although we can provide some constraints.
    | Kubernetes also distributes components, but we can provide advanced constraints
    using labels and other features. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 动态分配 | 应用程序组件及其副本将自动在整个集群中分配，尽管我们可以提供一些约束。 | Kubernetes 也会分配组件，但我们可以使用标签和其他功能提供高级约束。
    |'
- en: '| Automatic updates | Application components can be upgraded using rolling
    updates and rollbacks in the case of a failure. | Kubernetes also provides rolling
    updates and rollbacks. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 自动更新 | 应用程序组件可以使用滚动更新和回滚在发生故障时进行升级。 | Kubernetes 也提供滚动更新和回滚功能。 |'
- en: '| Publishing applications | Docker Swarm provides internal load balancing between
    service replicas and **router mesh** to publish an application''s service''s ports
    on all of the cluster nodes at the same time. | Kubernetes also provides internal
    load balancing, and `NodePort` type services will also publish the application''s
    components on all of the nodes at the same time. But Kubernetes also provides
    load balancing services (among other types) to auto-configure external load balancers
    to route requests to deployed services. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 发布应用程序 | Docker Swarm 在服务副本之间提供内部负载均衡，并通过**路由网格**同时在所有集群节点上发布应用程序的服务端口。 |
    Kubernetes 也提供内部负载均衡，`NodePort` 类型服务也会在所有节点上同时发布应用程序的组件。但 Kubernetes 还提供负载均衡服务（以及其他类型的服务），可以自动配置外部负载均衡器来将请求路由到已部署的服务。
    |'
- en: '| Cluster-internal networking | Containers that are deployed as tasks for each
    service can communicate with other containers deployed in the same network. Internal
    IP management will provide their IP addresses, and services can be consumed by
    their names so that there is internal DNS resolution. | Pod-to-pod communication
    works and IP addresses are provided by internal **Internet Protocol Address Management**
    (IPAM). We will also have service-to-service communication and resolution. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 集群内部网络 | 部署为每个服务任务的容器可以与同一网络中部署的其他容器通信。内部 IP 管理将提供其 IP 地址，服务可以通过其名称进行访问，从而实现内部
    DNS 解析。 | Pod 之间的通信正常工作，IP 地址由内部**互联网协议地址管理**（IPAM）提供。我们还将实现服务到服务的通信和解析。 |'
- en: '| Key-value store | Docker Swarm provides an internal store to manage all objects
    and their statuses. This store will have high availability with an odd number
    of master nodes. | Kubernetes also requires a key-value store to manage its resources.
    This component is provided using `etcd` and we can deploy it externally out of
    Kubernetes cluster nodes. We should provide an odd number of `etcd` nodes to provide
    high availability. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 键值存储 | Docker Swarm 提供一个内部存储来管理所有对象及其状态。该存储将具有高可用性，并且需要一个奇数个主节点。 | Kubernetes
    也需要一个键值存储来管理其资源。此组件使用 `etcd` 提供，并且我们可以将其部署在 Kubernetes 集群节点之外。我们应提供一个奇数个 `etcd`
    节点以提供高可用性。 |'
- en: 'The preceding table showed us the main similarities regarding solving common
    problems. The next table will show the main differences:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表格展示了我们在解决常见问题方面的主要相似之处。接下来的表格将展示主要差异：
- en: '| **Parameters** | **Docker Swarm** | **Kubernetes** |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **Docker Swarm** | **Kubernetes** |'
- en: '| Pods versus tasks | Docker Swarm deploys tasks for services. Each task will
    run one container at a time. If the container dies, a new one will be created
    to ensure the required number of replicas (tasks).Services are the smallest unit
    of deployment. We will deploy applications running their components as services.
    | Kubernetes has the concept of a pod. Each pod can run more than one container
    inside. All of them share the same IP address (networking namespace). Containers
    inside a pod share volumes and will always run on the same host. A pod''s life
    relies on containers. If one of them dies, a pod is unhealthy. Pods are the smallest
    unit of deployment in Kubernetes; therefore, we scale pods up and down, with all
    of their containers. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Pods 与任务 | Docker Swarm 为服务部署任务。每个任务一次只能运行一个容器。如果容器崩溃，会创建一个新的容器以确保所需的副本数（任务）。服务是最小的部署单元。我们将部署运行其组件的应用程序作为服务。
    | Kubernetes 有 pod 的概念。每个 pod 可以运行多个容器，并且它们共享相同的 IP 地址（网络命名空间）。pod 内的容器共享卷，并且始终运行在相同的主机上。pod
    的生命周期依赖于容器。如果其中一个容器崩溃，pod 会变得不健康。pod 是 Kubernetes 中最小的部署单元；因此，我们通过扩缩 pods 来调整它们的规模，包括所有容器。
    |'
- en: '| Services | Services in Docker Swarm are objects with an IP address for internal
    load balancing between replicas (by default, we can avoid this using the `dnsrr`
    endpoint mode). We create services to execute our application components, and
    we scale up or down the number of replicas required to be healthy. | In Kubernetes,
    services are different. They are logical resources. This means that they are deployed
    only to publish a group of pod resources. Kubernetes services are logical groupings
    of pods that work together. Kubernetes services also get an IP address for internal
    load balancing (`clusterIP`), and we can also avoid this situation by using the
    "headless" feature. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | Docker Swarm 中的服务是具有 IP 地址的对象，用于在副本之间进行内部负载均衡（默认情况下，我们可以使用 `dnsrr` 端点模式避免此情况）。我们创建服务来执行应用程序组件，并根据需要扩展或缩减副本数量，以确保服务健康。
    | 在 Kubernetes 中，服务有所不同。它们是逻辑资源。这意味着它们仅用于发布一组 pod 资源。Kubernetes 服务是共同工作的 pod 的逻辑分组。Kubernetes
    服务还会获取一个 IP 地址用于内部负载均衡（`clusterIP`），我们也可以通过使用“无头”功能来避免这种情况。 |'
- en: '| Networking | Docker Swarm deploys overlay networking by default. This ensures
    communications between an application''s components are deployed on different
    hosts.Stacks in Docker Swarm will be deployed on different networks. This means
    that we can provide a subnet for each application. Multiple networks for deployments
    will provide a good level of security because they are isolated from each other.
    This can be improved using available network encryption (disabled by default).
    However, on the other hand, they are difficult to manage and things can get complicated
    when we need to provide isolation on services integrated into multiple stacks.
    | Kubernetes provides a flat network using a common interface called a CNI. Networking
    has been decoupled from Kubernetes'' core to allow us to use multiple and different
    networking solutions. Each solution has its own features and implementation for
    routing on a cluster environment. A flat network makes things easier. All pods
    and services will see each other by default. On the other hand, security is not
    provided. We will deploy `NetworkPolicy` resources to ensure secure communications
    between resources in the cluster. These policies will manage who can talk to who
    in the Kubernetes world. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | Docker Swarm 默认部署覆盖网络。这确保了应用程序的组件在不同主机上部署时能相互通信。Docker Swarm 中的堆栈将部署在不同的网络上。这意味着我们可以为每个应用程序提供一个子网。多个网络的部署能提供较好的安全性，因为它们彼此隔离。通过使用可用的网络加密（默认禁用），可以进一步提高安全性。然而，另一方面，它们很难管理，当我们需要为集成到多个堆栈中的服务提供隔离时，事情会变得复杂。
    | Kubernetes 提供了一个扁平网络，使用一个名为 CNI 的公共接口。网络已从 Kubernetes 核心解耦，以允许我们使用多种不同的网络解决方案。每个解决方案都有其特性和在集群环境中的路由实现。扁平网络简化了操作，所有的
    pod 和服务默认都能互相看到。另一方面，安全性没有默认提供。我们将部署 `NetworkPolicy` 资源，以确保集群内资源之间的安全通信。这些策略将管理在
    Kubernetes 环境中谁可以与谁通信。 |'
- en: '| Authentication and authorization | Docker Swarm, by default, does not provide
    any mechanism to authenticate or authorize specific requests. Once a Docker Swarm
    node has published its daemon access (in a `daemon.json` configuration file),
    anyone can connect to it and manage the cluster if we use a manager node. This
    is a security risk that should always be avoided. We can create a secure client
    configuration with SSL/TLS certificates. But certificates in Docker Swarm will
    ensure secure communication only. There is no authorization validation. Docker
    Enterprise will provide the required features to provide RBAC to Docker Swarm''s
    clusters. | Kubernetes does provide authentication and authorization. In fact,
    it includes a full-featured RBAC system to manage users'' and applications'' accesses
    to the resources deployed within the Kubernetes cluster. This RBAC system allows
    us to set specific permissions for a user''s or team''s access. Using Kubernetes
    namespaces will also improve security in multi-tenant or team scenarios. |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 身份验证和授权 | Docker Swarm 默认不提供任何机制来验证或授权特定请求。一旦 Docker Swarm 节点公开了其守护进程访问权限（在
    `daemon.json` 配置文件中），任何人都可以连接到它并管理集群（如果我们使用管理节点）。这是一种安全风险，应始终避免。我们可以使用 SSL/TLS
    证书创建安全的客户端配置。但在 Docker Swarm 中，证书只确保安全的通信，并不会进行授权验证。Docker Enterprise 会提供所需的功能，为
    Docker Swarm 集群提供 RBAC。| Kubernetes 确实提供了身份验证和授权功能。实际上，它包含一个功能齐全的 RBAC 系统来管理用户和应用程序对
    Kubernetes 集群中资源的访问。这个 RBAC 系统允许我们为用户或团队的访问设置特定权限。使用 Kubernetes 命名空间也将在多租户或团队场景中提高安全性。|'
- en: '| Secrets | Docker encrypts secrets by default. They will only be readable
    inside containers while they are running. | Kubernetes will encode secrets using
    the Base64 algorithm by default. We will need to use external secret providers
    or additional encryption configuration (`EncryptionConfig`) to ensure a secret''s
    integrity. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 密钥 | Docker 默认情况下加密密钥。它们仅在容器运行时可读。| Kubernetes 默认使用 Base64 算法对密钥进行编码。我们需要使用外部密钥提供者或额外的加密配置（`EncryptionConfig`）来确保密钥的完整性。|'
- en: '| Publishing applications | Docker Swarm just provides a router mesh for publishing
    applications. This will publish application ports on all cluster nodes. This can
    be insecure because all nodes will have all the applications published and we
    will use a lot of ports (at least one for each published application). Docker
    Enterprise will provide Interlock, which has many features in common with ingress
    controllers. | Kubernetes provides ingress controller resources. Ingress controllers
    publish a few endpoints (using `NodePort` or any other cloud service definition),
    and this internal ingress will talk to services'' backends (pods). This will require
    fewer ports for applications (only those required to publish ingress controllers).
    Requests will be routed by these resources to real backend services. Security
    is improved because we add a smart piece of software in the middle of the requests
    to help us to decide which backends will process requests. The ingress controller
    acts as a reverse-proxy and it will verify whether a valid host header is used
    on every request. If none is used, requests will be forwarded to a default backend.
    If requests contain a valid header, they will be forwarded to the defined service''s
    virtual IP and the internal load balancer will choose which pod will finally receive
    them. The orchestrator will manage defined rules and clusters, and the internal
    or external load balancer will interpret them to ensure the right backend receives
    the user''s request.  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 发布应用程序 | Docker Swarm 仅提供用于发布应用程序的路由网格。这将把应用程序端口发布到所有集群节点。这可能不安全，因为所有节点都会发布所有应用程序，我们将使用大量端口（至少每个已发布的应用程序一个端口）。Docker
    Enterprise 将提供 Interlock，它具有与入口控制器相似的许多功能。| Kubernetes 提供了入口控制器资源。入口控制器发布一些端点（使用
    `NodePort` 或任何其他云服务定义），这些内部入口将与服务的后端（pods）进行通信。这将需要更少的端口来发布应用程序（仅需要发布入口控制器的端口）。请求将通过这些资源路由到实际的后端服务。由于我们在请求中间添加了一种智能软件，帮助我们决定哪些后端将处理请求，因此安全性得到了提升。入口控制器充当反向代理，并且会验证每个请求是否使用了有效的主机头。如果没有使用，请求将被转发到默认的后端。如果请求包含有效的头部，它们将被转发到定义的服务虚拟
    IP，内部负载均衡器将选择哪个 pod 最终接收它们。调度器将管理已定义的规则和集群，内部或外部负载均衡器将解读这些规则，以确保正确的后端接收到用户的请求。|'
- en: 'So far, we have learned that there are several similarities and differences
    between Docker Swarm and Kubernetes. We can note the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了 Docker Swarm 和 Kubernetes 之间的一些相似点和差异。我们可以注意到以下几点：
- en: Kubernetes provides more container density.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 提供了更高的容器密度。
- en: Docker Swarm provides cluster-wide networking with subnets for isolation by
    default.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm 默认提供集群范围的网络，并使用子网进行隔离。
- en: Kubernetes provides role-based access to cluster resources.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 提供基于角色的集群资源访问。
- en: Publishing applications in Kubernetes is better using ingress controllers.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中发布应用时，最好使用 ingress 控制器。
- en: Let's now review some of the topics we have learned by applying them to some
    easy labs.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过将所学应用到一些简单的实验中来回顾一些我们已经学过的主题。
- en: Chapter labs
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节实验
- en: We will now work through a long lab that will help us to review the concepts
    we've learned so far.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进行一个较长的实验，这将帮助我们回顾迄今为止所学的概念。
- en: Deploy `environments/kubernetes` from this book's GitHub repository ([https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git))
    if you have not done so yet. You can use your own Linux server. Use `vagrant up`
    from the `environments/kubernetes` folder to start your virtual environment. All
    files used during these labs can be found inside the `chapter9` folder.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请从本书的 GitHub 仓库中部署`environments/kubernetes`（[https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git)）。你也可以使用自己的
    Linux 服务器。从`environments/kubernetes`文件夹中运行`vagrant up`来启动你的虚拟环境。这些实验中使用的所有文件可以在`chapter9`文件夹中找到。
- en: 'Wait until all of the nodes are running. We can check the status of the nodes
    using `vagrant status`. Connect to your lab node using `vagrant ssh kubernetes-node1`.
    Vagrant deploys three nodes for you, and you will be using the `vagrant` user
    with root privileges using `sudo`. You should have the following output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 等待所有节点都启动。我们可以通过`vagrant status`来检查节点的状态。使用`vagrant ssh kubernetes-node1`连接到你的实验节点。Vagrant
    会为你部署三个节点，你将使用`vagrant`用户，并通过`sudo`获得 root 权限。你应该看到如下输出：
- en: '[PRE29]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Nodes will have three interfaces (IP addresses and virtual hardware resources
    can be modified by changing the `config.yml` file):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 节点将有三个接口（IP 地址和虚拟硬件资源可以通过修改`config.yml`文件来进行更改）：
- en: '`eth0 [10.0.2.15]`: This is an internal interface, required for Vagrant.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth0 [10.0.2.15]`：这是一个内部接口，Vagrant 所必需。'
- en: '`eth1 [10.10.10.X/24]`: This is prepared for Docker Kubernetes'' internal communication.
    The first node will get the `10.10.10.11` IP address and so on.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth1 [10.10.10.X/24]`：这是为 Docker Kubernetes 内部通信准备的。第一个节点将获得`10.10.10.11`
    IP 地址，依此类推。'
- en: '`eth2 [192.168.56.X/24]`: This is a host-only interface for communication between
    your host and the virtual nodes. The first node will get the `192.168.56.11` IP
    address and so on.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth2 [192.168.56.X/24]`：这是一个仅限主机的接口，用于主机与虚拟节点之间的通信。第一个节点将获得`192.168.56.11`
    IP 地址，依此类推。'
- en: We will use the `eth1` interface for Kubernetes, and we will be able to connect
    to published applications using the `192.168.56.X/24` IP address' range. All nodes
    have Docker Engine Community Edition installed and a Vagrant user is allowed to
    execute `docker`. A small Kubernetes cluster with one master (`kubernetes-node1`)
    and two worker nodes (`kubernetes-node2` and `kubernetes-node3`) will be deployed
    for you.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`eth1`接口来进行 Kubernetes，并且我们将能够使用`192.168.56.X/24` IP 地址范围来连接已发布的应用。所有节点都安装了
    Docker Engine Community Edition，并且允许 Vagrant 用户执行`docker`命令。一个小型的 Kubernetes 集群将为你部署，其中包含一个主节点（`kubernetes-node1`）和两个工作节点（`kubernetes-node2`和`kubernetes-node3`）。
- en: 'We can now connect to the first deployed virtual node using `vagrant ssh kubernetes-node1`.
    The process may vary if you have already deployed a Kubernetes virtual environment
    and have just started it using `vagrant up`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`vagrant ssh kubernetes-node1`连接到第一个已部署的虚拟节点。如果你已经部署了 Kubernetes 虚拟环境并且只是刚刚使用`vagrant
    up`启动它，过程可能会有所不同：
- en: '[PRE30]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now you are ready to start the labs. We will start these labs by deploying a
    simple application.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好开始实验了。我们将通过部署一个简单的应用程序来开始这些实验。
- en: Deploying applications in Kubernetes
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署应用
- en: Once Vagrant (or your own environment) is deployed, we will have three nodes
    (named `kubernetes-node<index>` from `1` to `3`) with Ubuntu Xenial and Docker
    Engine installed. Kubernetes will also be up and running for you, with one master
    node and two workers. The Calico CNI will also be deployed for you automatically.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Vagrant（或你自己的环境）部署完成，我们将拥有三个节点（命名为`kubernetes-node<index>`，从`1`到`3`），它们将安装
    Ubuntu Xenial 和 Docker Engine。Kubernetes 也会为你启动并运行，包含一个主节点和两个工作节点。Calico CNI 也会为你自动部署。
- en: First, review your node IP addresses (`10.10.10.11` to `10.10.10.13` if you
    used Vagrant, because the first interface will be Vagrant-internal).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查您的节点IP地址（如果您使用了Vagrant，范围是`10.10.10.11`到`10.10.10.13`，因为第一个接口将是Vagrant-internal）。
- en: 'The steps for deploying our application are as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 部署应用的步骤如下：
- en: 'Connect to `kubernetes-node1` and review the deployed Kubernetes cluster using
    `kubectl get nodes`. A file, named `config`, including the required credentials
    and the Kubernetes API endpoint will be copied under the `~/.kube`directory automatically.
    We''ll also refer to this file as **Kubeconfig**. This file configures the `kubectl`
    command line for you:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到`kubernetes-node1`并使用`kubectl get nodes`查看已部署的Kubernetes集群。一个名为`config`的文件（包含所需的凭据和Kubernetes
    API端点）将自动复制到`~/.kube`目录下。我们也将此文件称为**Kubeconfig**。该文件为您配置`kubectl`命令行：
- en: '[PRE31]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Kubernetes cluster version 1.14.00 has been deployed and is running. Notice
    that `kubernetes-node1` is the only master node in this cluster; therefore, we
    are not providing high availability.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群版本1.14.00已部署并正在运行。请注意，`kubernetes-node1`是该集群中唯一的主节点；因此，我们没有提供高可用性。
- en: Currently, we are using the `admin` user, and, by default, all deployments will
    run on the `default` namespace, unless another is specified. This configuration
    is also done in the `~/.kube/config` file.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们使用的是`admin`用户，默认情况下，所有部署将运行在`default`命名空间，除非另行指定。此配置也在`~/.kube/config`文件中完成。
- en: The Calico CNI was also deployed; hence, host-to-container networking should
    work cluster-wide.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Calico CNI也已部署，因此，集群范围内的主机与容器的网络连接应该是正常的。
- en: 'Create a deployment file, named `blue-deployment-simple.yaml`, using your favorite
    editor with the following content:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您喜欢的编辑器创建一个名为`blue-deployment-simple.yaml`的部署文件，内容如下：
- en: '[PRE32]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This will deploy two replicas of the`codegazers/colors:1.12` image. We will
    expect two running pods after it is deployed. We set the `COLOR` environment variable
    to `blue` and, as a result, all of the application components will be `blue`.
    Containers will expose port `3000` internally within the cluster.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这将部署`codegazers/colors:1.12`镜像的两个副本。部署后，我们预计会看到两个运行中的Pod。我们将`COLOR`环境变量设置为`blue`，因此，所有应用组件都会是`blue`。容器将在集群内部暴露端口`3000`。
- en: 'Let''s deploy this `blue-app` application using `kubectl create -f <KUBERNETES_RESOURCES_FILE>.yaml`:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl create -f <KUBERNETES_RESOURCES_FILE>.yaml`部署这个`blue-app`应用：
- en: '[PRE33]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This command line has created a deployment, named `blue-app`, with two replicas.
    Let''s review the deployment created using `kubectl get deployments`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令行创建了一个名为`blue-app`的部署，并且有两个副本。让我们使用`kubectl get deployments`查看已创建的部署：
- en: '[PRE34]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Therefore, two pods will be running, associated with the `blue-app`deployment.
    Let''s now review the deployed pods using `kubectl get pods`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将有两个Pod与`blue-app`部署关联。让我们使用`kubectl get pods`查看已部署的Pod：
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In this case, one pod runs on `kubernetes-node2` and another one runs on `kubernetes-node3`.
    Let''s try to connect to their virtual assigned IP addresses on the exposed port.
    Remember that IP addresses will be assigned randomly, hence they may vary on your
    environment. We will just use `curl` against the IP address of `kubernetes-node1`
    and the pod''s internal port:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个Pod运行在`kubernetes-node2`上，另一个Pod运行在`kubernetes-node3`上。让我们尝试连接到它们的虚拟分配IP地址，使用暴露的端口。请记住，IP地址会随机分配，因此在您的环境中可能会有所不同。我们将使用`curl`命令连接到`kubernetes-node1`的IP地址和Pod的内部端口：
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can connect from `kubernetes-node1` to pods running on other hosts correctly.
    So, Calico is working correctly.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从`kubernetes-node1`正确连接到其他主机上运行的Pod。因此，Calico工作正常。
- en: We should be able to connect to any pods' deployed IP addresses. These IP addresses
    will change whenever a container dies and a new pod is deployed. We will never
    connect to pods to consume their application processes. We will use services instead
    of pods to publish applications, as we have already discussed in this chapter.
    They will not change their IP addresses when application components, running as
    pods, have to be recreated.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该能够连接到任何Pod的部署IP地址。每当容器死亡并且新Pod部署时，这些IP地址将发生变化。我们永远不会直接连接Pod以消费其应用程序进程。正如我们在本章中已经讨论过的，我们将使用服务而不是Pod来发布应用程序。这样，当作为Pod运行的应用组件需要重新创建时，它们的IP地址将不会发生变化。
- en: 'Let''s create a service to load balance requests between deployed pods with
    a fixed virtual IP address. Create the `blue-service-simple.yaml` file with the
    following content:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个服务，用于在已部署的Pod之间进行负载均衡，并为其分配一个固定的虚拟IP地址。创建一个名为`blue-service-simple.yaml`的文件，内容如下：
- en: '[PRE37]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A random IP address will be associated with this service. This IP address will
    be fixed, and it will be valid even if pods die. Notice that we have exposed a
    new port for the service. This will be the service''s port, and requests reaching
    the defined port `80`will be routed to port `3000` on each pod. We will use `kubectl
    get svc` to retrieve the service''s port and IP address:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机的 IP 地址将与该服务关联。这个 IP 地址将是固定的，即使 Pod 死亡，它仍然有效。注意，我们为该服务暴露了一个新的端口。这个端口将是服务的端口，访问定义端口
    `80` 的请求将被路由到每个 Pod 上的 `3000` 端口。我们将使用 `kubectl get svc` 获取服务的端口和 IP 地址：
- en: '[PRE38]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s verify the internal load balance by sending some requests to the `blue-svc`
    service using `curl` against its IP address, accessing port `80`:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过使用 `curl` 访问其 IP 地址的 `blue-svc` 服务来验证内部负载均衡，访问端口 `80`：
- en: '[PRE39]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s try again using `curl`. We will test the internal load balancing by
    executing some requests to the service''s IP address and port:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次尝试使用 `curl`。我们将通过向服务的 IP 地址和端口发送请求来测试内部负载均衡：
- en: '[PRE40]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The service has load-balanced our requests between both pods. Let's now try
    to expose this service to be accessible to the application's users.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务已在两个 Pod 之间负载均衡了我们的请求。现在让我们尝试将此服务暴露出来，让应用程序的用户能够访问它。
- en: 'Now we will remove the previous service''s definition and deploy a new one
    with the service''s `NodePort` type. We will use `kubectl delete -f <KUBERNETES_RESOURCES_FILE>.yaml`:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将删除之前服务的定义，并部署一个新的服务，使用 `NodePort` 类型。我们将使用 `kubectl delete -f <KUBERNETES_RESOURCES_FILE>.yaml`：
- en: '[PRE41]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create a new definition, `blue-service-nodeport.yaml`, with the following content:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的定义文件 `blue-service-nodeport.yaml`，其内容如下：
- en: '[PRE42]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now just create a service definition and notice a random port associated
    with it. We will also use `kubectl create` and `kubectl get svc` after it is deployed:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在只需创建一个服务定义，并注意到与之关联的随机端口。部署完成后，我们还将使用 `kubectl create` 和 `kubectl get svc`：
- en: '[PRE43]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We learned that the `NodePort` service will act as Docker Swarm''s router mesh.
    Therefore, the service''s port will be fixed on every node. Let''s verify this
    feature using `curl` against any node''s IP address and assigned port. In this
    example, it is `32648`. This port may vary on your environment because it will
    be assigned dynamically:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们了解到，`NodePort` 服务将充当 Docker Swarm 的路由网格。因此，该服务的端口将在每个节点上固定。让我们使用 `curl` 来验证此功能，访问任何节点的
    IP 地址和分配的端口。在本例中，它是 `32648`。这个端口在您的环境中可能会有所不同，因为它会动态分配：
- en: '[PRE44]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Locally, on `node1` port `32648`, the service is accessible. It should be accessible
    on any of the nodes on the same port. Let''s try on `node3`, for example, using
    `curl`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地，`node1` 的端口 `32648` 上可以访问该服务。它应该可以在同一端口上在任何节点上访问。例如，我们可以尝试在 `node3` 上使用
    `curl`：
- en: '[PRE45]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We learned that even if a node does not run a related workload, the service
    will be accessible on the defined (or, in this case, random) port using `NodePort`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，即使一个节点没有运行相关工作负载，服务仍然可以通过定义的（或者在这种情况下是随机的）端口使用 `NodePort` 进行访问。
- en: 'We will finish this lab by upgrading the deployment images to a newer version.
    We will use `kubectl set image deployment`:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过升级部署镜像到新版本来完成这个实验。我们将使用 `kubectl set image deployment`：
- en: '[PRE46]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s review the deployment again to verify that the update was done. We will
    use `kubectl get all -o wide` to retrieve all of the created resources and their
    locations:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次查看部署，验证更新是否完成。我们将使用 `kubectl get all -o wide` 获取所有已创建的资源及其位置：
- en: '[PRE47]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Notice that new pods were created with a newer image. We can verify the update
    using `kubectl rollout status`:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，新创建的 Pod 使用了更新的镜像。我们可以通过 `kubectl rollout status` 来验证更新：
- en: '[PRE48]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can go back to the previous image version just by executing `kubectl rollout
    undo`. Let''s go back to the previous image version:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需执行 `kubectl rollout undo` 就可以回到之前的镜像版本。让我们回到之前的镜像版本：
- en: '[PRE49]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And now, we can verify that the current `blue-app` deployment runs the `codegazers/colors:1.12`
    images again. We will again review deployment locations using `kubectl get all`:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以验证当前的 `blue-app` 部署是否再次运行 `codegazers/colors:1.12` 镜像。我们将再次使用 `kubectl
    get all` 来检查部署位置：
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Going back to the previous state was very easy.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到之前的状态非常简单。
- en: We can set comments for each change using the `--record` option on the `update`
    commands.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `--record` 选项为每次更改设置注释，记录在 `update` 命令中。
- en: Using volumes
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷
- en: In this lab, we will deploy a simple web server using different volumes. We
    will use `webserver.deployment.yaml`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将使用不同的卷部署一个简单的 Web 服务器。我们将使用 `webserver.deployment.yaml`。
- en: 'We have prepared the following volumes:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好了以下卷：
- en: '`congigMap`: Config volume with `/etc/nginx/conf.d/default.conf`—the configuration
    file)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`congigMap`: 配置卷，路径为`/etc/nginx/conf.d/default.conf`（配置文件）'
- en: '`emptyDir`: Empty volume for NGINX logs, `/var/log/nginx`'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emptyDir`: NGINX日志的空卷，路径为`/var/log/nginx`'
- en: '`secret`: Secret volume to specify some variables to compose the `index.html`
    page'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`secret`: 秘密卷，用于指定一些变量来组成`index.html`页面'
- en: '`persistentVolumeClaim`: Data volume bound to the `hostPath` defined as `persistentVolume`
    using the host''s `/mnt` content'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persistentVolumeClaim`: 数据卷绑定到使用主机`/mnt`内容定义的`persistentVolume`的`hostPath`'
- en: 'We have declared one specific node for our web server to ensure the `index.html`
    file location under the `/mnt` directory. We have used `nodeName: kubernetes-node2`
    in our deployment file, `webserver.deployment.yaml`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经为我们的Web服务器声明了一个特定节点，以确保`index.html`文件位于`/mnt`目录下。我们在部署文件`webserver.deployment.yaml`中使用了`nodeName:
    kubernetes-node2`：'
- en: 'First, we verify that there is no file under the `/mnt` directory in the `kubernetes-node2`
    node. We connect to `kubernetes-node2` and then we review the `/mnt` content:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们验证在`kubernetes-node2`节点的`/mnt`目录下没有文件。我们连接到`kubernetes-node2`，然后查看`/mnt`目录的内容：
- en: '[PRE51]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we change to `kubernetes-node1` to clone our repository and launch the
    web server deployment:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们切换到`kubernetes-node1`来克隆我们的仓库并启动Web服务器部署：
- en: '[PRE52]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We move to `chapter9/nginx-lab/yaml`:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入`chapter9/nginx-lab/yaml`目录：
- en: '[PRE53]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We will use the `ConfigMap`, `Secret`, `Service`, `PersistentVolume`, and `PersistentVolumeClaim`
    resources in this lab using YAML files. We will deploy all of the resource files
    in the `yaml` directory:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在本实验中使用`ConfigMap`、`Secret`、`Service`、`PersistentVolume`和`PersistentVolumeClaim`资源，并通过YAML文件部署它们。所有资源文件将部署在`yaml`目录下：
- en: '[PRE54]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now we will review all of the resources created. We have not defined a namespace;
    therefore, the `default` namespace will be used (we omitted it in our commands
    because it is our default namespace). We will use `kubectl get all` to list all
    of the resources available in the default namespace:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将回顾所有创建的资源。我们没有定义命名空间，因此将使用`default`命名空间（我们在命令中省略了它，因为它是默认命名空间）。我们将使用`kubectl
    get all`列出默认命名空间中所有可用的资源：
- en: '[PRE55]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'However, not all of the resources are listed. The `PersistentVolume` and `PersistentVolumeClaim`
    resources are not shown. Therefore, we will ask the Kubernetes API about these
    resources using `kubectl get pv` (`PersisteVolumes`) and `kubectl get pvs` (`PersistenVolumeClaims`):'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并未列出所有资源。`PersistentVolume`和`PersistentVolumeClaim`资源没有显示。因此，我们将使用`kubectl
    get pv`（`PersistentVolumes`）和`kubectl get pvs`（`PersistentVolumeClaims`）命令向Kubernetes
    API查询这些资源：
- en: '[PRE56]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s send some requests to our web server. You can see, in `kubectl get all`
    output, that `webserver-svc` is published using `NodePort` on port `30080`, associating
    the host''s port `30080` with the service''s port `80`. As mentioned earlier,
    all hosts will publish port `30080`; therefore, we can use `curl` on the current
    host (`kubernetes-node1`) and port `30080` to try to reach our web server''s pods:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们向Web服务器发送一些请求。你可以在`kubectl get all`输出中看到，`webserver-svc`是通过`NodePort`在端口`30080`上发布的，将主机端口`30080`与服务端口`80`关联。如前所述，所有主机将发布端口`30080`；因此，我们可以在当前主机（`kubernetes-node1`）和端口`30080`上使用`curl`尝试访问我们的Web服务器的Pods：
- en: '[PRE57]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We have used ;a `ConfigMap` resource to specify an NGINX configuration file,
    `webserver.configmap.yaml`:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了一个`ConfigMap`资源来指定NGINX配置文件`webserver.configmap.yaml`：
- en: '[PRE58]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This configuration is included inside our deployment file, `webserver.deployment.yaml`
    . Here is the piece of code where it is defined:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置包含在我们的部署文件`webserver.deployment.yaml`中。以下是定义部分的代码：
- en: '[PRE59]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The first piece declares where this configuration file will be mounted, while
    the second part links the defined resource: `webserver-test-config`. Therefore,
    the data defined inside the `ConfigMap` resource will be integrated inside the
    web server''s pod as `/etc/nginx/conf.d/default.conf` (take a look at the data
    block).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分声明了这个配置文件将被挂载的位置，而第二部分链接了已定义的资源：`webserver-test-config`。因此，`ConfigMap`资源中定义的数据将集成到Web服务器的Pod中，路径为`/etc/nginx/conf.d/default.conf`（查看数据块）。
- en: 'As mentioned earlier, we also have a `Secret` resource (`webserver.secret.yaml`):'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，我们还拥有一个`Secret`资源（`webserver.secret.yaml`）：
- en: '[PRE60]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can verify, here, that keys are visible while values are not (encoded using
    the Base64 algorithm).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里验证，密钥是可见的，而值则不可见（使用Base64算法进行编码）。
- en: 'We can also create this secret using the imperative format with the `kubectl`
    command line:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`kubectl`命令行使用命令式格式来创建这个密钥：
- en: '`kubectl create secret generic webserver-secret \ --from-literal=PAGETITLE="Docker_Certified_DCA_Exam_Guide"
    \ --from-literal=PAGEBODY="Hello_World_from_Secret"`'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl create secret generic webserver-secret \ --from-literal=PAGETITLE="Docker_Certified_DCA_Exam_Guide"
    \ --from-literal=PAGEBODY="Hello_World_from_Secret"`'
- en: 'We also used this secret resource in our deployment:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在我们的部署中使用了这个密钥资源：
- en: '[PRE61]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In this case, the `PAGETITLE` and `PAGEBODY` keys will be integrated as environment
    variables inside the web server's pod. These values will be used in our lab as
    values for the `index.html` page. `DEFAULT_BODY` and `DEFAULT_TITLE` will be changed
    from the pod's container process.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`PAGETITLE`和`PAGEBODY`键将作为环境变量集成在Web服务器Pod内。这些值将在我们的实验中作为`index.html`页面的值使用。`DEFAULT_BODY`和`DEFAULT_TITLE`将会从Pod的容器进程中更改。
- en: 'This lab has another volume definition. In fact, we have `PersistentVolumeclaim`
    included as a volume in our deployment''s definition:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个实验还有另一个卷定义。实际上，我们在部署的定义中包含了`PersistentVolumeclaim`作为一个卷：
- en: '[PRE62]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The volume claim is used here and is mounted in `/wwwroot` inside the web server's
    pod. `PersistentVolume` and `PersistentVolumeClaim` are defined in `webserver.persistevolume.yaml`
    and `webserver.persistevolumeclaim.yaml`, respectively.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 卷声明在这里使用，并且挂载在Web服务器Pod内的`/wwwroot`目录。`PersistentVolume`和`PersistentVolumeClaim`分别定义在`webserver.persistevolume.yaml`和`webserver.persistevolumeclaim.yaml`中。
- en: 'Finally, we have an `emptyDir` volume definition. This will be used to bypass
    the container''s filesystem and save the NGINX logs:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有一个`emptyDir`卷定义。这个将用于绕过容器的文件系统并保存NGINX日志：
- en: '[PRE63]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The first pod execution will create a default `/wwwroot/index.html` file inside
    it. This is mounted inside the `kubernetes-node2` node's filesystem, inside the
    `/mount` directory. Therefore, after this first execution, we find that `/mnt/index.html`
    was created (you can verify this by following *step 1* again). The file was published,
    and we get it when we execute `curl 0.0.0.0:30080` in *step 5*.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个Pod执行将会在其中创建一个默认的`/wwwroot/index.html`文件。这个文件被挂载在`kubernetes-node2`节点的文件系统内，位于`/mount`目录。因此，在第一次执行后，我们发现`/mnt/index.html`文件被创建（你可以通过重新执行*步骤1*来验证）。文件已发布，并且我们可以在*步骤5*中执行`curl
    0.0.0.0:30080`来获取它。
- en: 'Our application is quite simple, but it is prepared to modify the content of
    the `index.html` file. As mentioned earlier, the default title and body will be
    changed with the values defined in the secret resource. This will happen after
    the creation of the container if the `index.html` file already exists. Now that
    it has been created, as verified in *step 10*, we can delete the web server''s
    pod. Kubernetes will create a new one, and, therefore, the application will change
    its content. We use `kubectl delete pod`:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的应用程序非常简单，但它准备修改`index.html`文件的内容。如前所述，默认的标题和正文将会被在密钥资源中定义的值所替代。如果`index.html`文件已存在，这将在容器创建后发生。现在它已经创建，如*步骤10*所验证，我们可以删除Web服务器的Pod。Kubernetes将创建一个新的Pod，因此应用程序的内容将会发生变化。我们使用`kubectl
    delete pod`：
- en: '[PRE64]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After a few seconds, a new pod is created (we are using a deployment and Kubernetes
    takes care of the application''s component resilience):'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，一个新的Pod被创建（我们正在使用部署，Kubernetes负责应用程序组件的容错性）：
- en: '[PRE65]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let''s again verify the content of our web server using `curl`:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次使用`curl`验证Web服务器的内容：
- en: '[PRE66]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now the content has changed inside the defined `PersistentVolume` resource.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 现在内容已更改，在定义的`PersistentVolume`资源中。
- en: 'We can also verify the `/mnt/index.html` content in `kubernetes-node2`:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以在`kubernetes-node2`中验证`/mnt/index.html`的内容：
- en: '[PRE67]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this lab, we have used four different volume resources, with different definitions
    and features. These labs were very simple, showing you how to deploy a small application
    on Kubernetes. All of the labs can be easily removed by destroying all the Vagrant
    nodes using `vagrant destroy` from the `environments/kubernetes` directory.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们使用了四种不同的卷资源，具有不同的定义和功能。这些实验非常简单，向你展示了如何在Kubernetes上部署一个小型应用程序。所有的实验都可以通过在`environments/kubernetes`目录中使用`vagrant
    destroy`销毁所有Vagrant节点来轻松删除。
- en: We highly recommend going further with Kubernetes because it will become a part
    of the exam in the near future. However, right now, Kubernetes is outside the
    scope of the Docker Certified Associate exam.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议深入学习Kubernetes，因为它将在不久的将来成为考试的一部分。然而，现在Kubernetes超出了Docker认证助理考试的范围。
- en: Summary
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we quickly reviewed some of Kubernetes' main features. We compared
    most of the must-have orchestration features with those discussed in [Chapter
    8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml), *Orchestration Using Docker Swarm*.
    Both provide workload deployment and the management of a distributed pool of nodes.
    They monitor an application's health and allow us to upgrade components without
    service interruption. They also provide networking and publishing solutions.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们快速回顾了 Kubernetes 的一些主要特性。我们将大多数必备的编排功能与 [第8章](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml)
    中讨论的内容进行了比较，*使用 Docker Swarm 进行编排*。两者都提供了工作负载的部署和分布式节点池的管理。它们监控应用程序的健康状况，并允许我们在不中断服务的情况下升级组件。它们还提供了网络和发布解决方案。
- en: Pods provide higher container density, allowing us to run more than one container
    at once. This concept is closer to applications running on virtual machines and
    makes container adoption easier. Services are logical groups of pods and we can
    use them to expose applications. Service discovery and load balancing work out
    of the box dynamically.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 提供更高的容器密度，允许我们同时运行多个容器。这个概念更接近于在虚拟机上运行的应用程序，使得容器的采用更加容易。服务是 Pod 的逻辑组合，我们可以用它们来暴露应用程序。服务发现和负载均衡开箱即用，并且是动态的。
- en: Cluster-wide networking requires additional plugins in Kubernetes, and we also
    learned that a flat network can facilitate routing on different hosts and make
    some things easier; however, it does not provide security by default. Kubernetes
    provides enough mechanisms to ensure network security using network policies and
    single endpoints for multiple services with ingress. Publishing applications is
    even easier with ingress. It adds internal load balancing features dynamically
    with rules managed using ingress resources. This allows us to save up node ports
    and public IP addresses within the environment.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 集群级别的网络需要在 Kubernetes 中安装额外的插件，我们还了解到，扁平化网络可以促进不同主机上的路由，并使一些操作更加简单；然而，它默认并不提供安全性。Kubernetes
    提供了足够的机制来确保网络安全，利用网络策略和单一端点提供多个服务的入口。通过入口（Ingress）发布应用程序变得更加容易。它动态地增加了内部负载均衡功能，并通过使用入口资源管理规则。这使我们能够节省节点端口和公共
    IP 地址。
- en: At the end of the chapter, we reviewed a number of points about Kubernetes security.
    We discussed how RBAC provides different environments to users running their workloads
    on the same cluster. We also talked about some features provided by Kubernetes
    to ensure default security on resources.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们回顾了 Kubernetes 安全性的多个要点。我们讨论了 RBAC 如何为在同一集群中运行工作负载的用户提供不同的环境。我们还谈到了一些
    Kubernetes 提供的功能，以确保资源的默认安全性。
- en: There is much more to learn about Kubernetes, but we will have to end this chapter
    here. We highly recommend that you follow the Kubernetes documentation and the
    release notes on the project's website ([https://kubernetes.io/](https://kubernetes.io/)).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Kubernetes，还有更多内容需要学习，但我们将在这里结束本章。我们强烈建议您查看 Kubernetes 文档和项目网站上的发布说明 ([https://kubernetes.io/](https://kubernetes.io/))。
- en: In the next chapter, we'll look at the differences and similarities between
    Swarm and Kubernetes, side by side.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将并排比较 Swarm 和 Kubernetes 的异同。
- en: Questions
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which of these features is not included in Kubernetes by default?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面哪个功能在 Kubernetes 中默认不包括？
- en: a) An internal key-value store.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: a) 一个内部的键值存储。
- en: b) Network communication between containers distributed on different Docker
    hosts.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: b) 分布在不同 Docker 主机上的容器之间的网络通信。
- en: c) Controllers for deploying workload updates without service interruptions.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: c) 用于在没有服务中断的情况下部署工作负载更新的控制器。
- en: d) None of these features are included.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: d) 这些功能都没有包含在内。
- en: Which of these statements is true about pods?
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于 Pod，下面哪个说法是正确的？
- en: a) Pods always run in pairs to provide an application with high availability.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: a) Pod 总是成对运行，以提供应用程序的高可用性。
- en: b) Pods are the minimum unit of deployment on Kubernetes.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: b) Pod 是 Kubernetes 中最小的部署单元。
- en: c) We can deploy more than one container per pod.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: c) 我们可以在每个 Pod 中部署多个容器。
- en: d) We need to choose which containers in a pod should be replicated when pods
    are scaled.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: d) 我们需要选择在 Pod 扩展时应该复制哪些容器。
- en: Which of these statements is true about pods?
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于 Pod，下面哪个说法是正确的？
- en: a) All pod containers run using a unique network namespace.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: a) 所有的 Pod 容器都使用独立的网络命名空间运行。
- en: b) All containers within a pod can share volumes.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: b) Pod 中的所有容器可以共享卷。
- en: c) All pods running on Docker Engine are accessible from the host using their
    IP addresses.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: c) 所有在 Docker 引擎上运行的 Pod 都可以通过它们的 IP 地址从主机进行访问。
- en: d) All of these statements are true.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: d) 以上所有陈述都正确。
- en: Kubernetes provides different controllers to deploy application workloads. Which
    of these statements is true?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 提供不同的控制器来部署应用程序工作负载。以下哪项陈述是正确的？
- en: a) `DaemonSet` will run one replica on each cluster node.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: a) `DaemonSet` 会在每个集群节点上运行一个副本。
- en: b) `ReplicaSet` will allow us to scale application pods up or down.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: b) `ReplicaSet` 允许我们扩展或缩减应用程序 pods。
- en: c) Deployments are higher-level resources. They manage `ReplicaSet`.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: c) Deployments 是更高层次的资源。它们管理 `ReplicaSet`。
- en: d) All of these statements are true.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: d) 以上所有陈述都正确。
- en: How can we expose services to users in Kubernetes? (Which of these statements
    is false?)
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何在 Kubernetes 中将服务暴露给用户？（以下哪项陈述是错误的？）
- en: a) ClusterIP services provide a virtual IP accessible to users.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: a) ClusterIP 服务提供一个虚拟 IP，用户可以访问。
- en: b) NodePort services listen on all nodes and route traffic using the provided
    ClusterIP to reach all service backends.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: b) NodePort 服务在所有节点上监听，并通过提供的 ClusterIP 路由流量，以便访问所有服务后端。
- en: c) LoadBalancer creates simple load balancers on cloud providers to load balance
    requests to service backends.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: c) LoadBalancer 在云服务提供商上创建简单的负载均衡器，以便将请求负载均衡到服务后端。
- en: d) Ingress controllers help us to use single endpoints (one per ingress controller)
    to load balance requests to non-published services.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: d) Ingress 控制器帮助我们使用单一端点（每个 ingress 控制器一个）将请求负载均衡到未公开的服务。
- en: Further reading
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following links for more information on topics covered
    in this chapter:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接，了解本章所涵盖的主题的更多信息：
- en: 'Kubernetes documentation: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 文档: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
- en: 'Kubernetes concepts: [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/)'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 概念: [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/)'
- en: 'Kubernetes learning tasks: [https://kubernetes.io/docs/tasks/](https://kubernetes.io/docs/tasks/)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 学习任务: [https://kubernetes.io/docs/tasks/](https://kubernetes.io/docs/tasks/)'
- en: 'Kubernetes on Docker Enterprise: [https://docs.docker.com/ee/ucp/kubernetes/kube-resources](https://docs.docker.com/ee/ucp/kubernetes/kube-resources)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes on Docker Enterprise: [https://docs.docker.com/ee/ucp/kubernetes/kube-resources](https://docs.docker.com/ee/ucp/kubernetes/kube-resources)'
- en: '*Getting Started with Kubernetes*: [https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes 入门指南*: [https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
