- en: Operating FaaS Clusters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作FaaS集群
- en: One of the hardest things about having a system up and running is administering
    and maintaining our own clusters. Although serverless is a paradigm aimed at solving
    this problem entirely, in reality, there are some situations where we still need
    to provision and take care of servers by ourselves.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让系统运行并稳定起来最困难的事情之一就是管理和维护我们自己的集群。尽管无服务器计算是一种旨在完全解决这个问题的范式，但实际上，在某些情况下，我们仍然需要自行配置和管理服务器。
- en: The idea behind serverless and Docker is to have a balance between reducing
    cluster maintenance and administration, and having full control of the cluster.
    Using Docker is a great way to help balance this.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器计算和Docker背后的理念是，在减少集群维护和管理的同时，能够完全控制集群。使用Docker是一种帮助实现这一平衡的好方法。
- en: Along with this balance, the most attractive driving factor for serverless is
    the *price model*. However, we have found that using Docker on EC2 Spot instances,
    given the competitive price, is sometimes even cheaper than AWS Lambda or other
    cloud functions. So with Spot instances, we will get the cheaper price, while
    our functions will not hit any limitation found in AWS Lambda or others.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种平衡之外，最具吸引力的无服务器计算驱动力是*价格模型*。然而，我们发现，在EC2 Spot实例上使用Docker，由于其具有竞争力的价格，有时甚至比AWS
    Lambda或其他云函数还便宜。因此，使用Spot实例时，我们可以获得更便宜的价格，同时我们的功能不会受到AWS Lambda或其他云平台所遇到的任何限制。
- en: Operating Docker-based FaaS clusters uses the same techniques as operating Docker
    clusters. We need to mix the techniques of running standalone Docker together
    with the techniques to utilize the Docker Swarm mode. This chapter focuses on
    *configuration stabilization*, how to prepare the new ingress layer, how to use
    a network plugin, how to set up the logging system, and how to operate the cluster
    using Golang scripting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 操作基于Docker的FaaS集群使用的技术与操作Docker集群相同。我们需要将独立运行Docker的技术与利用Docker Swarm模式的技术结合起来。本章重点介绍*配置稳定性*、如何准备新的入口层、如何使用网络插件、如何设置日志系统，以及如何使用Golang脚本操作集群。
- en: Stabilizing the configuration
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定配置
- en: 'Let''s start by carefully stabilizing the cluster configuration. At the time
    of writing, a Docker cluster works best with the following configuration. *Figure:
    7.1* illustrated in this section depicts it well:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们首先仔细稳定集群配置。在撰写本文时，Docker集群在以下配置下表现最佳。本节中图*Figure: 7.1*展示了这一点：'
- en: '**Ubuntu Server 16.04.3 LTS**: Although Red Hat Linux or CentOS may work best
    for you, Ubuntu Server is easy to handle. We are constantly informed that Docker
    has been really well tested with Ubuntu Server. If you choose to use Red Hat or
    CentOS, please go with version 7.4.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ubuntu Server 16.04.3 LTS**：尽管Red Hat Linux或CentOS可能更适合你，但Ubuntu Server更容易处理。我们不断得知，Docker在Ubuntu
    Server上的测试表现非常好。如果你选择使用Red Hat或CentOS，请选择7.4版本。'
- en: '**Linux Kernel 4.4 LTS**: The 4.4 kernel is an LTS and it''s great for Docker.
    You can also use kernel 4.9 but the kernel, like 4.13, is still too new for Docker.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux Kernel 4.4 LTS**：4.4内核是LTS版本，非常适合Docker使用。你也可以使用4.9内核，但像4.13这样的内核对于Docker来说仍然太新。'
- en: '**Overlay2** **as the Docker storage driver**: Although the **advanced multi-layered
    unification filesystem** (**AUFS**) has worked well for Docker for quite a long
    time, overlay2 should be the new default storage driver for Docker running on
    the 4.4+ kernel. If you get a chance to run a production cluster on CentOS or
    RHEL 7.4, overlay2 is also a good option on these distributions.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Overlay2** **作为Docker存储驱动程序**：尽管**高级多层统一文件系统**（**AUFS**）已在Docker中使用了很长时间并且表现良好，但对于运行4.4+内核的Docker来说，overlay2应该是新的默认存储驱动程序。如果你有机会在CentOS或RHEL
    7.4上运行生产集群，overlay2也是这些发行版上的一个不错选择。'
- en: '**Docker CE 17.06.2** **or 17.09.1**: Docker EE 17.06 is also a great option,
    if you can afford the enterprise edition:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker CE 17.06.2** **或17.09.1**：如果你能负担得起企业版，Docker EE 17.06也是一个不错的选择：'
- en: '![](img/43707075-e64a-4bdb-93a6-63f8b53205c4.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43707075-e64a-4bdb-93a6-63f8b53205c4.png)'
- en: 'Figure 7.1: A stabilized Docker Swarm stack with Træfik and WeaveWorks network
    plugin'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：一个稳定的Docker Swarm堆栈，配有Træfik和WeaveWorks网络插件
- en: Choosing the right network plugin
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的网络插件
- en: For a long time, people have said that the default Docker overlay network is
    not great for production. Although the quality of the overlay network driver is
    getting better and better, we may look at some other network plugins for optimum
    results. We can replace the default overlay driver with other plugins, for example,
    WeaveWorks or Contiv. We use WeaveWorks network plugin version 2 in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，人们一直说默认的 Docker 覆盖网络不适合生产环境。尽管覆盖网络驱动程序的质量越来越好，但我们可能会考虑一些其他的网络插件，以获得最佳效果。我们可以将默认的覆盖驱动程序替换为其他插件，例如
    WeaveWorks 或 Contiv。本章中我们使用的是 WeaveWorks 网络插件版本 2。
- en: '*Why WeaveWorks?*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么选择 WeaveWorks？*'
- en: The WeaveWorks network plugin for Docker uses the same underlying network implementation
    as those of Kubernetes CNI. It has also been battle tested by its development
    team, WeaveWorks Inc. Additionally, it has been working really great so far, on
    my production clusters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: WeaveWorks 网络插件用于 Docker，采用与 Kubernetes CNI 相同的底层网络实现。它还经过了开发团队 WeaveWorks Inc.
    的严格测试。此外，它在我的生产集群中表现非常出色。
- en: WeaveWorks network plugin version 2.1.3, in order to avoid disconnection bugs
    found in the current version of the overlay network driver, it is recommended
    entirely removing the default ingress network, which is based on the default overlay
    network driver, in production. A question may be raised here. If the ingress network
    is removed, we will lose the whole routing mesh, so then how can we route traffic
    into the cluster? The answer is in the next section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: WeaveWorks 网络插件版本 2.1.3，为了避免当前版本的覆盖网络驱动程序中发现的断开连接 bug，建议在生产环境中完全移除默认的入口网络，该网络基于默认的覆盖网络驱动程序。这里可能会有人提出疑问。如果移除入口网络，我们将失去整个路由网格，那么我们该如何将流量路由到集群呢？答案在下一节中。
- en: New ingress and routing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的入口和路由
- en: 'As previously mentioned, we will not use the default Docker *ingress network*
    for *routing requests* to the running container:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将不使用默认的 Docker *入口网络*来进行 *请求路由*到正在运行的容器：
- en: '![](img/98d729e9-1aa1-4d8b-8e19-a99df09ecaa6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98d729e9-1aa1-4d8b-8e19-a99df09ecaa6.png)'
- en: 'Figure 7.2: The new ingress layer built on top of Træfik, connected to underlying
    Swarm tasks to form a routing mesh'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：构建在 Træfik 上的新入口层，连接到底层的 Swarm 任务，形成路由网格
- en: 'Yes, we will lose the routing mesh, but we will build our own instead. As shown
    in the previous figure, we will replace the default routing mesh with a new ingress
    layer built on top of an L7 load balancer, **Træfik**. You can choose one from
    the following list of stable versions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们将失去路由网格，但我们将建立我们自己的路由网格。如前图所示，我们将用构建在 L7 负载均衡器 **Træfik** 上的新入口层替代默认的路由网格。你可以从以下稳定版本列表中选择一个：
- en: Træfik v1.4.5 (`traefik@sha256:9c299d9613`)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Træfik v1.4.5 (`traefik@sha256:9c299d9613`)
- en: Træfik v1.4.6 (`traefik@sha256:89cb51b507`)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Træfik v1.4.6 (`traefik@sha256:89cb51b507`)
- en: The advantage of using Træfik is that the newly built ingress layer is better
    stabilized. Each service is automatically resolved to be a list of IP addresses
    by Træfik. So you can choose to use either an IPVS-based load balancer offered
    by Docker Swarm, or the built-in mechanism offered by Træfik itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Træfik 的优势在于，新的入口层得到了更好的稳定性。每个服务都会由 Træfik 自动解析为一组 IP 地址。因此，你可以选择使用 Docker
    Swarm 提供的基于 IPVS 的负载均衡器，或者使用 Træfik 本身提供的内置机制。
- en: As Træfik works with the L7 layer, we are additionally allowed to match services
    with the hostname, and forward the request to a certain task of the matched service.
    Also, with this new implementation, we could flexibly restart or re-configure
    the ingress layer on-the-fly without touching the running services. This has been
    a weak point of the Docker's ingress layer for a very long time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Træfik 与 L7 层配合使用，我们还可以根据主机名来匹配服务，并将请求转发到匹配服务的某个任务。此外，通过这种新实现，我们可以灵活地重新启动或重新配置入口层，而无需触及正在运行的服务。这一直是
    Docker 入口层的一个弱点。
- en: Tracing component
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪组件
- en: 'In the architecture proposed in this book, we use Envoy as a sidecar proxy
    for every deployed function. With Envoy, it allows distributed trace calling between
    functions, as in illustrated in the following figure, even if they are prepared
    by or deployed to different FaaS platforms. This is really an important step for
    avoiding vendor lock-in. Envoy is compiled and pushed to Docker hub incrementally.
    We have picked a certain version of Envoy for this book: **E****nvoyProxy**, `envoyproxy/envoy:29989a38c017d3be5aa3c735a797fcf58b754fe5`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书提出的架构中，我们为每个部署的函数使用Envoy作为sidecar代理。有了Envoy，即使这些函数是由不同的FaaS平台准备或部署的，它也能实现函数之间的分布式追踪调用，如下图所示。这实际上是避免厂商锁定的重要一步。Envoy被编译并逐步推送到Docker
    hub。我们为本书选择了某个特定版本的Envoy：**EnvoyProxy**，`envoyproxy/envoy:29989a38c017d3be5aa3c735a797fcf58b754fe5`：
- en: '![](img/b96914b6-55b6-460b-bed4-f0241670efac.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b96914b6-55b6-460b-bed4-f0241670efac.png)'
- en: 'Figure 7.3: A block diagram showing the distributed tracing mechanism with
    Envoy'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：展示带有Envoy的分布式追踪机制的框图
- en: 'The following figure shows two levels of implementation for the sidecar proxy
    pattern. First, we directly tweak the `Dockerfile` of a function or a service
    by embedding the **EnvoyProxy** binary into the Docker image. This technique yields
    the best performance because **EnvoyProxy** talks to the function program through
    the **loopback** interface inside the container. But when we need to change the
    configuration of Envoy, such as *retry* or *circuit breaker*, we need to restart
    the **EnvoyProxy** together with the function instance, shown as the first (**1**)
    configuration in the following figure:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了sidecar代理模式的两种实现级别。首先，我们直接通过将**EnvoyProxy**二进制文件嵌入到Docker镜像中来调整函数或服务的`Dockerfile`。这种技术提供了最佳的性能，因为**EnvoyProxy**通过容器内的**loopback**接口与函数程序进行通信。但当我们需要更改Envoy的配置，如*重试*或*断路器*时，我们需要重新启动**EnvoyProxy**和函数实例，如下图所示的第一种（**1**）配置：
- en: '![](img/9ed59fa0-2acc-46b3-8d8a-1cc2742e5095.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ed59fa0-2acc-46b3-8d8a-1cc2742e5095.png)'
- en: 'Figure 7.4: Two configurations to implement Envoy as (1) sidecar proxy and
    (2) edge proxy'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：两种配置方式将Envoy实现为（1）sidecar代理和（2）边缘代理
- en: So the better configuration when it comes to flexibility and management is the
    second (**2**) configuration, where we separate **EnvoyProxy**, as an edge proxy,
    out of the function container. The trade-off here is the network overheads between
    them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在灵活性和管理性方面，最佳的配置是第二种（**2**）配置，在这种配置中，我们将**EnvoyProxy**作为边缘代理从函数容器中分离出来。这里的权衡是它们之间的网络开销。
- en: Retry and circuit breaker
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重试和断路器
- en: 'In this section, we discuss one of the most interesting topics to date: the
    retry and circuit breaker pattern. It would be great to get familiar with this
    concept before proceeding to implementing a production cluster.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论一个迄今为止最有趣的话题：重试和断路器模式。在继续实现生产集群之前，熟悉这一概念会非常有帮助。
- en: Retry
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重试
- en: 'The problem solved by retry and circuit breaker stems from cascade failures
    caused by a service or a function inside a chain of calling becoming unavailable.
    In the following figure, we assume that five different functions or services have
    99% availability, so they will fail once every 100 calls. The client observing
    this service''s chain will experience the availability of **A** at only **95.09%**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重试和断路器解决的问题源于服务或链中某个函数变得不可用所引发的级联故障。在下图中，我们假设五个不同的函数或服务的可用性为99%，因此它们每100次调用会失败一次。观察这个服务链的客户端将体验到的可用性为**A**，仅为**95.09%**：
- en: '![](img/bf31eea3-05e5-4a42-8990-4654a64e0bc7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf31eea3-05e5-4a42-8990-4654a64e0bc7.png)'
- en: 'Figure 7.5: A chain of functions or microservices would make their overall
    availability lower'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：一系列函数或微服务将使它们的整体可用性降低
- en: What does this imply? It means that when this chain becomes eight functions
    long, the availability will become 92.27%, and if it's 20 functions long, this
    figure will decrease to 81.79%. To reduce the failure rate, we should retry calling
    to another instance of function or service when an error, such as HTTP 500, occurs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么？这意味着当这个链条变成八个函数时，系统的可用性将降至92.27%；如果链条长达20个函数，这个数字将降到81.79%。为了降低故障率，当出现错误（如HTTP
    500）时，我们应该重试调用另一个实例的函数或服务。
- en: But a simple or constant-rate retry is not enough. If we use a simple strategy,
    our retry calls would increase unnecessary loads to the already broken service.
    This would cause more problems than it would solve.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，简单的或常规速率的重试是不够的。如果我们使用一个简单的策略，我们的重试调用会给已经故障的服务带来不必要的负载，这会引发比解决更多的问题。
- en: Circuit breaker
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电路断路器
- en: 'To resolve this problem, many retry pattern implementations usually come with
    **Exponential Back-off Retry**. With the exponential back-off strategy, we gradually
    increase the delay between each retry. For example, we may retry the second call
    to the service 3 seconds after the fault occurs. If the service still returns
    an error, we increase the delay to 9 seconds and 27 seconds, for the third and
    fourth calls respectively. This strategy leaves some room for the service to recover
    from transient faults. The difference between two kinds of retry strategies is
    shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，许多重试模式实现通常会使用**指数回退重试**。通过指数回退策略，我们逐步增加每次重试之间的延迟。例如，发生故障后，我们可能会在 3
    秒钟后重试第二次调用。如果服务仍然返回错误，我们将延迟增加到 9 秒和 27 秒，分别对应第三次和第四次调用。这种策略给服务留出一定的时间来恢复临时故障。两种重试策略的区别如下图所示：
- en: '![](img/0abeef59-8c82-41bf-9643-5f04d99e7d9a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0abeef59-8c82-41bf-9643-5f04d99e7d9a.png)'
- en: 'Figure 7.6: The difference between the constant-rate retry and exponential
    back-off retry strategies'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：恒定速率重试与指数回退重试策略的区别
- en: Preparing a production cluster
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备生产集群
- en: In this section, we will discuss how to prepare a production Docker Swarm cluster
    to run FaaS platforms at the cheapest rate possible on AWS Spot instances. The
    cost of deploying a Docker cluster would be as cheap as running codes on AWS Lambda,
    but it allows us to control almost everything in our cluster. If the deployment
    policy is cost-driven, this is the best way to go.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何准备生产级 Docker Swarm 集群，以便以最低的成本在 AWS Spot 实例上运行 FaaS 平台。部署 Docker
    集群的成本将与在 AWS Lambda 上运行代码的成本相当，但它让我们几乎可以控制集群中的所有内容。如果部署策略以成本为驱动，那么这是最佳的选择。
- en: Cost savings with Spot instances
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spot 实例节省成本
- en: 'When we are talking about the cloud, its on-demand instances are actually cheap
    already. However, in the long run, the price of using cloud instances will be
    similar to buying real machines. To solve this pricing problem, major cloud providers,
    such as Amazon EC2, and Google Cloud Platform, provide a new instance type, collectively
    called a **Spot instance** in this book:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论云时，它的按需实例实际上已经很便宜了。然而，从长远来看，使用云实例的价格将与购买实物机器相似。为了解决这个定价问题，主要的云服务提供商，如 Amazon
    EC2 和 Google Cloud Platform，提供了一种新的实例类型，在本书中统称为**Spot 实例**：
- en: '![](img/18255aa6-7399-4cf8-83df-42ed2065c696.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18255aa6-7399-4cf8-83df-42ed2065c696.png)'
- en: 'Figure 7.7: Comparison of shutdown signals of a Spot instance on AWS versus
    Google Cloud'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：AWS 与 Google Cloud 上 Spot 实例的关机信号对比
- en: Spot instances are far cheaper than on-demand instances. However, their weak
    point is the short life cycle and unexpected termination. That is, a Spot instance
    could be terminated at any time. When it is gone, you have a choice as to whether
    to preserve or completely discard the volumes. On AWS, the instance will get the
    notification around 120 seconds before termination via remote metadata, while
    on Google Cloud, the notification will be sent via an ACPI signal 30 seconds before
    the machine stops. The rough comparison is shown in the previous figure.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Spot 实例比按需实例便宜得多。然而，它们的弱点是生命周期短且会意外终止。也就是说，Spot 实例可能随时被终止。当它终止时，你可以选择保留或完全丢弃卷。在
    AWS 上，实例将在终止前约 120 秒通过远程元数据发出通知，而在 Google Cloud 上，则会在机器停止前 30 秒通过 ACPI 信号发送通知。粗略的对比如前图所示。
- en: We could put stateless computing to run on these kinds of instances. Both microservices
    and functions are naturally stateless, so Spot instances fit with the deployment
    of microservices and functions nicely.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将无状态计算部署在这些实例上。微服务和函数本身都是无状态的，因此 Spot 实例非常适合微服务和函数的部署。
- en: With this kind of infrastructure on cheap instances, its cost will be comparable
    to AWS Lambda or Google Cloud Functions, but we are more in control of the overall
    system, meaning no invocation timeout for functions on this kind of infrastructure.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种廉价实例的基础设施，其成本将与 AWS Lambda 或 Google Cloud Functions 相当，但我们对整个系统有更多的控制，这意味着在这种基础设施上运行的函数不会有调用超时问题。
- en: Using EC2 Spot instances
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 EC2 Spot 实例
- en: 'On Amazon EC2, go to [https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/) and
    we will find the page as shown in the following screenshot. Log onto the AWS Console
    for Spot instances to set up some of them:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon EC2 上，访问 [https://aws.amazon.com/ec2/spot/](https://aws.amazon.com/ec2/spot/)
    你将看到如下截图页面。登录 AWS 控制台，设置一些 Spot 实例：
- en: '![](img/0db6a15c-462f-4395-8ee5-28d1bf12fe96.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0db6a15c-462f-4395-8ee5-28d1bf12fe96.png)'
- en: 'Figure 7.8: The landing page of AWS Spot instances'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：AWS Spot 实例的首页
- en: 'On the navigation bar, we see Spot Requests. Click it to go to the Spot Requests
    screen as shown in the following screenshot. On this screen, clicking Request
    Spot Instances starts the request process:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在导航栏上，我们看到了 Spot Requests。点击它进入 Spot Requests 屏幕，如下截图所示。在此屏幕上，点击请求 Spot 实例 开始请求流程：
- en: '![](img/e688906e-192c-4731-a8c2-881348b7bc76.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e688906e-192c-4731-a8c2-881348b7bc76.png)'
- en: 'Figure 7.9: The Spot Requests screen on AWS displaying a request with its associated
    instances'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：AWS 上显示请求及其相关实例的 Spot Requests 屏幕
- en: 'There are three models for requesting Spot instances:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请求 Spot 实例有三种模式：
- en: One-time request. This is one time only, so when the instance is gone, we need
    to do another request.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次性请求。这是一次性的请求，所以当实例消失时，我们需要再次请求。
- en: Request a fleet of instances and let AWS maintain the number of target instances.
    When some instances are terminated, AWS will try its best, depending on our maximum
    bidding price, to allocate instances to meet the target numbers of each fleet.
    We have opted for this request model in this chapter.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求一组实例，并让 AWS 保持目标实例的数量。当一些实例被终止时，AWS 将尝试根据我们的最高竞价价格分配实例，以满足每个舰队的目标数量。在本章中，我们选择了此请求模型。
- en: Request instances for a fixed period of time. A fixed period is called a **Spot
    block**, which is between 1 and 6 hours. We will pay more if we set the longer
    period.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求一段固定时间内的实例。固定时间被称为**Spot block**，介于1到6小时之间。如果设置较长的时间段，我们将支付更多费用。
- en: 'The following diagram shows what the cluster in preparation will look like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了正在准备中的集群的结构：
- en: '![](img/e5d916bd-1965-4eec-b6cb-fe704ca74c96.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5d916bd-1965-4eec-b6cb-fe704ca74c96.png)'
- en: 'Figure 7.10: A Docker cluster forming on Spot instances using an automatic
    operator to take care of it'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：使用自动操作员在 Spot 实例上形成的 Docker 集群
- en: Assume that we already have three boxes provisioned to be managers. To get the
    cheapest rate possible, it is recommended using three on-demand EC2 nodes as Docker
    managers, and N-3 Spot instances as Docker workers. We start small with three
    Spot workers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经准备好三个箱子作为管理者。为了获得最便宜的费率，建议使用三个按需 EC2 节点作为 Docker 管理者，以及 N-3 个 Spot 实例作为
    Docker 工作节点。我们从三个 Spot 工作节点开始。
- en: If possible, choose a cloud provider that allows you to create a private network
    and floating IPs. We will form a Docker cluster on the private network. Most cloud
    providers allow this, so do not worry.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，请选择允许您创建私有网络和浮动 IP 的云服务提供商。我们将在私有网络上形成一个 Docker 集群。大多数云服务提供商都允许这样做，所以请不要担心。
- en: Let's start
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们开始
- en: 'First, SSH into a node we would like to be the first manager, install Docker,
    and run the `docker swarm init` command on it. The `eth0` is the private network
    interface provided by the cloud provider. Check yours using the `ip addr` command
    before proceeding. If you know which interface is the private one, initialize
    the cluster using the following command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，SSH 进入我们希望成为第一个管理节点的节点，安装 Docker，并在其上运行`docker swarm init`命令。`eth0`是云服务提供商提供的私有网络接口。在继续之前，请使用`ip
    addr`命令检查您的接口。如果您知道哪个接口是私有的，请使用以下命令初始化集群：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, SSH into the other two nodes. Install Docker and join the cluster using
    the `docker swarm join` command. Do not forget that we need to use the join token
    for the *manager*, not for the worker. The token in the following example is the
    manager token. Please note that my first manager''s IP is `172.31.4.52` during
    this setup. Replace it with your IP address:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，SSH 进入其他两个节点。安装 Docker，并使用`docker swarm join`命令加入集群。不要忘记我们需要使用*管理者*的加入令牌，而不是工作节点的。请注意，在此设置过程中，我的第一个管理者的
    IP 是`172.31.4.52`。请将其替换为您的 IP 地址：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For these first three nodes, do not forget to label them as managers to help
    you remember.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这三个第一节点，不要忘记将它们标记为管理者，以帮助您记住。
- en: 'Here, please make sure that `docker info` shows the list of managers, containing
    all their private IP addresses. We use `grep -A3` to see the next three lines
    after the target:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，请确保`docker info`显示包含所有管理器的私有 IP 地址列表。我们使用`grep -A3`来查看目标后的三行：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or, if you are familiar with the `jq` command, try the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您熟悉`jq`命令，可以尝试以下操作：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `docker info` command also accepts `--format` to let us customize the output.
    In the previous example, we used the JSON method provided by the template to generate
    JSON output. Then we used `jq` to query the IP addresses of all the Swarm managers.
    The combination of JSON templating and `jq` will be a great tool to build our
    own set of Docker-based scripts for operating clusters in the long term.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker info` 命令还支持 `--format` 选项，允许我们自定义输出。在前面的示例中，我们使用模板提供的 JSON 方法来生成 JSON
    输出。然后，我们使用 `jq` 查询所有 Swarm 管理节点的 IP 地址。JSON 模板和 `jq` 的结合将是构建我们自己的基于 Docker 脚本来长期操作集群的一个好工具。'
- en: Workers on Spot instances
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spot 实例上的工作节点
- en: 'Then, we will provision another three nodes as a fleet of Spot instances. Here,
    in the following screenshot, it shows the setup to request a fleet of three Spot
    instances. Choose the Request and Maintain option, then set the Target capacity
    to `3` instances:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将再配置三个节点作为 Spot 实例的舰队。在下图中，展示了请求一个包含三个 Spot 实例的舰队的设置。选择 "Request and Maintain"
    选项，然后将目标容量设置为 `3` 个实例：
- en: '![](img/69a23956-d79a-4188-964d-65b163e10155.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69a23956-d79a-4188-964d-65b163e10155.png)'
- en: 'Figure 7.11: Requesting and maintaining a fleet of 3 instances'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：请求并维护一个包含 3 个实例的舰队
- en: 'We configure the setup script to install Docker, join the node to the cluster,
    and set up the network driver upon the instance creation. The setup must be put
    into the User data section of the fleet setup, as shown in the following screenshot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们配置了设置脚本，以在实例创建时安装 Docker，加入节点到集群，并设置网络驱动程序。此设置必须放入舰队设置的用户数据部分，如下图所示：
- en: '![](img/3c4f9f38-655a-441b-bbdd-8119fd64fc6d.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c4f9f38-655a-441b-bbdd-8119fd64fc6d.png)'
- en: 'Figure 7.12: Putting join instructions into the request''s user data'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：将连接指令放入请求的用户数据中
- en: 'Here''s the script used in the User data section. Please replace `$TOKEN` with
    your worker''s token, and `$MANAGER_IP` with one of your manager''s private IP
    addresses:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在用户数据部分使用的脚本。请将 `$TOKEN` 替换为您的工作节点令牌，将 `$MANAGER_IP` 替换为您的一个管理节点的私有 IP 地址：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we wait until the fleet request is fulfilled.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们等待舰队请求完成。
- en: 'If we get into the first manager, we could check the current nodes in the cluster
    with the `docker node ls` command. If everything is OK, we should have six nodes
    in the cluster:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进入第一个管理节点，可以使用 `docker node ls` 命令查看集群中的当前节点。如果一切正常，集群中应该有六个节点：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With this technique, we can easily scale the cluster by simply adjusting the
    number of Spot instances.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此技术，我们可以通过简单地调整 Spot 实例的数量来轻松扩展集群。
- en: Working with the network plugin
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网络插件
- en: As we can see in the User data section in the fleet setup, there will be a line
    of the script that installs the network plugin for us. It is the WeaveWorks network
    plugin. The WeaveWorks network plugin uses the information from the `docker info` command
    to list the IP addresses of all the Swarm managers. The plugin then uses these
    IP addresses to bootstrap the network mesh.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在舰队设置中的用户数据部分所见，脚本中会有一行指令来为我们安装网络插件。它是 WeaveWorks 网络插件。WeaveWorks 网络插件使用来自
    `docker info` 命令的信息来列出所有 Swarm 管理节点的 IP 地址。然后，插件利用这些 IP 地址来启动网络网格。
- en: The WeaveWorks network plugin must be installed only after you successfully
    form the set of managers in the cluster.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在成功形成集群中的管理节点集合后，才能安装 WeaveWorks 网络插件。
- en: We use WeaveWorks network plugin 2.1.3\. This is the most stable version of
    it at the time of writing. It is also recommended upgrading to the next minor
    versions of this plugin, if available.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 WeaveWorks 网络插件 2.1.3。这是截至写作时最稳定的版本。如果有可用的版本，建议升级到此插件的下一个小版本。
- en: 'To install the network plugin, we use the `docker plugin install` command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装网络插件，我们使用 `docker plugin install` 命令：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We use `--grant-all-permissions` just to automate the installation step. Without
    this parameter, we must manually grant the permissions required by each plugin.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `--grant-all-permissions` 只是为了自动化安装步骤。如果没有此参数，我们必须手动授予每个插件所需的权限。
- en: We need to install a plugin for every single node in the cluster, which means
    we need to do this six times for our six boxes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为集群中的每个节点安装插件，这意味着我们需要为我们的六个节点执行此操作六次。
- en: 'We could check to see whether the network plugin is installed correctly using
    the following command:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查网络插件是否正确安装：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `ENABLED` status of the plugin will be `true`, meaning that it is currently
    active. To check the status of the WeaveWork plugin and its network mesh, the
    plain text status could be CURLed from `localhost:6782/status`. The following
    status information was obtained from a worker node. We can check the number of
    connections between peers, or a number of peers, for example, from that URL:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 插件的`ENABLED`状态为`true`，表示它当前处于活动状态。要检查WeaveWorks插件及其网络网格的状态，可以通过CURL从`localhost:6782/status`获取纯文本状态信息。以下状态信息来自一个工作节点。我们可以从该URL检查对等节点之间的连接数或对等节点的数量，例如：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The previous example shows us having six peers with five connections each. The
    IP range and the default subnet are important information for us to use when we
    create Docker networks. The IP range is `10.32.0.0/12`, so if we create a network
    with subnet `10.32.0.0/24`, it will be valid, while `10.0.0.0/24` will be invalid,
    for example.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例显示我们有六个对等节点，每个节点有五个连接。IP范围和默认子网是我们在创建Docker网络时需要使用的重要信息。IP范围是`10.32.0.0/12`，因此如果我们创建一个子网为`10.32.0.0/24`的网络，它将是有效的，而`10.0.0.0/24`则是无效的，例如。
- en: 'The following figure illustrates our WeaveWorks network topology. Each node
    has five connections to another five nodes, as shown by solid lines from an **mg**
    node pointing to others. To make the diagram comprehensible, it shows only an
    **mg** node and another **wk** node connecting their five lines to the rest of
    the peers in the cluster:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们的WeaveWorks网络拓扑。每个节点与其他五个节点有五个连接，图中的实线从**mg**节点指向其他节点。为了使图示易于理解，图中仅展示一个**mg**节点和另一个**wk**节点，它们将五条连接线连接到集群中其他的对等节点：
- en: '![](img/7d482584-1a08-4c7e-959a-f6fc0d0c7337.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d482584-1a08-4c7e-959a-f6fc0d0c7337.png)'
- en: 'Figure 7.13: Swarm nodes connecting together via a WeaveWorks full-mesh network'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13：Swarm节点通过WeaveWorks全网格网络相互连接
- en: 'For advanced troubleshooting, we could check the plugin''s running process,
    `weaver`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高级故障排除，我们可以检查插件的运行进程`weaver`：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see from grepping the output of `ps`, the final parts of the command
    are the list of Swarm manager IP addresses. If it looks like this, our networking
    layer is good to go. But if you do not see the list of manager IP addresses here,
    remove the plugin and start over again.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如从`ps`的输出中grep到的内容所示，命令的最后部分是Swarm管理器IP地址的列表。如果它看起来像这样，我们的网络层已经准备就绪。但如果你在这里没有看到管理器IP地址的列表，请删除插件并重新开始。
- en: Creating a network
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网络
- en: When we prepare a network with the WeaveWorks driver, please keep in mind that
    we always need to specify the `--subnet` and `--gateway` parameters as we do not
    use the default subnet value provided by the Docker's libnetwork. We need to make
    a network attachable, with `--attachable`, to allow containers started using `docker
    run` command attach to the network. Without this option, only Swarm services,
    started by `docker service create`, are allowed to join the network.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用WeaveWorks驱动程序准备网络时，请记住，我们始终需要指定`--subnet`和`--gateway`参数，因为我们不使用Docker的libnetwork提供的默认子网值。我们需要使网络可附加，使用`--attachable`，以便使用`docker
    run`命令启动的容器能够附加到该网络。如果没有这个选项，只有通过`docker service create`启动的Swarm服务才能加入该网络。
- en: 'For example, we can create a *class C* network using the following command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下命令创建一个*class C*网络：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Creating an operational control plane
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个操作控制平面
- en: An operational control plane is where we deploy operator containers to help
    operate the cluster. It is a concept that stems from the CoreOS's operator pattern,
    [https://coreos.com/blog/operators](https://coreos.com/blog/operators).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 操作控制平面是我们部署操作员容器以帮助操作集群的地方。它是源自CoreOS操作员模式的一个概念，[https://coreos.com/blog/operators](https://coreos.com/blog/operators)。
- en: 'Firstly, we create the control network to allow operator agents connecting
    to the manager nodes. Just name it `control`. We create this network to be a size
    of *class C*. So please be careful that the number of operator containers does
    not go beyond `255`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建控制网络，以便操作员代理能够连接到管理节点。将其命名为`control`。我们创建这个网络的大小为*class C*。因此，请注意操作员容器的数量不得超过`255`：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Operators in the `control` plane usually require access to Docker APIs to observe
    the cluster's state, to decide what to do, and to make changes back to the cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`control`平面中的操作员通常需要访问Docker API，以观察集群的状态，决定采取什么措施，并将更改反馈到集群中。'
- en: To make the Docker API accessible via every operator inside the same control
    network, we deploy the `docker-api` service in the control plane.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Docker API能够通过同一控制网络中的每个操作员访问，我们在控制平面中部署`docker-api`服务。
- en: We use `rancher/socat-docker` as the image of the `docker-api `service for the
    control plane because it is widely used and has proven stable for production.
    The `docker-api` will be deployed globally on every manager, using `node.role==manager`.
    The endpoint's mode will be set to `dnsrr` as each `docker-api` instance is stateless
    and the Docker managers are already taking care of the whole cluster state. So
    the `vip` endpoint mode is not necessary here.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`rancher/socat-docker`作为`docker-api`服务的镜像，因为它被广泛使用，并且已证明在生产环境中稳定。`docker-api`将部署在每个管理节点上，使用`node.role==manager`。端点模式将设置为`dnsrr`，因为每个`docker-api`实例都是无状态的，Docker
    管理器已经负责整个集群的状态。所以这里不需要`vip`端点模式。
- en: 'Each `docker-api` instance binds to `/var/run/docker.sock` on their Docker
    host to connect to their local manager:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`docker-api`实例绑定到其 Docker 主机上的`/var/run/docker.sock`，以连接到其本地管理器：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will run an operator container called **service balancer** as an example
    of using the operator pattern in production.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行一个名为**服务平衡器**的操作容器，作为在生产环境中使用操作容器模式的示例。
- en: Service balancer operator
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务平衡操作器
- en: Service rebalancing has been one of the requested features for Docker. However,
    it is better to have this feature running outside the orchestrator and to run
    it as an operator container.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 服务重平衡一直是 Docker 用户请求的功能之一。然而，最好将此功能运行在协调器之外，并作为操作容器运行。
- en: The problem is that after a new node joins the cluster, we usually rebalance
    the running services to spread loads across the cluster. The main reason this
    feature is not built into the orchestrator is because it is application-specific.
    Also, if the cluster keeps rebalancing everything when nodes dynamically come
    and go, running services may be broken all the time, and not in a good enough
    condition to serve requests.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，在新节点加入集群后，我们通常会重平衡正在运行的服务，以便将负载均匀分布到整个集群中。这个功能之所以没有内置到协调器中，是因为它是特定于应用的。而且，如果集群在节点动态加入和退出时不断进行重平衡，正在运行的服务可能会频繁中断，无法保持足够稳定的状态来处理请求。
- en: However, if we implement this kind of feature as an operator container, we can
    optionally disable it when necessary as it is running outside the orchestrator.
    Also, we can selectively pick only particular services to be rebalanced.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们将这种功能实现为操作容器，当它在协调器之外运行时，我们可以在必要时选择禁用它。此外，我们可以选择仅对特定服务进行重平衡。
- en: 'The service balancer is currently available as `chanwit/service-balancer` on
    Docker''s hub. We will be running only one instance of service balancer on any
    manager:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，服务平衡器可作为`chanwit/service-balancer`在 Docker Hub 上使用。我们将在任何管理节点上只运行一个服务平衡器实例：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Something to consider when using the auto-rebalancer is that `--update-delay`
    must be set to greater than the startup time of each task. This is really important,
    especially for Java-based services. This delay should be large enough, at least
    larger than the interval used by the health checking mechanism.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动重平衡器时需要考虑的一点是，`--update-delay`必须设置为大于每个任务的启动时间。这一点非常重要，特别是对于基于 Java 的服务。这个延迟应该足够大，至少要大于健康检查机制使用的间隔时间。
- en: Also, for the safest result, the value of `--update-parallelism` should start
    at `1`, and gradually increase when the system can stably serve the requests.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了获得最安全的结果，`--update-parallelism`的值应该从`1`开始，并在系统能够稳定提供请求时逐步增加。
- en: To allow a service to automatically rebalance, the service balancer operator
    checks the service's label `rebalance.on.node.create=true`. If this label is present
    on the service, it will be rebalanced every time a new node is added to the cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许服务自动重平衡，服务平衡操作器会检查服务的标签`rebalance.on.node.create=true`。如果该标签存在于服务中，则每次新节点加入集群时，服务都会被重平衡。
- en: Logging
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: When it comes to logging, one popular solution is to set up an Elasticsearch
    stack. The natural combination could be **Elasticsearch**-**Logstash**-**Kibana** (**ELK**).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志记录方面，一种流行的解决方案是建立一个 Elasticsearch 堆栈。自然的组合是**Elasticsearch**-**Logstash**-**Kibana**（**ELK**）。
- en: We use an ELK stack from [https://github.com/deviantony/docker-elk](https://github.com/deviantony/docker-elk)
    with modification to improve it by adding Docker Swarm configs, and to deploy
    each of them independently. The original Docker Compose file, `docker-compose.yml`,
    are split into three YML files, each for **Elasticsearch**, **Kibana**, and **Logstash**,
    respectively**. **Services must be deployed this way because we do not want to
    bring the whole logging system down when we change each service's configs. The
    fork used in this chapter is available at [https://github.com/chanwit/docker-elk](https://github.com/chanwit/docker-elk).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 [https://github.com/deviantony/docker-elk](https://github.com/deviantony/docker-elk)
    的 ELK 堆栈，并对其进行修改，通过添加 Docker Swarm 配置来改进它，并使每个组件可以独立部署。原始的 Docker Compose 文件 `docker-compose.yml`
    被拆分成三个 YML 文件，分别用于 **Elasticsearch**、**Kibana** 和 **Logstash**。**服务必须以这种方式部署，因为我们不希望在更改每个服务的配置时导致整个日志系统宕机。本章使用的分支可以在
    [https://github.com/chanwit/docker-elk](https://github.com/chanwit/docker-elk)
    获取。**
- en: 'The following figure shows what the stack will look like. All ELK components
    will be in **elk_net**. The **Logstash** instance will be exposed on port **5000**.
    On each Docker host, its local **Logspout** agent will forward log messages from
    the Docker host to the **Logstash** instance. **Logstash** will then transform
    each message and store them in **ElasticSearch**. Finally, a user can access **Kibana**
    via port **5601** to visualize all the logs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了堆栈的结构。所有 ELK 组件将位于 **elk_net** 中。**Logstash** 实例将在端口 **5000** 上暴露。在每个
    Docker 主机上，它的本地 **Logspout** 代理将把 Docker 主机的日志消息转发到 **Logstash** 实例。然后，**Logstash**
    将转换每条消息并将其存储到 **ElasticSearch** 中。最后，用户可以通过端口 **5601** 访问 **Kibana** 以可视化所有日志：
- en: '![](img/9d8fb58e-8213-4753-80eb-d8ca97450c92.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d8fb58e-8213-4753-80eb-d8ca97450c92.png)'
- en: 'Figure 7.14: An ELK stack block diagram for cluster-wide logging'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14：用于集群日志记录的 ELK 堆栈框图
- en: 'We start with the preparation of a dedicated network for our ELK stack. We
    name this network `elk_net` and use it for all ELK components:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先准备一个专用的 ELK 堆栈网络。我们将此网络命名为 `elk_net` 并将其用于所有 ELK 组件：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the source of `elasticsearch.yml`. We use Docker compose YAML
    specification version 3.3 throughout the chapter. This is the minimum requirement,
    as we will use Docker Swarm configs to manage all configuration files for us:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `elasticsearch.yml` 的源文件。我们在本章中使用 Docker Compose YML 规范版本 3.3。这是最低要求，因为我们将使用
    Docker Swarm 配置来管理所有配置文件。
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It is the requirement that `docker stack` needs the image name to be specified
    before it can be deployed. So, we need to build the container image using `docker-compose`
    first.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker stack` 需要在部署之前指定镜像名称，这是一个要求。因此，我们首先需要使用 `docker-compose` 构建容器镜像。'
- en: We use `docker-compose` only for building images.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用 `docker-compose` 来构建镜像。
- en: 'Let''s do it! We use `docker-compose` build to prepare images defined in the
    YML file. The `docker-compose` command also tags images for us too. As we have
    a separate YML file each service, we use `-f` to tell `docker-compose` to build
    the correct file:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！我们使用 `docker-compose build` 准备 YML 文件中定义的镜像。`docker-compose` 命令还会为我们标记镜像。由于我们为每个服务都有一个单独的
    YML 文件，因此我们使用 `-f` 来告诉 `docker-compose` 构建正确的文件：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When the image is ready, we can simply deploy the stack, `es`, using the following
    command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当镜像准备好后，我们可以通过以下命令简单地部署堆栈 `es`：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we move to the preparation and deployment of Kibana.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始准备和部署 Kibana。
- en: 'Here''s the stack YML file for Kibana. We have `kibana_config` pointing to
    our Kibana configuration. The Kibana port `5601` is published using Swarm''s host
    mode to bypass the ingress layer. Please remember that we do not really have the
    default ingress layer in our cluster. As previously mentioned, we use Træfik as
    our new ingress:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Kibana 的堆栈 YML 文件。我们有 `kibana_config` 指向我们的 Kibana 配置。Kibana 端口 `5601` 使用
    Swarm 的主机模式发布，以绕过 ingress 层。请记住，我们的集群中实际上没有默认的 ingress 层。如前所述，我们使用 Træfik 作为新的
    ingress：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Similar to Elasticsearch, now the Kibana image can be prepared using the `docker-compose
    build` command:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Elasticsearch 类似，现在可以使用 `docker-compose build` 命令准备 Kibana 镜像：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After that, we deploy Kibana with the stack name `kb`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们使用堆栈名称 `kb` 部署 Kibana：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With Logstash, there are two configuration files to consider. The most important
    one is the pipeline config, `logstash_pipeline_config`. We need to add custom
    rules to this file for log message transformation. It keeps changing, unlike the
    first two components of ELK. Logstash listens to port `5000`, both for TCP and
    UDP, inside `elk_net`. We will later plug Logspout into this network to convey
    log messages from Docker daemons to this Logstash service:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Logstash 时，有两个配置文件需要考虑。最重要的一个是管道配置文件`logstash_pipeline_config`。我们需要在这个文件中添加自定义规则，用于日志消息转换。与
    ELK 的前两个组件不同，这个文件会持续变化。Logstash 监听 `5000` 端口，包括 TCP 和 UDP，在 `elk_net` 网络内。稍后我们将把
    Logspout 插入到该网络中，将 Docker 守护进程的日志消息传输到 Logstash 服务：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next steps are to build and deploy, similar to the first two components:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是构建和部署，类似于前两个组件：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We started these three components as separate stacks linked together via `elk_net`.
    To check if all components are running, simply check this using `docker stack
    ls`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这三个组件作为独立堆栈，通过 `elk_net` 连接在一起。要检查所有组件是否正在运行，只需使用 `docker stack ls` 命令进行检查：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can redirect all logs from each Docker daemon to the ELK stack,
    the central service, using Logspout. This can be done by attaching each local
    `logspout` container to the `elk_net` so that they will all be able to connect
    to a Logstash instance inside the network. We start each Logspout using the following
    command:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 Logspout 将来自每个 Docker 守护进程的所有日志重定向到 ELK 堆栈中的中央服务。这可以通过将每个本地 `logspout`
    容器连接到 `elk_net` 来实现，使它们都能连接到网络内的 Logstash 实例。我们通过以下命令启动每个 Logspout：
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We are now able to log all messages via Logspout to Logstash, storing them in
    Elasticsearch, and visualizing them with Kibana.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过 Logspout 将所有日志消息发送到 Logstash，存储在 Elasticsearch 中，并通过 Kibana 进行可视化。
- en: Scripting Docker with Golang
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Golang 脚本化 Docker
- en: When it comes to operating and administrating Docker, we could do everything
    by controlling the cluster via the `docker` CLI using the `jq` command. Another
    powerful and very flexible way is to control the cluster via scripting. The most
    suitable programming language for scripting Docker cluster is, of course, Golang.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作和管理 Docker 时，我们可以通过 `docker` CLI 使用 `jq` 命令来控制集群。另一种强大且灵活的方式是通过脚本控制集群。当然，最适合脚本化
    Docker 集群的编程语言是 Golang。
- en: Why not Python? How could Golang, a statically compiled language, come to fit
    scripting?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不用 Python？静态编译语言 Golang 怎么适合脚本编程？
- en: First, Go is the language that Docker is written in. The Docker library written
    in the Go language is the same piece of codes used by Docker itself. So, the scripts
    written using this library will be naturally in high quality and greatly reliable.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，Go 是 Docker 所用的编程语言。用 Go 语言编写的 Docker 库就是 Docker 本身所使用的代码。因此，使用该库编写的脚本自然具备高质量和极高的可靠性。
- en: Second, the language constructs and the idioms fit the way Docker works. For
    example, the Go programming language has the channel construct and it fits nicely
    for processing event messages emitted by the Docker cluster.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，语言的构造和习语非常契合 Docker 的工作方式。例如，Go 编程语言有通道（channel）构造，它非常适合处理 Docker 集群发出的事件消息。
- en: Third, the Go compiler is incredibly fast. Also, once all related libraries
    get compiled, the compilation time is greatly reduced. We can normally use it
    to run scripts just like other scripting language interpreters.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，Go 编译器极其快速。此外，一旦所有相关库编译完成，编译时间大大缩短。我们通常可以像使用其他脚本语言解释器一样使用它来运行脚本。
- en: In this section, we will discuss how to use scripts written in Golang to control
    Docker directly via its API. This will become a powerful tool for taking care
    of running the cluster.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用 Golang 编写的脚本通过 Docker 的 API 直接控制 Docker。这将成为管理集群运行的强大工具。
- en: Preparing the tool
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工具
- en: Installing the Go compiler and making it ready to use is sometimes tricky. However, **Golang
    Version Manager** (**GVM**), is a tool that helps with installing and uninstalling
    different Go versions on the same machine. It also helps manage `GOPATH` effectively.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Go 编译器并使其准备好使用有时可能会有些棘手。然而，**Golang 版本管理器**（**GVM**）是一个帮助安装和卸载同一台机器上不同 Go
    版本的工具。它还帮助有效管理 `GOPATH`。
- en: 'What is GOPATH? It is defined as follows in Wikipedia:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 GOPATH？它在 Wikipedia 中是这样定义的：
- en: '"The GOPATH environment variable is used to specify directories outside of
    $GOROOT that contain the source for Go projects and their binaries."'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: “GOPATH 环境变量用于指定 $GOROOT 之外的目录，这些目录包含 Go 项目的源代码及其二进制文件。”
- en: 'To start using GVM, we first install the `gvm` command using the snippet provided
    on [https://github.com/moovweb/gvm](https://github.com/moovweb/gvm). It can be
    installed with a single command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 GVM，我们首先使用在[https://github.com/moovweb/gvm](https://github.com/moovweb/gvm)上提供的代码片段安装`gvm`命令。只需一条命令就能安装：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we have GVM installed already, and we continue by installing Go.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 GVM，接下来安装 Go。
- en: 'It is great to use Go''s most recent version-1.9.3\. The command to install
    is, of course, `gvm install`. We pass the `-B` parameter to the `install` command,
    so that it will download and use only the binary of the Go distribution:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Go 的最新版本1.9.3是很棒的。安装命令当然是`gvm install`。我们向`install`命令传递`-B`参数，这样它将仅下载并使用
    Go 分发版的二进制文件：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, if we choose to go with Go v1.9.3 when taking care of our cluster, we
    should make it the default version. Issue the `gvm use` command with the `--default`
    parameter to do so:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们在管理集群时选择使用 Go v1.9.3，我们应该将其设为默认版本。使用`gvm use`命令并加上`--default`参数来实现：
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Making Go scriptable
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使 Go 可脚本化
- en: 'Next, prepare the next tool, `gorun`, to make a Go program scriptable. With
    `gorun`, you can add a shebang to the first line of the script, as shown in the
    following command:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，准备下一个工具`gorun`，以便将 Go 程序脚本化。使用`gorun`，你可以在脚本的第一行添加 shebang，如以下命令所示：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The normal Go program will then be allowed to execute directly from the shell.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正常的 Go 程序将允许直接从 shell 执行。
- en: 'To install `gorun`, just do `go get`. The `gorun` binary will now be available
    under the path provided by the current `go1.9.3` managed by GVM. Please note that
    if you switch Go version with GVM, you need to do `go get` again:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`gorun`，只需执行`go get`。`gorun`二进制文件现在将在当前由 GVM 管理的`go1.9.3`提供的路径下。请注意，如果你通过
    GVM 切换 Go 版本，需要重新执行`go get`：
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We could install all necessary libraries for controlling Docker programmatically
    by installing the Docker client library itself:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过安装 Docker 客户端库本身，来安装所有必要的库以编程方式控制 Docker：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If nothing goes wrong, we will be ready to start writing a Golang script.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们将准备开始编写一个 Golang 脚本。
- en: Simple Docker script
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的 Docker 脚本
- en: 'Let''s write a simple script that interacts with Docker:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个与 Docker 交互的简单脚本：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: First, the script must have the first line with shebang and `gorun`. Second,
    import a line with Docker's client library, `github.com/docker/docker/client`.
    Although, Docker has been moved to `github.com/moby/moby`, but we still need to
    import all related library using the `docker/docker` repository name. Just `go
    get github.com/docker/docker/client` and everything is still working fine for
    us.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，脚本的第一行必须包含 shebang 和`gorun`。其次，导入 Docker 客户端库的代码行，`github.com/docker/docker/client`。尽管
    Docker 已经迁移到`github.com/moby/moby`，但我们仍然需要通过`docker/docker`仓库名导入所有相关库。只需执行`go
    get github.com/docker/docker/client`，一切仍然可以正常工作。
- en: Then we start programming our cluster by creating a client while also setting
    the API version to 1.30\. This script then calls `cli.Info(ctx)` to obtain the
    engine's information from the Docker daemon, as the `info` variable. It simply
    prints out the version of the Docker daemon we're talking to. The version information
    is stored in `info.ServerVersion`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们开始编程集群，通过创建客户端并将 API 版本设置为 1.30。这个脚本随后调用`cli.Info(ctx)`来从 Docker 守护进程获取引擎信息，存储在`info`变量中。它简单地打印出我们正在交互的
    Docker 守护进程的版本信息。版本信息保存在`info.ServerVersion`中。
- en: 'Save the script to a file named `server-version`. We can now run it as a normal
    shell script:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 将脚本保存为名为`server-version`的文件。我们现在可以将其作为普通的 shell 脚本运行：
- en: '[PRE32]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Script reacting to Docker events
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 响应 Docker 事件的脚本
- en: 'Next, we will write a script to monitor changes in the Docker cluster and then
    do a print out when a node is updated:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写一个脚本，用于监控 Docker 集群中的变化，并在节点更新时打印输出：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This is also a script executed by `gorun`. The script starts by creating a Docker
    client CLI pointing to the local socket, `/var/run/docker.sock`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个由`gorun`执行的脚本。脚本开始时创建一个指向本地套接字`/var/run/docker.sock`的 Docker 客户端 CLI。
- en: Then it creates a filter, the `filter` variable. This filter makes the event
    emitter select only the type of events we are interested in, in this case, when
    the `type` of events is `node`. This is equivalent to passing `--filter type=node`
    to the command line. The `cli.Events` method will return a Go channel for retrieving
    messages. A message is then retrieved inside the `for` loop. The program will
    be automatically blocked if the message is not available in the channel. So the
    script just becomes a single-thread style and easy to program.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它创建了一个过滤器，`filter` 变量。这个过滤器使事件发射器只选择我们感兴趣的事件类型，在这种情况下，就是事件的 `type` 为 `node`。这相当于在命令行中传递
    `--filter type=node`。`cli.Events` 方法将返回一个 Go 通道，用于接收消息。消息将在 `for` 循环内部被获取。如果通道中没有消息，程序将自动阻塞。所以脚本就变成了单线程风格，且易于编程。
- en: Inside the loop, we can manipulate information inside the message, for example,
    checking the action of a certain event. Normally, most types of event contain
    three possible actions, `create`, `update`, and `remove`. For a node, `create`
    means there is a new node added to the cluster. The `update` action means something
    has changed on a certain node. The `remove` action means the node is removed from
    the cluster.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环内部，我们可以处理消息中的信息，例如检查某个事件的动作。通常，大多数类型的事件包含三种可能的动作：`create`、`update` 和 `remove`。对于一个节点，`create`
    表示有一个新节点被添加到集群中。`update` 动作表示某个节点发生了变化。`remove` 动作表示节点从集群中移除。
- en: Just save this script to `./node-event`, then `chmod +x` it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 只需将此脚本保存到 `./node-event`，然后执行 `chmod +x`。
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `chmod` command will change executable bits of the script. With these bits,
    the Linux system will be able to detect that the file should be executed. Then,
    it will tell `gorun` to take care of that execution.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`chmod` 命令将更改脚本的可执行位。通过这些位，Linux 系统将能够检测到该文件应该被执行。然后，它将告诉 `gorun` 来处理这个执行。'
- en: Try changing some properties of the current working node. We may observe that
    the text `- Node updated.` will be printed out.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试更改当前工作节点的一些属性。我们可能会看到文本 `- Node updated.` 被打印出来。
- en: Exercises
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Please try to answer the following questions without going back to read the
    chapter''s content:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请尝试回答以下问题，而不回头阅读本章内容：
- en: List at least three components described in the stable cluster configuration.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出至少三个在稳定集群配置中描述的组件。
- en: Why are retry and circuit breaker important?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么重试和断路器如此重要？
- en: How do we replace the default ingress layer with the new one?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何用新的入口层替换默认的入口层？
- en: How can we install the network plugin?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何安装网络插件？
- en: What is the most frontal part of the ELK stack?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ELK 堆栈最前端的部分是什么？
- en: Why is the Go language suitable for scripting the Docker system?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 Go 语言适合用于脚本化 Docker 系统？
- en: How do we listen to Docker events of a certain type?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何监听特定类型的 Docker 事件？
- en: How do we set up a control plane?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何设置控制平面？
- en: What is the operator pattern? Why is it important?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是操作符模式？为什么它很重要？
- en: What is the characteristic of Spot instances that makes them cheaper than normal
    instances?
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spot 实例的特点是什么，使它们比普通实例便宜？
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed various topics on how to prepare and operate a Docker
    cluster with a stable configuration. We introduced a low-cost alternative to Lambda
    by deploying a Docker cluster on Spot instances. This chapter also introduced
    the concept of CoreOS's operator pattern, and how to use it practically to auto-balance
    the tasks of our cluster.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了如何准备和操作 Docker 集群的各种主题，介绍了通过在 Spot 实例上部署 Docker 集群来提供 Lambda 的低成本替代方案。本章还介绍了
    CoreOS 操作符模式的概念，并讨论了如何实用地使用它来自动平衡我们集群的任务。
- en: When it comes to logging, the ELK stack is usually the first choice. This chapter
    also discussed how to efficiently prepare ELK on Docker Swarm and it ended with
    how to operate a cluster with Golang scripts, the scripting technique that can
    fully leverage Docker and its ecosystem.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志记录方面，ELK 堆栈通常是首选。本章还讨论了如何在 Docker Swarm 上高效地准备 ELK，并以如何通过 Golang 脚本操作集群为结尾，这种脚本技巧可以充分利用
    Docker 及其生态系统。
- en: In the next chapter, we will put all FaaS platforms into the same cluster and
    make them work together to demonstrate a use case of event-driven FaaS systems
    over a Docker cluster.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把所有 FaaS 平台放入同一个集群，并使它们协同工作，以展示一个基于 Docker 集群的事件驱动 FaaS 系统的使用案例。
