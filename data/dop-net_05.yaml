- en: Chapter 5. Orchestrating Load Balancers Using Ansible
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：使用Ansible编排负载均衡器
- en: This chapter will focus on some of the popular load balancing solutions that
    are available today and the approaches that they take to load balancing applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍一些当今流行的负载均衡解决方案，以及它们在负载均衡应用程序方面的策略。
- en: With the emergence of cloud solutions, such as AWS, Microsoft Azure, Google
    Cloud, and OpenStack, we will look at the impact this has had on load balancing
    with distributed load and centralized load balancing strategies. This chapter
    will show practical configuration management processes that can be used to orchestrate
    load balancers using Ansible to help automate the load balancing needs for applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云解决方案的出现，如AWS、Microsoft Azure、Google Cloud和OpenStack，我们将探讨这些技术对负载均衡的影响，特别是分布式负载和集中式负载均衡策略。本章将展示可用于通过Ansible编排负载均衡器的实际配置管理过程，以帮助自动化应用程序的负载均衡需求。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Centralized and distributed load balancers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中式和分布式负载均衡器
- en: Popular load balancing solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的负载均衡解决方案
- en: Load balancing immutable and static servers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡不可变和静态服务器
- en: Using Ansible to orchestrate load balancers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ansible编排负载均衡器
- en: Centralized and distributed load balancers
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集中式和分布式负载均衡器
- en: With the introduction of microservice architectures allowing development teams
    to make changes to production applications more frequently, developers no longer
    just need to release software on a quarterly basis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务架构的引入，允许开发团队更频繁地对生产环境中的应用程序进行更改，开发者不再仅仅需要按季度发布软件。
- en: With the move towards Continuous Delivery and DevOps, applications are now released
    weekly, daily, or even hourly with only one or a subset of those microservices
    being updated and released.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着持续交付（Continuous Delivery）和DevOps的推进，应用程序现在每周、每天，甚至每小时都会发布，且通常只更新和发布其中一个或部分微服务。
- en: Organizations have found microservice architectures to be easier to manage and
    have moved away from building monolith applications. Microservice applications
    break a larger application into smaller manageable chunks. This allows application
    features to be released to customers on a more frequent basis, as the business
    does not have to redeploy the whole product each time they release. This means
    only a small microservice needs to be redeployed to deploy a feature. As the release
    process is more frequent and continuous, then it is better understood, normally
    completely automated, and ultimately load balanced.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 组织发现微服务架构更易于管理，因此逐渐摒弃了构建单体应用程序的方式。微服务应用程序将较大的应用程序拆分成更小的、可管理的模块。这使得应用程序功能可以更频繁地发布给客户，因为业务在每次发布时不需要重新部署整个产品。这意味着只需重新部署一个小的微服务即可发布一个新功能。由于发布过程更为频繁且持续进行，因此它更容易被理解，通常是完全自动化的，并最终得到负载均衡。
- en: Microservice architectures can also be beneficial for large businesses, which
    are distributed across many offices or countries as different teams can own different
    microservices and release them independently of one another.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构对于跨多个办公室或国家的庞大企业也有益处，因为不同的团队可以拥有不同的微服务，并独立发布它们。
- en: This, of course, means that development teams need a way of testing dependency
    management, and the onus is put on adequate testing to make sure that a microservice
    doesn't break other microservices when it is released.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然意味着开发团队需要一种方式来测试依赖管理，并且要确保充分测试，以保证微服务在发布时不会破坏其他微服务。
- en: As a result, developers need to create mocking and stubbing services, so microservice
    applications can be effectively tested against multiple software versions without
    having to deploy the full production estate.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，开发者需要创建模拟和存根服务，以便在不部署完整生产环境的情况下有效测试微服务应用程序对多个软件版本的兼容性。
- en: Creating a microservice architecture is a huge mindset shift for a business
    but a necessary one to remain competitive. Releasing monolithic applications is
    often difficult and time-consuming for an organization, and businesses that have
    quarterly release cycles will eventually lose out to competitors that can release
    their features in a quicker, more granular way.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建微服务架构是企业的一大思维转变，但这是保持竞争力的必要措施。发布单体应用程序通常对企业来说既困难又耗时，而采用季度发布周期的企业最终会被能够以更快、更精细的方式发布功能的竞争对手超越。
- en: The use of microservice architectures has meant that being able to utilize the
    same load balancing in test environments as production has become even more important
    due to how dynamic environments need to be.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构的使用意味着，由于环境的动态性，能够在测试环境中使用与生产环境相同的负载均衡变得更加重要。
- en: So having test environments load balancing configuration as close to production
    environments as possible is a must. Configuration management tooling can be used
    to control the desired state of the load balancer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，测试环境的负载均衡配置必须尽可能接近生产环境。可以使用配置管理工具来控制负载均衡器的期望状态。
- en: The delegation of responsibilities also needs to be reviewed to support microservice
    architectures, so control of some of the load balancing provisioning should move
    to development teams as opposed to being a request to the network team to make
    it manageable and not to impede development teams. This, of course, is a change
    in culture that needs sponsorship from senior management to make the required
    changes to the operational model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 责任的委托也需要审查，以支持微服务架构，因此，部分负载均衡配置的控制应移交给开发团队，而不是向网络团队提出请求，这样既能使其易于管理，又不妨碍开发团队。这当然是一种文化变革，需要高级管理层的支持，以推动操作模型的必要变革。
- en: Load balancing requirements when using microservice applications will evolve
    as an application is developed or scaled up and down in size, so it is important
    that these aspects are made available to developers to self-service requests rather
    than wait on a centralized network team to make load balancing changes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微服务应用时，负载均衡的要求将随着应用程序的开发或规模的扩大而演变，因此，重要的是这些方面要提供给开发人员进行自助请求，而不是等待集中式网络团队来进行负载均衡的更改。
- en: As a result of the shift towards microservices architectures, the networking
    and load balancing landscape has needed to evolve too to support those needs with
    PaaS solutions being created by many vendors to handle application deployment
    across hybrid cloud and load balancing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于向微服务架构的转变，网络和负载均衡的领域也需要不断发展，以支持这些需求，许多供应商创建了PaaS解决方案来处理跨混合云的应用程序部署和负载均衡。
- en: Off-the-shelf PaaS solutions are a great option for companies that maybe aren't
    tech-savvy and are unable to create their own deployment pipelines using configuration
    management tooling, such as Chef, Puppet, Ansible, and Salt, to deploy their applications
    into cloud environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现成的PaaS解决方案是对于那些可能不太懂技术且无法使用配置管理工具（如Chef、Puppet、Ansible、Salt等）创建自己部署管道的公司来说的一个不错选择，帮助他们将应用程序部署到云环境中。
- en: Regardless of the approach to deployment, roll your own or off-the-shelf PaaS.
    Both microservice and monolith applications still need to be supported when considering
    public, private, and hybrid clouds.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是采用哪种部署方式，自建还是现成的PaaS解决方案。在考虑公有云、私有云和混合云时，微服务和单体应用仍然需要得到支持。
- en: As a result, networking and load balancing need to be adaptable to support varied
    workloads. Although the end goal for an organization is ultimately a microservice
    architecture, the reality for most companies is having to adopt a hybrid approach
    catering to centralized and distributed load balancing methods to support both
    monolithic and cloud native microservices.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络和负载均衡需要具备适应性，以支持不同的工作负载。虽然组织的最终目标是实现微服务架构，但大多数公司现实中必须采用混合方式，兼顾集中式和分布式负载均衡方法，以支持单体应用和云原生微服务。
- en: Centralized load balancing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式负载均衡
- en: Traditionally, load balancers were installed as external physical appliances
    with very complex designs and used very expensive equipment. Load balancers would
    be configured to serve web content with SSL requests terminated on the expensive
    physical appliances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，负载均衡器作为外部物理设备安装，设计非常复杂，并且使用非常昂贵的设备。负载均衡器会配置为为Web内容提供服务，并在昂贵的物理设备上终止SSL请求。
- en: The load balancer would have complex configuration to route requests to applications
    using context switching, and requests would be served directly to the static backend
    servers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器将有复杂的配置来通过上下文切换将请求路由到应用程序，且请求会直接传递给静态后端服务器。
- en: 'This was optimal for monolith configurations as applications typically were
    self-contained and followed a three-tier model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这对单体配置来说是最优的，因为应用程序通常是自包含的，并且遵循三层模型：
- en: A frontend webserver
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端Web服务器
- en: A business logic layer
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务逻辑层
- en: A database layer
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库层
- en: This didn't require a lot of east to west traffic within the network as the
    traffic was north to south, traversing the frontend, business logic, and database.
    Networks were designed to minimize the amount of time taken to process the request
    and serve it back to the end user, and it was always served by the core network
    each time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不需要大量的东西向流量，因为流量是南北向的，穿越了前端、业务逻辑和数据库。网络设计的目的是最小化处理请求并将其返回给最终用户所需的时间，并且每次都由核心网络提供服务。
- en: Distributed load balancing
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式负载均衡
- en: With the evolution towards microservice architectures, the way that applications
    operate has changed somewhat. Applications are less self-contained and need to
    talk to dependent microservices applications that exist within the same tenant
    network, or even across multiple tenants.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务架构的发展，应用程序的运行方式发生了一定变化。应用程序不再是自包含的，而是需要与同一租户网络内或跨多个租户的依赖微服务应用程序进行通信。
- en: This means that east-west traffic within the data center is much higher, and
    that traffic in the data center doesn't always go through the core network like
    it once did.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着数据中心内的东西向流量大大增加，并且数据中心中的流量不再像以前那样始终通过核心网络。
- en: Clusters of microservices applications are instead instantiated and then load
    balanced within the tenant network using x86 software load balancing solutions
    with the endpoint of the microservices clusters **Virtual IP** (**VIP**) exposed
    to adjacent microservices that need to utilize it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务应用程序的集群被实例化并在租户网络内进行负载均衡，使用x86软件负载均衡解决方案，微服务集群的**虚拟IP**（**VIP**）暴露给需要使用它的相邻微服务。
- en: With the growing popularity of virtual machines, containers, and software-defined
    overlay networks, this means that software load balancing solutions are now used
    to load balance applications within the tenant network, as opposed to having to
    pin back to a centralized load balancing solution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随着虚拟机、容器和软件定义覆盖网络的日益普及，这意味着软件负载均衡解决方案现在被用来在租户网络内部对应用程序进行负载均衡，而不再需要回到集中式的负载均衡解决方案。
- en: As a result load balancing vendors have had to adapt and produce virtualized
    or containerized versions of their physical appliances to stay competitive with
    open source software load balancing solutions, which are routinely used with microservices.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，负载均衡供应商不得不适应并生产虚拟化或容器化版本的物理设备，以与开源软件负载均衡解决方案竞争，而这些解决方案通常与微服务一起使用。
- en: Popular load balancing solutions
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的负载均衡解决方案
- en: As applications have moved from monoliths to microservices, load balancing requirements
    have undoubtedly changed. Today, we have seen a move towards open source load
    balancing solutions, which are tightly integrated with virtual machines and containers
    to serve east to west traffic between VPC in AWS or a tenant network in OpenStack
    as opposed to pinning out to centralized physical appliances.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序从单体架构转向微服务架构，负载均衡的需求无疑发生了变化。今天，我们看到有向开源负载均衡解决方案的转变，这些解决方案与虚拟机和容器紧密集成，用于在AWS中的VPC之间或OpenStack中的租户网络之间提供东西向流量，而不是依赖于集中式物理设备。
- en: 'Open source load balancing solutions are now available from **Nginx** and **HAProxy**
    to help developers load balance their applications or AWS elastic load balancing
    feature:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开源负载均衡解决方案已经由**Nginx**和**HAProxy**提供，帮助开发人员对他们的应用程序进行负载均衡，或使用AWS的弹性负载均衡功能：
- en: '[https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/)'
- en: Just a few years ago, Citrix NetScalers ([https://www.citrix.com/products/netscaler-adc/](https://www.citrix.com/products/netscaler-adc/))
    and F5 Big-IP ([https://f5.com/products/big-ip](https://f5.com/products/big-ip))
    solutions had the monopoly in the enterprise load balancing space, but the load
    balancing landscape has changed significantly with a multitude of new solutions
    available.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，Citrix NetScalers（[https://www.citrix.com/products/netscaler-adc/](https://www.citrix.com/products/netscaler-adc/)）和F5
    Big-IP（[https://f5.com/products/big-ip](https://f5.com/products/big-ip)）解决方案在企业负载均衡领域拥有垄断地位，但随着众多新解决方案的出现，负载均衡领域发生了显著变化。
- en: New load balancing start-ups such as Avi networks ([https://avinetworks.com/](https://avinetworks.com/))
    focus on x86 compute and software solutions to deliver load balancing solutions,
    which have been created to assist with both modern micros-service applications
    and monolith applications to support both distributed and centralized load balancing
    strategies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的负载均衡初创公司，如 Avi Networks （[https://avinetworks.com/](https://avinetworks.com/)），专注于
    x86 计算和软件解决方案，以提供负载均衡解决方案，这些解决方案旨在支持现代微服务应用程序和单体应用程序，支持分布式和集中式负载均衡策略。
- en: The aim of this book is not about which load balancing vendor solution is the
    best; there is no *one size fits all* solution, and the load balancing solution
    chosen will depend on traffic patterns, performance, and portability that is required
    by an organization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目的是讨论哪种负载均衡供应商解决方案最好；没有*一刀切*的解决方案，选择的负载均衡解决方案将取决于组织所需的流量模式、性能和可移植性。
- en: This book will not delve into performance metrics; its goal is to look at the
    different load balancing strategies that are available today from each vendor
    and the configuration management methods that could be utilized to fully automate
    and orchestrate load balancers which will in turn help network teams automate
    load balancing network operations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不会深入探讨性能指标；它的目标是探讨当前各供应商提供的不同负载均衡策略以及可以用来完全自动化和编排负载均衡器的配置管理方法，这将帮助网络团队实现负载均衡网络操作的自动化。
- en: Citrix NetScaler
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Citrix NetScaler
- en: '**Citrix NetScaler** provides a portfolio of products to service an organization''s
    load balancing requirements. Citrix provide various different products to end
    users, such as the **MPX**, **SDX**, **VPX**, and more recently the **CPX** appliances,
    with flexible license costs available for each product based on the throughput
    they support.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Citrix NetScaler** 提供一系列产品来满足组织的负载均衡需求。Citrix 提供多种不同的产品供最终用户使用，如 **MPX**、**SDX**、**VPX**，以及最近推出的
    **CPX** 设备，每种产品的许可证费用根据其支持的吞吐量提供灵活的定价。'
- en: MPX and SDX are the NetScaler hardware appliances, whereas the VPX is a virtualized
    NetScaler and the CPX is a containerized NetScaler.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MPX 和 SDX 是 NetScaler 的硬件设备，而 VPX 是虚拟化的 NetScaler，CPX 是容器化的 NetScaler。
- en: All of these products support differing amounts of throughput based on the license
    that is purchased ([https://www.citrix.com/products/netscaler-adc/platforms.html](https://www.citrix.com/products/netscaler-adc/platforms.html)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些产品根据所购买的许可证支持不同的吞吐量（[https://www.citrix.com/products/netscaler-adc/platforms.html](https://www.citrix.com/products/netscaler-adc/platforms.html)）。
- en: All of the Citrix NetScaler family of products share the same common set of
    APIs and code, so the software is completely consistent. NetScaler has a REST
    API and a Python, Java, and C# Nitro SDK, which exposes all the NetScaler operations
    that are available in the GUI to the end user. All the NetScaler products allow
    programmatic control of NetScaler objects and entities that need to be set up
    to control load balancing or routing on MPX, SDX, VPX, or CPX.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Citrix NetScaler 系列产品共享相同的 API 和代码集，因此软件完全一致。NetScaler 提供 REST API 以及 Python、Java
    和 C# Nitro SDK，暴露所有可以在 GUI 中进行操作的 NetScaler 功能。所有 NetScaler 产品允许以编程方式控制需要设置的 NetScaler
    对象和实体，以控制 MPX、SDX、VPX 或 CPX 上的负载均衡或路由。
- en: The NetScaler MPX appliance is a centralized physical load balancing appliance
    that is used to deal with a high number of **Transactions Per Second** (**TPS**);
    MPX has numerous security features and complies with **Restriction of Hazardous
    Substances** (**RoHS**) and **Federal Information Processing Standard** (**FIPS**),
    so the solution can be used by heavily regulated industries that require businesses
    to comply with certain regulatory standards.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: NetScaler MPX 设备是一个集中式的物理负载均衡设备，用于处理大量的**每秒事务数**（**TPS**）；MPX 具有众多安全特性，并且符合**有害物质限制**（**RoHS**）和**联邦信息处理标准**（**FIPS**），因此该解决方案可以被需要遵守特定监管标准的高监管行业所使用。
- en: MPX is typically used to do SSL offloading; it supports a massive amount of
    SSL throughput, which can be very useful for very highly performant applications,
    so the SSL offloading can be done on the hardware appliance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: MPX 通常用于进行 SSL 卸载；它支持大量的 SSL 吞吐量，这对于需要高性能的应用程序非常有用，因此 SSL 卸载可以在硬件设备上完成。
- en: MPX can be used to direct traffic to different tenant networks using layer 4
    load balancing and layer 7 context switching or alternately direct traffic to
    a second load balancing tier.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MPX 可以通过第 4 层负载均衡和第 7 层上下文切换将流量引导到不同的租户网络，或者将流量引导到第二层负载均衡级别。
- en: The NetScaler SDX appliance is also a centralized physical appliance that is
    used to deal with a high number of TPS. SDX allows multiple VPX appliances to
    be set up as HA pairs and deployed on SDX to allow increased throughput and resiliency.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: NetScaler SDX 设备也是一种集中式物理设备，用于处理高 TPS（每秒事务数）。SDX 允许多个 VPX 设备作为 HA 对在 SDX 上设置并部署，从而提高吞吐量和可靠性。
- en: 'NetScaler also supports **Global Server Load Balancing** (**GSLB**), which
    allows load to be distributed across multiple VPX HA pairs in a scale out model
    utilizing **CNAME,** which directs traffic across multiple HA pairs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NetScaler 还支持 **全局服务器负载均衡**（**GSLB**），它允许在多个 VPX HA 对之间分配负载，采用扩展模型，并利用 **CNAME**
    将流量引导到多个 HA 对：
- en: '![Citrix NetScaler](img/B05559_05_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![Citrix NetScaler](img/B05559_05_01.jpg)'
- en: The VPX can be installed on any x86 hypervisor and be utilized as a VM appliance,
    and a new CPX is now available that puts the NetScaler inside a Docker container,
    so they can be deployed within a tenant network as opposed to being set up in
    a centralized model. All appliances allow SSL certificates to be assigned and
    used.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: VPX 可以安装在任何 x86 虚拟化平台上并作为虚拟机设备使用，此外还推出了新的 CPX，它将 NetScaler 放入 Docker 容器中，因此可以在租户网络内部署，而不是在集中式模型中设置。所有设备都允许分配和使用
    SSL 证书。
- en: 'Every NetScaler appliance, be it MPX, SDX, VPX, or CPX, utilize IP the same
    object model and code that has the following prominent entities defined in software
    to carry out application load balancing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 NetScaler 设备，无论是 MPX、SDX、VPX 还是 CPX，都采用相同的 IP 对象模型和代码，其中定义了以下显著的实体来执行应用负载均衡：
- en: '**Server**: A server entity on NetScaler binds a virtual machine or bare metal
    server''s IP address to the server entity. This means the IP address is a candidate
    for load balancing once it is bound to other NetScaler entities.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器**：NetScaler 上的服务器实体将虚拟机或裸机服务器的 IP 地址绑定到服务器实体。这意味着一旦绑定到其他 NetScaler 实体，该
    IP 地址就成为负载均衡的候选者。'
- en: '**Monitor**: The monitor entity on NetScaler are attached to services or service
    groups and provide health checks that are used to monitor the health of attached
    server entities. If the health checks, which could be as simple as a web-ping,
    are not positive, the service or service group will be marked as down, and NetScaler
    will not direct traffic to it.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控器**：NetScaler 上的监控器实体附加到服务或服务组，并提供用于监控附加服务器实体健康状况的健康检查。如果健康检查结果不正常（例如简单的网页
    ping），则服务或服务组将被标记为不可用，NetScaler 将不会将流量引导到该服务。'
- en: '**Service group**: A service group is a NetScaler entity used to bind a group
    of one or more servers to an `lbvserver` entity; a service group can have one
    or more monitors associated with it to health check the associated servers.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务组**：服务组是 NetScaler 实体，用于将一个或多个服务器绑定到 `lbvserver` 实体；服务组可以与一个或多个监控器关联，以对关联的服务器进行健康检查。'
- en: '**Service**: The service entity is used to bind one server entity and one or
    more monitor health checks to an `lbvserver` entity, which specifies the protocol
    and port to check the server on.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**：服务实体用于将一个服务器实体和一个或多个监控健康检查绑定到 `lbvserver` 实体，该实体指定用于检查服务器的协议和端口。'
- en: '**lbvserver**: An `lbvserver` entity determines the load balancing policy such
    as round robin or least connection and is connected to a service group entity
    or multiple service entities and will expose a virtual IP address that can be
    served to end users to access web applications or a web service endpoints.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lbvserver**：`lbvserver` 实体确定负载均衡策略，如轮询或最少连接，并与服务组实体或多个服务实体连接，暴露一个虚拟 IP 地址，供终端用户访问
    Web 应用程序或 Web 服务端点。'
- en: '**gslbvserver**: When DNS load balancing between NetScaler appliances is required,
    a `gslbvserver` entity is used to specify the `gslb` domain name and TTL.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gslbvserver**：当需要在 NetScaler 设备之间进行 DNS 负载均衡时，使用 `gslbvserver` 实体来指定 `gslb`
    域名和 TTL。'
- en: '**csvserver**: The `csvserver` entity is used to provide layer 7 context switching
    from a gslbvserver domain or lbvserver IP address to other lbvservers. This is
    used to route traffic using the NetScaler appliance.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**csvserver**：`csvserver` 实体用于提供从 gslbvserver 域或 lbvserver IP 地址到其他 lbvservers
    的第七层上下文切换。这用于通过 NetScaler 设备路由流量。'
- en: '**gslbservice**: The `gslbvservice` entity binds the `gslbvserver` domain to
    one or more `gslbservers` entities to distribute traffic across NetScaler appliances.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gslbservice**: `gslbvservice` 实体将 `gslbvserver` 域绑定到一个或多个 `gslbservers` 实体，从而在
    NetScaler 设备之间分配流量。'
- en: '**gslbserver**: The `gslbserver` entities are is the gslb-enabled IP addresses
    of the NetScaler appliances.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gslbserver**: `gslbserver` 实体是启用了 gslb 的 NetScaler 设备的 IP 地址。'
- en: Simple load balancing can be done utilizing the server, monitor, service group/service,
    and lbvserver combination. With `gslbvserver` and `csvserver`, context switching
    allows more complex requirements for complex routing and resiliency.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用服务器、监视器、服务组/服务和 lbvserver 的组合，可以实现简单的负载均衡。通过 `gslbvserver` 和 `csvserver`，上下文切换允许复杂路由和弹性要求。
- en: F5 Big-IP
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F5 Big-IP
- en: The **F5 Big-IP** suite is based on F5's very own custom TMOS real-time operating
    system, which is self-contained and runs on Linux. TMOS has a collection of operating
    systems and firmware, which all run on BIG-IP hardware appliances or within the
    BIG-IP virtual instances. BIG-IP and TMOS (and even TMM) can be used interchangeably
    depending on the use case.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**F5 Big-IP** 套件基于 F5 自有的定制 TMOS 实时操作系统，该操作系统是自包含的并运行在 Linux 上。TMOS 包含一系列操作系统和固件，所有这些都运行在
    BIG-IP 硬件设备或 BIG-IP 虚拟实例中。BIG-IP 和 TMOS（甚至 TMM）可以根据使用案例互换使用。'
- en: TMOS is at the heart of every F5 appliance and allows inspection of traffic.
    It makes forwarding decisions based on the type of traffic acting much in the
    same way as a firewall would, only allowing predefined protocols to flow through
    the F5 system.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TMOS 是每个 F5 设备的核心，允许流量检查。它根据流量类型做出转发决策，功能类似防火墙，只允许预定义的协议通过 F5 系统。
- en: TMOS also features iRules, which are programmatic scripts written using F5's
    very own **Tool Command Language** (**TCL**) that enables users to create unique
    functions triggered by specific events. This could be used to content switch traffic
    or red-order HTTP cookies; TCL is fully extensible and programmable and can carry
    out numerous operations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TMOS 还具有 iRules，它是使用 F5 自有的 **工具命令语言**（**TCL**）编写的程序化脚本，使用户能够创建由特定事件触发的独特功能。这可以用于内容交换流量或重新排序
    HTTP cookies；TCL 完全可扩展和可编程，能够执行众多操作。
- en: The F5 Big-IP solution is primarily a hardware load balancing solution, that
    provides multiple sets of physical hardware boxes that customers can purchase
    based on their throughput requirements, and the hardware can be clustered together
    for redundancy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: F5 Big-IP 解决方案主要是硬件负载均衡解决方案，提供多种物理硬件设备，客户可以根据其吞吐量要求购买，硬件设备可以集群在一起以实现冗余。
- en: The F5 Big-IP suite provides a multitude of products that provide services catering
    for load balancing, traffic management, and even firewalling.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: F5 Big-IP 套件提供了多种产品，提供负载均衡、流量管理，甚至防火墙服务。
- en: 'The main load balancing services provided by the F5 Big-IP Suite are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: F5 Big-IP 套件提供的主要负载均衡服务如下：
- en: '**Big-IP DNS:** F5''s global load balancing solution'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Big-IP DNS**: F5 的全球负载均衡解决方案。'
- en: '**Local traffic manager:** The main load balancing product of the F5 Big-IP
    suite'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Local traffic manager**: F5 Big-IP 套件的主要负载均衡产品。'
- en: The F5 Big-IP solution, like the Citrix NetScaler, implements an object model
    to allow load balancing to be programmatically defined and virtualized. F5 allows
    SSL certificates to be associated with entities.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: F5 Big-IP 解决方案与 Citrix NetScaler 类似，采用对象模型允许负载均衡被程序化定义和虚拟化。F5 允许将 SSL 证书与实体关联。
- en: 'The following local traffic manager object entities allow F5 Big-IP to load
    balance applications:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下本地流量管理器对象实体允许 F5 Big-IP 进行应用负载均衡：
- en: '**Pool members**: The pool member entity is mapped to a virtual or physical
    server''s IP address and can be bound to one or more pools. A pool member can
    have health monitors associated.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pool members**: 池成员实体映射到虚拟或物理服务器的 IP 地址，并且可以绑定到一个或多个池。池成员可以关联健康监视器。'
- en: '**Monitor**: The monitor entity returns the status on specific pool members
    and acts as a health check.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Monitor**: 监视器实体返回特定池成员的状态，并充当健康检查功能。'
- en: '**Pool**: The pool entity is a logical grouping of a cluster of pool members
    that are associated; a pool can have health monitors associated with it as well
    as **Quality of Service** (**QoS**).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pool**: 池实体是一个逻辑分组，包含已关联的池成员集；池也可以与健康监视器以及 **服务质量**（**QoS**）相关联。'
- en: '**Virtual servers**: The virtual server entity is associated with a pool or
    multiple pools, and the virtual server determines the load balancing policy, such
    as round robin or least connections. The F5 solution also will offer load balancing
    solutions based on capacity or fastest connection. Layer 7 profiles utilizing
    iRules can be configured against a virtual server and is used to expose an IP
    address to access pool members.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟服务器**：虚拟服务器实体与一个或多个池关联，虚拟服务器决定负载均衡策略，例如轮询或最少连接。F5 解决方案也提供基于容量或最快连接的负载均衡解决方案。利用
    iRules 的第七层配置文件可以针对虚拟服务器进行配置，并用于暴露 IP 地址以访问池成员。'
- en: '**iRules**: iRules utilize the programmatic TCL, so users can author particular
    load balancing rules based on events such as context switching to different pools.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**iRules**：iRules 使用编程语言 TCL，用户可以基于事件（如切换到不同池）编写特定的负载均衡规则。'
- en: '**Rate classes**: Rate classes implement rate shaping, and they are used to
    control bandwidth consumption on particular load balancing operations to cap throughput.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率类别**：速率类别实现了速率整形，用于控制特定负载均衡操作的带宽消耗，以限制吞吐量。'
- en: '**Traffic classes**: Traffic class entities are used to regulate traffic flow
    based on particular events.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流量类别**：流量类别实体用于根据特定事件调节流量流向。'
- en: Avi Networks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Avi Networks
- en: '**Avi Networks** are a relatively new start-up but have a very interesting
    load balancing product, which truly embraces the software-defined mandate. It
    is an enterprise software load balancing solution that comprises the **Avi Controller**
    that can be deployed on x86 compute. Avi is a pure software solution that deploys
    distributed Avi service engines into tenant networks and integrates with an AWS
    VPC and an OpenStack tenant:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**Avi Networks** 是一家相对较新的初创公司，但他们拥有一款非常有趣的负载均衡产品，真正体现了软件定义的理念。这是一款企业级软件负载均衡解决方案，包含可以部署在
    x86 计算平台上的 **Avi Controller**。Avi 是一种纯软件解决方案，将分布式 Avi 服务引擎部署到租户网络中，并与 AWS VPC
    及 OpenStack 租户进行集成：'
- en: '![Avi Networks](img/B05559_05_02.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![Avi Networks](img/B05559_05_02.jpg)'
- en: The Avi Networks solution offers automated provisioning of load balancing services
    on x86 hypervisors, and it can automatically scale out to meet load balancing
    needs elastically based on utilization rules that users can configure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Avi Networks 解决方案提供自动化的负载均衡服务配置功能，支持在 x86 虚拟化平台上进行自动扩展，根据用户配置的利用率规则，灵活地根据负载均衡需求进行扩展。
- en: The Avi Networks solution supports multiple or isolated tenants and has a real-time
    application monitoring and analytics engine that can work out where latency is
    occurring on the network and the location's packets are being routed from.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Avi Networks 解决方案支持多个或隔离的租户，并且拥有一个实时应用监控和分析引擎，可以定位网络中延迟的发生位置及数据包的路由来源地。
- en: Avi also supports a rich graphical interface that shows load balancing entities
    so users have a visual view of load balancing, and it additionally supports anti-DDoS
    support.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Avi 还支持丰富的图形界面，显示负载均衡实体，用户可以直观地查看负载均衡情况，同时还支持抗 DDoS 防护。
- en: 'All commands that are issued via GUI or API utilize the same REST API calls.
    The Avi Networks solution supports a Python and REST API. The Net Networks object
    model has numerous entities that are used to define load balancing in much the
    same way as NetScalers and F5:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通过 GUI 或 API 发出的命令都使用相同的 REST API 调用。Avi Networks 解决方案支持 Python 和 REST API。Net
    Networks 对象模型有许多实体，类似于 NetScalers 和 F5，能够定义负载均衡。
- en: '**Health monitor profile**: The health monitor pool profile entity specifies
    health checks for a pool of servers using health attributes.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康监测配置文件**：健康监测池配置文件实体指定池服务器的健康检查，使用健康属性进行监控。'
- en: '**Pool**: The pool entity specifies the IP addresses of virtual or physical
    servers in the form of a server list and has associated health monitor profiles;
    it also allows an event to be specified using a data script if a pool goes down.
    One or more pools are bound to the virtual service entity.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池**：池实体指定虚拟或物理服务器的 IP 地址，以服务器列表的形式呈现，并具有关联的健康监测配置文件；如果池出现故障，还可以通过数据脚本指定事件。一个或多个池与虚拟服务实体绑定。'
- en: '**Custom policy**: The custom policy allows users to programmatically specify
    policies against a virtual service.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义策略**：自定义策略允许用户以编程方式指定虚拟服务的策略。'
- en: '**App profile**: The app profile entity allows each application to be modeled
    with associated http attributes, security, DDoS, caching, compression, and PKI
    attributes specified as part of the app profile associated with a virtual service.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用配置文件**：应用配置文件实体允许将每个应用程序建模，并为其指定相关的 HTTP 属性、安全性、DDoS、缓存、压缩和 PKI 属性，这些都作为与虚拟服务关联的应用配置文件的一部分。'
- en: '**Analytics profile**: The analytics profile makes use of the Avi analytics
    engine and captures threat, metrics, health score as well as latency thresholds
    and failure codes that are mapped to the virtual service entity.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析配置文件**：分析配置文件利用 Avi 分析引擎，捕获威胁、指标、健康评分、延迟阈值以及失败代码，这些都映射到虚拟服务实体。'
- en: '**TCP/UDP profile**: The TCP/UDP profile governs if TCP or UDP is used and
    any DDoS L3/L4 profiles are set.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TCP/UDP 配置文件**：TCP/UDP 配置文件管理是否使用 TCP 或 UDP，并设置任何 DDoS L3/L4 配置文件。'
- en: '**SSL profile**: The SSL entity governs SSL ciphers that will be used by a
    virtual service entity.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SSL 配置文件**：SSL 实体管理虚拟服务实体将使用的 SSL 密码套件。'
- en: '**PKI profile**: The PKI profile entity is bound to the virtual service entity
    and specifies the certificate authority for the virtual service.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PKI 配置文件**：PKI 配置文件实体绑定到虚拟服务实体，并指定虚拟服务的证书颁发机构。'
- en: '**Policy set**: The policy set entity allows users to set security teams to
    set policies against each virtual service governing request and response polices.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略集**：策略集实体允许用户设置安全团队为每个虚拟服务设置针对请求和响应的策略。'
- en: '**Virtual service**: The virtual service entity is the entry point IP address
    to the load balanced pool of servers and is associated with all profiles to define
    the application pools load balancing and is bound to the TCP/UDP, app, SSL, SSL
    cert, policy, and analytics profiles.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟服务**：虚拟服务实体是负载均衡服务器池的入口点 IP 地址，并与所有配置文件关联，用于定义应用池的负载均衡，同时绑定到 TCP/UDP、应用、SSL、SSL
    证书、策略和分析配置文件。'
- en: Nginx
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nginx
- en: '**Nginx** ([https://www.nginx.com/](https://www.nginx.com/)) supports both
    commercial and open source versions. It is an x86 software load balancing solution.
    Nginx can be used as both an HTTP and TCP load balancer supporting HTTP, TCP,
    and even UDP, and can also support SSL/TLS termination.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nginx** ([https://www.nginx.com/](https://www.nginx.com/)) 支持商业版和开源版。它是一个基于
    x86 的软件负载均衡解决方案。Nginx 可以作为 HTTP 和 TCP 负载均衡器使用，支持 HTTP、TCP，甚至 UDP，还可以支持 SSL/TLS
    终结。'
- en: Nginx can be set up for redundancy in a highly available fashion using *keepalived*,so
    if there is an outage on one Nginx load balancer, it will seamlessly fail over
    to a backup with zero downtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx 可以通过使用 *keepalived* 设置冗余模式，以便在一个 Nginx 负载均衡器发生故障时，能够无缝切换到备份服务器，并且零停机时间。
- en: Nginx Plus is the commercial offering and is more fully featured than the open
    source version, supporting features such as active health checks, session persistence,
    and caching.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx Plus 是商业版，功能比开源版更完整，支持主动健康检查、会话持久性和缓存等功能。
- en: Load balancing on Nginx is set up by declaring syntax in the `nginx.conf` file.
    It works on the principle of wanting to simplify load balancing configuration.
    Unlike NetScalers, F5s, and Avi Networks, it does not utilize an object model
    to define load balancing rules, instead Nginx describes load balanced virtual
    or physical machines as backend servers using declarative syntax.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx 的负载均衡是通过在 `nginx.conf` 文件中声明语法来设置的。其工作原理是简化负载均衡配置。与 NetScalers、F5 和 Avi
    Networks 不同，它不使用对象模型来定义负载均衡规则，而是通过声明式语法将负载均衡的虚拟或物理机器描述为后端服务器。
- en: 'In the following simple example, we see three servers, `10.20.1.2`, `10.20.1.3`,
    and `10.20.1.4`, all load balanced on port `80` using Nginx declarative syntax,
    and it is served on `http://www.devopsfornetworking.com/devops_for_networking`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下简单示例中，我们看到三台服务器，`10.20.1.2`、`10.20.1.3` 和 `10.20.1.4`，所有的负载均衡都使用 Nginx 声明式语法在端口
    `80` 上进行，并且它通过 `http://www.devopsfornetworking.com/devops_for_networking` 提供服务：
- en: '![Nginx](img/B05559_05_03.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![Nginx](img/B05559_05_03.jpg)'
- en: By default, Nginx will load balance servers using round-robin load balancing
    method, but it also supports other load balancing methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Nginx 会使用轮询负载均衡方法来负载均衡服务器，但它也支持其他负载均衡方法。
- en: The Nginx `least_conn` load balancing method forwards to backend servers with
    the least connections at any particular time, whereas the Nginx `ip_hash` method
    of load balancing means that users can tie the same source address to the same
    target backend server for the entirety of a request.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx 的 `least_conn` 负载均衡方法会将请求转发到当前连接数最少的后端服务器，而 Nginx 的 `ip_hash` 负载均衡方法则意味着用户可以将相同的源地址绑定到同一目标后端服务器，以便整个请求过程中保持一致。
- en: This is useful as some applications require that all requests are tied to the
    same server using sticky sessions while transactions are processed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于某些应用程序非常有用，它们要求所有请求在事务处理过程中绑定到同一服务器上，即使用粘性会话。
- en: However, the proprietary Nginx Plus version supports an additional load balancing
    method named `least_time`, which calculates the lowest latency of backend servers
    based on the number of active connections and subsequently forwards requests appropriately
    based on those calculations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，专有版本的 Nginx Plus 支持一种额外的负载均衡方法，名为 `least_time`，该方法基于后端服务器的活动连接数计算最低延迟，并根据这些计算结果适当地转发请求。
- en: The Nginx load balancer uses a weighting system at all times when load balancing;
    all servers by default have a weight of `1`. If a weight other than `1` is placed
    on a server, it will not receive requests unless the other servers on a backend
    are not available to process requests. This can be useful when throttling specific
    amounts of traffic to backend servers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx 负载均衡器在进行负载均衡时始终使用权重系统；默认情况下，所有服务器的权重为 `1`。如果服务器的权重设置为 `1` 以外的值，除非后端的其他服务器无法处理请求，否则该服务器不会接收请求。此功能在对后端服务器进行流量限制时非常有用。
- en: 'In the following example, we can see that the backend servers have load balancing
    method least connection configured. Server `10.20.1.3` has a weight of `5`, meaning
    only when `10.20.1.2` and `10.20.1.4` are maxed out will requests is sent to the
    `10.20.1.3` backend server:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们可以看到后端服务器配置了最少连接数的负载均衡方法。服务器 `10.20.1.3` 的权重为 `5`，这意味着只有当 `10.20.1.2`
    和 `10.20.1.4` 达到最大负载时，才会将请求发送到 `10.20.1.3` 后端服务器：
- en: '![Nginx](img/B05559_05_04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Nginx](img/B05559_05_04.jpg)'
- en: By default, using round-robin load balancing in Nginx won't stop forwarding
    requests to servers that are not responding, so it utilizes `max_fails` and `fail_timeouts`
    for this.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Nginx 使用轮询负载均衡时，不会停止向未响应的服务器转发请求，因此需要利用 `max_fails` 和 `fail_timeouts`
    进行处理。
- en: 'In the following example, we can see server `10.20.1.2` and `10.20.1.4` have
    the `max_fail` count of `2` and a `fail_timeout` of `1` second; if this is exceeded
    then Nginx will stop directing traffic to these servers:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们可以看到服务器 `10.20.1.2` 和 `10.20.1.4` 的 `max_fail` 计数为 `2`，并且 `fail_timeout`
    为 `1` 秒；如果超过这个值，Nginx 将停止将流量引导到这些服务器：
- en: '![Nginx](img/B05559_05_05.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Nginx](img/B05559_05_05.jpg)'
- en: HAProxy
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HAProxy
- en: '**HAProxy** ([http://www.haproxy.org/](http://www.haproxy.org/)) is an open
    source x86 software load balancer that is session aware and can provide layer
    4 load balancing. The HAproxy load balancer can also carry out layer 7 context
    switching based on the content of the request as well as SSL/TLS termination.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**HAProxy** ([http://www.haproxy.org/](http://www.haproxy.org/)) 是一款开源的 x86
    软件负载均衡器，具有会话感知功能，并可以提供 4 层负载均衡。HAProxy 负载均衡器还可以根据请求的内容进行 7 层上下文切换，并支持 SSL/TLS
    终止。'
- en: HAProxy is primarily used for HTTP load balancing and can be set up in a redundant
    fashion using `keepalived` configuration using two apache configurations, so if
    the master fails, the slave will become the master to make sure there is no interruption
    in service for end users.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 主要用于 HTTP 负载均衡，并可以通过使用 `keepalived` 配置和两台 Apache 配置以冗余方式进行设置，这样如果主服务器出现故障，备份服务器将成为主服务器，以确保终端用户服务不中断。
- en: HAProxy uses declarative configuration files to support load balancing as opposed
    to an object model that proprietary load balancing solutions, such as NetScaler,
    F5 and Avi Networks, have adopted.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 使用声明式配置文件来支持负载均衡，而不是采用专有负载均衡解决方案（如 NetScaler、F5 和 Avi Networks）所采用的对象模型。
- en: 'The HAProxy configuration file has the following declarative configuration
    sections to allow load balancing to be set up:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 配置文件包含以下声明式配置部分，以允许设置负载均衡：
- en: '**Backend**: A backend declaration can contain one or more servers in it; backend
    servers are added in the format of a DNS record or an IP address. Multiple backend
    declarations can be set up on a HAProxy server. The load balancing algorithm can
    also be selected, such as round robin or least connection.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端**：后端声明可以包含一个或多个服务器；后端服务器可以以 DNS 记录或 IP 地址的形式添加。可以在 HAProxy 服务器上设置多个后端声明。还可以选择负载均衡算法，如轮询或最少连接数。'
- en: 'In the following example, we see two backend servers, `10.11.0.1` and `10.11.0.2`,
    load balanced using the round-robin algorithm on port `80`:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下示例中，我们看到两个后端服务器 `10.11.0.1` 和 `10.11.0.2`，使用轮询算法在端口 `80` 上进行负载均衡：
- en: '![HAProxy](img/B05559_05_06.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![HAProxy](img/B05559_05_06.jpg)'
- en: '**Check**: Checks avoid users having to manually remove a server from the backend
    if for any reason, it becomes unavailable and this mitigates outages. HAProxy''s
    default health always attempts to establish a TCP connection to the server using
    the default port and IP. HAProxy will automatically disable servers that are unable
    to serve requests to avoid outages. Servers will only be re-enabled when it passes
    its check. HAProxy will report whole backends as unavailable if all servers on
    a backend have failed their health checks.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查**：检查可以避免用户在服务器因某种原因变得不可用时手动将其从后端移除，从而减少停机时间。HAProxy 的默认健康检查总是尝试使用默认端口和
    IP 建立与服务器的 TCP 连接。如果服务器无法提供请求，HAProxy 会自动禁用该服务器以避免停机。服务器只有在通过健康检查后才会重新启用。如果后端所有服务器都未通过健康检查，HAProxy
    会将整个后端报告为不可用。'
- en: 'A number of different health checks can be put against backend servers by utilizing
    the option `{health-check}` line item; for instance, `tcp-check` in the following
    example can check on the health of port `8080` even though port `443` is being
    balanced:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以通过使用 `{health-check}` 选项对后端服务器进行多种健康检查；例如，在以下示例中，即使端口 `443` 正在进行负载均衡，`tcp-check`
    也可以检查端口 `8080` 的健康状况。
- en: '![HAProxy](img/B05559_05_07.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![HAProxy](img/B05559_05_07.jpg)'
- en: '**Access Control List (ACL)**: ACL declarations are used to inspect headers
    and forward to specific backend servers based on the headers. An ACL in HAProxy
    will try to find conditions and trigger actions based on this.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制列表（ACL）**：ACL 声明用于检查头信息并根据头信息将流量转发到特定的后端服务器。HAProxy 中的 ACL 将尝试根据条件触发动作。'
- en: '**Frontend**: The frontend declaration allows different kinds of traffic to
    be supported by the HAProxy load balancer.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前端**：前端声明允许 HAProxy 负载均衡器支持不同类型的流量。'
- en: 'In the following example, HAProxy will accept http traffic on port `80`, with
    an ACL matching requests only if the request starts with `/network` and it is
    then forwarded to the `high-perf-backend` if the ACL `/web-network` is matched:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下示例中，HAProxy 将接受端口 `80` 上的 http 流量，使用 ACL 只匹配以 `/network` 开头的请求，并且当匹配 `/web-network`
    的 ACL 时，它将被转发到 `high-perf-backend`：
- en: '![HAProxy](img/B05559_05_08.jpg)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![HAProxy](img/B05559_05_08.jpg)'
- en: Load balancing immutable and static infrastructure
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡不可变和静态基础设施
- en: With the introduction of public and private cloud solutions such as AWS and
    OpenStack, there has been a shift towards utilizing immutable infrastructure instead
    of traditional static servers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AWS 和 OpenStack 等公共和私有云解决方案的出现，已转向使用不可变基础设施，取代传统的静态服务器。
- en: This has raised a point of contention with *pets versus cattle* or, as Gartner
    defines it *bi-modal* ([http://www.gartner.com/it-glossary/bimodal/](http://www.gartner.com/it-glossary/bimodal/)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了关于 *宠物与牲畜* 的争论，或者正如 Gartner 定义的那样，*双模式* ([http://www.gartner.com/it-glossary/bimodal/](http://www.gartner.com/it-glossary/bimodal/))。
- en: Gartner has said that two different strategies need to be adopted, one for new
    microservices, *cattle,* and one for legacy infrastructure, *pets*. *Cattle* are
    servers that are killed off once they have served their purpose or have an issue,
    typically lasting one release iteration. Alternately, *pets* are servers that
    will have months or years of uptime and will be patched and cared for by operations
    staff.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Gartner 表示需要采取两种不同的策略，一种用于新的微服务，*牲畜*，另一种用于传统基础设施，*宠物*。*牲畜*是指一旦完成任务或出现问题就会被淘汰的服务器，通常生命周期为一个发布周期。相对地，*宠物*是指那些会有数月或数年正常运行时间并会由运维人员进行修补和照顾的服务器。
- en: Gartner defines *pets* as Mode 1 and *cattle* as Mode 2\. It is said that a
    *cattle* approach favors the stateless microservice cloud-native applications,
    whereas a *pet*, on the other hand, is any application that is a monolith, or
    potentially a single appliance or something that contains data, such as a database.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Gartner将*宠物*定义为模式1，将*牲畜*定义为模式2。据说，*牲畜*方法更适合无状态的微服务云原生应用，而*宠物*则是指任何单体应用，或者是包含数据的应用，比如数据库。
- en: Immutable infrastructure and solutions such as OpenStack and AWS are said by
    many to favor only the *cattle*, with monoliths and databases remaining pets still
    need a platform that caters for long-lived servers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人认为，不可变基础设施和像OpenStack和AWS这样的解决方案更倾向于*牲畜*，而单体应用和数据库仍然作为宠物，仍需要一个适用于长期运行服务器的平台。
- en: Personally, I find the *pets* versus *cattle* debate to be a very lazy argument
    and somewhat tiresome. Instead of dumping applications into two buckets, applications
    should be treated as a software delivery problem, which becomes a question of
    stateless read applications and stateful applications with caching and data. Cloud-native
    microservice applications still need data and state, so I am puzzled by the distinction.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我觉得*宠物*与*牲畜*的辩论是一个非常懒散的争论，令人有些疲惫。与其将应用程序划分为两类，不如把应用程序看作是一个软件交付问题，这就变成了无状态的读取应用和有状态的应用，后者涉及缓存和数据。云原生微服务应用仍然需要数据和状态，因此我对这种区分感到困惑。
- en: However, it is undisputed that the load balancer is key to immutable infrastructure,
    as at least one version of the application always needs to be exposed to a customer
    or other microservices to maintain that applications incur zero downtime and remain
    operational at all times.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，毫无争议的是，负载均衡器是不可变基础设施的关键，因为至少有一个版本的应用程序始终需要向客户或其他微服务公开，以确保该应用程序在任何时候都没有停机并保持运行状态。
- en: Static and immutable servers
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态和不可变服务器
- en: 'Historically, an operations team was used by companies to perform the following
    operations on servers:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在历史上，企业通常会使用运维团队执行以下服务器操作：
- en: Rack and cable
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装机架和电缆
- en: Providing firmware updates
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供固件更新
- en: Configuring the RAID configuration
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置RAID配置
- en: Installing an operating system
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装操作系统
- en: Patching the operating system
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打补丁操作系统
- en: This was all before making the servers available to developers. Static infrastructure
    can still exist within a cloud environment; for example, databases are still typically
    deployed as static, physical servers, given the volume of data that needs to be
    persisted on their local disk.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切发生在将服务器提供给开发人员之前。静态基础设施仍然可以存在于云环境中；例如，由于需要在本地磁盘上持久化大量数据，数据库通常仍作为静态的物理服务器部署。
- en: Static servers mean a set of long-lived servers that typically will contain
    state.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 静态服务器指的是一组通常会包含状态的长期运行服务器。
- en: Immutable servers, on the other hand, mean that every time a virtual machine
    is changed, a new virtual machine is deployed, complete with a new operating system
    and new software released on them, delete please. Immutable infrastructure means
    no in-place changes to a server's state.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，不可变服务器意味着每次虚拟机发生变化时，都会部署一个新的虚拟机，配备新的操作系统和新发布的软件，彻底删除旧的内容。不可变基础设施意味着不会对服务器状态进行就地更改。
- en: This moves away from the pain of doing in-place upgrades and makes sure that
    snowflake server configurations are a thing of the past, where every server, despite
    the best intentions, has drifted slightly from its desired state over a period
    of time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这避免了进行就地升级的痛苦，并确保雪花服务器配置成为过去，过去每台服务器尽管有最好的意图，但在一段时间后总会有轻微的偏离其期望状态。
- en: How many times when releasing software has a release worked on four out of five
    machines, and hours or days were wasted debugging why a particular software upgrade
    wasn't working on a particular server.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 发布软件时，有多少次软件只在五台机器中的四台上正常工作，浪费了数小时或数天的时间来调试为什么某个软件升级在特定服务器上无法正常工作。
- en: Immutable infrastructure builds servers from a known state promoting the same
    configuration to quality assurance, integration, performance testing, and production
    environments.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 不可变基础设施从已知状态构建服务器，推动相同的配置到质量保证、集成、性能测试和生产环境。
- en: Parts of cloud infrastructure can be made completely immutable to reap these
    benefits. The operating system is one such candidate; rather than doing in-place
    patching, a single golden image can be created and patched using automation tooling
    such as Packer in a fully automated fashion.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 云基础设施的部分组件可以完全不可变，以此来获取这些优势。操作系统就是其中一个候选对象；与其进行就地修补，不如创建一个单一的金镜像，并使用自动化工具（如Packer）进行完全自动化的补丁更新。
- en: Applications that require a caching layer are more stateful by nature so that
    cache needs to be available at all times to serve other applications. These caching
    applications should be deployed as clusters, which are load balanced, and rolling
    updates will be done to make sure one version of the cache data is always available.
    A new software release of that caching layer should synchronize the cache to the
    new release before the pervious release is destroyed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 需要缓存层的应用程序本质上更有状态，因此缓存需要始终可用，以便为其他应用程序提供服务。这些缓存应用程序应该以集群的形式部署，并进行负载均衡，滚动更新将确保始终有一个版本的缓存数据可用。该缓存层的新软件版本应该在销毁之前的版本之前同步缓存数据。
- en: Data, on the other hand, is always persistent, so can be stored on persistent
    storage and then mounted by the operating system. When doing an immutable rolling
    update, the operating system layer can mount the data on either persistent or
    shared storage as part of the release process.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据是持久的，因此可以存储在持久存储中，然后由操作系统挂载。当进行不可变的滚动更新时，操作系统层可以在发布过程中将数据挂载到持久或共享存储上。
- en: It is possible to separate the operating system and the data to make all virtual
    machines stateless, for instance, OpenStack Cinder ([https://wiki.openstack.org/wiki/Cinder](https://wiki.openstack.org/wiki/Cinder))
    can be utilized to store persistent data on volumes that can be attached to virtual
    machines.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将操作系统和数据分开，从而使所有虚拟机无状态。例如，可以利用OpenStack Cinder（[https://wiki.openstack.org/wiki/Cinder](https://wiki.openstack.org/wiki/Cinder)）将持久数据存储在可以附加到虚拟机上的卷中。
- en: With all these use cases considered, most applications can be designed to be
    deployed immutably through proper configuration management, even monoliths, as
    long as they are not a single point of failure. If any applications are single
    points of failure, they should be rearchitected as releasing software should never
    result in downtime. Although applications are stateful, each state can be updated
    in stages so that an overall immutable infrastructure model can be maintained.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些使用场景，大多数应用程序可以通过适当的配置管理设计为不可变的部署，甚至是单体应用程序，只要它们不是单点故障。如果某些应用程序是单点故障，它们应该被重新架构，因为发布软件不应导致停机。尽管应用程序是有状态的，但每个状态可以分阶段更新，从而保持一个总体的不可变基础设施模型。
- en: Blue/green deployments
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: The **blue green deployment** process is not a new concept. Before cloud solutions
    came to prominence, production servers would typically have a set of servers consisting
    of blue (no live traffic) and green (serving customer traffic) that would be utilized.
    These are typically known as blue and green servers, which alternated per release.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**蓝绿部署**过程并不是一个新概念。在云解决方案崭露头角之前，生产服务器通常会有一组由蓝色（无实时流量）和绿色（提供客户流量）服务器组成的服务器集群，这些服务器会交替使用。这些通常被称为蓝绿服务器，每次发布时会进行交替。'
- en: The blue green model in simple terms means that when a software upgrade needed
    to be carried out, the blue servers would be upgraded to the latest software version.
    Once the upgrade had been completed, the blue servers would become the new green
    servers with live traffic switched to serve from the newly upgraded servers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，蓝绿模型意味着，当需要进行软件升级时，蓝色服务器会被升级到最新的软件版本。升级完成后，蓝色服务器将成为新的绿色服务器，实时流量将切换到新升级的服务器上。
- en: The switching of live traffic was typically done by switching DNS entries to
    point at the newly upgraded servers. So once the DNS **Time To Live** (**TTL**)
    had propagated, end user requests would be served by the newly upgraded servers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 切换实时流量通常是通过将DNS条目切换到指向新升级服务器来完成的。因此，一旦DNS **生存时间**（**TTL**）传播完成，最终用户的请求就会由新升级的服务器来处理。
- en: This means that if there was an issue with a software release, rollback could
    be achieved by switching back the DNS entries to point at the previous software
    version.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果软件发布出现问题，可以通过将DNS条目切换回指向上一个软件版本来实现回滚。
- en: 'A typical blue green deployment process is described here:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的蓝绿部署过程如下所述：
- en: '**Release 1.1** would be deployed on servers 1, 2, and 3 and served on a load
    balancer to customers and made Green (live):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**Release 1.1** 将部署在服务器 1、2 和 3 上，并通过负载均衡器向客户提供服务，并设置为绿色（上线）：'
- en: '![Blue/green deployments](img/B05559_05_09.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![蓝绿部署](img/B05559_05_09.jpg)'
- en: '**Release 1.2** would be deployed on servers 4, 5, and 6 and then be patched
    to the latest patch version, upgraded to the latest release and tested. When ready,
    the operations team would toggle the load balancer to serve boxes 4, 5, and 6
    as the new production release, as shown later, and the previously green (live)
    deployment would become blue, and vice versa:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**Release 1.2** 将部署在服务器 4、5 和 6 上，然后打上最新的补丁版本，升级到最新的发布版本并进行测试。准备好后，运维团队将切换负载均衡器，将
    4、5 和 6 号服务器设置为新的生产发布版本，如后续所示，而之前的绿色（上线）部署将变为蓝色，反之亦然：'
- en: '![Blue/green deployments](img/B05559_05_10.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![蓝绿部署](img/B05559_05_10.jpg)'
- en: 'When the operations team came to do the next release, servers 1, 2, and 3 would
    be patched to the latest version, upgraded to **Release 1.3** from **Release 1.1**,
    tested, and when ready, the operations team would direct traffic to the new release
    using the load balancer, making **Release 1.2** blue and **Release 1.3** green,
    as shown in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当运维团队进行下一个发布时，服务器 1、2 和 3 将被打上最新的版本，升级到 **Release 1.3**（从 **Release 1.1** 升级），进行测试，准备好后，运维团队将使用负载均衡器将流量引导到新版本，使
    **Release 1.2** 为蓝色，**Release 1.3** 为绿色，如下图所示：
- en: '![Blue/green deployments](img/B05559_05_11.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![蓝绿部署](img/B05559_05_11.jpg)'
- en: This was traditionally the procedure of running a blue green deployment using
    static servers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这是使用静态服务器进行蓝绿部署的过程。
- en: 'However, when using an immutable model, instead of using long-lived static
    servers, such as Servers 1, 2, 3, 4, 5, and 6, after a release was successful,
    the servers would be destroyed, as shown here, as they have served their purpose:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在使用不可变模型时，与使用长寿命的静态服务器（例如服务器 1、2、3、4、5 和 6）不同，在发布成功后，服务器将被销毁，如此处所示，因为它们已经完成了使命：
- en: '![Blue/green deployments](img/B05559_05_12.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![蓝绿部署](img/B05559_05_12.jpg)'
- en: The next time servers 4, 5, and 6 were required, instead of doing an in-place
    upgrade, three new virtual machines would be created from the golden base image
    in a cloud environment. These golden images would already be patched up to the
    latest version, so brand new servers 7, 8, and 9 with the old servers destroyed
    and the new **Release 1.4** would be deployed on them, as shown later.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 下次需要服务器 4、5 和 6 时，不是进行就地升级，而是会从云环境中的黄金基础镜像创建三台新虚拟机。这些黄金镜像已经打好最新的补丁，因此全新的服务器
    7、8 和 9 会在销毁旧服务器后部署 **Release 1.4**，如后续所示。
- en: 'Once server 7, 8, and 9 were live, servers 1, 2, and 3 would be destroyed as
    they have served their purpose:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务器 7、8 和 9 上线，服务器 1、2 和 3 将被销毁，因为它们已经完成了使命：
- en: '![Blue/green deployments](img/B05559_05_13.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![蓝绿部署](img/B05559_05_13.jpg)'
- en: Using Ansible to Orchestrate load balancers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ansible 协调负载均衡器
- en: In [Chapter 4](ch04.html "Chapter 4. Configuring Network Devices Using Ansible"),
    *Configuring Network Devices Using Ansible*, we covered the basics of Ansible
    and how to use an Ansible Control Host, playbooks, and roles for configuration
    management of network devices. Ansible, though, has multiple different core operations
    that can help with orchestrating load balancers, which we will look at in this
    chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 4 章](ch04.html "第 4 章 配置网络设备使用 Ansible")，*使用 Ansible 配置网络设备* 中，我们介绍了 Ansible
    的基本概念，以及如何使用 Ansible 控制主机、playbooks 和角色进行网络设备的配置管理。然而，Ansible 还有许多不同的核心操作可以帮助协调负载均衡器，我们将在本章中详细讨论。
- en: Delegation
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 委托
- en: Ansible delegation is a powerful mechanism that means from a playbook or role,
    Ansible can carry out actions on the target servers specified in the inventory
    file by connecting to them using SSH or WinRM, or alternately execute commands
    from the Ansible Control Host. WinRM is the Microsoft remote management standard
    and the equivalent of SSH for Windows that allows administrators to connect to
    Windows guests and execute programs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible 委托是一种强大的机制，意味着从 playbook 或角色中，Ansible 可以通过 SSH 或 WinRM 连接到清单文件中指定的目标服务器，或者从
    Ansible 控制主机上交替执行命令。
- en: 'The following diagram shows these two alternative connection methods with the
    Ansible Control Host either logging in to boxes using SSH or WinRM to configure
    them or running an API call from the Ansible Control Host directly:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这两种替代连接方法，Ansible 控制主机要么通过 SSH 登录到机器配置它们，要么直接从 Ansible 控制主机运行 API 调用：
- en: '![Delegation](img/B05559_05_26.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![委托](img/B05559_05_26.jpg)'
- en: Both of these options can be carried out from the same role or playbook using
    `delegate_to`, which makes playbooks and roles extremely flexible as they can
    combine API calls and server-side configuration management tasks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种选项都可以通过相同的角色或剧本执行，使用 `delegate_to`，这使得剧本和角色非常灵活，因为它们可以结合 API 调用和服务器端配置管理任务。
- en: 'An example of delegation can be found later where the Ansible extras HAProxy
    modules are used, with `delegate_to` used to trigger an orchestration action that
    disables all backend services in the inventory file:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 委托的一个示例可以在后面找到，那里使用了 Ansible 附加的 HAProxy 模块，并使用 `delegate_to` 来触发一个协调操作，该操作禁用库存文件中的所有后端服务：
- en: '![Delegation](img/B05559_05_15.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![委托](img/B05559_05_15.jpg)'
- en: Utilizing serial to control roll percentages
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 serial 控制滚动百分比
- en: In order to release software without interruptions to service, a zero downtime
    approach is preferable, as it doesn't require a maintenance window to schedule
    a change or release. Ansible supports a *serial* option, which passes a percentage
    value to a playbook.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不打断服务的情况下发布软件，最好采用零停机时间的方法，因为它不需要维护窗口来安排更改或发布。Ansible 支持 *serial* 选项，它将百分比值传递给剧本。
- en: The *serial* option allows Ansible to iterate over the inventory and only carry
    out the action against a percentage of the boxes, completing the necessary playbook,
    before moving onto the next portion of the inventory. It is important to note
    that Ansible passes inventory as an unordered dictionary, so the percentage of
    the inventory that is processed will not be in a specific order.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*serial* 选项允许 Ansible 遍历库存，并仅对一部分机器执行操作，完成必要的剧本后，再移动到库存的下一部分。需要注意的是，Ansible
    将库存传递为一个无序的字典，因此处理的库存百分比将不是按特定顺序进行的。'
- en: 'Using the *serial* option that a blue/green strategy could be employed in Ansible,
    so boxes will need to be taken out of the load balancer and upgraded before being
    put back into service. Rather than doubling up on the number of boxes, three boxes
    are required, as shown in the following image, which all serve **Release 1.4**:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *serial* 选项，Ansible 可以采用蓝绿策略，因此需要将机器从负载均衡器中移除并进行升级，然后再投入使用。与增加机器数量不同，三个机器是足够的，如下图所示，所有这些机器都运行
    **版本 1.4**：
- en: '![Utilizing serial to control roll percentages](img/B05559_05_16.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![利用 serial 控制滚动百分比](img/B05559_05_16.jpg)'
- en: 'Utilizing the following Ansible playbook, using a combination of `delegate_to`
    and `serial`, each of the servers can be upgraded using a rolling update:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下 Ansible 剧本，结合 `delegate_to` 和 `serial`，可以通过滚动更新升级每台服务器：
- en: '![Utilizing serial to control roll percentages](img/B05559_05_17.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![利用 serial 控制滚动百分比](img/B05559_05_17.jpg)'
- en: 'The playbook will execute the following steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该剧本将执行以下步骤：
- en: The `serial 30%` will mean that only one server at a time is upgraded. So, **Server
    7** will be taken out of the HAProxy `backend_nodes` pool by disabling the service
    calling the HAProxy using a local `delegate_to` action on the Ansible Control
    Host. A `yum` update will then be executed to upgrade the server version new `application1`
    release **version 1.5** on **server 7**, as follows:![Utilizing serial to control
    roll percentages](img/B05559_05_18.jpg)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`serial 30%` 表示一次只有一个服务器进行升级。因此，**服务器 7** 将通过在 Ansible 控制主机上使用本地 `delegate_to`
    操作来从 HAProxy 的 `backend_nodes` 池中移除，禁用该服务，然后执行 `yum` 更新，升级服务器版本并安装新的 `application1`
    发布 **版本 1.5**，如下所示：![利用 serial 控制滚动百分比](img/B05559_05_18.jpg)'
- en: '**Server 7** will then be enabled again and put into service on the load balancer
    using a local `delegate_to` action. The serial command will iterate onto `server
    8` and disable it on HAProxy, before doing a `yum` update to upgrade the server
    version new `application1` release **version 1.5**, as follows:![Utilizing serial
    to control roll percentages](img/B05559_05_19.jpg)'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**服务器 7** 将再次启用，并通过本地 `delegate_to` 操作将其重新投入负载均衡器中。serial 命令将继续作用于 `server
    8`，在 HAProxy 上禁用它，然后执行 `yum` 更新，升级服务器版本并安装新的 `application1` 发布 **版本 1.5**，如下所示：![利用
    serial 控制滚动百分比](img/B05559_05_19.jpg)'
- en: The rolling update will then enable **Server 8** on the load balancer, and the
    serial command will iterate onto **Server 9**, disabling it on HAProxy before
    doing a yum update, which will upgrade the server with the new `application1`
    release **version 1.5** alternating when necessary between execution on the local
    server and the server, as shown here:![Utilizing serial to control roll percentages](img/B05559_05_20.jpg)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动更新将启用负载均衡器上的**服务器 8**，然后序列命令将切换到**服务器 9**，在 HAProxy 上禁用该服务器，然后执行 yum 更新，这将升级服务器并安装新的`application1`版本**1.5**，根据需要在本地服务器和远程服务器之间交替执行，如下所示：![使用序列控制滚动百分比](img/B05559_05_20.jpg)
- en: Finally, the playbook will finish by enabling **server 9** on the load balancer,
    and all servers will be upgraded to **Release 1.5** using Ansible as follows:![Utilizing
    serial to control roll percentages](img/B05559_05_21.jpg)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，剧本将通过在负载均衡器上启用**服务器 9**来完成，所有服务器将使用 Ansible 升级到**版本 1.5**，具体如下：![使用序列控制滚动百分比](img/B05559_05_21.jpg)
- en: Dynamic inventories
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态库存
- en: When dealing with cloud platforms, using just static inventories is sometimes
    not enough. It is useful to understand the inventory of servers that are already
    deployed within the estate and target subsets of them based on characteristics
    or profiles.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理云平台时，仅使用静态库存有时并不够。了解已部署在资源中的服务器库存，并根据特征或配置文件选择其中的子集是非常有用的。
- en: Ansible has an enhanced feature named the dynamic inventory. It allows users
    to query a cloud platform of their choosing with a Python script; this will act
    as an autodiscovery tool that can be connected to AWS or OpenStack, returning
    the server inventory in JSON format.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible 有一个增强功能，称为动态库存。它允许用户使用 Python 脚本查询他们选择的云平台；这将作为自动发现工具，可以连接到 AWS 或 OpenStack，并返回
    JSON 格式的服务器库存。
- en: This allows Ansible to load this JSON file into a playbook or role so that it
    can be iterated over. In the same way, a static inventory file can be via variables.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许 Ansible 将此 JSON 文件加载到剧本或角色中，以便进行迭代。同样，静态库存文件也可以通过变量传递。
- en: 'The dynamic inventory fits into the same command-line constructs instead of
    passing the following static inventory:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 动态库存适用于相同的命令行结构，而不是传递以下静态库存：
- en: '[PRE0]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, a dynamic inventory script, `openstack.py`, for the OpenStack cloud provider
    could be passed instead:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以传递 OpenStack 云提供商的动态库存脚本`openstack.py`：
- en: '[PRE1]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dynamic inventory script can be set up to allow specific limits. In the
    preceding case, the only server inventory that has been returned is the quality
    assurance servers, which is controlled using the `–l qa` limit.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 动态库存脚本可以设置特定限制。在前述情况下，返回的唯一服务器库存是质量保证服务器，这是通过`–l qa`限制来控制的。
- en: When using Ansible with immutable servers, the static inventory file can be
    utilized to spin up new virtual machines, whereas the static inventory can be
    used to query the estate and do supplementary actions when they have already been
    created.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Ansible 配置不可变服务器时，可以利用静态库存文件来启动新的虚拟机，而静态库存则可以在虚拟机已经创建后查询资源并执行附加操作。
- en: Tagging metadata
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签元数据
- en: When using dynamic inventory in Ansible, metadata becomes a very important component,
    as servers deployed in a cloud environment can be sorted and filtered using metadata
    that is tagged against virtual or physical machines.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ansible 中使用动态库存时，元数据变得非常重要，因为在云环境中部署的服务器可以使用标记在虚拟或物理机器上的元数据进行排序和过滤。
- en: When provisioning AWS, Microsoft Azure, or OpenStack instances in a public or
    private cloud, metadata can be tagged against servers to group them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置 AWS、Microsoft Azure 或 OpenStack 实例时，可以在公共或私有云中为服务器标记元数据，以便对它们进行分组。
- en: 'In the following example, we can see a playbook creating new OpenStack servers
    using the `os_server` OpenStack module. It will iterate over the static inventory,
    tagging each newly created group, and release metadata on the machine:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们可以看到一个剧本使用`os_server` OpenStack 模块创建新的 OpenStack 服务器。它将遍历静态库存，标记每个新创建的组，并释放机器上的元数据：
- en: '![Tagging metadata](img/B05559_05_22.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![标签元数据](img/B05559_05_22.jpg)'
- en: 'The dynamic inventory can then be filtered using the `–l` argument to specify
    boxes with `group: qa`. This will return a consolidated list of servers.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，可以使用`–l`参数过滤动态库存，指定`group: qa`的服务器。这将返回一个合并后的服务器列表。'
- en: Jinja2 filters
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jinja2 过滤器
- en: '**Jinja2 filters** allow Ansible to filter a playbook or role, allowing it
    to control which conditions need to be satisfied before executing a particular
    command or module. There are a wide variety of different jinja2 filters available
    out of the box with Ansible or custom filters can be written.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jinja2 过滤器**允许 Ansible 过滤 playbook 或角色，从而控制在执行特定命令或模块之前需要满足哪些条件。Ansible 提供了多种现成的
    Jinja2 过滤器，或者也可以编写自定义过滤器。'
- en: 'An example of a playbook using a jinja2 filter would only add the server to
    the NetScaler if its metadata `openstack.metadata.build` value is equal to the
    current build version:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Jinja2 过滤器的 playbook 示例将仅在服务器的元数据 `openstack.metadata.build` 值等于当前构建版本时，才将该服务器添加到
    NetScaler 中：
- en: '![Jinja2 filters](img/B05559_05_23.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![Jinja2 过滤器](img/B05559_05_23.jpg)'
- en: 'Executing the ansible-playbook `add_hosts_to_netscaler.yml command` with a
    limit `–l` on `qa` would only return boxes in the `qa` metadata group as the inventory.
    Then, the boxes can be further filtered at playbook or role using the when jinja2
    filter to only execute the `add into load balancer pool` command if the `openstack.metadata.build`
    number of the box matches the `current_build` variable of `9`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 ansible-playbook `add_hosts_to_netscaler.yml` 命令，并通过 `–l` 限制为 `qa`，将仅返回 `qa`
    元数据组中的主机作为库存。然后，主机可以在 playbook 或角色中进一步使用 when Jinja2 过滤器进行过滤，只有当主机的 `openstack.metadata.build`
    值与 `current_build` 变量 `9` 匹配时，才会执行“加入负载均衡池”命令：
- en: '[PRE2]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result of this would be that only the new boxes would be added to the NetScaler
    `lbvserver` VIP.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是，只有新的主机会被添加到 NetScaler 的 `lbvserver` VIP 中。
- en: 'The boxes could be removed in a similar way in the same playbook with a *not
    equal to* condition:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主机也可以以类似方式在同一 playbook 中使用 *不等于* 条件删除：
- en: '![Jinja2 filters](img/B05559_05_24.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![Jinja2 过滤器](img/B05559_05_24.jpg)'
- en: This could all be combined along with the serial percentage to roll percentages
    of the new release into service on the load balancer and decommission the old
    release utilizing dynamic inventory, delegation, jinja2 filters, and the serial
    rolling update features of Ansible together for simple orchestration of load balancers.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些可以结合使用，同时配合串行百分比，将新版本逐步部署到负载均衡器中，并利用动态库存、委托、Jinja2 过滤器和 Ansible 的串行滚动更新功能一起实现简单的负载均衡器编排。
- en: Creating Ansible networking modules
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Ansible 网络模块
- en: As Ansible can be used to schedule API commands against a load balancer, it
    can be easily utilized to build out a load balancer object model that popular
    networking solutions, such as Citrix NetScaler, F5 Big-IP, or Avi Networks, utilize.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Ansible 可用于调度对负载均衡器的 API 命令，因此可以轻松地用它来构建出流行网络解决方案（如 Citrix NetScaler、F5 Big-IP
    或 Avi Networks）所使用的负载均衡器对象模型。
- en: With the move to microservice architectures, load balancing configuration needs
    to be broken out to remain manageable, so it is application-centric , as opposed
    to living in a centralized monolith configuration file.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务架构的推广，负载均衡配置需要拆分以保持可管理性，因此它是以应用为中心的，而不是存储在集中式的单体配置文件中。
- en: This means that there are operational concerns when doing load balancing changes,
    so Ansible can be utilized by network operators to build out the complex load
    balancing rules, apply SSL certificates, and set up more complex layer 7 context
    switching or public IP addresses and provide this as a service to developers.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在进行负载均衡更改时会涉及操作问题，因此网络操作员可以使用 Ansible 来构建复杂的负载均衡规则、应用 SSL 证书、设置更复杂的第7层上下文切换或公共
    IP 地址，并将这些作为服务提供给开发人员。
- en: Utilizing the Python APIs provided by load balancing vendors, each operation
    could then be created as a module with a set of YAML `var` files describing the
    intended state of the load balancer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 利用负载均衡供应商提供的 Python API，每个操作可以作为一个模块创建，并使用一组 YAML `var` 文件来描述负载均衡器的预期状态。
- en: 'In the example mentioned later, we look at how Ansible var files could be utilized
    by developers to create a service and health monitor for every new virtual server
    on a NetScaler. These services are then bound to the `lbvserver` entity, which
    was created by the network team, with a roll percentage of 10%, which can be loaded
    into the playbook''s `serial` command. The playbook or role is utilized to create
    services, lbmonitors, 34 servers and bind services to lbvservers, whereas the
    var file describes the desired state of those NetScaler objects:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在稍后的示例中，我们将探讨开发人员如何利用 Ansible 的变量文件为每个新的虚拟服务器创建服务和健康监控器，这些虚拟服务器运行在 NetScaler
    上。然后，这些服务会绑定到由网络团队创建的 `lbvserver` 实体，并设置为 10% 的滚动百分比，可以加载到 playbook 的 `serial`
    命令中。playbook 或角色用于创建服务、lbmonitors、34 个服务器并将服务绑定到 lbvservers，而变量文件则描述了这些 NetScaler
    对象的期望状态：
- en: '![Creating Ansible networking modules](img/B05559_05_25.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![创建 Ansible 网络模块](img/B05559_05_25.jpg)'
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw that the varied load balancing solutions are available
    from proprietary vendors to open source solutions, and discussed the impact that
    microservices have had on load balancing, moving it from a centralized to distributed
    model to help serve east-west traffic.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到从专有供应商到开源解决方案都提供了各种负载均衡解决方案，并讨论了微服务对负载均衡的影响，将其从集中式模型转变为分布式模型，以帮助应对东西向流量。
- en: We then looked at blue/green deployment models, the merits of immutable and
    static servers, and how software releases can be orchestrated using Ansible in
    either model. In the process, we illustrated how useful Ansible is at orchestrating
    load balancers by utilizing dynamic inventory, rolling updates, delegation, and
    jinja2 filters can all be used to help fulfill load balancing requirements.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了蓝绿部署模型、不可变和静态服务器的优点，以及如何在这两种模型中使用 Ansible 协调软件发布。在此过程中，我们展示了 Ansible
    在协调负载均衡器时的有用性，通过利用动态库存、滚动更新、委派和 jinja2 过滤器，帮助实现负载均衡的需求。
- en: The key takeaways from this chapter are that microservice applications have
    changed the way applications need to be load balanced, and distributed load balancing
    is better suited when deploying microservice applications, which have more east-west
    traffic patterns.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键要点是，微服务应用程序改变了应用程序负载均衡的方式，分布式负载均衡在部署微服务应用程序时更为适合，因为微服务应用程序具有更多的东西-西行流量模式。
- en: The reasons that immutable infrastructure is well-suited to microservice applications
    should now be clear.. The chapter also defined ways that state and data can be
    separated from the operating system and that different rolling update models are
    required to support stateless and stateful applications. In the next chapter,
    we will look at applying these same automation principles to SDN Controllers,
    primarily focusing on the Nuage solution. It will cover configuring firewall rules
    and other SDN commands, so the whole network can be programmatically controlled
    and automated.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不可变基础设施为什么非常适合微服务应用程序的原因现在应该很清楚了。本章还定义了如何将状态和数据从操作系统中分离，以及支持无状态和有状态应用程序所需的不同滚动更新模型。在下一章中，我们将研究如何将这些相同的自动化原则应用到
    SDN 控制器上，主要聚焦于 Nuage 解决方案。它将涵盖配置防火墙规则和其他 SDN 命令，从而使整个网络可以通过编程进行控制和自动化。
