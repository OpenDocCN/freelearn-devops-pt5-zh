- en: Docker Swarm Networking and Reverse Proxy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm 网络和反向代理
- en: The most compelling reason for most people to buy a computer for the home will
    be to link it to a nationwide communications network. We’re just in the beginning
    stages of what will be a truly remarkable breakthrough for most people - as remarkable
    as the telephone.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人购买家庭计算机的最有力理由将是将其连接到全国范围的通信网络。我们正处于这一真正的突破性进展的初期阶段——对于大多数人来说，它将像电话一样令人震惊。
- en: –Steve Jobs
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: –史蒂夫·乔布斯
- en: '**Software-Defined Network** (**SDN**) is a cornerstone of efficient cluster
    management. Without it, services distributed across the cluster would not be able
    to find each other.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**软件定义网络**（**SDN**）是高效集群管理的基石。没有它，分布在集群中的服务将无法找到彼此。'
- en: Having proxies based on static configuration does not fit the world of highly
    dynamic scheduling. Services are created, updated, moved around the cluster, scaled
    and de-scaled, and so on. In such a setting, information changes all the time.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基于静态配置的代理并不适合高度动态调度的世界。服务会被创建、更新、在集群中移动、扩展和缩减，等等。在这种环境中，信息时刻在变化。
- en: One approach we can take is to use a proxy as a central communication point
    and make all the services speak with each other through it. Such a setting would
    require us to monitor changes in the cluster continuously and update the proxy
    accordingly. To make our lives easier, a monitoring process would probably use
    one of the service registries to store the information and a templating solution
    that would update proxy configuration whenever a change in the registry is detected.
    As you can imagine, building such a system is anything but trivial.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用的一种方法是使用代理作为中央通信点，让所有服务通过它进行通信。这样的设置将要求我们持续监控集群中的变化，并相应地更新代理。为了简化我们的工作，监控进程可能会使用其中一个服务注册中心来存储信息，并使用一个模板解决方案，每当注册中心检测到变化时，更新代理配置。正如你可以想象的那样，构建这样的系统一点也不简单。
- en: Fortunately, Swarm comes with a brand new networking capability. In a nutshell,
    we can create networks and attach them to services. All services that belong to
    the same network can speak with each other using only the name of the service.
    It goes even further. If we scale a service, Swarm networking will perform round-robin
    load balancing and distribute the requests across all the instances. When even
    that is not enough, we have a new network called `ingress` with `routing mesh`
    that has all those and a few additional features.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Swarm 提供了全新的网络功能。简而言之，我们可以创建网络并将它们附加到服务上。所有属于同一网络的服务可以仅通过服务名称互相通信。更进一步，如果我们扩展某个服务，Swarm
    网络将执行轮询负载均衡，并将请求分配到所有实例。当这一切仍然不够时，我们有一个新的网络，名为`ingress`，其包含`routing mesh`，具备所有这些功能及一些额外特性。
- en: Efficient usage of Swarm networking is not sufficient by itself. We still need
    a reverse proxy that will be a bridge between the external world and our services.
    Unless there are special requirements, the proxy does not need to perform load
    balancing (Swarm networking does that for us). However, it does need to evaluate
    request paths and forward requests to a destination service. Even in that case,
    Swarm networking helps a lot. Configuring reverse proxy becomes a relatively easy
    thing to do as long as we understand how networking works and can harness its
    full potential.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 高效使用 Swarm 网络本身并不足够。我们仍然需要一个反向代理，作为外部世界与我们的服务之间的桥梁。除非有特殊要求，否则代理不需要执行负载均衡（Swarm
    网络已经为我们完成这项工作）。然而，代理确实需要评估请求路径并将请求转发到目标服务。即使是这种情况，Swarm 网络仍然提供了很大的帮助。只要我们理解网络是如何工作的，并能够充分利用其潜力，配置反向代理变得相对容易。
- en: Let’s see the networking in practice.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来实践一下网络功能。
- en: Setting up a cluster
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置集群
- en: We’ll create a similar environment as we did in the previous chapter. We'll
    have three nodes which will form a Swarm cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个与上一章相似的环境。我们将有三台节点，它们将形成一个 Swarm 集群。
- en: All the commands from this chapter are available in the `03-networking.sh` ([https://gist.github.com/vfarcic/fd7d7e04e1133fc3c90084c4c1a919fe](https://gist.github.com/vfarcic/fd7d7e04e1133fc3c90084c4c1a919fe))
    Gist.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在`03-networking.sh` ([https://gist.github.com/vfarcic/fd7d7e04e1133fc3c90084c4c1a919fe](https://gist.github.com/vfarcic/fd7d7e04e1133fc3c90084c4c1a919fe))
    Gist中找到。
- en: 'By this time, you already know how to set up a cluster so we''ll skip the explanation
    and just do it:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，你已经知道如何设置集群了，所以我们跳过解释，直接开始：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the last command `node ls` is as follows (IDs were removed for
    brevity):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个命令`node ls`的输出如下（为了简洁，已去除ID）：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we have a cluster of three nodes with `node-1` being the only
    manager (and hence the leader).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有一个由三个节点组成的集群，其中`node-1`是唯一的管理节点（因此也是领导节点）。
- en: Now that we have a fully operating cluster, we can explore the benefits Docker
    networking provides in conjunction with Swarm. We already worked with Swarm networking
    in the previous chapter. Now its time to go deeper, gain a better understanding
    of what we already saw, and unlock some new features and use cases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个完全运行的集群，可以探索 Docker 网络与 Swarm 结合所提供的好处。我们在上一章中已经使用过 Swarm 网络。现在是时候深入了解我们已经看到的内容，并解锁一些新的功能和用例了。
- en: Requirements of secured and fault tolerant services running with high availability
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性下运行的安全且容错的服务需求
- en: Let us quickly go over the internals of the *go-demo* application. It consists
    of two services. Data is stored in a MongoDB. The database is consumed by a backend
    service called `go-demo`. No other service should access the database directly.
    If another service needs the data, it should send a request to the `go-demo` service.
    That way we have clear boundaries. Data is owned and managed by the `go-demo`
    service. It exposes an API that is the only access point to the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下*go-demo*应用程序的内部结构。它由两个服务组成。数据存储在 MongoDB 中。数据库由名为`go-demo`的后端服务使用。其他服务不应直接访问数据库。如果其他服务需要数据，它应该向`go-demo`服务发送请求。这样，我们就有了明确的边界。数据由`go-demo`服务拥有和管理。它暴露了一个
    API，作为访问数据的唯一入口。
- en: 'The system should be able to host multiple applications. Each will have a unique
    base URL. For example, the `go-demo` path starts with `/demo`. The other applications
    will have different paths (example:  `/users`, `/products`, and so on). The system
    will be accessible only through ports `80`  for HTTP and `443` HTTPS. Please note
    that there can be no two processes that can listen to the same port. In other
    words, only a single service can be configured to listen to port `80`.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 系统应该能够托管多个应用程序。每个应用程序将有一个唯一的基础 URL。例如，`go-demo`路径以`/demo`开头。其他应用程序将有不同的路径（例如：`/users`，`/products`，等等）。系统将只能通过`80`端口（HTTP）和`443`端口（HTTPS）进行访问。请注意，不能有两个进程监听相同的端口。换句话说，只能配置一个服务来监听端口`80`。
- en: To meet load fluctuations and use the resources effectively, we must be able
    to scale (or de-scale) each service individually and independently from the others.
    Any request to any of the services should pass through a load balancer that will
    distribute the load across all instances. As a minimum, at least two instances
    of any service should be running at any given moment. That way, we can accomplish
    high availability even in case one of the instances stops working. We should aim
    even higher than that and make sure that even a failure of a whole node does not
    interrupt the system as a whole.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对负载波动并有效利用资源，我们必须能够单独且独立地扩展（或缩减）每个服务。任何对服务的请求都应该通过负载均衡器，负载均衡器将把负载分配到所有实例上。至少，任何服务在任意时刻都应该运行至少两个实例。这样，即使其中一个实例停止工作，我们也能确保高可用性。我们的目标应该更高，确保即使整个节点发生故障，系统整体也不会受到影响。
- en: To meet performance and fail-over needs services should be distributed across
    the cluster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足性能和故障转移需求，服务应分布在集群中。
- en: 'We''ll make a temporary exception to the rule that each service should run
    multiple instances. Mongo volumes do not work with Docker Machine on OS X and
    Windows. Later on, when we reach the chapters that provide guidance towards production
    setup inside major hosting providers (example: AWS), we''ll remove this exception
    and make sure that the database is also configured to run with multiple instances.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对每个服务应该运行多个实例的规则做出一个临时例外。Mongo 卷在 OS X 和 Windows 上与 Docker Machine 不兼容。稍后，当我们进入关于如何在主要托管提供商（例如
    AWS）内部进行生产环境设置的章节时，我们将取消这一例外，并确保数据库也配置为支持多个实例运行。
- en: 'Taking all this into account, we can make the following requirements:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 综合考虑所有这些，我们可以提出以下要求：
- en: A **load balancer** will distribute requests evenly (*round-robin*) across all
    instances of any given service (**proxy** included). It should be fault tolerant
    and not depend on any single node.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**负载均衡器**将均匀地分配请求（*轮询方式*）到任何给定服务的所有实例上（**代理**包括在内）。它应该是容错的，并且不依赖于任何单一节点。'
- en: A reverse proxy will be in charge of routing requests based on their base URLs.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个反向代理将负责根据请求的基础 URL 路由请求。
- en: The **go-demo** service will be able to communicate freely with the **go-demo-db**
    service and will be accessible only through the reverse proxy.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**go-demo**服务将能够与**go-demo-db**服务自由通信，并且只能通过反向代理访问。'
- en: The database will be isolated from any but the service it belongs to **go-demo**.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据库将与任何不属于它的服务隔离，仅与**go-demo**服务进行通信。
- en: 'A logical architecture of what we''re trying to accomplish can be presented
    with the diagram that follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所尝试实现的逻辑架构可以通过接下来的图表展示：
- en: '![](img/go-demo-logical.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/go-demo-logical.png)'
- en: 'Figure 3-1: A logical architecture of the go-demo service'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-1：go-demo服务的逻辑架构
- en: How can we accomplish those requirements?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实现这些要求？
- en: Let us solve each of the four requirements one by one. We'll start from the
    bottom and move towards the top.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一解决这四个要求。我们将从底部开始，逐步向上解决。
- en: The first problem to tackle is how to run a database isolated from any but the
    service it belongs to.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个要解决的问题是如何让数据库与它所属的服务以外的任何服务隔离运行。
- en: Running a database in isolation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在隔离中运行数据库
- en: 'We can isolate a database service by not exposing its ports. That can be accomplished
    easily with the `service create` command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过不暴露其端口来隔离数据库服务。这可以通过`service create`命令轻松实现：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can confirm that the ports are indeed not exposed by inspecting the service:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查服务来确认端口确实没有暴露：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, there is no mention of any port. Our `go-demo-db` service is
    fully isolated and inaccessible to anyone. However, that is too much isolation.
    We want the service to be isolated from anything but the service it belongs to
    `go-demo`. We can accomplish that through the usage of Docker Swarm networking.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，未提及任何端口。我们的`go-demo-db`服务是完全隔离的，任何人都无法访问。然而，这种隔离性过强了。我们希望服务仅与它所属的`go-demo`服务隔离。我们可以通过使用Docker
    Swarm网络来实现这一点。
- en: 'Let us remove the service we created and start over:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除我们创建的服务并从头开始：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This time, we should create a network and make sure that the `go-demo-db` service
    is attached to it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们应该创建一个网络，并确保`go-demo-db`服务已附加到该网络：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We created an overlay network called `go-demo` followed with the `go-demo-db
    service`. This time, we used the `--network` argument to attach the service to
    the network. From this moment on, all services attached to the `go-demo` network
    will be accessible to each other.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`go-demo`的覆盖网络，并随后创建了`go-demo-db`服务。这次，我们使用了`--network`参数将服务附加到该网络。从此时起，所有附加到`go-demo`网络的服务将彼此可访问。
- en: 'Let''s inspect the service and confirm that it is indeed attached to the network:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下服务，并确认它是否确实已附加到网络：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the `service inspect` command is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`service inspect`命令的输出如下：'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, this time, there is a `Networks` entry with the value set to
    the ID of the `go-demo` network we created earlier.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这一次有一个`Networks`条目，值设置为我们之前创建的`go-demo`网络的ID。
- en: 'Let us confirm that networking truly works. To prove it, we''ll create a global
    service called `util`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认网络是否真的起作用。为了证明这一点，我们将创建一个名为`util`的全局服务：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Just as `go-demo-db`, the `util` service also has the `go-demo` network attached.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与`go-demo-db`类似，`util`服务也附加了`go-demo`网络。
- en: A new argument is `--mode`. When set to global, the service will run on every
    node of the cluster. That is a very useful feature when we want to set up infrastructure
    services that should span the whole cluster.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新参数是`--mode`。当设置为global时，服务将会在集群中的每个节点上运行。当我们需要设置跨越整个集群的基础设施服务时，这是一个非常有用的特性。
- en: 'We can confirm that it is running everywhere by executing the `service ps`
    command:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行`service ps`命令来确认它是否在每个节点上运行：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows (IDs and ERROR PORTS columns are removed for brevity):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已删除IDs和ERROR PORTS列）：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, the `util` service is running on all three nodes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`util`服务在所有三台节点上都在运行。
- en: We are running the `alpine` image (a minuscule Linux distribution). We put it
    to sleep for a very long time. Otherwise, since no processes are running, the
    service would stop, Swarm would restart it, it would stop again, and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在运行`alpine`镜像（一种微型Linux发行版）。我们将其置于长时间休眠状态。否则，由于没有进程在运行，服务将停止，Swarm将重新启动它，它会再次停止，依此类推。
- en: The purpose of the `util` service will be to demonstrate some of the concepts
    we're exploring. We'll exec into it and confirm that the networking truly works.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`util`服务的目的将是演示我们正在探索的一些概念。我们将进入该服务并确认网络是否确实起作用。'
- en: 'To enter the `util` container, we need to find out the ID of the instance running
    on the `node-1` (the node our local Docker is pointing to):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入`util`容器，我们需要找出在`node-1`（本地Docker所指向的节点）上运行的实例的ID：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We listed all the processes `ps` in quiet mode so that only IDs are returned
    **`-q`**, and limited the result to the service name util:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以安静模式列出了所有进程`ps`，以便只返回ID**`-q`**，并将结果限制为服务名称util：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result is stored as the environment variable ID.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果被存储为环境变量ID。
- en: 'We''ll install a tool called *drill*. It is a tool designed to get all sorts
    of information out of a DNS and it will come in handy very soon:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装一个名为*drill*的工具。它是一个旨在从DNS获取各种信息的工具，很快就会派上用场：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*Alpine* Linux uses the package management called `apk`, so we told it to add
    drill.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*Alpine* Linux使用名为`apk`的包管理工具，因此我们告诉它添加drill。'
- en: Now we can see whether networking truly works. Since both `go-demo-db` and util
    services belong to the same network, they should be able to communicate with each
    other using DNS names. Whenever we attach a service to the network, a new virtual
    IP is created together with a DNS that matches the name of the services.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查网络是否真正起作用。由于`go-demo-db`和util服务都属于同一个网络，它们应该能够通过DNS名称相互通信。每当我们将一个服务连接到网络时，一个新的虚拟IP将被创建，并且DNS与服务名称匹配。
- en: 'Let''s try it out as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按以下方式尝试：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We entered into one of the instances of the `util` service and "drilled" the
    DNS `go-demo-db`. The output is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入了`util`服务的一个实例，并“钻取”了DNS `go-demo-db`。输出如下：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The response code is `NOERROR` and the `ANSWER` is `1` meaning that the DNS
    `go-demo-db` responded correctly. It is reachable.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 响应代码是`NOERROR`，并且`ANSWER`是`1`，这意味着DNS `go-demo-db`正确响应了。它是可以访问的。
- en: 'We can also observe that the `go-demo-db` DNS is associated with the IP `10.0.0.2`.
    Every service attached to a network gets its IP. Please note that I said service,
    not an instance. That’s a huge difference that we''ll explore later. For now,
    it is important to understand that all services that belong to the same network
    are accessible through service names:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以观察到`go-demo-db`的DNS与IP `10.0.0.2`相关联。每个附加到网络的服务都会获得一个IP。请注意，我说的是服务，而不是实例。这是一个巨大的区别，我们稍后会深入探讨。现在，重要的是要理解，所有属于同一网络的服务都可以通过服务名称访问：
- en: '![](img/swarm-nodes-go-demo-db.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes-go-demo-db.png)'
- en: 'Figure 3-2: go-demo-db service attached to the go-demo network'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-2：附加到go-demo网络的go-demo-db服务
- en: Let's move up through the requirements.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向上推进，完成需求。
- en: Running a service through a reverse proxy
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过反向代理运行服务
- en: We want the `go-demo` service to be able to communicate freely with the `go-demo-db`
    service and to be accessible only through the reverse proxy. We already know how
    to accomplish the first part. All we have to do is make sure that both services
    belong to the same network `go-demo`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望`go-demo`服务能够与`go-demo-db`服务自由通信，并且只能通过反向代理访问。我们已经知道如何完成第一部分。我们要做的就是确保这两个服务都属于同一个`go-demo`网络。
- en: How can we accomplish the integration with a reverse proxy?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何完成与反向代理的集成？
- en: 'We can start by creating a new network and attach it to all services that should
    be accessible through a reverse proxy:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从创建一个新网络开始，并将其附加到所有应通过反向代理访问的服务：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s list the currently running `overlay` networks:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出当前正在运行的`overlay`网络：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have the `go-demo` and `proxy` networks we created earlier. The third one
    is called ingress. It is set up by default and has a special purpose that we'll
    explore later.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了之前创建的`go-demo`和`proxy`网络。第三个网络称为ingress。它是默认设置的，具有特殊的用途，我们稍后会深入探讨。
- en: Now we are ready to run the `go-demo` service. We want it to be able to communicate
    with the `go-demo-db` service so it must be attached to the `go-demo` network.
    We also want it to be accessible to a `proxy` (we'll create it soon) so we'll
    attach it to the `proxy` network as well.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备运行`go-demo`服务。我们希望它能够与`go-demo-db`服务通信，因此它必须附加到`go-demo`网络。我们还希望它可以通过`proxy`访问（我们很快会创建它），因此我们也会将它附加到`proxy`网络。
- en: 'The command that creates the `go-demo` service is as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`go-demo`服务的命令如下：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It is very similar to the command we executed in the previous chapter with
    the addition of the `--network proxy` argument:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在上一章执行的命令非常相似，只是在其中添加了`--network proxy`参数：
- en: '![](img/swarm-nodes-proxy-sdn.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes-proxy-sdn.png)'
- en: 'Figure 3-3: Docker Swarm cluster with three nodes, two networks and a few containers'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-3：包含三个节点、两个网络和若干容器的Docker Swarm集群
- en: Now both services are running somewhere inside the cluster and can communicate
    with each other through the `go-demo` network. Let's bring the proxy into the
    mix. We'll use the *Docker Flow Proxy* ([https:/](https://github.com/vfarcic/docker-flow-proxy)[/github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    project that is a combination of HAProxy ([http://www.haproxy.org/](http://www.haproxy.org/))
    and a few additional features that make it more dynamic. The principles we'll
    explore are the same no matter which one will be your choice.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在两个服务都在集群中的某个地方运行，并且可以通过`go-demo`网络互相通信。让我们将代理添加进来。我们将使用*Docker Flow Proxy*（[https:/](https://github.com/vfarcic/docker-flow-proxy)[/github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy)）项目，它结合了HAProxy（[http://www.haproxy.org/](http://www.haproxy.org/)）和一些额外的功能，使其更加动态。无论你选择哪个，本文所探讨的原则都是相同的。
- en: Please note that, at this moment, none of the services are accessible to anyone
    except those attached to the same network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前，除非与同一网络连接的用户，否则其他任何人都无法访问这些服务。
- en: Creating a reverse proxy service in charge of routing requests depending on
    their base URLs
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个反向代理服务，负责根据其基础URL路由请求。
- en: We can implement a reverse proxy in a couple of ways. One would be to create
    a new image based on HAProxy ([https://hub.docker.com/_/haproxy/](https://hub.docker.com/_/haproxy/))
    and include configuration files inside it. That approach would be a good one if
    the number of different services is relatively static. Otherwise, we'd need to
    create a new image with a new configuration every time there is a new service
    (not a new release).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式实现反向代理。一种方法是基于HAProxy（[https://hub.docker.com/_/haproxy/](https://hub.docker.com/_/haproxy/)）创建一个新镜像，并将配置文件包含其中。如果不同服务的数量相对静态，这种方法是可行的。否则，每当有新的服务（而不是新的版本）时，我们就需要创建一个新镜像，并带有新的配置。
- en: The second approach would be to expose a volume. That way, when needed, we could
    modify the configuration file instead building a whole new image. However, that
    has downsides as well. When Deploying to a cluster, we should avoid using volumes
    whenever they're not necessary. As you'll see soon, a proxy is one of those that
    do not require a volume. As a side note, `--volume` has been replaced with the
    `docker service` argument `--mount`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是暴露一个卷。这样，在需要时，我们可以修改配置文件，而不是构建一个全新的镜像。然而，这也有缺点。当部署到集群时，我们应该避免在不必要时使用卷。正如你很快会看到的，代理是其中之一，它不需要使用卷。顺便提一下，`--volume`已经被`docker
    service`参数`--mount`取代。
- en: The third option is to use one of the proxies designed to work with Docker Swarm.
    In this case, we'll use the container `vfarcic/docker-flow-proxy` ([https://hub.docker.com/r/vfarcic/docker-flow-proxy/](https://hub.docker.com/r/vfarcic/docker-flow-proxy/))
    It is based on HAProxy with additional features that allow us to reconfigure it
    by sending HTTP requests.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种选择是使用专为Docker Swarm设计的代理之一。在这种情况下，我们将使用容器`vfarcic/docker-flow-proxy`（[https://hub.docker.com/r/vfarcic/docker-flow-proxy/](https://hub.docker.com/r/vfarcic/docker-flow-proxy/)）。它基于HAProxy，并增加了额外的功能，使我们能够通过发送HTTP请求来重新配置它。
- en: Let's give it a spin.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看。
- en: 'The command that creates the `proxy` service is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`proxy`服务的命令如下：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We opened ports `80` and `443` that will serve Internet traffic (HTTP and HTTPS).
    The third port is 8080\. We'll use it to send configuration requests to the proxy.
    Further on, we specified that it should belong to the proxy network. That way,
    since go-demo is also attached to the same network, the proxy can access it through
    the proxy-SDN.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开放了`80`和`443`端口，用于互联网流量（HTTP和HTTPS）。第三个端口是8080。我们将使用它向代理发送配置请求。进一步地，我们指定它应该属于代理网络。这样，由于go-demo也连接到同一网络，代理可以通过proxy-SDN访问它。
- en: Through the **proxy** we just ran, we can observe one of the cool features of
    the network routing mesh. It does not matter which server the **proxy** is running
    in. We can send a request to any of the nodes and Docker networking will make
    sure that it is redirected to one of the proxies. We'll see that in action very
    soon.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们刚刚运行的**代理**，我们可以观察到网络路由网格的一个酷炫特性。无论**代理**在哪台服务器上运行，都没关系。我们可以向任何节点发送请求，Docker网络会确保将其重定向到其中一个代理。我们很快就会看到这一点。
- en: The last argument is the environment variable `MODE` that tells the proxy that
    containers will be deployed to a Swarm cluster. Please consult the project README
    ([https://github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    for other combinations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的参数是环境变量`MODE`，它告诉`proxy`容器将部署到Swarm集群中。有关其他组合，请参阅项目的README（[https://github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy)）。
- en: '![](img/swarm-nodes-proxy.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes-proxy.png)'
- en: 'Figure 3-4: Docker Swarm cluster with the proxy service'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-4：带有代理服务的Docker Swarm集群
- en: Please note that the **proxy**, even though it is running inside one of the
    nodes, is placed outside to illustrate the logical separation better.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**proxy**虽然运行在某个节点内部，但它被放置在外部以更好地展示逻辑分离。
- en: Before we move on, let's confirm that the `proxy` is running.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们确认`proxy`是否正在运行。
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can proceed if the `CURRENT STATE` is `Running`. Otherwise, please wait until
    the service is up and running.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`CURRENT STATE`是`Running`，我们可以继续。否则，请等到服务启动并运行。
- en: 'Now that the proxy is Deployed, we should let it know about the existence of
    the `go-demo` service:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`proxy`已部署，我们应该让它知道`go-demo`服务的存在：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The request was sent to reconfigure the `proxy` specifying the service name
    `go-demo`, base URL path of the API `/demo`, and the internal port of the service
    `8080`. From now on, all the requests to the `proxy` with the path that starts
    with `/demo` will be redirected to the `go-demo` service. This request is one
    of the additional features Docker Flow Proxy provides on top of HAProxy.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请求被发送以重新配置`proxy`，指定了服务名称`go-demo`，API的基本URL路径`/demo`，以及服务的内部端口`8080`。从现在起，所有路径以`/demo`开头的请求将被重定向到`go-demo`服务。这个请求是Docker
    Flow Proxy在HAProxy基础上提供的额外功能之一。
- en: Please note that we sent the request to `node-1`. The proxy could be running
    inside any of the nodes and, yet, the request was successful. That is where Docker's
    Routing Mesh plays a critical role. We'll explore it in more detail later. For
    now, the important thing to note is that we can send a request to any of the nodes,
    and it will be redirected to the service that listens to the same port (in this
    case `8080`).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将请求发送到了`node-1`。即使`proxy`可能在任何节点内部运行，请求仍然成功。这正是Docker的路由网格发挥重要作用的地方。我们稍后会更详细地探讨它。现在，重要的一点是，我们可以将请求发送到任何节点，它都会被重定向到监听同一端口的服务（在这个例子中是`8080`）。
- en: 'The output of the request is as follows (formatted for readability):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请求的输出如下所示（已格式化以便于阅读）：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: I won't go into details but note that the `Status` is `OK` indicating that the
    `proxy` was reconfigured correctly.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我不打算深入细节，但请注意，`Status`是`OK`，这表示`proxy`已正确重新配置。
- en: 'We can test that the `proxy` indeed works as expected by sending an HTTP request:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过发送HTTP请求来验证`proxy`是否如预期工作：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output of the `curl` command is as follows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl`命令的输出如下：'
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `proxy` works! It responded with the HTTP status `200` and returned the
    API response `hello, world!`. As before, the request was not, necessarily, sent
    to the node that hosts the service but to the routing mesh that forwarded it to
    the `proxy`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`proxy`工作正常！它响应了HTTP状态`200`，并返回了API响应`hello, world!`。和之前一样，请求不一定是发送到托管服务的节点，而是发送到了路由网格，再由它转发给`proxy`。'
- en: 'As an example, let''s send the same request but this time, to `node-3`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将发送相同的请求，不过这次是发送到`node-3`：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The result is still the same.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 结果仍然相同。
- en: Let's explore the configuration generated by the `proxy`. It will give us more
    insights into the Docker Swarm Networking inner workings. As another benefit,
    if you choose to roll your own `proxy` solution, it might be useful to understand
    how to configure the `proxy` and leverage new Docker networking features.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看由`proxy`生成的配置。这将为我们提供有关Docker Swarm网络内部工作的更多见解。另一个好处是，如果你选择自己构建`proxy`解决方案，了解如何配置`proxy`并利用Docker新的网络功能可能会很有用。
- en: We'll start by examining the configuration *Docker Flow Proxy* ([https://github.com/vfa](https://github.com/vfarcic/docker-flow-proxy)[rcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    created for us. We can do that by entering the running container to take a sneak
    peek at the file `/cfg/haproxy.cfg`. The problem is that finding a container run
    by Docker Swarm is a bit tricky. If we deployed it with Docker Compose, the container
    name would be predictable. It would use the format `<PROJECT>_<SERVICE>_<INDEX>`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先检查为我们创建的*Docker Flow Proxy*配置（[https://github.com/vfa](https://github.com/vfarcic/docker-flow-proxy)[rcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy)）。我们可以通过进入运行中的容器，快速查看`/cfg/haproxy.cfg`文件来实现。问题是，找到由Docker
    Swarm运行的容器有些棘手。如果我们使用Docker Compose部署，容器名称是可以预测的。它会采用`<PROJECT>_<SERVICE>_<INDEX>`格式。
- en: The `docker service command` runs containers with hashed names. The `docker-flow-proxy`
    created on my laptop has the name `proxy.1.e07jvhdb9e6s76mr9ol41u4sn`. Therefore,
    to get inside a running container deployed with Docker Swarm, we need to use a
    filter with, for example, an image name.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker service command`运行具有哈希名称的容器。我在笔记本上创建的`docker-flow-proxy`容器名为`proxy.1.e07jvhdb9e6s76mr9ol41u4sn`。因此，要进入由Docker
    Swarm部署的运行中容器，我们需要使用筛选器，例如使用镜像名称。'
- en: 'First, we need to find out on which node the `proxy` is running execute the
    following command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要找出`proxy`运行在哪个节点，执行以下命令：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We listed the `proxy` service processes `docker service ps proxy`, removed the
    header `tail -n +2`, and output the node that resides inside the fourth column
    `awk '{print $4}'`. The output is stored as the environment variable `NODE`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了`proxy`服务进程`docker service ps proxy`，去掉了标题`tail -n +2`，并输出了第四列中的节点`awk
    '{print $4}'`。输出结果被存储为环境变量`NODE`。
- en: 'Now we can point our local Docker Engine to the node where the `proxy` resides:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将本地Docker引擎指向`proxy`所在的节点：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, the only thing left is to find the ID of the `proxy` container. We
    can do that with the following command:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，只剩下找到`proxy`容器的ID。我们可以通过以下命令来实现：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have the container ID stored inside the variable, we can execute
    the command that will retrieve the HAProxy configuration:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将容器ID存储在变量中，我们可以执行命令来检索HAProxy配置：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The important part of the configuration is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 配置的重要部分如下：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first part `frontend` should be familiar to those who have used HAProxy.
    It accepts requests on ports `80` HTTP and `443` HTTPS. If the path starts with
    `/demo`, it will be redirected to the `backend go-demo-be`. Inside it, requests
    are sent to the address `go-demo` on the port `8080`. The address is the same
    as the name of the service we deployed. Since `go-demo` belongs to the same network
    as the `proxy`, Docker will make sure that the request is redirected to the destination
    container. Neat, isn't it? There is no need, anymore, to specify IPs and external
    ports.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分`frontend`应该对使用过HAProxy的人来说很熟悉。它接受端口`80`的HTTP请求和端口`443`的HTTPS请求。如果路径以`/demo`开头，系统会将请求重定向到`backend
    go-demo-be`。在里面，请求会被发送到端口`8080`上的`go-demo`地址。这个地址和我们部署的服务名称相同。由于`go-demo`与`proxy`属于同一个网络，Docker会确保请求被重定向到目标容器。很整洁，对吧？不再需要指定IP和外部端口了。
- en: The next question is how to do load balancing. How should we specify that the
    `proxy` should, for example, perform round-robin across all instances? Should
    we use a `proxy` for such a task?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的问题是如何进行负载均衡。我们应该如何指定`proxy`，例如，执行轮询遍历所有实例？我们应该使用`proxy`来完成这样的任务吗？
- en: Load balancing requests across all instances of a service
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在所有实例间进行负载均衡请求
- en: 'Before we explore load balancing, we need to have something to balance. We
    need multiple instances of a service. Since we already explored scaling in the
    previous chapter, the command should not come as a surprise:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索负载均衡之前，需要有些东西来进行负载均衡。我们需要多个实例的服务。由于我们在上一章已经探讨过扩展，所以这个命令应该不会让你感到惊讶：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Within a few moments, five instances of the `go-demo` service will be running:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，五个`go-demo`服务的实例将会启动：
- en: '![](img/swarm-nodes-proxy-scaled.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-nodes-proxy-scaled.png)'
- en: 'Figure 3-5: Docker Swarm cluster with the go-demo service scaled'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-5：Docker Swarm集群中的go-demo服务扩展
- en: What should we do to make the **proxy** load balance requests across all instances?
    The answer is nothing. No action is necessary on our part. Actually, the question
    is wrong. The **proxy** will not load balance requests at all. Docker Swarm networking
    will. So, let us reformulate the question. What should we do to make the *Docker
    Swarm network* load balance requests across all instances? Again, the answer is
    nothing. No action is necessary on our part.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要使**proxy**在所有实例之间负载平衡请求，我们该做什么？答案是什么也不用做。我们不需要采取任何行动。实际上，这个问题本身是错的。**proxy**根本不会负载平衡请求。Docker
    Swarm网络会。因此，让我们重新表述问题。要使*Docker Swarm网络*在所有实例之间负载平衡请求，我们该做什么？同样地，答案是什么也不用做。我们不需要采取任何行动。
- en: To understand load balancing, we might want to go back in time and discuss load
    balancing before Docker networking came into being.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解负载平衡，我们可能需要回到以前讨论Docker网络出现之前的负载平衡。
- en: 'Normally, if we didn''t leverage Docker Swarm features, we would have something
    similar to the following **proxy** configuration mock-up:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果我们没有利用Docker Swarm的功能，我们可能会有类似以下的**proxy**配置模型：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Every time a new instance is added, we would need to add it to the configuration
    as well. If an instance is removed, we would need to remove it from the configuration.
    If an instance failed… Well, you get the point. We would need to monitor the state
    of the cluster and update the `proxy` configuration whenever a change occurs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 每次添加新实例时，我们需要将其添加到配置中。如果删除实例，我们需要从配置中删除它。如果实例失败了……嗯，你明白了。我们需要监控集群的状态，并在发生更改时更新`proxy`配置。
- en: If you read *The DevOps 2.0 Toolkit*, you probably remember that I advised a
    combination of *Registrator* ([https://github.com/gliderlabs/registrator](https://github.com/gliderlabs/registrator)),
    *Consul* ([https://www](https://www.consul.io/)[.consul.io/](https://www.consul.io/)),
    and *Consul Template* ([https://github.com/hashicorp/consul-template](https://github.com/hashicorp/consul-template)).
    Registrator would monitor Docker events and update Consul whenever a container
    is created or destroyed. With the information stored in Consul, we would use Consul
    Template to update nginx or HAProxy configuration. There is no need for such a
    combination anymore. While those tools still provide value, for this particular
    purpose, there is no need for them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读过《DevOps 2.0工具包》，你可能还记得我建议使用*Registrator* ([https://github.com/gliderlabs/registrator](https://github.com/gliderlabs/registrator))、*Consul*
    ([https://www](https://www.consul.io/)[.consul.io/](https://www.consul.io/))和*Consul
    Template* ([https://github.com/hashicorp/consul-template](https://github.com/hashicorp/consul-template))的组合。Registrator会监控Docker事件，并在创建或销毁容器时更新Consul。通过Consul存储的信息，我们会使用Consul
    Template更新nginx或HAProxy配置。不再需要这样的组合了。虽然这些工具仍然有价值，但对于这个特定目的，不再需要它们。
- en: We are not going to update the `proxy` every time there is a change inside the
    cluster, for example, a scaling event. Instead, we are going to update the proxy
    every time a new service is created. Please note that service updates (Deployment
    of new releases) do not count as service creation. We create a service once and
    update it with each new release (among other reasons). So, only a new service
    requires a change in the `proxy` configuration.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 每当集群内部发生变化时（例如，扩展事件），我们不会更新`proxy`；而是每次创建新服务时才会更新`proxy`。请注意，服务更新（发布新版本）不算作服务创建。我们创建一次服务，然后每次发布新版本时更新它（还有其他原因）。因此，只有新服务需要更改`proxy`配置。
- en: 'The reason behind that reasoning is in the fact that load balancing is now
    part of Docker Swarm networking. Let''s do another round of drilling from the
    `util` service:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 背后的原因在于负载平衡现在是Docker Swarm网络的一部分。让我们从`util`服务再做一轮解析：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output of the previous command is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令的输出如下所示：
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The IP `10.0.0.8` represents the `go-demo` service, not an individual instance.
    When we sent a drill request, Swarm networking performed **load balancing** (**LB**)
    across all of the instances of the service. To be more precise, it performed *round-robin*
    LB.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: IP `10.0.0.8`代表`go-demo`服务，而不是单独的实例。当我们发送解析请求时，Swarm网络跨服务的所有实例执行**负载平衡**（**LB**）。更准确地说，它执行*轮询*
    LB。
- en: Besides creating a virtual IP for each service, each instance gets its own IP
    as well. In most cases, there is no need discovering those IPs (or any Docker
    network endpoint IP) since all we need is a service name, which gets translated
    to an IP and load balanced in the background.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为每个服务创建虚拟 IP 外，每个实例也会获得自己的 IP。大多数情况下，不需要发现这些 IP（或任何 Docker 网络端点 IP），因为我们只需要一个服务名称，该名称会被转换为一个
    IP，并在后台进行负载均衡。
- en: What now?
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: That concludes the exploration of basic concepts of the Docker Swarm networking.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对 Docker Swarm 网络基础概念的探索。
- en: Is this everything there is to know to run a Swarm cluster successfully? In
    this chapter, we went deeper into Swarm features, but we are not yet done. There
    are quite a few questions waiting to be answered. In the next chapter, we'll explore
    *service discovery* and the role it has in the Swarm Mode.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是成功运行 Swarm 集群所需了解的全部内容吗？在本章中，我们深入探讨了 Swarm 的特性，但我们还没有结束。还有很多问题等待解答。在下一章中，我们将探索*服务发现*及其在
    Swarm 模式中的作用。
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候休息一下，准备进入下一个章节了。和之前一样，我们将销毁创建的机器，重新开始：
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
