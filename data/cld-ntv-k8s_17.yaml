- en: '*Chapter 13*: Extending Kubernetes with CRDs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：通过CRD扩展Kubernetes'
- en: This chapter explains the many possibilities for extending the functionality
    of Kubernetes. It begins with a discussion of the `kubectl` commands such as `get`,
    `create`, `describe`, and `apply`. It is followed by a discussion of the Operator
    pattern, an extension of the CRD. It then details some of the hooks that cloud
    providers attach to their Kubernetes implementations, and ends with a brief introduction
    to the greater cloud-native ecosystem. Using the concepts learned in this chapter,
    you will be able to architect and develop extensions to your Kubernetes cluster,
    unlocking advanced usage patterns.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了扩展Kubernetes功能的多种可能性。首先讨论了`kubectl`命令，如`get`、`create`、`describe`和`apply`。接着讨论了Operator模式，它是CRD的一种扩展。然后详细介绍了一些云服务提供商在其Kubernetes实现中附加的钩子，最后简要介绍了更广泛的云原生生态系统。通过本章学到的概念，你将能够设计和开发扩展你的Kubernetes集群的功能，解锁高级使用模式。
- en: The case study in this chapter will include creating two simple CRDs to support
    an example application. We'll begin with CRDs, which will give you a good base
    understanding of how extensions can build on the Kubernetes API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的案例研究将包括创建两个简单的CRD来支持示例应用程序。我们将从CRD开始，这将为你提供一个良好的基础理解，帮助你理解扩展如何构建在Kubernetes
    API之上。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: How to extend Kubernetes with **Custom Resource Definitions** (**CRDs**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过**自定义资源定义**（**CRDs**）扩展Kubernetes
- en: Self-managing functionality with Kubernetes operators
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes操作员进行自我管理功能
- en: Using cloud-specific Kubernetes extensions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定云的Kubernetes扩展
- en: Integrating with the ecosystem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与生态系统集成
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，你需要一台支持`kubectl`命令行工具并且有一个正常运行的Kubernetes集群的计算机。请参见[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解几种快速启动Kubernetes的方法，并获取如何安装`kubectl`工具的说明。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在本书的GitHub仓库中找到，地址是[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13)。
- en: How to extend Kubernetes with custom resource definitions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何通过自定义资源定义扩展Kubernetes
- en: Let's start with the basics. What is a CRD? We know that Kubernetes has an API
    model where we can perform operations against resources. Some examples of Kubernetes
    resources (which you should be well acquainted with by now) are Pods, PersistentVolumes,
    Secrets, and others.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始。什么是CRD？我们知道Kubernetes有一个API模型，在该模型中，我们可以对资源进行操作。一些Kubernetes资源的例子（你现在应该已经很熟悉了）有Pods、PersistentVolumes、Secrets等。
- en: Now, what if we want to implement some custom functionality in our cluster,
    write our own controllers, and store the state of our controllers somewhere? We
    could, of course, store the state of our custom functionality in a SQL or NoSQL
    database running on Kubernetes or elsewhere (which is actually one of the strategies
    for extending Kubernetes) – but what if our custom functionality acts more as
    an extension of Kubernetes functionality, instead of a completely separate application?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们希望在集群中实现一些自定义功能，编写自己的控制器，并将控制器的状态存储在某个地方该怎么办？我们当然可以将自定义功能的状态存储在Kubernetes或其他地方运行的SQL或NoSQL数据库中（这实际上是扩展Kubernetes的一种策略）——但是如果我们的自定义功能更多地作为Kubernetes功能的扩展，而不是一个完全独立的应用程序呢？
- en: 'In cases like this, we have two options:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有两个选择：
- en: Custom resource definitions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源定义
- en: API aggregation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API聚合
- en: API aggregation allows advanced users to build their own resource APIs outside
    of the Kubernetes API server and use their own storage – and then aggregate those
    resources at the API layer so they can be queried using the Kubernetes API. This
    is obviously highly extensible and is essentially just using the Kubernetes API
    as a proxy to your own custom functionality, which may or may not actually integrate
    with Kubernetes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: API聚合允许高级用户在Kubernetes API服务器外构建自己的资源API，并使用自己的存储——然后在API层聚合这些资源，以便通过Kubernetes
    API进行查询。显然，这种方法具有高度的可扩展性，本质上是将Kubernetes API作为代理来访问你自己的自定义功能，这些功能可能与Kubernetes集成，也可能没有。
- en: The other option is CRDs, where we can use the Kubernetes API and underlying
    data store (`etcd`) instead of building our own. We can use the `kubectl` and
    `kube api` methods that we know to interact with our own custom functionality.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是CRD，我们可以使用Kubernetes API和底层数据存储（`etcd`），而不是构建自己的数据存储。我们可以使用我们熟悉的`kubectl`和`kube
    api`方法来与我们自己的自定义功能进行交互。
- en: In this book, we will not discuss API aggregation. While definitely more flexible
    than CRDs, this is an advanced topic that deserves a thorough understanding of
    the Kubernetes API and a thorough perusal of the Kubernetes documentation to do
    it right. You can learn more about API aggregation in the Kubernetes documentation
    at [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将不会讨论API聚合。尽管它比CRD更加灵活，但这是一个高级话题，需深入理解Kubernetes API并仔细阅读Kubernetes文档才能正确实现。你可以在[Kubernetes文档](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)中了解更多关于API聚合的信息。
- en: So, now that we know that we are using the Kubernetes control plane as our own
    stateful store for our new custom functionality, we need a schema. Similar to
    how the Pod resource spec in Kubernetes expects certain fields and configurations,
    we can tell Kubernetes what we expect for our new custom resources. Let's go through
    the spec for a CRD now.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道我们正在使用Kubernetes控制平面作为我们新自定义功能的有状态存储，我们需要一个架构。类似于Kubernetes中的Pod资源规范期望某些字段和配置一样，我们可以告诉Kubernetes我们对新自定义资源的期望。现在让我们通过CRD的规范来了解一下。
- en: Writing a custom resource definition
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写自定义资源定义
- en: For CRDs, Kubernetes uses the OpenAPI V3 specification. For more information
    on OpenAPI V3, you can check the official documentation at [https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md),
    but we'll soon see how exactly this translates into Kubernetes CRD definitions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CRD，Kubernetes使用OpenAPI V3规范。有关OpenAPI V3的更多信息，你可以查看[https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md)上的官方文档，但我们很快就会看到它是如何转化为Kubernetes
    CRD定义的。
- en: Let's take a look at an example CRD spec. Now let's be clear, this is not how
    YAMLs of any specific record of this CRD would look. Instead, this is simply where
    we define the requirements for the CRD inside of Kubernetes. Once created, Kubernetes
    will accept resources matching the spec and we can start making our own records
    of this type.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个CRD规范示例。现在需要明确的是，这并不是该CRD的任何特定记录的YAML格式。相反，这是我们在Kubernetes内部定义CRD要求的地方。创建之后，Kubernetes将接受符合规范的资源，我们可以开始创建自己的此类记录。
- en: 'Here''s an example YAML for a CRD spec, which we are calling `delayedjob`.
    This highly simplistic CRD is intended to start a container image job on a delay,
    which prevents users from having to script in a delayed start for their container.
    This CRD is quite brittle, and we don''t recommend anyone actually use it, but
    it does well to highlight the process of building a CRD. Let''s start with a full
    CRD spec YAML, then break it down:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于CRD规范的YAML示例，我们称之为`delayedjob`。这个极为简化的CRD旨在延迟启动一个容器镜像任务，这样用户就无需为容器编写延迟启动脚本。这个CRD非常脆弱，我们不建议任何人实际使用它，但它很适合展示构建CRD的过程。我们先看一下完整的CRD规范YAML，然后再逐步解析：
- en: Custom-resource-definition-1.yaml
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-resource-definition-1.yaml
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's review the parts of this file. At first glance, it looks like your typical
    Kubernetes YAML spec – and that's because it is! In the `apiVersion` field, we
    have `apiextensions.k8s.io/v1`, which is the standard since Kubernetes `1.16`
    (before then it was `apiextensions.k8s.io/v1beta1`). Our `kind` will always be
    `CustomResourceDefinition`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这个文件的部分内容。乍一看，它看起来像是典型的Kubernetes YAML规范——因为它确实是！在`apiVersion`字段中，我们有`apiextensions.k8s.io/v1`，这是Kubernetes
    `1.16`版本以来的标准（之前是`apiextensions.k8s.io/v1beta1`）。我们的`kind`将始终是`CustomResourceDefinition`。
- en: The `metadata` field is when things start to get specific to our resource. We
    need to structure the `name` metadata field as the `plural` form of our resource,
    then a period, then its group. Let's take a quick diversion from our YAML file
    to discuss how groups work in the Kubernetes API.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`metadata`字段是当事物开始变得具体到我们的资源时。我们需要将`name`元数据字段结构化为我们资源的复数形式，然后是一个点，接着是它的组名。让我们从YAML文件中稍作偏离，来讨论Kubernetes
    API中的组是如何工作的。'
- en: Understanding Kubernetes API groups
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解Kubernetes API组
- en: Groups are a way that Kubernetes segments resources in its API. Each group corresponds
    to a different subpath of the Kubernetes API server.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 组是Kubernetes在其API中对资源进行分段的一种方式。每个组对应Kubernetes API服务器的一个不同子路径。
- en: 'By default, there is a legacy group called the core group – which corresponds
    to resources accessed on the `/api/v1` endpoint in the Kubernetes REST API. By
    extension, these legacy group resources have `apiVersion: v1` in their YAML specs.
    An example of one of the resources in the core group is the Pod.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '默认情况下，有一个叫做核心组的遗留组——它对应于在Kubernetes REST API的`/api/v1`端点上访问的资源。由此，这些遗留组资源在它们的YAML规范中具有`apiVersion:
    v1`。核心组中的一个资源示例是Pod。'
- en: Next, there is the set of named groups – which correspond to resources that
    can be accessed on `REST` URLs formed as `/apis/<GROUP NAME>/<VERSION>`. These
    named groups form the bulk of Kubernetes resources. However, the oldest and most
    basic resources, such as the Pod, Service, Secret, and Volume, are in the core
    group. An example of a resource that is in a named group is the `StorageClass`
    resource, which is in the `storage.k8s.io` group.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是命名组的集合——它们对应于可以通过`REST` URL访问的资源，URL格式为`/apis/<GROUP NAME>/<VERSION>`。这些命名组构成了Kubernetes资源的大部分。然而，最古老和最基本的资源，如Pod、Service、Secret和Volume，属于核心组。一个属于命名组的资源示例是`StorageClass`资源，它位于`storage.k8s.io`组中。
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: To see which resource is in which group, you can check the official Kubernetes
    API docs for whatever version of Kubernetes you are using. For example, the version
    `1.18` docs would be at [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看哪些资源属于哪个组，你可以查阅官方的Kubernetes API文档，了解你使用的Kubernetes版本。例如，版本`1.18`的文档可以在[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18)找到。
- en: CRDs can specify their own named group, which means that the specific CRD will
    be available on a `REST` endpoint that the Kubernetes API server can listen on.
    With that in mind, let's get back to our YAML file, so we can talk about the main
    portion of the CRD – the versions spec.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CRD可以指定它们自己的命名组，这意味着特定的CRD将在Kubernetes API服务器可以监听的`REST`端点上可用。记住这一点后，让我们回到YAML文件，继续讨论CRD的主要部分——版本规范。
- en: Understanding custom resource definition versions
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自定义资源定义版本
- en: As you can see, we have chosen the group `delayedresources.mydomain.com`. This
    group would theoretically hold any other CRDs of the delayed kind – for instance,
    `DelayedDaemonSet` or `DelayedDeployment`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们选择了`delayedresources.mydomain.com`组。这个组理论上会包含所有其他延迟类型的CRD——例如，`DelayedDaemonSet`或`DelayedDeployment`。
- en: Next, we have the main portion of our CRD. Under `versions`, we can define one
    or more CRD versions (in the `name` field), along with the API specification for
    that version of the CRD. Then, when you create an instance of your CRD, you can
    define which version you will be using for the version parameter in the `apiVersion`
    key of your YAML – for instance, `apps/v1`, or in this case, `delayedresources.mydomain.com/v1`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有CRD的主要部分。在`versions`字段下，我们可以定义一个或多个CRD版本（在`name`字段中），以及该版本的API规范。然后，当你创建CRD实例时，可以在YAML的`apiVersion`键的版本参数中定义将使用的版本——例如，`apps/v1`，或者在这种情况下是`delayedresources.mydomain.com/v1`。
- en: Each version item also has a `served` attribute, which is essentially a way
    to define whether the given version is enabled or disabled. If `served` is `false`,
    the version will not be created by the Kubernetes API, and the API requests (or
    `kubectl` commands) for that version will fail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个版本项也有一个`served`属性，基本上是用来定义该版本是否启用。如果`served`为`false`，则 Kubernetes API 不会创建该版本，并且对于该版本的
    API 请求（或`kubectl`命令）将失败。
- en: 'In addition, it is possible to define a `deprecated` key on a specific version,
    which will cause Kubernetes to return a warning message when requests are made
    to the API using the deprecated version. This is how a CRD. `yaml` file with a
    deprecated version looks – we have removed some of the spec to keep the YAML short:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以在特定版本上定义一个`deprecated`键，这会导致当使用已弃用版本对 API 发出请求时，Kubernetes 返回警告信息。这就是带有弃用版本的
    CRD `yaml` 文件的样子——我们去掉了一些 spec 以保持 YAML 简洁：
- en: Custom-resource-definition-2.yaml
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-resource-definition-2.yaml
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we have marked `v1` as deprecated, and also include a deprecation
    warning for Kubernetes to send as a response. If we do not include a deprecation
    warning, a default message will be used.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已将`v1`标记为已弃用，并且还包括了 Kubernetes 作为响应发送的弃用警告。如果没有包括弃用警告，将使用默认消息。
- en: Moving further down, we have the `storage` key, which interacts with the `served`
    key. The reason this is necessary is that while Kubernetes supports multiple active
    (aka `served`) versions of a resource at the same time, only one of those versions
    can be stored in the control plane. However, the `served` attribute means that
    multiple versions of a resource can be served by the API. So how does that even
    work?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`storage`键，它与`served`键交互。之所以需要这个是因为虽然 Kubernetes 支持同时运行多个活动（即`served`）版本的资源，但只有一个版本可以存储在控制平面中。然而，`served`属性意味着
    API 可以提供资源的多个版本。那么它是如何工作的呢？
- en: The answer is that Kubernetes will convert the CRD object from whatever the
    stored version is to the version you ask for (or vice versa, when creating a resource).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是 Kubernetes 会将 CRD 对象从存储版本转换为你请求的版本（或者在创建资源时，反向转换）。
- en: How is this conversion handled? Let's skip past the rest of the version attributes
    to the `conversion` key to see how.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换是如何处理的呢？我们跳过其余的版本属性，看看`conversion`键是如何处理的。
- en: The `conversion` key lets you specify a strategy for how Kubernetes will convert
    CRD objects between whatever your served version is and whatever the stored version
    is. If the two versions are the same – for instance, if you ask for a `v1` resource
    and the stored version is `v1`, then no conversion will happen.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`conversion`键让你指定一个策略，用于 Kubernetes 如何在你请求的版本和存储的版本之间转换 CRD 对象。如果两个版本相同——例如，当你请求一个`v1`资源并且存储版本也是`v1`时，则不会发生任何转换。'
- en: The default value here as of Kubernetes 1.13 is `none`. With the `none` setting,
    Kubernetes will not do any conversion between fields. It will simply include the
    fields that are supposed to be present on the `served` (or stored, if creating
    a resource) version.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 Kubernetes 1.13，默认值是`none`。使用`none`设置时，Kubernetes 不会在字段之间进行任何转换。它只会包括在`served`（或创建资源时存储的）版本中应该出现的字段。
- en: 'The other possible conversion strategy is `Webhook`, which allows you to define
    a custom webhook that will take in one version and do the proper conversion to
    your intended version. Here is an example of our CRD with a `Webhook` conversion
    strategy – we''ve cut out some of the version schema for conciseness:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的转换策略是`Webhook`，它允许你定义一个自定义 webhook，该 webhook 将接受一个版本并将其转换为你所需的目标版本。以下是我们使用`Webhook`转换策略的
    CRD 示例——我们为了简洁去掉了一些版本模式：
- en: Custom-resource-definition-3.yaml
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-resource-definition-3.yaml
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, the `Webhook` strategy lets us define a URL that requests will
    be made to with information about the incoming resource object, its current version,
    and the version it needs to be converted to.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`Webhook`策略允许我们定义一个 URL，向其发送请求，传递关于传入资源对象的信息，包括其当前版本和需要转换到的版本。
- en: The idea is that our `Webhook` server will then handle the conversion and pass
    back the corrected Kubernetes resource object. The `Webhook` strategy is complex
    and can have many possible configurations, which we will not get into in depth
    in this book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其原理是我们的`Webhook`服务器将处理转换并返回经过修正的 Kubernetes 资源对象。`Webhook`策略很复杂，可能有许多配置选项，本书中不会详细探讨。
- en: Important note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To see how conversion Webhooks can be configured, check the official Kubernetes
    documentation at [https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何配置转换 Webhook，请查看官方 Kubernetes 文档：[https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/)。
- en: Now, back to our `version` entry in the YAML! Under the `served` and `storage`
    keys, we see the `schema` object, which contains the actual specification of our
    resource. As previously mentioned, this follows the OpenAPI Spec v3 schema.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们 YAML 文件中的 `version` 条目！在 `served` 和 `storage` 键下，我们看到 `schema` 对象，它包含我们资源的实际规格。如前所述，这遵循
    OpenAPI Spec v3 的规范。
- en: 'The `schema` object, which was removed from the preceding code block for space
    reasons, is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间原因，前面的代码块中移除了 `schema` 对象，下面是其内容：
- en: Custom-resource-definition-3.yaml (continued)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-resource-definition-3.yaml（续）
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, we support a field for `delaySeconds`, which will be an integer,
    and `image`, which is a string that corresponds to our container image. If we
    really wanted to make the `DelayedJob` production-ready, we would want to include
    all sorts of other options to make it closer to the original Kubernetes Job resource
    – but that isn't our intent here.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们支持一个名为 `delaySeconds` 的字段，它是一个整数，以及一个名为 `image` 的字段，它是一个字符串，表示我们的容器镜像。如果我们真的想让
    `DelayedJob` 适用于生产环境，我们还需要包括各种其他选项，使其更接近原始的 Kubernetes Job 资源——但这并非我们的目的。
- en: Moving further back in the original code block, outside the versions list, we
    see some other attributes. First is the `scope` attribute, which can be either
    `Cluster` or `Namespaced`. This tells Kubernetes whether to treat instances of
    the CRD object as namespace-specific resources (such as Pods, Deployments, and
    so on) or instead as cluster-wide resources – like namespaces themselves, since
    getting namespace objects within a namespace doesn't make any sense!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 回到原始代码块中的版本列表之外，我们看到了一些其他属性。首先是 `scope` 属性，它可以是 `Cluster` 或 `Namespaced`。这个属性告诉
    Kubernetes 是否将 CRD 对象的实例视为命名空间特定的资源（例如 Pods、Deployments 等），或者视为集群范围的资源——比如命名空间本身，因为在一个命名空间内获取命名空间对象是没有意义的！
- en: Finally, we have the `names` block, which lets you define both a plural and
    singular form of your resource name, to be used in various situations (for instance,
    `kubectl get pods` and `kubectl get pod` both work).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 `names` 块，它允许你定义资源名称的复数和单数形式，以供在不同情况下使用（例如，`kubectl get pods` 和 `kubectl
    get pod` 都可以使用）。
- en: The `names` block also lets you define the camel-cased `kind` value, which will
    be used in the resource YAML, as well as one or more `shortNames`, which can be
    used to refer to the resource in the API or `kubectl` – for instance, `kubectl
    get po`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`names` 块还允许你定义驼峰式命名的 `kind` 值，这将在资源 YAML 文件中使用，同时还可以定义一个或多个 `shortNames`，用于在
    API 或 `kubectl` 中引用该资源——例如，`kubectl get po`。'
- en: 'With our CRD specification YAML explained, let''s take a look at an instance
    of our CRD – as defined by the spec we just reviewed, the YAML will look like
    this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 解释了我们的 CRD 规范 YAML 后，让我们看一下我们 CRD 的一个实例——根据我们刚才审阅的规范，YAML 文件将如下所示：
- en: Delayed-job.yaml
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Delayed-job.yaml
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, this is just like our CRD defined this object. Now, with all
    our pieces in place, let's test out our CRD!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这与我们在 CRD 中定义的对象完全相同。现在，所有部分就位，让我们来测试我们的 CRD！
- en: Testing a custom resource definition
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试自定义资源定义
- en: 'Let''s go ahead and test out our CRD concept on Kubernetes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在 Kubernetes 上测试我们的 CRD 概念：
- en: 'First, let''s create the CRD spec in Kubernetes – the same way we would create
    any other object:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们在 Kubernetes 中创建 CRD 规范——就像我们创建任何其他对象一样：
- en: '[PRE5]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will result in the following output:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE6]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, Kubernetes will accept requests for our `DelayedJob` resource. We can
    test this out by finally creating one using the preceding resource YAML:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，Kubernetes 将接受对我们的 `DelayedJob` 资源的请求。我们可以通过使用前面提供的资源 YAML 最终创建一个实例来测试这一点：
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we''ve defined our CRD properly, we will see the following output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正确定义了 CRD，我们将看到以下输出：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the Kubernetes API server has successfully created our instance
    of `DelayedJob`!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Kubernetes API 服务器已成功创建我们的 `DelayedJob` 实例！
- en: Now, you may be asking a very relevant question – now what? This is an excellent
    question, because the truth is that we have accomplished nothing more so far than
    essentially adding a new `table` to the Kubernetes API database.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会问一个非常相关的问题 —— 接下来怎么办？这是一个非常好的问题，因为实际上到目前为止，我们所做的只是简单地向 Kubernetes API
    数据库中添加了一张新的 `table`。
- en: Just because we gave our `DelayedJob` resource an application image and a `delaySeconds`
    field does not mean that any functionality like what we intend will actually occur.
    By creating our instance of `DelayedJob`, we have just added an entry to that
    `table`. We can fetch it, edit it, or delete it using the Kubernetes API or `kubectl`
    commands, but no application functionality has been implemented.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为我们给我们的 `DelayedJob` 资源分配了一个应用镜像和一个 `delaySeconds` 字段，并不意味着我们期望的功能就会发生。通过创建我们的
    `DelayedJob` 实例，我们只是向该 `table` 添加了一个条目。我们可以使用 Kubernetes API 或 `kubectl` 命令获取、编辑或删除它，但没有实现任何应用功能。
- en: In order to actually get our `DelayedJob` resource to do something, we need
    a custom controller that will take our instance of `DelayedJob` and do something
    with it. In the end, we still need to implement actual container functionality
    using the official Kubernetes resources – Pods et al.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的 `DelayedJob` 资源执行某些操作，我们需要一个自定义控制器，它将处理我们的 `DelayedJob` 实例并执行相应操作。最终，我们仍然需要使用官方的
    Kubernetes 资源 —— 如 Pods 等，来实现实际的容器功能。
- en: This is what we're going to discuss now. There are many ways to build custom
    controllers for Kubernetes, but a popular way is the `DelayedJob` resource a life
    of its own.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们现在要讨论的内容。有许多方法可以为 Kubernetes 构建自定义控制器，但一种流行的方法是将 `DelayedJob` 资源赋予其自身的生命周期。
- en: Self-managing functionality with Kubernetes operators
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 操作符实现自管理功能
- en: No discussion of Kubernetes operators would be possible without first discussing
    the **Operator Framework**. A common misconception is that operators are specifically
    built via the Operator Framework. The Operator Framework is an open source framework
    originally created by Red Hat to make it easy to write Kubernetes operators.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论 Kubernetes 操作符之前，必须先讨论 **Operator Framework**。一个常见的误解是认为操作符是通过 Operator
    Framework 特别构建的。Operator Framework 是一个开源框架，最初由 Red Hat 创建，旨在简化 Kubernetes 操作符的编写。
- en: In reality, an operator is simply a custom controller that interfaces with Kubernetes
    and acts on resources. The Operator Framework is one opinionated way to make Kubernetes
    operators, but there are many other open source frameworks you can use – or, you
    can make one from scratch!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，操作符只是一个自定义控制器，它与 Kubernetes 接口并对资源进行操作。Operator Framework 是一种创建 Kubernetes
    操作符的标准方法，但你也可以使用许多其他开源框架，或者从零开始创建一个！
- en: When building an operator using frameworks, two of the most popular options
    are the aforementioned **Operator Framework** and **Kubebuilder**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用框架构建操作符时，最受欢迎的两个选项是前面提到的 **Operator Framework** 和 **Kubebuilder**。
- en: Both of these projects have a lot in common. They both make use of `controller-tools`
    and `controller-runtime`, which are two libraries for building Kubernetes controllers
    that are officially supported by the Kubernetes project. If you are building an
    operator from scratch, using these officially supported controller libraries will
    make things much easier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个项目有很多相似之处。它们都使用了 `controller-tools` 和 `controller-runtime`，这两个库是用于构建 Kubernetes
    控制器的官方支持库。如果你是从零开始构建操作符，使用这些官方支持的控制器库会让事情变得更容易。
- en: Unlike the Operator Framework, Kubebuilder is an official part of the Kubernetes
    project, much like the `controller-tools` and `controller-runtime` libraries –
    but both projects have their pros and cons. Importantly, both these options, and
    the Operator pattern in general, have the controller running on the cluster. It
    may seem obvious that this is the best option, but you could run your controller
    outside of the cluster and have it work the same. To get started with the Operator
    Framework, check the official GitHub at [https://github.com/operator-framework](https://github.com/operator-framework).
    For Kubebuilder, you can check [https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Operator Framework 不同，Kubebuilder 是 Kubernetes 项目的一部分，类似于 `controller-tools`
    和 `controller-runtime` 库 —— 但这两个项目都有其优缺点。重要的是，这两种选项，以及操作符模式，通常都要求控制器运行在集群内。乍一看，这似乎是最佳选择，但你也可以在集群外部运行控制器，并使其正常工作。要开始使用
    Operator Framework，可以访问其官方 GitHub：[https://github.com/operator-framework](https://github.com/operator-framework)。要了解
    Kubebuilder，可以访问：[https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)。
- en: Most operators, regardless of the framework, follow a control-loop paradigm
    – let's see how this idea works.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数操作符，无论框架如何，都遵循控制循环范式 – 让我们来看一下这个概念是如何工作的。
- en: Mapping the operator control loop
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射操作符控制循环
- en: A control loop is a control scheme in system design and programming that consists
    of a never-ending loop of logical processes. Typically, a control loop implements
    a measure-analyze-adjust approach, where it measures the current state of the
    system, analyzes what changes are required to bring it in line with the intended
    state, and then adjusts the system components to bring it in line with (or at
    least closer to) the intended state.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 控制循环是一种系统设计和编程中的控制方案，它由一个永不停息的逻辑处理循环组成。通常，控制循环采用测量-分析-调整的方法，其中它测量系统的当前状态，分析需要哪些变化以使其符合预期状态，然后调整系统组件，使其与预期状态一致（或至少更接近）。
- en: 'In Kubernetes operators or controllers specifically, this operation usually
    works like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes操作符或控制器中，这个操作通常是这样的：
- en: First, a `watch` step – that is, watching the Kubernetes API for changes in
    the intended state, which is stored in `etcd`.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是`监视`步骤 – 即监视Kubernetes API中的预期状态变化，该状态存储在`etcd`中。
- en: Then, an `analyze` step – which is the controller deciding what to do to bring
    the cluster state in line with the intended state.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`分析`步骤 – 即控制器决定如何操作以使集群状态与预期状态一致。
- en: And lastly, an `update` step – which is updating the cluster state to fulfill
    the intent of the cluster changes.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后是`更新`步骤 – 即更新集群状态以实现集群变化的预期目标。
- en: 'To help understand the control loop, here is a diagram showing how the pieces
    fit together:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解控制循环，这里有一个图示，展示了各个部分如何组合在一起：
- en: '![Figure 13.1 – Measure Analyze Update Loop](img/B14790_13_01.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 – 测量分析更新循环](img/B14790_13_01.jpg)'
- en: Figure 13.1 – Measure Analyze Update Loop
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 测量分析更新循环
- en: 'Let''s use the Kubernetes scheduler – which is itself a control loop process
    – to illustrate this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Kubernetes调度器 – 它本身就是一个控制循环过程 – 来说明这个问题：
- en: 'Let''s start with a hypothetical cluster in a steady state: all Pods are scheduled,
    Nodes are healthy, and everything is operating normally.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从一个假设的集群开始，该集群处于稳定状态：所有Pod都已调度，节点健康，所有操作正常。
- en: Then, a user creates a new Pod.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，用户创建一个新的Pod。
- en: 'We''ve discussed before that the kubelet works on a `pull` basis. This means
    that when a kubelet creates a Pod on its Node, that Pod was already assigned to
    that Node via the scheduler. However, when Pods are first created via a `kubectl
    create` or `kubectl apply` command, the Pod isn''t scheduled or assigned anywhere.
    This is where our scheduler control loop starts:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过，kubelet是基于`拉取`机制工作的。这意味着，当kubelet在其Node上创建Pod时，Pod已经通过调度器分配到了该Node。然而，当Pod通过`kubectl
    create`或`kubectl apply`命令首次创建时，Pod尚未被调度或分配到任何地方。这就是我们的调度器控制循环开始的地方：
- en: The first step is **Measure**, where the scheduler reads the state of the Kubernetes
    API. When listing Pods from the API, it discovers that one of the Pods is not
    assigned to a Node. It now moves to the next step.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是**测量**，即调度器读取Kubernetes API的状态。当从API列出Pods时，它发现其中一个Pod没有分配到Node。然后，它进入下一步。
- en: Next, the scheduler performs an analysis of the cluster state and Pod requirements
    in order to decide which Node the Pod should be assigned to. As we discussed in
    previous chapters, this takes into account Pod resource limits and requests, Node
    statuses, placement controls, and so on, which makes it a fairly complex process.
    Once this processing is complete, the update step can start.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，调度器对集群状态和Pod需求进行分析，以决定将Pod分配到哪个Node。正如我们在前几章中讨论的，这个过程考虑了Pod资源限制和请求、Node状态、放置控制等，因此是一个相当复杂的过程。一旦处理完成，更新步骤就可以开始。
- en: Finally, **Update** – the scheduler updates the cluster state by assigning the
    Pod to the Node obtained from the *step 2* analysis. At this point, the kubelet
    takes over on its own control loop and creates the relevant container(s) for the
    Pod on its Node.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，**更新** – 调度器通过将Pod分配到从*步骤2*分析中获得的Node，来更新集群状态。此时，kubelet接管自己的控制循环，并在其Node上为Pod创建相关容器。
- en: Next, let's take what we learned from the scheduler control loop and apply it
    to our very own `DelayedJob` resource.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们把从调度器控制循环中学到的东西应用到我们自己的`DelayedJob`资源中。
- en: Designing an operator for a custom resource definition
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为自定义资源定义设计操作符
- en: Actually, coding an operator for our `DelayedJob` CRD is outside the scope of
    our book since it requires knowledge of a programming language. If you're choosing
    a programming language to build an operator with, Go offers the most interoperability
    with the Kubernetes SDK, **controller-tools**, and **controller-runtime**, but
    any programming language where you can write HTTP requests will work, since that
    is the basis for all of the SDKs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，为我们的 `DelayedJob` CRD 编写操作符超出了本书的范围，因为它需要编程语言的知识。如果你选择编程语言来构建操作符，Go 提供了与
    Kubernetes SDK、**controller-tools** 和 **controller-runtime** 的最佳互操作性，但任何能编写 HTTP
    请求的编程语言都可以使用，因为这正是所有 SDK 的基础。
- en: However, we will still walk through the steps of implementing an operator for
    our `DelayedJob` CRD with some pseudocode. Let's take it step by step.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然会逐步演示如何为我们的 `DelayedJob` CRD 实现一个操作符，并附上一些伪代码。让我们一步一步来。
- en: 'Step 1: Measure'
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 1 步：测量
- en: First comes the `while` loop that runs forever. In a production implementation,
    there would be debouncing, error handling, and a bunch of other concerns, but
    we'll keep it simple for this illustrative example.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是一个永远运行的 `while` 循环。在生产环境中，应该有防抖、错误处理和其他一堆问题，但为了说明清楚，我们将保持简单。
- en: 'Take a look at the pseudo code for this loop, which is essentially the main
    function of our application:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下这个循环的伪代码，它本质上是我们应用程序的主函数：
- en: Main-function.pseudo
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Main-function.pseudo
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the loop in our `main` function calls the Kubernetes API to
    find a list of the `delayedjobs` CRDs stored in `etcd`. This is the `measure`
    step. It then calls the analysis step, and with the results of that, calls the
    update step to schedule any `DelayedJobs` that need to be scheduled.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的 `main` 函数中的循环调用 Kubernetes API 来查找存储在 `etcd` 中的 `delayedjobs` CRD 列表。这是
    `measure` 步骤。然后它调用分析步骤，依据分析结果，再调用更新步骤来调度需要调度的 `DelayedJobs`。
- en: Important note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Keep in mind that the Kubernetes scheduler is still going to do the actual container
    scheduling in this example – but we need to boil down our `DelayedJob` into an
    official Kubernetes resource first.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个例子中，Kubernetes 调度器仍然会执行实际的容器调度——但我们首先需要将 `DelayedJob` 转化为官方的 Kubernetes
    资源。
- en: After the update step, our loop waits for a full 5 seconds before performing
    the loop again. This sets the cadence of the control loop. Next, let's move on
    to the analysis step.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新步骤之后，我们的循环会等待完整的 5 秒钟，然后再次执行循环。这设置了控制循环的节奏。接下来，让我们进入分析步骤。
- en: 'Step 2: Analyze'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 2 步：分析
- en: 'Next, let''s review the `analyzeDelayedJobs` function in our controller pseudocode:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们回顾一下我们控制器伪代码中的 `analyzeDelayedJobs` 函数：
- en: Analysis-function.pseudo
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Analysis-function.pseudo
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the preceding function loops through the list of `DelayedJob`
    objects from the cluster as passed from the `DelayedJob` has been scheduled yet
    by checking the value of one of the object's annotations. If it hasn't been scheduled
    yet, it adds an object to an array called `listOfJobsToSchedule`, which contains
    the image specified in the `DelayedJob` object, a command to sleep for the number
    of seconds that was specified in the `DelayedJob` object, and the original name
    of the `DelayedJob`, which we will use to mark as scheduled in the **Update**
    step.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，前面的函数通过检查 `DelayedJob` 对象的某个注释值来遍历来自集群的 `DelayedJob` 对象列表，以确定该对象是否已被调度。如果尚未调度，它将一个对象添加到名为
    `listOfJobsToSchedule` 的数组中，该数组包含在 `DelayedJob` 对象中指定的图像、一个命令用于在 `DelayedJob`
    对象中指定的秒数后休眠，以及 `DelayedJob` 的原始名称，我们将在**更新**步骤中使用该名称来标记为已调度。
- en: Finally, in the `analyzeDelayedJobs` function returns our newly created `listOfJobsToSchedule`
    array back to the main function. Let's wrap up our Operator design with the final
    update step, which is the `scheduleDelayedJobs` function in our main loop.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 `analyzeDelayedJobs` 函数返回我们新创建的 `listOfJobsToSchedule` 数组给主函数后，我们将通过最终的更新步骤来完成我们的
    Operator 设计，即主循环中的 `scheduleDelayedJobs` 函数。
- en: 'Step 3: Update'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 3 步：更新
- en: 'Finally, the **Update** part of our control loop will take the outputs from
    our analysis and update the cluster as necessary to create the intended state.
    Here''s the pseudocode:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们控制循环的**更新**部分将根据分析的输出更新集群，以创建预期的状态。以下是伪代码：
- en: Update-function.pseudo
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Update-function.pseudo
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this case, we are taking our regular Kubernetes object, which was derived
    from our `DelayedJob` object, and creating it in Kubernetes so the `Kube` scheduler
    can pick up on it, create the relevant Pod, and manage it. Once we create the
    regular Job object with the delay, we also update our `DelayedJob` CRD instance
    with an annotation that sets the `is-scheduled` annotation to `true`, preventing
    it from getting rescheduled.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将常规的Kubernetes对象（它是从我们的`DelayedJob`对象派生的）创建到Kubernetes中，以便`Kube`调度器可以捕捉它，创建相关的Pod并进行管理。一旦我们创建了带有延迟的常规Job对象，我们还会更新`DelayedJob`
    CRD实例，添加一个注释，将`is-scheduled`注释设置为`true`，从而防止它被重新调度。
- en: This completes our control loop – from this point, the `Kube` scheduler takes
    over and our CRD is given life as a Kubernetes Job object, which controls a Pod,
    which is finally assigned to a Node and a container is scheduled to run our code!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的控制循环——从此时起，`Kube`调度器接管，我们的CRD作为Kubernetes Job对象诞生，它控制一个Pod，最终被分配到一个节点上，并且容器被安排运行我们的代码！
- en: This example is of course highly simplified, but you would be surprised how
    many Kubernetes operators perform a simple control loop to coordinate CRDs and
    boil them down to basic Kubernetes resources. Operators can get very complicated
    and perform application-specific functions such as backing up databases, emptying
    Persistent Volumes, and others – but this functionality is usually tightly coupled
    to whatever is being controlled.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子当然是高度简化的，但你会惊讶于有多少Kubernetes操作员执行简单的控制循环来协调CRD，并将其简化为基本的Kubernetes资源。操作员可以变得非常复杂，并执行特定应用的功能，比如备份数据库、清空持久化卷等——但这些功能通常与被控制的对象紧密耦合。
- en: Now that we've discussed the Operator pattern in a Kubernetes controller, we
    can talk about some of the open source options for cloud-specific Kubernetes controllers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了Kubernetes控制器中的Operator模式，我们可以谈论一些针对特定云的Kubernetes控制器的开源选项。
- en: Using cloud-specific Kubernetes extensions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特定于云的Kubernetes扩展
- en: Usually available by default in managed Kubernetes services such as Amazon EKS,
    Azure AKS, and Google Cloud's GKE, cloud-specific Kubernetes extensions and controllers
    can integrate tightly with the cloud platform in question and make it easy to
    control other cloud resources from Kubernetes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 云特定的Kubernetes扩展和控制器通常在托管的Kubernetes服务中默认可用，例如Amazon EKS、Azure AKS和Google Cloud的GKE，它们可以与相应的云平台紧密集成，并使从Kubernetes控制其他云资源变得容易。
- en: Even without adding any additional third-party components, a lot of this cloud-specific
    functionality is available in upstream Kubernetes via the **cloud-controller-manager**
    (**CCM**) component, which contains many options for integrating with the major
    cloud providers. This is the functionality that is usually enabled by default
    in the managed Kubernetes services on each public cloud – but they can be integrated
    with any cluster running on that specific cloud platform, managed or not.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 即使不添加任何额外的第三方组件，这些特定于云的功能也可以通过**云控制器管理器（CCM）**组件在上游Kubernetes中使用，该组件包含许多与主要云服务提供商集成的选项。这是公共云上每个托管Kubernetes服务通常默认启用的功能——但它们也可以与在该特定云平台上运行的任何集群集成，无论该集群是否是托管的。
- en: In this section, we will review a few of the more common cloud extensions to
    Kubernetes, both in **cloud-controller-manager (CCM)** and functionality that
    requires the installation of other controllers such as **external-dns** and **cluster-autoscaler**.
    Let's start with some of the heavily used CCM functionality.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一些常见的Kubernetes云扩展，包括**云控制器管理器（CCM）**和需要安装其他控制器的功能，如**external-dns**和**cluster-autoscaler**。让我们从一些常用的CCM功能开始。
- en: Understanding the cloud-controller-manager component
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解云控制器管理器组件
- en: As reviewed in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, CCM is an officially supported Kubernetes controller
    that provides hooks into the functionality of several public cloud services. To
    function, the CCM component needs to be started with access permissions to the
    cloud service in question – for instance, an IAM role in AWS.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*第一章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)《与Kubernetes通信》中回顾的那样，**云控制器管理器（CCM）**是一个官方支持的Kubernetes控制器，提供了与多个公共云服务功能的接口。为了正常运行，CCM组件需要在启动时具有访问相应云服务的权限——例如，AWS中的IAM角色。
- en: For officially supported clouds such as AWS, Azure, and Google Cloud, CCM can
    simply be run as a DaemonSet within the cluster. We use a DaemonSet since CCM
    can perform tasks such as creating persistent storage in the cloud provider, and
    it needs to be able to attach storage to specific Nodes. If you're using a cloud
    that isn't officially supported, you can run CCM for that specific cloud, and
    you should follow the specific instructions in that project. These alternate types
    of CCM are usually open source and can be found on GitHub. For the specifics of
    installing CCM, let's move on to the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AWS、Azure和Google Cloud等官方支持的云，CCM可以简单地作为DaemonSet在集群内运行。我们使用DaemonSet是因为CCM可以执行诸如在云提供商中创建持久存储等任务，它需要能够将存储附加到特定的节点。如果你使用的云不是官方支持的，你可以为该特定云运行CCM，且应遵循该项目中的具体说明。这些替代类型的CCM通常是开源的，可以在GitHub上找到。至于安装CCM的具体操作，让我们进入下一部分。
- en: Installing cloud-controller-manager
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装cloud-controller-manager
- en: Typically, CCM is configured when the cluster is created. As mentioned in the
    previous section, managed services such as EKS, AKS, and GKE will already have
    this component enabled, but even Kops and Kubeadm expose the CCM component as
    a flag in the installation process.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CCM在集群创建时就已配置。如前一节所述，EKS、AKS和GKE等托管服务已经启用了此组件，即便是Kops和Kubeadm在安装过程中也会以标志的形式暴露CCM组件。
- en: Assuming you have not installed CCM any other way and plan to use one of the
    officially supported public clouds from the upstream version, you can install
    CCM as a DaemonSet.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你没有以其他方式安装CCM，并打算使用上游版本支持的公共云中的一个，你可以将CCM作为DaemonSet安装。
- en: 'First, you will need a `ServiceAccount`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要一个`ServiceAccount`：
- en: Service-account.yaml
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Service-account.yaml
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This `ServiceAccount` will be used to give the necessary access to the CCM.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`ServiceAccount`将用于授予CCM所需的访问权限。
- en: 'Next, we''ll need a `ClusterRoleBinding`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个`ClusterRoleBinding`：
- en: Clusterrolebinding.yaml
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Clusterrolebinding.yaml
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, we need to give the `cluster-admin` role access to our CCM service
    account. The CCM will need to be able to edit Nodes, among other things.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们需要将`cluster-admin`角色的访问权限授予我们的CCM服务账户。CCM将需要能够编辑节点，此外还需要执行其他操作。
- en: Finally, we can deploy the CCM `DaemonSet` itself. You will need to fill in
    this YAML file with the proper settings for your specific cloud provider – check
    your cloud provider's documentation on Kubernetes for this information.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以部署CCM的`DaemonSet`本身。你需要根据你特定的云提供商填写这个YAML文件中的适当设置——请查阅你云提供商的Kubernetes文档以获取此信息。
- en: 'The `DaemonSet` spec is quite long, so we''ll review it in two parts. First,
    we have the template for the `DaemonSet` with the required labels and names:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`DaemonSet`规格相当长，因此我们将分两部分进行审查。首先，我们有带有必要标签和名称的`DaemonSet`模板：'
- en: Daemonset.yaml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Daemonset.yaml
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, to match our `ServiceAccount`, we are running the CCM in the
    `kube-system` namespace. We are also labeling the `DaemonSet` with the `k8s-app`
    label to distinguish it as a Kubernetes control plane component.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，为了匹配我们的`ServiceAccount`，我们将CCM部署在`kube-system`命名空间中。我们还使用`k8s-app`标签标记`DaemonSet`，以便将其区分为Kubernetes控制平面组件。
- en: 'Next, we have the spec of the `DaemonSet`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`DaemonSet`的规格：
- en: Daemonset.yaml (continued)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Daemonset.yaml（续）
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, there are a couple of places in this spec that you will need
    to review your chosen cloud provider's documentation or cluster networking setup
    to find the proper values. Particularly in the networking flags such as `--cluster-cidr`
    and `--configure-cloud-routes`, where values could change based on how you have
    set up your cluster, even on a single cloud provider.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在此规格中有几个地方需要你查看所选云提供商的文档或集群网络设置，以找到适当的值。特别是在网络标志如`--cluster-cidr`和`--configure-cloud-routes`中，值可能会根据你设置集群的方式发生变化，即使是在同一云提供商上。
- en: Now that we have CCM running on our cluster one way or another, let's dive into
    some of the capabilities it provides.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过某种方式在集群上运行了CCM，接下来让我们深入了解它提供的一些功能。
- en: Understanding the cloud-controller-manager capabilities
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解cloud-controller-manager的功能
- en: The default CCM provides capabilities in a few key areas. For starters, the
    CCM contains subsidiary controllers for Nodes, routes, and Services. Let's review
    each in turn to see what it affords us, starting with the Node/Node lifecycle
    controller.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的CCM在几个关键领域提供了功能。首先，CCM包含用于节点、路由和服务的附属控制器。让我们逐一回顾每个控制器，以了解它为我们提供了什么，首先从节点/节点生命周期控制器开始。
- en: The CCM Node/Node lifecycle controller
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CCM节点/节点生命周期控制器
- en: The CCM Node controller makes sure that the cluster state, as far as which Nodes
    are in the cluster, is equivalent to what is in the cloud provider's systems.
    A simple example of this is autoscaling groups in AWS. When using AWS EKS (or
    just Kubernetes on AWS EC2, though that requires additional configuration), it
    is possible to configure worker node groups in an AWS autoscaling group that will
    scale up or down depending on the CPU or memory usage of the nodes. When these
    nodes are added and initialized by the cloud provider, the CCM nodes controller
    will ensure that the cluster has a node resource for each Node presented by the
    cloud provider.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: CCM 节点控制器确保集群的状态（即集群中包含哪些节点）与云提供商系统中的状态一致。一个简单的例子是 AWS 中的自动扩展组。当使用 AWS EKS（或仅在
    AWS EC2 上运行 Kubernetes，尽管这需要额外的配置）时，可以在 AWS 自动扩展组中配置工作节点组，这些节点组会根据节点的 CPU 或内存使用情况进行扩展。当这些节点被云提供商添加并初始化时，CCM
    节点控制器将确保集群中有一个节点资源，与云提供商展示的每个节点相匹配。
- en: Next, let's move on to the routes controller.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续讨论路由控制器。
- en: The CCM routes controller
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CCM 路由控制器
- en: The CCM routes controller takes care of configuring your cloud provider's networking
    settings in a way that supports a Kubernetes cluster. This can include the allocation
    of IPs and setting routes between Nodes. The services controller also handles
    networking – but the external aspect.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CCM 路由控制器负责以支持 Kubernetes 集群的方式配置云提供商的网络设置。这可能包括 IP 分配和在节点之间设置路由。服务控制器也处理网络设置——但主要是外部方面。
- en: The CCM services controller
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CCM 服务控制器
- en: The CCM services controller provides a lot of the "magic" of running Kubernetes
    on a public cloud provider. One such aspect that we reviewed in [*Chapter 5*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127),
    *Services and Ingress – Communicating with the Outside World*, is the `LoadBalancer`
    service. For instance, on a cluster configured with AWS CCM, a Service of type
    `LoadBalancer` will automatically configure a matching AWS Load Balancer resource,
    providing an easy way to expose services in your cluster without dealing with
    `NodePort` settings or even Ingress.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: CCM 服务控制器提供了在公共云提供商上运行 Kubernetes 的许多“魔法”功能。我们在[*第 5 章*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127)中回顾的一个方面是`LoadBalancer`服务，标题为*服务和入口——与外部世界的通信*。例如，在配置了
    AWS CCM 的集群中，类型为`LoadBalancer`的服务将自动配置匹配的 AWS 负载均衡器资源，提供了一种轻松暴露集群服务的方法，无需处理`NodePort`设置，甚至不需要使用
    Ingress。
- en: Now that we understand what the CCM provides, we can venture further and talk
    about a couple of the other cloud provider extensions that are often used when
    running Kubernetes on the public cloud. First, let's look at `external-dns`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 CCM 提供的功能，我们可以更进一步，讨论在公共云上运行 Kubernetes 时，通常使用的其他云提供商扩展。首先，让我们看看`external-dns`。
- en: Using external-dns with Kubernetes
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `external-dns` 与 Kubernetes
- en: The `external-dns` library is an officially supported Kubernetes add-on that
    allows the cluster to configure external DNS providers to provide DNS resolution
    for services and ingress in an automated fashion. The `external-dns` add-on supports
    a broad range of cloud providers such as AWS and Azure, and also other DNS services
    such as Cloudflare.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`external-dns` 库是一个官方支持的 Kubernetes 插件，它允许集群配置外部 DNS 提供商，以自动化方式为服务和入口提供 DNS
    解析。`external-dns` 插件支持广泛的云提供商，如 AWS 和 Azure，以及其他 DNS 服务，如 Cloudflare。'
- en: Important note
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In order to install `external-dns`, you can check the official GitHub repository
    at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`external-dns`，可以访问官方的 GitHub 仓库 [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)。
- en: Once `external-dns` is implemented on your cluster, it's simple to create new
    DNS records in an automated fashion. To test `external-dns` with a service, we
    simply need to create a service in Kubernetes with the proper annotation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在集群中实现了`external-dns`，就可以以自动化的方式轻松创建新的 DNS 记录。要测试带有服务的`external-dns`，我们只需要在
    Kubernetes 中创建一个带有正确注解的服务。
- en: 'Let''s see what this looks like:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的样子：
- en: service.yaml
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: service.yaml
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, we only need to add an annotation for the `external-dns` controller
    to check, with the domain record to be created in DNS. The domain and hosted zone
    must of course be accessible by your `external-dns` controller – for instance,
    on AWS Route 53 or Azure DNS. Check the specific documentation on the `external-dns`
    GitHub repository for specifics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们只需要为 `external-dns` 控制器添加一个注释，待其检查并在 DNS 中创建域名记录。该域名和托管区域必须能够被你的 `external-dns`
    控制器访问——例如，AWS Route 53 或 Azure DNS。请参考 `external-dns` GitHub 仓库中的具体文档了解详细信息。
- en: Once the Service is up and running, `external-dns` will pick up the annotation
    and create a new DNS record. This pattern is excellent for multi-tenancy or per-version
    deploys since with something like a Helm chart, variables can be used to change
    the domain depending on which version or branch of the application is deployed
    – for instance, `v1.myapp.mydomain.com`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务启动并运行，`external-dns` 将拾取注释并创建一个新的 DNS 记录。这种模式非常适合多租户或按版本部署，因为像 Helm 图表这样的工具可以利用变量根据应用程序的版本或分支来更改域名——例如，`v1.myapp.mydomain.com`。
- en: 'For Ingress, this is even easier – you just need to specify a host on your
    Ingress record, like so:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Ingress，这更简单——你只需在 Ingress 记录中指定一个主机名，如下所示：
- en: ingress.yaml
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ingress.yaml
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This host value will automatically create a DNS record pointing to whatever
    method your Ingress is using – for instance, a Load Balancer on AWS.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主机值将自动创建一个 DNS 记录，指向你的 Ingress 所使用的方法——例如，AWS 上的负载均衡器。
- en: Next, let's talk about how the **cluster-autoscaler** library works.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来了解一下 **cluster-autoscaler** 库的工作原理。
- en: Using the cluster-autoscaler add-on
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 cluster-autoscaler 插件
- en: Similar to `external-dns`, `cluster-autoscaler` is an officially supported add-on
    for Kubernetes that supports some major cloud providers with specific functionality.
    The purpose of `cluster-autoscaler` is to trigger the scaling of the number of
    Nodes in a cluster. It performs this process by controlling the cloud provider's
    own scaling resources, such as AWS autoscaling groups.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `external-dns` 类似，`cluster-autoscaler` 是 Kubernetes 官方支持的插件，支持一些主要云服务提供商的特定功能。`cluster-autoscaler`
    的目的是触发集群中节点数量的自动扩展。它通过控制云服务提供商的自动扩展资源（例如 AWS 自动扩展组）来完成此过程。
- en: The cluster autoscaler will perform an upward scaling action the moment any
    single Pod fails to schedule due to resource constraints on a Node, but only if
    a Node of the existing Node size (for instance, a `t3.medium` sized Node in AWS)
    would allow the Pod to be scheduled.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当任何 Pod 因为节点资源限制而无法调度时，集群自动扩展器将执行向上扩展操作，但前提是现有节点大小（例如 AWS 中的 `t3.medium` 节点）允许该
    Pod 被调度。
- en: Similarly, the cluster autoscaler will perform a downward scaling action the
    moment any Node could be emptied of Pods without causing memory or CPU pressure
    on any of the other Nodes.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当任何节点上的 Pod 可以被清空而不对其他节点造成内存或 CPU 压力时，集群自动扩展器将执行向下扩展操作。
- en: To install `cluster-autoscaler`, simply follow the correct instructions from
    your cloud provider, for the cluster type and intended version of the `cluster-autoscaler`.
    For instance, the AWS installation instructions for `cluster-autoscaler` on EKS
    are found at [https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `cluster-autoscaler`，只需按照云服务提供商提供的正确安装指南进行操作，选择适合的集群类型和目标版本的 `cluster-autoscaler`。例如，AWS
    上 EKS 的 `cluster-autoscaler` 安装说明可以在[https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/)找到。
- en: Next, let's look at how you can find open and closed source extensions for Kubernetes
    by examining the Kubernetes ecosystem.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过检查 Kubernetes 生态系统，来看一下如何找到开源和闭源的 Kubernetes 扩展。
- en: Integrating with the ecosystem
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与生态系统集成
- en: The Kubernetes (and more generally, cloud-native) ecosystem is massive, consisting
    of hundreds of popular open source software libraries, and thousands more fledgling
    ones. This can be tough to navigate since every month brings new technologies
    to vet, and acquisitions, rollups, and companies going out of business can turn
    your favorite open source library into an unmaintained mess.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes（以及更广泛的云原生）生态系统庞大，包含数百个流行的开源软件库，还有成千上万个新兴的库。由于每个月都会有新技术需要评估，而且收购、整合或公司倒闭可能会将你最喜欢的开源库变成无人维护的烂摊子，因此在这个生态系统中导航是非常困难的。
- en: Thankfully, there is some structure in this ecosystem, and it's worth knowing
    about it in order to help navigate the dearth of options in cloud-native open
    source. The first big structural component of this is the **Cloud Native Computing
    Foundation** or **CNCF**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个生态系统中有一些结构，了解这些结构对于帮助我们在云原生开源项目的选择中导航非常重要。这个结构的第一个重要组成部分就是 **Cloud Native
    Computing Foundation** 或 **CNCF**。
- en: Introducing the Cloud Native Computing Foundation
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Cloud Native Computing Foundation
- en: The CNCF is a sub-foundation of the Linux Foundation, which is a non-profit
    entity that hosts open source projects and coordinates an ever-changing list of
    companies that contribute to and use open source software.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF 是 Linux 基金会的一个子基金会，Linux 基金会是一个非盈利实体，负责托管开源项目，并协调不断变化的公司名单，这些公司为开源软件做出贡献并使用开源软件。
- en: The CNCF was founded almost entirely to shepherd the future of the Kubernetes
    project. It was announced alongside the 1.0 release of Kubernetes and has since
    grown to encompass hundreds of projects in the cloud-native space – from Prometheus
    to Envoy to Helm, and many more.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF 的成立几乎完全是为了引领 Kubernetes 项目的未来。它在 Kubernetes 1.0 发布时宣布成立，随后扩展到涵盖了云原生领域的数百个项目
    —— 从 Prometheus 到 Envoy 再到 Helm，还有更多项目。
- en: The best way to see an overview of the CNCF's constituent projects is to check
    out the CNCF Cloud Native Landscape, which can be found at [https://landscape.cncf.io/](https://landscape.cncf.io/).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 CNCF 组成项目的概览的最佳方式是查看 CNCF 云原生景观，网址为 [https://landscape.cncf.io/](https://landscape.cncf.io/)。
- en: The CNCF Landscape is a good place to start if you are interested in possible
    solutions to a problem you are experiencing with Kubernetes or cloud-native. For
    every category (monitoring, logging, serverless, service mesh, and others), there
    are several open source options to vet and choose from.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 Kubernetes 或云原生领域中遇到的问题的可能解决方案感兴趣，CNCF Landscape 是一个很好的起点。在每个类别（监控、日志、无服务器、服务网格等）中，都有多个开源选项可以审核并选择。
- en: This is both a strength and weakness of the current ecosystem of cloud-native
    technologies. There are a significant number of options available, which makes
    the correct path often unclear, but also means that you will likely be able to
    find a solution that is close to your exact needs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当前云原生技术生态系统的优势和劣势。虽然有大量的选择可用，这使得正确的路径往往不明确，但也意味着你很可能能找到一个非常接近你需求的解决方案。
- en: The CNCF also operates an official Kubernetes forum, which can be joined from
    the Kubernetes official website at [kubernetes.io](http://kubernetes.io). The
    URL of the Kubernetes forums is [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF 还运营着一个官方的 Kubernetes 论坛，用户可以通过 Kubernetes 官方网站 [kubernetes.io](http://kubernetes.io)
    加入。Kubernetes 论坛的网址是 [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)。
- en: Finally, it is relevant to mention *KubeCon*/*CloudNativeCon*, a large conference
    that is run by the CNCF and encompasses topics including Kubernetes itself and
    many ecosystem projects. *KubeCon* gets larger every year, with almost 12,000
    attendees for *KubeCon* *North America* in 2019.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的是 *KubeCon*/*CloudNativeCon*，这是一个由 CNCF 主办的大型会议，涵盖了包括 Kubernetes 本身在内的许多生态系统项目。*KubeCon*
    每年都在增长，2019 年 *KubeCon North America* 会议的参会人数接近 12,000 人。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about extending Kubernetes. First, we talked about
    CRDs – what they are, some relevant use cases, and how to implement them in your
    cluster. Next, we reviewed the concept of an operator in Kubernetes and discussed
    how to use an operator, or custom controller, to give life to your CRD.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何扩展 Kubernetes。首先，我们讨论了 CRD —— 它是什么，相关的使用案例，以及如何在集群中实现它们。接下来，我们回顾了
    Kubernetes 中操作员的概念，并讨论了如何使用操作员或自定义控制器为 CRD 赋予生命。
- en: Then, we discussed cloud-provider-specific extensions to Kubernetes including
    `cloud-controller-manager`, `external-dns`, and `cluster-autoscaler`. Finally,
    we wrapped up with an introduction to the cloud-native open source ecosystem at
    large and some great ways to discover projects for your use case.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了 Kubernetes 针对云服务提供商的特定扩展，包括 `cloud-controller-manager`、`external-dns`
    和 `cluster-autoscaler`。最后，我们介绍了云原生开源生态系统的一些基本概念，并提供了一些发现适合自己用例的项目的优秀方法。
- en: The skills you used in this chapter will help you extend your Kubernetes cluster
    to interface with your cloud provider as well as your own custom functionality.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的技能将帮助你扩展 Kubernetes 集群，以便与云服务提供商及你自定义的功能进行交互。
- en: In the next chapter, we'll talk about two nascent architectural patterns as
    applied to Kubernetes – serverless and service meshes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论应用于Kubernetes的两种新兴架构模式——无服务器架构和服务网格。
- en: Questions
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between a served version and a stored version of a CRD?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CRD的服务版本与存储版本有什么区别？
- en: What are three typical parts of a custom controller or operator control loop?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义控制器或操作符控制循环的三个典型部分是什么？
- en: How does `cluster-autoscaler` interact with existing cloud provider scaling
    solutions such as AWS autoscaling groups?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cluster-autoscaler`如何与现有云提供商的自动扩展解决方案（如AWS自动扩展组）交互？'
- en: Further reading
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'CNCF Landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CNCF Landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
- en: 'Official Kubernetes Forums: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '官方Kubernetes论坛: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
