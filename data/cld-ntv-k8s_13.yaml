- en: '*Chapter 10*: Troubleshooting Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章*：排查 Kubernetes'
- en: This chapter reviews the best-practice methods for effectively troubleshooting
    Kubernetes clusters and the applications that run on them. This includes a discussion
    of common Kubernetes issues, as well as how to debug the masters and workers separately.
    The common Kubernetes issues will be discussed and taught in a case study format,
    split into cluster issues and application issues.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章回顾了有效排查 Kubernetes 集群及其运行应用程序的最佳实践方法。这包括讨论常见的 Kubernetes 问题，以及如何分别调试主节点和工作节点。常见的
    Kubernetes 问题将通过案例研究的形式进行讨论，分为集群问题和应用程序问题。
- en: We will start with a discussion of some common Kubernetes failure modes, before
    moving on to how to best troubleshoot clusters and applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论一些常见的 Kubernetes 故障模式，然后继续探讨如何最佳地排查集群和应用程序。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding failure modes for distributed applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布式应用程序的故障模式
- en: Troubleshooting Kubernetes clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查 Kubernetes 集群
- en: Troubleshooting applications on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上排查应用程序
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行本章中详细介绍的命令，你需要一台支持 `kubectl` 命令行工具的计算机，并且需要一个正常工作的 Kubernetes 集群。请参见[*第
    1 章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，《与 Kubernetes 通信》，其中提供了几种快速启动
    Kubernetes 的方法，并介绍了如何安装 `kubectl` 工具的步骤。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的代码可以在本书的 GitHub 仓库中找到，地址为 [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter10)。
- en: Understanding failure modes for distributed applications
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布式应用程序的故障模式
- en: Kubernetes components (and applications running on Kubernetes) are distributed
    by default if they run more than one replica. This can result in some interesting
    failure modes, which can be hard to debug.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 组件（以及在 Kubernetes 上运行的应用程序）默认是分布式的，只要它们运行了多个副本。这可能导致一些有趣的故障模式，调试起来可能比较困难。
- en: For this reason, applications on Kubernetes are less prone to failure if they
    are stateless – in which case, the state is offloaded to a cache or database running
    outside of Kubernetes. Kubernetes primitives such as StatefulSets and PersistentVolumes
    can make it much easier to run stateful applications on Kubernetes – and with
    every release, the experience of running stateful applications on Kubernetes improves.
    Still, deciding to run fully stateful applications on Kubernetes introduces complexity
    and therefore the potential for failure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 Kubernetes 上的应用程序是无状态的，它们就不太容易发生故障——在这种情况下，状态会被转移到 Kubernetes 外部运行的缓存或数据库中。Kubernetes
    中的原语，如 StatefulSets 和 PersistentVolumes，可以大大简化在 Kubernetes 上运行有状态应用程序的过程——随着每个版本的发布，Kubernetes
    上运行有状态应用程序的体验也在不断改善。尽管如此，决定在 Kubernetes 上运行完全有状态的应用程序仍然会引入复杂性，从而增加故障的潜在风险。
- en: Failure in distributed applications can be introduced by many different factors.
    Things as simple as network reliability and bandwidth constraints can cause major
    issues. These are so varied that *Peter Deutsch* at *Sun Microsystems* helped
    pen the *Fallacies of distributed computing* (along with *James Gosling*, who
    added the 8th point), which are commonly agreed-upon factors for failures in distributed
    applications. In the paper *Fallacies of distributed computing explained*, *Arnon
    Rotem-Gal-Oz* discusses the source of these fallacies ([https://www.rgoarchitects.com/Files/fallacies.pdf](https://www.rgoarchitects.com/Files/fallacies.pdf)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式应用程序的故障可能由许多不同因素引起。像网络可靠性和带宽限制这样简单的因素，就可能导致严重的问题。这些问题种类繁多，以至于*彼得·德意志*（*Sun
    Microsystems*）和*詹姆斯·戈斯林*（他增加了第八点）共同撰写了《分布式计算的谬误》一文，成为业界普遍认可的分布式应用程序故障的因素。在《分布式计算的谬误解析》一文中，*阿尔农·罗特姆-加尔-奥兹*讨论了这些谬误的来源（[https://www.rgoarchitects.com/Files/fallacies.pdf](https://www.rgoarchitects.com/Files/fallacies.pdf)）。
- en: 'The fallacies are as follows, in numerical order:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些谬误按数字顺序列出如下：
- en: The network is reliable.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络是可靠的。
- en: Latency is zero.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 延迟为零。
- en: Bandwidth is infinite.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带宽是无限的。
- en: The network is secure.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络是安全的。
- en: The topology doesn't change.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拓扑不会改变。
- en: There is one administrator.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有一个管理员。
- en: Transport cost is zero.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传输成本为零。
- en: The network is homogeneous.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络是同质化的。
- en: Kubernetes has been engineered and developed with these fallacies in mind and
    is therefore more tolerant. It also helps address these issues for applications
    running on Kubernetes – but not perfectly. It is therefore very possible that
    your applications, when containerized and running on Kubernetes, will exhibit
    problems when faced with any of these issues. Each fallacy, when assumed to be
    untrue and taken to its logical conclusion, can introduce failure modes in distributed
    applications. Let's go through each of the fallacies as applied to Kubernetes
    and applications running on Kubernetes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在设计和开发时就考虑到了这些错误观念，因此它更具容错性。它还帮助解决在Kubernetes上运行的应用程序面临的这些问题——但并不是完美的。因此，当你的应用程序容器化并在Kubernetes上运行时，很可能会遇到这些问题。每个错误观念，当被假定为不真实并推到其逻辑结论时，都可能在分布式应用程序中引入故障模式。我们来逐一分析这些错误观念，看看它们如何应用到Kubernetes以及在Kubernetes上运行的应用程序。
- en: The network is reliable
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络是可靠的。
- en: Applications running on multiple logical machines must communicate over the
    internet – so any reliability problems in the network can introduce issues. On
    Kubernetes specifically, the control plane itself can be distributed in a highly
    available setup (which means a setup with multiple master Nodes – see [*Chapter
    1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating with Kubernetes*),
    which means that failure modes can be introduced at the controller level. If the
    network is unreliable, then kubelets may not be able to communicate with the control
    plane, leading to Pod placement issues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在多个逻辑机器上的应用程序必须通过互联网进行通信——因此，网络中的任何可靠性问题都可能引发问题。特别是在Kubernetes上，控制平面本身可以通过高可用性设置进行分布式（这意味着使用多个主节点的设置——参见
    [*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*），这意味着控制器级别可能引入故障模式。如果网络不可靠，kubelet可能无法与控制平面通信，从而导致Pod调度问题。
- en: Similarly, the Nodes of the control plane may not be able to communicate with
    each other – though `etcd` is of course built with a consensus protocol that can
    tolerate communication failures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，控制平面的节点可能无法彼此通信——尽管`etcd`当然是采用共识协议构建的，可以容忍通信失败。
- en: Finally, the worker Nodes may not be able to communicate with each other – which,
    in a microservices scenario, could cause problems depending on Pod placement.
    In some cases, the workers may all be able to communicate with the control plane
    while still not being able to communicate with each other, which can cause issues
    with the Kubernetes overlay network.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，工作节点可能无法相互通信——这在微服务场景中可能会根据Pod的放置位置引发问题。在某些情况下，工作节点可能都能与控制平面通信，但仍然无法相互通信，这可能会导致Kubernetes覆盖网络出现问题。
- en: As with general unreliability, latency can also cause many of the same problems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与一般的不可靠性一样，延迟也会引发许多相同的问题。
- en: Latency is zero
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟为零。
- en: If network latency is significant, many of the same failures as with network
    unreliability will also apply. For instance, calls between kubelets and the control
    plane may fail, leading to periods of inaccuracy in `etcd` because the control
    plane may not be able to contact the kubelets – or properly update `etcd`. Similarly,
    requests could be lost between applications running on worker Nodes that would
    otherwise work perfectly if the applications were collocated on the same Node.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络延迟较大，许多与网络不可靠性相关的失败也可能会发生。例如，kubelet与控制平面之间的调用可能会失败，导致`etcd`中的数据不准确，因为控制平面可能无法联系到kubelet，或者无法正确更新`etcd`。类似地，在工作节点上运行的应用程序之间的请求可能会丢失，而如果这些应用程序运行在同一个节点上，它们本来是可以正常工作的。
- en: Bandwidth is infinite
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带宽是无限的。
- en: Bandwidth limitations can expose similar issues as with the previous two fallacies.
    Kubernetes does not currently have a fully supported method to place Pods based
    on bandwidth subscription. This means that Nodes that are hitting their network
    bandwidth limits can still have new Pods scheduled to them, causing increased
    failure rates and latency issues for requests. There have been requests to add
    this as a core Kubernetes scheduling feature (basically, a way to schedule on
    Node bandwidth consumption as with CPU and memory), but for now, the solutions
    are mostly restricted to **Container Network Interface** (**CNI**) plugins.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽限制可能会暴露与前两个谬误类似的问题。Kubernetes目前没有一个完全支持的方法来基于带宽订阅来放置Pods。这意味着那些达到网络带宽限制的节点仍然可以被调度新的Pod，导致请求的失败率和延迟问题增加。曾有请求将其作为Kubernetes调度的核心特性（基本上，是一种像CPU和内存一样，基于节点带宽消耗进行调度的方式），但目前的解决方案主要还是局限于**容器网络接口**（**CNI**）插件。
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For instance, the CNI bandwidth plugin supports traffic shaping at the Pod level
    – see [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CNI带宽插件支持在Pod级别进行流量整形 – 请参见[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)。
- en: Third-party Kubernetes networking implementations may also provide additional
    features around bandwidth – and many are compatible with the CNI bandwidth plugin.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方Kubernetes网络实现也可能提供与带宽相关的附加功能——并且许多与CNI带宽插件兼容。
- en: The network is secure
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络是安全的
- en: Network security has effects that reach far beyond Kubernetes – as any insecure
    network is privy to a whole class of attacks. Attackers may be able to gain SSH
    access to the master or worker Nodes in a Kubernetes cluster, which can cause
    significant breaches. Since so much of Kubernetes' magic happens over the network
    rather than in a single machine, access to the network is doubly problematic in
    an attack situation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全的影响远超Kubernetes本身——任何不安全的网络都容易受到一类攻击的威胁。攻击者可能通过SSH访问Kubernetes集群中的主节点或工作节点，这可能导致重大的安全漏洞。由于Kubernetes的许多“魔力”发生在网络上，而非单一机器中，因此在攻击情况下，访问网络会带来双重问题。
- en: The topology doesn't change
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拓扑不会改变
- en: This fallacy is extra relevant in the context of Kubernetes, since not only
    can the meta network topology change with new Nodes being added and removed –
    the overlay network topology is also altered directly by the Kubernetes control
    plane and CNI.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个谬误在Kubernetes环境中尤其相关，因为不仅元网络拓扑会因新增或移除节点而变化——覆盖网络拓扑也会直接受到Kubernetes控制平面和CNI的影响。
- en: For this reason, an application that is running in one logical location at one
    moment may be running in a completely different spot in the network. For this
    reason, the use of Pod IPs to identify logical applications is a bad idea – this
    is one of the purposes of the Service abstraction (see [*Chapter 5*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127),
    *Service and Ingress* – *Communicating with the outside world*). Any application
    concerns that do not assume an indefinite topology (at least concerning IPs) within
    the cluster may have issues. As an example, routing applications to a specific
    Pod IP only works until something happens to that Pod. If that Pod shuts down,
    the Deployment (for instance) controlling it will start a new Pod to replace it,
    but the IP will be completely different. A cluster DNS (and by extension, Services)
    offers a much better way to make requests between applications in a cluster, unless
    your application has the capability to adjust on the fly to cluster changes such
    as Pod placements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在某一时刻运行在一个逻辑位置的应用程序，可能会在网络中的完全不同位置运行。因此，使用Pod IP来标识逻辑应用程序是一个不好的主意 – 这是Service抽象的目的之一（请参见[*第5章*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127)，*Service和Ingress*
    – *与外部世界通信*）。任何没有假设集群内拓扑是无限的应用程序（至少是关于IP的）都可能会遇到问题。例如，将流量路由到特定Pod IP仅在该Pod存在时有效。如果该Pod关闭，控制它的Deployment（例如）将启动一个新的Pod来替代它，但IP会完全不同。集群DNS（以及扩展的Services）提供了一种更好的方式来在集群内的应用程序之间发起请求，除非你的应用程序有能力即时适应集群变化，比如Pod的位置调整。
- en: There is only one administrator
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 只有一个管理员
- en: Multiple administrators and conflicting rules can cause issues in the base network,
    and multiple Kubernetes administrators can cause further issues by changing resource
    configurations such as Pod resource limits, leading to unintended behavior. Use
    of Kubernetes **Role-Based Access Control** (**RBAC**) capabilities can help address
    this by giving Kubernetes users only the permissions they need (read-only, for
    instance).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多个管理员和冲突的规则可能会导致基础网络的问题，而多个 Kubernetes 管理员通过更改资源配置（例如 Pod 资源限制）可能会导致进一步的问题，从而导致意外行为。使用
    Kubernetes **基于角色的访问控制**（**RBAC**）功能可以通过仅授予 Kubernetes 用户所需的权限（例如只读权限）来帮助解决这个问题。
- en: Transport cost is zero
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传输成本为零
- en: There are two common ways this fallacy is interpreted. Firstly, that the latency
    cost of transport is zero – which is obviously untrue, as the speed of data transfer
    over wires is not infinite, and lower-level networking concerns add latency. This
    is essentially identical to the effects stemming from the *latency is zero* fallacy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种谬误有两种常见的解释方式。首先，认为传输的延迟成本为零——显然这是不正确的，因为通过线路传输数据的速度不是无限的，且低层次的网络问题会增加延迟。这本质上与*延迟为零*的谬误所带来的影响是相同的。
- en: Secondly, this statement can be interpreted to mean that the cost of creating
    and operating a network for the purposes of transport is zero – as in zero dollars
    and zero cents. While also being patently untrue (just look at your cloud provider's
    data transfer fees for proof of this), this does not specifically correspond to
    application troubleshooting on Kubernetes, so we will focus on the first interpretation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这个说法可以解释为：创建和运营网络用于传输的成本为零——即零美元零分。虽然这显然也是不正确的（只需要看看你的云服务提供商的数据传输费用就能证明这一点），但这与
    Kubernetes 上应用程序故障排除并没有直接关联，因此我们将重点讨论第一种解释。
- en: The network is homogeneous
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络是同质的
- en: This final fallacy has less to do with Kubernetes' components, and more to do
    with applications running on Kubernetes. However, the fact is that developers
    operating in today's environment are well aware that application networking may
    have different implementations across applications – from HTTP 1 and 2 to protocols
    such as *gRPC*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终的谬误与 Kubernetes 的组件关系较小，而与运行在 Kubernetes 上的应用程序更相关。然而，事实上，今天在这种环境中操作的开发人员都很清楚，应用程序的网络实现可能在不同应用之间有所不同——从
    HTTP 1 和 2 到像*gRPC* 这样的协议。
- en: Now that we've reviewed some major reasons for application failure on Kubernetes,
    we can dive into the actual process of troubleshooting both Kubernetes and applications
    that run on Kubernetes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了一些导致 Kubernetes 上应用程序失败的主要原因，我们可以深入实际的故障排除过程，排除 Kubernetes 本身以及运行在
    Kubernetes 上的应用程序的问题。
- en: Troubleshooting Kubernetes clusters
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 集群故障排除
- en: Since Kubernetes is a distributed system that has been designed to tolerate
    failure where applications are run, most (but not all) issues tend to be centered
    on the control plane and API. A worker Node failing, in most scenarios, will just
    result in the Pods being rescheduled to another Node – though compounding factors
    can introduce issues.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kubernetes 是一个分布式系统，设计时就考虑了容忍应用运行故障的情况，大多数（但不是全部）问题往往集中在控制平面和 API 上。工作节点失败，在大多数场景下，只会导致
    Pods 被重新调度到另一个节点——尽管复合因素可能会引发问题。
- en: In order to walk through common Kubernetes cluster issue scenarios, we will
    use a case study methodology. This should give you all the tools you need to investigate
    real-world cluster issues. Our first case study is centered on the failure of
    the API server itself.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了走过常见的 Kubernetes 集群问题场景，我们将使用案例研究方法。这将为你提供解决现实世界集群问题所需的所有工具。我们的第一个案例研究聚焦于
    API 服务器本身的故障。
- en: Important note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the purposes of this tutorial, we will assume a self-managed cluster. Managed
    Kubernetes services such as EKS, AKS, and GKE generally remove some of the failure
    domains (by autoscaling and managing master Nodes, for instance). A good rule
    is to check your managed service documentation first, as any issues may be specific
    to the implementation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程的目的，我们将假设一个自我管理的集群。像 EKS、AKS 和 GKE 这样的托管 Kubernetes 服务通常会去除一些故障域（例如通过自动扩展和管理主节点）。一个好的规则是首先检查托管服务的文档，因为任何问题可能都是实现特定的。
- en: Case study – Kubernetes Pod placement failure
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究——Kubernetes Pod 安排失败
- en: 'Let''s set the scene. Your cluster is up and running, but you are experiencing
    a problem with Pod scheduling. Pods stay stuck in the `Pending` state indefinitely.
    Let''s confirm this with the command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设定一下场景。你的集群已经启动并运行，但你遇到了 Pod 调度的问题。Pods 一直停留在 `Pending` 状态，无法调度。让我们通过以下命令来确认这一点：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the command is the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As we can see, none of our Pods are running. Furthermore, we''re running three
    replicas of the application and none of them are getting scheduled. A great next
    step would be to check the Node state and see if there are any issues there. Run
    the following command to get the output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，没有任何 Pods 在运行。此外，我们运行了三个副本的应用程序，但没有一个被调度。接下来的好步骤是检查节点状态，看看是否存在任何问题。运行以下命令以获取输出：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This output gives us some good information – we only have one worker Node, and
    it isn't available for scheduling. When a `get` command doesn't give us enough
    information to go by, `describe` is usually a good next step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出给了我们一些有用的信息——我们只有一个工作节点，并且该节点无法进行调度。当 `get` 命令没有提供足够的信息时，`describe` 通常是一个不错的后续步骤。
- en: 'Let''s run `kubectl describe node node-01` and check the `conditions` key.
    We''ve dropped a column in order to fit everything neatly on the page, but the
    most important columns are there:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行 `kubectl describe node node-01` 并检查 `conditions` 键。为了让所有内容整齐地显示在页面上，我们删除了一列，但最重要的列依然在：
- en: '![Figure 10.1 – Describe Node Conditions output](img/B14790_10_001.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 描述节点条件输出](img/B14790_10_001.jpg)'
- en: Figure 10.1 – Describe Node Conditions output
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 描述节点条件输出
- en: 'What we have here is an interesting split: both `MemoryPressure` and `DiskPressure`
    are fine, while the `OutOfDisk` and `Ready` conditions are unknown with the message
    `kubelet stopped posting node status`. At a first glance this seems nonsensical
    – how can `MemoryPressure` and `DiskPressure` be fine while the kubelet stopped
    working?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里遇到一个有趣的分歧：`MemoryPressure` 和 `DiskPressure` 一切正常，而 `OutOfDisk` 和 `Ready`
    状态未知，消息显示为 `kubelet 停止发布节点状态`。乍一看，这似乎不合常理——怎么会在 `MemoryPressure` 和 `DiskPressure`
    一切正常的情况下，kubelet 停止工作呢？
- en: The important part is in the `LastTransitionTime` column. The kubelet's most
    recent memory- and disk-specific communication sent positive statuses. Then, at
    a later time, the kubelet stopped posting its Node status, leading to `Unknown`
    statuses for the `OutOfDisk` and `Ready` conditions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要部分在 `LastTransitionTime` 列中。kubelet 最近的内存和磁盘相关通信发送了正面的状态。然后，稍后 kubelet 停止发布其节点状态，导致
    `OutOfDisk` 和 `Ready` 状态为 `Unknown`。
- en: At this point, we're certain that our Node is the problem – the kubelet is no
    longer sending the Node status to the control plane. However, we don't know why
    this occurred. It could be a network error, a problem with the machine itself,
    or something more specific. We'll need to dig further to figure it out.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们确信问题出在我们的节点上——kubelet 不再将节点状态发送到控制平面。然而，我们不知道发生了什么。可能是网络错误，机器本身的问题，或者更具体的原因。我们需要进一步挖掘来找出问题所在。
- en: A good next step here is to get closer to our malfunctioning Node, as we can
    reasonably assume that it is encountering some sort of issue. If you have access
    to the `node-01` VM or machine, now is a great time to SSH into it. Once we are
    in the machine, let's start troubleshooting further.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个好步骤是更接近我们故障的节点，因为我们可以合理地假设它遇到了一些问题。如果你可以访问 `node-01` 的虚拟机或机器，现在是通过 SSH
    登录它的好时机。进入机器后，我们可以进一步进行故障排除。
- en: 'First, let''s check whether the Node can access the control plane over the
    network. If not, this is an obvious reason why the kubelet wouldn''t be able to
    post statuses. Let''s assume a scenario where our cluster control plane (for instance
    an on-premise load balancer) is available at `10.231.0.1`. In order to check whether
    our Node can access the Kubernetes API server, we can ping the control plane as
    follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查节点是否能通过网络访问控制平面。如果不能访问，这是 kubelet 无法发布状态的明显原因。假设我们的集群控制平面（例如本地负载均衡器）可通过
    `10.231.0.1` 访问。为了检查节点是否可以访问 Kubernetes API 服务器，我们可以通过如下命令 ping 控制平面：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Important note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In order to find the control plane IP or DNS, please check your cluster configuration.
    In a managed Kubernetes service such as AWS Elastic Kubernetes Service or Azure
    AKS, this will likely be available to view in the console. If you bootstrapped
    your own cluster using kubeadm, for instance, this is a value that you provided
    during the setup as part of the installation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 若要找到控制平面的 IP 或 DNS，请检查您的集群配置。在像 AWS Elastic Kubernetes Service 或 Azure AKS 这样的托管
    Kubernetes 服务中，您可能可以在控制台查看到该信息。如果您使用 kubeadm 自行引导了集群，例如，这个值就是您在安装过程中提供的。
- en: 'Let''s check the results:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查结果：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That confirms it – our Node can indeed talk to the Kubernetes control plane.
    So, the network isn't the issue. Next, let's check the actual kubelet service.
    The Node itself seems to be operational, and the network is fine, so logically,
    the kubelet is the next thing to check.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这确认了——我们的节点确实能够与 Kubernetes 控制平面通信。所以，网络不是问题。接下来，让我们检查实际的 kubelet 服务。节点本身似乎正常运行，网络也没问题，所以合乎逻辑，下一步应该检查
    kubelet。
- en: Kubernetes components run as system services on Linux Nodes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 组件在 Linux 节点上作为系统服务运行。
- en: Important note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: On Windows Nodes, the troubleshooting instructions will be slightly different
    – see the Kubernetes documentation for more information ([https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 节点上，故障排除步骤会略有不同——有关更多信息，请参阅 Kubernetes 文档 ([https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/))。
- en: 'In order to find out the status of our `kubelet` service, we can run the following
    command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 若要查看我们 `kubelet` 服务的状态，可以运行以下命令：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives us the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Looks like our kubelet is currently not running – it exited with a failure.
    This explains everything we've seen as far as cluster status and Pod issues.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的 kubelet 当前没有在运行——它以失败状态退出了。这解释了我们在集群状态和 Pod 问题中看到的所有情况。
- en: 'To actually fix the issue, we can first try to restart the `kubelet` using
    the command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们可以首先尝试使用以下命令重启 `kubelet`：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s re-check the status of our `kubelet` with our status command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用状态命令重新检查 `kubelet` 的状态：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It looks like the `kubelet` failed again. We're going to need to source some
    additional information about the failure mode in order to find out what happened.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 `kubelet` 又失败了。我们需要获取更多关于失败模式的信息，以找出发生了什么。
- en: 'Let''s use the `journalctl` command to find out if there are any relevant logs:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `journalctl` 命令查找是否有相关日志：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output should show us logs of the `kubelet` service where a failure occurred:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该会显示 `kubelet` 服务在失败时的日志：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Looks like we've found the cause – Kubernetes does not support running on Linux
    machines with `swap` set to `on` by default. Our only choices here are either
    disabling `swap` or restarting the `kubelet` with the `--fail-swap-on` flag set
    to `false`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们已经找到了问题所在——Kubernetes 不支持在默认情况下 `swap` 设置为 `on` 的 Linux 机器上运行。我们唯一的选择是禁用
    `swap` 或者使用 `--fail-swap-on` 标志将 `kubelet` 重启并设置为 `false`。
- en: 'In our case, we''ll just change the `swap` setting by using the following command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将通过以下命令更改 `swap` 设置：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, restart the `kubelet` service:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重启 `kubelet` 服务：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, let''s check to see if our fix worked. Check the Nodes using the following
    command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查一下我们的修复是否有效。使用以下命令检查节点：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This should show output similar to the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示类似如下的输出：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our Node is finally posting a `Ready` status!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的节点终于显示 `Ready` 状态了！
- en: 'Let''s check on our Pod with the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令检查我们的 Pod：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This should show output like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示如下输出：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Success! Our cluster is healthy, and our Pods are running.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的集群健康，Pod 正在运行。
- en: Next, let's look at how to troubleshoot applications on Kubernetes once any
    cluster issues are sorted out.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在解决任何集群问题后，让我们看看如何在 Kubernetes 上进行应用故障排除。
- en: Troubleshooting applications on Kubernetes
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 应用故障排除
- en: A perfectly running Kubernetes cluster may still have application issues to
    debug. These could be due to bugs in the application itself, or due to misconfigurations
    in the Kubernetes resources that make up the application. As with troubleshooting
    the cluster, we will dive into these concepts by using a case study.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美运行的 Kubernetes 集群仍然可能存在应用程序问题。这些问题可能是由于应用程序本身的 bug，或者由于构成应用程序的 Kubernetes
    资源配置错误。与集群故障排查一样，我们将通过一个案例研究来深入探讨这些概念。
- en: Case study 1 – Service not responding
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究 1 – Service 无响应
- en: We're going to break this section down into troubleshooting at various levels
    of the Kubernetes stack, starting with higher-level components, then ending with
    a deep dive into Pod and container debugging.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这一部分分解为多个层次的 Kubernetes 堆栈故障排查，首先从更高层次的组件开始，然后深入到 Pod 和容器的调试。
- en: Let's assume that we have configured our application `app-1` to respond to requests
    via a `NodePort` Service, on port `32688`. The application listens on port `80`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经配置了应用程序 `app-1`，通过 `NodePort` Service 来响应请求，端口为 `32688`，而应用程序监听的端口是 `80`。
- en: 'We can try to access our application via a `curl` request on one of our Nodes.
    The command will look as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试通过一个 `curl` 请求访问我们的应用程序，命令如下：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the `curl` command if it fails will look like the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `curl` 命令失败，输出将如下所示：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'At this point, our `NodePort` Service isn''t routing requests to any Pod. Following
    our typical debug path, let''s first see which resources are running in the cluster
    with the following command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们的 `NodePort` Service 并未将请求路由到任何 Pod。按照我们的典型调试路径，首先让我们查看集群中运行了哪些资源，使用以下命令：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Add the `-o` wide flag to see additional information. Next, run the following
    command:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 `-o` wide 标志以查看额外信息。接着，运行以下命令：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This gives us the following output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It is clear that our Service exists with a proper Node port – but our requests
    are not being routed to the Pods, as is obvious from the failed `curl` command.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们的 Service 存在并且配置了正确的 Node port，但正如从失败的 `curl` 命令中看出，我们的请求并没有被路由到 Pods。
- en: 'To see which routes our Service has set up, let''s use the `get endpoints`
    command. This will list the Pod IPs, if any, for the Service as configured:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看我们的 Service 设置了哪些路由，我们可以使用 `get endpoints` 命令。这个命令将列出该 Service 的 Pod IP（如果有的话）：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s check the resulting output of the command:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查命令的结果输出：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Well, something is definitely wrong here.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，显然这里出了点问题。
- en: Our Service isn't pointing to any Pods. This likely means that there aren't
    any Pods matching our Service selector available. This could be because there
    are no Pods available at all – or because those Pods don't properly match the
    Service selector.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Service 没有指向任何 Pod。这很可能意味着没有任何 Pod 匹配我们的 Service 选择器。这可能是因为没有可用的 Pods，或者是因为这些
    Pods 没有正确匹配 Service 选择器。
- en: 'To check on our Service selector, let''s take the next step in the debug path
    and use the `describe` command as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的 Service 选择器，让我们按照调试路径的下一步，使用 `describe` 命令，如下所示：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This gives us an output like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出如下输出：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, our Service is configured to talk to the correct port on our
    application. However, the selector is looking for Pods that match the label `app
    = app-11`. Since we know our application is named `app-1`, this could be the cause
    of our issue.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的 Service 已经配置为与应用程序的正确端口进行通信。然而，选择器正在寻找标签为 `app = app-11` 的 Pods。由于我们知道我们的应用程序名为
    `app-1`，这可能是问题的根源。
- en: 'Let''s edit our Service to look for the correct Pod label, `app-1`, running
    another `describe` command to be sure:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编辑我们的 Service，查找正确的 Pod 标签 `app-1`，并再次运行 `describe` 命令来确保：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This gives the following output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, you can see in the output that our Service is looking for the proper Pod
    selector, but we still do not have any endpoints. Let''s check to see what is
    going on with our Pods by using the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在输出中看到我们的 Service 正在寻找正确的 Pod 选择器，但我们仍然没有任何端点。让我们通过以下命令检查一下我们的 Pods 出现了什么问题：
- en: '[PRE29]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This shows the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our Pods are still waiting to be scheduled. This explains why, even with the
    proper selector, our Service isn''t functioning. To get some granularity on why
    our Pods aren''t being scheduled, let''s use the `describe` command:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Pods 仍在等待调度。这解释了为什么即使有正确的选择器，我们的 Service 仍然无法正常工作。为了更细致地了解为什么我们的 Pods 没有被调度，我们可以使用
    `describe` 命令：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the output. Let''s focus on the `Events` section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出。我们关注 `Events` 部分：
- en: '![Figure 10.2 – Describe Pod Events output](img/B14790_10_002.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 描述Pod事件输出](img/B14790_10_002.jpg)'
- en: Figure 10.2 – Describe Pod Events output
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 描述Pod事件输出
- en: From the `Events` section, it looks like our Pod is failing to be scheduled
    due to container image pull failure. There are many possible reasons for this
    – our cluster may not have the necessary authentication mechanisms to pull from
    a private repository, for instance – but that would present a different error
    message.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Events`部分来看，我们的Pod未能成功调度，原因是容器镜像拉取失败。这可能有很多原因——例如，我们的集群可能没有从私有仓库拉取镜像所需的认证机制——但这种情况通常会显示为不同的错误信息。
- en: From the context and the `Events` output, we can probably assume that the issue
    is that our Pod definition is looking for a container named `myappimage:lates`
    instead of `myappimage:latest`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从上下文和`Events`输出来看，我们大概可以推测出问题所在：我们的Pod定义正在寻找名为`myappimage:lates`的容器，而不是`myappimage:latest`。
- en: Let's update our Deployment spec with the proper image name and roll out the
    update.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用正确的镜像名称更新我们的Deployment规范，并推出更新。
- en: 'Use the following command to get confirmation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令来确认：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output looks like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Our Pods are now running – let''s check to see that our Service has registered
    the proper endpoints. Use the following command to do this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Pods现在正在运行——让我们检查一下Service是否已注册正确的端点。使用以下命令来完成此操作：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output should look like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Success! Our Service is properly pointing to our application Pods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的Service已正确指向应用程序Pods。
- en: In the next case study, we'll dig a bit deeper by troubleshooting a Pod with
    incorrect startup parameters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的案例研究中，我们将通过故障排除启动参数不正确的Pod，深入分析。
- en: Case study 2 – Incorrect Pod startup command
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究2 – 错误的Pod启动命令
- en: Let's assume we have our Service properly configured and our Pods running and
    passing health checks. However, our Pod is not responding to requests as we would
    expect. We are sure that this is less of a Kubernetes problem and more of an application
    or configuration problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的Service已正确配置，Pods也在运行并通过了健康检查。然而，我们的Pod没有按照预期响应请求。我们确信这不是Kubernetes的问题，更可能是应用程序或配置的问题。
- en: 'Our application container works as follows: it takes a startup command with
    a flag for `color` and combines it with a variable for `version number` based
    on the container''s `image` tag, and echoes that back to the requester. We are
    expecting our application to return `green 3`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用容器工作原理如下：它接收一个启动命令，带有一个`color`标志，并将其与基于容器`image`标签的`version number`变量结合，然后将结果返回给请求者。我们希望应用程序返回`green
    3`。
- en: Thankfully, Kubernetes gives us some good tools to debug applications, which
    we can use to delve into our specific containers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes为我们提供了一些很好的工具来调试应用程序，我们可以使用这些工具深入分析具体的容器。
- en: 'First, let''s `curl` the application to see what response we get:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们`curl`应用程序，查看得到什么响应：
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We expected `green 3` but got `red 2`, so it looks like something is wrong with
    the input, and the version number variable. Let's start with the former.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本来期望`green 3`，但得到了`red 2`，所以看起来输入或者版本号变量出了问题。我们先从前者开始。
- en: 'As usual, we begin with checking our Pods with the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们通过以下命令来检查我们的Pods：
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output should look like the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE38]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Everything looks good in this output. It seems that our app is running as part
    of a Deployment (and therefore, a ReplicaSet) – we can make sure by running the
    following command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出看起来一切正常。似乎我们的应用程序作为Deployment的一部分（因此也是ReplicaSet的一部分）在运行——我们可以通过运行以下命令来确认：
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output should look like the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE40]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s look a bit closer at our Deployment to see how our Pods are configured
    using the following command:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下命令更仔细地查看Deployment，看看Pods是如何配置的：
- en: '[PRE41]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output looks like the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来如下所示：
- en: Broken-deployment-output.yaml
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: broken-deployment-output.yaml
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Let's see if we can fix our issue, which is really quite simple. We're using
    the wrong version of our application, and our startup command is wrong. In this
    case, let's assume we don't have a file with our Deployment spec – so let's just
    edit it in place.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否能修复这个问题，其实这非常简单。我们使用了错误版本的应用程序，而且启动命令也错了。在这种情况下，假设我们没有包含Deployment规范的文件——那么我们就直接在原地编辑它。
- en: 'Let''s use `kubectl edit deployment app-1-pod`, and edit the Pod spec to the
    following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`kubectl edit deployment app-1-pod`，并将Pod规范编辑为如下：
- en: fixed-deployment-output.yaml
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: fixed-deployment-output.yaml
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the Deployment is saved, you should start seeing your new Pods come up.
    Let''s double-check by using the following command:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Deployment被保存，你应该开始看到新的Pod启动。让我们通过以下命令再确认一下：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output should look like the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And finally – let''s make a `curl` request to check that everything is working:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后——让我们发起一个`curl`请求，检查一切是否正常：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output of the command is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Success!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！
- en: Case study 3 – Pod application malfunction with logs
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究 3 —— Pod应用程序故障与日志
- en: After spending the previous chapter, [*Chapter 9*](B14790_9_Final_PG_ePub.xhtml#_idTextAnchor212),
    *Observability on Kubernetes*, implementing observability to our applications,
    let's take a look at a case where those tools can really come in handy. We will
    use manual `kubectl` commands for the purposes of this case study – but know that
    by aggregating logs (for instance, in our EFK stack implementation), we could
    make the process of debugging this application significantly easier.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节[*第9章*](B14790_9_Final_PG_ePub.xhtml#_idTextAnchor212)，*Kubernetes上的可观测性*，我们实现了对应用程序的可观测性，接下来让我们看看这些工具如何真正派上用场。我们将在此案例研究中使用手动的`kubectl`命令——但要知道，通过聚合日志（例如，在我们的EFK堆栈实现中），我们可以显著简化调试过程。
- en: 'In this case study, we once again have a deployment of Pods – to check it,
    let''s run the following command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在此案例研究中，我们再次有一个Pod部署——要检查它，运行以下命令：
- en: '[PRE48]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output of the command is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE49]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: It looks like, in this case, we are working with a StatefulSet instead of a
    Deployment – a key characteristic here is the incrementing Pod IDs starting from
    0.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，在这种情况下，我们正在使用StatefulSet而不是Deployment——这里的一个关键特征是Pod ID从0开始递增。
- en: 'We can confirm this by checking for StatefulSets using the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下命令确认这一点，检查StatefulSet：
- en: '[PRE50]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output of the command is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Let's take a closer look at our StatefulSet with `kubectl get statefulset -o
    yaml app-2-ss`. By using the `get` command along with `-o yaml` we can get our
    `describe` output in the same format as the typical Kubernetes resource YAML.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行`kubectl get statefulset -o yaml app-2-ss`来仔细查看我们的StatefulSet。通过使用`get`命令并加上`-o
    yaml`，我们可以以典型的Kubernetes资源YAML格式获取`describe`输出。
- en: 'The output of the preceding command is as follows. We''ve removed the Pod spec
    section to keep it shorter:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下。我们删除了Pod spec部分，以便保持简洁：
- en: statefulset-output.yaml
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: statefulset-output.yaml
- en: '[PRE52]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We know that our app is using a service. Let's see which one it is!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的应用程序正在使用一个服务。让我们看看它是哪个服务！
- en: 'Run `kubectl get services -o wide`. The output should be something like the
    following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`kubectl get services -o wide`。输出应如下所示：
- en: '[PRE53]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'It''s clear that our service is called `app-2-svc`. Let''s see our exact service
    definition using the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们的服务名为`app-2-svc`。让我们使用以下命令查看我们的精确服务定义：
- en: '[PRE54]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To see exactly what our application is returning for a given input, we can
    use `curl` on our `NodePort` Service:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确查看我们的应用程序在给定输入下的返回情况，我们可以在`NodePort`服务上使用`curl`：
- en: '[PRE56]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Based on our existing knowledge of the application, we would assume that this
    call should return `2`, not `3`. The application developer on our team has asked
    us to investigate any logging output that would help them figure out what the
    issue is.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们对应用程序的现有了解，我们会假设此调用应返回`2`，而不是`3`。我们团队的应用程序开发人员要求我们调查任何日志输出，以帮助他们找出问题所在。
- en: 'We know from previous chapters that you can investigate the logging output
    with `kubectl logs <pod name>`. In our case, we have three replicas of our application,
    so we may not be able to find our logs in a single iteration of this command.
    Let''s pick a Pod at random and see if it was the one that served our request:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前面的章节中知道，你可以通过`kubectl logs <pod name>`来查看日志输出。在我们的案例中，我们有三个副本的应用程序，所以可能无法通过这条命令的单次迭代找到日志。让我们随机选一个Pod，看看它是否处理了我们的请求：
- en: '[PRE57]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: It looks like this was not the Pod that served our request, as our application
    developer has told us that the application definitely logs to `stdout` when a
    `GET` request is made to the server.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这不是处理我们请求的Pod，因为我们的应用程序开发人员告诉我们，应用程序在收到`GET`请求时肯定会记录到`stdout`。
- en: 'Instead of checking through the other two Pods individually, we can use a joint
    command to get logs from all three Pods. The command will be as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个联合命令从所有三个Pod中获取日志，而不是单独检查其他两个Pod。该命令如下所示：
- en: '[PRE58]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'And the output is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE59]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: That did the trick – and what's more, we can see some good insight into our
    issue.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 成功了——更重要的是，我们对问题有了一些好的见解。
- en: Everything seems as we would expect, other than the log line reading `Second
    Number`. Our request clearly used `1plus1` as the query string, which would make
    both the first number and the second number (split by the operator value) equal
    to one.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了日志行显示 `Second Number` 外，一切看起来都如我们预期。我们的请求明显使用了 `1plus1` 作为查询字符串，这将使第一个数字和第二个数字（由操作符值分割）都等于
    1。
- en: This will take some additional digging. We could triage this issue by sending
    additional requests and checking the output in order to guess what is happening,
    but in this case it may be better to just get bash access to the Pod and figure
    out what is going on.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一些额外的挖掘。我们可以通过发送额外的请求并检查输出以猜测发生了什么来进行问题诊断，但在这种情况下，最好是直接获得 Pod 的 bash 访问权限，弄清楚发生了什么。
- en: 'First, let''s check our Pod spec, which was removed from the preceding StatefulSet
    YAML. To see the full StatefulSet spec, check the GitHub repository:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查我们的 Pod 规格，该规格在之前的 StatefulSet YAML 中已被删除。要查看完整的 StatefulSet 规格，请访问
    GitHub 仓库：
- en: Statefulset-output.yaml
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Statefulset-output.yaml
- en: '[PRE60]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: It looks like our Pod is mounting an empty volume as a scratch disk. It also
    has two containers in each Pod – a sidecar used for application tracing, and our
    app itself. We'll need this information to `ssh` into one of the Pods (it doesn't
    matter which one for this exercise) using the `kubectl exec` command.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的 Pod 正在挂载一个空的卷作为临时磁盘。每个 Pod 中还包含两个容器——一个用于应用追踪的 sidecar 和我们的应用程序本身。我们需要这些信息来通过`ssh`进入其中一个
    Pod（对于本练习来说，哪个 Pod 无关紧要），使用 `kubectl exec` 命令。
- en: 'We can do it using the following command:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来做到这一点：
- en: '[PRE61]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This command should give you a bash terminal as the output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令应该会给你一个 bash 终端作为输出：
- en: '[PRE62]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, using the terminal we just created, we should be able to investigate our
    application code. For the purposes of this tutorial, we are using a highly simplified
    Node.js application.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用我们刚刚创建的终端，我们应该能够调查我们的应用程序代码。为了本教程的目的，我们使用了一个高度简化的 Node.js 应用程序。
- en: 'Let''s check our Pod filesystem to see what we''re working with using the following
    command:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的 Pod 文件系统，看看我们正在使用什么，通过以下命令：
- en: '[PRE63]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Looks like we have two JavaScript files, and our previously mentioned `scratch`
    folder. It's probably a good bet to assume that `app.js` contains the logic for
    bootstrapping and serving the application, and `calculate.js` contains our controller
    code for doing the calculations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们有两个 JavaScript 文件，以及我们之前提到的 `scratch` 文件夹。可以合理推测，`app.js` 包含启动和服务应用程序的逻辑，而
    `calculate.js` 包含我们的控制器代码，用于进行计算。
- en: 'We can confirm by printing the contents of the `calculate.js` file:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过打印`calculate.js`文件的内容来确认：
- en: Broken-calculate.js
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Broken-calculate.js
- en: '[PRE64]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Even with little to no knowledge of JavaScript, it's pretty obvious what the
    issue is here. The code is incrementing the `second` variable before performing
    the calculation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对 JavaScript 知识了解甚少，问题也非常明显。代码在执行计算之前已经递增了 `second` 变量。
- en: 'Since we''re inside of the Pod, and we''re using a non-compiled language, we
    can actually edit this file inline! Let''s use `vi` (or any text editor) to correct
    this file:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经进入了 Pod，并且我们使用的是一种非编译语言，我们实际上可以直接在线编辑这个文件！让我们使用 `vi`（或任何文本编辑器）来修正这个文件：
- en: '[PRE65]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'And edit the file to read as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 并编辑文件，内容如下：
- en: fixed-calculate.js
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: fixed-calculate.js
- en: '[PRE66]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now, our code should run properly. It's important to state that this fix is
    only temporary. As soon as our Pod shuts down or gets replaced by another Pod,
    it will revert to the code that was originally included in the container image.
    However, this pattern does allow us to try out quick fixes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的代码应该能够正常运行。重要的是要声明，这个修复是临时的。一旦我们的 Pod 关闭或被另一个 Pod 替换，它将恢复为原本包含在容器镜像中的代码。然而，这种模式确实允许我们尝试快速修复。
- en: 'After exiting the `exec` session using the `exit` bash command, let''s try
    our URL again:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `exit` bash 命令退出 `exec` 会话后，让我们再次尝试我们的 URL：
- en: '[PRE67]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As you can see, our hotfixed container shows the right result! Now, we can update
    our code and Docker image in a more permanent way with our fix. Using `exec` is
    a great way to troubleshoot and debug running containers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的热修复容器显示了正确的结果！现在，我们可以通过修复以更永久的方式更新我们的代码和 Docker 镜像。使用`exec`是排查和调试正在运行的容器的好方法。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about troubleshooting applications on Kubernetes.
    First, we covered some common failure modes of distributed applications. Then,
    we learned how to triage issues with Kubernetes components. Finally, we reviewed
    several scenarios where Kubernetes configuration and application debugging were
    performed. The Kubernetes debugging and troubleshooting techniques you learned
    in this chapter will help you when triaging issues with any Kubernetes clusters
    and applications you may work on.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在 Kubernetes 上排查应用程序的故障。首先，我们涵盖了分布式应用程序的一些常见故障模式。然后，我们学习了如何对 Kubernetes
    组件进行问题分类。最后，我们回顾了几种 Kubernetes 配置和应用程序调试的场景。本章学到的 Kubernetes 调试和故障排除技术将在您处理可能涉及的任何
    Kubernetes 集群和应用程序问题时提供帮助。
- en: In the next chapter, [*Chapter 11*](B14790_11_Final_PG_ePub.xhtml#_idTextAnchor251),
    *Template Code Generation and CI/CD on Kubernetes*, we will look into some ecosystem
    extensions for templating Kubernetes resource manifests and continuous integration/continuous
    deployment with Kubernetes.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[*第11章*](B14790_11_Final_PG_ePub.xhtml#_idTextAnchor251)，*Kubernetes 上的模板代码生成与持续集成/持续部署*，我们将探讨一些用于模板化
    Kubernetes 资源清单和使用 Kubernetes 进行持续集成/持续部署的生态系统扩展。
- en: Questions
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How does the distributed systems fallacy, "*the topology doesn't change*," apply
    to applications on Kubernetes?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式系统谬论 "*拓扑结构不会改变*" 如何适用于运行在 Kubernetes 上的应用程序？
- en: How are the Kubernetes control plane components (and kubelet) implemented at
    the OS level?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面组件（和 kubelet）如何在操作系统级别实现？
- en: How would you go about debugging an issue where Pods are stuck in the `Pending`
    status? What would be your first step? And your second?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理 Pods 处于 `Pending` 状态的问题时，您会如何进行调试？您的第一步是什么？第二步呢？
- en: Further reading
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The CNI plugin for traffic shaping: [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于流量整形的 CNI 插件：[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)
