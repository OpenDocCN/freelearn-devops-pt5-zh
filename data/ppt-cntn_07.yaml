- en: Chapter 7. Container Schedulers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 容器调度器
- en: Now, we are at the business end of the book. There is a lot of buzz at the moment
    around this topic. This is where containers are going to go in future, and schedulers
    solve a lot of problems, for instance, spreading the load of our application across
    multiple hosts for us pending on load and starting our containers on another instance
    if the original host fails. In this chapter, we will look at three different schedulers.
    First, we will look at Docker Swarm. This is a Docker open source scheduler. We
    will build five servers and look at how to create a replicated master. We will
    then run a few containers and look at how Swarm will schedule them across nodes.
    The next scheduler we will look at is Docker **UCP** (**Universal Control Plane**).
    This is a Docker enterprise solution that is integrated with Docker Compose. We
    will build a three-node cluster and deploy our Consul module. As UCP has a graphical
    interface, we will look at how UCP is scheduled from there. The final scheduler
    we will look at is Kubernetes. This is Google's offering and is also open source.
    For Kubernetes, we will build a single node using containers and use Puppet to
    define the more complex types. As you can see, we are going to look at each one
    differently, as they all have their individual strengths and weaknesses. Depending
    on your use case, you might decide on one or all of them to solve a problem that
    you may face.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经进入本书的核心部分。目前，关于这个话题有很多讨论。容器的未来将会是这样的，而调度器可以解决很多问题，比如根据负载将我们的应用程序负载分配到多个主机上，以及在原始主机失败时，在另一个实例上启动容器。在这一章中，我们将介绍三种不同的调度器。首先，我们将介绍Docker
    Swarm，这是一个Docker开源调度器。我们将搭建五台服务器，并演示如何创建一个复制的主节点。然后，我们将运行几个容器，看看Swarm如何在节点之间调度它们。接下来，我们将介绍Docker
    **UCP**（**Universal Control Plane**）。这是Docker的企业解决方案，并与Docker Compose集成。我们将搭建一个三节点集群并部署我们的Consul模块。由于UCP有图形化界面，我们将从那里了解UCP的调度方式。最后，我们将介绍Kubernetes。这是谷歌提供的解决方案，也是开源的。对于Kubernetes，我们将使用容器搭建一个单节点，并利用Puppet定义更复杂的类型。正如你所看到的，我们将从不同的角度来看待每个调度器，因为它们各自有不同的优缺点。根据你的使用场景，你可能会选择其中一个或多个调度器来解决你可能遇到的问题。
- en: Docker Swarm
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: For our first scheduler, we are going to look at Docker Swarm. This is a really
    solid product and in my opinion is a bit underrated compared to Kubernetes. It
    has really come on in leaps and bounds in the last few releases. It now supports
    replicated masters, rescheduling containers on failed hosts. So, let's look at
    the architecture of what we are building. Then, we will get into the coding.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个调度器，我们将介绍Docker Swarm。这是一个非常稳定的产品，在我看来，相较于Kubernetes，它有些被低估了。在过去几次发布中，Swarm有了长足的进展。现在它支持主节点复制，并能在主机失败时重新调度容器。那么，接下来我们将看一下我们要构建的架构，之后我们会进入编程部分。
- en: The Docker Swarm architecture
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker Swarm架构
- en: In this example, we are going to build five servers, where two will be replicated
    masters and the other three nodes will be in the swarm cluster. As Docker Swarm
    needs a key/value store backend, we will use Consul. In this instance, we are
    not going to use our Consul modules; instead, we are going to use [https://forge.puppetlabs.com/KyleAnderson/consul](https://forge.puppetlabs.com/KyleAnderson/consul).
    The reason for this is that in all three examples, we are going to use different
    design choices. So, when you are trying to build a solution, you are exposed to
    more than one way to skin the cat. In this example, we are going to install Swarm
    and Consul onto the OS using Puppet and then run containers on top of it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将搭建五台服务器，其中两台将作为主节点的复制，其他三台将加入Swarm集群。由于Docker Swarm需要一个键值存储后端，我们将使用Consul。在这个示例中，我们不会使用我们的Consul模块，而是使用[https://forge.puppetlabs.com/KyleAnderson/consul](https://forge.puppetlabs.com/KyleAnderson/consul)。这样做的原因是，在这三个示例中，我们将采用不同的设计选择。因此，在构建解决方案时，你将面临多种实现方式。在这个例子中，我们将使用Puppet将Swarm和Consul安装到操作系统上，然后在其上运行容器。
- en: Coding
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编程
- en: 'In this example, we are going create a new Vagrant repo. So, we will Git clone
    [https://github.com/scotty-c/vagrant-template.git](https://github.com/scotty-c/vagrant-template.git)
    into the directory of our choice. The first thing that we will edit is the Puppetfile.
    This can be found in the root of our Vagrant repo. We will add the following changes
    to the file:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个新的Vagrant仓库。所以，我们将使用Git克隆[https://github.com/scotty-c/vagrant-template.git](https://github.com/scotty-c/vagrant-template.git)到我们选择的目录中。我们首先要编辑的是Puppetfile。它位于Vagrant仓库的根目录中。我们将向文件中添加以下更改：
- en: '![Coding](img/B05201_07_02.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_02.jpg)'
- en: The next file we will edit is `servers.yaml`. Again, this is located in the
    root of our Vagrant repo. We are going to add five servers to it. So, I will break
    down this file into five parts, one for each server.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将编辑的文件是 `servers.yaml`。同样，该文件位于我们 Vagrant 仓库的根目录下。我们将向其中添加五个服务器。因此，我会将该文件分解为五个部分，每个部分对应一个服务器。
- en: 'First, let''s look at the code for server 1:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下服务器 1 的代码：
- en: '![Coding](img/B05201_07_03.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_03.jpg)'
- en: 'Now, let''s look at the code for server 2:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看服务器 2 的代码：
- en: '![Coding](img/B05201_07_04.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_04.jpg)'
- en: 'The following screenshot shows the code for server 3:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了服务器 3 的代码：
- en: '![Coding](img/B05201_07_05.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_05.jpg)'
- en: 'The following screenshot shows the code for server 4:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了服务器 4 的代码：
- en: '![Coding](img/B05201_07_06.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_06.jpg)'
- en: 'Finally, the code for server 5 is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，服务器 5 的代码如下：
- en: '![Coding](img/B05201_07_07.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_07.jpg)'
- en: All this should look fairly familiar to you. The one call out is that we have
    used servers one through three as our cluster nodes. Four and five will be our
    masters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切对你来说应该比较熟悉。需要特别注意的是，我们已使用了服务器 1 至 3 作为我们的集群节点。4 和 5 将是我们的主节点。
- en: 'The next thing we are going to do is add some values to Hiera. This is the
    first time we have used Hiera, so let''s look at our `hiera.yaml` file to see
    our configuration in the root of our Vagrant repo, which is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要做的是向 Hiera 添加一些值。这是我们第一次使用 Hiera，所以让我们查看位于 Vagrant 仓库根目录下的 `hiera.yaml`
    文件，看看我们的配置，如下所示：
- en: '![Coding](img/B05201_07_08.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_08.jpg)'
- en: 'As you can see, we have a basic hierarchy. We just have one file to look at,
    which is our `global.yaml` file. We can see that the file lives in our `hieradata`
    folder as it is declared as our `data` directory. So, if we open the `global.yaml`
    file, we are going to add the following values to it:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有一个基本的层级结构。我们只需要查看一个文件，即我们的 `global.yaml` 文件。我们可以看到该文件位于 `hieradata`
    文件夹中，因为它被声明为我们的 `data` 目录。因此，如果我们打开 `global.yaml` 文件，我们将向其中添加以下值：
- en: '![Coding](img/B05201_07_09.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_09.jpg)'
- en: The first value will tell Swarm what version we want to use. The last value
    sets the version of Consul that we will be using, which is `0.6.3`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个值将告诉 Swarm 我们要使用哪个版本。最后一个值设置了我们将使用的 Consul 版本，即 `0.6.3`。
- en: 'The next thing we need to do is write our module that will deploy both our
    Consul and Swarm clusters. We have created quite a few modules so far in the book,
    so we won''t cover it again. In this instance, we will create a module called
    `<AUTHOR>-config`. We will then move the module into our `modules` folder at the
    root of our Vagrant repo. Now, let''s look at what we are going to add to our
    `init.pp` file:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是编写部署 Consul 和 Swarm 集群的模块。到目前为止，我们在书中已经创建了相当多的模块，因此这里不再重复讲解。我们将创建一个名为
    `<AUTHOR>-config` 的模块。然后，我们将把该模块移到 Vagrant 仓库根目录下的 `modules` 文件夹中。现在，让我们看看我们将要添加到
    `init.pp` 文件中的内容：
- en: '![Coding](img/B05201_07_10.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_10.jpg)'
- en: 'As you can see, this sets up our module. We will need to create a `.pp` file
    for each of our entries in our `init.pp` file, for example, `consul_config.pp`,
    `swarm.pp`, and so on. We have also declared one variable called `consul_ip`,
    whose value will actually come from Hiera, as that is where we set the value earlier.
    We will look at our files in alphabetical order. So, we will start with `config::compose.pp`,
    which is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这将设置我们的模块。我们需要为 `init.pp` 文件中的每个条目创建一个 `.pp` 文件，例如 `consul_config.pp`、`swarm.pp`
    等等。我们还声明了一个名为 `consul_ip` 的变量，其值实际上来自 Hiera，因为我们之前就在那里设置了这个值。我们将按字母顺序查看我们的文件。因此，我们将从
    `config::compose.pp` 开始，它如下所示：
- en: '![Coding](img/B05201_07_11.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_11.jpg)'
- en: 'In this class, we are setting up a registrator. We are going to use Docker
    Compose for this, as it is a familiar configuration and something that we have
    already covered. You will note that there is an `if` statement in the code. This
    is a logic to run the container only on a cluster member. We don''t want to run
    container applications on our masters. The next file we will look at is `consul_config.pp`,
    which is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们正在设置一个注册器。我们将使用 Docker Compose 来实现这一点，因为它是一种熟悉的配置，而且我们之前已经讲解过。你会注意到代码中有一个
    `if` 语句。这是一个逻辑，只有在集群成员上才会运行容器。我们不希望在主节点上运行容器应用程序。接下来，我们将查看的文件是 `consul_config.pp`，它如下所示：
- en: '![Coding](img/B05201_07_12.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_12.jpg)'
- en: In this class, we will configure Consul, something that we have already covered
    in this book. I always like to look at multiple ways to do the same job, as you
    never know the solution that you might need to produce tomorrow and you always
    want the right tool for the right job. So, in this instance, we will not configure
    Consul in a container but natively on the host OS.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们将配置Consul，这个内容我们在本书中已经介绍过。我总是喜欢寻找多种方法来完成同样的任务，因为你永远不知道明天可能需要的解决方案，而且你总是想为每个任务选择合适的工具。因此，在这个例子中，我们将不会在容器中配置Consul，而是直接在主机操作系统上配置。
- en: 'You can see that the code is broken into three blocks. The first block bootstraps
    our Consul cluster. You will note that the configurations are familiar, as they
    are the same as those we used to set up our container in an earlier chapter. The
    second block of code sets the members of the cluster after the cluster is bootstrapped.
    We have not seen this third block before. This sets up a service for Consul to
    monitor. This is just a taste of what we can manage, but we will look at this
    in anger in the next chapter. Let''s look at our next file, that is, `dns.pp`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，代码被分成了三个块。第一个块引导我们的Consul集群。你会注意到，配置是熟悉的，因为它们与我们在前一章节中用来设置容器的配置相同。第二个代码块在集群引导后设置集群成员。我们之前没有见过这个第三个代码块。它为Consul设置了一个监控服务。这只是我们可以管理的一小部分，但我们将在下一章深入探讨。接下来让我们看看下一个文件，即`dns.pp`：
- en: '![Coding](img/B05201_07_13.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_13.jpg)'
- en: 'You will note that this file is exactly the same as we used in our `consul`
    module. So, we can move on to the next file, which is `run_containers.pp`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，这个文件与我们在`consul`模块中使用的文件完全相同。所以，我们可以继续下一个文件，也就是`run_containers.pp`：
- en: '![Coding](img/B05201_07_14.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_14.jpg)'
- en: 'This is the class that will run our containers on our cluster. At the top,
    we are declaring that we want master number two to initiate the call to the cluster.
    We are going to deploy three container applications. The first is `jenkins`, and
    as you can see, we will expose Jenkins web port `8080` to the host that the container
    runs on. The next container is `nginx`, and we are going to forward both `80`
    and `443` to the host, but also connect `nginx` to our private network across
    our `swarm-private` cluster. To get our logs from `nginx`, we will tell our container
    to use syslog as the logging driver. The last application that we will run is
    `redis`. This will be the backend of `nginx`. You will note that we are not forwarding
    any ports, as we are hiding Redis on our internal network. Now that we have our
    containers sorted, we have one file left, `swarm.pp`. This will configure our
    Swarm cluster and internal Swarm network as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在我们的集群上运行容器的类。顶部声明了我们希望第二个主机发起对集群的调用。我们将部署三个容器应用。第一个是`jenkins`，如你所见，我们将把Jenkins的Web端口`8080`暴露给容器运行的主机。下一个容器是`nginx`，我们将同时将`80`和`443`端口转发到主机，并且将`nginx`连接到我们的私有网络，即`swarm-private`集群。为了从`nginx`获取日志，我们将告诉容器使用syslog作为日志驱动程序。我们将运行的最后一个应用是`redis`。这将作为`nginx`的后端。你会注意到，我们没有转发任何端口，因为我们将Redis隐藏在我们的内部网络中。现在我们的容器已经配置完毕，我们还有一个文件没有处理，那就是`swarm.pp`。这个文件将配置我们的Swarm集群和内部Swarm网络，如下所示：
- en: '![Coding](img/B05201_07_15.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_15.jpg)'
- en: The first resource declaration will install the Swarm binaries. The next resource
    will configure our Docker network on each host. The next `if` state will define
    if the Swarm node is a master or part of the cluster via `hostname`. As you can
    see, we are declaring a few defaults that are the first values that tell Swarm
    what backend to use Consul with. The second value tells Swarm the IP address of
    the Consul backend which is `172.17.8.101`. The third value tells Swarm about
    the port it can access Swarm on, `8500`. The fourth value tells Swarm which interface
    to advertise the cluster on, which in our case is `enp0s8`. The last value sets
    the root of the key value store Swarm will use. Now, let's create our `templates`
    folder in the root of our module.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个资源声明将安装Swarm二进制文件。下一个资源将配置每个主机上的Docker网络。接下来的`if`语句将定义Swarm节点是主机还是集群的一部分，通过`hostname`来判断。如你所见，我们声明了一些默认值，这些值是告诉Swarm使用Consul作为后端的初始值。第二个值告诉Swarm
    Consul后端的IP地址，即`172.17.8.101`。第三个值告诉Swarm它可以在`8500`端口访问Swarm。第四个值告诉Swarm在哪个接口上广播集群，在我们的情况下是`enp0s8`。最后一个值设置Swarm将使用的键值存储的根目录。现在，让我们在模块的根目录下创建`templates`文件夹。
- en: 'We will create three files there. The first file will be `consul.conf.erb`
    and the contents will be as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在那里创建三个文件。第一个文件将是 `consul.conf.erb`，其内容如下：
- en: '![Coding](img/B05201_07_16.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_16.jpg)'
- en: 'The next file will be `named.conf.erb` and its contents will be as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个文件将是 `named.conf.erb`，其内容如下：
- en: '![Coding](img/B05201_07_17.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_17.jpg)'
- en: 'The last file will be `registrator.yml.erb`, and the file''s content will be
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的文件将是 `registrator.yml.erb`，文件内容如下：
- en: '![Coding](img/B05201_07_18.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_18.jpg)'
- en: 'The next thing that we need to add is our `config` class to our `default.pp`
    manifest file in the `manifest` folder in the root of our Vagrant repo:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要在 Vagrant 仓库根目录下的 `manifest` 文件夹中的 `default.pp` 清单文件中添加我们的 `config`
    类：
- en: '![Coding](img/B05201_07_23.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_23.jpg)'
- en: 'Now, our module is complete and we are ready to run our cluster. So, let''s
    open our terminal and change the directory to the root of our Vagrant repo and
    issue the `vagrant up` command. Now, we are building five servers, so be patient.
    Once our last server is built, our terminal output should look like that shown
    in this screenshot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模块已完成，准备运行我们的集群。所以，让我们打开终端，将目录更改为 Vagrant 仓库的根目录，并输入 `vagrant up` 命令。现在，我们正在构建五个服务器，所以请耐心等待。待最后一个服务器构建完成后，终端输出应该与截图中所示的内容类似：
- en: '![Coding](img/B05201_07_19.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_19.jpg)'
- en: Terminal output
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 终端输出
- en: 'We can now look at our Consul cluster''s web UI at `127.0.0.1:9501` (remember
    that we changed our ports on our `servers.yaml` file) as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过 `127.0.0.1:9501` 查看我们的 Consul 集群的 Web 界面（记得我们在 `servers.yaml` 文件中更改了端口），如下所示：
- en: '![Coding](img/B05201_07_20.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_20.jpg)'
- en: 'Now, let''s see what host our Jenkins service is running on:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的 Jenkins 服务运行在哪个主机上：
- en: '![Coding](img/B05201_07_21.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_21.jpg)'
- en: 'In this example, my service has come up on cluster node 101\. You could get
    a different host for your cluster. So, we need to check what port `8080` forwards
    to our `servers.yaml` file guest. In my example, it''s `8081`. So, if I go to
    my web browser and open a new tab and browse to `127.0.0.1:8081`, we will get
    the Jenkins page, which is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我的服务在集群节点 101 上启动。你可能会在集群中得到一个不同的主机。所以，我们需要检查 `8080` 端口转发到我们的 `servers.yaml`
    文件中的哪个端口。在我的例子中，它是 `8081`。所以，如果我打开浏览器，打开一个新标签页并访问 `127.0.0.1:8081`，我们会看到 Jenkins
    页面，内容如下：
- en: '![Coding](img/B05201_07_22.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_22.jpg)'
- en: You can do the same with nginx, and I will leave that up to you as a challenge.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用 nginx 做同样的事，我将把这个任务留给你作为挑战。
- en: Docker UCP
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker UCP
- en: In this topic, we will be looking at Docker's new offering, UCP. This product
    is not open sourced, so there is a licensing cost. You can get a trial license
    for 30 days ([https://www.docker.com/products/docker-universal-control-plane](https://www.docker.com/products/docker-universal-control-plane)),
    and that is what we will be using. Docker UCP takes out the complexity of managing
    all the moving parts of a scheduler. This could be a massive plus pending your
    use case. Docker UCP also brings with it a web UI for administration. So, if container
    schedulers seem daunting, this could be a perfect solution for you.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本主题将介绍 Docker 的新产品 UCP。该产品并非开源，因此需要付费许可。你可以获得一个 30 天的试用许可（[https://www.docker.com/products/docker-universal-control-plane](https://www.docker.com/products/docker-universal-control-plane)），我们将使用这个试用许可。Docker
    UCP 简化了管理调度器所有组件的复杂性，这可能在你的使用场景中是一个巨大的优势。Docker UCP 还带有一个 Web UI 用于管理。所以，如果容器调度器看起来令你感到棘手，这可能是一个完美的解决方案。
- en: The Docker UCP architecture
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker UCP 架构
- en: So, in this example, we are going to build three nodes. The first will be our
    UCP controller and the other two nodes will be UCP HA replicas that give the design
    some fault tolerance. As UCP is a wrapped product, I am not going to go into all
    the moving parts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将构建三个节点。第一个将是我们的 UCP 控制器，其他两个节点将是 UCP HA 副本，提供故障容忍性。由于 UCP 是一个封装产品，我不会深入讨论所有的组成部分。
- en: 'Refer to the following diagram to get a visualization of the main components:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下图，了解主要组件的可视化：
- en: '![The Docker UCP architecture](img/B05201_07_24.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Docker UCP 架构](img/B05201_07_24.jpg)'
- en: Coding
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码
- en: 'We are going to do something different in this module, just to show you that
    our modules are portable to any OS. We will build this module with the Ubuntu
    14.04 server. Again, for this environment, we are going to create a new Vagrant
    repo. So, let''s Git clone our Vagrant repo ([https://github.com/scotty-c/vagrant-template.git](https://github.com/scotty-c/vagrant-template.git)).
    As in the last topic, we will look at the plumbing first, before we write our
    module. The first thing we are going to do is create a file called `config.json`.
    This will have your Docker Hub auth in the file:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模块中，我们将做一些不同的事情，目的是展示我们的模块可以移植到任何操作系统。我们将使用Ubuntu 14.04服务器来构建这个模块。同样，对于这个环境，我们将创建一个新的Vagrant仓库。那么，让我们通过Git克隆我们的Vagrant仓库([https://github.com/scotty-c/vagrant-template.git](https://github.com/scotty-c/vagrant-template.git))。和上一个话题一样，我们首先会查看管道设置，然后再编写我们的模块。我们要做的第一件事是创建一个名为`config.json`的文件。该文件将包含你的Docker
    Hub认证信息：
- en: '![Coding](img/B05201_07_25.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_25.jpg)'
- en: The next will be `docker_subscription.lic`. This will contain your trial license.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`docker_subscription.lic`文件。该文件将包含你的试用许可证。
- en: 'Now, let''s look at our `servers.yaml` file in the root of our Vagrant repo,
    which is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下Vagrant仓库根目录中的`servers.yaml`文件，如下所示：
- en: '![Coding](img/B05201_07_26.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_26.jpg)'
- en: The main call out here is that now we are using the `puppetlabs/ubuntu-14.04-64-puppet-enterprise`
    vagrant box. We have changed `yum` to `apt-get`. Then, we are copying both our
    `config.json` and `docker_subscription.lic` files to their correct place on our
    vagrant box.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要内容是，我们现在使用的是`puppetlabs/ubuntu-14.04-64-puppet-enterprise` Vagrant盒子。我们已经将`yum`改为`apt-get`。接着，我们将`config.json`和`docker_subscription.lic`文件复制到Vagrant盒子上的正确位置。
- en: 'Now, we will look at the changes we need to make to our Puppetfile:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看需要在我们的Puppetfile中进行的更改：
- en: '![Coding](img/B05201_07_27.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_27.jpg)'
- en: You will see that we need a few new modules from the Forge. The Docker module
    is familiar, as is stdlib. We will need Puppetlab's `apt` module to control our
    repos that Ubuntu will use to pull Docker. The last module is the Puppetlabs module
    for UCP itself. To find out more information about this module, you can read all
    about it at [https://forge.puppetlabs.com/puppetlabs/docker_ucp](https://forge.puppetlabs.com/puppetlabs/docker_ucp).
    We will write a module that wraps this class and configures it for our environment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们需要从Forge下载一些新的模块。Docker模块是熟悉的，stdlib模块也一样。我们还需要Puppetlab的`apt`模块来控制Ubuntu用来拉取Docker的仓库。最后一个模块是Puppetlabs为UCP本身提供的模块。要了解更多关于这个模块的信息，可以访问[https://forge.puppetlabs.com/puppetlabs/docker_ucp](https://forge.puppetlabs.com/puppetlabs/docker_ucp)。我们将编写一个包装此类并为我们的环境配置的模块。
- en: 'Now, let''s look at our Hiera file in `hieradata/global.yaml`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，来看一下我们在`hieradata/global.yaml`中的Hiera文件：
- en: '![Coding](img/B05201_07_28.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_28.jpg)'
- en: As you can see, we have added two values. The first is `ucpconfig::ucp_url:`,
    which we will set to our first vagrant box. The next is the value for `ucpconfig::ucp_fingerprint:`,
    which we will leave blank for the moment. But remember it, as we will come back
    to this later in the topic.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们添加了两个值。第一个是`ucpconfig::ucp_url:`，我们将其设置为我们的第一个Vagrant盒子。接下来的值是`ucpconfig::ucp_fingerprint:`，我们暂时将其留空。但请记住它，因为我们稍后会回来处理这个话题。
- en: Now, we will create a module called `<AUTHOR>-ucpconfig`. We have done this
    a few times now, so once you have created a module, create a folder in the root
    of our Vagrant repo called `modules` and move `ucpconfig` into that folder.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个名为`<AUTHOR>-ucpconfig`的模块。我们已经做过几次了，所以一旦你创建了模块，就在我们的Vagrant仓库的根目录中创建一个名为`modules`的文件夹，并将`ucpconfig`移到该文件夹中。
- en: We will then create three manifests files in the module's `manifest` directory.
    The first will be `master.pp`, the second will be `node.pp`, and the last file
    will be `params.pp`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将在模块的`manifest`目录中创建三个清单文件。第一个文件将是`master.pp`，第二个文件将是`node.pp`，最后一个文件将是`params.pp`。
- en: 'Now, let''s add our code to the `params.pp` file, as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将代码添加到`params.pp`文件中，如下所示：
- en: '![Coding](img/B05201_07_29.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_29.jpg)'
- en: 'As you can see, we have four values: `Ucp_url`, which comes from Hiera; `ucp_username`,
    whose default value is set to `admin`; we then have `ucp_password` whose default
    value is set to `orca`. The last value is `ucp_fingerprint`, which again comes
    from Hiera. Now, in a production environment, I would set both the username and
    password in Hiera and overwrite the defaults, which we have set in `params.pp`.
    In this case, as it is a test lab, we will just use the defaults.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有四个值：`Ucp_url`，它来自 Hiera；`ucp_username`，其默认值设置为 `admin`；接下来是 `ucp_password`，其默认值设置为
    `orca`。最后一个值是 `ucp_fingerprint`，它也来自 Hiera。现在，在生产环境中，我会将用户名和密码都设置在 Hiera 中，并覆盖我们在
    `params.pp` 中设置的默认值。在这个测试实验室中，我们将使用默认值。
- en: 'The next file we will look at is our `init.pp` file, which is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要查看的文件是我们的 `init.pp` 文件，内容如下：
- en: '![Coding](img/B05201_07_30.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_30.jpg)'
- en: You can see that at the top of the class, we are mapping our `params.pp` file.
    The next declaration installs the `docker` class and sets the `socket_bind` parameter
    for the daemon. Now, the next bit of logic defines whether the node is a master
    or a node depending on the hostname. As you can see, we are only setting `ucp-01`
    as our master.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在类的顶部，我们正在映射我们的 `params.pp` 文件。接下来的声明安装 `docker` 类，并设置守护进程的 `socket_bind`
    参数。接下来的一段逻辑定义了节点是否为主节点或普通节点，这取决于主机名。如你所见，我们只将 `ucp-01` 设置为我们的主节点。
- en: 'Now, let''s look at `master.pp`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 `master.pp`：
- en: '![Coding](img/B05201_07_31.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_31.jpg)'
- en: In this class, we have the logic to install the UCP controller or master. At
    the top of the class, we are mapping our parameters to our `init.pp` file. The
    next block of code calls the `docker_ucp` class. As you can see, we are setting
    the value of controller to `true`, the host address to our second interface, the
    alternate name of the cluster to our first interface, and the version to `1.0.1`
    (which is the latest at the time of writing this book). We will then set the ports
    for both the controller and Swarm. Then, we will tell UCP about the Docker socket
    location and also the location of the license file.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们有安装 UCP 控制器或主节点的逻辑。在类的顶部，我们将参数映射到我们的 `init.pp` 文件。接下来的代码块调用了 `docker_ucp`
    类。如你所见，我们将控制器的值设置为 `true`，主机地址设置为我们的第二个接口，集群的备用名称设置为我们的第一个接口，版本设置为 `1.0.1`（这是编写本书时的最新版本）。然后，我们将为控制器和
    Swarm 设置端口。接着，我们会告诉 UCP Docker 套接字的位置，并提供许可证文件的位置。
- en: 'Now, let''s look at our last file, `node.pp`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下我们的最后一个文件，`node.pp`：
- en: '![Coding](img/B05201_07_32.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_32.jpg)'
- en: 'As you can see, most of the settings might look familiar. The call out for
    a node is that we need to point it to the controller URL (which we set in Hiera).
    We will get to know about the admin username and password and the cluster fingerprint
    just a little bit later. So, that completes our module. We now need to add our
    class to our nodes which we will do by adding the `default.pp` manifest file in
    the `manifests/default.pp` location from the root of our Vagrant repo, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，大部分设置可能看起来很熟悉。需要注意的是，我们需要将节点指向控制器 URL（我们在 Hiera 中设置了该 URL）。稍后我们将了解管理员用户名和密码以及集群指纹。所以，这就完成了我们的模块。现在，我们需要将我们的类添加到节点中，这将通过在
    Vagrant 仓库根目录下的 `manifests/default.pp` 位置添加 `default.pp` 清单文件来实现，具体如下：
- en: '![Coding](img/B05201_07_38.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_38.jpg)'
- en: Let's go to our terminal and change the directory to the root of our Vagrant
    repo. This time, we are going to do something different. We are going to issue
    the `vagrant up ucp-01` command. This will bring up only the first node. We do
    this as we need to get the fingerprint that is generated as UCP comes up.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入终端并将目录切换到我们 Vagrant 仓库的根目录。这次，我们要做些不同的事情。我们将执行命令`vagrant up ucp-01`。这将只启动第一个节点。这样做是因为我们需要获取在
    UCP 启动时生成的指纹。
- en: 'Our terminal output should look like that shown in the following screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的终端输出应如下图所示：
- en: '![Coding](img/B05201_07_33.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_33.jpg)'
- en: Terminal output
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 终端输出
- en: 'You will note that the fingerprint has been displayed on your terminal output.
    For my example, the fingerprint is `INFO[0031] UCP Server SSL: SHA1 Fingerprint=C2:7C:BB:C8:CF:26:59:0F:DB:BB:11:BC:02:18:C4:A4:18:C4:05:4E`.
    So, we will add this to our Hiera file, which is `global.yaml`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '你会注意到指纹已经显示在你的终端输出中。以我的示例为例，指纹是 `INFO[0031] UCP Server SSL: SHA1 Fingerprint=C2:7C:BB:C8:CF:26:59:0F:DB:BB:11:BC:02:18:C4:A4:18:C4:05:4E`。所以，我们将把这个指纹添加到我们的
    Hiera 文件中，即 `global.yaml`：'
- en: '![Coding](img/B05201_07_34.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_34.jpg)'
- en: 'Now that we have our first node up, we should be able to log in to the web
    UI. We do this in our browser. We will go to `https:127.0.0.1:8443` and get the
    login page as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的第一个节点已经启动，我们应该能够登录 Web UI。我们在浏览器中执行此操作。我们将访问 `https:127.0.0.1:8443`，并看到如下登录页面：
- en: '![Coding](img/B05201_07_35.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_35.jpg)'
- en: 'We will then add the username and password that we set in our `params.pp` file:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将添加在 `params.pp` 文件中设置的用户名和密码：
- en: '![Coding](img/B05201_07_36.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_36.jpg)'
- en: 'Then, after we log in, you can see that we have a health cluster, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们登录后，你会看到我们有一个健康集群，如下所示：
- en: '![Coding](img/B05201_07_37.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_37.jpg)'
- en: Health cluster after logging in
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后健康集群
- en: Now, let's return to our terminal and issue the `vagrant up ucp-02 && vagrant
    up ucp-03` command.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到终端，执行 `vagrant up ucp-02 && vagrant up ucp-03` 命令。
- en: 'Once that is complete, if we look at our web UI, we can see that we have three
    nodes in our cluster, which are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，如果我们查看 Web UI，就能看到集群中有三个节点，具体如下：
- en: '![Coding](img/B05201_07_39.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_39.jpg)'
- en: In this book, we are not going to go into how to administer the cluster through
    the web UI. I would definitely recommend that you explore this product, as it
    has some really cool features. All the documentation is available at [https://docs.docker.com/ucp/overview/](https://docs.docker.com/ucp/overview/).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们不会深入探讨如何通过 Web UI 管理集群。我强烈建议你探索这个产品，它具有一些非常酷的功能。所有文档都可以在 [https://docs.docker.com/ucp/overview/](https://docs.docker.com/ucp/overview/)
    获取。
- en: Kubernetes
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: There is massive buzz around Kubernetes at the moment. This is Google's offering
    to the container world. Kubernetes has some very heavy backers such as Google,
    CoreOS, and Netflix. Out of all the schedulers that we have looked at, Kubernetes
    is the most complex and is highly driven by APIs. If you are new to Kubernetes,
    I would suggest that you read further about the product ([http://kubernetes.io/](http://kubernetes.io/)).
    We are going to first look at the architecture of Kubernetes, as there are a few
    moving parts. Then, we will write our module, and we are going to build Kubernetes
    completely with containers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 Kubernetes 备受关注。这是 Google 为容器世界提供的解决方案。Kubernetes 得到了 Google、CoreOS 和 Netflix
    等重量级企业的支持。在我们考察过的所有调度器中，Kubernetes 是最复杂的，而且高度依赖于 API。如果你是 Kubernetes 的新手，我建议你进一步了解这个产品，参考
    [http://kubernetes.io/](http://kubernetes.io/)。我们将首先了解 Kubernetes 的架构，因为它包含一些动态组件。然后，我们将编写模块，使用容器来完全构建
    Kubernetes。
- en: The architecture
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: We will be building Kubernetes on a single node. The reason for this is that
    it will cut out some of the complexity on how to use flannel over the Docker bridge.
    This module will give you a good understanding of how Kubernetes works using more
    advanced Puppet techniques. If you want to take your knowledge further after you
    have mastered this chapter, I would recommend that you check out the module on
    the forge at [https://forge.puppetlabs.com/garethr/kubernetes](https://forge.puppetlabs.com/garethr/kubernetes).
    This module really takes Puppet and Kubernetes to the next level.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会在单个节点上构建 Kubernetes。这样做的原因是，它可以减少使用 Docker 桥接网络时 Flannel 的一些复杂性。本模块将帮助你深入了解
    Kubernetes 的工作原理，并使用更高级的 Puppet 技巧。如果你掌握了这一章的内容并希望更进一步，我建议你访问 [https://forge.puppetlabs.com/garethr/kubernetes](https://forge.puppetlabs.com/garethr/kubernetes)
    上的模块。这个模块能将 Puppet 和 Kubernetes 推向一个全新的水平。
- en: 'So what we are going to code looks like the following diagram:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要编写的代码如下图所示：
- en: '![The architecture](img/B05201_07_40.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![架构](img/B05201_07_40.jpg)'
- en: As you can see, we have a container that runs **etcd** (to read more about etcd,
    go to [https://coreos.com/etcd/docs/latest/](https://coreos.com/etcd/docs/latest/)).
    etcd is similar to Consul, which we are familiar with. In the next few containers,
    we are going to use hyperkube ([https://godoc.org/k8s.io/kubernetes/pkg/hyperkube](https://godoc.org/k8s.io/kubernetes/pkg/hyperkube)).
    This will load balance the required Kubernetes components across multiple containers
    for us. It seems pretty easy, right? Let's get into the code so that we get a
    better perspective on all the moving parts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有一个运行 **etcd** 的容器（要了解更多关于 etcd 的信息，请访问 [https://coreos.com/etcd/docs/latest/](https://coreos.com/etcd/docs/latest/)）。etcd
    类似于我们熟悉的 Consul。在接下来的几个容器中，我们将使用 hyperkube ([https://godoc.org/k8s.io/kubernetes/pkg/hyperkube](https://godoc.org/k8s.io/kubernetes/pkg/hyperkube))。它将为我们在多个容器中负载均衡所需的
    Kubernetes 组件。看起来挺简单，对吧？让我们进入代码，深入理解所有这些动态组件。
- en: Coding
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码
- en: We are going to create a new Vagrant repo again. We won't cover how to do that
    as we have covered it twice in this chapter. If you are unsure, just take a look
    at the initial part of this chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次创建一个新的 Vagrant 仓库。我们不会再重复创建的步骤，因为我们在本章中已经介绍过两次。如果你不确定，只需查看本章的前面部分。
- en: 'Once we have created our Vagrant repo, let''s open our `servers.yaml` file,
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了 Vagrant 仓库，接下来打开我们的 `servers.yaml` 文件，如下所示：
- en: '![Coding](img/B05201_07_41.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_41.jpg)'
- en: 'As you can see, there is nothing special there that we have not covered in
    this book. There''s just a single node that we mentioned earlier, `kubernetes`.
    The next file we will look at is our Puppetfile. We will, of course, need our
    Docker module, `stdlib`, and lastly, `wget`. We need `wget` to get `kubectl`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这里没有什么特别的内容，我们在本书中都已经涉及过了。这里只有我们之前提到的单节点 `kubernetes`。接下来我们要查看的文件是我们的 Puppetfile。我们当然需要我们的
    Docker 模块，`stdlib`，最后是 `wget`。我们需要 `wget` 来获取 `kubectl`：
- en: '![Coding](img/B05201_07_42.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_42.jpg)'
- en: That is all the plumbing that we need to set up for our repo. Let's create a
    new module called `<AUTHOR>-kubernetes_docker`. Once it is created, we will move
    our module to the `modules` directory in the root of our Vagrant repo.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们设置仓库所需的所有基础设施。让我们创建一个新的模块，名为 `<AUTHOR>-kubernetes_docker`。一旦它创建完成，我们将把它移动到
    Vagrant 仓库根目录下的 `modules` 目录中。
- en: 'We are going to create two new folders in our module. The first will be the
    `templates` folder, and the other folder will be the `lib` directory. We will
    get to the `lib` directory toward the end of our coding. The first file we will
    create and edit is `docker-compose.yml.erb`. The reason for this is that it is
    the foundation of our module. We will add the following code to it:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在模块中创建两个新文件夹。第一个将是 `templates` 文件夹，另一个是 `lib` 目录。我们将在编码的后期介绍 `lib` 目录。我们首先要创建和编辑的文件是
    `docker-compose.yml.erb`。这样做的原因是它是我们模块的基础。我们将向其中添加以下代码：
- en: '![Coding](img/B05201_07_43.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_43.jpg)'
- en: Let's break this file down into three chunks, as there is a lot going on there.
    The first block of code is going to set up our etcd cluster. You can see from
    the screenshot name that we are using Google's official images, and we are using
    etcd version 2.2.1\. We are giving the container access to the host network. Then,
    in the command resource, we pass some arguments to etcd as it starts.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个文件分成三个部分，因为里面有很多内容。第一块代码将会设置我们的 etcd 集群。你可以从截图的名称看出，我们使用的是 Google 官方镜像，并且我们使用的是
    etcd 版本 2.2.1。我们将容器的访问权限授予主机网络。然后，在命令资源中，我们在启动时传递一些参数给 etcd。
- en: The next container we create is hyperkube. Again, it is an official Google image.
    Now, we are giving this container access to a lot of host volumes, the host network,
    and the host process, making the container privileged. This is because the first
    container will bootstrap Kubernetes and it will spawn more containers running
    the various Kubernertes components. Now, in the command resource, we are again
    passing some arguments for hyperkube. The two major ones we need to worry about
    are the API server address and config manifests. You will note that we have a
    mapped folder from `/kubeconfig:/etc/kubernetes/manifests:ro`. We are going to
    modify our manifest file to make our Kubernetes environment available to the outside
    world. We will get to that next. But, we will finish looking at the code in this
    file first.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个我们创建的容器是 hyperkube。同样，它是一个官方的 Google 镜像。现在，我们将赋予这个容器访问大量主机卷、主机网络和主机进程的权限，使得容器具有特权。这是因为第一个容器将引导
    Kubernetes 启动，并且它将启动更多的容器来运行各种 Kubernetes 组件。现在，在命令资源中，我们再次传递一些参数给 hyperkube。我们需要关注的两个主要参数是
    API 服务器地址和配置清单。你会注意到我们已经将文件夹 `/kubeconfig:/etc/kubernetes/manifests:ro` 映射过来了。我们将修改我们的清单文件，使得
    Kubernetes 环境可以对外部可用。我们稍后会介绍这个，但我们先完成对这个文件中代码的分析。
- en: 'The last container and the third block of code is going to set up our service
    proxy. We are going to give this container access to the host network and process.
    In the command resource, we are going to specify that this container is a proxy.
    The next thing to take notice of is that we specify where the proxy can find the
    API. Now, let''s create the next file, `master.json.erb`. This is the file that
    hyperkube will use to schedule all the Kubernetes components, which are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的容器和第三块代码将会设置我们的服务代理。我们将赋予这个容器访问主机网络和进程的权限。在命令资源中，我们将指定该容器为代理。接下来需要注意的是，我们指定了代理可以找到
    API 的位置。现在，让我们创建下一个文件，`master.json.erb`。这是 hyperkube 用来调度所有 Kubernetes 组件的文件，具体如下：
- en: '![Coding](img/B05201_07_44.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_44.jpg)'
- en: 'As you can see, we have defined three more containers. This is the first time
    we will define a Kubernetes pod ([http://kubernetes.io/docs/user-guide/pods/](http://kubernetes.io/docs/user-guide/pods/)).
    A pod is a group of containers that creates an application. It is similar to what
    we have done with Docker Compose. As you can see, we have changed all the IP addresses
    to the `<%= @master_ip %>` parameter. We will create four new files: `apps.pp`,
    `config.pp`, `install.pp`, and `params.pp`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经定义了三个容器。这是我们第一次定义 Kubernetes pod（[http://kubernetes.io/docs/user-guide/pods/](http://kubernetes.io/docs/user-guide/pods/)）。Pod
    是一组容器，它们共同构成一个应用程序。这与我们在 Docker Compose 中做的类似。如你所见，我们已将所有 IP 地址更改为 `<%= @master_ip
    %>` 参数。我们将创建四个新文件：`apps.pp`、`config.pp`、`install.pp` 和 `params.pp`。
- en: We will now move on to our files in the `modules` manifest directory. Now, strap
    yourselves in, as this is where the magic happens. Well, that's not true. The
    magic happens here and in our `lib` directory. We will need to write some custom
    types and providers for Puppet to be able to control Kubernertes as `kubectl`
    is the user interface (for types, visit [https://docs.puppetlabs.com/guides/custom_types.html](https://docs.puppetlabs.com/guides/custom_types.html),
    and for providers, visit [https://docs.puppetlabs.com/guides/provider_development.html](https://docs.puppetlabs.com/guides/provider_development.html)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续操作 `modules` 清单目录中的文件。准备好，因为这里是魔法发生的地方。其实，这不完全正确。魔法发生在这里和我们的 `lib` 目录中。我们需要为
    Puppet 编写一些自定义类型和提供程序，以便它能够控制 Kubernetes，因为 `kubectl` 是用户界面（关于类型，请访问 [https://docs.puppetlabs.com/guides/custom_types.html](https://docs.puppetlabs.com/guides/custom_types.html)，关于提供程序，请访问
    [https://docs.puppetlabs.com/guides/provider_development.html](https://docs.puppetlabs.com/guides/provider_development.html)）。
- en: 'Let''s start with our `init,pp` file, which is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `init.pp` 文件开始，如下所示：
- en: '![Coding](img/B05201_07_45.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_45.jpg)'
- en: 'As you can see, there is not much in this file. We are going to use our `init.pp`
    file to control the order in which classes are executed. We are also declaring
    `param <%= @master_ip %>`. We will now move on to our `install.pp` file, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个文件里没有太多内容。我们将使用 `init.pp` 文件来控制类执行的顺序。我们还声明了 `param <%= @master_ip %>`。接下来，我们将继续操作
    `install.pp` 文件，如下所示：
- en: '![Coding](img/B05201_07_46.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_46.jpg)'
- en: 'In this file, we install Docker as we did before. We will place our two templates
    that we created earlier. Then, we will run Docker Compose to bring up our cluster.
    Now, we will move on to `config.pp`, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们像之前一样安装 Docker。我们将放置我们之前创建的两个模板。然后，我们将运行 Docker Compose 来启动我们的集群。接下来，我们将继续操作
    `config.pp`，如下所示：
- en: '![Coding](img/B05201_07_47.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_47.jpg)'
- en: 'The first thing that we declare is that we want `wget` to be our `kubectl`
    client, and we place it at `/usr/bin/` ([http://kubernetes.io/docs/user-guide/kubectl/kubectl/](http://kubernetes.io/docs/user-guide/kubectl/kubectl/)).
    You really need to understand what this interface does; otherwise, you might get
    a bit lost from here. So, I suggest that you have a fairly good idea of what kubectl
    is and what it is capable of. Next, we will make it executable and available for
    all our users. Now, this last piece of code does not make sense, as we have not
    called the `kubectl_config` class yet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先声明，我们希望将 `wget` 设置为我们的 `kubectl` 客户端，并将其放置在 `/usr/bin/` 目录下（[http://kubernetes.io/docs/user-guide/kubectl/kubectl/](http://kubernetes.io/docs/user-guide/kubectl/kubectl/)）。你需要真正理解这个接口的作用，否则接下来的步骤可能会让你有些迷茫。所以，我建议你对
    kubectl 有一个相对清晰的认识，并了解它的功能。接下来，我们将使其可执行并对所有用户可用。现在，最后这段代码没有意义，因为我们还没有调用 `kubectl_config`
    类：
- en: '![Coding](img/B05201_07_48.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_48.jpg)'
- en: 'We now need to jump to our `lib` directory. This first thing we will do is
    create all our folders that we need. The first folder that we will create is `puppet`
    under the `lib` directory. We will look at our custom types first. We will create
    a folder called `type` under `puppet`. The following screenshot will give you
    a visualization of the structure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要跳转到 `lib` 目录。首先，我们将创建所有需要的文件夹。我们将首先在 `lib` 目录下创建一个名为 `puppet` 的文件夹。我们将先来看一下我们的自定义类型。我们将在
    `puppet` 文件夹下创建一个名为 `type` 的文件夹。以下截图将帮助你理解结构：
- en: '![Coding](img/B05201_07_49.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_49.jpg)'
- en: 'Under the `type` folder, we will create a file called `kubectl_config.rb`.
    In that file, we will add the new parameters of `type` as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `type` 文件夹下，我们将创建一个名为 `kubectl_config.rb` 的文件。在该文件中，我们将添加新的 `type` 参数，如下所示：
- en: '![Coding](img/B05201_07_50.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![编码](img/B05201_07_50.jpg)'
- en: 'Let me explain what is happening here. In the first line, we are going to declare
    our new type, `kubectl_config`. We are then going to set the default value of
    the new type when it is declared as `present`. We are going to declare three values
    to our type, `name`, `cluster`, and `kube_context`. These are all settings that
    we will add to our `config` file that `kubectl` will use when we interface with
    it. Now, we will create a folder under the `lib` directory called `provider`.
    Then, under that, we will create a folder with the same name as our custom type,
    `kubectl_config`. Inside that folder, we will create a file called `ruby.rb`.
    In this file, we will put the Ruby code that provides logic to our type as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我解释一下这里发生了什么。在第一行中，我们将声明我们的新类型`kubectl_config`。然后，当新类型被声明为`present`时，我们将设置其默认值。接下来，我们将为我们的类型声明三个值：`name`、`cluster`和`kube_context`。这些都是我们将添加到`config`文件中的设置，该文件将在与`kubectl`交互时使用。现在，我们将在`lib`目录下创建一个名为`provider`的文件夹。然后，在其中创建一个文件夹，文件夹名称与我们自定义类型`kubectl_config`相同。在该文件夹内，我们将创建一个名为`ruby.rb`的文件。在这个文件中，我们将放置提供逻辑的Ruby代码，如下所示：
- en: '![Coding](img/B05201_07_51.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_51.jpg)'
- en: A provider needs to have three methods for Puppet to be able to run the code.
    They are `exsists?`, `create`, and `destroy`. These are pretty easy to understand.
    The `exists?` method checks whether the type has already been executed by Puppet,
    `create` runs the type, and `destroy` is invoked when the type is set to `absent`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 提供程序需要有三个方法，以便Puppet能够运行代码。它们是`exists?`、`create`和`destroy`。这些方法都很容易理解。`exists?`方法检查类型是否已经被Puppet执行，`create`运行类型，`destroy`在类型设置为`absent`时被调用。
- en: We are now going to work through the file, from top to bottom. We need to first
    load a few Ruby libraries for some of our methods. We will then tie this provider
    to our type. The next thing that we need to declare is the `kubectl` executable.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将从上到下地处理这个文件。我们需要首先加载一些Ruby库，以支持我们的某些方法。然后，我们将把这个提供程序与我们的类型绑定。接下来我们需要声明的是`kubectl`可执行文件。
- en: Now we will write our first method, `interface`. This will get the IP address
    from the hostname of the box. We will then create three more methods. We will
    also create an array and add all our configuration to them. You will note that
    we are mapping our parameters from our type in the arrays.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写我们的第一个方法`interface`。这个方法将从主机的主机名获取IP地址。接着我们将创建三个其他方法。我们还会创建一个数组，并将所有配置添加到其中。你会注意到，我们正在将我们的参数从类型映射到这些数组中。
- en: In our `exists?` method, we will check for our `kubectl` config file.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`exists?`方法中，我们将检查我们的`kubectl`配置文件。
- en: In our `create` method, we are calling our `kubectl` executable and then passing
    our arrays as arguments. We will then link our `config` file to roots' home directory
    (this is fine for our lab. In a production environment, I would use a named user
    account).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`create`方法中，我们调用`kubectl`可执行文件，并将我们的数组作为参数传递。然后，我们将把`config`文件链接到根目录的主目录（对于我们的实验室环境来说是可以的，在生产环境中，我会使用一个专门的用户账户）。
- en: 'Lastly, we will remove the `config` file if the type is set to `absent`. We
    will now go back to our `manifests` directory and look at our last file, which
    is `apps.pp`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果类型被设置为`absent`，我们将删除`config`文件。然后我们将回到`manifests`目录，查看我们的最后一个文件，即`apps.pp`：
- en: '![Coding](img/B05201_07_52.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_52.jpg)'
- en: In this file, we are going to run a container application on our Kubernetes
    cluster. Again, we will write another custom type and provider. Before we get
    to that, we should look at the code in this class. As you can see, our type is
    called `kubernetes_run`. We can see that our service is named `nginx`, the Docker
    image we will pull is `nginx`, and we will then expose port `80`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们将运行一个容器应用程序在我们的Kubernetes集群上。再一次，我们将编写另一个自定义类型和提供程序。在我们进入这个内容之前，我们应该先看看这个类中的代码。如你所见，我们的类型叫做`kubernetes_run`。我们可以看到我们的服务名称是`nginx`，我们将拉取的Docker镜像是`nginx`，然后我们将暴露端口`80`。
- en: 'Let''s go back to our `lib` directory. We will then create a file in our `type`
    folder called `kubernetes_run.rb`. In this file, we will set up our custom type
    as we did earlier:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到`lib`目录。然后，我们将在`type`文件夹中创建一个名为`kubernetes_run.rb`的文件。在这个文件中，我们将像之前一样设置我们的自定义类型：
- en: '![Coding](img/B05201_07_53.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_53.jpg)'
- en: 'As you can see, we are mapping the same parameters that we had in our `apps.pp`
    file. We will then create a folder under the `provider` folder with the same name
    as our `kubernetes_run` type. Again, in the newly created directory, we will create
    a file called `ruby.rb`. It will have the code shown in the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们正在映射与`apps.pp`文件中相同的参数。接着，我们将在`provider`文件夹下创建一个与`kubernetes_run`类型同名的文件夹。同样，在新创建的目录下，我们将创建一个名为`ruby.rb`的文件。它将包含如下截图中的代码：
- en: '![Coding](img/B05201_07_54.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_54.jpg)'
- en: 'In this file, we are going to add two commands this time: the first is `kubectl`
    and the second is `docker`. We will create two methods, again with arrays that
    map the values from our type.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们这次将添加两个命令：第一个是`kubectl`，第二个是`docker`。我们将创建两个方法，同样使用数组来映射我们类型中的值。
- en: Now, let's look at our `exists?` method. We are going to pass an array as an
    argument to `kubectl` to check whether the service exists. We are then going to
    catch the error if `kubectl` throws an error with the request and returns `false`.
    This is used if there are no services deployed on the cluster.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的`exists?`方法。我们将传递一个数组作为参数给`kubectl`，以检查服务是否存在。如果`kubectl`在请求时抛出错误并返回`false`，我们将捕获这个错误。这用于在集群中没有部署任何服务的情况。
- en: In our `create` method, we will first pass an array to `kubectl` to get the
    nodes in the cluster. We are using this as an arbitrary command to make sure that
    the cluster is up. Under that, we will capture the error and retry the command
    until it is successful. Once it is successful, we will deploy our container with
    the `ensure` resource.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`create`方法中，我们将首先传递一个数组给`kubectl`以获取集群中的节点。我们将使用这个命令作为一个任意命令来确保集群正常运行。接着，我们会捕获错误并重试命令直到成功。一旦成功，我们将通过`ensure`资源部署我们的容器。
- en: In the `destroy` method, we will use `docker` to remove our container.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在`destroy`方法中，我们将使用`docker`来移除我们的容器。
- en: 'Now we have all our coding done. We just need to add our class to our node
    by editing our `default.pp` file in the `manifests` folder in the root of our
    Vagrant repo as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了所有的代码编写。接下来，我们只需要通过编辑Vagrant仓库根目录下`manifests`文件夹中的`default.pp`文件，将我们的类添加到节点中，如下所示：
- en: '![Coding](img/B05201_07_55.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_55.jpg)'
- en: 'Now, let''s open our terminal and change the directory to the root of our Vagrant
    repo and issue the `vagrant up` command. Once Puppet has completed its run, our
    terminal should look like the following screenshot:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开终端并将目录切换到Vagrant仓库的根目录，然后执行`vagrant up`命令。Puppet执行完毕后，终端应该呈现如下截图：
- en: '![Coding](img/B05201_07_56.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_56.jpg)'
- en: 'We will now log in to our vagrant box by issuing the `vagrant ssh` command
    and then `sudo -i` and change to root. Once we are root, we will look at our service
    on our cluster. We will do this by issuing the `kubectl get svc` command as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过执行`vagrant ssh`命令登录到我们的vagrant盒子，然后输入`sudo -i`切换到root用户。成为root后，我们将查看集群中的服务。我们通过执行`kubectl
    get svc`命令来实现，如下所示：
- en: '![Coding](img/B05201_07_57.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_57.jpg)'
- en: 'As you can see, we have two services running our cluster: `Kubernetes` and
    `nginx`. If we go to our web browser and go to the address we gave to the second
    network interface, `http://172.17.9.101`, we will get the following nginx default
    page:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的集群上正在运行两个服务：`Kubernetes`和`nginx`。如果我们打开网页浏览器并访问我们为第二个网络接口设置的地址`http://172.17.9.101`，我们将看到以下nginx默认页面：
- en: '![Coding](img/B05201_07_58.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![Coding](img/B05201_07_58.jpg)'
- en: Now, our cluster is running successfully with our `nginx` service.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的集群已经成功运行，并且`nginx`服务也在正常工作。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We covered my favorite three container schedulers; each one of them has their
    own pros and cons. Now, you have the knowledge and the required code to give all
    three a really good test run. I would suggest that you do so so that you can make
    the right choice when choosing the design in your environment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了我最喜欢的三个容器调度器，每一个都有各自的优缺点。现在，你已经掌握了相关的知识和所需的代码，可以对这三者进行一次充分的测试。我建议你这样做，这样在选择环境设计时，你可以做出正确的选择。
