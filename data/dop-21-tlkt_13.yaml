- en: Creating and Managing a Docker Swarm Cluster in DigitalOcean
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 DigitalOcean 中创建和管理 Docker Swarm 集群
- en: '*                               Plan to throw one (implementation) away; you
    will, anyhow.**–Fred Brooks*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*                               计划将一个（实现）抛弃；反正你会的。**–弗雷德·布鲁克斯*'
- en: We already saw a few ways to create and operate a Swarm cluster in AWS. Now
    we'll try to do the same in *DigitalOcean* ([https://www.digitalocean.com/](https://www.digitalocean.com/)).
    We'll explore some of the tools and configurations that can be used with this
    hosting provider.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些在 AWS 中创建和操作 Swarm 集群的方法。现在我们将尝试在*DigitalOcean* ([https://www.digitalocean.com/](https://www.digitalocean.com/))中做同样的事情。我们将探索一些可以与此托管服务提供商一起使用的工具和配置。
- en: Unlike AWS that is known to everyone, DigitalOcean is relatively new and less
    well known. You might be wondering why I chose DigitalOcean before some other
    providers like Azure and GCE. The reason lies in the differences between AWS (and
    other similar providers) and DigitalOcean. The two differ in many aspects. Comparing
    them is like comparing David and Goliath. One is small while the other (AWS) is
    huge. DigitalOcean understands that it cannot compete with AWS on its own ground,
    so it decided to play a different game.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与大家熟知的 AWS 不同，DigitalOcean 相对较新，知名度较低。你可能会好奇为什么我选择了 DigitalOcean，而不是像 Azure
    和 GCE 这样的其他提供商。原因在于 AWS（以及其他类似提供商）与 DigitalOcean 之间的差异。两者在许多方面不同。比较它们就像比较大卫与哥利亚。一个很小，而另一个（AWS）则庞大无比。DigitalOcean
    明白自己无法在 AWS 的领域中与其竞争，因此它决定采取不同的游戏规则。
- en: DigitalOcean launched in 2011 and is focused on very specific needs. Unlike
    AWS with its *everything-to-everyone* approach, DigitalOcean provides virtual
    machines. There are no bells and whistles. You do not get lost in their catalog
    of services since it is almost non-existent. If you need a place to host your
    cluster and you do not want to use services designed to lock you in, DigitalOcean
    might be the right choice.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 于 2011 年推出，专注于非常特定的需求。与提供 *面面俱到* 服务的 AWS 不同，DigitalOcean 提供虚拟机。没有多余的花哨功能。你不会在他们的服务目录中迷失，因为几乎没有目录可言。如果你需要一个地方来托管你的集群，并且不想使用那些旨在将你“锁定”的服务，那么
    DigitalOcean 可能是一个合适的选择。
- en: DigitalOcean's main advantages are pricing, high performance, and simplicity.
    If that's what you're looking for, it is worth trying it out.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 的主要优点是定价、出色的性能和简单性。如果这些是你所追求的，那么值得尝试一下。
- en: Let's go through these three advantages one by one.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解这三个优点。
- en: DigitalOcean's pricing is probably the best among all cloud providers. No matter
    whether you are a small company in need of only a couple of servers, or a big
    entity that is looking for a place to instantiate hundreds or even thousands of
    servers, chances are that DigitalOcean will be cheaper than any other provider.
    That might leave you wondering about the quality. After all, cheaper things tend
    to sacrifice it. Is that the case with DigitalOcean?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 的定价可能是所有云服务提供商中最具竞争力的。无论你是需要仅仅几个服务器的小公司，还是寻找一个能够部署数百甚至数千台服务器的大型企业，DigitalOcean
    可能比任何其他提供商都便宜。这可能会让你对质量产生疑问。毕竟，便宜的东西往往会在质量上做出牺牲。那 DigitalOcean 会是这种情况吗？
- en: DigitalOcean offers very high-performance machines. All disk drives are SSD,
    network speed is 1 Gbps, and it takes less than a minute to create and initialize
    droplets (their name for VMs). As a comparison, AWS EC2 instance startup time
    can vary between one and three minutes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 提供了非常高性能的机器。所有磁盘驱动器都是 SSD，网络速度为 1 Gbps，而且创建和初始化 Droplets（它们对虚拟机的称呼）不到一分钟。作为对比，AWS
    EC2 实例的启动时间可能在 1 到 3 分钟之间。
- en: The last advantages DigitalOcean offers are their UI and API. Both are clean
    and easy to understand. Unlike AWS that can have a steep learning curve, you should
    have no trouble learning how to use them in a few hours.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 提供的最后一个优点是他们的 UI 和 API。两者都简洁易懂。与 AWS 可能具有陡峭学习曲线不同，你应该在几个小时内就能学会如何使用它们。
- en: Enough with the words of praise. Not everything can be great. What are the disadvantages?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，夸奖的话说得够多了，并非一切都很完美。那么，缺点是什么呢？
- en: DigitalOcean does not offer a plethora of services. It does a few things, and
    it does them well. It is a bare-bone **Infrastructure as a service **(**IaaS**)
    provider. It assumes that you will set up the services yourself. There is no load
    balancing, centralized logging, sophisticated analytics, hosted databases, and
    so on. If you need those things, you are expected to set them up yourself. Depending
    on your use case, that can be an advantage or a disadvantage.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 提供的服务种类不多。它做几件事，但做得很好。它是一个简化的 **基础设施即服务** (**IaaS**) 提供商。它假设你会自己设置服务。没有负载均衡、集中式日志记录、复杂的分析、托管数据库等。如果你需要这些功能，预计你需要自己进行设置。根据你的使用情况，这可能是优势，也可能是劣势。
- en: A comparison between DigitalOcean and AWS is unfair since the scope of what
    each does is different. DigitalOcean is not trying to compete with AWS as a whole.
    If pressed to compare something, that would be DigitalOcean against AWS EC2\.
    In such a case, DigitalOcean wins hands down.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DigitalOcean 与 AWS 进行比较是不公平的，因为它们的功能范围不同。DigitalOcean 并没有试图与 AWS 整体竞争。如果一定要做比较，那就是
    DigitalOcean 与 AWS EC2 之间的对比。在这种情况下，DigitalOcean 完全占优势。
- en: I will assume that you already have a DigitalOcean account. If that's not the
    case, please register using: [https://m.do.co/c/ee6d08525457](https://m.do.co/c/ee6d08525457)
    . You'll get 10 in credit. That should be more than enough to run the examples
    in this chapter. DigitalOcean is so cheap that you will probably finish this chapter
    with more than 9 remaining balance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你已经有了一个 DigitalOcean 账户。如果没有，请使用以下链接注册：[https://m.do.co/c/ee6d08525457](https://m.do.co/c/ee6d08525457)。你将获得
    10 美元的信用额度。这应该足够用于本章中的示例。DigitalOcean 价格非常便宜，你可能会发现到本章结束时，账户余额仍然有超过 9 美元。
- en: Even if you've already made a firm decision to use a different cloud computing
    provider or on-premise servers, I highly recommend going through this chapter.
    It will help you compare DigitalOcean with your provider of choice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你已经决定使用其他云计算提供商或本地服务器，我还是强烈推荐你阅读本章。它将帮助你将 DigitalOcean 与你选择的提供商进行比较。
- en: Let's give DigitalOcean a spin and judge through examples whether it is a good
    choice to host our Swarm cluster.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用 DigitalOcean，并通过示例来判断它是否是托管我们的 Swarm 集群的一个好选择。
- en: You might notice that some parts of this chapter are very similar, or even the
    same as those you read in the other cloud computing chapters like Chapter 12, *Creating
    and Managing a Docker Swarm Cluster in Amazon Web Services*. The reason for the
    partial repetition is the goal to make the cloud computing chapters useful not
    only to those who read everything, but also those who skipped other providers
    and jumped right here.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，本章的某些部分与你在其他云计算章节中阅读的内容非常相似，甚至完全相同，比如第12章，*在亚马逊 Web 服务中创建和管理 Docker
    Swarm 集群*。部分内容重复的原因是为了让云计算章节对那些阅读过全部内容的人有用，同时也能对跳过其他提供商直接来到这里的人有所帮助。
- en: Before we move into practical exercises, we'll need to get the access keys and
    decide the region where we'll run the cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行实际操作之前，我们需要获取访问密钥并决定将在哪个区域运行集群。
- en: Setting up the environment variables
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置环境变量
- en: In the Chapter 12, *Creating And Managing A Docker Swarm Cluster in Amazon Web
    Services,* we installed AWS **Command Line Interface** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)) that
    helped us with some of the tasks. DigitalOcean has a similar interface called
    doctl. Should we install it? I don't think we need a CLI for DigitalOcean. Their
    API is clean and well defined, and we can accomplish everything a CLI would do
    with simple curl requests. DigitalOcean proves that a well designed API goes a
    long way and can be the only entry point into the system, saving us the trouble
    of dealing with middle-man applications like CLIs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章，*在亚马逊 Web 服务中创建和管理 Docker Swarm 集群*，我们安装了 AWS **命令行接口** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))，它帮助我们完成了一些任务。DigitalOcean
    有一个类似的接口，叫做 doctl。我们需要安装它吗？我认为我们不需要 DigitalOcean 的 CLI。它们的 API 清晰且定义明确，我们可以通过简单的
    curl 请求完成 CLI 的所有功能。DigitalOcean 证明了一个设计良好的 API 能够做到这一点，并且可以成为进入系统的唯一入口，省去了我们处理像
    CLI 这样的中介应用的麻烦。
- en: Before we start using the API, we should generate an access token that will
    serve as the authentication method.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用 API 之前，我们应该生成一个访问令牌，作为认证方法。
- en: 'Please open the *DigitalOcean tokens* screen ([https://cloud.digitalocean.com/settings/api/tokens](https://cloud.digitalocean.com/settings/api/tokens))
    and click theGenerate New Token button. You''ll be presented with the New personal
    access token popup, as shown in the following image:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开*DigitalOcean令牌*屏幕 ([https://cloud.digitalocean.com/settings/api/tokens](https://cloud.digitalocean.com/settings/api/tokens))并点击生成新令牌按钮。你将看到新个人访问令牌的弹出窗口，如下图所示：
- en: '![](img/new-personal-access-token.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/new-personal-access-token.png)'
- en: 'Figure 12-1: DigitalOcean new personal access token screen'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图12-1：DigitalOcean 新个人访问令牌屏幕
- en: Type `devops21` as Token name and click the Generate Token button. You'll see
    the newly generated token. We'll put it into the environment variable `DIGITALOCEAN_ACCESS_TOKEN`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`devops21`作为令牌名称，然后点击生成令牌按钮。你将看到新生成的令牌。我们会将它放入环境变量`DIGITALOCEAN_ACCESS_TOKEN`中。
- en: All the commands from this chapter are available in the Gist `12-digital-ocean.sh` ([https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e](https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在Gist `12-digital-ocean.sh`中找到 ([https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e](https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e))。
- en: 'Copy the token before running the command that follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行接下来的命令之前，请先复制令牌：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please replace `[...]` with the actual token.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请将`[...]`替换为实际的令牌。
- en: Now we can decide which region our cluster will run in.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以决定我们的集群将在哪个区域运行。
- en: 'We can see the currently available regions by sending a request to [https://api.digitalocean.com/v2/regions](https://api.digitalocean.com/v2/regions):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向[https://api.digitalocean.com/v2/regions](https://api.digitalocean.com/v2/regions)发送请求来查看当前可用的区域：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We sent an HTTP `GET` request to the regions API. The request contains the access
    token. The response is piped to `jq`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向地区API发送了一个HTTP `GET`请求。请求中包含访问令牌。响应通过管道传递给`jq`。
- en: 'A part of the output is as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see from the bottom of the response, DigitalOcean currently supports
    twelve regions. Each contains the information about available droplet sizes and
    the supported features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从响应底部可以看出，DigitalOcean目前支持十二个区域。每个区域都包含有关可用Droplet大小和支持的功能的信息。
- en: Throughout this chapter, I will be using **San Francisco 2 **(**sfo2**) region.
    Feel free to change it to the region closest to your location. If you choose to
    run the examples in a different region, please make sure that it contains the
    `private_networking` feature.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将使用**旧金山2**（**sfo2**）区域。你可以根据自己的位置随意更改为最接近的区域。如果你选择在其他区域运行示例，请确保该区域包含`private_networking`功能。
- en: 'We''ll put the region inside the environment variable `DIGITALOCEAN_REGION`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把区域放入环境变量`DIGITALOCEAN_REGION`中：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we are all set with the prerequisites that will allow us to create the first
    Swarm cluster in DigitalOcean. Since we used Docker Machine throughout most of
    the book, it will be our first choice.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好所有前提条件，可以在DigitalOcean中创建第一个Swarm集群。由于我们在本书的大部分时间里都使用了Docker Machine，它将是我们的首选。
- en: Setting up a Swarm cluster with Docker Machine and DigitalOcean API
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Machine和DigitalOcean API设置Swarm集群
- en: 'We''ll continue using the `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository. It contains configurations and scripts that''ll help us out. You already
    have it cloned. To be on the safe side, we''ll pull the latest version:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用`vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    仓库，它包含了帮助我们的配置和脚本。你已经将它克隆。为了保险起见，我们将拉取最新版本：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s create the first droplet:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建第一个Droplet：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We specified that the *Docker Machine* should use the `digitalocean` driver
    to create an instance in the region we defined as the environment variable `DIGITALOCEAN_REGION`.
    The size of the droplet is 1 GB and it has private networking enabled.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定*Docker Machine*使用`digitalocean`驱动程序，在我们定义为环境变量`DIGITALOCEAN_REGION`的区域中创建一个实例。Droplet的大小为1GB，并启用了私有网络。
- en: Docker Machine launched a DigitalOcean droplet, provisioned it with Ubuntu,
    and installed and configured Docker Engine.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Machine启动了一个DigitalOcean Droplet，配置了Ubuntu，并安装和配置了Docker Engine。
- en: As you no doubt already noticed, everyone is trying to come up with a different
    name for the same thing. DigitalOcean is no exception. They came up with the term
    *droplet*. It is a different name for a virtual private server. Same thing, different
    name.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你无疑已经注意到的，大家都在尝试为相同的事物想出不同的名称。DigitalOcean也不例外。它们提出了术语*droplet*，即虚拟私人服务器的不同名称。是同样的东西，不同的名字。
- en: Now we can initialize the cluster. We should use private IPs for all communication
    between nodes. Unfortunately, `docker-machine ip` command returns only the public
    IP, so we'll have to resort to a different method to get the private IP.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化集群了。我们应该使用私有IP进行节点之间的所有通信。不幸的是，`docker-machine ip` 命令只返回公共IP，因此我们必须采取不同的方法来获取私有IP。
- en: 'We can send a `GET` request to the droplets API:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向 `droplets` API 发送 `GET` 请求：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A part of the output is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一部分输出如下：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `droplets` API returned all information about all the droplets we own (at
    the moment only one). We are interested only in the private IP of the newly created
    instance called `swarm-1`. We can get it by filtering the results to include only
    the droplet named `swarm-1` and selecting the `v4` element with the type `private`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`droplets` API 返回了我们拥有的所有 droplet 的信息（目前只有一个）。我们只关心新创建的名为 `swarm-1` 实例的私有IP。我们可以通过过滤结果，只包括名为
    `swarm-1` 的 droplet，并选择类型为 `private` 的 `v4` 元素来获取它。'
- en: We'll use `jq` ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    to filter the output and get what we need. If you haven't already, please download
    and install the jq distribution suited for your OS.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `jq` ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    来过滤输出并获取我们需要的信息。如果你还没有安装，请下载并安装适合你操作系统的 jq 版本。
- en: 'The command that sends the request, filters the result, and stores the private
    IP as an environment variable is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 发送请求、过滤结果并将私有IP存储为环境变量的命令如下：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We sent a `GET` request to the `droplets` API, used the `jq select` statement
    to discard all the entries except the one with the name `swarm-1`. That was followed
    with another select statement that returned only the private address. The output
    was stored as the environment variable `MANAGER_IP`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向 `droplets` API 发送了 `GET` 请求，使用 `jq select` 语句丢弃了所有条目，除了名为 `swarm-1` 的条目。接着用了另一个
    select 语句，只返回了私有地址。输出被存储为环境变量 `MANAGER_IP`。
- en: 'To be on the safe side, we can echo the value of the newly created variable:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保险起见，我们可以回显新创建变量的值：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can execute the `swarm init` command in the same way as we did in the
    previous chapters:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像在前几章中一样执行 `swarm init` 命令：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s confirm that the cluster is indeed initialized:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认集群确实已经初始化：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，ID已删除）：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we initialized the cluster, we can add more nodes. We''ll start by
    creating two new instances and joining them as managers:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在集群已经初始化，我们可以添加更多节点。我们将首先创建两个新实例，并将它们作为管理节点加入：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There's no need to explain the commands we just executed since they are the
    combination of those we used before.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要解释我们刚刚执行的命令，因为它们是之前使用过的命令的组合。
- en: 'We''ll add a few worker nodes as well:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将添加一些工作节点：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s confirm that all five nodes are indeed forming the cluster:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认这五个节点确实在组成集群：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，ID已删除）：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: That's it. Our cluster is ready. The only thing left is to deploy a few services
    and confirm that the cluster is working as expected.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们的集群已经准备好。剩下的就是部署几个服务，并确认集群按预期工作。
- en: 'Since we already created the services quite a few times, we''ll speed up the
    process with the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    Compose stacks. They''ll create the `proxy`, `swarm-listener`, `go-demo-db`, and
    `go-demo` services:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经创建了服务很多次，我们将通过 `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    和 `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    Compose 堆栈加速这个过程。它们将创建 `proxy`、`swarm-listener`、`go-demo-db` 和 `go-demo` 服务：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Non-Windows users do not need to enter the `swarm-1` machine and can accomplish
    the same result by deploying the stacks directly from their laptops.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 非Windows用户无需进入 `swarm-1` 机器，可以直接从他们的笔记本电脑部署堆栈来实现相同的结果。
- en: 'It''ll take a few moments until all the images are downloaded. After a while,
    the output of the `service ls command` should be as follows (IDs are removed for
    brevity):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下载所有镜像需要一些时间。过一会儿，`service ls command` 的输出应该如下所示（为了简洁，已去除 ID）：
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s confirm that the `go-demo` service is accessible:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认 `go-demo` 服务是否可访问：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We set up the whole Swarm cluster using Docker Machine and the DigitalOcean
    API. Is that everything we need? That depends on the requirements we might define
    for our cluster. We should probably add a few Floating IP addresses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Docker Machine 和 DigitalOcean API 设置了整个 Swarm 集群。这是我们所需的一切吗？这取决于我们为集群定义的需求。我们可能应该添加一些浮动
    IP 地址。
- en: A DigitalOcean Floating IP is a publicly-accessible static IP address that can
    be assigned to one of your Droplets. A Floating IP can also be instantly remapped,
    via the DigitalOcean Control Panel or API, to one of your other Droplets in the
    same data center. This instant remapping capability grants you the ability to
    design and create **High Availability **(**HA**) server infrastructure setups
    that do not have a single point of failure, by adding redundancy to the entry
    point, or gateway, to your servers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 的浮动 IP 是一个公开可访问的静态 IP 地址，可以分配给您的一个 Droplet。通过 DigitalOcean 控制面板或
    API，浮动 IP 还可以立即重新映射到同一数据中心中的其他 Droplets。这种即时重映射的能力使您能够设计和创建**高可用性**（**HA**）服务器基础设施，这样就没有单点故障，通过为服务器的入口点或网关增加冗余，提升可靠性。
- en: In other words, we should probably set at least two Floating IPs and map them
    to two of the droplets in the cluster. Those two (or more) IPs would be set as
    our DNS records. That way, when an instance fails, and we replace it with a new
    one, we can remap the Elastic IP without affecting our users.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可能应该设置至少两个浮动 IP 并将它们映射到集群中的两个 Droplets。这两个（或更多）IP 将作为我们的 DNS 记录。这样，当某个实例故障并且我们用新实例替换时，我们可以重新映射弹性
    IP，而不会影响我们的用户。
- en: There are quite a few other improvements we could do. However, that would put
    us in an awkward position. We would be using a tool that is not meant for setting
    up a complicated cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 还有不少其他改进我们可以做。然而，这样做会让我们陷入一个尴尬的境地。我们将使用一个并不适合用来搭建复杂集群的工具。
- en: The creation of VMs was quite slow. Docker Machine spent too much time provisioning
    it with Ubuntu and installing Docker Engine. We can reduce that time by creating
    snapshots with Docker Engine pre-installed. However, with such an action, the
    main reason for using Docker Machine would be gone. Its primary usefulness is
    simplicity. Once we start complicating the setup with other resources, we'll realize
    that the simplicity is being replaced with too many ad-hoc commands.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 创建虚拟机的速度相当慢。Docker Machine 花费了太多时间来为其配置 Ubuntu 并安装 Docker 引擎。我们可以通过创建预装了 Docker
    引擎的快照来减少这一时间。然而，进行这样的操作后，使用 Docker Machine 的主要理由就会消失。它的主要优势是简单性。一旦我们开始用其他资源来复杂化设置，就会发现简单性被太多的临时命令所取代。
- en: Running `docker-machine` combined with API requests works great when we are
    dealing with a small cluster, especially when we want to create something fast
    and potentially not very durable. The biggest problem is that everything we've
    done so far has been ad-hoc commands. Chances are that we would not be able to
    reproduce the same steps the second time. Our infrastructure is not documented
    so our team would not know what constitutes our cluster.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `docker-machine` 并结合 API 请求，当我们处理一个小型集群时效果很好，特别是当我们想要快速创建某些东西并且这些东西可能不太持久时。最大的问题是，到目前为止我们所做的一切都是临时命令。很可能我们无法在第二次重复相同的步骤。我们的基础设施没有文档记录，因此我们的团队不知道我们的集群是什么组成的。
- en: My recommendation is to use `docker-machine` in DigitalOcean as a quick and
    dirty way to create a cluster mostly for demo purposes. It can be useful for production
    as well, as long as the cluster is relatively small.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是，在 DigitalOcean 使用 `docker-machine` 来快速搭建一个集群，主要用于演示目的。这对于生产环境也可以有用，只要集群相对较小。
- en: We should look at alternatives if we'd like to set up a more complex, bigger,
    and potentially more permanent solution.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要设置一个更复杂、更大且可能更持久的解决方案，我们应该考虑其他替代方案。
- en: 'Let us delete the cluster we created and explore the alternatives with a clean
    slate:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除我们创建的集群，并从零开始探索替代方案：
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you read the previous chapter, at this point you are probably expecting to
    see a sub-chapter named *Docker for DigitalOcean*. There is no such thing. At
    least not at the time I'm writing this chapter. Therefore, we'll jump right into
    *Packer* and *Terraform*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读过上一章，你可能会期待看到一个名为 *Docker for DigitalOcean* 的子章节。实际上并没有这样的章节。至少在我写这章的时候没有。因此，我们将直接进入
    *Packer* 和 *Terraform*。
- en: Setting up a Swarm cluster with Packer and Terraform
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Packer 和 Terraform 设置 Swarm 集群
- en: This time we'll use a set of tools completely unrelated to Docker. It'll be
    *Packer *([https://www.packer.io/](https://www.packer.io/)) and *Terraform* ([https://www.terraform.io/](https://www.terraform.io/)).
    Both are coming from *HashiCorp* ([https://www.hashicorp.com/](https://www.hashicorp.com/)).
    Packer allows us to create machine images. With Terraform we can create, change,
    and improve cluster infrastructure. Both tools support almost all the major providers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将使用一套与 Docker 完全无关的工具。它将是*Packer* ([https://www.packer.io/](https://www.packer.io/))
    和 *Terraform* ([https://www.terraform.io/](https://www.terraform.io/))。这两个工具都来自
    *HashiCorp* ([https://www.hashicorp.com/](https://www.hashicorp.com/))。Packer
    允许我们创建机器镜像，而 Terraform 则帮助我们创建、修改和改进集群基础设施。这两个工具几乎支持所有主要的云服务提供商。
- en: They can be used with Amazon EC2, CloudStack, DigitalOcean, **Google Compute
    Engine** (**GCE**), Microsoft Azure, VMWare, VirtualBox, and quite a few others.
    The ability to be infrastructure agnostic allows us to avoid vendor lock-in. With
    a minimal change in configuration, we can easily transfer our cluster from one
    provider to another. Swarm is designed to work seamlessly no matter which hosting
    provider we use, as long as the infrastructure is properly defined. With Packer
    and Terraform we can define infrastructure in such a way that transitioning from
    one to another is as painless as possible.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以与 Amazon EC2、CloudStack、DigitalOcean、**Google Compute Engine** (**GCE**)、Microsoft
    Azure、VMWare、VirtualBox 等多个平台一起使用。基础设施的独立性使我们能够避免厂商锁定。只需进行最小的配置更改，我们就能轻松地将集群从一个提供商迁移到另一个。Swarm
    被设计为无缝工作，无论我们使用哪个托管提供商，只要基础设施定义得当。使用 Packer 和 Terraform，我们可以以一种方式定义基础设施，使得从一个提供商切换到另一个提供商变得尽可能平滑。
- en: Using Packer to create DigitalOcean snapshots
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Packer 创建 DigitalOcean 快照
- en: 'The `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository already has the Packer and Terraform configurations we''ll use. They
    are located in the directory `terraform/do`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    仓库已经包含了我们将要使用的 Packer 和 Terraform 配置。它们位于 `terraform/do` 目录下：'
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The first step is to use Packer to create a snapshot. To do that, we''ll need
    our DigitalOcean API token set as the environment variable `DIGITALOCEAN_API_TOKEN`.
    It is the same token we set as the environment variable `DIGITALOCEAN_ACCESS_TOKEN`.
    Unfortunately, Docker Machine and Packer have different naming standards:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用 Packer 创建一个快照。为此，我们需要将 DigitalOcean 的 API 令牌设置为环境变量 `DIGITALOCEAN_API_TOKEN`。这与我们设置为环境变量
    `DIGITALOCEAN_ACCESS_TOKEN` 的令牌是相同的。不幸的是，Docker Machine 和 Packer 使用了不同的命名标准：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Please replace `[...]` with the actual token.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请将 `[...]` 替换为实际的令牌。
- en: We'll instantiate all Swarm nodes from the same snapshot. It'll be based on
    Ubuntu and have the latest Docker Engine installed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从同一个快照实例化所有 Swarm 节点。该快照将基于 Ubuntu，并安装最新版本的 Docker 引擎。
- en: 'The JSON definition of the image we are about to build is in `terraform/do/packer-ubuntu-docker.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json)):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将构建的镜像的 JSON 定义位于 `terraform/do/packer-ubuntu-docker.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json))：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The configuration consists of two sections: `builders` and `provisioners`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 配置由两个部分组成：`builders` 和 `provisioners`：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `builders` section defines all the information Packer needs to build a snapshot.
    The `provisioners` section describes the commands that will be used to install
    and configure software for the machines created by builders. The only required
    section is builders.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`builders` 部分定义了 Packer 构建快照所需的所有信息。`provisioners` 部分描述了用于为由 builders 创建的机器安装和配置软件的命令。唯一必须的部分是
    builders。'
- en: Builders are responsible for creating machines and generating images from them
    for various platforms. For example, there are separate builders for EC2, VMware,
    VirtualBox, and so on. Packer comes with many builders by default, and can also
    be extended to add new builders.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 构建器负责创建机器并生成适用于各种平台的镜像。例如，EC2、VMware、VirtualBox等都有单独的构建器。Packer默认包含许多构建器，并且还可以扩展以添加新的构建器。
- en: 'The builders section we''ll use is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的构建器部分如下：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Each type of builder has specific arguments that can be used. We specified that
    the `type` is `digitalocean`. Please visit the DigitalOcean Builder page ([https://www.packer.io/docs/builders/digitalocean.html](https://www.packer.io/docs/builders/digitalocean.html)) for
    more info.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的构建器都有特定的参数可以使用。我们指定了`type`为`digitalocean`。有关更多信息，请访问DigitalOcean构建器页面([https://www.packer.io/docs/builders/digitalocean.html](https://www.packer.io/docs/builders/digitalocean.html))。
- en: Please note, when using the `digitalocean` type, we have to provide the token.
    We could have specified it through the `api_token` field. However, there is an
    alternative. If the field is not specified, Packer will try to get the value from
    the environment variable `DIGITALOCEAN_API_TOKEN`. Since we already exported it,
    there's no need to repeat ourselves with the token inside Packer configuration.
    Moreover, the token should be secret.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用`digitalocean`类型时，我们必须提供token。我们可以通过`api_token`字段指定它。不过，还有另一种方法。如果没有指定该字段，Packer会尝试从环境变量`DIGITALOCEAN_API_TOKEN`中获取值。由于我们已经导出了它，因此无需在Packer配置中重复填写token。此外，token应该保密。
- en: Putting it inside the config would risk exposure.The region is critical since
    a snapshot can be created only within one region. If we wanted to share the same
    machine across multiple regions, each would need to be specified as a separate
    builder.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将其放入配置中可能会导致暴露风险。区域非常重要，因为快照只能在一个区域内创建。如果我们想要在多个区域共享相同的机器，每个区域都需要作为一个单独的构建器来指定。
- en: We set the image to `ubuntu-16-04-x64`. That will be the base image which we'll
    use to create our own. The size of the snapshot is not directly related to the
    size of the droplets we'll create, so there's no need to make it big. We set it
    to 1 GB.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将镜像设置为`ubuntu-16-04-x64`。这将是我们用来创建自己镜像的基础镜像。快照的大小与我们将要创建的Droplet的大小没有直接关系，因此无需将其设置得太大。我们将其设置为1GB。
- en: By default, DigitalOcean enables only public networking, so we defined `private_networking`
    as `true`. Later on, we'll set up Swarm communication to be available only through
    the private network.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，DigitalOcean只启用公共网络，因此我们将`private_networking`设置为`true`。稍后，我们将设置Swarm通信，仅通过私有网络进行。
- en: The `snapshot_name` field is the name we'll give to this snapshot. Since there
    is no option to overwrite an existing snapshot, the name must be unique so we
    added `timestamp` to the name.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`snapshot_name`字段是我们为此快照指定的名称。由于没有选项可以覆盖现有快照，因此名称必须是唯一的，因此我们在名称中加入了`timestamp`。'
- en: Please visit the DigitalOcean Builder page ([https://www.packer.io/docs/builders/digitalocean.html](https://www.packer.io/docs/builders/digitalocean.html)) for
    more information.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请访问DigitalOcean构建器页面([https://www.packer.io/docs/builders/digitalocean.html](https://www.packer.io/docs/builders/digitalocean.html))。
- en: The second section is provisioners. It contains an array of all the provisioners
    that Packer should use to install and configure software within running machines
    before turning them into snapshots.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分是provisioners。它包含了Packer在将机器转换为快照之前，安装和配置软件时应该使用的所有provisioner的数组。
- en: There are quite a few `provisioner` types we can use. If you read *The DevOps
    2.0 Toolkit *([https://www.amazon.com/dp/B01BJ4V66M](https://www.amazon.com/dp/B01BJ4V66M)),
    you know that I advocated Ansible as the `provisioner` of choice. Should we use
    it here as well? In most cases, when building images meant to run Docker containers,
    I opt for a simple shell. The reasons for a change from Ansible to Shell lies
    in the different objectives `provisioners` should fulfill when running on live
    servers, as opposed to building images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相当多的`provisioner`类型。如果你读过*《DevOps 2.0工具包》*([https://www.amazon.com/dp/B01BJ4V66M](https://www.amazon.com/dp/B01BJ4V66M))，你会知道我提倡将Ansible作为首选的`provisioner`。我们是否也应该在这里使用它？在大多数情况下，当构建用于运行Docker容器的镜像时，我更倾向于使用简单的Shell。将Ansible更改为Shell的原因在于，`provisioner`在运行实时服务器时需要完成的目标与在构建镜像时有所不同。
- en: Unlike Shell, Ansible (and most other `provisioners`) are idempotent. They verify
    the actual state and execute one action or another depending on what should be
    done for the desired state to be fulfilled. That's a great approach since we can
    run Ansible as many times as we want and the result will always be the same. For
    example, if we specify that we want to have JDK 8, Ansible will SSH into a destination
    server, discover that the JDK is not present and install it. The next time we
    run it, it'll discover that the JDK is already there and do nothing. Such an approach
    allows us to run Ansible playbooks as often as we want and it'll always result
    in JDK being installed. If we tried to accomplish the same through a Shell script,
    we'd need to write lengthy `if/else` statements. If JDK is installed, do nothing,
    if it's not installed, install it, if it's installed, but the version is not correct,
    upgrade it, and so on.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Shell 不同，Ansible（以及大多数其他`provisioners`）是幂等的。它们会验证实际状态，并根据需要执行一个或另一个操作，以确保所需状态得到满足。这是一种很好的方法，因为我们可以随意运行
    Ansible，结果总是相同的。例如，如果我们指定要安装 JDK 8，Ansible 会通过 SSH 连接到目标服务器，发现 JDK 没有安装，然后进行安装。下次运行时，它会发现
    JDK 已经安装并不做任何操作。这样的做法允许我们随时运行 Ansible playbook，最终都会安装 JDK。如果我们试图通过 Shell 脚本实现相同的功能，就需要编写冗长的`if/else`语句。如果
    JDK 已安装，则什么也不做；如果未安装，则安装；如果安装了但版本不对，则升级，依此类推。
- en: So, why not use it with Packer? The answer is simple. We do not need idempotency
    since we'll run it only once while creating an image. We won't use it on running
    instances. Do you remember the *pets vs cattle* discussion? Our VMs will be instantiated
    from an image that already has everything we need. If the state of that VM changes,
    we'll terminate it and create a new one. If we need to do an upgrade or install
    additional software, we won't do it inside the running instance, but create a
    new image, destroy running instances, and instantiate new ones based on the updated
    image.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不与 Packer 一起使用它呢？答案很简单。我们不需要幂等性，因为在创建镜像时只会运行一次。我们不会在运行的实例上使用它。你还记得 *宠物与牲畜*
    的讨论吗？我们的虚拟机将从已经包含我们所需的一切的镜像中实例化。如果该虚拟机的状态发生变化，我们将终止它并创建一个新的。如果我们需要进行升级或安装额外的软件，我们不会在运行中的实例内完成，而是创建一个新的镜像，销毁运行中的实例，并基于更新后的镜像实例化新的虚拟机。
- en: Is idempotency the only reason we would use Ansible? Definitely not! It is a
    very handy tool when we need to define complicated server setup. However, in our
    case the setup is simple. We need Docker Engine and not much more. Almost everything
    will be running inside containers. Writing a few Shell commands to install Docker
    is easier and faster than defining Ansible playbooks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等性是我们使用 Ansible 的唯一原因吗？绝对不是！当我们需要定义复杂的服务器配置时，Ansible 是一个非常方便的工具。然而，在我们的情况下，配置非常简单。我们只需要
    Docker 引擎，其他的没有太多需求。几条 Shell 命令就能轻松安装 Docker，比起定义 Ansible playbook，写 Shell 命令更简单、更快。
- en: It would probably take the same number of commands to install Ansible as to
    install Docker.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Ansible 所需的命令数量可能与安装 Docker 相同。
- en: Long story short, we'll use shell as our `provisioner` of choice for building
    AMIs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们将使用 shell 作为构建 AMI 的`provisioner`工具。
- en: 'The `provisioners` section we''ll use is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的`provisioners`部分如下：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The shell type is followed by a set of commands. They are the same as the commands
    we can find in the Install Docker *on Ubuntu* ([https://docs.docker.com/engine/installation/linux/ubuntulinux/](https://docs.docker.com/engine/installation/linux/ubuntulinux/))
    page.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: shell 类型后面跟着一组命令。这些命令与我们可以在《在 Ubuntu 上安装 Docker》([https://docs.docker.com/engine/installation/linux/ubuntulinux/](https://docs.docker.com/engine/installation/linux/ubuntulinux/))页面上找到的命令相同。
- en: 'Now that we have a general idea how Packer configuration works, we can proceed
    and build an image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们大致了解了 Packer 配置的工作原理，接下来可以继续构建镜像：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We run the `packer build` of the `packer-ubuntu-docker.json` with the `machine-readable`
    output sent to the `packer-ubuntu-docker.log` file. Machine readable output will
    allow us to parse it easily and retrieve the ID of the snapshot we just created.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行`packer build`命令，使用`packer-ubuntu-docker.json`并将`machine-readable`格式的输出发送到`packer-ubuntu-docker.log`文件。机器可读的输出将使我们能够轻松解析，并获取刚刚创建的快照的
    ID。
- en: 'The final lines of the output are as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Apart from the confirmation that the build was successful, the relevant part
    of the output is the line `id,sfo2:21373017`. It contains the snapshot ID we'll
    need to instantiate VMs based on the image. You might want to store the `packer-ubuntu-docker.log`
    in your code repository in case you need to get the ID from a different server.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确认构建成功外，输出的相关部分是行 `id,sfo2:21373017`。它包含我们需要的快照 ID，以便根据该镜像实例化虚拟机。你可能希望将 `packer-ubuntu-docker.log`
    存储在代码库中，以防你需要从其他服务器获取该 ID。
- en: The flow of the process we executed can be described through *figure 12-2:*
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的过程流可以通过 *图 12-2* 描述：
- en: '![](img/cloud-architecture-images-1.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cloud-architecture-images-1.png)'
- en: 'Figure 12-2: The flow of the Packer process'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12-2：Packer 过程的流程
- en: Now we are ready to create a Swarm cluster with VMs based on the snapshot we
    built.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好使用基于我们构建的快照的虚拟机来创建一个 Swarm 集群。
- en: Using Terraform to create a Swarm cluster in DigitalOcean
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Terraform 在 DigitalOcean 上创建 Swarm 集群
- en: 'Terraform is the third member of the *everyone-uses-a-different-environment-variable-for-the-token*
    club. It expects the token to be stored as environment variable `DIGITALOCEAN_TOKEN`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 是“*每个人都使用不同的环境变量来存储 token*”俱乐部的第三个成员。它期望将 token 存储为环境变量 `DIGITALOCEAN_TOKEN`：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Please replace `[...]` with the actual token.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请将 `[...]` 替换为实际的 token。
- en: Terraform does not force us to have any particular file structure. We can define
    everything in a single file. However, that does not mean that we should. Terraform
    configs can get big, and separation of logical sections into separate files is
    often a good idea. In our case, we'll have three `tf` files. The `terraform/do/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf))
    holds all the variables. If we need to change any parameter, we'll know where
    to find it. The `terraform/do/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf))
    file contains definitions of the elements that might be potentially reusable on
    other occasions. Finally, the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    file has the Swarm-specific resources. We'll explore each of the Terraform configuration
    files separately.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 不强制要求我们采用任何特定的文件结构。我们可以在一个文件中定义所有内容。然而，这并不意味着我们应该这样做。Terraform 配置文件可能会变得很大，将逻辑部分拆分到不同的文件中通常是一个好主意。在我们的案例中，我们将有三个
    `tf` 文件。`terraform/do/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf))
    文件包含所有变量。如果我们需要更改任何参数，我们知道在哪里可以找到它。`terraform/do/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf))
    文件包含可能在其他场合重复使用的元素定义。最后，`terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    文件包含与 Swarm 相关的资源。我们将分别探索每个 Terraform 配置文件。
- en: 'The content of the `terraform/do/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf))
    file is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform/do/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf))
    文件的内容如下：'
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `swarm_manager_token` and `swarm_worker_token` will be required to join
    the nodes to the cluster. The `swarm_snapshot_id` will hold the ID of the snapshot
    we created with Packer. The `swarm_manager_ip` variable is the IP of one of the
    managers that we'll need to provide for the nodes to join the cluster. The `swarm_managers`
    and `swarm_workers` define how many nodes we want of each. The `swarm_region`
    defines the region our cluster will run in while the `swarm_instance_size` is
    set to 1 GB. Feel free to change it to a bigger size if you start using this Terraform
    config to create a real cluster. Finally, the `swarm_init` variable allows us
    to specify whether this is the first run and the node should initialize the cluster.
    We'll see its usage very soon.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`swarm_manager_token`和`swarm_worker_token`将用于将节点加入集群。`swarm_snapshot_id`将保存我们使用Packer创建的快照的ID。`swarm_manager_ip`变量是我们需要提供的其中一个管理节点的IP，以便节点能够加入集群。`swarm_managers`和`swarm_workers`定义了我们希望拥有的每种类型节点的数量。`swarm_region`定义了集群运行的区域，而`swarm_instance_size`设置为1GB。如果您开始使用此Terraform配置创建一个实际的集群，可以随时更改为更大的大小。最后，`swarm_init`变量允许我们指定是否为第一次运行，并且节点应初始化集群。我们很快会看到它的用法。'
- en: 'The content of the `terraform/do/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf))
    file is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform/do/common.tf`（[https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf)）文件的内容如下：'
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Each resource is defined with a type (For example, `digitalocean_ssh_key`) and
    a name (For example, `docker`). The type determines which resource should be created
    and must be one of those currently supported.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个资源都定义了一个类型（例如，`digitalocean_ssh_key`）和一个名称（例如，`docker`）。类型决定了应该创建哪个资源，并且必须是当前支持的类型之一。
- en: The first resource `digitalocean_ssh_key` allows us to manage SSH keys for droplet
    access. Keys created with this resource can be referenced in your droplet configuration
    via their ID or fingerprint. We set it as the value of the `devops21-do.pub` file
    that we'll create soon.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个资源`digitalocean_ssh_key`允许我们管理用于访问滴水（droplet）的SSH密钥。通过此资源创建的密钥可以通过其ID或指纹在滴水配置中引用。我们将其设置为稍后将创建的`devops21-do.pub`文件的值。
- en: The second resource we're using is the `digitalocean_floating_ip`. It represents
    a publicly-accessible static IP address that can be mapped to one of our droplets.
    We defined three of those. They would be used in our DNS configuration. That way,
    when a request is made to your domain, DNS redirects it to one of the floating
    IPs. If one the droplets is down, DNS should use one of the other entries. That
    way, you'd have time to change the floating IP from the failed to a new droplet.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的第二个资源是`digitalocean_floating_ip`。它代表一个可公开访问的静态IP地址，可以映射到我们的一个滴水上。我们定义了三个这样的IP地址。它们将在我们的DNS配置中使用。这样，当对您的域名发出请求时，DNS会将其重定向到其中一个浮动IP。如果其中一个滴水宕机，DNS应该使用其他条目之一。这样，您就有时间将浮动IP从失败的滴水切换到新的滴水。
- en: Please consult the *DIGITALOCEAN_SSH_KEY* ([https://www.terraform.io/docs/providers/do/r/ssh_key.html](https://www.terraform.io/docs/providers/do/r/ssh_key.html))
    and *DIGITALOCEAN_FLOATING_IP* ([https://www.terraform.io/docs/providers/do/r/floating_ip.html](https://www.terraform.io/docs/providers/do/r/floating_ip.html))
    pages for more info.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*DIGITALOCEAN_SSH_KEY*（[https://www.terraform.io/docs/providers/do/r/ssh_key.html](https://www.terraform.io/docs/providers/do/r/ssh_key.html)）和*DIGITALOCEAN_FLOATING_IP*（[https://www.terraform.io/docs/providers/do/r/floating_ip.html](https://www.terraform.io/docs/providers/do/r/floating_ip.html)）页面以获取更多信息。
- en: Besides the resources, we also defined a few outputs. They represent values
    that will be displayed when Terraform apply is executed and can be queried easily
    using the output command.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了资源，我们还定义了几个输出。它们表示在执行Terraform apply时将显示的值，并且可以通过输出命令轻松查询。
- en: When building potentially complex infrastructure, Terraform stores hundreds
    or thousands of attribute values for all resources. But, as a user, we may only
    be interested in a few values of importance, such as manager IPs. Outputs are
    a way to tell Terraform what data is relevant.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建潜在的复杂基础设施时，Terraform会为所有资源存储数百或数千个属性值。但是，作为用户，我们可能只对几个重要的值感兴趣，例如管理节点IP。输出是告诉Terraform哪些数据是相关的一种方式。
- en: In our case, the outputs are the addresses of the floating IPs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，输出是浮动IP的地址。
- en: Please consult the *Output Configuration* ([https://www.terraform.io/docs/configuration/outputs.html](https://www.terraform.io/docs/configuration/outputs.html))
    page for more info.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*输出配置*([https://www.terraform.io/docs/configuration/outputs.html](https://www.terraform.io/docs/configuration/outputs.html))页面以获取更多信息。
- en: Now comes the real deal. The `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    file contains the definition of all the instances we'll create.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入真正的重点。`terraform/do/swarm.tf`([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))文件包含了我们将要创建的所有实例的定义。
- en: Since the content of this file is a bit bigger than the others, we'll examine
    each resource separately.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个文件的内容比其他文件稍大，我们将分别检查每个资源。
- en: 'The first resource in line is the `digitalocean_droplet` type named `swarm-manager`.
    Its purpose is to create Swarm manager nodes:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项资源是名为`swarm-manager`的`digitalocean_droplet`类型。它的目的是创建Swarm管理节点：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The resource contains the image that references the snapshot we created with
    Packer. The actual value is a variable that we'll define at runtime. The size
    specifies the size of the instance we want to create. The default value is fetched
    from the variable `swarm_instance_size`. By default, it is set to 1 GB. Just as
    any other variable, it can be overwritten at runtime.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 该资源包含了引用我们用Packer创建的快照的镜像。实际的值是一个变量，我们将在运行时定义。`size`指定了我们想要创建的实例的大小。默认值从变量`swarm_instance_size`中获取，默认设置为1
    GB。像其他变量一样，它可以在运行时被重写。
- en: The count field defines how many managers we want to create. The first time
    we run terraform, the value should be 1 since we want to start with one manager
    that will initialize the cluster. Afterward, the value should be whatever is defined
    in variables. We'll see the use case of both combinations soon.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`count`字段定义了我们想要创建的管理节点数量。当我们第一次运行terraform时，值应该是1，因为我们希望从一个管理节点开始，初始化集群。之后，值应根据变量中定义的内容进行调整。我们很快就会看到这两种组合的使用情况。'
- en: The name, the region, and the `private_networking` should be self-explanatory.
    The ssh-keys type is an array that, at this moment, contains only one element;
    the ID of the `digitalocean_ssh_key` resource we defined in the `common.tf` file.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`name`、`region`和`private_networking`应该是自解释的。`ssh-keys`类型是一个数组，此时仅包含一个元素；即我们在`common.tf`文件中定义的`digitalocean_ssh_key`资源的ID。'
- en: The connection field defines the SSH connection details. The user will be root.
    Instead of a password, we'll use the `devops21-do` key.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`connection`字段定义了SSH连接的详细信息。用户将是root。我们将使用`devops21-do`密钥，而不是密码。'
- en: Finally, the `provisioner` is defined. The idea is to do as much provisioning
    as possible during the creation of the images. That way, instances are created
    much faster since the only action is to create a VM out of an image. However,
    there is often a part of provisioning that cannot be done when creating an image.
    The `swarm init` command is one of those. We cannot initialize the first Swarm
    node until we get the IP of the server. In other words, the server needs to be
    running (and therefore has an IP) before the `swarm init` command is executed.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，定义了`provisioner`。这个想法是在创建镜像的过程中尽可能多地进行预配置。这样，实例创建的速度会更快，因为唯一的操作就是从镜像创建虚拟机。然而，通常有些预配置是无法在创建镜像时完成的。`swarm
    init`命令就是其中之一。在获取服务器的IP之前，我们无法初始化第一个Swarm节点。换句话说，服务器需要运行（因此拥有IP）后，才能执行`swarm init`命令。
- en: Since the first node has to initialize the cluster while any other should join,
    we're using `if` statements to distinguish one case from the other. If the variable
    `swarm_init` is `true`, the `docker swarm init` command will be executed. On the
    other hand, if the variable `swarm_init` is set to `false`, the command will be
    `docker swarm join`. In that case, we are using another variable `swarm_manager_ip` to
    tell the node which manager to use to join the cluster. Please note that the IP
    is obtained using the special syntax `self.ipv4_address_private`. We are referencing
    oneself and getting the `ipv4_address_private`. There are many other attributes
    we can get from a resource. Please consult the *DIGITALOCEAN_DROPLET* ([https://www.terraform.io/docs/providers/do/r/droplet.html](https://www.terraform.io/docs/providers/do/r/droplet.html))
    page for more info.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第一个节点需要初始化集群，而其他节点则应加入集群，因此我们使用`if`语句来区分两种情况。如果变量`swarm_init`为`true`，则将执行`docker
    swarm init`命令。另一方面，如果变量`swarm_init`设置为`false`，则命令将是`docker swarm join`。在这种情况下，我们使用另一个变量`swarm_manager_ip`来告诉节点使用哪个管理节点加入集群。请注意，IP
    是通过特殊语法`self.ipv4_address_private`获得的。我们引用了自身并获取了`ipv4_address_private`。从资源中我们可以获取许多其他属性。请参阅*DIGITALOCEAN_DROPLET*([https://www.terraform.io/docs/providers/do/r/droplet.html](https://www.terraform.io/docs/providers/do/r/droplet.html))页面获取更多信息。
- en: 'Let''s take a look at the `digitalocean_droplet` resource named `swarm-worker`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下名为`swarm-worker`的`digitalocean_droplet`资源：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `swarm-worker` resource is almost identical to `swarm-manager`. The only
    difference is in the count field that uses the `swarm_workers` variable and the
    `provisioner`. Since a worker cannot initialize a cluster, there was no need for
    `if` statements, so the only command we want to execute is `docker swarm join`.
    Terraform uses a naming convention that allows us to specify values as environment
    variables by adding the `TF_VAR_ prefix`. For example, we can specify the value
    of the variable `swarm_snapshot_id` by setting the environment variable `TF_VAR_swarm_snapshot_id`.
    The alternative is to use the `-var` argument. I prefer environment variables
    since they allow me to specify them once instead of adding `-var` to every command.
    The last part of the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    specification are outputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`swarm-worker`资源与`swarm-manager`几乎相同。唯一的区别在于使用`swarm_workers`变量的`count`字段和`provisioner`。由于工作节点无法初始化集群，因此不需要`if`语句，因此我们只需要执行`docker
    swarm join`命令。Terraform 使用命名约定允许我们通过添加`TF_VAR_`前缀来指定值作为环境变量。例如，我们可以通过设置环境变量`TF_VAR_swarm_snapshot_id`来指定变量`swarm_snapshot_id`的值。另一种方式是使用`-var`参数。我更喜欢使用环境变量，因为它们允许我只指定一次，而不是在每个命令中都添加`-var`。`terraform/do/swarm.tf`([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))规范的最后一部分是输出。'
- en: 'The outputs we defined are as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的输出如下：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: They are public and private IPs of the managers. Since there are only a few
    (if any) reasons to know worker IPs, we did not define them as outputs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是管理节点的公有和私有 IP 地址。由于只有少数（如果有的话）需要知道工作节点的 IP 地址，我们没有将其定义为输出。
- en: 'Since we''ll use the snapshot we created with Packer, we need to retrieve the
    ID from the `packer-ubuntu-docker.log`. Let''s take another look at the file:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用通过 Packer 创建的快照，因此我们需要从`packer-ubuntu-docker.log`中检索 ID。让我们再看一遍文件：
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The important line of the output is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的重要行如下：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The command that follows parses the output and extracts the ID:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的命令解析输出并提取 ID：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s double-check that the command worked as expected:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再确认一下命令是否按预期执行：
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We got the ID of the snapshot. Before we start creating the resources, we need
    to create the SSH key `devops21-do` we referenced in the Terraform config.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了快照的 ID。在开始创建资源之前，我们需要创建在 Terraform 配置中引用的 SSH 密钥`devops21-do`。
- en: 'We''ll create the SSH key using `ssh-keygen`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`ssh-keygen`创建 SSH 密钥：
- en: '[PRE42]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: When asked to *enter file in which to save the key*, please answer with `devops21-do`.
    The rest of the questions can be answered in any way you see fit. I'll leave them
    all empty.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示*输入保存密钥的文件*时，请回答`devops21-do`。其他问题可以按您的喜好回答。我会将它们全部留空。
- en: 'The output should be similar to the one that follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于以下内容：
- en: '[PRE43]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now that the `devops21-do` key is created, we can start using Terraform. Before
    we create our cluster and the infrastructure around it, we should ask Terraform
    to show us the execution plan.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`devops21-do`密钥已经创建，我们可以开始使用 Terraform。在创建集群及其周围基础设施之前，我们应该要求 Terraform 显示执行计划。
- en: '**A note to Terraform v0.8+users. **'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**给 Terraform v0.8+ 用户的注意事项。**'
- en: 'Normally, we would not need to specify targets to see the whole execution plan.
    However, Terraform *v0.8* introduced a bug that sometimes prevents us from outputting
    a plan if a resource has a reference to another, not yet created, resource. In
    this case, the `digitalocean_floating_ip.docker_2` and `digitalocean_floating_ip.docker_3`
    are such resources. The targets from the command that follows are intended to
    act as a workaround until the problem is fixed:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不需要指定目标就能查看完整的执行计划。然而，Terraform *v0.8* 引入了一个 bug，有时会阻止我们输出计划，特别是当资源引用了一个尚未创建的其他资源时。在这种情况下，`digitalocean_floating_ip.docker_2`和`digitalocean_floating_ip.docker_3`就是这样的资源。接下来的命令中的目标是作为一种临时解决方案，直到问题被修复：
- en: '[PRE44]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The result is an extensive list of resources and their properties. Since the
    output is too big to be printed, I''ll limit the output only to the resource types
    and names:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含资源及其属性的详细列表。由于输出过大，无法全部打印，我将仅限于资源类型和名称：
- en: '[PRE45]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Since this is the first execution, all the resources would be created if we
    were to execute `terraform apply`. We would get five droplets; three managers
    and two workers. That would be accompanied by three floating IPs and one SSH key.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是第一次执行，如果我们执行`terraform apply`，所有资源将会被创建。我们将获得五个 droplet，三个管理节点和两个工作节点。与此同时，还会创建三个浮动
    IP 和一个 SSH 密钥。
- en: If you see the complete output, you'll notice that some of the property values
    are set to `<computed>`. That means that Terraform cannot know what will be the
    actual values until it creates the resources. A good example is IPs. They do not
    exist until the droplet is created.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看完整的输出，你会注意到一些属性值被设置为`<computed>`。这意味着 Terraform 在创建资源之前无法知道实际的值。一个好的例子是
    IP 地址。它们在 droplet 创建之前是不存在的。
- en: 'We can also output the plan using the `graph` command:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`graph`命令输出计划：
- en: '[PRE46]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE47]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: That, in itself, is not very useful.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这本身并不是很有用。
- en: The `graph` command is used to generate a visual representation of either a
    configuration or an execution plan. The output is in the DOT format, which can
    be used by GraphViz to make graphs.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph`命令用于生成配置或执行计划的可视化表示。输出采用 DOT 格式，可以被 GraphViz 用来生成图形。'
- en: Please open *Graphviz* Download page ([http://www.graphviz.org/Download.php](http://www.graphviz.org/Download.php))
    and download and install the distribution compatible with your OS.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问*Graphviz*下载页面([http://www.graphviz.org/Download.php](http://www.graphviz.org/Download.php))，下载并安装与您的操作系统兼容的发行版。
- en: 'Now we can combine the `graph` command with `dot`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将`graph`命令与`dot`结合使用：
- en: '[PRE48]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output should be the same as in the *Figure 11-10:*
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该与*图 11-10：*中的相同。
- en: '![](img/terraform-graph-1.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/terraform-graph-1.png)'
- en: 'Figure 12-3: The image generated by Graphviz from the output of the terraform
    graph command'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12-3：由 Graphviz 从 terraform graph 命令的输出生成的图像
- en: Visualization of the plan allows us to see the dependencies between different
    resources. In our case, all resources will use the `digitalocean` provider. Both
    instance types will depend on the SSH key docker, and the floating IPs will be
    attached to manager droplets.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对计划的可视化让我们能够看到不同资源之间的依赖关系。在我们的案例中，所有资源都将使用`digitalocean`提供程序。两种实例类型将依赖于 SSH
    密钥 docker，而浮动 IP 将附加到管理节点的 droplet 上。
- en: When dependencies are defined, we don't need to specify explicitly all the resources
    we need.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义了依赖关系时，我们不需要明确指定所有需要的资源。
- en: 'As an example, let''s take a look at the plan Terraform will generate when
    we limit it only to one Swarm manager node so that we can initialize the cluster:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，让我们来看一下当我们限制只有一个 Swarm 管理节点时，Terraform 生成的计划，以便初始化集群：
- en: '[PRE49]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The runtime variables `swarm_init` and `swarm_managers` will be used to tell
    Terraform that we want to initialize the cluster with one manager. The plan command
    takes those variables into account and outputs the execution plan.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时变量`swarm_init`和`swarm_managers`将被用来告诉 Terraform 我们想用一个管理节点来初始化集群。plan 命令会考虑这些变量并输出执行计划。
- en: 'The output, limited only to resource types and names, is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于资源类型和名称，如下所示：
- en: '[PRE50]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Even though the specified that we want only the plan for the `swarm-manager`
    resource, Terraform noticed that it depends on the SSH key docker, and included
    it in the execution plan.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们指定只想要`swarm-manager`资源的计划，Terraform仍然注意到它依赖于SSH密钥docker，并将其包含在执行计划中。
- en: 'We''ll start small and create only one manager instance that will initialize
    the cluster. As we saw from the plan, it depends on the SSH key, so Terraform
    will create it as well:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从小做起，首先创建一个管理节点实例来初始化集群。正如我们从计划中看到的，它依赖于SSH密钥，因此Terraform也会创建它：
- en: '[PRE51]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The output is too big to be presented fully in the book. If you look at it from
    your terminal, you'll notice that the SSH key is created first since `swarm-manager`
    depends on it. Please note that we did not specify the dependency explicitly.
    However, since the resource has it specified in the `ssh_keys` field, Terraform
    understood that it is the dependency.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输出太大，无法在书中完全展示。如果你从终端查看，会注意到首先创建的是SSH密钥，因为`swarm-manager`依赖于它。请注意，我们没有显式指定依赖关系。然而，由于资源在`ssh_keys`字段中指定了它，Terraform理解它是依赖关系。
- en: Once the `swarm-manager` instance is created, Terraform waited until SSH access
    is available. After it had managed to connect to the new instance, it executed
    provisioning commands that initialized the cluster.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`swarm-manager`实例创建完成，Terraform会等待直到SSH访问可用。在成功连接到新实例后，它执行了初始化集群的配置命令。
- en: 'The final lines of the output are as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE52]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The outputs are defined at the bottom of the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    file. Please note that not all outputs are listed but only those of the resources
    that were created.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输出定义在`terraform/do/swarm.tf`文件的底部（[https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf)）。请注意，并非所有输出都列出，而是只列出了已创建资源的输出。
- en: We can use the public IP of the newly created droplet and SSH into it.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用新创建的Droplet的公共IP并通过SSH连接到它。
- en: You might be inclined to copy the IP. There's no need for that. Terraform has
    a command that can be used to retrieve any information we defined as the output.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会倾向于复制IP地址，但不需要这样做。Terraform有一个命令可以用来检索我们定义为输出的任何信息。
- en: 'The command that retrieves the public IP of the first, and currently the only
    manager is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 检索第一个且当前唯一管理节点的公共IP的命令如下：
- en: '[PRE53]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE54]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can leverage the output command to construct SSH commands. As an example,
    the command that follows will SSH into the machine and retrieve the list of Swarm
    nodes:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用输出命令构建SSH命令。作为一个例子，接下来的命令将通过SSH连接到机器并获取Swarm节点列表：
- en: '[PRE55]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已移除ID）：
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'From now on, we won''t be limited to a single manager node that initialized
    the cluster. We can create all the rest of the nodes. However, before we do that,
    we need to discover the manager and worker tokens. For security reasons, it is
    better that they are not stored anywhere, so we''ll create environment variables:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们不再局限于初始化集群的单个管理节点。我们可以创建所有其他节点。然而，在此之前，我们需要获取管理节点和工作节点的令牌。出于安全原因，最好不要将其存储在任何地方，因此我们将创建环境变量：
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We''ll also need to set the environment variable `swarm_manager_ip`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要设置环境变量`swarm_manager_ip`：
- en: '[PRE58]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Even though we could use `digitalocean_droplet.swarm-manager.0.private_ip` inside
    the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    it is a good idea to have it defined as an environment variable. That way, if
    the first manager fails, we can easily change it to `swarm_manager_2_private_ip`
    without modifying the `.tf` files.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们可以在`terraform/do/swarm.tf`中使用`digitalocean_droplet.swarm-manager.0.private_ip`（[https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf)），但将其定义为环境变量是一个好主意。这样，如果第一个管理节点失败，我们可以轻松将其更改为`swarm_manager_2_private_ip`，而无需修改`.tf`文件。
- en: 'Now, let us see the plan for the creation of the rest of Swarm nodes:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看创建其余Swarm节点的计划：
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The relevant lines of the output are as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关行如下：
- en: '[PRE60]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can see that the plan is to create four new resources. Since we already have
    one manager running and specified that the desired number is three, two additional
    managers will be created together with two workers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，计划是创建四个新资源。由于我们已经有一个管理节点在运行，并且指定了所需数量为三个，所以将创建两个额外的管理节点和两个工作节点。
- en: 'Let''s apply the execution plan:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用执行计划：
- en: '[PRE61]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The last lines of the output are as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE62]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: All four resources were created, and we got the output of the manager public
    and private IPs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 四个资源已被创建，并且我们得到了管理节点的公有和私有IP的输出。
- en: 'Let''s enter into one of the managers and confirm that the cluster indeed works:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入其中一个管理节点并确认集群确实工作正常：
- en: '[PRE63]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output of the `node ls` command is as follows (IDs are removed for brevity):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`node ls`命令的输出如下（为了简洁，ID已被移除）：'
- en: '[PRE64]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: All the nodes are present, and the cluster seems to be working.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所有节点都存在，集群似乎在正常工作。
- en: 'To be fully confident that everything works as expected, we''ll deploy a few
    services. Those will be the same services we were creating throughout the book,
    so we''ll save us some time and deploy the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    stacks:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全确信一切按预期工作，我们将部署一些服务。这些将是我们在整本书中创建的相同服务，因此我们将节省一些时间并部署`vfarcic/docker-flow-proxy/docker-compose-stack.yml`（[https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml)）和`vfarcic/go-demo/docker-compose-stack.yml`（[https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml)）堆栈：
- en: '[PRE65]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We downloaded the stacks from the repositories and executed the `stack deploy`
    commands.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从仓库下载了堆栈并执行了`stack deploy`命令。
- en: 'All we have to do now is wait for a few moments, execute the `service ls` command,
    and confirm that all the replicas are running:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要做的就是等待片刻，执行`service ls`命令，确认所有副本都在运行：
- en: '[PRE66]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output of the `service ls` command should be as follows (IDs are removed
    for brevity):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ls`命令的输出应如下所示（为了简洁，ID已被移除）：'
- en: '[PRE67]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, let''s send a request to the `go-demo` service through the `proxy`.
    If it returns the correct response, we''ll know that everything works correctly:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过`proxy`向`go-demo`服务发送请求。如果返回正确的响应，我们就知道一切正常：
- en: '[PRE68]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE69]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: It works!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 它工作正常！
- en: 'Are we finished? We probably are. As a last check, let''s validate that the
    `proxy` is accessible from outside the servers. We can confirm that by exiting
    the server and sending a request from our laptop:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了吗？我们可能完成了。作为最后的检查，让我们验证`proxy`是否可以从服务器外部访问。我们可以通过退出服务器并从我们的笔记本发送请求来确认：
- en: '[PRE70]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE71]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We are still missing floating IPs. While they are not necessary for this demo,
    we would create them if this were a production cluster, and use them to configure
    our DNSes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然缺少浮动IP。虽然在这个演示中它们不是必须的，但如果这是生产集群，我们会创建它们并用它们来配置我们的DNS。
- en: 'This time, we can create the plan without specifying the targets:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们可以在不指定目标的情况下创建计划：
- en: '[PRE72]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The relevant parts of the output are as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE73]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: As you can see, Terraform detected that all the resources except floating IPs
    are already created so it created the plan that would execute the creation of
    only three resources.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Terraform检测到除了浮动IP外，所有资源已经创建，因此它生成了一个计划，只执行三项资源的创建。
- en: 'Let''s apply the plan:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用该计划：
- en: '[PRE74]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE75]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The floating IPs were created and we can see the output of their IPs.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 浮动IP已被创建，我们可以看到它们的IP输出。
- en: 'The only thing left is to confirm that floating IPs are indeed created and
    configured correctly. We can do that by sending a request through one of them:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的就是确认浮动IP确实被创建并正确配置。我们可以通过发送请求通过其中一个IP来确认：
- en: '[PRE76]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'As expected, the output is status `200 OK`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，输出是状态`200 OK`：
- en: '[PRE77]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Let's see what happens if we simulate a failure of an instance.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果模拟一个实例故障会发生什么。
- en: We'll delete an instance using the DigitalOcean API. We could use Terraform
    to remove an instance. However, removing it with the API will be a closer simulation
    of an unexpected failure of a node.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用DigitalOcean API删除一个实例。我们也可以使用Terraform来删除实例。然而，使用API删除将更接近模拟一个节点的意外失败。
- en: To remove an instance, we need to find its ID. We can do that with the `terraform
    show` command.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除一个实例，我们需要找到它的ID。我们可以通过`terraform show`命令来做到这一点。
- en: 'Let''s say that we want to remove the second worker. The command to find all
    its information is as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要移除第二个工作节点。查找其所有信息的命令如下：
- en: '[PRE78]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The output is as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE79]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Among other pieces of data, we got the ID. In my case, it is `33909722`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他数据中，我们获得了ID。在我的例子中，它是`33909722`。
- en: 'Before running the command that follows, please change the ID to the one you
    got from the `terraform state show` command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行接下来的命令之前，请将ID更改为您从`terraform state show`命令中获得的ID：
- en: '[PRE80]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The relevant part of the output is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE81]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: DigitalOcean does not provide any response content to `DELETE` requests, so
    the status `204` indicates that the operation was successful.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean对`DELETE`请求不提供任何响应内容，因此状态`204`表示操作成功。
- en: It will take a couple of moments until the droplet is removed entirely.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 直到Droplet被完全移除大约需要几分钟。
- en: 'Let''s run the `terraform plan` command one more time:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再运行一次`terraform plan`命令：
- en: '[PRE82]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The relevant parts of the output are as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE83]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Terraform deduced that one resource `swarm-worker.1` needs to be added to reconcile
    the discrepancy between the state it has stored locally and the actual state of
    the cluster.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform推断出需要添加一个资源`swarm-worker.1`，以解决它本地存储的状态与集群实际状态之间的差异。
- en: 'All we have to do to restore the cluster to the desirable state is to run `terraform
    apply`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要将集群恢复到期望状态，我们所要做的就是运行`terraform apply`：
- en: '[PRE84]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The relevant parts of the output are as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE85]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: We can see that one resource was added. The terminated worker has been recreated,
    and the cluster continues operating at its full capacity.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到添加了一个资源。已终止的工作节点已经被重新创建，集群继续以其满负荷运转。
- en: The state of the cluster is stored in the `terraform.tfstate` file. If you are
    not running it always from the same computer, you might want to store that file
    in your repository together with the rest of your configuration files. The alternative
    is to use Remote State ([https://www.terraform.io/docs/state/remote/index.html](https://www.terraform.io/docs/state/remote/index.html))
    and, for example, store it in Consul.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的状态存储在`terraform.tfstate`文件中。如果你不是总是在同一台电脑上运行它，可能需要将这个文件与其他配置文件一起存储在你的代码库中。另一种选择是使用远程状态([https://www.terraform.io/docs/state/remote/index.html](https://www.terraform.io/docs/state/remote/index.html))，例如将其存储在Consul中。
- en: Changing the desired state of the cluster is easy as well. All we have to to
    is add more resources and rerun `terraform apply`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 更改集群的期望状态也很简单。我们所需要做的就是添加更多资源并重新运行`terraform apply`。
- en: We are finished with the brief introduction to Terraform for DigitalOcean.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了对Terraform在DigitalOcean上的简要介绍。
- en: 'The flow of the process we executed can be described through *Figure 12-4*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的过程流可以通过*图12-4*来描述：
- en: '![](img/cloud-architecture-instances-1.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cloud-architecture-instances-1.png)'
- en: 'Figure 12-4: The flow of the Terraform process'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图12-4：Terraform过程的流程
- en: 'Let''s destroy what we did before we compare the different approaches we took
    to create and manage a Swarm cluster in DigitalOcean:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们比较不同的创建和管理Swarm集群的方法之前，让我们销毁之前的操作：
- en: '[PRE86]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The last line of the output is as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后一行如下：
- en: '[PRE87]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The cluster is gone as if it never existed, saving us from unnecessary expenses.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 集群已经消失，仿佛它从未存在过，帮助我们节省了不必要的开支。
- en: Let's see how to remove a snapshot.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何删除一个快照。
- en: Before we remove the snapshot we created, we need to find its `ID`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除我们创建的快照之前，我们需要找到它的`ID`。
- en: 'The request that will return the list of all snapshots is as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回所有快照列表的请求如下：
- en: '[PRE88]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The output of the response is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 响应的输出如下：
- en: '[PRE89]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'We’ll use `jq` to get the snapshot ID:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`jq`获取快照ID：
- en: '[PRE90]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: We sent an `HTTP GET` request to retrieve all snapshots and used `jq` to retrieve
    only the ID. The result was stored in the environment variable `SNAPSHOT_ID`.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发送了一个`HTTP GET`请求来获取所有快照，并使用`jq`仅提取ID。结果存储在环境变量`SNAPSHOT_ID`中。
- en: 'Now we can send a `DELETE` request that will remove the snapshot:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以发送一个`DELETE`请求来删除快照：
- en: '[PRE91]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The relevant output of the response is as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 响应的相关输出如下：
- en: '[PRE92]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: The snapshot has been removed. No resources are running on the DigitalOcean
    account, and you will not be charged anything more than what you spent from running
    the exercises in this chapter.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 快照已被删除。DigitalOcean 账户上没有正在运行的资源，你将不会被收取超过本章练习所花费的费用。
- en: Choosing the right tools to create and manage Swarm clusters in DigitalOcean
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的工具来创建和管理 DigitalOcean 中的 Swarm 集群
- en: We tried two different combinations to create a Swarm cluster in DigitalOcean.
    We used *Docker Machine* with the *DigitalOcean API* and *Packer* with *Terraform*.
    That is, by no means, the final list of the tools we can use. The time is limited,
    and I promised to myself that this book will be shorter than *War and Peace*,
    so I had to draw the line somewhere. Those two combinations are, in my opinion,
    the best candidates as your tools of choice. Even if you do choose something else,
    this chapter, hopefully, gave you an insight into the direction you might want
    to take.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了两种不同的组合来在 DigitalOcean 上创建 Swarm 集群。我们使用了*Docker Machine*与*DigitalOcean
    API*，以及*Packer*与*Terraform*。这绝不是我们可以使用的工具的最终清单。时间有限，我也向自己承诺，这本书会比*战争与和平*要短，所以我必须在某些地方划定界限。在我看来，这两种组合是作为首选工具的最佳候选。即使你选择其他工具，本章也希望能给你一些关于你可能想走的方向的启示。
- en: Most likely you won't use both combinations so the million dollar question is
    which one should it be?
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能你不会同时使用这两种组合，那么百万美元的问题是，应该选择哪一个？
- en: Only you can answer that question. Now you have the practical experience that
    should be combined with the knowledge of what you want to accomplish. Each use
    case is different, and no combination would be the best fit for everyone.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 只有你能回答这个问题。现在你已经有了实践经验，这些经验应该与想要实现的目标相结合。每个使用场景都不同，没有一种组合能适合所有人。
- en: Nevertheless, I will provide a brief overview and some of the use-cases that
    might be a good fit for each combination.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我还是会提供一个简要概述，并介绍一些可能适合每种组合的使用场景。
- en: To Docker Machine or not to Docker Machine?
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker Machine 还是不使用 Docker Machine？
- en: Docker Machine is the weaker solution we explored. It is based on ad-hoc commands
    and provides little more than a way to create droplets and install Docker Engine.
    It uses *Ubuntu 15.10* as the base snapshot. Not only that it is old but is a
    temporary release. If we choose to use Ubuntu, the correct choice is 16.04 **Long
    Term Support **(**LTS**).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Machine 是我们探索的较弱解决方案。它基于临时命令，提供的功能仅限于创建 droplets 和安装 Docker 引擎。它使用 *Ubuntu
    15.10* 作为基础镜像。它不仅过时，而且是一个临时版本。如果我们选择使用 Ubuntu，正确的选择应是 16.04 **长期支持**（**LTS**）版本。
- en: If Docker Machine would, at least, provide the minimum setup for Swarm Mode
    (as it did with the old Standalone Swarm), it could be a good choice for a small
    cluster.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Docker Machine 至少能为 Swarm 模式提供最基本的设置（就像它为旧版 Standalone Swarm 提供的那样），它可能是小型集群的一个不错选择。
- en: As it is now, the only true benefit Docker Machine provides when working with
    a Swarm cluster in DigitalOcean is Docker Engine installation on a remote node
    and the ability to use the `docker-machine env` command to make our local Docker
    client seamlessly communicate with the remote cluster. Docker Engine installation
    is simple so that alone is not enough. On the other hand, `docker-machine env`
    command should not be used in a production environment. Both benefits are too
    weak.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Docker Machine 在与 DigitalOcean 上的 Swarm 集群配合使用时提供的唯一真正的好处，是能够在远程节点上安装 Docker
    引擎，并使用 `docker-machine env` 命令，使我们的本地 Docker 客户端能够与远程集群无缝通信。Docker 引擎的安装很简单，因此仅此一点并不足够。另一方面，`docker-machine
    env` 命令不应在生产环境中使用。这两个好处都太薄弱了。
- en: Many of the current problems with Docker Machine can be fixed with some extra
    arguments (For example, `--digitalocean-image`) and in combination with other
    tools. However, that only diminishes the primary benefit behind Docker Machine.
    It was supposed to be simple and work out of the box. That was partly true before
    Docker 1.12\. Now, at least in DigitalOcean, it is lagging behind.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Machine 当前的许多问题可以通过一些额外的参数（例如，`--digitalocean-image`）和与其他工具结合使用来修复。然而，这只是削弱了
    Docker Machine 的主要优势。它本应该是简单且开箱即用的。这个说法在 Docker 1.12 之前部分成立。现在，至少在 DigitalOcean
    上，它已经落后了。
- en: Does that mean we should discard Docker Machine when working with DigitalOcean?
    Not always. It is still useful when we want to create an ad-hoc cluster for demo
    purposes or maybe experiment with some new features. Also, if you don't want to
    spend time learning other tools and just want something you're familiar with,
    Docker Machine might be the right choice. I doubt that's your case. The fact that
    you reached this far in this book tells me that you do want to explore better
    ways of managing a cluster.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着在使用DigitalOcean时我们应该放弃Docker Machine？并非总是如此。当我们想要创建一个临时集群用于演示或可能实验一些新功能时，Docker
    Machine仍然有用。而且，如果你不想花时间学习其他工具，只是想使用自己熟悉的工具，Docker Machine可能是合适的选择。我怀疑这就是你的情况。你能读到这本书的这一部分，说明你确实想要探索更好的集群管理方式。
- en: The final recommendation is to keep Docker Machine as the tool of choice when
    you want to simulate a Swarm cluster locally as we did in the previous chapters.
    There are better choices for DigitalOcean.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的建议是，当你想要在本地模拟Swarm集群时，继续使用Docker Machine，正如我们在前几章所做的那样。在DigitalOcean上有更好的选择。
- en: To Terraform or not to Terraform?
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要不要使用Terraform？
- en: Terraform, when combined with Packer, is an excellent choice. HashiCorp managed
    to make yet another tool that changes the way we configure and provision servers.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 当Terraform与Packer结合使用时，它是一个非常优秀的选择。HashiCorp成功地又开发了一款工具，改变了我们配置和预置服务器的方式。
- en: Configuration management tools have as their primary objective the task of making
    a server always be in the desired state. If a web server stops, it will be started
    again. If a configuration file is changed, it will be restored. No matter what
    happens to a server, its desired state will be restored. Except, when there is
    no fix to the issue. If a hard disk fails, there's nothing configuration management
    can do.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 配置管理工具的主要目标是确保服务器始终处于期望的状态。如果一个Web服务器停止运行，它会被重新启动。如果一个配置文件被更改，它会被恢复。无论服务器发生什么，它都会恢复到期望的状态。除非问题没有修复的方法。如果硬盘出现故障，配置管理就无能为力了。
- en: The problem with configuration management tools is that they were designed to
    work with physical, not virtual servers. Why would we fix a faulty virtual server
    when we can create a new one in a matter of seconds? Terraform understands how
    cloud computing works better than anyone and embraces the idea that our servers
    are not pets anymore. They are cattle. It'll make sure that all your resources
    are available.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 配置管理工具的问题在于，它们是为物理服务器设计的，而不是虚拟服务器。为什么要修复一个有故障的虚拟服务器，而我们可以在几秒钟内创建一个新的？Terraform比任何人都更了解云计算的工作原理，并接受我们的服务器不再是“宠物”这一观点。它们是“牲畜”。它会确保你的所有资源都可用。
- en: When something is wrong on a server, it will not try to fix it. Instead, it
    will destroy it and create a new one based on the image we choose.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器出现问题时，它不会尝试修复它。相反，它会销毁该服务器，并根据我们选择的镜像创建一个新的服务器。
- en: Does that mean that there is no place for Puppet, Chef, Ansible, and other similar
    tools? Are they obsolete when operating in the cloud? Some are more outdated than
    others. Puppet and Chef are designed to run an agent on each server, continuously
    monitoring its state and modifying it if things go astray. There is no place for
    such tools when we start treating our servers as cattle. Ansible is in a bit better
    position since it is more useful than others, as a tool designed to configure
    a server instead of monitor it. As such, it could be very helpful when creating
    images.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着Puppet、Chef、Ansible以及其他类似的工具没有用武之地？它们在云计算环境中是否已经过时？其中一些工具比其他的更为过时。Puppet和Chef设计的目的是在每个服务器上运行一个代理，持续监控其状态，并在出现问题时进行修改。当我们开始把服务器当作“牲畜”来管理时，这类工具就失去了作用。而Ansible的处境稍微好一点，因为它更有用，它是一个用于配置服务器的工具，而不是监控服务器。因此，当我们创建镜像时，它会非常有帮助。
- en: We can combine Ansible with Packer. Packer would create a new VM, Ansible would
    provision that VM with everything we need, and leave it to Packer to create an
    image out of it. If the server setup is complicated, that makes a lot of sense.
    We don't create a lot of system users since we do not log into a machine to deploy
    software. Swarm does that for us. We do not install web servers and runtime dependencies
    anymore. They are inside containers. Is there a true benefit from using configuration
    management tools to install a few things into VMs that will be converted into
    images? More often than not, the answer is no. The few things we need can be just
    as easily installed and configured with a few Shell commands. Configuration management
    of our cattle can, and often should, be done with bash.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Ansible与Packer结合使用。Packer创建一个新的虚拟机，Ansible为这个虚拟机配置所需的一切，然后交给Packer将其转换为镜像。如果服务器配置比较复杂，这样做非常有意义。我们不会创建很多系统用户，因为我们不需要登录到机器上部署软件。Swarm会为我们做这件事。我们也不再安装Web服务器和运行时依赖项，它们都在容器内。使用配置管理工具为即将转为镜像的虚拟机安装一些东西真的有意义吗？大多数情况下，答案是否定的。我们需要的那些东西可以通过几条Shell命令轻松安装和配置。对我们的“牛群”进行配置管理，通常应该用bash来做。
- en: I might have been too harsh. Ansible is still a great tool if you know when
    to use it and for what purpose. If you prefer it over bash to install and configure
    a server before it becomes an image, go for it. If you try to use it to control
    your nodes and create DigitalOcean resources, you're on a wrong path. Terraform
    does that much better. If you think that it is better to provision a running node
    instead of instantiating images that already have everything inside, you must
    have much more patience than I do.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能说得有些严厉。Ansible依然是一个很棒的工具，前提是你知道何时使用它以及用来做什么。如果你更倾向于用它而非bash来安装和配置服务器，直到它变成镜像，那就去使用吧。如果你尝试用它来控制你的节点并创建DigitalOcean资源，那你就走错了路。Terraform做得更好。如果你认为最好是为一个运行中的节点配置，而不是实例化已经包含一切内容的镜像，那你可能需要比我更多的耐心。
- en: The final recommendation is to use *Terraform* with *Packer* if you want to
    have control of all the pieces that constitute your cluster, or if you already
    have a set of rules that need to be followed. Be ready to spend some time tuning
    the configs until you reach the optimum setup. Unlike AWS that, for good or bad,
    forces us to deal with many types of resources, DigitalOcean is simple. You create
    droplets, add a few floating IPs, and that's it. You might want to install a firewall
    on the machines. If you do, the best way would be to do that during the creation
    of a snapshot with Packer. It is questionable whether a firewall is needed when
    using Swarm networking, but that's a discussion for some other time.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 最终推荐是，如果你想要控制构成你集群的所有部分，或者你已经有一套需要遵循的规则，建议使用*Terraform*和*Packer*。准备花些时间调优配置，直到达到最佳设置。与AWS不同，无论好坏，它迫使我们处理许多类型的资源，DigitalOcean则简单许多。你只需创建Droplets，添加几个浮动IP，就完成了。你可能想在机器上安装防火墙。如果确实要安装，最佳方式是在用Packer创建快照时一起完成。至于是否在使用Swarm网络时需要防火墙，这个问题值得另行讨论。
- en: Since there is no such thing as *Docker for DigitalOcean*, Terraform is a clear
    winner. DigitalOcean is simple, and that simplicity is reflected through Terraform
    configuration.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有类似于*Docker for DigitalOcean*的东西，Terraform显然是赢家。DigitalOcean简单，这种简单性在Terraform配置中得到了体现。
- en: The final verdict
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终结论
- en: Terraform wins over Docker Machine in a landsilde. If there is such a thing
    as *Docker for DigitalOcean*, this discussion would be longer. As it is now, the
    choice is easy. If you choose DigitalOcean, manage your cluster with Packer and
    Terraform.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform在与Docker Machine的对比中取得了压倒性的胜利。如果有类似于*Docker for DigitalOcean*的东西，这个讨论可能会更长。但目前来说，选择是非常明确的。如果你选择了DigitalOcean，就用Packer和Terraform来管理你的集群。
- en: To DigitalOcean or not to DigitalOcean
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是否使用DigitalOcean
- en: Generally speaking, I like products and services that are focused on very few
    things and do them well. DigitalOcean is one of those. It is an **Infrastructure
    as a Service**(**IaaS**) provider and nothing else. The services it provides are
    few in number (For example, floating IP) and limited to those that are a real
    necessity. If you're looking for a provider that will offer you everything you
    can imagine, choose **Amazon Web Services**(**AWS**), Azure, GCE, or any other
    cloud computing provider that aims at delivering not only hosting but also numerous
    services on top. The fact that you reached this far in this book tells me that
    there are strong chances that you are interested in setting up infrastructure
    services yourself. If that's the case, DigitalOcean is worth a try. Doing everything
    often means not doing anything really well. DigitalOcean does a few things, and
    it does them well. What is does, it does better than most.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我喜欢那些专注于少数几件事并且做得很好的产品和服务。DigitalOcean就是其中之一。它是一个**基础设施即服务**（**IaaS**）提供商，除此之外没有其他业务。它提供的服务数量很少（例如，浮动IP），并且仅限于那些真正必要的服务。如果你正在寻找一个可以提供你所能想象的所有服务的提供商，选择**亚马逊网络服务**（**AWS**）、Azure、GCE，或者任何其他不仅提供托管服务，还提供大量附加服务的云计算提供商。从你已经读到这本书的这一点来看，意味着你很可能有兴趣自己搭建基础设施服务。如果是这样，DigitalOcean值得一试。做得太多往往意味着什么都做不好。DigitalOcean做了几件事，而且做得很好。它做的事情，比大多数其他云服务做得都要好。
- en: The real question is whether you need only an **Infrastructure as a Service **(**IaaS**)
    provider or you need **Platform as a Service **(**PaaS**) as well. In my opinion,
    containers make PaaS obsolete. It will be gradually replaced with containers managed
    by schedulers (For example, *Docker Swarm*) or **Containers as a Service **(**CaaS**).
    You might not agree with me. If you do, a huge part of AWS becomes obsolete leaving
    it with EC2, storage, VPCs, and only a handful of other services. In such a case,
    DigitalOcean is a mighty competitor and an excellent choice. The few things it
    does, it does better than AWS at a lower price. The performance is impressive.
    It's enough to measure the time it requires to create a droplet and compare it
    with the time AWS requires to create and initiate an EC2 instance. The difference
    is huge. The first time I created a droplet, I thought that there's something
    wrong. My brain could not comprehend that it could be done in less than a minute.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的问题是你是否只需要**基础设施即服务**（**IaaS**）提供商，还是还需要**平台即服务**（**PaaS**）。在我看来，容器使PaaS变得过时。它将逐渐被由调度程序管理的容器（例如，*Docker
    Swarm*）或**容器即服务**（**CaaS**）所取代。你可能不同意我的看法。如果你同意，那么AWS的大部分服务将变得过时，剩下的只有EC2、存储、VPC和少数其他服务。在这种情况下，DigitalOcean是一个强大的竞争者，也是一个非常好的选择。它做的几件事，比AWS做得更好，而且价格更低。它的性能令人印象深刻。只需测量创建一个droplet所需的时间，并将其与AWS创建和启动EC2实例所需的时间进行比较，你会发现差异巨大。我第一次创建droplet时，甚至觉得出了什么问题。我的大脑无法理解为什么这件事可以在不到一分钟的时间内完成。
- en: Did I mention simplicity? DigitalOcean is simple, and I love simplicity. Therefore,
    the logical conclusion is that I love DigitalOcean. The real mastery is to make
    complex things easy to use. That's where both Docker and DigitalOcean shine.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到过简洁吗？DigitalOcean很简洁，而我喜欢简洁。因此，合乎逻辑的结论是，我喜欢DigitalOcean。真正的高手是能让复杂的事情变得易于使用。这正是Docker和DigitalOcean的闪光之处。
