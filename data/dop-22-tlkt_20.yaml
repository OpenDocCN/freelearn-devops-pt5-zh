- en: Blueprint Of A Self-Sufficient System
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自给自足系统的蓝图
- en: We came a long way, and now we are at the end of the first stage of the journey.
    What will happen next is up to you. You’ll have to expand on the knowledge I tried
    to transmit and improve the system we built. It is a base that needs to be extended
    to suit your needs. Each system is different, and no blueprint can be followed
    blindly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们走了很长一段路，现在我们已经处于旅程的第一阶段的终点。接下来会发生什么，取决于你。你需要扩展我试图传递的知识，并改进我们构建的系统。它是一个需要根据你的需求扩展的基础。每个系统都是不同的，没有任何蓝图可以盲目跟随。
- en: Every good story needs an ending, and this one should not be an exception. I’ll
    try to summarize the knowledge passed through the previous chapters. Still, I
    feel I should be brief. If you need a long summary of everything we explored so
    far, it would mean that I did not do my job well. I did not explain things well
    enough, or it was so dull that you skipped some parts hoping that they will be
    summarized at the end. Please let me know if I failed and I’ll do my best to improve.
    For now, I’ll assume that you got the gist behind the topics we discussed and
    dedicate this chapter to a concise summary of everything.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 每个好的故事都需要有一个结局，而这个故事也不例外。我会尽量总结前面章节中传递的知识，尽管我觉得我应该简明扼要。如果你需要一个我们所探讨内容的长篇总结，那就意味着我没有做好我的工作。我没有讲解得够清晰，或者内容太枯燥，你跳过了某些部分，期待它们在最后被总结。请告诉我如果我做得不够好，我会尽力改进。现在，我假设你已经理解了我们讨论的主题的要点，并将本章作为对所有内容的简洁总结。
- en: We split the tasks that a self-sufficient system should perform into those related
    to services and those oriented towards infrastructure. Even though some of the
    tools are used in both groups, the division between the two allowed us to keep
    a clean separation between infrastructure and services running on top of it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将自给自足系统应执行的任务分为与服务相关的任务和面向基础设施的任务。尽管一些工具在这两组中都有使用，但这两者之间的划分使我们能够保持基础设施与运行其上的服务之间的清晰分离。
- en: Service Tasks
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务任务
- en: Service tasks are related to flows that are in charge of making sure that services
    are running, that correct versions are deployed, that information is propagated
    to all dependencies, that they are reachable, that they behave as expected, and
    so on. In other words, everything related to services is under this umbrella.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 服务任务与负责确保服务正常运行的流程有关，包括正确版本的部署、信息传播到所有依赖项、服务可访问性、按预期行为执行等。换句话说，所有与服务相关的任务都在这个范围之内。
- en: We’ll group service related tasks into self-healing, deployment, reconfiguration,
    request, and self-adaptation flows.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将与服务相关的任务分为自我修复、部署、重配置、请求和自我适应流程。
- en: Self-Healing Flow
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自我修复流程
- en: Docker Swarm (or any other scheduler) is taking care of self-healing. As long
    as there’s enough hardware capacity, it will make sure that the desired number
    of replicas of each service is (almost) always up-and-running. If a replica goes
    down, it’ll be rescheduled. If a whole node is destroyed or loses connection to
    other managers, all replicas that were running on it will be rescheduled. Self-healing
    comes out of the box. Still, there are quite a few other tasks we should define
    if we’d want our solution to be self-sufficient and (almost) fully autonomous.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm（或任何其他调度器）负责自我修复。只要硬件资源足够，它会确保每个服务的期望副本数量几乎总是处于运行状态。如果某个副本宕机，它将被重新调度。如果整个节点被销毁或失去与其他管理节点的连接，它上面运行的所有副本将被重新调度。自我修复是开箱即用的。然而，如果我们希望我们的解决方案能够自给自足并且（几乎）完全自主，仍然有许多其他任务需要定义。
- en: Deployment Flow
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署流程
- en: A commit to a repository is the last human action we hope to have. That might
    not always be the case. No matter how smart and autonomous our system is, there
    will always be a problem that cannot be solved automatically by the system. Still,
    we should aim for an entirely non-human system. Even though we won’t manage to
    get there, it is a worthy goal that will keep us focused and prevent us from taking
    shortcuts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提交到代码库是我们希望的最后一个人工操作。虽然这并不总是如此。无论我们的系统多么智能和自主，总会有一些问题是系统无法自动解决的。然而，我们仍应以完全无人工干预的系统为目标。即使我们无法完全实现这一点，它依然是一个值得追求的目标，能够让我们保持专注，避免走捷径。
- en: What happens when we commit code? A code repository (e.g., GitHub) executes
    a webhook that sends a request to our continuous deployment tool of choice. We
    used Jenkins throughout the book but, just as any other tool we used, it can be
    replaced with a different solution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提交代码时会发生什么？代码仓库（例如，GitHub）执行一个 webhook，向我们选择的持续部署工具发送请求。全书中我们使用的是 Jenkins，但就像我们使用的其他工具一样，它也可以被替换成其他解决方案。
- en: The webhook trigger initiates a new Jenkins job that runs our CD pipeline. It
    runs unit tests, builds a new image, runs functional tests, publishes the image
    to Docker Hub (or any other registry), and so on and so forth. At the end of the
    process, the Jenkins pipeline instructs Swarm to update the service associated
    with the commit. The update should, as a minimum, change the image associated
    with the service to the one we just built.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: webhook 触发器启动一个新的 Jenkins 任务，该任务运行我们的持续部署（CD）管道。它运行单元测试，构建新镜像，执行功能测试，将镜像发布到
    Docker Hub（或任何其他注册表）等。流程结束时，Jenkins 管道指示 Swarm 更新与提交相关的服务。至少，更新应将与服务相关联的镜像更改为我们刚刚构建的镜像。
- en: Once Docker Swarm receives the instruction to update the service, it executes
    rolling updates process that will replace one replica at the time (unless specified
    otherwise). With a process like that, and assuming that our services are designed
    to be cloud-friendly, new releases do not produce any downtime, and we can run
    them as often as we want.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Docker Swarm 收到更新服务的指令，它会执行滚动更新过程，一次替换一个副本（除非另有指定）。通过这样的过程，并假设我们的服务是以云友好的方式设计的，新版本不会造成任何停机，我们可以根据需要随时运行它们。
- en: '![Figure 16-1: Continuous deployment process](img/00093.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 16-1：持续部署过程](img/00093.jpeg)'
- en: 'Figure 16-1: Continuous deployment process'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-1：持续部署过程
- en: Reconfiguration Flow
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重新配置流程
- en: Deploying a new release is only part of the process. In most cases, other services
    need to be reconfigured to include the information about the deployed service.
    Monitoring (e.g., [Prometheus](https://prometheus.io/)) and proxy (e.g., [HAProxy](http://www.haproxy.org/)
    or [nginx](https://www.nginx.com)) are only two out of many examples of services
    that need to know about other services in the cluster. We’ll call them infrastructure
    services since, from the functional point of view, their scope is not business
    related. They are usually in charge of making the cluster operational or, at least,
    visible.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 部署新版本只是过程的一部分。在大多数情况下，其他服务需要重新配置以包含有关已部署服务的信息。监控（例如，[Prometheus](https://prometheus.io/)）和代理（例如，[HAProxy](http://www.haproxy.org/)
    或 [nginx](https://www.nginx.com)）只是需要了解集群中其他服务的服务中的两个例子。我们将它们称为基础设施服务，因为从功能角度来看，它们的范围与业务无关。它们通常负责使集群能够正常运行，或至少能被识别。
- en: If we’re running a highly dynamic cluster, infrastructure services need to be
    dynamic as well. High level of dynamism cannot be accomplished by manually modifying
    configurations whenever a business service is deployed. We must have a process
    that monitors changes to services running inside a cluster and updates all those
    that require info about deployed or updated services.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行的是一个高度动态的集群，基础设施服务也需要是动态的。高度的动态性无法通过每次部署业务服务时手动修改配置来实现。我们必须有一个过程来监控集群内服务的变化，并更新所有需要关于已部署或已更新服务信息的服务。
- en: There are quite a few ways to solve the problem of automatic updating of infrastructure
    services. Throughout this book, we used one of many possible processes. We assumed
    that info about a service would be stored as labels. That allowed us to focus
    on a service at hand and let the rest of the system discover that information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解决自动更新基础设施服务问题的方式有很多种。本书中，我们使用了多种可能过程中的一种。我们假设服务的信息会存储为标签，这使我们可以专注于当前服务，让系统的其余部分发现这些信息。
- en: We used [Docker Flow Swarm Listener (DFSL)](http://swarmlistener.dockerflow.com/)
    to detect changes in services (new deployments, updates, and removals). Whenever
    a change is detected, relevant information is sent to specified addresses. In
    our case, those addresses are pointing to the proxy ([Docker Flow Monitor](http://monitor.dockerflow.com/))
    and Prometheus ([Docker Flow Proxy](http://proxy.dockerflow.com/)). Once those
    services receive a request with information about a new (or updated, or removed)
    service, they change their configurations and reload the main process. With this
    flow of events, we can guarantee that all infrastructure services are always up-to-date
    without us having to worry about their configuration. Otherwise, we’d need to
    create a much more complex pipeline that would not only deploy a new release but
    also make sure that all other services are up-to-date.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了[Docker Flow Swarm Listener (DFSL)](http://swarmlistener.dockerflow.com/)来检测服务的变化（如新的部署、更新和移除）。每当检测到变化时，相关信息会被发送到指定的地址。在我们的案例中，这些地址指向代理服务器（[Docker
    Flow Monitor](http://monitor.dockerflow.com/)）和 Prometheus（[Docker Flow Proxy](http://proxy.dockerflow.com/)）。一旦这些服务收到关于新（或更新，或移除）服务的请求，它们会改变配置并重新加载主进程。通过这种事件流，我们可以确保所有基础设施服务始终保持最新，而无需担心它们的配置。否则，我们将需要创建一个更复杂的管道，不仅部署新版本，还要确保所有其他服务保持最新。
- en: '![Figure 16-2: Reconfiguration flow](img/00094.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 16-2: 重新配置流程](img/00094.jpeg)'
- en: 'Figure 16-2: Reconfiguration flow'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16-2: 重新配置流程'
- en: Request Flow
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 请求流
- en: When a user (or an external client) sends a request to one of our services,
    that request is first captured by the Ingress network. Every port published by
    a service results in that port being open in Ingress. Since the network’s scope
    is global, a request can be sent to any of the nodes. When captured, Ingress will
    evaluate the request and forward it to one of the replicas of a service that published
    the same port. While doing so, Ingress network performs round-robin load balancing
    thus guaranteeing that all replicas receive (more or less) the same number of
    requests.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户（或外部客户端）向我们的服务发送请求时，该请求首先会被 Ingress 网络捕获。每个服务发布的端口都会使该端口在 Ingress 中打开。由于网络的范围是全球性的，请求可以发送到任何节点。当请求被捕获时，Ingress
    会评估该请求并将其转发到发布了相同端口的服务副本之一。在此过程中，Ingress 网络执行轮询负载均衡，从而确保所有副本接收到（或多或少）相同数量的请求。
- en: Overlay network (Ingress being a flavor of it), is not only in charge of forwarding
    requests to a service that publishes the same port as the request, but is also
    making sure that only healthy replicas are included in round-robin load balancing.
    `HEALTHCHECK` defined in Docker images is essential in guaranteeing zero-downtime
    deployments. When a new replica is deployed, it will not be included in load balancing
    algorithm until it reports that it is healthy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Overlay 网络（Ingress 是其中的一种类型）不仅负责将请求转发到发布了相同端口的服务，还确保只有健康的副本被纳入轮询负载均衡中。Docker
    镜像中定义的`HEALTHCHECK`对于确保零停机时间部署至关重要。当一个新的副本被部署时，它在报告健康之前不会被纳入负载均衡算法。
- en: Throughout the book, [Docker Flow Proxy (DFP)](http://proxy.dockerflow.com/)
    was the only service that published any port. That allowed us to channel all traffic
    through ports `80` and `443`. Since it is dynamic and works well with DFSL, we
    did not need to worry about HAProxy configuration beneath it. That means that
    all requests to our cluster are picked by Ingress network and forwarded to DFP
    which would evaluate request paths, domains, and other info coming from headers,
    and decide which service should receive a request. Once that decision is made,
    it would forward requests further. Assuming that both the proxy and the destination
    service are attached to the same network, those forwarded requests would be picked,
    one more time, by the Overlay network which would perform round-robin load balancing
    and forward requests to their final destination.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，[Docker Flow Proxy (DFP)](http://proxy.dockerflow.com/)是唯一发布任何端口的服务。这使我们能够将所有流量通过端口`80`和`443`进行通道化。由于其动态特性并且与
    DFSL 配合良好，我们无需担心其下方的 HAProxy 配置。这意味着，所有到达我们集群的请求都由 Ingress 网络捕获，并转发到 DFP，后者会评估请求路径、域名及其他来自请求头的信息，然后决定哪个服务应接收请求。一旦做出决策，DFP
    会将请求进一步转发。假设代理和目标服务都连接到相同的网络，那么这些转发的请求将再次被 Overlay 网络捕获，进行轮询负载均衡并将请求转发到最终目的地。
- en: Even though the flow of a request might seem complex, it is very straight-forward
    from a perspective of a service owner. All that he (or she) needs to do is define
    a few service labels that would tell the proxy the desired path or a domain that
    distinguishes that service from others. User’s, on the other hand, never experience
    downtime no matter how often we deploy new releases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管请求的流程看起来可能很复杂，但从服务所有者的角度来看，它非常直观。他（或她）所需要做的只是定义几个服务标签，告诉代理所需的路径或区分该服务与其他服务的域名。另一方面，用户无论我们多频繁部署新版本，都不会经历停机。
- en: '![Figure 16-3: Request service flow](img/00095.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图16-3：请求服务流程](img/00095.jpeg)'
- en: 'Figure 16-3: Request service flow'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-3：请求服务流程
- en: Self-Adaptation Flow
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应流程
- en: Once we manage to create flows that allow us to deploy new releases without
    downtime and, at the same time, reconfigure all dependent services, we can move
    forward and solve the problem of self-adaptation applied to services. The goal
    is to create a system that would scale (and de-scale) services depending on metrics.
    That way, our services can operate efficiently no matter the changes imposed from
    outside. For example, we could increase the number of replicas if response times
    of a predefined percentile are too high.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们成功创建了能够在没有停机的情况下部署新版本，同时重新配置所有依赖服务的流程，我们就可以向前推进，解决应用于服务的自适应问题。目标是创建一个系统，根据指标来扩展（或缩减）服务。这样，我们的服务就能高效地运行，无论外部变化如何。例如，如果预定义百分位的响应时间过长，我们可以增加副本的数量。
- en: '[Prometheus](https://prometheus.io/) periodically scrapes metrics both from
    generic exporters as well as from our services. We accomplished the latter by
    instrumenting them. Exporters are useful for global metrics like those generated
    by containers (e.g., [cAdvisor](https://github.com/google/cadvisor)) or nodes
    (e.g., [Node exporter](https://github.com/prometheus/node_exporter)). Instrumentation,
    on the other hand, is useful when we want more detailed metrics specific to our
    service (e.g., the response time of a specific function).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prometheus](https://prometheus.io/)定期从通用导出器以及我们的服务中抓取指标。我们通过对其进行仪表化实现了后者。导出器适用于诸如容器（例如，[cAdvisor](https://github.com/google/cadvisor)）或节点（例如，[Node
    exporter](https://github.com/prometheus/node_exporter)）等生成的全局指标。仪表化则适用于我们希望获取更详细的、特定于服务的指标（例如，特定功能的响应时间）。'
- en: We configured Prometheus (through [Docker Flow Monitor (DFM)](http://monitor.dockerflow.com/))
    not only to scrape metrics from exporters and instrumented services but also to
    evaluate alerts that are fired to [Alertmanager](https://github.com/prometheus/alertmanager).
    It, in turn, filters fired alerts and sends notifications to other parts of the
    system (internal or external).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过[Docker Flow Monitor (DFM)](http://monitor.dockerflow.com/)配置了Prometheus，不仅能够从导出器和仪表化服务中抓取指标，还能评估触发的警报，并将其发送到[Alertmanager](https://github.com/prometheus/alertmanager)。Alertmanager会过滤触发的警报，并将通知发送到系统的其他部分（内部或外部）。
- en: When possible, alert notifications should be sent to one or more services that
    will “correct” the state of the cluster automatically. For example, alert notification
    that was fired because response times of a service are too long should result
    in scaling of that service. Such an action is relatively easy to script. It is
    a repeatable operation that can be easily executed by a machine and, therefore,
    is a waste of human time. We used Jenkins as a tool that allows us to perform
    tasks like scaling (up or down).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，警报通知应发送到一个或多个服务，这些服务将自动“修正”集群的状态。例如，因服务响应时间过长而触发的警报通知应导致该服务进行扩容。这样的操作相对容易编写脚本。它是一个可重复的操作，机器可以轻松执行，因此不应浪费人力时间。我们使用Jenkins作为工具，允许我们执行诸如扩容（增容或缩容）之类的任务。
- en: Alert notifications should be sent to humans only if they are a result of an
    unpredictable situation. Alerts based on conditions that never happened before
    are a good candidate for human intervention. We’re good at solving unexpected
    issues; machines are good at repeatable tasks. Still, even in those never-seen-before
    cases, we (humans) should not only solve the problem, but also create a script
    that will repeat the same steps the next time the same issue occurs. The first
    time an alert resulted in a notification to a human, it should be converted into
    a notification to a machine that will employ the same steps we did previously.
    In other words, solve the problem yourself when it happens the first time, and
    let the machines repeat the solution if it happens again. Throughout the book,
    we used Slack as a notification engine to humans, and Jenkins as a machine receptor
    of those notifications.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在警报是由于不可预测的情况发生时，才应将通知发送给人类。基于从未发生过的条件的警报是需要人工干预的好候选。我们擅长解决突发问题；机器擅长执行可重复的任务。尽管如此，即使在这些前所未见的情况下，我们（人类）也不仅要解决问题，还应该创建一个脚本，以便下次遇到相同问题时能够重复相同的步骤。第一次触发警报并通知人类时，应该将其转换为通知机器，机器将按照我们之前的步骤操作。换句话说，第一次发生时自己解决问题，若再次发生则让机器重复解决方案。在整本书中，我们使用了
    Slack 作为发送给人类的通知引擎，Jenkins 作为接收这些通知的机器端。
- en: '![Figure 16-4: Self-adaptation services flow](img/00096.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 16-4：自适应服务流程](img/00096.jpeg)'
- en: 'Figure 16-4: Self-adaptation services flow'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-4：自适应服务流程
- en: Infrastructure Tasks
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施任务
- en: Infrastructure tasks are related to flows that are in charge of making sure
    that hardware is operational and that nodes are forming the cluster. Just as service
    replicas, those nodes are dynamic. Their numbers are fluctuating as a result of
    ever_changing needs behind our services. Everything related to hardware or, more
    often, VMs and their ability to be members of a cluster is under this umbrella.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施任务与确保硬件正常工作和节点组成集群的流程相关。就像服务副本一样，这些节点是动态的。由于服务背后的需求不断变化，它们的数量在波动。一切与硬件相关的内容，或者更常见的，与虚拟机及其作为集群成员的能力相关，都属于这一范畴。
- en: We’ll group infrastructure related tasks into self-healing, request, and self-adaptation
    flows.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把与基础设施相关的任务分为自愈、请求和自适应流程。
- en: Self-Healing Flow
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自愈流程
- en: A system that automatically manages infrastructure is not much different from
    the system we built around services. Just as Docker Swarm (or any other scheduler)
    is in charge of making sure that services are (almost) always up-and-running and
    in the desired capacity, auto-scaling groups in AWS are making sure that desired
    number of nodes is (almost) always available. Most other hosting vendors and on-premise
    solutions have a similar feature under a different name.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自动管理基础设施的系统与我们围绕服务构建的系统没有太大区别。就像 Docker Swarm（或任何其他调度器）负责确保服务（几乎）始终运行并具备所需的容量一样，AWS
    中的自动扩展组确保所需数量的节点（几乎）始终可用。大多数其他托管供应商和本地解决方案都有类似的功能，只是名称不同。
- en: Auto-scaling groups are only part of the self-healing solution applied to infrastructure.
    Recreating a failed node is not enough by itself. We need to have a process that
    will join that node to the existing cluster. Throughout the book, we used [Docker
    For AWS](https://docs.docker.com/docker-for-aws/) that already has a solution
    to that problem. Each node runs a few system containers. One of them is periodically
    checking whether the node it is running on is the lead manager. If it is, information
    like join tokens and its IP are stored in a central location (at the time of this
    writing in DynamoDB). When a new node is created, one of the system containers
    retrieves that data and uses it to join the cluster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展组只是应用于基础设施的自愈解决方案的一部分。仅仅重建一个失败的节点是不够的。我们需要一个过程，将该节点加入现有的集群。在整本书中，我们使用了 [Docker
    For AWS](https://docs.docker.com/docker-for-aws/)，它已经为这个问题提供了解决方案。每个节点都运行一些系统容器，其中一个容器定期检查其运行的节点是否是主节点。如果是，它会将加入令牌和节点的
    IP 等信息存储在一个中央位置（截至写作时存储在 DynamoDB 中）。当一个新节点被创建时，系统容器会检索这些数据并使用它来加入集群。
- en: If you are not using Docker For AWS or Azure, you might need to roll up your
    sleeves and write your own solution or, if you’re lazy, search for it. There are
    plenty of open source snippets that can help you out.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用 Docker For AWS 或 Azure，你可能需要自己动手编写解决方案，或者，如果你懒的话，去找现成的解决方案。有很多开源代码片段可以帮助你。
- en: No matter the solution you choose (or build yourself), the steps are almost
    always the same. Create auto-scaling groups (or whatever is available with your
    hosting provider) that will maintain the desired number of nodes. Store join tokens
    and IP of the lead manager in a fault tolerant location (an external database,
    service registry, network drive, and so on) and use it to join new nodes to the
    cluster.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择（或自己构建）什么解决方案，步骤几乎总是相同的。创建自动扩展组（或你托管服务商提供的任何功能），以维持所需的节点数量。将加入令牌和主管理节点的IP地址存储在容错位置（外部数据库、服务注册表、网络驱动器等），并利用它将新节点加入到集群中。
- en: Finally, stateful services are unavoidable. Even if all the services you developed
    are stateless, the state has to be stored somewhere. For some of the cases, we
    need to store the state on disk. Using local storage is not an option. Sooner
    or later a replica will be rescheduled and might end up on a different node. That
    can be due to a process failure, upgrade, or because a node is not operational
    anymore. No matter the cause behind rescheduling, the fact is that we must assume
    that it will not run on the same node forever. The only reasonable way to prevent
    data loss when the state is stored on disk is to use a network drive or distributed
    file system. Throughout the book, we used AWS Elastic File System (EFS) since
    it works in multiple availability zones. In some other cases, you might opt for
    EBS if IO speed is of the essence. If you choose some other vendor, the solution
    will be different, but the logic will be the same. Create a network drive and
    attach it to a service as volume. Docker For AWS and Azure comes with CloudStor
    volume driver. If you choose a different solution for creating a cluster, you
    might have to look for a different driver. [REXRay](http://rexray.readthedocs.io/)
    is one of the solutions since it supports most of the commonly used hosting vendors
    and operating systems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有状态服务是不可避免的。即使你开发的所有服务都是无状态的，状态也必须存储在某个地方。对于一些情况，我们需要将状态存储到磁盘上。使用本地存储不是一个选项。迟早，一个副本将被重新调度，并且可能会被调度到一个不同的节点。这可能是由于进程故障、升级，或因为某个节点不再可操作。无论重新调度的原因是什么，事实是我们必须假设它不会永远在同一个节点上运行。唯一合理的防止数据丢失的方法是在磁盘上存储状态时使用网络驱动器或分布式文件系统。在本书中，我们使用了AWS弹性文件系统（EFS），因为它可以在多个可用区中工作。在其他一些情况下，如果IO速度至关重要，你可能会选择EBS。如果你选择其他供应商，解决方案会有所不同，但逻辑是相同的。创建一个网络驱动器并将其作为卷附加到服务上。AWS和Azure的Docker都附带了CloudStor卷驱动程序。如果你选择了其他创建集群的解决方案，可能需要寻找其他驱动程序。[REXRay](http://rexray.readthedocs.io/)是其中一个解决方案，因为它支持大多数常用的托管供应商和操作系统。
- en: Before you jump into volumes attached to network drives, make sure you really
    need them. A common mistake is to assume that state generated by a database needs
    to be persisted. While in some cases that is true, in many others it is not. Modern
    databases can replicate data between different instances. In such cases, the persistence
    of that data might not be required (or even welcome). If multiple instances have
    the same data, failure of one of them does not mean that data is lost. That instance
    will be rescheduled and, when appropriately configured, it will retrieve data
    from one of the replicas that did not fail.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用附加到网络驱动器的卷之前，请确保你真的需要它们。一个常见的错误是认为数据库生成的状态需要持久化。虽然在某些情况下这是对的，但在许多其他情况下并非如此。现代数据库可以在不同实例之间复制数据。在这种情况下，持久化数据可能不需要（甚至可能不欢迎）。如果多个实例拥有相同的数据，那么其中一个实例的失败并不意味着数据丢失。该实例将被重新调度，并且在适当配置的情况下，它将从没有失败的副本中检索数据。
- en: '![Figure 16-5: Self-healing infrastructure flow](img/00097.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图16-5：自愈基础设施流程](img/00097.jpeg)'
- en: 'Figure 16-5: Self-healing infrastructure flow'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-5：自愈基础设施流程
- en: Request Flow
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 请求流程
- en: We already explored how to make sure that a request initiated by a user or a
    client outside the cluster reaches the destination service. However, there was
    one piece of the puzzle missing. We guaranteed that a request would find its way
    once it enters the cluster but we failed to provide enough assurance that it will
    reach the cluster. We cannot configure DNS with IP of one of the nodes since that
    server might fail at any moment. We have to add something in between the DNS and
    the cluster. That something should have a single goal. It should make sure that
    a request reaches any of the healthy nodes. It does not matter which one since
    Ingress network will take over and initiate the request flow we discussed. That
    element in between can be an external load balancer, elastic IP, or any other
    solution. As long as it is fault-tolerant and is capable of performing health
    checks to determine which node is operational, any solution should do. The only
    challenge is to make sure that the list of the nodes is always up-to-date. That
    means that any new node added to the cluster should be added to that list. That
    might be overkill, and you might want to reduce the scope to, for example, all
    current and future manager nodes. Fortunately, Docker For AWS (or Azure) already
    has that feature baked into its template and system-level containers. Never the
    less, if you are using a different solution to create your cluster, it should
    be relatively easy to find a similar alternative or write your own solution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何确保用户或客户在集群外部发起的请求能够到达目标服务。然而，还有一个关键部分未解决。我们保证了一旦请求进入集群就能找到路径，但我们未能提供足够的保证它能够到达集群。我们不能配置
    DNS 为节点之一的 IP，因为该服务器随时可能会失败。我们必须在 DNS 和集群之间添加一些东西。那个东西应该有一个单一的目标。它应确保请求到达任何一个健康节点。无论是哪一个节点，因为
    Ingress 网络将接管并启动我们讨论过的请求流程。介于其中的元素可以是外部负载均衡器、弹性 IP 或其他任何解决方案。只要它是容错的并且能够执行健康检查以确定哪个节点是运行的，任何解决方案都可以做到。唯一的挑战是确保节点列表始终保持最新。这意味着集群添加新节点时应将其添加到该列表中。这可能有些多余，你可能希望将范围缩小到当前和未来的管理节点。幸运的是，Docker
    For AWS（或 Azure）已经将该功能集成到其模板和系统级容器中。尽管如此，如果你使用不同的解决方案来创建你的集群，找到类似的替代方案或编写你自己的解决方案应该相对容易。
- en: '![Figure 16-6: Request infra flow](img/00098.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 16-6: 请求基础设施流程](img/00098.jpeg)'
- en: 'Figure 16-6: Request infra flow'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16-6: 请求基础设施流程'
- en: Self-Adaptation Flow
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应流程
- en: Self-adaptation applied to infrastructure is conceptually the same as the one
    used for services. We need to collect metrics and store them somewhere (Prometheus)
    and we need to define alerts and have a system that evaluates them against metrics
    (Prometheus). When alerts reach a threshold and a specified time passed, they
    need to be filtered and, depending on the problem, transformed into notifications
    that will be sent to other services (Alertmanager). We used Jenkins as a receptor
    of those notifications. If the problem can be solved by the system, pre-defined
    actions would be executed. Since our examples use AWS, Jenkins would run tasks
    through AWS CLI. When, on the other hand, alerts result in a new problem that
    requires a creative solution, the final receptor of the notification is a human
    (in our case through Slack).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施的自适应概念上与服务的自适应相同。我们需要收集指标并将它们存储在某个地方（Prometheus），并定义警报并有一个系统根据指标对其进行评估（Prometheus）。当警报达到阈值并经过指定时间后，它们需要被过滤，并根据问题转换为通知发送到其他服务（Alertmanager）。我们使用
    Jenkins 作为这些通知的接收器。如果问题可以由系统解决，将执行预定义的操作。由于我们的示例使用 AWS，Jenkins 将通过 AWS CLI 运行任务。另一方面，如果警报导致需要创造性解决方案的新问题，通知的最终接收者是人类（在我们的情况下通过
    Slack）。
- en: '![Figure 16-7: Self-adaptation infrastructure flow](img/00099.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 16-7: 自适应基础设施流程](img/00099.jpeg)'
- en: 'Figure 16-7: Self-adaptation infrastructure flow'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16-7: 自适应基础设施流程'
- en: Logic Matters, Tools Might Vary
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑至关重要，工具可能会有所不同
- en: Do not take the tools we used thus far for granted. Technology changes way too
    often. By the time you read this, at least one of them will be obsolete. There
    might be better alternatives. Technology changes with such speed that it is impossible
    to follow even if we’d dedicate all our time only on evaluation of “new toys.”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不要认为我们迄今使用的工具是理所当然的。技术变化实在太快。当你读到这篇文章时，至少有一个已经过时了。可能存在更好的替代方案。技术变化速度如此之快，以至于即使我们全部时间都用于评估“新玩具”，也无法跟上。
- en: Processes and logic are also not static nor everlasting. They should not be
    taken for granted nor followed forever. There’s no such thing as best-practice-forever-and-ever.
    Still, logic changes must slower than tools. It has much higher importance since
    it lasts longer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 流程和逻辑也不是静止不变的，亦非永恒的。它们不应被视为理所当然，也不应永远被遵循。没有所谓的“永远最佳实践”。然而，逻辑的变化远远慢于工具。它更为重要，因为它能持续更长时间。
- en: I believe that the logic and processes described in this book will outlive the
    tools we used. Take that for what it’s worth. Explore other tools. Seek for those
    that better fit your goals. As for me, I haven’t even finished writing this book,
    and I can already see quite a few improvements over the tools we used. Some of
    them could be replaced with something better. Others might not have been the best
    choice from the start. But that does not matter as much as it might seem. Processes
    and logic are what truly matters, and I hope that those we explored will survive
    for a while longer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信本书中所描述的逻辑和流程将超越我们所使用的工具。对此，你可以自行评估其价值。探索其他工具，寻找那些更符合你目标的工具。至于我，我甚至还没写完这本书，但我已经看到了我们使用的工具可以改进的地方。它们中的一些可以被更好的工具替代，其他的可能一开始就不是最佳选择。但这并不像看起来那么重要。真正重要的是流程和逻辑，我希望我们探索过的那些能够再持续一段时间。
- en: Do not let this pessimistic attitude discourage you from implementing what you
    learned. Blame it on me and my never-ending quest for new and better ways to do
    something.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 不要让这种悲观的态度阻碍你去实现你所学到的东西。把责任归咎于我和我那永无止境的探索，去寻找更好、更有效的方式来做事。
- en: What Now?
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现在该怎么办？
- en: This is the end. Go and apply what you learned. Improve it. Contribute back
    to the community.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是结尾了。去应用你所学的，改进它，回馈社区。
- en: '**So long, and thanks for all the fish.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**再见了，感谢所有的鱼。**'
