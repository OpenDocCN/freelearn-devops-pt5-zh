- en: Creating and Managing Stateful Services in a Swarm Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Swarm集群中创建和管理有状态服务
- en: Any sufficiently advanced technology is indistinguishable from magic.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 任何足够先进的技术都与魔法 indistinguishable（无法区分）。
- en: '- Arthur C. Clarke'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 阿瑟·C·克拉克'
- en: If you're attending conferences, listening to podcasts, reading forums, or you
    are involved in any other form of a debate related to containers and cloud-native
    applications, you must have heard the mantra stateless services. It's almost like
    a cult. Only stateless services are worthy. Everything else is heresy. The solution
    to any problem is to remove the state. How do we scale this application? Make
    it stateless. How do we put this into a container? Make it stateless. How do we
    make it fault tolerant? Make it stateless. No matter the problem, the solution
    is to be stateless.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你参加过会议、听过播客、读过论坛，或者参与过任何与容器和云原生应用相关的辩论，你一定听过“无状态服务”这个口号。它几乎像是一种教义。只有无状态服务才是值得的，其他的都是异端。任何问题的解决方案都是移除状态。我们如何扩展这个应用程序？让它变得无状态。我们如何将其放入容器中？让它变得无状态。我们如何让它具有容错能力？让它变得无状态。无论是什么问题，解决方案都是无状态的。
- en: Are all the services we used until now stateless? They're not. Therefore, the
    logic dictates, we did not yet solve all the problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直到现在所使用的所有服务都是无状态的吗？并不是。所以，逻辑上来说，我们还没有解决所有问题。
- en: Before we start exploring stateless services, we should go back in time and
    discuss The twelve-factor app methodology.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索无状态服务之前，我们应该回顾一下并讨论一下十二因素应用程序方法论。
- en: Exploring the twelve-factor app methodology
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索十二因素应用程序方法论
- en: Assuming that my memory still serves me well, *Heroku* ([https://www.heroku.com/](https://www.heroku.com/))
    became popular somewhere around 2010\. It showed us how to leverage *Software-as-a-Service*
    principles. It freed developers from thinking too much about underlying infrastructure.
    It allowed them to concentrate on development and leave the rest to others. All
    we had to do is push our code to Heroku. It would detect the programming language
    we use, create a VM and install all the dependencies, build, launch, and so on.
    The result would be our application running on a server.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我的记忆没错，*Heroku*（[https://www.heroku.com/](https://www.heroku.com/)）大约在2010年左右开始流行。它向我们展示了如何利用*软件即服务（SaaS）*原则。它使开发者不再需要过多考虑底层基础设施，能够专注于开发，其他的交给别人来做。我们所需要做的只是将代码推送到Heroku，它会自动检测我们使用的编程语言，创建虚拟机，安装所有依赖，构建、启动，等等。结果就是我们的应用程序在服务器上运行。
- en: Sure, in some cases Heroku would not manage to figure out everything by itself.
    When that happens, all we'd have to do is create a simple config that would give
    it a few extra pieces of information. Still very easy and efficient.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在某些情况下，Heroku自己无法自动搞定一切。当这种情况发生时，我们所要做的就是创建一个简单的配置文件，提供一些额外的信息。依然非常简单高效。
- en: Startups loved it (some still do). It allowed them to concentrate on developing
    new features and leave everything else to Heroku. We write software, and someone
    else runs it. This is **Software-as-a-Service (SaaS)** at its best. The idea and
    the principles behind it become so popular that many decided to clone the idea
    and create their own Heroku-like services.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 初创公司很喜欢它（有些现在仍然喜欢）。它让他们能够专注于开发新功能，把其他的交给Heroku来做。我们编写软件，其他人来运行它。这就是**软件即服务（SaaS）**的最佳体现。它背后的理念和原则变得非常流行，以至于许多人决定复制这个想法，创建自己的Heroku类服务。
- en: Shortly after Heroku received a broad adoption, its creators realized that many
    applications did not perform as expected. It's one thing to have a platform that
    frees developers from operations, but quite another thing to actually write code
    that fares well under SaaS providers. So, Heroku folks and a few others came up
    with *The Twelve-Factor App* ([https://12factor.net/](https://12factor.net/))
    principles. If your application fulfills all twelve factors, it will work well
    as SaaS. Most of those factors are valid for any modern application, no matter
    whether it will run inside on-premise servers or through a cloud computing provider,
    inside PaaS, SaaS, a container, or none of the above. Every modern application
    should be made using The twelve-factor app methodology. Or, at least, that's what
    many are saying.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Heroku 广泛被采用后不久，它的创始人意识到许多应用程序并没有如预期般运行。拥有一个解放开发者的操作平台是一回事，但实际编写能在 SaaS 提供商环境下良好运行的代码又是另一回事。因此，Heroku
    的团队和一些其他人提出了 *The Twelve-Factor App*（[https://12factor.net/](https://12factor.net/)）原则。如果你的应用满足这十二个因素，它将在
    SaaS 中运行良好。这些因素大多数适用于任何现代应用程序，无论它是在本地服务器上运行，还是通过云计算提供商、PaaS、SaaS、容器等运行。每个现代应用都应该采用十二因素应用方法论，或者至少，很多人是这么说的。
- en: Let us explore each of those factors and see how well we fare against it. Maybe,
    just maybe, what we learned so far will make us twelve-factor compliant. We'll
    go through all the factors and compare them with the services we used through
    this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索每个因素，看看我们在这些方面表现如何。也许，仅仅是也许，我们迄今为止学到的东西将使我们符合十二因素原则。我们将逐一分析所有因素，并将它们与本书中使用的服务进行比较。
- en: '**Codebase**'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码库**'
- en: One codebase tracked in revision control, many deploys. The `go-demo` service
    is in a single Git repository. Every commit is deployed to testing and production
    environments. All other services we created are released by someone else. – Passed
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个代码库通过版本控制追踪，多个部署。`go-demo` 服务在一个 Git 仓库中。每次提交都部署到测试和生产环境中。我们创建的所有其他服务由其他人发布。
    – 通过
- en: '**Dependencies**'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**依赖关系**'
- en: Explicitly declare and isolate dependencies. All the dependencies are inside
    Dockers images. Excluding Docker Engine, there is no system-wide dependency. Docker
    images fulfill this principle by default.*– Passed*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 明确声明并隔离依赖关系。所有依赖项都包含在 Docker 镜像中。除去 Docker 引擎外，没有系统级别的依赖项。Docker 镜像默认遵循此原则。*–
    通过*
- en: '**Config**'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置**'
- en: Store config in the environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将配置存储在环境中。
- en: The `go-demo` service does not have any configuration file. Everything is set
    through environment variables. The same can be said for all other services we
    created. Service discovery through networking was a huge help accomplishing this,
    by allowing us to find services without any configuration. Please note that this
    principle applies only to configurations that vary between deployments. Everything
    else can continue being a file as long as it stays the same no matter when and
    where the service is deployed. *-* Passed
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo` 服务没有任何配置文件。所有内容都通过环境变量进行设置。我们创建的所有其他服务也可以这样说。通过网络进行的服务发现极大地帮助了我们实现这一点，它允许我们在没有任何配置的情况下找到服务。请注意，这一原则仅适用于在不同部署间会发生变化的配置。其他任何配置，只要在服务部署时无论何时何地保持一致，都可以继续存储为文件。
    *–* 通过'
- en: '**Backing services**'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**后端服务**'
- en: Treat backing services as attached resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将后端服务视为附加资源。
- en: In our case, MongoDB is a backing service. It is attached to the primary service
    `go-demo` through networking. *– Passed*
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，MongoDB 是一个后端服务。它通过网络连接到主服务 `go-demo`。 *– 通过*
- en: '**Build, release, run**'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建、发布、运行**'
- en: Strictly separate build and run stages.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 严格分离构建和运行阶段。
- en: In this context, everything except running services is considered the build
    phase. In our case, the build phase is clearly separated from run stages. Jenkins
    is building our services while Swarm is running them. Building and running are
    performed in separate clusters. *–* Passed
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，除运行服务外的所有内容都被视为构建阶段。在我们的情况下，构建阶段与运行阶段清晰分离。Jenkins 构建我们的服务，而 Swarm 负责运行它们。构建和运行在不同的集群中进行。
    *–* 通过
- en: '**Processes**'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**进程**'
- en: Execute the app as one or more stateless processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用程序作为一个或多个无状态进程执行。
- en: We are failing this principle big time. Even though the `go-demo` service is
    stateless, almost everything else (`docker-flow-proxy`, `jenkins`, `prometheus`,
    and so on) is not. – Failed
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一原则上严重失败。尽管 `go-demo` 服务是无状态的，但几乎所有其他服务（`docker-flow-proxy`、`jenkins`、`prometheus`
    等）都不是无状态的。 – 失败
- en: '**Port binding**'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**端口绑定**'
- en: Export services via port binding.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过端口绑定导出服务。
- en: Docker networking and the `docker-flow-proxy` are taking care of port bindings.
    In many cases, the only service that will bind any port is the `proxy`. Everything
    else should be inside one or more networks and made accessible through the `proxy`.
    – Passed
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 网络和 `docker-flow-proxy` 负责端口绑定。在许多情况下，唯一会绑定任何端口的服务是 `proxy`。其他一切服务应该位于一个或多个网络中，并通过
    `proxy` 使其可访问。– 已通过
- en: '**Concurrency**'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**并发性**'
- en: Scale out via the process model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进程模型进行扩展。
- en: 'This factor is directly related to statelessness. Stateless services (example:
    `go-demo`) are easy to scale. Some non-stateless services (example: `docker-flow-proxy`)
    are designed to be scalable, so they fulfill this principle as well. Many other
    stateful services (example: Jenkins, Prometheus, and so on) cannot be scaled horizontally.
    Even when they can, the process is often too complicated and prone to errors.
    – Failed'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这一因素直接与无状态性相关。无状态服务（例如：`go-demo`）易于扩展。一些非无状态服务（例如：`docker-flow-proxy`）被设计为可扩展的，因此它们也符合这一原则。许多其他有状态服务（例如：Jenkins、Prometheus
    等）无法进行水平扩展。即使可以，过程往往太复杂且容易出错。– 失败
- en: '**Disposability**'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可丢弃性**'
- en: Maximize robustness with fast startup and graceful shutdown.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过快速启动和优雅关机最大化系统的健壮性。
- en: Stateless services are disposable by default. They can be started and stopped
    at a moments notice, and they tend to be fault tolerant. In case an instance fails,
    Swarm will reschedule it in one of the healthy nodes. The same cannot be said
    for all the services we used. Jenkins and MongoDB, just to name a few, will lose
    their state in case of a failure. That makes them anything but disposable. – Failed
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态服务默认是可丢弃的。它们可以随时启动和停止，并且通常具有容错能力。如果一个实例失败，Swarm 会将其重新调度到健康的节点上。我们使用的所有服务并非如此。比如
    Jenkins 和 MongoDB，如果发生故障，它们会丢失状态，这使得它们根本不可丢弃。– 失败
- en: '**Dev/prod parity**'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开发/生产一致性**'
- en: Keep development, staging, and production as similar as possible.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 保持开发、预发布和生产环境尽可能相似。
- en: That is one of the main benefits Docker provides. Since containers are created
    from immutable images, a service will be the same no matter whether it runs on
    our laptop, testing environment, or production. – Passed
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Docker 提供的主要优点之一。由于容器是从不可变的镜像创建的，因此无论是在我们的笔记本电脑、测试环境还是生产环境中运行，服务都是相同的。– 已通过
- en: '**Logs**'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**日志**'
- en: Treat logs as event streams.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志视为事件流。
- en: The *ELK* stack together with *LogSpout* fulfills this principle. All the logs
    from all the containers are streamed into *ElasticSearch* as long as applications
    inside the containers are outputting them to `stdout`. Jenkins, as we run it,
    is the exception since it writes some of the logs to files. However, that is configurable,
    so we won't fault it for that. – Passed
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*ELK* 堆栈和 *LogSpout* 实现了这一原则。只要容器内部的应用程序将日志输出到 `stdout`，所有容器的日志都会被流式传输到 *ElasticSearch*。我们运行的
    Jenkins 是个例外，因为它将部分日志写入文件。不过，这个行为是可配置的，所以我们不对它提出批评。– 已通过'
- en: '**Admin processes**'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**管理进程**'
- en: Run `admin/management` tasks as one-off processes. In our case, all the processes
    are executed as Docker containers, apparently fulfilling this factor. – Passed
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `admin/management` 任务作为一次性进程运行。在我们的案例中，所有进程都是作为 Docker 容器执行的，显然符合这一因素。– 已通过
- en: We passed nine out of twelve factors. Should we aim for all twelve? Actually,
    the question is wrong. A better-phrased question would be whether we can aim for
    all twelve factors. We often can't. The world was not built yesterday, and we
    cannot throw away all the legacy code and start fresh. Even if we could, twelve-factor
    app principles have one big fallacy. They assume that there is such a thing as
    a system comprised completely of stateless services.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过了十二项因素中的九项。我们应该追求全部十二项吗？实际上，这个问题本身就有问题。一个更好的提问方式是：我们是否能追求全部十二项？我们通常做不到。这个世界不是昨天才建立的，我们无法丢弃所有遗留代码从头开始。即便我们能，十二因子应用原则也有一个重大缺陷：它假设有一个完全由无状态服务组成的系统。
- en: No matter which architecture style we adopt (microservices included), applications
    have a state! In a microservices style architecture, each service can have multiple
    instances, and each service instance should be designed to be stateless. What
    that means, is that a service instance does not store any data across operations.
    Hence, being stateless means that any service instance can retrieve all application
    state required to execute a behavior, from somewhere else. That is a significant
    architectural constraint of microservices style applications, as it enables resiliency,
    elasticity, and allows any available service instance to execute any task. Even
    though the state is not inside a service we are developing, it still exists and
    needs to be managed somehow. The fact that we did not develop the database where
    the state is stored does not mean that it should not follow the same principles
    and be scalable, fault tolerant, resilient, and so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们采用哪种架构风格（包括微服务），应用程序都有状态！在微服务架构中，每个服务可以有多个实例，每个服务实例应该设计为无状态的。无状态的意思是，服务实例在操作过程中不会存储任何数据。因此，无状态意味着任何服务实例都可以从其他地方检索执行某个行为所需的所有应用状态。这是微服务架构应用的一个重要限制，因为它使得系统具备弹性、可扩展性，并允许任何可用的服务实例执行任何任务。即使状态不在我们正在开发的服务内部，它仍然存在，并需要以某种方式进行管理。我们没有开发存储状态的数据库，并不意味着它不应遵循相同的原则，并具备可扩展性、容错性、弹性等特性。
- en: So, all systems have a state, but a service can be stateless if it cleanly separates
    behaviors from data, and can fetch data required to perform any behavior.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有系统都有状态，但如果一个服务能够干净地将行为与数据分离，并能够获取执行行为所需的数据，那么该服务可以是无状态的。
- en: Could the authors of twelve-factor app principles be so shortsighted as to assume
    that state does not exist? They indeed aren't. They assume that everything but
    the code we write will be a service maintained by someone else. Take MongoDB as
    an example. Its primary purpose is to store state, so it is, of course, stateful.
    The twelve-factor app authors assume that we are willing to let someone else manage
    stateful services and focus only on those we are developing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 十二要素应用的作者们是否会短视到认为状态不存在呢？他们显然不会。他们假设，除了我们编写的代码之外，其他一切都将是由其他人维护的服务。以 MongoDB
    为例，它的主要用途是存储状态，因此它当然是有状态的。十二要素应用的作者们假设，我们愿意让其他人管理有状态的服务，而只专注于我们正在开发的那些服务。
- en: While, in some cases, we might choose to use Mongo as a service maintained by
    one of the cloud providers, in many others such a choice is not the most efficient.
    If anything else, such services tend to be very expensive. That cost is often
    worth paying when we do not have the knowledge or the capacity to maintain our
    backing services. However, when we do, we can expect better and cheaper results
    if we run a database ourselves. In such a case, it is one of our services, and
    it is obviously stateful. The fact that we did not write all the services does
    not mean we are not running them and are, therefore, responsible for them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在某些情况下，我们可能会选择使用 Mongo 作为由云服务提供商维护的服务，但在许多其他情况下，这种选择并不是最有效的。无论如何，这类服务往往非常昂贵。当我们没有足够的知识或能力来维护我们的后台服务时，这种费用通常是值得支付的。然而，当我们有能力时，如果我们自己运行数据库，往往能得到更好、更便宜的结果。在这种情况下，数据库就是我们的一个服务，显然它是有状态的。我们没有编写所有服务并不意味着我们不在运行它们，因此，我们仍然对它们负责。
- en: The good news is that all three principles we failed are related to statefulness.
    If we manage to create services in a way that their state is preserved on shutdown
    and shared between all instances, we'll manage to make the whole system *cloud
    native*. We'll be able to run it anywhere, scale its services as needed, and make
    the system fault tolerant.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们未能实现的三个原则都与有状态性相关。如果我们能够以一种方式创建服务，使得它们的状态在关闭时被保存，并且在所有实例之间共享，那么我们就能够让整个系统变得*云原生*。我们将能够在任何地方运行它，根据需要扩展其服务，并使系统具备容错能力。
- en: Creating and managing stateful services is the only major piece of the puzzle
    we are missing. After this chapter, you will be on your way to running any type
    of services inside your Swarm cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和管理有状态服务是我们目前缺失的唯一重要环节。完成本章内容后，您将能够在 Swarm 集群内运行任何类型的服务。
- en: We will start the practical part of this chapter by creating a Swarm cluster.
    We'll use AWS only as a demonstration. The principles explored here can be applied
    to almost any cloud computing provider as well as to on-premise servers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实际部分将从创建 Swarm 集群开始。我们将仅使用 AWS 作为示范。这里探讨的原则可以应用于几乎任何云计算提供商，以及本地服务器。
- en: Setting up a Swarm cluster and the proxy
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Swarm 集群和代理
- en: We'll use *Packer* ([https://www.packer.io/](https://www.packer.io/)) and *Terraform* ([https://www.terraform.io/](https://www.terraform.io/))
    to create a Swarm cluster in AWS. For now, the configuration we'll use will be
    (almost) the same as the one we explored in the Chapter 12, *Creating and Managing
    a Docker Swarm Cluster in Amazon Web Services (AWS)*. We'll extend it later on
    when we reach more complex scenarios.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用*Packer*([https://www.packer.io/](https://www.packer.io/))和*Terraform*([https://www.terraform.io/](https://www.terraform.io/))在
    AWS 上创建一个 Swarm 集群。目前，我们将使用的配置将（几乎）与第12章中探索的配置相同，*在 Amazon Web Services (AWS)
    中创建和管理 Docker Swarm 集群*。在后续更复杂的场景中，我们会进一步扩展该配置。
- en: All the commands from this chapter are available in the `13-volumes.sh` ([https://gist.github.com/vfarcic/338e8f2baf2f0c9aa1ebd70daac31899](https://gist.github.com/vfarcic/338e8f2baf2f0c9aa1ebd70daac31899))
    Gist.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在`13-volumes.sh`([https://gist.github.com/vfarcic/338e8f2baf2f0c9aa1ebd70daac31899](https://gist.github.com/vfarcic/338e8f2baf2f0c9aa1ebd70daac31899))
    Gist 中找到。
- en: 'We''ll continue using the `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository. It contains configurations and scripts that''ll help us out. You already
    have it cloned. To be on the safe side, we''ll `pull` the latest version:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用`vfarcic/cloud-provisioning`([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    仓库。它包含了一些配置和脚本，将帮助我们完成工作。你已经克隆了这个仓库。为了保险起见，我们将`pull`最新版本：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Packer and Terraform configurations are in the `terraform/aws-full` ([https://github.com/vfarcic/cloud-provisioning/tree/master/terraform/aws-full](https://github.com/vfarcic/cloud-provisioning/tree/master/terraform/aws-full))
    directory:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Packer 和 Terraform 配置位于`terraform/aws-full`([https://github.com/vfarcic/cloud-provisioning/tree/master/terraform/aws-full](https://github.com/vfarcic/cloud-provisioning/tree/master/terraform/aws-full))目录下：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We''ll define a few environment variables that will provide Packer the information
    it needs when working with AWS:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一些环境变量，这些变量将为 Packer 提供在使用 AWS 时所需的信息：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Please replace `[...]` with the actual values. Consult the Chapter 12, *Creating
    And Managing A Docker Swarm Cluster in Amazon Web Services* if you lost the keys
    and forgot how to create them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请将`[...]`替换为实际值。如果你丢失了密钥并忘记了如何创建它们，请参考第12章，*在 Amazon Web Services 中创建和管理 Docker
    Swarm 集群*。
- en: 'We are ready to create the first image we''ll use in this chapter. The Packer
    configuration we''ll use is in `terraform/aws-full/packer-ubuntu-docker-compose.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-compose.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-compose.json)).
    It is almost the same as the one we used before so we''ll comment only the relevant
    differences. They are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好创建本章将使用的第一个镜像了。我们将使用的 Packer 配置位于`terraform/aws-full/packer-ubuntu-docker-compose.json`([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-compose.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-compose.json))。它与我们之前使用的几乎相同，因此我们只会评论相关的不同之处。具体如下：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The file provisioner copies the docker.service file into the VM. The commands
    from the shell provisioner will move the uploaded file to the correct directory,
    give it correct permissions, and restart the `docker service`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文件提供器将 `docker.service` 文件复制到虚拟机中。来自 shell 提供器的命令将把上传的文件移动到正确的目录，赋予正确的权限，并重启`docker
    service`。
- en: 'The `docker.service` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/docker.service](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/docker.service))
    file is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker.service`([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/docker.service](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/docker.service))
    文件内容如下：'
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Docker service configuration is almost identical to the default one. The
    only difference is `-H tcp://0.0.0.0:2375` in `ExecStart`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 服务配置与默认配置几乎相同。唯一的区别是`ExecStart`中的`-H tcp://0.0.0.0:2375`。
- en: By default, Docker Engine does not allow remote connections. If its configuration
    is left unchanged, we cannot send commands from one server to another. By adding
    `-H tcp://0.0.0.0:2375`, we are telling Docker to accept requests from any address
    `0.0.0.0`. Normally, that would be a big security risk. However, all AWS ports
    are closed by default. Later on, we'll open `2375` only to servers that belong
    to the same security group. As a result, we will be able to control any Docker
    Engine as long as we are inside one of our servers. As you will see soon, this
    will come in handy in quite a few examples that follow.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Docker Engine不允许远程连接。如果配置不变，我们无法从一台服务器向另一台服务器发送命令。通过添加`-H tcp://0.0.0.0:2375`，我们告诉Docker接受来自任何地址`0.0.0.0`的请求。通常，这会带来很大的安全风险。然而，所有AWS的端口默认是关闭的。稍后，我们将只对属于同一安全组的服务器开放`2375`端口。因此，只要我们在其中一台服务器内，我们就能够控制任何Docker
    Engine。正如你将很快看到的，这在接下来的几个例子中会非常有用。
- en: 'Let''s build the AMI defined in `packer-ubuntu-docker-compose.json`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建`packer-ubuntu-docker-compose.json`中定义的AMI：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can turn our attention to Terraform that''ll create our cluster. We''ll
    copy the SSH key `devops21.pem` that we created earlier and declare a few environment
    variables that will allow Terraform to access our AWS account:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将注意力转向Terraform，它将创建我们的集群。我们将复制之前创建的SSH密钥`devops21.pem`，并声明一些环境变量，以便Terraform能够访问我们的AWS账户：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Terraform expects environment variables to be prefixed with `TF_VAR`, so we
    had to create new ones, even though their values are the same as those we used
    for Packer. The value of the environment variable `KEY_PATH` is only an example.
    You might have it stored somewhere else. If that's the case, please change the
    value to the correct path.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform要求环境变量以`TF_VAR`为前缀，因此我们不得不创建新的环境变量，尽管它们的值和我们在Packer中使用的一样。环境变量`KEY_PATH`的值仅为示例。你可能将它存储在其他地方。如果是这样，请更改为正确的路径。
- en: The last command filters the `packer-ubuntu-docker.log` and stores the AMI ID
    as the environment variable `TF_VAR_swarm_ami_id`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条命令过滤了`packer-ubuntu-docker.log`并将AMI ID存储为环境变量`TF_VAR_swarm_ami_id`。
- en: 'Now we can create a Swarm cluster. Three VMs should suffice for the exercises
    that follow, so we''ll only create managers. Since the commands will be the same
    as those we executed in the previous chapters, we''ll just skip the explanation
    and run them:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个Swarm集群了。接下来的练习中，三个虚拟机就足够了，因此我们只会创建管理节点。由于命令和我们在前几章中执行的一样，我们将跳过解释，直接运行它们：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We created the first server and initialized the Swarm cluster. Later on, we
    retrieved the token and the IP of one of the managers and used that data to create
    and join two additional nodes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了第一个服务器并初始化了Swarm集群。随后，我们获取了令牌和其中一个管理节点的IP，并使用这些数据创建并加入了另外两个节点。
- en: 'To be on the safe side, we''ll enter one of the managers and list the nodes
    that form the cluster:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们将进入其中一个管理节点并列出组成集群的节点：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为简洁起见，已删除ID）：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Where are the workers?**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作节点在哪里？**'
- en: We did not create any worker nodes. The reason is simple. For the exercises
    in this chapter, three nodes are more than enough. That should not prevent you
    from adding worker nodes when you start using a similar cluster setup in your
    organization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有创建任何工作节点。原因很简单。对于本章中的练习，三个节点就足够了。这并不妨碍你在开始使用类似的集群设置时为你的组织添加工作节点。
- en: 'To add worker nodes, please execute the commands that follow:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加工作节点，请执行以下命令：
- en: '`export TF_VAR_swarm_worker_token=$(ssh\ ''-i devops21.pem ''''ubuntu@$(terraform
    output ''''swarm_manager_1_public_ip)'' ''docker swarm join-token -q worker) terraform
    apply\''-target aws_instance.swarm-worker''`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`export TF_VAR_swarm_worker_token=$(ssh\ ''-i devops21.pem ''''ubuntu@$(terraform
    output ''''swarm_manager_1_public_ip)'' ''docker swarm join-token -q worker) terraform
    apply\''-target aws_instance.swarm-worker''`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4/jenkins` in
    your browser.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出为`1.2.3.4`，你应该在浏览器中打开`http://1.2.3.4/jenkins`。
- en: 'We are almost done. The only thing left, before we move into statefulness,
    is to run the `docker-flow-proxy` and `docker-flow-swarm-listener` services. Since
    we already created them quite a few times, there''s no need for an explanation
    so we can speed up the process by deploying the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    stack:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快完成了。在进入有状态性之前，唯一剩下的就是运行`docker-flow-proxy`和`docker-flow-swarm-listener`服务。由于我们已经创建过这些服务很多次，因此不需要进一步解释，我们可以通过部署`vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))堆栈来加快过程：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running stateful services without data persistence
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无数据持久化的有状态服务运行
- en: We'll start the exploration of stateful services in a Swarm cluster by taking
    a look at what would happen if we deploy them as any other service.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Swarm集群中开始探索有状态服务，首先看看如果像其他服务一样部署它们，会发生什么情况。
- en: A good example is Jenkins. Every job we create is an XML file. Every plugin
    we install is an HPI file. Every configuration change is stored as XML. You get
    the picture. Everything we do in Jenkins ends up being a file. All those files
    form its state. Without it, Jenkins would not be able to operate. Jenkins is also
    a good example of the problems we have with legacy applications. If we were to
    design it today, it would probably use a database to store its state. That would
    allow us to scale it since all instances would share the same state by being connected
    to the same database. There are quite a few other design choices we would probably
    make if we were to design it today from scratch. Being legacy is not necessarily
    a bad thing. Sure, having the experience we have today would help us avoid some
    of the pitfalls of the past. On the other hand, being around for a long time means
    that it is battle tested, has a high rate of adoption, a huge number of contributors,
    a big user base, and so on. Everything is a trade-off, and we cannot have it all.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是Jenkins。我们创建的每个任务都是一个XML文件。我们安装的每个插件都是一个HPI文件。每次配置更改都会以XML格式存储。你可以明白了，Jenkins中的一切操作最终都会形成一个文件。这些文件构成了它的状态。如果没有这些，Jenkins将无法运行。Jenkins也是我们在遗留应用中遇到问题的一个很好的例子。如果我们今天重新设计它，可能会使用数据库来存储其状态。这样做可以让我们进行扩展，因为所有实例将通过连接到同一个数据库来共享相同的状态。如果我们今天从头开始设计，可能会做出很多其他的设计选择。成为遗留系统并不一定是坏事。当然，今天的经验帮助我们避免了一些过去的陷阱。另一方面，长时间的存在意味着它经过了战斗考验，拥有高采纳率，庞大的贡献者数量，广泛的用户基础等等。一切都有权衡，我们无法得到所有的好处。
- en: 'We''ll put aside the pros and cons of having a well established and battle-tested
    versus young and modern, but often unproven application. Instead, let''s take
    a look at how Jenkins, as a representative of a stateful service, behaves when
    running inside the Swarm cluster we created with Terraform:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将暂时搁置有一个成熟且经过验证的应用与年轻且现代但往往未经验证的应用之间的优缺点。相反，我们来看看作为有状态服务的Jenkins，在我们使用Terraform创建的Swarm集群中运行时的表现：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We entered one of the managers and created the `jenkins` service.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入了其中一台管理节点并创建了`jenkins`服务。
- en: Please wait a few moments until `jenkins` service is running. You can use docker
    `service ps jenkins` to check the current state.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请稍等片刻，直到`jenkins`服务运行起来。你可以使用docker命令`service ps jenkins`来检查当前状态。
- en: 'Now that Jenkins is running, we should open it in a browser:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Jenkins已经运行，我们应该在浏览器中打开它：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**A note to Windows users** Git Bash might not be able to use the `open` command.
    If that''s the case, execute `terraform output` `swarm_manager_1_public_ip` to
    find out the IP of the manager and open the URL directly in your browser of choice.
    For example, the command above should be replaced with the command that follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意** Git Bash 可能无法使用`open`命令。如果是这种情况，请执行`terraform output` `swarm_manager_1_public_ip`来查找管理节点的IP，并在你选择的浏览器中直接打开该URL。例如，上述命令应替换为以下命令：'
- en: '`terraform output swarm_manager_1_public_ip`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform output swarm_manager_1_public_ip`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4/jenkins` in
    your browser.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是`1.2.3.4`，你应该在浏览器中打开`http://1.2.3.4/jenkins`。
- en: As you remember from the [Chapter 6](600ac100-1492-4de3-9607-5ad11b628bbb.xhtml), *Automating
    Continuous Deployment Flow with Jenkins*, we need to retrieve the password from
    logs or its file system. However, this time, doing that is a bit more complicated.
    Docker Machine mounts local (laptop's) directory into every VM it creates so we
    could retrieve the `initialAdminPassword` without even entering VMs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从[第6章](600ac100-1492-4de3-9607-5ad11b628bbb.xhtml)《*使用 Jenkins 自动化持续部署流程*》中记得的那样，我们需要从日志或文件系统中提取密码。然而，这次，操作会稍微复杂一些。Docker
    Machine 将本地（笔记本电脑）目录挂载到它创建的每个虚拟机中，因此我们可以直接获取`initialAdminPassword`，而无需进入虚拟机。
- en: There is no such thing with AWS *at least not yet*, so we need to find out which
    EC2 instance hosts Jenkins, find the ID of the container, and enter into it to
    get the file. Such a thing would be easy to do manually but, since we are committed
    to automation, we'll do it the hard way.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 中并没有这样的事情，*至少现在没有*，所以我们需要找出哪个 EC2 实例托管了 Jenkins，获取容器的 ID，并进入其中以获取文件。虽然手动做这件事很容易，但由于我们坚持自动化，我们将采用更难的方式。
- en: 'We''ll start the quest of finding the password by entering one of the managers
    and list service tasks:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过进入其中一个管理界面并列出服务任务，开始寻找密码的任务：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows (IDs and ERROR coloumn are removed for brevity):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，ID和ERROR列已被移除）：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Luckily, AWS EC2 instances contain internal IP in their names. We can use that
    to our advantage:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，AWS EC2 实例的名称中包含了内部 IP。我们可以利用这一点：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We listed the service tasks and piped it to tail so that only the last line
    is returned. Then we used `awk` to get the fourth column. The cut command printed
    the result from the fourth byte effectively removing `ip-`. All that was piped
    to tr that replaced - with Finally, the result was stored in the environment variable
    `JENKINS_IP`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了服务任务并通过管道传递给`tail`，以便只返回最后一行。然后我们使用`awk`获取第四列。`cut`命令打印了第四个字节的结果，有效地移除了`ip-`。所有结果都通过`tr`命令替换了`-`，最后，结果被存储在环境变量`JENKINS_IP`中。
- en: If this was too freaky for you, feel free to assign the value manually (in my
    case it was `172.31.16.159).`
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这对你来说太过奇怪，可以手动指定该值（在我的情况下是`172.31.16.159`）。
- en: Now that we know which node is hosting Jenkins, we need to retrieve the ID of
    the container. Since we modified the `docker.service` config to allow us to send
    commands to a remote engine, we can use the `-H` argument.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了哪个节点托管了 Jenkins，我们需要获取容器的 ID。由于我们修改了`docker.service`配置，使得可以向远程引擎发送命令，我们可以使用`-H`参数。
- en: 'The command that retrieves the ID of the Jenkins container is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检索 Jenkins 容器 ID 的命令如下：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We used `-H` to tell the local client to connect to the remote engine running
    in `tcp://$JENKINS_IP:2375`. We listed all running containers `ps` in quiet mode
    `-q` so that only IDs are returned. We also applied a filter, so that only the
    service named Jenkins is retrieved. The result was stored in the environment variable
    `JENKINS_ID`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`-H`告诉本地客户端连接到在`tcp://$JENKINS_IP:2375`上运行的远程引擎。我们列出了所有正在运行的容器`ps`，并在安静模式`-q`下显示，确保只返回
    ID。同时我们应用了过滤器，只检索名为 Jenkins 的服务。结果被存储在环境变量`JENKINS_ID`中。
- en: Now we can use the IP and the ID to enter the container and output the password
    stored in the file `/var/jenkins_home/secrets/initialAdminPassword`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 IP 和 ID 进入容器并输出存储在文件`/var/jenkins_home/secrets/initialAdminPassword`中的密码。
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is, in my case, as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，输出如下：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Please copy the password, return to the Jenkins UI, and paste it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请复制密码，返回到 Jenkins UI 并粘贴。
- en: Complete the Jenkins setup before proceeding further. You already know the drill
    from the [Chapter 6](600ac100-1492-4de3-9607-5ad11b628bbb.xhtml), *Automating
    Continuous Deployment Flow with Jenkins*, so I'll let you do it in peace.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请完成 Jenkins 的设置。你已经从[第6章](600ac100-1492-4de3-9607-5ad11b628bbb.xhtml)《*使用
    Jenkins 自动化持续部署流程*》中了解了流程，所以我就不再多说了，让你安静地完成它。
- en: The result should be the screen similar to the one in the *figure 13-1:*
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是一个类似于*图 13-1*的屏幕：
- en: '![](img/jenkins-home.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/jenkins-home.png)'
- en: 'Figure 13-1: Jenkins home screen after the initial setup'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-1：初始设置后的 Jenkins 主屏幕
- en: Here comes the easy question which I'm sure you'll know how to answer. What
    would happen if, for whatever reason, Jenkins instance fails?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的问题，我相信你知道如何回答。如果由于某种原因，Jenkins 实例失败，会发生什么情况？
- en: 'Let''s simulate the failure and observe the outcome:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们模拟失败并观察结果：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We used the environment variables `JENKINS_IP` and `JENKINS_ID` to send the
    forced remove `rm -f` command to the remote node that hosts Jenkins.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了环境变量`JENKINS_IP`和`JENKINS_ID`，将强制移除`rm -f`命令发送到托管Jenkins的远程节点。
- en: Nothing lasts forever. Sooner or later, the service would fail. If it doesn't,
    the node where it runs will. By removing the container, we simulated what would
    happen in a real-world situation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么是永恒的。迟早，服务会失败。如果它不失败，运行它的节点会失败。通过移除容器，我们模拟了现实世界中可能发生的情况。
- en: 'After a while, Swarm will detect that the jenkins replica failed and instantiate
    a new one. We can confirm that by listing jenkins tasks:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，Swarm会检测到jenkins副本失败并实例化一个新的副本。我们可以通过列出jenkins任务来确认这一点：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，ID已被移除）：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So far, so good. Swarm is doing what we want it to do. It is making sure that
    our services are (almost) always running.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。Swarm正在按照我们希望的方式运行。它确保我们的服务（几乎）始终在运行。
- en: The only thing left is to go back to the UI and refresh the screen.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一任务是返回UI并刷新屏幕。
- en: The screen should look similar to the one in *figure 13-2:*
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕应该看起来像*图13-2*那样：
- en: '![](img/jenkins-setup.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/jenkins-setup.png)'
- en: 'Figure 13-2: Jenkins initial setup screen'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图13-2：Jenkins初始设置屏幕
- en: That's embarrassing. Everything we did is lost, and we are back to square one.
    Since Jenkins state was not persisted outside the container, when Swarm created
    a new one, it started with a blank slate.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这很尴尬。我们所做的一切都丢失了，我们又回到了原点。由于Jenkins的状态没有保存在容器外部，当Swarm创建了一个新的容器时，它从一个空白状态开始。
- en: How can we solve this problem? Which solutions can we employ to address the
    persistence issue?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该如何解决这个问题？我们可以采用哪些解决方案来解决持久化问题？
- en: 'Please remove the `jenkins` service before proceeding:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请在继续之前移除`jenkins`服务：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Persisting stateful services on the host
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在主机上持久化有状态服务
- en: Host-based persistence was very common in the early Docker days when people
    were running containers on predefined nodes without schedulers like Docker Swarm,
    Kubernetes, or Mesos. Back then, we would choose a node where we'll run a container
    and put it there. Upgrades were performed on the same server. In other words,
    we packaged applications as containers and, for the most part, treated them as
    any other traditional service. If a node fails... tough luck! It's a disaster
    with or without containers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker早期，当人们在没有Docker Swarm、Kubernetes或Mesos等调度程序的预定义节点上运行容器时，主机上的持久化非常常见。当时，我们会选择一个节点来运行容器，并将其放在那里。升级会在同一服务器上进行。换句话说，我们将应用程序打包成容器，并在大多数情况下将其视为任何传统服务。如果一个节点发生故障……运气不好！不管有没有容器，都会是灾难。
- en: Since serves were prederfined, we could persist the state on the host and rely
    on backups when that host dies. Depending on the backup frequency, we could lose
    a minute, an hour, a day, or even a whole week worth of data. Life is hard.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于服务是预定义的，我们可以在主机上持久化状态，并在主机故障时依赖备份。根据备份的频率，我们可能会丢失一分钟、一小时、一天，甚至一整周的数据。生活真是艰难。
- en: The only positive thing about this approach is that persistence is easy. We
    would mount a host volume inside a container. Files are persisted outside the
    container so no data would be lost under "normal" circumstances. If the container
    is restarted as a result of a failure or an upgrade, data would still be there
    when we run a new container.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法唯一的优点是持久化很简单。我们会在容器内挂载一个主机卷。文件会保存在容器外部，因此在“正常”情况下不会丢失数据。如果容器因为故障或升级而重新启动，当我们运行新容器时，数据依然存在。
- en: There are other single-host variations of the model. Data volumes, data only
    containers, and so on. All of them share the same drawback. They remove portability.
    Without portability, there is no fault tolerance, nor is there scaling. There
    is no Swarm.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他单主机的模型变体，比如数据卷、仅数据容器等。它们都有相同的缺点。它们消除了可移植性。没有可移植性，就没有容错，也没有扩展能力。没有Swarm。
- en: Host-based persistence is unacceptable, so I won't waste any more of your time.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 基于主机的持久化是不可接受的，因此我不会再浪费你们的时间。
- en: If you have a sysadmin background, you are probably wondering why I haven’t
    mentioned N**etwork File System **(**NFS**). The reason is simple. I wanted you
    to feel the pain before diving into the obvious solution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有系统管理员的背景，可能会想知道为什么我没有提到网络文件系统（**NFS**）。原因很简单。我想让你先感受一下痛苦，然后再深入探讨显而易见的解决方案。
- en: Persisting stateful services on a Network File System
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网络文件系统上持久化有状态服务
- en: We need to find a way to retain state outside containers that run our services.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到一种方法来在运行我们服务的容器之外保留状态。
- en: We could mount a volume on the host. That would allow us to preserve state if
    a container fails and is rescheduled on the same node. The problem is that such
    a solution is too limited. There is no guarantee that Swarm will reschedule the
    service to the same node unless we constrain it. If we would do something like
    that, we'd prevent Swarm from ensuring service availability. When that node would
    fail (every node fails sooner or later), Swarm could not reschedule the service.
    We would be fault tolerant only as long as our servers are running.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在主机上挂载一个卷。这样，如果容器失败并在同一节点重新调度，它将允许我们保持状态。问题在于，这样的解决方案过于局限。除非我们加以约束，否则无法保证Swarm会将服务重新调度到相同的节点。如果我们做了这样的事情，就会妨碍Swarm确保服务的可用性。当该节点发生故障时（每个节点总有一天会故障），Swarm无法重新调度服务。只有当我们的服务器正常运行时，我们才能保证容错能力。
- en: We can solve the problem of a node failure by mounting a NFS to each of the
    servers. That way, every server would have access to the same data, and we could
    mount a Docker volume to it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将NFS挂载到每台服务器上来解决节点故障问题。这样，每台服务器都可以访问相同的数据，我们可以将Docker卷挂载到它上面。
- en: We'll use **Amazon Elastic File System** (**EFS**) ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)).
    Since this book is not dedicated to AWS, I’ll skip the comparison of different
    AWS file systems and only note that the choice of EFS is based on its ability
    to be used across multiple availability zones.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**Amazon弹性文件系统**(**EFS**)([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/))。由于本书并未专门讲解AWS，我将跳过不同AWS文件系统的比较，仅提到选择EFS是因为它可以跨多个可用区使用。
- en: 'Please open the *EFS home *([https://console.aws.amazon.com/efs/home](https://console.aws.amazon.com/efs/home))
    screen:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开*EFS首页*([https://console.aws.amazon.com/efs/home](https://console.aws.amazon.com/efs/home))界面：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**A note to Windows users**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意**'
- en: Git Bash might not be able to use the `open` command. If that's the case, please
    replace `$AWS_DEFAULT_REGION` with the region where your cluster is running (for
    example, `us-east-1`) and open it in a browser.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash可能无法使用`open`命令。如果是这种情况，请将`$AWS_DEFAULT_REGION`替换为您的集群所在的区域（例如，`us-east-1`），并在浏览器中打开它。
- en: Click the Create file system button. For each of the availability zones, replace
    the default security group with *docker* (we created it earlier with Terraform).
    Click the button Next Step twice, followed by Create File System.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“创建文件系统”按钮。在每个可用区中，将默认的安全组替换为*docker*（我们之前使用Terraform创建了它）。然后点击“下一步”按钮两次，最后点击“创建文件系统”。
- en: We should wait until Life cycle state is set to Available for each of the zones.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应等待直到每个可用区的生命周期状态设置为“可用”。
- en: Now we are ready to mount the EFS in each of the nodes. The easiest way to do
    that is by clicking the Amazon EC2 mount instructions link. We are interested
    only in the command from the third point of the Mounting your file system section.
    Please copy it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备在每个节点上挂载EFS。最简单的做法是点击Amazon EC2挂载说明链接。我们只需要复制“挂载文件系统”部分第三点中的命令。
- en: 'All that''s left is to enter each of the nodes and execute the command that
    will mount the EFS volume:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来只需进入每个节点并执行挂载EFS卷的命令：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We entered the first manager and created `/mnt/efs` directory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入了第一个管理节点并创建了`/mnt/efs`目录。
- en: Paste the command you copied from the EC2 mount instructions screen. We'll make
    a tiny modification before executing it. Please change the destination path from
    `efs` to `/mnt/efs` and execute it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 粘贴您从EC2挂载说明界面复制的命令。在执行之前，我们需要做一个小的修改。请将目标路径从`efs`改为`/mnt/efs`，然后执行命令。
- en: 'In my case, the command is as follows (yours will be different):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的案例中，命令如下（您的命令会有所不同）：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We should also create a sub-directory where we''ll store Jenkins state:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该创建一个子目录，用于存储Jenkins状态：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We created the directory `/mnt/efs/jenkins,` gave full permissions to everyone,
    and exited the server. Since Swarm might decide to create the service on any of
    the nodes, we should repeat the same process on the rest of the servers. Please
    note that your mount will be different, so do not simply paste the `sudo mount`
    command that follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了目录`/mnt/efs/jenkins`，并授予了所有人完全权限，然后退出了服务器。由于Swarm可能会选择在任意节点上创建服务，我们应在其余服务器上重复相同的过程。请注意，您的挂载路径会有所不同，因此不要直接粘贴下面的`sudo
    mount`命令：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can, finally, try again to create the `jenkins` service. Hopefully,
    this time the state will be preserved in case of a failure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以再次尝试创建 `jenkins` 服务。希望这次在发生故障时状态能够得到保留：
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The only difference between this command and the one we used before is in the
    `--mount` argument. It tells Docker to mount host directory `/mnt/efs/jenkins`
    as `/var/jenkins_home` inside the container. Since we mounted `/mnt/efs` as EFS
    volume on all nodes, the `jenkins` service will have access to the same files
    no matter which server it will run in.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令和我们之前使用的命令唯一的区别在于`--mount`参数。它告诉 Docker 将主机目录 `/mnt/efs/jenkins` 挂载为容器内的
    `/var/jenkins_home`。由于我们在所有节点上将 `/mnt/efs` 挂载为 EFS 卷，因此无论 `jenkins` 服务运行在哪台服务器上，它都能访问相同的文件。
- en: 'Now we should wait until the service is running. Please execute the `service
    ps` command to see the current state:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该等待直到服务启动。请执行 `service ps` 命令查看当前状态：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s open Jenkins UI in a browser:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在浏览器中打开 Jenkins UI：
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**A note to Windows users**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows 用户注意**'
- en: 'Git Bash might not be able to use the `open` command. If that''s the case,
    execute `terraform output swarm_manager_1_public_ip` to find out the IP of the
    manager and open the URL directly in your browser of choice. For example, the
    preceding command should be replaced with the command that follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 `open` 命令。如果是这种情况，请执行 `terraform output swarm_manager_1_public_ip`
    查找管理节点的 IP 地址，然后直接在你选择的浏览器中打开该 URL。例如，上面的命令应该替换为以下命令：
- en: '`terraform output swarm_manager_1_public_ip`If the output would be `1.2.3.4`,
    you should open `http://1.2.3.4/jenkins` in your browser.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform output swarm_manager_1_public_ip` 如果输出是 `1.2.3.4`，你应该在浏览器中打开 `http://1.2.3.4/jenkins`。'
- en: 'This time, since Jenkins home directory is mounted as `/mnt/efs/jenkins,` finding
    the password will be much easier. All we have to to is output the contents of
    the file `/mnt/efs/jenkins/secrets/initialAdminPassword` from one of the servers:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，由于 Jenkins 主目录被挂载为 `/mnt/efs/jenkins`，查找密码变得更加容易。我们只需要从其中一台服务器输出 `/mnt/efs/jenkins/secrets/initialAdminPassword`
    文件的内容：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Please copy the password and paste it to the Administrator password field in
    the Jenkins UI. Complete the setup:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请复制密码并将其粘贴到 Jenkins UI 中的管理员密码字段。完成设置：
- en: '![](img/jenkins-home.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/jenkins-home.png)'
- en: 'Figure 13-3: Jenkins home screen after the initial setup'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-3：初始设置后的 Jenkins 主屏幕
- en: 'We''ll simulate a failure one more time and observe the results. The commands
    that follow are the same as those we executed previously so there should be no
    reason to comment them:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次模拟故障并观察结果。接下来的命令与我们之前执行的命令相同，所以没有理由对它们进行评论：
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Please wait until Swarm instantiates a new replica and refresh the Jenkins
    UI screen in your browser. This time, we are presented with the login page instead
    of going back to the initial setup. The state was preserved, making our `jenkins`
    service fault tolerant. In the worst case scenario, when the service or the entire
    node fails, we''ll have a short period of downtime until Swarm recreates the failed
    replica. You might be wondering: why did I force you to go through manual steps
    to create an EFS and mount it? Shouldn''t that be done automatically through Terraform?
    The reason is simple. This solution is not worth automating. It has quite a few
    downsides.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请等待 Swarm 启动一个新的副本，并在浏览器中刷新 Jenkins UI 屏幕。这一次，我们看到的是登录页面，而不是返回到初始设置。状态得以保存，使得我们的
    `jenkins` 服务具备容错能力。在最坏的情况下，当服务或整个节点失败时，我们会经历短暂的停机，直到 Swarm 重新创建失败的副本。你可能会想：为什么我强制你手动创建
    EFS 并挂载它？难道这不应该通过 Terraform 自动完成吗？原因很简单：这个解决方案不值得自动化，它有很多缺点。
- en: We would need to place states from all the services into the same EFS drive.
    A better solution would be to create an EFS volume for each service. The problem
    with such an approach is that we would need to alter the Terraform config every
    time someone adds a new stateful service to the cluster. In that case, Terraform
    would not be of much help since it is not meant to have service-specific configs.
    It should act as a method to setup a cluster that could host any service. Even
    if we accept a single EFS volume for all services, we would still need to create
    a new sub-directory for each service. Wouldn't it be much better if we leave Terraform
    as a tool for creating infrastructure and Docker for all tasks related to services?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将所有服务的状态放置到同一个EFS驱动器中。更好的解决方案是为每个服务创建一个EFS卷。采用这种方法的问题是，每当有人向集群添加一个新的有状态服务时，我们都需要修改Terraform配置。在这种情况下，Terraform的帮助有限，因为它并不是为了具有特定服务配置而设计的。它应当作为一种设置集群的方法，能够托管任何服务。即使我们接受为所有服务使用一个单独的EFS卷，我们仍然需要为每个服务创建一个新的子目录。如果我们将Terraform作为创建基础设施的工具，而将Docker用于所有与服务相关的任务，岂不是更好？
- en: Fortunately, there are better ways to create and mount EFS volumes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，还有更好的方法来创建和挂载EFS卷。
- en: 'Before we explore alternatives, please remove the `jenkins` service and `exit`
    the server:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索其他替代方案之前，请先移除`jenkins`服务并`exit`服务器：
- en: '[PRE33]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: There is no reason to keep the EFS volume we created earlier, so please head
    back to *EFS console* ([https://console.aws.amazon.com/efs](https://console.aws.amazon.com/efs)),
    select the file system, and click Actions followed with the Delete file system
    button. For the rest of the steps, please follow the instructions on the screen.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前创建的EFS卷没有必要保留，所以请返回到*EFS控制台*（[https://console.aws.amazon.com/efs](https://console.aws.amazon.com/efs)），选择文件系统，然后点击“操作”并点击“删除文件系统”按钮。其余步骤请按照屏幕上的说明进行操作。
- en: Data volume orchestration
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据卷编排
- en: There are quite a few storage orchestration solutions that integrate with Docker
    through its volume plugins. We won’t compare them. Such an attempt would require
    a whole chapter, maybe even a book.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多存储编排解决方案通过其卷插件与Docker集成。我们不会对它们进行比较。这样的尝试需要整整一章内容，甚至可能一本书。
- en: Even if you choose a different solution, the principles that will be explained
    shortly apply to (almost) all others. For a complete list of currently supported
    plugins, please visit the *Volume plugins* ([https://docs.docker.com/engine/extend/legacy_plugins/#/volume-plugins](https://docs.docker.com/engine/extend/legacy_plugins/#/volume-plugins))
    section of the *Use Docker Engine Plugins* ([https://docs.docker.com/engine/extend/legacy_plugins/](https://docs.docker.com/engine/extend/legacy_plugins/))
    documentation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你选择了不同的解决方案，接下来要解释的原则也适用于（几乎）所有其他解决方案。有关当前支持的插件的完整列表，请访问*卷插件*（[https://docs.docker.com/engine/extend/legacy_plugins/#/volume-plugins](https://docs.docker.com/engine/extend/legacy_plugins/#/volume-plugins)）部分，详情请见*使用Docker引擎插件*（[https://docs.docker.com/engine/extend/legacy_plugins/](https://docs.docker.com/engine/extend/legacy_plugins/)）文档。
- en: '*REX-Ray* ([https://github.com/codedellemc/rexray](https://github.com/codedellemc/rexray))
    is a vendor agnostic storage orchestration engine. It is built on top of the *libStorage* ([http://libstorage.readthedocs.io](http://libstorage.readthedocs.io))
    framework. It supports *EMC*, *Oracle VirtualBox*, and *Amazon EC2*. At the time
    of this writing, support for *GCE*, *Open Stack*, *Rackspace*, and *DigitalOcean*
    is under way.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*REX-Ray*（[https://github.com/codedellemc/rexray](https://github.com/codedellemc/rexray)）是一个供应商无关的存储编排引擎。它建立在*libStorage*（[http://libstorage.readthedocs.io](http://libstorage.readthedocs.io)）框架之上。它支持*EMC*、*Oracle
    VirtualBox*和*Amazon EC2*。在撰写本文时，*GCE*、*Open Stack*、*Rackspace*和*DigitalOcean*的支持正在进行中。'
- en: I find it easier to grasp something when I see it in action. In that spirit,
    instead of debating for a long time what REX-Ray does and how it works, we'll
    jump right into a practical demonstration.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现当我看到某个事物实际操作时，更容易理解它。秉持这种精神，我们将不再长时间辩论REX-Ray的功能和工作原理，而是直接进入实际演示。
- en: Persisting stateful services with REX-Ray
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用REX-Ray持久化有状态服务
- en: We'll start by setting up REX-Ray manually. If it turns out to be a good solution
    for our stateful services, we'll move it to Packer and Terraform configurations.
    Another reason for starting with a manual setup is to give you a better understanding
    how it works.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从手动设置REX-Ray开始。如果它证明是我们有状态服务的一个好解决方案，我们将把它转移到Packer和Terraform配置中。我们从手动设置开始的另一个原因是让你更好地理解它是如何工作的。
- en: Let's get going.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: 'Besides the AWS access keys and the region that we already used quite a few
    times, we''ll also need the ID of the security group we created previously with
    Terraform:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们已经使用过很多次的 AWS 访问密钥和区域，我们还需要通过 Terraform 创建的安全组 ID：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output should be similar to the one that follows (yours will be different):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于下面的内容（你的输出会有所不同）：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Please copy the value. We'll need it soon.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请复制该值，我们很快会用到它。
- en: 'We''ll enter one of the nodes where we''ll install and configure REX-Ray:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进入其中一个节点，在那里安装和配置 REX-Ray：
- en: '[PRE36]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'REX-Ray is fairly simple to set up. That''s one of the reasons I prefer it
    over some other solutions:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: REX-Ray 的设置相对简单，这也是我比其他一些解决方案更喜欢它的原因之一：
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We installed *REX-Ray version 0.6.3*, as well as its dependency *libStorage
    version 0.3.5*. In your case, versions might be newer.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们安装了 *REX-Ray 版本 0.6.3* 以及其依赖 *libStorage 版本 0.3.5*。在你的情况下，版本可能会更新。
- en: 'Next, we''ll create environment variables with values required for REX-Ray
    configuration:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建所需的环境变量，用于 REX-Ray 配置：
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Please replace `[...]` with the actual values. The value of the security group
    should be the same as the one we previously retrieved with the `terraform output
    security_group_id` command.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 请将 `[...]` 替换为实际值。安全组的值应与我们之前通过 `terraform output security_group_id` 命令获取的相同。
- en: 'Now we are ready to configure REX-Ray through its YML configuration file stored
    in `/etc/rexray/config.yml`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备通过位于 `/etc/rexray/config.yml` 的 YML 配置文件来配置 REX-Ray：
- en: '[PRE40]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We set the driver to `efs` and provided it with the AWS data. The result was
    output to the `/etc/rexray/config.yml` file.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将驱动程序设置为 `efs` 并提供了 AWS 数据。结果输出到 `/etc/rexray/config.yml` 文件。
- en: 'Now we can start the service:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以启动服务：
- en: '[PRE41]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'REX-Ray is running, and we can `exit` the node:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: REX-Ray 正在运行，我们可以 `exit` 节点：
- en: '[PRE43]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Since we do not know which node will host our stateful services, we need to
    set up REX-Ray on every node of the cluster. Please repeat the setup steps on
    Swarm manager nodes 2 and 3.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不知道哪个节点将托管我们的有状态服务，我们需要在集群的每个节点上设置 REX-Ray。请在 Swarm 管理节点 2 和 3 上重复这些设置步骤。
- en: 'Once REX-Ray is running on all the nodes, we can give it a spin. Please enter
    one of the managers:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 REX-Ray 在所有节点上运行，我们可以开始尝试。请进入其中一个管理节点：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'REX-Ray can be used directly through the `rexray` binary we installed. For
    example, we can list all the volumes:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过我们安装的 `rexray` 二进制文件直接使用 REX-Ray。例如，我们可以列出所有的卷：
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: There's not much to see since we have not created any volumes yet. We can do
    that with the `rexray` volume create command. However, there is no need for such
    a thing. Thanks to its integration with Docker, there is not much need to use
    the binary directly for any operation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们尚未创建任何卷，所以没有太多可查看的内容。我们可以使用 `rexray` 卷创建命令来实现这一点。然而，没有必要这么做。得益于与 Docker
    的集成，实际上没有太多必要直接使用二进制文件进行任何操作。
- en: 'Let''s try one more time to create the `jenkins` service. This time, we''ll
    use REX-Ray as the volume driver:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次创建 `jenkins` 服务。这一次，我们将使用 REX-Ray 作为卷驱动：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The only difference between the command we just executed and the previous attempt
    to create the `jenkins` service is in the `--mount` argument. The source is now
    simply a name `jenkins`. It represents the name of the volume. The target is still
    the same and represents Jenkins home inside a container. The important difference
    is the addition of the `volume-driver` argument. That was the instruction that
    Docker should use `rexray` to mount a volume.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚执行的命令与之前尝试创建 `jenkins` 服务的唯一区别在于 `--mount` 参数。源现在只是一个名称 `jenkins`，它代表卷的名称。目标依然相同，表示容器内的
    Jenkins 主目录。重要的区别是添加了 `volume-driver` 参数。这条指令告诉 Docker 使用 `rexray` 来挂载卷。
- en: 'If integration between REX-Ray and Docker worked, we should see a `jenkins`
    volume:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 REX-Ray 和 Docker 的集成正常，我们应该能看到一个 `jenkins` 卷：
- en: '[PRE48]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE49]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This time, the output of the `rexray` volume get command is not empty. We can
    see the `jenkins` volume. As I already mentioned, there''s not much need to use
    the `rexray` binary. We can accomplish many of its features directly through Docker.
    For example, we can execute `docker volume ls` command to list all the volumes:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，`rexray` 卷获取命令的输出不为空。我们可以看到 `jenkins` 卷。如我之前提到的，实际上没有必要使用 `rexray` 二进制文件。我们可以通过
    Docker 直接实现它的许多功能。例如，我们可以执行 `docker volume ls` 命令来列出所有卷：
- en: '[PRE50]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE51]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Listing volumes proves only that Docker and REX-Ray registered a new mount.
    Let''s take a look at what happened in AWS:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列出卷仅证明 Docker 和 REX-Ray 注册了一个新的挂载。让我们看看在 AWS 中发生了什么：
- en: '[PRE52]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**A note to Windows users**'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**给 Windows 用户的提示**'
- en: Git Bash might not be able to use the `open` command. If that's the case, please
    replace `$AWS_DEFAULT_REGION` with the region where your cluster is running (for
    example, `us-east-1`) and open it in a browser.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 `open` 命令。如果是这种情况，请将 `$AWS_DEFAULT_REGION` 替换为你集群所在的区域（例如，`us-east-1`），然后在浏览器中打开。
- en: 'You should see a screen similar to the one presented in *Figure 13-4*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一个类似于 *图 13-4* 中展示的屏幕：
- en: '![](img/efs-rexray.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efs-rexray.png)'
- en: 'Figure 13-4: AWS EFS volume created and mounted with REX-Ray'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13-4：AWS EFS 卷与 REX-Ray 一起创建并挂载
- en: As you can see, REX-Ray created a new EFS volume called `rexray/jenkins` and
    mounted a target in the same availability zone as the node that hosts the `jenkins`
    service.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，REX-Ray 创建了一个名为 `rexray/jenkins` 的新 EFS 卷，并在与托管 `jenkins` 服务的节点相同的可用区挂载了一个目标。
- en: The only thing missing to satisfy my paranoid nature is to kill Jenkins and
    confirm that REX-Ray mounted the EFS volume on a new container that will be re-scheduled
    by Swarm.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一缺少的部分，以满足我多疑的本性，就是杀掉 Jenkins，并确认 REX-Ray 是否在 Swarm 重新调度的一个新容器上挂载了 EFS 卷。
- en: 'As before, we’ll start by setting up Jenkins:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将从设置 Jenkins 开始：
- en: '[PRE53]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**A note to Windows users**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**给 Windows 用户的提示**'
- en: 'Git Bash might not be able to use the `open` command. If that’s the case, execute
    `terraform output` `swarm_manager_1_public_ip` to find out the IP of the manager
    and open the URL directly in your browser of choice. For example, the preceding
    command should be replaced with the command that follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 `open` 命令。如果是这种情况，请执行 `terraform output` `swarm_manager_1_public_ip`
    以查找管理器的 IP，并直接在你选择的浏览器中打开该 URL。例如，前面的命令应替换为以下命令：
- en: '`terraform output swarm_manager_1_public_ip`If the output would be `1.2.3.4`,
    you should open `http://1.2.3.4/jenkins` in your browser.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform output swarm_manager_1_public_ip` 如果输出是 `1.2.3.4`，你应该在浏览器中打开 `http://1.2.3.4/jenkins`。'
- en: We are faced with a recurring challenge. How to find the initial Jenkins administrator
    password. On the bright side, the challenge is useful as a demonstration of different
    ways to access content inside containers.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临着一个反复出现的挑战。如何找到初始 Jenkins 管理员密码。从好的一面看，这个挑战作为演示不同方式访问容器内内容的示例非常有用。
- en: 'This time, we''ll leverage REX-Ray to access data stored in the EFS volume
    instead of trying to find the node and the ID of the container that hosts the
    `jenkins` service:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将利用 REX-Ray 访问存储在 EFS 卷中的数据，而不是尝试查找托管 `jenkins` 服务的节点和容器 ID：
- en: '[PRE54]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output should be similar to the one that follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该与以下内容类似：
- en: '[PRE55]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We created a new alpine container that also used the rexray volume driver to
    attach to the jenkins EFS volume. The command output the contents of the `/var/jenkins_home/secrets/initialAdminPassword`
    file that contains the password. Since we specified the `--rm` argument, Docker
    removed the container after the process `cat` exited. The final result is the
    password output to the screen. Please copy it and paste it to the Administrator
    password field in the Jenkins UI. Complete the setup.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个新的 alpine 容器，并使用 rexray 卷驱动程序将其附加到 jenkins EFS 卷。命令输出了 `/var/jenkins_home/secrets/initialAdminPassword`
    文件的内容，该文件包含密码。由于我们指定了 `--rm` 参数，Docker 在进程 `cat` 退出后会移除容器。最终结果是密码输出到屏幕。请复制并粘贴到
    Jenkins UI 中的管理员密码字段，完成设置。
- en: 'Now we need to go through the painful processes of finding the node that hosts
    Jenkins, getting the ID of the container, and executing the `docker rm` command
    on the remote engine. In other words, we''ll run the same set of commands we executed
    during previous murderous attempts:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要经历一个痛苦的过程，找到托管 Jenkins 的节点，获取容器的 ID，并在远程引擎上执行 `docker rm` 命令。换句话说，我们将运行与之前尝试时相同的一组命令：
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: A few moments later, Swarm will re-schedule the container, and Jenkins will
    be running again.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，Swarm 将重新调度容器，Jenkins 将再次运行。
- en: 'Please wait until the service current state is running:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请等待直到服务当前状态为运行中：
- en: '[PRE57]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Reload Jenkins UI and observe that you are redirected to the login screen instead
    to the initial setup. The state was preserved.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载 Jenkins UI，观察你是否被重定向到登录屏幕，而不是初始设置页面。状态已被保存。
- en: 'We''re finished with this cluster. Now we need to remove the volume manually.
    Otherwise, since it was not created by Terraform, it would be preserved even after
    we destroy the cluster and AWS would continue charging us for it. The problem
    is that a volume cannot be removed as long as one or more services are using it,
    so we''ll need to destroy `jenkins` service as well:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了这个集群。现在我们需要手动删除卷。否则，由于它不是由 Terraform 创建的，即使我们销毁集群，AWS 仍会继续收取费用。问题是，如果有一个或多个服务正在使用这个卷，它是不能被删除的，因此我们还需要销毁
    `jenkins` 服务：
- en: '[PRE58]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Choosing the persistence method for stateful services
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为有状态服务选择持久化方法
- en: 'There are quite a few other tools we could use to persist state. Most of them
    fall into one of the groups we explored. Among different approaches we can take,
    the three most commonly taken are as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用很多其他工具来持久化状态。它们中的大多数可以归为我们探索过的几类。在我们可以采取的不同方法中，最常用的三种方法如下：
- en: Do not persist the state.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不持久化状态。
- en: Persist the state on the host.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态持久化到主机上。
- en: Persist the state somewhere outside the cluster.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态持久化到集群外部的某个地方。
- en: There’s no reason to debate why persisting data from stateful services is critical,
    so the first option is automatically discarded.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要争论为什么持久化有状态服务的数据是至关重要的，因此第一个选项自动被淘汰。
- en: 'Since we are operating a cluster, we cannot rely on any given host to be always
    available. It might fail at any given moment. Even if a node does not fail, sooner
    or later a service will, and Swarm will reschedule it. When that happens, there
    is no guarantee that Swarm will run a new replica on the same host. Even if, against
    all odds, the node never fails, and the service is unbreakable, the first time
    we execute an update of that service (example: a new release), Swarm will, potentially,
    create the new replica somewhere else. All in all, we don’t know where the service
    will run nor how long it will stay there. The only way to invalidate that statement
    is to use constraints that would tie a service to a particular host. However,
    if we do that, there would be no purpose in using Swarm nor reading this book.
    All in all, the state should not be persisted on a specific host.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在操作一个集群，我们不能依赖任何单一主机始终可用。它可能随时发生故障。即使某个节点没有故障，迟早某个服务会故障，Swarm 会重新调度它。发生这种情况时，我们无法保证
    Swarm 会在同一主机上运行新副本。即使不幸的是，节点从未发生故障，且服务永不出问题，在我们第一次执行该服务的更新（例如：发布新版本）时，Swarm 也可能会在其他地方创建新副本。总之，我们不知道服务会在哪儿运行，也不知道它会在那里停留多久。唯一能否定这个说法的方法是使用约束，将服务绑定到特定主机上。然而，如果我们这么做，那就没有必要再使用
    Swarm 或者阅读这本书了。总之，状态不应持久化到特定主机上。
- en: That leaves us with the third option. The state should be persisted somewhere
    outside the cluster, probably on a network drive. Traditionally, sysadmins would
    mount a network drive on all hosts, thus making the state available to services
    no matter where they’re running. There are quite a few problems with that approach,
    the main one being the need to mount a single drive and expect all stateful services
    to persist their state to it. We could, theoretically, mount a new drive for every
    single service. Such a requirement would quickly become a burden. If, for example,
    we used Terraform to manage our infrastructure, we’d need to update it every time
    there is a new service. Do you remember the first principle of twelve-factor apps?
    One service should have one codebase. Everything that a service needs should be
    in a single repository. Therefore, Terraform or any other infrastructure configuration
    tool should not contain any details specific to a service.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们剩下了第三个选择。状态应该被持久化到集群外部的某个地方，可能是一个网络驱动器。传统上，系统管理员会在所有主机上挂载一个网络驱动器，从而使服务无论运行在哪里都能访问状态。这个方法有很多问题，主要的问题是需要挂载一个驱动器，并期望所有有状态服务将它们的状态持久化到这个驱动器上。理论上，我们可以为每个服务挂载一个新的驱动器，但这样的要求很快就会成为负担。例如，如果我们使用
    Terraform 来管理基础设施，每当有新服务时，我们就需要更新它。你还记得十二因子应用的第一个原则吗？每个服务应该有一个代码库。服务所需的所有内容都应该放在一个单一的仓库中。因此，Terraform
    或任何其他基础设施配置工具不应包含任何与服务相关的细节。
- en: 'The solution to the problem is to manage volumes using similar principles as
    those we''re using to manage services. Just as we adopted schedulers (example:
    Swarm) that are managing our services, we should adopt volume schedulers that
    should handle our mounts.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的解决方案是使用与管理服务类似的原则来管理数据卷。正如我们采用了调度器（例如：Swarm）来管理我们的服务，我们也应该采用卷调度器来管理我们的挂载点。
- en: Since we adopted Docker containers as a way to run our services, a volume scheduler
    should be able to integrate with it and provide a seamless experience. In other
    words, managing volumes should be an integral part of managing services. Docker
    volume plugins allow us precisely that. Their purpose is to integrate third party
    solutions into the Docker ecosystem and make volume management transparent.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们采用了 Docker 容器作为运行服务的方式，卷调度程序应该能够与它集成并提供无缝体验。换句话说，管理卷应该是管理服务的一个重要部分。Docker
    卷插件正好允许我们做到这一点。它们的目的是将第三方解决方案集成到 Docker 生态系统中，使卷管理变得透明。
- en: REX-Ray is one of the solutions we explored. There are many others, and I'll
    leave it to you to compare them and make your decision which volume scheduler
    works the best in your use case.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: REX-Ray 是我们探索过的解决方案之一。还有许多其他解决方案，我将留给你去比较它们，并决定哪种卷调度器在你的使用场景中效果最好。
- en: If we are presented only with the choices we explored in this chapter, REX-Ray
    is a clear winner. It allows us to persist data across the cluster in a transparent
    way. The only extra requirement is to make sure that REX-Ray is installed. After
    that, we mount volumes with its driver as if they are regular host volumes. Behind
    the scenes, REX-Ray does the heavy lifting. It creates a network drive, mounts
    it, and manages it.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅仅在本章中探索的选项中选择，REX-Ray 无疑是一个明确的赢家。它允许我们以透明的方式在集群中持久化数据。唯一的额外要求是确保安装了 REX-Ray。之后，我们像挂载常规主机卷一样挂载带有其驱动程序的卷。在背后，REX-Ray
    负责繁重的工作，它会创建一个网络驱动器，挂载它并进行管理。
- en: Long story short, we'll use REX-Rey for all our stateful services. That is not
    entirely accurate so let me rephrase. We'll use REX-Ray for all our stateful services
    that do not use replication and synchronization between instances. If you are
    wondering what that means, all I can say is that patience is a virtue. We’ll get
    there soon.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说，我们将使用 REX-Ray 来管理所有有状态的服务。这个说法并不完全准确，让我重新表述一下。我们将使用 REX-Ray 来管理所有不使用实例间复制和同步的有状态服务。如果你在想这是什么意思，我只能说，耐心是一种美德。我们很快就会讲解到。
- en: Now that we decided that REX-Ray will be part of our stack, it is worthwhile
    adding it to our Packer and Terraform configs.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们决定将 REX-Ray 作为我们技术栈的一部分，值得将其添加到我们的 Packer 和 Terraform 配置中。
- en: Adding REX-Ray to packer and terraform
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 REX-Ray 添加到 Packer 和 Terraform
- en: We already went through REX-Ray manual setup so it should be relatively easy
    to add it to Packer and Terraform configurations. We'll add to Packer the static
    parts that do not depend on runtime resources and the rest to Terraform. What
    that means is that Packer will create AMIs with REX-Ray installed and Terraform
    will create its configuration and start the service.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了 REX-Ray 手动设置，因此将其添加到 Packer 和 Terraform 配置中应该相对容易。我们将把那些不依赖于运行时资源的静态部分添加到
    Packer 中，其余的则添加到 Terraform 中。这意味着 Packer 将创建带有 REX-Ray 安装的 AMI，而 Terraform 将创建其配置并启动服务。
- en: 'Let''s take a look at the `terraform/aws-full/packer-ubuntu-docker-rexray.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-rexray.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-rexray.json))
    file:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们来看看 `terraform/aws-full/packer-ubuntu-docker-rexray.json` 文件：([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-rexray.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-rexray.json)) '
- en: '[PRE59]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The only difference when compared with the `terraform/aws-full/packer-ubuntu-docker.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker.json))
    config we used before is an additional command in the shell `provisioner`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前使用的 `terraform/aws-full/packer-ubuntu-docker.json` 配置文件（[https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker.json)）相比，唯一的区别是在
    shell `provisioner` 中多了一个命令：
- en: '[PRE60]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: When creating a VM which will later become an AMI, Packer will execute the same
    command that we ran when we installed REX-Ray manually.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建一个 VM，之后它将成为 AMI 时，Packer 将执行我们安装 REX-Ray 时所运行的相同命令。
- en: 'Let''s build the AMI:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来构建 AMI：
- en: '[PRE61]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We built the AMI and stored the ID in the environment variable `TF_VAR_swarm_ami_id.`
    It'll be used by Terraform soon.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了 AMI，并将 ID 存储在环境变量 `TF_VAR_swarm_ami_id` 中。它将很快被 Terraform 使用。
- en: Defining the Terraform part of the REX-Ray setup is a bit more complex since
    its configuration needs to be dynamic and decided at runtime.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 REX-Ray 设置的 Terraform 部分稍微复杂一些，因为它的配置需要动态生成，并在运行时决定。
- en: 'The configuration is defined in the `terraform/aws-full/rexray.tpl` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl))
    template:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 配置定义在`terraform/aws-full/rexray.tpl` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl))
    模板中：
- en: '[PRE62]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE63]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As you can see, the AWS keys, the region, and the security groups are defined
    as variables. The magic happens in the `terraform/aws-full/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/common.tf))
    file:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，AWS密钥、区域和安全组被定义为变量。魔法发生在`terraform/aws-full/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/common.tf))
    文件中：
- en: '[PRE64]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The relevant part of the output is the `template_file` datasource. It is as
    follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分是`template_file`数据源，内容如下：
- en: '[PRE65]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Contents of the template are in the `rexray.tpl` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl))
    file we explored earlier. The variables from the template are defined in the vars
    section. The value of the last variable `aws_security_group` will be decided at
    runtime once the `aws_security_group` called docker is created.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 模板的内容在之前探索过的`rexray.tpl` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl))
    文件中。模板中的变量在vars部分定义。最后一个变量`aws_security_group`的值将在运行时决定，一旦名为docker的`aws_security_group`被创建。
- en: 'Finally, the last piece of the puzzle is in the `terraform/aws-full/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf))
    file:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，拼图的最后一块在`terraform/aws-full/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf))
    文件中：
- en: '[PRE66]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Both `swarm-manager` and `swarm-worker` AWS instances have two extra lines
    in the `remote-exec provisioners`. They are as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`swarm-manager`和`swarm-worker`的AWS实例在`remote-exec provisioners`中有两行额外的配置。它们如下：'
- en: '[PRE67]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Commands are inside an `if` statement. That allows us to decide at runtime whether
    REX-Ray should be configured and started or not. Normally, you would not need
    the if statement. You'll either choose to use REX-Ray, or not. However, at the
    beginning of this chapter we needed a cluster with REX-Ray and I did not want
    to maintain two almost identical copies of the configuration (one with, and the
    other without REX-Ray).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 命令位于`if`语句内部。这样我们可以在运行时决定是否配置并启动REX-Ray。通常，你不需要`if`语句。你要么选择使用REX-Ray，要么不使用。然而，在本章开头，我们需要一个包含REX-Ray的集群，我不想维护两个几乎相同的配置副本（一个包含REX-Ray，另一个不包含REX-Ray）。
- en: The important part is inside the if statement. The first line puts the content
    of the template into the `/etc/rexray/config.yml` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf))
    file. The second starts the service.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 关键部分在`if`语句内部。第一行将模板内容放入`/etc/rexray/config.yml` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf))
    文件中。第二行启动服务。
- en: 'Now that it is evident how we defined REX-Ray inside Terraform configs, it
    is time to create a cluster with it automatically:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经显而易见，我们是如何在Terraform配置中定义REX-Ray的，接下来是自动创建包含REX-Ray的集群：
- en: '[PRE68]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The first node that initializes the Swarm cluster was created and we can proceed
    by adding two more manager nodes:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化Swarm集群的第一个节点已创建，我们可以继续添加另外两个管理节点：
- en: '[PRE69]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We retrieved the token and the IP of the first manager and used that info to
    create the rest of the nodes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取了令牌和第一个管理节点的IP，并使用这些信息创建了其余的节点。
- en: 'Let''s enter one of the servers and confirm that REX-Ray is installed:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入其中一台服务器，确认REX-Ray已安装：
- en: '[PRE70]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output of the `rexray version` command is as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`rexray version`命令的输出如下：'
- en: '[PRE71]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Both REX-Ray and libStorage are installed. Finally, before we check whether
    it is working, let's have a quick look at the config.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: REX-Ray和libStorage都已安装。最后，在检查是否正常工作之前，让我们快速看一下配置。
- en: '[PRE72]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output is as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE73]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: I obscured my AWS account details for obvious reasons. Nevertheless, the config
    looks *OK* and we can give REX-Ray a spin.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 出于显而易见的原因，我已经隐藏了我的AWS账户详情。尽管如此，配置看起来*没问题*，我们可以试试REX-Ray。
- en: 'We’ll deploy the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    stack and create the `jenkins` service in the same way we did while we were experimenting
    with REX-Ray installed manually:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署`vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))堆栈，并按照我们在手动安装REX-Ray时所做的方式创建`jenkins`服务：
- en: '[PRE74]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'It should take only a few moments until the script is finished. Now we can
    check the Docker volumes:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本完成应该只需要几分钟。现在我们可以检查Docker卷：
- en: '[PRE75]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output is as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE76]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'As expected, the `jenkins` service created the `rexray` volume with the same
    name. We should wait until Jenkins is running and open it in a browser:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`jenkins`服务创建了一个同名的`rexray`卷。我们应该等到Jenkins运行起来，然后在浏览器中打开它：
- en: '[PRE77]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '**A note to Windows users**'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意**'
- en: 'Git Bash might not be able to use the open command. If that’s the case, execute
     `terraform output swarm_manager_1_public_ip` to find out the IP of the manager
    and open the URL directly in your browser of choice. For example, the command
    above should be replaced with the command that follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash可能无法使用open命令。如果是这样，请执行`terraform output swarm_manager_1_public_ip`以查找管理器的IP，并直接在你选择的浏览器中打开URL。例如，上面的命令应替换为以下命令：
- en: '`terraform output swarm_manager_1_public_ip`'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '`terraform output swarm_manager_1_public_ip`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4/jenkins` in
    your browser.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出为`1.2.3.4`，你应该在浏览器中打开`http://1.2.3.4/jenkins`。
- en: 'The only thing left is to recuperate the initial administrative password and
    use it to setup Jenkins:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是恢复初始的管理员密码，并用它来设置Jenkins：
- en: '[PRE78]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: I'll leave the rest to you. Finish the setup, destroy the container, wait until
    Swarm reschedules it, confirm that the state is preserved, and so on and so forth.
    Spend some time playing with it.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的交给你了。完成设置，销毁容器，等待Swarm重新调度它，确认状态是否得到保留，等等。花些时间与它玩耍。
- en: 'Unless you developed an emotional attachment with Jenkins and REX-Ray, please
    remove the service and the volume. We won''t need it anymore:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你对Jenkins和REX-Ray有感情依赖，否则请删除服务和卷。我们以后不再需要它：
- en: '[PRE79]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Prometheus, ElasticSearch, and Mongo are only a few examples of services that
    store the state. Should we add REX-Ray mounts for all of them? Not necessarily.
    Some stateful services already have a mechanism to preserve their state. Before
    we start attaching REX-Ray mount like there is no tomorrow, we should first check
    whether a service already has a data replication mechanism.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus、ElasticSearch和Mongo只是存储状态的几个服务例子。我们是否应该为它们所有服务都添加REX-Ray挂载？不一定。有些有状态的服务已经有了保存其状态的机制。在我们开始盲目添加REX-Ray挂载之前，我们应该先检查服务是否已有数据复制机制。
- en: Persisting stateful services without replication
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化无复制的有状态服务
- en: Jenkins is a good example of a stateful service that forces us to preserve its
    state. Moreover, it is incapable of sharing or synchronizing state between multiple
    instances. As a result, it cannot scale. There cannot be two Jenkins masters with
    the same or replicated state. Sure, you can create as many masters as you want
    but each will be an entirely separate service without any relation to other instances.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins是一个很好的有状态服务例子，迫使我们保存其状态。而且，它无法在多个实例之间共享或同步状态。因此，它不能水平扩展。不能有两个Jenkins主节点拥有相同或复制的状态。当然，你可以创建任意多个主节点，但每个都将是完全独立的服务，与其他实例没有任何关系。
- en: The most obvious negative side-effect of Jenkins inability to scale horizontally
    is performance. If a master is under heavy load, we cannot create a new instance
    and thus reduce the burden from the original.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins无法横向扩展的最明显负面副作用是性能。如果一个主节点负载过重，我们无法创建新实例来减轻原本的负担。
- en: 'There are only three types of services that can be scaled. They need to be
    stateless, stateful and capable of using shared state, or stateful and capable
    of synchronizing state. Jenkins is none of those and, therefore, it cannot be
    scaled horizontally. The only thing we can do to increase Jenkins capacity is
    to add more resources (for example: CPU and memory). Such an action would improve
    its performance but would not provide high-availability. When Jenkins fails, Swarm
    will re-schedule it. Still, there is a period between a failure and a new replica
    being fully operational.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 只有三种类型的服务能够进行扩展。它们需要是无状态的、有状态的并能够使用共享状态，或者是有状态的并能够同步状态。Jenkins 既不是这三者中的任何一种，因此无法进行水平扩展。我们能做的唯一事情就是增加
    Jenkins 的资源（例如：CPU 和内存）。这样的操作可以提高其性能，但无法提供高可用性。当 Jenkins 发生故障时，Swarm 会重新调度它。尽管如此，在故障发生和新的副本完全恢复之间，仍然会有一段时间的间隔。
- en: During that time, Jenkins would not be functional. Without the ability to scale,
    there is no high-availability.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段时间里，Jenkins 将无法正常工作。如果没有扩展能力，就没有高可用性。
- en: A big part of Jenkins workload is performed by its agents, so many organizations
    will not need to deal with the fact that it is not scalable.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins 工作负载的大部分是由其代理执行的，因此许多组织无需处理其无法扩展的问题。
- en: The reason for this transgression into Jenkins statefulness is to demonstrate
    one of the ways stateful services can be designed. When running stateful services
    that do not have a synchronization mechanism, there is no better option we can
    employ than to mount a volume from an external drive. That does not mean that
    mounting a volume is the only option we can use to deploy stateful services, but
    that is a preferable way to treat those that cannot share or synchronize their
    state across multiple replicas.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Jenkins 转变为有状态服务的原因，是为了展示有状态服务的设计方法之一。当运行没有同步机制的有状态服务时，我们能采用的最好的方法就是挂载一个来自外部驱动器的卷。这并不意味着挂载卷是我们唯一可以用来部署有状态服务的方式，但它是处理那些无法在多个副本之间共享或同步状态的服务的优选方法。
- en: Let's explore stateful services that do implement state synchronization.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索那些实现了状态同步的有状态服务。
- en: Persisting stateful services with synchronization and replication
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化有状态服务的同步和复制
- en: When creating stateful services, the natural reaction is to think of a way to
    preserve their state. While in many cases that is the correct thing to do, in
    some others it isn't. It depends on service’s architecture.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建有状态服务时，自然的反应是想办法保留它们的状态。虽然在许多情况下这是正确的做法，但在某些情况下却不是。这取决于服务的架构。
- en: Throughout this book, we explored at least two stateful services that can synchronize
    their state across all instances. Those are *Docker Flow Proxy* and *MongoDB*.
    In a nutshell, the ability to synchronize state means that when data inside one
    instance changes, it is capable of propagating that change to all other instances.
    The biggest problem with that process is how to guarantee that everyone has the
    same data without sacrificing availability. We'll leave that discussion for some
    other time and place. Instead, let us go through the `docker-flow-proxy` and `mongo`
    services and decide which changes (if any) we need to apply to accomplish high
    availability and performance. We'll use them as examples how to treat stateful
    services capable of data replication and synchronization.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们探讨了至少两种可以同步其状态的有状态服务。它们是 *Docker Flow Proxy* 和 *MongoDB*。简而言之，同步状态的能力意味着当一个实例中的数据发生变化时，它能够将变化传播到所有其他实例。这个过程中的最大问题是如何确保每个人都拥有相同的数据，而不牺牲可用性。我们会将这个讨论留到其他时间和地点。相反，我们将深入了解
    `docker-flow-proxy` 和 `mongo` 服务，决定需要进行哪些更改（如果有的话）来实现高可用性和性能。我们将以它们为例，展示如何处理能够进行数据复制和同步的有状态服务。
- en: Not everyone uses Mongo for storing data, nor everyone thinks that *Docker Flow
    Proxy* is the best choice for routing requests. The chances are that your choice
    of a database and the proxy is different. Even in that case, I strongly suggest
    you read the text that follows since it uses those two services only as examples
    of how you could design your replication and how to set up third-party stateful
    service that already has it incorporated. Most DBs use the same principles for
    replication and synchronization, and you should have no problem taking MongoDB
    examples as a blueprint for creating your database services.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是每个人都使用 Mongo 来存储数据，也不是每个人都认为 *Docker Flow Proxy* 是路由请求的最佳选择。你的数据库和代理的选择很可能不同。即使如此，我仍然强烈建议你阅读以下内容，因为它仅使用这两项服务作为如何设计复制机制的示例，以及如何设置已经内置了复制机制的第三方有状态服务。大多数数据库使用相同的复制和同步原理，你应该不会有问题将
    MongoDB 的示例作为创建数据库服务的蓝图。
- en: Persisting docker flow proxy state
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化 Docker Flow Proxy 状态
- en: '*Docker Flow Proxy* is a stateful service. However, that did not prevent us
    from scaling it. Its architecture is made in a way that, even though it is stateful,
    all instances have the same data. The mechanism to accomplish that has quite a
    few names. I prefer calling it state replication and synchronization.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '*Docker Flow Proxy* 是一个有状态服务。然而，这并没有阻止我们对其进行扩展。它的架构设计使得，即使它是有状态的，所有实例的数据也是相同的。实现这一点的机制有很多种叫法。我更喜欢称之为状态复制和同步。'
- en: When one of the instances receives a new instruction that changes its state,
    it should find all the other replicas and propagate the change.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 当其中一个实例接收到一个改变其状态的新指令时，它应该找到所有其他副本并传播这一变化。
- en: 'The replication flow is usually as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 复制流程通常如下所示：
- en: An instance receives a request that changes its state.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个实例接收到一个改变其状态的请求。
- en: It finds the addresses of all the other instances of the same service.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它找到同一服务的所有其他实例的地址。
- en: It re-sends the request to all other instances of the same service.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它重新将请求发送给同一服务的所有其他实例。
- en: The ability to propagate changes is not enough. When a new instance is created,
    a stateful service with data replication needs to be capable of requesting a complete
    state from one of the other instances. The first action it needs to perform when
    initialized is to reach the same state as other replicas. That can be accomplished
    through a pull mechanism. While propagation of a change of state of one instance
    often entails a push to all other instances, initialization of a new instance
    is often followed by a data pull.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 传播变化的能力并不足够。当一个新实例被创建时，一个带有数据复制的有状态服务需要能够从其他实例请求完整的状态。它在初始化时需要执行的第一项操作是与其他副本达到相同的状态。这可以通过拉取机制来实现。虽然一个实例的状态变化的传播通常涉及到推送到所有其他实例，但新实例的初始化通常伴随着数据拉取。
- en: 'The synchronization flow is often as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 同步流程通常如下所示：
- en: A new instance of a service is created.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的服务实例。
- en: It finds the address of one of the other instances of the same service.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它找到同一服务的其他实例的地址。
- en: It pulls data from the other instance of the same service.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从同一服务的另一个实例中拉取数据。
- en: You already saw *Docker Flow Proxy* in action quite a few times. We scaled it,
    and we simulated a failure which resulted in re-scheduling. In both cases, all
    replicas always had the same state or, to be more precise, the same configuration.
    You saw it before, so there's no need to go into another round of a practical
    demonstration of proxy's capabilities.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经多次看到过 *Docker Flow Proxy* 的实际操作。我们对其进行了扩展，并模拟了故障，这导致了重新调度。在这两种情况下，所有副本始终保持相同的状态，或者更准确地说，保持相同的配置。你之前已经看过了，所以没有必要再进行一次代理功能的实际演示。
- en: Understanding how replication and synchronization works does not mean that we
    should write our services as stateful and employ those mechanisms ourselves. Quite
    the contrary. When appropriate, design your services to be stateless and store
    their state in a database. Otherwise, you might quickly run into problems and
    realize that you'll have to reinvent the wheel. For example, you might be faced
    with consensus problems that are already solved in protocols like Raft and Paxos.
    You might need to implement a variation of a Gossip protocol. And so on, and so
    forth. Concentrate on what brings value to your project and use proven solutions
    for everything else.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 理解复制和同步是如何工作的，并不意味着我们应该将服务编写为有状态的并亲自实现这些机制。恰恰相反。在合适的情况下，设计你的服务为无状态，并将其状态存储在数据库中。否则，你可能很快就会遇到问题，并意识到你不得不重新发明轮子。例如，你可能会遇到已经在
    Raft 和 Paxos 协议中解决的共识问题。你可能需要实现某种变体的 Gossip 协议。诸如此类。集中精力在那些能为你的项目带来价值的地方，其它问题则使用经过验证的解决方案。
- en: Recommending the usage of an external database instead of storing the state
    inside our services might sound conflicting knowing that *Docker Flow Proxy* did
    the opposite. It is a stateful application without any external data store (at
    least when running in Swarm mode). The reason is simple. The proxy was not written
    from scratch. It uses HAProxy in the background which, in turn, does not have
    the ability to store its configs (state) externally. If I were to write a proxy
    from scratch, it would save its state externally. I might do that one day. Until
    then, HAProxy is stateful and so is *Docker Flow Proxy*. From a user's perspective,
    that should not be an issue since it employs data replication and synchronization
    between all instances. The problem is for developers working on the project.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐使用外部数据库而不是将状态存储在我们服务中的做法，可能听起来与 *Docker Flow Proxy* 的做法相冲突。后者是一个有状态的应用，没有任何外部数据存储（至少在
    Swarm 模式下是这样的）。原因很简单。这个代理并不是从零开始编写的。它在后台使用了 HAProxy，而 HAProxy 本身没有能力将其配置（状态）存储到外部。如果我要从零开始编写一个代理，它会将状态保存到外部。或许有一天我会这么做。在此之前，HAProxy
    是有状态的，*Docker Flow Proxy* 也是有状态的。从用户的角度来看，这不应该是问题，因为它通过数据复制和所有实例之间的同步来确保一致性。问题出在开发者身上，特别是正在参与该项目的开发者。
- en: Let's take a look at another example of a stateful service with data replication.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个具有数据复制功能的有状态服务的例子。
- en: Persisting MongoDB state
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化 MongoDB 状态
- en: We used `go-demo` service throughout the book. It helped us understand better
    how Swarm works. Among other things, we scaled the service quite a few times.
    That was easy to do since it is stateless. We can create as many replicas as we
    want without having to worry about data. It is stored somewhere else.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整本书中都使用了 `go-demo` 服务。这帮助我们更好地理解了 Swarm 的工作原理。除了其他内容，我们还多次扩展了该服务。因为它是无状态的，所以扩展非常容易。我们可以创建任意多的副本，而不必担心数据的存储。数据存储在其他地方。
- en: The `go-demo` service externalizes its state to MongoDB. If you paid attention,
    we never scaled the database. The reason is simple. MongoDB cannot be scaled with
    a simple `docker service scale` command.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo` 服务将其状态外部化到 MongoDB。如果你留意过，你会发现我们从未扩展过数据库。原因很简单，MongoDB 不能通过简单的 `docker
    service scale` 命令进行扩展。'
- en: Unlike *Docker Flow Proxy* that was designed from the ground up to leverage
    Swarm's networking to find other instances before replicating data, MongoDB is
    network agnostic. It cannot auto-discover its replicas. To make things more complicated,
    only one instance can be primary meaning that only one instance can receive write
    requests. All that means that we cannot scale Mongo using Swarm. We need a different
    approach. Let's try to set up three MongoDBs with data replication by creating
    a replica set. We'll start with a manual process that will provide us with an
    understanding of the problems we might face and solutions we might employ. Later
    on, once we reach a satisfactory outcome, we'll try to automate the process.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 与 *Docker Flow Proxy* 从头开始设计，利用 Swarm 网络查找其他实例再进行数据复制不同，MongoDB 是网络无关的。它不能自动发现其副本。更复杂的是，只有一个实例可以是主节点，意味着只有一个实例可以接收写请求。这一切意味着我们不能通过
    Swarm 扩展 MongoDB。我们需要一种不同的方法。让我们尝试通过创建副本集来设置三个 MongoDB 实例并实现数据复制。我们将从手动过程开始，这将帮助我们理解可能面临的问题以及可以采取的解决方案。稍后，当我们得到令人满意的结果时，我们会尝试自动化这个过程。
- en: 'We''ll start by entering one of the manager nodes:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从进入其中一个管理节点开始：
- en: '[PRE80]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'All members of a Mongo replica set need to be able to communicate with each
    other, so we''ll create the same old `go-demo` network:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo 副本集中的所有成员都需要能够相互通信，因此我们将创建一个老旧的 `go-demo` 网络：
- en: '[PRE81]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: If we were to create a single service with three replicas, Swarm would create
    a single network endpoint for the service and load balance requests among all
    instances. The problem with that approach is in MongoDB configuration. It needs
    a fixed address of every DB that will belong to the replica set.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建一个包含三个副本的单一服务，Swarm 会为该服务创建一个网络端点，并在所有实例之间进行负载均衡。这样做的问题在于 MongoDB 配置。它需要每个属于副本集的数据库的固定地址。
- en: 'Instead of creating three replicas of a service, we''ll create three services:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会创建一个包含三个副本的服务，而是创建三个服务：
- en: '[PRE82]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The command we executed created services `go-demo-db-rs1`, `go-demo-db-rs2`,
    and `go-demo-db-rs3`. They all belong to the `go-demo` network so that they can
    communicate with each other freely. The command specified for all services `mongod
    --replSet "rs0"`, making them all belong to the same Mongo replica set called
    `rs0.` Please don't confuse Swarm replicas with Mongo replica sets. While they
    have a similar objective, the logic behind them is quite different.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的命令创建了 `go-demo-db-rs1`、`go-demo-db-rs2` 和 `go-demo-db-rs3` 三个服务。它们都属于 `go-demo`
    网络，因此可以自由地相互通信。该命令为所有服务指定了 `mongod --replSet "rs0"`，使它们都属于名为 `rs0` 的 Mongo 副本集。请不要混淆
    Swarm 副本和 Mongo 副本集。虽然它们的目标类似，但背后的逻辑有很大不同。
- en: 'We should wait until all the services are running:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该等待所有服务启动完成：
- en: '[PRE83]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The relevant part of the output is as follows (IDs are removed for brevity):'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下（为了简洁，已省略 ID）：
- en: '[PRE84]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now we should configure Mongo''s replica set. We''ll do that by creating one
    more `mongo` service:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该配置 Mongo 的副本集。我们将通过创建另一个 `mongo` 服务来实现：
- en: '[PRE85]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: We made the service global so that we ensure it will run on the same node we're
    in. That makes the process easier than trying to figure out the IP of a node it
    runs in. It belongs to the same `go-demo` network so that it can access the other
    DB services.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将服务设置为全局模式，以确保它将在我们所在的同一节点上运行。这样比试图找出其运行节点的 IP 更加简便。它属于同一个 `go-demo` 网络，因此可以访问其他的数据库服务。
- en: We do not want to run Mongo server inside this service. The purpose of `go-demo-db-util`
    is to give us a Mongo client that we can use to connect to other DBs and configure
    them. Therefore, we replaced the default command `mongod` with a very long sleep.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望在这个服务中运行 Mongo 服务器。`go-demo-db-util` 的目的是提供一个 Mongo 客户端，我们可以使用它连接到其他数据库并进行配置。因此，我们将默认命令
    `mongod` 替换为一个非常长的睡眠命令。
- en: 'To enter into one of the containers of the `go-demo-db-util` service, we need
    to find its ID:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入 `go-demo-db-util` 服务的某个容器，我们需要找到其 ID：
- en: '[PRE86]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now that we have the ID of the `go-demo-db-util` replica running on the same
    server, we can enter inside the container:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了在同一服务器上运行的 `go-demo-db-util` 副本的 ID，可以进入容器内部：
- en: '[PRE87]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The next step is to execute a command that will initiate Mongo''s replica set:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是执行一个命令来启动 Mongo 的副本集：
- en: '[PRE88]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: We used the local `mongo` client to issue the command on the server running
    inside `go-demo-db-rs1`. It initiated the replica set with the ID `rs0` and specified
    that the three services we created previously should be its members. Thanks to
    Docker Swarm networking, we do not need to know the IPs. It was enough to specify
    the names of the services.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用本地的 `mongo` 客户端，在运行在 `go-demo-db-rs1` 内的服务器上执行命令。它以 `rs0` 为 ID 启动了副本集，并指定了我们之前创建的三个服务作为其成员。由于
    Docker Swarm 网络的存在，我们不需要知道 IP 地址，仅指定服务名称就足够了。
- en: 'The response is as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 响应如下：
- en: '[PRE89]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'We should not trust the acknowledgment alone. Let''s take a look at the config:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应仅仅依赖确认消息。让我们查看一下配置：
- en: '[PRE90]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We issued another command to the remote server running in `go-demo-db-rs1`.
    It retrieved the replica set configuration. Part of the output is as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向运行在 `go-demo-db-rs1` 的远程服务器发出了另一个命令。它检索了副本集的配置。输出的一部分如下：
- en: '[PRE91]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: We can see that the replica set has three members (two were removed for brevity).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到副本集中有三个成员（两个已被省略）。
- en: 'Let''s send one more command to the remote Mongo running in `go-demo-db-rs1`.
    This time, we''ll check the status of the replica set:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再向远程运行在 `go-demo-db-rs1` 上的 Mongo 发送一个命令。这一次，我们将检查副本集的状态：
- en: '[PRE92]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Part of the output is as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '[PRE93]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Info about two replicas is removed for brevity.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 由于简洁性，已省略两个副本的信息。
- en: We can see that all the Mongo replicas are running. The `go-demo-db-rs1` service
    is acting as the primary, while the other two are secondary nodes.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有 Mongo 副本都在运行。`go-demo-db-rs1` 服务充当主节点，而另外两个是从节点。
- en: Setting up a Mongo replica set means that data will be replicated to all its
    members. One is always a primary, while the rest are secondary servers. With the
    current configuration, we can read and write data only to the primary server.
    Replica set can be configured to allow read access to all the servers. Writing
    is always restricted to the primary.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Mongo 副本集意味着数据将被复制到所有成员中。一个始终是主节点，其余的是从节点。在当前配置下，我们只能对主节点进行读写操作。副本集可以配置为允许对所有服务器进行读取访问，但写入始终限制在主节点上。
- en: 'Let us generate some sample data:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些示例数据：
- en: '[PRE94]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: We entered into the remote Mongo running on `go-demo-db-rs1`.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入了运行在 `go-demo-db-rs1` 上的远程 Mongo。
- en: 'The output is as follows:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE95]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: As you can see from the prompt, we are inside the primary database server.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如从提示中所见，我们处于主数据库服务器内。
- en: 'We''ll create a few records in the database test:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在数据库测试中创建一些记录：
- en: '[PRE96]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The previous command retrieved all the records from the database test.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个命令从数据库测试中检索了所有记录。
- en: 'The output is as follows:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE97]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Now that we have the replica set configured and a few sample records, we can
    simulate failure of one of the servers and observe the result:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了副本集并且有了一些样本记录，我们可以模拟其中一台服务器的故障并观察结果：
- en: '[PRE98]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: We exited MongoDB and the `go-demo-db-util` service replica. Then we found the
    IP of `go-demo-db-rs1` (primary member of the Mongo replica set) and listed all
    the containers running on the server.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们退出了 MongoDB 和 `go-demo-db-util` 服务副本。然后，我们找到了 `go-demo-db-rs1`（Mongo 副本集的主成员）的
    IP，并列出了服务器上运行的所有容器。
- en: 'The output is as follows (IDs and STATUS columns are removed for brevity):'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已删除 ID 和 STATUS 列）：
- en: '[PRE99]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Now we can find the ID of the `go-demo-db-rs1` service replica and simulate
    failure by removing it:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以找到 `go-demo-db-rs1` 服务副本的 ID，并通过移除它来模拟故障：
- en: '[PRE100]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Let''s take a look at the `go-demo-db-rs1` tasks:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下 `go-demo-db-rs1` 的任务：
- en: '[PRE101]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Swarm discovered that one of the replicas failed and re-scheduled it. A new
    instance will be running a few moments later.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 发现其中一个副本失败并重新调度了它。稍后会有一个新的实例运行。
- en: 'The output of the `service ps` command is as follows (IDs are removed for brevity):'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps` 命令的输出如下（为了简洁，已删除 ID）：'
- en: '[PRE102]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We''ll enter the `go-demo-db-util` service replica one more time and output
    the status of the Mongo replica set:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次进入 `go-demo-db-util` 服务副本，并输出 Mongo 副本集的状态：
- en: '[PRE103]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The relevant part of the output is as follows:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE104]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We can see that the `go-demo-db-rs2` become the primary Mongo replica. A simplified
    flow of what happened is as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `go-demo-db-rs2` 成为了主 Mongo 副本。发生的简化流程如下：
- en: Mongo replica `go-demo-db-rs1` failed
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mongo 副本 `go-demo-db-rs1` 失败
- en: The remaining members noticed its absence and promoted `go-demo-db-rs2` to the
    PRIMARY status
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的成员注意到它的缺失，并将 `go-demo-db-rs2` 提升为主节点状态。
- en: In the meantime, Swarm rescheduled the failed service replica
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与此同时，Swarm 重新调度了失败的服务副本。
- en: The primary Mongo replica noticed that the `go-demo-db-rs1` server came back
    online and joined the Mongo replica set as secondary
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主 Mongo 副本注意到 `go-demo-db-rs1` 服务器恢复在线并加入 Mongo 副本集作为从节点。
- en: The newly created `go-demo-db-rs1` synchronized its data from one of the other
    members of the Mongo replica set
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新创建的 `go-demo-db-rs1` 从 Mongo 副本集的其他成员同步了其数据。
- en: One of the key elements for all this to work is Docker networking. When the
    rescheduled service replica came back online, it maintained the same address `go-demo-db-rs1`,
    and we did not need to change the configuration of the Mongo replica set.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切能够正常工作的关键元素之一是 Docker 网络。当重新调度的服务副本重新上线时，它保持了相同的地址 `go-demo-db-rs1`，因此我们无需更改
    Mongo 副本集的配置。
- en: If we used VMs and, in the case of AWS, *Auto Scaling Groups* to host Mongo,
    when a node fails, a new one would be created in its place. However, the new node
    would receive a new IP and would not be able to join the Mongo replica set without
    modifications to the configuration. The are ways we could accomplish the same
    in AWS without containers, but none would be so simple and elegant as with Docker
    Swarm and networking.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用虚拟机，并且在 AWS 中使用 *Auto Scaling Groups* 托管 Mongo，当一个节点失败时，系统会创建一个新的节点来替代它。然而，新节点将会分配到一个新的
    IP，并且在没有配置修改的情况下无法加入 Mongo 副本集。我们也可以在没有容器的情况下在 AWS 中实现相同的功能，但没有任何方法能像 Docker Swarm
    和网络配置那样简单和优雅。
- en: What happened to the sample data we created? Remember, we wrote data to the
    primary Mongo replica `go-demo-db-rs1` and, later on, removed it. We did not use
    REX-Ray or any other solution to persist data.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的示例数据发生了什么？记住，我们将数据写入了主 Mongo 副本`go-demo-db-rs1`，然后将其删除。我们没有使用 REX-Ray 或任何其他解决方案来持久化数据。
- en: 'Let’s enter the new primary Mongo replica:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入新的主 Mongo 副本：
- en: '[PRE105]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: In your cluster, the new primary might be `go-demo-db-rs3`. If that's the case,
    please change the above command.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的集群中，新的主节点可能是`go-demo-db-rs3`。如果是这种情况，请更改上面的命令。
- en: 'Next, we''ll specify that we want to use the test database and retrieve all
    data:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将指定要使用测试数据库并检索所有数据：
- en: '[PRE106]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'The output is as follows:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE107]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Even though we did not setup up data persistence, all data is there.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们没有设置数据持久化，所有数据仍然存在。
- en: The main purpose of Mongo replica sets is to provide fault tolerance. If a DB
    fails, other members will take over. Any change to data (state) is replicated
    among all members of a replica set.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo 副本集的主要目的是提供容错性。如果一个数据库失败，其他成员会接管。任何数据（状态）的变更会在副本集的所有成员之间复制。
- en: Does that mean that we do not need to preserve the state to an external drive?
    That depends on the use case. If data we are operating with is massive, we might
    employ some form of disk persistence to speed up the synchronization process.
    In any other circumstances, using volumes is a waste since most databases are
    designed to provide data replication and synchronization.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 那是不是意味着我们不需要将状态保存在外部驱动器上？这取决于使用场景。如果我们操作的数据量非常庞大，可能会采用某种磁盘持久化方式来加快同步过程。在其他情况下，使用卷是浪费，因为大多数数据库都设计了数据复制和同步机制。
- en: The current solution worked well, and we should seek a way to set it up in a
    more automated (and simpler) way.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的解决方案运行良好，我们应该寻找一种更自动化（且更简单）的方法来进行配置。
- en: 'We''ll exit the MongoDB and the `go-demo-db-util` service replica, remove all
    the DB services, and start over:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将退出 MongoDB 和`go-demo-db-util`服务副本，移除所有 DB 服务并重新开始：
- en: '[PRE108]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Initializing MongoDB replica set through a swarm service
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 Swarm 服务初始化 MongoDB 副本集
- en: Let's try to define a better and easier way to set up a MongoDB replica set.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试定义一种更好且更简单的方式来设置 MongoDB 副本集。
- en: 'We''ll start by creating three `mongo` services. Later on, each will become
    a member of a Mongo replica set:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建三个`mongo`服务开始。稍后，每个服务都会成为 Mongo 副本集的成员：
- en: '[PRE109]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: The only difference, when compared with the previous command we used to create
    `mongo` services, is the addition of the environment variable `MEMBERS.` It holds
    service names of all MongoDBs. We'll use that as the argument for the next service.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前用于创建`mongo`服务的命令唯一的不同之处在于增加了环境变量`MEMBERS`。它包含所有 MongoDB 的服务名称。我们将使用它作为下一个服务的参数。
- en: Since the official `mongo` image does not have a mechanism to configure Mongo
    replica sets, we'll use a custom one. Its purpose will be only to configure Mongo
    replica sets.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 由于官方的`mongo`镜像没有配置 Mongo 副本集的机制，我们将使用一个自定义的镜像。它的唯一目的是配置 Mongo 副本集。
- en: 'The definition of the image is in the `conf/Dockerfile.mongo` ([https://github.com/vfarcic/cloud-provisioning/blob/master/conf/Dockerfile.mongo](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/Dockerfile.mongo))
    file. Its content is as follows:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像的定义在`conf/Dockerfile.mongo`文件中（[https://github.com/vfarcic/cloud-provisioning/blob/master/conf/Dockerfile.mongo](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/Dockerfile.mongo)）。其内容如下：
- en: '[PRE110]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '`Dockerfile.mongo` extends the official `mongo` image, adds a custom `init-mongo-rs.sh`
    script, gives it execute permissions, and sets it as the entry point.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dockerfile.mongo`扩展了官方的`mongo`镜像，添加了一个自定义的`init-mongo-rs.sh`脚本，赋予其执行权限，并将其设置为入口点。'
- en: '`ENTRYPOINT` defines the executable that will run whenever a container is run.
    Any command arguments we specify will be appended to it.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '`ENTRYPOINT`定义了每当启动容器时运行的可执行文件。我们指定的任何命令参数都会附加到它后面。'
- en: 'The `conf/init-mongo-rs.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/conf/init-mongo-rs.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/init-mongo-rs.sh))
    script is as follows:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`conf/init-mongo-rs.sh`脚本（[https://github.com/vfarcic/cloud-provisioning/blob/master/conf/init-mongo-rs.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/init-mongo-rs.sh)）的内容如下：'
- en: '[PRE111]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: The first section loops over all `DB` addresses (defined as script arguments)
    and checks whether they are available. If they're not, it waits for three seconds
    before repeating the loop.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分遍历所有`DB`地址（定义为脚本参数），检查它们是否可用。如果不可用，它会等待三秒钟，然后重新执行循环。
- en: The second section formats a JSON string that defines the list of all members
    (id and host). Finally, we initiate the replica set, wait for three seconds, and
    output its status.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分格式化了一个JSON字符串，定义了所有成员（ID和主机）的列表。最后，我们初始化副本集，等待三秒钟，并输出其状态。
- en: 'This script is a slightly more elaborate version of the commands we executed
    previously when we set up the replica set manually. Instead of hard-coding values
    (for example: `service names`), it is written in a way that it can be reused for
    multiple Mongo replica sets with varying number of members.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本是我们之前手动设置副本集时执行的命令的稍微复杂版本。它不是硬编码的值（例如：`服务名称`），而是以可重用的方式编写，可以用于多个Mongo副本集，且成员数可以变化。
- en: 'All that''s left is to run the container as a Swarm service. I already built
    the image as `vfarcic/mongo-devops21` and pushed it to Docker Hub:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是将容器作为Swarm服务运行。我已经构建了镜像`vfarcic/mongo-devops21`并推送到Docker Hub：
- en: '[PRE112]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: When the script is finished, the container will stop. Typically, Swarm would
    interpret a stopped container as a failure and re-schedule it. That's not the
    behavior we need. We want this service to perform some tasks (configure replica
    set) and stop when finished. We accomplished that with the `--restart-condition
    none` argument. Otherwise, Swarm would enter into an endless loop of continuously
    re-scheduling a service replica that keeps failing a few moments later.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 当脚本完成后，容器将停止。通常，Swarm会将停止的容器视为故障并重新调度它。这不是我们需要的行为。我们希望该服务执行一些任务（配置副本集），完成后停止。我们通过`--restart-condition
    none`参数实现了这一点。否则，Swarm会进入一个无限循环，不断重新调度一个几秒钟后就会失败的服务副本。
- en: The command of the service is `$MEMBERS.` When appended to the `ENTRYPOINT`,
    the full command was `init-mongo-rs.sh` `go-demo-db-rs1 go-demo-db-rs2 go-demo-db-rs3`.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的命令是`$MEMBERS`。当它附加到`ENTRYPOINT`时，完整的命令是`init-mongo-rs.sh` `go-demo-db-rs1
    go-demo-db-rs2 go-demo-db-rs3`。
- en: 'Let''s confirm that all services (except `go-demo-db-init`) are running:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认所有服务（除了`go-demo-db-init`）都在运行：
- en: '[PRE113]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The output is as follows:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE114]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: The only service that is not running is `go-demo-db-init`. By this time, it
    finished executing and, since we used the `--restart-condition none` argument,
    Swarm did not re-schedule it.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一没有运行的服务是`go-demo-db-init`。到这时，它已经完成了执行，并且由于我们使用了`--restart-condition none`参数，Swarm没有重新调度它。
- en: 'We already developed a level of trust, and you probably believe me that the
    `go-demo-db-init` did its job correctly. Nevertheless, it doesn''t hurt to double-check
    it. Since the last command of the script output the replica set status, we can
    check its logs to see whether everything is configured correctly. That means we''ll
    need to go one more time into trouble of finding the IP and the ID of the container:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经建立了一定的信任，你可能相信`go-demo-db-init`完成了它的工作。尽管如此，仔细检查一遍并不会有坏处。由于脚本的最后一个命令输出了副本集的状态，我们可以查看其日志，看看一切是否配置正确。这意味着我们需要再一次麻烦地找到容器的IP和ID：
- en: '[PRE115]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The relevant parts of the output of the `logs` command are as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '`logs`命令的相关输出如下：'
- en: '[PRE116]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Mongo replica set is indeed configured with all three members. We have a working
    group of fault tolerant set of MongoDBs that provide high availability. We can
    use them with our `go-demo` (or any other) service:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo副本集确实已经配置了所有三个成员。我们拥有一个工作中的故障容错MongoDB集群，提供高可用性。我们可以与我们的`go-demo`（或任何其他）服务一起使用它们：
- en: '[PRE117]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: There is only one difference between this command and those we used in the previous
    chapters. If we continued using a single address of the primary MongoDB, we would
    not have high availability. When that DB fails, the service would not be able
    to serve requests. Even though Swarm would re-schedule it, the address of the
    primary would become different since the replica set would elect a new one.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令与我们在前几章使用的命令之间只有一个区别。如果我们继续使用主MongoDB的单一地址，我们将无法实现高可用性。当该数据库失败时，服务将无法处理请求。尽管Swarm会重新调度它，但由于副本集会选举一个新的主节点，主节点的地址会发生变化。
- en: This time we specified all three MongoDBs as the value of the environment variable
    `DB`. The code of the service will pass that string to the MongoDB driver. In
    turn, the driver will use those addresses to deduce which DB is primary and use
    it to send requests. All Mongo drivers have the same mechanism to specify members
    of a replica set.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们指定了所有三个MongoDB作为环境变量`DB`的值。服务的代码将这个字符串传递给MongoDB驱动程序。然后，驱动程序将使用这些地址来推断哪个数据库是主节点，并使用它来发送请求。所有Mongo驱动程序都有相同的机制来指定副本集的成员。
- en: 'Finally, let''s confirm that all three replicas of the `go-demo` service are
    indeed running. Remember, the service is coded in a way that it would fail if
    the connection to the database could not be established. If all service replicas
    are running, it is the proof that we set up everything correctly:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们确认`go-demo`服务的三个副本确实在运行。记住，服务是以这样一种方式编写的，如果无法建立与数据库的连接，它将会失败。如果所有服务副本都在运行，那么这就是我们一切设置正确的证明：
- en: '[PRE118]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'The output is as follows (IDs and ERROR columns are removed for brevity):'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已删除了ID和ERROR列）：
- en: '[PRE119]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: What now?
  id: totrans-500
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来怎么办？
- en: Not all stateful services should be treated in the same way. Some might need
    an external drive mounted, while others already have some kind of a replication
    and synchronization incorporated. In some cases, you might want to combine both
    mounting and replication, while in others replication itself is enough.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有的状态服务都应该以相同的方式对待。有些可能需要挂载外部驱动器，而其他的可能已经内建了某种形式的复制和同步。在某些情况下，你可能想将挂载和复制结合起来使用，而在另一些情况下，单纯的复制就足够了。
- en: Please keep in mind that there are many other combinations we did not explore.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们并没有探索所有的其他组合。
- en: The important thing is to understand how a service works and how it was designed
    to persist its state. In many cases, the logic of the solution is the same no
    matter whether we use containers or not. Containers often do not make things different,
    only easier.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解一个服务是如何工作的，以及它是如何设计来持久化其状态的。在许多情况下，无论我们是否使用容器，解决方案的逻辑都是相同的。容器通常并不会改变事情，只是让它们变得更简单。
- en: With the right approach, there is no reason why stateful services would not
    be cloud-friendly, fault tolerant, with high availability, scalable, and so on.
    The major question is whether you want to manage them yourself or you'd prefer
    leaving it to your cloud computing provider (if you use one). The important thing
    is that you got a glimpse how to manage stateful services yourself.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 采用正确的方法，状态服务完全可以是云友好、容错、高可用、可扩展等。关键问题是你是否希望自己管理它们，还是更倾向于将管理工作交给云计算提供商（如果你使用的是云计算）。重要的是你已经初步了解了如何自己管理状态服务。
- en: 'Let''s destroy the cluster before we move on:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们销毁集群：
- en: '[PRE120]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
