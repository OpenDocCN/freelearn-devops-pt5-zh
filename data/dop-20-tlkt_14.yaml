- en: Chapter 14. Clustering and Scaling Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 集群与服务扩展
- en: '|   | *Organizations which design systems ... are constrained to produce designs
    which are copies of the communication structures of these organizations.* |  
    |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *设计系统的组织……受到这些组织沟通结构的制约，必须生产出这些沟通结构的复制品。* |   |'
- en: '|   | --*M. Conway* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*M. Conway* |'
- en: Many will tell you that they have a *scalable system*. After all, scaling is
    easy. Buy a server, install WebLogic (or whichever other monster application server
    you're using) and deploy your applications. Then wait for a few weeks until you
    discover that everything is so fast that you can click a button, have some coffee,
    and, by the time you get back to your desk, the result will be waiting for you.
    What do you do? You scale. You buy few more servers, install your monster applications
    servers and deploy your monster applications on top of them. Which part of the
    system was the bottleneck? Nobody knows. Why did you duplicate everything? Because
    you must. And then some more time passes, and you continue scaling until you run
    out of money and, simultaneously, people working for you go crazy. Today we do
    not approach scaling like that. Today we understand that scaling is about many
    other things. It's about elasticity. It's about being able to quickly and easily
    scale and de-scale depending on variations in your traffic and growth of your
    business, and that, during that process, you should not go bankrupt. It's about
    the need of almost every company to scale their business without thinking that
    IT department is a liability. It's about getting rid of those monsters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人会告诉你他们有一个*可扩展的系统*。毕竟，扩展很简单。买台服务器，安装WebLogic（或者你使用的其他大型应用服务器），然后部署你的应用程序。接下来，等上几周，直到你发现一切都那么快，你可以点击一个按钮，去喝咖啡，当你回到桌前时，结果就等着你了。你怎么办？你扩展。你再买几台服务器，安装你的大型应用服务器，把你的大型应用程序部署到它们上面。系统的瓶颈在哪里？没人知道。为什么要复制所有的东西？因为你必须这么做。然后，时间继续过去，你不断扩展，直到钱用光，同时，员工也开始崩溃。今天，我们不再这样处理扩展问题。今天我们明白，扩展涉及到许多其他方面。它关乎弹性。它意味着能够根据流量变化和业务增长，快速而轻松地进行扩展和缩减，而且在这个过程中，你不应该破产。它关乎几乎每家公司都需要扩展其业务，而不是把IT部门当作负担。它关乎摆脱那些庞然大物。
- en: Scalability
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展性
- en: Let us, for a moment take a step back and discuss why we want to scale applications.
    The main reason is *high availability*. Why do we want high availability? We want
    it because we want our business to be available under any load. The bigger the
    load, the better (unless you are under DDoS). It means that our business is booming.
    With high availability our users are happy. We all want speed, and many of us
    simply leave the site if it takes too long to load. We want to avoid having outages
    because every minute our business is not operational can be translated into a
    money loss. What would you do if an online store is not available? Probably go
    to another. Maybe not the first time, maybe not the second, but, sooner or later,
    you would get fed up and switch it for another. We are used to everything being
    fast and responsive, and there are so many alternatives that we do not think twice
    before trying something else. And if that something else turns up to be better...
    One man's loss is another man's gain. Do we solve all our problems with scalability?
    Not even close. Many other factors decide the availability of our applications.
    However, scalability is an important part of it, and it happens to be the subject
    of this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时退后一步，讨论一下为什么我们要扩展应用程序。主要原因是*高可用性*。为什么我们需要高可用性？我们需要它，因为我们希望我们的业务在任何负载下都能保持可用。负载越大越好（除非你遭遇了DDoS攻击）。这意味着我们的业务正在蓬勃发展。有了高可用性，我们的用户会很满意。我们都希望速度快，很多人如果加载太慢，就直接离开网站。我们希望避免宕机，因为每一分钟我们的业务无法运作，都意味着金钱的损失。如果一个在线商店无法使用，你会怎么做？可能去别的商店。也许第一次不会，第二次也不一定，但迟早你会受够了，换到别的商店。我们已经习惯了所有东西都很快、很响应，并且有这么多的替代选项，我们在尝试其他选择时不会再犹豫。如果那个替代选项更好……一个人的损失是另一个人的收获。我们通过扩展解决了所有问题吗？远远不够。还有许多其他因素决定着我们应用程序的可用性。然而，扩展性是其中一个重要部分，而且它正是本章讨论的主题。
- en: What is scalability? It is a property of a system that indicates its ability
    to handle increased load in a graceful manner or its potential to be enlarged
    as demand increases. It is the ability to accept increased volume or traffic.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是扩展性？它是系统的一种特性，表示其在优雅地应对增加的负载时的能力，或者表示其随着需求增加而扩展的潜力。它是接受增加的流量或负载的能力。
- en: The truth is that the way we design our applications dictates the scaling options
    available. Applications will not scale well if they are not designed to scale.
    That is not to say that an application not designed for scaling cannot scale.
    Everything can scale, but not everything can scale well.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，应用程序的设计方式决定了可用的扩展选项。如果应用程序没有设计成可以扩展的，它就无法很好地扩展。这并不是说没有扩展设计的应用程序就无法扩展。一切都可以扩展，但不是所有东西都能扩展得很好。
- en: 'Commonly observed scenario is as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的情况如下：
- en: We start with a simple architecture, sometimes with load balancer sometimes
    without, setup a few application servers and one database. Everything is great,
    complexity is low, and we can develop new features very fast. The cost of operations
    is low, income is high (considering that we just started), and everyone is happy
    and motivated.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的架构开始，有时有负载均衡器，有时没有，设置几个应用服务器和一个数据库。一切都很顺利，复杂度低，我们可以很快地开发新功能。运营成本低，收入高（考虑到我们刚刚起步），每个人都很开心和有动力。
- en: Business is growing, and the traffic is increasing. Things are beginning to
    fail, and performance is dropping. Firewalls are added, additional load balancers
    are set up, the database is scaled, more application servers are added and so
    on. Things are still relatively simple. We are faced with new challenges, but
    obstacles can be overcome in time. Even though the complexity is increasing, we
    can still handle it with relative ease. In other words, what we're doing is still
    more or less the same but bigger. Business is doing well, but it is still relatively
    small.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 业务在增长，流量也在增加。事情开始出现故障，性能下降。于是添加了防火墙，设置了额外的负载均衡器，扩展了数据库，增加了更多的应用服务器，等等。事情仍然相对简单。我们面临着新的挑战，但障碍可以及时克服。尽管复杂度在增加，但我们仍能相对轻松地应对。换句话说，我们做的事情大致上还是一样，只是规模更大了。业务做得不错，但仍然相对较小。
- en: And then it happens. The big thing you've been waiting for. Maybe one of the
    marketing campaigns hit the spot. Maybe there was a negative change in your competition.
    Maybe that last feature was indeed a killer one. No matter the reasons, business
    got a big boost. After a short period of happiness due to this change, your pain
    increases tenfold. Adding more databases does not seem to be enough. Multiplying
    application servers does not appear to fulfill the needs. You start adding caching
    and what so not. You start getting the feeling that every time you multiply something,
    benefits are not equally big. Costs increase, and you are still not able to meet
    the demand. Database replications are too slow. New application servers do not
    make such a big difference anymore. Operational costs are increasing faster than
    you expected. The situation hurts the business and the team. You are starting
    to realize that the architecture you were so proud of cannot fulfill this increase
    in load. You can not split it. You cannot scale things that hurt the most. You
    cannot start over. All you can do is continue multiplying with ever decreasing
    benefits of such actions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它发生了。你一直期待的重大事件。也许是某个营销活动打中了要害，也许是竞争对手发生了不利变化，或者最后一个功能真的是个杀手级功能。不管原因是什么，业务获得了大幅提升。在短暂的欣喜之后，你的痛苦增加了十倍。增加更多的数据库似乎还不够，增加应用服务器也似乎无法满足需求。你开始增加缓存等等。你开始有这种感觉：每次增加某个组件时，带来的好处并没有成比例地增长。成本在增加，而你仍然无法满足需求。数据库复制太慢，新的应用服务器已经不再产生那么大的效果了。运营成本增长得比你预期的快。局势开始伤害到业务和团队。你开始意识到，你曾经引以为豪的架构无法应对这种负载的增加。你无法将它拆分。你无法扩展那些最痛苦的部分。你不能从头开始。你能做的就是不断地增加组件，但每次增加带来的好处都在减少。
- en: The situation described above is quite common. What was good at the beginning,
    is not necessarily right when the demand increases. We need to balance the need
    for **You ain't going to need it** (**YAGNI**) principle and the longer term vision.
    We cannot start with the system optimized for large companies because it is too
    expensive and does not provide enough benefits when business is small. On the
    other hand, we cannot lose the focus from one of the main objectives of any business.
    We cannot not think about scaling from the very first day. Designing scalable
    architecture does not mean that we need to start with a cluster of a hundred servers.
    It does not mean that we have to develop something big and complex from the start.
    It means that we should start small, but in the way that, when it becomes big,
    it is easy to scale. While microservices are not the only way to accomplish that
    goal, they are indeed a good way to approach this problem. The cost is not in
    development but operations. If operations are automated, that cost can be absorbed
    quickly and does not need to represent a massive investment. As you already saw
    (and will continue seeing throughout the rest of the book), there are excellent
    open source tools at our disposal. The best part of automation is that the investment
    tends to have lower maintenance cost than when things are done manually.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 上述情况非常常见。一开始有效的方法，随着需求的增加，不一定就能继续适用。我们需要平衡**你不需要它**（**YAGNI**）原则和长期愿景。我们不能一开始就构建一个为大型公司优化的系统，因为它成本过高，并且当业务还很小的时候，它并不能带来足够的好处。另一方面，我们不能忽视任何业务的主要目标之一。我们不能从第一天起就不考虑扩展性。设计可扩展的架构并不意味着我们需要从一开始就部署一个百台服务器的集群。它也不意味着我们从一开始就要开发一个庞大复杂的系统。它意味着我们应该从小做起，但在设计时考虑到，当它变得庞大时，扩展会变得容易。虽然微服务并不是实现这一目标的唯一方式，但它们确实是解决这个问题的一种好方法。成本不在于开发，而在于运营。如果运营是自动化的，那么这一成本可以迅速被吸收，并且不需要大规模的投资。如你所见（并且在本书的其余部分你将继续看到），我们有优秀的开源工具可供使用。自动化的最大优点是，投资的维护成本通常低于手动操作时的维护成本。
- en: We already discussed microservices and automation of their deployments on a
    tiny scale. Now it's time to convert this small scale to something bigger. Before
    we jump into practical parts, let us explore what are some of the different ways
    one might approach scaling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了微服务及其在小规模上的自动化部署。现在是时候将这个小规模转变为更大的规模了。在我们进入实际部分之前，让我们先探讨一下可能的不同扩展方式。
- en: We are often limited by our design and choosing the way applications are constructed
    limits our choices severely. Although there are many different ways to scale,
    most common one is called *Axis Scali* *ng*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常受限于设计，选择应用程序的构建方式严重限制了我们的选择。虽然有许多不同的缩放方式，但最常见的一种叫做*坐标轴缩放*。
- en: Axis scaling
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 坐标轴缩放
- en: Axis scaling can be best represented through three dimensions of a cube; *x-axis*,
    *y-axis* and *z-axis*. Each of those dimensions describes a type of scaling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标轴缩放可以通过一个立方体的三维来最好地表示；*x轴*、*y轴*和*z轴*。每个维度描述了一种缩放方式。
- en: '**X-Axis**: Horizontal duplication'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X轴**：水平复制'
- en: '**Y-Axis**: Functional decomposition'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y轴**：功能分解'
- en: '**Z-Axis**: Data partitioning![Axis scaling](img/B05848_14_01.jpg)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z轴**：数据分区![坐标轴缩放](img/B05848_14_01.jpg)'
- en: Figure 14-1 – Scale cube
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14-1 – 缩放立方体
- en: Let's go through axes, one at the time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解各个轴。
- en: X-Axis scaling
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X轴缩放
- en: In a nutshell, *x-axis scaling* is accomplished by running multiple instances
    of an application or a service. In most cases, there is a load balancer on top
    that makes sure that the traffic is shared among all those instances. The biggest
    advantage of x-axis scaling is simplicity. All we have to do is deploy the same
    application on multiple servers. For that reason, this is the most commonly used
    type of scaling. However, it comes with its set of disadvantages when applied
    to monolithic applications. Having a huge application usually requires big cache
    that demands heavy usage of memory. When such an application is multiplied, everything
    is multiplied by it, including the cache.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*x轴缩放*是通过运行多个应用程序或服务实例来实现的。在大多数情况下，顶部有一个负载均衡器，确保流量在所有这些实例之间共享。x轴缩放的最大优势是简单性。我们所需要做的就是在多个服务器上部署相同的应用程序。因此，这种缩放方式是最常用的。然而，当它应用于单体应用程序时，它也有一系列的缺点。拥有一个庞大的应用程序通常需要大量缓存，这会消耗大量内存。当这样的应用程序被复制时，所有东西都会随之复制，包括缓存。
- en: 'Another, often more important, problem is inappropriate usage of resources.
    Performance problems are almost never related to the whole application. Not all
    modules are equally affected, and, yet, we multiply everything. That means that
    even though we could be better of by scaling only part of the application that
    require such an action, we scale everything. Never the less, x-scaling is important
    no matter the architecture. The major difference is the effect that such a scaling
    has. By using microservices, we are not removing the need for x-axis scaling but
    making sure that due to their architecture such scaling has more effect than with
    alternative and more traditional approaches to architecture. With microservices
    we have the option to fine-tune scaling. We can have many instances of services
    that suffer a lot under heavy load and only a few instances of those that are
    used less often or require fewer resources. On top of that, since they are small,
    we might never reach a limit of a service. A small service in a big server would
    need to receive a truly massive amount of traffic before the need for scaling
    arises. Scaling microservices is more often related to fault tolerance than performance
    problems. We want to have multiple copies running so that, if one of them dies,
    the others can take over until recovery is performed:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个，通常更重要的问题是资源的使用不当。性能问题几乎从来不会与整个应用程序相关。并非所有模块都受到相同的影响，然而我们却将一切都进行扩展。这意味着，尽管我们本可以通过仅扩展那些需要此类操作的部分应用来获得更好的效果，但我们却扩展了所有部分。尽管如此，X轴扩展在任何架构中都是重要的。主要区别在于这种扩展的效果。通过使用微服务，我们并不是消除了X轴扩展的需求，而是确保由于其架构的原因，这种扩展的效果比传统架构方法更为显著。在微服务中，我们可以精细调节扩展。我们可以为负载较重的服务提供多个实例，而对于使用较少或需求较少资源的服务，只提供少量实例。除此之外，由于微服务较小，我们可能永远不会达到服务的限制。一个小型服务在一个大服务器上，必须接收非常庞大的流量，才会需要扩展。扩展微服务更常与容错而非性能问题相关。我们希望有多个副本在运行，以便如果其中一个副本失败，其他副本能够接管，直到恢复：
- en: '![X-Axis scaling](img/B05848_14_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![X轴扩展](img/B05848_14_02.jpg)'
- en: Figure 14-2 – Monolithic application scaled inside a cluster
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-2 – 在集群内部扩展的单体应用
- en: Y-Axis scaling
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Y轴扩展
- en: Y-axis scaling is all about decomposition of an application into smaller services.
    Even though there are different ways to accomplish this decomposition, microservices
    are probably the best approach we can take. When they are combined with immutability
    and self-sufficiency, there is indeed no better alternative (at least from the
    prism of y-axis scaling). Unlike x-axis scaling, the y-axis is not accomplished
    by running multiple instances of the same application but by having multiple different
    services distributed across the cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Y轴扩展关注的是将应用程序分解成更小的服务。尽管有不同的方法可以完成这种分解，微服务可能是我们可以采取的最佳方法。当它们与不可变性和自足性结合时，的确没有更好的选择（至少从Y轴扩展的角度来看）。与X轴扩展不同，Y轴扩展并不是通过运行多个相同实例的应用来实现，而是通过将多个不同的服务分布在集群中来实现。
- en: Z-Axis scaling
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Z轴扩展
- en: Z-axis scaling is rarely applied to applications or services. Its primary and
    most common usage is among databases. The idea behind this type of scaling is
    to distribute data among multiple servers thus reducing the amount of work that
    each of them needs to perform. Data is partitioned and distributed so that each
    server needs to deal only with a subset of the data. This type of the separation
    is often called sharding, and there are many databases specially designed for
    this purpose. Benefits of z-axis scaling are most noticeable in I/O and cache
    and memory utilization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Z轴扩展很少应用于应用程序或服务。它的主要和最常见的应用是在数据库中。这种扩展的背后思想是将数据分布到多个服务器之间，从而减少每个服务器需要执行的工作量。数据被分割和分布，使得每个服务器只需要处理数据的一个子集。这种分割方式通常被称为分片（sharding），并且有许多数据库是专门为此目的设计的。Z轴扩展的好处在于I/O操作和缓存与内存利用率上的显著提升。
- en: Clustering
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群
- en: A server cluster consists of a set of connected servers that work together and
    can be seen as a single system. They are usually connected through fast local
    area network (LAN). The major difference between a cluster and simply a group
    of servers is that the cluster acts as a single system trying to provide high
    availability, load balancing, and parallel processing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器集群由一组连接在一起的服务器组成，这些服务器协同工作，并且可以被视为一个单一的系统。它们通常通过高速局域网（LAN）连接。集群与普通服务器组的主要区别在于，集群作为一个单一系统，旨在提供高可用性、负载均衡和并行处理。
- en: 'If we deploy applications, or services, to individually managed servers and
    treat them as separate units, the utilization of resources is sub-optimum. We
    cannot know in advance which group of services should be deployed to a server
    and utilize resources to their maximum. More importantly, resource usage tends
    to fluctuate. While, in the morning, some service might require a lot of memory,
    during the afternoon that usage might be lower. Having predefined servers does
    not allow us elasticity that would balance that usage in the best possible way.
    Even if such a high level of dynamism is not required, predefined servers tend
    to create problems when something goes wrong, resulting in manual actions to redeploy
    the affected services to a healthy node:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将应用程序或服务部署到单独管理的服务器上，并将它们视为独立单元，那么资源的利用率将会低于最佳水平。我们无法提前知道哪些服务组应该部署到某台服务器上，并最大化资源利用率。更重要的是，资源使用往往会波动。例如，早晨某个服务可能需要大量内存，而下午该服务的内存使用量可能会较低。预定义的服务器无法提供足够的弹性，以最佳方式平衡这种资源使用。即使不需要如此高程度的动态性，预定义的服务器也会在出现故障时造成问题，导致需要手动重新部署受影响的服务到健康节点：
- en: '![Clustering](img/B05848_14_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![集群](img/B05848_14_03.jpg)'
- en: Figure 14-3 – Cluster with containers deployed to predefined servers
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-3 – 部署到预定义服务器的集群与容器
- en: Real clustering is accomplished when we stop thinking in terms of individual
    servers and start thinking of a cluster; of all servers as one big entity. That
    can be better explained if we drop to a bit lower level. When we deploy an application,
    we tend to specify how much memory or CPU it might need. However, we do not decide
    which memory slots our application will use nor which CPUs it should utilize.
    For example, we don't specify that some application should use CPUs 4, 5 and 7\.
    That would be inefficient and potentially dangerous. We only decide that three
    CPUs are required. The same approach should be taken on a higher level. We should
    not care where an application or a service will be deployed but what it needs.
    We should be able to define that the service has certain requirements and tell
    some tool to deploy it to whichever server in our cluster, as long as it fulfills
    the needs we have. The best (if not the only) way to accomplish that is to consider
    the whole cluster as one entity. We can increase or decrease the capacity of that
    cluster by adding or removing servers but, no matter what we do, it should still
    be a single entity. We define a strategy and let our services be deployed somewhere
    inside the cluster. Those using cloud providers like **Amazon Web Services** (**AWS**),
    Microsoft's Azure and **Google Cloud Engine** (**GCP**) are already accustomed
    to this approach, even though they might not be aware of it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的集群是在我们停止考虑单独的服务器，而开始考虑整个集群时实现的；把所有服务器视为一个大的整体。如果我们稍微降到较低的层次，可以更好地解释这一点。当我们部署应用程序时，通常会指定它可能需要多少内存或CPU。然而，我们并不决定应用程序将使用哪些内存插槽，或它应该利用哪些CPU。例如，我们不会指定某个应用程序应该使用CPU
    4、5和7。这样做既低效又可能危险。我们只决定需要多少CPU。我们应该在更高的层次上采取相同的做法。我们不应该关心应用程序或服务将部署到哪里，而是关心它需要什么。我们应该能够定义该服务的具体需求，并告诉某个工具将它部署到集群中的任何服务器，只要它满足我们的需求。实现这一目标的最佳方法（如果不是唯一的方法）就是将整个集群视为一个整体。我们可以通过增加或移除服务器来增加或减少集群的容量，但无论我们做什么，集群仍然应该是一个单一的整体。我们定义一个策略，并让我们的服务在集群内的某个地方部署。像**Amazon
    Web Services**（**AWS**）、微软的Azure和**Google Cloud Engine**（**GCP**）等云服务提供商的用户，尽管可能没有意识到，但已经习惯了这种方式。
- en: 'Throughout the rest of this chapter, we''ll explore ways to create our cluster
    and explore tools that can help us with that objective. The fact that we''ll be
    simulating the cluster locally does not mean that the same strategies cannot be
    applied to public or private clouds and data centers. Quite the opposite:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章接下来的部分，我们将探索创建集群的方法，并探讨一些可以帮助我们实现这一目标的工具。我们将在本地模拟集群，这并不意味着这些相同的策略无法应用于公有云、私有云和数据中心。恰恰相反：
- en: '![Clustering](img/B05848_14_04.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![集群](img/B05848_14_04.jpg)'
- en: Figure 14-4 – Cluster with containers deployed to servers based on a predefined
    strategy
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-4 – 根据预定义策略部署到服务器的集群与容器
- en: Docker Clustering Tools Compared – Kubernetes versus Docker Swarm versus Mesos
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker集群工具比较 – Kubernetes与Docker Swarm与Mesos
- en: Kubernetes and Docker Swarm are probably the two most commonly used tools to
    deploy containers inside a cluster. Both are created as helper platforms that
    can be used to manage a cluster of containers and treat all servers as a single
    unit. While their goals are, somewhat, similar, they differ considerably in their
    approach.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes和Docker Swarm可能是当前最常用的两个容器部署工具。它们都是作为辅助平台创建的，可用于管理集群中的容器，并将所有服务器视为一个整体。虽然它们的目标在某种程度上相似，但在方法上有显著差异。
- en: Kubernetes
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is based on Google's experience of many years working with Linux
    containers. It is, in a way, a replica of what Google has been doing for a long
    time but, this time, adapted to Docker. That approach is great in many ways, most
    important being that they used their experience from the start. If you started
    using Kubernetes around Docker version 1.0 (or earlier), the experience with Kubernetes
    was great. It solved many of the problems that Docker itself had. We can mount
    persistent volumes that allow us to move containers without losing data, it uses
    flannel to create networking between containers, it has load balancer integrated,
    it uses etcd for service discovery, and so on. However, Kubernetes comes at a
    cost. When compared with Docker, it uses a different CLI, different API, and different
    YAML definitions. In other words, you cannot use Docker CLI, nor you can use Docker
    Compose to define containers. Everything needs to be done from scratch exclusively
    for Kubernetes. It's as if the tool was not written for Docker (which is partly
    true). Kubernetes brought clustering to a new level but at the expense of usability
    and steep learning curve.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes基于Google多年来使用Linux容器的经验。从某种意义上说，它是Google长期以来所做工作的复制品，但这一次，它是为Docker量身定制的。这个方法在很多方面都很棒，最重要的是他们从一开始就利用了自己的经验。如果你在Docker
    1.0版本（或更早）左右开始使用Kubernetes，Kubernetes的体验非常好。它解决了Docker本身的许多问题。我们可以挂载持久化卷，使我们能够在不丢失数据的情况下移动容器，它使用flannel在容器之间创建网络，它集成了负载均衡器，使用etcd进行服务发现，等等。然而，Kubernetes是有代价的。与Docker相比，它使用了不同的CLI、不同的API和不同的YAML定义。换句话说，你不能使用Docker
    CLI，也不能使用Docker Compose来定义容器。所有的操作都必须从头开始，完全为Kubernetes定制。就好像这个工具并不是为Docker编写的（某种程度上是事实）。Kubernetes将集群管理提升到了一个新层次，但代价是可用性和陡峭的学习曲线。
- en: Docker Swarm
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: Docker Swarm took a different approach. It is a native clustering for Docker.
    The best part is that it exposes standard Docker API meaning that any tool that
    you used to communicate with Docker (Docker CLI, Docker Compose, Dokku, Krane,
    and so on) can work equally well with Docker Swarm. That in itself is both an
    advantage and a disadvantage at the same time. Being able to use familiar tools
    of your choosing is great but for the same reasons we are bound by the limitations
    of Docker API. If the Docker API doesn't support something, there is no way around
    it through Swarm API, and some clever tricks need to be performed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm采取了不同的方法。它是Docker的本地集群管理工具。最棒的部分是它暴露了标准的Docker API，这意味着你曾经用来与Docker交互的任何工具（Docker
    CLI、Docker Compose、Dokku、Krane等）都可以与Docker Swarm完美兼容。这本身既是优点也是缺点。能够使用你熟悉的工具当然很好，但正因为如此，我们也受限于Docker
    API的限制。如果Docker API不支持某个功能，那么Swarm API也无法绕过这个问题，需要一些巧妙的技巧来实现。
- en: Apache Mesos
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: The next in line of tools that can be used to manage a cluster is Apache Mesos.
    It is the clustering veteran. Mesos abstracts CPU, memory, storage, and other
    resources away from machines (physical or virtual), enabling fault-tolerant and
    elastic distributed systems to be easily built and run efficiently.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来可以用来管理集群的工具是Apache Mesos。它是集群管理的老兵。Mesos将CPU、内存、存储和其他资源从机器（物理或虚拟）中抽象出来，使得容错和弹性分布式系统能够轻松构建并高效运行。
- en: Mesos is made using the same principles as the Linux kernel, only at a different
    level of abstraction. Mesos kernel runs on every machine and provides applications
    with APIs for resource management and scheduling across entire datacenter and
    cloud environments. Unlike Kubernetes and Docker Swarm, Mesos is not limited to
    containers. It can work with almost any type of deployments including Docker containers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos使用与Linux内核相同的原理，只是抽象的层次不同。Mesos内核运行在每台机器上，为应用程序提供资源管理和调度的API，覆盖整个数据中心和云环境。与Kubernetes和Docker
    Swarm不同，Mesos不限于容器。它几乎可以与任何类型的部署工作，包括Docker容器。
- en: Mesos uses Zookeeper for service discovery. It uses Linux containers to isolate
    processes. If, for example, we deploy Hadoop without using Docker, Mesos will
    run it as a native Linux container providing similar features as if we packed
    it as a Docker container.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos 使用 Zookeeper 进行服务发现。它使用 Linux 容器来隔离进程。例如，如果我们在不使用 Docker 的情况下部署 Hadoop，Mesos
    会将其作为一个本地 Linux 容器运行，提供类似于将其打包为 Docker 容器的功能。
- en: Mesos provides few features that Swarm doesn't have at this moment, mainly more
    powerful scheduler. Apart from the scheduler, what makes Mesos attractive is that
    we can use it for both Docker and non-Docker deployments. Many organizations might
    not want to use Docker, or they might decide to use a combination of both Docker
    and non-Docker deployments. In such a case, Mesos is truly an excellent option
    if we do not want to deal with two sets of clustering tools; one for containers
    and the other for the rest of deployments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos 提供了一些 Swarm 当前没有的功能，主要是更强大的调度器。除了调度器之外，使 Mesos 吸引人的地方在于，我们可以将其用于 Docker
    和非 Docker 部署。许多组织可能不想使用 Docker，或者他们可能决定同时使用 Docker 和非 Docker 部署的组合。在这种情况下，如果我们不想处理两套集群工具——一种用于容器，另一种用于其他部署——Mesos
    真的是一个很好的选择。
- en: However, Mesos is old and too big for what we're trying to accomplish. More
    importantly, Docker containers are an afterthought. The platform was not designed
    with them in mind but added Docker support later on. Working with Docker and Mesos
    feels awkward, and it becomes apparent from the very start that those two were
    not meant to be used together. Given the existence of Swarm and Kubernetes, there
    is nothing that Mesos can offer to those decided to embrace Docker. Mesos is falling
    behind. The main advantage it has over the other two tools is its wide adoption.
    Many started using it before the emergence of Docker and might choose stick with
    it. For those that have the option to start fresh, the choice should fall between
    Kubernetes and Docker Swarm.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Mesos 有些过时，并且对我们想要实现的目标来说过于庞大。更重要的是，Docker 容器是其事后添加的。该平台最初并没有为 Docker 设计，但后来才添加了对
    Docker 的支持。与 Docker 一起使用 Mesos 感觉很别扭，并且从一开始就显而易见，这两者并不是为了共同使用而设计的。考虑到 Swarm 和
    Kubernetes 的存在，Mesos 对那些决定拥抱 Docker 的人来说已经毫无优势。Mesos 正在落后。它相较于另外两种工具的主要优势是广泛的采用。许多人在
    Docker 出现之前就开始使用它，并可能选择继续使用它。对于那些可以重新开始的人，选择应该在 Kubernetes 和 Docker Swarm 之间做出。
- en: We'll explore Kubernetes and Docker Swarm in more details and leave Mesos behind.
    The exploration will be based on their setup and features they provide for running
    containers in a cluster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地探讨 Kubernetes 和 Docker Swarm，抛开 Mesos。探索将基于它们的设置和它们为在集群中运行容器提供的功能。
- en: Setting It Up
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置它
- en: Setting up Docker Swarm is easy, straightforward and flexible. All we have to
    do is install one of the service discovery tools and run the `swarm` container
    on all nodes. Since the distribution itself is packed in a Docker container, it
    works in the same way no matter the operating system. We run the `swarm` container,
    expose a port and inform it about the address of the service discovery. It could
    hardly be easier than that. We can even start using it without any service discovery
    tool, see whether we like it and when our usage of it becomes more serious, add
    etcd, Consul or some of the other supported tools.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Docker Swarm 既简单又直接，且具有灵活性。我们需要做的就是安装其中一个服务发现工具，并在所有节点上运行 `swarm` 容器。由于该分发本身已打包在
    Docker 容器中，因此无论操作系统如何，它的工作方式都相同。我们运行 `swarm` 容器，暴露端口，并告知其服务发现地址。做到这一点几乎不可能更简单了。我们甚至可以在没有任何服务发现工具的情况下开始使用它，看看是否喜欢，并在使用变得更加严肃时，添加
    etcd、Consul 或其他支持的工具。
- en: Kubernetes setup is quite more complicated and obfuscated. Installation instructions
    differ from OS to OS and provider to provider. Each OS or a hosting provider comes
    with its set of instructions, each of them having a separate maintenance team
    with a different set of problems. As an example, if you choose to try it out with
    Vagrant, you are stuck with Fedora. That does not mean that you cannot run it
    with Vagrant and, let's say, Ubuntu or CoreOS. You can, but you need to start
    searching for instructions outside the official Kubernetes Getting Started page.
    Whatever your needs are, it's likely that the community has the solution, but
    you still need to spend some time searching for it and hoping that it works from
    the first attempt. The bigger problem is that the installation relies on a bash
    script. That would not be a big deal in itself if we would not live in the era
    where configuration management is a must. We might not want to run a script but
    make Kubernetes be part of our Puppet, Chef, or Ansible definitions. Again, this
    can be overcome as well. You can find Ansible playbooks for running Kubernetes,
    or you can write your own. None of those issues are a big problem but, when compared
    with Swarm, they are a bit painful. With Docker, we were supposed not to have
    installation instructions (aside from a few `docker run` arguments). We were supposed
    to run containers. Swarm fulfills that promise, and Kubernetes doesn't.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的设置要复杂得多且不透明。安装说明因操作系统和提供商而异。每个操作系统或托管提供商都有自己的安装说明，每个说明都有不同的维护团队，面临不同的问题。例如，如果你选择使用
    Vagrant 来尝试，那么你就只能使用 Fedora。这并不意味着你不能在 Vagrant 上运行它，并且选择使用 Ubuntu 或 CoreOS 也是可以的。你可以，但需要开始在官方
    Kubernetes 入门页面以外寻找说明。无论你的需求是什么，社区很可能已经有了解决方案，但你仍然需要花费一些时间去寻找，并希望它能够在第一次尝试时就成功。更大的问题是，安装依赖于一个
    bash 脚本。如果我们不生活在一个配置管理成为必须的时代，这本身并不是一个大问题。如果我们不想运行脚本，而是希望 Kubernetes 成为我们 Puppet、Chef
    或 Ansible 配置的一部分，那也可以克服。你可以找到用于运行 Kubernetes 的 Ansible 剧本，或者编写自己的剧本。虽然这些问题并不算大问题，但与
    Swarm 相比，它们还是有点痛苦。对于 Docker，我们本应不需要安装说明（除了几个 `docker run` 参数）。我们本应只运行容器。Swarm
    实现了这个承诺，而 Kubernetes 并没有。
- en: While some might not care about which discovery tool is used, I love the simplicity
    of Swarm and the logic "batteries included but removable". Everything works out-of-the-box,
    but we still have the option to substitute one component for the other. Unlike
    Swarm, Kubernetes is an opinionated tool. You need to live with the choices it
    made for you. If you want to use Kubernetes, you have to use etcd. I'm not trying
    to say that etcd is bad (quite contrary), but if you prefer, for example, to use
    Consul, you're in a very complicated situation and would need to use one for Kubernetes
    and the other for the rest of your service discovery needs. Another thing I dislike
    about Kubernetes is its need to know things in advance, before the setup. You
    need to tell it the addresses of all your nodes, which role each of them has,
    how many minions there are in the cluster and so on. With Swarm, we just bring
    up a node and tell it to join the network. Nothing needs to be set in advance
    since the information about the cluster is propagated through the `gossip` protocol.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有些人可能不在乎使用哪个发现工具，但我喜欢 Swarm 的简单性以及“包含电池但可拆卸”的逻辑。一切开箱即用，但我们仍然可以选择替换其中的某个组件。与
    Swarm 不同，Kubernetes 是一个有明确观点的工具。你需要接受它为你做出的选择。如果你想使用 Kubernetes，你必须使用 etcd。我并不是说
    etcd 很差（恰恰相反），但如果你比如说更喜欢使用 Consul，那么你就会陷入一个非常复杂的局面，你需要为 Kubernetes 使用一个工具，而为其他服务发现需求使用另一个工具。我还不喜欢
    Kubernetes 的另一点是它需要在设置之前就知道一些事情。你需要告诉它所有节点的地址、每个节点的角色、集群中有多少个从节点等等。而使用 Swarm，我们只需要启动一个节点并让它加入网络。无需提前设置任何信息，因为集群的相关信息会通过
    `gossip` 协议进行传播。
- en: Setup might not be the most significant difference between those tools. No matter
    which tool you choose, sooner or later everything will be up and running, and
    you'll forget any trouble you might have had during the process. You might say
    that we should not choose one tool over the other only because one is easier to
    set up. Fair enough. Let's move on and speak about differences in how you define
    containers that should be run with those tools.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 设置可能不是这些工具之间最显著的区别。无论选择哪个工具，迟早一切都会顺利运行，你会忘记在过程中遇到的任何问题。你可能会说，我们不应仅因为某个工具更容易设置而选择它。这个说法有道理。我们继续讨论如何定义应该使用这些工具运行的容器之间的区别。
- en: Running Containers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行容器
- en: How do you define all the arguments needed for running Docker containers with
    Swarm? You don't! Actually, you do, but not in any form or way different from
    the way you were defining them before Swarm. If you are used to running containers
    through Docker CLI, you can keep using it with (almost) the same commands. If
    you prefer to use Docker Compose to run containers, you can continue using it
    to run them inside the Swarm cluster. Whichever way you've used to run your containers,
    the chances are that you can continue doing the same with Swarm but on a much
    larger scale.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何定义在 Swarm 中运行 Docker 容器所需的所有参数？你不需要！其实，你需要，但这与你在 Swarm 之前定义它们的方式没有任何不同。如果你习惯通过
    Docker CLI 运行容器，你可以继续使用它，几乎是相同的命令。如果你更喜欢使用 Docker Compose 来运行容器，你可以继续在 Swarm 集群中使用它来运行容器。不管你以前是如何运行容器的，你很可能可以继续用相同的方式在
    Swarm 上运行，只不过是在更大规模上。
- en: Kubernetes requires you to learn its CLI and configurations. You cannot use
    `docker-compose.yml` definitions you created earlier. You'll have to create Kubernetes
    equivalents. You cannot use Docker CLI commands you learned before. You'll have
    to learn Kubernetes CLI and, likely, make sure that the whole organization learns
    it as well.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 要求你学习它的 CLI 和配置。你不能使用你之前创建的 `docker-compose.yml` 定义。你必须创建 Kubernetes
    的等效配置。你不能使用你之前学过的 Docker CLI 命令。你必须学习 Kubernetes CLI，并且很可能需要确保整个组织也都学会它。
- en: No matter which tool you choose for deployments to your cluster, chances are
    you are already familiar with Docker. You are probably already used to Docker
    Compose as a way to define arguments for the containers you'll run. If you played
    with it for more than a few hours, you are using it as a substitute for Docker
    CLI. You run containers with it, tail their logs, scale them, and so on. On the
    other hand, you might be a hard-core Docker user who does not like Docker Compose
    and prefers running everything through Docker CLI or you might have your bash
    scripts that run containers for you. No matter what you choose, it should work
    with Docker Swarm.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪个工具来进行集群部署，很可能你已经熟悉 Docker。你可能已经习惯了使用 Docker Compose 来定义你将运行的容器的参数。如果你玩了几个小时，你已经在将
    Docker Compose 当作 Docker CLI 的替代品使用了。你用它来运行容器、查看日志、扩展容器，等等。另一方面，你可能是一个硬核的 Docker
    用户，不喜欢 Docker Compose，宁愿通过 Docker CLI 来运行一切，或者你可能有自己的 bash 脚本来为你运行容器。无论你选择什么，它都应该能与
    Docker Swarm 一起使用。
- en: If you adopt Kubernetes, be prepared to have multiple definitions of the same
    thing. You will need Docker Compose to run your containers outside Kubernetes.
    Developers will continue needing to run containers on their laptops, your staging
    environments might or might not be a big cluster, and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你采用 Kubernetes，做好准备，你将会有多个不同的定义来描述相同的事物。你需要 Docker Compose 来在 Kubernetes 之外运行你的容器。开发人员仍然需要在他们的笔记本电脑上运行容器，你的暂存环境可能是一个大集群，也可能不是，等等。
- en: In other words, once you adopt Docker, Docker Compose or Docker CLI are unavoidable.
    You have to use them one way or another. Once you start using Kubernetes you will
    discover that all your Docker Compose definitions (or whatever else you might
    be using) need to be translated to Kubernetes way of describing things and, from
    there on, you will have to maintain both. With Kubernetes, everything will have
    to be duplicated resulting in higher cost of maintenance. And it's not only about
    duplicated configurations. Commands you'll run outside the cluster will be different
    from those inside the cluster. All those Docker commands you learned and love
    will have to get their Kubernetes equivalents inside the cluster.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一旦你采用了 Docker，Docker Compose 或 Docker CLI 是不可避免的。你必须以某种方式使用它们。一旦你开始使用 Kubernetes，你会发现所有的
    Docker Compose 定义（或你可能在使用的其他工具）需要转换为 Kubernetes 描述事物的方式，之后你将不得不同时维护两者。在 Kubernetes
    中，一切都需要被重复定义，导致更高的维护成本。而且这不仅仅是关于重复的配置。你在集群外运行的命令将与在集群内运行的命令不同。你学会并喜爱的所有 Docker
    命令，将不得不在集群内找到它们的 Kubernetes 对应命令。
- en: Guys behind Kubernetes are not trying to make your life miserable by forcing
    you to do things "their way". The reason for such a big differences is in different
    approaches Swarm and Kubernetes are using to tackle the same problem. Swarm team
    decided to match their API with the one from Docker. As a result, we have (almost)
    full compatibility. Almost everything we can do with Docker we can do with Swarm
    as well only on a much larger scale. There's nothing new to do, no configurations
    to be duplicated and nothing new to learn. No matter whether you use Docker CLI
    directly or go through Swarm, API is (more or less) the same. The negative side
    of that story is that if there is something you'd like Swarm to do and that something
    is not part of the Docker API, you're in for a disappointment. Let us simplify
    this a bit. If you're looking for a tool for deploying containers in a cluster
    that will use Docker API, Swarm is the solution. On the other hand, if you want
    a tool that will overcome Docker limitations, you should go with Kubernetes. It
    is power (Kubernetes) against simplicity (Swarm). Or, at least, that's how it
    was until recently. But, I'm jumping ahead of myself.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 背后的团队并不是想通过强迫你按“他们的方式”做事来让你的生活变得痛苦。这么大的差异的原因在于 Swarm 和 Kubernetes
    采用了不同的方法来解决相同的问题。Swarm 团队决定将其 API 与 Docker 的 API 相匹配。结果，我们几乎实现了完全兼容。几乎所有 Docker
    可以做的事情，Swarm 也能做，只不过是规模更大。没有什么新东西要做，也没有配置需要重复，也没有什么新东西要学习。无论你是直接使用 Docker CLI
    还是通过 Swarm，API （或多或少）都是相同的。这种做法的负面一面是，如果你希望 Swarm 执行某些 Docker API 没有的操作，那么你就会失望。让我们简化一下。如果你正在寻找一个用于在集群中部署容器的工具，并且希望使用
    Docker API，那么 Swarm 就是解决方案。另一方面，如果你需要一个能够克服 Docker 限制的工具，那么你应该选择 Kubernetes。它是功能（Kubernetes）与简易性（Swarm）之间的较量。或者，至少直到最近，它一直是这样。但，我现在有些超前了。
- en: The only question unanswered is what those limitations are. Two of the major
    ones were networking, persistent volumes and automatic failover in case one or
    more containers or a whole node stopped working.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一未解答的问题是这些限制是什么。两个主要的限制是网络、持久化卷和在一个或多个容器或整个节点停止工作时的自动故障转移。
- en: Until Docker Swarm release 1.0 we could not link containers running on different
    servers. We still cannot link them, but now we have `multi-host networking` to
    help us connect containers running on different servers. It is a very powerful
    feature. Kubernetes used `flannel` to accomplish networking and now, since the
    Docker release 1.9, that feature is available as part of Docker CLI.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker Swarm 1.0 发布之前，我们不能将运行在不同服务器上的容器进行链接。我们现在依然无法将它们链接起来，但现在我们有了 `multi-host
    networking` 来帮助我们连接在不同服务器上运行的容器。这是一个非常强大的功能。Kubernetes 使用 `flannel` 来实现网络连接，现在自
    Docker 1.9 版本起，该功能已作为 Docker CLI 的一部分提供。
- en: Another problem was persistent volumes. Docker introduced them in release 1.9\.
    Until recently, if you persist a volume, that container was tied to the server
    that volume resides. It could not be moved around without, again, resorting to
    some nasty tricks like copying volume directory from one server to another. That
    in itself is a slow operation that defies the goals of the tools like Swarm. Besides,
    even if you have time to copy a volume from one to the other server, you do not
    know where to copy since clustering tools tend to treat your whole datacenter
    as a single entity. Your containers will be deployed to a location most suitable
    for them (least number of containers running, most CPUs or memory available, and
    so on). Now we have persistent volumes supported by Docker natively.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是持久化卷。Docker 在 1.9 版本中引入了持久化卷。直到最近，如果你持久化一个卷，那么该容器就会绑定到存储该卷的服务器上。它无法在没有一些复杂操作的情况下被迁移，比如从一台服务器复制卷目录到另一台服务器。这个操作本身就是一个非常慢的过程，违背了像
    Swarm 这类工具的目标。此外，即使你有时间将卷从一台服务器复制到另一台，你也不知道该复制到哪里，因为集群工具往往将整个数据中心视为一个单一的实体。你的容器会被部署到最适合它们的位置（容器运行数量最少，CPU
    或内存可用最多，等等）。现在，Docker 本身就原生支持持久化卷。
- en: Finally, automatic failover is probably the only feature advantage Kubernetes
    has over Swarm. However, failover solution provided by Kuberentes is incomplete.
    If a container goes down, Kubernetes will detect that and start it again on a
    healthy node. The problem is that containers or whole nodes often do not fail
    for no reason. Much more needs to be done than a simple re-deployment. Someone
    needs to be notified, information before a failure needs to be evaluated, and
    so on. If re-deployment is all you need, Kubernetes is a good solution. If more
    is needed, Swarm, due to its "batteries included but removable" philosophy, allows
    you to build your solution. Regarding the failover, it's a question whether to
    aim for an out-of-the-box solution (Kubernetes) that is hard to extend or go for
    a solution that is built with the intention to be easily extended (Swarm).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，自动故障转移可能是 Kubernetes 相比 Swarm 唯一的特性优势。然而，Kubernetes 提供的故障转移解决方案并不完整。如果一个容器崩溃，Kubernetes
    会检测到并在健康的节点上重新启动它。问题在于，容器或整个节点通常不会无缘无故地失败。所需的工作远远超过简单的重新部署。需要有人被通知，故障前的信息需要进行评估，等等。如果仅仅需要重新部署，Kubernetes
    是一个不错的解决方案。如果需要更多的功能，由于 Swarm 提供的“自带但可移除”的哲学，它允许你构建自己的解决方案。在故障转移方面，问题在于是选择一个开箱即用且难以扩展的解决方案（Kubernetes），还是选择一个为了容易扩展而构建的解决方案（Swarm）。
- en: Both networking and persistent volumes problems were one of the features supported
    by Kubernetes for quite some time and the reason many were choosing it over Swarm.
    That advantage disappeared with Docker release 1.9\. Automatic fail-over remains
    an advantage Kubernetes has over Swarm when looking at out-of-the-box solutions.
    In the case of Swarm, we need to develop failover strategies ourselves.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 网络和持久卷的问题曾是 Kubernetes 支持的特性之一，也是许多人选择它而不是 Swarm 的原因。然而，这一优势在 Docker 1.9 版本发布后消失了。自动故障转移仍然是
    Kubernetes 相较于 Swarm 的一个优势，尤其是在考虑开箱即用的解决方案时。对于 Swarm，我们需要自己开发故障转移策略。
- en: The Choice
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择
- en: When trying to make a choice between Docker Swarm and Kubernetes, think in following
    terms. Do you want to depend on Docker solving problems related to clustering.
    If you do, choose Swarm. If Docker does not support something, it will be unlikely
    that it will be supported by Swarm since it relies on Docker API. On the other
    hand, if you want a tool that works around Docker limitations, Kubernetes might
    be the right one for you. Kubernetes was not built around Docker but is based
    on Google's experience with containers. It is opinionated and tries to do things
    in its own way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出 Docker Swarm 和 Kubernetes 之间的选择时，可以从以下几个方面进行思考。你是否希望依赖 Docker 来解决与集群相关的问题？如果是的话，选择
    Swarm。如果 Docker 不支持某些功能，那么 Swarm 也不太可能支持这些功能，因为它依赖于 Docker API。另一方面，如果你希望使用一个能够绕过
    Docker 限制的工具，Kubernetes 可能更适合你。Kubernetes 不是围绕 Docker 构建的，而是基于 Google 在容器方面的经验。它有明确的理念，并且尝试以自己的方式做事。
- en: The real question is whether Kubernetes' way of doing things, which is quite
    different from how we use Docker, is overshadowed by advantages it gives. Or,
    should we place our bets into Docker itself and hope that it will solve those
    problems? Before you answer those questions, take a look at the Docker release
    1.9\. We got persistent volumes and software networking. We also got `unless-stopped`
    restart policy that will manage our unwanted failures. Now, there are three things
    less of a difference between Kubernetes and Swarm. Actually, these days there
    are very few advantages Kubernetes has over Swarm. Automatic failover featured
    by Kubernetes is a blessing and a curse at the same time. On the other hand, Swarm
    uses Docker API meaning that you get to keep all your commands and Docker Compose
    configurations. Personally, I'm placing my bets on Docker engine getting improvements
    and Docker Swarm running on top of it. The difference between the two is small.
    Both are production ready but Swarm is easier to set up, easier to use and we
    get to keep everything we built before moving to the cluster; there is no duplication
    between cluster and non-cluster configurations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的问题是 Kubernetes 的做事方式，与我们使用 Docker 的方式完全不同，是否被它提供的优势所掩盖。或者，我们应该把赌注押在 Docker
    本身上，并希望它能解决这些问题？在你回答这些问题之前，看一看 Docker 的 1.9 版本。我们得到了持久卷和软件网络。我们还得到了 `unless-stopped`
    重启策略，可以管理我们不想要的故障。现在，Kubernetes 和 Swarm 之间的差距已经变少了。事实上，如今 Kubernetes 比 Swarm 拥有的优势已经很少了。Kubernetes
    提供的自动故障转移既是一种福音，又是一种诅咒。另一方面，Swarm 使用 Docker API，这意味着您可以保留所有命令和 Docker Compose
    配置。就我个人而言，我把赌注押在 Docker 引擎的改进和运行在其上的 Docker Swarm 上。这两者之间的差异很小。两者都已经准备好投入生产，但
    Swarm 更容易设置，更易于使用，并且在移动到集群之前，我们可以保留所有之前构建的内容；在集群和非集群配置之间没有重复。
- en: My recommendation is to go with Docker Swarm. Kubernetes is too opinionated,
    hard to set up, too different from Docker CLI/API and at the same time, besides
    automatic failover, it doesn't have real advantages over Swarm since the Docker
    release 1.9\. That doesn't mean that there are no features available in Kubernetes
    that are not supported by Swarm. There are feature differences in both directions.
    However, those differences are, in my opinion, not significant ones and the gap
    is getting smaller with each Docker release. Actually, for many use cases, there
    is no gap at all while Docker Swarm is easier to set up, learn and use.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是选择 Docker Swarm。Kubernetes 太过于主观，设置困难，与 Docker CLI/API 有很大不同，并且除了自动故障转移之外，在
    Docker 1.9 版本发布之后，它没有真正的优势。这并不意味着 Kubernetes 没有不受 Swarm 支持的功能。在两个方向上都存在功能差异。然而，在我看来，这些差异并不是重大的，而且随着每个
    Docker 版本的发布，这种差距正在变小。实际上，对于许多用例来说，根本就没有差距，而 Docker Swarm 更容易设置、学习和使用。
- en: Let us give Docker Swarm a spin and see how it fares.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来试试 Docker Swarm，并看看它的表现如何。
- en: Docker Swarm walkthrough
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm 演练
- en: 'To set up Docker Swarm, we need one of the service discovery tools. Consul
    served us well, and we''ll continue using it for this purpose. It is a great tool
    and works well with Swarm. We''ll set up three servers. One will act as master
    and the other two as cluster nodes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置 Docker Swarm，我们需要其中一种服务发现工具。Consul 在这方面表现良好，我们将继续使用它。它是一个很棒的工具，并且与 Swarm
    配合良好。我们将设置三个服务器。一个将充当主节点，另外两个将作为集群节点：
- en: '![Docker Swarm walkthrough](img/B05848_14_05.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演练](img/B05848_14_05.jpg)'
- en: Figure 14-5 – Docker Swarm cluster with Consul used for service discovery
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-5 – 使用 Consul 的 Docker Swarm 集群进行服务发现
- en: 'Swarm will use Consul instances to register and retrieve information about
    nodes and services deployed in them. Whenever we bring up a new node or halt an
    existing one, that information will be propagated to all Consul instances and
    reach Docker Swarm, which, in turn, will know where to deploy our containers.
    The master node will have Swarm master running. We''ll use its API to instruct
    Swarm what to deploy and what the requirements are (number of CPUs, the amount
    of memory, and so on). Node servers will have Swarm nodes deployed. Each time
    Swarm master receives an instruction to deploy a container, it will evaluate the
    current situation of the cluster and send instructions to one of the nodes to
    perform the deployment:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 将使用 Consul 实例来注册和检索有关节点和其上部署的服务的信息。每当我们启动一个新节点或停止一个现有节点时，该信息会传播到所有 Consul
    实例并传递到 Docker Swarm，后者会知道在哪里部署我们的容器。主节点将运行 Swarm 主程序。我们将使用其 API 来指示 Swarm 部署什么内容以及其要求是什么（如
    CPU 数量、内存大小等）。节点服务器将部署 Swarm 节点。每当 Swarm 主节点收到部署容器的指令时，它会评估当前集群的状态，并将指令发送到某个节点执行部署：
- en: '![Docker Swarm walkthrough](img/B05848_14_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演示](img/B05848_14_06.jpg)'
- en: Figure 14-6 – Docker Swarm cluster with one master and two nodes
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-6 – Docker Swarm 集群，包含一个主节点和两个节点
- en: 'We''ll start with the *spread* strategy that will deploy containers to a node
    that has the least number of containers running. Since, in the beginning, nodes
    will be empty, when given instruction to deploy the first container, Swarm master
    will propagate it to one of the nodes since both are empty at the moment:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从*分布*策略开始，该策略会将容器部署到运行容器数量最少的节点上。由于开始时所有节点都是空的，当给出部署第一个容器的指令时，Swarm 主节点会将容器部署到其中一个节点，因为此时两个节点都是空的：
- en: '![Docker Swarm walkthrough](img/B05848_14_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演示](img/B05848_14_07.jpg)'
- en: Figure 14-7 – Docker Swarm cluster with the first container deployed
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-7 – Docker Swarm 集群，已部署第一个容器
- en: 'When given the second instruction to deploy a container, Swarm master will
    decide to propagate it to the other Swarm node, since the first already has one
    container running:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当收到第二个容器部署指令时，Swarm 主节点会决定将其传播到另一个 Swarm 节点，因为第一个节点已经有一个容器在运行：
- en: '![Docker Swarm walkthrough](img/B05848_14_08.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演示](img/B05848_14_08.jpg)'
- en: Figure 14-8 – Docker Swarm cluster with the second container deployed
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-8 – Docker Swarm 集群，已部署第二个容器
- en: 'If we continue deploying containers, at some point our tiny cluster will become
    saturated, and something would need to be done before the server collapses:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续部署容器，过一段时间后我们的微型集群将会变得饱和，在服务器崩溃之前必须采取一些措施：
- en: '![Docker Swarm walkthrough](img/B05848_14_09.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演示](img/B05848_14_09.jpg)'
- en: Figure 14-9 – Docker Swarm cluster with all nodes full
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-9 – Docker Swarm 集群，所有节点已满
- en: 'The only thing we would need to do to increase the cluster capacity is to bring
    up a new server with Consul and Swarm node. As soon as such a node is brought
    up, its information would be propagated throughout Consul instances as well as
    to Swarm master. From that moment on, Swarm would have that node in the account
    for all new deployments. Since this server would start with no containers and
    we are using a simple *spread* strategy, all new deployments would be performed
    on that node until it reaches the same number of running containers as the others:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的唯一事情来增加集群容量，就是启动一台新的服务器并部署 Consul 和 Swarm 节点。只要这样的节点被启动，它的信息就会在所有 Consul
    实例之间传播，并传递到 Swarm 主节点。从那一刻起，Swarm 会将这个节点包含在内，作为所有新部署的一部分。由于这台服务器启动时没有容器，并且我们使用的是简单的*分布*策略，所有新部署的容器都会部署到这个节点，直到它运行的容器数量与其他节点相同：
- en: '![Docker Swarm walkthrough](img/B05848_14_10.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演示](img/B05848_14_10.jpg)'
- en: Figure 14-10 – Docker Swarm cluster with container deployed to the new node
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-10 – Docker Swarm 集群，容器部署到新节点
- en: 'Opposite scenario can be observed in case one node stops responding due to
    a failure. Consul cluster would detect that one of it''s members is not responding
    and propagate that information throughout the cluster, thus reaching Swarm master.
    From that moment on, all new deployments would be sent to one of the healthy nodes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个节点因故障停止响应的情况下，可以观察到相反的情况。Consul 集群会检测到其某个成员未响应，并将该信息传播到整个集群，从而到达 Swarm 主节点。从那一刻起，所有新部署的容器都会被发送到健康节点之一：
- en: '![Docker Swarm walkthrough](img/B05848_14_11.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm 演示](img/B05848_14_11.jpg)'
- en: Figure 14-11 – Docker Swarm cluster one node failed and containers distributed
    over healthy nodes
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-11 – Docker Swarm 集群，某个节点失败，容器分布到健康的节点上
- en: Let us dive into simple examples we just discussed. Later on, we'll explore
    other strategies as well as the ways Swarm behaves when certain constraints are
    set; CPU, memory and the like.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入讨论刚才提到的简单示例。稍后，我们将探索其他策略以及在设置某些约束时 Swarm 的行为方式；例如 CPU、内存等。
- en: Setting Up Docker Swarm
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Docker Swarm
- en: 'To see Docker Swarm in action, we''ll simulate an Ubuntu cluster. We''ll bring
    up the `cd` node that we''ll use for orchestration, one node that will act as
    Swarm master and two nodes that will form the cluster. Up to this point, we always
    used Ubuntu 14.04 LTS (long term support) since it is considered stable and supported
    for a long time. The next long term support version will be 15.04 LTS (not released
    at the time this book was written). Since some of the features we''ll explore
    later on, throughout this chapter, will need a relatively new Kernel, the `swarm`
    nodes will be running Ubuntu 15.04\. If you open the Vagrantfile, you''ll notice
    that Swarm master and nodes have the following line:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到 Docker Swarm 的工作过程，我们将模拟一个 Ubuntu 集群。我们将启动用于编排的 `cd` 节点，一个将充当 Swarm 主节点的节点以及两个将形成集群的节点。到目前为止，我们始终使用
    Ubuntu 14.04 LTS（长期支持），因为它被认为是稳定且长期受支持的。下一个长期支持版本将是 15.04 LTS（在书写本书时尚未发布）。由于我们稍后将要探索的一些功能需要相对较新的内核，`swarm`
    节点将运行 Ubuntu 15.04\. 如果您打开 Vagrantfile，您会注意到 Swarm 主节点和节点有以下行：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Vivid64` is the code name for Ubuntu 15.04.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Vivid64` 是 Ubuntu 15.04 的代号。'
- en: 'Let us bring up the nodes:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动节点：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With all the four nodes up and running, we can proceed, and create the Swarm
    cluster. As before, we''ll do the provisioning using Ansible:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四个节点都已启动并运行，我们可以继续创建 Swarm 集群。与以往一样，我们将使用 Ansible 进行配置：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let us use our time wisely and explore the `swarm.yml` playbook, while Ansible
    is provisioning our servers. The content of the `swarm.yml` file is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们明智地利用时间，探索 `swarm.yml` playbook，同时 Ansible 正在为我们的服务器进行配置。`swarm.yml` 文件的内容如下：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We started by setting up *docker*. Since this time we're using a different version
    of Ubuntu, we had to specify those differences as variables, so that the correct
    repository is used (`debian_version`), as well as to reload service configuration
    (`is_systemd`). We also set the `docker_cfg_dest` variable so that the configuration
    file is sent to the correct location.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从设置 *docker* 开始。由于这次我们使用的是不同版本的 Ubuntu，因此我们必须将这些差异作为变量进行指定，以便使用正确的存储库 (`debian_version`)，以及重新加载服务配置
    (`is_systemd`)。我们还设置了 `docker_cfg_dest` 变量，以便将配置文件发送到正确的位置。
- en: 'We have few more variables set in the `hosts/prod` file:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `hosts/prod` 文件中设置了几个额外的变量：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We'll explore `swarm_master` and `swarm_master_ip` later on. For now, please
    remember that they are defined in the `prod` file so that they can be applied
    (or omitted) based on the server type (master or node). Depending on whether we
    are provisioning master or node, Docker configuration file is `docker-swarm-master.service`
    or `docker-swarm-node.service`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将探索 `swarm_master` 和 `swarm_master_ip`。现在，请记住它们在 `prod` 文件中定义，以便根据服务器类型（主节点或节点）应用（或省略）它们。根据我们是配置主节点还是节点，Docker
    配置文件分别为 `docker-swarm-master.service` 或 `docker-swarm-node.service`。
- en: 'Let''s take a look at the `ExecStart` part of the master node Docker configuration
    (the rest is the same as the standard one that comes with the Docker package)
    defined in `roles/docker/templates/docker-swarm-master.service`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 `roles/docker/templates/docker-swarm-master.service` 中主节点 Docker 配置的
    `ExecStart` 部分（其余部分与 Docker 软件包提供的标准配置相同）：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We''re telling Docker to allow insecure registry on the IP/port where our private
    registry runs (located in the `cd` node). We''re also specifying that Swarm cluster
    information should be stored in Consul running on the same node, as well as that
    it should be advertised to the port `2375`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们告诉 Docker 在我们的私有注册表运行的 IP/端口上允许不安全注册。我们还指定 Swarm 集群信息应存储在同一节点上运行的 Consul 中，并且应该广播到端口
    `2375`：
- en: 'The node configuration defined in `roles/docker/templates/docker-swarm-node.service`
    has few more arguments:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `roles/docker/templates/docker-swarm-node.service` 中定义的节点配置有几个额外的参数：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Apart from those arguments that are the same as in the master node, we''re
    telling Docker to allow communication on the port `2375` (`-H tcp://0.0.0.0:2375`)
    as well as through the socket (`-H unix:///var/run/docker.sock`):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与主节点相同的参数外，我们还告诉 Docker 允许通过端口 `2375` (`-H tcp://0.0.0.0:2375`) 和通过套接字 (`-H
    unix:///var/run/docker.sock`) 进行通信：
- en: Both `master` and `node` configurations are following the standard settings
    recommended by the official Docker Swarm documentation when used in conjunction
    with Consul.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`master`和`node`配置都遵循官方Docker Swarm文档中推荐的标准设置，特别是与Consul一起使用时。'
- en: 'The rest of the roles used in the `swarm.yml` playbook are `consul`, `swarm`,
    and `registrator`. Since we already used and saw Consul and Registrator roles,
    we''ll explore only tasks belonging to the `swarm` role defined in the `roles/swarm/tasks/main.yml`
    file:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`swarm.yml` playbook中使用的其余角色有`consul`、`swarm`和`registrator`。由于我们已经使用并看到过Consul和Registrator角色，接下来我们只探索`roles/swarm/tasks/main.yml`文件中定义的`swarm`角色相关任务：'
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, running Swarm is pretty straightforward. All we have to do is
    run the `swarm` container and, depending on whether it's master or node, specify
    one command or the other. If server acts as a Swarm node, the command is `join
    --advertise={{ ip }}:2375 consul://{{ ip }}:8500/swarm` which, translated into
    plain words, means that it should join the cluster, advertise its existence on
    port `2375` and use Consul running on the same server for service discovery. The
    command that should be used in the Swarm master is even shorter; `manage consul://{{
    ip }}:8500/swarm`. All we had to do is specify that this Swarm container should
    be used to manage the cluster and, as with Swarm nodes, use Consul for service
    discovery.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，运行Swarm相当简单。我们需要做的就是运行`swarm`容器，并根据它是主节点还是普通节点，指定不同的命令。如果服务器作为Swarm节点运行，使用的命令是`join
    --advertise={{ ip }}:2375 consul://{{ ip }}:8500/swarm`，翻译成通俗的语言就是：它应该加入集群，在端口`2375`上发布自己的存在，并使用在同一服务器上运行的Consul进行服务发现。在Swarm主节点上使用的命令则更短；`manage
    consul://{{ ip }}:8500/swarm`。我们只需要指定这个Swarm容器应该用来管理集群，并且与Swarm节点一样，使用Consul进行服务发现。
- en: Hopefully, the playbook we run earlier finished its execution. If it didn't,
    grab a coffee and continue reading once it's done. We're about to check whether
    our Swarm cluster is working as expected.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们之前运行的playbook已经完成执行。如果没有，去喝杯咖啡，等它执行完再继续阅读。接下来我们将检查我们的Swarm集群是否按预期工作。
- en: Since we are still inside the `cd` node, we should tell Docker CLI to use a
    different host.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仍然在`cd`节点内，我们应该告诉Docker CLI使用不同的主机。
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the Docker client running on `cd` and using the `swarm-master` node as
    a host, we can control the Swarm cluster remotely. For a start, we can check the
    information of our cluster:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cd`上运行Docker客户端，并使用`swarm-master`节点作为主机，我们可以远程控制Swarm集群。首先，我们可以查看集群的相关信息：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Isn't this great? With a single command, we have an overview of the whole cluster.
    While, at this moment, we have only two servers (`swarm-node-1` and `swarm-node-2`),
    if there would be hundred, thousand, or even more nodes, `docker info` would provide
    information about all of them. In this case, we can see that four containers are
    running and four images. That is correct since each node is running Swarm and
    Registrator containers. Further on, we can see the `Role`, `Strategy`, and `Filters`.
    Next in the line are nodes that constitute our cluster followed by information
    about each of them. We can see how many containers each is running (currently
    two), how many CPUs and memory is reserved for our containers, and labels associated
    with each node. Finally, we can see the total number of CPUs and memory of the
    whole cluster. Everything presented by `docker info` acts not only as information
    but also a functionality of the Swarm cluster. For now, please note that all this
    information is available for inspection. Later on, we'll explore how to utilize
    it for our benefit.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是很棒吗？只需一个命令，我们就能查看整个集群的概览。虽然目前我们只有两台服务器（`swarm-node-1`和`swarm-node-2`），但如果有一百台、一千台甚至更多的节点，`docker
    info`会提供所有节点的信息。在这种情况下，我们可以看到四个容器正在运行，并且有四个镜像。这是正确的，因为每个节点都在运行Swarm和Registrator容器。接下来，我们可以看到`Role`、`Strategy`和`Filters`。然后是构成集群的各个节点，后面是每个节点的信息。我们可以看到每个节点正在运行多少个容器（当前是两个），为容器预留了多少CPU和内存，以及与每个节点关联的标签。最后，我们还可以看到整个集群的CPU和内存总数。`docker
    info`展示的所有信息不仅是数据，也是Swarm集群的功能。现在请注意，所有这些信息都可以用来检查。稍后我们会探索如何利用这些信息来为我们带来更多好处。
- en: 'The best part of Docker Swarm is that it shares the same API as Docker, so
    all the commands we already used throughout this book are available. The only
    difference is that instead of operating Docker on a single server, with Swarm
    we are operating a whole cluster. For example, we can list all images and processes
    throughout the entire Swarm cluster:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 的最大优点在于它与 Docker 使用相同的 API，因此我们在本书中已经使用过的所有命令都可以继续使用。唯一的区别是，使用
    Swarm 时，我们操作的是整个集群，而不是单一服务器。例如，我们可以列出整个 Swarm 集群中的所有镜像和进程：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'By running `docker images` and `docker ps -a` we can observe that there are
    two images pulled into the cluster and four containers running (two containers
    on each of the two servers). The only visual difference is that names of running
    containers are prefixed with the name of the server they are running on. For example,
    the container named `registrator` is presented as `swarm-node-1/registrator` and
    `swarm-node-2/registrator`. The combined output of those two commands is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行 `docker images` 和 `docker ps -a`，我们可以观察到集群中拉取了两个镜像，并且有四个容器在运行（每台服务器上各有两个容器）。唯一的视觉差异是，运行的容器名称前面会加上它们运行的服务器的名称。例如，名为
    `registrator` 的容器显示为 `swarm-node-1/registrator` 和 `swarm-node-2/registrator`。这两个命令的组合输出如下：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we know that Docker commands work in the same way when run against
    the remote server (`swarm-master`) and can be used to control the whole cluster
    (`swarm-node-1` and `swarm-nod` `e-2`), let's try to deploy our `books-ms` service.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，在远程服务器（`swarm-master`）上运行 Docker 命令时，它的工作方式与本地一样，并且可以用来控制整个集群（`swarm-node-1`
    和 `swarm-node-2`）。让我们尝试部署我们的 `books-ms` 服务。
- en: Deploying with Docker Swarm
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 部署
- en: 'We''ll start by repeating the same deployment process we did before, but, this
    time, we''ll be sending commands to the Swarm master:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从重复之前的部署过程开始，但这次我们将向 Swarm 主节点发送命令：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We cloned the `books-ms` repository and, now, we can run the service through
    Docker Compose:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们克隆了 `books-ms` 仓库，现在可以通过 Docker Compose 运行该服务：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Since the `app` target is linked with the `db`, Docker Compose run both. So
    far, everything looks the same as if we run the same command without Docker Swarm.
    Let us take a look at the processes that were created:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `app` 目标与 `db` 目标相连，Docker Compose 一并启动了它们。目前为止，效果与我们在没有 Docker Swarm 的情况下运行相同命令的情况没有什么不同。让我们来看一下创建的进程：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see, both containers are running on `swarm-node-2`. In your case,
    it could be `swarm-node-1`. We did not make the decision where to deploy the containers.
    Swarm did that for us. Since we are using the default strategy that, without specifying
    additional constraints, runs containers on a server that has the least number
    of them running. Since both `swarm-node-1` and `swarm-node-2` were equally empty
    (or full), Swarm had an easy choice and could have placed containers on either
    one of those servers. In this case, it chose `swarm-node-2`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这两个容器都在 `swarm-node-2` 上运行。在你的情况下，它可能是 `swarm-node-1`。我们并没有决定在哪里部署容器，Swarm
    为我们做了这个决定。因为我们使用的是默认策略，它会在没有指定额外约束的情况下，将容器部署到正在运行容器数量最少的服务器上。由于 `swarm-node-1`
    和 `swarm-node-2` 都是空闲状态（或都已满），Swarm 很容易做出选择，并可以将容器部署到其中任何一台服务器上。在这个例子中，它选择了 `swarm-node-2`。
- en: The problem with the deployment we just performed is that the two targets (`app`
    and `db`) are linked. In such a case, Docker has no other option but to place
    both containers on the same server. That, in a way, defies the objective we're
    trying to accomplish. We want to deploy containers to the cluster and, as you'll
    soon discover, be able to scale them easily. If both containers need to be run
    on the same server, we are limiting Swarm's ability to distribute them properly.
    In this example, those two containers would be better of running on separate servers.
    If, before deploying those containers, both servers had the equal number of containers
    running, it would make more sense to run the `app` on one and the `db` on the
    other. That way we'd distribute resource usage much better. As it is now, the
    `swarm-node-2` needs to do all the work, and the `swarm-node-1` is empty. The
    first thing we should do is to get rid of the link.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才执行的部署存在一个问题，即两个目标（`app` 和 `db`）是相互链接的。在这种情况下，Docker 别无选择，只能将这两个容器放在同一台服务器上。这从某种意义上说违背了我们想要实现的目标。我们希望将容器部署到集群中，并且正如你很快会发现的那样，能够轻松地扩展它们。如果这两个容器必须运行在同一台服务器上，那么我们就限制了
    Swarm 正确分布它们的能力。在这个例子中，这两个容器最好运行在不同的服务器上。如果在部署这些容器之前，两个服务器上运行的容器数量相等，那么将 `app`
    运行在一个服务器上，将 `db` 运行在另一个服务器上会更有意义。这样我们就能更好地分配资源使用。现在，`swarm-node-2` 必须承担所有工作，而
    `swarm-node-1` 是空闲的。我们首先应该做的是去除链接。
- en: 'Let''s stop the containers we''re running and start over:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停止正在运行的容器并重新开始：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: That was another example of advantages Swarm provides. We sent the `stop` and
    `rm` commands to the Swarm master and it located containers for us. From now on,
    all the behavior will be the same, in the sense that, through the Swarm master,
    we'll treat the whole cluster as one single unit oblivious of the specifics of
    each server.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是 Swarm 提供的另一个优势示例。我们将 `stop` 和 `rm` 命令发送给 Swarm 主节点，它为我们定位了容器。从现在起，所有行为将是相同的，意味着通过
    Swarm 主节点，我们将把整个集群当作一个单一的单位来处理，而不关心每台服务器的具体情况。
- en: Deploying with Docker Swarm without Links
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在没有链接的情况下使用 Docker Swarm 部署
- en: 'To deploy containers to Docker Swarm cluster properly, we''ll use a different
    file for Docker Compose definition; `docker-compose-no-links.yml`. The targets
    are as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确地将容器部署到 Docker Swarm 集群中，我们将使用一个不同的文件来定义 Docker Compose；`docker-compose-no-links.yml`。目标如下：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The only significant difference between `app` and `db` targets defined in `docker-compose.yml`
    and `docker-compose-swarm.yml` is that the latter does not use links. As you will
    see soon, this will allow us to distribute freely containers inside the cluster.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `docker-compose.yml` 和 `docker-compose-swarm.yml` 中定义的 `app` 和 `db` 目标之间唯一显著的区别是，后者没有使用链接。正如你很快会看到的，这将允许我们在集群内自由地分配容器。
- en: Let's take a look at what happens if we bring up *db* and *app* containers without
    the link.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们在没有链接的情况下启动 *db* 和 *app* 容器会发生什么。
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of the `docker ps` command is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker ps` 命令的输出如下：'
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, this time, Swarm decided to place each container on a different
    server. It brought up the first container and, since from that moment on one server
    had more containers than the other, it choose to bring up the second on the other
    node.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这次，Swarm 决定将每个容器放置在不同的服务器上。它启动了第一个容器，并且由于从那时起一台服务器上的容器数量多于另一台，它选择在另一个节点上启动第二个容器。
- en: 'By removing linking between containers, we solved one problem but introduced
    another. Now our containers can be distributed much more efficiently, but they
    cannot communicate with each other. We can address this issue by using a `proxy`
    service (nginx, HAProxy, and so on). However, our `db` target does not expose
    any ports to the outside world. A good practice is to expose only ports of services
    that are publicly accessible. For that reason, the `app` target exposes port `8080`
    and the `db` target doesn''t expose any. The `db` target is meant to be used internally,
    and only by the `app`. Since the Docker release 1.9, linking can be considered
    deprecated, for a new feature called *networking*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过移除容器之间的链接，我们解决了一个问题，但引入了另一个问题。现在我们的容器可以更有效地分布，但它们无法相互通信。我们可以通过使用 `proxy` 服务（如
    nginx、HAProxy 等）来解决这个问题。然而，我们的 `db` 目标没有对外暴露任何端口。一个好的做法是仅暴露那些对外公开的服务端口。因此，`app`
    目标暴露了端口 `8080`，而 `db` 目标没有暴露任何端口。`db` 目标仅供内部使用，并且仅供 `app` 使用。从 Docker 1.9 版本开始，链接可以被视为已废弃，因为出现了一个名为
    *networking* 的新特性：
- en: 'Let''s remove the containers and try to bring them up networking enabled:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除容器并尝试启用网络功能启动它们：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Deploying with Docker Swarm and Docker Networking
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 和 Docker 网络部署
- en: 'At the time I was writing this chapter, Docker introduced the new release 1.9\.
    It is, without a doubt, the most significant release, since version 1.0\. It gave
    us two long awaited features; multi-host networking and persistent volumes. Networking
    makes linking deprecated and is the feature we need to connect containers across
    multiple hosts. There is no more need for proxy services to connect containers
    internally. That is not to say that proxy is not useful, but that we should use
    a proxy as a public interface towards our services and networking for connecting
    containers that form a logical group. The new Docker networking and proxy services
    have different advantages and should be used for different use cases. Proxy services
    provide load balancing and can control the access to our services. Docker networking
    is a convenient way to connect separate containers that form a single service
    and reside on the same network. A typical use case for Docker networking would
    be a service that requires a connection to a database. We can connect those two
    through networking. Furthermore, the service itself might need to be scaled and
    have multiple instances running. A proxy service with load balancer should fulfill
    that requirement. Finally, other services might need to access this service. Since
    we want to take advantage of load balancing, that access should also be through
    a proxy:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我写这章时，Docker 推出了新的 1.9 版本。这无疑是自 1.0 版本以来最重要的版本。它给我们带来了两个期待已久的功能：多主机网络和持久化存储卷。网络功能使得链接功能不再被推荐使用，这是我们连接跨多个主机的容器所需要的功能。现在不再需要代理服务来进行容器内部的连接。这并不是说代理不重要，而是我们应该将代理用作面向外部服务的公共接口，而使用网络来连接构成逻辑组的容器。新的
    Docker 网络和代理服务有不同的优点，应该用于不同的场景。代理服务提供负载均衡，并且可以控制对我们服务的访问。Docker 网络是一种方便的方式，用于连接在同一网络上、构成单一服务的不同容器。Docker
    网络的典型应用场景是一个需要连接数据库的服务。我们可以通过网络将它们连接起来。此外，服务本身可能需要扩展并运行多个实例。一个带负载均衡器的代理服务应该满足这一需求。最后，其他服务可能需要访问该服务。由于我们希望利用负载均衡，因此这种访问也应该通过代理进行：
- en: '![Deploying with Docker Swarm and Docker Networking](img/B05848_14_12.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Docker Swarm 和 Docker 网络部署](img/B05848_14_12.jpg)'
- en: Figure 14-12 – Multi-host networking combined with a proxy and load balancing
    service
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-12 – 多主机网络与代理和负载均衡服务的结合
- en: The figure 14-12 represents one common use case. We have a scaled service with
    two instances running on `nodes-01` and `nodes-03`. All communication to those
    services is performed through a proxy service that takes care of load balancing
    and security. Any service (be it external or internal) that wants to access our
    service needs to go through the proxy. Internally, the service uses the database.
    The communication between the service instances and the database is internal and
    performed through the multi-host network. This setting allows us to scale easily
    within the cluster while keeping internal all communication between containers
    that compose a single service. In other words, all communication between containers
    that compose a single service is done through networking while the communication
    between services is performed through the proxy.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-12 展示了一个常见的应用场景。我们有一个扩展的服务，在`nodes-01`和`nodes-03`上运行了两个实例。所有对这些服务的通信都通过一个代理服务进行，代理服务负责负载均衡和安全性。任何想要访问我们服务的服务（无论是外部的还是内部的）都需要通过代理。内部服务使用数据库。服务实例和数据库之间的通信是内部的，并通过多主机网络进行。这种设置使我们能够在集群内轻松扩展，同时保持容器之间的所有内部通信仅限于构成单一服务的容器之间。换句话说，构成单一服务的容器之间的所有通信都通过网络进行，而服务之间的通信则通过代理进行。
- en: 'There are different ways to create a multi-host network. We can set up the
    network manually:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 创建多主机网络有不同的方法。我们可以手动设置网络：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the `network ls` command is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`network ls`命令的输出如下：'
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can see that one of the networks is `my-network` we created earlier. It
    spans the whole Swarm cluster and we can use it with the `--net` argument:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到其中一个网络是我们之前创建的`my-network`。它跨越了整个 Swarm 集群，我们可以使用`--net`参数来使用它：
- en: '[PRE24]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We started two containers that compose a single service; `books-ms` is the API
    that communicates with `books-ms-db` that acts as a database. Since both containers
    had the `--net my-network` argument, they both belong to the `my-network` network.
    As a result, Docker updated hosts file providing each container with an alias
    that can be used for internal communication.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动了两个组成单一服务的容器；`books-ms`是与`books-ms-db`通信的API，后者充当数据库。由于两个容器都具有`--net my-network`参数，它们都属于`my-network`网络。因此，Docker更新了hosts文件，为每个容器提供了一个别名，可用于内部通信。
- en: 'Let''s enter the `books-ms` container and take a look at the hosts file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入`books-ms`容器并查看主机文件：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the `exec` command is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`exec`命令的输出如下所示：'
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The interesting part of the `hosts` file are the last two entries. Docker detected
    that the `books-ms-db` container uses the same network as the `books-ms` container,
    and updated the `hosts` file by adding `books-ms-db` and `books-ms-db.my-network`
    aliases. If some convention is used, it is trivial to code our services in a way
    that they use aliases like that one to communicate with resources located in a
    separate container (in this case with the database).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`hosts`文件的有趣部分是最后两个条目。Docker检测到`books-ms-db`容器使用与`books-ms`容器相同的网络，并通过添加`books-ms-db`和`books-ms-db.my-network`别名来更新`hosts`文件。如果使用某种约定，编写代码以使服务使用类似的别名与位于单独容器中的资源通信是微不足道的（在这种情况下是与数据库通信）。'
- en: 'We also passed an environment variable `DB_HOST` to the `book-ms`. That indicates
    to our service which host to use to connect to the database. We can see this by
    outputting environments of the container:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还向`book-ms`传递了一个名为`DB_HOST`的环境变量。这表明我们的服务将使用哪个主机来连接数据库。我们可以通过输出容器的环境变量来查看这一点：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of the command is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下所示：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you can see, one of the environment variables is `DB_HOST` with the value
    `books-ms-db`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，其中一个环境变量是值为`books-ms-db`的`DB_HOST`。
- en: What we have right now is Docker networking that created hosts alias `books-ms-db`
    pointing to the IP of the network Docker created. We also have an environment
    variable `DB_HOST` with value `books-ms-db`. The code of the service uses that
    variable to connect to the database.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有由Docker网络创建的主机别名`books-ms-db`指向的IP。我们还有一个环境变量`DB_HOST`，其值为`books-ms-db`。服务的代码使用该变量连接到数据库。
- en: As expected, we can specify `network` as part of our Docker Compose specification.
    Before we try it out, let's remove those two containers and the network.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们可以在Docker Compose规范中指定`network`。在尝试之前，让我们删除这两个容器和网络。
- en: '[PRE29]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This time, we''ll run containers through Docker Compose. We''ll use the `net`
    argument inside `docker-compose-swarm.yml` and, in that way, do the same process
    as we did earlier. The alternative would be to use new Docker Compose argument
    `--x-networking` that would create the network for us but, at this moment, it
    is in the experimental stage and not entirely reliable. Before we proceed, let
    us take a quick look at the relevant targets inside the `docker-compose-swarm.yml`
    file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将通过Docker Compose运行容器。我们将在`docker-compose-swarm.yml`中使用`net`参数，并以与之前相同的方式执行操作。另一种方法是使用新的Docker
    Compose参数`--x-networking`，它会为我们创建网络，但目前处于试验阶段并不完全可靠。在继续之前，让我们快速查看`docker-compose-swarm.yml`文件中的相关目标：
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The only important difference is the addition of the `net` argument. Everything
    else is, more or less, the same as in many other targets we explored by now.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一重要的区别是添加了`net`参数。其他方面与我们目前探索的许多其他目标基本相同。
- en: 'Let us create the network and run our containers through Docker Compose:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过Docker Compose创建网络并运行我们的容器：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of the command we just run is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚运行的命令的输出如下所示：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Before creating the services `app` and `db`, we created a new network called
    `books-ms`. The name of the network is the same as the value of the *net* argument
    specified in the `docker-compose-swarm.yml` file.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建`app`和`db`服务之前，我们创建了一个名为`books-ms`的新网络。网络的名称与`docker-compose-swarm.yml`文件中指定的*net*参数的值相同。
- en: 'We can confirm that the network was created by running the `docker network
    ls` command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行`docker network ls`命令，我们可以确认网络已创建：
- en: '[PRE33]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the `overlay` network `books-ms` has been created.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`overlay`网络`books-ms`已创建。
- en: 'We can also double check that the `hosts` file inside containers has been updated:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以再次检查容器内的`hosts`文件是否已更新：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下所示：
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, let''s see how did Swarm distribute our containers:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看 Swarm 是如何分配我们的容器的：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Swarm deployed the `app` container to the `swarm-node-1` and the `db` container
    to the `swarm-node-2`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 将 `app` 容器部署到 `swarm-node-1`，将 `db` 容器部署到 `swarm-node-2`。
- en: 'Finally, let''s test whether the `book-ms` service is working properly. We
    do not know where did Swarm deploy the container nor which port is exposed. Since
    we do not (yet) have a proxy, we''ll retrieve the IP and the port of the service
    from Consul, send a PUT request to store some data in the database residing in
    a different container and, finally, send a GET request to check whether we can
    retrieve the record. Since we do not have a proxy service that would make sure
    that requests are redirected to the correct server and port, we''ll have to retrieve
    the IP and the port from Consul:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们测试一下 `book-ms` 服务是否正常工作。我们不知道 Swarm 将容器部署到了哪里，也不知道暴露了哪个端口。由于我们（还）没有代理服务，我们将从
    Consul 获取服务的 IP 和端口，发送 PUT 请求将数据存储到不同容器中的数据库中，最后发送 GET 请求检查是否能检索到记录。由于我们没有代理服务来确保请求被重定向到正确的服务器和端口，我们必须从
    Consul 获取 IP 和端口：
- en: '[PRE39]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If the service could not communicate with the database located on a different
    node, we would not be able to put, nor to get data. Networking between containers
    deployed to separate servers worked! All we had to do is use an additional argument
    with Docker Compose (*net*) and make sure that the service code utilizes information
    from the hosts file.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务无法与位于不同节点的数据库通信，我们将无法进行数据的插入和读取。部署到不同服务器上的容器之间的网络连接正常！我们所需要做的就是在 Docker
    Compose 中使用额外的参数（*net*），并确保服务代码使用主机文件中的信息。
- en: Another advantage of Docker networking is that, if one container stops working,
    we can redeploy it (potentially to a separate server) and, assuming that the service
    can handle the temporary connection loss, continue using it as if nothing happened.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 网络的另一个优点是，如果某个容器停止工作，我们可以重新部署它（可能部署到另一个服务器），并且假设服务能够处理临时的连接中断，我们可以像什么都没发生一样继续使用它。
- en: Scaling Services with Docker Swarm
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 扩展服务
- en: 'As you''ve already seen, scaling with Docker Compose is easy. While examples
    we run by now were limited to a single server, with Docker Swarm we can extend
    scaling to the whole cluster. Now that we have one instance of `books-ms` running,
    we can scale it to, let''s say, three:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，使用 Docker Compose 进行扩展非常简单。虽然到目前为止我们运行的例子都局限于单台服务器，但使用 Docker Swarm
    我们可以将扩展范围扩展到整个集群。现在我们有一个 `books-ms` 实例在运行，我们可以将其扩展到三个实例：
- en: '[PRE40]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output of the `ps` command is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`ps` 命令的输出如下：'
- en: '[PRE41]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can see that Swarm continues distributing containers evenly. Each node is
    currently running two containers. Since we asked Docker Swarm to scale the `books-ms`
    containers to three, two of them are now running alone and the third one is deployed
    together with the database. Later on, when we start working on the automation
    of the deployment to the Docker Swarm cluster, we'll also make sure that all the
    instances of the service are properly set in the proxy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 Swarm 继续均匀地分配容器。每个节点当前运行两个容器。由于我们请求 Docker Swarm 将 `books-ms` 容器扩展到三个，所以现在其中两个容器独立运行，第三个容器与数据库一起部署。以后，当我们开始自动化部署到
    Docker Swarm 集群时，我们还会确保所有服务实例正确设置到代理中。
- en: 'For the future reference, we might want to store the number of instances in
    Consul. Later on, it might come in handy if, for example, we want to increase
    or decrease that number:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以后参考，我们可能想将实例数量存储在 Consul 中。以后，当我们想增加或减少实例数量时，这会非常有用：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Since we told Swarm to scale (down) to one instance and, at that moment, there
    were three of them running, Swarm removed instances two and three leaving the
    system with only one running. That can be observed from the output of the `docker
    ps` command that is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们指示 Swarm 将实例数量缩放（减少）到一个，而当时有三个实例在运行，Swarm 删除了第二和第三个实例，最终系统只剩下一个实例。这可以从 `docker
    ps` 命令的输出中看到，输出如下：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We descaled and went back to the beginning, with one instance of each target
    running.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了缩容并回到了最初的状态，每个目标运行一个实例。
- en: 'We are about to explore few more Swarm options. Before we proceed, let us stop
    and remove running containers, and start over:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将探讨更多的 Swarm 选项。在继续之前，让我们停止并删除当前运行的容器，然后重新开始：
- en: '[PRE44]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Scheduling Containers Depending on Reserved CPUs and Memory
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据保留的 CPU 和内存调度容器
- en: Up until now, Swarm was scheduling deployments to servers that have the least
    number of them running. That is the default strategy applied when there is no
    other constraint specified. It is often not realistic to expect that all containers
    require equal access to resources. We can further refine Swarm deployments by
    giving hints of what we expect from containers. For example, we can specify how
    many CPUs we need for a particular container. Let's give it a spin.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Swarm 一直在将部署调度到运行容器最少的服务器上。这是没有指定其他约束条件时应用的默认策略。通常情况下，不现实地期望所有容器都能平等访问资源。我们可以通过向
    Swarm 提供容器期望的提示来进一步优化部署。例如，我们可以指定某个容器需要多少个 CPU。让我们试试看。
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The relevant parts of the output of the command are as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 命令输出的相关部分如下：
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Even though we are already running two containers on each node (`Registrator`
    and `Swarm`), there are no reserved CPUs, nor reserved memory. When we run those
    containers, we did not specify that CPU or memory should be reserved.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们每个节点已经运行了两个容器（`Registrator` 和 `Swarm`），但没有预留 CPU，也没有预留内存。当我们运行这些容器时，并未指定需要预留
    CPU 或内存。
- en: Let's try running Mongo DB with one CPU reserved for the process. Keep in mind
    that this is only a hint and will not prevent other containers already deployed
    on those servers from using that CPU.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试运行 Mongo DB，并为进程预留一个 CPU。请记住，这只是一个提示，并不会阻止已经部署在这些服务器上的其他容器使用该 CPU。
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Since each node has only one CPU assigned, we could not assign more than one.
    The relevant parts of the output of the `docker info` command are as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个节点只有一个 CPU 被分配，我们无法分配更多。`docker info` 命令的相关部分输出如下：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This time, `swarm-node-1` has one (out of one) CPU reserved. Since there are
    no more available CPUs on that node, if we repeat the process and bring up one
    more Mongo DB with the same constraint, Swarm will have no option but to deploy
    it to the second node. Let''s try it out:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，`swarm-node-1` 保留了一个（共一个）CPU。由于该节点没有更多的 CPU 可用，如果我们重复该过程并启动另一个 Mongo DB，且设置相同的约束，Swarm
    将别无选择，只能将其部署到第二个节点。让我们试试看：
- en: '[PRE49]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The relevant parts of the output of the `ps` command are as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`ps` 命令输出的相关部分如下：'
- en: '[PRE50]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This time, both nodes have all the CPUs reserved.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，两个节点都预留了所有的 CPU。
- en: 'We can take a look at the processes and confirm that both DBs are indeed running:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看进程，并确认两个数据库确实正在运行：
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE52]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Indeed, both containers are running, one on each node.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，两个容器都在运行，每个节点一个。
- en: 'Let''s see what happens if we try to bring up one more container that requires
    one CPU:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看，如果我们尝试启动一个需要一个 CPU 的容器，会发生什么：
- en: '[PRE53]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This time, Swarm returned the following error message:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，Swarm 返回了以下错误消息：
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We requested deployment of a container that requires one CPU, and Swarm got
    back to us saying that there are no available nodes that fulfill that requirement.
    Before we proceed to explore other constraints, please bear in mind that *CPU
    Shares* do not work in the same way with Swarm as when applied to a Docker running
    on a single server. For more information regarding such a case, please consult
    [https://docs.docker.com/engine/reference/run/#cpu-share-constraint](https://docs.docker.com/engine/reference/run/#cpu-share-constraint)
    page for more information.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请求部署一个需要一个 CPU 的容器，Swarm 回复说没有可用的节点能满足这个要求。在我们继续探索其他约束之前，请记住，*CPU Shares*
    在 Swarm 中的工作方式与在单个服务器上运行的 Docker 中不同。有关此类情况的更多信息，请参阅 [https://docs.docker.com/engine/reference/run/#cpu-share-constraint](https://docs.docker.com/engine/reference/run/#cpu-share-constraint)
    页面。
- en: 'Let''s remove our containers and start over:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们移除容器并重新开始：
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can also use memory as a constraint. For example, we can direct Swarm to
    deploy a container reserving one CPU and one GB of memory:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将内存作为一种约束。例如，我们可以指示 Swarm 部署一个保留一个 CPU 和一 GB 内存的容器：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output of the `docker info` command is as follows (limited to relevant
    parts):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker info` 命令的输出如下（仅限相关部分）：'
- en: '[PRE57]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This time not only that one CPU is reserved, but almost all of the memory as
    well. While we could not demonstrate much when using CPU constraints, since our
    nodes have only one each, with memory we have a bit bigger margin to experiment.
    For example, we can bring up three instances of Mongo DB with 100 MB reserved
    for each:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这次不仅保留了一个 CPU，而且几乎所有的内存也被保留。尽管我们使用 CPU 约束时未能演示出太多内容，因为我们的节点每个只有一个，但在内存方面我们有更大的操作余地。例如，我们可以启动三个
    Mongo DB 实例，每个实例预留 100 MB 内存：
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output of the `docker info` command is as follows (limited to relevant
    parts):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker info`命令的输出如下（只列出相关部分）：'
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'It is obvious that all of those three containers were deployed to the `swarm-node-2`.
    Swarm realized that the second node had less available memory on the `swarm-node-1`
    and decided to deploy the new container to the `swarm-node-2`. That decision was
    repeated two more times since the same constraints were used. As a result, the
    `swarm-node-2` now has all those three containers running and 300 MB of memory
    reserved. We can confirm that by checking the running processes:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，所有这三个容器都被部署到`swarm-node-2`。Swarm意识到第二个节点在`swarm-node-1`上的可用内存较少，决定将新容器部署到`swarm-node-2`。由于使用了相同的限制，这一决策被重复了两次。因此，`swarm-node-2`现在运行着这三个容器，并预留了300MB的内存。我们可以通过检查运行中的进程来确认这一点：
- en: '[PRE60]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE61]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: There are many other ways we can give hints to Swarm where to deploy containers.
    We won't explore all of them. I invite you to check Docker documentation for Strategies
    ([https://docs.docker.com/swarm/scheduler/strategy/](https://docs.docker.com/swarm/scheduler/strategy/))and
    Filters ([https://docs.docker.com/swarm/scheduler/filter/](https://docs.docker.com/swarm/scheduler/filter/)).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式向Swarm提供容器部署提示，但我们不会探索所有方法。我邀请你查看Docker文档中的策略([https://docs.docker.com/swarm/scheduler/strategy/](https://docs.docker.com/swarm/scheduler/strategy/))和过滤器([https://docs.docker.com/swarm/scheduler/filter/](https://docs.docker.com/swarm/scheduler/filter/))。
- en: At this moment, we have more than enough knowledge to attempt deployment automation
    to the Docker Swarm cluster.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此刻，我们已经掌握了足够的知识，可以尝试将部署自动化到Docker Swarm集群。
- en: 'Before we proceed, let''s remove the containers we run until now:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们先删除到目前为止运行的容器：
- en: '[PRE62]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Automating Deployment with Docker Swarm and Ansible
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Swarm和Ansible自动化部署
- en: We are already familiar with Jenkins Workflow, and it should be relatively easy
    to extend this knowledge to Docker Swarm deployments.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉Jenkins工作流，应该相对容易将这些知识扩展到Docker Swarm的部署上。
- en: 'First things first. We need to provision our `cd` node with Jenkins:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，最重要的是。我们需要为`cd`节点配置Jenkins：
- en: '[PRE63]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The two playbooks deployed the familiar Jenkins instance with two nodes. This
    time, the slaves we are running are `cd` and `swarm-master`. Among other jobs,
    the playbook created the `books-ms-swarm` job based on the `Multibranch Workflow`.
    The only difference between this and the other multibranch jobs we used earlier
    is in the `Include branches` filter that, this time, is set to `swarm`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个剧本部署了一个熟悉的Jenkins实例，包含两个节点。这次，我们运行的从节点是`cd`和`swarm-master`。在其他任务中，剧本基于`Multibranch
    Workflow`创建了`books-ms-swarm`任务。与之前使用的其他多分支任务的唯一区别在于`Include branches`过滤器，这次设置为`swarm`：
- en: '![Automating Deployment with Docker Swarm and Ansible](img/B05848_14_13.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![使用Docker Swarm和Ansible自动化部署](img/B05848_14_13.jpg)'
- en: Figure 14-13 – Configuration screen of the books-ms-swarm Jenkins job
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-13 – books-ms-swarm Jenkins任务的配置屏幕
- en: Let's index the branches and let the job run while we explore the Jenkinsfile
    located in the `books-ms swarm` branch.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们索引分支并让任务运行，同时我们可以查看位于`books-ms swarm`分支中的Jenkinsfile。
- en: Please open the `books-ms-swarm` job and click **Branch Indexing** followed
    by **Run Now**. Since there is only one branch matching the specified filter,
    Jenkins will create one subproject called `swarm` and start building it. If you
    are curious about the progress of the build, you can monitor the progress by opening
    the build console.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开`books-ms-swarm`任务并点击**Branch Indexing**，然后点击**Run Now**。由于只有一个分支符合指定的过滤器，Jenkins将创建一个名为`swarm`的子项目并开始构建。如果你对构建进度感兴趣，可以通过打开构建控制台来监视进展。
- en: Examining the Swarm Deployment Playbook
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查Swarm部署剧本
- en: 'The content of the Jenkins workflow defined in the Jenkinsfile is as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkinsfile中定义的Jenkins工作流内容如下：
- en: '[PRE64]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: I added comments to the modified and added lines (when compared with Jenkinsfile
    from the previous chapter) so that we can explore the differences from the Jenkinsfile
    defined in the blue-green branch.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我在修改和新增的行上添加了注释（与上一章中的Jenkinsfile进行对比），这样我们可以探索与蓝绿分支中定义的Jenkinsfile的差异。
- en: The variables `prodIp` and `proxyIp` have been changed to point to the `swarm-master`
    node. This time, we are using two Ansible playbooks to provision the cluster.
    The `swarmPlaybook` variable holds the name of the playbook that configures the
    whole `Swarm` cluster while the `proxyPlaybook` variable references the playbook
    in charge of setting up the `nginx` proxy on the `swarm-master` node. In real
    world situations, Swarm master and the proxy service should be separated but,
    in this case, I opted against an additional VM to save a bit of resources on your
    laptop. Finally, the `instances` variable with the default value of `1` is added
    to the script. We'll explore its usage shortly.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`prodIp`和`proxyIp`已经修改为指向`swarm-master`节点。这一次，我们使用了两个Ansible playbook来配置集群。`swarmPlaybook`变量保存配置整个`Swarm`集群的playbook名称，而`proxyPlaybook`变量引用的是负责在`swarm-master`节点上设置`nginx`代理的playbook。在现实情况下，Swarm主节点和代理服务应当分开，但在这里，我选择不使用额外的虚拟机，以节省你笔记本上的一些资源。最后，`instances`变量（默认值为`1`）被添加到了脚本中。我们将很快探讨它的使用。
- en: 'The only truly notable difference is the usage of the `deploySwarm` function
    that replaces `deployBG`. It is one more utility function defined in the `workflow-util.groovy`
    script. Its contents are as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一真正显著的区别是使用了`deploySwarm`函数来替代`deployBG`。这是在`workflow-util.groovy`脚本中定义的另一个工具函数。其内容如下：
- en: '[PRE65]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As before, we start by pulling the latest container from the registry. The new
    addition is the creation of a Docker network. Since it can be created only once,
    and all subsequent attempts will result in an error, the `sh` command is enclosed
    inside a `try/catch` block that will prevent the script from failing.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如同之前，我们首先从注册表中拉取最新的容器。新增的内容是创建一个Docker网络。由于网络只能创建一次，所有后续尝试都会导致错误，因此`sh`命令被封装在`try/catch`块中，这样可以防止脚本失败。
- en: The creation of the network is followed by deployment of the `db` and `app`
    targets. Unlike DB that, in this scenario, is always deployed as a single instance,
    the `app` target might need to be scaled. For that reason, the first one is deployed
    through the `up` and the other through the `scale` command available through Docker
    Compose. The `scale` command utilizes the `instances` variable to determine how
    many copies of the release should be deployed. We can increase or decrease their
    number simply by changing the `instances` variable in the Jenkinsfile. Once such
    a change is committed to the repository, Jenkins will run a new build and deploy
    as many instances as we specified.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 创建网络后，接着是部署`db`和`app`目标。与始终作为单个实例部署的数据库不同，`app`目标可能需要扩展。因此，第一个目标通过`up`命令部署，而第二个通过Docker
    Compose提供的`scale`命令进行部署。`scale`命令利用`instances`变量来确定应部署多少个副本。我们可以通过简单地改变Jenkinsfile中的`instances`变量来增加或减少副本数。一旦此更改提交到仓库，Jenkins会运行新的构建并部署我们指定数量的实例。
- en: Finally, we are putting the number of instances to Consul by invoking the helper
    function `putInstances` which, in turn. executed a simple Shell command. Even
    though we won't be using the information right now, it will come in handy in the
    next chapter when we start building a self-healing system.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过调用辅助函数`putInstances`将实例数传递给Consul，该函数执行了一个简单的Shell命令。尽管我们现在不会使用这些信息，但它将在下一章构建自愈系统时派上用场。
- en: That's it. There were only a few changes we had to apply to the Jenkinsfile
    to have the `blue-green` deployment extended from a single server to the whole
    Swarm cluster. Both Docker Swarm and Jenkins Workflow proved to be very easy to
    work with, even easier to maintain, and, yet, very powerful.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们只需对Jenkinsfile做几个小修改，就能将`blue-green`部署从单一服务器扩展到整个Swarm集群。Docker Swarm和Jenkins
    Workflow证明非常易于使用，甚至更容易维护，而且依然非常强大。
- en: 'By this time, the build of the `swarm` sub-project probably finished. We can
    validate that from the build console screen or, directly, by opening the `books-ms-swarm`
    job and confirming that the status of the last build is represented with the `blue`
    ball. If you are curious why the success is represented with blue instead of green
    color, please read the *Why does Jenkins have blue balls?* article at [https://jenkins.io/blog/2012/03/13/why-does-jenkins-have-blue-balls/](https://jenkins.io/blog/2012/03/13/why-does-jenkins-have-blue-balls/):'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，`swarm` 子项目的构建可能已经完成。我们可以从构建控制台屏幕验证这一点，或者直接通过打开 `books-ms-swarm` 作业，确认最后一次构建的状态由
    `blue` 球表示。如果你想知道为什么成功状态是用蓝色而不是绿色表示的，请阅读 *为什么 Jenkins 显示蓝球？* 文章，链接在此：[https://jenkins.io/blog/2012/03/13/why-does-jenkins-have-blue-balls/](https://jenkins.io/blog/2012/03/13/why-does-jenkins-have-blue-balls/)：
- en: '![Examining the Swarm Deployment Playbook](img/B05848_14_14.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![检查 Swarm 部署剧本](img/B05848_14_14.jpg)'
- en: Figure 14-14 – The books-ms-swarm Jenkins job screen
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-14 – books-ms-swarm Jenkins 作业屏幕
- en: Now that we understand what is behind the *Jenkinsfile* script and the build
    is finished, we can manually validate that everything seems to be working correctly.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了 *Jenkinsfile* 脚本的背后含义，并且构建已完成，我们可以手动验证一切是否正常工作。
- en: Running the Swarm Jenkins Workflow
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 Swarm Jenkins 工作流
- en: The first run of the swarm subproject was initiated by Jenkins automatically
    once it finished indexing branches. All that's left for us is to double check
    that the whole process was indeed executed correctly.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 子项目的第一次运行是在 Jenkins 自动启动的，一旦完成分支索引。剩下的工作就是再次检查整个过程是否确实正确执行。
- en: 'This was the first deployment so the blue release should be running somewhere
    inside the cluster. Let''s take a look where did Swarm decide to deploy our containers:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一次部署，因此蓝色版本应该在集群的某个地方运行。让我们来看看 Swarm 决定将我们的容器部署在哪里：
- en: '[PRE66]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output of the ps command is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ps 命令的输出如下：
- en: '[PRE67]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In this case, Swarm deployed the `books-ms` container to the `swarm-node-2`
    and the Mongo DB to the `swarm-node-1`. We can also verify whether the service
    was correctly stored in Consul:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Swarm 将 `books-ms` 容器部署到了 `swarm-node-2`，将 Mongo DB 部署到了 `swarm-node-1`。我们还可以验证服务是否已正确存储在
    Consul 中：
- en: '[PRE68]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output of all three commands is as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个命令的输出如下：
- en: '[PRE69]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: According to Consul, the release was deployed to `swarm-node-2` (`10.100.192.202`)
    and has the port `32768`. We are currently running the `blue` release, and have
    only one instance running.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Consul，版本已部署到 `swarm-node-2` (`10.100.192.202`)，并且端口为 `32768`。我们当前正在运行的是
    `blue` 版本，且仅有一个实例在运行。
- en: 'Finally, we can double check that the service is indeed working by sending
    a few requests to it:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过向服务发送几个请求来再次确认服务确实在工作：
- en: '[PRE70]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The first request was PUT, sending a signal to the service that we want to store
    the book. The second retrieved the list of all books.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个请求是 PUT，向服务发送信号，表示我们想要存储这本书。第二个请求则是检索所有书籍的列表。
- en: The automated process seems to be working correctly when run for the first time.
    We'll execute the build again and deploy the green release.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化过程在第一次运行时似乎工作正常。我们将再次执行构建并部署绿色版本。
- en: The Second Run of the Swarm Deployment Playbook
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Swarm 部署剧本的第二次运行
- en: Let's deploy the next release.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署下一个版本。
- en: 'Please open the swarm subproject and click the Build Now link. The build will
    start, and we can monitor it from the Console screen. After a few minutes, the
    build will finish executing, and we''ll be able to check the result:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开 swarm 子项目并点击 "立即构建" 链接。构建将开始，我们可以从控制台屏幕进行监控。几分钟后，构建将完成执行，我们将能够检查结果：
- en: '[PRE71]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output of the ps command is as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ps 命令的输出如下：
- en: '[PRE72]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Since we run the `green` release, the `blue` release is in the `Exited` status.
    We can observe the information about the currently running release from Consul:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们运行的是 `green` 版本，`blue` 版本处于 `Exited` 状态。我们可以通过 Consul 查看当前运行版本的信息：
- en: '[PRE73]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The response from the Consul request is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Consul 请求的响应如下：
- en: '[PRE74]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now we can test the service itself:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以测试服务本身：
- en: '[PRE75]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Since we already have the Consul UI running, please open the `http://10.100.192.200:8500/ui`
    address in your favorite browser to get a visual representation of services we
    deployed.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经启动了 Consul UI，请在您喜欢的浏览器中打开 `http://10.100.192.200:8500/ui` 地址，以获取我们已部署服务的可视化展示。
- en: As an exercise, fork the `books-ms` repository and modify the job to use you
    repository. Open the *Jenkinsfile* inside the `swarm` branch, change it to deploy
    three instances of the service, and push the changes. Run the build again and,
    once it's finished, confirm that three instances were deployed to the cluster.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个练习，fork `books-ms` 仓库并修改作业以使用你自己的仓库。打开 `swarm` 分支中的 *Jenkinsfile*，将其修改为部署三个实例的服务，并推送更改。再次运行构建，完成后确认三个实例已部署到集群中。
- en: Cleaning Up
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理
- en: 'This concludes our tour of Docker Swarm. We''ll use it more throughout the
    next chapters. Before moving to the next subject, lets destroy the VMs. We''ll
    create them again when we need them:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对 Docker Swarm 的介绍。我们将在接下来的章节中更多地使用它。在进入下一个主题之前，让我们销毁虚拟机。需要时我们会重新创建它们：
- en: '[PRE76]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The solution we developed still has quite a few problems. The system is not
    fault tolerant, and is difficult to monitor. The next chapter will address the
    first of those problems through creation of a self-healing system.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发的解决方案仍然存在不少问题。系统不具备容错能力，且难以监控。下一章将通过创建自愈系统来解决第一个问题。
