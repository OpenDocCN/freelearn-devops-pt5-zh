- en: Using Ingress to Forward Traffic
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ingress 转发流量
- en: Applications that are not accessible to users are useless. Kubernetes Services
    provide accessibility with a usability cost. Each application can be reached through
    a different port. We cannot expect users to know the port of each service in our
    cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无法访问的应用程序是无用的。Kubernetes 服务提供了可访问性，但有一定的可用性成本。每个应用程序都可以通过不同的端口访问。我们不能指望用户知道我们集群中每个服务的端口。
- en: Ingress objects manage external access to the applications running inside a
    Kubernetes cluster. While, at first glance, it might seem that we already accomplished
    that through Kubernetes Services, they do not make the applications truly accessible.
    We still need forwarding rules based on paths and domains, SSL termination and
    a number of other features. In a more traditional setup, we'd probably use an
    external proxy and a load balancer. Ingress provides an API that allows us to
    accomplish these things, in addition to a few other features we expect from a
    dynamic cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 对象管理着外部访问运行在 Kubernetes 集群内部的应用程序。乍一看，似乎我们已经通过 Kubernetes 服务完成了这一任务，但它们并没有真正让应用程序可访问。我们仍然需要基于路径和域的转发规则、SSL
    终止以及其他许多功能。在更传统的设置中，我们可能会使用外部代理和负载均衡器。Ingress 提供了一个 API，让我们能够实现这些功能，并附带了一些我们期望从动态集群中获得的其他特性。
- en: We'll explore the problems and the solutions through examples. For now, we first
    need to create a cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过示例探索问题和解决方案。现在，首先我们需要创建一个集群。
- en: Creating a cluster
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: As every other chapter so far, we'll start by creating a Minikube single-node
    cluster.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的每一章一样，我们将首先创建一个 Minikube 单节点集群。
- en: All the commands from this chapter are available in the `07-ingress.sh` ([https://gist.github.com/vfarcic/54ef6592bce747ff2d1b089834fc755b](https://gist.github.com/vfarcic/54ef6592bce747ff2d1b089834fc755b))
    Gist.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 `07-ingress.sh` ([https://gist.github.com/vfarcic/54ef6592bce747ff2d1b089834fc755b](https://gist.github.com/vfarcic/54ef6592bce747ff2d1b089834fc755b))
    Gist 中找到。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The cluster should be up-and-running, and we can move on.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 集群应已启动并运行，我们可以继续。
- en: Exploring deficiencies when enabling external access through Kubernetes services
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索通过 Kubernetes 服务启用外部访问时的不足
- en: We cannot explore solutions before we know what the problems are. Therefore,
    we'll re-create a few objects using the knowledge we already gained. That will
    let us see whether Kubernetes services satisfy all the needs users of our applications
    might have. Or, to be more explicit, we'll explore which features we're missing
    when making our applications accessible to users.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解问题之前，我们无法探索解决方案。因此，我们将重新创建一些对象，利用我们已经掌握的知识。这将帮助我们查看 Kubernetes 服务是否满足应用程序用户的所有需求。或者，更明确地说，我们将探索在使我们的应用程序对用户可访问时，缺少哪些功能。
- en: We already discussed that it is a bad practice to publish fixed ports through
    services. That method is likely to result in conflicts or, at the very least,
    create the additional burden of carefully keeping track of which port belongs
    to which service. We already discarded that option before, and we won't change
    our minds now. Since we've clarified that, let's go back and create the Deployments
    and the Services from the previous chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过，通过服务发布固定端口是一个不好的做法。这种方法很可能导致冲突，或者至少会增加额外的负担，需要仔细跟踪每个端口属于哪个服务。我们之前已经放弃了这个选项，现在也不会改变主意。既然我们已经澄清了这一点，让我们回过头来创建上一章中的
    Deployments 和 Services。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the `get` command is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`get` 命令的输出如下：'
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, these are the same Services and Deployments we previously created.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这些就是我们之前创建的相同的 Services 和 Deployments。
- en: Before we move on, we should wait until all the Pods are up and running.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们应该等到所有 Pods 都启动并运行。
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If, in your case, some of the Pods are not yet running, please wait a few moments
    and re-execute the `kubectl get pods` command. We'll continue once they're ready.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在你的情况下，某些 Pods 尚未运行，请等待片刻并重新执行 `kubectl get pods` 命令。我们将在它们准备好后继续。
- en: 'One obvious way to access the applications is through Services:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 访问应用程序的一种明显方法是通过服务：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We retrieved the Minikube IP and the port of the `go-demo-2-api` Service. We
    used that information to send a request.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取了 Minikube 的 IP 地址以及 `go-demo-2-api` 服务的端口。我们使用这些信息发送了请求。
- en: 'The output of the `curl` command is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 命令的输出如下：'
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The application responded with the status code `200` thus confirming that the
    Service indeed forwards the requests.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序返回了状态码 `200`，从而确认服务确实转发了请求。
- en: 'While publishing a random, or even a hard-coded port of a single application
    might not be so bad, if we''d apply the same principle to more applications, the
    user experience would be horrible. To make the point a bit clearer, we''ll deploy
    another application:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管发布一个随机端口，甚至是硬编码的单一应用程序端口可能不会太糟，但如果我们将同样的原则应用到更多的应用程序上，用户体验将会非常糟糕。为了让这个问题更清楚，我们将部署另一个应用程序：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This application follows similar logic to the first. From the latter command,
    we can see that it contains a Deployment and a Service. The details are of no
    importance since the YAML definition is very similar to those we used before.
    What matters is that now we have two applications running inside the cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序遵循与第一个应用程序类似的逻辑。从后面的命令中我们可以看到，它包含一个 Deployment 和一个 Service。由于 YAML 定义与之前使用的非常相似，因此细节不太重要。关键是现在我们有两个应用程序在集群内运行。
- en: 'Let''s check whether the new application is indeed reachable:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下新应用程序是否真的可以访问：
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We retrieved the port of the new Service and opened the application in a browser.
    You should see a simple front-end with *The DevOps Toolkit* books. If you don't,
    you might want to wait a bit longer until the containers are pulled, and try again.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取了新服务的端口，并在浏览器中打开应用程序。你应该能看到一个包含*《DevOps 工具包》*书籍的简单前端。如果没有看到，你可能需要稍等一会儿，直到容器拉取完成，再试一次。
- en: 'A simplified flow of requests is depicted in the *Figure 7-1*. A user sends
    a request to one of the nodes of the cluster. That request is received by a Service
    and load balanced to one of the associated Pods. It''s a bit more complicated
    than that, with iptables, kube DNS, kube proxy, and a few other things involved
    in the process. We explored them in more detail in [Chapter 5](e499b152-2e33-455d-84be-5b3d201829f9.xhtml), *Using
    Services to Enable Communication Between Pods*, and there''s probably no need
    to go through them all again. For the sake of brevity, the simplified diagram
    should do:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请求的简化流程如*图 7-1*所示。用户向集群的某个节点发送请求。请求由一个服务接收，并通过负载均衡转发到其中一个关联的 Pod。其实这个过程要比这复杂，涉及到
    iptables、kube DNS、kube proxy 和其他一些组件。我们在[第 5 章](e499b152-2e33-455d-84be-5b3d201829f9.xhtml)《使用服务启用
    Pod 之间的通信》中对这些进行了更详细的探讨，可能不需要再一一介绍。为了简洁起见，简化的图示应该足够：
- en: '![](img/3a4b96e8-7b0b-4bd1-8646-e5cc1ef6736a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a4b96e8-7b0b-4bd1-8646-e5cc1ef6736a.png)'
- en: 'Figure 7-1: Applications access through Services'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-1：通过服务访问应用程序
- en: We cannot expect our users to know specific ports behind each of those applications.
    Even with only two, that would not be very user-friendly. If that number would
    rise to tens or even hundreds of applications, our business would be very short-lived.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能指望用户知道每个应用程序背后的具体端口。即使只有两个应用程序，这样也不太符合用户友好性。如果应用程序数量增加到几十个甚至几百个，我们的业务也将很快消亡。
- en: What we need is a way to make all services accessible through standard HTTP
    (`80`) or HTTPS (`443`) ports. Kubernetes Services alone cannot get us there.
    We need more.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一种通过标准的 HTTP (`80`) 或 HTTPS (`443`) 端口使所有服务都能访问的方法。单独使用 Kubernetes 服务无法实现这一点，我们还需要更多的东西。
- en: 'What we need is to grant access to our services on predefined paths and domains.
    Our `go-demo-2` service could be distinguished from others through the base path
    `/demo`. Similarly, the books application could be reachable through the `devopstoolkitseries.com`
    domain. If we could accomplish that, we could access them with the commands the
    follow:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是在预定义的路径和域名上授予访问我们的服务的权限。我们的`go-demo-2`服务可以通过基础路径`/demo`与其他服务区分开来。同样，书籍应用程序可以通过`devopstoolkitseries.com`域名访问。如果我们能做到这一点，我们就可以通过以下命令访问它们：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The request received the `Connection refused` response. There is no process
    listening on port `80`, so this outcome is not a surprise. We could have changed
    one of the Services to publish the fixed port `80` instead assigning a random
    one. Still, that would provide access only to one of the two applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请求收到了 `Connection refused` 响应。端口 `80` 上没有进程在监听，所以这个结果并不令人意外。我们本可以将其中一个服务修改为发布固定端口
    `80`，而不是分配一个随机端口。然而，这样做仍然只会提供对两个应用程序中的一个的访问。
- en: We often want to associate each application with a different domain or sub-domain.
    Outside the examples we're running, the books application is accessible through
    the `devopstoolkitseries.com` ([http://www.devopstoolkitseries.com/](http://www.devopstoolkitseries.com/))
    domain. Since I'm not going to give you permissions to modify my domain's DNS
    records, we'll simulate it by adding the domain to the `Host` header.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常希望将每个应用程序与不同的域名或子域名关联。在我们运行的示例之外，书籍应用程序可以通过 `devopstoolkitseries.com` ([http://www.devopstoolkitseries.com/](http://www.devopstoolkitseries.com/))
    域名访问。由于我不会给你修改我的域名 DNS 记录的权限，我们将通过将域名添加到 `Host` 头来模拟这一点。
- en: 'The command that should verify whether the application running inside our cluster
    is accessible through the `devopstoolkitseries.com` domain is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 应该验证我们集群内运行的应用程序是否可以通过 `devopstoolkitseries.com` 域名访问的命令如下：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As expected, the request is still refused.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，请求仍然被拒绝。
- en: Last, but not least, we should be able to make some, if not all, applications
    (partly) secure by enabling HTTPS access. That means that we should have a place
    to store our SSL certificates. We could put them inside our applications, but
    that would only increase the operational complexity. Instead, we should aim towards
    SSL offloading somewhere between clients and the applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们应该能够通过启用 HTTPS 访问，使一些应用程序（如果不是全部）变得部分安全。这意味着我们应该有地方存储我们的 SSL 证书。我们可以将它们放在应用程序内部，但那样只会增加操作复杂性。相反，我们应该朝着在客户端和应用程序之间进行
    SSL 卸载的方向努力。
- en: The problems that we are facing are common, and it should come as no surprise
    that Kubernetes has a solution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的问题是常见的，因此 Kubernetes 有解决方案也就不足为奇了。
- en: Enabling Ingress controllers
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用 Ingress 控制器
- en: We need a mechanism that will accept requests on pre-defined ports (for example,
    `80` and `443`) and forward them to Kubernetes services. It should be able to
    distinguish requests based on paths and domains as well as to be able to perform
    SSL offloading.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个机制来接受预定义端口（例如 `80` 和 `443`）上的请求，并将其转发到 Kubernetes 服务。它应该能够根据路径和域名区分请求，并能够执行
    SSL 卸载。
- en: Kubernetes itself does not have a ready-to-go solution for this. Unlike other
    types of Controllers that are typically part of the `kube-controller-manager`
    binary, Ingress Controller needs to be installed separately. Instead of a Controller,
    `kube-controller-manager` offers *Ingress resource* that other third-party solutions
    can utilize to provide requests forwarding and SSL features. In other words, Kubernetes
    only provides an *API*, and we need to set up a Controller that will use it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 本身没有现成的解决方案来实现这一点。与通常作为 `kube-controller-manager` 二进制文件一部分的其他类型的控制器不同，Ingress
    控制器需要单独安装。与控制器不同，`kube-controller-manager` 提供了 *Ingress 资源*，其他第三方解决方案可以利用这些资源来提供请求转发和
    SSL 功能。换句话说，Kubernetes 只提供了一个 *API*，我们需要设置一个将使用它的控制器。
- en: Fortunately, the community already built a myriad of Ingress controllers. We
    won't evaluate all of the available options since that would require a lot of
    space, and it would mostly depend on your needs and your hosting vendor. Instead,
    we'll explore how Ingress controllers work through the one that is already available
    in Minikube.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，社区已经构建了众多 Ingress 控制器。我们不会评估所有可用的选项，因为这需要大量空间，而且大多取决于你的需求和你的托管供应商。相反，我们将通过
    Minikube 中已经可用的 Ingress 控制器来探索它是如何工作的。
- en: 'Let''s take a look at the list of the Minikube addons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 Minikube 插件的列表：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see that `ingress` is available as one of the Minikube addons. However,
    it is disabled by default, so our next action will be to enable it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `ingress` 作为 Minikube 插件之一可用。然而，它默认是禁用的，因此我们的下一步操作将是启用它。
- en: If you used Minikube before, the `ingress` addon might already be enabled. If
    that's the case, please skip the command that follows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前使用过 Minikube，`ingress` 插件可能已经启用。如果是这种情况，请跳过接下来的命令。
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that `ingress` addon is enabled, we''ll check whether it is running inside
    our cluster:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `ingress` 插件已启用，我们将检查它是否在我们的集群内运行：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Ignore the `-n` argument. We did not yet explore Namespaces. For now, please
    note that the output of the command should show that `nginx-ingress-controller-...`
    Pod is running.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略 `-n` 参数。我们尚未探索命名空间。目前，请注意命令的输出应显示 `nginx-ingress-controller-...` Pod 正在运行。
- en: If the output is empty, you might need to wait for a few moments until the containers
    are pulled, and re-execute the `kubectl get all --namespace ingress-nginx` command
    again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出为空，可能需要稍等片刻，直到容器被拉取完毕，然后重新执行 `kubectl get all --namespace ingress-nginx`
    命令。
- en: The Ingress controller that ships with Minikube is based on the `gcr.io/google_containers/nginx-ingress-controller`
    ([https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/nginx-ingress-controller?gcrImageListsize=50](https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/nginx-ingress-controller?gcrImageListsize=50))
    image hosted in **Google Cloud Platform** (**GCP**) Container Registry. The image
    is based on NGINX Ingress Controller ([https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md)).
    It is one of the only two currently supported and maintained by the Kubernetes
    community. The other one is GLBC ([https://github.com/kubernetes/ingress-gce/blob/master/README.md](https://github.com/kubernetes/ingress-gce/blob/master/README.md))
    that comes with **Google Compute Engine** (**GCE**) ([https://cloud.google.com/compute/](https://cloud.google.com/compute/))
    Kubernetes hosted solution.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube 附带的 Ingress 控制器基于 **Google Cloud Platform** (**GCP**) 容器注册中心中托管的 `gcr.io/google_containers/nginx-ingress-controller`
    ([https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/nginx-ingress-controller?gcrImageListsize=50](https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/nginx-ingress-controller?gcrImageListsize=50))
    镜像。该镜像基于 NGINX Ingress 控制器 ([https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md))。它是目前
    Kubernetes 社区支持和维护的仅有的两个控制器之一。另一个是 GLBC ([https://github.com/kubernetes/ingress-gce/blob/master/README.md](https://github.com/kubernetes/ingress-gce/blob/master/README.md))，它与
    **Google Compute Engine** (**GCE**) ([https://cloud.google.com/compute/](https://cloud.google.com/compute/))
    Kubernetes 托管解决方案一起使用。
- en: By default, the Ingress controller is configured with only two endpoints.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Ingress 控制器只配置了两个端点。
- en: If we'd like to check Controller's health, we can send a request to `/healthz`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想检查控制器的健康状态，可以向 `/healthz` 发送请求。
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It responded with the status code `200 OK`, thus indicating that it is healthy
    and ready to serve requests. There's not much more to it so we'll move to the
    second endpoint.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 它以状态码 `200 OK` 响应，表示它是健康的并准备好处理请求。没什么复杂的，接下来我们会进入第二个端点。
- en: 'The Ingress controller has a default catch-all endpoint that is used when a
    request does not match any of the other criteria. Since we did not yet create
    any Ingress Resource, this endpoint should provide the same response to all requests
    except `/healthz`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器有一个默认的 catch-all 端点，当请求不匹配其他任何条件时会使用该端点。由于我们还没有创建任何 Ingress 资源，这个端点应该对除
    `/healthz` 外的所有请求返回相同的响应：
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We got the response indicating that the requested resource could not be found.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了响应，表示请求的资源未找到。
- en: Now we're ready to create our first Ingress Resource.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建我们的第一个 Ingress 资源。
- en: Creating Ingress Resources based on paths
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于路径创建 Ingress 资源
- en: We'll try to make our `go-demo-2-api` service available through the port `80`.
    We'll do that by defining an Ingress resource with the rule to forward all requests
    with the path starting with `/demo` to the service `go-demo-2-api`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试通过端口 `80` 使我们的 `go-demo-2-api` 服务可用。我们将通过定义一个 Ingress 资源并设置规则，将所有路径以 `/demo`
    开头的请求转发到 `go-demo-2-api` 服务来实现。
- en: 'Let''s take a look at the Ingress'' YAML definition:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 Ingress 的 YAML 定义：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This time, `metadata` contains a field we haven't used before. The `annotations`
    section allows us to provide additional information to the Ingress controller.
    As you'll see soon, Ingress API specification is concise and limited. That is
    done on purpose. The specification API defines only the fields that are mandatory
    for all Ingress controllers. All the additional info an Ingress controller needs
    is specified through `annotations`. That way, the community behind the Controllers
    can progress at great speed, while still providing basic general compatibility
    and standards.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，`metadata` 包含了一个我们之前没有使用过的字段。`annotations` 部分允许我们向 Ingress 控制器提供额外的信息。如你将很快看到的，Ingress
    API 规范简洁且有限。这样做是有目的的。该规范 API 仅定义了所有 Ingress 控制器必须的字段。Ingress 控制器所需的所有额外信息都通过 `annotations`
    来指定。这样，控制器背后的社区可以以极快的速度发展，同时仍然提供基本的通用兼容性和标准。
- en: The list of general annotations and the Controllers that support them can be
    found in the Ingress Annotations page([https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md)).
    For those specific to the NGINX Ingress controller ([https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md)),
    please visit the NGINX Annotations ([https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md))page,
    and for those specific to GCE Ingress, visit the `ingress-gce` ([https://github.com/kubernetes/ingress-gce](https://github.com/kubernetes/ingress-gce))
    page.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一般注解的列表和支持它们的控制器可以在Ingress注解页面找到([https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md))。有关NGINX
    Ingress控制器的注解，请访问NGINX注解页面([https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md))，而针对GCE
    Ingress的注解，请访问`ingress-gce`页面([https://github.com/kubernetes/ingress-gce](https://github.com/kubernetes/ingress-gce))。
- en: You'll notice that documentation uses `nginx.ingress.kubernetes.io/` annotation
    prefixes. That is a relatively recent change that, at the time of this writing,
    applies to the beta versions of the Controller. We're combining it with `ingress.kubernetes.io/`
    prefixes so that the definitions work in all Kubernetes versions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到文档使用了`nginx.ingress.kubernetes.io/`注解前缀。这是一个相对较新的变化，在撰写本文时，它适用于控制器的测试版本。我们将其与`ingress.kubernetes.io/`前缀结合使用，以便定义在所有Kubernetes版本中都能生效。
- en: 'We specified only one annotation. `nginx.ingress.kubernetes.io/ssl-redirect:
    "false"` tells the Controller that we do NOT want to redirect all HTTP requests
    to HTTPS. We''re forced to do so since we do not have SSL certificates for the
    exercises that follow.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '我们只指定了一个注解。`nginx.ingress.kubernetes.io/ssl-redirect: "false"`告诉控制器，我们不希望将所有HTTP请求重定向到HTTPS。我们必须这样做，因为接下来的练习没有SSL证书。'
- en: Now that we shed some light on the `metadata annotations`, we can move to the
    `ingress` specification.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对`metadata annotations`有了一些了解后，我们可以继续研究`ingress`的规格。
- en: We specified a set of `rules` in the `spec` section. They are used to configure
    Ingress resource. For now, our rule is based on `http` with a single `path` and
    a `backend`. All the requests with the `path` starting with `/demo` will be forwarded
    to the service `go-demo-2-api` on the port `8080`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`spec`部分指定了一组`rules`。它们用于配置Ingress资源。现在，我们的规则是基于`http`的，具有一个单一的`path`和`backend`。所有以`/demo`开头的请求都会被转发到`go-demo-2-api`服务的`8080`端口。
- en: Now that we had a short tour around some of the Ingress configuration options,
    we can proceed and create the resource.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经简要了解了一些Ingress配置选项，我们可以继续创建资源了。
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output of the latter command is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 后者命令的输出如下：
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can see that the Ingress resource was created. Don't panic if, in your case,
    the address is blank. It might take a while for it to obtain it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到Ingress资源已创建。如果在你的情况下，地址为空，不用慌张，它可能需要一些时间才能获取到。
- en: Let's see whether requests sent to the base path `/demo` work.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以成功地将请求发送到基础路径`/demo`。
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The status code `200 OK` is a clear indication that this time, the application
    is accessible through the port `80`. If that's not enough of assurance, you can
    observe the `hello, world!` response as well.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 状态码`200 OK`清楚地表明这一次，应用程序通过端口`80`可以访问。如果这还不足以让你放心，你还可以观察到`hello, world!`的响应。
- en: 'The `go-demo-2` service we''re currently using is no longer properly configured
    for our Ingress setup. Using `type: NodePort`, it is configured to export the
    port `8080` on all of the nodes. Since we''re expecting users to access the application
    through the Ingress Controller on port `80`, there''s probably no need to allow
    external access through the port `8080` as well. We should switch to the `ClusterIP`
    type. That will allow direct access to the Service only within the cluster, thus
    limiting all external communication through Ingress.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们当前使用的 `go-demo-2` 服务不再适合我们的 Ingress 配置。使用 `type: NodePort`，它配置为在所有节点上导出端口
    `8080`。由于我们期望用户通过端口 `80` 通过 Ingress 控制器访问应用程序，因此可能不需要通过端口 `8080` 允许外部访问。我们应该切换到
    `ClusterIP` 类型。这样只允许集群内部直接访问该服务，从而通过 Ingress 限制所有外部通信。'
- en: 'We cannot just update the Service with a new definition. Once a Service port
    is exposed, it cannot be un-exposed. We''ll delete the `go-demo-2` objects we
    created and start over. Besides the need to change the Service type, that will
    give us an opportunity to unify everything in a single YAML file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅仅通过新的定义来更新 Service。一旦 Service 端口被暴露，就不能再取消暴露。我们将删除创建的 `go-demo-2` 对象并重新开始。除了需要更改
    Service 类型之外，这还将使我们有机会将所有内容统一到一个 YAML 文件中。
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We removed the objects related to `go-demo-2`, and now we can take a look at
    the unified definition.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们删除了与 `go-demo-2` 相关的对象，现在可以看看统一的定义。
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We won''t go into details of the new definition since it does not have any
    significant changes. It combines `ingress/go-demo-2-ingress.yml` and `ingress/go-demo-2-deploy.yml`
    into a single file, and it removes `type: NodePort` from the `go-demo-2` Service.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们不会详细讨论新的定义，因为它没有任何显著变化。它将 `ingress/go-demo-2-ingress.yml` 和 `ingress/go-demo-2-deploy.yml`
    合并为一个文件，并从 `go-demo-2` 服务中移除了 `type: NodePort`。'
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We created the objects from the unified definition and sent a request to validate
    that everything works as expected. The response should be `200 OK` indicating
    that everything (still) works as expected.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从统一的定义中创建了对象，并发送了请求来验证一切是否按预期工作。响应应该是 `200 OK`，表示一切（仍然）按预期工作。
- en: Please note that Kubernetes needs a few seconds until all the objects are running
    as expected. If you were too fast, you might have received the response `404 Not
    Found` instead `200 OK`. If that was the case, all you have to do is send the
    `curl` request again.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Kubernetes 需要几秒钟才能让所有对象按预期运行。如果你操作太快，可能会收到 `404 Not Found` 响应，而不是 `200 OK`。如果发生这种情况，你只需要再次发送
    `curl` 请求。
- en: Let's see, through a sequence diagram, what happened when we created the Ingress
    resource.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个顺序图来看看，当我们创建 Ingress 资源时发生了什么。
- en: The Kubernetes client (`kubectl`) sent a request to the API server requesting
    the creation of the Ingress resource defined in the `ingress/go-demo-2.yml` file.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 客户端（`kubectl`）向 API 服务器发送请求，要求创建在 `ingress/go-demo-2.yml` 文件中定义的
    Ingress 资源。
- en: The ingress controller is watching the API server for new events. It detected
    that there is a new Ingress resource.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ingress 控制器正在监视 API 服务器的新事件。它检测到有一个新的 Ingress 资源。
- en: The ingress controller configured the load balancer. In this case, it is nginx
    which was enabled by `minikube addons enable ingress` command. It modified `nginx.conf`
    with the values of all `go-demo-2-api` endpoints.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ingress 控制器配置了负载均衡器。在这个例子中，它是 nginx，通过 `minikube addons enable ingress` 命令启用。它修改了
    `nginx.conf`，并加入了所有 `go-demo-2-api` 端点的值。
- en: '![](img/f97fc587-e82a-4442-be25-8b4464320e11.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f97fc587-e82a-4442-be25-8b4464320e11.png)'
- en: 'Figure 7-2: The sequence of events followed by the request to create an Ingress
    resource'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-2：创建 Ingress 资源请求后事件的顺序
- en: Now that one of the applications is accessible through Ingress, we should apply
    the same principles to the other.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，其中一个应用程序可以通过 Ingress 访问，我们应该将相同的原则应用于另一个应用程序。
- en: Let's take a look at the full definition of all the objects behind the `devops-toolkit`
    application.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 `devops-toolkit` 应用程序背后所有对象的完整定义。
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output, limited to the Ingress object, is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 限制为 Ingress 对象的输出如下：
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `devops-toolkit` Ingress resource is very similar to `go-demo-2`. The only
    significant difference is that the `path` is set to `/`. It will serve all requests.
    It would be a much better solution if we'd change it to a unique base path (for
    example, `/devops-toolkit`) since that would provide a unique identifier. However,
    this application does not have an option to define a base path, so an attempt
    to do so in Ingress would result in a failure to retrieve resources. We'd need
    to write `rewrite` rules instead. We could, for example, create a rule that rewrites
    path base `/devops-toolkit` to `/`. That way if, for example, someone sends a
    request to `/devops-toolkit/something`, Ingress would rewrite it to `/something`
    before sending it to the destination Service. While such an action is often useful,
    we'll ignore it for now. I have better plans for this application. Until I decide
    to reveal them, `/` as the base `path` should do.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`devops-toolkit` Ingress资源与`go-demo-2`非常相似。唯一显著的区别是`path`设置为`/`。它将处理所有请求。如果我们将其改为一个唯一的基础路径（例如`/devops-toolkit`），那将是一个更好的解决方案，因为这将提供一个唯一的标识符。然而，这个应用程序没有定义基础路径的选项，因此如果尝试在Ingress中定义基础路径，它将导致无法检索资源。我们需要编写`rewrite`规则来代替。例如，我们可以创建一个规则，将路径基础`/devops-toolkit`重写为`/`。这样，如果有人发送请求到`/devops-toolkit/something`，Ingress会在发送到目标服务之前将其重写为`/something`。虽然这种做法通常很有用，但我们暂时忽略它。我有更好的计划，直到我决定揭示它们之前，`/`作为基础`path`应该足够了。'
- en: 'Apart from adding Ingress to the mix, the definition removed `type: NodePort`
    from the Service. This is the same type of action we did previously with the `go-demo-2`
    service. We do not need external access to the Service.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '除了添加Ingress之外，定义中还移除了服务中的`type: NodePort`。这是我们之前在`go-demo-2`服务上做过的相同操作。我们不需要外部访问该服务。'
- en: 'Let''s remove the old objects and create those defined in the `ingress/devops-toolkit.yml`
    file:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们移除旧的对象，并创建`ingress/devops-toolkit.yml`文件中定义的对象：
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We removed the old `devops-toolkit` and created new ones.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了旧的`devops-toolkit`并创建了新的。
- en: 'Let''s take a look at the Ingresses running inside the cluster:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下集群内部运行的Ingress：
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can see that now we have multiple Ingress resources. The Ingress controller
    (in this case NGINX) configured itself taking both of those resources into account.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在我们有了多个Ingress资源。Ingress控制器（在本例中为NGINX）会根据这两个资源进行配置。
- en: We can define multiple Ingress resources that will configure a single Ingress
    controller.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义多个Ingress资源来配置单个Ingress控制器。
- en: Let's confirm that both applications are accessible through HTTP (port `80`).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认这两个应用程序是否可以通过HTTP（端口`80`）访问。
- en: '[PRE33]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The first command opened one of the applications in a browser, while the other
    returned the already familiar `hello, world!` message.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令在浏览器中打开了其中一个应用程序，而另一个则返回了我们熟悉的`hello, world!`消息。
- en: Ingress is a (kind of) Service that runs on all nodes of a cluster. A user can
    send requests to any and, as long as they match one of the rules, they will be
    forwarded to the appropriate Service.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 是一种（类似）服务，运行在集群的所有节点上。用户可以向任何节点发送请求，只要请求匹配其中一个规则，它将被转发到相应的服务。
- en: '![](img/e7cc7cb0-6848-4f69-b323-a82bb583d64e.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7cc7cb0-6848-4f69-b323-a82bb583d64e.png)'
- en: 'Figure 7-3: Applications accessed through Ingress controller'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-3：通过Ingress控制器访问的应用程序
- en: Even though we can send requests to both applications using the same port (`80`),
    that is often a sub-optimal solution. Our users would probably be happier if they
    could access those applications through different domains.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们可以通过相同的端口（`80`）向两个应用程序发送请求，这通常并不是最优的解决方案。如果用户能够通过不同的域名访问这些应用程序，他们可能会更高兴。
- en: Creating Ingress resources based on domains
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于域名创建Ingress资源
- en: We'll try to refactor our `devops-toolkit` Ingress definition so that the Controller
    forwards requests coming from the `devopstoolkitseries.com` domain. The change
    should be minimal, so we'll get down to it right away.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试重构我们的`devops-toolkit` Ingress定义，以便控制器能够转发来自`devopstoolkitseries.com`域名的请求。此更改应该是最小化的，因此我们马上开始处理。
- en: '[PRE34]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'When compared with the previous definition, the only difference is in the additional
    entry `host: devopstoolkitseries.com`. Since that will be the only application
    accessible through that domain, we also removed the `path: /` entry.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '与之前的定义相比，唯一的区别在于新增的条目`host: devopstoolkitseries.com`。由于这个域名将是唯一可以通过该域访问的应用程序，我们还移除了`path:
    /`条目。'
- en: 'Let''s `apply` the new definition:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们`apply`新的定义：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'What would happen if we send a similar domain-less request to the Application?
    I''m sure you already know the answer, but we''ll check it out anyways:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向应用程序发送一个类似的无域名请求，会发生什么呢？我相信你已经知道答案了，但我们还是来验证一下：
- en: '[PRE36]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE37]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: There is no Ingress resource defined to listen to `/`. The updated Ingress will
    forward requests only if they come from `devopstoolkitseries.com`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 没有定义Ingress资源来监听`/`路径。更新后的Ingress将仅在请求来自`devopstoolkitseries.com`时转发请求。
- en: 'I own the `devopstoolkitseries.com` domain, and I''m not willing to give you
    the access to my DNS registry to configure it with the IP of your Minikube cluster.
    Therefore, we won''t be able to test it by sending a request to `devopstoolkitseries.com`.
    What we can do is to "fake" it by adding that domain to the request header:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我拥有`devopstoolkitseries.com`域名，并且不愿意将我的DNS注册信息提供给你来配置它指向你Minikube集群的IP。因此，我们无法通过向`devopstoolkitseries.com`发送请求来进行测试。我们可以做的是通过在请求头中添加该域名来“伪造”它：
- en: '[PRE38]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE39]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now that Ingress received a request that looks like it's coming from the domain
    `devopstoolkitseries.com`, it forwarded it to the `devops-toolkit` Service which,
    in turn, load balanced it to one of the `devops-toolkit` Pods. As a result, we
    got the response `200 OK`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Ingress接收到一个看起来像是来自`devopstoolkitseries.com`域名的请求，它将请求转发给了`devops-toolkit`服务，后者又将其负载均衡到其中一个`devops-toolkit`
    Pod。结果，我们得到了`200 OK`响应。
- en: Just to be on the safe side, we'll verify whether `go-demo-2` Ingress still
    works.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保万无一失，我们将验证`go-demo-2` Ingress是否仍然有效。
- en: '[PRE40]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We got the famous `hello, world!` response, thus confirming that both Ingress
    resources are operational. Even though we "faked" the last request as if it's
    coming from `acme.com`, it still worked. Since the `go-demo-2` Ingress does not
    have any `host` defined, it accepts any request with the `path` starting with
    `/demo`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了著名的`hello, world!`响应，从而确认两个Ingress资源都在正常工作。即使我们“伪造”了最后一个请求，仿佛它来自`acme.com`，它仍然正常工作。由于`go-demo-2`
    Ingress没有定义任何`host`，它接受所有以`/demo`开头的请求。
- en: We're still missing a few things. One of those is a setup of a default backend.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还缺少一些东西，其中之一就是设置默认后端。
- en: Creating an Ingress resource with default backends
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建带有默认后端的Ingress资源
- en: In some cases, we might want to define a default backend. We might want to forward
    requests that do not match any of the Ingress rules.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能希望定义一个默认后端。我们可能希望将不符合任何Ingress规则的请求转发到默认后端。
- en: 'Let''s take a look at an example:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个示例：
- en: '[PRE41]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: So far, we have two sets of Ingress rules in our cluster. One accepts all requests
    with the base path `/demo`. The other forwards all requests coming from the `devopstoolkitseries.com`
    domain. The request we just sent does not match either of those rules, so the
    response was once again 404 Not Found.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的集群中有两组Ingress规则。一组接受所有以`/demo`为基础路径的请求。另一组转发所有来自`devopstoolkitseries.com`域名的请求。我们刚刚发送的请求并不符合这两组规则，因此响应再次是404
    Not Found。
- en: 'Let''s imagine that it would be a good idea to forward all requests with the
    wrong domain to the `devops-toolkit` application. Of course, by "wrong domain",
    I mean one of the domains we own, and not one of those that are already included
    in Ingress rules:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 假设将所有带有错误域名的请求转发到`devops-toolkit`应用程序是个好主意。当然，这里的“错误域名”是指我们拥有的域名，而不是那些已经包含在Ingress规则中的域名：
- en: '[PRE42]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE43]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: There's no Deployment, nor is there a Service. This time, we're creating only
    an Ingress resource.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有Deployment，也没有Service。这次，我们只创建了一个Ingress资源。
- en: The `spec` has no rules, but only a single `backend`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec`没有规则，只有一个单一的`backend`。'
- en: When an Ingress `spec` is without rules, it is considered a default backend.
    As such, it will forward all requests that do not match paths and/or domains set
    as rules in the other Ingress resources.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当Ingress的`spec`没有规则时，它被视为默认后端。因此，它将转发所有不匹配其他Ingress资源中的路径和/或域名规则的请求。
- en: We can use the default backend as a substitute for the default `404` pages or
    for any other occasion that is not covered by other rules.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用默认后端作为默认的`404`页面，或在其他规则未涵盖的情况下使用。
- en: You'll notice that the `serviceName` is `devops-toolkit`. The example would
    be much better if I created a separate application for this purpose. At the risk
    of you calling me lazy, I'll say that it does not matter for this example. All
    we want, at the moment, is to see something other than `404 Not Found` response.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 `serviceName` 是 `devops-toolkit`。如果我为此创建一个单独的应用程序，示例会更好。冒着被你叫懒的风险，我想说，这个例子并不重要。我们现在只想看到一些不同于
    `404 Not Found` 的响应。
- en: '[PRE44]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We created the Ingress resource with the default backend, and now we can test
    whether it truly works:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了带有默认后端的 Ingress 资源，现在可以测试它是否真正有效：
- en: '[PRE45]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This time, the output is different. We got `200 OK` instead of the `404 Not
    Found` response.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，输出不同了。我们得到了 `200 OK`，而不是 `404 Not Found` 响应。
- en: '[PRE46]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: What now?
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: We explored some of the essential functions of Ingress resources and Controllers.
    To be more concrete, we examined almost all those that are defined in the Ingress
    API.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了 Ingress 资源和控制器的一些基本功能。具体来说，我们几乎审视了 Ingress API 中定义的所有功能。
- en: One notable feature we did not explore is TLS configuration. Without it, our
    services cannot serve HTTPS requests. To enable it, we'd need to configure Ingress
    to offload SSL certificates.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有探索的一个显著功能是 TLS 配置。没有它，我们的服务无法提供 HTTPS 请求。为了启用它，我们需要配置 Ingress 来卸载 SSL 证书。
- en: There are two reasons we did not explore TLS. For one, we do not have a valid
    SSL certificate. On top of that, we did not yet study Kubernetes Secrets. I'd
    suggest you explore SSL setup yourself once you make a decision which Ingress
    controller to use. Secrets, on the other hand, will be explained soon.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有探索 TLS 的原因有两个。首先，我们没有有效的 SSL 证书。除此之外，我们还没有学习 Kubernetes Secrets。我建议你在决定使用哪个
    Ingress 控制器后，自己探索 SSL 配置。而 Secrets 会很快进行解释。
- en: We'll explore other Ingress controllers once we move our cluster to "real" servers
    that we'll create with one of the hosting vendors. Until then, you might benefit
    from reading NGINX Ingress controller ([https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md))
    documentation in more detail. Specifically, I suggest you pay close attention
    to its annotations ([https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将集群迁移到我们将与某个托管供应商一起创建的“真实”服务器上，我们将探索其他 Ingress 控制器。在此之前，你可以通过更详细地阅读 NGINX
    Ingress 控制器的 [文档](https://github.com/kubernetes/ingress-nginx/blob/master/README.md)
    来受益。具体来说，我建议你特别关注它的注解部分，[文档链接](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md)。
- en: Now that another chapter is finished, we'll destroy the cluster and let your
    laptop rest for a while. It deserves a break.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，另一章已经完成，我们将销毁集群，让你的笔记本休息一下，它也该休息了。
- en: '[PRE47]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: If you'd like to know more about Ingress, please explore Ingress v1beta1 extensions
    ([https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#ingress-v1beta1-extensions](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#ingress-v1beta1-extensions))
    API documentation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 Ingress 的信息，请查看 Ingress v1beta1 扩展的 [API 文档](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#ingress-v1beta1-extensions)。
- en: Before we move into the next chapter, we'll explore the differences between
    Kubernetes Ingress and its Docker Swarm equivalent.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一章之前，我们将探索 Kubernetes Ingress 与 Docker Swarm 中的等效项之间的区别。
- en: '![](img/7da80e08-4e4f-49b8-ae73-d32018eebff9.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7da80e08-4e4f-49b8-ae73-d32018eebff9.png)'
- en: 'Figure 7-4: The components explored so far'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-4：到目前为止已探索的组件
- en: Kubernetes Ingress compared to Docker Swarm equivalent
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes Ingress 与 Docker Swarm 的等效项比较
- en: Both Kubernetes and Docker Swarm have Ingress, and it might sound compelling
    to compare them and explore the differences. While that, at first glance, might
    seem like the right thing to do, there is a problem. Ingress works quite differently
    across the two.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和 Docker Swarm 都有 Ingress，比较它们并探索差异可能会很有吸引力。虽然从表面上看，似乎这是正确的做法，但问题在于，Ingress
    在两者中的工作方式差异很大。
- en: Swarm Ingress networking is much more similar to Kubernetes services. Both can,
    and should, be used to expose ports to clients both inside and outside a cluster.
    If we compare the two products, we'll discover that Kubernetes services are similar
    to a combination of Docker Swarm's Overlay and Ingress networking. The Overlay
    is used to provide communication between applications inside a cluster, and Swarm's
    Ingress is a flavor of Overlay network that publishes ports to the outside world.
    The truth is that Swarm does not have an equivalent to Kubernetes Ingress controllers.
    That is, *if we do not include Docker Enterprise Edition to the mix*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 的 Ingress 网络更像是 Kubernetes 服务。两者都可以并且应该用于向集群内外的客户端暴露端口。如果我们对这两个产品进行比较，我们会发现
    Kubernetes 服务类似于 Docker Swarm 的 Overlay 和 Ingress 网络的组合。Overlay 用于提供集群内应用程序之间的通信，而
    Swarm 的 Ingress 是一种 Overlay 网络，用于将端口发布到外部世界。事实是，Swarm 并没有 Kubernetes Ingress 控制器的等效物。也就是说，*如果我们不将
    Docker 企业版纳入考虑的话*。
- en: The fact that a Kubernetes Ingress equivalent does not ship with Docker Swarm
    does not mean that similar functionality cannot be accomplished through other
    means. It can. [Traefik](https://traefik.io/), for example, can act both as a
    Kubernetes Ingress Controller, as well as a dynamic Docker Swarm proxy. It provides,
    more or less, the same functionality no matter which scheduler you choose. If
    you're looking for a Swarm specific alternative, you might choose Docker Flow
    Proxy ([http://proxy.dockerflow.com/](http://proxy.dockerflow.com/)) (written
    by yours truly).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Ingress 等效的功能没有随 Docker Swarm 一起发布，并不意味着无法通过其他方式实现类似的功能。它是可以的。例如，[Traefik](https://traefik.io/)
    可以同时作为 Kubernetes Ingress 控制器，也可以作为动态的 Docker Swarm 代理。无论选择哪个调度程序，它提供的功能或多或少是相同的。如果你在寻找
    Swarm 特定的替代方案，你可能会选择 Docker Flow Proxy ([http://proxy.dockerflow.com/](http://proxy.dockerflow.com/))（由我亲自编写）。
- en: All in all, as soon as we stop comparing Ingress on both platforms and start
    looking for a similar set of functionality, we can quickly conclude that both
    Kubernetes and Docker Swarm allow a similar set of features. We can use paths
    and domains to route traffic from a single set of ports (for example, `80` and
    `443`) to a specific application that matches the rules. Both allow us to offload
    SSL certificates, and both provide solutions that make all the necessary configurations
    dynamically.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，一旦我们停止比较两个平台上的 Ingress，并开始寻找相似的功能集，我们可以迅速得出结论，Kubernetes 和 Docker Swarm
    都提供了相似的功能集。我们可以使用路径和域名将流量从一组端口（例如，`80` 和 `443`）路由到匹配规则的特定应用程序。两者都允许我们卸载 SSL 证书，并且都提供了使所有必要配置动态化的解决方案。
- en: If on the functional level both platforms provide a very similar set of features,
    can we conclude that there is no essential difference between the two schedulers
    when taking into account only dynamic routing and load balancing? *I would say
    no*. Some important differences might not be of functional nature.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从功能层面上讲，两个平台提供了非常相似的功能集，那么在仅考虑动态路由和负载均衡时，我们能得出结论说两个调度程序没有本质区别吗？*我认为不能*。一些重要的区别可能并非功能性方面的。
- en: 'Kubernetes provides a well-defined Ingress API that third-party solutions can
    utilize to deliver a seamless experience. Let''s take a look at one example:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了一个明确定义的 Ingress API，第三方解决方案可以利用它来提供无缝的体验。让我们看一个例子：
- en: '[PRE48]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This definition can be used with many different solutions. Behind this Ingress
    resource could be nginx, voyager, haproxy, or trafficserver Ingress controller.
    All of them use the same Ingress API to deduce which Services should be used by
    forwarding algorithms. Even Traefik, known for its incompatibility with commonly
    used Ingress annotations, would accept that YAML definition.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义可以与许多不同的解决方案一起使用。在这个 Ingress 资源后面可以是 nginx、voyager、haproxy 或 trafficserver
    Ingress 控制器。它们都使用相同的 Ingress API 来推断应该使用哪些服务进行转发算法。即使是以与常用的 Ingress 注解不兼容著称的 Traefik，也会接受这个
    YAML 定义。
- en: Having a well-defined API still leaves a lot of room for innovation. We can
    use `annotations` to provide the additional information our Ingress controller
    of choice might need. Some of the same annotations are used across different solutions,
    while the others are specific to a controller.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个明确定义的 API 仍然为创新留下了很大的空间。我们可以使用 `annotations` 提供我们的 Ingress 控制器可能需要的额外信息。某些注解在不同的解决方案中使用，而其他注解则是特定于某个控制器的。
- en: All in all, Kubernetes Ingress controller combines a well-defined (and simple)
    specification that all Ingress controllers must accept and, at the same time,
    it leaves ample room for innovation through custom `annotations` specified in
    `metadata`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Kubernetes Ingress 控制器结合了一个明确定义（且简单）的规范，所有 Ingress 控制器都必须接受，并且同时通过元数据中指定的自定义
    `annotations` 提供了创新的空间。
- en: Docker Swarm does not have anything resembling an Ingress API. Functionality
    similar to Kubernetes Ingress controllers can be accomplished either by using
    Swarm Kit or using the Docker API. The problem is that there is no defined API
    that third-party solutions should follow, so each is a world in itself. For example,
    understanding how Traefik works will not help you much when trying to switch to
    Docker Flow Proxy. Each is operated differently in isolation. There is no standard
    because Docker did not focus on making one.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 没有类似 Ingress API 的东西。类似 Kubernetes Ingress 控制器的功能可以通过使用 Swarm Kit
    或使用 Docker API 来实现。问题在于没有定义的 API 应该遵循第三方解决方案，因此每个解决方案都是独立的世界。例如，了解 Traefik 的工作原理并不能帮助你在尝试切换到
    Docker Flow Proxy 时多少有所帮助。每个解决方案都是孤立运行的，操作方式各不相同。由于 Docker 没有专注于制定标准，因此没有标准可循。
- en: Docker's approach to scheduling is based entirely on the features baked into
    Docker Server. There is only one way to do things. Often, that provides a very
    user-friendly and reliable experience. If Swarm does what you need it to do, it
    is an excellent choice. However, the problem occurs when you need more. In that
    case, you might experience difficulties finding a solution with Docker Swarm.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 对调度的方法完全基于内置到 Docker Server 中的功能。只有一种方法可以做到这一点。通常，这提供了非常用户友好和可靠的体验。如果
    Swarm 能够满足你的需求，那么它是一个很好的选择。但问题在于当你需要更多功能时，可能会在寻找 Docker Swarm 解决方案时遇到困难。
- en: When we compared Kubernetes ReplicaSets, Services, and Deployments with their
    Docker Swarm equivalents, the result was the same set of features. There was no
    substantial difference on the functional level. From the user experience perspective,
    Swarm provided much better results. Its YAML file was much more straightforward
    and more concise. With only those features in mind, Swarm had the edge over Kubernetes.
    This time it's different.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 Kubernetes 的 ReplicaSets、Services 和 Deployments 与它们在 Docker Swarm 中的等效物进行比较时，功能级别上没有实质性差异。从用户体验的角度来看，Swarm
    提供了更好的结果。它的 YAML 文件更为简单和简洁。仅仅考虑这些功能，Swarm 比 Kubernetes 更具优势。这一次情况不同了。
- en: Kubernetes strategy is primarily based on API. Once a specific type of a resource
    is defined, any solution can utilize it to provide the given functionality. That
    is especially true with Ingress. We can choose among a myriad of solutions. Some
    of them are developed and maintained by the Kubernetes community (for example,
    GLBC and NGINX Ingress controllers), while others are provided by third-parties.
    No matter where the solution comes from, it adheres to the same API and, therefore,
    to the same YAML definition. As a result, we have a more substantial number of
    solutions to choose from, without sacrificing consistency in how we define resources.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的策略主要基于 API。一旦定义了某种资源类型，任何解决方案都可以利用它来提供所需的功能。这在 Ingress 方面尤为明显。我们可以在众多解决方案中进行选择。其中一些由
    Kubernetes 社区开发和维护（例如 GLBC 和 NGINX Ingress 控制器），而其他则由第三方提供。无论解决方案来自何方，都遵循相同的 API
    和 YAML 定义。因此，我们有更多的解决方案可供选择，而不会牺牲资源定义的一致性。
- en: If we limit the comparison to Kubernetes Ingress controllers and their equivalents
    in Docker Swarm, the former is a clear winner. Assuming that the current strategy
    continues, Docker would need to add layer 7 forwarding into Docker Server if it
    is to get back in the game on this front. If we limit ourselves only to this set
    of features, Kubernetes wins through its Ingress API that opened the door, not
    only to internal solutions, but also to third-party Controllers.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将比较局限于 Kubernetes Ingress 控制器及其在 Docker Swarm 中的等效物，前者显然是胜者。假设当前策略持续下去，Docker
    需要将第 7 层转发添加到 Docker Server 中，才能在这一前沿重新进入竞争。如果我们仅限于这一组功能，Kubernetes 通过其 Ingress
    API 赢得胜利，它不仅打开了内部解决方案的大门，还包括第三方控制器。
- en: We are still at the beginning. There are many more features worth comparing.
    We only scratched the surface. Stay tuned for more.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还处于起步阶段。还有许多值得比较的特性。我们只是触及了表面。请继续关注更多信息。
