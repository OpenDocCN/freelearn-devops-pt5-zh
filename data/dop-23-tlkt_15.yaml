- en: Persisting State
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化状态
- en: Having fault-tolerance and high-availability is of no use if we lose application
    state during rescheduling. Having state is unavoidable, and we need to preserve
    it no matter what happens to our applications, servers, or even a whole datacenter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在重新调度时丢失应用状态，那么无论我们有多高的容错能力和高可用性，都没有意义。拥有状态是不可避免的，我们需要无论发生什么，始终保存它，不论是应用程序、服务器，甚至整个数据中心。
- en: The way to preserve the state of our applications depends on their architecture.
    Some are storing data in-memory and rely on periodic backups. Others are capable
    of synchronizing data between multiple replicas, so that loss instance of one
    does not result in loss of data. Most, however, are relying on disk to store their
    state. We'll focus on that group of stateful applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 保存应用状态的方式取决于它们的架构。一些应用将数据存储在内存中，并依赖于定期备份。其他应用能够在多个副本之间同步数据，因此一个副本的丢失不会导致数据丢失。然而，大多数应用依赖于磁盘来存储它们的状态。我们将重点关注这一类有状态的应用。
- en: If we are to build fault-tolerant systems, we need to make sure that failure
    of any part of the system is recoverable. Since speed is of the essence, we cannot
    rely on manual operations to recuperate from failures. Even if we could, no one
    wants to be the person sitting in front of a screen, waiting for something to
    fail, only to bring it back to its previous state.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要构建容错系统，就需要确保系统任何部分的故障都能恢复。由于速度至关重要，我们不能依赖手动操作来从故障中恢复。即使我们能做到，也没有人愿意坐在屏幕前，等待某些东西发生故障，然后再把它恢复到先前的状态。
- en: We already saw that Kubernetes would, in most cases, recuperate from a failure
    of an application, of a server, or even of a whole datacenter. It'll reschedule
    Pods to healthy nodes. We also experienced how AWS and kops accomplish more or
    less the same effect on the infrastructure level. Auto-scaling groups will recreate
    failed nodes and, since they are provisioned with kops startup processes, new
    instances will have everything they need, and they will join the cluster.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，Kubernetes 在大多数情况下会从应用程序、服务器，甚至整个数据中心的故障中恢复。它会将 Pods 重新调度到健康节点。我们也体验过
    AWS 和 kops 在基础设施层面上实现大致相同效果的方式。自动扩展组会重新创建故障节点，且由于它们使用 kops 启动过程进行配置，新的实例会拥有所需的所有内容，并加入集群。
- en: The only thing that prevents us from saying that our system is (mostly) highly
    available and fault tolerant is the fact that we did not solve the problem of
    persisting state across failures. That's the subject we'll explore next.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一阻止我们称系统（大多数情况下）具备高可用性和容错性的是我们没有解决在故障发生时如何保持状态的问题。这也是我们接下来将要探讨的主题。
- en: We'll try to preserve our data no matter what happens to our stateful applications
    or the servers where they run.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的有状态应用程序或其运行的服务器发生什么，我们都会尽力保留我们的数据。
- en: Creating a Kubernetes cluster
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Kubernetes 集群
- en: 'We''ll start by recreating a similar cluster as the one we used in the previous
    chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过重新创建与上一章相似的集群开始：
- en: All the commands from this chapter are available in the [`15-pv.sh`](https://gist.github.com/41c86eb385dfc5c881d910c5e98596f2)
    ([https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2](https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2))
    Gist.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令可以在 [`15-pv.sh`](https://gist.github.com/41c86eb385dfc5c881d910c5e98596f2)
    ([https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2](https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2))
    Gist 中找到。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We entered the local copy of the `k8s-specs` repository, pulled the latest code,
    and went into the `cluster` directory.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入了 `k8s-specs` 仓库的本地副本，拉取了最新的代码，并进入了 `cluster` 目录。
- en: In the previous chapter, we stored the environment variables we used in the
    `kops` file. Let's take a quick look at them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将使用的环境变量存储在 `kops` 文件中。让我们快速看一下它们。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output, without the keys, is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果（不包括密钥）如下：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By storing the environment variables in a file, we can fast-track the process
    by loading them using the `source` command.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将环境变量存储在文件中，我们可以通过 `source` 命令快速加载它们，从而加速该过程。
- en: In the older editions of the book, there was an error in the command we used
    to store the environment variables in the `kops` file. The `export` commands were
    missing. Please ensure that your copy of the file has all the lines starting with
    `export`. If that's not the case, please update it accordingly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在书籍的早期版本中，我们用于存储环境变量到 `kops` 文件的命令存在错误。`export` 命令缺失。请确保你文件中的所有行都以 `export`
    开头。如果不是这种情况，请相应地更新它。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that the environment variables are set, we can proceed to create an `S3`
    bucket:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在环境变量已设置好，我们可以继续创建`S3`存储桶：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The command that creates the `kops` alias is as follows. Execute it only if
    you are a **Windows user**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`kops`别名的命令如下。只有**Windows用户**执行此命令：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we can, finally, create a new Kubernetes cluster in AWS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以在AWS中创建一个新的Kubernetes集群了。
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If we compare that command with the one we executed in the previous chapter,
    we'll notice only a few minor changes. We increased `node-count` to `2` and `node-size`
    to `t2.medium`. That will give us more than enough capacity for the exercises
    we'll run in this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将此命令与上一章执行的命令进行比较，会发现只有几个小变化。我们将`node-count`增加到`2`，`node-size`增大为`t2.medium`。这将为我们提供足够的容量，以应对本章中的所有练习。
- en: 'Let''s validate the cluster:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证集群：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Assuming that enough time passed since we executed `kops create cluster`, the
    output should indicate that the `cluster devops23.k8s.local is ready`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在我们执行`kops create cluster`后已经过了一段时间，输出应表明`cluster devops23.k8s.local is ready`。
- en: A note to Windows users
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给Windows用户的提示
- en: Kops was executed inside a container. It changed the context inside the container
    that is now gone. As a result, your local `kubectl` context was left intact. We'll
    fix that by executing `kops export kubecfg --name ${NAME}` and `export KUBECONFIG=$PWD/config/kubecfg.yaml`.
    The first command exported the config to `/config/kubecfg.yaml`. That path was
    specified through the environment variable `KUBECONFIG` and is mounted as `config/kubecfg.yaml`
    on local hard disk. The latter command exports `KUBECONFIG` locally. Through that
    variable, `kubectl` is now instructed to use the configuration in `config/kubecfg.yaml`
    instead of the default one. Before you run those commands, please give AWS a few
    minutes to create all the EC2 instances and for them to join the cluster. After
    waiting and executing those commands, you'll be all set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kops在容器内执行。它改变了容器内的上下文，但现在上下文已消失。结果，你本地的`kubectl`上下文保持不变。我们可以通过执行`kops export
    kubecfg --name ${NAME}`和`export KUBECONFIG=$PWD/config/kubecfg.yaml`来解决这个问题。第一个命令将配置导出到`/config/kubecfg.yaml`。该路径通过环境变量`KUBECONFIG`指定，并被挂载为本地硬盘上的`config/kubecfg.yaml`。后一个命令将在本地导出`KUBECONFIG`。通过这个变量，`kubectl`现在会使用`config/kubecfg.yaml`中的配置，而不是默认配置。在你执行这些命令之前，请给AWS一些时间来创建所有EC2实例，并让它们加入集群。等待并执行这些命令后，一切就绪。
- en: We'll need Ingress if we'd like to access the applications we'll deploy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望访问将要部署的应用程序，我们将需要Ingress。
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Ingress will not help us much without the ELB DNS, so we''ll get that as well:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有ELB DNS，Ingress对我们帮助不大，所以我们也需要获取ELB DNS：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output of the latter command should end with `us-east-2.elb.amazonaws.com`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出应该以`us-east-2.elb.amazonaws.com`结尾。
- en: Finally, now that we are finished with the cluster setup, we can go back to
    the repository root directory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，既然集群设置已经完成，我们可以回到仓库根目录。
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Deploying stateful applications without persisting state
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署无状态持久化的状态应用
- en: We'll start the exploration by deploying a stateful application without any
    mechanism to persist its state. That will give us a better insight into benefits
    behind of some of the Kubernetes concepts and resources we'll use in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过部署一个没有任何持久化机制的状态应用程序来开始探索。这将帮助我们更好地理解在本章中使用的一些Kubernetes概念和资源的好处。
- en: We already deployed Jenkins a few times. Since it is a stateful application,
    it is an excellent candidate to serve as a playground.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经部署过几次Jenkins。由于它是一个有状态的应用程序，非常适合用作实验平台。
- en: Let's take a look at a definition stored in the `pv/jenkins-no-pv.yml` file.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下存储在`pv/jenkins-no-pv.yml`文件中的定义。
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The YAML defines the `jenkins` namespace, an Ingress controller, and a service.
    We're already familiar with those types of resources so we'll skip explaining
    them and jump straight to the Deployment definition.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: YAML定义了`jenkins`命名空间、Ingress控制器和服务。我们已经熟悉这些资源类型，因此跳过解释，直接进入Deployment定义。
- en: 'The output of the `cat` command, limited to the `jenkins` Deployment, is as
    follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`cat`命令的输出，限定为`jenkins`部署，如下所示：'
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There's nothing special about this Deployment. We already used a very similar
    one. Besides, by now, you're an expert at Deployment controllers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署没有什么特别的地方。我们已经使用过非常类似的部署。此外，到现在为止，你已经是部署控制器的专家了。
- en: The only thing worth mentioning is that there is only one volume mount and it
    references a secret we're using to provide Jenkins with the initial administrative
    user. Jenkins is persisting its state in `/var/jenkins_home`, and we are not mounting
    that directory.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一值得一提的是，只有一个卷挂载，它引用了我们用来为 Jenkins 提供初始管理员用户的秘密。Jenkins 将其状态保存在 `/var/jenkins_home`
    中，而我们没有挂载该目录。
- en: 'Let''s create the resources defined in `pv/jenkins-no-pv.yml`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建在 `pv/jenkins-no-pv.yml` 中定义的资源：
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We''ll take a quick look at the events as a way to check that everything was
    deployed successfully:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速查看事件，作为检查一切是否成功部署的方式：
- en: '[PRE15]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output, limited to relevant parts, is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（仅限相关部分）如下：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can see that the setup of the only volume failed since it could not find
    the secret referenced as `jenkins-creds`. Let''s create it:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到唯一的卷设置失败，因为它找不到作为 `jenkins-creds` 引用的秘密。让我们创建它：
- en: '[PRE17]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, with the secret `jenkins-creds` created in the `jenkins` namespace, we
    can confirm that the rollout of the Deployment was successful.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着 `jenkins` 命名空间中创建的秘密 `jenkins-creds`，我们可以确认部署的发布成功。
- en: '[PRE18]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see, from the output, that the `deployment "jenkins" was successfully
    rolled out`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从输出中看到，`deployment "jenkins"` 已成功发布。
- en: 'Now that everything is up and running, we can open Jenkins UI in a browser:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切正常运行，我们可以在浏览器中打开 Jenkins 用户界面：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: A note to Windows users
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Windows 用户的说明
- en: Git Bash might not be able to use the `open` command. If that's the case, please
    replace the `open` command with `echo`. As a result, you'll get the full address
    that should be opened directly in your browser of choice.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 `open` 命令。如果是这种情况，请将 `open` 命令替换为 `echo`。这样，你将得到应在你选择的浏览器中直接打开的完整地址。
- en: Please click the Log in link, type `jdoe` as the User, and `incognito` as the
    Password. When finished, click the log in button.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击登录链接，输入 `jdoe` 作为用户，`incognito` 作为密码。完成后，点击登录按钮。
- en: Now that we are authenticated as jdoe administrator, we can proceed and create
    a job. That will generate a state that we can use to explore what happens when
    a stateful application fails.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们以 jdoe 管理员身份认证成功，我们可以继续创建一个任务。这将生成一个状态，我们可以用来探索有状态应用程序失败时会发生什么。
- en: Please click the create new jobs link, type `my-job` as the item name, select
    Pipeline as the job type, and press the OK button.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击创建新任务链接，输入 `my-job` 作为项目名称，选择管道作为任务类型，并点击确定按钮。
- en: You'll be presented with the job configuration screen. There's no need to do
    anything here since we are not, at the moment, interested in any specific Pipeline
    definition. It's enough to click the Save button.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到任务配置屏幕。这里无需做任何操作，因为我们目前并不关心任何特定的管道定义。只需点击保存按钮即可。
- en: Next, we'll simulate a failure by killing `java` process running inside the
    Pod created by the `jenkins` Deployment. To do that, we need to find out the name
    of the Pod.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过终止 `jenkins` 部署中创建的 Pod 内运行的 `java` 进程来模拟一个故障。为了做到这一点，我们需要找出 Pod 的名称。
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We retrieved the Pods from the `jenkins` namespace, filtered them with the selector
    `api=jenkins`, and formatted the output as `json`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `jenkins` 命名空间中获取了 Pods，使用选择器 `api=jenkins` 进行过滤，并将输出格式化为 `json`。
- en: 'The output, limited to the relevant parts, as is follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（仅限相关部分）如下：
- en: '[PRE21]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that the name is inside `metadata` entry of one of the `items`.
    We can use that to formulate `jsonpath` that will retrieve only the name of the
    Pod:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到名称位于某个 `items` 的 `metadata` 条目中。我们可以利用这一点来构建 `jsonpath`，仅提取 Pod 的名称：
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The name of the Pod is now stored in the environment variable `POD_NAME`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的名称现在存储在环境变量 `POD_NAME` 中。
- en: 'The output of the latter command is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 后续命令的输出如下：
- en: '[PRE23]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that we know the name of the Pod hosting Jenkins, we can proceed and kill
    the `java` process:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了托管 Jenkins 的 Pod 名称，我们可以继续并终止 `java` 进程：
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The container failed once we killed Jenkins process. We already know from experience
    that a failed container inside a Pod will be recreated. As a result, we had a
    short downtime, but Jenkins is running once again.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们终止了 Jenkins 进程，容器就失败了。从经验来看，我们知道 Pod 内部的失败容器会被重新创建。结果是，我们有一个短暂的停机时间，但 Jenkins
    已经再次运行。
- en: 'Let''s see what happened to the job we created earlier. I''m sure you know
    the answer, but we''ll check it anyway:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看之前创建的任务发生了什么。我相信你知道答案，但我们还是检查一下：
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As expected, `my-job` is nowhere to be found. The container that was hosting
    `/var/jenkins_home` directory failed, and it was replaced with a new one. The
    state we created is lost.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`my-job` 无法找到。托管 `/var/jenkins_home` 目录的容器失败了，并且被替换成了一个新的容器。我们创建的状态丢失了。
- en: Truth be told, we already saw in the [Chapter 8](a8a54c79-1dda-41ec-8ad5-4986c17b7041.xhtml), *Using
    Volumes to Access Host's File System* that we can mount a volume in an attempt
    to preserve state across failures. However, in the past, we used `emptyDir` which
    mounts a local volume. Even though that's better than nothing, such a volume exists
    only as long as the server it is stored in is up and running. If the server would
    fail, the state stored in `emptyDir` would be gone. Such a solution would be only
    slightly better than not using any volume. By using local disk we would only postpone
    inevitable, and, sooner or later, we'd get to the same situation. We'd be left
    wondering why we lost everything we created in Jenkins. We can do better than
    that.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，我们已经在 [第八章](a8a54c79-1dda-41ec-8ad5-4986c17b7041.xhtml)，*使用卷访问主机文件系统* 中看到了我们可以挂载卷来尝试保持状态跨故障的情况。然而，过去我们使用的是
    `emptyDir`，它挂载了一个本地卷。尽管比什么都不使用要好，但这种卷仅在存储它的服务器运行时存在。如果服务器发生故障，存储在 `emptyDir` 中的状态就会丢失。这样的解决方案仅比不使用任何卷稍好一些。通过使用本地磁盘，我们只是在推迟不可避免的故障，迟早我们会遇到同样的情况。最后我们会困惑于为什么在
    Jenkins 中创建的所有内容都丢失了。我们可以做得更好。
- en: Creating AWS volumes
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 AWS 卷
- en: If we want to persist state that will survive even server failures, we have
    two options we can choose. We could, for example, store data locally and replicate
    it to multiple servers. That way, a container could use local storage knowing
    that the files are available on all the servers. Such a setup would be too complicated
    if we'd like to implement the process ourselves. Truth be told, we could use one
    of the volume drivers for that. However, we'll opt for a more commonly used method
    to persist the state across failures. We'll use external storage.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望保存即使在服务器故障后也能持久化的状态，我们有两个可选择的方案。例如，我们可以将数据存储在本地，并将其复制到多个服务器上。这样，容器可以使用本地存储，并确保文件在所有服务器上都能访问到。如果我们自己实现这一过程，配置起来会非常复杂。说实话，我们可以使用某些卷驱动程序来实现这一点。但我们会选择一种更常用的方法来确保状态在故障中持久化。我们将使用外部存储。
- en: Since we are running our cluster in AWS, we can choose between [`S3`](https://aws.amazon.com/s3/)
    ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)), [**Elastic File System**
    (**EFS**)](https://aws.amazon.com/efs/) ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)),
    and [**Elastic Block Store**](https://aws.amazon.com/ebs/) (**EBS**) ([https://aws.amazon.com/ebs/](https://aws.amazon.com/ebs/)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在 AWS 中运行我们的集群，我们可以在 [`S3`](https://aws.amazon.com/s3/) ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))、[**弹性文件系统**（**EFS**)](https://aws.amazon.com/efs/)
    ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)) 和 [**弹性块存储**](https://aws.amazon.com/ebs/)
    (**EBS**)([https://aws.amazon.com/ebs/](https://aws.amazon.com/ebs/)) 之间进行选择。
- en: S3 is meant to be accessed through its API and is not suitable as a local disk
    replacement. That leaves us with EFS and EBS.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: S3 是通过其 API 访问的，并不适合作为本地磁盘的替代品。因此，我们只剩下了 EFS 和 EBS 作为选择。
- en: EFS, has a distinct advantage that it can be mounted to multiple EC2 instances
    spread across multiple availability zones. It is closest we can get to fault-tolerant
    storage. Even if a whole zone (datacenter) fails, we'll still be able to use EFS
    in the rest of the zones used by our cluster. However, that comes at a cost. EFS
    introduces a performance penalty. It is, after all, a **network file system**
    (**NFS**), and that entails higher latency.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: EFS 有一个显著的优势，它可以挂载到多个 EC2 实例，并跨多个可用区进行分布。它是我们能够实现的最接近容错存储的解决方案。即使整个可用区（数据中心）发生故障，我们仍然可以在集群所使用的其他可用区中使用
    EFS。然而，这也带来了一定的成本。EFS 会引入性能惩罚。毕竟，它是一个 **网络文件系统**（**NFS**），这意味着它会带来更高的延迟。
- en: '**Elastic Block Store** (**EBS**) is the fastest storage we can use in AWS.
    Its data access latency is very low thus making it the best choice when performance
    is the primary concern. The downside is availability. It doesn''t work in multiple
    availability zones. Failure of one will mean downtime, at least until the zone
    is restored to its operational state.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性块存储**（**EBS**）是我们在 AWS 中可以使用的最快存储。它的数据访问延迟非常低，因此在性能是主要关注点时，它是最佳选择。缺点是可用性。它不能跨多个可用区工作。如果某个区发生故障，意味着会有停机时间，至少直到该区恢复正常运行状态。'
- en: We'll choose EBS for our storage needs. Jenkins depends heavily on IO, and we
    need data access to be as fast as possible. However, there is another reason for
    such a choice. EBS is fully supported by Kubernetes. EFS will come but, at the
    time of this writing, it is still in the experimental stage. As a bonus advantage,
    EBS is much cheaper than EFS.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择 EBS 来满足我们的存储需求。Jenkins 强烈依赖 I/O，因此我们需要尽可能快速地访问数据。然而，选择 EBS 还有一个原因。EBS
    完全支持 Kubernetes。EFS 将会推出，但在写作时，它仍处于实验阶段。作为一个额外的优势，EBS 比 EFS 便宜得多。
- en: Given the requirements and what Kubernetes offers, the choice is obvious. We'll
    use EBS, even though we might run into trouble if the availability zone where
    our Jenkins will run goes down. In such a case, we'd need to migrate EBS volume
    to a healthy zone. There's no such thing as a perfect solution.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到需求和 Kubernetes 提供的功能，选择显而易见。我们将使用 EBS，尽管如果 Jenkins 所在的可用区发生故障，我们可能会遇到问题。在这种情况下，我们需要将
    EBS 卷迁移到一个健康的区域。没有完美的解决方案。
- en: We are jumping ahead of ourselves. We'll leave Kubernetes aside for a while
    and concentrate on creating an EBS volume.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有些急于求成了。我们暂时搁置 Kubernetes，集中精力创建 EBS 卷。
- en: Each EBS volume is tied to an availability zone. Unlike EFS, EBS cannot span
    multiple zones. So, the first thing we need to do is to find out which are the
    zones worker nodes are running in. We can get that information by describing the
    EC2 instances belonging to the security group `nodes.devops23.k8s.local`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 EBS 卷都与一个可用区绑定。与 EFS 不同，EBS 不能跨多个可用区。所以，我们首先需要做的是找出工作节点所在的区域。我们可以通过描述属于安全组
    `nodes.devops23.k8s.local` 的 EC2 实例来获取这些信息。
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，限定为相关部分，如下：
- en: '[PRE27]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the information is inside the `Reservations.Instances` array.
    To get the zone, we need to filter the output by the `SecurityGroups.GroupName`
    field. Zone name is located in the `Placement.AvailabilityZone` field.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到信息位于 `Reservations.Instances` 数组中。要获取区域，我们需要通过 `SecurityGroups.GroupName`
    字段筛选输出。区域名称位于 `Placement.AvailabilityZone` 字段中。
- en: 'The command that does the filtering and retrieves the availability zones of
    the worker nodes is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 进行筛选并获取工作节点可用区的命令如下：
- en: '[PRE28]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can see that the two worker nodes are located in the zones `us-east-2a` and
    `us-east-2c`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，两个工作节点位于区域 `us-east-2a` 和 `us-east-2c`。
- en: 'The commands that retrieve the zones of the two worker nodes and store them
    in environment variables is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 检索两个工作节点区域并将其存储在环境变量中的命令如下：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We retrieved the zones and stored the output into the `zones` file. Further
    on, we retrieved the first row with the `head` command and stored it in the environment
    variable `AZ_1`. Similarly, we stored the last (the second) row in the variable
    `AZ_2`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索了区域并将输出存储到 `zones` 文件中。接下来，我们用 `head` 命令检索了第一行并将其存储在环境变量 `AZ_1` 中。同样地，我们将最后一行（第二行）存储在变量
    `AZ_2` 中。
- en: Now we have all the information we need to create a few volumes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了创建几个卷所需的所有信息。
- en: The command that follows requires a relatively newer version of `aws`. If it
    fails, please update your AWS CLI binary to the latest version.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的命令需要较新的 `aws` 版本。如果执行失败，请更新您的 AWS CLI 二进制文件到最新版本。
- en: '[PRE31]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We executed `aws ec2 create-volume` command three times. As a result, we created
    three EBS volumes. Two of them are in one zone, while the third is in another.
    They all have `10` GB of space. We chose `gp2` as the type of the volumes. The
    other types either require bigger sizes or are more expensive. When in doubt,
    `gp2` is usually the best choice for EBS volumes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行了三次 `aws ec2 create-volume` 命令，结果创建了三个 EBS 卷。两个在同一区域，第三个在另一区域。它们的空间都是 `10`
    GB。我们选择了 `gp2` 作为卷类型。其他类型要么需要更大的大小，要么更贵。若有疑问，`gp2` 通常是 EBS 卷的最佳选择。
- en: We also defined a tag that will help us distinguish the volumes dedicated to
    this cluster from those we might have in our AWS account for other purposes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个标签，用于帮助我们区分专门为该集群分配的卷与我们在 AWS 账户中为其他用途可能拥有的卷。
- en: Finally, `jq` filtered the output so that only the volume ID is retrieved. The
    results are stored in the environment variables `VOLUME_ID_1`, `VOLUME_ID_2`,
    and `VOLUME_ID_3`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`jq` 筛选了输出，只提取了卷 ID。结果存储在环境变量 `VOLUME_ID_1`、`VOLUME_ID_2` 和 `VOLUME_ID_3`
    中。
- en: 'Let''s take a quick look at one of the IDs we stored as an environment variable:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看一下我们作为环境变量存储的其中一个 ID：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Finally, to be on the safe side, we'll list the volume that matches the ID and
    thus confirm, without doubt, that the EBS was indeed created.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了安全起见，我们将列出与 ID 匹配的卷，从而毫无疑问地确认 EBS 确实已创建。
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE35]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that the EBS volumes are indeed `available` and in the same zones as the
    worker nodes, we can proceed and create Kubernetes persistent volumes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 EBS 卷确实是 `available` 并且与工作节点位于同一可用区，我们可以继续创建 Kubernetes 持久化卷。
- en: '![](img/3c2542d1-2653-4569-9932-9be7c575aa3f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c2542d1-2653-4569-9932-9be7c575aa3f.png)'
- en: 'Figure 15-1: EBS volumes created in the same zones as the worker nodes'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-1：在与工作节点相同的可用区中创建的 EBS 卷
- en: Creating Kubernetes persistent volumes
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Kubernetes 持久化卷
- en: The fact that we have a few EBS volumes available does not mean that Kubernetes
    knows about their existence. We need to add PersistentVolumes that will act as
    a bridge between our Kubernetes cluster and AWS EBS volumes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个 EBS 卷可用，但这并不意味着 Kubernetes 知道它们的存在。我们需要添加 PersistentVolumes，它们将作为 Kubernetes
    集群与 AWS EBS 卷之间的桥梁。
- en: PersistentVolumes allow us to abstract details of how storage is provided (for
    example, EBS) from how it is consumed. Just like Volumes, PersistentVolumes are
    resources in a Kubernetes cluster. The main difference is that their lifecycle
    is independent of individual Pods that are using them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: PersistentVolumes 使我们能够抽象存储提供的细节（例如 EBS）与如何使用存储的细节。就像 Volumes 一样，PersistentVolumes
    是 Kubernetes 集群中的资源。主要的区别是它们的生命周期独立于使用它们的单个 Pod。
- en: 'Let''s take a look at a definition that will create a few PersistentVolumes:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个定义，它将创建一些 PersistentVolumes：
- en: '[PRE36]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output, limited to the first of the three volumes, is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果，限于三卷中的第一卷，具体如下：
- en: '[PRE37]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `spec` section features a few interesting details. We set `manual-ebs` as
    the storage class name. We'll see later what is its function. For now, just remember
    the name.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec` 部分包含了一些有趣的细节。我们将 `manual-ebs` 设置为存储类名称。稍后我们将了解它的功能。目前，只需要记住这个名称。'
- en: We defined that the storage capacity is `5Gi`. It does not need to be the same
    as the capacity of the EBS we created earlier, as long as it is not bigger. Kubernetes
    will try to match `PersistentVolume` with, in this case, EBS that has a similar,
    if not the same capacity. Since we have only one EBS volume with 10 GB, it is
    the closest (and the only) match to the `PersistentVolume` request of `5Gi`. Ideally,
    persistent volumes capacity should match EBS size, but I wanted to demonstrate
    that any value equal to or less then the actual size should do.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了存储容量为 `5Gi`。它不需要与我们之前创建的 EBS 容量相同，只要不大于即可。Kubernetes 会尽量匹配 `PersistentVolume`
    和容量相似的 EBS，在这种情况下，容量相同或类似的 EBS。如果我们只有一个 10 GB 的 EBS 卷，它就是 `5Gi` 请求的最接近（也是唯一）匹配。理想情况下，持久卷的容量应与
    EBS 大小匹配，但我想展示任何小于或等于实际大小的值都能满足要求。
- en: We specified that the access mode should be `ReadWriteOnce`. That means that
    we'll be able to mount the volume as read-write only once. Only one Pod will be
    able to use it at any given moment. Such a strategy fits us well since EBS cannot
    be mounted to multiple instances. Our choice of the access mode is not truly a
    choice, but more an acknowledgment of the way how EBS works. The alternative modes
    are `ReadOnlyMany` and `ReadWriteMany`. Both modes would result in volumes that
    could be mounted to multiple Pods, either as read-only or read-write. Those modes
    would be more suitable for NFS like, for example, EFS, which can be mounted by
    multiple instances.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了访问模式为 `ReadWriteOnce`。这意味着我们将能够以只读写一次的方式挂载卷。任何时候只有一个 Pod 能够使用它。由于 EBS 不能挂载到多个实例，这种策略对我们非常适合。我们选择的访问模式不算真正的选择，而是对
    EBS 工作方式的确认。其他访问模式是 `ReadOnlyMany` 和 `ReadWriteMany`。这两种模式会导致卷可以挂载到多个 Pod，分别以只读或读写模式。这些模式更适合像
    EFS 这样的 NFS 系统，后者可以被多个实例挂载。
- en: The `spec` fields we explored so far are common to all persistent volume types.
    Besides those, there are entries specific to the actual volume we are associating
    with a Kubernetes `PersistentVolume`. Since we're going to use EBS, we specified
    `awsElasticBlockStore` with the volume ID and file system type. Since I could
    not know in advance what will be the ID of your EBS volume, the definition has
    the value set to `REPLACE_ME`. Later on, we'll replace it with the ID of the EBS
    we created earlier.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们探讨过的`spec`字段是所有持久卷类型通用的。除此之外，还有一些条目是特定于我们与Kubernetes `PersistentVolume`关联的实际卷的。由于我们将使用EBS，因此我们指定了`awsElasticBlockStore`，并提供了卷ID和文件系统类型。由于我无法提前知道您EBS卷的ID，所以在定义中将其值设置为`REPLACE_ME`。稍后我们将用之前创建的EBS的ID来替换它。
- en: There are many other types we could have specified instead. If this cluster
    would run on Azure, we could use `azureDisk` or `azureFile`. In **Google Compute
    Engine** (**GCE**) it would be `GCEPersistentDisk`. We could have setup `Glusterfs`.
    Or, if we would have this cluster running in an on-prem data center, it would
    probably be `nfs`. There are quite a few others we could use but, since we're
    running the cluster in AWS, many would not work, while others could be too difficult
    to set up. Since EBS is already available, we'll just roll with it. All in all,
    this cluster is in AWS, and `awsElasticBlockStore` is the easiest, if not the
    best choice.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本来可以指定许多其他类型。如果这个集群运行在Azure上，我们可以使用`azureDisk`或`azureFile`。在**Google Compute
    Engine**（**GCE**）中，它将是`GCEPersistentDisk`。我们也可以设置`Glusterfs`。或者，如果我们把这个集群运行在本地数据中心，可能会使用`nfs`。我们可以使用的类型还有很多，但由于我们在AWS上运行集群，许多类型将无法使用，而其他一些可能会设置起来太复杂。由于EBS已经可用，我们就选择它吧。总的来说，这个集群运行在AWS上，而`awsElasticBlockStore`是最简单的，若不是最好的选择。
- en: 'Now that we have an understanding of the YAML definition, we can proceed and
    create the `PersistentVolume`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了YAML定义，我们可以继续创建`PersistentVolume`：
- en: '[PRE38]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We used `cat` to output the contents of the `pv/pv.yml` file and pipe it into
    `sed` commands which, in turn, replaced the `REPLACE_ME_*` strings with the IDs
    of the EBS volumes we created earlier. The result was sent to the `kubectl create`
    command that created persistent volumes. As a result, we can see from the output
    that all three PersistentVolumes were created.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`cat`命令输出`pv/pv.yml`文件的内容，并将其管道传输到`sed`命令，这些命令又将`REPLACE_ME_*`字符串替换为我们之前创建的EBS卷的ID。结果被传递给`kubectl
    create`命令，后者创建了持久卷。从输出结果可以看到，所有三个PersistentVolume都已创建。
- en: Let's take a look at the persistent volumes currently available in our cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下当前在集群中可用的持久卷。
- en: '[PRE39]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE40]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'It should come as no surprise that we have three volumes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇的是，我们有三个卷：
- en: The interesting part of the information we're seeing are the statuses. The persistent
    volumes are `available`. We created them, but no one is using them. They just
    sit there waiting for someone to claim them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的有趣部分是状态信息。持久卷是`可用的`。我们已经创建了它们，但没有人使用它们。它们只是静静地等待某人来声明它们。
- en: '![](img/44bc1af7-6d59-4e75-96e4-ef26a0e9ff49.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44bc1af7-6d59-4e75-96e4-ef26a0e9ff49.png)'
- en: 'Figure 15-2: Kubernetes persistent volumes tied to EBS volumes'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-2：与EBS卷关联的Kubernetes持久卷
- en: Claiming persistent volumes
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明持久卷
- en: Kubernetes persistent volumes are useless if no one uses them. They exist only
    as objects with relation to, in our case, specific EBS volumes. They are waiting
    for someone to claim them through the `PersistentVolumeClaim` resource.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有人使用它们，Kubernetes持久卷是没有用的。它们仅作为与特定EBS卷相关的对象存在。它们在等待通过`PersistentVolumeClaim`资源来声明它们。
- en: Just like Pods which can request specific resources like memory and CPU, `PersistentVolumeClaims`
    can request particular sizes and access modes. Both are, in a way, consuming resources,
    even though of different types. Just as Pods should not specify on which node
    they should run, `PersistentVolumeClaims` cannot define which volume they should
    mount. Instead, Kubernetes scheduler will assign them a volume depending on the
    claimed resources.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 就像可以请求特定资源（如内存和CPU）的Pod一样，`PersistentVolumeClaims`也可以请求特定的大小和访问模式。尽管它们的类型不同，但两者从某种程度上来说都在消耗资源。就像Pod不应指定在哪个节点上运行一样，`PersistentVolumeClaims`也不能定义应挂载哪个卷。相反，Kubernetes调度器将根据请求的资源为它们分配一个卷。
- en: 'We''ll use `pv/pvc.yml` to explore how we could claim a persistent volume:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pv/pvc.yml`来探讨如何声明一个持久卷：
- en: '[PRE41]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The YAML file defines a `PersistentVolumeClaim` with the storage class name
    `manual-ebs`. That is the same class as the persistent volumes `manual-ebs-*`
    we created earlier. The access mode and the storage request are also matching
    what we defined for the persistent volume.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 文件定义了一个存储类名称为 `manual-ebs` 的 `PersistentVolumeClaim`。这与我们之前创建的持久卷 `manual-ebs-*`
    使用的存储类相同。访问模式和存储请求也与我们为持久卷定义的内容匹配。
- en: Please note that we are not specifying which volume we'd like to use. Instead,
    this claim specifies a set of attributes (`storageClassName`, `accessModes`, and
    `storage`). Any of the volumes in the system that match those specifications might
    be claimed by the `PersistentVolumeClaim` named `jenkins`. Bear in mind that `resources`
    do not have to be the exact match. Any volume that has the same or bigger amount
    of storage is considered a match. A claim for `1Gi` can be translated to *at least
    1Gi*. In our case, a claim for `1Gi` matches all three persistent volumes since
    they are set to `5Gi`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有指定要使用哪个卷。相反，这个声明指定了一组属性（`storageClassName`、`accessModes` 和 `storage`）。系统中任何符合这些规格的卷都可能被名为`jenkins`的`PersistentVolumeClaim`声明。请记住，`resources`不必完全匹配。任何具有相同或更大存储量的卷都被视为匹配。对于`1Gi`的请求可以转化为*至少1Gi*。在我们的案例中，`1Gi`的请求匹配所有三个持久卷，因为它们的存储量被设置为`5Gi`。
- en: 'Now that we explored the definition of the claim, we can proceed, and create
    it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了声明的定义，我们可以继续并创建它：
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The output indicates that the `persistentvolumeclaim "jenkins" was created`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示 `persistentvolumeclaim "jenkins" 已创建`。
- en: 'Let''s list the claims and see what we got:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出这些声明，看看我们得到了什么：
- en: '[PRE44]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE45]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We see from the output that the status of the claim is `Bound`. That means
    that the claim found a matching persistent volume and bounded it. We can confirm
    that by listing the volumes:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从输出中看到，声明的状态是`Bound`。这意味着声明找到了匹配的持久卷并将其绑定。我们可以通过列出卷来确认这一点：
- en: '[PRE46]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE47]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We can see that one of the volumes (`manual-ebs-02`) changed the status from
    `Available` to `Bound`. That is the volume bound to the claim we created a moment
    ago. We can see that the claim comes from `jenkins` namespace and `jenkins``PersistentVolumeClaim`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，其中一个卷（`manual-ebs-02`）将状态从`Available`更改为`Bound`。这就是我们刚刚创建的索赔所绑定的卷。我们可以看到，该索赔来自`jenkins`命名空间和`jenkins``PersistentVolumeClaim`。
- en: '![](img/ba1b318f-887f-4c15-a39a-4affbbabffe2.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba1b318f-887f-4c15-a39a-4affbbabffe2.png)'
- en: 'Figure 15-3: Creation of a Persistent Volume Claim'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-3：持久卷声明的创建
- en: Please note that if a PersistentVolumeClaim cannot find a matching volume, it
    will remain unbound indefinitely, unless we add a new PersistentVolume with the
    matching specifications.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果 `PersistentVolumeClaim` 无法找到匹配的卷，它将会永远处于未绑定状态，除非我们添加一个符合规格的新持久卷。
- en: We still haven't accomplished our goal. The fact that we claimed a volume does
    not mean that anyone uses it. On the other hand, our Jenkins needs to persist
    its state. We'll join our `PersistentVolumeClaim` with a Jenkins container.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然没有达到我们的目标。声明一个卷并不意味着任何人都会使用它。另一方面，我们的 Jenkins 需要持久化它的状态。我们将把`PersistentVolumeClaim`与
    Jenkins 容器关联起来。
- en: Attaching claimed volumes to Pods
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将已声明的卷附加到 Pods
- en: '[PRE48]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The relevant parts of the output is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE49]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: You'll notice that, this time, we added a new volume `jenkins-home`, which references
    the `PersistentVolumeClaim` called `jenkins`. From the container's perspective,
    the claim is a volume.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，这次我们添加了一个新的卷 `jenkins-home`，它引用了名为 `jenkins` 的 `PersistentVolumeClaim`。从容器的角度来看，声明就是一个卷。
- en: Let's deploy Jenkins resources and confirm that everything works as expected.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署 Jenkins 资源，并确认一切按预期工作。
- en: '[PRE50]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE51]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We'll wait until the Deployment rolls out before proceeding with a test that
    will confirm whether Jenkins state is now persisted.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将等到部署完成后，再进行一个测试，确认 Jenkins 的状态现在已经被持久化。
- en: '[PRE52]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Once the rollout is finished, we'll see a message stating that the `deployment
    "jenkins" was successfully rolled out`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署完成，我们将看到一条消息，表示 `deployment "jenkins"` 已成功滚动部署。
- en: We sent a request to the Kubernetes API to create a Deployment. As a result,
    we got a `ReplicaSet` that, in turn, created the `jenkins` Pod. It mounted the
    `PersistentVolumeClaim`, which is bound to the `PersistenceVolume`, that is tied
    to the EBS volume. As a result, the EBS volume was mounted to the `jenkins` container
    running in a Pod.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发送了一个请求到 Kubernetes API 来创建一个 Deployment。结果，我们得到了一个`ReplicaSet`，它进一步创建了`jenkins`
    Pod。它挂载了`PersistentVolumeClaim`，该声明绑定到`PersistenceVolume`，后者与 EBS 卷相关联。结果，EBS
    卷被挂载到运行在 Pod 中的`jenkins`容器。
- en: A simplified version of the sequence of events is depicted in the *Figure 15-4*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-4 的简化版本显示了事件序列。
- en: '![](img/0b4b1b21-dd3f-4e03-9ba1-7d22b85a55ac.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b4b1b21-dd3f-4e03-9ba1-7d22b85a55ac.png)'
- en: 'Figure 15-4: The sequence of events initiated with a request to create a Jenkins
    Pod with the PersistentVolumeClaim'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-4：事件序列始于请求创建一个具有 PersistentVolumeClaim 的 Jenkins Pod
- en: We executed `kubectl` command
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们执行了`kubectl`命令
- en: '`kubectl` sent a request to `kube-apiserver` to create the resources defined
    in `pv/jenkins-pv.yml`'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl`向`kube-apiserver`发送了一个请求，以创建`pv/jenkins-pv.yml`中定义的资源'
- en: Among others, the `jenkins` Pod was created in one of the worker nodes
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`jenkins` Pod 是在一个工作节点上创建的'
- en: Since `jenkins` container in the Pod has a `PersistentVolumeClaim`, it mounted
    it as a logical volume
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 Pod 中的`jenkins`容器具有`PersistentVolumeClaim`，它将其挂载为逻辑卷
- en: The `PersistentVolumeClaim` was already bound to one of the PersistentVolumes
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PersistentVolumeClaim 已经绑定到一个 PersistentVolume
- en: The PersistentVolume is associated with one of the EBS volumes
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PersistentVolume 与一个 EBS 卷相关联
- en: The EBS volume was mounted as a physical volume to the `jenkins` Pod
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: EBS 卷作为物理卷被挂载到`jenkins` Pod
- en: Now that Jenkins is up-and-running, we'll execute a similar set of steps as
    before, and validate that the state is persisted across failures.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Jenkins 已经启动运行，我们将执行与之前类似的一套步骤，并验证状态是否在故障时得到了保留。
- en: '[PRE53]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We opened Jenkins home screen. If you are not authenticated, please click the
    Log in link and type `jdoe` as the User and `**incognito*` as the Password. Click
    the log in button.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打开了 Jenkins 主屏幕。如果您没有通过身份验证，请点击“登录”链接，并输入`jdoe`作为用户名和`**incognito*`作为密码。点击登录按钮。
- en: You'll see the create new jobs link. Click it. Type `my-job` as the item name,
    select `Pipeline` as the job type, and click the OK button. Once inside the job
    configuration screen, all we have to do is click the Save button. An empty job
    will be enough to test persistence.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到一个创建新作业的链接。点击它。将`my-job`输入为项目名称，选择`Pipeline`作为作业类型，然后点击“确定”按钮。一旦进入作业配置屏幕，我们只需点击“保存”按钮。一个空作业足以测试持久性。
- en: Now we need to find out the name of the Pod created through the `jenkins` Deployment.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要找出通过`jenkins` Deployment 创建的 Pod 的名称。
- en: '[PRE54]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: With the name of the Pod stored in the environment variable `POD_NAME`, we can
    proceed and kill `java` process that's running Jenkins.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通过环境变量`POD_NAME`存储的 Pod 名称，我们可以继续并终止运行 Jenkins 的`java`进程。
- en: '[PRE55]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We killed the Jenkins process and thus simulated failure of the container. As
    a result, Kubernetes detected the failure and recreated the container.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们杀死了 Jenkins 进程，从而模拟容器的故障。结果，Kubernetes 检测到故障并重新创建了容器。
- en: A minute later, we can open Jenkins home screen again, and check whether the
    state (the job we created) was preserved.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一分钟后，我们可以再次打开 Jenkins 主屏幕，并检查状态（我们创建的作业）是否被保留。
- en: '[PRE56]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As you can see, the job is still available thus proving that we successfully
    mounted the EBS volume as the directory where Jenkins preserves its state.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，作业仍然可用，从而证明我们成功地将 EBS 卷挂载为 Jenkins 保存其状态的目录。
- en: If instead of destroying the container, we terminated the server where the Pod
    is running, the result, from the functional perspective, would be the same. The
    Pod would be rescheduled to a healthy node. Jenkins would start again and restore
    its state from the EBS volume. Or, at least, that's what we'd hope. However, such
    behavior is not guaranteed to happen in our cluster.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不销毁容器，而是终止运行 Pod 的服务器，从功能角度来看，结果是一样的。Pod 将重新调度到一个健康的节点。Jenkins 会重新启动，并从
    EBS 卷恢复其状态。或者，至少我们希望是这样。然而，在我们的集群中，不能保证会发生这样的行为。
- en: We have only two worker nodes, distributed in two (out of three) availability
    zones. If the node that hosted Jenkins failed, we'd be left with only one node.
    To be more precise, we'd have only one worker node running in the cluster until
    the auto-scaling group detects that an EC2 instance is missing and recreates it.
    During those few minutes, the single node we're left with is not in the same zone.
    As we already mentioned, each EBS instance is tied to a zone, and the one we mounted
    to the Jenkins Pod would not be associated with the zone where the other EC2 instance
    is running. As a result, the PersistentVolume could not re-bound the EBS volume
    and, therefore, the failed container could not be recreated, until the failed
    EC2 instance is recreated.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有两个工作节点，分布在两个（从三个中选择）可用区。如果托管 Jenkins 的节点发生故障，我们将只剩下一个节点。更准确地说，直到自动扩展组检测到缺少
    EC2 实例并重新创建它，我们只会有一个工作节点在集群中运行。在这几分钟内，我们剩下的唯一节点并不在同一区域。如前所述，每个 EBS 实例都绑定到一个区域，而我们挂载到
    Jenkins Pod 的 EBS 实例不会与另一个 EC2 实例所在的区域关联。因此，PersistentVolume 无法重新绑定 EBS 卷，导致故障的容器无法重建，直到故障的
    EC2 实例被重建。
- en: The chances are that the new EC2 instance would not be in the same zone as the
    one where the failed server was running. Since we're using three availability
    zones, and one of them already has an EC2 instance, AWS would recreate the failed
    server in one of the other two zones. We'd have fifty percent chances that the
    new EC2 would be in the same zone as the one where the failed server was running.
    Those are not good odds.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 EC2 实例可能不会和故障服务器所在的区域处于同一区域。因为我们使用了三个可用区，其中一个已包含 EC2 实例，AWS 会在其他两个可用区中的一个重建故障服务器。我们有
    50% 的机会，新的 EC2 实例会和故障服务器所在的区域在同一可用区。这并不是一个理想的概率。
- en: In the real-world scenario, we'd probably have more than two worker nodes. Even
    a slight increase to three nodes would give us a very good chance that the failed
    server would be recreated in the same zone. Auto-scaling groups are trying to
    distribute EC2 instances more or less equally across all the zones. However, that
    is not guaranteed to happen. A good minimum number of worker nodes would be six.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的场景中，我们可能会有超过两个工作节点。即使只是稍微增加到三个节点，也会大大增加故障服务器在同一区域重建的机会。自动扩展组会尽可能地在所有可用区之间均衡分配
    EC2 实例，但这并不能保证一定会发生。一个合理的最小工作节点数是六个。
- en: The more servers we have, the higher are the chances that the cluster is fault
    tolerant. That is especially true if we are hosting stateful applications. As
    it goes, we almost certainly have those. There's hardly any system that does not
    have a state in some form or another.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务器越多，集群具有容错能力的机会就越大。特别是当我们托管有状态应用时，这一点尤为重要。事实上，我们几乎肯定有这些应用。几乎没有系统是没有某种形式的状态的。
- en: If it's better to have more servers than less, we might be in a complicated
    position if our system is small and needs, let's say, less than six servers. In
    such cases, I'd recommend running smaller VMs. If, for example, you planned to
    use three `t2.xlarge` EC2 instances for worker nodes, you might reconsider that
    and switch to six `t2.large` servers. Sure, more nodes mean more resource overhead
    spent on operating systems, Kubernetes system Pods, and few other things. However,
    I believe that is compensated with bigger stability of your cluster.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果更多的服务器更好，那么如果我们的系统较小并且需要，比如说，不到六台服务器时，我们可能会处于一个复杂的情况。在这种情况下，我建议使用更小的虚拟机。例如，如果你打算使用三台
    `t2.xlarge` 的 EC2 实例作为工作节点，你可以重新考虑并改用六台 `t2.large` 的服务器。当然，更多的节点意味着在操作系统、Kubernetes
    系统 Pod 和一些其他方面会有更多的资源开销。然而，我相信这种开销会通过集群的更高稳定性得到补偿。
- en: There is still one more situation we might encounter. A whole availability zone
    (data center) might fail. Kubernetes will continue operating correctly. It'll
    have two instead of three master nodes, and the failed worker nodes will be recreated
    in healthy zones. However, we'd run into trouble with our stateful services. Kubernetes
    would not be able to reschedule those that were mounted to EBS volumes from the
    failed zone. We'd need to wait for the availability zone to come back online,
    or we'd need to move the EBS volume to a healthy zone manually. The chances are
    that, in such a case, the EBS would not be available and, therefore, could not
    be moved.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种情况可能会遇到。整个可用区（数据中心）可能会出现故障。Kubernetes会继续正常运行。它将只有两个而不是三个主节点，故障的工作节点将在健康的区域重新创建。然而，我们的有状态服务将遇到问题。Kubernetes无法重新调度那些挂载到故障区EBS卷的服务。我们需要等待可用区恢复在线，或者需要手动将EBS卷移到健康区。很有可能在这种情况下，EBS将不可用，因此无法迁移。
- en: We could create a process that would be replicating data in (near) real-time
    between EBS volumes spread across multiple availability zones, but that also comes
    with a downside. Such an operation would be expensive and would likely slow down
    state retrieval while everything is fully operational. Should we choose lower
    performance over high-availability? Is the increased operational overhead worth
    the trouble? The answer to those questions will differ from one use-case to another.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个过程，在多个可用区之间实时（接近）复制EBS卷中的数据，但这也有一个缺点。这样的操作会非常昂贵，而且在一切正常运作时，可能会减慢状态的恢复。我们是否应该选择较低的性能而换取更高的可用性？增加的运营开销是否值得？这些问题的答案因具体场景而异。
- en: There is yet another option. We could use [EFS](https://aws.amazon.com/efs/)
    ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)) instead of EBS. But,
    that would also impact performance since EFS tends to be slower than EBS. On top
    of that, there is no production-ready EFS support in Kubernetes. At the time of
    this writing, the EFS provisioner ([https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs))
    is still in beta phase. By the time you read this, things might have changed.
    Or maybe they didn't. Even when the *efs provisioner* becomes stable, it will
    still be slower and more expensive solution than EBS.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种选择。我们可以使用[EFS](https://aws.amazon.com/efs/) ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/))替代EBS。但是，这也会影响性能，因为EFS通常比EBS慢。除此之外，Kubernetes目前并不支持生产就绪的EFS。在写这篇文章时，EFS
    provisioner ([https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs))仍处于测试阶段。到你读这篇文章时，情况可能已经有所变化，或者可能没有变化。即使*efs
    provisioner*稳定下来，它仍然是比EBS更慢且更贵的解决方案。
- en: Maybe you'll decide to ditch EBS (and EFS) in favor of some other type of persistent
    storage. There are many different options you can choose. We won't explore them
    since an in-depth comparison of all the popular solutions would require much more
    space than what we have left. Consider them an advanced topic that will be covered
    in the next book. Or maybe it won't. I do not yet know the scope of *The DevOps
    2.4 Toolkit* book.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会决定放弃EBS（和EFS），转而选择其他类型的持久存储。你可以选择许多不同的选项。我们不会深入探讨这些选项，因为对所有流行方案的深入比较需要更多的篇幅，而我们剩余的篇幅有限。可以将它们视为一个高级话题，下一本书会讲解。或者也许不会。我还不清楚*The
    DevOps 2.4 Toolkit*一书的具体范围。
- en: All in all, every solution has pros and cons and none would fit all use-cases.
    For good or bad, we'll stick with EBS for the remainder of this book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，每种解决方案都有优缺点，且没有一种能适用于所有场景。无论好坏，我们将继续使用EBS，直到本书结束。
- en: Going back to PersistentVolumes tied to EBS...
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 回到与EBS绑定的PersistentVolumes…
- en: Now that we explored how to manage static persistent volumes, we'll try to accomplish
    the same results using dynamic approach. But, before we do that, we'll see what
    happens when some of the resources we created are removed.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经探索了如何管理静态持久卷，我们将尝试使用动态方法实现相同的结果。但在此之前，我们先来看看当我们创建的一些资源被删除时会发生什么。
- en: Let's delete the `jenkins` Deployment.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除`jenkins`部署。
- en: '[PRE57]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The output shows us that the `deployment "jenkins" was deleted`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们`deployment "jenkins"已被删除`。
- en: Did anything happen with the PersistentVolumeClaim and the PersistentVolume?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: PersistentVolumeClaim和PersistentVolume发生了什么事情吗？
- en: '[PRE58]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The combined output of both commands is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 两个命令的合并输出如下：
- en: '[PRE59]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Even though we removed Jenkins Deployment and, with it, the Pod that used the
    claim, both the PersistentVolumeClaim and PersistentVolumes are intact. The `manual-ebs-01`
    volume is still bound to the `jenkins` claim.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们删除了Jenkins部署，并且删除了使用该声明的Pod，PersistentVolumeClaim和PersistentVolumes仍然保持不变。`manual-ebs-01`卷仍然绑定到`jenkins`声明。
- en: What would happen if we remove the PersistentVolumeClaim `jenkins`?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们删除PersistentVolumeClaim `jenkins`，会发生什么？
- en: '[PRE60]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The output shows that the `persistentvolumeclaim "jenkins" was deleted`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示`persistentvolumeclaim "jenkins"已被删除`。
- en: 'Now, let''s see what happened with the PersistentVolumes:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下PersistentVolumes的情况：
- en: '[PRE61]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE62]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This time, the `manual-ebs-2` volume is `Released`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，`manual-ebs-2`卷已被`Released`。
- en: This might be a good moment to explain the `Retain` policy applied to the PersistentVolumes
    we created.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的时机，来解释我们创建的PersistentVolumes所应用的`Retain`策略。
- en: '`ReclaimPolicy` defines what should be done with a volume after it''s released
    from its claim. The policy was applied the moment we deleted the PersistentVolumeClaim
    that was bound to `manual-ebs-02`. When we created the PersistentVolumes, we did
    not specify `ReclaimPolicy`, so the volumes were assigned the default policy which
    is `Retain`.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReclaimPolicy`定义了一个卷在从其声明中释放后应该如何处理。该策略在我们删除绑定到`manual-ebs-02`的PersistentVolumeClaim时就已经应用。当我们创建PersistentVolumes时，我们没有指定`ReclaimPolicy`，所以这些卷被分配了默认的`Retain`策略。'
- en: The `Retain` reclaim policy enforces manual reclamation of the resource. When
    the PersistentVolumeClaim is deleted, the PersistentVolume still exists, and the
    volume is considered `released`. But it is not yet available for other claims
    because the previous claimant's data remains on the volume. In our case, that
    data is Jenkins state. If we'd like this PersistentVolume to become available,
    we'd need to delete all the data on the EBS volume.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`Retain`回收策略强制手动回收资源。当PersistentVolumeClaim被删除时，PersistentVolume仍然存在，并且该卷被认为是`released`（已释放）。但它尚未对其他声明可用，因为先前声明者的数据仍然保留在该卷上。在我们的例子中，这些数据就是Jenkins的状态。如果我们希望这个PersistentVolume变得可用，我们需要删除EBS卷上的所有数据。'
- en: Since we are running the cluster in AWS, it is easier to delete than to recycle
    resources, so we'll remove the released PersistentVolume instead of trying to
    clean everything we generated inside the EBS. Actually, we'll remove all the volumes
    since we are about to explore how we can accomplish the same effects dynamically.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在AWS中运行集群，删除资源比回收资源更容易，因此我们将删除已释放的PersistentVolume，而不是尝试清理我们在EBS中生成的所有内容。实际上，我们将删除所有的卷，因为我们即将探索如何动态地实现相同的效果。
- en: The other two reclaim policies are `Recycle` and `Delete`. `Recycle` is considered
    deprecated so we won't waste time explaining it. The `Delete` policy requires
    dynamic provisioning, but we'll postpone the explanation until we explore that
    topic.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两种回收策略是`Recycle`和`Delete`。`Recycle`被认为已经弃用，所以我们不会浪费时间来解释它。`Delete`策略需要动态配置，但我们会推迟解释，直到我们探讨该主题。
- en: 'Let''s delete some stuff:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除一些内容：
- en: '[PRE63]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE64]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We can see that all three PersistentVolumes were deleted. However, only Kubernetes
    resources were removed. We still need to manually delete the EBS volumes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有三个PersistentVolumes都被删除了。然而，只有Kubernetes资源被移除。我们仍然需要手动删除EBS卷。
- en: 'If you go to your AWS console, you''ll see that all three EBS volumes are now
    in the `available` state and waiting to be mounted. We''ll delete them all:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进入AWS控制台，你会看到所有三个EBS卷现在处于`available`状态，等待挂载。我们将删除它们：
- en: '[PRE65]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We are finished with our tour around manual creation of persistent volumes.
    If we'd use this approach to volume management, cluster administrator would need
    to ensure that there is always an extra number of available volumes that can be
    used by new claims. It is tedious work that often results in having more volumes
    than we need. On the other hand, if we don't have a sufficient number of available
    (unused) volumes, we're risking that someone will create a claim that will not
    find a suitable volume to mount.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了对手动创建持久卷的巡回讲解。如果我们采用这种卷管理方式，集群管理员需要确保始终有额外的可用卷供新的声明使用。这是一个繁琐的工作，往往会导致拥有比实际需要更多的卷。另一方面，如果我们没有足够的可用（未使用）卷，我们就有可能面临某个声明无法找到合适的卷进行挂载的风险。
- en: Manual volume management is sometimes unavoidable, especially if chose to use
    on-prem infrastructure combined with NFS. However, this is not our case. AWS is
    all about dynamic resource provisioning, and we'll exploit that to its fullest.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 手动卷管理有时是不可避免的，特别是当你选择使用本地基础设施结合NFS时。然而，这不是我们的情况。AWS的核心就是动态资源配置，我们将充分利用这一点。
- en: Using storage classes to dynamically provision persistent volumes
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用存储类动态配置持久卷
- en: So far, we used static PersistentVolumes. We had to create both EBS volumes
    and Kubernetes PersistentVolumes manually. Only after both became available were
    we able to deploy Pods that are mounting those volumes through PersistentVolumeClaims.
    We'll call this process static volume provisioning.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了静态的PersistentVolumes。我们必须手动创建EBS卷和Kubernetes PersistentVolumes。只有当两者都可用时，我们才能部署挂载这些卷的Pod，通过PersistentVolumeClaims进行挂载。我们将这个过程称为静态卷配置。
- en: In some cases, static volume provisioning is a necessity. Our infrastructure
    might not be capable of creating dynamic volumes. That is often the case with
    on-premise infrastructure with volumes based on NFS. Even then, with a few tools,
    a change in processes, and right choices for supported volume types, we can often
    reach the point where volume provisioning is dynamic. Still, that might prove
    to be a challenge with legacy processes and infrastructure.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，静态卷配置是必要的。我们的基础设施可能无法创建动态卷。这通常出现在基于NFS的本地基础设施中。即便如此，通过使用一些工具、改变流程以及选择合适的支持卷类型，我们通常可以达到动态配置卷的效果。不过，这在遗留流程和基础设施中可能仍然是一个挑战。
- en: Since our cluster is in AWS, we cannot blame legacy infrastructure for provisioning
    volumes manually. Indeed, we could have jumped straight into this section. After
    all, AWS is all about dynamic infrastructure management. However, I felt that
    it will be easier to understand the processes by exploring manual provisioning
    first. The knowledge we obtained thus far will help us understand better what's
    coming next. The second reason for starting with manual provisioning lies in my
    inability to predict your plans. Maybe you will run a Kubernetes cluster on infrastructure
    that has to be static. Even though we're using AWS for the examples, everything
    you learned this far can be implemented on static infrastructure. You'll only
    have to change EBS with NFS and go through NFSVolumeSource ([https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core))
    documentation. There are only three NFS-specific fields so you should be up-and-running
    in no time.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的集群在AWS中，我们不能将手动配置卷归咎于遗留的基础设施。事实上，我们本可以直接进入这一部分。毕竟，AWS就是为了动态基础设施管理而存在的。然而，我认为通过首先探索手动配置，会更容易理解这些过程。到目前为止我们获得的知识将帮助我们更好地理解接下来要做的事情。开始手动配置的第二个原因是我无法预测你的计划。也许你会在必须是静态的基础设施上运行Kubernetes集群。即使我们在示例中使用的是AWS，但到目前为止你所学到的所有内容也可以在静态基础设施上实现。你只需要将EBS替换为NFS，并参考NFSVolumeSource（[https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core)）文档。NFS特定的字段只有三个，所以你应该能很快上手。
- en: Before we discuss how to enable dynamic persistent volume provisioning, we should
    understand that it will be used only if none of the static PersistentVolumes match
    our claims. In other words, Kubernetes will always select statically created PersistentVolumes
    over dynamic ones.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何启用动态持久卷配置之前，我们应该理解，只有当没有静态的PersistentVolumes匹配我们的声明时，才会使用动态配置。换句话说，Kubernetes总是优先选择静态创建的PersistentVolumes，而不是动态的。
- en: Dynamic volume provisioning allows us to create storage on-demand. Instead of
    manually pre-provisioning storage, we can provision it automatically when a resource
    requests it.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 动态卷配置允许我们按需创建存储。我们可以在资源请求时自动配置存储，而不是手动预配置存储。
- en: We can enable dynamic provisioning through the usage of StorageClasses from
    the `storage.k8s.io` API group. They allow us to describe the types of storage
    that can be claimed. On the one hand, cluster administrator can create as many
    StorageClasses as there are storage flavours. On the other hand, the users of
    the cluster do not have to worry about the details of each available external
    storage. It's a win-win situation where the administrators do not have to create
    PersistentVolumes in advance, and the users can simply claim the storage type
    they need.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用来自 `storage.k8s.io` API 组的 StorageClasses 来启用动态供给。它们允许我们描述可以声明的存储类型。一方面，集群管理员可以根据存储类型创建任意数量的
    StorageClasses。另一方面，集群的用户无需担心每个外部存储的细节。这是一个双赢的局面，管理员不需要预先创建 PersistentVolumes，而用户只需声明他们需要的存储类型。
- en: 'To enable dynamic provisioning, we need to create at least one StorageClass
    object. Luckily for us, kops already set up a few, so we might just as well take
    a look at the StorageClasses currently available in our cluster:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用动态供给，我们需要创建至少一个 StorageClass 对象。幸运的是，kops 已经设置了一些，我们不妨看看目前集群中可用的 StorageClasses：
- en: '[PRE66]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output is as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE67]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We can see that there are two StorageClasses in our cluster. Both are using
    the same `aws-ebs` provisioner. Besides the names, the only difference, at least
    in this output, is that one of them is marked as `default`. We'll explore what
    that means a bit later. For now, we'll trust that kops configured those classes
    correctly and try to claim a PersistentVolume.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，集群中有两个 StorageClasses。两者都使用相同的 `aws-ebs` 供应者。除了名称外，唯一的区别，至少在这个输出中，是其中一个被标记为
    `default`。稍后我们将探讨这意味着什么。现在，我们相信 kops 正确配置了这些类，并尝试声明一个 PersistentVolume。
- en: 'Let''s take a quick look at yet another `jenkins` definition:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看另一个 `jenkins` 定义：
- en: '[PRE68]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示：
- en: '[PRE69]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This Jenkins definition is almost the same as the one we used before. The only
    difference is in the PersistentVolumeClaim that, this time, specified `gp2` as
    the `StorageClassName`. There is one more difference though. This time we do not
    have any PersistentVolume pre-provisioned. If everything works as expected, a
    new PersistentVolume will be created dynamically.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Jenkins 定义与我们之前使用的几乎相同。唯一的区别在于 PersistentVolumeClaim，这次指定了 `gp2` 作为 `StorageClassName`。不过还有一个区别。这一次我们没有预先
    provision 的 PersistentVolume。如果一切按预期工作，将会动态创建一个新的 PersistentVolume。
- en: '[PRE70]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We can see that some of the resources were re-configured, while others were
    created.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些资源被重新配置了，而另一些则被创建了。
- en: 'Next, we''ll wait until the `jenkins` Deployment is rolled out successfully:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将等待直到 `jenkins` 部署成功滚动完成：
- en: '[PRE71]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now we should be able to see what happened through the `jenkins` namespace events.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该能够通过 `jenkins` 命名空间的事件来查看发生了什么。
- en: '[PRE72]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output, limited to the last few lines, is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于最后几行，如下所示：
- en: '[PRE73]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We can see that a new PersistentVolume was `successfully provisioned`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，一个新的 PersistentVolume 已经 `成功 provision`。
- en: Let's take a look at the status of the PersistentVolumeClaim.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 PersistentVolumeClaim 的状态。
- en: '[PRE74]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE75]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The part of the output that matters is the status. We can see that it `Bound`
    with the PersistentVolume thus confirming, again, that the volume was indeed created
    dynamically.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中重要的部分是状态。我们可以看到它已与 PersistentVolume `绑定`，从而再次确认该卷确实是动态创建的。
- en: 'To be on the safe side, we''ll list the PersistentVolumes as well:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们也会列出 PersistentVolumes：
- en: '[PRE76]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The output is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE77]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: As expected, the PersistentVolume was created, it is bound to the PersistentVolumeClaim,
    and its reclaim policy is `Delete`. We'll see the policy in action soon.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，PersistentVolume 已经创建，它与 PersistentVolumeClaim 绑定，并且其回收策略为 `Delete`。我们很快就能看到该策略的实际效果。
- en: 'Finally, the last verification we''ll perform is to confirm that the EBS volume
    was created as well:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进行的最后一次验证是确认 EBS 卷是否也已创建：
- en: '[PRE78]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示：
- en: '[PRE79]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: We can see that a new EBS volume was created in the availability zone `us-east-2c`,
    that the type is `gp2`, and that its state is `in-use`.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，一个新的 EBS 卷已在可用区 `us-east-2c` 中创建，类型为 `gp2`，状态为 `in-use`。
- en: Dynamic provisioning works! Given that we're using AWS, it is a much better
    solution than using static resources.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 动态供给工作正常！考虑到我们使用的是 AWS，这比使用静态资源要好得多。
- en: Before we move into a next subject, we'll explore the effect of the reclaim
    policy `Delete`. To do so, we'll delete the Deployment and the PersistentVolumeClaim.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一个主题之前，我们将探讨回收策略`Delete`的效果。为此，我们将删除Deployment和PersistentVolumeClaim。
- en: '[PRE80]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE81]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Now that the claim to the volume was removed, we can check what happened with
    the dynamically provisioned PersistentVolumes.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请求的卷已被移除，我们可以检查动态供应的PersistentVolumes发生了什么。
- en: '[PRE82]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The output shows that `no resources` were found, clearly indicating that the
    PersistentVolume that was created through the claim is now gone.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示`未找到资源`，清楚地表明通过请求创建的PersistentVolume已经不存在了。
- en: How about the AWS EBS volume? Was it removed as well?
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 那么AWS EBS卷怎么样？它也被移除了吗？
- en: '[PRE83]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The output is as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE84]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We got an empty array proving that the EBS volume was removed as well.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个空数组，证明EBS卷也已被移除。
- en: Through dynamic volume provisioning, not only that volumes are created when
    resources claim them, but they are also removed when the claims are released.
    Dynamic removal is accomplished through the reclaim policy `Delete`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态卷供应，不仅在资源请求时创建卷，而且当请求释放时，这些卷也会被删除。动态删除通过回收策略`Delete`完成。
- en: Using default storage classes
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用默认存储类
- en: Working with dynamic provisioning simplifies a few things. Still, a user needs
    to know which volume type to use. While in many cases that is an important choice,
    there are often situations when a user might not want to worry about that. It
    might be easier to use the cluster administrator's choice for volume types and
    let all claims that do not specify `storageClassName` get a default volume. We'll
    try to accomplish that through one of the admission controllers.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用动态供应简化了一些事情。然而，用户仍然需要知道使用哪种卷类型。虽然在很多情况下这是一个重要的选择，但也常常会有用户不希望为此担忧的情况。可能更方便的是使用集群管理员选择的卷类型，并让所有没有指定`storageClassName`的请求得到默认卷。我们将尝试通过其中一个Admission控制器来实现这一点。
- en: Admission controllers are intercepting requests to the Kubernetes API server.
    We won't go into details of admission controllers since the list of those supported
    by Kubernetes is relatively big. We are interested only in the `DefaultStorageClass`
    which happens to be already enabled in the cluster we created with kops.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Admission 控制器正在拦截请求到Kubernetes API服务器的请求。我们不会深入讨论Admission控制器，因为Kubernetes支持的控制器列表相对较大。我们只对`DefaultStorageClass`感兴趣，而这个控制器恰好在我们使用kops创建的集群中已经启用。
- en: '`DefaultStorageClass` admission controller observes creation of PersistentVolumeClaims.
    Through it, those that do not request any specific storage class are automatically
    added a default storage class to them. As a result, PersistentVolumeClaims that
    do not request any special storage class are bound to PersistentVolumes created
    from the default `StorageClass`. From user''s perspective, there''s no need to
    care about volume types since they will be provisioned based on the default type
    unless they choose a specific class.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`DefaultStorageClass` Admission控制器观察PersistentVolumeClaims的创建。通过它，所有没有请求特定存储类的PersistentVolumeClaims会自动被添加上默认的存储类。因此，那些没有请求任何特殊存储类的PersistentVolumeClaims将绑定到由默认`StorageClass`创建的PersistentVolumes。从用户的角度来看，不需要关心卷的类型，因为它们将根据默认类型进行配置，除非用户选择了特定的类。'
- en: 'Let''s take a look at the storage classes currently available in our cluster:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看当前在集群中可用的存储类：
- en: '[PRE85]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The output is as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE86]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: This is not the first time we're listing the storage classes in our cluster.
    However, we did not discuss that one of the two (`gp2`) is marked as the default
    `StorageClass`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们第一次列出集群中的存储类。然而，我们并没有讨论到两个存储类（`gp2`）中的一个被标记为默认`StorageClass`。
- en: Let's describe the `gp2` class.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下`gp2`类。
- en: '[PRE87]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 限制在相关部分的输出如下：
- en: '[PRE88]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The important part lies in the annotations. One of them is `".../is-default-class":"true"`.
    It sets that `StorageClass` as default. As a result, it will be used to create
    PersistentVolumes by any PersistentVolumeClaim that does not specify StorageClass
    name.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 重要部分在于注解。其之一是`".../is-default-class":"true"`。它将该`StorageClass`设置为默认。结果，任何未指定StorageClass名称的PersistentVolumeClaim都会使用该StorageClass来创建PersistentVolumes。
- en: Let's try to adapt Jenkins stack to use the ability to dynamically provision
    a volume associated with the `DefaultStorageClass`.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试调整Jenkins堆栈，利用动态供应与`DefaultStorageClass`相关联的卷的能力。
- en: 'The new Jenkins definition is as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 新的Jenkins定义如下：
- en: '[PRE89]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The output, limited to the `PersistentVolumeClaim`, is as follows.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于 `PersistentVolumeClaim`，如下所示。
- en: '[PRE90]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'It''s hard to spot the difference between that YAML file and the one we used
    before. It is very small and hard to notice change so we''ll execute `diff` to
    compare the two:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 很难区分这个 YAML 文件和我们之前使用的文件之间的差异。它非常小，难以察觉，因此我们将执行 `diff` 来对比这两个文件：
- en: '[PRE91]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The output is as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE92]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'As you can see, the only difference is that `pv/jenkins-dynamic.yml` doesn''t
    have `storageClassName: gp2`. That field is omitted from the new definition. Our
    new `PersistentVolumeClaim` does not have an associated StorageClass.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '如你所见，唯一的不同是 `pv/jenkins-dynamic.yml` 没有 `storageClassName: gp2`。这个字段在新的定义中被省略了。我们新的
    `PersistentVolumeClaim` 没有关联的 StorageClass。'
- en: 'Let''s `apply` the new definition:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们 `apply` 新的定义：
- en: '[PRE93]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output is as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE94]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: What we're interested in are PersistentVolumes, so let's retrieve them.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的是 PersistentVolumes，所以让我们来获取它们。
- en: '[PRE95]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: As you can see, even though we did not specify any StorageClass, a volume was
    created based on the `gp2` class, which happens to be the default one.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，尽管我们没有指定任何 StorageClass，但还是基于 `gp2` 类创建了一个卷，而 `gp2` 恰好是默认的存储类。
- en: We'll delete the `jenkins` Deployment and PersistentVolumeClaim before we explore
    how we can create our own StorageClasses.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索如何创建自己的 StorageClasses 之前，我们将删除 `jenkins` Deployment 和 PersistentVolumeClaim。
- en: '[PRE96]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The output is as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE97]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Creating storage classes
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建存储类
- en: Even though kops created two StorageClasses, both are based on `gp2`. While
    that is the most commonly used EBS type, we might want to create volumes based
    on one of the other three options offered by AWS.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 kops 创建了两个 StorageClasses，它们都基于 `gp2`。虽然这是最常用的 EBS 类型，但我们可能希望基于 AWS 提供的其他三个选项之一来创建卷。
- en: Let's say that we want the fastest EBS volume type for our Jenkins. That would
    be `io1`. Since kops did not create a StorageClass of that type, we might want
    to create our own.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望为 Jenkins 使用最快的 EBS 卷类型，那就是 `io1`。由于 kops 并没有创建这种类型的 StorageClass，我们可能需要自己创建一个。
- en: YAML file that creates StorageClass based on EBS `io1` is defined in `pv/sc.yml`.
    Let's take a quick look.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 创建基于 EBS `io1` 的 StorageClass 的 YAML 文件定义在 `pv/sc.yml` 中。我们快速看一下。
- en: '[PRE98]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The output is as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE99]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: We used `kubernetes.io/aws-ebs` as the `provisioner`. It is a mandatory field
    that determines the plugin that will be used for provisioning PersistentVolumes.
    Since we are running the cluster in AWS, `aws-ebs` is the logical choice. There
    are quite a few other provisioners we could choose. Some of them are specific
    to a hosting provider (for example, `GCEPersistentDisk` and `AzureDisk`) while
    others can be used anywhere (for example, `GlusterFS`).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 `kubernetes.io/aws-ebs` 作为 `provisioner`。这是一个必填字段，用来确定用于供应 PersistentVolumes
    的插件。由于我们在 AWS 上运行集群，`aws-ebs` 是合适的选择。我们还可以选择其他一些 provisioners，其中一些是特定于某个托管提供商的（例如，`GCEPersistentDisk`
    和 `AzureDisk`），而其他一些则可以在任何地方使用（例如，`GlusterFS`）。
- en: 'The list of supported provisioners is growing. At the time of this writing,
    the following types are supported:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的 provisioners 列表在不断增长。在撰写本文时，支持的类型如下：
- en: '| Volume Plugin | Internal Provisioner |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Volume Plugin | Internal Provisioner |'
- en: '| AWSElasticBlockStore | yes |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| AWSElasticBlockStore | yes |'
- en: '| AzureFile | yes |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| AzureFile | yes |'
- en: '| AzureDisk | yes |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| AzureDisk | yes |'
- en: '| CephFS | no |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| CephFS | no |'
- en: '| Cinder | yes |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Cinder | yes |'
- en: '| FC | no |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| FC | no |'
- en: '| FlexVolume | no |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| FlexVolume | no |'
- en: '| Flocker | yes |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Flocker | yes |'
- en: '| GCEPersistentDisk | yes |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| GCEPersistentDisk | yes |'
- en: '| Glusterfs | yes |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Glusterfs | yes |'
- en: '| iSCSI | no |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| iSCSI | no |'
- en: '| PhotonPersistentDisk | yes |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| PhotonPersistentDisk | yes |'
- en: '| Quobyte | yes |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Quobyte | yes |'
- en: '| NFS | no |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| NFS | no |'
- en: '| RBD | yes |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| RBD | yes |'
- en: '| VsphereVolume | yes |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| VsphereVolume | yes |'
- en: '| PortworxVolume | yes |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| PortworxVolume | yes |'
- en: '| ScaleIO | yes |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| ScaleIO | yes |'
- en: '| StorageOS | yes |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| StorageOS | yes |'
- en: '| Local | no |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Local | no |'
- en: The internal provisioners are those with names prefixed with `kubernetes.io`
    (for example, `kubernetes.io/aws-ebs`). They are shipped with Kubernetes. External
    provisioners, on the other hand, are independent programs shipped separately from
    Kubernetes. An example of a commonly used external provisioner is `NFS`. The parameters
    depend on the StorageClass. We used the `aws-ebs` provisioner which allows us
    to specify the `type` parameter that defines one of the supported Amazon EBS volume
    types. It can be EBS Provisioned IOPS SSD (`io1`), EBS **General Purpose SSD**
    (**gp2**), Throughput Optimized HDD (`st1`), and Cold HDD (`sc1`). We set it to
    `io1` which is the highest performance SSD volume. Please consult [*Parameters*](https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters)
    ([https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters](https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters))
    section of the *Storage Classes* documentation for more info. Finally, we set
    the `reclaimPolicy` to `Delete`. Unlike `Retain` that forces us to delete the
    contents of the released volume before it becomes available to new PersistentVolumeClaims,
    `Delete` removes both the PersistentVolume as well as the associated volume in
    the external architecture. The `Delete` reclaim policy works only with some of
    the external volumes like AWS EBS, Azure Disk, or Cinder volume. Now that we dipped
    our toes into the StorageClass definition, we can proceed and create it.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 内部供应器是那些名称以`kubernetes.io`为前缀的（例如`kubernetes.io/aws-ebs`）。它们是与Kubernetes一起发布的。另一方面，外部供应器是与Kubernetes分开发布的独立程序。一个常用的外部供应器例子是`NFS`。参数依赖于StorageClass。我们使用了`aws-ebs`供应器，它允许我们指定`type`参数，该参数定义了支持的Amazon
    EBS卷类型之一。它可以是EBS Provisioned IOPS SSD（`io1`）、EBS **普通用途SSD**（**gp2**）、吞吐优化HDD（`st1`）和冷HDD（`sc1`）。我们将其设置为`io1`，这是性能最高的SSD卷。更多信息请参考[*参数*](https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters)（[https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters](https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters)）部分的*Storage
    Classes*文档。最后，我们将`reclaimPolicy`设置为`Delete`。与`Retain`不同，`Retain`要求我们在释放的卷变得可供新PersistentVolumeClaims使用之前删除卷内容，而`Delete`会删除PersistentVolume及其在外部架构中的关联卷。`Delete`回收策略仅适用于某些外部卷，如AWS
    EBS、Azure Disk或Cinder卷。现在我们已经初步了解了StorageClass定义，可以继续创建它。
- en: '[PRE100]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: The output shows that the `storageclass "fast" was created`, so we'll list,
    one more time, the StorageClassses in our cluster.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示`storageclass "fast" 已创建`，因此我们将再次列出集群中的StorageClasses。
- en: '[PRE101]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The output is as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE102]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: We can see that, this time, we have a new StorageClass.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这次我们使用了一个新的StorageClass。
- en: Let's take a look at yet another Jenkins definition.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个Jenkins定义。
- en: '[PRE103]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限相关部分，如下：
- en: '[PRE104]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: The only difference, when compared with the previous definition, is that we
    are now using the newly created StorageClass named `fast`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的定义相比，唯一的区别是我们现在使用了新创建的名为`fast`的StorageClass。
- en: Finally, we'll confirm that the new StorageClass works by deploying the new
    `jenkins` definition.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过部署新的`jenkins`定义来确认新的StorageClass是否正常工作。
- en: '[PRE105]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The output is as follows:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE106]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: As the final verification, we'll list the EBS volumes and confirm that a new
    one was created based on the new class.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最终验证，我们将列出EBS卷并确认是否基于新的类创建了一个新卷。
- en: '[PRE107]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限相关部分，如下：
- en: '[PRE108]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: We can see that the type of the newly created EBS volume is `io1` and that it
    is `in-use`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，新创建的EBS卷类型是`io1`，并且它是`in-use`状态。
- en: '![](img/a3596ac4-ca51-4225-aaee-457ec369a86e.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3596ac4-ca51-4225-aaee-457ec369a86e.png)'
- en: 'Figure 15-5: The sequence of events initiated with a request to create a Jenkins
    Pod with the PersistentVolumeClaim using a custom StorageClass'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-5：通过请求创建一个带有PersistentVolumeClaim并使用自定义StorageClass的Jenkins Pod，启动的事件序列
- en: 'A simplified version of the flow of events initiated with the creation of the
    `jenkins` Deployment is as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简化版本的事件流，可以看到在创建`jenkins` Deployment时，事件的流转过程如下：
- en: We created the `jenkins` Deployment, which created a ReplicaSet, which, in turn,
    created a Pod.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了`jenkins` Deployment，随后创建了一个ReplicaSet，ReplicaSet又创建了一个Pod。
- en: The Pod requested persistent storage through the PersistentVolumeClaim.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod通过PersistentVolumeClaim请求持久存储。
- en: The PersistentVolumeClaim requested PersistentStorage with the StorageClass
    name `fast`.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PersistentVolumeClaim请求了带有StorageClass名称`fast`的PersistentStorage。
- en: StorageClass `fast` is defined to create a new EBS volume, so it requested one
    from the AWS API.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: StorageClass `fast` 被定义为创建一个新的 EBS 卷，因此它向 AWS API 请求了一个卷。
- en: AWS API created a new EBS volume.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS API 创建了一个新的 EBS 卷。
- en: EBS volume was mounted to the `jenkins` Pod.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: EBS 卷已挂载到 `jenkins` Pod。
- en: We're finished exploring persistent volumes. You should be equipped with the
    knowledge how to persist your stateful applications, and the only pending action
    is to remove the volumes and the cluster.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了持久卷的探索。你应该已经掌握了如何持久化你的有状态应用程序，唯一待做的就是删除这些卷和集群。
- en: What now?
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那现在该怎么办？
- en: There's nothing left to do but to destroy what we did so far.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在没什么可做的了，除了销毁我们迄今为止所做的操作。
- en: This time, we cannot just delete the cluster. Such an action would leave the
    EBS volumes running. So, we need to remove them first.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不能直接删除集群。这样做会导致 EBS 卷继续运行。因此，我们需要先删除这些卷。
- en: We could remove EBS volumes through AWS CLI. However, there is an easier way.
    If we delete all the claims to EBS volumes, they will be deleted as well since
    our PersistentVolumes are created with the reclaim policy set to `Delete`. EBS
    volumes are created when needed and destroyed when not.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 AWS CLI 删除 EBS 卷。然而，还有一种更简便的方法。如果我们删除所有对 EBS 卷的声明，它们也会被删除，因为我们的 PersistentVolumes
    是按照回收策略 `Delete` 创建的。EBS 卷在需要时创建，不需要时销毁。
- en: Since all our claims are in the `jenkins` namespace, removing it is the easiest
    way to delete them all.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有的声明都在 `jenkins` 命名空间中，删除它是删除所有资源的最简便方法。
- en: '[PRE109]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: The output shows that the `namespace "jenkins" was deleted` and we can proceed
    to delete the cluster as well.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示 `namespace "jenkins" 被删除`，我们可以继续删除集群。
- en: '[PRE110]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: We can see from the output that the cluster `devops23.k8s.local` was deleted
    and we are left only with the S3 bucket used for kops state. We'll delete it as
    well.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到集群 `devops23.k8s.local` 已被删除，剩下的只有用于 kops 状态的 S3 存储桶，我们也将删除它。
- en: '[PRE111]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Before you leave, please consult the following API references for more information
    about volume-related resources.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开之前，请参考以下 API 文档以了解更多关于卷相关资源的信息。
- en: '[PersistentVolume v1 core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolume-v1-core)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage))'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PersistentVolume v1 核心](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolume-v1-core)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage))'
- en: '[PersistentVolumeClaim v1 core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core))'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PersistentVolumeClaim v1 核心](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core))'
- en: '[StorageClass v1 storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage))'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[StorageClass v1 存储](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage))'
- en: That's it. There's nothing left.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，没剩下什么了。
