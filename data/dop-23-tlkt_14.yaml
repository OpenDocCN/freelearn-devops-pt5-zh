- en: Creating a Production-Ready Kubernetes Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个生产就绪的 Kubernetes 集群
- en: Creating a Kubernetes cluster is not trivial. We have to make many choices,
    and we can easily get lost in the myriad of options. The number of permutations
    is getting close to infinite and, yet, our clusters need to be configured consistently.
    Experience from the first attempt to set up a cluster can easily convert into
    a nightmare that will haunt you for the rest of your life.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Kubernetes 集群并非简单任务。我们需要做出许多选择，并且很容易在各种选项中迷失。可选择的排列组合几乎是无限的，但我们的集群需要一致地进行配置。从第一次尝试设置集群的经验中，很容易就会变成一个困扰你一生的噩梦。
- en: Unlike Docker Swarm that packs almost everything into a single binary, Kubernetes
    clusters require quite a few separate components running across the nodes. Setting
    them up can be very easy, or it can become a challenge. It all depends on the
    choices we make initially. One of the first things we need to do is choose a tool
    that we'll use to create a Kubernetes cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与将几乎所有内容打包成单一二进制文件的 Docker Swarm 不同，Kubernetes 集群需要在各个节点上运行多个独立的组件。设置这些组件可以非常简单，也可能变得非常复杂，这一切取决于我们最初做出的选择。我们需要做的第一件事之一就是选择一个工具，用来创建
    Kubernetes 集群。
- en: If we'd decide to install a Docker Swarm cluster, all we'd need to do is to
    install Docker engine on all the servers, and execute `docker swarm init` or `docker
    swarm join` command on each of the nodes. That's it. Docker packs everything into
    a single binary. Docker Swarm setup process is as simple as it can get. The same
    cannot be said for Kubernetes. Unlike Swarm that is highly opinionated, Kubernetes
    provides much higher freedom of choice. It is designed around extensibility. We
    need to choose among many different components. Some of them are maintained by
    the core Kubernetes project, while others are provided by third-parties. Extensibility
    is probably one of the main reasons behind Kubernetes' rapid growth. Almost every
    software vendor today is either building components for Kubernetes or providing
    a service that sits on top of it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定安装一个 Docker Swarm 集群，我们只需要在所有服务器上安装 Docker 引擎，并在每个节点上执行 `docker swarm
    init` 或 `docker swarm join` 命令，仅此而已。Docker 将一切打包到一个二进制文件中。Docker Swarm 的设置过程简单至极。而
    Kubernetes 就不一样了。与高度意见化的 Swarm 不同，Kubernetes 提供了更高的自由选择。它的设计注重扩展性。我们需要在众多组件中做出选择。其中一些是由
    Kubernetes 核心项目维护的，而其他则由第三方提供。扩展性可能是 Kubernetes 快速发展的主要原因之一。今天，几乎每个软件供应商都在为 Kubernetes
    构建组件，或者提供其之上的服务。
- en: Besides the intelligent design and the fact that it solves problems related
    to distributed, scalable, fault-tolerant, and highly available systems, Kubernetes'
    power comes from adoption and support from a myriad of individuals and companies.
    You can use that power, as long as you understand that it comes with responsibilities.
    It's up to you, dear reader, to choose how will your Kubernetes cluster look like,
    and which components it'll host. You can decide to build it from scratch, or you
    can use one of the hosted solutions like **Google Cloud Platform** (**GCE**) Kubernetes
    Engine. There is a third option though. We can choose to use one of the installation
    tools. Most of them are highly opinionated with a limited amount of arguments
    we can use to tweak the outcome.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 除了智能设计以及解决与分布式、可扩展、容错和高可用系统相关的问题外，Kubernetes 的强大之处还在于它得到了无数个人和公司广泛的采用和支持。只要你理解它背后的责任，你就可以利用这股力量。亲爱的读者，如何构建你的
    Kubernetes 集群，选择哪些组件来托管，完全取决于你。你可以选择从零开始构建，也可以使用像**Google Cloud Platform**（**GCE**）Kubernetes
    引擎这样的托管解决方案。不过，实际上还有第三种选择。我们可以选择使用其中一个安装工具。大多数工具都有明确的使用意见，且可调整的参数非常有限。
- en: You might be thinking that creating a cluster from scratch using `kubeadm` cannot
    be that hard. You'd be right if running Kubernetes is all we need. But, it isn't.
    We need to make it fault tolerant and highly available. It needs to stand the
    test of time. Constructing a robust solution would require a combination of Kubernetes
    core and third-party components, AWS know-how, and quite a lot of custom scripts
    that would tie the two together. We won't go down that road. At least, not now.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为，使用 `kubeadm` 从零开始创建一个集群并不难。如果运行 Kubernetes 就是我们唯一需要做的事，你是对的。但是事实并非如此。我们还需要确保它是容错的并且具有高可用性。它需要经得起时间的考验。构建一个稳健的解决方案将需要结合
    Kubernetes 核心和第三方组件、AWS 的专业知识，以及大量的自定义脚本来将这两者结合起来。但我们不会走这条路，至少现在不走。
- en: We'll use **Kubernetes Operations** (**kops**) to create a cluster. It is somewhere
    in the middle between do-it-yourself-from-scratch and hosted solutions (for example,
    GCE). It's an excellent fit for both newbies and veterans. You'll learn which
    components are required for running a Kubernetes cluster. You'll be able to make
    some choices. And, yet, we won't go down the rabbit hole of setting up the cluster
    from scratch. Believe me, that hole is very deep, and it might take us a very
    long time to get out of it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 **Kubernetes Operations**（**kops**）来创建集群。它介于从零开始自己动手和托管解决方案（例如 GCE）之间。对于新手和老手来说，它都是一个很好的选择。你将学习运行
    Kubernetes 集群所需的组件。你将能够做出一些选择。然而，我们不会深入讨论从零开始设置集群的问题。相信我，这个坑非常深，要想爬出来可能需要很长时间。
- en: Typically, this would be a great place to explain the most significant components
    of a Kubernetes cluster. Heck, you were probably wondering why we didn't do that
    early on when we began the journey. Still, we'll postpone the discussion for a
    while longer. I believe it'll be better to create a cluster first and discuss
    the components through live examples. I feel that it's easier to understand something
    we can see and touch, instead of keeping it purely on the theoretical level.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这是一个很好的机会来解释 Kubernetes 集群中最重要的组件。天哪，你可能已经在想，为什么我们不在开始时就做这个了。尽管如此，我们会再推迟一会儿再讨论。我相信，先创建一个集群，再通过实时示例来讨论组件会更好。我觉得，通过实际的操作和触摸去理解某些东西，要比仅仅停留在理论层面要容易得多。
- en: All in all, we'll create a cluster first, and discuss its components later.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，我们将先创建一个集群，稍后再讨论它的组件。
- en: Since I already mentioned that we'll use **kops** to create a cluster, we'll
    start with a very brief introduction to the project behind it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我已经提到我们将使用 **kops** 来创建集群，那我们就先简单介绍一下它背后的项目。
- en: What is kubernetes operations (kops) project?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Kubernetes 操作（kops）项目？
- en: If you visit **Kubernetes Operations** (**kops**) ([https://github.com/kubernetes/kops](https://github.com/kubernetes/kops))
    project, the first sentence you'll read is that it is "the easiest way to get
    a production-grade Kubernetes cluster up and running." In my humble opinion, that
    sentence is accurate only if we exclude **Google Kubernetes Engine** (**GKE**).
    Today (February 2018), other hosting vendors did not yet release their Kubernetes-as-a-service
    solutions. Amazon's **Elastic Container Service for Kubernetes** (**EKS**) ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))
    is still not open to the public. **Azure Container Service** (**AKS**) ([https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/))
    is also a new addition that still has a few pain points. By the time you read
    this, all major hosting providers might have their solutions. Still, I prefer
    kops since it provides almost the same level of simplicity without taking away
    the control of the process. It allows us to tweak the cluster more than we would
    be permitted with hosted solutions. It is entirely open source, it can be stored
    in version control, and it is not designed to lock you into a vendor.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你访问 **Kubernetes Operations**（**kops**）([https://github.com/kubernetes/kops](https://github.com/kubernetes/kops))
    项目，你看到的第一句话是它是“让生产级 Kubernetes 集群快速上线的最简单方法。”在我看来，只有在排除 **Google Kubernetes Engine**（**GKE**）的情况下，这句话才是准确的。今天（2018年2月），其他托管服务商还没有发布他们的
    Kubernetes 即服务解决方案。亚马逊的 **Elastic Container Service for Kubernetes**（**EKS**）([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))
    仍然没有对公众开放。**Azure Container Service**（**AKS**）([https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/))
    也是一个新的增添，仍然有一些痛点。等到你阅读本文时，所有主要托管商可能都会有他们的解决方案。不过，我更倾向于使用 kops，因为它提供了几乎相同的简易性，同时没有剥夺我们对过程的控制。它让我们能够对集群进行比托管解决方案更多的定制。它完全是开源的，可以存储在版本控制中，也不是设计来让你被某个供应商锁定。
- en: If your hosting vendor is AWS, kops is, in my opinion, the best way to create
    a Kubernetes cluster. Whether that's true for GCE, is open for debate since GKE
    works great. We can expect kops to be extended in the future to other vendors.
    For example, at the time of this writing, VMWare is in alpha and should be stable
    soon. Azure and Digital Ocean support are being added as I write this.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的托管服务商是 AWS，我认为 kops 是创建 Kubernetes 集群的最佳方式。至于 GCE 是否适用，尚有争议，因为 GKE 表现非常出色。我们可以预期未来
    kops 会扩展到其他托管商。例如，在本文撰写时，VMWare 正处于 alpha 阶段，应该很快就会稳定。Azure 和 Digital Ocean 的支持正在增加，正如我写这篇文章时所看到的那样。
- en: We'll use kops to create a Kubernetes cluster in AWS. This is the part of the
    story that might get you disappointed. You might have chosen to run Kubernetes
    somewhere else. Don't be depressed. Almost all Kubernetes clusters follow the
    same principles even though the method of setting them up might differ. The principles
    are what truly matters, and I'm confident that, once you set it up successfully
    on AWS, you'll be able to transfer that knowledge anywhere else.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 kops 在 AWS 中创建 Kubernetes 集群。这是故事的部分内容，可能会让你感到失望。你可能选择在其他地方运行 Kubernetes，但不要沮丧。几乎所有
    Kubernetes 集群遵循相同的原则，尽管它们的设置方法可能不同。原则才是真正重要的，一旦你成功地在 AWS 上设置了 Kubernetes，你将能够将这些知识迁移到其他地方。
- en: The reason for choosing AWS lies in its adoption. It is the hosting vendor with,
    by far, the biggest user-base. If I'd have to place a blind bet on your choice,
    it would be AWS solely because that is statistically the most likely choice. I
    could not explore all the options in a single chapter. If I am to go through all
    hosting vendors and different projects that might help with the installation,
    we'd need to dedicate a whole book to that. Instead, I invite you to explore the
    subject further once you're finished with installing Kubernetes in AWS with kops.
    As an alternative, ping me on `slack.devops20toolkit.com` or send me an email
    to `viktor@farcic.com` and I'll give you a hand. If I receive enough messages,
    I might even dedicate a whole book to Kubernetes installations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 AWS 的原因在于其广泛的使用。它是目前拥有最多用户群的托管服务提供商。如果我要盲目下注你的选择，那一定是 AWS，因为从统计上看，这是最有可能的选择。我无法在单一章节中覆盖所有的选项。如果我需要遍历所有托管提供商和可能帮助安装的不同项目，我们得为此写一本完整的书。相反，我邀请你在完成在
    AWS 上使用 kops 安装 Kubernetes 后，进一步探索该主题。作为替代方案，可以通过 `slack.devops20toolkit.com`
    与我联系，或者发邮件到 `viktor@farcic.com`，我会帮助你。如果我收到足够多的消息，我甚至可能会为 Kubernetes 安装专门写一本书。
- en: I went astray from kops...
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我偏离了 kops……
- en: Kops lets us create a production-grade Kubernetes cluster. That means that we
    can use it not only to create a cluster, but also to upgrade it (without downtime),
    update it, or destroy it if we don't need it anymore. A cluster cannot be called
    "production grade" unless it is highly available and fault tolerant. We should
    be able to execute it entirely from the command line if we'd like it to be automated.
    Those and quite a few other things are what kops provides, and what makes it great.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Kops 让我们可以创建一个生产级的 Kubernetes 集群。这意味着我们不仅可以用它来创建集群，还可以用它来升级集群（没有停机时间）、更新集群，或者如果不再需要它时销毁集群。一个集群如果没有高度可用和容错能力，不能称为“生产级”。如果我们希望它能够自动化运行，应该能够完全通过命令行执行。这些以及其他很多功能，正是
    kops 所提供的，这也使它如此优秀。
- en: Kops follows the same philosophy as Kubernetes. We create a set of JSON or YAML
    objects which are sent to controllers that create a cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kops 遵循与 Kubernetes 相同的哲学。我们创建一组 JSON 或 YAML 对象，并将其发送到控制器，控制器负责创建集群。
- en: We'll discuss what kops can and cannot do in more detail soon. For now, we'll
    jump into the hands-on part of this chapter and ensure that all the prerequisites
    for the installation are set.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快更详细地讨论 kops 能做和不能做的事情。现在，我们将进入本章的实际操作部分，确保所有安装的前置条件已设置好。
- en: Preparing for the cluster setup
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为集群设置做准备
- en: We'll continue using the specifications from the `vfarcic/k8s-specs` repository,
    so the first thing we'll do is to go inside the directory where we cloned it,
    and pull the latest version.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用 `vfarcic/k8s-specs` 仓库中的规格，因此我们首先要做的是进入克隆该仓库的目录，并拉取最新版本。
- en: All the commands from this chapter are available in the `14-aws.sh` ([https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134](https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134))
    Gist.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 `14-aws.sh` ([https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134](https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134))
    Gist 中找到。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I will assume that you already have an AWS account. If that's not the case,
    please head over to Amazon Web Services ([https://aws.amazon.com/](https://aws.amazon.com/))
    and sign-up.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你已经有了一个 AWS 账户。如果没有，请访问 Amazon Web Services ([https://aws.amazon.com/](https://aws.amazon.com/))
    并注册。
- en: If you are already proficient with AWS, you might want to skim through the text
    that follows and only execute the commands.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉 AWS，你可能只需要浏览接下来的内容并执行命令。
- en: The first thing we should do is get the AWS credentials.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做的第一件事是获取 AWS 凭证。
- en: Please open Amazon EC2 Console ([https://console.aws.amazon.com/ec2/v2/home](https://console.aws.amazon.com/ec2/v2/home)),
    click on your name from the top-right menu and select My Security Credentials.
    You will see the screen with different types of credentials. Expand the Access
    Keys (Access Key ID and Secret Access Key) section and click the Create New Access
    Key button. Expand the Show Access Key section to see the keys.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开 Amazon EC2 控制台 ([https://console.aws.amazon.com/ec2/v2/home](https://console.aws.amazon.com/ec2/v2/home))，点击右上角菜单中的您的名字并选择“我的安全凭证”。您将看到不同类型凭证的屏幕。展开“访问密钥（Access
    Key ID 和 Secret Access Key）”部分，点击“创建新访问密钥”按钮。展开“显示访问密钥”部分以查看密钥。
- en: You will not be able to view the keys later on, so this is the only chance you'll
    have to *Download Key File*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您将无法稍后查看密钥，因此这是唯一一次能够*下载密钥文件*的机会。
- en: We'll put the keys as environment variables that will be used by the **AWS Command
    Line Interface** (**AWS CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把密钥作为环境变量，这些变量将由 **AWS 命令行界面** (**AWS CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    使用。
- en: 'Please replace `[...]` with your keys before executing the commands that follow:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请在执行以下命令之前，将 `[...]` 替换为您的密钥：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We'll need to install AWS **Command Line Interface** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    and gather info about your account.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装 AWS **命令行界面** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    并收集您的帐户信息。
- en: If you haven't already, please open the Installing the AWS Command Line Interface
    ([https://docs.aws.amazon.com/cli/latest/userguide/installing.html](https://docs.aws.amazon.com/cli/latest/userguide/installing.html))
    page, and follow the installation method best suited for your OS.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有，请打开 [安装 AWS 命令行界面](https://docs.aws.amazon.com/cli/latest/userguide/installing.html)
    页面，并按照适合您操作系统的安装方法进行安装。
- en: 'A note to Windows users: I found the most convenient way to get AWS CLI installed
    on Windows is to use Chocolatey ([https://chocolatey.org/](https://chocolatey.org/)).
    Download and install Chocolatey, then run `choco install awscli` from an Administrator
    Command Prompt. Later on in the chapter, Chocolatey will be used to install jq.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给 Windows 用户的提示：我发现最方便的在 Windows 上安装 AWS CLI 的方法是使用 Chocolatey ([https://chocolatey.org/](https://chocolatey.org/))。下载并安装
    Chocolatey，然后在管理员命令提示符下运行 `choco install awscli`。本章稍后将使用 Chocolatey 安装 jq。
- en: Once you're done, we'll confirm that the installation was successful by outputting
    the version.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们将通过输出版本来确认安装是否成功。
- en: 'A note to Windows users: You might need to reopen your *GitBash* terminal for
    the changes to the environment variable `PATH` to take effect.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给 Windows 用户的提示：您可能需要重新打开您的 *GitBash* 终端，以使环境变量 `PATH` 的更改生效。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output (from my laptop) is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（来自我的笔记本电脑）如下：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Amazon EC2 is hosted in multiple locations worldwide. These locations are composed
    of regions and availability zones. Each region is a separate geographic area composed
    of multiple isolated locations known as availability zones. Amazon EC2 provides
    you the ability to place resources, such as instances, and data in multiple locations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EC2 托管在全球多个位置。这些位置由区域和可用区组成。每个区域是一个由多个隔离位置组成的独立地理区域，这些位置称为可用区。Amazon
    EC2 使您能够将资源（如实例）和数据放置在多个位置。
- en: Next, we'll define the environment variable `AWS_DEFAULT_REGION` that will tell
    AWS CLI which region we'd like to use by default.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义环境变量 `AWS_DEFAULT_REGION`，该变量将告诉 AWS CLI 默认使用哪个区域。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For now, please note that you can change the value of the variable to any other
    region, as long as it has at least three availability zones. We'll discuss the
    reasons for using `us-east-2` region and the need for multiple availability zones
    soon.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，请注意，您可以将变量的值更改为任何其他区域，只要该区域至少有三个可用区。我们很快会讨论为什么选择 `us-east-2` 区域以及需要多个可用区的原因。
- en: 'Next, we''ll create a few **Identity and Access Management** (**IAM**) resources.
    Even though we could create a cluster with the user you used to register to AWS,
    it is a good practice to create a separate account that contains only the privileges
    we''ll need for the exercises that follow:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一些 **身份和访问管理** (**IAM**) 资源。尽管我们可以使用您注册 AWS 时使用的用户创建一个集群，但创建一个仅包含我们后续练习所需权限的独立帐户是一个良好的实践：
- en: 'First, we''ll create an IAM group called `kops`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个名为 `kops` 的 IAM 组：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We don't care much for any of the information from the output except that it
    does not contain an error message thus confirming that the group was created successfully.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确认没有错误信息、证明组已成功创建外，我们不太关心输出中的任何信息。
- en: Next, we'll assign a few policies to the group thus providing the future users
    of the group with sufficient permissions to create the objects we'll need.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为该组分配几个策略，从而为该组的未来用户提供足够的权限，创建我们所需的对象。
- en: Since our cluster will consist of EC2 ([https://aws.amazon.com/ec2/](https://aws.amazon.com/ec2/))
    instances, the group will need to have the permissions to create and manage them.
    We'll need a place to store the state of the cluster so we'll need access to S3
    ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)). Furthermore, we need
    to add VPCs ([https://aws.amazon.com/vpc/](https://aws.amazon.com/vpc/)) to the
    mix so that our cluster is isolated from prying eyes. Finally, we'll need to be
    able to create additional IAMs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的集群将由EC2（[https://aws.amazon.com/ec2/](https://aws.amazon.com/ec2/)）实例组成，因此该组需要拥有创建和管理这些实例的权限。我们还需要一个地方存储集群的状态，因此我们需要访问S3（[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)）。此外，我们需要添加VPC（[https://aws.amazon.com/vpc/](https://aws.amazon.com/vpc/)）以使我们的集群与外界隔离。最后，我们还需要能够创建额外的IAM。
- en: In AWS, user permissions are granted by creating policies. We'll need *AmazonEC2FullAccess*,
    *AmazonS3FullAccess*, *AmazonVPCFullAccess*, and *IAMFullAccess*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，用户权限通过创建策略来授予。我们需要的策略包括*AmazonEC2FullAccess*、*AmazonS3FullAccess*、*AmazonVPCFullAccess*和*IAMFullAccess*。
- en: 'The commands that attach the required policies to the `kops` group are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将所需策略附加到`kops`组的命令如下：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have a group with the sufficient permissions, we should create a
    user as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个拥有足够权限的组，接下来我们应该创建一个用户。
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Just as when we created the group, the contents of the output are not important,
    except as a confirmation that the command was executed successfully.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们创建组时一样，输出的内容并不重要，唯一重要的是确认命令已成功执行。
- en: 'The user we created does not yet belong to the `kops` group. We''ll fix that
    next:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的用户还没有加入`kops`组。接下来我们来解决这个问题：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, we'll need access keys for the newly created user. Without them, we
    would not be able to act on its behalf.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还需要为新创建的用户生成访问密钥。没有这些密钥，我们将无法代表其执行操作。
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We created access keys and stored the output in the `kops-creds` file. Let's
    take a quick look at its content.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了访问密钥，并将输出存储在`kops-creds`文件中。让我们快速查看它的内容。
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Please note that I removed the values of the keys. I do not yet trust you enough
    with the keys of my AWS account.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我已移除了密钥的值。我还不够信任你，将我的AWS账户密钥交给你。
- en: We need the `SecretAccessKey` and `AccessKeyId` entries. So, the next step is
    to parse the content of the `kops-creds` file and store those two values as the
    environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`SecretAccessKey`和`AccessKeyId`字段。因此，接下来的步骤是解析`kops-creds`文件的内容，并将这两个值存储为环境变量`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`。
- en: In the spirit of full automation, we'll use `jq` ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    to parse the contents of the `kops-creds` file. Please download and install the
    distribution suited for your OS.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现完全自动化，我们将使用`jq`（[https://stedolan.github.io/jq/](https://stedolan.github.io/jq/)）解析`kops-creds`文件的内容。请下载并安装适合你操作系统的发行版。
- en: 'A note to Windows users: Using Chocolatey, install `jq` from an Administrator
    Command Prompt via `choco install jq`.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示Windows用户：通过Chocolatey在管理员命令提示符下使用`choco install jq`来安装`jq`。
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We used `cat` to output contents of the file and combined it with `jq` to filter
    the input so that only the field we need is retrieved.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`cat`命令输出文件内容，并结合`jq`命令过滤输入，以便只提取我们需要的字段。
- en: From now on, all the AWS CLI commands will not be executed by the administrative
    user you used to register to AWS, but as `kops`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，所有的AWS CLI命令将不再由你用于注册AWS的管理员用户执行，而是由`kops`执行。
- en: It is imperative that the `kops-creds` file is secured and not accessible to
    anyone but people you trust. The best method to secure it depends from one organization
    to another. No matter what you do, do not write it on a post-it and stick it to
    your monitor. Storing it in one of your GitHub repositories is even worse.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`kops-creds`文件必须得到妥善保护，只有你信任的人才能访问。如何保护它的方法因组织而异。不管你采取什么措施，千万不要把它写在便签上并贴在显示器上，存放在GitHub仓库中更是一个坏主意。'
- en: Next, we should decide which availability zones we'll use. So, let's take a
    look at what's available in the `us-east-2` region.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该决定使用哪些可用区。因此，让我们查看 `us-east-2` 区域中有哪些可用区。
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see, the region has three availability zones. We'll store them in
    an environment variable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，该区域有三个可用区。我们将把它们存储在一个环境变量中。
- en: 'A note to Windows users: Please use `tr ''\r\n'' '', ''` instead of `tr ''\n''
    '',''` in the command that follows.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒 Windows 用户：请在接下来的命令中使用 `tr '\r\n' ', '`，而不是 `tr '\n' ','`。
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Just as with the access keys, we used `jq` to limit the results only to the
    zone names, and we combined that with `tr` that replaced new lines with commas.
    The second command removes the trailing comma.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 就像访问密钥一样，我们使用 `jq` 将结果限制为仅显示区域名称，并将其与 `tr` 结合，替换掉换行符为逗号。第二条命令会去掉尾部的逗号。
- en: 'The output of the last command that echoed the values of the environment variable
    is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条命令的输出，回显了环境变量的值，结果如下：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We'll discuss the reasons behind the usage of three availability zones later
    on. For now, just remember that they are stored in the environment variable `ZONES`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讨论使用三个可用区的原因。现在，只需要记住它们已存储在环境变量 `ZONES` 中。
- en: The last preparation step is to create SSH keys required for the setup. Since
    we might create some other artifacts during the process, we'll create a directory
    dedicated to the creation of the cluster.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的准备步骤是创建设置所需的 SSH 密钥。由于在此过程中我们可能会创建其他一些工件，我们将创建一个专门用于集群创建的目录。
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'SSH keys can be created through the `aws ec2` command `create-key-pair`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `aws ec2` 命令 `create-key-pair` 创建 SSH 密钥：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We created a new key pair, filtered the output so that only the `KeyMaterial`
    is returned, and stored it in the `devops23.pem` file.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个新的密钥对，过滤输出，使得只有 `KeyMaterial` 被返回，并将其存储在 `devops23.pem` 文件中。
- en: For security reasons, we should change the permissions of the `devops23.pem`
    file so that only the current user can read it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 出于安全考虑，我们应该更改 `devops23.pem` 文件的权限，使得只有当前用户可以读取它。
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Finally, we'll need only the public segment of the newly generated SSH key,
    so we'll use `ssh-keygen` to extract it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需要新生成的 SSH 密钥的公钥部分，因此我们将使用 `ssh-keygen` 提取它。
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: All those steps might look a bit daunting if this is your first contact with
    AWS. Nevertheless, they are pretty standard. No matter what you do in AWS, you'd
    need to perform, more or less, the same actions. Not all of them are mandatory,
    but they are good practice. Having a dedicated (non-admin) user and a group with
    only required policies is always a good idea. Access keys are necessary for any
    `aws` command. Without SSH keys, no one can enter into a server.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次接触 AWS，那么这些步骤可能看起来有些令人生畏。然而，它们其实是非常标准的。无论在 AWS 中做什么，您都需要或多或少地执行相同的操作。并非所有步骤都是强制性的，但它们都是良好的实践。拥有一个专门的（非管理员）用户和仅包含所需策略的用户组总是一个好主意。访问密钥对于任何
    `aws` 命令都是必需的。如果没有 SSH 密钥，任何人都无法进入服务器。
- en: The good news is that we're finished with the prerequisites, and we can turn
    our attention towards creating a Kubernetes cluster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是我们已经完成了前提条件的设置，现在可以将注意力转向创建 Kubernetes 集群。
- en: Creating a kubernetes cluster in AWS
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 中创建 Kubernetes 集群
- en: We'll start by deciding the name of our soon to be created cluster. We'll choose
    to call it `devops23.k8s.local`. The latter part of the name (`.k8s.local`) is
    mandatory if we do not have a DNS at hand. It's a naming convention kops uses
    to decide whether to create a gossip-based cluster or to rely on a publicly available
    domain. If this would be a "real" production cluster, you would probably have
    a DNS for it. However, since I cannot be sure whether you do have one for the
    exercises in this book, we'll play it safe, and proceed with the gossip mode.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先决定即将创建的集群的名称。我们选择将其命名为 `devops23.k8s.local`。如果没有 DNS，这个名称的后缀（`.k8s.local`）是必须的。这是
    kops 用来决定是否创建一个基于 gossip 的集群，或者依赖于公开可用域名的命名约定。如果这是一个“真实”的生产集群，您可能会有一个 DNS。然而，由于我无法确定您是否在本书中的练习中有
    DNS，我们将采取更为保守的做法，使用 gossip 模式继续。
- en: We'll store the name into an environment variable so that it is easily accessible.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把名称存储在一个环境变量中，以便它易于访问。
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When we create the cluster, kops will store its state in a location we're about
    to configure. If you used Terraform, you'll notice that kops uses a very similar
    approach. It uses the state it generates when creating the cluster for all subsequent
    operations. If we want to change any aspect of a cluster, we'll have to change
    the desired state first, and then apply those changes to the cluster.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建集群时，kops 会将其状态存储在我们即将配置的位置。如果你使用过 Terraform，你会注意到 kops 使用了非常相似的方法。它使用在创建集群时生成的状态进行所有后续操作。如果我们想改变集群的任何方面，首先需要更改所需的状态，然后再将这些更改应用到集群中。
- en: At the moment, when creating a cluster in AWS, the only option for storing the
    state are `Amazon S3` ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    buckets. We can expect availability of additional stores soon. For now, S3 is
    our only option.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在 AWS 中创建集群时，唯一存储状态的选项是 `Amazon S3`（[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)）存储桶。我们可以预期很快会有更多的存储选项。现在，S3
    是我们唯一的选择。
- en: 'The command that creates an S3 bucket in our region is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们所在区域的 S3 存储桶的命令如下：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We created a bucket with a unique name and the output is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个唯一名称的存储桶，输出结果如下：
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For simplicity, we'll define the environment variable `KOPS_STATE_STORE`. Kops
    will use it to know where we store the state. Otherwise, we'd need to use `--store`
    argument with every `kops` command.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将定义环境变量 `KOPS_STATE_STORE`。kops 将使用它来知道我们存储状态的位置。否则，我们需要在每个 `kops` 命令中都使用
    `--store` 参数。
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There's only one thing missing before we create the cluster. We need to install
    kops.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群之前，只差一步了。我们需要安装 kops。
- en: If you are a **MacOS user**, the easiest way to install `kops` is through `Homebrew`
    ([https://brew.sh/](https://brew.sh/)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是 **MacOS 用户**，安装 `kops` 最简单的方法是通过 `Homebrew`（[https://brew.sh/](https://brew.sh/)）。
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As an alternative, we can download a release from GitHub.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为替代方案，我们可以从 GitHub 下载一个发布版本。
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If, on the other hand, you''re a **Linux user**, the commands that will install
    `kops` are as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是 **Linux 用户**，安装 `kops` 的命令如下：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Finally, if you are a **Windows user**, you cannot install *kops*. At the time
    of this writing, its releases do not include Windows binaries. Don't worry. I
    am not giving up on you, dear *Windows user*. We'll manage to overcome the problem
    soon by exploiting Docker's ability to run any Linux application. The only requirement
    is that you have Docker for Windows ([https://www.docker.com/docker-windows](https://www.docker.com/docker-windows))
    installed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你是 **Windows 用户**，你将无法安装 *kops*。在撰写本文时，它的发布版本不包括 Windows 二进制文件。别担心，我不会放弃你，亲爱的
    *Windows 用户*。我们很快就能通过利用 Docker 运行任何 Linux 应用的能力来解决这个问题。唯一的要求是你安装了 Docker for Windows（[https://www.docker.com/docker-windows](https://www.docker.com/docker-windows)）。
- en: I already created a Docker image that contains `kops` and its dependencies.
    So, we'll create an alias `kops` that will create a container instead running
    a binary. The result will be the same.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经创建了一个包含 `kops` 及其依赖项的 Docker 镜像。因此，我们将创建一个别名 `kops`，该别名将创建一个容器，而不是运行二进制文件。结果是一样的。
- en: 'The command that creates the `kops` alias is as follows. Execute it only if
    you are a **Windows user**:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `kops` 别名的命令如下。如果你是 **Windows 用户**，请执行该命令：
- en: '[PRE30]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We won't go into details of all the arguments the `docker run` command uses.
    Their usage will become clear when we start using `kops`. Just remember that we
    are passing all the environment variables we might use as well as mounting the
    SSH key and the directory where `kops` will store `kubectl` configuration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论 `docker run` 命令使用的所有参数。它们的用法将在我们开始使用 `kops` 时变得清晰。只需要记住，我们传递了所有可能使用的环境变量，并挂载了
    SSH 密钥和 `kops` 存储 `kubectl` 配置的目录。
- en: We are, finally, ready to create a cluster. But, before we do that, we'll spend
    a bit of time discussing the requirements we might have. After all, not all clusters
    are created equal, and the choices we are about to make might severely impact
    our ability to accomplish the goals we might have.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好创建集群了。但是，在我们开始之前，我们将花一点时间讨论我们可能需要的要求。毕竟，并不是所有的集群都是一样的，我们即将做出的选择可能会严重影响我们实现目标的能力。
- en: The first question we might ask ourselves is whether we want to have high-availability.
    It would be strange if anyone would answer no. Who doesn't want to have a cluster
    that is (almost) always available? Instead, we'll ask ourselves what the things
    that might bring our cluster down are.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能首先问自己的是，我们是否想要高可用性。如果有人回答“不”，那将是很奇怪的。谁不希望拥有一个（几乎）始终可用的集群呢？相反，我们会问自己是什么因素可能导致集群的宕机。
- en: When a node is destroyed, Kubernetes will reschedule all the applications that
    were running inside it into the healthy nodes. All we have to do is to make sure
    that, later on, a new server is created and joined the cluster, so that its capacity
    is back to the desired values. We'll discuss later how are new nodes created as
    a reaction to failures of a server. For now, we'll assume that will happen somehow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个节点被销毁时，Kubernetes 会将所有运行在该节点上的应用程序重新调度到健康的节点上。我们需要做的就是确保，稍后一个新的服务器被创建并加入集群，从而使其容量恢复到期望的值。我们稍后会讨论新节点是如何应对服务器故障时创建的。现在，我们假设这一切会以某种方式发生。
- en: Still, there is a catch. Given that new nodes need to join the cluster, if the
    failed server was the only master, there is no cluster to join. All is lost. The
    part is where master servers are. They host the critical components without which
    Kubernetes cannot operate.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但仍然有一个陷阱。因为新节点需要加入集群，如果失败的服务器是唯一的主节点，就没有集群可以加入了。一切都完了。关键在于主服务器所在的位置。它们承载着 Kubernetes
    无法运作的关键组件。
- en: So, we need more than one master node. How about two? If one fails, we still
    have the other one. Still, that would not work.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们需要不止一个主节点。两个怎么样？如果一个失败了，另一个还能继续工作。但这依然行不通。
- en: Every piece of information that enters one of the master nodes is propagated
    to the others, and only after the majority agrees, that information is committed.
    If we lose majority (50%+1), masters cannot establish a quorum and cease to operate.
    If one out of two masters is down, we can get only half of the votes, and we would
    lose the ability to establish the quorum. Therefore, we need three masters or
    more. Odd numbers greater than one are "magic" numbers. Given that we won't create
    a big cluster, three should do.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每一条进入主节点的信息都会传播到其他主节点，只有在多数节点同意后，该信息才会被提交。如果我们失去多数（50%+1），主节点就无法建立法定人数（quorum）并停止工作。如果两个主节点中有一个宕机，我们只能获得一半的投票，因此我们将失去建立法定人数的能力。因此，我们需要三个或更多的主节点。大于1的奇数是“魔术”数字。鉴于我们不会创建一个大型集群，三个就足够了。
- en: With three masters, we are safe from a failure of any single one of them. Given
    that failed servers will be replaced with new ones, as long as only one master
    fails at the time, we should be fault tolerant and have high availability.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有了三个主节点，我们就能防范任何一个主节点的故障。鉴于失败的服务器会被新的服务器替代，只要在任何时候只有一个主节点失败，我们应该具备容错性并具有高可用性。
- en: Always set an odd number greater than one for master nodes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 始终为主节点设置一个大于1的奇数数量。
- en: The whole idea of having multiple masters does not mean much if an entire data
    center goes down.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有多个主节点的整体想法，如果一个完整的数据中心宕机的话，将不再有太大意义。
- en: Attempts to prevent a data center from failing are commendable. Still, no matter
    how well a data center is designed, there is always a scenario that might cause
    its disruption. So, we need more than one data center. Following the logic behind
    master nodes, we need at least three. But, as with almost anything else, we cannot
    have any three (or more) data centers. If they are too far apart, the latency
    between them might be too high. Since every piece of information is propagated
    to all the masters in a cluster, slow communication between data centers would
    severely impact the cluster as a whole.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽力防止数据中心故障是值得称赞的。然而，无论数据中心设计得多么完美，总有可能出现导致其中断的情况。所以，我们需要不止一个数据中心。按照主节点的逻辑，我们至少需要三个。但和几乎所有事情一样，我们不能随便选择这三个（或更多）数据中心。如果它们相距太远，之间的延迟可能会太高。由于每一条信息都需要传播到集群中的所有主节点，数据中心之间的缓慢通信将严重影响整个集群。
- en: All in all, we need three data centers that are close enough to provide low
    latency, and yet physically separated, so that failure of one does not impact
    the others. Since we are about to create the cluster in AWS, we'll use **availability
    zones** (**AZs**) which are physically separated data centers with low latency.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们需要三个数据中心，它们要足够接近以提供低延迟，同时又要物理上分隔，以便一个数据中心的故障不会影响到其他数据中心。由于我们打算在 AWS 中创建集群，我们将使用**可用区**（**AZs**），它是物理上分隔且具有低延迟的数据中心。
- en: Always spread your cluster between at least three data centers which are close
    enough to warrant low latency.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 始终将你的集群分布在至少三个数据中心，这些数据中心距离足够近，以确保低延迟。
- en: There's more to high-availability to running multiple masters and spreading
    a cluster across multiple availability zones. We'll get back to this subject later.
    For now, we'll continue exploring the other decisions we have to make.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性不仅仅是运行多个主节点和将集群分布在多个可用区之间的问题，我们稍后会回到这个话题。现在，我们将继续探索我们必须做出的其他决策。
- en: Which networking shall we use? We can choose between *kubenet*, *CNI*, *classic*,
    and *external* networking.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择哪种网络？我们可以选择 *kubenet*、*CNI*、*经典* 或 *外部* 网络。
- en: The classic Kubernetes native networking is deprecated in favor of kubenet,
    so we can discard it right away.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的 Kubernetes 原生网络已被弃用，转而支持 kubenet，因此我们可以立即排除它。
- en: The external networking is used in some custom implementations and for particular
    use cases, so we'll discard that one as well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 外部网络通常用于一些自定义实现和特定用例，因此我们也将排除这一选项。
- en: That leaves us with kubenet and CNI.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就剩下 kubenet 和 CNI。
- en: '**Container Network Interface** (**CNI**) allows us to plug in a third-party
    networking driver. Kops supports Calico ([https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/](https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/)),
    flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel)),
    Canal (Flannel + Calico) ([https://github.com/projectcalico/canal](https://github.com/projectcalico/canal)),
    kopeio-vxlan ([https://github.com/kopeio/networking](https://github.com/kopeio/networking)),
    kube-router ([https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer](https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer)),
    romana ([https://github.com/romana/romana](https://github.com/romana/romana)),
    weave ([https://github.com/weaveworks-experiments/weave-kube](https://github.com/weaveworks-experiments/weave-kube)),
    and `amazon-vpc-routed-eni` ([https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend](https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend))
    networks. Each of those networks comes with pros and cons and differs in its implementation
    and primary objectives. Choosing between them would require a detailed analysis
    of each. We''ll leave a comparison of all those for some other time and place.
    Instead, we''ll focus on `kubenet`.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器网络接口**（**CNI**）允许我们插入第三方网络驱动程序。Kops 支持 Calico ([https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/](https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/))、flannel
    ([https://github.com/coreos/flannel](https://github.com/coreos/flannel))、Canal（Flannel
    + Calico）([https://github.com/projectcalico/canal](https://github.com/projectcalico/canal))、kopeio-vxlan
    ([https://github.com/kopeio/networking](https://github.com/kopeio/networking))、kube-router
    ([https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer](https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer))、romana
    ([https://github.com/romana/romana](https://github.com/romana/romana))、weave ([https://github.com/weaveworks-experiments/weave-kube](https://github.com/weaveworks-experiments/weave-kube))
    和 `amazon-vpc-routed-eni` ([https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend](https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend))
    网络。每种网络都有优缺点，且在实现方式和主要目标上各不相同。选择其中之一需要对每个方案进行详细分析。我们将留待以后再做这方面的比较，今天我们专注于 `kubenet`。'
- en: Kubenet is kops' default networking solution. It is Kubernetes native networking,
    and it is considered battle tested and very reliable. However, it comes with a
    limitation. On AWS, routes for each node are configured in AWS VPC routing tables.
    Since those tables cannot have more than fifty entries, kubenet can be used in
    clusters with up to fifty nodes. If you're planning to have a cluster bigger than
    that, you'll have to switch to one of the previously mentioned CNIs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 是 kops 的默认网络解决方案。它是 Kubernetes 原生的网络方案，被认为经过实战检验，非常可靠。然而，它也有一个限制。在 AWS
    上，每个节点的路由都配置在 AWS VPC 路由表中。由于这些路由表不能有超过五十条条目，因此 kubenet 只能在最多五十个节点的集群中使用。如果你打算建立一个更大的集群，必须切换到之前提到的某个
    CNI。
- en: Use kubenet networking if your cluster is smaller than fifty nodes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的集群小于五十个节点，使用 kubenet 网络。
- en: The good news is that using any of the networking solutions is easy. All we
    have to do is specify the `--networking` argument followed with the name of the
    network.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，使用任何网络解决方案都很容易。我们只需要指定 `--networking` 参数，并跟上网络的名称。
- en: Given that we won't have the time and space to evaluate all the CNIs, we'll
    use kubenet as the networking solution for the cluster we're about to create.
    I encourage you to explore the other options on your own (or wait until I write
    a post or a new book).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们没有足够的时间和空间来评估所有的 CNI，我们将使用 kubenet 作为我们即将创建的集群的网络解决方案。我鼓励你自己探索其他选项（或者等到我写一篇文章或新书时再了解）。
- en: Finally, we are left with only one more choice we need to make. What will be
    the size of our nodes? Since we won't run many applications, `t2.small` should
    be more than enough and will keep AWS costs to a minimum. `t2.micro` is too small,
    so we elected the second smallest among those AWS offers.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只剩下一个选择需要做出决定。我们的节点大小应该是多少？由于我们不会运行很多应用，`t2.small` 应该足够了，而且能将 AWS 成本控制到最低。`t2.micro`
    太小，因此我们选择了 AWS 提供的第二小的实例类型。
- en: You might have noticed that we did not mention persistent volumes. We'll explore
    them in the next chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们没有提到持久化存储卷。我们将在下一章中进行探讨。
- en: 'The command that creates a cluster using the specifications we discussed is
    as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们讨论过的规格创建集群的命令如下：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We specified that the cluster should have three masters and one worker node.
    Remember, we can always increase the number of workers, so there's no need to
    start with more than what we need at the moment.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定集群应有三个主节点和一个工作节点。记住，我们始终可以增加工作节点的数量，因此目前不需要比实际需求更多的节点。
- en: The sizes of both worker nodes and masters are set to `t2.small`. Both types
    of nodes will be spread across the three availability zones we specified through
    the environment variable `ZONES`. Further on, we defined the public key and the
    type of networking.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点和主节点的大小都设置为 `t2.small`。这两种类型的节点将分布在我们通过环境变量 `ZONES` 指定的三个可用区中。接下来，我们定义了公钥和网络类型。
- en: We used `--kubernetes-version` to specify that we prefer to run version `v1.8.4`.
    Otherwise, we'd get a cluster with the latest version considered stable by kops.
    Even though running latest stable version is probably a good idea, we'll need
    to be a few versions behind to demonstrate some of the features kops has to offer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `--kubernetes-version` 参数指定我们偏好运行版本 `v1.8.4`。否则，我们将得到一个由 kops 认为稳定的最新版本的集群。尽管运行最新的稳定版本可能是个不错的主意，但我们需要稍微滞后几个版本，以展示
    kops 提供的一些特性。
- en: By default, kops sets `authorization` to `AlwaysAllow`. Since this is a simulation
    of a production-ready cluster, we changed it to `RBAC`, which we already explored
    in one of the previous chapters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，kops 将 `authorization` 设置为 `AlwaysAllow`。由于这是一个生产就绪集群的模拟，我们将其更改为 `RBAC`，这是我们在之前的章节中已经探索过的。
- en: The `--yes` argument specifies that the cluster should be created right away.
    Without it, `kops` would only update the state in the S3 bucket, and we'd need
    to execute `kops apply` to create the cluster. Such two-step approach is preferable,
    but I got impatient and would like to see the cluster in all its glory as soon
    as possible.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`--yes` 参数指定集群应立即创建。如果没有它，`kops` 只会更新 S3 存储桶中的状态，我们需要执行 `kops apply` 来创建集群。虽然这种两步法更为推荐，但我有点急切，想尽快看到集群的最终效果。'
- en: 'The output of the command is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can see that the `kubectl` context was changed to point to the new cluster
    which is starting, and will be ready soon. Further down are a few suggestions
    of the next actions. We'll skip them, for now.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `kubectl` 上下文已更改，指向正在启动的新集群，并且即将就绪。接下来列出了一些建议的后续操作。我们暂时跳过它们。
- en: A note to Windows users
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 用户注意
- en: Kops was executed inside a container. It changed the context inside the container
    that is now gone. As a result, your local `kubectl` context was left intact. We'll
    fix that by executing `kops export kubecfg --name ${NAME}` and `export KUBECONFIG=$PWD/config/kubecfg.yaml`.
    The first command exported the config to `/config/kubecfg.yaml`. That path was
    specified through the environment variable `KUBECONFIG` and is mounted as `config/kubecfg.yaml`
    on local hard disk. The latter command exports `KUBECONFIG` locally. Through that
    variable, kubectl is now instructed to use the configuration in `config/kubecfg.yaml`
    instead of the default one. Before you run those commands, please give AWS a few
    minutes to create all the EC2 instances and for them to join the cluster. After
    waiting and executing those commands, you'll be all set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Kops是在容器内执行的。它更改了容器内的上下文，但这个上下文现在已经消失。因此，你的本地`kubectl`上下文保持不变。我们可以通过执行`kops
    export kubecfg --name ${NAME}`和`export KUBECONFIG=$PWD/config/kubecfg.yaml`来修复这个问题。第一个命令将配置导出了`/config/kubecfg.yaml`。这个路径是通过环境变量`KUBECONFIG`指定的，并且在本地硬盘上挂载为`config/kubecfg.yaml`。第二个命令将`KUBECONFIG`导出到本地。通过这个变量，kubectl现在被指示使用`config/kubecfg.yaml`中的配置，而不是默认配置。在运行这些命令之前，请等待AWS几分钟，让所有的EC2实例创建完成并加入集群。等待后，执行这些命令，你就一切准备好了。
- en: We'll use kops to retrieve the information about the newly created cluster.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用kops来获取有关新创建集群的信息。
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This information does not tell us anything new. We already knew the name of
    the cluster and the zones it runs in.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息没有告诉我们任何新内容。我们已经知道集群的名称以及它运行的区域。
- en: How about `kubectl cluster-info`?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl cluster-info`怎么样？'
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can see that the master is running as well as KubeDNS. The cluster is probably
    ready. If in your case KubeDNS did not appear in the output, you might need to
    wait for a few more minutes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到主节点正在运行，KubeDNS也在运行。集群可能已经准备好了。如果在你的情况下KubeDNS没有出现在输出中，你可能需要再等几分钟。
- en: We can get more reliable information about the readiness of our new cluster
    through the `kops validate` command.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`kops validate`命令获得更可靠的集群准备状态信息。
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: That is useful. We can see that the cluster uses four instance groups or, to
    use AWS terms, four **auto-scaling groups** (**ASGs**). There's one for each master,
    and there's one for all the (worker) nodes.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有用。我们可以看到集群使用了四个实例组，或者用AWS的术语来说，四个**自动扩展组**（**ASG**）。每个主节点都有一个，所有（工作）节点共用一个。
- en: The reason each master has a separate ASG lies in need to ensure that each is
    running in its own **availability zone** (**AZ**). That way we can guarantee that
    failure of the whole AZ will affect only one master. Nodes (workers), on the other
    hand, are not restricted to any specific AZ. AWS is free to schedule nodes in
    any AZ that is available.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主节点都有一个单独的ASG，其原因在于需要确保每个主节点都在自己的**可用区**（**AZ**）中运行。这样，我们可以保证整个AZ的故障只会影响一个主节点。而节点（工作节点）则不受任何特定AZ的限制。AWS可以在任何可用的AZ中调度节点。
- en: We'll discuss ASGs in more detail later on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会更详细地讨论ASG。
- en: Further down the output, we can see that there are four servers, three with
    masters, and one with worker node. All are ready.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中进一步显示，我们可以看到四台服务器，其中三台为主节点，一台为工作节点。所有节点都已准备好。
- en: Finally, we got the confirmation that our `cluster devops23.k8s.local is ready`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们得到了确认信息：我们的`cluster devops23.k8s.local已准备好`。
- en: Using the information we got so far, we can describe the cluster through the
    *figure 14-1*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用到目前为止获得的信息，我们可以通过*图14-1*来描述集群。
- en: '![](img/41026644-f0e5-4485-94ef-86b733e7c2e7.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41026644-f0e5-4485-94ef-86b733e7c2e7.png)'
- en: 'Figure 14-1: The servers that form the Kubernetes cluster'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-1：构成Kubernetes集群的服务器
- en: There's apparently much more to the cluster than what is depicted in the *figure
    14-1*. So, let's try to discover the goodies kops created for us.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，集群中包含的内容比*图14-1*所展示的要多得多。那么，让我们来发现kops为我们创建的那些好东西吧。
- en: Exploring the components that constitute the cluster
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索构成集群的组件
- en: When kops created the VMs (EC2 instances), the first thing it did was to execute
    *nodeup*. It, in turn, installed a few packages. It made sure that Docker, Kubelet,
    and Protokube are up and running.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当kops创建VM（EC2实例）时，第一件事就是执行*nodeup*。它进而安装了一些软件包。它确保Docker、Kubelet和Protokube都已经启动并运行。
- en: '**Docker** runs containers. It would be hard for me to imagine that you don''t
    know what Docker does, so we''ll skip to the next in line.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker**用于运行容器。我很难想象你不知道Docker的作用，因此我们跳到下一个内容。'
- en: '**Kubelet** is Kubernetes'' node agent. It runs on every node of a cluster,
    and its primary purpose is to run Pods. Or, to be more precise, it ensures that
    the containers described in PodSpecs are running as long as they are healthy.
    It primarily gets the information about the Pods it should run through Kubernetes''
    API server. As an alternative, it can get the info through files, HTTP endpoints,
    and HTTP servers.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubelet** 是 Kubernetes 的节点代理。它在集群的每个节点上运行，其主要目的就是运行 Pods。或者，更准确地说，它确保 PodSpecs
    中描述的容器在健康的情况下始终运行。它主要通过 Kubernetes 的 API 服务器获取应该运行的 Pods 信息。作为替代方案，它还可以通过文件、HTTP
    端点和 HTTP 服务器获取这些信息。'
- en: Unlike Docker and Kubelet, **Protokube** is specific to kops. Its primary responsibilities
    are to discover master disks, to mount them, and to create manifests. Some of
    those manifests are used by Kubelet to create system-level Pods and to make sure
    that they are always running.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Docker 和 Kubelet 不同，**Protokube** 是 kops 特有的。它的主要职责是发现主节点磁盘、挂载磁盘并创建清单。其中一些清单被
    Kubelet 用于创建系统级 Pods，并确保它们始终运行。
- en: Besides starting the containers defined through Pods in the manifests (created
    by Protokube), Kubelet also tries to contact the API server which, eventually,
    is also started by it. Once the connection is established, Kubelet registers the
    node where it is running.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过清单中定义的 Pods 启动容器（由 Protokube 创建）之外，Kubelet 还会尝试联系 API 服务器，而该服务器最终也是由 Kubelet
    启动的。一旦连接建立，Kubelet 会注册其所在的节点。
- en: 'All three packages are running on all the nodes, no matter whether they are
    masters or workers:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个软件包在所有节点上运行，无论它们是主节点还是工作节点：
- en: '![](img/895f0fd4-538c-4178-886e-beefe7b0362b.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/895f0fd4-538c-4178-886e-beefe7b0362b.png)'
- en: 'Figure 14-2: The servers that form the Kubernetes cluster'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-2：组成 Kubernetes 集群的服务器
- en: 'Let''s take a look at the system-level Pods currently running in our cluster:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看当前在集群中运行的系统级 Pods：
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, quite a few core components are running.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，许多核心组件正在运行。
- en: We can divide core (or system-level) components into two groups. Master components
    run only on masters. In our case, they are `kube-apiserver`, `kube-controller-manager`,
    `kube-scheduler`, `etcd`, and `dns-controller`. Node components run on all the
    nodes, both masters and workers. We already discussed a few of those. In addition
    to Protokube, Docker, and Kubelet, we got `kube-proxy`, as one more node component.
    Since this might be the first time you heard about those core components, we'll
    briefly explain each of their functions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将核心（或系统级）组件分为两组。主组件仅在主节点上运行。在我们的例子中，它们是 `kube-apiserver`、`kube-controller-manager`、`kube-scheduler`、`etcd`
    和 `dns-controller`。节点组件在所有节点上运行，包括主节点和工作节点。我们已经讨论了其中的一些。除了 Protokube、Docker 和
    Kubelet 之外，我们还需要了解 `kube-proxy`，这是另一个节点组件。由于这可能是你第一次听说这些核心组件，我们将简要解释每个组件的功能。
- en: '**Kubernetes API Server** (`kube-apiserver`) accepts requests to create, update,
    or remove Kubernetes resources. It listens on ports `8080` and `443`. The former
    is insecure and is only reachable from the same server. Through it, the other
    components can register themselves without requiring a token. The former port
    (`443`) is used for all external communications with the API Server. That communication
    can be user-facing like, for example, when we send a `kubectl` command. Kubelet
    also uses `443` port to reach the API server and register itself as a node.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes API 服务器**（`kube-apiserver`）接受请求以创建、更新或删除 Kubernetes 资源。它监听 `8080`
    和 `443` 端口。前者是不安全的，仅能从同一服务器访问。通过它，其他组件可以在不需要令牌的情况下注册自己。后者（`443` 端口）用于与 API 服务器的所有外部通信。这些通信可以是面向用户的，例如，当我们发送
    `kubectl` 命令时。Kubelet 也使用 `443` 端口与 API 服务器通信并注册自己为节点。'
- en: No matter who initiates communication with the API Server, its purpose is to
    validate and configure API object. Among others, those can be Pods, Services,
    ReplicaSets, and others. Its usage is not limited to user-facing interactions.
    All the components in the cluster interact with the API Server for the operations
    that require a cluster-wide shared state.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 无论谁发起与 API 服务器的通信，其目的都是验证和配置 API 对象。其他的对象包括 Pods、Services、ReplicaSets 等等。它的使用不仅限于面向用户的交互。集群中的所有组件都与
    API 服务器进行交互，以执行需要集群范围共享状态的操作。
- en: The shared state of the cluster is stored in `etcd` ([https://github.com/coreos/etcd](https://github.com/coreos/etcd)).
    It is a key/value store where all cluster data is kept, and it is highly available
    through consistent data replication. It is split into two Pods, where `etcd-server`
    holds the state of the cluster and `etcd-server-events` stores the events.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的共享状态存储在`etcd`（[https://github.com/coreos/etcd](https://github.com/coreos/etcd)）中。它是一个键/值存储，其中保存了所有集群数据，并通过一致性数据复制保持高可用性。它分为两个
    Pod，其中`etcd-server`保存集群的状态，`etcd-server-events`存储事件。
- en: Kops creates an **EBS volume** for each `etcd` instance. It serves as its storage.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Kops 为每个`etcd`实例创建一个**EBS 卷**。它充当其存储。
- en: '**Kubernetes Controller Manager** (`kube-controller-manager`) is in charge
    of running controllers. You already saw a few controllers in action like `ReplicaSets`
    and `Deployments`. Apart from object controllers like those, `kube-controller-manager`
    is also in charge of Node Controllers responsible for monitoring servers and responding
    when one becomes unavailable.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes Controller Manager** (`kube-controller-manager`) 负责运行控制器。您已经看到一些控制器在运行，比如`ReplicaSets`和`Deployments`。除了这些对象控制器外，`kube-controller-manager`还负责节点控制器，负责监视服务器并在其中一个不可用时做出响应。'
- en: '**Kubernetes Scheduler** (`kube-scheduler`) watches the API Server for new
    Pods and assigns them to a node. From there on, those Pods are run by Kubelet
    on the allocated node.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes Scheduler** (`kube-scheduler`) 监视 API 服务器以获取新的 Pod，并将它们分配给一个节点。从那时起，这些
    Pod 由分配的节点上的 Kubelet 运行。'
- en: '**DNS Controller** (`dns-controller`) allows nodes and users to discover the
    API Server.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**DNS Controller** (`dns-controller`) 允许节点和用户发现 API 服务器。'
- en: '**Kubernetes Proxy** (`kube-proxy`) reflects Services defined through the API
    Server. It is in charge of TCP and UDP forwarding. It runs on all nodes of the
    cluster (both masters and workers).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes Proxy** (`kube-proxy`) 反映了通过 API 服务器定义的服务。它负责 TCP 和 UDP 转发。它在集群的所有节点上运行（包括主节点和工作节点）。'
- en: '![](img/3b27529b-a97b-4ebd-b2a1-52e9976c3280.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b27529b-a97b-4ebd-b2a1-52e9976c3280.png)'
- en: 'Figure 14-3: The core components of the cluster'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-3：集群的核心组件
- en: There's much more going on in our new cluster. For now, we explored only the
    major components.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的新集群中，还有更多的工作。目前，我们只探索了主要的组件。
- en: Next, we'll try to update our cluster.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试更新我们的集群。
- en: Updating the cluster
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新集群
- en: No matter how much we plan, we will never manage to have a cluster with capacity
    that should serve us equally well today as tomorrow. Things change, and we'll
    need to be able to adapt to those changes. Ideally, our cluster should increase
    and decrease its capacity automatically by evaluating metrics and firing alerts
    that would interact with kops or directly with AWS. However, that is an advanced
    topic that we won't be able to cover. For now, we'll limit the scope to manual
    cluster updates.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们计划多少，我们都永远无法管理一个集群，它的能力应该在今天和明天同样好。事情在变化，我们需要能够适应这些变化。理想情况下，我们的集群应该通过评估指标和触发警报来自动增加和减少其容量，这些警报将与
    kops 或直接与 AWS 交互。但这是一个我们目前无法涵盖的高级主题。目前，我们将限制范围在手动更新集群上。
- en: With kops, we cannot update the cluster directly. Instead, we edit the desired
    state of the cluster stored, in our case, in the S3 bucket. Once the state is
    changed, kops will make the necessary changes to comply with the new desire.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 kops，我们不能直接更新集群。相反，我们编辑存储在 S3 存储桶中的集群期望状态。一旦状态改变，kops 将进行必要的更改以符合新的期望。
- en: We'll try to update the cluster so that the number of worker nodes is increased
    from one to two. In other words, we want to add one more server to the cluster.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试更新集群，从一个工作节点增加到两个。换句话说，我们希望在集群中添加一个服务器。
- en: Let's see the sub-commands provided through `kops edit`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过`kops edit`提供的子命令。
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output, limited to the available commands, is as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于可用命令，如下所示：
- en: '[PRE42]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We have three types of edits we can make. We did not set up federation, so that
    one is out of the game. You might think that `cluster` would provide the possibility
    to create a new worker node. However, that is not the case. If you execute `kops
    edit cluster --name $NAME`, you'll see that nothing in the configuration indicates
    how many nodes we should have. That is normal considering that we should not create
    servers in AWS directly. Just as Kubernetes, AWS also prefers declarative approach
    over imperative. At least, when dealing with EC2 instances.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行三种类型的编辑。由于我们没有设置联合，因此这一项排除在外。你可能认为`cluster`会提供创建新工作节点的可能性。然而，事实并非如此。如果你执行`kops
    edit cluster --name $NAME`，你会看到配置中并没有指示我们应该有多少个节点。这是正常的，因为我们不应直接在AWS中创建服务器。正如Kubernetes一样，AWS也更倾向于使用声明式方法而非命令式方法。至少在处理EC2实例时是如此。
- en: Instead of sending an imperative instruction to create a new node, we'll change
    the value of the **Auto-Scaling Group** (**ASG**) related to worker nodes. Once
    we change ASG values, AWS will make sure that it complies with the new desire.
    It'll not only create a new server to comply with the new ASG sizes, but it will
    also monitor EC2 instances and maintain the desired number in case one of them
    fails.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会直接发送命令式指令来创建新节点，而是会修改与工作节点相关的**自动扩展组**（**ASG**）的值。一旦我们更改了ASG值，AWS将确保其符合新的期望值。它不仅会创建一台新服务器以符合新的ASG大小，而且还会监控EC2实例，确保在其中一台实例出现故障时，保持期望的实例数量。
- en: So, we'll choose the third `kops edit` option.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将选择第三个`kops edit`选项。
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We executed `kops edit ig` command, where `ig` is one of the aliases of `instancegroup`.
    We specified the name of the cluster with the `--name` argument. Finally, we set
    the type of the servers to `nodes`. As a result, we are presented with the `InstanceGroup`
    config for the Auto-Scaling Group associated with worker nodes.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行了`kops edit ig`命令，其中`ig`是`instancegroup`的一个别名。我们使用`--name`参数指定了集群的名称。最后，我们将服务器的类型设置为`nodes`。结果，我们看到了与工作节点相关的自动扩展组（Auto-Scaling
    Group）的`InstanceGroup`配置。
- en: The output is as follows.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE44]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Bear in mind that what you're seeing on the screen is not the standard output
    (`stdout`). Instead, the configuration is opened in your default editor. In my
    case, that is `vi`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，你在屏幕上看到的并不是标准输出（`stdout`）。相反，配置会在你默认的编辑器中打开。在我的情况下，它是`vi`。
- en: We can see some useful information from this config. For example, the `image`
    used to create EC2 instances is based on Debian. It is custom made for kops. The
    `machineType` represents EC2 size which is set to `t2.small`. Further down, you
    can see that we're running the VMs in three subnets or, since we're in AWS, three
    availability zones.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个配置中看到一些有用的信息。例如，用于创建EC2实例的`image`是基于Debian的，专为kops定制。`machineType`表示EC2的大小，设置为`t2.small`。再往下看，我们可以看到我们在三个子网中运行虚拟机，或者说，因为我们在AWS中，实际上是三个可用区。
- en: The parts of the config we care about are the `spec` entries `maxSize` and `minSize`.
    Both are set to `1` since that is the number of worker nodes we specified when
    we created the cluster. Please change the values of those two entries to `2`,
    save, and exit.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关心的配置部分是`spec`条目中的`maxSize`和`minSize`。这两个值都设置为`1`，因为那是我们在创建集群时指定的工作节点数。请将这两个条目的值更改为`2`，然后保存并退出。
- en: If you're using `vi` as your default editor, you'll need to press *I* to enter
    into the `insert` mode. From there on, you can change the values. Once you're
    finished editing, please press the *ESC* key, followed by `:wq`. Colon (`:`) allows
    us to enter commands, `w` is translated to save, and `q` to quit. Don't forget
    to press the enter key. If, on the other hand, you are not using `vi`, you're
    on your own. I'm sure that you'll know how to operate your default editor. If
    not, Google is your friend.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用`vi`作为默认编辑器，你需要按*I*键进入`插入`模式。从那时起，你可以更改值。编辑完成后，请按*ESC*键，然后输入`:wq`。冒号（`:`）允许我们输入命令，`w`代表保存，`q`代表退出。别忘了按回车键。如果你使用的不是`vi`，那就看你自己了。我相信你知道如何操作你的默认编辑器。如果不确定，Google是你的好朋友。
- en: '![](img/2ce16a02-cba2-4136-b00d-e4282ebd6cc0.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ce16a02-cba2-4136-b00d-e4282ebd6cc0.png)'
- en: 'Figure 14-4: The process behind the `kops edit` command'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-4：`kops edit`命令背后的过程
- en: Now that we changed the configuration, we need to tell kops that we want it
    to update the cluster to comply with the new desired state.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经更改了配置，我们需要告诉kops，我们希望它更新集群，以符合新的期望状态。
- en: '[PRE45]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output, limited to the last few lines, is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We can see that kops set our `kubectl` context to the cluster we updated. There
    was no need for that since that was already our context, but it did that anyway.
    Further on, we got the confirmation that the changes `have been applied to the
    cloud`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，kops 将我们的 `kubectl` 上下文设置为我们更新的集群。其实没有必要这样做，因为那已经是我们的上下文了，但它还是执行了此操作。接下来，我们收到了变更“已应用到云端”的确认。
- en: The last sentence is interesting. It informed us that we can use `kops rolling-update`.
    The `kops update` command applies all the changes to the cluster at once. That
    can result in downtime. For example, if we wanted to change the image to a newer
    version, running `kops update` would recreate all the worker nodes at once. As
    a result, we'd have downtime from the moment instances are shut down until the
    new ones are created, and Kubernetes schedules the Pods in them. Kops knows that
    such an action should not be allowed so, if the update requires that servers are
    replaced, it does nothing expecting that you'll execute `kops rolling-update`
    afterward. That is not our case. Adding new nodes does not require restarts or
    replacement of the existing servers.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一句话很有意思。它告诉我们可以使用 `kops rolling-update`。`kops update` 命令会一次性将所有更改应用到集群。这可能会导致停机。例如，如果我们想将镜像更换为更新版本，运行
    `kops update` 会一次性重新创建所有工作节点。因此，从实例被关闭的那一刻到新实例创建完成并且 Kubernetes 调度 Pods 的过程中，我们会经历停机。Kops
    知道此类操作不应被允许，因此，如果更新需要替换服务器，它什么也不做，而是期望你在之后执行 `kops rolling-update`。但这不是我们的情况。添加新节点不需要重启或替换现有服务器。
- en: The `kops rolling-update` command intends to apply the changes without downtime.
    It would apply them to one server at the time so that most of the servers are
    always running. In parallel, Kubernetes would be rescheduling the Pods that were
    running on the servers that were brought down.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`kops rolling-update` 命令旨在无停机地应用更改。它会一次应用到一台服务器，这样大多数服务器始终处于运行状态。与此同时，Kubernetes
    会重新调度在被停机的服务器上运行的 Pods。'
- en: As long as our applications are scaled, `kops rolling-update` should not produce
    downtime.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们的应用程序被扩展，`kops rolling-update` 应该不会导致停机。
- en: Let's see what happened when we executed the `kops update` command.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们执行 `kops update` 命令时发生了什么。
- en: Kops retrieved the desired state from the S3 bucket.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kops 从 S3 存储桶中获取了期望的状态。
- en: Kops sent requests to AWS API to change the values of the workers ASG.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kops 向 AWS API 发送请求，以更改工作节点 ASG 的值。
- en: AWS modified the values of the workers ASG by increasing them by 1.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS 修改了工作节点 ASG 的值，增加了 1。
- en: ASG created a new EC2 instance to comply with the new sizing.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ASG 创建了一个新的 EC2 实例，以符合新的大小要求。
- en: Protokube installed Kubelet and Docker and created the manifest file with the
    list of Pods.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Protokube 安装了 Kubelet 和 Docker，并创建了包含 Pods 列表的清单文件。
- en: Kubelet read the manifest file and run the container that forms the `kube-proxy`
    Pod (the only Pod on the worker nodes).
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubelet 读取了清单文件，并运行了形成 `kube-proxy` Pod（唯一在工作节点上的 Pod）的容器。
- en: Kubelet sent a request to the `kube-apiserver` (through the `dns-controller`)
    to register the new node and join it to the cluster. The information about the
    new node is stored in `etcd`.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubelet 向 `kube-apiserver`（通过 `dns-controller`）发送请求，以注册新节点并将其加入集群。关于新节点的信息被存储在
    `etcd` 中。
- en: This process is almost identical to the one used to create the nodes of the
    cluster.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程几乎与用于创建集群节点的过程相同。
- en: '![](img/a24d8159-561d-4817-b0ea-89d5683c7081.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a24d8159-561d-4817-b0ea-89d5683c7081.png)'
- en: 'Figure 14-5: The process behind the `kops update` command'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-5：`kops update` 命令背后的过程
- en: Unless you are a very slow reader, ASG created a new EC2 instance, and Kubelet
    joined it to the cluster. We can confirm that through the `kops validate` command.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你是一个非常慢的读者，否则 ASG 创建了一个新的 EC2 实例，Kubelet 将其加入了集群。我们可以通过 `kops validate` 命令来确认这一点。
- en: '[PRE47]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We can see that now we have two nodes (there was one before) and that they are
    located somewhere inside the three `us-east-2` availability zones.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在我们有两个节点（之前只有一个），它们位于 `us-east-2` 区域的三个可用区中的某个位置。
- en: Similarly, we can use `kubectl` to confirm that Kubernetes indeed added the
    new worker node to the cluster.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用 `kubectl` 来确认 Kubernetes 确实将新的工作节点添加到了集群中。
- en: '[PRE49]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE50]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: That was easy, wasn't it? From now on, we can effortlessly add or remove nodes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单，不是吗？从现在开始，我们可以轻松地添加或移除节点。
- en: How about upgrading?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，升级怎么样呢？
- en: Upgrading the cluster manually
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动升级集群
- en: The process to upgrade the cluster depends on what we want to do.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 升级集群的过程取决于我们想做什么。
- en: If we'd like to upgrade it to a specific Kubernetes version, we can execute
    a similar process like the one we used to add a new worker node.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将其升级到特定的 Kubernetes 版本，可以执行类似我们添加新工作节点时的过程。
- en: '[PRE51]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Just as before, we are about to edit the cluster definition. The only difference
    is that this time we're not editing a specific instance group, but the cluster
    as a whole.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前一样，我们将编辑集群定义。唯一的不同是这次我们不是编辑特定的实例组，而是整个集群。
- en: If you explore the YAML file in front of you, you'll see that it contains the
    information we specified when we created the cluster, combined with the kops default
    values that we omitted to set.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看面前的 YAML 文件，你会看到它包含了我们创建集群时指定的信息，以及我们未设置的 kops 默认值。
- en: For now, we're interested in `kubernetesVersion`. Please find it and change
    the version from `v1.8.4` to `v1.8.5`. Save and exit.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们关注的是`kubernetesVersion`。请找到它并将版本从`v1.8.4`更改为`v1.8.5`。保存并退出。
- en: Now that we modified the desired state of the cluster, we can proceed with `kops
    update`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们修改了集群的期望状态，就可以继续执行`kops update`。
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The last line of the output indicates that we *must specify* `--yes` *to apply
    changes*. Unlike the previous time we executed `kops update`, now we did not specify
    the argument `--yes`. As a result, we got a preview, or a dry-run, of what would
    happen if we apply the change. Previously, we added a new worker node, and that
    operation did not affect the existing servers. We were brave enough to update
    the cluster without previewing which resources will be created, updated, or destroyed.
    However, this time we are upgrading the servers in the cluster. Existing nodes
    will be replaced with new ones, and that is potentially dangerous operation. Later
    on, we might trust kops to do what's right and skip the preview altogether. But,
    for now, we should evaluate what will happen if we proceed.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后一行指出我们*必须指定*`--yes`*以应用更改*。与上次执行`kops update`时不同，这次我们没有指定`--yes`参数。因此，我们获得了一个预览，或者说是一个干运行，显示了如果我们应用更改会发生什么。之前，我们添加了一个新工作节点，这个操作不会影响现有的服务器。我们当时足够大胆，直接更新了集群，而没有预览会创建、更新或销毁哪些资源。然而，这次我们要升级集群中的服务器。现有节点将被新节点替代，这是一项潜在的危险操作。以后，我们可能会信任
    kops 自动执行正确的操作，完全跳过预览。但现在，我们应该评估一下如果继续操作会发生什么。
- en: Please go through the output. You'll see a git-like diff of the changes that
    will be applied to some of the resources that form the cluster. Take your time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看输出。你将看到一个类似 Git 的差异，列出了将应用于集群中某些资源的更改。请慢慢查看。
- en: Now that you are confident with the changes, we can apply them.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你对更改充满信心，我们就可以应用这些更改。
- en: '[PRE53]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The last line of the output states that `changes may require instances to restart:
    kops rolling-update cluster`. We already saw that message before but, this time,
    the update was not performed. The reason is simple, even though not necessarily
    very intuitive. We can update auto-scaling groups since that results in creation
    or destruction of nodes. But, when we need to replace them, as in this case, it
    would be disastrous to execute a simple update. Updating everything at once would,
    at best, produce a downtime. In our case, it''s even worse. Destroying all the
    masters at once would likely result in a loss of quorum. A new cluster might not
    be able to recuperate.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后一行指出`更改可能需要重新启动实例：kops rolling-update cluster`。我们之前已经看到过这个消息，但这次没有执行更新。原因很简单，虽然不一定直观。我们可以更新自动扩展组，因为那会导致节点的创建或销毁。但当我们需要替换它们时，像这种情况，执行一个简单的更新可能会造成灾难性的后果。一次性更新所有内容，充其量会造成停机。在我们的案例中，情况更糟。一次性销毁所有主节点可能导致法定人数丧失，新的集群可能无法恢复。
- en: All in all, kops requires an extra step when "big bang" updating of the cluster
    might result in undesirable results. So, we need to execute the `kops rolling-update`
    command. Since we're still insecure, we'll run a preview first.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，kops 在进行“big bang”更新时需要额外的步骤，因为这可能会导致不期望的结果。因此，我们需要执行`kops rolling-update`命令。由于我们仍然处于不安全状态，所以我们首先运行预览。
- en: '[PRE54]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We can see that all the nodes require an update. Since we already evaluated
    the changes through the output of the `kops update` command, we'll proceed and
    apply rolling updates.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有节点都需要更新。由于我们已经通过`kops update`命令的输出评估了这些更改，我们将继续进行并应用滚动更新。
- en: '[PRE56]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The rolling update process started, and it will take approximately 30 minutes
    to complete.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新过程已启动，预计需要大约30分钟才能完成。
- en: 'We''ll explore the output as it comes:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步查看输出：
- en: '[PRE57]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output starts with the same information we got when we asked for a preview,
    so there''s not much to comment:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出内容与我们在请求预览时获得的信息相同，因此没有太多需要评论的地方：
- en: '[PRE58]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Instead of destroying the first node, kops picked one masters and drained it.
    That way, the applications running on it can shut down gracefully. We can see
    that it drained `etcd-server-events`, `etcd-server-ip`, `kube-apiserver`, `kube-controller-manager`,
    `kube-proxy`, `kube-scheduler` Pods running on the server `ip-172-20-40-167`.
    As a result, Kubernetes rescheduled them to one of the healthy nodes. That might
    not be true for all the Pods but only for those that can be rescheduled.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: kops 选择了排空其中一个主节点，而不是销毁第一个节点。这样，运行在该节点上的应用可以优雅地关闭。我们可以看到，它排空了在服务器 `ip-172-20-40-167`
    上运行的 `etcd-server-events`、`etcd-server-ip`、`kube-apiserver`、`kube-controller-manager`、`kube-proxy`
    和 `kube-scheduler` Pods。因此，Kubernetes 将它们重新调度到其他健康的节点上。可能并非所有的 Pods 都如此，但对于那些可以重新调度的
    Pods 来说是成立的。
- en: '[PRE59]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We can see that after draining finished, the master node was stopped. Since
    each master is associated with an auto-scaling group, AWS will detect that the
    node is no more, and start a new one. Once the new server is initialized, `nodeup`
    will execute and install Docker, Kubelet, and Protokube. The latter will create
    the manifest that will be used by Kubelet to run the Pods required for a master
    node. Kubelet will also register the new node with one of the healthy masters.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在排空完成后，主节点被停止。由于每个主节点都与一个自动扩展组相关联，AWS 会检测到该节点已不存在，并启动一个新节点。新服务器初始化后，`nodeup`
    将执行并安装 Docker、Kubelet 和 Protokube。后者将创建一个清单，用于 Kubelet 运行主节点所需的 Pods。Kubelet 还会将新节点注册到某个健康的主节点上。
- en: That part of the process is the same as the one executed when creating a new
    cluster or when adding new servers. It is the part that takes longest to complete
    (around five minutes).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程与创建新集群或添加新服务器时执行的过程相同。这是整个过程中最耗时的一部分（大约需要五分钟）。
- en: '[PRE60]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can see that, after waiting for everything to settle, kops validated the
    cluster, thus confirming that upgrade of the first master node finished successfully.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在等待一切稳定后，kops 验证了集群，从而确认了第一个主节点的升级已成功完成。
- en: '![](img/cf42f01c-d9a5-4b8a-8e49-38cee4dd01d1.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf42f01c-d9a5-4b8a-8e49-38cee4dd01d1.png)'
- en: 'Figure 14-6: Rolling upgrade of one of the master nodes'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-6：滚动升级其中一个主节点
- en: As soon as it validated the upgrade of the first master, kops proceeded with
    the next node. During next ten to fifteen minutes, the same process will be repeated
    with the other two masters. Once all three are upgraded, kops will execute the
    same process with the worker nodes, and we'll have to wait for another ten to
    fifteen minutes.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦验证完第一个主节点的升级，kops 会继续进行下一个节点的升级。在接下来的十到十五分钟内，其他两个主节点会重复相同的过程。所有三个主节点升级完成后，kops
    会对工作节点执行相同的过程，我们还需要再等十到十五分钟。
- en: '[PRE61]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Finally, once all the servers were upgraded, we can see that rolling update
    was completed.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦所有服务器都升级完成，我们可以看到滚动更新已经完成。
- en: The experience was positive but long. Auto-scaling groups need a bit of time
    to detect that a server is down. It takes a minute or two for a new VM to be created
    and initialized. Docker, Kubelet, and Protokube need to be installed. Containers
    that form core Pods need to be pulled. All in all, quite a few things need to
    happen.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 整个体验是积极的，但过程较长。自动扩展组需要一些时间来检测到服务器已宕机。创建并初始化新虚拟机需要一两分钟。Docker、Kubelet 和 Protokube
    需要被安装。构成核心 Pod 的容器需要被拉取。总的来说，需要完成的工作还不少。
- en: The upgrade process would be faster if kops would use immutable approach and
    bake everything into images (AMIs). However, the choice was made to decouple OS
    with packages and core Pods, so the installation needs to be done at runtime.
    Also, the default distribution is Debian, which is not as light as, let's say,
    CoreOS. Due to those, and a few other design choices, the process is somehow lengthy.
    When combined with inevitable time AWS needs to do its part of the process, we're
    looking at over five minutes of upgrade duration for each node in a cluster. Even
    with only five nodes, the whole process is around thirty minutes. If we'd have
    a bigger cluster, it could take hours, or even days to upgrade.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 kops 使用不可变的方法并将所有内容打包成镜像（AMI），升级过程会更快。然而，选择将操作系统与软件包和核心 Pod 解耦，因此安装需要在运行时进行。另外，默认的发行版是
    Debian，它不像 CoreOS 那样轻量。由于这些以及其他一些设计选择，整个过程较为冗长。再加上 AWS 执行其部分过程所需的时间，每个集群节点的升级时长超过五分钟。即便只有五个节点，整个过程也需要大约三十分钟。如果集群规模更大，升级可能需要几个小时，甚至几天。
- en: Even though it takes considerable time to upgrade, the process is hands-free.
    If we are brave enough, we can let kops do its job and spend our time working
    on something more exciting. Assuming that our applications are designed to be
    scalable and fault-tolerant, we won't experience downtime. That is what matters
    much more than whether we'll be able to watch the process unfold. If we trust
    the system, we can just as well run it in the background and ignore it. However,
    earning trust is hard. We need to successfully run the process a few times before
    we put our fate in it. Even then, we should build a robust monitoring and alerting
    system that will notify us if things go wrong. Unfortunately, we won't cover those
    subjects in this book. You'll have to wait for the next one or explore it yourself.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管升级过程需要相当长的时间，但它是免手动的。如果我们足够勇敢，可以让 kops 完成任务，然后将时间投入到更有趣的事情上。假设我们的应用程序设计得可扩展且具容错能力，我们不会经历停机。这比是否能够观看升级过程更重要。如果我们信任系统，就可以在后台运行它并忽略它。然而，赢得信任是很难的。在将我们的命运交给它之前，我们需要成功执行几次升级。即便如此，我们仍应建立一个强大的监控和报警系统，以便在出现问题时通知我们。不幸的是，这本书不涉及这些内容。你需要等到下一本书或者自己探索。
- en: Let's go back to our cluster and verify that Kubernetes was indeed upgraded.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到集群，验证 Kubernetes 是否确实已经升级。
- en: '[PRE62]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE63]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Judging by versions of each of the nodes, all were upgraded to `v1.8.5`. The
    process worked.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个节点的版本来看，所有节点都已升级到 `v1.8.5`。升级过程成功。
- en: Try to upgrade often. As a rule of thumb, you should upgrade one minor release
    at a time.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试经常升级。作为一个经验法则，你应该一次升级一个次要版本。
- en: Even if you are a couple of minor releases behind the stable kops-recommended
    release, it's better if you execute multiple rolling upgrades (one for each minor
    release) than to jump to the latest at once. By upgrading to the next minor release,
    you'll minimize potential problems and simplify rollback if required.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你落后于稳定的 kops 推荐版本几个次要版本，执行多次滚动升级（每次升级一个次要版本）也比一次性跳到最新版本更好。通过升级到下一个次要版本，你将最小化潜在问题，并简化回滚操作（如果需要的话）。
- en: Even though kops is fairly reliable, you should not trust it blindly. It's relatively
    easy to create a small testing cluster running the same release as production,
    execute the upgrade process, and validate that everything works as expected. Once
    finished, you can destroy the test cluster and avoid unnecessary expenses.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 kops 相当可靠，你也不应盲目相信它。创建一个与生产环境相同版本的小型测试集群，执行升级过程，并验证一切是否按预期工作是相对简单的。完成后，你可以销毁测试集群，避免不必要的开销。
- en: Don't trust anyone. Test upgrades in a separate cluster.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 不要轻易相信任何人。在单独的集群中测试升级。
- en: Upgrading the cluster automatically
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动升级集群
- en: We edited cluster's desired state before we started the rolling update process.
    While that worked well, we're likely to always upgrade to the latest stable version.
    In those cases, we can execute the `kops upgrade` command.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始滚动更新过程之前，我们编辑了集群的期望状态。尽管这有效，但我们通常会始终升级到最新的稳定版本。在这种情况下，我们可以执行 `kops upgrade`
    命令。
- en: '[PRE64]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Please note that this time we skipped the preview by setting the `--yes` argument.
    The output is as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这次我们通过设置 `--yes` 参数跳过了预览步骤。输出如下：
- en: '[PRE65]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We can see that the current Kubernetes version is `v1.8.5` and, in case we choose
    to proceed, it will be upgraded to the latest which, at the time of this writing,
    is `v1.8.6`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到当前的 Kubernetes 版本是 `v1.8.5`，如果选择继续操作，它将升级到最新版本，在撰写本文时，最新版本是 `v1.8.6`。
- en: '[PRE66]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Just as before, we can see from the last entry that `changes may require instances
    to restart: kops rolling-update cluster`.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所看到的，从最后一条记录可以看出，`更改可能需要实例重启：kops rolling-update cluster`。
- en: 'Let''s proceed:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续：
- en: '[PRE67]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: I'll skip commenting on the output since it is the same as the previous time
    we upgraded the cluster. The only significant difference, from the process perspective,
    is that we did not edit cluster's desired state by specifying the version we want,
    but initiated the process through the `kops upgrade` command. Everything else
    was the same in both cases.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我将跳过对输出的评论，因为它与上次升级集群时相同。从过程的角度来看，唯一的显著不同是我们没有通过指定所需版本来编辑集群的期望状态，而是通过 `kops
    upgrade` 命令启动了升级过程。其他方面在两种情况下完全相同。
- en: If we are to create a test cluster and write a set of tests that verify the
    upgrade process, we could execute the upgrade process periodically. We could,
    for example, create a job in Jenkins that would upgrade every month. If there
    isn't new Kubernetes release, it would do nothing. If there is, it would create
    a new cluster with the same release as production, upgrade it, validate that everything
    works as expected, destroy the testing cluster, upgrade the production cluster,
    and run another round of test. However, it takes time and experience to get to
    that point. Until then, manually executed upgrades are the way to go.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要创建一个测试集群，并编写一组测试来验证升级过程，我们可以定期执行升级过程。例如，我们可以在Jenkins中创建一个作业，每月升级一次。如果没有新的Kubernetes版本，它将什么也不做。如果有新版本，它将创建一个与生产环境相同版本的集群，对其进行升级，验证一切是否按预期工作，销毁测试集群，升级生产集群，并进行下一轮测试。然而，达到这一点需要时间和经验。在那之前，手动执行的升级是可行的方式。
- en: We are missing one more thing before we can deploy applications to our simulation
    of a production cluster.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够将应用程序部署到模拟的生产集群之前，我们还缺少一件事情。
- en: Accessing the cluster
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问集群
- en: We need a way to access the cluster. So far, we saw that we can, at least, interact
    with the Kubernetes API. Every time we executed `kubectl`, it communicated with
    the cluster through the API server. That communication is established through
    AWS Elastic Load Balancer (ELB). Let's take a quick look at it.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方式来访问集群。到目前为止，我们看到我们至少可以与Kubernetes API进行交互。每次执行`kubectl`时，它都会通过API服务器与集群进行通信。这种通信是通过AWS弹性负载均衡器（ELB）建立的。让我们快速看一下它。
- en: '[PRE68]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE69]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Judging from the `Listener` section, we can see that only port `443` is opened,
    thus allowing only SSL requests. The three instances belong to managers. We can
    safely assume that this load balancer is used only for the access to Kubernetes
    API. In other words, we are still missing access to worker nodes through which
    we'll be able to communicate with our applications. We'll come back to this issue
    in a moment.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Listener`部分可以看出，只有`443`端口是开放的，因此只允许SSL请求。这三个实例属于管理节点。我们可以放心地假设，这个负载均衡器仅用于访问Kubernetes
    API。换句话说，我们仍然缺少通过工作节点访问的方式，通过它我们可以与应用程序进行通信。稍后我们会回到这个问题。
- en: The entry that matters, from user's perspective, is `DNSName`. That is the address
    we need to use if we want to communicate with Kubernetes' API Server. Load Balancer
    is there to ensure that we have a fixed address and that requests will be forwarded
    to one of the healthy masters.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对用户来说，重要的条目是`DNSName`。这是我们需要使用的地址，如果我们想要与Kubernetes的API服务器通信的话。负载均衡器的作用是确保我们拥有一个固定地址，并且请求会被转发到一个健康的主节点。
- en: Finally, the name of the load balancer is `api-devops23-k8s-local-ivnbim`. It
    is important that you remember that it starts with `api-devops23`. You'll see
    soon why the name matters.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，负载均衡器的名称是`api-devops23-k8s-local-ivnbim`。记住它以`api-devops23`开头非常重要。你很快就会明白为什么这个名称很重要。
- en: 'We can confirm that the `DNSName` is indeed the door to the API by examining
    `kubectl` configuration:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查`kubectl`配置来确认`DNSName`确实是通向API的入口：
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE71]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We can see that the `devops23.k8s.local` is set to use `amazonaws.com` subdomain
    as the server address and that it is the current context. That is the DNS of the
    ELB.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`devops23.k8s.local`被设置为使用`amazonaws.com`子域作为服务器地址，并且它是当前的上下文。这是ELB的DNS。
- en: '![](img/df4178fe-c7a7-4479-927a-9e9f757388d1.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df4178fe-c7a7-4479-927a-9e9f757388d1.png)'
- en: 'Figure 14-7: Load balancer behind Kubernetes API Server'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-7：Kubernetes API 服务器后的负载均衡器
- en: The fact that we can access the API does not get us much closer to having a
    way to access applications we are soon to deploy. We already learned that we can
    use Ingress to channel requests to a set of ports (usually `80` and `443`). However,
    even if we deploy Ingress, we still need an entry point to the worker nodes. We
    need another load balancer sitting above the nodes.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够访问API，但这并没有让我们更接近能够访问我们即将部署的应用程序。我们已经了解到，我们可以使用Ingress将请求引导到一组端口（通常是`80`和`443`）。然而，即使我们部署了Ingress，我们仍然需要一个进入工作节点的入口点。我们需要另一个位于节点之上的负载均衡器。
- en: Fortunately, kops has a solution.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，kops有一个解决方案。
- en: We can use kops' add-ons to deploy additional core services. You can get the
    list of those currently available by exploring directories in [https://github.com/kubernetes/kops/tree/master/addons](https://github.com/kubernetes/kops/tree/master/addons).
    Even though most of them are useful, we'll focus only on the task at hand.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 kops 的插件来部署额外的核心服务。你可以通过浏览 [https://github.com/kubernetes/kops/tree/master/addons](https://github.com/kubernetes/kops/tree/master/addons)
    中的目录，获取当前可用插件的列表。尽管大多数插件都很有用，但我们只关注当前任务。
- en: Add-ons are, in most cases, Kubernetes resources defined in a YAML file. All
    we have to do is pick the addon we want, choose the version we prefer, and execute
    `kubectl create`. We'll create the resources defined in `ingress-nginx` version
    `v1.6.0`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 插件通常是 Kubernetes 资源，定义在 YAML 文件中。我们所需要做的就是选择想要的插件，选择我们偏好的版本，并执行`kubectl create`。我们将创建在`ingress-nginx`版本`v1.6.0`中定义的资源。
- en: We won't go into details behind the definition YAML file we are about to use
    to create the resources kops assembled for us. I'll leave that up to you. Instead,
    we'll proceed with `kubectl create`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨我们即将使用的定义 YAML 文件背后的细节，这个文件用于创建 kops 为我们组装的资源。我将把这部分留给你自己。相反，我们将继续使用`kubectl
    create`。
- en: '[PRE72]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output is as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE73]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We can see that quite a few resources were created in the Namespace `kube-ingress`.
    Let's take a look what's inside.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在命名空间`kube-ingress`中创建了相当多的资源。让我们来看看里面有什么。
- en: '[PRE74]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE75]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We can see that it created two deployments, which created two `ReplicaSets`,
    which created Pods. In addition, we got two Services as well. As a result, Ingress
    is running inside our cluster and are a step closer to being able to test it.
    Still, we need to figure out how to access the cluster.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，它创建了两个部署，这些部署创建了两个`ReplicaSets`，进而创建了 Pods。此外，我们还得到了两个服务。因此，Ingress 正在我们的集群中运行，距离能够测试它又近了一步。尽管如此，我们仍然需要弄清楚如何访问集群。
- en: One of the two Services (`ingress-nginx`) is `LoadBalancer`. We did not explore
    that type when we discussed Services.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 两个服务中的一个（`ingress-nginx`）是`LoadBalancer`类型。我们在讨论服务时并未探索这种类型。
- en: '`LoadBalancer` Service type exposes the service externally using a cloud provider''s
    load balancer. `NodePort` and `ClusterIP` services, to which the external load
    balancer will route, are automatically created. Ingress is "intelligent" enough
    to know how to create and configure an AWS ELB. All it needed is an annotation
    `service.beta.kubernetes.io/aws-load-balancer-proxy-protocol` (defined in the
    YAML file).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer`服务类型通过云提供商的负载均衡器将服务暴露到外部。`NodePort`和`ClusterIP`服务会自动创建，外部负载均衡器将路由到这些服务。Ingress
    是“智能的”，知道如何创建和配置 AWS ELB。它所需要的只是一个注解`service.beta.kubernetes.io/aws-load-balancer-proxy-protocol`（在
    YAML 文件中定义）。'
- en: You'll notice that the `ingress-nginx` Service published port `30107` and mapped
    it to `80`. `30430` was mapped to `443`. This means that, from inside the cluster,
    we should be able to send HTTP requests to `30107` and HTTPS to `30430`. However,
    that is only part of the story. Since the Service is the `LoadBalancer` type,
    we should expect some changes to AWS **Elastic Load Balancers** (**ELBs**) as
    well.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，`ingress-nginx`服务发布了端口`30107`并将其映射到`80`。`30430`被映射到`443`。这意味着，在集群内部，我们应该能够向`30107`发送
    HTTP 请求，向`30430`发送 HTTPS 请求。然而，这只是部分情况。由于该服务是`LoadBalancer`类型，我们也应该期望 AWS **弹性负载均衡器**（**ELBs**）发生一些变化。
- en: Let's check the state of the load balancers in our cluster.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下集群中负载均衡器的状态。
- en: '[PRE76]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（只显示相关部分）如下：
- en: '[PRE77]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We can observe from the output that a new load balancer was added.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，新增了一个负载均衡器。
- en: The new load balancer publishes port `80` (HTTP) and maps it to `30107`. This
    port is the same as the `ingress-nginx` Service published. Similarly, the LB published
    port `443` (HTTPS) and mapped it to `30430`. From the `Instances` section, we
    can see that it currently maps to the two worker nodes.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 新的负载均衡器发布了端口`80`（HTTP），并将其映射到`30107`。这个端口与`ingress-nginx`服务发布的端口相同。类似地，LB 发布了端口`443`（HTTPS），并将其映射到`30430`。从`Instances`部分我们可以看到，它当前映射到两个工作节点。
- en: Further down, we can see the `DNSName`. We should retrieve it but, unfortunately,
    `LoadBalancerName` does not follow any format. However, we do know that now there
    are two load balancers and that the one dedicated to masters has a name that starts
    with `api-devops23`. So, we can retrieve the other LB by specifying that it should
    not contain that prefix. We'll use `jq` instruction `not` for that.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以看到 `DNSName`。我们应该获取它，但不幸的是，`LoadBalancerName` 并不遵循任何特定格式。然而，我们知道现在有两个负载均衡器，其中专用于主节点的负载均衡器的名称以
    `api-devops23` 开头。因此，我们可以通过指定名称中不包含该前缀来获取另一个负载均衡器。我们将使用 `jq` 指令中的 `not` 来实现这一点。
- en: 'The command that retrieves DNS from the new load balancer is as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从新负载均衡器获取 DNS 的命令：
- en: '[PRE78]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We'll come back to the newly created Ingress and the load balancer soon. For
    now, we'll move on and deploy the `go-demo-2` application.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快会回到新创建的 Ingress 和负载均衡器。现在，我们继续部署 `go-demo-2` 应用程序。
- en: Deploying applications
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署应用程序
- en: Deploying resources to a Kubernetes cluster running in AWS is no different from
    deployments anywhere else, including Minikube. That's one of the big advantages
    of Kubernetes, or of any other container scheduler. We have a layer of abstraction
    between hosting providers and our applications. As a result, we can deploy (almost)
    any YAML definition to any Kubernetes cluster, no matter where it is. That's huge.
    It gives up a very high level of freedom and allows us to avoid vendor locking.
    Sure, we cannot effortlessly switch from one scheduler to another, meaning that
    we are "locked" into the scheduler we chose. Still, it's better to depend on an
    open source project than on a commercial hosting vendor like AWS, GCE, or Azure.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 将资源部署到 AWS 上运行的 Kubernetes 集群与在其他任何地方的部署没有区别，包括 Minikube。这就是 Kubernetes 的一个重要优势，或者说是任何其他容器调度器的优势。我们在托管提供商和我们的应用程序之间有一层抽象。因此，我们可以将几乎任何
    YAML 定义部署到任何 Kubernetes 集群，无论它位于何处。这非常重要。它为我们提供了很高的自由度，避免了供应商锁定。当然，我们不能轻松地从一个调度器切换到另一个调度器，这意味着我们被“锁定”在了我们选择的调度器中。尽管如此，依赖开源项目总比依赖
    AWS、GCE 或 Azure 等商业托管供应商要好。
- en: We need to spend time setting up a Kubernetes cluster, and the steps will differ
    from one hosting provider to another. However, once a cluster is up-and-running,
    we can create any Kubernetes resource (almost) entirely ignoring what's underneath
    it. The result is the same no matter whether our cluster is AWS, GCE, Azure, on-prem,
    or anywhere else.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要花时间设置 Kubernetes 集群，而不同的托管提供商步骤会有所不同。然而，一旦集群搭建完成，我们可以创建几乎任何 Kubernetes 资源，完全忽略其底层的具体实现。无论我们的集群是在
    AWS、GCE、Azure、私有云还是其他地方，结果都是一样的。
- en: 'Let''s get back to the task at hand and create `go-demo-2` resources:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到当前任务，创建 `go-demo-2` 资源：
- en: '[PRE79]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We moved back to the repository''s root directory, and created the resources
    defined in `aws/go-demo-2.yml`. The output is as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到仓库的根目录，创建了在 `aws/go-demo-2.yml` 中定义的资源。输出结果如下：
- en: '[PRE80]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Next, we should wait until `go-demo-2-api` Deployment is rolled out.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该等待 `go-demo-2-api` 部署完成。
- en: '[PRE81]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The output is as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE82]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Finally, we can validate that the application is running and is accessible
    through the DNS provided by the AWS **Elastic Load Balancer** (**ELB**):'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以验证应用程序是否正在运行，并且可以通过 AWS **弹性负载均衡器**（**ELB**）提供的 DNS 进行访问：
- en: '[PRE83]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We got response code `200` and the message `hello, world!`. The Kubernetes cluster
    we set up in AWS works!
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了响应码 `200` 和消息 `hello, world!`。我们在 AWS 上设置的 Kubernetes 集群运行正常！
- en: When we sent the request to the ELB dedicated to workers, it performed round-robin
    and forwarded it to one of the healthy nodes. Once inside the worker, the request
    was picked by the `nginx` Service, forwarded to Ingress, and, from there, to one
    of the containers that form the replicas of the `go-demo-2-api` ReplicaSet.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向专用于工作节点的 ELB 发送请求时，它执行了轮询并将请求转发到一个健康的节点。一旦进入工作节点，请求被 `nginx` 服务接收，转发到 Ingress，然后再转发到构成
    `go-demo-2-api` ReplicaSet 副本的某个容器。
- en: '![](img/93b6e8a9-7415-46fe-b8c6-3ca283045100.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93b6e8a9-7415-46fe-b8c6-3ca283045100.png)'
- en: 'Figure 14-8: Load balancer behind Kubernetes worker nodes'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-8：Kubernetes 工作节点背后的负载均衡器
- en: It might be worth pointing out that containers that form our applications are
    always running in worker nodes. Master servers, on the other hand, are entirely
    dedicated to running Kubernetes system. That does not mean that we couldn't create
    a cluster in the way that masters and workers are combined into the same servers,
    just as we did with Minikube. However, that is risky, and we're better off separating
    the two types of nodes. Masters are more reliable when they are running on dedicated
    servers. Kops knows that, and it does not even allow us to mix the two.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 值得指出的是，构成我们应用程序的容器总是运行在worker节点上。另一方面，master服务器完全专用于运行Kubernetes系统。并不意味着我们不能像在Minikube中那样将master和worker组合在同一台服务器上创建集群。然而，这是有风险的，最好将两种类型的节点分开。Master节点在专用服务器上运行时更可靠。Kops知道这一点，甚至不允许我们将两者混合。
- en: Exploring high-availability and fault-tolerance
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索高可用性和容错性
- en: The cluster would not be reliable if it wouldn't be fault tolerant. Kops intents
    to make it so, but we're going to validate that anyways.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集群没有容错能力，它将不可靠。Kops的目的是使其具备容错能力，但我们还是要验证一下。
- en: 'Let''s retrieve the list of worker node instances:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取worker节点实例的列表：
- en: '[PRE84]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We used `aws ec2 describe-instances` to retrieve all the instances (five in
    total). The output was sent to `jq`, which filtered them by the security group
    dedicated to worker nodes.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`aws ec2 describe-instances`来获取所有实例（共五个）。输出被发送到`jq`，由它按专门用于worker节点的安全组进行筛选。
- en: 'The output is as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE85]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: We'll terminate one of the worker nodes. To do that, we'll pick a random one,
    and retrieve its ID.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将终止其中一个worker节点。为了做到这一点，我们将随机选择一个，并获取它的ID。
- en: '[PRE86]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: We used the same command as before and added `tail -n 1`, so that the output
    is limited to a single line (entry). We stored the result in the `INSTANCE_ID`
    variable. Now we know which instance to terminate.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了和之前相同的命令，并添加了`tail -n 1`，这样输出就限制为一行（条目）。我们将结果存储在`INSTANCE_ID`变量中。现在我们知道要终止哪个实例。
- en: '[PRE87]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The output is as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE88]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: We can see from the output that the instance is shutting down. We can confirm
    that by listing all the instances from the security group `nodes.devops23.k8s.local`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到该实例正在关闭。我们可以通过列出来自安全组`nodes.devops23.k8s.local`的所有实例来确认这一点。
- en: '[PRE89]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The output is as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE90]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: As expected, we are now running only one instance. All that's left is to wait
    for a minute, and repeat the same command.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，现在我们只运行了一个实例。剩下的就是等待一分钟，然后重复相同的命令。
- en: '[PRE91]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The output is as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE92]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: This time, we can see that there are again two instances. The only difference
    is that this time one of the instance IDs is different.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们可以看到又有了两个实例。唯一的不同是，这一次其中一个实例ID不同。
- en: AWS auto-scaling group discovered that the instances do not match the desired
    number, and it created a new one.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: AWS自动扩展组发现实例与期望的数量不符，于是创建了一个新实例。
- en: 'The fact that AWS created a node to replace the one we terminated does not
    mean that the new server joined the Kubernetes cluster. Let''s verify that:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: AWS创建了一个新节点来替换我们终止的节点，这并不意味着新服务器已加入Kubernetes集群。让我们验证一下：
- en: '[PRE93]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output is as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE94]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: If you were fast enough, your output should also show that there is only one
    (worker) `node`. Once AWS created a new server, it takes a bit of time until Docker,
    Kubelet, and Protokube are installed, containers are pulled and run, and the node
    is registered through one of the masters.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你足够快，你的输出应该也会显示只有一个(worker) `node`。一旦AWS创建了一个新服务器，直到Docker、Kubelet和Protokube安装完成，容器拉取并运行，并通过其中一个master注册节点，还需要一点时间。
- en: Let's try it again.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再试一次。
- en: '[PRE95]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The output is as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE96]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: This time, the number of (worker) nodes is back to two. Our cluster is back
    in the desired state.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，(worker)节点的数量回到了两个。我们的集群恢复到了期望的状态。
- en: What we just experienced is, basically, the same as when we executed the rolling
    upgrade. The only difference is that we terminated an instance as a way to simulate
    a failure. During the upgrade process, kops does the same. It shuts down one instance
    at a time and waits until the cluster goes back to the desired state.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才经历的，基本上和执行滚动升级时一样。唯一的区别是我们终止了一个实例，以模拟故障。在升级过程中，kops也会做同样的事情。它一次关闭一个实例，并等待直到集群恢复到期望的状态。
- en: Feel free to do a similar test with master nodes. The only difference is that
    you'll have to use `masters` instead of `nodes` as the prefix of the security
    group name. Since everything else is the same, I trust you won't need instructions
    and explanations.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以对主节点进行类似的测试。唯一的区别是，你需要使用`masters`而不是`nodes`作为安全组名称的前缀。由于其他一切都相同，我相信你不需要更多的指令和解释。
- en: Giving others access to the cluster
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 给予他人访问集群的权限
- en: 'Unless you''re planning to be the only person in your organization with the
    access to the cluster, you''ll need to create a `kubectl` configuration that you
    can distribute to your coworkers. Let''s see the steps:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你打算成为组织中唯一拥有集群访问权限的人，否则你需要创建一个`kubectl`配置，分发给你的同事。让我们来看看步骤：
- en: '[PRE97]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'We went back to the `cluster` directory, created the sub-directory `config`,
    and exported `KUBECONFIG` variable with the path to the file where we''d like
    to store the configuration. Now we can execute `kops export`:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到`cluster`目录，创建了`config`子目录，并导出了`KUBECONFIG`变量，指向我们希望存储配置文件的路径。现在，我们可以执行`kops
    export`：
- en: '[PRE98]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The output of the latter command is as follows:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出如下：
- en: '[PRE99]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Now you can pass that configuration to one of your coworkers, and he'll have
    the same access as you.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将这个配置文件交给你的同事，他将拥有和你一样的访问权限。
- en: Truth be told, you should create a new user and a password or, even better,
    an SSH key and let each user in your organization access the cluster with their
    own authentication. You should also create RBAC permissions for each user or a
    group of users. We won't go into the steps how to do that since they are already
    explained in the [Chapter 12](36d9d538-dc85-4366-bd95-72076b27cb28.xhtml), *Securing
    Kubernetes Clusters*.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，你应该创建一个新用户和密码，或者更好的是，创建一个SSH密钥，并让你组织中的每个用户使用自己的认证来访问集群。你还应该为每个用户或一组用户创建RBAC权限。我们不会详细讲解这些步骤，因为它们已经在[第12章](36d9d538-dc85-4366-bd95-72076b27cb28.xhtml)《保护Kubernetes集群》中解释过了，*Securing
    Kubernetes Clusters*。
- en: Destroying the cluster
  id: totrans-422
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 销毁集群
- en: The chapter is almost finished, and we do not need the cluster anymore. We want
    to destroy it as soon as possible. There's no good reason to keep it running when
    we're not using it. But, before we proceed with the destructive actions, we'll
    create a file that will hold all the environment variables we used in this chapter.
    That will help us the next time we want to recreate the cluster.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 本章即将结束，我们不再需要集群。我们希望尽快销毁它。没有什么理由在不使用时让它继续运行。但在进行破坏性操作之前，我们将创建一个文件，存储本章使用的所有环境变量。这样下次我们想重建集群时会更加方便。
- en: '[PRE100]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We echoed the variables with the values into the `kops` file, and now we can
    delete the cluster:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将变量及其值写入了`kops`文件，现在我们可以删除集群：
- en: '[PRE101]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The output is as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE102]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Kops removed references of the cluster from our `kubectl` configuration and
    proceeded to delete all the AWS resources it created. Our cluster is no more.
    We can proceed and delete the S3 bucket as well.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Kops从我们的`kubectl`配置中删除了集群的引用，并开始删除它所创建的所有AWS资源。我们的集群不复存在。我们可以继续并删除S3存储桶。
- en: '[PRE103]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We will not remove the IAM resources (group, user, access key, and policies).
    It does not cost to keep them in AWS, and we'll save ourselves from re-running
    the commands that create them. However, I will list the commands as a reference.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会删除IAM资源（组、用户、访问密钥和策略）。在AWS中保留它们不会产生费用，我们也能避免重新执行创建它们的命令。然而，我会列出这些命令作为参考。
- en: Do NOT execute following commands. They are only a reference. We'll need those
    resources in the next chapter.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 请勿执行以下命令。它们仅作为参考。我们将在下一章使用这些资源。
- en: '[PRE104]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: What now?
  id: totrans-434
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: We have a production-ready Kubernetes cluster running in AWS. Isn't that something
    worthwhile a celebration?
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在AWS中运行着一个生产就绪的Kubernetes集群。这难道不是值得庆祝的事情吗？
- en: Kops proved to be relatively easy to use. We executed more `aws` than `kops`
    commands. If we exclude them, the whole cluster can be created with a single `kops`
    command. We can easily add or remove worker nodes. Upgrades are simple and reliable,
    if a bit long. The important part is that through rolling upgrades we can avoid
    downtime.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: Kops证明了它相对易于使用。我们执行的`aws`命令比`kops`命令更多。如果不考虑这些命令，整个集群可以通过单个`kops`命令创建。我们可以轻松地添加或删除工作节点。升级过程简单且可靠，虽然有些漫长。关键是，通过滚动升级，我们可以避免停机。
- en: There are a few `kops` command we did not explore. I feel that now you know
    the important parts and that you will be able to figure out the rest through the
    documentation.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些`kops`命令我们没有探讨。我觉得现在你已经掌握了重要部分，并且能够通过文档弄清楚剩下的内容。
- en: You might be inclined to think that you are ready to apply everything you learned
    so far. Do not open that champagne bottle you've been saving for special occasions.
    There's still one significant topic we need to explore. We postponed the discussion
    about stateful services since we did not have the ability to use external drives.
    We did use volumes, but they were all local, and do not qualify as persistent.
    Failure of a single server would prove that. Now that we are running a cluster
    in AWS, we can explore how to deploy stateful applications.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得自己已经准备好应用到目前为止所学的一切。别急着打开那瓶你为特殊场合准备的香槟。还有一个重要的话题我们需要探索。我们推迟了关于有状态服务的讨论，因为当时我们没有使用外部存储的能力。我们确实使用了卷，但它们都是本地卷，不具备持久性。单个服务器的故障就能证明这一点。现在我们在AWS上运行集群，我们可以探讨如何部署有状态应用。
- en: Kubernetes operations (kops) compared to Docker for AWS
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes操作（kops）与Docker for AWS的比较
- en: Docker for AWS (D4AWS) quickly became the preferable way to create a Docker
    Swarm cluster in AWS (and Azure). Similarly, kops is the most commonly used tool
    to create Kubernetes clusters in AWS. At least, at the time of this writing.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: Docker for AWS (D4AWS)很快成为了在AWS（和Azure）上创建Docker Swarm集群的首选方式。同样，kops是创建Kubernetes集群的最常用工具，至少在本文写作时是如此。
- en: The result, with both tools, is more or less the same. Both create Security
    Groups, VPCs, Auto-Scaling Groups, Elastic Load Balancers, and everything else
    a cluster needs. In both cases, Auto-Scaling Groups are in charge of creating
    EC2 instances. Both rely on external storage to keep the state of the cluster
    (kops in S3 and D4AWS in DynamoDB). In both cases, EC2 instances brought to life
    by Auto-Scaling Groups know how to run system-level services and join the cluster.
    If we exclude the fact that one solution runs Docker Swarm and that the other
    uses Kubernetes, there is no significant functional difference if we observe only
    the result (the cluster). So, we'll focus on user experience instead.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个工具得到的结果大致相同。它们都会创建安全组、VPC、自动伸缩组、弹性负载均衡器以及集群所需的其他所有组件。在这两种情况下，自动伸缩组负责创建EC2实例。两者都依赖外部存储来保持集群的状态（kops使用S3，D4AWS使用DynamoDB）。在这两种情况下，由自动伸缩组创建的EC2实例知道如何运行系统级服务并加入集群。如果我们排除一个方案使用Docker
    Swarm而另一个使用Kubernetes的事实，仅观察结果（即集群），就没有什么显著的功能差异。因此，我们将重点关注用户体验。
- en: Both tools can be executed from the command line and that's where we can spot
    the first difference.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 两个工具都可以通过命令行执行，这就是我们能察觉到的第一个区别。
- en: Docker for AWS relies on CloudFormation templates, so we need to execute `aws
    cloudformation` command. Docker provides a template and we should use parameters
    to customize it. In my humble opinion, the way CloudFormation expects us to pass
    parameters is just silly.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Docker for AWS依赖于CloudFormation模板，因此我们需要执行`aws cloudformation`命令。Docker提供了一个模板，我们应使用参数来定制它。在我看来，CloudFormation要求我们传递参数的方式实在是有些愚蠢。
- en: 'Let''s take a look at an example:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子：
- en: '[PRE105]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Having to write something like `ParameterKey=ManagerSize,ParameterValue=3` instead
    of `ManagerSize=3` is annoying at best.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 写出像`ParameterKey=ManagerSize,ParameterValue=3`这样的内容，而不是`ManagerSize=3`，无疑是让人烦恼的。
- en: 'A sample command that creates Kubernetes cluster using `kops` is as follows:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kops`创建Kubernetes集群的示例命令如下：
- en: '[PRE106]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Isn't that easier and more intuitive?
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不是更简单更直观？
- en: Moreover, kops is a binary with everything we would expect. We can, for example,
    execute `kops --help` and see the available options and a few examples. If we'd
    like to know which parameters are available with Docker For AWS, we'd need to
    go through the template. That's definitely less intuitive and more difficult than
    running `kops create cluster --help`. Even if we don't mind browsing through the
    Docker For AWS template, we still don't have examples at hand (from the command
    line, not browser). From user experience perspective, kops wins over Docker For
    AWS if we restrict the comparison only to command line interface. Simply put,
    executing a well-defined binary dedicated to managing a cluster is better than
    executing `aws cloudformation` commands with remote templates.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，kops 是一个包含我们所需要的一切的二进制文件。例如，我们可以执行 `kops --help` 来查看可用选项和一些示例。如果我们想知道 Docker
    For AWS 可以使用哪些参数，我们需要浏览模板。这显然没有运行 `kops create cluster --help` 那样直观和简单。即使我们不介意查看
    Docker For AWS 模板，我们仍然没有命令行上的示例（而不是浏览器中的示例）。从用户体验的角度来看，如果我们仅限于命令行接口，kops 胜过 Docker
    For AWS。简单来说，执行一个专门用于管理集群的明确定义的二进制文件比执行带有远程模板的 `aws cloudformation` 命令要好。
- en: Did Docker make a mistake for choosing CloudFormation? I don't think so. Even
    if command line experience is suboptimal, it is apparent that they wanted to provide
    an experience native to hosting vendor. In our case that's AWS, but the same can
    be said for Azure. If you will always operate cluster from the command line (as
    I think you should), this is where the story ends and kops is a winner with a
    very narrow margin.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 选择 CloudFormation 是不是一个错误？我不这么认为。即使命令行体验不尽如人意，显然他们希望提供一种与托管供应商原生兼容的体验。在我们的案例中，这是
    AWS，但对于 Azure 也可以说是同样的道理。如果你总是通过命令行操作集群（就像我认为你应该做的那样），那么这个故事就到此为止，kops 以微弱优势获胜。
- en: The fact that we can create Docker For AWS cluster using CloudFormation means
    that we can take advantage of it from AWS Console. That translates into UI experience.
    We can use AWS Console UI to create, update, or delete a cluster. We can see the
    events as they progress, explore the resources that were created, roll back to
    the previous version, and so on. By choosing CloudFormation template, Docker decided
    to provide not only command line but also a visual experience.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够使用 CloudFormation 创建 Docker For AWS 集群，意味着我们可以通过 AWS 控制台来利用它。这就转化为 UI 体验。我们可以使用
    AWS 控制台的 UI 来创建、更新或删除集群。我们可以查看事件的进展，探索创建的资源，回滚到先前的版本，等等。通过选择 CloudFormation 模板，Docker
    决定不仅提供命令行，还提供了可视化体验。
- en: Personally, I think that UIs are evil and that we should do everything from
    the command line. That being said, I'm fully aware that not everybody feels the
    same. Even if you do choose never to use UI for "real" work, it is very helpful,
    at least at the beginning, as a learning experience of what can and what cannot
    be done, and how all the steps tie together.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，我认为用户界面是邪恶的，我们应该通过命令行来完成所有操作。话虽如此，我完全理解并不是每个人都有相同的看法。即使你决定永远不使用 UI 进行“实际”工作，至少在开始时，UI
    仍然非常有帮助，它可以作为一种学习体验，让你了解可以做什么，不能做什么，以及所有步骤如何关联在一起。
- en: '![](img/dc704627-7611-4e26-81cf-b8efabc6ceb8.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc704627-7611-4e26-81cf-b8efabc6ceb8.png)'
- en: 'Figure 14-9: Docker For AWS UI'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-9：Docker For AWS 用户界面
- en: It's a tough call. What matters is that both tools are creating reliable clusters.
    Kops is more user-friendly from the command line, but it has no UI. Docker For
    AWS, on the other hand, works as native AWS solution through CloudFormation. That
    gives it the UI, but at the cost of suboptimal command line experience.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个艰难的抉择。重要的是，这两个工具都能创建可靠的集群。kops 在命令行方面更具用户友好性，但没有 UI。另一方面，Docker For AWS
    通过 CloudFormation 作为原生 AWS 解决方案工作。这样它就拥有了 UI，但也牺牲了命令行体验的最佳效果。
- en: You won't have to choose one over the other since the choice will not depend
    on which one you like more, but whether you want to use Docker Swarm or Kubernetes.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 你不必在两者之间做出选择，因为这个选择不取决于你更喜欢哪个，而是取决于你是否想使用 Docker Swarm 或 Kubernetes。
