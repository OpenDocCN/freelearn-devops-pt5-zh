- en: Zero-Downtime Deployments and Secrets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署与密钥
- en: In the previous chapter, we explored Docker Swarm and its resources in detail.
    We learned how to build a highly available swarm locally and in the cloud. Then,
    we discussed Swarm services and stacks in depth. Finally, we created services
    and stacks in the swarm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们详细探讨了 Docker Swarm 及其资源。我们学习了如何在本地和云中构建一个高可用的 Swarm 集群。接着，我们深入讨论了 Swarm
    服务和堆栈。最后，我们在 Swarm 中创建了服务和堆栈。
- en: In this chapter, we will show you how we can update services and stacks running
    in Docker Swarm without interrupting their availability. This is called zero-downtime
    deployment. We are also going to introduce swarm secrets as a means to securely
    provide sensitive information to containers of a service using those secrets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将展示如何在不中断服务可用性的情况下，更新运行在 Docker Swarm 中的服务和堆栈。这被称为零停机部署。我们还将介绍 Swarm 密钥，作为一种安全地将敏感信息提供给使用这些密钥的服务容器的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Zero-downtime deployment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零停机部署
- en: Storing configuration data in the swarm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Swarm 中存储配置数据
- en: Protecting sensitive data with Docker Secrets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 密钥保护敏感数据
- en: 'After finishing this chapter, you will be able to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将能够完成以下任务：
- en: List two to three different deployment strategies commonly used to update a
    service without downtime.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出两到三种常用的部署策略，用于在不中断服务的情况下更新服务。
- en: Update a service in batches without causing a service interruption.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以批次更新服务而不导致服务中断。
- en: Define a rollback strategy for a service that is used if an update fails.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义用于服务回滚的策略，以防更新失败。
- en: Store non-sensitive configuration data using Docker configs.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 配置存储非敏感的配置数据。
- en: Use a Docker secret with a service.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 密钥与服务配合。
- en: Update the value of a secret without causing downtime.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新密钥的值而不导致停机。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code files for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition).
    If you have checked out the repository as indicated in [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml),
    *Setting up a Working Environment*, then you'll find the code at `~/fod-solution/ch14`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到，网址为[https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition)。如果你已经按照[第2章](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml)中提到的步骤克隆了仓库，*设置工作环境*，那么你将会在`~/fod-solution/ch14`找到代码。
- en: Zero-downtime deployment
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署
- en: One of the most important aspects of a mission-critical application that needs
    frequent updates is the ability to do updates in a fashion that requires no outage
    at all. We call this a zero-downtime deployment. At all times, the application
    that is updated must be fully operational.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要频繁更新的关键任务应用程序最重要的一个方面是能够以零停机的方式进行更新。我们称之为零停机部署。在整个过程中，更新的应用程序必须始终保持完全可用。
- en: Popular deployment strategies
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的部署策略
- en: 'There are various ways to achieve this. Some of them are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以实现这一点，以下是其中一些方法：
- en: Rolling updates
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新
- en: Blue-green deployments
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Canary releases
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Docker Swarm supports rolling updates out of the box. The other two types of
    deployments can be achieved with some extra effort from our side.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 开箱即支持滚动更新。其他两种类型的部署需要我们付出额外的努力来实现。
- en: Rolling updates
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动更新
- en: In a mission-critical application, each application service has to run in multiple
    replicas. Depending on the load, that can be as few as two to three instances
    and as many as dozens, hundreds, or thousands of instances. At any given time,
    we want to have a clear majority when it comes to all the service instances running.
    So, if we have three replicas, we want to have at least two of them up and running
    at all times. If we have 100 replicas, we can be content with a minimum of, say,
    90 replicas, being available. By doing this, we can define a batch size of replicas
    that we may take down to upgrade. In the first case, the batch size would be 1
    and in the second case, it would be 10.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个关键任务的应用中，每个应用服务都必须以多个副本运行。根据负载，副本数量可能从两三个实例少到几十个、上百个或成千上万个实例。任何时候，我们都希望确保所有服务实例中有明显的多数在运行。因此，如果我们有三个副本，我们希望至少有两个副本始终在运行。如果有100个副本，我们也能接受至少有90个副本在可用状态。通过这样做，我们可以定义一个副本的批量大小，用来进行升级时的停机。在第一个例子中，批量大小是1，而在第二个例子中，批量大小是10。
- en: 'When we take replicas down, Docker Swarm will automatically take those instances
    out of the load balancing pool and all traffic will be load balanced across the
    remaining active instances. Those remaining instances will thus experience a slight
    increase in traffic. In the following diagram, prior to the start of the rolling
    update, if **Task A3** wanted to access **Service B**, it could have been load
    balanced to any of the three tasks of **Service B** by SwarmKit. Once the rolling
    update started, SwarmKit took down **Task B1** for updates. Automatically, this
    task is then taken out of the pool of targets. So, if **Task A3** now requests
    to connect to **Service B**, load balancing will only select from the remaining
    tasks, that is, **B2** and **B3**. Thus, those two tasks might experience a higher
    load temporarily:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们停机副本时，Docker Swarm会自动将这些实例从负载均衡池中移除，所有流量将会重新在剩下的活跃实例中进行负载均衡。因此，这些剩余的实例将会经历流量的轻微增加。在下图中，在滚动更新开始之前，如果**任务
    A3**想要访问**服务 B**，它可能会被SwarmKit负载均衡到**服务 B**的任意三个任务中的一个。一旦滚动更新开始，SwarmKit会停止**任务
    B1**进行更新。此时，这个任务会自动从目标池中移除。所以，如果**任务 A3**现在请求连接**服务 B**，负载均衡只会从剩余的任务中选择，也就是**B2**和**B3**。因此，这两个任务可能会暂时经历更高的负载：
- en: '![](img/b5692dbe-f8b2-4050-bc4b-04147a063825.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5692dbe-f8b2-4050-bc4b-04147a063825.png)'
- en: Task B1 is taken down to be updated
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 任务 B1 被停机更新
- en: The stopped instances are then replaced by an equivalent number of new instances
    of the new version of the application service. Once the new instances are up and
    running, we can have the Swarm observe them for a given period of time and make
    sure they're healthy. If all is well, then we can continue by taking down the
    next batch of instances and replacing them with instances of the new version.
    This process is repeated until all the instances of the application service have
    been replaced.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 被停止的实例将被等量的新实例替换，这些新实例是应用服务的新版本。一旦新实例启动并运行，我们可以让Swarm监控它们一段时间，并确保它们健康。如果一切正常，我们就可以继续进行，停掉下一批实例，并用新版本的实例替换它们。这个过程会重复，直到所有应用服务的实例都被替换。
- en: 'In the following diagram, we can see that **Task B1** of **Service B** has
    been updated to version 2\. The container of **Task B1** was assigned a new **IP**
    address, and it was deployed to another worker node with free resources:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到**服务 B**的**任务 B1**已更新至版本 2。**任务 B1**的容器被分配了新的**IP**地址，并且部署到了具有空闲资源的另一工作节点：
- en: '![](img/2e0094c4-5dce-4763-8401-394a87cc79b3.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e0094c4-5dce-4763-8401-394a87cc79b3.png)'
- en: The first batch being updated in a rolling update
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新中，第一批更新的副本
- en: It is important to understand that when the task of a service is updated, in
    most cases, it gets deployed to a different worker node than the one it used to
    live on. But that should be fine as long as the corresponding service is stateless.
    If we have a stateful service that is location- or node-aware and we'd like to
    update it, then we have to adjust our approach, but this is outside of the scope
    of this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，当一个服务的任务更新时，在大多数情况下，它会被部署到不同于原来所在的工作节点。但只要对应的服务是无状态的，这应该没问题。如果我们有一个有状态的服务，它依赖于位置或节点，并且我们想要更新它，那么我们必须调整方法，但这超出了本书的范围。
- en: 'Now, let''s look at how we can actually instruct the Swarm to perform a rolling
    update of an application service. When we declare a service in a stack file, we
    can define multiple options that are relevant in this context. Let''s look at
    a snippet of a typical stack file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何实际指示 Swarm 执行应用服务的滚动更新。当我们在堆栈文件中声明一个服务时，可以定义多个在这个上下文中相关的选项。让我们看一下典型堆栈文件的片段：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this snippet, we can see a section, `update_config`, with the `parallelism`
    and `delay` properties. `parallelism` defines the batch size of how many replicas
    are going to be updated at a time during a rolling update. `delay` defines how
    long Docker Swarm is going to wait between updating individual batches. In the
    preceding case, we have `10` replicas that are being updated in two instances
    at a time and, between each successful update, Docker Swarm waits for `10` seconds.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，我们可以看到一个部分 `update_config`，其中有 `parallelism` 和 `delay` 属性。`parallelism`
    定义了在滚动更新过程中每次更新多少副本。`delay` 定义了 Docker Swarm 在更新每个批次之间等待的时间。在前述示例中，我们有 `10` 个副本，每次更新两个副本，并且在每次成功更新后，Docker
    Swarm 等待 `10` 秒。
- en: Let's test such a rolling update. Navigate to the `ch14` subfolder of our `labs`
    folder and use the `stack.yaml` file to create a web service that's been configured
    for a rolling update. The service uses an Alpine-based Nginx image whose version
    is `1.12-alpine`. We will update the service to a newer version, that is, `1.13-alpine`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下滚动更新。在 `labs` 文件夹下的 `ch14` 子文件夹中，使用 `stack.yaml` 文件创建一个已配置滚动更新的 web 服务。该服务使用基于
    Alpine 的 Nginx 镜像，版本为 `1.12-alpine`。我们将把服务更新到一个更新的版本，即 `1.13-alpine`。
- en: 'To start, we will deploy this service to our swarm that we created locally
    in VirtualBox. Let''s take a look:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将把这个服务部署到我们在 VirtualBox 中本地创建的 swarm 上。让我们来看一下：
- en: 'First, we need to make sure that we have our Terminal window configured so
    that we can access one of the master nodes of our cluster. Let''s take the leader,
    that is, `node-1`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要确保终端窗口已配置好，可以访问我们集群中的一个主节点。我们以 leader 节点 `node-1` 为例：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can deploy the service using the stack file:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用堆栈文件部署服务：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding command looks like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令的输出如下所示：
- en: '![](img/1f326e7f-883f-4cc7-b643-3844164cc739.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f326e7f-883f-4cc7-b643-3844164cc739.png)'
- en: Deployment of the web stack
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 web 堆栈
- en: 'Once the service has been deployed, we can monitor it using the following command:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务部署完成后，我们可以使用以下命令进行监控：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will see the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/909a831e-a9a3-4ae8-98b1-addeb1ac75a7.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/909a831e-a9a3-4ae8-98b1-addeb1ac75a7.png)'
- en: Service web of the web stack running in Swarm with 10 replicas
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Swarm 中运行的 web 堆栈的 web 服务，包含 10 个副本
- en: If you're working on a macOS machine, you need to make sure your watch tool
    is installed. Use the `brew install watch` command to do so.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 macOS 上工作，需要确保已安装 watch 工具。可以使用 `brew install watch` 命令进行安装。
- en: The previous command will continuously update the output and provide us with
    a good overview of what happens during the rolling update.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将持续更新输出，并为我们提供滚动更新过程中发生的事情的良好概览。
- en: 'Now, we need to open a second Terminal and configure it for remote access for
    the manager node of our swarm. Once we have done that, we can execute the `docker`
    command, which will update the image of the `web` service of the stack, also called
    `web`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要打开第二个终端，并为我们的 swarm 的管理节点配置远程访问。完成这些后，我们可以执行 `docker` 命令，它将更新堆栈中 `web`
    服务的镜像，也叫做 `web`：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding command leads to the following output, indicating the progress
    of the rolling update:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令输出如下，显示了滚动更新的进度：
- en: '![](img/80e17241-6cbe-414b-b393-b874ba9f475a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80e17241-6cbe-414b-b393-b874ba9f475a.png)'
- en: Screen showing the progress of the rolling update
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 显示滚动更新进度的屏幕
- en: The preceding output indicates that the first two batches, each with two tasks,
    have been successful and that the third batch is preparing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前述输出表示，前两批，每批两个任务，已经成功，并且第三批正在准备中。
- en: 'In the first Terminal window, where we''re watching the stack, we should now
    see how Docker Swarm updates the service batch by batch with an interval of `10
    seconds`. After the first batch, it should look like the following screenshot:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个终端窗口中，我们应该能看到 Docker Swarm 如何以 `10` 秒的间隔逐批更新服务。第一批更新后，它应该像下面的截图一样：
- en: '![](img/0c203143-48d9-4eb6-8207-bf098224f2d8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c203143-48d9-4eb6-8207-bf098224f2d8.png)'
- en: Rolling update for a service in Docker Swarm
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 服务的滚动更新
- en: In the preceding screenshot, we can see that the first batch of the two tasks,
    `8` and `9`, has been updated. Docker Swarm is waiting for `10 seconds` to proceed
    with the next batch.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的截图中，我们可以看到第一批两个任务（`8` 和 `9`）已经更新。Docker Swarm正在等待`10秒`后继续执行下一批任务。
- en: It is interesting to note that in this particular case, SwarmKit deploys the
    new version of the task to the same node as the previous version. This is accidental
    since we have five nodes and two tasks on each node. SwarmKit always tries to
    balance the workload evenly across the nodes. So, when SwarmKit takes down a task,
    the corresponding node has a smaller workload than all the others, so the new
    instance is scheduled to it. Normally, you cannot expect to find the new instance
    of a task on the same node. Just try it out yourself by deleting the stack with
    `docker stack rm web` and changing the number of replicas to say, seven, and then
    redeploy and update it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在这个特定的案例中，SwarmKit将任务的新版本部署到与旧版本相同的节点上。这是偶然发生的，因为我们有五个节点，每个节点上有两个任务。SwarmKit总是尽力平衡各个节点的负载。所以，当SwarmKit终止一个任务时，相应的节点的负载会比其他节点小，因此新实例会被调度到该节点上。通常，你不能指望在同一个节点上找到任务的新实例。你可以通过删除堆栈（使用`docker
    stack rm web`）并将副本数改为七个，然后重新部署和更新来亲自试一下。
- en: 'Once all the tasks have been updated, the output of our `docker stack ps web`
    command will look similar to the following screenshot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有任务更新完成，`docker stack ps web`命令的输出将类似于下面的截图：
- en: '![](img/4def2ed1-1be6-4416-9e98-041b0ec8d8d6.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4def2ed1-1be6-4416-9e98-041b0ec8d8d6.png)'
- en: All tasks have been updated successfully
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有任务已成功更新
- en: Please note that SwarmKit does not immediately remove the containers of the
    previous versions of the tasks from the corresponding nodes. This makes sense
    as we might want to, for example, retrieve the logs from those containers for
    debugging purposes, or we might want to retrieve their metadata using `docker
    container inspect`. SwarmKit keeps the four latest terminated task instances around
    before it purges older ones so that it doesn't clog the system with unused resources.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SwarmKit不会立即从相应的节点删除旧版本任务的容器。这是有道理的，因为我们可能希望，例如，检索这些容器的日志以进行调试，或者我们可能希望使用`docker
    container inspect`检索它们的元数据。SwarmKit会保留最近终止的四个任务实例，在清除更旧的任务之前，确保不会让未使用的资源堵塞系统。
- en: We can use the `--update-order` parameter to instruct Docker to start the new
    container replica before stopping the old one. This can improve application availability.
    Valid values are `"start-first"` and `"stop-first"`. The latter is the default.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`--update-order`参数指示Docker在停止旧容器之前先启动新容器副本。这可以提高应用程序的可用性。有效值为`"start-first"`和`"stop-first"`，后者是默认值。
- en: 'Once we''re done, we can tear down the stack using the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以使用以下命令来销毁堆栈：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Although using stack files to define and deploy applications is the recommended
    best practice, we can also define the update behavior in a service `create` statement.
    If we just want to deploy a single service, this might be the preferred way of
    doing things. Let''s look at such a `create` command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用堆栈文件定义和部署应用程序是推荐的最佳实践，但我们也可以在服务`create`语句中定义更新行为。如果我们只想部署一个单独的服务，这可能是更优的做法。让我们看一下这样的`create`命令：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This command defines the same desired state as the preceding stack file. We
    want the service to run with `10` replicas and we want a rolling update to happen
    in batches of two tasks at a time, with a 10-second interval between consecutive
    batches.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令定义了与前面的堆栈文件相同的期望状态。我们希望服务以`10`个副本运行，并希望以每次两个任务的批次进行滚动更新，连续批次之间的间隔为10秒。
- en: Health checks
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: To make informed decisions, for example, during a rolling update of a Swarm
    service regarding whether or not the just-installed batch of new service instances
    is running OK or if a rollback is needed, the SwarmKit needs a way to know about
    the overall health of the system. On its own, SwarmKit (and Docker) can collect
    quite a bit of information. But there is a limit. Imagine a container containing
    an application. The container, as seen from the outside, can look absolutely healthy
    and carry on just fine. But that doesn't necessarily mean that the application
    running inside the container is also doing well. The application could, for example,
    be in an infinite loop or be in a corrupt state, yet still running. However, as
    long as the application runs, the container runs and from outside, everything
    looks perfect.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出明智的决策，例如，在Swarm服务的滚动更新过程中，判断刚安装的新一批服务实例是否运行正常，或者是否需要回滚，SwarmKit需要一种方式来了解系统的整体健康状况。仅靠SwarmKit（和Docker）可以收集到大量信息，但也有其局限性。试想，一个包含应用程序的容器。从外部看，容器可能看起来完全健康，运行得很好。但这并不一定意味着容器内部运行的应用程序也一样好。应用程序可能例如处于无限循环中或处于损坏状态，但仍然在运行。然而，只要应用程序在运行，容器也在运行，从外部看，一切都完美无缺。
- en: Thus, SwarmKit provides a seam where we can provide it with some help. We, the
    authors of the application services running inside the containers in the swarm,
    know best as to whether or not our service is in a healthy state. SwarmKit gives
    us the opportunity to define a command that is executed against our application
    service to test its health. What exactly this command does is not important to
    Swarm; the command just needs to return `OK`, `NOT OK`, or `time out`. The latter
    two situations, namely `NOT OK` or `timeout`, will tell SwarmKit that the task
    it is investigating is potentially unhealthy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SwarmKit提供了一个接口，我们可以在其中为其提供一些帮助。我们，作为在Swarm中容器内部运行的应用服务的作者，最清楚我们的服务是否处于健康状态。SwarmKit让我们有机会定义一个命令，该命令会针对我们的应用服务执行健康检查。这个命令具体做什么对Swarm并不重要；它只需要返回`OK`、`NOT
    OK`或`time out`。后两种情况，即`NOT OK`或`timeout`，会告诉SwarmKit它正在检查的任务可能是不健康的。
- en: 'Here, I am writing potentially on purpose and later, we will see why:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我故意这么写，稍后我们会看到原因：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding snippet from a `Dockerfile`, we can see the keyword `HEALTHCHECK`.
    It has a few options or parameters and an actual command, that is, `CMD`. Let''s
    discuss the options:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的`Dockerfile`代码片段中，我们可以看到关键字`HEALTHCHECK`。它有几个选项或参数，以及一个实际的命令，也就是`CMD`。让我们来讨论这些选项：
- en: '`--interval`: Defines the wait time between health checks. Thus, in our case,
    the orchestrator executes a check every `30` seconds.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--interval`：定义健康检查之间的等待时间。因此，在我们的例子中，调度器每`30`秒执行一次检查。'
- en: '`--timeout`: This parameter defines how long Docker should wait if the health
    check does not respond until it times out with an error. In our sample, this is
    `10` seconds. Now, if one health check fails, SwarmKit retries a couple of times
    until it gives up and declares the corresponding task as unhealthy and opens the
    door for Docker to kill this task and replace it with a new instance.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--timeout`：此参数定义了如果健康检查没有响应，Docker等待多长时间才会因超时而报错。在我们的示例中，这是`10`秒。现在，如果某次健康检查失败，SwarmKit会重试几次，直到放弃并声明相应的任务为不健康，并允许Docker终止该任务并用新实例替换它。'
- en: The number of retries is defined with the `--retries` parameter. In the preceding
    code, we want to have three retries.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重试次数由`--retries`参数定义。在前面的代码中，我们希望设置为三次重试。
- en: Next, we have the start period. Some containers take some time to start up (not
    that this is a recommended pattern, but sometimes it is inevitable). During this
    startup time, the service instance might not be able to respond to health checks.
    With the start period, we can define how long SwarmKit should wait before it executes
    the very first health check and thus give the application time to initialize.
    To define the startup time, we use the `--start-period` parameter. In our case,
    we do the first check after `60` seconds. How long this start period needs to
    be depends on the application and its startup behavior. The recommendation is
    to start with a relatively low value and if you have a lot of false positives
    and tasks that are restarted many times, you might want to increase the time interval.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们有启动时间。一些容器需要一些时间才能启动（虽然这不是推荐的模式，但有时是不可避免的）。在启动期间，服务实例可能无法响应健康检查。通过启动时间，我们可以定义SwarmKit在执行第一次健康检查之前应该等待多长时间，从而为应用程序提供初始化的时间。要定义启动时间，我们使用`--start-period`参数。在我们的例子中，我们在`60`秒后进行第一次检查。启动时间的长短取决于应用程序及其启动行为。建议从相对较小的值开始，如果出现许多假阳性并且任务被多次重启，则可以考虑增加时间间隔。
- en: 'Finally, we define the actual probing command on the last line with the `CMD`
    keyword. In our case, we are defining a request to the `/health` endpoint of `localhost`
    at port `3000` as a probing command. This call is expected to have three possible
    outcomes:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在最后一行用`CMD`关键字定义了实际的探测命令。在我们的例子中，我们定义了一个向`localhost`的`/health`端点发送请求的探测命令，端口为`3000`。这个调用预期会有三种可能的结果：
- en: The command succeeds.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令成功执行。
- en: The command fails.
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令执行失败。
- en: The command times out.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令超时。
- en: The latter two are treated the same way by SwarmKit. This is the orchestrator
    telling us that the corresponding task might be unhealthy. I did say *might *with
    intent since SwarmKit does not immediately assume the worst-case scenario but
    assumes that this might just be a temporary fluke of the task and that it will
    recover from it. This is the reason why we have a `--retries` parameter. There,
    we can define how many times SwarmKit should retry before it can assume that the
    task is indeed unhealthy, and consequently kill it and reschedule another instance
    of this task on another free node to reconcile the desired state of the service.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 后两者被SwarmKit以相同方式处理。这是编排器告诉我们，相应的任务可能处于不健康状态。我说 *可能* 是有意为之，因为SwarmKit并不会立即假设最坏的情况，而是认为这可能只是任务的一个暂时性问题，并且它会从中恢复。这就是为什么我们有`--retries`参数的原因。在这里，我们可以定义SwarmKit在认为任务确实不健康之前应该重试多少次，随后它会终止任务并重新调度另一个实例到其他空闲节点上，以便将服务的期望状态恢复。
- en: '*Why can we use localhost in our probing command?* This is a very good question,
    and the reason is because SwarmKit, when probing a container running in the Swarm,
    executes this `probing` command inside the container (that is, it does something
    like `docker container exec <containerID> <probing command>`). Thus, the command
    executes in the same network namespace as the application running inside the container.
    In the following diagram, we can see the life cycle of a service task from its
    beginning:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么我们可以在探测命令中使用localhost？* 这是一个很好的问题，原因是因为当SwarmKit探测在Swarm中运行的容器时，它会在容器内部执行这个`探测`命令（也就是说，它做的是类似于`docker
    container exec <containerID> <probing command>`的操作）。因此，这个命令会在与容器内部运行的应用程序相同的网络命名空间中执行。在下面的图中，我们可以看到一个服务任务从开始到结束的生命周期：'
- en: '![](img/04607fa9-4a95-4188-9437-5db991b5d3b1.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04607fa9-4a95-4188-9437-5db991b5d3b1.png)'
- en: Service task with transient health failure
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 服务任务出现暂时性的健康失败
- en: First, SwarmKit waits to probe until the start period is over. Then, we have
    our first health check. Shortly thereafter, the task fails when probed. It fails
    two consecutive times but then it recovers. Thus, **health check 4** is successful
    and SwarmKit leaves the task running.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，SwarmKit会等到启动时间结束才开始探测。然后，我们进行第一次健康检查。不久后，任务在探测时失败。它连续失败了两次，但随后恢复。因此，**健康检查4**是成功的，SwarmKit让任务继续运行。
- en: 'Here, we can see a task that is permanently failing:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到一个永久失败的任务：
- en: '![](img/69eb3697-f61f-446c-b5cc-3c5d36bbe6d0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69eb3697-f61f-446c-b5cc-3c5d36bbe6d0.png)'
- en: Permanent failure of a task
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 任务的永久失败
- en: 'We have just learned how we can define a health check for a service in the
    `Dockerfile` of its image. But this is not the only way we can do this. We can
    also define the health check in the stack file that we use to deploy our application
    into Docker Swarm. Here is a short snippet of what such a stack file would look
    like:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学到了如何在服务的 `Dockerfile` 中为其镜像定义健康检查。但这并不是我们唯一可以做到的方式。我们还可以在用于将应用程序部署到 Docker
    Swarm 的堆栈文件中定义健康检查。以下是一个堆栈文件的简短示例：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding snippet, we can see how the health check-related information
    is defined in the stack file. First and foremost, it is important to realize that
    we have to define a health check for every service individually. There is no health
    check at an application or global level.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们可以看到健康检查相关的信息是如何在堆栈文件中定义的。首先要意识到的是，每个服务必须单独定义健康检查。应用程序级别或全局级别没有健康检查。
- en: Similar to what we defined previously in the `Dockerfile`, the command that
    is used to execute the health check by SwarmKit is `curl -f http://localhost:3000/health`.
    We also have definitions for `interval`, `timeout`, `retries`, and `start_period`.
    These four key-value pairs have the same meaning as the corresponding parameters
    we used in the `Dockerfile`. If there are health check-related settings defined
    in the image, then the ones defined in the stack file override the ones from the
    `Dockerfile`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 `Dockerfile` 中之前定义的类似，SwarmKit 用于执行健康检查的命令是 `curl -f http://localhost:3000/health`。我们还定义了
    `interval`、`timeout`、`retries` 和 `start_period`。这四个键值对的含义与我们在 `Dockerfile` 中使用的相同。如果镜像中已经定义了健康检查相关的设置，那么堆栈文件中的设置会覆盖
    `Dockerfile` 中的设置。
- en: 'Now, let''s try to use a service that has a health check defined. In our `lab`
    folder, we have a file called `stack-health.yaml` with the following content:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用一个已定义健康检查的服务。在我们的 `lab` 文件夹中，有一个名为 `stack-health.yaml` 的文件，内容如下：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s deploy this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来部署这个：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can find out where the single task was deployed to using `docker stack ps
    myapp`. On that particular node, we can list all the containers to find one of
    our stacks. In my example, the task had been deployed to `node-3`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `docker stack ps myapp` 找出单个任务部署的位置。在那个特定的节点上，我们可以列出所有容器，找到我们堆栈中的一个容器。在我的示例中，任务已部署到
    `node-3`：
- en: '![](img/b39744ac-ae71-456f-b8d1-3b34c99837e2.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b39744ac-ae71-456f-b8d1-3b34c99837e2.png)'
- en: Displaying the health status of a running task instance
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 显示正在运行的任务实例的健康状态
- en: The interesting thing in this screenshot is the `STATUS` column. Docker, or
    more precisely, SwarmKit, has recognized that the service has a health check function
    defined and is using it to determine the health of each task of the service.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这张截图中有趣的是 `STATUS` 列。Docker，或者更准确地说，SwarmKit，已经识别出该服务定义了健康检查功能，并正在使用它来确定服务中每个任务的健康状态。
- en: Rollback
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚
- en: Sometimes, things don't go as expected. A last-minute fix in an application
    release may have inadvertently introduced a new bug, or the new version significantly
    decreases the throughput of the component, and so on. In such cases, we need to
    have a plan B, which in most cases means the ability to roll back the update to
    the previous good version.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，事情并不如预期那样发展。应用程序发布中的临时修复可能无意中引入了一个新漏洞，或者新版本显著降低了组件的吞吐量，等等。在这种情况下，我们需要有备选计划，通常意味着能够回滚更新到先前的良好版本。
- en: As with the update, the rollback has to happen in such a way that it does not
    cause any outages in terms of the application; it needs to cause zero-downtime.
    In that sense, a rollback can be looked at as a reverse update. We are installing
    a new version, yet this new version is actually the previous version.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新类似，回滚必须以不会导致应用程序中断的方式进行；它需要实现零停机时间。从这个意义上来说，回滚可以看作是一次反向更新。我们安装了一个新版本，但这个新版本实际上是上一个版本。
- en: 'As with the update behavior, we can declare, either in our stack files or in
    the Docker service `create` command, how the system should behave in case it needs
    to execute a rollback. Here, we have the stack file that we used previously, but
    this time with some rollback-relevant attributes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新操作类似，我们可以在堆栈文件或 Docker 服务的 `create` 命令中声明，如果需要执行回滚，系统应该如何处理。在这里，我们使用了之前的堆栈文件，不过这次加入了一些与回滚相关的属性：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this stack file, which is available in our lab as `stack-rollback.yaml`,
    we defined the details about the rolling update, the health checks, and the behavior
    during rollback. The health check is defined so that after an initial wait time
    of `2` seconds, the orchestrator starts to poll the service on `http://localhost`
    every `2` seconds and it retries `3` times before it considers a task as unhealthy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个堆栈文件中，我们定义了滚动更新、健康检查和回滚时的行为，文件在我们的实验室中以 `stack-rollback.yaml` 呈现。健康检查被定义为，在初始等待时间
    `2` 秒后，协调器开始每 `2` 秒轮询一次服务（在 `http://localhost` 上），并在考虑任务为不健康之前，重试 `3` 次。
- en: If we do the math, then it takes at least 8 seconds until a task will be stopped
    if it is unhealthy due to a bug. So, now under deploy, we have a new entry called
    `monitor`. This entry defines how long newly deployed tasks should be monitored
    for health and whether or not to continue with the next batch in the rolling update.
    Here, in this sample, we have given it `10` seconds. This is slightly more than
    the 8 seconds we calculated it takes to discover that a defective service has
    been deployed, so this is good.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们做一下计算，假设任务由于 bug 导致不健康，那么至少需要 8 秒钟才能停止任务。因此，现在在部署下，我们新增了一个名为 `monitor` 的条目。该条目定义了新部署的任务应该监控多久的健康状态，以及是否继续进行滚动更新中的下一批。在这个示例中，我们设置了
    `10` 秒。这个时间比我们计算出来的发现服务故障的 8 秒稍长，因此是合适的。
- en: We also have a new entry, `failure_action`, which defines what the orchestrator
    will do if it encounters a failure during the rolling update, such as that the
    service is unhealthy. By default, the action is just to stop the whole update
    process and leave the system in an intermediate state. The system is not down
    since it is a rolling update and at least some healthy instances of the service
    are still operational, but an operations engineer would be better at taking a
    look and fixing the problem.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还新增了一个条目，`failure_action`，用于定义在滚动更新过程中如果遇到失败（例如服务不健康）时，协调器将采取什么措施。默认情况下，行动是停止整个更新过程，并将系统保持在中间状态。由于这是滚动更新，系统并没有完全宕机，至少一些健康的服务实例仍然在运行，但操作工程师会更擅长检查并修复问题。
- en: In our case, we have defined the action to be a `rollback`. Thus, in case of
    failure, SwarmKit will automatically revert all tasks that have been updated back
    to their previous version.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们已经定义了回滚操作。因此，在失败的情况下，SwarmKit 会自动将所有已更新的任务回滚到其先前的版本。
- en: Blue–green deployments
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: In [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml), *Distributed Application
    Architecture*, we discussed what blue-green deployments are, in an abstract way.
    It turns out that, on Docker Swarm, we cannot really implement blue-green deployments
    for arbitrary services. The service discovery and load balancing between two services
    running in Docker Swarm are part of the Swarm routing mesh and cannot be (easily)
    customized.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml)《*分布式应用架构*》中，我们以抽象的方式讨论了蓝绿部署是什么。事实证明，在
    Docker Swarm 中，我们无法对任意服务实现蓝绿部署。服务发现和负载均衡在 Docker Swarm 中由 Swarm 路由网格处理，不能（轻易地）进行自定义。
- en: 'If **Service A** wants to call **Service B**, then Docker does this implicitly.
    Docker, given the name of the target service, will use the Docker **DNS** service
    to resolve this name to a **virtual IP** (**VIP**) address. When the request is
    then targeted at the **VIP**, the Linux **IPVS** service will do another lookup
    in the Linux kernel IP tables with the **VIP** and load balance the request to
    one of the physical IP addresses of the tasks of the service represented by the
    **VIP**, as shown in the following diagram:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果**服务A**想调用**服务B**，那么 Docker 会隐式处理这个过程。Docker 在给定目标服务的名称后，会使用 Docker **DNS**
    服务将该名称解析为**虚拟IP**（**VIP**）地址。当请求指向**VIP**时，Linux **IPVS** 服务会在 Linux 内核的 IP 表中根据**VIP**进行查找，并将请求负载均衡到服务代表的任务之一的物理
    IP 地址，如下图所示：
- en: '![](img/f7103312-f96e-4301-8f61-6a75c5c74a43.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7103312-f96e-4301-8f61-6a75c5c74a43.png)'
- en: How service discovery and load balancing work in Docker Swarm
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 中的服务发现和负载均衡工作原理
- en: Unfortunately, there is no easy way to intercept this mechanism and replace
    it with a custom behavior. But this would be needed to allow for a true blue-green
    deployment of **Service B**, which is the target service in our example. As we
    will see in [Chapter 16](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml), *Deploying,
    Updating, and Securing an Application with Kubernetes,* Kubernetes is more flexible
    in this area.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前没有简单的方法来拦截这一机制并用自定义行为替代。但这对于实现我们示例中的目标服务**Service B**的真正蓝绿部署是必要的。正如我们将在[第16章](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml)《*使用
    Kubernetes 部署、更新和保护应用程序*》中看到的那样，Kubernetes 在这一领域更具灵活性。
- en: That being said, we can always deploy the public-facing services in a blue-green
    fashion. We can use interlock 2 and its layer 7 routing mechanism to allow for
    a true blue-green deployment.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们始终可以以蓝绿方式部署面向公众的服务。我们可以使用 Interlock 2 及其第七层路由机制来实现真正的蓝绿部署。
- en: Canary releases
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Technically speaking, rolling updates are a kind of canary release. But due
    to their lack of seams, where you could plug customized logic into the system,
    rolling updates are only a very limited version of canary releases.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，滚动更新是一种金丝雀发布。但由于缺少可以插入自定义逻辑的“缝隙”，滚动更新只是金丝雀发布的一个非常有限的版本。
- en: True canary releases require us to have more fine-grained control over the update
    process. Also, true canary releases do not take down the old version of the service
    until 100% of the traffic has been funneled through the new version. In that regard,
    they are treated like blue-green deployments.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的金丝雀发布要求我们对更新过程进行更细粒度的控制。此外，真正的金丝雀发布在所有流量都已完全切换到新版本之前，不会停止旧版本的服务。从这个角度来看，它们与蓝绿部署类似。
- en: In a canary release scenario, we don't just want to use things such as health
    checks as deciding factors regarding whether or not to funnel more and more traffic
    through the new version of the service; we also want to consider external input
    in the decision-making process, such as metrics that are collected and aggregated
    by a log aggregator or tracing information. An example that could be used as a
    decision-maker includes conformance to **service-level agreements** (**SLAs**),
    namely if the new version of the service shows response times that are outside
    of the tolerance band. This can happen if we add new functionality to an existing
    service, yet this new functionality degrades the response time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀发布场景中，我们不仅仅希望使用健康检查等指标来决定是否将越来越多的流量引导到新版本的服务中；我们还希望在决策过程中考虑外部输入，例如由日志聚合器收集并汇总的度量数据或追踪信息。一个可以作为决策依据的示例是遵循**服务水平协议**（**SLA**），即如果新版本的服务响应时间超出了容忍范围。这可能发生在我们为现有服务添加新功能时，而这个新功能导致了响应时间的下降。
- en: Storing configuration data in the swarm
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Swarm 中存储配置数据
- en: If we want to store non-sensitive data such as configuration files in Docker
    Swarm, then we can use Docker configs. Docker configs are very similar to Docker
    secrets, which we will discuss in the next section. The main difference is that
    config values are not encrypted at rest, while secrets are. Docker configs can
    only be used in Docker Swarm, that is, they cannot be used in your non-Swarm development
    environment. Docker configs are mounted directly into the container's filesystem.
    Configuration values can either be strings or binary values up to a size of 500
    KB.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要在 Docker Swarm 中存储非敏感数据，如配置文件，我们可以使用 Docker 配置。Docker 配置与我们将在下一节中讨论的 Docker
    密钥非常相似。主要的区别在于，配置值在静态存储时没有加密，而密钥则是加密的。Docker 配置只能在 Docker Swarm 中使用，也就是说，它们不能在非
    Swarm 开发环境中使用。Docker 配置会直接挂载到容器的文件系统中。配置值可以是字符串或最大为 500 KB 的二进制值。
- en: With the use of Docker configs, you can separate the configuration from Docker
    images and containers. This way, your services can easily be configured with environment-specific
    values. The production swarm environment has different configuration values than
    the staging swarm, which in turn has different config values than the development
    or integration environment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Docker 配置，您可以将配置与 Docker 镜像和容器分离。这样，您的服务可以轻松地根据环境特定的值进行配置。生产 Swarm 环境的配置值与暂存
    Swarm 的配置值不同，而暂存 Swarm 的配置值又与开发或集成环境的配置值不同。
- en: We can add configs to services and also remove them from running services. Configs
    can even be shared among different services running in the swarm.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将配置添加到服务中，也可以从正在运行的服务中删除它们。配置甚至可以在 Swarm 中运行的不同服务之间共享。
- en: 'Now, let''s create some Docker configs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一些 Docker 配置：
- en: 'First, we start with a simple string value:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从一个简单的字符串值开始：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding command creates the `Hello world` configuration value and uses
    it as input to the config named `hello-config`. The output of this command is
    the unique `ID` of this new config that's being stored in the swarm.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令创建了名为 `Hello world` 的配置值，并将其用作名为 `hello-config` 的配置输入。该命令的输出是这个新配置在 swarm
    中的唯一 `ID`。
- en: 'Let''s see what we got and use the list command to do so:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看结果，并使用列表命令来查看：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the list command shows the `ID` and the `NAME` of the config
    we just created, as well as its `CREATED` and (last) updated time. But since configs
    are non-confidential, we can do more and even output the content of a config,
    like so:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表命令的输出显示了我们刚刚创建的配置的 `ID` 和 `NAME`，以及其 `CREATED` 和（最后一次）更新时间。但由于配置是非机密的，我们可以做更多的操作，甚至输出配置的内容，如下所示：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Hmmm, interesting. In the `Spec` subnode of the preceding JSON-formatted output,
    we have the `Data` key with a value of `SGVsbG8gd29ybGQK`. Didn''t we just say
    that the config data is not encrypted at rest? It turns out that the value is
    just our string encoded as `base64`, as we can easily verify:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，挺有意思的。在前述 JSON 格式输出的 `Spec` 子节点中，我们看到 `Data` 键的值是 `SGVsbG8gd29ybGQK`。难道我们刚才没有说过配置数据在静态存储时并没有加密吗？事实证明，这个值只是我们字符串的
    `base64` 编码，我们可以很容易地验证这一点：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So far, so good.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。
- en: 'Now, let''s define a somewhat more complicated Docker config. Let''s assume
    we are developing a Java application. Java''s preferred way of passing configuration
    data to the application is the use of so-called `properties` files. A `properties`
    file is just a text file containing a list of key-value pairs. Let''s take a look:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个稍微复杂一点的 Docker 配置。假设我们正在开发一个 Java 应用程序。Java 推荐的将配置数据传递给应用程序的方式是使用所谓的
    `properties` 文件。`properties` 文件只是一个包含键值对列表的文本文件。让我们看一下：
- en: 'Let''s create a file called `my-app.properties` and add the following content
    to it:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 `my-app.properties` 的文件，并添加以下内容：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Save the file and create a Docker config called `app.properties` from it:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存文件并从中创建一个名为 `app.properties` 的 Docker 配置：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can use this (somewhat contrived) command to get the clear text value
    of the config we just created:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个（稍微做作的）命令来获取我们刚创建的配置的明文值：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is exactly what we expected.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们所期望的结果。
- en: 'Now, let''s create a Docker service that uses the preceding config. For simplicity,
    we will be using the nginx image to do so:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个使用前述配置的 Docker 服务。为了简化，我们将使用 nginx 镜像来实现：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The interesting part in the preceding service `create` command is the line that
    contains `--config`. With this line, we're telling Docker to use the config named
    `app.properties` and mount it as a file at `/etc/my-app/conf/app.properties` inside
    the container. Furthermore, we want that file to have the mode `0440` assigned
    to it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的服务 `create` 命令中，关键部分是包含 `--config` 的那一行。通过这一行，我们告诉 Docker 使用名为 `app.properties`
    的配置，并将其作为文件挂载到容器内的 `/etc/my-app/conf/app.properties`。此外，我们希望该文件的权限模式为 `0440`。
- en: 'Let''s see what we got:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the preceding output, we can see that the only instance of the service is
    running on node `node-1`. On this node, I can now list the containers to get the
    `ID` of the nginx instance:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述输出中，我们可以看到服务的唯一实例正在 `node-1` 节点上运行。在这个节点上，我现在可以列出容器以获取 nginx 实例的 `ID`：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we can `exec` into that container and output the value of the `/etc/my-app/conf/app.properties`
    file:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以 `exec` 进入该容器并输出 `/etc/my-app/conf/app.properties` 文件的值：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: No surprise here; this is exactly what we expected.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 没有惊讶，这正是我们所期望的结果。
- en: 'Docker configs can, of course, also be removed from the swarm, but only if
    they are not being used. If we try to remove the config we were just using previously,
    without first stopping and removing the service, we would get the following output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Docker 配置也可以从 swarm 中删除，但前提是它们没有被使用。如果我们在没有先停止并删除服务的情况下尝试删除我们刚才使用的配置，将会得到以下输出：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We get an error message in which Docker is nice enough to tell us that the config
    is being used by our service called `nginx`. This behavior is somewhat similar
    to what we are used to when working with Docker volumes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了一条错误信息，Docker 亲切地告诉我们配置正被名为`nginx`的服务使用。这种行为与我们在使用 Docker 卷时的行为有些相似。
- en: 'Thus, first, we need to remove the service and then we can remove the config:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先我们需要移除服务，然后才能移除配置：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It is important to note once more that Docker configs should never be used to
    store confidential data such as secrets, passwords, or access keys and key secrets.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 需要再次强调的是，Docker 配置不应存储机密数据，如秘密、密码、访问密钥和密钥机密。
- en: In the next section, we will discuss how to handle confidential data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何处理机密数据。
- en: Protecting sensitive data with Docker secrets
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker 秘密保护敏感数据
- en: Secrets are used to work with confidential data in a secure way. Swarm secrets
    are secure at rest and in transit. That is, when a new secret is created on a
    manager node, and it can only be created on a manager node, its value is encrypted
    and stored in the raft consensus storage. This is why it is secure at rest. If
    a service gets a secret assigned to it, then the manager reads the secret from
    storage, decrypts it, and forwards it to all the containers who are instances
    of the swarm service that requested the secret. Since node-to-node communication
    in Docker Swarm uses mutual **transport layer security** (**TLS**), the secret
    value, although decrypted, is still secure in transit. The manager forwards the
    secret only to the worker nodes that a service instance is running on. Secrets
    are then mounted as files into the target container. Each secret corresponds to
    a file. The name of the secret will be the name of the file inside the container,
    and the value of the secret is the content of the respective file. Secrets are
    never stored on the filesystem of a worker node and are instead mounted using
    `tmpFS` into the container. By default, secrets are mounted into the container
    at `/run/secrets`, but you can change that to any custom folder.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密用于以安全的方式处理机密数据。Swarm 秘密在静态和传输过程中都是安全的。也就是说，当在管理节点上创建一个新秘密时，它的值会被加密并存储在 Raft
    共识存储中，这就是为什么它在静态时是安全的原因。如果某个服务分配了秘密，那么管理节点会从存储中读取秘密，解密后将其转发给所有请求该秘密的 Swarm 服务实例的容器。由于
    Docker Swarm 中的节点间通信使用了 **传输层安全** (**TLS**)，即使秘密值被解密，它在传输中仍然是安全的。管理节点仅将秘密转发给服务实例所在的工作节点。然后，秘密作为文件挂载到目标容器中。每个秘密对应一个文件，秘密的名称将是容器内部的文件名，秘密的值则是该文件的内容。秘密从不存储在工作节点的文件系统中，而是通过
    `tmpFS` 挂载到容器中。默认情况下，秘密挂载到容器中的 `/run/secrets`，但你可以将其更改为任何自定义文件夹。
- en: It is important to note that secrets will not be encrypted on Windows nodes
    since there is no concept similar to `tmpfs`. To achieve the same level of security
    that you would get on a Linux node, the administrator should encrypt the disk
    of the respective Windows node.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Windows 节点上不会对秘密进行加密，因为没有类似 `tmpfs` 的概念。为了达到与 Linux 节点相同的安全级别，管理员应该加密相应
    Windows 节点的磁盘。
- en: Creating secrets
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建秘密
- en: 'First, let''s see how we can actually create a secret:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下如何实际创建一个秘密：
- en: '[PRE25]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This command creates a secret called `sample-secret` with the `sample secret
    value` value. Please note the hyphen at the end of the `docker secret create`
    command. This means that Docker expects the value of the secret from standard
    input. This is exactly what we're doing by piping the `sample secret value` value
    into the `create` command.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令创建一个名为 `sample-secret` 的秘密，其值为 `sample secret value`。请注意 `docker secret create`
    命令末尾的连字符。这意味着 Docker 期望从标准输入中获取秘密的值。这正是我们通过将 `sample secret value` 值传输到 `create`
    命令中所做的。
- en: 'Alternatively, we can use a file as the source for the secret value:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以使用文件作为秘密值的来源：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, the value of the secret with the name `other-secret` is read from a file
    called `~/my-secrets/secret-value.txt`. Once a secret has been created, there
    is no way to access the value of it. We can, for example, list all our secrets
    to get the following output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，名为 `other-secret` 的秘密的值是从一个名为 `~/my-secrets/secret-value.txt` 的文件中读取的。一旦创建了一个秘密，就无法访问它的值。例如，我们可以列出所有秘密以获取以下输出：
- en: '![](img/2b31dcdd-9f1d-44eb-ac20-1d8263bf8f1c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b31dcdd-9f1d-44eb-ac20-1d8263bf8f1c.png)'
- en: List of all secrets
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机密的列表
- en: 'In this list, we can only see the `ID` and `NAME` of the secret, plus some
    other metadata, but the actual value of the secret is not visible. We can also
    use `inspect` on a secret, for example, to get more information about `other-secret`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，我们只能看到秘密的 `ID` 和 `NAME`，以及其他一些元数据，但秘密的实际值是不可见的。我们还可以对一个秘密使用 `inspect`
    命令，例如，获取有关 `other-secret` 的更多信息：
- en: '![](img/82cad794-8a35-44db-81d7-51a9bbd67b29.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82cad794-8a35-44db-81d7-51a9bbd67b29.png)'
- en: Inspecting a swarm secret
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Swarm 秘密
- en: 'Even here, we do not get the value of the secret back. This is, of course,
    intentional: a secret is a secret and thus needs to remain confidential. We can
    assign labels to secrets if we want and we can even use a different driver to
    encrypt and decrypt the secret if we''re not happy with what Docker delivers out
    of the box.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，我们也无法获取到密钥的值。这当然是有意为之：密钥就是密钥，需要保持机密。如果需要，我们可以为密钥分配标签，并且如果Docker默认的密钥加解密方式不合适，我们还可以使用不同的驱动来加密和解密密钥。
- en: Using a secret
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用密钥
- en: 'Secrets are used by services that run in the swarm. Usually, secrets are assigned
    to a service at creation time. Thus, if we want to run a service called `web`
    and assign it a secret, say, `api-secret-key`, the syntax would look as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 密钥是由在集群中运行的服务使用的。通常，密钥会在服务创建时分配给该服务。因此，如果我们想运行一个名为`web`的服务并分配给它一个密钥，比如`api-secret-key`，语法如下：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This command creates a service called `web` based on the `fundamentalsofdocker/whoami:latest`
    image, publishes the container port `8000` to port `8000` on all swarm nodes,
    and assigns it the secret called `api-secret-key`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令基于`fundamentalsofdocker/whoami:latest`镜像创建一个名为`web`的服务，将容器端口`8000`映射到所有集群节点上的`8000`端口，并为其分配名为`api-secret-key`的密钥。
- en: 'This will only work if the secret called `api-secret-key` is defined in the
    swarm; otherwise, an error will be generated with the text `secret not found:
    api-secret-key`. Thus, let''s create this secret now:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '只有当名为`api-secret-key`的密钥在集群中被定义时，这个命令才会生效；否则，会生成一个错误信息，内容为`secret not found:
    api-secret-key`。因此，我们现在来创建这个密钥：'
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, if we rerun the service `create` command, it will succeed:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们重新运行服务`create`命令，它将成功执行：
- en: '![](img/4dd798d2-4a33-41b6-ae25-39f71d55386e.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dd798d2-4a33-41b6-ae25-39f71d55386e.png)'
- en: Creating a service with a secret
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个带密钥的服务
- en: 'Now, we can use `docker service ps web` to find out on which node the sole
    service instance has been deployed, and then `exec` into this container. In my
    case, the instance has been deployed to `node-3`, so I need to `SSH` into that
    node:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`docker service ps web`来查找唯一的服务实例在哪个节点上部署，然后`exec`进入该容器。在我的情况下，该实例已经部署到`node-3`，所以我需要通过`SSH`进入该节点：
- en: '[PRE29]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, I list all my containers on that node to find the one instance belonging
    to my service and copy its `container ID`. We can then run the following command
    to make sure that the secret is indeed available inside the container under the
    expected filename containing the secret value in clear text:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我列出该节点上的所有容器，找到属于我的服务的那个实例，并复制它的`container ID`。接着，我们可以运行以下命令，确保密钥确实在容器中以预期的文件名存在，并且文件内容是明文的密钥值：
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once again, in my case, this looks like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，在我的情况下，命令如下：
- en: '![](img/397cb02b-4760-4f21-9a10-0364294c20b9.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/397cb02b-4760-4f21-9a10-0364294c20b9.png)'
- en: A secret as a container sees it
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 容器如何看到密钥
- en: 'If, for some reason, the default location where Docker mounts the secrets inside
    the container is not acceptable to you, you can define a custom location. In the
    following command, we mount the secret to `/app/my-secrets`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某些原因，Docker将密钥挂载到容器中的默认位置不符合你的需求，你可以定义一个自定义位置。在以下命令中，我们将密钥挂载到`/app/my-secrets`：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this command, we are using the extended syntax to define a secret that includes
    the destination folder.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，我们使用扩展语法来定义一个包含目标文件夹的密钥。
- en: Simulating secrets in a development environment
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在开发环境中模拟密钥
- en: When working in development, we usually don't have a local swarm on our machine.
    But secrets only work in a swarm. So, *what can we do*? Well, luckily, this answer
    is really simple. Due to the fact that secrets are treated as files, we can easily
    mount a volume that contains the secrets into the container to the expected location,
    which by default is at `/run/secrets`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发时，我们通常在本地机器上没有一个集群。但密钥只在集群中有效。那么，*我们该怎么办*？幸运的是，答案非常简单。由于密钥被当作文件处理，我们可以轻松地将包含密钥的卷挂载到容器中预期的位置，默认位置是`/run/secrets`。
- en: 'Let''s assume that we have a folder called `./dev-secrets` on our local workstation.
    For each secret, we have a file named the same as the secret name and with the
    unencrypted value of the secret as the content of the file. For example, we can
    simulate a secret called `demo-secret` with a secret value of `demo secret value`
    by executing the following command on our workstation:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在本地工作站上有一个名为`./dev-secrets`的文件夹。对于每个密钥，我们有一个与密钥名称相同的文件，文件内容是该密钥的未加密值。例如，我们可以通过在工作站上执行以下命令来模拟一个名为`demo-secret`的密钥，密钥值为`demo
    secret value`：
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we can create a container that mounts this folder, like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个挂载该文件夹的容器，方法如下：
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The process running inside the container will be unable to distinguish these
    mounted files from the ones originating from a secret. So, for example, `demo-secret`
    is available as a file called `/run/secrets/demo-secret` inside the container
    and has the expected value `demo secret value`. Let''s take a look at this in
    more detail in the following steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内运行的进程将无法区分这些挂载的文件和来自机密的文件。例如，`demo-secret`作为一个名为`/run/secrets/demo-secret`的文件存在于容器中，并具有预期的值`demo
    secret value`。我们将在以下步骤中更详细地了解这一点：
- en: 'To test this, we can `exec` a shell inside the preceding container:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试这一点，我们可以在前面的容器中执行一个shell：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we can navigate to the `/run/secrets` folder and display the content of
    the `demo-secret` file:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以导航到`/run/secrets`文件夹并显示`demo-secret`文件的内容：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Next, we will be looking at secrets and legacy applications.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看机密和遗留应用程序。
- en: Secrets and legacy applications
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机密和遗留应用程序
- en: Sometimes, we want to containerize a legacy application that we cannot easily,
    or do not want to, change. This legacy application might expect a secret value
    to be available as an environment variable. *How are we going to deal with this
    now?* Docker presents us with the secrets as files but the application is expecting
    them in the form of environment variables.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们想要将一个遗留应用程序容器化，这个应用程序我们不能轻易更改，或者不想更改。这个遗留应用程序可能期望一个机密值作为环境变量提供。*我们现在该如何处理呢？*
    Docker将机密呈现为文件，但应用程序期望以环境变量的形式获取这些机密。
- en: 'In this situation, it is helpful to define a script that runs when the container
    is started (a so-called entry point or startup script). This script will read
    the secret value from the respective file and define an environment variable with
    the same name as the file, assigning the new variable the value read from the
    file. In the case of a secret called `demo-secret` whose value should be available
    in an environment variable called `DEMO_SECRET,` the necessary code snippet in
    this startup script could look like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，定义一个在容器启动时运行的脚本是很有帮助的（所谓的入口点或启动脚本）。该脚本将从相应的文件中读取机密值，并定义一个与文件同名的环境变量，将新变量赋值为从文件中读取的值。对于一个名为`demo-secret`的机密，其值应以名为`DEMO_SECRET`的环境变量的形式提供，那么启动脚本中的必要代码片段可能如下所示：
- en: '[PRE36]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Similarly, let''s say we have a legacy application that expects the secret
    values to be present as an entry in, say, a YAML configuration file located in
    the `/app/bin` folder and called `app.config`, whose relevant part looks like
    this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设我们有一个遗留应用程序，它期望机密值作为`/app/bin`文件夹中的YAML配置文件中的一个条目，该文件名为`app.config`，其相关部分如下所示：
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Our initialization script now needs to read the secret value from the `secret`
    file and replace the corresponding placeholder in the config file with the `secret`
    value. For `demo-secret`, this could look like this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始化脚本现在需要从`secret`文件中读取机密值，并将配置文件中相应的占位符替换为`secret`值。对于`demo-secret`，它可能如下所示：
- en: '[PRE38]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding snippet, we're using the `sed` tool to replace a placeholder
    with a value in place. We can use the same technique for the other two secrets
    in the config file.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`sed`工具来将占位符替换为实际值。我们可以对配置文件中的另外两个机密使用相同的技巧。
- en: We put all the initialization logic into a file called `entrypoint.sh`, make
    this file executable and, for example, add it to the root of the container's filesystem.
    Then, we define this file as `ENTRYPOINT` in the `Dockerfile`, or we can override
    the existing `ENTRYPOINT` of an image in the `docker container run` command.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有初始化逻辑放入一个名为`entrypoint.sh`的文件中，并使该文件具有可执行权限，并将其添加到容器文件系统的根目录中。然后，我们在`Dockerfile`中将该文件定义为`ENTRYPOINT`，或者我们可以在`docker
    container run`命令中覆盖镜像的现有`ENTRYPOINT`。
- en: 'Let''s make a sample. Let''s assume that we have a legacy application running
    inside a container defined by the `fundamentalsofdocker/whoami:latest` image that
    expects a secret called `db_password` to be defined in a file, `whoami.conf`,
    in the application folder. Let''s take a look at these steps:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个示例。假设我们有一个在`fundamentalsofdocker/whoami:latest`镜像定义的容器中运行的遗留应用程序，该应用程序期望一个名为`db_password`的机密定义在应用程序文件夹中的`whoami.conf`文件中。让我们来看一下这些步骤：
- en: 'We can define a file, `whoami.conf`, on our local machine that contains the
    following content:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在本地机器上定义一个文件`whoami.conf`，其中包含以下内容：
- en: '[PRE39]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The important part is line 3 of this snippet. It defines where the secret value
    has to be put by the startup script.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的部分是这个片段的第3行。它定义了启动脚本需要将机密值放置的位置。
- en: 'Let''s add a file called `entrypoint.sh` to the local folder that contains
    the following content:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们向本地文件夹中添加一个名为`entrypoint.sh`的文件，文件内容如下：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The last line in the preceding script stems from the fact that this is the start
    command that was used in the original `Dockerfile`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 前面脚本中的最后一行来自于原始`Dockerfile`中使用的启动命令。
- en: 'Now, change the mode of this file to an executable:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将该文件的权限更改为可执行：
- en: '[PRE41]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now, we define a `Dockerfile` that inherits from the `fundamentalsofdocker/whoami:latest`
    image.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个继承自`fundamentalsofdocker/whoami:latest`镜像的`Dockerfile`。
- en: 'Add a file called `Dockerfile` to the current folder that contains the following
    content:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向当前文件夹添加一个名为`Dockerfile`的文件，文件内容如下：
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s build the image from this `Dockerfile`:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从这个`Dockerfile`构建镜像：
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the image has been built, we can run a service from it. But before we
    can do that, we need to define the secret in Swarm:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦镜像构建完成，我们就可以从中运行服务。但在此之前，我们需要在 Swarm 中定义机密：
- en: '[PRE44]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can create a service that uses the following secret:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个使用以下机密的服务：
- en: '[PRE45]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Updating secrets
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新机密
- en: At times, we need to update a secret in a running service since secrets could
    be leaked out to the public or be stolen by malicious people, such as hackers.
    In this case, we need to change our confidential data since the moment it is leaked
    to a non-trusted entity, it has to be considered as insecure.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要在运行中的服务中更新机密，因为机密可能已经泄露到公共环境或被恶意人员（如黑客）窃取。在这种情况下，我们需要更改我们的机密数据，因为一旦它泄露给不可信实体，它就必须被视为不安全的。
- en: Updating secrets, like any other update, has to happen in a way that requires
    zero-downtime. Docker SwarmKit supports us in this regard.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何其他更新一样，更新机密必须以零停机时间的方式进行。Docker SwarmKit 在这方面提供了支持。
- en: 'First, we create a new secret in the swarm. It is recommended to use a versioning
    strategy when doing so. In our example, we use a version as a postfix of the secret
    name. We originally started with the secret named `db-password` and now the new
    version of this secret is called `db-password-v2`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在 Swarm 中创建了一个新的机密。建议在执行此操作时使用版本控制策略。在我们的示例中，我们将版本作为机密名称的后缀。我们最初使用的机密名为`db-password`，而现在此机密的新版本名为`db-password-v2`：
- en: '[PRE46]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s assume that the original service that used the secret had been created
    like this:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始使用该机密的服务是这样创建的：
- en: '[PRE47]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The application running inside the container was able to access the secret
    at `/run/secrets/db-password`. Now, SwarmKit does not allow us to update an existing
    secret in a running service, so we have to remove the now obsolete version of
    the secret and then add the new one. Let''s start with removal with the following
    command:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内运行的应用能够访问`/run/secrets/db-password`中的机密信息。现在，SwarmKit 不允许我们更新正在运行的服务中的现有机密，因此我们必须删除现已过时的机密版本，然后添加新的机密。让我们首先使用以下命令进行删除：
- en: '[PRE48]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can add the new secret with the following command:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令添加新的机密：
- en: '[PRE49]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Please note the extended syntax of `--secret-add` with the `source` and `target`
    parameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`--secret-add`的扩展语法，其中包含`source`和`target`参数。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how SwarmKit allows us to update services without
    requiring downtime. We also discussed the current limits of SwarmKit in regard
    to zero-downtime deployments. In the second part of this chapter, we introduced
    secrets as a means to provide confidential data to services in a highly secure
    way.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 SwarmKit 如何允许我们在不需要停机的情况下更新服务。我们还讨论了 SwarmKit 在零停机部署方面的当前限制。在本章的第二部分，我们介绍了机密作为一种以高度安全的方式向服务提供机密数据的方法。
- en: In the next chapter, we will introduce the currently most popular container
    orchestrator, Kubernetes. We'll discuss the objects that are used to define and
    run a distributed, resilient, robust, and highly available application in a Kubernetes
    cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that's
    used to locally deploy a Kubernetes application, and also demonstrate the integration
    of Kubernetes with Docker for macOS and Docker for Windows.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍当前最流行的容器编排工具 Kubernetes。我们将讨论用于在 Kubernetes 集群中定义和运行分布式、弹性、强健且高度可用的应用的对象。此外，本章还将帮助我们熟悉
    MiniKube，这是一个用于在本地部署 Kubernetes 应用的工具，并展示 Kubernetes 与 Docker for macOS 和 Docker
    for Windows 的集成。
- en: Questions
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'To assess your understanding of the topics that were discussed in this chapter,
    please answer the following questions:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估你对本章讨论主题的理解，请回答以下问题：
- en: In a few simple sentences, explain to an interested layman what zero-downtime
    deployment means.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用几句话向一位对技术不太熟悉的外行解释什么是零停机时间部署。
- en: How does SwarmKit achieve zero-downtime deployments?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SwarmKit 如何实现零停机时间部署？
- en: Contrary to traditional (non-containerized) systems, why does a rollback in
    Docker Swarm just work? Explain this in a few short sentences.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与传统（非容器化）系统不同，为什么 Docker Swarm 中的回滚操作能顺利进行？请用几句话简要解释。
- en: Describe two to three characteristics of a Docker secret.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述 Docker secret 的两到三种特性。
- en: 'You need to roll out a new version of the `inventory` service. What does your
    command look like? Here is some more information:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要推出一个新版本的`inventory`服务。你的命令应该是什么样的？以下是一些额外信息：
- en: The new image is called `acme/inventory:2.1`.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的镜像名为`acme/inventory:2.1`。
- en: We want to use a rolling update strategy with a batch size of two tasks.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用滚动更新策略，每批任务的大小为两个任务。
- en: We want the system to wait for one minute after each batch.
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望系统在每批任务执行完后等待一分钟。
- en: You need to update an existing service named `inventory` with a new password
    that is provided through a Docker secret. The new secret is called `MYSQL_PASSWORD_V2`.
    The code in the service expects the secret to be called `MYSQL_PASSWORD`. What
    does the update command look like? (Note that we do not want the code of the service
    to be changed!)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要通过 Docker secret 更新一个名为`inventory`的现有服务，新的密码通过一个名为`MYSQL_PASSWORD_V2`的 Docker
    secret 提供。服务中的代码期望这个 secret 名为`MYSQL_PASSWORD`。更新命令应该是什么样的？（请注意，我们不希望更改服务的代码！）
- en: Further reading
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here are some links to external sources:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些外部资源的链接：
- en: Apply rolling updates to a service, at [https://dockr.ly/2HfGjlD](https://dockr.ly/2HfGjlD)
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对服务应用滚动更新，见[https://dockr.ly/2HfGjlD](https://dockr.ly/2HfGjlD)
- en: Managing sensitive data with Docker secrets, at [https://dockr.ly/2vUNbuH](https://dockr.ly/2vUNbuH)
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker secrets 管理敏感数据，见[https://dockr.ly/2vUNbuH](https://dockr.ly/2vUNbuH)
- en: Introducing Docker secrets management, at [https://dockr.ly/2k7zwzE](https://dockr.ly/2k7zwzE)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Docker secrets 管理，见[https://dockr.ly/2k7zwzE](https://dockr.ly/2k7zwzE)
- en: From env variables to Docker secrets, at [https://bit.ly/2GY3UUB](https://bit.ly/2GY3UUB)
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从环境变量到 Docker secrets，见[https://bit.ly/2GY3UUB](https://bit.ly/2GY3UUB)
