- en: Chapter 9. Proxy Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 代理服务
- en: We reached the point where we need something that will tie together the containers
    we're deploying. We need to simplify the access to the services and unify all
    the servers and ports our containers are (or will be) deployed on. Multiple solutions
    are trying to solve this problem, with **Enterprise Service Bus** (**ESB**) products
    being most commonly used. That is not to say that their only goal is redirection
    towards destination services. It indeed isn't, and that is one of the reasons
    we rejected ESB as (part of) the solution for our architecture. The significant
    difference in the approach is that ESBs tend to do a lot (much more than we need)
    while we are trying to compose our system by using very specific small components
    or services that do (almost) exactly what we need. Not more, not less. ESBs are
    an antithesis of microservices and, in a way, are betraying the initial ideas
    behind service-oriented architecture. With us being committed to microservices
    and looking for more concrete solutions, the alternative is a proxy service. It
    stands to reason that we should dedicate a bit more time discussing what proxy
    services are and which products might be able to help us in our architecture and
    processes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到了需要将我们正在部署的容器连接在一起的阶段。我们需要简化对服务的访问，并统一我们容器部署所在的所有服务器和端口。许多解决方案正在尝试解决这个问题，其中**企业服务总线**（**ESB**）产品是最常见的。然而，这并不是说它们的唯一目标是将请求重定向到目标服务。事实并非如此，这也是我们拒绝将ESB作为（我们架构中的一部分）解决方案的原因之一。其方法的显著区别在于，ESB倾向于做很多（远远超出我们的需求），而我们则试图通过使用非常具体的小型组件或服务来构建系统，这些组件或服务几乎恰好做我们需要的事，不多，也不少。ESB与微服务相对立，从某种程度上讲，它背离了面向服务架构的初衷。由于我们致力于微服务并寻找更具体的解决方案，因此替代方案就是代理服务。显然，我们应该多花一些时间讨论什么是代理服务，以及哪些产品可能帮助我们实现架构和流程。
- en: A *proxy service* is a service that acts as an intermediary between clients
    performing requests and services that serve those requests. A client sends a request
    to the proxy service that, in turn, redirects that request to the destination
    service thus simplifying and controlling complexity laying behind the architecture
    where the services reside.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*代理服务*是指在执行请求的客户端和提供这些请求的服务之间充当中介的服务。客户端将请求发送到代理服务，代理服务再将该请求转发到目标服务，从而简化并控制架构中服务背后的复杂性。'
- en: 'There are at least three different types of proxy services:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有三种不同类型的代理服务：
- en: A *gateway* or *tunneling service* is the kind of a proxy service that redirect
    requests to the destination services and responses back to the clients that made
    those requests.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网关*或*隧道服务*是一种代理服务，它将请求重定向到目标服务，并将响应返回给发出请求的客户端。'
- en: A *forward proxy* is used for retrieving data from different (mostly internet)
    sources.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正向代理*用于从不同（主要是互联网）来源检索数据。'
- en: A *reverse proxy* is usually used to control and protect access to a server
    or services on a private network. Besides its primary function, a reverse proxy
    often also performs tasks such as load-balancing, decryption, caching and authentication.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向代理*通常用于控制和保护对私有网络上服务器或服务的访问。除了其主要功能外，反向代理通常还执行负载均衡、解密、缓存和身份验证等任务。'
- en: A reverse proxy is probably the best solution for the problem at hand, so we'll
    spend a bit more time trying to understand it better.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 反向代理可能是解决当前问题的最佳方案，因此我们将花更多时间来更好地理解它。
- en: Reverse Proxy Service
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向代理服务
- en: The main purpose of the proxy service is to hide the rest of the services as
    well as to redirect requests to their final destination. The same holds true for
    responses. Once a service responds to a request, that response goes back to the
    proxy service and from there is redirected to the client that initially requested
    it. For all purposes, from the point of view of the destination service, the request
    came from the proxy. In other words, neither the client that generates the request
    knows what is behind the proxy nor the service responding to the request knows
    that it originated from beyond the proxy. In other words, both clients and services
    know only about the existence of the proxy service.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 代理服务的主要目的是隐藏其余服务并将请求重定向到最终目的地。响应也是如此。一旦某个服务响应请求，该响应将返回给代理服务，并从代理服务重定向回最初请求的客户端。从目的地服务的角度来看，所有请求都是来自代理的。换句话说，生成请求的客户端不知道代理背后是什么，响应请求的服务也不知道请求来自代理之外。也就是说，客户端和服务只知道代理服务的存在。
- en: We'll concentrate on usages of a proxy service in the context of an architecture
    based on (micro) services. However, most of the concepts are the same if a proxy
    service would be used on whole servers (except that it would be called proxy server).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将集中讨论基于（微）服务架构中的代理服务使用。然而，如果代理服务应用于整个服务器，许多概念也是相同的（只是它会被称为代理服务器）。
- en: 'Some of the main purposes of a proxy services (beyond orchestration of requests
    and responses) are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 代理服务的主要目的之一（除了请求和响应的协调）如下：
- en: While almost any applications server can provide **encryption** (most commonly
    **Secure Sockets Layer** (**SSL**)), it is often easier to let the middle man
    be in charge of it.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管几乎任何应用服务器都可以提供**加密**（最常见的是**安全套接字层**（**SSL**）），但通常让中间人负责这一过程更为简便。
- en: '**Load balancing** is the process when, in this case, proxy service balances
    loads between multiple instances of the same service. In most cases, those instances
    would be scaled over multiple servers. With that combination (load balancing and
    scaling), especially when architecture is based on microservices, we can quickly
    accomplish performance improvements and avoid timeouts and downtimes.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**是指在这种情况下，代理服务在多个相同服务的实例之间平衡负载。在大多数情况下，这些实例会分布在多个服务器上。结合**负载均衡**和**扩展**，特别是在微服务架构的基础上，我们可以快速实现性能提升，避免超时和停机。'
- en: '**Compression** is another candidate for a feature that is easily accomplished
    when centralized in a single service. Main products that act as proxy services
    are very efficient in compression and allow relatively easy setup. The primary
    reason for a compression of the traffic is a speedup of the load time. The smaller
    the size, the faster the load.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩**是另一个在单一服务中集中实现的功能候选项。作为代理服务的主要产品在压缩方面非常高效，并且设置相对简单。压缩流量的主要原因是加快加载时间。文件越小，加载速度就越快。'
- en: '**Caching** is another one of the features that are easy to implement within
    a proxy service that (in some cases) benefits from being centralized. By caching
    responses, we can offload part of the work our services need to do. The gist of
    caching is that we set up the rules (for example, cache requests related to the
    products listing) and cache timeouts. From there on, the proxy service will send
    a request to the destination service only the first time and store the responses
    internally. From there on, as long as the request is the same, it will be served
    directly by the proxy without even sending the request to the service. That is,
    until the timeout is reached and the process is repeated. The are much more complicated
    combinations we can employ, but the most common usage is the one we described.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存**是代理服务中另一个容易实现的功能，在某些情况下，它的集中化管理是有利的。通过缓存响应，我们可以卸载部分服务需要处理的工作。缓存的核心思想是我们设定规则（例如，缓存与产品列表相关的请求）和缓存超时。之后，代理服务只会第一次向目标服务发送请求，并将响应存储在内部。从那时起，只要请求相同，代理服务就会直接提供响应，而无需将请求转发到目标服务。直到超时发生，流程才会重复。我们还可以采用更复杂的组合方式，但最常见的用法就是我们描述的这种。'
- en: Most proxy services serve as a *single point of entry* to the public APIs exposed
    through services. That in itself increases **security**. In most cases only ports
    `80` (`HTTP`) and `443` (`HTTPS`) would be available to the public usage. All
    other ports required by services should be open only to the internal use.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数代理服务作为*单一入口点*服务公开的公共 API。仅此一点就增加了**安全性**。在大多数情况下，只有端口`80`（`HTTP`）和`443`（`HTTPS`）会对外公开，所有其他服务所需的端口应仅对内部使用开放。
- en: Different types of authentication (for example OAuth) can be implemented through
    the proxy service. When the request does not have the user identification, the
    proxy service can be set to return with an appropriate response code to the caller.
    On the other hand, when identification is present, a proxy can choose to continue
    going to the destination and leave the verification of that identification to
    the target service or perform it itself. Of course, many variations can be used
    to implement the authentication. The crucial thing to note is that if a proxy
    is used, it will most likely be involved in this process one way or another.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的身份验证（例如 OAuth）可以通过代理服务实现。当请求没有用户身份时，代理服务可以设置为返回适当的响应码给调用者。另一方面，当身份信息存在时，代理可以选择继续访问目标，并将身份验证交由目标服务处理，或者由代理自行处理。当然，许多不同的变体可以用来实现身份验证。关键要注意的是，如果使用了代理，它很可能会在这个过程中以某种方式参与其中。
- en: This list is by no means extensive nor final but contains some of the most commonly
    used cases. Many other combinations are possible involving both legal and illegal
    purposes. As an example, a proxy is an indispensable tool for any hacker that
    wants to stay anonymous.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这份清单绝不是详尽无遗的，也不是最终的，但包含了一些最常见的使用案例。许多其他组合也可能存在，包括合法和非法的用途。例如，代理是任何想要保持匿名的黑客不可或缺的工具。
- en: Throughout this books, we'll focus mostly on its primary function; we'll use
    proxy services to act as proxies. They will be in charge of the orchestration
    of all traffic between microservices we'll be deploying. We'll start with simple
    usages used in deployments and slowly progress towards more complicated orchestration,
    namely *blue-green deployment*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将主要关注代理服务的基本功能；我们将使用代理服务作为代理。它们将负责所有微服务之间流量的调度，这些微服务将被部署。我们从部署中使用的简单用法开始，逐步推进到更复杂的调度方式，即*蓝绿部署*。
- en: To some, it might sound that a proxy service deviates from microservices approach
    since it can do (as is often the case) multiple things. However, when looking
    from the functional point of view, it has a single purpose. It provides a bridge
    between the outside world and all the services we host internally. At the same
    time, it tends to have a very low resource usage and can be handled with only
    a few configuration files.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些人来说，代理服务可能偏离了微服务的思路，因为它（通常情况下）可以做多件事。然而，从功能角度看，它只有一个单一的目的。它提供了外部世界与我们内部托管的所有服务之间的桥梁。同时，它往往资源占用非常低，只需要几个配置文件即可处理。
- en: Equipped with the basic understanding about proxy services, the time has come
    to take a look at some of the products we can use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了代理服务的基本概念后，现在是时候了解我们可以使用的一些产品了。
- en: From now on, we'll refer to *reverse proxy* as, simply, *proxy*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将*反向代理*简化为*代理*。
- en: How can Proxy Service help our project?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理服务如何帮助我们的项目？
- en: By now we managed to have a controlled way to deploy our services. Due to the
    nature of deployments we are trying to accomplish, those services should be deployed
    on ports and, potentially, servers that are unknown to us in advance. Flexibility
    is the key to scalable architecture, fault tolerance, and many other concepts
    we'll explore further on. However, that flexibility comes at a cost. We might
    not know in advance where will the services be deployed nor which ports they are
    exposing. Even if this information would be available before the deployment, we
    should not force users of our services to specify different ports and IPs when
    sending requests. The solution is to centralize all communications both from third
    parties as well as from internal services at a single point. The singular place
    that will be in charge of redirecting requests is a proxy service. We'll explore
    some of the tools that are at our disposal and compare their strengths and weaknesses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经成功地实现了一种控制方式来部署我们的服务。由于我们尝试实现的部署性质，这些服务应该部署在我们事先无法确定的端口和可能是服务器上。灵活性是可扩展架构、容错能力以及我们将进一步探讨的许多其他概念的关键。然而，这种灵活性是有代价的。我们可能无法提前知道服务将部署在哪些地方，或者它们暴露了哪些端口。即使这些信息在部署之前可以获得，我们也不应该强迫我们的服务用户在发送请求时指定不同的端口和
    IP。解决方案是将来自第三方和内部服务的所有通信集中到一个单一的点。负责重定向请求的唯一地方将是一个代理服务。我们将探讨一些可用的工具，并比较它们的优缺点。
- en: As before, we'll start by creating virtual machines that we'll use to experiment
    with different proxy services. We'll recreate the `cd` node and use it to provision
    the `proxy` server with different proxy services.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将从创建虚拟机开始，利用这些虚拟机来实验不同的代理服务。我们将重新创建 `cd` 节点，并用它来为 `proxy` 服务器配置不同的代理服务。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first tool we'll explore is `nginx`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索的第一个工具是 `nginx`。
- en: nginx
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: nginx
- en: The nginx (engine x) is an HTTP and reverse proxy server, a mail proxy server,
    and a generic TCP proxy server. Igor Sysoev originally wrote it. In the beginning,
    it powered many Russian sites. Since then, it become a server of choice for some
    of the busiest sites in the world (NetFlix, Wordpress, and FastMail are only a
    few of the examples). According to Netcraft, nginx served or proxied around 23%
    of busiest sites in September 2015\. That makes it second to Apache. While numbers
    provided by Netcraft might be questionable, it is clear that nginx is highly popular
    and probably is closer to the third place after Apache and IIS. Since everything
    we did by now is based on Linux, Microsoft IIS should be discarded. That leaves
    us with Apache as a valid candidate to be our proxy service or choice. Stands
    to reason that the two should be compared.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: nginx（引擎 x）是一个 HTTP 和反向代理服务器，一个邮件代理服务器，以及一个通用的 TCP 代理服务器。它最初由 Igor Sysoev 编写。最初，它为许多俄罗斯网站提供服务。从那时起，它成为了世界上一些访问量最大的网站的首选服务器（Netflix、Wordpress
    和 FastMail 只是其中的一些例子）。根据 Netcraft 的数据，nginx 在 2015 年 9 月服务或代理了约 23% 的最繁忙网站。这使得它仅次于
    Apache。虽然 Netcraft 提供的数据可能值得怀疑，但显然 nginx 非常受欢迎，可能在 Apache 和 IIS 之后位居第三。由于我们到目前为止所做的工作都是基于
    Linux，因此 Microsoft IIS 应该被排除在外。这使得 Apache 成为我们选择代理服务的有效候选者。合理的推测是，这两者应该进行比较。
- en: Apache has been available for many years and built a massive user base. Its
    huge popularity is partly thanks to Tomcat that runs on top of Apache and is one
    of the most popular application servers today. Tomcat is only one out of many
    examples of Apache's flexibility. Through its modules, it can be extended to process
    almost any programming language.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Apache 已经存在多年，并建立了庞大的用户基础。它的巨大受欢迎程度部分得益于 Tomcat，后者运行在 Apache 之上，是目前最受欢迎的应用服务器之一。Tomcat
    只是 Apache 灵活性的众多例子之一。通过其模块，Apache 可以扩展以处理几乎任何编程语言。
- en: Being most popular does not necessarily makes something the best choice. Apache
    can slow down to a crawl under a heavy load due to its design deficiencies. It
    spawns new processes that, in turn, consume quite a lot of memory. On top of that,
    it creates new threads for all requests making them compete with each others for
    access to CPU and memory. Finally, if it reaches the configurable limit of processes,
    it just refuses new connections. Apache was not designed to serve as a proxy service.
    That function is very much an after-thought.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎并不一定意味着是最好的选择。由于设计缺陷，Apache 在重负载下可能会变得极为缓慢。它会生成新的进程，这些进程又会消耗大量的内存。此外，它会为每个请求创建新的线程，导致这些线程竞争
    CPU 和内存的访问权限。最后，如果它达到可配置的进程限制，它将拒绝新的连接。Apache 并不是为了作为代理服务而设计的。这个功能实际上是事后才加入的。
- en: nginx was created to address some of the problems Apache has, in particular,
    the C10K problem. At the time, C10K was a challenge for web servers to begin handling
    ten thousand concurrent connections. nginx was released in 2004 and met the goal
    of the challenge. Unlike Apache, its architecture is based on asynchronous, non-blocking,
    event-driven architecture. Not only that it beats Apache in the number of concurrent
    requests it can handle, but its resource usage was much lower. It was born after
    Apache and designed from ground up as a solution for concurrency problems. We
    got a server capable of handling more requests and a lower cost.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: nginx是为了解决Apache的一些问题而创建的，尤其是C10K问题。当时，C10K对于Web服务器来说是一个挑战，要求能够处理一万个并发连接。nginx于2004年发布，并达成了这一目标。与Apache不同，nginx的架构基于异步、非阻塞、事件驱动架构。不仅如此，它在处理并发请求的数量上超过了Apache，而且它的资源使用率也低得多。它是在Apache之后诞生的，从零开始设计，解决了并发问题。我们得到了一个能够处理更多请求、成本更低的服务器。
- en: nginx' downside is that it is designed to serve static content. If you need
    a server to serve content generated by Java, PHP, and other dynamic languages,
    Apache is a better option. In our case, this downside is of almost no importance
    since we are looking for a proxy service with the capability to do load balancing
    and few more features. We will not be serving any content (static or dynamic)
    directly by the proxy, but redirect requests to specialized services.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: nginx的缺点是它是为提供静态内容而设计的。如果你需要一个能够提供由Java、PHP及其他动态语言生成的内容的服务器，Apache是更好的选择。在我们的情况下，这个缺点几乎不重要，因为我们只需要一个能够进行负载均衡和一些其他功能的代理服务。我们不会通过代理直接提供任何内容（无论是静态的还是动态的），而是将请求重定向到专门的服务。
- en: All in all, while Apache might be a good choice in a different setting, nginx
    is a clear winner for the task we're trying to accomplish. It will perform much
    better than Apache if its only task is to act as a proxy and load balancing. It's
    memory consumption will be minuscule and it will be capable of handling a vast
    amount of concurrent requests. At least, that is the conclusion before we get
    to other contestants for the proxy supremacy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，虽然在其他场景中Apache可能是一个不错的选择，但对于我们要完成的任务，nginx显然是更好的选择。如果它的唯一任务是充当代理和负载均衡器，它的性能将远超Apache。它的内存消耗非常少，并且能够处理大量的并发请求。至少，在我们考虑其他代理竞选者之前，这是我们的结论。
- en: Setting Up nginx
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置nginx
- en: 'Before we set up the nginx proxy service, let''s take a quick look at the Ansible
    files that we''re about to run. The nginx.yml playbook is similar to those we
    used before. We''ll be running the roles we already run before with the addition
    of nginx:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们配置nginx代理服务之前，先快速浏览一下我们即将运行的Ansible文件。nginx.yml playbook类似于我们之前使用的文件。我们将运行之前已经运行过的角色，并加上nginx角色：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`oles/nginx/tasks/main.yml` role also doesn''t contain anything extraordinary:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`oles/nginx/tasks/main.yml`角色也没有包含什么特别的内容：'
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are creating few directories, making sure that the nginx container is running,
    passing few files and, if any of them changed, reloading nginx. Finally, we are
    putting the nginx IP to Consul in case we need it for later. The only important
    thing to notice is the nginx configuration file `roles` `/nginx/files/services.conf`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了几个目录，确保nginx容器正在运行，传送了一些文件，并且如果它们有变动，则重新加载nginx。最后，我们将nginx的IP地址写入Consul，以备后用。需要注意的唯一重要事项是nginx的配置文件`roles`
    `/nginx/files/services.conf`：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For the moment, you can ignore log formatting and jump to the `server` specification.
    We specified that nginx should `listen` to the standard HTTP port `80` and accept
    requests sent to any server (`server_name _`). Next are the `include` statements.
    Instead of specifying all the configuration in one place, with includes we'll
    be able to add configuration for each service separately. That, in turn, will
    allow us to focus on one service at a time and make sure that the one we deploy
    is configured correctly. Later on, we'll explore in more depth which types of
    configurations go into each of those includes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你可以忽略日志格式，并跳到`server`规格说明。我们指定nginx应该`监听`标准HTTP端口`80`，并接受发送到任何服务器的请求（`server_name
    _`）。接下来是`include`语句。通过使用include，我们可以为每个服务单独添加配置，而不是将所有配置集中在一个地方。这样，我们可以专注于一次配置一个服务，并确保我们部署的服务配置正确。稍后，我们将更深入地探讨这些includes中包含的不同类型的配置。
- en: 'Let''s run the nginx playbook and start *playing* with it. We''ll enter the
    `cd` node and execute the playbook that will provision the `proxy` node:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行nginx playbook并开始*玩*它。我们将进入`cd`节点并执行该playbook，它将配置`proxy`节点：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Living without a Proxy
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 没有代理的生活
- en: 'Before we see nginx in action, it might be worthwhile to refresh our memory
    of the difficulties we are facing without a proxy service. We''ll start by running
    the `books-ms` application:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到 nginx 正在工作之前，回顾一下没有代理服务时我们所面临的困难可能是值得的。我们将通过运行`books-ms`应用程序来开始：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the last command is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上一条命令的输出如下：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Even though we run the application with `docker-compose` and confirmed that
    it is running on the `proxy` node by executing `docker-compose ps`, we observed
    through `curl` that the service is not accessible on the standard HTTP port 80
    (there was a `404 Not Found` message served through nginx). This result was to
    be expected since our service is running on a random port. Even if we did specify
    the port (we already discussed why that is a bad idea), we could not expect users
    to memorize a different port for each separately deployed service. Besides, we
    already have service discovery with Consul in place:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用`docker-compose`运行应用程序，并通过执行`docker-compose ps`确认它在`proxy`节点上运行，但通过`curl`我们发现服务无法在标准
    HTTP 端口 80 上访问（通过 nginx 返回了`404 Not Found`消息）。这一结果是可以预期的，因为我们的服务运行在一个随机端口上。即使我们指定了端口（我们已经讨论过这样做是一个坏主意），也不能指望用户记住每个单独部署的服务的不同端口。而且，我们已经通过
    Consul 实现了服务发现：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the last command is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上一条命令的输出如下：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can also obtain the port by inspecting the container:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过检查容器来获取端口：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We inspected the container, applied formatting to retrieve only the port of
    the service and stored that information in the `PORT` variable. Later on, we used
    that variable to make a proper request to the service. As expected, this time,
    the result was correct. Since there is no data, the service returned an empty
    JSON array (this time without the 404 error).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了容器，应用格式化操作以仅获取服务的端口，并将该信息存储在`PORT`变量中。之后，我们使用该变量向服务发出正确的请求。正如预期的那样，这次结果是正确的。由于没有数据，服务返回了一个空的
    JSON 数组（这次没有出现 404 错误）。
- en: 'Be it as it may, while this operation was successful, it is even less acceptable
    one for our users. They cannot be given access to our servers only so that they
    can query Consul or inspect containers to obtain the information they need. Without
    a proxy, services are unreachable. They are running, but no one can use them:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这次操作成功了，但对我们的用户来说，这样的方式是不可接受的。我们不能仅仅给他们访问服务器的权限，让他们查询 Consul 或检查容器以获取所需的信息。没有代理，服务是无法访问的。它们虽然在运行，但没人能使用它们：
- en: '![Living without a Proxy](img/B05848_09_01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![没有代理的生活](img/B05848_09_01.jpg)'
- en: Figure 9-1 – Services without proxy
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1 – 没有代理的服务
- en: Now that we felt the pain our users would feel without a proxy, let us configure
    nginx correctly. We'll start with manual configuration, and from there on, progress
    towards automated one.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经感受到没有代理时用户会遇到的痛苦，接下来让我们正确配置 nginx。我们将从手动配置开始，然后逐步过渡到自动化配置。
- en: Manually Configuring nginx
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动配置 nginx
- en: 'Do you remember the first `includes` statement in the nginx configuration?
    Let''s use it. We already have the `PORT` variable, and all that we have to do
    is make sure that all requests coming to nginx on port `80` and starting with
    the address `/api/v1/books` are redirected to the correct port. We can accomplish
    that by running the following commands:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得 nginx 配置中的第一个`includes`语句吗？让我们使用它。我们已经有了`PORT`变量，接下来我们要做的就是确保所有进入 nginx
    端口`80`并以`/api/v1/books`地址开头的请求被重定向到正确的端口。我们可以通过运行以下命令来实现：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We created the `books-ms.conf` file that will proxy all requests for `/api/v1/books`
    to the correct IP and port. The `location` statement will match all requests starting
    with `/api/v1/books` and proxy them to the same address running on the specified
    IP and port. While IP was not necessary, it is a good practice to use it since,
    in most cases, the proxy service will run on a separate server. Further on, we
    used **secure copy** (**scp**) to transfer the file to the `/data/nginx/includes/`
    directory in the `proxy` node. Once the configuration was copied, all we had to
    do was reload nginx using `kill -s HUP` command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了`books-ms.conf`文件，该文件将所有对`/api/v1/books`的请求代理到正确的 IP 和端口。`location`语句会匹配所有以`/api/v1/books`开头的请求，并将其代理到运行在指定
    IP 和端口上的相同地址。虽然 IP 并不是必须的，但使用它是一种好习惯，因为在大多数情况下，代理服务会运行在单独的服务器上。接下来，我们使用**安全复制**（**scp**）将该文件传输到`proxy`节点的`/data/nginx/includes/`目录中。配置文件复制完成后，我们只需使用`kill
    -s HUP`命令重新加载 nginx：
- en: 'Let''s see whether the change we just did works correctly:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们刚才所做的更改是否正确生效：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We successfully made a `PUT` request that inserted a book to the database and
    queried the service that returned that same book. Finally, we can make requests
    without worrying about the ports.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地进行了一个 `PUT` 请求，将一本书插入数据库，并查询了返回相同书籍的服务。最后，我们可以在不担心端口问题的情况下发起请求。
- en: 'Are our problems solved? Only partly. We still need to figure out the way to
    make these updates to the nginx configuration automatic. After all, if we''ll
    be deploying our microservices often, we cannot rely on human operators to continuously
    monitor deployments and perform configuration updates:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的问题解决了吗？仅部分解决。我们仍然需要找到一种方法，使这些 nginx 配置的更新自动化。毕竟，如果我们将频繁部署微服务，我们不能依赖人工操作员持续监控部署并进行配置更新：
- en: '![Manually Configuring nginx](img/B05848_09_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![手动配置 nginx](img/B05848_09_02.jpg)'
- en: Figure 9-2 – Services with manual proxy
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-2 – 使用手动代理的服务
- en: Automatically Configuring nginx
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动配置 nginx
- en: We already discussed service discovery tools and the nginx playbook we run earlier
    made sure that Consul, Registrator, and Consul Template are properly configured
    on the *proxy* node. That means that Registrator detected the service container
    we ran and stored that information to the Consul registry. All that is left is
    to make a template, feed it to Consul Template that will output the configuration
    file and reload nginx.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过服务发现工具，之前运行的 nginx 剧本确保 Consul、Registrator 和 Consul Template 在 *代理* 节点上得到了正确配置。这意味着
    Registrator 检测到了我们运行的服务容器，并将该信息存储到了 Consul 注册表中。剩下的就是创建一个模板，将其传递给 Consul Template，后者将输出配置文件并重新加载
    nginx。
- en: 'Let''s make the situation a bit more complicated and scale our service by running
    two instances. Scaling with Docker Compose is relatively easy:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让情况变得更复杂一些，通过运行两个实例来扩展我们的服务。使用 Docker Compose 进行扩展相对简单：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the latter command is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出如下：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can observe that there are two instances of our service, both using different
    random ports. Concerning nginx, this means several things, most important being
    that we cannot proxy in the same way as before. It would be pointless to run two
    instances of the service and redirect all requests only to one of them. We need
    to combine proxy with *load* *balancing*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，我们的服务有两个实例，分别使用不同的随机端口。对于 nginx 来说，这意味着几件事，其中最重要的一点是我们不能像以前那样进行代理。运行两个实例并将所有请求仅重定向到其中一个实例是没有意义的。我们需要将代理与
    *负载* *均衡* 结合起来。
- en: 'We won''t go into all possible load balancing techniques. Instead, we''ll use
    the simplest one called *round robin* that is used by nginx by default. Round
    robin means that the proxy will distribute requests equally among all services.
    As before, things closely related to a project should be stored in the repository
    together with the code and nginx configuration files and templates should not
    be an exception.take a look at the `nginx-includes.conf` configuration file:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨所有可能的负载均衡技术。相反，我们将使用最简单的技术——*轮询*，它是 nginx 默认使用的。轮询意味着代理将均等地分配请求到所有服务之间。如前所述，项目中密切相关的内容应该与代码一起存储在仓库中，nginx
    配置文件和模板也不应例外。看一下 `nginx-includes.conf` 配置文件：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This time, instead of specifying IP and port, we're using `books_ms`. Obviously,
    that domain does not exist. It is a way for us to tell nginx to proxy all requests
    from the location to an upstream. Additionally, we also added `proxy_next_upstream`
    instruction. If an error, timeout, invalid header or an error 500 is received
    as a service response, nginx will pass to the next upstream connection.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不是指定 IP 和端口，而是使用 `books_ms`。显然，这个域名并不存在。它是我们告诉 nginx 将所有来自该位置的请求代理到上游的一种方式。此外，我们还添加了
    `proxy_next_upstream` 指令。如果服务响应返回错误、超时、无效的头部或错误 500，nginx 将转发请求到下一个上游连接。
- en: 'That is the moment when we can start using the second include statement from
    the main configuration file. However, since we do not know the IPs and ports the
    service will use, the upstream is the Consul Template file `nginx-upstreams.ctmpl`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这时我们可以开始使用主配置文件中的第二个包含语句。然而，由于我们不知道服务将使用的 IP 和端口，上游就是 Consul Template 文件 `nginx-upstreams.ctmpl`：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: What this means is that the upstream request `books-ms` we set as the proxy
    upstream will be load balanced between all instances of the service and that data
    will be obtained from Consul. We'll see the result once we run Consul Template.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们设置的上游请求 `books-ms` 会在该服务的所有实例之间进行负载均衡，并且数据将从 Consul 中获取。我们运行 Consul Template
    后就能看到结果。
- en: 'First things first. Let''s download the two files we just discussed:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载我们刚刚讨论的两个文件：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that the proxy configuration and the upstream template are on the `cd`
    server, we should run Consul Template:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代理配置和上游模板已经放置在`cd`服务器上，我们应该运行Consul模板：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Consul Template took the downloaded template as the input and created the `books-ms.conf`
    upstream configuration. The second command output the result that should look
    similar to the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Consul模板将下载的模板作为输入，并创建了`books-ms.conf`上游配置。第二个命令的输出应该类似于以下内容：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since we are running two instances of the same service, Consul template retrieved
    their IPs and ports and put them in the format we specified in the `books-ms.ctmpl`
    template.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们运行的是两个相同服务的实例，Consul模板获取了它们的IP和端口，并将其放入我们在`books-ms.ctmpl`模板中指定的格式。
- en: Please note that we could have passed the third argument to Consul Template,
    and it would run any command we specify. We'll use it later on throughout the
    book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们本可以将第三个参数传递给Consul模板，它会运行我们指定的任何命令。我们将在本书后续章节中使用它。
- en: 'Now that all the configuration files are created, we should copy them to the
    `proxy` node and reload nginx:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有配置文件已经创建完成，我们应该将它们复制到`proxy`节点并重新加载nginx：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All that''s left is to double check that proxy works and is balancing requests
    among those two instances:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是再次确认代理是否正常工作，并且在这两个实例之间正确地负载均衡请求：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After making four requests we output nginx logs that should look like following
    (timestamps are removed for brevity).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行了四次请求后，我们输出了nginx日志，日志应该如下所示（时间戳已去除以简化显示）。
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'While ports might be different in your case, it is obvious that the first request
    was sent to the port `32768`, the next one to the `32769`, then to the `32768`
    again, and, finally, to the `32769`. It is a success, with nginx not only acting
    as a proxy but also load balancing requests among all instances of the service
    we deployed:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然端口在你的情况下可能不同，但显然第一个请求被发送到了端口`32768`，接下来的请求发送到`32769`，然后又回到`32768`，最后又发送到`32769`。这是成功的，nginx不仅充当了代理，还在我们部署的所有服务实例之间进行了负载均衡：
- en: '![Automatically Configuring nginx](img/B05848_09_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![自动配置nginx](img/B05848_09_03.jpg)'
- en: Figure 9-3 – Services with automatic proxy with Consul Template
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-3 – 使用Consul模板自动配置代理的服务
- en: 'We still haven''t tested the error handling we set up with the `proxy_next_upstream`
    instruction. Let''s remove one of the service instances and confirm that nginx
    handles failures correctly:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然没有测试使用`proxy_next_upstream`指令设置的错误处理。让我们移除一个服务实例，并确认nginx是否正确处理故障：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We stopped one service instance and made several requests. Without the *proxy_next_upstream*
    instruction, nginx would fail on every second request since one of the two services
    set as upstreams are not working anymore. However, all four requests worked correctly.
    We can observe what nginx did by taking a look at its logs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们停止了一个服务实例并进行了几次请求。如果没有*proxy_next_upstream*指令，nginx会在每第二个请求时失败，因为设置为上游的两个服务之一已经无法工作了。然而，所有四次请求都正确地处理了。我们可以通过查看nginx日志来观察它的行为：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output should be similar to the following (timestamps are removed for brevity):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容（时间戳已去除以简化显示）：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first request went to the port `32768` served by the instance that is still
    running. As expected, nginx sent the second request to the port `32768`. Since
    the response was `111` (Connection refused), it decided to temporarily disable
    this upstream and try with the next one in line. From there on, all the rest of
    requests were proxied to the port `32768`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个请求被发送到了由仍在运行的实例提供服务的端口`32768`。正如预期，nginx将第二个请求发送到了端口`32768`。由于响应是`111`（连接被拒绝），它决定暂时禁用这个上游，并尝试下一个上游。之后，所有的请求都被代理到端口`32768`。
- en: With only a few lines in configuration files, we managed to set up the proxy
    and combine it with load balancing and failover strategy. Later on, when we get
    to the chapter that will explore *self-healing systems*, we'll go even further
    and make sure not only that proxy works only with running services, but also how
    to restore the whole system to a healthy state.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在配置文件中添加几行，我们就成功设置了代理，并将其与负载均衡和故障转移策略结合起来。稍后，当我们进入探讨*自愈系统*的章节时，我们将进一步深入，确保代理不仅仅与运行中的服务工作，还会恢复整个系统到健康状态。
- en: 'When nginx is combined with service discovery tools, we have an excellent solution.
    However, we should not use the first tool that comes along, so we''ll evaluate
    a few more options. Let us stop the nginx container and see how *HAProxy* behaves:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当 nginx 与服务发现工具结合使用时，我们有了一个优秀的解决方案。然而，我们不应使用第一个遇到的工具，因此我们会评估更多的选项。让我们停止 nginx
    容器，看看*HAProxy*的表现：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: HAProxy
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HAProxy
- en: Just like nginx, HAProxy is a free, very fast and reliable solution offering
    high availability, load balancing, and proxying. It is particularly suited for
    very high traffic websites and powers quite many of the world's most visited ones.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与 nginx 一样，HAProxy 是一个免费的、非常快速和可靠的解决方案，提供高可用性、负载均衡和代理功能。它特别适合高流量网站，并为世界上许多最受欢迎的网站提供服务。
- en: We'll speak about the differences later on when we compare all proxy solutions
    we're exploring. For now, suffice to say that HAProxy is an excellent solution
    and probably the best alternative to nginx.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讨论所有代理解决方案的比较时的差异。目前，只需说明 HAProxy 是一个优秀的解决方案，可能是 nginx 最好的替代品。
- en: 'We''ll start with practical exercises and try to accomplish with HAProxy the
    same behavior as the one with have with nginx. Before we provision the *proxy*
    node with HAProxy, let us take a quick look at the tasks in the Ansible role haproxy:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从实际操作开始，并尝试使用 HAProxy 实现与 nginx 相同的行为。在为 *proxy* 节点配置 HAProxy 之前，让我们快速查看
    Ansible 角色 haproxy 中的任务：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The *haproxy* role is very similar to the one we used for nginx. We created
    some directories and copied some files (we''ll see them later on). The major thing
    to note is that, unlike most other containers not built by us, we''re not using
    the official *haproxy* container. The main reason is that the official image has
    no way to reload HAProxy configuration. We''d need to restart the container every
    time we update HAProxy configuration, and that would produce some downtime. Since
    one of the goals is to accomplish zero-downtime, restarting the container is not
    an option. Therefore, we had to look at alternatives, and the user *million12*
    has just what we need. The `million12/haproxy` container comes with *inotify*
    (*inode notify*). It is a Linux kernel subsystem that acts by extending filesystems
    to notice changes, and report them to applications. In our case, inotify will
    reload HAProxy whenever we change its configuration:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*haproxy* 角色与我们为 nginx 使用的非常相似。我们创建了一些目录并复制了一些文件（稍后我们会看到它们）。需要注意的主要一点是，与我们没有构建的其他大多数容器不同，我们没有使用官方的
    *haproxy* 容器。主要原因是官方镜像没有办法重新加载 HAProxy 配置。每次更新 HAProxy 配置时，我们都需要重启容器，而这会导致停机时间。由于我们的目标之一是实现零停机，因此重启容器并不是一个可选方案。因此，我们不得不寻找替代方案，用户
    *million12* 恰好提供了我们所需要的。`million12/haproxy` 容器自带 *inotify*（*inode notify*）。它是一个
    Linux 内核子系统，作用是扩展文件系统以检测变化并将其报告给应用程序。在我们的案例中，每当我们更改 HAProxy 配置时，inotify 会重新加载
    HAProxy：'
- en: 'Let us proceed and provision HAProxy on the proxy node:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在代理节点上配置 HAProxy：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Manually Configuring HAProxy
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动配置 HAProxy
- en: 'We''ll start by checking whether HAProxy is running:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查 HAProxy 是否在运行：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `docker ps` command showed that the *haproxy* container has the status
    `Exited`, and the logs produced the output similar to the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker ps` 命令显示 *haproxy* 容器的状态为 `Exited`，日志输出类似于以下内容：'
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: HAProxy complained that there is no `haproxy.cfg` configuration file and stopped
    the process. Actually, the fault is in the playbook we run. The only file we created
    is *haproxy.cfg.orig* (more about it later) and that there is no `haproxy.cfg`.
    Unlike nginx, HAPRoxy cannot be run without having, at least, one proxy set. We'll
    set up the first proxy soon but, at the moment, we have none. Since creating the
    configuration without any proxy is a waste of time (HAProxy fails anyway) and
    we cannot provide one when provisioning the node for the first time since at that
    point there would be no services running, we just skipped the creation of the
    *haproxy.cfg*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 报告没有 `haproxy.cfg` 配置文件，进程停止了。实际上，问题出在我们运行的 playbook 上。我们创建的唯一文件是 *haproxy.cfg.orig*（稍后会详细介绍），而没有
    `haproxy.cfg` 文件。与 nginx 不同，HAProxy 不能在没有至少一个代理设置的情况下运行。我们很快就会设置第一个代理，但目前我们还没有。由于在没有任何代理的情况下创建配置是浪费时间（HAProxy
    无论如何都会失败），而在第一次配置节点时我们也无法提供一个代理，因为到那个时候没有服务在运行，我们因此跳过了 *haproxy.cfg* 的创建。
- en: Before we proceed with the configuration of the first proxy, let us comment
    another difference that might complicate the process. Unlike nginx, HAProxy does
    not allow includes. The complete configuration needs to be in a single file. That
    will pose certain problems since the idea is to add or modify only configurations
    of the service we are deploying and ignore the rest of the system. We can, however,
    simulate includes by creating parts of the configuration as separate files and
    concatenate them every time we deploy a new container. For this reason, we copied
    the `haproxy.cfg.orig` file as part of the provisioning process. Feel free to
    take a look at it. We won't go into details since it contains mostly the default
    settings and HAProxy has a decent documentation that you can consult. The important
    thing to note is that the `haproxy.cfg.orig` file contains settings without a
    single proxy being set.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续配置第一个代理之前，让我们先提到另一个可能会使过程复杂化的差异。与 nginx 不同，HAProxy 不允许使用 include。完整的配置需要在一个文件中。这将带来一些问题，因为我们的目标是仅添加或修改我们正在部署的服务的配置，忽略系统的其他部分。不过，我们可以通过将配置的部分内容创建为独立的文件，并在每次部署新容器时将它们连接起来，从而模拟
    include。正因如此，我们在配置过程中复制了 `haproxy.cfg.orig` 文件。随时可以查看它。我们不会详细说明，因为它主要包含默认设置，并且
    HAProxy 有一份不错的文档供你查阅。需要注意的重要一点是，`haproxy.cfg.orig` 文件包含的是没有设置任何代理的配置。
- en: 'We''ll create the HAProxy configuration related to the service we have running
    in the similar way as we did before:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以与之前类似的方式创建与我们运行的服务相关的 HAProxy 配置：
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We started by inspecting the `vagrant_app_1` container in order to assign the
    current port to the `PORT` variable and use it to create the `books-ms.service.cfg`
    file.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查了 `vagrant_app_1` 容器，以便将当前端口分配给 `PORT` 变量，并使用它来创建 `books-ms.service.cfg`
    文件。
- en: HAProxy uses similar logic as nginx even though things are named differently.
    The *frontend* defines how requests should be forwarded to *backends*. In a way,
    the *frontend* is analogous to the nginx *location* instruction and the *backend*
    to the `upstream`. What we did can be translated to the following. Define a frontend
    called `books-ms-fe`, bind it to the port `80` and, whenever the request part
    starts with `/api/v1/books`, use the backend called `books-ms-be`. The backend
    `books-ms-be` has (at the moment) only one server defined with the IP `10.100.193.200`
    and the port assigned by Docker. The `check` argument has (more or less) the same
    meaning as in nginx and is used to skip proxying to services that are not healthy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 HAProxy 与 nginx 使用的名称不同，但其逻辑是类似的。*frontend* 定义了如何将请求转发到 *backends*。某种程度上，*frontend*
    类似于 nginx 的 *location* 指令，*backend* 则类似于 `upstream`。我们所做的可以转化为以下配置。定义一个名为 `books-ms-fe`
    的 frontend，将其绑定到端口 `80`，每当请求的路径以 `/api/v1/books` 开头时，使用名为 `books-ms-be` 的 backend。当前，backend
    `books-ms-be` 只定义了一个服务器，IP 为 `10.100.193.200`，端口由 Docker 分配。`check` 参数（或多或少）与
    nginx 中的含义相同，用于跳过对不健康服务的代理。
- en: 'Now that we have the general settings in the file `haproxy.cfg.orig` and those
    specific to services we''re deploying (named with the `.service.cfg` extension),
    we can concatenate them into a single `haproxy.cfg` configuration file and copy
    it to the `proxy` node:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了 `haproxy.cfg.orig` 文件中的通用设置以及针对我们正在部署的服务的特定设置（以 `.service.cfg` 扩展名命名），我们可以将它们连接成一个单一的
    `haproxy.cfg` 配置文件，并将其复制到 `proxy` 节点：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Since the container is not running, we''ll need to start it (again), and then
    we can check whether the proxy is working correctly by querying the service:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器没有运行，我们需要启动它（再次），然后可以通过查询服务来检查代理是否正常工作：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first request returned the `Connection refused` error. We used it to confirm
    that no proxy is running. Then we started the `haproxy` container and saw through
    the container logs that the configuration file we created is valid and indeed
    used by the proxy service. Finally, we sent the request again, and, this time,
    it returned a valid response.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个请求返回了 `Connection refused` 错误。我们利用这个错误确认没有代理在运行。然后我们启动了 `haproxy` 容器，并通过容器日志看到我们创建的配置文件是有效的，确实被代理服务使用了。最后，我们再次发送请求，这次返回了有效的响应。
- en: So far, so good. We can proceed and automate the process using Consult Template.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。我们可以继续并使用 Consult Template 来自动化这一过程。
- en: Automatically Configuring HAProxy
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动配置 HAProxy
- en: We'll try to do the same or very similar steps as what we did before with nginx.
    That way you can compare the two tools more easily.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试执行与之前在 nginx 上所做的相同或非常相似的步骤。这样，你可以更容易地比较这两个工具。
- en: 'We''ll start by scaling the service:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从扩展服务开始：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next we should download the `haproxy.ctmpl` template from the code repository.
    Before we do that, let us take a quick look at its contents:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们应该从代码仓库中下载`haproxy.ctmpl`模板。在我们操作之前，让我们快速查看一下它的内容：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The way we created the template follows the same pattern as the one we used
    with nginx. The only difference is that HAProxy needs each server to be uniquely
    identified so we added the service `Node` and `Port` that will serve as the server
    ID.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建模板的方式遵循了与nginx相同的模式。唯一的不同是，HAProxy需要每个服务器都有唯一标识，因此我们添加了服务`Node`和`Port`，作为服务器ID。
- en: 'Let''s download the template and run it through Consul Template:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载模板并通过Consul Template运行它：
- en: '[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We downloaded the template using `wget` and run the `consul-template` command.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`wget`下载了模板，并运行了`consul-template`命令。
- en: 'Let us concatenate all the files into haproxy.cfg, copy it to the `proxy` node
    and take a look at `haproxy` logs:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有文件连接成haproxy.cfg，复制到`proxy`节点，并查看`haproxy`日志：
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'All that''s left is to double check whether the proxy balancing works with
    two instances:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是仔细检查负载均衡是否在两个实例下正常工作：
- en: '[PRE37]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Unfortunately, HAProxy cannot output logs to stdout (preferred way to log Docker
    containers) so we cannot confirm that balancing works. We could output logs to
    syslog, but that is outside of the scope of this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，HAProxy无法将日志输出到stdout（Docker容器首选的日志输出方式），因此我们无法确认负载均衡是否正常工作。我们可以将日志输出到syslog，但这超出了本章的讨论范围。
- en: 'We still haven''t tested the error handling we set up with the `backend` instruction.
    Let''s remove one of the service instances and confirm that HAProxy handles failures
    correctly:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然没有测试我们通过`backend`指令设置的错误处理。让我们移除一个服务实例，并确认HAProxy是否能正确处理故障：
- en: '[PRE38]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We stopped one service instance and made several requests, and all of them worked
    properly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们停止了一个服务实例，并进行了几次请求，所有请求都正常工作。
- en: Without the possibility to include files into HAProxy configuration, our job
    was slightly more complicated. Not being able to log to stdout can be solved with
    syslog but will go astray from one of the containers best practices. There is
    a reason for this HAProxy behavior. Logging to stdout slows it down (noticeable
    only with an enormous number of requests). However, it would be better if that
    is left as our choice and maybe the default behavior, instead of not being supported
    at all. Finally, not being able to use the official HAProxy container might be
    considered a minor inconvenience. None of those problems are of great importance.
    We solved the lack of includes, could log into syslog and ended up using the container
    from `million12/haproxy` (we could also create our own that would extend from
    the official one).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无法将文件包含到HAProxy配置中，我们的工作稍微复杂了一些。无法将日志输出到stdout的问题可以通过syslog解决，但这会偏离容器最佳实践之一。HAProxy这种行为是有原因的。日志输出到stdout会拖慢它的速度（只有在大量请求时才会明显）。然而，如果可以将此作为我们的选择，甚至作为默认行为，而不是完全不支持，可能会更好。最后，无法使用官方的HAProxy容器可能被认为是一个小小的不便。这些问题并不重大。我们解决了缺少includes的问题，可以将日志输出到syslog，并最终使用了来自`million12/haproxy`的容器（我们也可以创建一个自己的容器，基于官方容器进行扩展）。
- en: Proxy Tools Compared
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理工具对比
- en: Apache, nginx and HAProxy are by no means the only solutions we could use. There
    are many projects available and making a choice is harder than ever.f the open
    source projects worth trying out is `lighttpd` (`pron. lighty`). Just like nginx
    and HAProxy, it was designed for security, speed, compliance, flexibility and
    high performance. It features a small memory footprint and efficient management
    of the CPU-load.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Apache、nginx和HAProxy绝不是我们可以使用的唯一解决方案。现在有许多项目可供选择，做出决策比以往任何时候都要困难。值得尝试的开源项目之一是`lighttpd`（发音为lighty）。与nginx和HAProxy一样，它也设计用于安全性、速度、合规性、灵活性和高性能。它具有小巧的内存占用和高效的CPU负载管理。
- en: If JavaScript is your language of preference, [node-http-proxy] could be a worthy
    candidate. Unlike other products we explored, node-http-proxy uses JavaScript
    code to define proxies and load balancing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果JavaScript是你喜欢的语言，[node-http-proxy]可能是一个值得考虑的候选工具。与我们探索的其他产品不同，node-http-proxy使用JavaScript代码来定义代理和负载均衡。
- en: The VulcanD is a project to keep an eye on. It is programmable proxy and load
    balancer backed by etcd. A similar process that we did with Consul Template and
    nginx/HAProxy is incorporated inside VulcanD. It can be combined with Sidekick
    to provide functionality similar to `check` arguments in nginx and HAProxy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: VulcanD是一个值得关注的项目。它是一个由etcd支持的可编程代理和负载均衡器。我们与Consul Template和nginx/HAProxy的相似过程也被VulcanD采用。它可以与Sidekick结合，提供类似于nginx和HAProxy中的`check`参数的功能。
- en: There are many similar projects available, and it is certain that new and existing
    ones are into making. We can expect more `unconventional` projects to appear that
    will combine proxy, load balancing, and service discovery in many different ways.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类似的项目可供选择，而且新旧项目都在不断制作中。我们可以预见到会有更多`非传统`项目出现，它们将在许多不同的方式中结合代理、负载均衡和服务发现。
- en: However, my choice, for now, stays with nginx or HAProxy. None of the other
    products we spoke about has anything to add and, in turn, each of them, at least,
    one deficiency.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前为止，我的选择依然是nginx或HAProxy。我们讨论的其他产品都没有什么可添加的优点，反而每个产品至少存在一个缺点。
- en: Apache is process based, making its performance when faced with a massive traffic
    less than desirable. At the same time, its resource usage skyrockets easily. If
    you need a server that will serve dynamic content, Apache is a great option, but
    should not be used as a proxy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Apache是基于进程的，这使得它在面对大量流量时的表现不太理想。与此同时，它的资源使用会迅速飙升。如果你需要一个可以提供动态内容的服务器，Apache是一个很好的选择，但不应作为代理使用。
- en: Lighttpd was promising when it appeared but faced many obstacles (memory leaks,
    CPU usage, and so on) that made part of its users switch to alternatives. The
    community maintaining it is much smaller than the one working on nginx and HAProxy.
    While it had its moment and many had high expectations from it, today it is not
    the recommended solution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Lighttpd刚推出时很有前景，但遇到了许多障碍（内存泄漏、CPU使用率过高等），导致部分用户转向其他替代品。它的维护社区远小于nginx和HAProxy的社区。尽管它曾经有过一段辉煌时光，许多人对它寄予了很高的期望，但如今它已不再是推荐的解决方案。
- en: What can be said about `node-http-proxy`? Even though it does not outperform
    nginx and HAProxy, it is very close. The major obstacle would be its programmable
    configuration that is not well suited for continuously changing proxies. If your
    language of choice is JavaScript and proxies should be relatively static, node-http-proxy
    is a valid option. However, it still doesn't provide any benefit over nginx and
    HAProxy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于`node-http-proxy`可以说些什么呢？尽管它不如nginx和HAProxy强大，但它非常接近。主要的障碍在于它的可编程配置，这对于需要频繁变化的代理并不太合适。如果你的编程语言选择是JavaScript，且代理需要相对静态，node-http-proxy是一个有效的选择。然而，它仍然没有提供比nginx和HAProxy更大的优势。
- en: VulcanD, in conjunction with Sidekick, is a project to keep an eye on, but it
    is not yet production ready (at least, not at the time this text was written).
    It is very unlikely that it will manage to outperform main players. The potential
    problem with VulcanD is that it is bundled with etcd. If that's what you're already
    using, great. On the other hand, if your choice fell to some other type of Registry
    (for example Consul or Zookeeper), there is nothing VulcanD can offer. I prefer
    keeping proxy and service discovery separated and put the glue between them myself.
    Real value VulcanD provides is in a new way of thinking that combines proxy service
    with service discovery, and it will probably be considered as one of the pioneers
    that opened the door for new types of proxy services.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: VulcanD与Sidekick配合使用是一个值得关注的项目，但它还没有达到生产环境的准备状态（至少在本文撰写时尚未就绪）。它很可能无法超越主要竞争者。VulcanD的潜在问题是它捆绑了etcd。如果你已经在使用etcd，那就太好了。另一方面，如果你选择了其他类型的注册表（例如Consul或Zookeeper），那么VulcanD就无法提供任何帮助。我更倾向于将代理和服务发现分开，并自己将它们连接起来。VulcanD真正提供的价值在于它结合代理服务和服务发现的新思路，它可能会被视为开辟新型代理服务的大门之一。
- en: That leaves us with nginx and HAProxy. If you spend some more time investigating
    opinions, you'll see that both camps have an enormous number of users defending
    one over the other. There are areas where nginx outperforms HAProxy and others
    where it underperforms. There are some features that HAProxy doesn't have and
    other missing in nginx. But, the truth is that both are battle-tested, both are
    an excellen solution, both have a huge number of users, and both are successfully
    used in companies that have colossal traffic. If what you're looking for is a
    proxy service with load balancing, you cannot go wrong with either of them.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，剩下的就是nginx和HAProxy了。如果你多花点时间调查各方意见，你会发现两边都有大量用户支持各自的优点。有些领域中nginx优于HAProxy，而另一些领域中则相反。HAProxy缺少某些功能，而nginx也有缺失。然而，事实是，二者都经过了严酷的实战检验，都是极好的解决方案，都有庞大的用户基础，且都在流量巨大的公司中成功使用。如果你正在寻找一个具有负载均衡功能的代理服务，那么选择它们任何一个都不会错。
- en: I am slightly more inclined towards nginx due to its better (official) Docker
    container (for example, it allows configuration reloads with a HUP signal), option
    to log to stdout and the ability to include configuration files. Excluding Docker
    container, HAProxy made the conscious decision not to support those features due
    to possible performance issues they can create. However, I prefer having the ability
    to choose when it's appropriate to use them and when it isn't. All those are truly
    preferences of no great importance and, in many cases, the choice is made depending
    on a particular use case one is trying to accomplish. However, there is one critical
    nginx feature that HAProxy does not support. HAProxy can drop traffic during reloads.
    If microservices architecture, continuous deployment, and blue-green processes
    are adopted, configuration reloads are very common. We can have several or even
    hundreds of reloads each day. No matter the reload frequency, with HAProxy there
    is a possibility of downtime.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍微倾向于使用nginx，因为它有更好的（官方）Docker容器（例如，它允许通过HUP信号重新加载配置）、能够将日志输出到stdout，并且支持包含配置文件。排除Docker容器外，HAProxy做出了有意识的决定，不支持这些功能，因为它们可能会带来性能问题。然而，我更喜欢在适当的时候能够选择使用这些功能，而在不适合的时候不使用。所有这些其实都只是些无关紧要的偏好，在许多情况下，选择依据是具体的使用场景。不过，有一个关键的nginx特性是HAProxy不支持的。HAProxy在重新加载时可能会丢失流量。如果采用微服务架构、持续部署和蓝绿部署过程，配置重新加载是非常常见的。我们每天可能会有几次甚至几百次重新加载。无论重新加载的频率如何，使用HAProxy时都可能会出现停机时间。
- en: We have to make a choice, and it falls to nginx. It will be out proxy of choice
    throughout the rest of the book.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须做出选择，而它最终选择了nginx。它将是本书余下部分中我们选择的代理。
- en: 'With that being said, let us destroy the VMs we used in this chapter and finish
    the implementation of the deployment pipeline. With service discovery and the
    proxy, we have everything we need:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们销毁本章中使用的虚拟机，并完成部署管道的实现。通过服务发现和代理，我们已经拥有了一切所需的工具：
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
