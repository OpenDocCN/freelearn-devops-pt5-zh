- en: '*Chapter 12*: Kubernetes Security and Compliance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：Kubernetes 安全性与合规性'
- en: In this chapter, you will learn about some of the key pieces of Kubernetes security.
    We'll discuss some recent Kubernetes security issues, and the finding of a recent
    audit that was performed on Kubernetes. Then, we'll look at implementing security
    at each level of our cluster, starting with the security of Kubernetes resources
    and their configurations, moving on to container security, and then finally, runtime
    security with intrusion detection. To start, we will discuss some key security
    concepts as they relate to Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，你将了解一些 Kubernetes 安全性的关键内容。我们将讨论一些近期的 Kubernetes 安全问题，以及对 Kubernetes 进行的最新审计发现。然后，我们将讨论如何在集群的各个层级实现安全性，从
    Kubernetes 资源及其配置的安全性开始，接着是容器安全，最后是通过入侵检测实现的运行时安全。首先，我们将讨论与 Kubernetes 相关的一些关键安全概念。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将讨论以下内容：
- en: Understanding security on Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 安全性
- en: Reviewing CVEs and security audits for Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查 Kubernetes 的 CVE 和安全审计
- en: Implementing tools for cluster configuration and container security
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现集群配置和容器安全的工具
- en: Handling intrusion detection, runtime security, and compliance on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上处理入侵检测、运行时安全性和合规性
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool, along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章节中详细介绍的命令，你需要一台支持 `kubectl` 命令行工具的计算机，以及一个正常运行的 Kubernetes 集群。请参阅 [*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与
    Kubernetes 通信*，了解几种快速启动 Kubernetes 的方法，以及如何安装 `kubectl` 工具的说明。
- en: Additionally, you will need a machine that supports the Helm CLI tool, which
    typically has the same prerequisites as `kubectl` – for details, check the Helm
    documentation at [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要一台支持 Helm CLI 工具的计算机，通常它的前提条件与 `kubectl` 相同—详细信息请查看 Helm 文档，地址为 [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中使用的代码可以在本书的 GitHub 仓库找到，地址为 [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12)。
- en: Understanding security on Kubernetes
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 安全性
- en: When discussing security on Kubernetes, it is very important to note security
    boundaries and shared responsibility. The *Shared Responsibility Model* is a common
    term used to describe how security is handled in public cloud services. It states
    that the customer is responsible for the security of their applications, and the
    security of their configuration of public cloud components and services. The public
    cloud provider, on the other hand, is responsible for the security of the services
    themselves as well as the infrastructure they run on, all the way to the data
    center and physical layer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论 Kubernetes 安全性时，特别需要注意安全边界和共享责任。*共享责任模型*是一个常用术语，用来描述公共云服务中如何处理安全问题。该模型指出，客户负责应用程序的安全性以及公共云组件和服务配置的安全性。而公共云提供商则负责服务本身的安全性，以及它们运行的基础设施的安全性，直到数据中心和物理层。
- en: Similarly, security on Kubernetes is shared. Though upstream Kubernetes is not
    a commercial product, the thousands of Kubernetes contributors and significant
    organizational heft from large tech companies ensure that the security of Kubernetes
    components is maintained. Additionally, the large ecosystem of individual contributors
    and companies using the technology ensures that it gets better as CVEs are reported
    and handled. Unfortunately, as we will discuss in the next section, the complexity
    of Kubernetes means that there are many possible attack vectors.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，Kubernetes 上的安全性也是共享的。虽然上游 Kubernetes 并不是一个商业产品，但成千上万的 Kubernetes 贡献者和大型科技公司背后的组织力量确保了
    Kubernetes 组件的安全性得以维护。此外，广泛的个人贡献者和公司使用该技术，确保了 Kubernetes 在 CVE 被报告和处理时不断改进。不幸的是，正如我们在下一节将讨论的那样，Kubernetes
    的复杂性意味着存在许多潜在的攻击途径。
- en: Applying the shared responsibility model then, as a developer you are responsible
    for the security of how you configure Kubernetes components, the security of the
    applications that you run on Kubernetes, and access-level security in your cluster
    configuration. While the security of your applications and containers themselves
    are not quite in scope for this book, they are definitely important to Kubernetes
    security. We will spend most of our time discussing configuration-level security,
    access security, and runtime security.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，应用共享责任模型，作为开发者，你需要负责如何配置 Kubernetes 组件的安全性、在 Kubernetes 上运行的应用程序的安全性，以及集群配置中的访问级别安全性。虽然应用程序和容器本身的安全性不在本书的讨论范围内，但它们对
    Kubernetes 安全性至关重要。我们将花费大部分时间讨论配置级别的安全性、访问安全性和运行时安全性。
- en: Either Kubernetes itself or the Kubernetes ecosystem provides tooling, libraries,
    and full-blown products to handle security at each of these levels – and we'll
    be reviewing some of these options in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 Kubernetes 本身还是 Kubernetes 生态系统，都提供了工具、库和完整的产品来处理各个层次的安全性——我们将在本章中回顾其中的一些选项。
- en: Now, before we discuss these solutions, it's best to start with a base understanding
    of why they may be needed in the first place. Let's move on to the next section,
    where we'll detail some issues that Kubernetes has encountered in the realm of
    security.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论这些解决方案之前，最好先从一个基本的理解出发，弄清楚它们为什么在一开始就可能是需要的。接下来，让我们进入下一部分，详细说明 Kubernetes
    在安全领域遇到的一些问题。
- en: Reviewing CVEs and security audits for Kubernetes
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查 Kubernetes 的 CVE 和安全审计
- en: Kubernetes has encountered several `kubernetes`. Each one of these is related
    either directly to Kubernetes, or to a common open source solution that runs on
    Kubernetes (like the NGINX ingress controller, for instance).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 遇到了多个 `kubernetes`。每个问题都直接或间接地与 Kubernetes 本身，或与在 Kubernetes 上运行的常见开源解决方案（例如
    NGINX Ingress 控制器）相关。
- en: Several of these were critical enough to require hotfixes to the Kubernetes
    source, and thus they list the affected versions in the CVE description. A full
    list of all CVEs related to Kubernetes can be found at [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes).
    To give you an idea of some of the issues that have been found, let's review a
    few of these CVEs in chronological order.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些问题严重到需要对 Kubernetes 源代码进行热修复，因此它们在 CVE 描述中列出了受影响的版本。所有与 Kubernetes 相关的 CVE
    完整列表可以在 [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)
    找到。为了让你了解一些已发现的问题，我们将按时间顺序回顾其中的一些 CVE。
- en: Understanding CVE-2016-1905 – Improper admission control
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 CVE-2016-1905 —— 不当的准入控制
- en: This CVE was one of the first major security issues with production Kubernetes.
    The National Vulnerability Database (a NIST website) gives this issue a base score
    of 7.7, putting it in the high-impact category.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 CVE 是 Kubernetes 生产环境中遇到的第一个重大安全问题之一。国家漏洞数据库（NVD，国家标准与技术研究院网站）给出了 7.7 的基础分数，将其归类为高影响级别。
- en: With this issue, a Kubernetes admission controller would not ensure that a `kubectl
    patch` command followed admission rules, allowing users to completely work around
    the admission controller – a nightmare in a multitenant scenario.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的关键在于，Kubernetes 的准入控制器未能确保 `kubectl patch` 命令遵循准入规则，从而允许用户完全绕过准入控制器——在多租户场景中，这是一个噩梦。
- en: Understanding CVE-2018-1002105 – Connection upgrading to the backend
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 CVE-2018-1002105 —— 后端连接升级
- en: This CVE was likely the most critical to date in the Kubernetes project. In
    fact, NVD gives it a 9.8 criticality score! In this CVE, it was found that it
    was possible in some versions of Kubernetes to piggyback on an error response
    from the Kubernetes API server and then upgrade the connection. Once the connection
    was upgraded, it was possible to send authenticated requests to any backend server
    in the cluster. This allowed a malicious user to essentially emulate a perfectly
    authenticated TLS request without proper credentials.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 CVE 可能是迄今为止 Kubernetes 项目中最为关键的一个。事实上，NVD 给它的严重性评分为 9.8！在这个 CVE 中，发现某些版本的
    Kubernetes 中，攻击者可以利用来自 Kubernetes API 服务器的错误响应，并升级连接。一旦连接被升级，就可以向集群中的任何后端服务器发送经过身份验证的请求。这使得恶意用户能够在没有适当凭据的情况下，基本模拟一个完全认证的
    TLS 请求。
- en: In addition to these CVEs (and likely partially driven by them), the CNCF sponsored
    a third-party security audit of Kubernetes in 2019\. The results of the audit
    are open source and publicly available and are worth a review.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些 CVE（并且可能部分由它们驱动），CNCF 在 2019 年资助了对 Kubernetes 的第三方安全审计。审计结果是开源的，并且可以公开访问，值得一读。
- en: Understanding the 2019 security audit results
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 2019 年的安全审计结果
- en: As we mentioned in the previous section, the 2019 Kubernetes security audit
    was performed by a third party, and the results of the audit are completely open
    source. The full audit report with all sections can be found at [https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/](https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一节提到的，2019 年的 Kubernetes 安全审计是由第三方进行的，审计结果是完全开源的。完整的审计报告可以在 [https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/](https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/)
    找到。
- en: 'In general, this audit focused on the following pieces of Kubernetes functionality:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这次审计关注了 Kubernetes 功能的以下几个方面：
- en: '`kube-apiserver`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`'
- en: '`etcd`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`'
- en: '`kube-scheduler`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`'
- en: '`kube-controller-manager`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`'
- en: '`cloud-controller-manager`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud-controller-manager`'
- en: '`kubelet`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubelet`'
- en: '`kube-proxy`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy`'
- en: The Container Runtime
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时
- en: The intent was to focus on the most important and relevant pieces of Kubernetes
    when it came to security. The results of the audit included not just a full security
    report, but also a threat model and a penetration test, as well as a whitepaper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 审计的目的是聚焦于 Kubernetes 中最重要和最相关的安全部分。审计结果不仅包括完整的安全报告，还包括威胁模型、渗透测试和白皮书。
- en: Diving deep into the audit results is not in the scope of this book, but there
    are some major takeaways that are great windows into the crux of many of the biggest
    Kubernetes security issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深入审查审计结果不在本书的范围之内，但有一些主要的结论可以为我们提供许多 Kubernetes 安全问题的核心窗口。
- en: In short, the audit found that since Kubernetes is a complex, highly networked
    system with many different settings, there are many possible configurations that
    inexperienced engineers may perform and in doing so, open their cluster to outside
    attackers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，审计发现，由于 Kubernetes 是一个复杂的、网络高度集成的系统，拥有许多不同的设置，缺乏经验的工程师可能会执行某些配置，从而使集群暴露于外部攻击者。
- en: This idea of Kubernetes being complex enough that an insecure configuration
    could happen easily is important to note and take to heart.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 足够复杂，以至于一个不安全的配置可能很容易发生，这一点很重要，需要牢记。
- en: The entire audit is worth a read – for those with significant knowledge of network
    security and containers, it is an excellent view of some of the security decisions
    that were made as part of the development of Kubernetes as a platform.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 整个审计报告值得阅读——对于那些具有丰富网络安全和容器知识的人来说，这是 Kubernetes 平台开发过程中做出的某些安全决策的绝佳视角。
- en: Now that we have discussed where Kubernetes security issues have been found,
    we can start looking into ways to increase the security posture of your clusters.
    Let's start with some default Kubernetes functionality for security.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了 Kubernetes 安全问题的发现，我们可以开始探讨如何提高集群的安全性。让我们从 Kubernetes 的一些默认安全功能开始。
- en: Implementing tools for cluster configuration and container security
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施集群配置和容器安全的工具
- en: 'Kubernetes gives us many inbuilt options for the security of cluster configurations
    and container permissions. Since we''ve already talked about RBAC, TLS Ingress,
    and encrypted Kubernetes Secrets, let''s discuss a few concepts that we haven''t
    had time to review yet: admission controllers, Pod security policies, and network
    policies.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 为集群配置和容器权限的安全性提供了许多内建选项。既然我们已经讨论过 RBAC、TLS Ingress 和加密的 Kubernetes
    Secrets，那么接下来我们将讨论一些尚未讨论的概念：入站控制器、Pod 安全策略和网络策略。
- en: Using admission controllers
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用入站控制器
- en: Admission controllers are an often overlooked but extremely important Kubernetes
    feature. Many of Kubernetes' advanced features use admission controllers under
    the hood. In addition, you can create new admission controller rules in order
    to add custom functionality to your cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 入站控制器是一个常常被忽视，但非常重要的 Kubernetes 特性。Kubernetes 的许多高级特性在背后使用了入站控制器。此外，你还可以创建新的入站控制器规则，为集群添加自定义功能。
- en: 'There are two general types of admission controllers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 入站控制器有两种主要类型：
- en: Mutating admission controllers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改入站控制器
- en: Validating admission controllers
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证准入控制器
- en: Mutating admission controllers take in Kubernetes resource specifications and
    return an updated resource specification. They also perform side-effect calculations
    or make external calls (in the case of custom admission controllers).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 变异准入控制器接收 Kubernetes 资源规范并返回更新后的资源规范。它们还执行副作用计算或进行外部调用（对于自定义准入控制器而言）。
- en: On the other hand, validating admission controllers simply accept or deny Kubernetes
    resource API requests. It is important to know that both types of controllers
    only act on create, update, delete, or proxy requests. These controllers cannot
    mutate or change requests to list resources.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，验证准入控制器仅仅接受或拒绝 Kubernetes 资源 API 请求。需要了解的是，这两种类型的控制器仅对创建、更新、删除或代理请求起作用。这些控制器不能变异或更改列出资源的请求。
- en: When a request of one of those types comes into the Kubernetes API server, it
    will first run the request through all the relevant mutating admission controllers.
    Then, the output, which may be mutated, will pass through the validating admission
    controllers, before finally being acted upon (or not, if the call is denied by
    an admission controller) in the API server.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当此类请求进入 Kubernetes API 服务器时，它会首先通过所有相关的变异准入控制器进行处理。然后，可能被变异的输出会通过验证准入控制器，最后在
    API 服务器中执行（如果被准入控制器拒绝，则不会执行）。
- en: 'Structurally, the Kubernetes-provided admission controllers are functions or
    "plugins," which run as part of the Kubernetes API server. They rely on two webhook
    controllers (which are admission controllers themselves, just special ones): **MutatingAdmissionWebhook**
    and **ValidatingAdmissionWebhook**. All other admission controllers use either
    one of these webhooks under the hood, depending on their type. In addition, any
    custom admission controllers you write can be attached to either one of these
    webhooks.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从结构上来看，Kubernetes 提供的准入控制器是作为 Kubernetes API 服务器的一部分运行的函数或“插件”。它们依赖于两个 webhook
    控制器（它们本身也是准入控制器，只不过是特殊的）：**MutatingAdmissionWebhook** 和 **ValidatingAdmissionWebhook**。所有其他准入控制器在内部使用这两个
    webhook 中的一个，具体取决于它们的类型。此外，您编写的任何自定义准入控制器都可以附加到这两个 webhook 中的任意一个。
- en: Before we look at the process of creating a custom admission controller, let's
    review a few of the default admission controllers that Kubernetes provides. For
    a full list, check the Kubernetes official documentation at [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探讨如何创建自定义准入控制器之前，让我们回顾一下 Kubernetes 提供的几个默认准入控制器。欲了解完整列表，请参考 Kubernetes 官方文档：[https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do)。
- en: Understanding default admission controllers
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解默认准入控制器
- en: There are quite a few default admission controllers present in a typical Kubernetes
    setup – many of which are required for some fairly important basic functionality.
    Here are some examples of default admission controllers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 Kubernetes 设置中，存在相当多的默认准入控制器，其中许多对于一些非常重要的基本功能是必需的。以下是一些默认准入控制器的示例。
- en: The NamespaceExists admission controller
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NamespaceExists 准入控制器
- en: The **NamespaceExists** admission controller checks any incoming Kubernetes
    resource (other than namespaces themselves). This is to check whether the namespace
    attached to the resource exists. If not, it denies the resource request at the
    admission controller level.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**NamespaceExists** 准入控制器检查任何传入的 Kubernetes 资源（除了命名空间本身）。这是为了检查资源所附加的命名空间是否存在。如果不存在，它会在准入控制器级别拒绝该资源请求。'
- en: The PodSecurityPolicy admission controller
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PodSecurityPolicy 准入控制器
- en: The **PodSecurityPolicy** admission controller supports Kubernetes Pod security
    policies, which we will learn about momentarily. This controller prevents resources
    that do not follow Pod security policies from being created.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**PodSecurityPolicy** 准入控制器支持 Kubernetes Pod 安全策略，我们稍后将了解该策略。此控制器防止不遵循 Pod
    安全策略的资源被创建。'
- en: In addition to the default admission controllers, we can create custom admission
    controllers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了默认的准入控制器，我们还可以创建自定义准入控制器。
- en: Creating custom admission controllers
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义准入控制器
- en: 'Creating a custom admission controller can be done dynamically using one of
    the two webhook controllers. The way this works is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义准入控制器可以通过动态使用两种 webhook 控制器之一来完成。其工作方式如下：
- en: You must write your own server or script that runs separately to the Kubernetes
    API server.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须编写自己的服务器或脚本，它需要独立于 Kubernetes API 服务器运行。
- en: Then, you configure one of the two previously mentioned webhook triggers to
    make a request with resource data to your custom server controller.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你需要配置前面提到的两种 webhook 触发器之一，向你的自定义服务器控制器发送包含资源数据的请求。
- en: Based on the result, the webhook controller will then tell the API server whether
    or not to proceed.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据结果，webhook 控制器会告知 API 服务器是否继续执行。
- en: 'Let''s start with the first step: writing a quick admission server.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始：编写一个简单的准入服务器。
- en: Writing a server for a custom admission controller
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自定义准入控制器的服务器
- en: To create our custom admission controller server (which will accept webhooks
    from the Kubernetes control plane), we can use any programming language. As with
    most extensions to Kubernetes, Go has the best support and libraries that make
    the task of writing a custom admission controller easier. For now, we will use
    some pseudocode.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的自定义准入控制器服务器（它将接收来自 Kubernetes 控制平面的 webhook），我们可以使用任何编程语言。与大多数 Kubernetes
    扩展一样，Go 语言拥有最好的支持和库，使得编写自定义准入控制器变得更加容易。现在，我们将使用一些伪代码。
- en: 'The control flow for our server will look something like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们服务器的控制流大致如下：
- en: Admission-controller-server.pseudo
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Admission-controller-server.pseudo
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have a simple server for our custom admission controller, we can
    configure a Kubernetes admission webhook to call it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个简单的服务器来处理我们的自定义准入控制器，我们可以配置一个 Kubernetes 准入 webhook 来调用它。
- en: Configuring Kubernetes to call a custom admission controller server
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 Kubernetes 调用自定义准入控制器服务器
- en: In order to tell Kubernetes to call our custom admission server, it needs a
    place to call. We can run our custom admission controller anywhere – it doesn't
    need to be on Kubernetes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 Kubernetes 调用我们的自定义准入服务器，它需要一个调用的地址。我们可以将自定义准入控制器部署在任何地方——它不必部署在 Kubernetes
    上。
- en: 'That being said, it''s easy for the purposes of this chapter to run it on Kubernetes.
    We won''t go through the full manifest, but let''s assume we have a Service and
    a Deployment that it is pointed at, running a container that is our server. The
    Service would look something like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在本章的目的下，在 Kubernetes 上运行它是很容易的。我们不会详细介绍完整的清单，但假设我们有一个 Service 和一个指向它的 Deployment，运行着一个容器作为我们的服务器。Service
    的配置大致如下：
- en: Service-webhook.yaml
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Service-webhook.yaml
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It's important to note that our server needs to use HTTPS in order for Kubernetes
    to accept webhook responses. There are many ways to configure this, and we won't
    get into it in this book. The certificate can be self-signed, but the common name
    of the certificate and CA needs to match the one used when setting up the Kubernetes
    cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们的服务器必须使用 HTTPS，以便 Kubernetes 能接受 webhook 响应。配置方式有很多种，我们在本书中不会深入讨论。证书可以是自签名的，但证书的通用名称和
    CA 必须与设置 Kubernetes 集群时使用的名称匹配。
- en: Now that we have our server running and accepting HTTPS requests, let's tell
    Kubernetes where to find it. To do this, we use `MutatingWebhookConfiguration`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的服务器已经在运行并接收 HTTPS 请求，让我们告诉 Kubernetes 在哪里可以找到它。为此，我们使用`MutatingWebhookConfiguration`。
- en: 'An example of `MutatingWebhookConfiguration` is shown in the following code
    block:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了 `MutatingWebhookConfiguration` 的示例：
- en: Mutating-webhook-config-service.yaml
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Mutating-webhook-config-service.yaml
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's pick apart the YAML for our `MutatingWebhookConfiguration`. As you can
    see, we can configure more than one webhook in this configuration – though we've
    only done one in this example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分析一下 `MutatingWebhookConfiguration` 的 YAML 配置。正如你所见，我们可以在这个配置中配置多个 webhook——尽管在这个示例中我们只配置了一个。
- en: For each webhook, we set `name`, `rules`, and a `configuration`. The `name`
    is simply the identifier for the webhook. The `rules` allow us to configure exactly
    in which cases Kubernetes should make a request to our admission controller. In
    this case, we have configured our webhook to fire whenever a `CREATE` event for
    resources of the types `pods`, `deployments`, and `configmaps` occurs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 webhook，我们设置`name`、`rules` 和 `configuration`。`name`只是 webhook 的标识符。`rules`
    让我们配置 Kubernetes 在哪些情况下应该向我们的准入控制器发起请求。在此案例中，我们已经将 webhook 配置为在 `pods`、`deployments`
    和 `configmaps` 类型的资源发生 `CREATE` 事件时触发。
- en: Finally, we have the `clientConfig`, where we specify exactly where and how
    Kubernetes should make the webhook request. Since we're running our custom server
    on Kubernetes, we specify the Service name as in the previous YAML, in addition
    to the path on our server to hit (`"/mutate"` is a best practice here), and the
    CA of the cluster to compare to the certificate of the HTTPS termination. If your
    custom admission server is running somewhere else, there are other possible configuration
    fields – check the docs if you need them ([https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`clientConfig`，在这里我们指定 Kubernetes 应该如何以及在何处发起 webhook 请求。由于我们在 Kubernetes
    上运行自定义服务器，因此除了指定服务器路径（`"/mutate"` 是此处的最佳实践）外，我们还需要指定服务名称，正如之前的 YAML 文件中所示，并提供集群的
    CA 以便与 HTTPS 终止证书进行比较。如果您的自定义准入服务器运行在其他地方，还有其他可能的配置字段——如果需要，请查阅文档（[https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)）。
- en: Once we create the `MutatingWebhookConfiguration` in Kubernetes, it is easy
    to test the validation. All we need to do is create a Pod, Deployment, or ConfigMap
    as normal, and check whether our requests are denied or patched according to the
    logic in our server.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在 Kubernetes 中创建了 `MutatingWebhookConfiguration`，测试验证就变得很容易。我们需要做的就是像往常一样创建一个
    Pod、Deployment 或 ConfigMap，并检查我们的请求是否根据服务器中的逻辑被拒绝或修补。
- en: Let's assume for now that our server is set to deny any Pod with a name that
    includes the string `deny-me`. It is also set up to add an error response to the
    `AdmissionReviewResponse`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的服务器目前被设置为拒绝任何名称中包含字符串 `deny-me` 的 Pod。它还设置了一个错误响应到 `AdmissionReviewResponse`。
- en: 'Let''s use a Pod spec as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用如下的 Pod 规格：
- en: To-deny-pod.yaml
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: To-deny-pod.yaml
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can create our Pod to check the admission controller. We can use the
    following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建 Pod 来检查准入控制器。我们可以使用以下命令：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And that's it! Our custom admission controller has successfully denied a Pod
    that doesn't match the conditions we specified in our server. For resources that
    are patched (not denied, but altered), `kubectl` will not show any special response.
    You will need to fetch the resource in question to see the patch in action.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们的自定义准入控制器成功地拒绝了一个与我们服务器中指定条件不匹配的 Pod。对于被修补的资源（不是被拒绝，而是被修改），`kubectl` 不会显示任何特殊响应。您需要获取相关资源，查看补丁的效果。
- en: Now that we've explored custom admission controllers, let's look at another
    way to impose cluster security practices – Pod security policies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了自定义准入控制器，让我们看看另一种实施集群安全实践的方法——Pod 安全策略。
- en: Enabling Pod security policies
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用 Pod 安全策略
- en: The basics of Pod security policies are that they allow a cluster administrator
    to create rules that Pods must follow in order to be scheduled onto a node. Technically,
    Pod security policies are just another type of admission controller. However,
    this feature is officially supported by Kubernetes and is worth an in-depth discussion,
    since many options are available.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 安全策略的基本原理是，它们允许集群管理员创建规则，要求 Pods 必须遵循这些规则才能被调度到节点上。从技术上讲，Pod 安全策略仅是另一种类型的准入控制器。然而，这一功能已被
    Kubernetes 官方支持，值得深入讨论，因为有许多可用的选项。
- en: Pod security policies can be used to prevent Pods from running as root, put
    limits on ports and volumes used, restrict privilege escalation, and much more.
    We will review a subset of Pod security policy capabilities now, but for a full
    list of Pod security policy configuration types, check the official PSP documentation
    at [https://kubernetes.io/docs/concepts/policy/pod-security-policy/](https://kubernetes.io/docs/concepts/policy/pod-security-policy/).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 安全策略可以用来防止 Pods 以 root 身份运行，限制使用的端口和卷，限制特权提升等。我们现在将回顾 Pod 安全策略的一部分功能，但要查看完整的
    Pod 安全策略配置类型列表，请查阅官方的 PSP 文档 [https://kubernetes.io/docs/concepts/policy/pod-security-policy/](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)。
- en: As a final note, Kubernetes also supports low-level primitives for controlling
    container permissions – namely *AppArmor*, *SELinux*, and *Seccomp*. These configurations
    are outside the scope of this book, but they can be useful for highly secure environments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要注意的是，Kubernetes 还支持用于控制容器权限的低级原语——即 *AppArmor*、*SELinux* 和 *Seccomp*。这些配置超出了本书的范围，但它们在高安全性环境中可能会非常有用。
- en: Steps to create a Pod security policy
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 Pod 安全策略的步骤
- en: 'There are several steps to implementing Pod security policies:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实现Pod安全策略的步骤如下：
- en: First, the Pod security policy admission controller must be enabled.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，必须启用Pod安全策略准入控制器。
- en: This will prevent all Pods from being created in your cluster since it requires
    a matched Pod security policy and role to be able to create a Pod. You will likely
    want to create your Pod security policies and roles before enabling the admission
    controller for this reason.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将阻止在集群中创建所有Pod，因为它要求匹配的Pod安全策略和角色才能创建Pod。由于这个原因，您可能希望在启用准入控制器之前先创建Pod安全策略和角色。
- en: After the admission controller is enabled, the policy itself must be created.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用准入控制器后，必须创建相应的策略。
- en: Then, a `Role` or `ClusterRole` object must be created with access to the Pod
    security policy.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，必须创建一个`Role`或`ClusterRole`对象，并授予其访问Pod安全策略的权限。
- en: Finally, that role can be bound with a `accountService` account, allowing Pods
    created with that service account to use permissions available to the Pod security
    policy.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，可以将该角色与`accountService`账户绑定，从而允许使用该服务账户创建的Pod使用Pod安全策略中可用的权限。
- en: In some cases, you may not have the Pod security policy admission controller
    enabled by default on your cluster. Let's look at how to enable it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您的集群可能默认未启用Pod安全策略准入控制器。让我们来看一下如何启用它。
- en: Enabling the Pod security policy admission controller
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用Pod安全策略准入控制器
- en: In order to enable the PSP admission controller, the `kube-apiserver` must be
    started with a flag that specifies admission controllers to start with. On managed
    Kubernetes (EKS, AKS, and others), the PSP admission controller will likely be
    enabled by default, along with a privileged Pod security policy created for use
    by the initial admin user. This prevents the PSP from causing any issues with
    creating Pods in the new cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用PSP准入控制器，`kube-apiserver`必须使用一个标志启动，该标志指定要启动的准入控制器。在托管Kubernetes（如EKS、AKS等）上，PSP准入控制器可能会默认启用，同时为初始管理员用户创建了一个特权Pod安全策略。这可以防止PSP在新集群中创建Pod时引发任何问题。
- en: 'If you''re self-managing Kubernetes and you haven''t yet enabled the PSP admission
    controller, you can do so by restarting the `kube-apiserver` component with the
    following flags:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是自行管理Kubernetes并且尚未启用PSP准入控制器，您可以通过以下标志重新启动`kube-apiserver`组件来启用它：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If your Kubernetes API server is run using a `systemd` file (as it would be
    if following *Kubernetes: The Hard Way*), you should update the flags there instead.
    Typically, `systemd` files are placed in the `/etc/systemd/system/` folder.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您的Kubernetes API服务器是通过`systemd`文件运行的（如果按照*Kubernetes: The Hard Way*的方式进行部署），则应在此文件中更新标志。通常，`systemd`文件位于`/etc/systemd/system/`文件夹中。'
- en: 'In order to find out which admission plugins are already enabled, you can run
    the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出哪些准入插件已经启用，您可以运行以下命令：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This command will present a long list of admission plugins that are enabled.
    For instance, you will see the following admission plugins in the output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将列出启用的所有准入插件。例如，您将在输出中看到以下准入插件：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we are sure the PSP admission controller is enabled, we can actually
    create a PSP.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确认PSP准入控制器已启用，我们实际上可以创建一个PSP。
- en: Creating the PSP resource
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建PSP资源
- en: 'Pod security policies themselves can be created using typical Kubernetes resource
    YAML. Here''s a YAML file for a privileged Pod security policy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全策略本身可以使用典型的Kubernetes资源YAML文件创建。下面是一个特权Pod安全策略的YAML文件：
- en: Privileged-psp.yaml
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: privileged-psp.yaml
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This Pod security policy allows the user or service account (via a `PodSecurityPolicy`
    would be able to bind to the host network on ports `2000`-`65535`, run as any
    user, and bind to any volume type. In addition, we have an annotation for a `seccomp`
    restriction on `allowedProfileNames` – to give you an idea of how `Seccomp` and
    `AppArmor` annotations work with `PodSecurityPolicies`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此Pod安全策略允许用户或服务账户（通过`PodSecurityPolicy`）绑定主机网络上的`2000`至`65535`端口，可以以任何用户身份运行，并且可以绑定任何类型的卷。此外，我们还为`allowedProfileNames`添加了一个`seccomp`限制注释—以帮助您理解`Seccomp`和`AppArmor`注释如何与`PodSecurityPolicies`配合使用。
- en: As we mentioned previously, just creating the PSP does nothing. For any service
    account or user that will be creating privileged Pods, we need to give them access
    to the Pod security policy via a `ClusterRole` and `ClusterRoleBinding`).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，仅创建PSP并不会做任何事情。对于任何将创建特权Pod的服务账户或用户，我们需要通过`ClusterRole`和`ClusterRoleBinding`为他们提供对Pod安全策略的访问权限。
- en: 'In order to create a `ClusterRole` that has access to this PSP, we can use
    the following YAML:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个可以访问这个 PSP 的 `ClusterRole`，我们可以使用以下 YAML：
- en: Privileged-clusterrole.yaml
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Privileged-clusterrole.yaml
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can bind our newly created `ClusterRole` to the user or service account
    with which we intend to create privileged Pods. Let''s do this with a `ClusterRoleBinding`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将新创建的 `ClusterRole` 绑定到我们打算用来创建特权 Pod 的用户或服务账户。我们可以通过 `ClusterRoleBinding`
    来实现：
- en: Privileged-clusterrolebinding.yaml
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Privileged-clusterrolebinding.yaml
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In our case, we want to let every authenticated user on the cluster create privileged
    Pods, so we bind to the `system:authenticated` group.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们希望让集群中的每个经过身份验证的用户都能创建特权 Pod，因此我们将其绑定到 `system:authenticated` 组。
- en: Now, it is likely that we do not want all our users or Pods to be privileged.
    A more realistic Pod security policy places restrictions on what Pods are capable
    of.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可能我们不希望所有用户或 Pod 都具备特权。一个更现实的 Pod 安全策略是对 Pod 能做的事情进行限制。
- en: 'Let''s take a look at some example YAML of a PSP that has these restrictions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下一个示例 YAML，它展示了具有以下限制的 PSP：
- en: unprivileged-psp.yaml
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: unprivileged-psp.yaml
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can tell, this Pod security policy is vastly different in the restrictions
    it imposes on created Pods. No Pods under this policy are allowed to run as root
    or escalate to root. They also have restrictions on the types of volumes they
    can bind to (this section has been highlighted in the preceding code snippet)
    – and they cannot use host networking or bind directly to host ports.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这个 Pod 安全策略在对创建的 Pod 施加的限制方面与其他策略有很大不同。根据该策略，没有 Pod 被允许以 root 身份运行或提升为
    root 权限。它们在绑定卷类型上也有所限制（这一部分在前面的代码片段中已被突出显示）——并且它们不能使用主机网络或直接绑定到主机端口。
- en: In this YAML, both the `runAsUser` and `supplementalGroups` sections control
    the Linux user ID and group IDs that can run or be added by the container, while
    the `fsGroup` key controls the filesystem groups that can be used by the container.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 YAML 中，`runAsUser` 和 `supplementalGroups` 部分控制可以由容器运行或添加的 Linux 用户 ID 和组
    ID，而 `fsGroup` 键控制容器可以使用的文件系统组。
- en: In addition to using rules like `MustRunAsNonRoot`, it is possible to directly
    specify which user ID a container can run with – and any Pods not running specifically
    with that ID in their spec will not be able to schedule onto a Node.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用像 `MustRunAsNonRoot` 这样的规则之外，还可以直接指定容器可以运行的用户 ID——任何未在规范中明确指定该 ID 的 Pod
    将无法调度到节点上。
- en: 'For a sample PSP that restricts users to a specific ID, look at the following
    YAML:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个限制用户为特定 ID 的示例 PSP，请参考以下 YAML：
- en: Specific-user-id-psp.yaml
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Specific-user-id-psp.yaml
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This Pod security policy, when applied, will prevent any Pods from running as
    user ID `0` or `3001`, or higher. In order to create a Pod that satisfies this
    condition, we use the `runAs` option in the `securityContext` in a Pod spec.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Pod 安全策略应用后，将防止任何 Pod 以用户 ID `0` 或 `3001` 或更高的 ID 运行。为了创建符合这一条件的 Pod，我们在
    Pod 规范中的 `securityContext` 中使用 `runAs` 选项。
- en: 'Here is an example Pod that satisfies this constraint and would be successfully
    scheduled even with this Pod security policy in place:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个满足此限制的 Pod 示例，即使在启用该 Pod 安全策略的情况下，它也会成功调度：
- en: Specific-user-pod.yaml
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Specific-user-pod.yaml
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, in this YAML, we give our Pod a specific user to run with, ID
    `1000`. We also disallowed our Pod from escalating to root. This Pod spec would
    successfully schedule even when `specific-user-psp` is in place.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这个 YAML 中，我们为 Pod 指定了一个特定的用户 ID `1000`。我们还禁止 Pod 提升为 root 权限。即使在 `specific-user-psp`
    策略生效的情况下，这个 Pod 规范也能成功调度。
- en: Now that we've discussed how Pod security policies can secure Kubernetes by
    placing restrictions on how a Pod runs, we can move onto network policies, where
    we can restrict how Pods network.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何通过对 Pod 运行的限制来保障 Kubernetes 的安全，接下来我们可以讨论网络策略，利用它我们可以限制 Pod 之间的网络通信。
- en: Using network policies
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用网络策略
- en: Network policies in Kubernetes work similar to firewall rules or route tables.
    They allow users to specify a group of Pods via a selector and then determine
    how and where those Pods can communicate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的网络策略与防火墙规则或路由表类似。它们允许用户通过选择器指定一组 Pod，并确定这些 Pod 如何以及在哪里进行通信。
- en: For network policies to work, your chosen Kubernetes network plugin (such as,
    *Weave*, *Flannel*, or *Calico*) must support the network policy spec. Network
    policies can be created as all other Kubernetes resources are – through a YAML
    file. Let's start with a very simple network policy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使网络策略生效，您选择的Kubernetes网络插件（如*Weave*、*Flannel*或*Calico*）必须支持网络策略规格。网络策略可以像所有其他Kubernetes资源一样通过YAML文件创建。我们从一个非常简单的网络策略开始。
- en: 'Here is a network policy spec that restricts access to Pods with the label
    `app=server`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个限制访问标签为`app=server`的Pod的网络策略规格：
- en: Label-restriction-policy.yaml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Label-restriction-policy.yaml
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, let's pick apart this network policy YAML since it will help us explain
    some more complicated network policies as we progress.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解析这个网络策略YAML，因为它将帮助我们在接下来的内容中解释一些更复杂的网络策略。
- en: First, in our spec, we have a `podSelector`, which works similarly to node selectors
    in functionality. Here, we are using `matchLabels` to specify that this network
    policy will only affect Pods with the label `app=server`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在我们的规格中，我们有一个`podSelector`，其功能类似于节点选择器。这里，我们使用`matchLabels`来指定此网络策略将只影响标签为`app=server`的Pod。
- en: 'Next, we specify a policy type for our network policy. There are two policy
    types: `ingress` and `egress`. A network policy can specify one or both types.
    `ingress` refers to making network rules that come into effect for connections
    to the matched Pods, and `egress` refers to network rules that come into effect
    for connections leaving the matched Pods.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为网络策略指定一个策略类型。有两种策略类型：`ingress`和`egress`。一个网络策略可以指定一个或两个类型。`ingress`指的是为连接到匹配的Pod的流量设置网络规则，而`egress`指的是为离开匹配Pod的连接设置网络规则。
- en: 'In this specific network policy, we are simply dictating a single `ingress`
    rule: the only traffic that will be accepted by Pods with the label `app=server`
    is traffic that originates from Pods with the label `app:frontend`. Additionally,
    the only port that will accept traffic on Pods with the label `app=server` is
    `80`.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的网络策略中，我们仅定义了一个`ingress`规则：只有来自标签为`app:frontend`的Pod的流量才会被标签为`app=server`的Pod接受。此外，标签为`app=server`的Pod上唯一会接受流量的端口是`80`。
- en: There can be multiple `from` blocks in an `ingress` policy set that correspond
    to multiple traffic rules. Similarly, with `egress`, there can be multiple `to`
    blocks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ingress`策略集中可以有多个`from`块，它们对应多个流量规则。类似地，在`egress`中，也可以有多个`to`块。
- en: It is important to note that network policies work by namespace. By default,
    if there isn't a single network policy in a namespace, there are no restrictions
    on Pod-to-Pod communication within that namespace. However, as soon as a specific
    Pod is selected by a single network policy, all traffic to and from that Pod must
    explicitly match a network policy rule. If it doesn't match a rule, it will be
    blocked.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，网络策略是按命名空间工作的。默认情况下，如果一个命名空间中没有任何网络策略，则该命名空间内的Pod之间没有限制通信。然而，一旦某个特定Pod被单个网络策略选中，所有进出该Pod的流量必须明确匹配网络策略规则。如果不匹配规则，则会被阻止。
- en: 'With this in mind, we can easily create policies that enforce broad restrictions
    on Pod networking. Let''s take a look at the following network policy:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个考虑，我们可以轻松创建强制执行对Pod网络的广泛限制的策略。让我们来看一下以下网络策略：
- en: Full-restriction-policy.yaml
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Full-restriction-policy.yaml
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this `NetworkPolicy`, we specify that we will be including both an `Ingress`
    and `Egress` policy, but we don't write a block for either of them. This has the
    effect of automatically denying any traffic for both `Egress` and `Ingress` since
    there are no rules for traffic to match against.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`NetworkPolicy`中，我们指定将同时包含`Ingress`和`Egress`策略，但我们没有为它们编写任何块。这会导致自动拒绝任何`Egress`和`Ingress`流量，因为没有规则可以匹配流量。
- en: Additionally, our `{}` Pod selector value corresponds to selecting every Pod
    in the namespace. The end result of this rule is that every Pod in the `development`
    namespace will not be able to accept ingress traffic or send egress traffic.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的`{}` Pod选择器值对应于选择命名空间中的每个Pod。此规则的最终结果是，`development`命名空间中的每个Pod都无法接受`ingress`流量或发送`egress`流量。
- en: Important note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is also important to note that network policies are interpreted by combining
    all the separate network policies that affect a Pod and then applying the combination
    of all those rules to Pod traffic.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，网络策略是通过将所有影响Pod的单独网络策略结合起来进行解释，然后将所有这些规则的组合应用于Pod流量。
- en: This means that even though we have restricted all ingress and egress traffic
    in the `development` namespace in our preceding example, we can still enable it
    for specific Pods by adding another network policy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，即使我们在前面的例子中已经限制了`development`命名空间中所有的进出流量，我们仍然可以通过添加另一个网络策略来为特定Pod启用流量。
- en: Let's assume that now our `development` namespace has complete traffic restriction
    for Pods, we want to allow a subset of Pods to receive network traffic on port
    `443` and send traffic on port `6379` to a database Pod. In order to do this,
    we simply need to create a new network policy that, by the additive nature of
    policies, allows this traffic.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在我们的`development`命名空间对Pod之间的流量进行了完全限制，我们希望允许一部分Pod在`443`端口接收网络流量，并在`6379`端口向数据库Pod发送流量。为了实现这一点，我们只需要创建一个新的网络策略，通过策略的累加特性，允许这些流量。
- en: 'This is what the network policy looks like:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是网络策略的样子：
- en: Override-restriction-network-policy.yaml
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Override-restriction-network-policy.yaml
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this network policy, we are allowing our server Pods in the `development`
    namespace to receive traffic from frontend Pods on port `443` and send traffic
    to database Pods on port `6379`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络策略中，我们允许`development`命名空间中的服务器Pod接收来自前端Pod的`443`端口流量，并将流量发送到数据库Pod的`6379`端口。
- en: 'If instead, we wanted to open up all Pod-to-Pod communication without any restrictions,
    while still actually instituting a network policy, we could do so with the following
    YAML:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要开放所有Pod之间的通信，而没有任何限制，同时仍然实际实施一个网络策略，我们可以使用以下的YAML配置：
- en: All-open-network-policy.yaml
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: All-open-network-policy.yaml
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we have discussed how we can use network policies to set rules on Pod-to-Pod
    traffic. However, it is also possible to use network policies as an external-facing
    firewall of sorts. To do this, we create network policy rules based not on Pods
    as origin or destination, but external IPs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何使用网络策略来设置Pod之间流量的规则。然而，网络策略也可以作为外部防火墙使用。为此，我们需要创建基于外部IP地址的网络策略规则，而不是基于Pod的来源或目的地。
- en: 'Let''s look at an example network policy where we are restricting communication
    to and from a Pod, with a specific IP range as the target:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，网络策略限制了一个Pod的进出通信，并且指定了一个特定的IP范围作为目标：
- en: External-ip-network-policy.yaml
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: External-ip-network-policy.yaml
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this network policy, we are specifying a single `Ingress` rule and a single
    `Egress` rule. Each of these rules accepts or denies traffic based not on which
    Pod it is coming from but on the source IP of the network request.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络策略中，我们指定了一个`Ingress`规则和一个`Egress`规则。每个规则基于网络请求的源IP地址来决定是否接受或拒绝流量，而不是基于流量来自哪个Pod。
- en: In our case, we have selected a `/16` subnet mask range (with a specified `/24`
    CIDR exception) for both our `Ingress` and `Egress` rules. This has the side effect
    of preventing any traffic from within our cluster from reaching these Pods since
    none of our Pod IPs will match the rules in a default cluster networking setup.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们为`Ingress`和`Egress`规则选择了一个`/16`子网掩码范围（并指定了一个`/24`CIDR例外）。这会导致任何来自集群内的流量无法到达这些Pod，因为在默认的集群网络设置中，我们的Pod
    IP地址都不会与这些规则匹配。
- en: However, traffic from outside the cluster in the specified subnet mask (and
    not in the exception range) will be able to both send traffic to the `worker`
    Pods and also be able to accept traffic from the `worker` Pods.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，来自指定子网掩码内（而不在例外范围内）集群外部的流量将能够向`worker` Pod发送流量，并且能够接收来自`worker` Pod的流量。
- en: With the end of our discussion on network policies, we can move onto a completely
    different layer of the security stack – runtime security and intrusion detection.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论完网络策略之后，我们可以转到安全堆栈的完全不同层次——运行时安全性和入侵检测。
- en: Handling intrusion detection, runtime security, and compliance on Kubernetes
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理Kubernetes中的入侵检测、运行时安全性和合规性
- en: Once you have set your Pod security policies and network policies – and generally
    ensured that your configuration is as watertight as possible – there are still
    many attack vectors that are possible in Kubernetes. In this section, we will
    focus on attacks from within a Kubernetes cluster. Even with highly specific Pod
    security policies in place (which definitely do help, to be clear), it is possible
    for containers and applications running in your cluster to perform unexpected
    or malicious operations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了 Pod 安全策略和网络策略，并且基本确保你的配置尽可能地严密，Kubernetes 中仍然存在许多潜在的攻击途径。在这一部分，我们将重点讨论来自
    Kubernetes 集群内部的攻击。即使你设置了高度特定的 Pod 安全策略（这无疑会有所帮助，明确一点），集群中运行的容器和应用程序仍然可能执行意外或恶意操作。
- en: In order to solve this problem, many professionals look to runtime security
    tools, which allow constant monitoring and alerting of application processes.
    For Kubernetes, a popular open source tool that can accomplish this is *Falco*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，许多专业人员依赖于运行时安全工具，这些工具可以持续监控并警报应用进程。对于 Kubernetes，一个常用的开源工具是 *Falco*。
- en: Installing Falco
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Falco
- en: Falco bills itself as a *behavioral activity monitor* for processes on Kubernetes.
    It can monitor both your containerized applications running on Kubernetes as well
    as the Kubernetes components themselves.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Falco 将自己定义为 Kubernetes 上进程的 *行为活动监控器*。它可以监控运行在 Kubernetes 上的容器化应用程序以及 Kubernetes
    组件本身。
- en: How does Falco work? In real time, Falco parses system calls from the Linux
    kernel. It then filters these system calls through rules – which are sets of configurations
    that can be applied to the Falco engine. Whenever a rule is broken by a system
    call, Falco triggers an alert. It's that simple!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Falco 是如何工作的？在实时情况下，Falco 解析来自 Linux 内核的系统调用。然后，它通过规则对这些系统调用进行过滤——这些规则是可以应用于
    Falco 引擎的一组配置。每当一个系统调用违反规则时，Falco 就会触发警报。就是这么简单！
- en: Falco ships with an extensive set of default rules that add significant observability
    at the kernel level. Custom rules are of course supported by Falco – and we will
    show you how to write them.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Falco 默认带有一套详尽的规则，这些规则提供了内核级别的显著可观察性。Falco 当然支持自定义规则——我们将展示如何编写这些规则。
- en: First, however, we need to install Falco on our cluster! Luckily, Falco can
    be installed using Helm. However, it is very important to note that there are
    a few different ways to install Falco, and they differ significantly in how effective
    they can be in the event of a breach.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在此之前，我们需要先在集群中安装 Falco！幸运的是，Falco 可以通过 Helm 安装。不过，需要特别注意的是，安装 Falco 有几种不同的方法，在发生安全漏洞时，不同的方法效果会有显著差异。
- en: We're going to be installing Falco using the Helm chart, which is simple and
    works well for managed Kubernetes clusters, or any scenario where you may not
    have direct access to the worker nodes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Helm 图表来安装 Falco，这种方式简单并且适用于管理型 Kubernetes 集群，或任何可能无法直接访问工作节点的场景。
- en: However, for the best possible security posture, Falco should be installed directly
    onto the Kubernetes nodes at the Linux level. The Helm chart, which uses a DaemonSet
    is great for ease of use but is inherently not as secure as a direct Falco installation.
    To install Falco directly to your nodes, check the installation instructions at
    [https://falco.org/docs/installation/](https://falco.org/docs/installation/).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了获得最佳的安全防护，应该将 Falco 直接安装到 Linux 层的 Kubernetes 节点上。使用 DaemonSet 的 Helm 图表非常便捷，但从安全角度来看，直接安装
    Falco 比通过 Helm 图表安装要更为安全。要将 Falco 直接安装到你的节点，请查看安装说明：[https://falco.org/docs/installation/](https://falco.org/docs/installation/)。
- en: 'With that caveat out of the way, we can install Falco using Helm:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述说明之后，我们可以使用 Helm 安装 Falco：
- en: 'First, we need to add the `falcosecurity` repo to our local Helm:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将 `falcosecurity` 仓库添加到本地 Helm 中：
- en: '[PRE20]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we can proceed with actually installing Falco using Helm.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们可以继续使用 Helm 安装 Falco。
- en: Important note
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Falco Helm chart has many possible variables that can be changed in the
    values file – for a full review of those, you can check the official Helm chart
    repo at [https://github.com/falcosecurity/charts/tree/master/falco](https://github.com/falcosecurity/charts/tree/master/falco).
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Falco Helm 图表有许多可以在 values 文件中更改的变量 – 如果要查看完整的配置项，可以查看官方的 Helm 图表仓库：[https://github.com/falcosecurity/charts/tree/master/falco](https://github.com/falcosecurity/charts/tree/master/falco)。
- en: 'To install Falco, run the following:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装 Falco，请运行以下命令：
- en: '[PRE21]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This command will install Falco using the default values, which you can see
    at [https://github.com/falcosecurity/charts/blob/master/falco/values.yaml](https://github.com/falcosecurity/charts/blob/master/falco/values.yaml).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将使用默认值安装 Falco，默认值可以在 [https://github.com/falcosecurity/charts/blob/master/falco/values.yaml](https://github.com/falcosecurity/charts/blob/master/falco/values.yaml)
    查看。
- en: Next, let's dive into what Falco offers a security-conscious Kubernetes administrator.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解 Falco 能为注重安全的 Kubernetes 管理员提供哪些功能。
- en: Understanding Falco's capabilities
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 Falco 的功能
- en: As mentioned previously, Falco ships with a set of default rules, but we can
    easily add more rules using new YAML files. Since we're using the Helm version
    of Falco, passing custom rules to Falco is as simple as either creating a new
    values file or editing the default one with custom rules.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Falco 自带一套默认规则，但我们可以通过新的 YAML 文件轻松添加更多规则。由于我们使用的是 Helm 版本的 Falco，向 Falco
    传递自定义规则只需创建一个新的 values 文件或编辑默认文件并添加自定义规则即可。
- en: 'Adding custom rules looks like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 添加自定义规则如下所示：
- en: Custom-falco.yaml
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-falco.yaml
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now is a good time to discuss the structure of a Falco rule. To illustrate,
    let's borrow a few lines of rules from the `Default` Falco ruleset that ships
    with the Falco Helm chart.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是讨论 Falco 规则结构的好时机。为说明这一点，让我们借用 Falco Helm Chart 中随附的 `Default` Falco 规则集中的几行规则。
- en: When specifying Falco configuration in YAML, we can use three different types
    of keys to help compose our rules. These are macros, lists, and rules themselves.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定 Falco 配置时，我们可以使用三种不同类型的键来帮助编写规则。这些键分别是宏、列表和规则本身。
- en: The specific rule we're looking at in this example is called `Launch Privileged
    Container`. This rule will detect when a privileged container has been started
    and log some information about the container to `STDOUT`. Rules can do all sorts
    of things when it comes to alerts, but logging to `STDOUT` is a good way to increase
    observability when high-risk events happen.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个示例中查看的特定规则叫做 `Launch Privileged Container`。这个规则将检测何时启动一个特权容器，并将容器的一些信息记录到
    `STDOUT`。规则可以在警报方面执行各种操作，但记录到 `STDOUT` 是当发生高风险事件时提高可观察性的好方法。
- en: 'First, let''s look at the rule entry itself. This rule uses a few helper entries,
    several macros, and lists – but we''ll get to those in a second:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看规则条目本身。这个规则使用了一些辅助条目、几个宏和列表——稍后我们会详细介绍这些内容：
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, a Falco rule has several parts. First, we have the rule name
    and description. Then, we specify the triggering condition for the rule – which
    acts as a filter for Linux system calls. If a system call matches all the logic
    filters in the `condition` block, the rule is triggered.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Falco 规则有几个部分。首先，我们有规则的名称和描述。接下来，我们指定规则的触发条件——这充当 Linux 系统调用的过滤器。如果某个系统调用符合
    `condition` 块中的所有逻辑过滤条件，则触发该规则。
- en: When a rule is triggered, the output key allows us to set a format for how the
    text of the output appears. The `priority` key lets us assign a priority, which
    can be one of `emergency`, `alert`, `critical`, `error`, `warning`, `notice`,
    `informational`, and `debug`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当规则被触发时，`output` 键允许我们设置输出文本的显示格式。`priority` 键让我们为规则指定优先级，优先级可以是 `emergency`、`alert`、`critical`、`error`、`warning`、`notice`、`informational`
    或 `debug` 之一。
- en: Finally, the `tags` key applies tags to the rule in question, making it easier
    to categorize rules. This is especially important when using alerts that aren't
    simply plain text `STDOUT` entries.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`tags` 键为相关规则添加标签，从而使规则分类更容易。这在使用并非单纯文本的 `STDOUT` 警报时尤其重要。
- en: The syntax for `condition` is especially important here, and we will focus on
    how this system of filtering works.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`condition` 的语法在这里尤为重要，我们将重点讲解这一过滤系统的工作原理。'
- en: First off, since the filters are essentially logical statements, you will see
    some familiar syntax (if you have ever programmed or written pseudocode) – and,
    and not, and so on. This syntax is pretty simple to learn, and a full discussion
    of it – the *Sysdig* filter syntax – can be found at [https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering](https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于过滤器本质上是逻辑语句，如果你曾经编写过程序或伪代码，你会看到一些熟悉的语法——例如 if、and、not 等。这些语法非常简单易学，关于 *Sysdig*
    过滤器语法的详细讨论可以在 [https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering](https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering)
    中找到。
- en: As a note, the Falco open source project was originally created by *Sysdig*,
    which is why it uses the common *Sysdig* filter syntax.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Falco 开源项目最初是由*Sysdig*创建的，这也是为什么它使用了常见的*Sysdig*过滤器语法。
- en: Next, you will see reference to `container_started` and `container`, as well
    as `falco_privileged_containers` and `user_privileged_containers`. These are not
    plain strings but the use of macros – references to other blocks in the YAML that
    specify additional functionality, and generally make it much easier to write rules
    without repeating a lot of configuration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你会看到对`container_started`和`container`的引用，以及`falco_privileged_containers`和`user_privileged_containers`。这些并非普通字符串，而是宏的使用
    —— 引用 YAML 中的其他块来指定附加功能，通常使得编写规则更加简便，无需重复大量配置。
- en: 'To see how this rule really works, let''s look at a full reference for all
    the macros that were referenced in the preceding rule:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个规则的工作原理，我们来看一下前面规则中引用的所有宏的完整参考：
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will see in the preceding YAML that each macro is really just a reusable
    block of `Sysdig` filter syntax, often using other macros to accomplish the rule
    functionality. Lists, not pictured here, are like macros except that they do not
    describe filter logic. Instead, they include a list of string values that can
    be used as part of a comparison using the filter syntax.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到前面的 YAML 中，每个宏实际上只是一个可重用的`Sysdig`过滤器语法块，通常使用其他宏来实现规则功能。列表（此处未显示）类似于宏，但它们不描述过滤逻辑。相反，它们包含一组字符串值，可以作为比较的一部分，使用过滤语法进行处理。
- en: 'For instance, `(``trusted_images)` in the `falco_privileged_containers` macro
    references a list called `trusted_images`. Here''s the source for that list:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`(``trusted_images)` 在 `falco_privileged_containers` 宏中引用了一个名为`trusted_images`的列表。以下是该列表的来源：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, this particular list is empty in the default rules, but a custom
    ruleset could use a list of trusted images in this list, which would then automatically
    be consumed by all the other macros and rules that use the `trusted_image` list
    as part of their filter rules.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，默认规则中这个特定的列表是空的，但自定义规则集可以在此列表中使用受信任的镜像列表，随后所有使用`trusted_image`列表作为过滤规则的一部分的宏和规则都会自动使用该列表。
- en: As mentioned previously, in addition to tracking Linux system calls, Falco can
    also track Kubernetes control plane events as of Falco v0.13.0.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，除了跟踪 Linux 系统调用外，Falco 从 v0.13.0 版本开始还可以跟踪 Kubernetes 控制平面事件。
- en: Understanding Kubernetes audit event rules in Falco
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Falco 中的 Kubernetes 审计事件规则
- en: 'Structurally, these Kubernetes audit event rules work the same way as Falco''s
    Linux system call rules. Here''s an example of one of the default Kubernetes rules
    in Falco:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 结构上，这些 Kubernetes 审计事件规则与 Falco 的 Linux 系统调用规则工作方式相同。以下是 Falco 中默认 Kubernetes
    规则的一个示例：
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This rule acts on Kubernetes audit events in Falco (essentially, control plane
    events) to alert when a Pod is created that isn't on the list `allowed_k8s_containers`.
    The default `k8s` audit rules contain many similar rules, most of which output
    formatted logs when triggered.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则作用于 Falco 中的 Kubernetes 审计事件（本质上是控制平面事件），当创建一个不在`allowed_k8s_containers`列表中的
    Pod 时，触发警报。默认的`k8s`审计规则包含许多类似的规则，其中大多数在触发时会输出格式化的日志。
- en: 'Now, we talked about Pod security policies a bit earlier in this chapter –
    and you may be seeing some similarities between PSPs and Falco Kubernetes audit
    event rules. For instance, take this entry from the default Kubernetes Falco rules:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本章早些时候我们提到过 Pod 安全策略 —— 你可能会发现 PSP 和 Falco Kubernetes 审计事件规则之间有一些相似之处。例如，来看一下默认的
    Kubernetes Falco 规则中的这条记录：
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This rule, which is triggered when a Pod is attempting to start using the host
    network, maps directly to host network PSP settings.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pod 尝试使用主机网络启动时，这条规则会被触发，直接映射到主机网络的 PSP 设置。
- en: Falco capitalizes on this similarity by letting us use Falco as a way to `trial`
    new Pod security policies without applying them cluster-wide and causing issues
    with running Pods.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Falco 利用这种相似性，让我们可以通过使用 Falco 来`试用`新的 Pod 安全策略，而无需将其应用于整个集群，从而避免影响正在运行的 Pod。
- en: For this purpose, `falcoctl` (the Falco command-line tool) comes with the `convert
    psp` command. This command takes in a Pod security policy definition and turns
    it into a set of Falco rules. These Falco rules will just output logs to `STDOUT`
    when triggered (instead of causing Pod scheduling failures like a PSP mismatch),
    which makes it much easier to test out new Pod security policies in an existing
    cluster.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，`falcoctl`（Falco 的命令行工具）提供了`convert psp`命令。该命令接受一个 Pod 安全策略定义，并将其转化为一组 Falco
    规则。这些 Falco 规则在触发时仅会将日志输出到`STDOUT`（而不是像 PSP 不匹配时导致 Pod 调度失败），这使得在现有集群中测试新的 Pod
    安全策略变得更加容易。
- en: To learn how to use the `falcoctl` conversion tool, check out the official Falco
    documentation at [https://falco.org/docs/psp-support/](https://falco.org/docs/psp-support/).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习如何使用 `falcoctl` 转换工具，请参阅官方的 Falco 文档：[https://falco.org/docs/psp-support/](https://falco.org/docs/psp-support/)。
- en: Now that we have a good grounding on the Falco tool, let's discuss how it can
    be used to implement compliance controls and runtime security.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Falco 工具有了基本了解，让我们讨论它如何用于实施合规性控制和运行时安全性。
- en: Mapping Falco to compliance and runtime security use cases
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Falco 应用于合规性和运行时安全的使用案例
- en: Because of its extensibility and ability to audit low-level Linux system calls,
    Falco is a great tool for continuous compliance and runtime security.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其可扩展性以及能够审计低级 Linux 系统调用，Falco 是一个非常适合持续合规性和运行时安全性的工具。
- en: On the compliance side, it is possible to leverage Falco rulesets that map specifically
    to the requirements of a compliance standard – for instance, PCI or HIPAA. This
    allows users to quickly detect and act on any processes that do not comply with
    the standard in question. There are open and closed source Falco rulesets for
    several standards.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在合规性方面，可以利用专门映射到合规性标准要求的 Falco 规则集——例如 PCI 或 HIPAA。这使得用户能够迅速检测并对任何不符合相关标准的进程采取行动。有多个标准的开源和闭源
    Falco 规则集可供使用。
- en: Similarly, for runtime security, Falco exposes an alerting/eventing system,
    which means that any runtime events that trigger an alert can also trigger automated
    intervention and remediation processes. This can work for both security and compliance.
    As an example, if a Pod triggers a Falco alert for non-compliance, a process can
    work off that alert and delete the offending Pod immediately.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于运行时安全性，Falco 提供了一个警报/事件系统，这意味着任何触发警报的运行时事件也可以触发自动干预和修复过程。这对安全性和合规性都适用。举个例子，如果一个
    Pod 触发了 Falco 警报，表示不符合规定，则可以根据该警报执行一个流程，并立即删除违规的 Pod。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about security in the context of Kubernetes. First,
    we reviewed the basics of security on Kubernetes – which layers of the security
    stack are relevant to our cluster and some broad strokes of how to manage that
    complexity. Next, we learned about some of the major security issues that Kubernetes
    has encountered, as well as discussing the results of the 2019 security audit.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 Kubernetes 环境中的安全性。首先，我们回顾了 Kubernetes 安全的基础知识——哪些安全层与我们的集群相关，以及如何管理这一复杂性的一些大致方法。接着，我们了解了
    Kubernetes 遇到的一些主要安全问题，并讨论了 2019 年安全审计的结果。
- en: Then, we implemented security at two different levels of the stack in Kubernetes
    – first, in configuration with Pod security policies and network policies, and
    finally, runtime security with Falco.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在 Kubernetes 的两层堆栈中实施了安全性——首先，在配置中使用 Pod 安全策略和网络策略，最后，通过 Falco 实现运行时安全性。
- en: In the next chapter, we will learn how to make Kubernetes your own by building
    custom resources. This will allow you to add significant new functionality to
    your cluster.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何通过构建自定义资源来使 Kubernetes 适应自己的需求。这将使你能够为集群添加重要的新功能。
- en: Questions
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the names of the two webhook controllers that a custom admission controller
    can use?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义准入控制器可以使用的两个 webhook 控制器的名称是什么？
- en: What effect does a blank `NetworkPolicy` for ingress have?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空白的 `NetworkPolicy` 对入站流量有什么影响？
- en: What sort of Kubernetes control plane events would be valuable to track in order
    to prevent attackers from altering Pod functionality?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了防止攻击者更改 Pod 功能，跟踪哪些类型的 Kubernetes 控制平面事件是有价值的？
- en: Further reading
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Kubernetes CVE Database: [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes CVE 数据库：[https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)
