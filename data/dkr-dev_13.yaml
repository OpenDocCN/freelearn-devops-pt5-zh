- en: '*Chapter 11*: Scaling and Load Testing Docker Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：扩展和负载测试 Docker 应用'
- en: Technology giants such as Google, Facebook, Lyft, and Amazon use container orchestration
    systems in part so that they can run their massive computing resources at very
    high levels of utilization. To do that, you must have a way to scale your applications
    across a fleet of servers, which might be dynamically allocated from a cloud provider.
    Even if you have a cluster that can scale out with high traffic and scale back
    in when demand subsides, you may still need additional tools to make sure it operates
    correctly. You also need to ensure that the service degrades gracefully if capacity
    limits are exceeded.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Google、Facebook、Lyft 和 Amazon 这样的科技巨头部分采用容器编排系统，是为了能够以非常高的利用率运行他们的海量计算资源。为了做到这一点，你必须有一种方法将你的应用扩展到一批服务器上，这些服务器可能是从云服务提供商动态分配的。即使你有一个能够在高流量时扩展、在需求减少时缩减的集群，你仍然可能需要额外的工具来确保其正确运行。你还需要确保如果超出容量限制，服务能够平稳降级。
- en: You can use a service mesh such as Envoy, Istio, or Linkerd to handle those
    concerns. Envoy is one of the simpler options in the service mesh arena; it provides
    both load balancing and advanced traffic routing and filtering capabilities. All
    these capabilities provide the glue needed to serve traffic to demanding users.
    Some of the more complex service meshes use Envoy as a building block since it
    is so flexible.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用像 Envoy、Istio 或 Linkerd 这样的服务网格来处理这些问题。Envoy 是服务网格领域中较为简单的选项之一；它提供了负载均衡以及先进的流量路由和过滤能力。所有这些能力提供了所需的粘合剂，以便为高需求的用户提供流量。一些更复杂的服务网格使用
    Envoy 作为构建块，因为它非常灵活。
- en: To prove that the scaling strategy works, you need to perform load testing.
    To do this, we will use k6.io, a cloud-native load testing and API testing tool.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明扩展策略有效，你需要进行负载测试。为此，我们将使用 k6.io，一个云原生负载测试和 API 测试工具。
- en: In this chapter, you are going to learn how to use the Horizontal Pod Autoscaler,
    the Vertical Pod Autoscaler, and the Cluster Autoscaler to configure your Kubernetes
    cluster so that it scales out. You will learn about Envoy and why you might use
    it to provide a proxy layer and service mesh on top of Kubernetes. This includes
    how to create an Envoy service mesh on top of a Kubernetes cluster, as well as
    how to configure it with a circuit breaker. Then, you will learn how to verify
    that the service mesh and autoscaler mechanisms are working as expected. Finally,
    you will learn how to run a load test with k6.io and observe how the service fails
    when subjected to a stress test.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用 Horizontal Pod Autoscaler（水平 Pod 自动扩展器）、Vertical Pod Autoscaler（垂直
    Pod 自动扩展器）和 Cluster Autoscaler（集群自动扩展器）来配置你的 Kubernetes 集群，以便实现扩展。你将了解 Envoy 以及为什么你可能需要使用它来提供一个代理层和服务网格，搭建在
    Kubernetes 之上。这包括如何在 Kubernetes 集群之上创建一个 Envoy 服务网格，以及如何为其配置断路器。接下来，你将学习如何验证服务网格和自动扩展机制是否按预期工作。最后，你将学习如何使用
    k6.io 进行负载测试，并观察当服务面临压力测试时如何失败。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下内容：
- en: Scaling your Kubernetes cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展你的 Kubernetes 集群
- en: What is Envoy, and why might I need it?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Envoy，为什么我可能需要它？
- en: Testing scalability and performance with k6
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 k6 测试可扩展性和性能
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to have both a local Kubernetes learning environment and a working
    Kubernetes cluster in the cloud, as set up in [*Chapter 8*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157),
    *Deploying Docker Apps to Kubernetes*. You will also need to have a current version
    of the AWS CLI, as well as `kubectl` and `helm` 3.x installed on your local workstation,
    as described in the previous chapter. The Helm commands in this chapter use `helm`
    3.x syntax.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要拥有一个本地 Kubernetes 学习环境以及一个可用的云端 Kubernetes 集群，如在 [*第8章*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157)
    中所述，*将 Docker 应用部署到 Kubernetes*。你还需要在本地工作站上安装当前版本的 AWS CLI、`kubectl` 和 `helm`
    3.x，正如前一章所描述的那样。本章中的 Helm 命令使用的是 `helm` 3.x 语法。
- en: For your local Kubernetes learning environment, you should have a working NGINX
    Ingress Controller configured, which you can install by running the `chapter11/bin/`[deploy-nginx-ingress.sh](http://deploy-nginx-ingress.sh)
    script. You should also have a local Jaeger operator, which you can install by
    running the `chapter11/bin/deploy-jaeger.sh` script.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地 Kubernetes 学习环境，你应该配置一个可用的 NGINX Ingress Controller，你可以通过运行 `chapter11/bin/`[deploy-nginx-ingress.sh](http://deploy-nginx-ingress.sh)
    脚本来安装它。你还需要有一个本地 Jaeger 操作符，你可以通过运行 `chapter11/bin/deploy-jaeger.sh` 脚本来安装它。
- en: For the cloud-hosted cluster, you can reuse the AWS `eksctl`. The EKS cluster
    must have a working ALB Ingress Controller set up. You should also have an `deploy-jaeger.sh`
    script against your cloud cluster as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于云托管的集群，您可以重用 AWS 的`eksctl`。EKS 集群必须已设置并且工作正常的 ALB Ingress Controller。你还应该拥有一个用于云集群的
    `deploy-jaeger.sh` 脚本。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码如何运行：
- en: '[https://bit.ly/2CwdZeo](https://bit.ly/2CwdZeo)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/2CwdZeo](https://bit.ly/2CwdZeo)'
- en: Using the updated ShipIt Clicker v8
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用更新后的 ShipIt Clicker v8
- en: 'We will use the version of ShipIt Clicker provided in the `chapter11` directory
    of the following GitHub repository: [https://github.com/PacktPublishing/Docker-for-Developers/](https://github.com/PacktPublishing/Docker-for-Developers/).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下 GitHub 仓库中 `chapter11` 目录提供的 ShipIt Clicker 版本：[https://github.com/PacktPublishing/Docker-for-Developers/](https://github.com/PacktPublishing/Docker-for-Developers/)。
- en: This version of the application you use, similar to what we did in the previous
    chapter, depends on an externally installed version of Redis from the `bitnami/redis`
    Helm Charts when used in Kubernetes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用的应用程序版本与上一章中的操作类似，在 Kubernetes 中使用时，依赖于通过 `bitnami/redis` Helm Charts 安装的外部
    Redis 版本。
- en: Understanding the differences from the previous version of ShipIt Clicker
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解与之前版本 ShipIt Clicker 的区别
- en: In each chapter, we have made enhancements to ShipIt Clicker to illustrate changes
    related to the chapter content, as well as to polish the application the same
    way we would do as part of a production release process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一章中，我们都对 ShipIt Clicker 进行了增强，以展示与章节内容相关的更改，并且像生产发布过程中的修复一样完善应用程序。
- en: This version of ShipIt Clicker is similar to the one provided in the previous
    chapter, but it has one more API endpoint called `/faults/spin` that's used as
    a part of a *fault injection* testing strategy to induce CPU load on the nodes
    running the application, in order to test cluster autoscaling strategies. The
    `spin` endpoint will get slower the more frequently it is called but will recover
    and get faster if calls subside. This simulates the way that an application with
    poor performance behaves, without having to devise a complicated real set of poorly
    performing code and database servers. It provides an artificial CPU load that
    is convenient for testing CPU-based autoscaling. See the code in `chapter11/src/server/common/spin.js`
    and `chapter11/src/server/controllers/faults/controller.js` to see how this works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的 ShipIt Clicker 与上一章中提供的版本类似，但它新增了一个名为`/faults/spin`的 API 端点，作为*故障注入*测试策略的一部分，用于在运行应用程序的节点上诱发
    CPU 负载，以测试集群自动扩缩容策略。`spin`端点在调用频率增加时会变慢，但如果调用频率减少，它会恢复并变快。这模拟了一个性能较差的应用程序的行为，无需编写复杂的实际低效代码和数据库服务器。它提供了一种人工的
    CPU 负载，便于进行基于 CPU 的自动扩缩容测试。请查看`chapter11/src/server/common/spin.js`和`chapter11/src/server/controllers/faults/controller.js`中的代码，了解其工作原理。
- en: 'This version of ShipIt Clicker also has an enhancement related to Prometheus
    metrics: it exposes these metrics on a separate port by configuring Express to
    listen on a separate port so that it serves up the `/metrics` endpoint. This helps
    us avoid exposing metrics that contain information about the application that
    an ordinary user does not need and makes it possible for multiple containers in
    the same pod as ShipIt Clicker to also expose Prometheus metrics. See the code
    in the `chapter11/src/server/index.js` file, which adds another HTTP listener
    and a router for metrics. The Helm templates in `chapter11/shipitclicker/templates/deployment.yaml`
    also have changes to support this new endpoint.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的 ShipIt Clicker 还增强了与 Prometheus 指标相关的功能：通过配置 Express 监听一个独立的端口，使其暴露这些指标，并提供
    `/metrics` 端点。这样有助于避免暴露包含应用程序信息的指标，普通用户不需要这些信息，并且使得在与 ShipIt Clicker 同一个 Pod 中的多个容器也能暴露
    Prometheus 指标。请查看 `chapter11/src/server/index.js` 文件中的代码，它增加了另一个 HTTP 监听器和一个用于指标的路由。`chapter11/shipitclicker/templates/deployment.yaml`
    中的 Helm 模板也进行了更改，以支持这个新端点。
- en: Next, we'll build and install ShipIt Clicker into our local Kubernetes learning
    environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把 ShipIt Clicker 构建并安装到本地 Kubernetes 学习环境中。
- en: Installing the latest version of ShipIt Clicker locally
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在本地安装 ShipIt Clicker 的最新版本
- en: 'In this section, we will build the ShipIt Clicker Docker container, tag it,
    and push it to Docker Hub, as we did in previous chapters. Issue the following
    commands, replacing `dockerfordevelopers` with your Docker Hub username:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建 ShipIt Clicker Docker 容器，给它打标签，并像前几章一样将其推送到 Docker Hub。执行以下命令，将 `dockerfordevelopers`
    替换为你的 Docker Hub 用户名：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inspect the running pods and services using `kubectl get all` to verify the
    pod is running, note its name, and then inspect the logs with `kubectl logs` to
    see the startup logs. There should be no errors in the log.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl get all` 检查正在运行的 pod 和服务，以验证 pod 是否正在运行，记下其名称，然后使用 `kubectl logs`
    检查日志以查看启动日志。日志中不应有任何错误。
- en: Next, we'll install this version in EKS.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 EKS 中安装此版本。
- en: Installing the latest version of ShipIt Clicker in EKS through ECR
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 ECR 在 EKS 中安装 ShipIt Clicker 的最新版本
- en: 'Now that you have built the Docker containers and installed this locally, we''ll
    install it in AWS EKS via ECR. Edit `chapter11/values.yaml` to give this a hostname
    in the Route 53 DNS zone such as [shipit-v8.eks.example.com](http://shipit-v8.eks.example.com)
    (replace the ECR reference with the one corresponding to your AWS account and
    region and replace `example.com` with your domain name):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了 Docker 容器并在本地安装了它，我们将在 AWS EKS 中通过 ECR 安装它。编辑 `chapter11/values.yaml`
    文件，为其在 Route 53 DNS 区域中设置一个主机名，例如 [shipit-v8.eks.example.com](http://shipit-v8.eks.example.com)（将
    ECR 引用替换为与你的 AWS 账户和区域对应的引用，并将 `example.com` 替换为你的域名）：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Inspect the Kubernetes logs to make sure that the application has deployed
    cleanly to the cluster:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Kubernetes 日志，确保应用程序已成功部署到集群中：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If all is well with the deployment, get the AWS ALB Ingress Controller's address,
    as described in [*Chapter 9*](B11641_09_Final_NM_ePub.xhtml#_idTextAnchor191),
    *Cloud-Native Continuous Deployment Using Spinnaker*, and create DNS entries in
    the Route 53 console for the deployed application with the ALB address. You should
    then be able to reach your application at a URL similar to [https://shipit-v8.eks.example.com/](https://shipit-v8.eks.example.com/)
    (replace `example.com` with your domain name).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果部署一切正常，获取 AWS ALB Ingress 控制器的地址，如 [*第9章*](B11641_09_Final_NM_ePub.xhtml#_idTextAnchor191)
    中所述，*使用 Spinnaker 的云原生持续部署*，并在 Route 53 控制台中为已部署的应用程序创建 DNS 条目，使用 ALB 地址。然后你应该能够通过类似
    [https://shipit-v8.eks.example.com/](https://shipit-v8.eks.example.com/) 的 URL
    访问你的应用程序（将 `example.com` 替换为你的域名）。
- en: Scaling your Kubernetes cluster
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展你的 Kubernetes 集群
- en: 'To support more traffic and more applications, your Kubernetes cluster may
    need to grow beyond its initial size. You can use both manual methods and dynamic
    programmed methods to do this, especially if you are working with a cloud-based
    Kubernetes cluster. To scale out an application, you need to control two dimensions:
    the number of pods running a particular application and the number of nodes in
    a cluster. You can''t scale the number of pods infinitely on a cluster with the
    same number of nodes; practical limits related to CPU, memory, and network concerns
    will ultimately demand that the cluster scales out the number of nodes as well.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持更多的流量和更多的应用程序，你的 Kubernetes 集群可能需要扩大其初始规模。你可以使用手动方法和动态编程方法来实现这一点，特别是当你在使用基于云的
    Kubernetes 集群时。要扩展一个应用程序，你需要控制两个维度：运行特定应用程序的 pod 数量和集群中的节点数量。在具有相同节点数量的集群中，你不能无限扩展
    pod 数量；与 CPU、内存和网络相关的实际限制最终会要求集群也要扩展节点数量。
- en: 'The method that''s used to scale out a cluster will vary considerably, depending
    on the cloud vendor and Kubernetes distribution. The Kubernetes documentation
    explains both the general process and some specific instructions for clusters
    running in the Google and Microsoft Azure clouds:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展集群的方式会有很大差异，具体取决于云服务提供商和 Kubernetes 分发版。Kubernetes 文档解释了通用流程，并提供了一些针对在 Google
    和 Microsoft Azure 云中运行的集群的具体说明：
- en: '[https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/](https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/](https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/)'
- en: 'Generally speaking, you must start and configure a new server that is set up
    similarly to the existing cluster nodes, and then join it to the cluster by using
    the `kubeadm join` command:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你必须启动并配置一台新服务器，该服务器的配置应与现有集群节点类似，然后通过 `kubeadm join` 命令将其加入集群：
- en: '[https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/)'
- en: Kubernetes distributions and cloud vendors make this easier by relying on mechanisms
    such as machine images and autoscaling groups. We will show you how to scale your
    cluster by using Amazon EKS. In [*Chapter 8*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157),
    *Deploying Docker Apps to Kubernetes*, we set up EKS with AWS Quick Start CloudFormation
    templates in the *Spinning up AWS EKS with CloudFormation* section. The following
    sections assume that you have used that method to set up a cluster that uses autoscaling
    groups.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 发行版和云服务提供商通过依赖机器镜像和自动扩展组等机制使这一过程变得更加简便。我们将向您展示如何通过 Amazon EKS 扩展您的集群。在
    [*第 8 章*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157)，*将 Docker 应用程序部署到 Kubernetes*
    中，我们在 *使用 CloudFormation 启动 AWS EKS* 部分中设置了 EKS 与 AWS Quick Start CloudFormation
    模板。以下部分假设您已经使用该方法设置了一个使用自动扩展组的集群。
- en: Scaling the cluster manually
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动扩展集群
- en: 'Given that we want to increase the number of nodes in our cluster, we will
    want to identify and follow the procedures that are specific to our Kubernetes
    installation. For Amazon EKS clusters, see the following documentation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们想要增加集群中的节点数量，我们将需要识别并遵循针对我们的 Kubernetes 安装的特定程序。对于 Amazon EKS 集群，请参阅以下文档：
- en: '[https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html](https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html](https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html)'
- en: You could just launch an entirely new group of nodes, but you can often adjust
    a parameter or two in order to increase the size of your cluster. This is done
    when you increase the size of a cluster, which is called *scaling out*, and when
    you decrease the size of a cluster, which is called *scaling in*. Next, we will
    learn how to adjust a simple parameter so that we can scale out the number of
    nodes in the cluster.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以启动一个全新的节点组，但通常只需要调整一个或两个参数就能增加集群的大小。这种方法用于扩大集群的规模，称为 *扩展*，而当缩小集群的规模时，称为
    *缩减*。接下来，我们将学习如何调整一个简单的参数，以便扩展集群中的节点数量。
- en: Scaling nodes out manually
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动扩展节点
- en: 'For the sake of simplicity, let''s assume you used the AWS Quick Start for
    EKS CloudFormation templates to create your cluster initially. Since that uses
    CloudFormation to manage the cluster, you should prefer using CloudFormation to
    update the cluster''s configuration. To manually scale your cluster out, go to
    the AWS console and update the CloudFormation deployment, changing the default
    values for **Number of nodes** and **Maximum number of nodes** from their current
    values to higher values, such as **4** and **8**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们假设您最初使用了 AWS Quick Start 的 EKS CloudFormation 模板来创建您的集群。由于该方法使用 CloudFormation
    来管理集群，因此您应该优先使用 CloudFormation 来更新集群的配置。要手动扩展您的集群，请进入 AWS 控制台，更新 CloudFormation
    部署，将 **节点数量** 和 **最大节点数量** 的默认值从当前值修改为更高的值，如 **4** 和 **8**：
- en: '![](img/B11641_11_001.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_11_001.jpg)'
- en: Figure 11.1 – Updating the AWS EKS Quick Start CloudFormation template
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 更新 AWS EKS Quick Start CloudFormation 模板
- en: Continue through the CloudFormation update forms and apply the changes. Look
    at the CloudFormation events for updates and wait a few minutes. You can then
    check that the update to the CloudFormation template worked fine. Then, you can
    check the size of the autoscaling group to make sure it has grown.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 继续完成 CloudFormation 更新表单并应用更改。查看 CloudFormation 事件以获取更新，并等待几分钟。然后，您可以检查 CloudFormation
    模板的更新是否顺利完成。接着，您可以检查自动扩展组的大小，确保其已经增加。
- en: 'You could also update the autoscaling group sizes through the EC2 console,
    thereby setting the minimum, desired, and maximum number of nodes to **4**, **4**,
    and **8**, respectively. This will cause your deployed configuration to drift
    from its CloudFormation templates, however, which is undesirable as the actual
    state will no longer match the model that CloudFormation expects. See the following
    post for more on why that is problematic: [https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/](https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过 EC2 控制台更新自动扩展组的大小，分别将最小、期望和最大节点数设置为 **4**、**4** 和 **8**。但是，这会导致您的部署配置与
    CloudFormation 模板产生偏差，这是不理想的，因为实际状态将与 CloudFormation 期望的模型不再匹配。有关此问题的更多信息，请参见以下帖子：[https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/](https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/)。
- en: If you used `eksctl` to create your cluster instead, you can follow the instructions
    at [https://eksctl.io/usage/managing-nodegroups/](https://eksctl.io/usage/managing-nodegroups/)
    to scale the node groups it creates.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 `eksctl` 创建了集群，你可以按照 [https://eksctl.io/usage/managing-nodegroups/](https://eksctl.io/usage/managing-nodegroups/)
    中的说明扩展其创建的节点组。
- en: Scaling nodes in manually
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动扩展节点
- en: 'You can reverse the process to scale in the cluster (reducing its size), but
    beware that scaling a cluster in manually is trickier. Doing this safely involves
    a process called draining, which is described in the following Kubernetes documentation:
    [https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/).
    Just changing the autoscaling group''s size on its own will terminate an instance
    without letting you choose which instance to terminate or giving you a chance
    to drain the instance. If you *really* wanted to do this, you would have to do
    all the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以逆向操作来缩减集群（减小其规模），但请注意，手动缩减集群规模更加复杂。安全地进行此操作需要一个叫做“驱逐”的过程，详情请参阅以下 Kubernetes
    文档：[https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)。仅更改自动扩展组的大小会终止实例，而不允许你选择终止哪个实例或给你机会去驱逐该实例。如果你*真的*想这样做，你必须完成以下所有步骤：
- en: Decrement the autoscaling group minimum size by one.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自动扩展组的最小规模减少一个。
- en: Drain the node with `kubectl drain`.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `kubectl drain` 命令驱逐节点。
- en: Terminate the node using an AWS CLI command that decrements the desired capacity.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS CLI 命令终止节点，该命令会减少期望容量。
- en: 'After you''ve adjusted the autoscaling group''s minimum size, you could issue
    the following commands (replace the node name and instance ID in each of these
    commands with the ones that match the node you want to terminate):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 调整完自动扩展组的最小规模后，你可以执行以下命令（在每个命令中替换为要终止的节点的节点名称和实例 ID）：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This process is involved and could easily lead to manual error. It will also
    lead to configuration drift from the CloudFormation template, so you should either
    seek to script it or rely on automatic scaling mechanisms instead.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程涉及较多，容易导致手动错误。它还可能导致与 CloudFormation 模板的配置漂移，因此你应该考虑编写脚本，或者依赖自动扩展机制。
- en: Scaling pods manually through deployments
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过部署手动扩展 Pod
- en: 'Manually scaling the number of pods in a deployment or ReplicaSet is quite
    easy, assuming that you have enough resources in your cluster. You can use the
    `kubectl scale` command to set the number of replicas. You might have to issue
    several `kubectl get` commands before you see all the replicas become ready, as
    shown in this transcript:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 手动扩展部署或 ReplicaSet 中的 Pod 数量相对简单，只要集群中有足够的资源。你可以使用 `kubectl scale` 命令来设置副本数量。你可能需要执行几个
    `kubectl get` 命令，直到你看到所有副本都变为就绪状态，如以下转录所示：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we will examine how we can apply programmatic scaling to the cluster,
    for both nodes and pods.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何将程序化扩展应用于集群，既包括节点，也包括 Pod。
- en: Scaling the cluster dynamically (autoscaling)
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态扩展集群（自动扩展）
- en: 'Now that you''ve completed many of the exercises in the preceding three chapters,
    which explored the complex concepts that go along with the Kubernetes container
    orchestration system, you might be wondering: is all this effort worth it? In
    this section, we will explore the key feature that can make the pain of managing
    these systems worth it – autoscaling. By dynamically scaling the applications
    in a cluster, and the cluster itself, you can drive high utilization of cluster
    resources, meaning that you will need fewer computers (virtual or physical) to
    run your systems. When you combine dynamic scaling with the self-healing capabilities
    of the Kubernetes system, this becomes compelling, even though it has high complexity
    and a high learning curve in some areas.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，经过前面三章的练习，你已经探讨了 Kubernetes 容器编排系统相关的复杂概念，你可能会想：所有这些努力值得吗？在本节中，我们将探讨一个关键特性，它可以让管理这些系统的痛苦变得值得——自动扩展。通过动态扩展集群中的应用程序和集群本身，你可以实现集群资源的高利用率，这意味着你需要更少的计算机（无论是虚拟的还是物理的）来运行你的系统。当你将动态扩展与
    Kubernetes 系统的自愈能力结合时，即使在某些领域具有较高的复杂性和学习曲线，这一特性仍然非常吸引人。
- en: Kubernetes supports several dynamic scaling mechanisms, including the Cluster
    Autoscaler, the Horizontal Pod Autoscaler, and the Vertical Pod Autoscaler. Let's
    explore each of these.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持几种动态扩展机制，包括集群自动扩展器、水平 Pod 自动扩展器和垂直 Pod 自动扩展器。我们来一一探讨这些机制。
- en: Configuring the Cluster Autoscaler
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 Cluster Autoscaler
- en: The `kube-system` namespace and uses cloud APIs to launch and terminate nodes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-system` 命名空间使用云 API 来启动和终止节点。'
- en: 'If you used the AWS EKS Quick Start Cloudformation templates to create your
    cluster and told it to enable the Cluster Autoscaler, no further configuration
    is needed. If you used `eksctl` or another method to create the cluster, you may
    need to configure it further using the directions provided here: [https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 AWS EKS 快速启动 Cloudformation 模板来创建集群并告诉它启用 Cluster Autoscaler，则无需进一步配置。如果您使用
    `eksctl` 或其他方法创建集群，则可能需要使用此处提供的说明进一步配置：[https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html)。
- en: 'You can verify that the Cluster Autoscaler is running by querying it:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查询来验证 Cluster Autoscaler 是否正在运行：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have learned a bit about the Cluster Autoscaler, let's discover
    how we might configure an application to take advantage of its features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些有关 Cluster Autoscaler 的信息，让我们发现如何配置应用程序以利用其功能。
- en: Configuring a stateless application to work with the Cluster Autoscaler
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置无状态应用程序以与 Cluster Autoscaler 协同工作
- en: 'A stateless application, such as ShipIt Clicker, can tolerate starting and
    stopping any one of its pods and can run on any node in the cluster. It doesn''t
    require special configuration to work with the Cluster Autoscaler. Stateful applications
    that mount local storage and some other classes of applications must avoid some
    scaling operations if possible and may require special handling. See the Autoscaling
    FAQ for more details: [https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态应用程序（例如 ShipIt Clicker）可以容忍启动和停止其任何一个 pod，并且可以在集群中的任何节点上运行。它不需要特殊配置即可与 Cluster
    Autoscaler 协同工作。需要挂载本地存储和某些其他类别应用程序的有状态应用程序，在可能的情况下必须避免某些扩展操作，并可能需要特殊处理。有关详细信息，请参阅
    Autoscaling FAQ：[https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md)。
- en: 'You can give the Cluster Autoscaler a hint that it should not scale in pods
    beyond a certain point, and that it should strive to keep a certain number or
    percentage of healthy pods available by using a **PodDisruptionBudget** (**PDB**):
    [https://kubernetes.io/docs/tasks/run-application/configure-pdb/](https://kubernetes.io/docs/tasks/run-application/configure-pdb/).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过 **PodDisruptionBudget**（**PDB**）来提示 Cluster Autoscaler 不应缩减超出某一点的 pod
    数量，并且它应努力保持一定数量或百分比的健康 pod 可用性：[https://kubernetes.io/docs/tasks/run-application/configure-pdb/](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)。
- en: We have configured ShipIt Clicker with a PDB in its Helm Chart. See `chapter11/src/shipitclicker/templates/pdb.yaml`
    for more information. You can find the default values for it in `chapter11/src/shipitclicker/values.yaml`.
    The defaults now have ShipIt Clicker configured to deploy two pods and have a
    PDB with a minimum of one pod available. This provides hints to the Cluster Autoscaler
    and other Kubernetes applications that it should always keep at least one pod
    alive, even as node maintenance is underway.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在其 Helm Chart 中使用 PDB 配置了 ShipIt Clicker。有关更多信息，请参见 `chapter11/src/shipitclicker/templates/pdb.yaml`。您可以在
    `chapter11/src/shipitclicker/values.yaml` 中找到其默认值。默认情况下，ShipIt Clicker 现在配置为部署两个
    pod，并且具有至少一个可用 pod 的 PDB。这为 Cluster Autoscaler 和其他 Kubernetes 应用程序提供了提示，表明即使在进行节点维护时，也应始终保持至少一个
    pod 处于运行状态。
- en: Next, we will demonstrate the Cluster Autoscaler in action.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将演示 Cluster Autoscaler 的运行情况。
- en: Demonstrating the Cluster Autoscaler in action
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 演示 Cluster Autoscaler 的运行情况
- en: 'In order to get the Cluster Autoscaler to make changes to the size of the cluster,
    we can start more pods than it has capacity to handle currently. To watch this
    process in action, it is helpful to tail the logs of the `cluster-autoscaler`
    service. Open a Terminal window and run the following commands to tail the logs
    of the service:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 Cluster Autoscaler 更改集群大小，我们可以启动超过其当前处理能力的更多 pod。要观察此过程的执行过程，可以尾随 `cluster-autoscaler`
    服务的日志。打开终端窗口并运行以下命令以尾随服务的日志：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Every 10 seconds, you will see log entries indicating that the service is looking
    for *unschedulable* pods (which would cause the cluster to scale out the number
    of nodes) and for nodes that are eligible for scaling in.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每 10 秒，您将看到日志条目，指示服务正在寻找*不可调度*的 pod（这将导致集群扩展节点的数量），以及符合缩放条件的节点。
- en: 'Then, in a different Terminal window, manually scale the deployment of ShipIt
    Clicker to `50` pods:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在另一个终端窗口中，手动将 ShipIt Clicker 的部署扩展到 `50` 个 pod：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each of the `t3.medium` nodes in the default EKS cluster can handle approximately
    4 to 16 ShipIt Clicker pods, depending on how many other pods are also running
    on each node. This will trip the Cluster Autoscaler and make it scale out by at
    least one additional node. You will see entries in the Cluster Autoscaler log
    noting that it has found unschedulable pods, and shortly afterward, that it has
    completed scaling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 默认 EKS 集群中的每个`t3.medium`节点大约可以处理 4 到 16 个 ShipIt Clicker pod，具体取决于每个节点上运行的其他
    pod 数量。这会触发集群自动扩展器，并使其至少扩展一个额外的节点。你将会在集群自动扩展器的日志中看到它已发现无法调度的 pod，并且很快，它会完成扩展操作。
- en: 'To see the progress from the perspective of the nodes and pods in the deployment,
    issue the following commands every few seconds:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要从节点和 pod 的角度查看进度，可以每隔几秒钟执行以下命令：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will see nodes launching and more and more replicas becoming ready until
    the set of replicas stabilizes. Once that happens, scale it back down to a lower
    default state:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到节点启动并且越来越多的副本变为就绪状态，直到副本集稳定。当这发生时，将其缩减回一个较低的默认状态：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once you've done that, you may notice that the nodes do not scale in immediately
    as they enter a cooldown condition for 10 minutes after a scale out operation
    completes. However, a minute after the cooldown period expires, the Cluster Autoscaler
    will notice that the CPU utilization of these nodes is close to zero and it will
    scale in the cluster, terminating the nodes that no longer have pods available.
    The Cluster Autoscaler will respect the PDB when it performs this scale in operation
    as well – allowing you to be as conservative as required when shrinking the number
    of pods and nodes in the cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 完成操作后，你可能会注意到节点不会立即缩减，因为在扩展操作完成后，节点会进入一个 10 分钟的冷却状态。然而，在冷却期结束后的一分钟，集群自动扩展器会发现这些节点的
    CPU 使用率接近零，然后它会对集群进行缩减，终止那些不再有 pod 可用的节点。集群自动扩展器在执行缩减操作时会遵循 PDB 策略——这允许你在缩减集群中的
    pod 和节点数量时，根据需求谨慎操作。
- en: Now that you have learned how to scale the cluster nodes in and out using the
    Cluster Autoscaler, let's learn how to use the Horizontal Pod Autoscaler to set
    scaling policies.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何使用集群自动扩展器来扩展和缩减集群节点，接下来让我们学习如何使用 Horizontal Pod Autoscaler 设置扩展策略。
- en: Configuring the Horizontal Pod Autoscaler
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 Horizontal Pod Autoscaler
- en: 'The **Horizontal Pod Autoscaler** allows you to set up rules for scaling out
    sets of Kubernetes pods using rules that can take into account CPU utilization
    or other custom metrics. This service can also scale pods controlled by deployments,
    ReplicaSets, and replication controllers. You can read more about the theory of
    how it works here: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**Horizontal Pod Autoscaler** 允许你设置规则，通过考虑 CPU 使用率或其他自定义指标来扩展 Kubernetes pod
    集合。这个服务还可以扩展由部署、ReplicaSets 和复制控制器管理的 pod。你可以在这里阅读更多关于它如何工作的理论：[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)。'
- en: This is the last big piece of the puzzle you need before you can achieve a cluster
    that automatically scales in and out in response to demand.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你实现一个可以根据需求自动扩展和缩减的集群之前需要解决的最后一个关键步骤。
- en: You need Metrics Server for the Horizontal Pod Autoscaler to work. We will install
    this next.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装 Metrics Server，以使 Horizontal Pod Autoscaler 正常工作。我们接下来将进行安装。
- en: Installing Metrics Server
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 Metrics Server
- en: 'To have more detailed statistics available in your Kubernetes cluster for use
    by the software components that enable dynamic scaling (including the Horizontal
    Pod Autoscaler), you need to run the standard **Metrics Server**. It aggregates
    statistics across the cluster regarding the memory, CPU, and other resource utilization
    of the nodes and among the pods in a format that the various Kubernetes autoscaler
    mechanisms can understand and act upon. The AWS EKS guide talks about installing
    that here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 若要在你的 Kubernetes 集群中获取更详细的统计信息，供支持动态扩展的组件使用（包括 Horizontal Pod Autoscaler），你需要运行标准的**Metrics
    Server**。它会聚合集群中关于节点的内存、CPU 以及其他资源利用率的统计信息，并以 Kubernetes 各种自动扩展机制可以理解和操作的格式提供这些数据。AWS
    EKS 的指南中有关于如何安装的介绍，详情见此：
- en: '[https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html](https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html](https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html)'
- en: 'To install it, ensure your `kubectl config` context is set to your cloud cluster.
    Then, issue the following command from your local workstation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装它，确保你的 `kubectl config` 上下文已设置为你的云集群。然后，从你的本地工作站发出以下命令：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once you have installed Metrics Server, verify that it is running:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Metrics Server 后，验证它是否正在运行：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next, we will activate the Horizontal Pod Autoscaler for the ShipIt Clicker
    application to demonstrate how it works.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为 ShipIt Clicker 应用启用 Horizontal Pod Autoscaler，以演示它是如何工作的。
- en: Activating the Horizontal Pod Autoscaler
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 启用 Horizontal Pod Autoscaler
- en: 'The AWS EKS guide shows the steps needed to install the Horizontal Pod Autoscaler:
    [https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EKS 指南展示了安装 Horizontal Pod Autoscaler 所需的步骤：[https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html)。
- en: 'The main thing we need to install is the metrics service. It turns out that
    the Horizontal Pod Autoscaler is baked into Kubernetes itself. We can issue a
    command such as the following one to activate a Horizontal Pod Autoscaler for
    a deployment:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装的主要组件是指标服务。事实证明，Horizontal Pod Autoscaler 已经内置在 Kubernetes 中。我们可以发出如下命令，启用某个部署的
    Horizontal Pod Autoscaler：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you need to edit these parameters, you can do so with the following command:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要编辑这些参数，可以使用以下命令：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can get a detailed view of what the Horizontal Pod Autoscaler has done
    recently by issuing this command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过发出以下命令，详细查看 Horizontal Pod Autoscaler 最近执行的操作：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To test whether the Horizontal Pod Autoscaler and Cluster Autoscaler are working
    as expected, we need to drive CPU load. That''s where the `/faults/spin` endpoint
    comes in handy. Later in this chapter, in the *Testing scalability and performance
    with k6* section, we will see how to construct a realistic load test for the ShipIt
    Clicker application. However, to exercise autoscaling, we are going to use a brute-force
    method by using the Apache Bench utility that''s run via Docker (replace `example.com`
    with your domain name):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试 Horizontal Pod Autoscaler 和 Cluster Autoscaler 是否按预期工作，我们需要施加 CPU 负载。这时，`/faults/spin`
    端点就派上用场了。在本章稍后的 *使用 k6 测试可扩展性和性能* 部分，我们将看到如何为 ShipIt Clicker 应用构建一个真实的负载测试。然而，为了验证自动扩缩容，我们将使用一个暴力方法，通过
    Docker 运行 Apache Bench 工具（将 `example.com` 替换为你的域名）：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Use the `kubectl get deployments`, `kubectl get pods`, `kubectl get nodes`,
    and `kubectl describe hpa` commands repeatedly to watch the deployment replicas
    grow. Alternatively, use a Kubernetes monitoring tool such as k9s ([https://k9scli.io/](https://k9scli.io/))
    to watch the pod and node counts grow over the first 10 minutes or so, and then
    subside in the 15 minutes afterward. You could also look at some Grafana dashboards
    and Jaeger traces, as described in the previous chapter, to see how the cluster
    is handling the load, or even look at the CloudWatch metrics that surfaced in
    the EC2 console for the active nodes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl get deployments`、`kubectl get pods`、`kubectl get nodes` 和 `kubectl
    describe hpa` 命令反复执行，观察部署副本的增长。或者，使用 Kubernetes 监控工具，例如 k9s（[https://k9scli.io/](https://k9scli.io/)）来观察
    pod 和节点数在最初 10 分钟内的增长，然后在接下来的 15 分钟内逐渐减少。你也可以查看上一章中描述的 Grafana 仪表盘和 Jaeger 跟踪，以查看集群如何处理负载，或者查看在
    EC2 控制台中出现的 CloudWatch 指标，这些是针对活动节点的。
- en: Next, we will consider when we might use the Vertical Pod Autoscaler.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑何时使用 Vertical Pod Autoscaler。
- en: Configuring the Vertical Pod Autoscaler
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 Vertical Pod Autoscaler
- en: The Vertical Pod Autoscaler is a newer scaling mechanism that observes the amount
    of memory and CPU usage that pods request, versus what they actually use, in order
    to optimize memory and CPU requests – it performs right-sizing to drive better
    cluster utilization. This is the most useful scaling mechanism for stateful pods.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Vertical Pod Autoscaler 是一种较新的扩展机制，它观察 pod 请求的内存和 CPU 使用量，以及实际使用的量，从而优化内存和 CPU
    请求——它执行右-sizing，以提升集群利用率。这是最适合有状态 Pod 的扩展机制。
- en: 'However, the Vertical Pod Autoscaler documentation currently states that it
    is not compatible with the Horizontal Pod Autoscaler, so you should avoid configuring
    it so that it manages the same pods. You can explore using it for your application,
    but keep in mind the advice it specifies about not mixing it with the Horizontal
    Pod Autoscaler using CPU metrics. The installation procedure for the Vertical
    Pod Autoscaler is also more involved than configuring either of the other autoscalers,
    so we won''t show all the steps in detail here – please refer to the Vertical
    Pod Autoscaler documentation for detailed configuration instructions: [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Vertical Pod Autoscaler 的文档目前指出，它与 Horizontal Pod Autoscaler 不兼容，因此你应避免配置它以管理相同的
    Pods。你可以尝试将它用于你的应用，但请记住文档中关于避免将其与 Horizontal Pod Autoscaler 使用 CPU 指标进行混合的建议。Vertical
    Pod Autoscaler 的安装过程也比配置其他自动扩展器要复杂，因此我们不会在此详细展示所有步骤——请参考 Vertical Pod Autoscaler
    文档获取详细配置说明：[https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)。
- en: In this section, we learned all about how we can scale our application using
    both manual and dynamic methods. In the next section, we will learn all about
    Envoy, a service mesh that provides some advanced controls and sanity regarding
    communications between pods in a Kubernetes cluster.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用手动和动态方法来扩展我们的应用程序。在下一节中，我们将了解 Envoy，它是一个服务网格，提供一些关于 Kubernetes
    集群中 Pod 通信的高级控制和安全性。
- en: What is Envoy, and why might I need it?
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Envoy，为什么我可能需要它？
- en: Envoy ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) is a C++ open
    source **service mesh** and edge proxy geared toward microservice deployments.
    Developed by a team at Lyft, it is especially useful for teams developing Kubernetes-hosted
    applications, such as the ones you have seen throughout this book.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) 是一个 C++ 开源的
    **服务网格** 和边缘代理，专为微服务部署而设计。由 Lyft 团队开发，它特别适用于开发基于 Kubernetes 托管的应用程序的团队，正如本书中你所看到的应用。
- en: So, why exactly would we need to deploy Envoy? When developing cloud-based production
    systems that use multiple containers to host a distributed service, many of the
    problems you will encounter are related to observability and networking.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们需要部署 Envoy 呢？在开发基于云的生产系统时，使用多个容器来托管分布式服务，你会遇到许多与可观察性和网络相关的问题。
- en: Envoy aims to solve these two problems by introducing a proxy service that offers
    runtime-configurable networking and metrics collection that can be used as a building
    block for creating higher-level systems that manage these concerns. Whether you're
    building out a small distributed application or a large microservice architecture
    designed around the service mesh model, Envoy's features allow us to abstract
    the thorny problem of networking in a cloud and platform-agnostic fashion.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 旨在通过引入一个代理服务来解决这两个问题，该服务提供可运行时配置的网络和指标收集功能，可以作为创建更高级别系统的构建模块，帮助管理这些问题。无论你是在构建一个小型的分布式应用，还是一个围绕服务网格模型设计的大型微服务架构，Envoy
    的功能使我们能够以云平台无关的方式抽象出网络这一棘手的问题。
- en: 'The team at Lyft developed Envoy using the following concepts:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Lyft 团队使用以下概念开发了 Envoy：
- en: '**Out of process architecture**: Envoy is a self-contained process that can
    be deployed alongside existing applications.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程外架构**：Envoy 是一个自包含的进程，可以与现有应用程序一起部署。'
- en: '`localhost` and are ignorant of the network topology. An L3/L4 filter architecture
    is used for networking proxying. You can add custom filters to the proxy to support
    tasks such as TLS client certificate authentication.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`localhost` 并且对网络拓扑一无所知。采用 L3/L4 过滤架构用于网络代理。你可以向代理中添加自定义过滤器，以支持诸如 TLS 客户端证书认证等任务。'
- en: '**Language agnosticism**: Envoy works with multiple languages and allows you
    to mix and match application frameworks. For example, through the use of Envoy
    PHP and Python, containerized applications can communicate with each other.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言无关性**：Envoy 支持多种语言，并允许你混合使用不同的应用框架。例如，通过使用 Envoy 的 PHP 和 Python，容器化应用程序可以互相通信。'
- en: '**HTTP L7 filters and routing**: As with L3/L4 filters, filtering is also supported
    at the L7 layer. This allows plugins to be developed for different tasks, ranging
    from buffering to interacting with AWS services such as DynamoDB. Envoy''s routing
    feature allows you to deploy a routing subsystem that can redirect requests based
    on a variety of criteria, such as path and content type.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP L7 过滤器和路由**：与 L3/L4 过滤器一样，L7 层也支持过滤。这允许开发用于不同任务的插件，从缓冲到与 AWS 服务（如 DynamoDB）交互。Envoy
    的路由功能允许你部署一个路由子系统，根据多种标准（如路径和内容类型）重定向请求。'
- en: '**Load balancing and front/edge proxy support**: Envoy supports advanced load
    balancing techniques, including automatic retries, circuit breakers, health checking,
    and rate limiting. Additionally, you can deploy Envoy at the network edge to handle
    TLS termination and HTTP/2 requests.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡和前端/边缘代理支持**：Envoy 支持高级负载均衡技术，包括自动重试、断路器、健康检查和限流。此外，你可以在网络边缘部署 Envoy
    以处理 TLS 终止和 HTTP/2 请求。'
- en: '**Observability and transparency**: Envoy collects statistics to support observability
    at both the application and networking layer. You can combine Envoy with Prometheus,
    Jaeger, Datadog, and other monitoring platforms that support metrics and tracing.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性和透明性**：Envoy 收集统计数据以支持应用层和网络层的可观察性。你可以将 Envoy 与 Prometheus、Jaeger、Datadog
    和其他支持指标和追踪的监控平台结合使用。'
- en: Let's explore some of Envoy's features in more detail so that we can understand
    these concepts better.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探索一下 Envoy 的一些功能，以便更好地理解这些概念。
- en: Network traffic management with an Envoy service mesh
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Envoy 服务网格进行网络流量管理
- en: You should already be familiar with the concept of a load balancer, which is
    one type of network traffic manager. But what exactly is a service mesh? Why would
    you need to use one? How does Envoy help us in this regard?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经熟悉负载均衡器的概念，它是一种网络流量管理器。但究竟什么是服务网格？为什么你需要使用它？Envoy 如何在这方面帮助我们？
- en: 'A service mesh is an infrastructure layer dedicated to handling service-to-service
    communications, typically through a proxy service. The benefits of using a service
    mesh are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是一个基础设施层，专门用于处理服务间通信，通常通过代理服务实现。使用服务网格的好处如下：
- en: Transparency and observability into network communications.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对网络通信的透明性和可观察性。
- en: You can support secure connections across the network.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以支持网络上的安全连接。
- en: Metrics collection, including length of time for a retry to succeed when a service
    fails.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标收集，包括服务失败时重试成功所需的时间。
- en: You can deploy proxies as **sidecars**. This means they run alongside each service
    rather than within it. In turn, this allows us to decouple the proxying service
    from the application itself.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将代理部署为**边车**。这意味着它们与每个服务并行运行，而不是嵌入其中。反过来，这使我们能够将代理服务与应用程序本身解耦。
- en: 'An example of a four-application service mesh can be visualized as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 四个应用程序的服务网格示例可以如下可视化：
- en: '![](img/B11641_11_002.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_11_002.jpg)'
- en: Figure 11.2 – Example of a service mesh with four microservices and sidecar
    proxies
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 一个包含四个微服务和边车代理的服务网格示例
- en: Here, each of our containerized applications has a corresponding sidecar proxy.
    The application communicates with the proxy, which, in turn, communicates across
    the service mesh with the other containerized services we are hosting. The application
    does not know that the proxy exists and does not need any modifications to work
    with the proxy. All the configuration can be done by wiring ports together using
    the container orchestration system, in a way that is invisible to the application.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的每个容器化应用程序都有一个相应的边车代理。应用程序与代理进行通信，代理则通过服务网格与我们托管的其他容器化服务进行通信。应用程序并不知道代理的存在，也不需要任何修改即可与代理协作。所有配置都可以通过容器编排系统将端口连接起来，方式对应用程序是不可见的。
- en: Now, let's gets our hands dirty and get Envoy up and running.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们动手实际操作，启动并运行 Envoy。
- en: Setting up Envoy
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Envoy
- en: 'Because of Envoy''s architecture, you have flexibility in terms of how you
    can deploy the software:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Envoy 的架构，你在部署软件时具有灵活性：
- en: Configured explicitly as a sidecar container, with a static configuration file,
    alongside an application container
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确配置为边车容器，使用静态配置文件，与应用容器并排部署
- en: Configured dynamically as part of a service mesh control plane, where the container
    might be injected into a Kubernetes pod as a component, using software such as
    Istio ([https://istio.io/](https://istio.io/)) or AWS App Mesh ([https://aws.amazon.com/app-mesh/](https://aws.amazon.com/app-mesh/))
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态配置作为服务网格控制平面的一部分，容器可能作为组件被注入到 Kubernetes pod 中，使用如 Istio ([https://istio.io/](https://istio.io/))
    或 AWS App Mesh ([https://aws.amazon.com/app-mesh/](https://aws.amazon.com/app-mesh/))
    等软件。
- en: The second option offers additional power at the cost of adding major complexity.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择提供了更大的功能，但增加了较大的复杂性。
- en: The Envoy sample configurations (see [https://www.envoyproxy.io/docs/envoy/latest/start/start#sandboxes](https://www.envoyproxy.io/docs/envoy/latest/start/start#sandboxes))
    are all of the first variety, with explicit Envoy proxy configurations. To learn
    about Envoy, it is simpler to consider the explicit configuration examples. The
    version of ShipIt Clicker provided in this chapter has been modified so that you
    can add an Envoy sidecar container using a static configuration file when it is
    deployed in Kubernetes, with a minimalist approach that allows us to demonstrate
    Envoy's features.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 的示例配置（参见 [https://www.envoyproxy.io/docs/envoy/latest/start/start#sandboxes](https://www.envoyproxy.io/docs/envoy/latest/start/start#sandboxes)）都属于第一类，具有明确的
    Envoy 代理配置。要了解 Envoy，考虑这些明确的配置示例更为简单。本章提供的 ShipIt Clicker 版本已经被修改，可以在 Kubernetes
    部署时通过静态配置文件添加一个 Envoy sidecar 容器，采用简化的方式展示 Envoy 的特性。
- en: Configuring ShipIt Clicker for Envoy
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为 Envoy 配置 ShipIt Clicker
- en: 'Now, let''s examine the specific changes that need to be made for Envoy to
    be supported in ShipIt Clicker. The application JavaScript code does not require
    any changes; all the changes are in the Helm Charts. See the Helm Charts in `chapter11/shipitclicker`
    and compare them with the ones in `chapter10/shipitclicker`; you will see a new
    Envoy sidecar container defined in `chapter11/shipitclicker/templates/deployment.yaml`,
    configured with an image defined in `chapter11/shipitclicker/values.yml`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来检查一下为了支持 Envoy 在 ShipIt Clicker 中所需进行的具体更改。应用程序的 JavaScript 代码不需要任何更改；所有更改都在
    Helm Charts 中。查看 `chapter11/shipitclicker` 中的 Helm Charts，并与 `chapter10/shipitclicker`
    中的进行对比；你将看到在 `chapter11/shipitclicker/templates/deployment.yaml` 中定义了一个新的 Envoy
    sidecar 容器，使用在 `chapter11/shipitclicker/values.yml` 中定义的镜像进行配置：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding lines in the template launch the Envoy container using a configuration
    file, `/etc/envoy-config/config.yaml`, defined in a ConfigMap. Envoy needs both
    a port definition for its administrative interface and a port definition for each
    service it manages or proxies:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 模板中的前几行使用配置文件 `/etc/envoy-config/config.yaml` 启动 Envoy 容器，该配置文件在 ConfigMap 中定义。Envoy
    需要为其管理或代理的每个服务提供端口定义，包括其管理界面的端口定义：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can query the administrative API to ensure that Envoy is both live and ready
    to accept traffic, in accordance with Kubernetes best practices:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查询管理 API，确保 Envoy 处于活动状态并准备好接受流量，符合 Kubernetes 的最佳实践：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To expose the configuration file to the container, we use a volume mount that
    exposes the `config.yaml` file:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将配置文件暴露给容器，我们使用一个卷挂载来暴露 `config.yaml` 文件：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `config.yaml` file is defined in `chapter11/shipitclicker/templates/configmap-envoy.yaml`
    and has definitions for listeners and clusters for the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.yaml` 文件在 `chapter11/shipitclicker/templates/configmap-envoy.yaml`
    中定义，并为以下内容定义了监听器和集群：'
- en: An ingress proxy for the ShipIt Clicker container inside the pod
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个为 pod 内部的 ShipIt Clicker 容器提供的入口代理。
- en: An egress proxy for the Redis Kubernetes service that can be reached at `redis-master`
    in the cluster
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Redis Kubernetes 服务配置的出口代理，可以在集群中的 `redis-master` 处访问。
- en: An ingress proxy that allows Prometheus to scrape metrics from the Envoy sidecar
    in the pod
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个入口代理，允许 Prometheus 从 pod 中的 Envoy sidecar 拉取指标。
- en: The ConfigMap for ShipIt Clicker in `chapter11/shipitclicker/templates/configmap.yaml`
    has been modified so that it connects to `localhost:6379` for Redis, which Envoy
    listens for and proxies out via a TCP L4 proxy to the Redis service. This listens
    elsewhere in the cluster at `redis-master:6379`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`chapter11/shipitclicker/templates/configmap.yaml` 中的 ShipIt Clicker 的 ConfigMap
    已被修改，以便它连接到 `localhost:6379` 的 Redis，Envoy 监听此端口并通过 TCP L4 代理将其代理到 Redis 服务。该服务在集群中的其他地方的
    `redis-master:6379` 处进行监听。'
- en: The Kubernetes service in `chapter11/shipitclicker/templates/service.yaml` now
    calls the `envoy-http` port instead of directly calling the application container's
    port.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`chapter11/shipitclicker/templates/service.yaml` 中的 Kubernetes 服务现在调用 `envoy-http`
    端口，而不是直接调用应用程序容器的端口。'
- en: Why not use the Envoy Redis protocol proxy?
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么不使用 Envoy Redis 协议代理？
- en: The example files used here use a plain TCP proxy, instead of Envoy's Redis
    protocol proxy (see [https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/redis_proxy/v3/redis_proxy.proto](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/redis_proxy/v3/redis_proxy.proto)
    and [https://github.com/envoyproxy/envoy/tree/master/examples/redis](https://github.com/envoyproxy/envoy/tree/master/examples/redis)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的示例文件使用的是普通的 TCP 代理，而不是 Envoy 的 Redis 协议代理（请参见 [https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/redis_proxy/v3/redis_proxy.proto](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/redis_proxy/v3/redis_proxy.proto)
    和 [https://github.com/envoyproxy/envoy/tree/master/examples/redis](https://github.com/envoyproxy/envoy/tree/master/examples/redis)）。
- en: 'This is because the ShipIt Clicker application has a Redis password authentication
    set up that is not compatible with Envoy''s Redis proxy. ShipIt Clicker is set
    up to use a password it retrieves from a Kubernetes Secret that the Bitnami Redis
    Helm Chart stores. However, Envoy does not pass through this password; when configured
    with the Redis protocol proxy, it emitted an error message stating `Warning: Redis
    server does not require a password, but a password was supplied` when ShipIt Clicker
    tried to authenticate. It turns out that if you use the Envoy Redis protocol support,
    you must configure the proxy itself with password authentication for the client,
    and optionally the server, through the configuration file stored in a ConfigMap.
    However, the password that the Bitnami Redis server uses is only available as
    a Kubernetes secret, so reworking the system to support this would add complexity.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 ShipIt Clicker 应用程序设置了 Redis 密码认证，而该认证与 Envoy 的 Redis 代理不兼容。ShipIt Clicker
    被配置为使用从 Kubernetes Secret 中检索的密码，而该密码由 Bitnami Redis Helm Chart 存储。然而，Envoy 不会通过该密码；当配置为使用
    Redis 协议代理时，ShipIt Clicker 尝试进行身份验证时，出现了错误信息：`警告：Redis 服务器不需要密码，但提供了密码`。事实证明，如果使用
    Envoy Redis 协议支持，则必须通过存储在 ConfigMap 中的配置文件为客户端（以及可选的服务器）配置代理本身的密码认证。然而，Bitnami
    Redis 服务器使用的密码仅作为 Kubernetes 秘密存在，因此重新构建系统以支持这一点会增加复杂性。
- en: As an exercise, you could install Redis without a password and remove the password
    from the configuration for ShipIt Clicker if you wanted to do this. If you did
    this, you could also switch Redis implementations to the Bitnami Redis Cluster
    Helm Chart (see [https://github.com/bitnami/charts/tree/master/bitnami/redis-cluster](https://github.com/bitnami/charts/tree/master/bitnami/redis-cluster)),
    and then use the Envoy support for Redis clusters in order to implement the reader/writer
    split pattern.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，如果你愿意，可以安装没有密码的 Redis，并从 ShipIt Clicker 的配置中移除密码。如果你这么做了，你还可以将 Redis 实现切换到
    Bitnami Redis Cluster Helm Chart（请参见 [https://github.com/bitnami/charts/tree/master/bitnami/redis-cluster](https://github.com/bitnami/charts/tree/master/bitnami/redis-cluster)），然后使用
    Envoy 支持 Redis 集群来实现读写分离模式。
- en: So far, we've seen how to deploy Envoy to create a service mesh. Next, we are
    going to explore the circuit breaker pattern.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何部署 Envoy 来创建服务网格。接下来，我们将探索电路断路器模式。
- en: Configuring Envoy's support for the circuit breaker pattern
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 Envoy 支持电路断路器模式
- en: The circuit breaker pattern is a mechanism that's used to configure thresholds
    for failures. The goal here is to prevent cascading failures spreading across
    your microservice platform and to stop continuous requests to a non-responsive
    service.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 电路断路器模式是一种配置失败阈值的机制。其目标是防止故障蔓延到你的微服务平台，并停止持续请求一个未响应的服务。
- en: Configuring the pattern on Envoy is relatively simple. We can configure circuit
    breaking values as part of an Envoy cluster definition via the `circuit_breakers`
    field.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Envoy 上配置该模式相对简单。我们可以通过 `circuit_breakers` 字段，将电路断路值配置为 Envoy 集群定义的一部分。
- en: 'To see how this works, examine the following ConfigMap file, which contains
    a definition of a circuit breaker (`chapter11/shipitclicker/templates/configmap-envoy.yaml`):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看其工作原理，请查看以下 ConfigMap 文件，其中包含了电路断路器的定义（`chapter11/shipitclicker/templates/configmap-envoy.yaml`）：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This threshold definition specifies the maximum number of connections Envoy
    will make and the maximum number of parallel requests. In our example, we have
    a configuration for a default priority threshold and a second one for high priority
    (used for HTTP 1.1) and the maximum number of requests (used for HTTP/2). If the
    rate of traffic that Envoy detects exceeds these thresholds, it will throw an
    error and deny the requests, without passing the request to the target service.
    Notice that since we are using Helm Charts, we specify the actual values using
    the Helm template variable substitution with the values coming from `chapter11/shipitclicker/values.yaml`
    or one of the override mechanisms for Helm Chart values. The default values are
    from a section of the `values.yaml` file that specifies Envoy-specific values:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阈值定义指定了 Envoy 将建立的最大连接数和最大并发请求数。在我们的示例中，我们有一个默认优先级阈值配置，以及一个用于高优先级（用于 HTTP
    1.1）和最大请求数（用于 HTTP/2）的第二个阈值。如果 Envoy 检测到的流量超过这些阈值，它将抛出错误并拒绝请求，而不会将请求传递到目标服务。请注意，由于我们使用
    Helm Charts，我们通过 Helm 模板变量替换指定实际值，这些值来自 `chapter11/shipitclicker/values.yaml`
    或 Helm Chart 值的某个覆盖机制。默认值来自 `values.yaml` 文件中指定 Envoy 特定值的部分：
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: These default values are suitable for production for this application, but how
    can we test that the circuit breaker works, without inducing a massive load? We
    will show you how do that next.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些默认值适用于此应用程序的生产环境，但我们如何测试断路器是否有效，而不产生巨大的负载呢？我们接下来会展示如何做到这一点。
- en: Testing the Envoy circuit beaker
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试 Envoy 断路器
- en: 'In order to test that the Envoy circuit breaker is working properly, we''ll
    deploy ShipIt Clicker to the cloud Kubernetes cluster with an artificially lowered
    request limit and perform a quick load test to verify that it works. Issue a Helm
    `upgrade` command, followed by a `kubectl rollout restart` command, similar to
    the following, to set the maximum simultaneous requests to `10` (replace `image.repository`
    with your ECR repository reference):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试 Envoy 断路器是否正常工作，我们将把 ShipIt Clicker 部署到云 Kubernetes 集群中，设置一个人为降低的请求限制，并执行快速负载测试以验证其是否有效。发出
    Helm `upgrade` 命令，随后执行 `kubectl rollout restart` 命令，类似以下内容，以将最大并发请求设置为 `10`（将
    `image.repository` 替换为你的 ECR 仓库引用）：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we''ll use Apache Bench to test the deployed application, starting with
    a single concurrent request:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Apache Bench 测试已部署的应用程序，从单个并发请求开始：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, you can see that when run with only one concurrent request, all the requests
    succeeded. Next, we''ll increase the concurrency to `50` simultaneous connections:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到当只运行一个并发请求时，所有请求都成功。接下来，我们将把并发增加到 `50` 个并发连接：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If we set the concurrency to `50` simultaneous requests, many of them will fail
    as the circuit breaker kicks in. We've already seen how to set up a basic circuit
    breaker with two thresholds for our cluster. More advanced circuit breaker patterns
    exist, including breaking on latency and retries. We'll leave you to explore this
    further if you think your applications will need it.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将并发设置为 `50` 个并发请求，其中许多将因断路器触发而失败。我们已经看到如何为我们的集群设置一个具有两个阈值的基本断路器。还有更高级的断路器模式，包括基于延迟和重试的断路器。如果你认为你的应用程序需要，你可以进一步探索这些模式。
- en: Now that you have tested the circuit breaker with low connection thresholds,
    reset the thresholds to their original values and redeploy the application to
    help set up the application for more load testing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经用低连接阈值测试过了断路器，请将阈值恢复到原始值，并重新部署应用程序，以便为更多负载测试做好准备。
- en: If we had a good measurement of how much real user traffic each pod could handle
    without failing, we could use this to set a better value for the circuit breaker.
    However, Apache Bench is a blunt instrument that does not let us simulate a realistic
    user load. For that, we need to use a more sophisticated load test framework.
    Now, we'll take a look at how we can test scalability with k6, a Docker-based
    load testing framework.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能很好地衡量每个 Pod 在不失败的情况下能够处理多少真实用户流量，我们可以用它来为断路器设置一个更好的值。然而，Apache Bench 是一个笨重的工具，它不能让我们模拟现实的用户负载。为此，我们需要使用一个更复杂的负载测试框架。现在，我们将看看如何使用
    k6 来测试可扩展性，k6 是一个基于 Docker 的负载测试框架。
- en: Testing scalability and performance with k6
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k6 测试可扩展性和性能
- en: The k6 framework ([https://k6.io](https://k6.io)) is a programmable open source
    load testing tool. We are going to show you how to use it to generate a more realistic
    load pattern than you could generate using a simple load generator such as **Apache
    Bench** (**ab**).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: k6框架([https://k6.io](https://k6.io))是一个可编程的开源负载测试工具。我们将向你展示如何使用它生成比简单负载生成器（如**Apache
    Bench**（**ab**））更真实的负载模式。
- en: This framework is quite simple to set up and use thanks to its Docker image,
    which is available on Docker Hub. You can find the Quick Start instructions at
    [https://k6.io/docs/getting-started/running-k6](https://k6.io/docs/getting-started/running-k6).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架由于其Docker镜像的存在，设置和使用起来相当简单，该镜像可以在Docker Hub上找到。你可以在[https://k6.io/docs/getting-started/running-k6](https://k6.io/docs/getting-started/running-k6)找到快速开始说明。
- en: 'To create a load test using k6, you need to use JavaScript using k6''s library
    routines. To perform a smoke test, your script would need to look something like
    this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用k6创建负载测试，你需要使用JavaScript并调用k6的库函数。要进行冒烟测试，你的脚本大致需要如下所示：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This script is roughly equivalent to using the `ab` utility to stress test a
    web server. Create a file called `hello.js` using the preceding source code, replacing
    `shipit-v8.eks.example.com` with the fully qualified domain name of one of your
    websites.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本大致相当于使用`ab`工具对Web服务器进行压力测试。创建一个名为`hello.js`的文件，使用前面的源代码，并将`shipit-v8.eks.example.com`替换为你网站的完全限定域名。
- en: 'Following Docker best practices, you should ensure that you add the `--rm`
    flag to the Docker command line so that you do not accumulate stale containers
    in your local installation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循Docker最佳实践，你应该确保在Docker命令行中添加`--rm`标志，以避免在本地安装中积累过时的容器。
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This will run k6 and retrieve the URL specified in `hello.js`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行k6并获取`hello.js`中指定的URL。
- en: 'There are just a few key concepts you must know about:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个关键概念是你必须了解的：
- en: You must provide a default function.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须提供一个默认函数。
- en: K6 is *not* Node.js. It has no event loop.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K6是*不是*Node.js，它没有事件循环。
- en: Your default function is known as a **Virtual User (VU)**.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的默认函数被称为**虚拟用户（VU）**。
- en: Code defined outside of the default function is evaluated once, on program startup.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在默认函数外定义的代码会在程序启动时执行一次。
- en: The default function is run repeatedly until the test is over.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认函数会重复执行，直到测试结束。
- en: You can run your test with as many VUs as you want, and for as long as you want.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以根据需要使用任意数量的虚拟用户，并运行任意时长。
- en: Note
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: There are many command-line options you can use with k6 to ramp up and down
    VUs over time, as well as to specify how long to run the test and how many VUs
    to simulate. The defaults have only one VU, and only one test iteration.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: k6提供了许多命令行选项，可以用来在一段时间内逐步增加或减少虚拟用户（VU），以及指定测试的运行时长和要模拟的虚拟用户数量。默认设置只有一个虚拟用户（VU），并且只有一个测试迭代。
- en: 'Let''s use some of those options to run the test with more users and for a
    longer duration:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一些选项来用更多的用户并更长时间运行测试：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Running k6 like this will perform a load test almost identical to an Apache
    Bench load test, with a concurrency of `50` and a duration of `30` seconds.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式运行k6将执行与Apache Bench负载测试几乎相同的负载测试，默认并发为`50`，持续时间为`30`秒。
- en: However, since you have the full power of JavaScript available, you can write
    more nuanced load tests using a variety of strategies.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于你可以使用JavaScript的全部功能，你可以利用多种策略编写更细致的负载测试。
- en: Recording and replaying network sessions
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录并重放网络会话
- en: An alternative to writing a script such as `hello.js` by hand is to use a record-and-replay
    strategy. Many load testing frameworks support this paradigm, including k6\. To
    do this, use the Chrome browser and its **Inspect** feature. You can use the debugger's
    **Network** tab to capture and save network traffic to and from the application's
    backend.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 编写类似`hello.js`的脚本的替代方案是使用记录和重放策略。许多负载测试框架，包括k6，都支持这种范式。为此，使用Chrome浏览器及其**检查**功能。你可以使用调试器的**网络**选项卡来捕获并保存与应用程序后端之间的网络流量。
- en: You start with an empty (cleared) network history in the debugger. Then, you
    load and play the game. Each click will cause API requests to occur between the
    application running in the browser and the backend.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你从调试器中的空网络历史记录开始。然后，加载并开始游戏。每次点击都会触发应用程序和后端之间的API请求。
- en: 'When you are satisfied with your recording, right-click on the **Network**
    pane and choose **copy all as HAR**. This puts the HAR-formatted text in the system
    clipboard:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对录制的内容满意时，右键点击**网络**面板，选择**复制所有内容为HAR**。这将把HAR格式的文本放入系统剪贴板：
- en: '![](img/B11641_11_003.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_11_003.jpg)'
- en: Figure 11.3 – Google Chrome inspector debugging console – Copy all as HAR
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – Google Chrome检查器调试控制台 – 复制全部为HAR
- en: 'Paste from the clipboard into a file named `chapter11/src/test/k6/session.har`.
    Then, run a conversion script to transform the HAR file into a JavaScript file
    at `chapter11/src/test/k6/har-session.js`, and run another shell script that will
    run k6 via Docker with the right arguments to initiate a one-user, 60-second test:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从剪贴板粘贴到一个名为`chapter11/src/test/k6/session.har`的文件中。然后，运行一个转换脚本，将HAR文件转换为位于`chapter11/src/test/k6/har-session.js`的JavaScript文件，并运行另一个shell脚本，通过Docker和正确的参数运行k6，启动一个60秒、1个用户的测试：
- en: '[PRE28]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `k6-run-har.sh` script is set up to use environment variables that override
    the VUs with the `USERS` variable, and to override the test duration with the
    `DURATION` variable. So, you can prefix the script with those variables like this
    and run a 10-user test for `300` seconds:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`k6-run-har.sh`脚本设置了使用环境变量覆盖VUs的`USERS`变量，并使用`DURATION`变量覆盖测试时长。因此，你可以像这样为脚本加上前缀，并运行一个持续`300`秒的10用户测试：'
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There are some wrinkles to note about using this playback and record strategy,
    though: the process is quite literal, and results in a file that has no delays
    between requests. Running the test will induce a large, machine-speed load on
    the target service. There is no randomization of the delays that should happen
    between requests, which is something you want to do in order to closely model
    the load that a real user''s session would put on a service.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用这种播放和记录策略时有一些细节需要注意：该过程是非常字面意义上的，导致生成的文件在请求之间没有延迟。运行该测试会在目标服务上产生巨大的机器速度负载。请求之间的延迟没有随机化，这是你需要做的，以便更真实地模拟一个真实用户会对服务造成的负载。
- en: To create a more realistic test, we are going to have to do some JavaScript
    programming.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个更真实的测试，我们需要进行一些JavaScript编程。
- en: Hand-crafting a more realistic load test
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手工制作一个更真实的负载测试
- en: In the `chapter11/src/tests/k6/` directory, there is a `test.js` script designed
    to realistically test ShipIt Clicker, whether it's deployed locally or in the
    cloud.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在`chapter11/src/tests/k6/`目录下，有一个`test.js`脚本，旨在真实地测试ShipIt Clicker，无论它是部署在本地还是在云端。
- en: 'This script mimics a human playing the game by using these strategies:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本通过以下策略模拟一个人类玩游戏的过程：
- en: Fetches the HTML, stylesheets, images, and JavaScript files that make up the
    application
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取构成应用程序的HTML、样式表、图像和JavaScript文件
- en: Performs HTTP post to start a new game
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行HTTP POST以开始一个新游戏
- en: Gets the initial score, deployments, and `nextPurchase` values
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取初始得分、部署次数和`nextPurchase`值
- en: Attempts to simulate the click stream a human player would make
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试模拟人类玩家点击流的行为
- en: The HTTP requests were identified by playing the game in a web browser such
    as Google Chrome, using its **Inspect** feature, and viewing the **Network** tab
    as the game loads and is played. Then, we wrote a test that simulated the series
    of requests in a way that is closely modeled after real user behavior, including
    having realistic random delays.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这些HTTP请求是通过在浏览器（如Google Chrome）中玩游戏时，通过其**检查**功能并查看**网络**选项卡来识别的，在游戏加载并开始时记录请求。然后，我们编写了一个测试，模拟了一系列请求，尽可能贴近真实用户行为，包括加入了真实的随机延迟。
- en: 'Let''s examine the code in `chapter11/src/test/k6/test.js`. Here, we import
    the `http` class and the `sleep()` method from the k6 supplied libraries:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下`chapter11/src/test/k6/test.js`中的代码。这里，我们从k6提供的库中导入了`http`类和`sleep()`方法：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We pass parameters to the `test.js` script as environment variables:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过环境变量将参数传递给`test.js`脚本：
- en: The `DEBUG` environment variable lets us trigger more verbose logging.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEBUG`环境变量让我们触发更详细的日志记录。'
- en: The `MOVES` environment variable contains the number of moves per game.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MOVES`环境变量包含每个游戏的移动次数。'
- en: The `TARGET` environment variable would be something like `http://192.2.0.10:3011`
    for `localhost` development, where `192.2.0.10` is the IPv4 LAN address of your
    workstation.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TARGET`环境变量类似于`http://192.2.0.10:3011`，用于`localhost`开发，其中`192.2.0.10`是你的工作站的IPv4局域网地址。'
- en: 'These parameters get retrieved from the `__ENV` object, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数从`__ENV`对象中获取，如下所示：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `ENDPOINTS` array gets used to iterate through the three main elements
    that the game tracks:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`ENDPOINTS`数组用于迭代游戏跟踪的三个主要元素：'
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `deploy()` method simulates a human clicking on the `http.patch()` twice
    – once to update the deployment count and once to update the score:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`deploy()`方法模拟人类点击`http.patch()`两次——一次更新部署次数，一次更新得分：'
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This function also updates the score:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数还会更新分数：
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `validate()` method that the `deploy()` method calls simply verifies that
    the server returns a valid response:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`validate()` 方法是 `deploy()` 方法调用的，用于简单验证服务器是否返回有效响应：'
- en: '[PRE35]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `getStaticAssets()` method simulates the user''s browser fetching the HTML,
    CSS, images, and JavaScript that make up the game:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`getStaticAssets()` 方法模拟了用户浏览器获取构成游戏的 HTML、CSS、图片和 JavaScript 文件：'
- en: '[PRE36]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `getGameId()` method simulates the start of a new game:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`getGameId()` 方法模拟了新游戏的开始：'
- en: '[PRE37]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `getScores()` method retrieves the existing scores using the `map` functional
    programming technique to both iterate over the endpoints and to run a validation
    function on the HTTP response:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`getScores()` 方法使用 `map` 函数式编程技术检索现有分数，既用于迭代端点，也用于对 HTTP 响应运行验证函数：'
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `putScores()` method is used to reset all the game scores, such as when
    a new game begins:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`putScores()` 方法用于重置所有游戏分数，例如在新游戏开始时：'
- en: '[PRE39]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The default function is the one that k6 loops through for each virtual user:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 默认函数是 k6 为每个虚拟用户循环执行的函数：
- en: '[PRE40]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'After this function loads the static assets, it sleeps for a random delay to
    simulate a user waiting at the splash screen:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数加载静态资源后，它会随机延迟一段时间，模拟用户在启动画面等待的状态：
- en: '[PRE41]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'After another delay, when simulating the user seeing the game screen, the test
    program enters a loop where it starts rapidly simulating clicks:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟用户看到游戏画面后的另一次延迟之后，测试程序进入一个循环，开始快速模拟点击：
- en: '[PRE42]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Notice that we use a randomly generated delay between moves with a Gaussian
    distribution that has a mean of `125` milliseconds and a standard deviation of
    `25` milliseconds. This simulates clicking at about 8 clicks/second, which is
    the rate we measured when playing ShipIt Clicker on an iPhone – in 1 minute, we
    recorded 480 clicks:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用带有高斯分布的随机生成延迟，其中均值为 `125` 毫秒，标准差为 `25` 毫秒。这模拟了约 8 次点击/秒的速率，这是我们在 iPhone
    上玩 ShipIt Clicker 时测得的速率——在 1 分钟内，我们记录了 480 次点击：
- en: '[PRE43]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `default` function that's used for each virtual user fetches the same URLs
    that a user's browser would fetch on first page load. Note all the random delays
    that realistically simulate the delays that a real user would make. In a tight
    loop, the test simulates the user clicking as fast as a human would. The delay
    between clicks is subtly randomized using a random number with a normal distribution
    to simulate the fact that a human cannot click with robotic precision.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 用于每个虚拟用户的 `default` 函数会获取与用户浏览器首次加载页面时相同的 URL。请注意所有的随机延迟，这些延迟真实地模拟了一个真实用户所做的延迟。在一个紧凑的循环中，测试模拟了用户尽可能快地点击，点击之间的延迟通过一个正常分布的随机数微妙地随机化，模拟了人类无法像机器人一样精准点击的事实。
- en: 'The `chapter11/bin/k6-run.sh` script runs the test using the same environment
    variable pattern override that the `k6-har-run.sh` script did, but with more variables.
    It allows you to set these parameters:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`chapter11/bin/k6-run.sh` 脚本使用与 `k6-har-run.sh` 脚本相同的环境变量模式覆盖来运行测试，但包含更多变量。它允许你设置以下参数：'
- en: '`USERS`: Number of users'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`USERS`: 用户数量'
- en: '`DURATION`: Duration in seconds'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DURATION`: 持续时间（秒）'
- en: '`MOVES`: Number of moves in a game'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MOVES`: 游戏中的移动次数'
- en: '`STAGES`: Specify a set of k6 stages, which can vary VUs over time'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STAGES`: 指定一组 k6 阶段，可以随时间变化虚拟用户数量'
- en: The script requires a command-line argument, which is the URL target for the
    test. As mentioned earlier, this might be something like `http://192.2.0.10:80/`
    to test against the application infrastructure deployed on your workstation. Or,
    it could be the application as it was deployed to your cluster in the cloud, such
    as [https://shipit-v8.eks.shipitclicker.com/](https://shipit-v8.eks.shipitclicker.com/).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本需要一个命令行参数，这是测试的目标 URL。如前所述，这可能是类似 `http://192.2.0.10:80/` 的地址，用于测试部署在工作站上的应用程序基础设施。或者，也可以是已部署到云中集群上的应用程序，例如
    [https://shipit-v8.eks.shipitclicker.com/](https://shipit-v8.eks.shipitclicker.com/)。
- en: Running a stress test
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行压力测试
- en: 'In order to run a stress test, you want to ramp up the amount of load on an
    application until it starts showing signs of failing. We can try doing that using
    the `script.js` k6 program and the `k6-run.sh` test harness. The key element that
    we must specify is the `STAGES` parameter:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行压力测试，你需要逐步增加应用程序的负载，直到它开始出现故障的迹象。我们可以尝试使用 `script.js` k6 程序和 `k6-run.sh`
    测试工具来实现这一点。我们必须指定的关键元素是 `STAGES` 参数：
- en: '[PRE44]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You will likely find that with the default settings of two pods, this initial
    test will not show any signs of failure. You can use the `kubectl` command, plus
    Prometheus, Grafana, and Jaeger to monitor the test progress, plus the CPU and
    memory utilization in the cluster, as described in the previous chapter. For example,
    here is a screenshot of Grafana after the preceding load test:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会发现，使用默认设置下的两个 pod 时，这个初步测试不会显示出任何故障迹象。您可以使用 `kubectl` 命令，结合 Prometheus、Grafana
    和 Jaeger 来监控测试进度，以及集群中的 CPU 和内存利用率，正如前一章所描述的那样。例如，这是前述负载测试后 Grafana 的截图：
- en: '![](img/B11641_11_004.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_11_004.jpg)'
- en: Figure 11.4 – The Grafana dashboard showing the rate of ShipIt Clicker deployments
    during the load test
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – Grafana 仪表板显示负载测试期间 ShipIt Clicker 部署的速率
- en: 'In order to get this deployment to fail during the stress test, we don''t want
    it to automatically scale out. So, we will delete the Horizontal Pod Autoscaler:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在压力测试过程中让这个部署失败，我们不希望它自动扩展。因此，我们将删除 Horizontal Pod Autoscaler：
- en: '[PRE45]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We also want to stress test a single pod in order to see how much it can take,
    so we will shrink the number of replicas in the deployment to only `1`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望对单个 pod 进行压力测试，以了解它能承受多少压力，因此我们将部署中的副本数量缩减为仅 `1`：
- en: '[PRE46]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'At this point, we can rerun the stress test using the preceding `k9-run.sh`
    command. Watch the output. You will probably see some failed requests, which should
    be logged in the k9 output with a warning that looks something like this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以使用前述的 `k9-run.sh` 命令重新运行压力测试。观察输出。您可能会看到一些失败的请求，这些请求应以警告的形式记录在 k9 输出中，类似于以下内容：
- en: '[PRE47]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Once we are done stress testing, we can recreate the Horizontal Pod Autoscaler
    and reset the number of replicas for the deployment to a higher number.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成压力测试，我们可以重新创建 Horizontal Pod Autoscaler，并将部署的副本数量重置为更高的数字。
- en: At this point, we've learned how to use k6 to create a realistic load test and
    used it to perform a stress test of ShipIt Clicker.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经学习了如何使用 k6 创建一个现实的负载测试，并用它来对 ShipIt Clicker 进行压力测试。
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the topic of scaling out clusters in Kubernetes
    by using the Cluster Autoscaler and the Horizontal Pod Autoscaler. We then explored
    the topic of service meshes and set up a minimalistic Envoy service mesh in order
    to provide proxying and transparent network communications for complex microservice
    architectures.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们探讨了如何使用 Cluster Autoscaler 和 Horizontal Pod Autoscaler 扩展 Kubernetes 集群。随后，我们研究了服务网格的概念，并设置了一个简化的
    Envoy 服务网格，以提供代理和透明的网络通信，用于复杂的微服务架构。
- en: Following this, we looked at how we could use the circuit breaker pattern to
    prevent a service from becoming overwhelmed by traffic. Then, we used connection
    thresholds to test that the circuit breaker worked, in conjunction with a simple
    load test technique, using Docker and Apache Bench. After this, we learned about
    progressively more sophisticated load testing techniques when using k6, including
    both record-and-playback and detailed hand-crafted load tests designed to mimic
    real user behavior.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了如何使用断路器模式来防止服务被流量压垮。然后，我们使用连接阈值来测试断路器是否有效，并结合简单的负载测试技术，使用 Docker 和
    Apache Bench 进行测试。之后，我们学习了使用 k6 时更加复杂的负载测试技巧，包括录制和回放以及精心设计的详细负载测试，旨在模拟真实用户行为。
- en: This brings us to the end of our *Running Containers in Production* section
    of this book. We're going to move on and look at security next. Here, we will
    learn how to apply some techniques to the projects and skills we have developed
    so far in this book to improve our container security posture. So, let's move
    on to [*Chapter 12*](B11641_12_Final_NM_ePub.xhtml#_idTextAnchor278), *Introduction
    to Container Security*.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们《*生产中运行容器*》部分的结束。接下来我们将转向安全性方面的内容。在这一部分中，我们将学习如何将一些技术应用到我们迄今为止在本书中开发的项目和技能中，以改善我们的容器安全性态势。那么，接下来让我们进入
    [*第 12 章*](B11641_12_Final_NM_ePub.xhtml#_idTextAnchor278)，*容器安全入门*。
- en: Further reading
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Use the following resources to expand your knowledge of autoscaling, the Envoy
    service mesh, and load testing:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下资源来扩展您对自动扩展、Envoy 服务网格和负载测试的知识：
- en: 'Envoy presentation from Lyft: [https://www.slideshare.net/datawire/lyfts-envoy-from-monolith-to-service-mesh-matt-klein-lyft](https://www.slideshare.net/datawire/lyfts-envoy-from-monolith-to-service-mesh-matt-klein-lyft).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyft 的 Envoy 演示文稿：[https://www.slideshare.net/datawire/lyfts-envoy-from-monolith-to-service-mesh-matt-klein-lyft](https://www.slideshare.net/datawire/lyfts-envoy-from-monolith-to-service-mesh-matt-klein-lyft)。
- en: '*Performance Remediation Using New Relic and JMeter*, a three-part article
    series by the *Docker for Developers* co-author Richard Bullington-McGuire. This
    covers load testing and performance improvement basics. You can adapt these techniques
    to Kubernetes using Prometheus, Grafana, Jaeger, and k6.io: [https://moduscreate.com/blog/performance-remediation-using-new-relic-jmeter-part-1-3/](https://moduscreate.com/blog/performance-remediation-using-new-relic-jmeter-part-1-3/).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 New Relic 和 JMeter 进行性能修复*，这是 *Docker for Developers* 合著者 Richard Bullington-McGuire
    的三部文章系列之一。涵盖负载测试和性能改进基础知识。您可以使用 Prometheus、Grafana、Jaeger 和 k6.io 将这些技术适配到 Kubernetes
    中：[https://moduscreate.com/blog/performance-remediation-using-new-relic-jmeter-part-1-3/](https://moduscreate.com/blog/performance-remediation-using-new-relic-jmeter-part-1-3/).'
- en: 'Using a Network Load Balancer with the NGINX Ingress Controller on Amazon EKS
    – an economical and flexible alternative to using the ALB Ingress Controller for
    many scenarios: [https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/](https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Amazon EKS 上使用 Network Load Balancer 和 NGINX Ingress Controller – 对于许多场景来说，这是使用
    ALB Ingress Controller 的经济且灵活的替代方案：[https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/](https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/).
- en: '*Kubernetes Autoscaling 101: Cluster Autoscaler, Horizontal Pod Autoscaler,
    and Vertical Pod Autoscaler*: [https://levelup.gitconnected.com/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-pod-autoscaler-and-vertical-pod-2a441d9ad231](https://levelup.gitconnected.com/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-pod-autoscaler-and-vertical-pod-2a441d9ad231).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes 自动缩放 101：集群自动缩放器、水平 Pod 自动缩放器和垂直 Pod 自动缩放器*：[https://levelup.gitconnected.com/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-pod-autoscaler-and-vertical-pod-2a441d9ad231](https://levelup.gitconnected.com/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-pod-autoscaler-and-vertical-pod-2a441d9ad231).'
- en: 'Velero to backup and restore your Kubernetes cluster. Backup and restore your
    entire cluster, a namespace, or objects, filtered by tags: [https://velero.io/](https://velero.io/).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Velero 备份和恢复您的 Kubernetes 集群。可以备份和恢复整个集群、一个命名空间或按标签过滤的对象：[https://velero.io/](https://velero.io/).
- en: 'Expose Envoy Prometheus metrics as `/metrics`. See this issue for the workaround
    that''s integrated into ShipIt Clicker''s Envoy configuration that lets you expose
    Envoy''s metrics to the Prometheus metrics scraper by adding an additional Envoy
    mapping: [https://github.com/prometheus/prometheus/issues/3756](https://github.com/prometheus/prometheus/issues/3756).'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Envoy 的 Prometheus 指标公开为 `/metrics`。查看此问题，了解集成到 ShipIt Clicker 的 Envoy 配置中的解决方法，通过添加额外的
    Envoy 映射，使 Envoy 的指标可以被 Prometheus 指标抓取器获取：[https://github.com/prometheus/prometheus/issues/3756](https://github.com/prometheus/prometheus/issues/3756).
- en: 'Microservicing with Envoy, Istio, and Kubernetes: [https://thenewstack.io/microservicing-with-envoy-istio-and-kubernetes/](https://thenewstack.io/microservicing-with-envoy-istio-and-kubernetes/).'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Envoy、Istio 和 Kubernetes 进行微服务化：[https://thenewstack.io/microservicing-with-envoy-istio-and-kubernetes/](https://thenewstack.io/microservicing-with-envoy-istio-and-kubernetes/).
- en: 'Jaeger Native Tracing with Envoy – an advanced tracing strategy: [https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/jaeger_native_tracing](https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/jaeger_native_tracing).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Envoy 进行 Jaeger 原生跟踪 – 一种高级跟踪策略：[https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/jaeger_native_tracing](https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/jaeger_native_tracing).
- en: 'Redis with Envoy Cheatsheet – setting up Redis and Envoy using TLS and Redis
    Auth: [https://blog.salrashid.me/posts/redis_envoy/](https://blog.salrashid.me/posts/redis_envoy/).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis 与 Envoy 速查表 – 使用 TLS 和 Redis Auth 设置 Redis 和 Envoy：[https://blog.salrashid.me/posts/redis_envoy/](https://blog.salrashid.me/posts/redis_envoy/).
- en: '*Introduction to Modern Network Load Balancing and Proxying*, from Lyft''s
    Matt Klein: [https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236](https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*现代网络负载均衡和代理简介*，来自 Lyft 的 Matt Klein：[https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236](https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236).'
- en: '*Matt Klein on the Success of Envoy and the Future of the Service Mesh*: [https://thenewstack.io/matt-klein-on-the-success-of-envoy-and-the-future-of-the-service-mesh/](https://thenewstack.io/matt-klein-on-the-success-of-envoy-and-the-future-of-the-service-mesh/).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Matt Klein 谈 Envoy 的成功与服务网格的未来*：[https://thenewstack.io/matt-klein-on-the-success-of-envoy-and-the-future-of-the-service-mesh/](https://thenewstack.io/matt-klein-on-the-success-of-envoy-and-the-future-of-the-service-mesh/).'
- en: '*Cost Optimization for Kubernetes on AWS*. Once you get a handle on scaling,
    the next step is to reduce costs. The EKS cluster might cost between $10-20 per
    day to run with the defaults given in the AWS EKS Quick Start CloudFormation templates:
    [https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/](https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS上Kubernetes的成本优化*。一旦掌握了扩展操作，下一步就是降低成本。使用AWS EKS快速启动CloudFormation模板中的默认设置，EKS集群每天的运行成本可能在$10到$20之间：[https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/](https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/)。'
