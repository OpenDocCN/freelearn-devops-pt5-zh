<html><head></head><body><div class="chapter" title="Chapter&#xA0;15.&#xA0;Self-Healing Systems"><div class="titlepage"><div><div><h1 class="title"><a id="ch15"/>Chapter 15. Self-Healing Systems</h1></div></div></div><div class="blockquote"><table border="0" cellpadding="0" cellspacing="0" class="blockquote" summary="Block quote" width="100%"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>Healing takes courage, and we all have courage, even if we have to dig a little to find it.</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td align="right" colspan="2" style="text-align: center" valign="top">--<span class="attribution"><span class="emphasis"><em>Tori Amos</em></span></span></td></tr></table></div><p>Let's face it. The systems we are creating are not perfect. Sooner or later, one of our applications will fail, one of our services will not be able to handle the increased load, one of our commits will introduce a fatal bug, a piece of hardware will break, or something entirely unexpected will happen.</p><p>How do we fight the unexpected? Most of us are trying to develop a bullet proof system. We are attempting to create what no one did before. We strive for the ultimate perfection, hoping that the result will be a system that does not have any bugs, is running on hardware that never fails, and can handle any load. Here's a tip. There is no such thing as perfection. No one is perfect, and nothing is without fault. That does not mean that we should not strive for perfection. We should, when time and resources are provided. However, we should also embrace the inevitable, and design our systems not to be perfect, but able to recuperate from failures, and able to predict likely future. We should hope for the best but prepare for the worst.</p><p>There are plenty of examples of resilient systems outside software engineering, none of them better than life itself. We can take ourselves, humanity, as an example. We're the result of a very long experiment based on small and incremental evolutionary improvements, performed over millions of years. We can learn a lot from a human body, and apply that knowledge to our software and hardware. One of the fascinating abilities we (humans) possess is the capacity to self-heal.</p><p>Human body has an amazing capacity to heal itself. The most fundamental unit of human body is cell. Throughout our life, cells inside our body are working to bring us back to a state of equilibrium. Each cell is a dynamic, living unit that is continuously monitoring and adjusting its own processes, working to restore itself according to the original DNA code it was created with, and to maintain balance within the body. Cells have the ability to heal themselves, as well as to make new cells that replace those that have been permanently damaged or destroyed. Even when a large number of cells are destroyed, the surrounding cells replicate to make new cells, thereby quickly replacing the cells that were destroyed. This ability does not make us, individuals, immune to death, but it does make us very resilient. We are continuously attacked by viruses. We succumb to diseases and yet, in most cases, we come out victorious. However, looking at us as individuals would mean that we are missing the big picture. Even when our own lives end, the life itself not only survives, but thrives, ever growing, and ever adapting.</p><p>We can think of a computer system as a human body that consists of cells of various types. They can be hardware or software. When they are software units, the smaller they are, the easier it is for them to self-heal, recuperate from failures, multiply, or even get destroyed when that is needed. We call those small units microservices, and they can, indeed, have behaviors similar to those observed in a human body. The microservices-based system we are building can be made in a way that is can self-heal. That is not to say that self-healing we are about to explore is applicable only to microservices. It is not. However, like most other techniques we explored, self-healing can be applied to almost any type of architecture, but provides best results when combined with microservices. Just like life that consists of individuals that form a whole ecosystem, each computer system is part of something bigger. It communicates, cooperates, and adapts to other systems forming a much larger whole.</p><div class="section" title="Self-Healing Levels and Types"><div class="titlepage"><div><div><h1 class="title"><a id="ch15lvl1sec38"/>Self-Healing Levels and Types</h1></div></div></div><p>In software systems, the <a class="indexterm" id="id689"/>self-healing term describes any application, service, or a system that can discover that it is not working correctly and, without any human intervention, make the necessary changes to restore itself to the normal or designed state. Self-healing is about making the system capable of making its decisions by continually checking and optimizing its state and automatically adapting to changing conditions. The goal is to make fault tolerant and responsive system capable of responding to<a class="indexterm" id="id690"/> changes in demand and recuperation from failures.</p><p>Self-healing systems can be divided into three levels, depending on size and type of resources we are monitoring, and acting upon. Those levels are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Application level</li><li class="listitem" style="list-style-type: disc">System level</li><li class="listitem" style="list-style-type: disc">Hardware level</li></ul></div><p>We'll explore each of those three types separately.</p><div class="section" title="Self-Healing on the Application Level"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec84"/>Self-Healing on the Application Level</h2></div></div></div><p>Application level healing is the ability of an application, or a service, to heal itself internally. Traditionally, we're used to capturing problems through exceptions and, in most cases, logging them for further examination. When such an exception occurs, we tend to ignore it and move on (after logging), as if nothing happened, hoping for the best in the future. In other cases, we tend to stop the application if an exception of certain type occurs. An example<a class="indexterm" id="id691"/> would be a connection to a database. If the connection is not established when the application starts, we often stop the whole process. If we are a bit more experienced, we might try to repeat the attempt to connect to the database. Hopefully, those attempts are limited, or we might easily enter a never ending loop, unless database connection failure was temporary and the DB gets back online soon afterwards. With time, we got better ways to deal with problems inside applications. One of them is Akka. It's usage of supervisor, and design patterns it promotes, allow us to create internally self-healing applications and services. Akka is not the only one. Many other libraries and frameworks enable us to create fault tolerant applications capable of recuperation from potentially disastrous circumstances. Since we are trying to be agnostic to programming languages, I'll leave it to you, dear reader, investigation of ways to self-heal your applications internally. Bear in mind that self-healing in this context refers to internal processes and does not provide, for example, recuperation from failed processes. Moreover, if we adopt microservices architecture, we can quickly end up with services written in different languages, using different frameworks, and so on. It is truly up to developers of each service to design it in a way that it can heal itself and recuperate from failures.</p><p>Let's jump into the second level.</p></div><div class="section" title="Self-Healing on the System Level"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec85"/>Self-Healing on the System Level</h2></div></div></div><p>Unlike the application level healing that depends on a programming language and design patterns that we apply internally, system level self-healing can be generalized and be applied to all services and applications, independently from their internals. This is the type of self-healing that we can design on the level of the whole system. While there are many things that can happen at the system level, the two most commonly monitored aspects are failures of <a class="indexterm" id="id692"/>processes and response time. If a process fails, we need to redeploy the service, or restart the process. On the <a class="indexterm" id="id693"/>other hand, if the response time is not adequate, we need to scale, or descale, depending whether we reached upper or lower response time limits. Recuperating from process failures is often not enough. While such actions might restore our system to the desired state, human intervention is often still needed. We need to investigate the cause of the failure, correct the design of the service, or fix a bug. That is, self-healing often goes hand in hand with investigation of the causes of that failure. The system automatically recuperates and we (humans) try to learn from those failures, and improve the system as a whole. For that reason, some kind of a notification is required as well. In both cases (failures and increased traffic), the system needs to monitor itself and take some actions.</p><p>How does the system<a class="indexterm" id="id694"/> monitor itself? How does it check<a class="indexterm" id="id695"/> the status of its components? There are many ways, but two most commonly used are TTLs and pings.</p><div class="section" title="Time-To-Live"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl3sec39"/>Time-To-Live</h3></div></div></div><p>
<span class="strong"><strong>Time-to-live</strong></span> (<span class="strong"><strong>TTL</strong></span>) checks expect a service, or an application, to periodically confirm that it is operational. The system that receives TTL signals keeps track of the last known reported state for a given TTL. If that state is not updated within a predefined period, the monitoring system <a class="indexterm" id="id696"/>assumes that the service failed and needs to be restored to its designed state. For example, a healthy service could send an HTTP request announcing that it is alive. If the process the service is running in fails, it will be incapable to send the request, TTL will expire, and reactive measures will be executed.</p><p>The main problem with TTL is coupling. Applications and services need to be tied to the monitoring system. Implementing TTL would be one of the microservices anti-patterns since we are trying to design them in a way that they are as autonomous as possible. Moreover, microservices should have a clear function and a single purpose. Implementing TTL requests inside them would add additional functionality and complicate the development:</p><div class="mediaobject"><img alt="Time-To-Live" src="graphics/B05848_15_01.jpg"/><div class="caption"><p>Figure 15-01 – System level self-healing with time-to-live (TTL)</p></div></div></div><div class="section" title="Pinging"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl3sec40"/>Pinging</h3></div></div></div><p>The idea behind pinging is to check the state of an application, or a service, externally. The monitoring <a class="indexterm" id="id697"/>system should ping each service periodically and, if no response is received, or the content of the response is not adequate, execute healing measures. Pinging can come in many forms. If a service exposes HTTP API, it is often a simple request, where desired response should be HTTP status in 2XX range. In other cases, when HTTP API is not exposed, pinging can be done with a script, or any other method that can validate the state of the service.</p><p>Pinging is opposite from TTL, and, when possible, is a preferable way of checking the status of individual parts of the system. It removes repetition, coupling, and complications that could occur when implementing TTL inside each service.</p><div class="mediaobject"><img alt="Pinging" src="graphics/B05848_15_02.jpg"/><div class="caption"><p>Figure 15-02 – System level self-healing with pings</p></div></div></div></div><div class="section" title="Self-Healing on the Hardware Level"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec86"/>Self-Healing on the Hardware Level</h2></div></div></div><p>Truth be told, there is<a class="indexterm" id="id698"/> no such a thing as hardware self-healing. We cannot have a process that will automatically heal failed memory, repare broken hard disk, fix malfunctioning CPU, and so on. What healing on this level truly means is redeployment of services from an unhealthy to one of the healthy nodes. As with the system level, we need to periodically check the status of different hardware components, and act accordingly. Actually, most healing caused due to hardware level will happen at the system level. If hardware is not working correctly, chances are that the service will fail, and thus be fixed by system level healing. Hardware level healing is more related to preventive types of checks that we'll discuss shortly:</p><div class="mediaobject"><img alt="Self-Healing on the Hardware Level" src="graphics/B05848_15_03.jpg"/><div class="caption"><p>Figure 15-03 – Hardware level self-healing</p></div></div><p>Besides the division <a class="indexterm" id="id699"/>based on the check levels, we can also divide it based on the moment actions are taken. We can react to a failure, or we can try to prevent it.</p></div><div class="section" title="Reactive healing"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec87"/>Reactive healing</h2></div></div></div><p>Most of the organizations that implemented some kind of self-healing systems focused on reactive healing. After a failure is detected, the system reacts and restores itself to the designed state. A service process is dead, ping returns the code 404 (not found), corrective actions are<a class="indexterm" id="id700"/> taken, and the service is operational again. This works no matter whether service failed because its process failed, or the whole node stopped being operational (assuming that we have a system that can redeploy to a healthy node). This is the most important type of healing and, at the same time, the easiest one to implement. As long as we have all the checks in place, as well as actions that should be performed in case of a failure, and we have each service scaled to at least two instances distributed on separate physical nodes, we should (almost) never have downtime. I said almost never because, for example, the whole datacenter might loose power, thus stopping all nodes. It's all about evaluating risks against costs of preventing those risks.</p><p>Sometimes, it is worthwhile to have two datacenters in different locations, and in other cases it's not. The objective is to strive towards zero-downtime, while accepting that some cases are not worthwhile trying to prevent.</p><p>No matter whether we are striving for zero-downtime, or almost zero-downtime, reactive self-healing should be a must for any but smallest settings, especially since it does not require big investment. You might invest in spare hardware, or you might invest in separate datacenters. Those decisions are not directly related with self-healing, but with the level of risks that are acceptable for a given use case. Reactive self-healing investment is primarily in knowledge how to do it, and time to implement it. While time is an investment in itself, we can spend it wisely, and create a general solution that would work for (almost) all <a class="indexterm" id="id701"/>cases, thus reducing the time we need to spend implementing such a system.</p></div><div class="section" title="Preventive healing"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec88"/>Preventive healing</h2></div></div></div><p>The idea behind preventive healing is to predict the problems we might have in the future, and act in a way that those problems are avoided. How do we predict the future? To be more precise, what data do we use to predict the future?</p><p>Relatively easy, but less<a class="indexterm" id="id702"/> reliable way of predicting the future, is to base assumptions on (near) real-time data. For example, if one of the HTTP requests we're using to check the health of a service responded in more than 500 milliseconds, we might want to scale that service. We can even do the opposite. Following the same example, if it took less than 100 milliseconds to receive the response, we might want to descale the service, and reassign those resources to another one that might need it more. The problem with taking into account the current status when predicting the future is variability. If it took a long time between the request and the response, it might indeed be the sign that scaling is needed, but it might also be a temporary increase in traffic, and the next check (after the traffic spike is gone) will deduce that there is a need to descale. If microservices architecture is applied, this can be a minor issue, since they are small and easy to move around. They are easy to scale, and descale. Monolithic applications are often much more problematic if this strategy is chosen.</p><p>If historical data is taken into account, preventive healing becomes much more reliable but, at the same time, much more complicated to implement. Information (response times, CPU, memory, and so on) needs to be stored somewhere and, often complex, algorithms need to be employed to evaluate tendencies, and make conclusions. For example, we might observe that, during the last hour, memory usage has been steadily increasing, and that it reached a critical point of, let's say, 90%. That would be a clear indication that the service that is causing that increase needs to be scaled. The system could also take into account longer period of time, and deduce that every Monday there is a sudden increase in traffic, and scale services well in advance to prevent long responses. What would be, for example, the meaning of a steady increase in memory usage from the moment a service is deployed, and sudden decrease when a new version is released? Probably memory leaks and, in such a case, the system would need to restart the application when certain threshold is reached, and hope that developers would fix the issue (hence the need for notifications).</p><p>Let us change the focus, and discuss architecture.</p></div></div></div>
<div class="section" title="Self-Healing Architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch15lvl1sec39"/>Self-Healing Architecture</h1></div></div></div><p>No matter the internal <a class="indexterm" id="id703"/>processes and tools, every self-healing system will have some common elements.</p><p>In the very beginning, there is a cluster. A single server cannot be made fault tolerant. If a piece of its hardware fails, there is nothing we can do to heal that. There is no readily available replacement. Therefore, the system must start with a cluster. It can be composed out of two or two hundred servers. The size is not of the essence, but the ability to move from one hardware to another in the case of a failure. Bear in mind that we always need to evaluate benefits versus costs. If financially viable, we would have at least two physically and geographically separated datacenters. In such a case, if there is a power outage in one, the other one would be fully operational. However, in many instances that is not a financially viable option:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_04.jpg"/><div class="caption"><p>Figure 15-04 – Self-healing system architecture: Everything starts with a cluster</p></div></div><p>Once we have the cluster up and running, we can begin deploying our services. However, managing services inside a cluster without some orchestrator is tedious, at best. It requires time and often ends up with a very unbalanced usage of resources:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_05.jpg"/><div class="caption"><p>Figure 15-05 – Self-healing system architecture: Services are deployed to the cluster, but with a very unbalanced usage of resources</p></div></div><p>In most cases, people treat a cluster as a set of individual servers, which is wrong, knowing that today we have tools at our disposal that can help us do the orchestration in a much better way. With Docker Swarm, Kubernetes, or Apache Mesos, we can solve the orchestration within a cluster. Cluster orchestration is important, not only to ease the deployment of our services, but also as a way to provide fast re-deployments to healthy nodes in case of a failure (be it of software or hardware nature). Bear in mind that we need at least two instances of every service running behind a proxy. Given such a situation, if one instance fails, the others can take over its load, thus avoiding any downtime while the system re-deploys the failed instance:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_06.jpg"/><div class="caption"><p>Figure 15-06 – Self-healing system architecture: Some deployment orchestrator is required to distribute services across the cluster</p></div></div><p>The basis of any self-healing system is monitoring of the state of deployed services, or applications, as well as the underlying hardware. The only way we can monitor them is to have information <a class="indexterm" id="id704"/>about their existence. That information can be available in many different forms, ranging from manually maintained configuration files, through traditional databases, all the way until highly available distributed service registries like <code class="literal">Consul</code>, <code class="literal">etcd</code>, or <code class="literal">Zookeeper</code>. In some cases, the service registry can be chosen by us, while in others it comes as part of the cluster orchestrator. For example, Docker Swarm has the flexibility that allows it to work with a couple of registries, while Kubernetes is tied to <code class="literal">etcd</code>:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_07.jpg"/><div class="caption"><p>Figure 15-07 – Self-healing system architecture: Primary requirement for monitoring the state of the system is to have the information of the system stored service registry</p></div></div><p>No matter the tool we choose to act as a service registry, the next obstacle is to put the information into the service registry of choice. The principle is a simple one. Something needs to monitor hardware and services and update the registry whenever a new one is added, or an existing one is removed. There are plenty of tools that can do that. We are already familiar with Registrator, which fulfills this role pretty well. As with service registries, some <a class="indexterm" id="id705"/>cluster orchestrators already come with their own ways to register and de-register services. No matter which tool we choose, the primary requirement is to be able to monitor the cluster and send information to service registry in near-realtime:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_08.jpg"/><div class="caption"><p>Figure 15-08 – Self-healing system architecture: Service registry is useless if no mechanism will monitor the system and store new information</p></div></div><p>Now that we have the cluster with services up and running, and we have the information about the system in the service registry, we can employ some health monitoring that will detect anomalies. Such a tool needs to know not only what the desired state is, but, also, what the actual situation is at any moment. Consul Watches can fulfill this role while Kubernetes and Mesos come with their own tools for this type of tasks. In a more traditional environment, Nagios or Icinga (only to name a few), can fulfill this role as well:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_09.jpg"/><div class="caption"><p>Figure 15-09 – Self-healing system architecture: With all the relevant information stored in a service registry, some health monitoring tools can utilize it to verify whether the desired state is maintained</p></div></div><p>The next piece of the puzzle is a tool that would be able to execute corrective actions. When the health monitor <a class="indexterm" id="id706"/>detects an anomaly, it would send a message to perform a corrective measure. As a minimum, that corrective action should send a signal to the cluster orchestrator, which, in turn, would redeploy the failed service. Even if a failure was caused by a hardware problem, cluster orchestrator would (temporarily) fix that by redeploying the service to a healthy node. In most cases, corrective actions are not that simple. There could be a mechanism to notify interested parties, record what happened, revert to an older version of the service, and so on. We already adopted Jenkins, and it is a perfect fit to act as the tool that can receive a message from the health monitor and, as a result, initiate corrective actions:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_10.jpg"/><div class="caption"><p>Figure 15-10 – Self-healing system architecture: As a minimum, corrective action should send a signal to the cluster orchestrator to redeploy the service that failed</p></div></div><p>The process, as it is for now, is dealing only with reactive healing. The system is continuously monitored and, if a failure is detected, corrective actions are taken, which, in turn, will restore the system to the desired state. Can we take it a step further and try to accomplish preventive healing? Can we predict the future and act accordingly? In many cases we can, in some <a class="indexterm" id="id707"/>we can't. We cannot know that a hard disk will fail tomorrow. We cannot predict that there will be an outage today at noon. However, in some cases, we can see that the traffic is increasing, and will soon reach a point that will require some of our services to be scaled. We can predict that a marketing campaign we are about to launch will increase the load. We can learn from our mistakes, and teach the system how to behave in certain situations. The essential elements of such a set of processes are similar to those we should employ for reactive healing. We need a place to store data and a process that collects them. Unlike service registry that deals with a relatively small amount of data and benefits from being distributed, preventive healing requires quite bigger storage and capabilities that would allow us to perform some analytic operations:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_11.jpg"/><div class="caption"><p>Figure 15-11 – Self-healing system architecture: Preventive healing requires historical data to be analyzed</p></div></div><p>Similarly to the registrator service, we'll also need some data collector that will be sending historical data. That data can be quite massive and include, but not be limited by, CPU, HD, network<a class="indexterm" id="id708"/> traffic, system and service logs, and so on. Unlike the registrator that listens to events, mostly generated by the cluster orchestrator, data collector should be continuously collecting data, digesting the input, and producing an output that should be stored as historical data:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_12.jpg"/><div class="caption"><p>Figure 15-12 – Self-healing system architecture: Preventive healing requires vast quantities of data to be collected continuously</p></div></div><p>We already used some of the tools needed for reactive self-healing. Docker Swarm can be used as the cluster orchestrator, Registrator and Consul for service discovery, and Jenkins for performing, among other duties, corrective actions. The only tool that we haven't used are two<a class="indexterm" id="id709"/> subsets of Consul; checks and watches. Preventive healing will require exploration of some new processes and tools, so we'll leave it for later on:</p><div class="mediaobject"><img alt="Self-Healing Architecture" src="graphics/B05848_15_13.jpg"/><div class="caption"><p>Figure 15-13 – Self-healing system architecture: One of the combinations of tools</p></div></div><p>Let's see if we can set up a sample reactive self-healing system.</p></div>
<div class="section" title="Self-Healing with Docker, Consul Watches, and Jenkins"><div class="titlepage"><div><div><h1 class="title"><a id="ch15lvl1sec40"/>Self-Healing with Docker, Consul Watches, and Jenkins</h1></div></div></div><p>The good news is that <a class="indexterm" id="id710"/>we already used all the tools that we require to make a reactive self-healing system. We have Swarm that will make sure that containers will be deployed to healthy nodes (or at least nodes that are operational). We have<a class="indexterm" id="id711"/> Jenkins that can be used to execute the healing process and, potentially, send notifications. Finally, we can use Consul not only to store service information, but also to perform health checks and send requests to Jenkins. The <a class="indexterm" id="id712"/>only piece we haven't used until now are Consul watches that can be programmed to perform health checks.</p><p>One thing to note about how Consul does health checks is that it differs from traditional way Nagios and other similar tools are operating. Consul avoids the thundering herd problem by using gossip, and only alerts on state changes.</p><p>As always, we'll start by creating VMs we'll use throughout the rest of the chapter. We'll create the familiar combination of one <code class="literal">cd</code> and three <code class="literal">swarm</code> servers (one master and two nodes).</p><div class="section" title="Setting Up the Environments"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec89"/>Setting Up the Environments</h2></div></div></div><p>The following command will create the four VMs we'll use in this chapter. We'll create the <code class="literal">cd</code> node and use it to<a class="indexterm" id="id713"/> provision the other nodes with Ansible. This VM will also host Jenkins, that will be an important part of the self-healing <a class="indexterm" id="id714"/>process. The other three VMs will form the Swarm cluster.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd swarm-master swarm-node-1 swarm-node-2</strong></span>
</pre></div><p>With all the VMs operational, we can proceed and set up the Swarm cluster. We'll start by provisioning the cluster in the same way as we did before, and then discuss changes we need to make it self-heal.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant ssh cd</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/swarm.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>
</pre></div><p>Finally, the time has come to provision the <code class="literal">cd</code> server with Jenkins.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook /vagrant/ansible/jenkins-node-swarm.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/jenkins.yml \</strong></span>
<span class="strong"><strong>    --extra-vars "main_job_src=service-healing-config.xml" \</strong></span>
<span class="strong"><strong>    -c local</strong></span>
</pre></div><p>We reached the point where the whole cluster is operational, and Jenkins server will be up and running soon. We set one Swarm master (<code class="literal">swarm-master</code>), two Swarm nodes (<code class="literal">swarm-node-1</code> and <code class="literal">swarm-node-2</code>), and one server with Ansible and, soon to be running, Jenkins (<code class="literal">cd</code>). Feel free to continue reading while Jenkins provisioning is running. We won't need it right away.</p><div class="section" title="Setting Up Consul Health Checks and Watches for Monitoring Hardware"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl3sec41"/>Setting Up Consul Health Checks and Watches for Monitoring Hardware</h3></div></div></div><p>We can send instructions to<a class="indexterm" id="id715"/> Consul to perform periodic checks of services or entire nodes. It does not come with predefined checks. Instead, it runs scripts, performs HTTP requests, or wait for TTL signals defined by us. While the lack of predefined checks might seem like a disadvantage, it gives us the freedom to design the process as we see fit. In case we're using scripts to perform checks, Consul will expect them to exit with certain codes. If we exit from the check script with the <code class="literal">code 0</code>, Consul will assume that everything works correctly. Exit <code class="literal">code 1</code> is expected to be a warning, and the exit <code class="literal">code 2</code> is an error.</p><p>We'll start by creating a few scripts that will perform hardware checks. Getting information of, let's say, hard disk utilization is relatively easy with the <code class="literal">df</code> command.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>df -h</strong></span>
</pre></div><p>We used the <code class="literal">-h</code> argument to output <code class="literal">human-readable</code> information, and the output is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Filesystem      Size  Used Avail Use% Mounted on</strong></span>
<span class="strong"><strong>udev            997M   12K  997M   1% /dev</strong></span>
<span class="strong"><strong>tmpfs           201M  440K  200M   1% /run</strong></span>
<span class="strong"><strong>/dev/sda1        40G  4.6G   34G  13% /</strong></span>
<span class="strong"><strong>none            4.0K     0  4.0K   0% /sys/fs/cgroup</strong></span>
<span class="strong"><strong>none            5.0M     0  5.0M   0% /run/lock</strong></span>
<span class="strong"><strong>none           1001M     0 1001M   0% /run/shm</strong></span>
<span class="strong"><strong>none            100M     0  100M   0% /run/user</strong></span>
<span class="strong"><strong>none            465G  118G  347G  26% /vagrant</strong></span>
<span class="strong"><strong>none            465G  118G  347G  26% /tmp/vagrant-cache</strong></span>
</pre></div><p>Bear in mind that in your case the output might be slightly different.</p><p>What we truly need are numbers from the root directory (the third row in the output). We can filter the <a class="indexterm" id="id716"/>output of the <code class="literal">df</code> command so that only the row with the value <code class="literal">/</code> of the last column is displayed. After the filter, we should extract the percentage of used disk space (column 5). While we are extracting data, we might just as well get the disk size (column 2), and the amount of used space (column 3). Data that we extract should be stored as variables that we could use later on. The commands we can use to accomplish all that is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>set -- $(df -h | awk '$NF=="/"{print $2" "$3" "$5}')</strong></span>
<span class="strong"><strong>total=$1</strong></span>
<span class="strong"><strong>used=$2</strong></span>
<span class="strong"><strong>used_percent=${3::-1}</strong></span>
</pre></div><p>Since the value that represents the used space percentage contains the <code class="literal">%</code> sign, we removed the last character before assigning the value to the <code class="literal">used_percent</code> variable.</p><p>We can double-check whether the variables we created contain correct values with a simple <code class="literal">printf</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>printf "Disk Usage: %s/%s (%s%%)\n" $used $total $used_percent</strong></span>
</pre></div><p>The output of the last command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Disk Usage: 4.6G/40G (13%)</strong></span>
</pre></div><p>The only thing left is to exit with 1 (warning) or 2 (error) when a threshold is reached. We'll define the error threshold as 95% and warning as 80%. The only thing missing is a simple <code class="literal">if</code>/<code class="literal">elif</code>/<code class="literal">else</code> statement:</p><div class="informalexample"><pre class="programlisting">if [ $used_percent -gt 95 ]; then
  echo "Should exit with 2"
elif [ $used_percent -gt 80 ]; then
  echo "Should exit with 1"
else
  echo "Should exit with 0"
fi</pre></div><p>For testing purposes, we put echos. The script that we are about to make should exit with <code class="literal">2</code>, <code class="literal">1</code> or <code class="literal">0</code>.</p><p>Let's move into the <code class="literal">swarm-master</code> node, create the script, and test it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant ssh swarm-master</strong></span>
</pre></div><p>We'll start by creating a directory where Consul scripts will reside:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo mkdir -p /data/consul/scripts</strong></span>
</pre></div><p>Now we can<a class="indexterm" id="id717"/> create the script with the commands we practiced:</p><div class="informalexample"><pre class="programlisting">echo '#!/usr/bin/env bash
set -- $(df -h | awk '"'"'$NF=="/"{print $2" "$3" "$5}'"'"')
total=$1
used=$2
used_percent=${3::-1}
printf "Disk Usage: %s/%s (%s%%)\n" $used $total $used_percent
if [ $used_percent -gt 95 ]; then
  exit 2
elif [ $used_percent -gt 80 ]; then
  exit 1
else
  exit 0
fi</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>' | sudo tee /data/consul/scripts/disk.sh</strong></span>
<span class="strong"><strong>sudo chmod +x /data/consul/scripts/disk.sh</strong></span>
</pre></div><p>Let's try it out. Since there's quite a lot of free disk space, the script should echo the disk usage and return zero:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/data/consul/scripts/disk.sh</strong></span>
</pre></div><p>The command provided an output similar to the following.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Disk Usage: 3.3G/39G (9%)</strong></span>
</pre></div><p>We can easily display the exit code of the last command with <code class="literal">$?</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo $?</strong></span>
</pre></div><p>The echo returned zero, and the script seems to be working. You can test the rest of exit codes by modifying the threshold to be below the current disk usage. I'll leave that to you, as a simple exercise.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>
<span class="strong"><strong>Consul check threshold exercise</strong></span>
</p><p>Modify the <code class="literal">disk.sh</code> script in a way that warning and error thresholds are lower than the current HD usage. Test the changes by running the script and outputting the exit code. Once the exercise is done, revert the script to its original values.</p></div></div><p>Now that we have<a class="indexterm" id="id718"/> the script that checks the disk usage, we should tell Consul about its existence. Consul uses JSON format for specifying checks. The definition that utilizes the script we just created is as follows:</p><div class="informalexample"><pre class="programlisting">{
  "checks": [
    {
      "id": "disk",
      "name": "Disk utilization",
      "notes": "Critical 95% util, warning 80% util",
      "script": "/data/consul/scripts/disk.sh",
      "interval": "10s"
    }
  ]
}</pre></div><p>That JSON would tell Consul that there is a check with the ID <code class="literal">disk</code>, name <code class="literal">Disk utilization</code> and notes <code class="literal">Critical 95% util</code>, <code class="literal">warning 80% util</code>. The <code class="literal">name</code> and <code class="literal">notes</code> are purely for visualization purposes (as you'll see soon). Next, we are specifying the path to the script to be <code class="literal">/data/consul/scripts/disk.sh</code>. Finally, we are telling Consul to run the script every <code class="literal">10</code> seconds:</p><p>Let's create the JSON file:</p><div class="informalexample"><pre class="programlisting">echo '{
  "checks": [
    {
      "id": "disk",
      "name": "Disk utilization",
      "notes": "Critical 95% util, warning 80% util",
      "script": "/data/consul/scripts/disk.sh",
      "interval": "10s"
    }
  ]
}</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo tee /data/consul/config/consul_check.json</strong></span>
</pre></div><p>When we started Consul (through the Ansible playbook), we specified that configuration files are<a class="indexterm" id="id719"/> located in the <code class="literal">/data/consul/config/</code> directory. We still need to reload it, so that it picks up the new file we just created. The easiest way to reload Consul is by sending it the <code class="literal">HUP</code> signal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo killall -HUP consul</strong></span>
</pre></div><p>We managed to create hard disk checks in Consul. It will run the script every ten seconds and, depending on its exit code, determine the health of the node it runs on (in this case <code class="literal">swarm-master</code>):</p><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Hardware" src="graphics/B05848_15_14.jpg"/><div class="caption"><p>Figure 15-14 – Hard disk checks in Consul</p></div></div><p>Let's take a look at the Consul UI by opening <code class="literal">http://10.100.192.200:8500/ui/</code> from a browser. Once<a class="indexterm" id="id720"/> the UI is opened, please click the <span class="strong"><strong>Nodes</strong></span> button, and then the <code class="literal">swarm-master</code> node. Among other information, you'll see two checks. One of them is <code class="literal">Serf Health Status</code>. It's Consul's internal check based on TTL. If one of the Consul nodes is down, that information will be propagated throughout the cluster. The check check, called <code class="literal">Disk utilization</code>, is the one we just created, and, hopefully, the status is <code class="literal">passing</code>:</p><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Hardware" src="graphics/B05848_15_15.jpg"/><div class="caption"><p>Figure 15-15 – Hard disk checks in Consul UI</p></div></div><p>Now that we know how easy it is to add a check in Consul, we should define what action should be performed when a check fails. We do that through Consul watches. As with checks, Consul does not offer an out-of-the-box final solution. It provides a mechanism for us to create the solution that fits our needs.</p><p>Consul supports seven different types of watches:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>key</strong></span>: Watch a <a class="indexterm" id="id721"/>specific KV pair</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>keyprefix</strong></span>: Watch a<a class="indexterm" id="id722"/> prefix in the KV store</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>services</strong></span>: Watch the list <a class="indexterm" id="id723"/>of available services</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>nodes</strong></span>: Watch<a class="indexterm" id="id724"/> the list of nodes</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>service</strong></span>: Watch the<a class="indexterm" id="id725"/> instances of a service</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>checks</strong></span>: Watch the <a class="indexterm" id="id726"/>value of health checks</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>event</strong></span>: Watch for <a class="indexterm" id="id727"/>custom user events</li></ul></div><p>Each of the types is useful in certain situations and, together, they provide a very comprehensive framework for building your self-healing, fault tolerant, system. We'll concentrate on the <code class="literal">checks</code> type, since it will allow us to utilize the hard disk check we created earlier. Please <a class="indexterm" id="id728"/>consult the watches documentation for more info.</p><p>We'll start by creating the script that will be run by Consul watcher. The <code class="literal">manage_watches.sh</code> script is as follows (please don't run it):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/usr/bin/env bash</strong></span>

<span class="strong"><strong>RED="\033[0;31m"</strong></span>
<span class="strong"><strong>NC="\033[0;0m"</strong></span>

<span class="strong"><strong>read -r JSON</strong></span>
<span class="strong"><strong>echo "Consul watch request:"</strong></span>
<span class="strong"><strong>echo "$JSON"</strong></span>

<span class="strong"><strong>STATUS_ARRAY=($(echo "$JSON" | jq -r ".[].Status"))</strong></span>
<span class="strong"><strong>CHECK_ID_ARRAY=($(echo "$JSON" | jq -r ".[].CheckID"))</strong></span>
<span class="strong"><strong>LENGTH=${#STATUS_ARRAY[*]}</strong></span>

<span class="strong"><strong>for (( i=0; i&lt;=$(( $LENGTH -1 )); i++ ))</strong></span>
<span class="strong"><strong>do</strong></span>
<span class="strong"><strong>    CHECK_ID=${CHECK_ID_ARRAY[$i]}</strong></span>
<span class="strong"><strong>    STATUS=${STATUS_ARRAY[$i]}</strong></span>
<span class="strong"><strong>    echo -e "${RED}Triggering Jenkins job http://10.100.198.200:8080/job/hardware-notification/build${NC}"</strong></span>
<span class="strong"><strong>    curl -X POST http://10.100.198.200:8080/job/hardware-notification/build \</strong></span>
<span class="strong"><strong>        --data-urlencode json="{\"parameter\": [{\"name\":\"checkId\", \"value\":\"$CHECK_ID\"}, {\"name\":\"status\", \"value\":\"$STATUS\"}]}"</strong></span>
<span class="strong"><strong>done</strong></span>
</pre></div><p>We started by defining <code class="literal">RED</code> and <code class="literal">NC</code> variables that will help us paint critical parts of the output in red. Then, we are reading the Consul input and storing it into the <code class="literal">JSON</code> variable. That is followed by the creation of <code class="literal">STATUS_ARRAY</code> and <code class="literal">CHECK_ID_ARRAY</code> arrays that will hold <code class="literal">Status</code> and <code class="literal">CheckID</code> values for each element from the JSON. Finally, those arrays allow us to iterate through each item, and send a POST request to Jenkins to build the <code class="literal">hardware-notification</code> job (we'll take a look at it later). The request uses Jenkins friendly format for passing the <code class="literal">CHECK_ID</code> and <code class="literal">STATUS</code> variables. Please consult Jenkins remote access API for more information.</p><p>Let's create the script:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo '#!/usr/bin/env bash</strong></span>

<span class="strong"><strong>RED="\033[0;31m"</strong></span>
<span class="strong"><strong>NC="\033[0;0m"</strong></span>

<span class="strong"><strong>read -r JSON</strong></span>
<span class="strong"><strong>echo "Consul watch request:"</strong></span>
<span class="strong"><strong>echo "$JSON"</strong></span>

<span class="strong"><strong>STATUS_ARRAY=($(echo "$JSON" | jq -r ".[].Status"))</strong></span>
<span class="strong"><strong>CHECK_ID_ARRAY=($(echo "$JSON" | jq -r ".[].CheckID"))</strong></span>
<span class="strong"><strong>LENGTH=${#STATUS_ARRAY[*]}</strong></span>

<span class="strong"><strong>for (( i=0; i&lt;=$(( $LENGTH -1 )); i++ ))</strong></span>
<span class="strong"><strong>do</strong></span>
<span class="strong"><strong>    CHECK_ID=${CHECK_ID_ARRAY[$i]}</strong></span>
<span class="strong"><strong>    STATUS=${STATUS_ARRAY[$i]}</strong></span>
<span class="strong"><strong>    echo -e "${RED}Triggering Jenkins job http://10.100.198.200:8080/job/hardware-notification/build${NC}"</strong></span>
<span class="strong"><strong>    curl -X POST http://10.100.198.200:8080/job/hardware-notification/build \</strong></span>
<span class="strong"><strong>        --data-urlencode json="{\"parameter\": [{\"name\":\"checkId\", \"value\":\"$CHECK_ID\"}, {\"name\":\"status\", \"value\":\"$STATUS\"}]}"</strong></span>
<span class="strong"><strong>done</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>' | sudo tee /data/consul/scripts/manage_watches.sh</strong></span>

<span class="strong"><strong>sudo chmod +x /data/consul/scripts/manage_watches.sh</strong></span>
</pre></div><p>Now that we <a class="indexterm" id="id729"/>have the script that will be executed whenever there is a check with the <code class="literal">warning</code> or <code class="literal">critical</code> status, we'll inform Consul about its existence. The Consul watches definition is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "watches": [</strong></span>
<span class="strong"><strong>    {</strong></span>
<span class="strong"><strong>      "type": "checks",</strong></span>
<span class="strong"><strong>      "state": "warning",</strong></span>
<span class="strong"><strong>      "handler": "/data/consul/scripts/manage_watches.sh &gt;&gt;/data/consul/logs/watches.log"</strong></span>
<span class="strong"><strong>    }, {</strong></span>
<span class="strong"><strong>      "type": "checks",</strong></span>
<span class="strong"><strong>      "state": "critical",</strong></span>
<span class="strong"><strong>      "handler": "/data/consul/scripts/manage_watches.sh &gt;&gt;/data/consul/logs/watches.log"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>  ]</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>This definition should be self-explanatory. We defined two watches, both of type <code class="literal">checks</code>. The first one will be run in case of a <code class="literal">warning</code>, and the second when a check is in the <code class="literal">critical</code> state. We're trying to keep things simple by specifying, in both instances, the<a class="indexterm" id="id730"/> same handler <code class="literal">manage_watches.sh</code>. In a real world setting, you should differentiate those two states and run different actions.</p><p>Let's create the watches file:</p><div class="informalexample"><pre class="programlisting">echo '{
  "watches": [
    {
      "type": "checks",
      "state": "warning",
      "handler": "/data/consul/scripts/manage_watches.sh &gt;&gt;/data/consul/logs/watches.log"
    }, {
      "type": "checks",
      "state": "critical",
      "handler": "/data/consul/scripts/manage_watches.sh &gt;&gt;/data/consul/logs/watches.log"
    }
  ]
}'</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo tee /data/consul/config/watches.json</strong></span>
</pre></div><p>Before we proceed, and reload Consul, we should have a quick discussion about the Jenkins job <code class="literal">hardware-notification</code>. It was already created when we provisioned Jenkins. Its configuration can be seen by opening <code class="literal">http://10.100.198.200:8080/job/hardware-notification/configure</code>. It contains two parameters, <code class="literal">checkId</code> and <code class="literal">status</code>. We're using those two parameters as a way to avoid creating separate jobs for each hardware check. Whenever Consul watcher sends the POST request to build this job, it passes values to those two variables. In the build phase, we are simply running an <code class="literal">echo</code> command that sends values of those two variables to standard output (<code class="literal">STDOUT</code>). In a real world situation, this job would do some actions. For example, if disk space is low, it could remove unused logs and temporary files. Another example would be creation of additional nodes, if we're using one of the cloud services like Amazon AWS. In some other situations, no automated reaction is possible. In any case, besides concrete actions like those, this job should also send some kind of a notification (email, instant messaging, and so on) so that operators are informed about the potential problem. Since those situations would be difficult to reproduce locally, the<a class="indexterm" id="id731"/> initial definition of this job does nothing of the sort. I'll leave it up to you to extend it for your own needs.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>
<span class="strong"><strong>The hardware-notification Jenkins job exercise</strong></span>
</p><p>Modify the <code class="literal">hardware-notification</code> Jenkins job so that logs are deleted in case the <code class="literal">checkId</code> value is <code class="literal">disk</code>. Create mock logs (feel free to use the <code class="literal">touch</code> command to create files) on the server and run the job manually. Once the job build is finished, confirm that the logs were indeed removed.</p></div></div><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Hardware" src="graphics/B05848_15_16.jpg"/><div class="caption"><p>Figure 15-16 – Settings screen of the Jenkins job hardware-notification</p></div></div><p>The problem we have right now is that the hard disk on the <code class="literal">swarm-master</code> node is mostly empty, thus preventing us from testing the system we just set up. We'll have to change the thresholds defined in the <code class="literal">disk.sh</code>. Let's modify the 80% warning threshold to <code class="literal">2%</code>. Current HD usage is surely more than that:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo sed -i "s/80/2/" /data/consul/scripts/disk.sh</strong></span>
</pre></div><p>Finally, let's<a class="indexterm" id="id732"/> reload Consul and see what happens:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo killall -HUP consul</strong></span>
</pre></div><p>The first thing we should check is the watches log:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /data/consul/logs/watches.log</strong></span>
</pre></div><p>The relevant part of the output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Consul watch request:</strong></span>
<span class="strong"><strong>[{"Node":"swarm-master","CheckID":"disk","Name":"Disk utilization","Status":"warning","Notes":"Critical 95% util, warning 80% util","Output":"Disk Usage: 3.3G/39G (9%)\n","ServiceID":"","ServiceName":""}]</strong></span>
<span class="strong"><strong>Triggering Jenkins job http://10.100.198.200:8080/job/hardware-notification/build</strong></span>
</pre></div><p>Please note that it might take a few seconds until Consul's check is run. If you did not receive the similar output from logs, repeat the <code class="literal">cat</code> command.</p><p>We can see the JSON that consul sent to the script and that the request to build the Jenkins job <code class="literal">hardware-notification</code> has been dispatched. We can also take a look at the Jenkins Console Output of this job by opening <code class="literal">http://10.100.198.200:8080/job/hardware-notification/lastBuild/console</code> URL in a browser:</p><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Hardware" src="graphics/B05848_15_17.jpg"/><div class="caption"><p>Figure 15-17 – Console output of the Jenkins job hardware-notification</p></div></div><p>Since, at this moment, we have only one Consul check used for hard disk utilization, we should implement at least one more. The suitable candidate is memory. Even if we do not do any corrective action when some hardware check fails, having the information in Consul is already very useful in itself.</p><p>Now that we understand the process, we can do better, and use Ansible to set up everything. Besides, different checks should be set up not only in the <code class="literal">swarm-master</code> node but also<a class="indexterm" id="id733"/> in the rest of the cluster, and we don't want to do that manually unless it's for learning purposes.</p><p>Before we proceed, let's exit the <code class="literal">swarm-master</code> node:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
</pre></div></div></div></div>
<div class="section" title="Automatically Setting Up Consul Health Checks and Watches for Monitoring Hardware"><div class="titlepage"><div><div><h1 class="title"><a id="ch15lvl1sec41"/>Automatically Setting Up Consul Health Checks and Watches for Monitoring Hardware</h1></div></div></div><p>At this moment, we<a class="indexterm" id="id734"/> have one hardware watcher configured only in the <code class="literal">swarm-master</code> node. Now that we are familiar with the way Consul watches work, we can use Ansible to deploy hardware monitoring to all the nodes of the Swarm cluster.</p><p>We'll run the Ansible playbook first, and then explore the roles that were used to setup the checks:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant ssh cd</strong></span>

<span class="strong"><strong>ansible-playbook /vagrant/ansible/swarm-healing.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>
</pre></div><p>The <code class="literal">swarm-healing.yml</code> playbook is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: swarm</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  vars:</strong></span>
<span class="strong"><strong>    - debian_version: vivid</strong></span>
<span class="strong"><strong>    - docker_cfg_dest: /lib/systemd/system/docker.service</strong></span>
<span class="strong"><strong>    - is_systemd: true</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - docker</strong></span>
<span class="strong"><strong>    - consul-healing</strong></span>
<span class="strong"><strong>    - swarm</strong></span>
<span class="strong"><strong>    - registrator</strong></span>
</pre></div><p>The only difference, when compared with the <code class="literal">swarm.yml</code> playbook, is the usage of the <code class="literal">consul-healing</code> role. Those two roles (<code class="literal">consul</code> and <code class="literal">consul-healing</code>) are very similar. The major difference is that the latter copies few more files to destination servers (<code class="literal">roles/consul-healing/files/consul_check.json, roles/consul-healing/files/disk.sh</code>, and <code class="literal">roles/consul-healing/files/mem.sh</code>). We already created all those files manually, except the <code class="literal">mem.sh</code> that is used to check memory, and follows the similar logic as the <code class="literal">disk.sh</code> script. The <code class="literal">roles/consul-healing/templates/manage_watches.sh</code> and <code class="literal">roles/consul-healing/templates/watches.json</code> files are defined as templates so that a few things can be customized through Ansible variables. All in all, we are mostly replicating manual steps through Ansible, so that provisioning and configuration of the whole cluster can be done automatically.</p><p>Please open the <code class="literal">http://10.100.192.200:8500/ui/#/dc1/nodes</code> URL, and click on any of the nodes. You'll <a class="indexterm" id="id735"/>notice that each has <code class="literal">Disk utilization</code> and <code class="literal">Memory utilization</code> watches that, in the case of a failure, will start the build of the Jenkins job <code class="literal">hardware-notification/</code>.</p><p>While watching hardware resources, and performing predefined actions in case a threshold is reached, is interesting and useful, there is often a limitation to corrective actions that can be taken. If, for example, a whole node is down, the only thing we can do, in most cases, is to send a notification to someone who will manually investigate the problem. The real benefits are obtained by monitoring services.</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec90"/></h2></div></div></div><div class="section" title="Setting Up Consul Health Checks and Watches for Monitoring Services"><div class="titlepage"><div><div><h3 class="title"><a id="ch15lvl3sec42"/>Setting Up Consul Health Checks and Watches for Monitoring Services</h3></div></div></div><p>Before we dive into <a class="indexterm" id="id736"/>service checks and watches, let's initiate deployment of our <code class="literal">books-ms</code> container. That way we'll use our time wisely, and discuss the subject while Jenkins is working hard to have the service up and running.</p><p>We'll start by indexing the branches defined in the Jenkins job <code class="literal">books-ms</code>. Please open it in a browser, click the <span class="strong"><strong>Branch Indexing</strong></span> link located in the left-hand menu, and follow it with <span class="strong"><strong>Run Now</strong></span>. Once the indexing is done, Jenkins will detect that the <code class="literal">swarm</code> branch matches the filter, create the subproject, and run the first build. When finished, we'll have the <code class="literal">books-ms</code> service deployed to the cluster, and we'll be able to experiment with more self-healing techniques. You can monitor the build progress from the console screen.</p><p>The first step in self-healing is identifying that something is wrong. On the system level, we can observe services we're deploying and, if one of them does not respond, perform some corrective actions. We can continue using Consul checks in a similar manner as with did with memory and disk verifications. The major difference is that this time we'll be better of by using <code class="literal">http</code> instead <code class="literal">script</code> checks. Consul will perform periodic requests to our services, and send failures to the watches we already set up.</p><p>Before we proceed, we should discuss what should be checked. Should we check each service container? Should we check auxiliary containers like databases? Should we care about containers at all? Each of those checks can be useful depending on specific scenarios. In our case, we'll use a more general approach and monitor the service as a whole. Are we losing control if we are not monitoring each container separately? The answer to that question depends on the goals we're trying to accomplish. What do we care about? Do we care if all containers are running, or whether our services are working and performing as expected? If we'd need to choose, I'd say that the latter is more important. If our service is scaled to five instances and it continues performing well even after two of them stop working, there is probably nothing we should do. Only if service as a whole stops working, or if it doesn't perform as expected, some corrective actions should be taken.</p><p>Unlike hardware<a class="indexterm" id="id737"/> checks that benefit from uniformity, and should be located in one place, system checks can vary from one service to another. In order to avoid dependencies between a team that maintains a service and a team in charge of the overall CD processes, we'll keep check definitions inside the service code repository. That way, service team has full freedom to define checks they think are appropriate for the service they're developing. Since parts of the checks are variables, we'll define them through the Consul Template format. We'll, also, employ naming convention and always use the same name for the file. The <code class="literal">consul_check.ctmpl</code> describes checks for the <code class="literal">books-ms</code> service, and is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "service": {</strong></span>
<span class="strong"><strong>    "name": "books-ms",</strong></span>
<span class="strong"><strong>    "tags": ["service"],</strong></span>
<span class="strong"><strong>    "port": 80,</strong></span>
<span class="strong"><strong>    "address": "{{key "proxy/ip"}}",</strong></span>
<span class="strong"><strong>    "checks": [{</strong></span>
<span class="strong"><strong>      "id": "api",</strong></span>
<span class="strong"><strong>      "name": "HTTP on port 80",</strong></span>
<span class="strong"><strong>      "http": "http://{{key "proxy/ip"}}/api/v1/books",</strong></span>
<span class="strong"><strong>      "interval": "10s",</strong></span>
<span class="strong"><strong>      "timeout": "1s"</strong></span>
<span class="strong"><strong>    }]</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>We defined not only checks but also the service named <code class="literal">books-ms</code>, the tag <code class="literal">service</code>, port it is running on and the address. Please note that, since this is the definition of the service as a whole, the port is <code class="literal">80</code>. In our case, the service as a whole is accessible through the proxy, no matter how many containers we deploy, nor ports they are running on. The address is obtained from Consul, through the <code class="literal">proxy/ip</code> key. This service should behave the same, no matter which color is currently deployed.</p><p>Once the service is defined, we proceed with the checks (in this case only one). Each check has an ID and a name, which are used for informational purposes only. The key entry is <code class="literal">http</code> that defines the address Consul will use to ping this service. Finally, we specified that ping <a class="indexterm" id="id738"/>should be performed every ten seconds and that the timeout should be one second. How do we use this template? To answer that question, we should explore the Jenkinsfile, located in the <code class="literal">master</code> branch of the <code class="literal">books-ms</code> repository:</p><div class="informalexample"><pre class="programlisting">node("cd") {
    def serviceName = "books-ms"
    def prodIp = "10.100.192.200"
    def proxyIp = "10.100.192.200"
    def swarmNode = "swarm-master"
    def proxyNode = "swarm-master"
    def registryIpPort = "10.100.198.200:5000"
    def swarmPlaybook = "swarm-healing.yml"
    def proxyPlaybook = "swarm-proxy.yml"
    def instances = 1

    def flow = load "/data/scripts/workflow-util.groovy"

    git url: "https://github.com/vfarcic/${serviceName}.git"
    flow.provision(swarmPlaybook)
    flow.provision(proxyPlaybook)
    flow.buildTests(serviceName, registryIpPort)
    flow.runTests(serviceName, "tests", "")
    flow.buildService(serviceName, registryIpPort)

    def currentColor = flow.getCurrentColor(serviceName, prodIp)
    def nextColor = flow.getNextColor(currentColor)

    flow.deploySwarm(serviceName, prodIp, nextColor, instances)
    flow.runBGPreIntegrationTests(serviceName, prodIp, nextColor)
    flow.updateBGProxy(serviceName, proxyNode, nextColor)
    flow.runBGPostIntegrationTests(serviceName, prodIp, proxyIp, proxyNode, currentColor, nextColor)
    flow.updateChecks(serviceName, swarmNode)
}</pre></div><p>The only significant difference, when compared with Jenkinsfiles we used in previous chapters, is the last line that invokes the <code class="literal">updateChecks</code> function from the <code class="literal">roles/jenkins/files/scripts/workflow-util.groovy</code> utility script. The function is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def updateChecks(serviceName, swarmNode) {</strong></span>
<span class="strong"><strong>    stage "Update checks"</strong></span>
<span class="strong"><strong>    stash includes: 'consul_check.ctmpl', name: 'consul-check'</strong></span>
<span class="strong"><strong>    node(swarmNode) {</strong></span>
<span class="strong"><strong>        unstash 'consul-check'</strong></span>
<span class="strong"><strong>        sh "sudo consul-template -consul localhost:8500 \</strong></span>
<span class="strong"><strong>            -template 'consul_check.ctmpl:/data/consul/config/${serviceName}.json:killall -HUP consul' \</strong></span>
<span class="strong"><strong>            -once"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>In a nutshell, the function copies the file <code class="literal">consul_check.ctmpl</code> to the <code class="literal">swarm-master</code> node, and runs<a class="indexterm" id="id739"/> Consul Template. The result is the creation of, yet another, Consul configuration file that will perform service checks:</p><p>With the checks defined, we should take a closer look at the <code class="literal">roles/consul-healing/templates/manage_watches.sh</code> script. The relevant part is as follows:</p><div class="informalexample"><pre class="programlisting">    if [[ "$CHECK_ID" == "mem" || "$CHECK_ID" == "disk" ]]; then
        echo -e "${RED}Triggering Jenkins job http://{{ jenkins_ip }}:8080/job/hardware-notification/build${NC}"
        curl -X POST http://{{ jenkins_ip }}:8080/job/hardware-notification/build \
            --data-urlencode json="{\"parameter\": [{\"name\":\"checkId\", \"value\":\"$CHECK_ID\"}, {\"name\":\"status\", \"value\":\"$STATUS\"}]}"
    else
        echo -e "${RED}Triggering Jenkins job http://{{ jenkins_ip }}:8080/job/service-redeploy/buildWithParameters?serviceName=${SERVICE_ID}${NC}"
        curl -X POST http://{{ jenkins_ip }}:8080/job/service-redeploy/buildWithParameters?serviceName=${SERVICE_ID}
    fi</pre></div><p>Since we aim at performing two types of checks (hardware and services), we had to introduce an <code class="literal">if</code>/<code class="literal">else</code> statement. When hardware failure is discovered (<code class="literal">mem</code> or <code class="literal">disk</code>), build request is sent to the Jenkins job <code class="literal">hardware-notification</code>. This part is the same as the definition we created earlier. On the other hand, we're assuming that any other type of checks is related to services, and a request is sent to the <code class="literal">service-redeploy</code> job. In our case, when <code class="literal">books-ms</code> service fails, Consul will send a request to build the <code class="literal">service-redeploy</code> job, and pass <code class="literal">books-ms</code> as the <code class="literal">serviceName</code> parameter. We're creating this job in Jenkins in the same way as we created others. The main difference is the usage of the <code class="literal">roles/jenkins/templates/service-redeploy.groovy</code> script. The content is as follows:</p><div class="informalexample"><pre class="programlisting">node("cd") {
    def prodIp = "10.100.192.200"
    def swarmIp = "10.100.192.200"
    def proxyNode = "swarm-master"
    def swarmPlaybook = "swarm-healing.yml"
    def proxyPlaybook = "swarm-proxy.yml"

    def flow = load "/data/scripts/workflow-util.groovy"
    def currentColor = flow.getCurrentColor(serviceName, prodIp)
    def instances = flow.getInstances(serviceName, swarmIp)

    deleteDir()
    git url: "https://github.com/vfarcic/${serviceName}.git"
    try {
        flow.provision(swarmPlaybook)
        flow.provision(proxyPlaybook)
    } catch (e) {}

    flow.deploySwarm(serviceName, prodIp, currentColor, instances)
    flow.updateBGProxy(serviceName, proxyNode, currentColor)
}</pre></div><p>You probably noticed that the script is much shorter than the <code class="literal">Jenkinsfile</code> we used before. We could easily use the same script to redeploy as the one we're using for deployment, and the end result would be (almost) the same. However, the objectives differ. One of the crucial requirements is speed. If our service failed, we want to redeploy is as fast as possible, while having into account as many different scenarios as possible. One of the important differences is that we are not running tests during redeployment. All tests already passed during deployment, or the service would not be running in the first place and there <a class="indexterm" id="id740"/>would be nothing to fail. Besides, the same set of tests running against the same release will always produce the same result, or our tests are flaky and unreliable, indicating grave mistakes in the testing process. You'll also notice that building and pushing to the registry is missing. We do not want to build and deploy a new release, that's what deployment is for. We want to get the latest release back to production as soon as possible. Our need is to restore the system to the same state as it was before the service failed. Now that we covered what is, intentionally, missing from the redeployment script, let's go through it.</p><p>The first change is in the way how we obtain the number of instances that should be running. Up until now, Jenkinsfile, residing in the service repository, was deciding how many instances to deploy. We had the statement <code class="literal">def instances = 1</code> in the Jenkinsfile. However, since this redeployment job should be used for all services, we had to create a new function called <code class="literal">getInstances</code> that will retrieve the number stored in Consul. It represents the <code class="literal">desired</code> number of instances, and corresponds with the value specified in the Jenkinsfile. Without it, we would risk deploying a fixed number of containers and, potentially, destroying someone else's intention. Maybe developers decided to run two instances of the service, or maybe they scaled it to five after realizing that the load is too big. For that reason, we have to discover how many instances to deploy, and put that information to good use. The <code class="literal">getInstances</code> function defined in the <code class="literal">roles/jenkins/files/scripts/workflow-util.groovy</code> script is as follows:</p><div class="informalexample"><pre class="programlisting">def getInstances(serviceName, swarmIp) {
    return sendHttpRequest("http://${swarmIp}:8500/v1/kv/${serviceName}/instances?raw")
}</pre></div><p>The function sends a simple request to Consul and returns the number of instances of the specified service.</p><p>Next, we are <a class="indexterm" id="id741"/>deleting the job workspace directory before cloning the code from GitHub. This removal of the files is necessary since the Git repository is different from one service to another, and Git repository cannot be cloned on top of the other. We don't need all the code, but rather few configuration files, specifically, those for Docker Compose and Consul. Never the less, it's easier if we clone everything. If the repository is big, you might consider getting only the files you need.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    deleteDir()</strong></span>
<span class="strong"><strong>    git url: "https://github.com/vfarcic/${serviceName}.git"</strong></span>
</pre></div><p>Now that all the files we'll need (and many more that we won't) are in the workspace, we can initiate the redeployment. Before we proceed, let's discuss what might have caused the failure in the first place. We can identify three main culprits. One of the nodes stopped working, one of the infrastructure services is down (Swarm, Consul, and so on), or our own service failed. We'll skip the first possibility and leave it for later. If one of the infrastructure services stopped working, we could fix that by running Ansible playbooks. On the other hand, if the cluster is operating as expected, all we have to do is redeploy the container with our service.</p><p>Let's explore provisioning with Ansible. The part of the script that runs Ansible playbooks is as follows:</p><div class="informalexample"><pre class="programlisting">    try {
        flow.provision(swarmPlaybook)
        flow.provision(proxyPlaybook)
    } catch (e) {}</pre></div><p>The major difference, when compared with the previous Jenkins Workflow scripts, is that, this time, provisioning is inside the <code class="literal">try</code>/<code class="literal">catch</code> block. The reason is a possible node failure. If the culprit for this redeployment is one malfunctioning node, provisioning will fail. That's not a problem in itself if the rest of the script is run. For that reason, we have this script block under <code class="literal">try</code>/<code class="literal">catch</code>, thus ensuring that the script continues running no matter the provisioning result. After all, if a node is not working, Swarm will redeploy the service somewhere else (explained in more detail later on). Let's move onto the next use case:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    flow.deploySwarm(serviceName, prodIp, currentColor, instances)</strong></span>
<span class="strong"><strong>    flow.updateBGProxy(serviceName, proxyNode, currentColor)</strong></span>
</pre></div><p>Those two lines are the same as in the deployment script in the Jenkinsfile. The only, subtle, difference is that the number of instances is not hardcoded, but, as we saw earlier, discovered.</p><p>That's it for now. With the <a class="indexterm" id="id742"/>script we explored, we have two out of three scenarios covered. Our system will recover if one of infrastructure or one of our services fails. Let's try it out.</p><p>We'll stop one of the infrastructure services and see whether the system will get restored to the original state. There is probably no better candidate than <code class="literal">nginx</code>. It is part of our services infrastructure and, without it, none of our services work.</p><p>Without nginx, our service is not accessible through the port <code class="literal">80</code>. At no point, Consul will know that <code class="literal">nginx</code> failed. Instead, Consul checker will detect that the <code class="literal">books-ms</code> service is not operational, and initiate a new build of the Jenkins job <code class="literal">service-redeploy</code>. As a result, provisioning and redeployment will be executed. Part of Ansible provisioning is in charge of ensuring that, among others, nginx is running:</p><p>Let's enter the <code class="literal">swarm-master</code> node and stop the <code class="literal">nginx</code> container.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>

<span class="strong"><strong>vagrant ssh swarm-master</strong></span>

<span class="strong"><strong>docker stop nginx</strong></span>

<span class="strong"><strong>exit</strong></span>

<span class="strong"><strong>vagrant ssh cd</strong></span>
</pre></div><p>With nginx process dead, the <code class="literal">books-ms</code> service is not accessible (at least not through the port <code class="literal">80</code>). We can confirm that by sending an HTTP request to it. Please bear in mind that Consul will initiate redeployment through Jenkins, so hurry up before it becomes operational again:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl swarm-master/api/v1/books</strong></span>
</pre></div><p>As expected, <code class="literal">curl</code> returned the <code class="literal">Connection refused</code> error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl: (7) Failed to connect to swarm-master port 80: Connection refused</strong></span>
</pre></div><p>We can also take a look at the Consul UI. The Service <code class="literal">books-ms</code> check should be in the critical state. You can click on the <code class="literal">swarm-master</code> link to get more details about all the service running<a class="indexterm" id="id743"/> on that node and their statuses. As a side note, <code class="literal">books-ms</code> is registered as running on the <code class="literal">swarm-master</code> server because that's where the proxy is. There is also <code class="literal">books-ms-blue</code> or <code class="literal">books-ms-green</code> service that contains data specific to deployed containers:</p><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Services" src="graphics/B05848_15_18.jpg"/><div class="caption"><p>Figure 15-18 – Consul status screen with one check in the critical status</p></div></div><p>Finally, We can take a look at the service-redeploy console screen. The redeployment process should be on the way, or, more likely, finished by now.</p><p>Once the build of the <code class="literal">service-redeploy</code> job is finished, everything should be restored to the original status, and we can use our service:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I swarm-master/api/v1/books</strong></span>
</pre></div><p>The output of the response is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>HTTP/1.1 200 OK</strong></span>
<span class="strong"><strong>Server: nginx/1.9.9</strong></span>
<span class="strong"><strong>Date: Tue, 19 Jan 2016 21:53:00 GMT</strong></span>
<span class="strong"><strong>Content-Type: application/json; charset=UTF-8</strong></span>
<span class="strong"><strong>Content-Length: 2</strong></span>
<span class="strong"><strong>Connection: keep-alive</strong></span>
<span class="strong"><strong>Access-Control-Allow-Origin: *</strong></span>
</pre></div><p>The proxy service has been, indeed, redeployed, and everything is working as expected.</p><p>What would happen if, instead stopping one of the infrastructure services, we remove the <code class="literal">book-ms</code> instance entirely? Let's remove the service container, and see what happens:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://swarm-master:2375</strong></span>

<span class="strong"><strong>docker rm -f $(docker ps --filter name=booksms --format "{{.ID}}")</strong></span>
</pre></div><p>Go ahead and<a class="indexterm" id="id744"/> open the <code class="literal">service-redeploy</code> Jenkins console screen. It might take a couple of seconds until Consul initiates a new build. Once started, all we need to do is wait a bit longer, until the build finishes running. Once you see the <span class="strong"><strong>Finished: Success</strong></span> message, we can double check whether the service is indeed operational:</p><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Services" src="graphics/B05848_15_19.jpg"/><div class="caption"><p>Figure 15-19 – Output of the service-redeploy build</p></div></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>

<span class="strong"><strong>curl -I swarm-master/api/v1/books</strong></span>
</pre></div><p>The combined <a class="indexterm" id="id745"/>output of both commands is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app-blue_1</strong></span>
<span class="strong"><strong>swarm-node-1/books-ms-db</strong></span>

<span class="strong"><strong>...</strong></span>

<span class="strong"><strong>HTTP/1.1 200 OK</strong></span>
<span class="strong"><strong>Server: nginx/1.9.9</strong></span>
<span class="strong"><strong>Date: Tue, 19 Jan 2016 22:05:50 GMT</strong></span>
<span class="strong"><strong>Content-Type: application/json; charset=UTF-8</strong></span>
<span class="strong"><strong>Content-Length: 2</strong></span>
<span class="strong"><strong>Connection: keep-alive</strong></span>
<span class="strong"><strong>Access-Control-Allow-Origin: *</strong></span>
</pre></div><p>Our service is, indeed, running and accessible through the proxy. The system healed itself. We can stop almost any process, on any of the Swarm nodes, and, with a few seconds delay, system will restore itself to the previous state. The only thing we haven't tried is to stop the<a class="indexterm" id="id746"/> whole node. Such an action would require a few more changes to our scripts. We'll explore those changes later on. Please be aware that this is a demo setting and it does not mean that the system, as it is now, is ready for production. On the other hand, it's not far either. With a bit of tweaking, you could consider applying this to you system. You might want to add some notifications (email, Slack, and so on) and adapt the process to your needs. The important part is the process. Once we understand what we want, and how to get there, the rest is usually only a question of time:</p><p>The process we have, at this moment, is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Consul performs periodic HTTP requests, runs custom scripts or waits for <span class="strong"><strong>time-to-live</strong></span> (<span class="strong"><strong>TTL</strong></span>) <a class="indexterm" id="id747"/>messages from services.</li><li class="listitem" style="list-style-type: disc">In case Consul's request does not return status code <code class="literal">200</code>, the script returns a non-zero exit code, or TTL message was not received, Consul sends a request to Jenkins.</li><li class="listitem" style="list-style-type: disc">Upon receiving a request from Consul, Jenkins initiates redeployment process, sends notification messages, and so on:</li></ul></div><div class="mediaobject"><img alt="Setting Up Consul Health Checks and Watches for Monitoring Services" src="graphics/B05848_15_20.jpg"/><div class="caption"><p>Figure 15-20 – Checking and healing Consul pinging services</p></div></div><p>We explored a few examples of reactive healing. Those were, by no means, exhaustive enough to <a class="indexterm" id="id748"/>provide you with everything you need to set up your own system, but, hopefully, provided you with a path that you should explore in more depth, and adapt to your own needs. Right now, we'll move our attention to preventive measure we can take. We'll examine scheduled scaling and descaling. It is a good candidate as an introduction to preventive healing since it is probably the easiest one to implement.</p></div></div></div>
<div class="section" title="Preventive Healing through Scheduled Scaling and Descaling"><div class="titlepage"><div><div><h1 class="title"><a id="ch15lvl1sec42"/>Preventive Healing through Scheduled Scaling and Descaling</h1></div></div></div><p>Preventive healing is a<a class="indexterm" id="id749"/> huge topic in itself and, in all but the simplest scenarios, requires historical data that can be used to analyze the system and predict the future. Since, at this moment, we neither have the data, nor the tools to generate them, we'll start with a very simple example that does not require any of those.</p><p>The scenario we'll explore is as follows. We are working on an online book store. Marketing department decided that, starting from the new years eve, all readers will be able to purchase books with a discount. The campaign will last for a day, and we expect it to generate a huge interest. In technical terms, that means that during 24 hours, starting from midnight, January the first, our system will be under heavy load. What should we do? We already have <a class="indexterm" id="id750"/>processes and tools that allow us to scale our system (or parts that will be most affected). What we need to do is scale selected services before the campaign starts and, once it's finished, restore it to the original state. The problem is that no one wants to celebrate new years eve in the office. We can fix that easily with Jenkins. We can create a scheduled job that will scale, and, later on, descale our services. With this problem solved, another one emerges. To how many instances should we scale? We can define a number in advance but, in that way, we risk making a mistake. For example, we might decide to scale to three instances (at this moment we have only one). Between today and the start of the campaign, due to some other reason, the number of instances might increase to five. In such a scenario, not only that we would not increase the capacity of our system, but would accomplish quite contrary. Our scheduled job would descale the service from five to three. The solution might be to use relative values. Instead of specifying that the system should be scaled to three instances, we should set it up in a way that the number of instances should be increased by two. If there is one instance running, such a process would launch two more and increase the overall number to three. On the other hand, if someone already scaled the service to five, the end result would be seven containers running inside our cluster. The similar logic can be employed after the campaign is finished. We can create the second scheduled job that would decrease the number of running instances by two. From three, to one. From five, to three. It does not matter how many will be running at that moment since we would decrease that number by two.</p><p>This process of preventive healing is similar to the usage of vaccinations. Their primary use is not to heal an existing infection, but to develop immunity that will prevent them from spreading in the first place. In the same way, we'll schedule scaling (and later on descaling), in order to prevent the increased load affecting our system in unexpected ways. Instead of healing an infected system, we'll prevent it from getting into bad shape.</p><p>Let's see such the process in action.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Please open the Jenkins <code class="literal">books-ms-scale</code> configuration screen. The job configuration is very straightforward. It has one parameter called <code class="literal">scale</code> with the default value of <code class="literal">2</code>. It can be adjusted when starting a build. <code class="literal">Build Triggers</code> is set to <code class="literal">build periodically</code> with the value <code class="literal">45 23 31 12</code>. If you already used cron scheduling, this should look familiar. The format is <code class="literal">MINUTE HOUR DOM MONTH DOW</code>. The first number represents minutes, the second hours, the third is the day of a month, followed by month and the day of the week. Asterisk, can be translated to any. So, the value we are using is fourty fifth minute of the twenty third hour, on thirty first day of the twelfth month. In other words, fifteen minutes before new years eve. That is more than enough time for us to increase the number of instances before the campaign starts. For more information about the scheduling format, please click the icon with a question mark located right of the <code class="literal">Schedule*</code> field.</p></div></div><p>The third, at last, part of the job configuration is the following Workflow script:</p><div class="informalexample"><pre class="programlisting">node("cd") {
    def serviceName = "books-ms"
    def swarmIp = "10.100.192.200"

    def flow = load "/data/scripts/workflow-util.groovy"
    def instances = flow.getInstances(serviceName, swarmIp).toInteger() + scale.toInteger()
    flow.putInstances(serviceName, swarmIp, instances)
    build job: "service-redeploy", parameters: [[$class: "StringParameterValue", name: "serviceName", value: serviceName]]
}</pre></div><p>Since there is no real reason to duplicate the code, we are using the helper functions from the <code class="literal">roles/jenkins/files/scripts/workflow-util.groovy</code> script.</p><p>We start by defining the<a class="indexterm" id="id751"/> number of instances we want to run. We do that by adding the value of the <code class="literal">scale</code> parameter (defaults to two) to the number of instances our service is currently using. We get the latter by invoking the <code class="literal">getInstances</code> function we already utilized in a couple of cases throughout the book. That new value is put to Consul through the <code class="literal">putInstances</code> function. Finally, we run a build of the <code class="literal">service-redeploy</code> job which does the redeployment that we need. To summarize, since the <code class="literal">service-redeploy</code> job reads the number of instances from Consul, all we had to do in this script, before invoking the <code class="literal">service-redeploy</code> build, was to change the <code class="literal">scale</code> value in Consul. From there on, <code class="literal">service-redeploy</code> job will do what's needed to scale the number of containers. By invoking the <code class="literal">service-redeploy</code> job, we avoided replicating the code that is already used elsewhere:</p><div class="mediaobject"><img alt="Preventive Healing through Scheduled Scaling and Descaling" src="graphics/B05848_15_21.jpg"/><div class="caption"><p>Figure 15-21 – Configuration of the books-ms-scale job representing scheduled scaling</p></div></div><p>Now we have two paths we can take. One is to wait until new years eve and confirm that the job works. I will take liberty <a class="indexterm" id="id752"/>and assume that you do not have so much patience, and proceed with the alternative. We'll run the job manually. Before we do that, let's take a quick look at the current situation inside our Swarm cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://swarm-master:2375</strong></span>

<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>

<span class="strong"><strong>curl swarm-master:8500/v1/kv/books-ms/instances?raw</strong></span>
</pre></div><p>The combined output of the commands is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app-blue_1</strong></span>
<span class="strong"><strong>swarm-node-2/books-ms-db</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>1</strong></span>
</pre></div><p>We can see that only one instance of the books-ms service is running (<code class="literal">booksms_app-blue_1</code>) and that <a class="indexterm" id="id753"/>Consul has the value of <code class="literal">1</code> stored as the <code class="literal">books-ms/instances</code> key.</p><p>Let's run the <code class="literal">books-ms-scale</code> Jenkins job. If everything works as expected, it should increase the number of <code class="literal">books-ms</code> instances by two, resulting in three in total. Please open the <code class="literal">books-ms-scale</code> build screen and click the <span class="strong"><strong>Build</strong></span> button. You can monitor the progress by opening the <code class="literal">books-ms-scale</code> console screen. You'll see that, after storing the new number of instances in Consul, it will invoke a build of the <code class="literal">service-redeploy</code> job. After a few seconds, the build will finish, and we'll be able to verify the result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>

<span class="strong"><strong>curl swarm-master:8500/v1/kv/books-ms/instances?raw</strong></span>
</pre></div><p>The combined output of the commands is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app-blue_3</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app-blue_2</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app-blue_1</strong></span>
<span class="strong"><strong>swarm-node-2/books-ms-db</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>3</strong></span>
</pre></div><p>As we can see, this time, three instances of the service are running. We can observe the same result from the Consul UI, by navigating to the <code class="literal">key/value books-ms/instances screen</code>:</p><div class="mediaobject"><img alt="Preventive Healing through Scheduled Scaling and Descaling" src="graphics/B05848_15_22.jpg"/><div class="caption"><p>Figure 15-22 – Consul UI Key/Value books-ms/instances screen</p></div></div><p>Our system is now ready to take the increased load during those 24 hours. As you saw, we were very generous by scheduling it to run 15 minutes before the due date. The execution of the build lasted only a couple of seconds. We could speed it even more by skipping the <a class="indexterm" id="id754"/>provisioning part of the <code class="literal">service-redeploy</code> job. I'll leave that to you as an exercise.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>
<span class="strong"><strong>Add conditional to the service-redeploy job</strong></span>
</p><p>Modify the service-redeploy Jenkins job so that provisioning is optional. You'll have to add a new<a class="indexterm" id="id755"/> parameter that accepts boolean value and add an if/else statement to the workflow script. Make sure that the parameter has the default value set to true so that provisioning is always performed unless specified otherwise. Once finished, switch to the configuration of the <code class="literal">books-ms-scale</code> job and modify it so that the call to the service-redeploy job passes the signal to skip provisioning.</p></div></div><p>What happens after 24 hours passes, and the campaign is over? The Jenkins job <code class="literal">books-ms-descale</code> will be run. It is the same as the <code class="literal">books-ms-scale</code> job with two notable differences. The <code class="literal">scale</code> parameter is set to <code class="literal">-2</code> and it is scheduled to run on the second of January, fifteen minutes after midnight (<code class="literal">15 0 2 1 *</code>). We gave our system fifteen minutes of cool-down time. The Workflow script is the same.</p><p>Let's run it by opening the <code class="literal">books-ms-descale</code> build screen, and clicking the <span class="strong"><strong>Build</strong></span> button. It will reduce the number of instances by two, and run a build of the <code class="literal">service-redeploy</code> job. Once finished, we can have another look at our cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>

<span class="strong"><strong>curl swarm-master:8500/v1/kv/books-ms/instances?raw</strong></span>
</pre></div><p>The combined output of the commands is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app-blue_1</strong></span>
<span class="strong"><strong>swarm-node-2/books-ms-db</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>1</strong></span>
</pre></div><p>We are back where we started. The campaign is finished, and the service is reduced from three instances to one. The value in Consul is restored as well. The system survived horde of visitors desperately trying to benefit from our new years eve discount, business is happy that we were able to serve them all, and life continues as it was.</p><p>We could have created different formulas to accomplish our goals. It could be as simple as multiplying the number of existing instances. That would give us a bit more realistic scenario. Instead of adding two new containers, we could have multiplied them by two. If three were running before, six would be running afterwards. As you can imagine, those formulas can often be much more complicated. More importantly, they would require much more consideration. If, instead of running one, we were running fifty different services, we would not apply the same formula to all of them. Some would need to be scaled a lot, some not so much, while other not at all. The best way to proceed would be to employ some kind of stress tests that would tell us which pieces of the system require scaling, and how<a class="indexterm" id="id756"/> much that scaling should be. There's plethora of tools that can run those tests, with JMeter and Gatling (my favorite) being only a few.</p><p>I mentioned, at the beginning of this chapter, that preventive healing is based on historical data. This was a very poor, yet very efficient and simple way of demonstrating that. In this case, historical data was in our heads. We knew that a marketing campaign will increase the load on our service, and acted in a way that potential problems are avoided. The real, and much more complicated, way to create preventive healing require more than our memory. It requires a system capable of storing and analyzing data. We'll discuss requirements for such a system in the next chapter.</p><div class="section" title="Reactive Healing with Docker Restart Policies"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec91"/>Reactive Healing with Docker Restart Policies</h2></div></div></div><p>Those more familiar<a class="indexterm" id="id757"/> with Docker might be asking why I did not mention Docker restart policies. On a first look, they seem to be a very effective way to recuperate failed containers. They are, indeed, the easiest way to define when to restart containers. We can use the <code class="literal">--restart</code> flag on <code class="literal">docker run</code> (or the equivalent Docker Compose definition), and the container will be restarted on exit. The following table summarizes the currently supported restart policies:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Policy</p>
</th><th style="text-align: left" valign="bottom">
<p>Result</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">no</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Do not automatically restart the container when it exits. This is the default.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">on-failure[:max-retries]</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Restart only if the container exits with a non-zero exit status. Optionally, limit the number of restart retries the Docker daemon attempts.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">always</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Always<a class="indexterm" id="id758"/> restart the container regardless of the exit status. When you specify always, the Docker daemon will try to restart the container indefinitely. The container will also always start on daemon startup, regardless of the current state of the container.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">unless-stopped</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Always restart the container regardless of the exit status, but do not start it on daemon startup if the container has been put to a stopped state before.</p>
</td></tr></tbody></table></div><p>An example of the usage of restart policy is as follows (please do not run it).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run --restart=on-failure:3 mongo</strong></span>
</pre></div><p>In that case, mongo would be restarted up to three times. The restart would occur only if the process running inside the mongo container exits with a non-zero status. If we stop that container, the restart policy would not be applied.</p><p>The problem with restart policies is that there are too many corner cases not contemplated. The process running inside a container might fail due problems not directly related to the container that failed. For example, a service inside the container might be trying to connect to a database through a proxy. It might have been designed to stop if the connection could not be established. If, for some reason, the node with the proxy is not operational, it doesn't matter how many times we restart the container, the result will always be the same. There is nothing wrong in trying, but, sooner or later, someone needs to be notified about the problem. Maybe provisioning scripts need to be run to restore the system to the desired state. Maybe more nodes need to be added to the cluster. Maybe even the whole data center is not operational. No matter the cause, there are many more possible paths that could be taken than what restart policy permits. For those reasons, we do need a more robust system to deal with all those circumstances, and we are already on the way of creating it. The flow we have established is much more robust than simple restart policies, and it already covers the same problems as those that can be solved with the Docker restart policy. Actually, as it is now, we have many more paths covered. We perform containers orchestration with Docker Swarm that will make sure that our services are deployed to the most suited nodes inside the cluster. We use Ansible that is continuously (with each deploy) provisioning the cluster, thus ensuring that the whole infrastructure is in the correct state. We are using Consul in combination with Registrator and Consul Template for service discovery, making sure that the registry of all our services is always up to date. Finally, Consul health checks are continuously monitoring the state of our cluster and, in case of a failure, sends requests to Jenkins that will initiate appropriate corrective actions.</p><p>We are utilizing<a class="indexterm" id="id759"/> the Docker's slogan <span class="emphasis"><em>batteries included but removable</em></span> to our benefit by extending the system to suit our needs.</p></div><div class="section" title="Combining On-Premise with Cloud Nodes"><div class="titlepage"><div><div><h2 class="title"><a id="ch15lvl2sec92"/>Combining On-Premise with Cloud Nodes</h2></div></div></div><p>I won't start a discussion whether to use on-premise servers or cloud hosting. Both have their advantages and disadvantages. The decision what to use depends on individual needs. Besides, such an attempt would be better suited inside the clustering and scaling chapter. However, there is a clear use case in favour of cloud hosting that would suit very well the needs of, at least, one of the scenarios from this chapter.</p><p>Cloud hosting shines <a class="indexterm" id="id760"/>when we need a temporary increase in the cluster capacity. A good example would be our fictional scenario with the new years eve campaign. We needed to boost our capacity for a day. If you are already hosting all your servers in the cloud, this scenario would require a few more nodes to be created and, later on, destroyed, once the load is reduced to its former size. On the other hand, if you use on-premise hosting, that would be an opportunity to contract cloud hosting only for those additional nodes. Buying a new set of servers that will be used only during a short period is very costly, especially if we take into account that the cost cosists not only of hardware price, but also maintenance. If, in such cases, we use cloud nodes, the invoice would be paid only for the time we use them (assuming that we destroy them afterwards). Since we have all the scripts for provisioning and deploying services, the setup of those nodes would be almost effortless.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Personally, I prefer the combination of on-premise and cloud hosting. My on-premise servers are fulfilling the need for the minimum capacity, while cloud hosting nodes are being created (and destroyed) whenever that capacity needs to be temporarily increased. Please note that such a combination is only my personal preference, and might not apply to your use cases.</p></div></div><p>The important part is that<a class="indexterm" id="id761"/> everything you learned from this book is equally applicable to both situations (on-premise or cloud). The only significant difference is that you should not be using Vagrant on production servers. We are using it only to create quickly virtual machines on you laptop. If you are looking for a way to create production VMs in a similar way as with Vagrant, I suggest you explore another HashiCorp product called <span class="strong"><strong>Packer</strong></span>.</p></div></div>
<div class="section" title="Self-Healing Summary (So Far)"><div class="titlepage"><div><div><h1 class="title"><a id="ch15lvl1sec43"/>Self-Healing Summary (So Far)</h1></div></div></div><p>What we built so far is, in some cases, close to what Kubernetes and Mesos offer out of the box, while in others<a class="indexterm" id="id762"/> exceeds their functionality. The real advantage of the system we are working on is its the ability to fine-tune it to your needs. That is not to say that Kubernetes and Mesos should not be used. You should, at least, be familiar with them. Do not take anyone's word for granted (not even mine). Try them out and make your own conclusions. There are as many use cases as there are projects, and each is different from the other. While in some cases the system we built would provides a good base to build upon, there are others where, for example, Kubernetes or Mesos might be more appropriate. I could not fit all the possible combinations in detail inside a single book. That would increase it to an unmanageable size.</p><p>Instead, I choose to explore ways we can build systems that are highly extensible. Almost any piece we used by now can be extended, or replaced with another. I feel that this approach gives you more possibilities to adapt examples to your own needs and, at the same time, learn not only how something works, but why we chose it.</p><p>We went far from the humble beginnings of this book, and we are not yet done. The exploration of self-healing systems will continue. However, first we need turn our attention to different ways of collecting data generated inside our cluster.</p><p>As the first part of the self-healing subject is closing to an end, let us destroy our VMs, and start the new chapter fresh.</p><p>You know what follows next. We'll destroy everything we did, and begin the next chapter fresh:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>

<span class="strong"><strong>vagrant halt</strong></span>
</pre></div></div></body></html>