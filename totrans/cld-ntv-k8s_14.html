<html><head></head><body>
		<div><p><a id="_idTextAnchor250"/></p>
			<h1 id="_idParaDest-238"><em class="italic"><a id="_idTextAnchor251"/>Chapter 11</em>: Template Code Generation and CI/CD on Kubernetes</h1>
			<p>This chapter discusses some easier ways to template and configure large Kubernetes deployments with many resources. It also details a number of methods for implementing <strong class="bold">Continuous Integration</strong>/<strong class="bold">Continuous Deployment</strong> (<strong class="bold">CI</strong>/<strong class="bold">CD</strong>) on Kubernetes, as <a id="_idIndexMarker536"/>well as the pros and cons associated with each possible method. Specifically, we talk about in-cluster CI/CD, where some or all of the CI/CD steps are performed in our Kubernetes cluster, and out-of-cluster CI/CD, where all the steps take place outside our cluster.</p>
			<p>The case study in this chapter will include creating a Helm chart from scratch, along with an explanation of each piece of a Helm chart and how it works.</p>
			<p>To begin, we will cover the landscape of Kubernetes resource template generation, and the reasons why a template generation tool should be used at all. Then, we will cover implementing CI/CD to Kubernetes, first with AWS CodeBuild, and next with FluxCD.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding options for template code generation on Kubernetes</li>
				<li>Implementing templates on Kubernetes with Helm and Kustomize</li>
				<li>Understanding CI/CD paradigms on Kubernetes – in-cluster and out-of-cluster</li>
				<li>Implementing in-cluster and out-of-cluster CI/CD with Kubernetes</li>
			</ul>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor252"/>Technical requirements</h1>
			<p>In order to run the commands detailed in this chapter, you will need a computer that supports the <code>kubectl</code> command-line tool along with a working Kubernetes cluster. Refer to <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, for several methods for getting up and running with Kubernetes quickly, and for instructions on how to install the kubectl tool. Additionally, you will need a machine that supports the Helm CLI tool, which typically has the same prerequisites as kubectl – for details, check out the Helm documentation at <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a>.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository at </p>
			<p><a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter11">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter11</a>.</p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor253"/>Understanding options for template code generation on Kubernetes</h1>
			<p>As discussed in <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, one of the greatest strengths of Kubernetes i<a id="_idIndexMarker537"/>s that its API can communicate in terms of declarative resource files. This allows us to run commands such as <code>kubectl apply</code> and have the control plane ensure that whatever resources are running in the cluster match our YAML or JSON file.</p>
			<p>However, this capability introduces some unwieldiness. Since we want to have all our workloads declared in configuration files, any large or complex applications, especially if they include many microservices, could result in a large number of configuration files to write and maintain. </p>
			<p>This issue is further compounded with multiple environments. Say we want development, staging, UAT, and production environments, this would require four separate YAML files per Kubernetes resource, assuming we wanted to maintain one resource per file for cleanliness. </p>
			<p>One way to fix these issues is to work with templating systems that support variables, allowing a single template file to work for multiple applications or multiple environments by injecting different sets of variables.</p>
			<p>There are several popular community-supported open source options for this purpose. In this book, we will focus on two of the most popular ones:</p>
			<ul>
				<li>Helm</li>
				<li>Kustomize</li>
			</ul>
			<p>There are many other options available, including Kapitan, Ksonnet, Jsonnet, and more, but a full review of all of them is not within the scope of this book. Let's start by reviewing Helm, which<a id="_idIndexMarker538"/> is, in many ways, the most popular templating tool.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor254"/>Helm</h2>
			<p>Helm actually <a id="_idIndexMarker539"/>plays <a id="_idIndexMarker540"/>double duty as a templating/code generation tool and a CI/CD tool. It allows you to create YAML-based templates that can be hydrated with variables, allowing for code and template reuse across applications and environments. It also comes with a Helm CLI tool to roll out changes to applications based on the templates themselves.</p>
			<p>For this reason, you are likely to see Helm all over the Kubernetes ecosystem as the default way to install tools or applications. We'll be using Helm for both of its purposes in this chapter.</p>
			<p>Now, let's move on to Kustomize, which is quite different to Helm.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor255"/>Kustomize</h2>
			<p>Unlike Helm, Kustomize<a id="_idIndexMarker541"/> is officially supported <a id="_idIndexMarker542"/>by the Kubernetes project, and support is integrated directly into <code>kubectl</code>. Unlike Helm, Kustomize operates using vanilla YAML without variables, and instead recommends a <em class="italic">fork and patch</em> workflow where sections of YAML are replaced with new YAML depending on the patch chosen.</p>
			<p>Now that we have a basic understanding of how the tools differ, we can use them in practice.</p>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor256"/>Implementing templates on Kubernetes with Helm and Kustomize</h1>
			<p>Now that <a id="_idIndexMarker543"/>we know our options, we can<a id="_idIndexMarker544"/> implement<a id="_idIndexMarker545"/> each of them with an <a id="_idIndexMarker546"/>example application. This will allow us to understand the specifics of how each tool handles variables and the process of templating. Let's start with Helm.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor257"/>Using Helm with Kubernetes</h2>
			<p>As mentioned<a id="_idIndexMarker547"/> previously, Helm is an open source project that<a id="_idIndexMarker548"/> makes it easy to template and deploy applications on Kubernetes. For the purposes of this book, we will be focused on the newest version (as of the time of writing), Helm V3. A previous version, Helm V2, had more moving parts, including a controller, called <em class="italic">Tiller</em>, that would run on the cluster. Helm V3 is simplified and only contains the Helm CLI tool. It does, however, use custom resource definitions on the cluster to track releases, as we will see shortly.</p>
			<p>Let's start by installing Helm. </p>
			<h3>Installing Helm</h3>
			<p>If you want to use <a id="_idIndexMarker549"/>a specific version of Helm, you can install it by following the specific version docs at <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a>. For our use case, we will simply use the <code>get helm</code> script, which will install the newest version.</p>
			<p>You can fetch and run the script as follows:</p>
			<pre>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh</pre>
			<p>Now, we should be able to run <code>helm</code> commands. By default, Helm will automatically use your existing <code>kubeconfig</code> cluster and context, so in order to switch clusters for Helm, you just need to use <code>kubectl</code> to change your <code>kubeconfig</code> file, as you would normally do.</p>
			<p>To install an application using Helm, run the <code>helm install</code> command. But how does Helm decide what and how to install? We'll need to discuss the concepts of Helm charts, Helm repositories, and Helm releases.</p>
			<h3>Helm charts, repositories, and releases</h3>
			<p>Helm provides <a id="_idIndexMarker550"/>a way to template and deploy applications on Kubernetes with <a id="_idIndexMarker551"/>variables. In order to do this, we specify workloads via <a id="_idIndexMarker552"/>a set of templates, which is called a <em class="italic">Helm chart</em>.</p>
			<p>A Helm chart consists of one or more templates, some chart metadata, and a <code>values</code> file that fills in the template variables with final values. In practice, you would then have one <code>values</code> file per environment (or app, if you are reusing your template for multiple apps), which would hydrate the shared template with a new configuration. This combination of template and values would then be used to install or deploy an application to your cluster.</p>
			<p>So, where can you store Helm charts? You can put them in a Git repository as you would with any other Kubernetes YAML (which works for most use cases), but Helm also supports the concept of repositories. A Helm repository is represented by a URL and can contain multiple Helm charts. For instance, Helm has its own official repository at <a href="https://hub.helm.sh/charts">https://hub.helm.sh/charts</a>. Again, each Helm chart consists of a folder with a metadata file, a <code>Chart.yaml</code> file, one or more template files, and optionally a values file.</p>
			<p>In order to install a local Helm chart with a local values file, you can pass a path for each to <code>helm install</code>, as shown in the following command:</p>
			<pre>helm install -f values.yaml /path/to/chart/root</pre>
			<p>However, for commonly installed charts, you can also install the chart directly from a chart repository, and you can optionally add a custom repository to your local Helm in order to be able to install charts easily from non-official sources.</p>
			<p>For instance, in order to install Drupal via the official Helm chart, you can run the following command:</p>
			<pre>helm install -f values.yaml stable/drupal</pre>
			<p>This code installs charts out of the official Helm chart repository. To use a custom repository, you just need to add it to Helm first. For instance, to install <code>cert-manager</code>, which is hosted on the <code>jetstack</code> Helm repository, we can do the following:</p>
			<pre>helm repo add jetstack https://charts.jetstack.io
helm install certmanager --namespace cert-manager jetstack/cert-manager</pre>
			<p>This code adds the <code>jetstack</code> Helm repository to your local Helm CLI tool, and then installs <code>cert-manager</code> via the charts hosted there. We also name the release as <code>cert-manager</code>. A Helm release is a concept implemented using Kubernetes secrets in Helm V3. When we create a Release in Helm, it will be stored as a secret in the same namespace.</p>
			<p>To illustrate this, we can create a Helm release using the preceding <code>install</code> command. Let's do it now:</p>
			<pre>helm install certmanager --namespace cert-manager jetstack/cert-manager</pre>
			<p>This command <a id="_idIndexMarker553"/>should result in the following output, which may be slightly different <a id="_idIndexMarker554"/>depending on the current version of Cert Manager. We'll <a id="_idIndexMarker555"/>split the output into two sections for readability.</p>
			<p>First, the output of the command gives us a status of the Helm release:</p>
			<pre>NAME: certmanager
LAST DEPLOYED: Sun May 23 19:07:04 2020
NAMESPACE: cert-manager
STATUS: deployed
REVISION: 1
TEST SUITE: None</pre>
			<p>As you can see, this section contains a timestamp for the deployment, namespace information, a revision, and a status. Next, we'll see the notes section of the output:</p>
			<pre>NOTES:
cert-manager has been deployed successfully!
In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).
More information on the different types of issuers and how to configure them
can be found in our documentation:
https://cert-manager.io/docs/configuration/
For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:
https://cert-manager.io/docs/usage/ingress/</pre>
			<p>As you can see, our Helm <code>install</code> command has resulted in a success message, which also gives us <a id="_idIndexMarker556"/>some information from <code>cert-manager</code> about how to use it. This<a id="_idIndexMarker557"/> output can be helpful to look at when installing Helm packages, as <a id="_idIndexMarker558"/>they sometimes include documentation such as the previous snippet. Now, to see how our release object looks in Kubernetes, we can run the following command:</p>
			<pre>Kubectl get secret -n cert-manager</pre>
			<p>This results in the following output:</p>
			<div><div><img src="img/B14790_11_001.jpg" alt="Figure 11.1 – Secrets List output from kubectl"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Secrets List output from kubectl</p>
			<p>As you can see, one of the secrets has its type as <code>helm.sh/release.v1</code>. This is the secret that Helm is using to track the Cert Manager release.</p>
			<p>Finally, to see the release listed in the Helm CLI, we can run the following command:</p>
			<pre>helm ls -A</pre>
			<p>This command will list Helm releases in all namespaces (just like <code>kubectl get pods -A</code> would list pods in all namespaces). The output will be as follows:</p>
			<div><div><img src="img/B14790_11_002.jpg" alt="Figure 11.2 – Helm Release List output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Helm Release List output</p>
			<p>Now, Helm has more moving parts, including <code>upgrades</code>, <code>rollbacks</code> and more, and we'll review these <a id="_idIndexMarker559"/>in the next section. In order to show off what Helm can <a id="_idIndexMarker560"/>do, we<a id="_idIndexMarker561"/> will create and install a chart from scratch.</p>
			<h3>Creating a Helm chart</h3>
			<p>So, we want to create <a id="_idIndexMarker562"/>a Helm chart for our application. Let's set the stage. Our goal is to deploy a simple Node.js application easily to multiple environments. To this end, we will create a chart with the component pieces of our application, and then combine it with three separate values files (<code>dev</code>, <code>staging</code>, and <code>production</code>) in order to deploy our application to our three environments.</p>
			<p>Let's start with the folder structure of our Helm chart. As we mentioned previously, a Helm chart consists of templates, a metadata file, and optional values. We're going to inject the values when we actually install our chart, but we can structure our folder like this:</p>
			<pre>Chart.yaml
charts/
templates/
dev-values.yaml
staging-values.yaml
production-values.yaml</pre>
			<p>One thing we haven't yet mentioned is that you can actually have a folder of Helm charts inside an existing chart! These subcharts can make it easy to split up complex applications into components. For the purpose of this book, we will not be using subcharts, but if your application is getting too complex or modular for a singular chart, this is a valuable feature.</p>
			<p>Also, you can see that we have a different environment file for each environment, which we will use during our installation command.</p>
			<p>So, what does a <code>Chart.yaml</code> file look like? This file will contain some basic metadata about your chart, and<a id="_idIndexMarker563"/> typically looks something like this as a minimum:</p>
			<pre>apiVersion: v2
name: mynodeapp
version: 1.0.0</pre>
			<p>The <code>Chart.yaml</code> file supports many optional fields, which you can see at <a href="https://helm.sh/docs/topics/charts/">https://helm.sh/docs/topics/charts/</a>, but for the purposes of this tutorial, we will keep it simple. The mandatory fields are <code>apiVersion</code>, <code>name</code>, and <code>version</code>.</p>
			<p>In our <code>Chart.yaml</code> file, <code>apiVersion</code> corresponds to the version of Helm that the chart corresponds to. Somewhat confusingly, the current release of Helm, Helm V3, uses <code>apiVersion</code> <code>v2</code>, while older versions of Helm, including Helm V2, also use <code>apiVersion</code> <code>v2</code>. </p>
			<p>Next, the <code>name</code> field corresponds to the name of our chart. This is pretty self-explanatory, although remember that we have the ability to name a specific release of a chart – something that comes in handy for multiple environments.</p>
			<p>Finally, we have the <code>version</code> field, which corresponds to the version of the chart. This field supports <strong class="bold">SemVer</strong> (semantic versioning).</p>
			<p>So, what do our templates actually look like? Helm charts use the Go templates library under the hood (see <a href="https://golang.org/pkg/text/template/">https://golang.org/pkg/text/template/</a> for more information) and support all sorts of powerful manipulations, helper functions, and much, much more. For now, we will keep things extremely simple to give you an idea of the basics. A full discussion of Helm chart creation could be a book on its own!</p>
			<p>To start, we can use a Helm CLI command to autogenerate our <code>Chart</code> folder, with all the previous files and folders, minus subcharts and values files, generated for you. Let's try it – first create a new Helm chart with the following command:</p>
			<pre>helm create myfakenodeapp</pre>
			<p>This command will create an autogenerated chart in a folder named <code>myfakenodeapp</code>. Let's check the contents of our <code>templates</code> folder with the following command:</p>
			<pre>Ls myfakenodeapp/templates</pre>
			<p>This command will result in the following output:</p>
			<pre>helpers.tpl
deployment.yaml
NOTES.txt
service.yaml</pre>
			<p>This autogenerated chart<a id="_idIndexMarker564"/> can help a lot as a starting point, but for the purposes of this tutorial, we will make these from scratch. </p>
			<p>Create a new folder called <code>mynodeapp</code> and put the <code>Chart.yaml</code> file we showed you earlier in it. Then, create a folder inside called <code>templates</code>. </p>
			<p>One thing to keep in mind: a Kubernetes resource YAML is, by itself, a valid Helm template. There is no requirement to use any variables in your templates. You can just write regular YAML, and Helm installs will still work.</p>
			<p>To show this, let's get started by adding a single template file to our templates folder. Call it <code>deployment.yaml</code> and include the following non-variable YAML:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">deployment.yaml:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-myapp
  labels:
    app: frontend-myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend-myapp
  template:
    metadata:
      labels:
        app: frontend-myapp
    spec:
      containers:
      - name: frontend-myapp
        image: myrepo/myapp:1.0.0
        ports:
        - containerPort: 80</pre>
			<p>As you can see, this <a id="_idIndexMarker565"/>YAML is just a regular Kubernetes resource YAML. We aren't using any variables in our template.</p>
			<p>Now, we have enough to actually install our chart. Let's do that next.</p>
			<h3>Installing and uninstalling a Helm chart</h3>
			<p>To install<a id="_idIndexMarker566"/> a <a id="_idIndexMarker567"/>chart with Helm V3, you run a <code>helm install</code> command from the <code>root</code> directory of the chart:</p>
			<pre>helm install myapp .</pre>
			<p>This installation command creates a Helm release called <code>frontend-app</code> and installs our chart. Right now, our chart only consists of a single deployment with two pods, and we should be able to see it running in our cluster with the following command:  </p>
			<pre>kubectl get deployment</pre>
			<p>This should result in the following output:</p>
			<pre>NAMESPACE  NAME            READY   UP-TO-DATE   AVAILABLE   AGE
default    frontend-myapp  2/2     2            2           2m</pre>
			<p>As you can see from the output, our Helm <code>install</code> command has successfully created a deployment object in Kubernetes.</p>
			<p>Uninstalling our chart is just as easy. We can install all the Kubernetes resources installed via our chart by running the following command:</p>
			<pre>helm uninstall myapp</pre>
			<p>This <code>uninstall</code> command (<code>delete</code> in Helm V2) just takes the name of our Helm release.  </p>
			<p>Now, so far, we <a id="_idIndexMarker568"/>have not used any of the real power of Helm – we've been using <a id="_idIndexMarker569"/>it as a <code>kubectl</code> alternative without any added features. Let's change this by implementing some variables in our chart.</p>
			<h3>Using template variables</h3>
			<p>Adding variables to <a id="_idIndexMarker570"/>our Helm chart templates is as simple as using double bracket – <code>{{ }}</code> – syntax. What we put in the double brackets will be taken directly from the values that we use when installing our chart using dot notation.</p>
			<p>Let's look at a quick example. So far, we have our app name (and container image name/version) hardcoded into our YAML file. This constrains us significantly if we want to use our Helm chart to deploy different applications or different application versions.</p>
			<p>In order to address this, we're going to add template variables to our chart. Take a look at this resulting template:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Templated-deployment.yaml:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-{{ .Release.Name }}
  labels:
    app: frontend-{{ .Release.Name }}
    chartVersion: {{ .Chart.version }}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend-{{ .Release.Name }}
  template:
    metadata:
      labels:
        app: frontend-{{ .Release.Name }}
    spec:
      containers:
      - name: frontend-{{ .Release.Name }}
        image: myrepo/{{ .Values.image.name }}
:{{ .Values.image.tag }}
        ports:
        - containerPort: 80</pre>
			<p>Let's go over<a id="_idIndexMarker571"/> this YAML file and review our variables. We're using a few different types of variables in this file, but they all use the same dot notation.</p>
			<p>Helm actually supports a few different top-level objects. These are the main objects you can reference in your templates:</p>
			<ul>
				<li><code>.Chart</code>: Used to reference metadata values in the <code>Chart.yaml</code> file</li>
				<li><code>.Values</code>: Used to reference values passed into the chart from a <code>values</code> file at install time</li>
				<li><code>.Template</code>: Used to reference some info about the current template file</li>
				<li><code>.Release</code>: Used to reference information about the Helm release</li>
				<li><code>.Files</code>: Used to reference files in the chart that are not YAML templates (for instance, <code>config</code> files)</li>
				<li><code>.Capabilities</code>: Used to reference information about the target Kubernetes cluster (in other words, version)</li>
			</ul>
			<p>In our YAML file, we're<a id="_idIndexMarker572"/> using several of these. Firstly, we're referencing the <code>name</code> of our release (contained within the <code>.Release</code> object) in several places. Next, we are leveraging the <code>Chart</code> object to inject metadata into the <code>chartVersion</code> key. Finally, we are using the <code>Values</code> object to reference both the container image <code>name</code> and <code>tag</code>.</p>
			<p>Now, the last thing we're missing is the actual values that we will inject via <code>values.yaml</code>, or in the CLI command. Everything else will be created using <code>Chart.yaml</code>, or values that we will inject at runtime via the <code>helm</code> command itself.  </p>
			<p>With that in mind, let's create our values file from our template that we will be passing in our image <code>name</code> and <code>tag</code>. So, let's include those in the proper format:</p>
			<pre>image:
  name: myapp
  tag: 2.0.1</pre>
			<p>Now we can install our app via our Helm chart! Do this with the following command:</p>
			<pre>helm install myrelease -f values.yaml .</pre>
			<p>As you can see, we are passing in our values with the <code>-f</code> key (you can also use <code>--values</code>). This command will install the release of our application.</p>
			<p>Once we have a <a id="_idIndexMarker573"/>release, we can upgrade to a new version or roll back to an old one using the Helm CLI – we'll cover this in the next section.</p>
			<h3>Upgrades and rollbacks</h3>
			<p>Now that we <a id="_idIndexMarker574"/>have an active <a id="_idIndexMarker575"/>Helm release, we can upgrade it. Let's make a small change to our <code>values.yaml</code>:</p>
			<pre>image:
  name: myapp
  tag: 2.0.2</pre>
			<p>To make this a new version of our release, we also need to change our chart YAML:</p>
			<pre>apiVersion: v2
name: mynodeapp
version: 1.0.1</pre>
			<p>Now, we can upgrade our release using the following command:</p>
			<pre>helm upgrade myrelease -f values.yaml .</pre>
			<p>If, for any reason, we wanted to roll back to an earlier version, we can do so with the following command:</p>
			<pre>helm rollback myrelease 1.0.0</pre>
			<p>As you can see, Helm allows for seamless templating, releases, upgrades, and rollbacks of applications. As we mentioned previously, Kustomize hits many of the same points but does it in a much different way – let's see how.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor258"/>Using Kustomize with Kubernetes</h2>
			<p>While Helm <a id="_idIndexMarker576"/>charts can get quite complex, Kustomize uses YAML<a id="_idIndexMarker577"/> without any variables, and instead uses a patch and override-based method of applying different configurations to a base set of Kubernetes resources.</p>
			<p>Using Kustomize is extremely simple, and as we mentioned earlier in the chapter, there's no prerequisite CLI tool. Everything works by using the <code>kubectl apply -k /path/kustomize.yaml</code> command without installing anything new. However, we will also demonstrate the flow using the Kustomize CLI tool. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In order to install the Kustomize CLI tool, you can check the installation instructions at <a href="https://kubernetes-sigs.github.io/kustomize/installation">https://kubernetes-sigs.github.io/kustomize/installation</a>.</p>
			<p>Currently, the installation uses the following command:</p>
			<pre>curl -s "https://raw.githubusercontent.com/\
kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash</pre>
			<p>Now that we<a id="_idIndexMarker578"/> have Kustomize installed, let's apply Kustomize to<a id="_idIndexMarker579"/> our existing use case. We're going to start from our plain Kubernetes YAML (before we started adding Helm variables):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">plain-deployment.yaml:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-myapp
  labels:
    app: frontend-myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend-myapp
  template:
    metadata:
      labels:
        app: frontend-myapp
    spec:
      containers:
      - name: frontend-myapp
        image: myrepo/myapp:1.0.0
        ports:
        - containerPort: 80</pre>
			<p>With our initial <code>deployment.yaml</code> created, we can now create a Kustomization file, which we call <code>kustomize.yaml</code>. </p>
			<p>When we later <a id="_idIndexMarker580"/>call a <code>kubectl</code> command with the <code>-k</code> parameter, <code>kubectl</code> will look <a id="_idIndexMarker581"/>for this <code>kustomize</code> YAML file and use it to determine which patches to apply to all the other YAML files passed to the <code>kubectl</code> command.</p>
			<p>Kustomize lets us patch individual values or set common values to be automatically set. In general, Kustomize will create new lines, or update old lines if the key already exists in the YAML. There are three ways to apply these changes:</p>
			<ul>
				<li>Specify changes directly in a Kustomization file.</li>
				<li>Use the <code>PatchStrategicMerge</code> strategy with a <code>patch.yaml</code> file along with a Kustomization file.</li>
				<li>Use the <code>JSONPatch</code> strategy with a <code>patch.yaml</code> file along with a Kustomization file.</li>
			</ul>
			<p>Let's start with using a Kustomization file specifically to patch the YAML.</p>
			<h3>Specifying changes directly in a Kustomization file</h3>
			<p>If we want to directly<a id="_idIndexMarker582"/> specify changes within the Kustomization file, we can do so, but our options are somewhat limited. The types of keys we can use for a Kustomization file are as follows:</p>
			<ul>
				<li><code>resources</code> – Specifies which files are to be customized when patches are applied</li>
				<li><code>transformers</code> – Ways to directly apply patches from within the Kustomization file</li>
				<li><code>generators</code> – Ways to create new resources from the Kustomization file</li>
				<li><code>meta</code> – Sets metadata fields that can influence generators, transformers, and resources</li>
			</ul>
			<p>If we want to specify direct patches in our Kustomization file, we need to use transformers. The aforementioned <code>PatchStrategicMerge</code> and <code>JSONPatch</code> merge strategies are two types of transformers. However, to directly apply changes to the Kustomization file, we can use one of several transformers, which include <code>commonLabels</code>, <code>images</code>, <code>namePrefix</code>, and <code>nameSuffix</code>.</p>
			<p>In the following Kustomization file, we are applying changes to our initial deployment <code>YAML</code> using both <code>commonLabels</code> and <code>images</code> transformers. </p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Deployment-kustomization-1.yaml:</p>
			<pre>apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
namespace: default
commonLabels:
  app: frontend-app
images:
  - name: frontend-myapp
    newTag: 2.0.0
    newName: frontend-app-1</pre>
			<p>This particular <code>Kustomization.yaml</code> file updates the image tag from <code>1.0.0</code> to <code>2.0.0</code>, updates the name of the app from <code>frontend-myapp</code> to <code>frontend-app</code>, and updates the name of the container from <code>frontend-myapp</code> to <code>frontend-app-1</code>. </p>
			<p>For a full rundown of the specifics of each of these transformers, you can check the Kustomize docs at <a href="https://kubernetes-sigs.github.io/kustomize/">https://kubernetes-sigs.github.io/kustomize/</a>. The Kustomize file assumes that <code>deployment.yaml</code> is in the same folder as itself.</p>
			<p>To see the result when our Kustomize file is applied to our deployment, we can use the Kustomize CLI tool. We<a id="_idIndexMarker583"/> will use the following command to generate the kustomized output:</p>
			<pre>kustomize build deployment-kustomization1.yaml</pre>
			<p>This command will give the following output:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-myapp
  labels:
    app: frontend-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend-app
  template:
    metadata:
      labels:
        app: frontend-app
    spec:
      containers:
      - name: frontend-app-1
        image: myrepo/myapp:2.0.0
        ports:
        - containerPort: 80</pre>
			<p>As you can see, the customizations from our Kustomization file have been applied. Because a <code>kustomize build</code> command outputs Kubernetes YAML, we can easily deploy the<a id="_idIndexMarker584"/> output to Kubernetes as follows:</p>
			<pre>kustomize build deployment-kustomization.yaml | kubectl apply -f -</pre>
			<p>Next, let's see how we can patch our deployment using a YAML file with <code>PatchStrategicMerge</code>.</p>
			<h3>Specifying changes using PatchStrategicMerge</h3>
			<p>To illustrate <a id="_idIndexMarker585"/>a <code>PatchStrategicMerge</code> strategy, we once again start with our same <code>deployment.yaml</code> file. This time, we will issue our changes via a combination of the <code>kustomization.yaml</code> file and a <code>patch.yaml</code> file.</p>
			<p>First, let's create our <code>kustomization.yaml</code> file, which looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US"> Deployment-kustomization-2.yaml:</p>
			<pre>apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
namespace: default
patchesStrategicMerge:
  - deployment-patch-1.yaml</pre>
			<p>As you can see, our Kustomization file references a new file, <code>deployment-patch-1.yaml</code>, in the <code>patchesStrategicMerge</code> section. Any number of patch YAML files can be added here.</p>
			<p>Then, our <code>deployment-patch-1.yaml</code> file is a simple file that mirrors our deployment with the<a id="_idIndexMarker586"/> changes we intend to make. Here is what it looks like:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Deployment-patch-1.yaml:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-myapp
  labels:
    app: frontend-myapp
spec:
  replicas: 4</pre>
			<p>This patch file is a subset of the fields in the original deployment. In this case, it simply updates the <code>replicas</code> from <code>2</code> to <code>4</code>. Once again, to apply the changes, we can use the following command:</p>
			<pre> kustomize build deployment-kustomization2.yaml</pre>
			<p>However, we can also use the <code>-k</code> flag in a <code>kubectl</code> command! This is how it looks:</p>
			<pre>Kubectl apply -k deployment-kustomization2.yaml</pre>
			<p>This command is the equivalent of the following:</p>
			<pre>kustomize build deployment-kustomization2.yaml | kubectl apply -f -</pre>
			<p>Similar<a id="_idIndexMarker587"/> to <code>PatchStrategicMerge</code>, we can also specify JSON-based patches in our Kustomization – let's look at that now.</p>
			<h3>Specifying changes using JSONPatch</h3>
			<p>To specify changes <a id="_idIndexMarker588"/>with a JSON patch file, the process is very similar to that involving a YAML patch.</p>
			<p>First, we need our Kustomization file. It looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Deployment-kustomization-3.yaml:</p>
			<pre>apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
namespace: default
patches:
- path: deployment-patch-2.json
  target:
    group: apps
    version: v1
    kind: Deployment
    name: frontend-myapp</pre>
			<p>As you can see, our Kustomize file has a section, <code>patches</code>, which references a JSON patch file along with a target. You can reference as many JSON patches as you want in this section. <code>target</code> is used to determine which Kubernetes resource specified in the resources section will receive the patch.</p>
			<p>Finally, we need our patch JSON itself, which looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Deployment-patch-2.json:</p>
			<pre>[
  {
   "op": "replace",
   "path": "/spec/template/spec/containers/0/name",
   "value": "frontend-myreplacedapp"
  }
]</pre>
			<p>This patch, when applied will perform the <code>replace</code> operation on the name of our first container. You can follow the path along with our original <code>deployment.yaml</code> file to see that it references the name of that first container. It will replace this name with the new value, <code>frontend-myreplacedapp</code>.</p>
			<p>Now that we have a solid foundation in Kubernetes resource templating and releases with Kustomize and <a id="_idIndexMarker589"/>Helm, we can move on to the automation of deployments to Kubernetes. In the next section, we'll look at two patterns to accomplishing CI/CD with Kubernetes.</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor259"/>Understanding CI/CD paradigms on Kubernetes – in-cluster and out-of-cluster</h1>
			<p>Continuous<a id="_idIndexMarker590"/> integration and deployment to Kubernetes can take<a id="_idIndexMarker591"/> many forms. </p>
			<p>Most DevOps engineers will be familiar with tools such as Jenkins, TravisCI, and others. These tools are fairly similar in that they provide an execution environment to build applications, perform tests, and call arbitrary Bash scripts in a controlled environment. Some of these tools run commands inside containers, while others don't.</p>
			<p>When it comes to Kubernetes, there are multiple schools of thought in how and where to use these tools. There is also a newer breed of CI/CD platforms that are much more tightly coupled to Kubernetes primitives, and many that are architected to run on the cluster itself.</p>
			<p>To thoroughly discuss how tooling can pertain to Kubernetes, we will split our pipelines into two logical steps:</p>
			<ol>
				<li><strong class="bold">Build</strong>: Compiling, testing applications, building container images, and sending to image repositories</li>
				<li><strong class="bold">Deploy</strong>: Updating Kubernetes resources via kubectl, Helm, or a different tool</li>
			</ol>
			<p>For the purposes of this book, we are going to focus mostly on the second deploy-focused step. Though many of the options available handle both build and deploy steps, the build step can happen just about anywhere, and is not worth our focus in a book relating to the <a id="_idIndexMarker592"/>specifics <a id="_idIndexMarker593"/>of Kubernetes. </p>
			<p>With this in mind, to discuss our tooling options, we will split our set of tools into two categories as far as the Deploy part of our pipelines:</p>
			<ul>
				<li>Out-of-cluster CI/CD</li>
				<li>In-cluster CI/CD</li>
			</ul>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor260"/>Out-of-cluster CI/CD</h2>
			<p>In the first<a id="_idIndexMarker594"/> pattern, our CI/CD tool runs outside of our target Kubernetes cluster. We call this out-of-cluster CI/CD. There is a gray area where the tool may run in a separate Kubernetes cluster that is focused on CI/CD, but we will ignore that option for now as the difference between the two categories is still mostly valid.</p>
			<p>You'll often find industry standard tooling such as Jenkins used with this pattern, but any CI tool that has the ability to run scripts and retain secret keys in a secure way can work here. A few examples are <code>,</code> <strong class="bold">CircleCI</strong>, <strong class="bold">TravisCI</strong>, <strong class="bold">GitHub Actions</strong>, and <strong class="bold">AWS CodeBuild</strong>. Helm is also a big part of this pattern, as out-of-cluster CI scripts can call Helm commands in lieu of kubectl.</p>
			<p>Some of the strengths of this pattern are to be found in its simplicity and extensibility. This is a <code>push</code>-based pattern where changes to code synchronously trigger changes in Kubernetes workloads.</p>
			<p>Some of the weaknesses of out-of-cluster CI/CD are scalability when pushing to many clusters, and the need <a id="_idIndexMarker595"/>to keep cluster credentials in the CI/CD pipeline so it has the ability to call kubectl or Helm commands.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor261"/>In-cluster CI/CD</h2>
			<p>In the second pattern, our<a id="_idIndexMarker596"/> tool runs on the same cluster that our applications run on, which means that CI/CD happens within the same Kubernetes context as our applications, as pods. We call this in-cluster CI/CD. This in-cluster pattern can still have the "build" steps occur outside the cluster, but the deploy step happens from within the cluster.</p>
			<p>These types of tools have been gaining popularity since Kubernetes was released, and many use custom resource definitions and custom controllers to do their jobs. Some examples are <strong class="bold">FluxCD</strong>, <strong class="bold">Argo CD</strong>, <strong class="bold">JenkinsX</strong>, and <strong class="bold">Tekton Pipelines</strong>. The <strong class="bold">GitOps</strong> pattern, where a Git repository is used as the source of truth for what applications should be running on a cluster, is popular in these tools.</p>
			<p>Some of the strengths of the in-cluster CI/CD pattern are scalability and security. By having the cluster "pull" changes from GitHub via a GitOps operating model, the solution can be scaled to many clusters. Additionally, it removes the need to keep powerful cluster credentials in the CI/CD system, instead having GitHub credentials on the cluster itself, which can be much better from a security standpoint.</p>
			<p>The weaknesses of the in-cluster CI/CD pattern include complexity, since this pull-based operation is slightly asynchronous (as <code>git pull</code> usually occurs on a loop, not always occurring exactly when changes are pushed).</p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor262"/>Implementing in-cluster and out-of-cluster CI/CD with Kubernetes</h1>
			<p>Since there are <a id="_idIndexMarker597"/>so many options for CI/CD with <a id="_idIndexMarker598"/>Kubernetes, we will <a id="_idIndexMarker599"/>choose two options and implement <a id="_idIndexMarker600"/>them one by one so you can compare their feature sets. First, we'll implement CI/CD to Kubernetes on AWS CodeBuild, which is a great example implementation that can be reused with any external CI system that can run Bash scripts, including Bitbucket Pipelines, Jenkins, and others. Then, we'll move on to FluxCD, an in-cluster GitOps-based CI option that is Kubernetes-native. Let's start with the external option.</p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor263"/>Implementing Kubernetes CI with AWS Codebuild</h2>
			<p>As mentioned<a id="_idIndexMarker601"/> earlier, our AWS CodeBuild CI<a id="_idIndexMarker602"/> implementation will be easy to duplicate in any script- based CI system. In many cases, the pipeline YAML definition we'll use is near identical. Also, as we discussed earlier, we are going to skip the actual building of the container image. We will instead focus on the actual deployment piece.</p>
			<p>To quickly introduce AWS CodeBuild, it is a script-based CI tool that runs Bash scripts, like many other similar tools. In the context of AWS CodePipeline, a higher-level tool, multiple separate AWS CodeBuild steps can be combined into larger pipelines.</p>
			<p>In our example, we will be using both AWS CodeBuild and AWS CodePipeline. We will not be discussing in depth how to use these two tools, but instead will keep our discussion tied specifically to how to use them for deployment to Kubernetes.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We highly recommend that you read and review the documentation for both CodePipeline and CodeBuild, since we will not be covering all of the basics in this chapter. You can find the documentation at <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a> for CodeBuild, and <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome.html</a> for CodePipeline.</p>
			<p>In practice, you would have two CodePipelines, each with one or more CodeBuild steps. The first CodePipeline is triggered on a code change in either AWS CodeCommit or another Git repository (such as GitHub). </p>
			<p>The first CodeBuild step for this pipeline runs tests and builds the container image, pushing the image <a id="_idIndexMarker603"/>to AWS <strong class="bold">Elastic Container Repository</strong> (<strong class="bold">ECR</strong>). The second CodeBuild step for the first pipeline deploys the new image to Kubernetes.</p>
			<p>The second CodePipeline is triggered anytime we commit a change to our secondary Git repository with Kubernetes resource files (infrastructure repository). It will update the Kubernetes resources using the same process. </p>
			<p>Let's start with the first CodePipeline. As mentioned earlier, it contains two CodeBuild steps:</p>
			<ol>
				<li value="1">First, to test and build the container image and push it to the ECR</li>
				<li>Second, to deploy the updated container to Kubernetes</li>
			</ol>
			<p>As we mentioned earlier in this section, we will not be spending much time on the code-to-container-image <a id="_idIndexMarker604"/>pipeline, but here is an<a id="_idIndexMarker605"/> example (not production ready) <code>codebuild</code> YAML for implementing this first step:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Pipeline-1-codebuild-1.yaml:</p>
			<pre>version: 0.2
phases:
  build:
    commands:
      - npm run build
  test:
    commands:
      - npm test
  containerbuild:
    commands:
      - docker build -t $ECR_REPOSITORY/$IMAGE_NAME:$IMAGE_TAG .
  push:
    commands:
      - docker push_$ECR_REPOSITORY/$IMAGE_NAME:$IMAGE_TAG</pre>
			<p>This CodeBuild pipeline consists of four phases. CodeBuild pipeline specs are written in YAML, and contain a <code>version</code> tag that corresponds to the version of the CodeBuild spec. Then, we have a <code>phases</code> section, which is executed in order. This CodeBuild first runs a <code>build</code> command, and then runs a <code>test</code> command in the test phase. Finally, the <code>containerbuild</code> phase creates the container image, and the <code>push</code> phase pushes the image to our container repository. </p>
			<p>One thing to keep in mind is that every value with a <code>$</code> in front of it in CodeBuild is an environment variable. These can be customized via the AWS Console or the AWS CLI, and some can come directly from the Git repository.</p>
			<p>Let's now take a<a id="_idIndexMarker606"/> look at the YAML for the second <a id="_idIndexMarker607"/>CodeBuild step of our first CodePipeline:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Pipeline-1-codebuild-2.yaml:</p>
			<pre>version: 0.2
phases:
  install:
    commands:
      - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/darwin/amd64/kubectl  
      - chmod +x ./kubectl
      - mkdir -p $HOME/bin &amp;&amp; cp ./kubectl $HOME/bin/kubectl &amp;&amp; export PATH=$PATH:$HOME/bin
      - echo 'export PATH=$PATH:$HOME/bin' &gt;&gt; ~/.bashrc
      - source ~/.bashrc
  pre_deploy:
    commands:
      - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $K8S_CLUSTER
  deploy:
    commands:
      - cd $CODEBUILD_SRC_DIR
      - kubectl set image deployment/$KUBERNETES-DEPLOY-NAME myrepo:"$IMAGE_TAG"</pre>
			<p>Let's break this file down. Our CodeBuild setup is broken down into three phases: <code>install</code>, <code>pre_deploy</code>, and <code>deploy</code>. In the <code>install</code> phase, we install the kubectl CLI tool.  </p>
			<p>Then, in the <code>pre_deploy</code> phase, we use an AWS CLI command and a couple of environment variables to update our <code>kubeconfig</code> file for communicating with our EKS cluster. In any other CI tool (or when not using EKS) you could use a different method for giving cluster credentials to your CI tool. It is important to use a safe option here, as including the <code>kubeconfig</code> file directly in your Git repository is not secure. Typically, some combination of environment variables would be great here. Jenkins, CodeBuild, CircleCI, and<a id="_idIndexMarker608"/> more have their own systems for<a id="_idIndexMarker609"/> this.</p>
			<p>Finally, in the <code>deploy</code> phase, we use <code>kubectl</code> to update our deployment (also contained in an environment variable) with the new image tag specified in the first CodeBuild step. This <code>kubectl rollout restart</code> command will ensure that new pods are started for our deployment. In combination with using the <code>imagePullPolicy</code> of <code>Always</code>, this will result in our new application version being deployed.</p>
			<p>In this case, we are patching our deployment with a specific image tag name in the ECR. The <code>$IMAGE_TAG</code> environment variable will be auto populated with the newest tag from GitHub so we can use that to automatically roll out the new container image to our deployment.</p>
			<p>Next, let's take a look at our second CodePipeline. This one contains only one step – it listens to changes from a separate GitHub repository, our "infrastructure repository". This repository does not contain code for our applications themselves, but instead Kubernetes resource YAMLs. Thus, we can change a Kubernetes resource YAML value – for instance, the number of replicas in a deployment, and see it updated in Kubernetes after the CodePipeline runs. This pattern can be extended to use Helm or Kustomize very easily.</p>
			<p>Let's take a look at the first, and only, step of our second CodePipeline:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Pipeline-2-codebuild-1.yaml:</p>
			<pre>version: 0.2
phases:
  install:
    commands:
      - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/darwin/amd64/kubectl  
      - chmod +x ./kubectl
      - mkdir -p $HOME/bin &amp;&amp; cp ./kubectl $HOME/bin/kubectl &amp;&amp; export PATH=$PATH:$HOME/bin
      - echo 'export PATH=$PATH:$HOME/bin' &gt;&gt; ~/.bashrc
      - source ~/.bashrc
  pre_deploy:
    commands:
      - aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $K8S_CLUSTER
  deploy:
    commands:
      - cd $CODEBUILD_SRC_DIR
      - kubectl apply -f .</pre>
			<p>As you can see, this CodeBuild spec is quite similar to our previous one. As before, we install kubectl and<a id="_idIndexMarker610"/> prep it for use with our<a id="_idIndexMarker611"/> Kubernetes cluster. Since we are running on AWS, we do it using the AWS CLI, but this could be done any number of ways, including by just adding a <code>Kubeconfig</code> file to our CodeBuild environment.</p>
			<p>The difference here is that instead of patching a specific deployment with a new version of an application, we are running an across-the-board <code>kubectl apply</code> command while piping in our entire infrastructure folder. This could then make any changes performed in Git be applied to the resources in our cluster. For instance, if we scaled our deployment from 2 replicas to 20 replicas by changing the value in the <code>deployment.yaml</code> file, it would be deployed to Kubernetes in this CodePipeline step and the deployment would scale up.</p>
			<p>Now that we've<a id="_idIndexMarker612"/> covered the basics of using an<a id="_idIndexMarker613"/> out-of-cluster CI/CD environment to make changes to Kubernetes resources, let's take a look at a completely different CI paradigm, where the pipeline runs on our cluster.</p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor264"/>Implementing Kubernetes CI with FluxCD</h2>
			<p>For our in-cluster <a id="_idIndexMarker614"/>CI tool, we will be using <strong class="bold">FluxCD</strong>. There <a id="_idIndexMarker615"/>are several options for in-cluster CI, including <strong class="bold">ArgoCD</strong> and <strong class="bold">JenkinsX</strong>, but we like <strong class="bold">FluxCD</strong> for its relative simplicity, and for the fact that it automatically updates pods with new container versions without any additional configuration. As an added twist, we will use FluxCD's Helm integration for managing deployments. Let's start with the installation of FluxCD (we'll assume you already have Helm installed from the previous parts of the chapter). These installations follow the official FluxCD installation instructions for Helm compatibility, as of the time of writing of this book. </p>
			<p>The official FluxCD docs <a id="_idIndexMarker616"/>can be found at <a href="https://docs.fluxcd.io/">https://docs.fluxcd.io/</a>, and we highly recommend you give them a look! FluxCD is a very complex tool, and we are only scratching the surface in this book. A full review is not in scope – we are simply trying to introduce you to the in-cluster CI/CD pattern and relevant tooling.</p>
			<p>Let's start our review by installing FluxCD on our cluster.</p>
			<h3>Installing FluxCD (H3)</h3>
			<p>FluxCD can <a id="_idIndexMarker617"/>easily be installed using Helm in a few steps:</p>
			<ol>
				<li value="1">First, we need to add the Flux Helm chart repository:<pre><strong class="bold">helm repo add fluxcd https://charts.fluxcd.io</strong></pre></li>
				<li>Next, we need to add a custom resource definition that FluxCD requires in order to be able to work with Helm releases:<pre><strong class="bold">kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml</strong></pre></li>
				<li>Before we can install the FluxCD Operator (which is the core of FluxCD functionality on Kubernetes) and the FluxCD Helm Operator, we need to create a namespace for FluxCD to live in:<pre><strong class="bold">kubectl create namespace flux</strong></pre><p>Now we can install the main pieces of FluxCD, but we'll need to give FluxCD some additional information about our Git repository.</p><p>Why? Because FluxCD uses a GitOps pattern for updates and deployments. This means that FluxCD will actively reach out to our Git repository every few minutes, instead of responding to Git hooks such as CodeBuild, for instance. </p><p>FluxCD will also <a id="_idIndexMarker618"/>respond to new ECR images via a pull-based strategy, but we'll get to that in a bit.</p></li>
				<li>To install the main pieces of FluxCD, run the following two commands and replace <code>GITHUB_USERNAME</code> and <code>REPOSITORY_NAME</code> with the GitHub user and repository that you will be storing your workload specs (Kubernetes YAML or Helm charts) in. <p>This instruction set assumes that the Git repository is public, which it likely isn't. Since most organizations use private repositories, FluxCD has specific configurations to handle this case – just check the docs at <a href="https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/">https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/</a>. In fact, to see the real power of FluxCD, you'll need to give it advanced access to your Git repository in any case, since FluxCD can write to your Git repository and automatically update manifests as new container images are created. However, we won't be getting into that functionality in this book. The FluxCD docs are definitely worth a close read as this is a complex piece of technology with many features. To tell FluxCD which GitHub repository to look at, you can set variables when installing using Helm, as in the following command:</p><pre><strong class="bold">helm upgrade -i flux fluxcd/flux \</strong>
<strong class="bold">--set git.url=git@github.com:GITHUB_USERNAME/REPOSITORY_NAME \</strong>
<strong class="bold">--namespace flux</strong>
<strong class="bold">helm upgrade -i helm-operator fluxcd/helm-operator \</strong>
<strong class="bold">--set git.ssh.secretName=flux-git-deploy \</strong>
<strong class="bold">--namespace flux</strong></pre><p>As you can see, we need to pass our GitHub username, the name of our repository, and a name that will be used for our GitHub secret in Kubernetes.</p><p>At this point, FluxCD is fully installed in our cluster and pointed at our infrastructure repository on Git! As mentioned before, this GitHub repository will contain<a id="_idIndexMarker619"/> Kubernetes YAML or Helm charts on the basis of which FluxCD will update workloads running in the cluster.</p></li>
				<li>To actually give Flux something to do, we need to create the actual manifest for Flux. We do so using a <code>HelmRelease</code> YAML file, which looks like the following:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">         helmrelease-1.yaml:</p>
			<pre>apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: myapp
  annotations:
    fluxcd.io/automated: "true"
    fluxcd.io/tag.chart-image: glob:myapp-v*
spec:
  releaseName: myapp
  chart:
    git: ssh://git@github.com/&lt;myuser&gt;/&lt;myinfrastructurerepository&gt;/myhelmchart
    ref: master
    path: charts/myapp
  values:
    image:
      repository: myrepo/myapp
      tag: myapp-v2</pre>
			<p>Let's pick this file apart. We are specifying the Git repository where Flux will find the Helm chart for our application. We are also marking the <code>HelmRelease</code> with an <code>automated</code> annotation, which tells Flux to go and poll the container image repository every few minutes and see<a id="_idIndexMarker620"/> whether there is a new version to deploy. To aid this, we include a <code>chart-image</code> filter pattern, which the tagged container image must match in order to trigger a redeploy. Finally, in the values section, we have Helm values that will be used for the initial installation of the Helm chart.</p>
			<p>To give FluxCD this information, we simply need to add this file to the root of our GitHub repository and push up a change. </p>
			<p>Once we add this release file, <code>helmrelease-1.yaml</code>, to our Git repository, Flux will pick it up within a few minutes, and then look for the specified Helm chart in the <code>chart</code> value. There's just one problem – we haven't made it yet!</p>
			<p>Currently, our infrastructure repository on GitHub only contains our single Helm release file. The folder contents look like this:</p>
			<pre>helmrelease1.yaml</pre>
			<p>To close the loop and allow Flux to actually deploy our Helm chart, we need to add it to this infrastructure repository. Let's do so, making the final folder contents in our GitHub repository look like this:</p>
			<pre>helmrelease1.yaml
myhelmchart/
  Chart.yaml
  Values.yaml
  Templates/
    … chart templates</pre>
			<p>Now, when FluxCD next checks the infrastructure repository on GitHub, it will first find the Helm release YAML file, which will then point it to our new Helm chart.</p>
			<p>FluxCD, with a new release and a Helm chart, will then deploy our Helm chart to Kubernetes!</p>
			<p>Then, any time a change is made to either the Helm release YAML or any file in our Helm chart, FluxCD will pick it up and, within a few minutes (on its next loop), will deploy the change.</p>
			<p>In addition, any time a new container image with a matching tag to the filter pattern is pushed to the image<a id="_idIndexMarker621"/> repository, a new version of the app will automatically be deployed – it's that easy. This means that FluxCD is listening to two locations – the infrastructure GitHub repository and the container repository, and will deploy any changes to either location.</p>
			<p>You can see how this maps to our out-of-cluster CI/CD implementation where we had one CodePipeline to deploy new versions of our App container, and another CodePipeline to deploy any changes to our infrastructure repository. FluxCD does the same thing in a pull-based way.</p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor265"/>Summary</h1>
			<p>In this chapter, we learned about template code generation on Kubernetes. We reviewed how to create flexible resource templates using both Helm and Kustomize. With this knowledge, you will be able to template your complex applications using either solution, create, or deploy releases. Then, we reviewed two types of CI/CD on Kubernetes; first, external CI/CD deployment to Kubernetes via kubectl, and then in-cluster CI paradigms using FluxCD. With these tools and techniques, you will be able to set up CI/CD to Kubernetes for production applications.</p>
			<p>In the next chapter, we will review security and compliance on Kubernetes, an important topic in today's software environment.</p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor266"/>Questions</h1>
			<ol>
				<li value="1">What are two differences between Helm and Kustomize templating?</li>
				<li>How should Kubernetes API credentials be handled when using an external CI/CD setup?</li>
				<li>What are some of the reasons as to why an in-cluster CI setup may be preferable to an out-of-cluster setup? And vice versa?</li>
			</ol>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor267"/>Further reading</h1>
			<ul>
				<li>Kustomize docs: https:<a href="https://kubernetes-sigs.github.io/kustomize/">https://kubernetes-sigs.github.io/kustomize/</a></li>
				<li>Helm docs <a href="https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/">https://docs.fluxcd.io/en/latest/tutorials/get-started-helm/</a></li>
			</ul>
		</div>
	</body></html>