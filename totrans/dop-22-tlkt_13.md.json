["```\n`1` chmod +x scripts/dm-swarm-09.sh\n`2` \n`3` ./scripts/dm-swarm-09.sh\n`4` \n`5` `eval` `$(`docker-machine env swarm-1`)`\n`6` \n`7` docker stack ls \n```", "`````````````````````````````````````````````````````````````````` We executed the `dm-swarm-09.sh` script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks, and deployed the stacks. The last command listed all the stacks in the cluster. We are running `proxy`, `monitor`, `exporter`, and `go-demo` stacks. Those four comprise the whole toolkit we used by now.    ### Preparing The System For Alerts    We’ll deploy the stack defined in [stacks/jenkins.yml](https://github.com/vfarcic/docker-flow-monitor/blob/master/stacks/jenkins.yml). The definition is as follows.    ```  `1` `version``:` `'3.1'`  `2`   `3` `services``:`  `4`   `5`  `master``:`  `6`    `image``:` `vfarcic``/``jenkins`  `7`    `ports``:`  `8`      `-` `50000``:``50000`  `9`    `environment``:` `10 `      `-` `JENKINS_OPTS``=``\"--prefix=/jenkins\"` `11 `    `networks``:` `12 `      `-` `proxy` `13 `      `-` `default` `14 `    `deploy``:` `15 `      `labels``:` `16 `        `-` `com``.``df``.``notify``=``true` `17 `        `-` `com``.``df``.``distribute``=``true` `18 `        `-` `com``.``df``.``servicePath=/jenkins` `19 `        `-` `com``.``df``.``port``=``8080` `20 `    `extra_hosts:` `21 `      `-` `\"${SLACK_HOST:-devops20.slack.com}:${SLACK_IP:-54.192.78.227}\"` `22 `    `secrets``:` `23 `      `-` `jenkins``-``user` `24 `      `-` `jenkins``-``pass` `25`  `26 `  `agent``:` `27 `    `image``:` `vfarcic``/``jenkins``-``swarm``-``agent` `28 `    `environment``:` `29 `      `-` `USER_NAME_SECRET``=/run``/``secrets/``$``{``JENKINS_USER_SECRET``:-``jenkins``-``user``}` `30 `      `-` `PASSWORD_SECRET``=/run``/``secrets/``$``{``JENKINS_PASS_SECRET``:-``jenkins``-``pass``}` `31 `      `-` `COMMAND_OPTIONS``=-master` `http``:``//``master``:``8080``/``jenkins` `-``labels` `'prod'` `-``execu\\` `32` `tors` `4` `33 `    `networks``:` `34 `      `-` `default` `35 `    `volumes``:` `36 `      `-` `/``var``/``run``/``docker``.``sock``:``/``var``/``run``/``docker``.``sock` `37 `    `secrets``:` `38 `      `-` `jenkins``-``user` `39 `      `-` `jenkins``-``pass` `40 `    `deploy``:` `41 `      `placement``:` `42 `        `constraints``:` `[``node``.``role` `==` `manager``]` `43`  `44` `networks``:` `45 `  `proxy``:` `46 `    `external``:` `true` `47 `  `default``:` `48 `    `external``:` `false` `49`  `50` `secrets``:` `51 `  `jenkins``-``user``:` `52 `    `external``:` `true` `53 `  `jenkins``-``pass``:` `54 `    `external``:` `true`  ```   ````````````````````````````````````````````````````````````````` The stack contains two services. The first one is Jenkins master. We are running `vfarcic/jenkins` instead the [official Jenkins image](https://hub.docker.com/_/jenkins/). The `vfarcic/jenkins` image is already built with an administrative user and has all the plugins we’ll need. With it, we’ll be able to skip Jenkins’ setup process. I won’t go into more detail about the image. If you’re curious, please read the [Automating Jenkins Docker Setup](https://technologyconversations.com/2017/06/16/automating-jenkins-docker-setup/) article.    The `master` service from the stack publishes port `50000` so that other agents from this, or other clusters, can connect to it. If all the agents run inside the same cluster, there would be no need for this port. Instead, they would be attached to the same Overlay network. Since, in most cases, Jenkins agents tend to be spread across multiple clusters, having the port open is a must.    Environment variable `JENKINS_OPTS` defines `/jenkins` as the prefix so that [Docker Flow Proxy](http://proxy.dockerflow.com/) can distinguish requests meant for Jenkins from those that should be forwarded to the other services inside the cluster. The service will be attached to the `proxy` and `default` networks. The first one will be used for communication with *Docker Flow Proxy* while the second is meant to connect it to the agent. Labels are there to provide sufficient information to the proxy so that it can reconfigure itself.    We had to add the Slack address as the extra host. Otherwise, Jenkins would not know the address of the `devops20.slack.com` domain. Finally, we specified two secrets (`jenkins-user` and `jenkins-pass`) that will define the credentials of the administrative user.    The `agent` follows a similar logic. We’re using `vfarcic/jenkins-swarm-agent` image that contains Docker, Docker Compose, and [Jenkins Swarm Plugin](https://wiki.jenkins.io/display/JENKINS/Swarm+Plugin). The latter allows us to connect to the master automatically. The alternative would be to use the “traditional” approach of adding agents manually through Jenkins’ UI.    Please note that the environment variable `COMMAND_OPTIONS` has the `-labels` argument set to `prod`. Since this agent will run on the production cluster, we need to identify it as such. Even though in this chapter we won’t use Jenkins for continuous deployment processes, it is important to label agents from the start so that, later on, we can add others that will serve a different purpose.    Just as the `main` service, the agent uses `jenkins-user` and `jenkins-pass` secrets to provide credentials that will be used to connect to Jenkins master.    Finally, we need the agent to communicate with one of the Docker managers, so we set the `node.role == manager` constraint. Without this constraint, agents would not be able to spin new services since only managers are allowed to perform such actions. Containers that form Jenkins agents have Docker socket mounted so that Docker commands executed inside them spin containers on one of the nodes, not inside the container. The later would produce Docker-in-Docker (DinD) which is, in most cases, not a good idea. If you do not want to take my word for granted, please read Jerome’s post [Using Docker-in-Docker for your CI or testing environment? Think twice.](http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/)  ![Figure 9-1: Jenkins agents connected to a master and Docker managers](img/00043.jpeg)  Figure 9-1: Jenkins agents connected to a master and Docker managers    Now that we have a general idea about the services inside the `jenkins` stack, we can deploy it.    ```  `1` `echo` `\"admin\"` `|` `\\`  `2`    docker secret create jenkins-user -  `3`   `4` `echo` `\"admin\"` `|` `\\`  `5`    docker secret create jenkins-pass -  `6`   `7` `export` `SLACK_IP``=``$(`ping `\\`  `8`    -c `1` devops20.slack.com `\\`  `9`    `|` awk -F`'[()]'` `'/PING/{print $2}'``)` `10`  `11` docker stack deploy `\\` `12 `    -c stacks/jenkins.yml jenkins  ```   ```````````````````````````````````````````````````````````````` We created the secrets and deployed the stack. The value of the environment variable `SLACK_IP` was obtained by pinging `devops20.slack.com` domain.    All that is left, before we start using Jenkins, is a bit of patience. We need to wait until Docker pulls the images. Please execute `docker stack ps jenkins` to confirm that the services are running.    Let’s open Jenkins UI in a browser.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins\"`  ```   ``````````````````````````````````````````````````````````````` If Jenkins does not open, please wait a few moments and refresh the screen. The fact that Docker service is running does not mean that the process inside it is initialized. Jenkins needs ten to fifteen seconds (depending on hardware) to start.    Once you see the Jenkins home screen, please click the *Log in* link located in the top-right corner of the screen, and use *admin* as both username and password. Click the *log in* button to authenticate.    We should confirm that the agent was added to the master by observing the *computer* screen.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/computer\"`  ```   `````````````````````````````````````````````````````````````` You should see two agents. The *master* agent is set up by default with each Jenkins instance. The second agent identified with a hash name was added through the `agent` service in the stack.  ![Figure 9-2: Jenkins agent automatically added to the master](img/00044.jpeg)  Figure 9-2: Jenkins agent automatically added to the master    ### Creating A Scaling Pipeline    Now comes the exciting part. We’re about to start writing a Pipeline job that will serve as the base for the first self-adaptation script.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/newJob\"`  ```   ````````````````````````````````````````````````````````````` Once inside the *New Job* screen, please type *service-scale* as the item name. Select *Pipeline* as job type and click the *OK* button.    Since Jenkins service we created comes with enabled authorization, we need an authentication mechanism for triggering builds. We could use the administrative *username* and *password*. A better option is to make a trigger that will be independent of any particular user. That can be accomplished with tokens.    Please select the *Trigger builds remotely* checkbox from the *Build Trigger* section of the job configuration screen. Type *DevOps22* as the *Authentication Token*. We’ll use it to authenticate remote requests which will trigger a build of this job.    Now we can start writing a Pipeline script. There are quite a few things that it should do so we’ll go step by step. The first thing we need is parameters.    AS a minimum, we need to know which service should be scaled and how many replicas to add or remove. We’ll assume that if the number of replicas is positive, we should scale up. Similarly, if the value is negative, we should scale down.    Please type the script that follows inside the *Pipeline Script* field.    ```  `1` `pipeline` `{`  `2`  `agent` `{`  `3`    `label` `\"prod\"`  `4`  `}`  `5`  `parameters` `{`  `6`    `string``(`  `7`      `name:` `\"service\"``,`  `8`      `defaultValue:` `\"\"``,`  `9`      `description:` `\"The name of the service that should be scaled\"` `10 `    `)` `11 `    `string``(` `12 `      `name:` `\"scale\"``,` `13 `      `defaultValue:` `\"\"``,` `14 `      `description:` `\"Number of replicas to add or remove.\"` `15 `    `)` `16 `  `}` `17 `  `stages` `{` `18 `    `stage``(``\"Scale\"``)` `{` `19 `      `steps` `{` `20 `        `echo` `\"Scaling $service by $scale\"` `21 `      `}` `22 `    `}` `23 `  `}` `24` `}`  ```   ```````````````````````````````````````````````````````````` If you do not like typing, feel free to copy and paste the contents of the [service-scale-1.groovy Gist](https://gist.github.com/vfarcic/98778e9f414f1af1ab30cd07e39b015a).    Don’t forget to click the *Save* button.    Since we’re trying to scale services running in production, we defined the agent as such.    Next, we set the parameters `service` and `scale`.    Finally, we have only one stage (`Scale`) with a single step that prints a message. Each pipeline has one or more stages, and each stage is a collection of steps. A step (in this case `echo`) is a task or logic that should be executed.    Please note that we are using [Declarative](https://jenkins.io/doc/book/pipeline/syntax/#declarative-pipeline) instead [Scripted](https://jenkins.io/doc/book/pipeline/syntax/#scripted-pipeline) Pipeline syntax. Both have pros and cons. Declarative is a more opinionated and structured syntax while Scripted provides more freedom. The main reason we’re using Declarative flavor is that it has better support for the new [Blue Ocean](https://jenkins.io/projects/blueocean/) UI. Moreover, I happen to know the Jenkins roadmap and Declarative Pipeline is at its center.    The default Jenkins UI is not among the prettiest in town. It, kind of, hurts the eyes if you look at it for more than a couple of seconds. Since I do not want your health to deteriorate as a result of reading this book, we’ll switch to *Blue Ocean*. It is available as the alternative UI (soon to become the default) and we already have it installed as one of the plugins.    Please click the *Open Blue Ocean* link located in the left-hand menu.    And… Lo and behold… We just jumped through time from the 80s to the present tense (at least from the aesthetic perspective).    Now we can see our simple pipeline script in action.    Since we did not yet run this Pipeline, you will be presented with the *This job has not been run* message and the *Run* button. Please click it.    The job will fail the first time we run it. You can consider it a bug that will, hopefully, be fixed shortly. It failed because it got confused with the parameters we specified. I’ll skip the debate about the reasons behind this bug since the workaround is straightforward. Just rerun it by pressing the *Run* button located in the top-left corner.    You’ll be presented with a screen that contains the input parameters we specified in the script. Please type *go-demo_main* as *the name of the service that should be scaled* and *2* as the *number of replicas to add or remove*. Click the *Run* button.    This time the Pipeline worked, and we can observe the result by clicking on the row of the last build which, in this case, should be *2*.    We specified only one stage that contains a single step that prints the message. Please click the *Print Message* row to see the result. The output should be as follows.    ``` `1` Scaling go-demo_main by 2  ```  ``````````````````````````````````````````````````````````` ![Figure 9-3: Jenkins Pipeline with a simple Print Message step](img/00045.jpeg)  Figure 9-3: Jenkins Pipeline with a simple Print Message step    Even though Blue Ocean UI is very pleasing, our goal is not to use it to execute builds. Instead, we should invoke it through an HTTP request. That way, we can be confident that Alertmanager will be capable of invoking it as well.    Please execute the command that follows.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=2\"`  ```   `````````````````````````````````````````````````````````` The request we sent is very straightforward. We invoked `buildWithParameters` endpoint of the job and passed the token and required inputs as query parameters.    We received no response and can consider that no news is good news. The job was run, and we can confirm that through the UI.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/blue/organizations/jenkins/ser\\` `2` `vice-scale/activity\"`  ```   ````````````````````````````````````````````````````````` You’ll see the list of builds (there should be three). While the *admin* user executed the first two through the UI, the last one was triggered remotely. We can see that by observing the *started by the remote host* message.    Please click the row of the last build and observe that the *Print Message* is the same as when we executed the job through UI.    Similarly, we can change the `scale` parameter to a negative value if we’d like to scale down.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=-1\"`  ```   ```````````````````````````````````````````````````````` If you repeat the steps from before, the output of the *Print Message* should be *Scaling go-demo_main by -1*.    The Pipeline we have does not do anything but accept parameters and print a message that confirms that parameters are passed correctly. As you probably guessed, we are missing the main ingredient.    We need to tell Docker to scale the service. The problem is that Swarm does not accept relative scale values. We cannot instruct it to increase the number of replicas by two nor to decrease it by, let’s say, one. We can overcome this limitation by finding out the current number of replicas and adding or subtracting the value of the `scale` parameter.    First things first. How can we find out the current number of replicas? The answer lies in the `docker service inspect` command.    Let’s see the output Docker provides if we inspect the `go-demo_main` service.    ``` `1` docker service inspect go-demo_main  ```   ``````````````````````````````````````````````````````` The output is too long to be presented here. Instead, we’ll focus on the part that interests us. In particular, we need the `Replicas` value. The relevant part of the output is as follows.    ```  `1` [  `2`    {  `3`        ...  `4`        \"Spec\": {  `5`            ...  `6`            \"Mode\": {  `7`                \"Replicated\": {  `8`                    \"Replicas\": 3  `9`                } `10 `            }, `11 `            ...  ```   `````````````````````````````````````````````````````` As you can see, we got the information that the service runs three replicas.    We can execute the same command from Jenkins pipeline, capture the output, and filter it in a way that only the value of the `Replicas` key is retrieved.    In the spirit of brevity, we’ll go only through the `stages` section of the Pipeline. The whole scripts are available as Gist in case you want to copy and paste them in their entirety.    ```  `1` `...`  `2` `stages` `{`  `3`  `stage``(``\"Scale\"``)` `{`  `4`    `steps` `{`  `5`      `script` `{`  `6`          `def` `inspectOut` `=` `sh` `script:` `\"docker service inspect $service\"``,`  `7`                              `returnStdout:` `true`  `8`          `def` `inspectJson` `=` `readJSON` `text:` `inspectOut``.``trim``()`  `9`          `def` `currentReplicas` `=` `inspectJson``[``0``].``Spec``.``Mode``.``Replicated``.``Replicas` `10 `          `def` `newReplicas` `=` `currentReplicas` `+` `scale``.``toInteger``()` `11 `          `echo` `\"We should scale from $currentReplicas to $newReplicas replicas\"` `12 `      `}` `13 `    `}` `14 `  `}` `15` `}` `16` `...`  ```   ````````````````````````````````````````````````````` Due to Declarative Pipeline’s decision not to allow an easy way to declare variables, we coded everything as one `script` step.    The script is executing `docker service inspect` as an `sh` step. The `returnStdout` argument is mandatory if we want to be able to capture the output of a command. Later on, we’re using the `readJSON` step that converts plain text to JSON map. The current number of replicas is retrieved by filtering JSON array. We limited the output to the first element and navigated through `Spec`, `Mode`, `Replicated`, and `Replicas` items. The result is stored in the variable `currentReplicas`.    From there on, it is a simple math of subtracting the current number of replicas with the `scale` parameter. Since it is a string, we had to convert it to an integer.    Finally, we are outputting the result using the `echo` step.    The complete code can be found in the [service-scale-2.groovy Gist](https://gist.github.com/vfarcic/77bc5baae1b19d13a7d048f27d03eaff).    Let’s open the *service-scale* configure screen and modify the script.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/configure\"`  ```   ```````````````````````````````````````````````````` Feel free to replace the current script with the one from the [service-scale-2.groovy Gist](https://gist.github.com/vfarcic/77bc5baae1b19d13a7d048f27d03eaff). Personally, I learn better when I write code instead of copying and pasting snippets. No matter the choice, please click the *Apply* button once the Pipeline is updated.    Let us repeat the build request and see the outcome.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=2\"`  ```   ``````````````````````````````````````````````````` We’ll go the the job activity screen and observe the result.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/blue/organizations/jenkins/ser\\` `2` `vice-scale/activity\"`  ```   `````````````````````````````````````````````````` Please click the row of the top-most (most recent) build followed with the click on the last (bottom) step with the *Print Message* label.    The output should be as follows.    ``` `1` We should scale from 3 to 5 replicas  ```  ````````````````````````````````````````````````` ![Figure 9-4: Jenkins Pipeline with a Print Message stating that we should scale to five replicas](img/00046.jpeg)  Figure 9-4: Jenkins Pipeline with a Print Message stating that we should scale to five replicas    Let us confirm that de-scaling calculation works as well.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=-1\"`  ```   ```````````````````````````````````````````````` If we open the details of the last build and expand the last step, the message should be as follows.    ``` `1` We should scale from 3 to 2 replicas  ```   ``````````````````````````````````````````````` We are still not performing scaling but, at this moment, we are capable of discovering the current number of replicas and performing a simple calculation that provides us with the number of replicas our system should have.    Now we are ready to expand the script and truly scale the service.    Equipped with the desired number of replicas stored in the variable `newReplicas`, all we have to do is execute `docker service scale` command. The updated Pipeline script, limited to the relevant parts, is as follows.    ```  `1` `...`  `2` `script` `{`  `3`    `def` `inspectOut` `=` `sh` `script:` `\"docker service inspect $service\"``,`  `4`                        `returnStdout:` `true`  `5`    `def` `inspectJson` `=` `readJSON` `text:` `inspectOut``.``trim``()`  `6`    `def` `currentReplicas` `=` `inspectJson``[``0``].``Spec``.``Mode``.``Replicated``.``Replicas`  `7`    `def` `newReplicas` `=` `currentReplicas` `+` `scale``.``toInteger``()`  `8`    `sh` `\"docker service scale $service=$newReplicas\"`  `9`    `echo` `\"$service was scaled from $currentReplicas to $newReplicas replicas\"` `10` `}` `11` `...`  ```   `````````````````````````````````````````````` The only addition is the `sh \"docker service scale $service=$newReplicas\"` line. It should be pretty obvious what it does so we’ll just go ahead and modify it in Jenkins.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/configure\"`  ```   ````````````````````````````````````````````` Please update the current script or replace it with the [service-scale-3.groovy Gist](https://gist.github.com/vfarcic/2b160b93c6cc08320be80d284eb03017). When finished, please press the *Apply* button.    Let us run the build one more time and observe the result.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=2\"`  ```   ```````````````````````````````````````````` This time, we do not need to open Jenkins UI to see the outcome. If everything went as planned, we should see that the `go-demo_main` service is scaled from three to five replicas.    ``` `1` docker stack ps `\\` `2 `    -f desired-state`=`Running go-demo  ```   ``````````````````````````````````````````` We listed all the processes that belong to the `go-demo` stack. As a way to reduce noise from those that previously failed or were shut down, we used the filter that limited the output only to those with `Running` as the desired state.    The output is as follows (IDs are removed for brevity).    ``` `1` NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \\ `2 `  ERROR PORTS `3` go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 2 hours ago `4` go-demo_db.1   mongo:latest           swarm-1 Running       Running 2 hours ago `5` go-demo_main.2 vfarcic/go-demo:latest swarm-3 Running       Running 2 hours ago `6` go-demo_main.3 vfarcic/go-demo:latest swarm-2 Running       Running 2 hours ago `7` go-demo_main.4 vfarcic/go-demo:latest swarm-2 Running       Running 2 minutes ago `8` go-demo_main.5 vfarcic/go-demo:latest swarm-1 Running       Running 2 minutes ago  ```   `````````````````````````````````````````` As you can see, the number of `go-demo_main` replicas is now five. Two of them are running for only a few minutes.    Since I am a paranoid person, I like testing at least a few combinations of any code or script I write. Let’s see whether it works if we choose to scale by a negative number.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=-1\"`  ```   ````````````````````````````````````````` After a few moments, the number of replicas should scale down from five to four. Let’s double-check it.    ``` `1` docker stack ps `\\` `2 `    -f desired-state`=`Running go-demo  ```   ```````````````````````````````````````` The output is as follows (IDs are removed for brevity).    ``` `1` NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \\ `2 `   ERROR PORTS `3` go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 2 hours ago `4` go-demo_db.1   mongo:latest           swarm-1 Running       Running 2 hours ago `5` go-demo_main.2 vfarcic/go-demo:latest swarm-3 Running       Running 2 hours ago `6` go-demo_main.3 vfarcic/go-demo:latest swarm-2 Running       Running 2 hours ago `7` go-demo_main.4 vfarcic/go-demo:latest swarm-2 Running       Running 25 minutes a\\ `8` go  ```   ``````````````````````````````````````` As you can see, the replica number five disappeared, proving that the script works in both directions. We can use it to scale or de-scale services.    As a side note, don’t get alarmed if some other replica disappeared. There is no guarantee that, when we scale down by one replica, it will be the last one that is removed from the system. For example, replica number two could have been removed instead of the replica five. Indexes are not of importance. What matters is that only four replicas are running inside the cluster.  ![Figure 9-5: Manual scaling through Jenkins](img/00047.jpeg)  Figure 9-5: Manual scaling through Jenkins    ### Preventing The Scaling Disaster    On the first look, the script we created works correctly. Doesn’t it?. I’ve seen similar scripts in other places, and there is only one thing I have to say. **Do not run this pipeline in production!!!** It is too dangerous. It can easily crash your entire cluster or make your service disappear. Can you guess why?    Let us imagine the following situation. Prometheus detects that certain threshold is reached (e.g. memory utilization, response time, and so on) and send a notification to Alertmanager. It sends a build request to Jenkins which, in turn, scales the service by increasing the number of replicas by one. So far, so good.    What happens if scaling does not resolve the problem? What if the threshold reached in Prometheus persists? After a while, the process will be repeated, and the service will be scaled up one more time. That might be correct. Maybe there was a significant increase in requests. Maybe that new feature convinced a huge number of new users to start using our service. In such a situation, scaling twice is a legitimate operation. But, what if the second round of scaling did not produce results. What if the system continues scaling up until all the resources are used, and the nodes start failing one by one? The whole cluster could be destroyed.    If you think that scenario is bad, let me tell you that it can get much worse. Let’s assume that there is a system in place that would create new nodes when resources are over certain threshold. In that scenario, scaling up indefinitely would result in infinite addition of new nodes. As a result, the bill from AWS could ruin your company. Fortunately, there is a limit to how many nodes an account can create. Still, the unlimited increase in the number of replicas together with the growth of nodes up to a limit would only produce a massive bill, and the cluster would still fail at the end.    As you can imagine, neither of those scenarios is pretty.    What happens if the system decides to de-scale? Maybe you set up a lower threshold for a memory limit or for response time. When that boundary is reached, the system should scale-down. Following the similar logic from the previous examples, scaling-down could continue until the number of replicas reaches zero. At that moment, the service is as good as if it would be removed from the system. As a result, we’d have downtime. The major difference is that we would not get a huge bill from our hosting vendor and only a part of the system would experience downtime. The rest of the services should work correctly unless they also start experiencing the same fate.    What we need to do is set some limits. We should define what the minimum and the maximum number of replicas of a service is.    However, the trick is not only to know what information should be defined but also where to put that information. Jenkins needs to know what are those limits and I can think of a few ways to provide that information.    We could add two new input parameters to the `service-scale` Pipeline job. They could be `scaleMin` and `scaleMax`. The problem, in that case, is that Alertmanager would need to pass those parameters when sending requests to Jenkins. But, Alertmanager does not have that information. It would need to rely on Prometheus which could get it from the labels scraped from cAdvisor. However, that would assume that all alerts are generated with data that come from cAdvisor. That might not be the case.    So, if neither Alertmanager nor Prometheus are the right places to define (or discover) the scaling limits of a service, the only option left is for Jenkins job to discover it directly from the service. Since Pipeline code has, through its agents, access to Docker Manager, it could, simply, request that information. That should be the optimum solution since it would follow the pattern we used before. We would continue specifying all the information related to a service inside the service itself. To be more precise, we could add a few additional labels and let Jenkins “discover them”.    The [stacks/go-demo-scale.yml](https://github.com/vfarcic/docker-flow-monitor/blob/master/stacks/go-demo-scale.yml) is a slightly modified version of the one we used by now. It defines two new labels.    The relevant parts of the stack are as follows.    ```  `1` version: '3'  `2`   `3` services:  `4`   `5`  main:  `6`    image: vfarcic/go-demo  `7`    ...  `8`    deploy:  `9`      ... `10 `      labels: `11 `        ... `12 `        - com.df.scaleMin=2 `13 `        - com.df.scaleMax=4 `14 `      ...  ```   `````````````````````````````````````` We used the `com.df.scaleMin` and `com.df.scaleMax` labels to define that the minimum number of replicas is two and the maximum four.    Let’s update the stack with the new definition.    ``` `1` docker stack deploy `\\` `2 `    -c stacks/go-demo-scale.yml `\\` `3 `    go-demo  ```   ````````````````````````````````````` Please note that the `go-demo-scale.yml` stack has the number of replicas set to three, so the deployment of the stack will remove any extra replicas we created previously.    Let us update the Pipeline script. The new version is as follows.    ```  `1` `...`  `2` `script` `{`  `3`  `def` `inspectOut` `=` `sh` `script:` `\"docker service inspect $service\"``,`  `4`                      `returnStdout:` `true`  `5`  `def` `inspectJson` `=` `readJSON` `text:` `inspectOut``.``trim``()`  `6`  `def` `currentReplicas` `=` `inspectJson``[``0``].``Spec``.``Mode``.``Replicated``.``Replicas`  `7`  `def` `newReplicas` `=` `currentReplicas` `+` `scale``.``toInteger``()`  `8`  `def` `minReplicas` `=` `inspectJson``[``0``].``Spec``.``Labels``[``\"com.df.scaleMin\"``].``toInteger``()`  `9`  `def` `maxReplicas` `=` `inspectJson``[``0``].``Spec``.``Labels``[``\"com.df.scaleMax\"``].``toInteger``()` `10 `  `if` `(``newReplicas` `>` `maxReplicas``)` `{` `11 `    `error` `\"$service is already scaled to the maximum number of $maxReplicas repl\\` `12` `icas\"` `13 `  `}` `else` `if` `(``newReplicas` `<` `minReplicas``)` `{` `14 `    `error` `\"$service is already descaled to the minimum number of $minReplicas re\\` `15` `plicas\"` `16 `  `}` `else` `{` `17 `    `sh` `\"docker service scale $service=$newReplicas\"` `18 `    `echo` `\"$service was scaled from $currentReplicas to $newReplicas replicas\"` `19 `  `}` `20` `}` `21` `...`  ```   ```````````````````````````````````` Let us go through the new additions to the script.    We are extending the usage of JSON obtained through `docker service inspect` command. In addition to the number of replicas, we are retrieving the values of the labels `com.df.scaleMin` and `com.df.scaleMax`.    Further on, we have a simple conditional. If the new number of replicas is more than the maximum allowed, throw an error. Similarly, if the number of replicas is less than the minimum allowed, throw an error as well. We are scaling the service only if neither of those conditions is met. The script is still relatively simple and straight forward.    Let’s go back to the job configuration screen.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/configure\"`  ```   ``````````````````````````````````` Please replace the current pipeline with the contents of the [service-scale-4.groovy Gist](https://gist.github.com/vfarcic/fd15bcae2278d3a5ca223d67fe2f2e64) or edit it manually and test your ability to type while reading a book. Either way, press the *Apply* button when finished.    Now we can test whether our scaling process can destroy the cluster.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=1\"`  ```   `````````````````````````````````` Let us open the job activity screen and check the result of the last build.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/blue/organizations/jenkins/ser\\` `2` `vice-scale/activity\"`  ```   ````````````````````````````````` As before, please navigate to the details of the last build and expand the last step. The output should be as follows.    ``` `1` go-demo_main was scaled from 3 to 4 replicas  ```  ```````````````````````````````` ![Figure 9-6: Jenkins job scaled the service](img/00048.jpeg)  Figure 9-6: Jenkins job scaled the service    We’ll confirm the same result by listing the running processes of the `go-demo` stack.    ``` `1` docker stack ps `\\` `2 `    -f desired-state`=`Running go-demo  ```   ``````````````````````````````` The output is as follows (IDs are removed for brevity).    ``` `1` NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \\ `2 `      ERROR PORTS `3` go-demo_db.1   mongo:latest           swarm-1 Running       Running about an hou\\ `4` r ago `5` go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 6 hours ago `6` go-demo_main.2 vfarcic/go-demo:latest swarm-3 Running       Running 6 hours ago `7` go-demo_main.3 vfarcic/go-demo:latest swarm-2 Running       Running 6 hours ago `8` go-demo_main.4 vfarcic/go-demo:latest swarm-2 Running       Running 16 seconds a\\ `9` go  ```   `````````````````````````````` As expected, the service scaled from three to four replicas.    And now comes the moment of truth. Will our service continue scaling indefinitely or the limits will be respected? I know you know the answer, but I like being melodramatic every once in a while.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=1\"`  ```   ````````````````````````````` If everything worked as planned, the last build threw an error. Feel free to check it yourself. If there is a purpose in UIs, that’s to announce in red color that something failed.    More importantly than the error message in Jenkins, we should confirm that the number of replicas is still four.    ``` `1` docker stack ps `\\` `2 `    -f desired-state`=`Running go-demo  ```   ```````````````````````````` The output is as follows (IDs are removed for brevity).    ``` `1` NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \\ `2 `      ERROR PORTS `3` go-demo_db.1   mongo:latest           swarm-1 Running       Running about an hou\\ `4` r ago `5` go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 6 hours ago `6` go-demo_main.2 vfarcic/go-demo:latest swarm-3 Running       Running 6 hours ago `7` go-demo_main.3 vfarcic/go-demo:latest swarm-2 Running       Running 6 hours ago `8` go-demo_main.4 vfarcic/go-demo:latest swarm-2 Running       Running 17 minutes a\\ `9` go  ```   ``````````````````````````` I’ll skip the instructions for scaling down and observing that the lower limit is maintained. Feel free to play with it yourself, or just take my word for granted and trust me blindly. Either way, there is one more important thing missing.    ### Notifying Humans That Scaling Failed    We made significant progress by creating upper and lower limit for scaling. From now on, the script will not exceed them. However, the fact that we will stay within those limit does not mean that the problem that initiated the procedure is gone. Whichever process decided that a service should be scaled probably did that based on some metrics. If, for example, the average response time was slow and the system failed to scale up, the problem will persist unless there is some dark magic involved. We can categorize this situation as “the body tried to self-adapt, it failed, it’s time to consult a doctor.” Since we live in the 21st century, we won’t call him but send him a Slack message.    Before we proceed and modify the script one more time, we need to configure Slack in Jenkins.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/configure\"`  ```   `````````````````````````` Once inside the *Configure System* screen, please scroll down to the *Global Slack Notifier Settings* section. Please enter *devops20* in the *Team Subdomain* field and *2Tg33eiyB0PfzxII2srTeMbd* in the *Integration Token* field.    Now there is another bug or an undocumented feature. I guess it all depends on who you ask. We cannot test the connection before clicking the *Apply* button. There is an explanation for that, but we won’t go through it now. Once you applied the configuration, please click the *Test Connection* button. If everything worked as expected, you should see the *Success* message.    At the same time, the *#df-monitor-tests* channel inside [DevOps20 team](https://devops20.slack.com) should have received a message similar to *Slack/Jenkins plugin: you’re all set on http://192.168.99.100/jenkins/*.    Feel free to change the subdomain and the token to match your own Slack channel. You’ll find the token in *Slack* > *App & Integrations* > *Manage* > *Jenkins CI* screen.    All that’s left is to *Save* the changes to the config and update the Pipeline script. We’ll add `post` section.    ```  `1` `...`  `2` `post` `{`  `3`  `failure` `{`  `4`    `slackSend``(`  `5`      `color:` `\"danger\"``,`  `6`      `message:` `\"\"\"$service could not be scaled.`  `7` `Please check Jenkins logs for the job ${env.JOB_NAME} #${env.BUILD_NUMBER}`  `8` `${env.RUN_DISPLAY_URL}\"\"\"`  `9`    `)` `10 `  `}` `11` `}`  ```   ````````````````````````` Post sections in Declarative Pipeline are always executed no matter the outcome of the build steps. We can fine tune it by adding conditions. In our case, we specified that it should be executed only on `failure`. Inside it, we used the `slackSend` step from the [Slack Notification Plugin](https://jenkins.io/doc/pipeline/steps/slack/). There are quite a few arguments we could have specified but, in this case, we constrained ourselves to only two. We set the `color` to `danger` and the mandatory `message`. Please consult the plugin for more information if you’d like to fine-tune the behavior to your needs.    Now we can open the job configuration page and apply the changes.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/configure\"`  ```   ```````````````````````` Please modify the script yourself or replace it with the [service-scale-5.groovy Gist](https://gist.github.com/vfarcic/aeb332b2ab889a81377833f904148d10). When finished, please press the *Apply* button.    We can quickly confirm whether notifications to Slack work by sending a request that would scale way below the limit.    ``` `1` curl -X POST `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `2` `dWithParameters?token=DevOps22&service=go-demo_main&scale=-123\"`  ```   ``````````````````````` Please open the *#df-monitor-tests* channel in [https://devops20.slack.com/](https://devops20.slack.com/) and confirm that the message was sent.  ![Figure 9-7: Jenkins notification in Slack](img/00049.jpeg)  Figure 9-7: Jenkins notification in Slack    Now that we have a Jenkins job that is in charge of scaling our services, we should make sure that the system can execute it when certain thresholds are met.    ### Integrating Alertmanager With Jenkins    At the moment, we are running Alertmanager configured in the previous chapter. It creates a Slack notification on all alerts. Let’s try to change it so that alerts trigger a remote invocation of the Jenkins job `service-scale`.    Since Alertmanager configuration is stored as a Docker secret and they are immutable (we cannot update them), we need to remove the service and the secret and create them again.    ``` `1` docker service rm monitor_alert-manager `2`  `3` docker secret rm alert_manager_config  ```   `````````````````````` Let us define a Slack config that will send build requests to the *service-scale* job. The command that creates the service with the configuration is as follows.    ```  `1` `echo` `\"route:`  `2``  group_by: [service]`  `3``  repeat_interval: 1h`  `4``  receiver: 'jenkins-go-demo_main'`  `5`   `6` `receivers:`  `7``  - name: 'jenkins-go-demo_main'`  `8``    webhook_configs:`  `9``      - send_resolved: false` `10 ``        url: 'http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `11` `dWithParameters?token=DevOps22&service=go-demo_main&scale=1'` `12` `\"` `|` docker secret create alert_manager_config -  ```   ````````````````````` Unlike the previous configuration, this time we’re using [webhook_config](https://prometheus.io/docs/alerting/configuration/#<webhook_config>). The URL is the same as the one we used before. If the alert is executed, it will send a `buildWithParameters` request that will build `service-scale` job with `go-demo_main` as the `service`.    You’ll notice that the parameters of the request are hard-coded. This time we are not using templating to customize the config. The problem is that `url` cannot use templated fields. For good or bad, that is part of the design. Instead, it sends all the fields of the alert as payload and expects the endpoint to translate it for its own needs. That would be great except for the fact that Jenkins does not accept job input fields in any other but its own format. All in all, both Alertmanager and Prometheus expect the other to adapt. So, we’re in a bit of a trouble and have to specify an entry for each service. That is far from optimum.    Later on, we might discuss alternatives to this approach. We might come to the conclusion that Alertmanager should be extended with `jenkins_config`. Maybe we’ll extend Alertmanager with our own custom code that reconfigures it using labels. It could be `Docker Flow Alertmanager`. We might choose a different tool altogether. We are engineers, and we should not accept limitations of other tools but extend them to suit our needs or build our own. Everything in this book is based on open source, and we should contribute back to the community.    However, we will not do any of those. For now, we’ll just accept the limitation and move on. The important thing to note is that you’d need a receiver for every service that should be scaled. It’s not the best solution, but it should do until a better solution emerges.    If you’re interested in a discussion about the decision not to allow templates in `url` fields, please explore the [Alertmanager issue 684](https://github.com/prometheus/alertmanager/issues/684).    Since we removed the `monitor_alert-manager` service, we should redeploy the `monitor` stack. This time, we’ll use a slightly modified version of the stack. The only difference is that we’ll (temporarily) publish Alertmanager’s port 9093\\. That will allow us to test the configuration by sending HTTP requests to it.    ``` `1` `DOMAIN``=``$(`docker-machine ip swarm-1`)` `\\` `2 `    docker stack deploy `\\` `3 `    -c stacks/docker-flow-monitor-slack-9093.yml `\\` `4 `    monitor  ```   ```````````````````` Please wait a few moments until `monitor_alert-manager` service is up and running. You can check the status by listing processes of the `monitor` stack (e.g. `docker stack ps monitor`).    Before we test the integration with Alertmanager, we should reset the number of replicas of `go-demo_main` service back to three.    ``` `1` docker service scale go-demo_main`=``3`  ```   ``````````````````` Now that Alertmanager with the new configuration is running, we’ll send it a request that will help us validate that everything works as expected.    ``` `1` curl -H `\"Content-Type: application/json\"` `\\` `2 `    -d `'[{\"labels\":{\"service\":\"it-does-not-matter\"}}]'` `\\` `3 `    `$(`docker-machine ip swarm-1`)`:9093/api/v1/alerts  ```   `````````````````` Please note that this time we did not specify `go-demo_main` as the service. Since all alerts are forwarded to the same Jenkins job and with the same parameters, it does not matter what we put in the request. We’ll fix that soon. For now, we should open Jenkins and see the activity of the `service-scale` job.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/blue/organizations/jenkins/ser\\` `2` `vice-scale/activity\"`  ```   ````````````````` Alertmanager sent a request to Jenkins which, in turn, run a new build of the `service-scale` job. As a result, `go-demo_main` service should be scaled from three to four replicas. Let us confirm that.    ``` `1` docker service ps `\\` `2 `    -f desired-state`=`Running go-demo_main  ```   ```````````````` The output is as follows (IDs are removed for brevity).    ``` `1` NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \\ `2 `  ERROR PORTS `3` go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 3 hours ago `4` go-demo_main.2 vfarcic/go-demo:latest swarm-2 Running       Running 3 hours ago `5` go-demo_main.3 vfarcic/go-demo:latest swarm-3 Running       Running 3 hours ago `6` go-demo_main.4 vfarcic/go-demo:latest swarm-1 Running       Running 3 minutes ago  ```   ``````````````` As you can see from the output, the service is scaled to four replicas.  ![Figure 9-8: Alertmanager triggering of a Jenkins job that results in scaling of a service](img/00050.jpeg)  Figure 9-8: Alertmanager triggering of a Jenkins job that results in scaling of a service    Being able to send requests from Alertmanager to Jenkins works fine if all the alerts are the same. However, that is almost never the case. We should start distinguishing alerts. One easy improvement we can do is to create a default receiver. We can, for example, say that by default all alerts are sent to Slack and specify explicitly those that should be forwarded somewhere else.    Let us remove the secret and the service and discuss the new configuration.    ``` `1` docker service rm monitor_alert-manager `2`  `3` docker secret rm alert_manager_config  ```   `````````````` The configuration that envelops both Slack and Jenkins as receivers is as follows.    ```  `1` `echo` `\"route:`  `2``  group_by: [service]`  `3``  repeat_interval: 1h`  `4``  receiver: 'slack'`  `5``  routes:`  `6``  - match:`  `7``      service: 'go-demo_main'`  `8``    receiver: 'jenkins-go-demo_main'`  `9`  `10` `receivers:` `11 ``  - name: 'slack'` `12 ``    slack_configs:` `13 ``      - send_resolved: true` `14 ``        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is \\` `15` `in danger!'` `16 ``        title_link: 'http://``$(`docker-machine ip swarm-1`)``/monitor/alerts'` `17 ``        text: '{{ .CommonAnnotations.summary}}'` `18 ``        api_url: 'https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0KvvyStV\\` `19` `nIt3ZWpIaLnqLCu'` `20 ``  - name: 'jenkins-go-demo_main'` `21 ``    webhook_configs:` `22 ``      - send_resolved: false` `23 ``        url: 'http://``$(`docker-machine ip swarm-1`)``/jenkins/job/service-scale/buil\\` `24` `dWithParameters?token=DevOps22&service=go-demo_main&scale=1'` `25` `\"` `|` docker secret create alert_manager_config -  ```   ````````````` The `route` section defines `slack` as the receiver. Further down, the `routes` section uses `match` to filter alerts. We specified that any alert with the `service` label set to `go-demo_main` should be sent to the `jenkins-go-demo_main` receiver. In other words, every alert will be sent to Slack unless it matches one of the `routes`.    The `receivers` section defines `slack` and `jenkins-go-demo_main` entries. They are the same as those we used in previous configurations.    We should be able to test the whole system now. We should generate a situation that will create an alert in Prometheus, fire it to Alertmanager, and, depending on the alert type, see the result in Slack or Jenkins. But, first things should come first. We should create the new `monitor_alert-manager` service by redeploying the stack.    ``` `1` `DOMAIN``=``$(`docker-machine ip swarm-1`)` `\\` `2 `    docker stack deploy `\\` `3 `    -c stacks/docker-flow-monitor-slack.yml `\\` `4 `    monitor  ```   ```````````` As before, please execute `docker stack ps monitor` to confirm that all the services in the stack are running.    We’ll also revert the number of replicas of the `go-demo_main` service to three. Since we set the maximum to four, an intent to scale up would fail if we do not put it back to three.    ``` `1` docker service scale go-demo_main`=``3`  ```   ``````````` Finally, we’ll simulate the “disaster” scenario by changing the `alertIf` conditions of our services. The first we’ll play with is `node-exporter` service from the `exporter` stack. We’ll set its node memory limit to one percent. That is certain to be lower than the actual usage.    ``` `1` docker service update `\\` `2 `    --label-add com.df.alertIf.1`=`@node_mem_limit:0.01 `\\` `3 `    exporter_node-exporter  ```   `````````` If everything went as planned, the chain of the events is about to unfold. The first stop is Prometheus. Let’s open the alerts screen.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/monitor/alerts\"`  ```   ````````` The `exporter_nodeexporter_mem_load` alert should change its status to pending (orange color) and then to firing (red). If it’s still green, please wait a few moments and refresh the screen.    Prometheus fired the alert to Alertmanager. Since it does not match any of the `routes` (`service` is not `go-demo_main`), it falls into “default” category and will be forwarded to Slack. That is the logical flow of actions. Since we do not (yet) have a mechanism that scales nodes, the only reasonable action is to notify humans through Slack and let them solve this problem.    Feel free to visit the *#df-monitor-tests* channel inside [devops20.slack.com](https://devops20.slack.com/). The message generated with the alert from your system should be waiting for you.    Before we proceed, we’ll revert the `exporter_node-exporter` service to its original state.    ``` `1` docker service update `\\` `2 `    --label-add com.df.alertIf.1`=`@node_mem_limit:0.8 `\\` `3 `    exporter_node-exporter  ```   ```````` Soon, another message will appear in Slack stating that the problem with the `exporter_node-exporter` is resolved.    Let’s see what happens when an alert is generated and matches one of the routes. We’ll simulate another “disaster”.    ``` `1` docker service update `\\` `2 `    --label-add com.df.alertIf`=`@service_mem_limit:0.01 `\\` `3 `    go-demo_main  ```   ``````` You should know the drill by now. Wait until Prometheus fires the alert, wait a bit more, and, this time, confirm it by opening the `service-scale` activity screen in Jenkins.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/jenkins/blue/organizations/jenkins/ser\\` `2` `vice-scale/activity\"`  ```   `````` Alertmanager filtered the alert, deduced that it matches a specific `route` and sent it to the matching receiver. This time, that receiver was `webhook_config` that sends requests to build `service-scale` Jenkins job using `go-demo_main` as the input parameter.    All in all, our service was scaled one more time, and we’ll confirm that by listing all the running processes of the service.    ``` `1` docker service ps `\\` `2 `    -f desired-state`=`Running go-demo_main  ```   ````` The output is as follows (IDs are removed for brevity).    ``` `1` NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \\ `2 `   ERROR PORTS `3` go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 3 seconds ago `4` go-demo_main.2 vfarcic/go-demo:latest swarm-2 Running       Running 3 hours ago `5` go-demo_main.3 vfarcic/go-demo:latest swarm-3 Running       Running 3 hours ago `6` go-demo_main.4 vfarcic/go-demo:latest swarm-1 Running       Running 16 minutes a\\ `7` go  ```   ```` A new replica (with index `1`) was created three seconds ago. We averted the “disaster” that could be caused by an imaginary increase in traffic that resulted in the increase in memory usage of the service.  ![Figure 9-9: The full self-adaptive system applied to services](img/00051.jpeg)  Figure 9-9: The full self-adaptive system applied to services    Unfortunately, we do not have a mechanism in place to scale down. The good news is that we will have it soon.    ### What Now    We explored how we can add Jenkins to the mix and make it scale any service. We used relative scaling and made sure that there are some limits so that the service will always be within some boundaries.    Jenkins, by itself, proved to be very flexible and allowed us to set up a reasonably bullet-proof scaling mechanism with only a few lines of Declarative Pipeline code. Unfortunately, we hit some limits when integrating Alertmanager with Jenkins. As a result, Alertmanager config is not as generic as we’d like it to be. We might revisit that subject later and apply some alternative strategy. We might want to extend it. The solution might be called `Docker Flow Alertmanager`. Or, we might choose to replace Jenkins with our own solution. Since I’m fond of names that start with *Docker Flow*, we might add *Scaler* to the mix. We might opt for something completely unexpected, or we might say that the current solution is good enough. Time will tell. For now, the important thing to note is that we made a very important step towards having a *Self-Adapting* system that works on Swarm’s out-of-the-box *Self-Healing* capabilities.    There are still a few critical problems we need to work on. Our *Self-Adapting* system applied to services does not scale down. The reason is simple. We need more data. Using memory as a metric is very important but not very reliable. Having memory below some threshold hardly gives us enough reason to scale up, and it definitely does not provide a valid metric that would let us decide to scale down. We need something else, and I’ll leave you guessing what that is.    Another major missing piece of the puzzle is hardware. We are yet to build a system that *Self-Heals* and *Self-Adapts* servers. For now, we were concentrated only on services.    That was the longest chapter by now. You must be wasted. If you’re not, I am, and this is where we’ll make a break. As always, hardware needs to rest as much as we do so we’ll destroy the machines we created in this chapter and start the next one fresh.    ``` `1` docker-machine rm -f swarm-1 swarm-2 swarm-3  ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` `````````````````` ``````````````````` ```````````````````` ````````````````````` `````````````````````` ``````````````````````` ```````````````````````` ````````````````````````` `````````````````````````` ``````````````````````````` ```````````````````````````` ````````````````````````````` `````````````````````````````` ``````````````````````````````` ```````````````````````````````` ````````````````````````````````` `````````````````````````````````` ``````````````````````````````````` ```````````````````````````````````` ````````````````````````````````````` `````````````````````````````````````` ``````````````````````````````````````` ```````````````````````````````````````` ````````````````````````````````````````` `````````````````````````````````````````` ``````````````````````````````````````````` ```````````````````````````````````````````` ````````````````````````````````````````````` `````````````````````````````````````````````` ``````````````````````````````````````````````` ```````````````````````````````````````````````` ````````````````````````````````````````````````` `````````````````````````````````````````````````` ``````````````````````````````````````````````````` ```````````````````````````````````````````````````` ````````````````````````````````````````````````````` `````````````````````````````````````````````````````` ``````````````````````````````````````````````````````` ```````````````````````````````````````````````````````` ````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````` `````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````` ```````````````````````````````````````````````````````````````` ````````````````````````````````````````````````````````````````` ``````````````````````````````````````````````````````````````````"]