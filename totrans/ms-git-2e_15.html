<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-273"><a id="_idTextAnchor302" class="pcalibre1 pcalibre calibre6"/>12</h1>
<h1 id="_idParaDest-274" class="calibre5"><a id="_idTextAnchor303" class="pcalibre1 pcalibre calibre6"/>Managing Large Repositories</h1>
<p class="calibre3">Because of its distributed nature, Git includes the full change history in each copy of the repository. Every clone gets not only all the files but every revision of every file ever committed. This allows for efficient development (local operations not involving a network are usually fast enough so that they are not a bottleneck) and efficient collaboration with others (their distributed nature allows for many collaborative workflows).</p>
<p class="calibre3">But what happens when the repository you want to work on is huge? Can we avoid taking a large amount of disk space for version control storage? Is it possible to reduce the amount of data that end users need to retrieve while cloning the repository? Do we need to have all files present to be able to work on a project?</p>
<p class="calibre3">If you think about it, there are broadly three main reasons for repositories to become massive: they can accumulate a very long history (every revision direction), they can include huge binary assets that need to be managed together with code, the project can include a large number of files (every file direction), or any combination of those. For those scenarios, the techniques and workarounds are different and can be applied independently, though modern Git also includes a one-stop solution.</p>
<p class="calibre3">Submodules (presented in the previous chapter, <em class="italic">Managing Subprojects</em>) are sometimes used to manage large-size assets. This chapter will describe how this can be done while also presenting alternate solutions to the problem of handling large binary files and other large assets in Git.</p>
<p class="calibre3">In this chapter, we will cover the following topics:</p>
<ul class="calibre16">
<li class="calibre15">Git and large files</li>
<li class="calibre15">Handling repositories with a very long history with a shallow clone</li>
<li class="calibre15">Storing large binary files in a submodule or outside the repository</li>
<li class="calibre15">Reducing the size of the working directory with sparse checkout</li>
<li class="calibre15">How to make a local repository smaller with a sparse clone</li>
<li class="calibre15">Which operations will require network access in different variants of sparse clone</li>
<li class="calibre15">Faster operations with filesystem monitor</li>
</ul>
<h1 id="_idParaDest-275" class="calibre5"><a id="_idTextAnchor304" class="pcalibre1 pcalibre calibre6"/>Scalar – Git at scale for everyone</h1>
<p class="calibre3">The simplest way to configure Git so that it works better with large repositories, apart from enabling the relevant Git<a id="_idIndexMarker1068" class="pcalibre1 pcalibre calibre6"/> features, is to use the built-in <code>scalar</code> tool. This executable has been present in Git since version 2.38, which was released in 2022. Earlier, it was a separate project, then part of Microsoft’s fork of Git.</p>
<p class="calibre3">Using it is very simple: instead of using <code>git clone</code>, you use <code>scalar clone</code>. If the repository has already been cloned, you can run <code>scalar register</code> to achieve the same result. One of the things that the command does is schedule background maintenance; you can stop this and remove the repository from the list of repositories that have been registered with <code>scalar</code> by using the <code>scalar unregister</code> command. The <code>scalar delete</code> command unregisters the repository and removes it from the filesystem.</p>
<p class="calibre3">After a <code>scalar</code> upgrade (which might be caused by moving to newer Git), you can run <code>scalar reconfigure --all</code> to upgrade all repositories registered with Scalar.</p>
<p class="calibre3">By registering the repository with Scalar (or the top-level directory of the project, which is called the <strong class="bold">enlistment</strong> in the <a id="_idIndexMarker1069" class="pcalibre1 pcalibre calibre6"/>Scalar documentation), you can turn on <strong class="bold">partial clone</strong> and <strong class="bold">sparse-checkout</strong>, configure Git to use <strong class="bold">filesystem monitor</strong>, and turn on <strong class="bold">background maintenance</strong> tasks such as <strong class="bold">repository prefetching</strong>.</p>
<p class="calibre3">All these features will be described in the following sections, as will some other features for handling large Git repositories that are more specific to users’ needs.</p>
<h1 id="_idParaDest-276" class="calibre5"><a id="_idTextAnchor305" class="pcalibre1 pcalibre calibre6"/>Handling repositories with a very long history</h1>
<p class="calibre3">Even though Git can <a id="_idIndexMarker1070" class="pcalibre1 pcalibre calibre6"/>effectively handle repositories with a long history, very old projects spanning a huge number of revisions can become a pain to clone. In many cases, you aren’t interested in ancient history and don’t want to pay the time to get all the revisions of a project and the disk space to store them. In this section, we will talk about techniques that you can use to clone truncated history, or how to make Git fast despite the long history.</p>
<p class="calibre3">For example, if you want to propose a new feature or a bug fix, you might not want to wait for the full clone to finish, which <a id="_idIndexMarker1071" class="pcalibre1 pcalibre calibre6"/>may take a while.</p>
<p class="callout-heading">Editing project files online</p>
<p class="callout">Some Git repository hosting services, such as GitHub, offer a web-based interface to manage repositories, including in-browser file<a id="_idIndexMarker1072" class="pcalibre1 pcalibre calibre6"/> management and editing. They may even automatically create a fork of the repository so that you can write and propose changes.</p>
<p class="callout">But a web-based interface doesn’t cover everything, and you might be using self-hosted repositories or a service that doesn’t provide this feature.</p>
<p class="calibre3">However, fixing the bug might require running <code>git bisect</code> on your machine, where the regression bug is easily reproducible (see <a href="B21194_04.xhtml#_idTextAnchor083" class="pcalibre1 pcalibre calibre6"><em class="italic">Chapter 4</em></a>,<em class="italic"> Exploring Project History</em>, for how to use bisection). If you’re tight on space and time, you might want to try to do either a shallow clone (described in the following subsection) or a sparse clone (described later in this chapter).</p>
<h2 id="_idParaDest-277" class="calibre7"><a id="_idTextAnchor306" class="pcalibre1 pcalibre calibre6"/>Using shallow clones to get truncated history</h2>
<p class="calibre3">The simple solution to a fast clone <a id="_idIndexMarker1073" class="pcalibre1 pcalibre calibre6"/>and to save disk space is to <a id="_idIndexMarker1074" class="pcalibre1 pcalibre calibre6"/>perform a <strong class="bold">shallow clone</strong> using Git. This operation allows you to get a local copy of the repository with the history truncated to a particular specified <strong class="bold">depth</strong> – that is, the number of latest revisions.</p>
<p class="calibre3">How do you do it? Just use the <code>--</code><code>depth</code> option:</p>
<pre class="console">
$ git clone --depth=1 https://git.company.com/project</pre>
<p class="calibre3">The preceding command only clones the most recent revision of the primary branch. This trick can save quite a bit of time and relieve a great deal of load from the servers. Often, a shallow clone finishes in seconds rather than minutes, which is a significant improvement. This can be useful if you’re only interested in checking out project files, and not in the whole history, such as what’s inside Git hooks or GitHub Actions – that is, the case of builds where you delete the clone immediately after the action.</p>
<p class="calibre3">Since version 1.9, Git supports pull and <a id="_idIndexMarker1075" class="pcalibre1 pcalibre calibre6"/>push operations even with shallow clones, though some care is still required. You can change the depth of a shallow clone by providing the <code>--depth=&lt;n&gt;</code> option to <code>git fetch</code> (however, note that tags for the deepened commits aren’t fetched). To turn a shallow repository into a complete one, use <code>--unshallow</code>.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Since the commit history in a shallow clone is truncated, commands such as <strong class="source-inline1">git merge-base</strong> and <strong class="source-inline1">git log</strong> show different results than they would in a full clone. This will happen if you try to go outside the depth of the clone. Also, because of how the Git server is optimized, incremental fetch in a shallow repository might take longer than using fetch in a full repository. Fetch might also unexpectedly make the repository not so shallow.</p>
<p class="calibre3">Note that <code>git clone --depth=1</code> may still get all the branches and all the tags. This can happen if the remote repository doesn’t have <code>HEAD</code>, so it doesn’t have a primary branch selected; otherwise, only the tip of the said single branch is fetched. Long-lived projects usually had many releases during their long history. To save time, you would need to combine shallow clone with the next solution: branch limiting.</p>
<p class="calibre3">With modern Git, it might make more sense to<a id="_idIndexMarker1076" class="pcalibre1 pcalibre calibre6"/> use the <strong class="bold">partial clone</strong> feature instead.</p>
<h2 id="_idParaDest-278" class="calibre7"><a id="_idTextAnchor307" class="pcalibre1 pcalibre calibre6"/>Cloning only a single branch</h2>
<p class="calibre3">By default, Git clones all the <a id="_idIndexMarker1077" class="pcalibre1 pcalibre calibre6"/>branches and tags (if you want to fetch notes or replacements, you need to specify them explicitly). You can limit the amount of history you clone by specifying that you want to <strong class="bold">clone only a </strong><strong class="bold">single branch</strong>:</p>
<pre class="console">
$ git clone --branch master --single-branch \
  https://git.company.com/project</pre>
<p class="calibre3">Because most of the project history (most of the DAG of revisions) is shared among branches, with very few exceptions, you probably won’t see a huge difference using this technique.</p>
<p class="calibre3">This feature might be quite useful if you don’t want detached orphan branches or the opposite: you only want an orphan branch (for example, with a web page for a project, or a branch used for GitHub Pages). Single-branch cloning works well in regard to saving disk space when they’re used<a id="_idIndexMarker1078" class="pcalibre1 pcalibre calibre6"/> together with a very shallow clone (with so short a history that most branches don’t have time to converge).</p>
<h2 id="_idParaDest-279" class="calibre7"><a id="_idTextAnchor308" class="pcalibre1 pcalibre calibre6"/>Making operations faster in repositories with a long history</h2>
<p class="calibre3">One of the features that makes Git faster <a id="_idIndexMarker1079" class="pcalibre1 pcalibre calibre6"/>on repositories with a very long history is the <strong class="bold">commit-graph</strong> file. Using this feature, which<a id="_idIndexMarker1080" class="pcalibre1 pcalibre calibre6"/> is turned on by default as of Git 2.24, configures Git to periodically write or update a helper file with a serialized (and easy-to-access) graph of revisions. This makes Git operations that query project history much faster.</p>
<p class="calibre3">You can turn this feature off by setting the <code>core.commitGraph</code> configuration variable to <code>false</code>. If you need to refresh the helper file, you can do this with the <code>git commit-graph </code><code>write</code> command.</p>
<p class="callout-heading">Avoiding doing the work</p>
<p class="callout">One unexpected place that might get slower with long history is running <strong class="source-inline1">git status</strong>. This is caused by the command in question computing detailed ahead/behind counts for the current branch (how many commits you have on the local branch ahead of the upstream branch in the remote repository, how many commits in the remote repository you are behind).</p>
<p class="callout">You can turn off computing this information with the <strong class="source-inline1">--no-ahead-behind</strong> option, or by setting the <strong class="source-inline1">status.aheadBehind</strong> configuration variable to <strong class="source-inline1">false</strong>. Nowadays, <strong class="source-inline1">git status</strong> will print this advice when it is slowed by ahead/behind calculations.</p>
<h1 id="_idParaDest-280" class="calibre5"><a id="_idTextAnchor309" class="pcalibre1 pcalibre calibre6"/>Handling repositories with large binary files</h1>
<p class="calibre3">In some specific circumstances, you might need to track <strong class="bold">huge binary assets</strong> in the code base. For example, gaming teams have to handle huge 3D models, and web development teams might need to track<a id="_idIndexMarker1081" class="pcalibre1 pcalibre calibre6"/> raw image assets or Photoshop documents. Both gaming development and web development might require video files to be under version control. Additionally, sometimes, you might want the convenience of including large binary deliverables that are difficult or expensive to generate – for example, storing a snapshot of a virtual machine image.</p>
<p class="calibre3">There are some tweaks you can make to improve how binary assets are handled by Git. For binary files that change significantly from version to version (and not just change some metadata headers), you might want to<a id="_idIndexMarker1082" class="pcalibre1 pcalibre calibre6"/> turn off the <code>-delta</code> explicitly for specific types of files in a <code>.gitattributes</code> file (see <a href="B21194_03_split_000.xhtml#_idTextAnchor049" class="pcalibre1 pcalibre calibre6"><em class="italic">Chapter 3</em></a>, <em class="italic">Managing Your </em><em class="italic">Worktrees</em>, and <a href="B21194_13_split_000.xhtml#_idTextAnchor320" class="pcalibre1 pcalibre calibre6"><em class="italic">Chapter 13</em></a>, <em class="italic">Customizing and Extending Git</em>). Git will automatically turn off delta compression for any file above the <code>core.bigFileThreshold</code> size, which is 512 MiB by default. You may also want to turn the compression off (for example, if a file is in the compressed format already). However, because <code>core.compression</code> and <code>core.looseCompression</code> are global for the whole repository, it makes more sense if binary assets are in a separate repository (submodule).</p>
<h2 id="_idParaDest-281" class="calibre7"><a id="_idTextAnchor310" class="pcalibre1 pcalibre calibre6"/>Splitting the binary asset folder into a separate submodule</h2>
<p class="calibre3">One possible way of handling large binary asset <em class="italic">folders</em> is to split them into a separate repository and pull the assets into your main <a id="_idIndexMarker1083" class="pcalibre1 pcalibre calibre6"/>project as a <strong class="bold">submodule</strong>. The use of submodules gives you a way to<a id="_idIndexMarker1084" class="pcalibre1 pcalibre calibre6"/> control when assets are updated. Moreover, if a developer doesn’t need those binary assets to work, they can simply exclude the submodule with assets from fetching.</p>
<p class="calibre3">The limitation is that you need to have a separate folder with these huge binary assets that you want to handle this way. Additionally, the service hosting the submodule repository with those large assets needs to be able to store those large files; many Git hosting sites impose hard limits on the maximum size of a file, or of the repository.</p>
<h2 id="_idParaDest-282" class="calibre7"><a id="_idTextAnchor311" class="pcalibre1 pcalibre calibre6"/>Storing large binary files outside the repository</h2>
<p class="calibre3">Another solution is to use one of the many third-party tools that try to solve the problem of handling large binary files<a id="_idIndexMarker1085" class="pcalibre1 pcalibre calibre6"/> in Git repositories. Many of them use a similar paradigm, namely storing the contents of huge binary files outside the repository while providing some kind of pointers to the contents in the checkout.</p>
<p class="calibre3">There are three parts to each such implementation:</p>
<ul class="calibre16">
<li class="calibre15">How they store the information about the contents of the managed files inside the repository</li>
<li class="calibre15">How they share large binary files between a team</li>
<li class="calibre15">How they integrate with Git (and their performance penalty)</li>
</ul>
<p class="calibre3">While choosing a solution, you need to take this data into account, along with the operating system support, ease of use, and the size of the community.</p>
<p class="calibre3">What’s stored in the repository and what’s checked in might be a <em class="italic">symlink</em> to the file or the key, or it might be a <em class="italic">pointer file</em> (often plain text), which acts as a reference to the actual file contents (by name or by the cryptographic hash of file contents). The tracked files need to be stored in some kind of backend for collaboration (cloud service, rsync, shared directory, and so on). Backends might be accessed directly by the client, or there might be a separate server with a defined API into which the blobs are written, which would, in turn, offload the storage elsewhere.</p>
<p class="calibre3">The tool might either require the use of separate commands for checking out and committing large files and for fetching from and pushing to the backend, or it might be integrated into Git. The integrated solution uses the <code>clean</code>/<code>smudge</code> filters to handle check-out and check-in transparently, and the <code>pre-push</code> hook to send large file contents transparently together. You only need to state which files to track and, of course, initialize the repository for the tool use.</p>
<p class="calibre3">The advantage of a filter-based approach is its ease of use; however, there is a performance penalty because of how this approach works. Using separate commands to handle large binary assets makes the learning curve a bit steeper but provides better performance. Some tools provide both interfaces.</p>
<p class="calibre3">Among different<a id="_idIndexMarker1086" class="pcalibre1 pcalibre calibre6"/> solutions, there’s <strong class="bold">git-annex</strong>, which has a large community and support for various backends, and <strong class="bold">Git-Large File Storage</strong> (<strong class="bold">Git-LFS</strong>), created by GitHub, which provides good <a id="_idIndexMarker1087" class="pcalibre1 pcalibre calibre6"/>Microsoft Windows support, a client-server approach, and transparency (with support for a filter-based approach). The Git-LFS extension is supported not only by GitHub <a id="_idIndexMarker1088" class="pcalibre1 pcalibre calibre6"/>but also by other Git hosting sites and software forges, such as GitLab, Bitbucket, and Gitea. Specialized services and projects for implementing Git-LFS also exist.</p>
<p class="calibre3">There are many other such tools, but those two are the most popular, and both are still maintained.</p>
<p class="callout-heading">Versioning data files for data analysis and machine learning</p>
<p class="callout">Machine learning projects often process large files or large numbers of files. Those include the raw dataset, but also the results of various pre-processing steps, as well as the trained model.</p>
<p class="callout">You want to store those large files or directories somewhere to avoid having to re-download or re-compute them. On the other hand, you also want to be able to recreate everything from scratch, to make the science reproducible. Those requirements are different enough from the ones that are encountered in typical software projects that need to handle large assets, where specialized solutions for integrating data handling and version control are <a id="_idIndexMarker1089" class="pcalibre1 pcalibre calibre6"/>necessary. Among <a id="_idIndexMarker1090" class="pcalibre1 pcalibre calibre6"/>such solutions, there’s <strong class="bold">Data Version Control</strong> (<strong class="bold">DVC</strong>) and <strong class="bold">Pachyderm</strong>.</p>
<h1 id="_idParaDest-283" class="calibre5"><a id="_idTextAnchor312" class="pcalibre1 pcalibre calibre6"/>Handling repositories with a large number of files</h1>
<p class="calibre3">The rise in the use of monorepos (this concept was explained in detail in <a href="B21194_11.xhtml#_idTextAnchor270" class="pcalibre1 pcalibre calibre6"><em class="italic">Chapter </em><em class="italic">11</em></a><em class="italic">, Managing Subprojects</em>) has led to the need to handle repositories with<a id="_idIndexMarker1091" class="pcalibre1 pcalibre calibre6"/> large amounts of files. In a monorepo – that is, a repository composed of many interconnected subprojects – you would usually work on a single subproject and access and change files only within a specific subdirectory.</p>
<h2 id="_idParaDest-284" class="calibre7"><a id="_idTextAnchor313" class="pcalibre1 pcalibre calibre6"/>Limiting the number of working directory files with sparse checkout</h2>
<p class="calibre3">Git includes<a id="_idIndexMarker1092" class="pcalibre1 pcalibre calibre6"/> the <code>core.sparseCheckout</code> configuration variable to <code>true</code> and uses the <code>.git/info/sparse-checkout</code> file with the gitignore-like syntax to specify what is to appear in the working directory. The index (also known as the staging area) is populated in full, with the <code>skip-worktree</code> flag set for files missing from checkout.</p>
<p class="calibre3">While it can be helpful if you have a huge tree of folders, it doesn’t affect the overall size of the local repository itself. To reduce the<a id="_idIndexMarker1094" class="pcalibre1 pcalibre calibre6"/> size of the repository, it needs to be used together with <strong class="bold">sparse clone</strong> (which will be described later).</p>
<p class="calibre3">However, sparse checkout definitions are extremely generic. This makes the feature very flexible but at the cost of bad performance for large definitions and large amounts of files. With a monorepo, you don’t need that flexibility as each subproject is contained in its own subdirectory – you only directory matches in sparse checkout definitions.</p>
<p class="calibre3">To achieve this, you need to use <code>sparse-checkout</code> feature is deprecated (see, for example, the <code>git sparse-checkout</code> command’s documentation). This mode has the additional advantage that it is much easier to use. Everything is managed with the help of the <code>git </code><code>sparse-checkout</code> command.</p>
<p class="calibre3">To restrict your working directory to a given set of directories, run the following command:</p>
<pre class="console">
$ git sparse-checkout set &lt;directory_1&gt; &lt;directory_2&gt;</pre>
<p class="calibre3">Earlier versions of the feature required you to run <code>git sparse-checkout init --cone</code> first, but using this command is no longer needed, and the <code>init</code> subcommand is itself being deprecated.</p>
<p class="callout-heading">Tip</p>
<p class="callout">If you’re cloning a repository with a large number of files, you can avoid filling out the working directory with them by using the <strong class="source-inline1">--no-checkout</strong> or <strong class="source-inline1">--sparse</strong> option of <strong class="source-inline1">git clone</strong> (the second option will only check out files in the top directory of the project). You can add the <strong class="source-inline1">--filter=blob:none</strong> option for even more speed (turning on blobless sparse clone).</p>
<p class="calibre3">At any point, you can check which directories are included in your <code>sparse-checkout</code> definitions, and are present in <a id="_idIndexMarker1095" class="pcalibre1 pcalibre calibre6"/> your working directory, using the following command:</p>
<pre class="console">
$ git sparse-checkout list</pre>
<p class="calibre3">You can add a new directory to your existing sparse checkout with the <code>add</code> subcommand, as shown in the following example:</p>
<pre class="console">
$ git sparse-checkout add &lt;new_directory&gt;</pre>
<p class="calibre3">At the time of writing, there’s no <code>remove</code> subcommand. To remove a directory from the list of checked-out files, you would need to edit the contents of the <code>.git/info/sparse-checkout</code> file and then run the following subcommand:</p>
<pre class="console">
$ git sparse-checkout reapply</pre>
<p class="calibre3">This subcommand reapplies the existing sparse directory specifications to make the working directory match. It can also be used when some operation updates the working directory without fully respecting <code>sparse-checkout</code> definitions. This might be caused by using tools external to Git, or by running Git commands that do not fully support sparse checkouts.</p>
<p class="calibre3">You can turn off this feature and restore the working directory so that it includes all files by running the <code>git sparse-checkout </code><code>disable</code> command.</p>
<h2 id="_idParaDest-285" class="calibre7"><a id="_idTextAnchor314" class="pcalibre1 pcalibre calibre6"/>Reducing the local repository size with sparse clone</h2>
<p class="calibre3">The initial section of <a href="B21194_11.xhtml#_idTextAnchor270" class="pcalibre1 pcalibre calibre6"><em class="italic">Chapter 11</em></a>, <em class="italic">Managing Subprojects</em>, described how Git stores the history of the project, which includes a <a id="_idIndexMarker1096" class="pcalibre1 pcalibre calibre6"/>description of changes, directory structure, and file contents at each revision. This data is stored using different types of objects: tag objects, commit <a id="_idIndexMarker1097" class="pcalibre1 pcalibre calibre6"/>objects, tree objects, and blob objects. Objects reference other objects: tags point to commits, commits point to a parent commit(s), trees represent the state of the project at a given revision, and trees point to other trees and blobs.</p>
<p class="calibre3">When running the ordinary <code>git clone</code> command, the client asks the server for the latest commits (representing the latest revisions). The server provides those objects, all objects they point to, all objects those objects point to, and so on. In short, the server provides those commit objects and every other reachable object (excluding possibly those objects that the client already has). The result is that you have the whole history of the whole project available locally.</p>
<p class="calibre3">Nowadays, however, many developers have<a id="_idIndexMarker1098" class="pcalibre1 pcalibre calibre6"/> network connections available as they work. Modern Git only allows you to download a subset of objects via <strong class="bold">partial clone</strong>. In this case, Git remembers where it can get the rest of the objects, and later asks the server for more data when it<a id="_idIndexMarker1099" class="pcalibre1 pcalibre calibre6"/> turns out to be necessary.</p>
<p class="calibre3">Git’s partial clone feature can be enabled by specifying the <code>--filter</code> option when running the <code>git clone</code> command. There are several filters available, but the server that hosts the repository you’re cloning can choose to deny your filter and revert to creating a full clone.</p>
<p class="calibre3">Running <code>git fetch</code> in sparse clone preserves sparse clone filters, and it doesn’t download those types of objects that would not be downloaded by the initial clone.</p>
<p class="calibre3">The two most commonly used filters that should be supported by most Git hosting sites are as follows:</p>
<ul class="calibre16">
<li class="calibre15"><strong class="bold">Blobless clone</strong>: <strong class="source-inline1">git clone --</strong><strong class="source-inline1">filter=blob:none &lt;url&gt;</strong></li>
<li class="calibre15"><strong class="bold">Treeless clone</strong>: <strong class="source-inline1">git clone --</strong><strong class="source-inline1">filter=tree:0 &lt;url&gt;</strong></li>
</ul>
<p class="calibre3">When using the <code>--filter=blob:none</code> option, the<a id="_idIndexMarker1100" class="pcalibre1 pcalibre calibre6"/> initial <code>git clone</code> command will download everything but the blob objects (which ordinarily contain different versions of file contents for different files). The checkout part of the <code>clone</code> operation (if not suppressed) will download blobs for current versions of project files. The Git client knows how to batch those download requests to ask the server only for the missing blobs.</p>
<p class="calibre3">With <code>git log</code>, <code>git merge-base</code>, and other commands that do not examine file contents run without the need for additional download of blob objects.</p>
<p class="calibre3">Moreover, to examine if the file has been changed, Git can simply compare object IDs, and it doesn’t need to access the actual contents. Therefore, examining file history with <code>git log -- &lt;path&gt;</code> doesn’t need to download any objects either. This command runs with the same performance as <a id="_idIndexMarker1102" class="pcalibre1 pcalibre calibre6"/>in a full clone. This is the result of the fact that the object ID is based on the cryptographic hash of file contents (Git currently uses SHA-1 for this purpose).</p>
<p class="calibre3">Git commands such as <code>git checkout</code>/<code>git switch</code>, <code>git reset --hard &lt;revision&gt;</code>, and <code>git merge</code> need to download blobs to populate the working directory, the index (the staging area), or both. To compute diffs, Git also needs to have blobs to compare; therefore, commands <a id="_idIndexMarker1103" class="pcalibre1 pcalibre calibre6"/>such as <code>git diff</code> or <code>git blame &lt;path&gt;</code> might trigger blob downloads the first time they are run with specific arguments.</p>
<p class="calibre3">In some repositories, the tree data might be a significant portion of the repository’s size. This might happen if the repository has a large amount of files and directories and deep and wide directory hierarchies. In such cases, using <a id="_idIndexMarker1104" class="pcalibre1 pcalibre calibre6"/>a <code>--filter=tree:0</code> option might offer a better solution.</p>
<p class="calibre3">Note that any objects that are only referenced by those objects that were skipped due to the selected filter will also be missing. This means that the treeless clone is more sparse than the blobless clone (as only trees can point to blobs… well, a tag object can point to a blob object, but you won’t typically encounter this).</p>
<p class="calibre3">The advantage of a treeless clone over a blobless clone is a much faster initial clone and faster subsequent fetches. The disadvantage is that working in a treeless clone is more difficult because downloading a missing tree when needed is more expensive. It is also more difficult for the server to notice that the client already has some tree objects locally, so the request might send more data than necessary. Additionally, more commands require additional data to be downloaded. An example of this is <code>git log -- &lt;file&gt;</code>, which in the blobless clone could be run without the need to download anything extra. In a treeless clone, the command will start downloading trees for almost every commit in the history.</p>
<p class="callout-heading">Treeless clones and submodules</p>
<p class="callout">The repositories that contain <a id="_idIndexMarker1105" class="pcalibre1 pcalibre calibre6"/>submodules (see <a href="B21194_11.xhtml#_idTextAnchor270" class="pcalibre1 pcalibre calibre6"><em class="italic">Chapter 11</em></a>, <em class="italic">Managing Subprojects</em>) may behave poorly with treeless clones. If you get too many tree download requests, you can either <em class="italic">turn off the automatic fetching of submodules</em> by ensuring that the <strong class="source-inline1">fetch.recurseSubmodules</strong> configuration variable is set to <strong class="source-inline1">false</strong> (or by using the <strong class="source-inline1">--no-recurse-submodules</strong> option) or <em class="italic">also filter submodules</em> by setting the <strong class="source-inline1">clone.filterSubmodules</strong> config option (or using the <strong class="source-inline1">--recurse-submodules --filter=tree:0 --also-filter-submodules</strong> combination of command-line options).</p>
<p class="calibre3">Treeless clones are helpful for automatic builds, when you want to quickly clone the project, check out a single revision, compile it and/or run a test, and then throw away the repository (instead of <a id="_idIndexMarker1106" class="pcalibre1 pcalibre calibre6"/>using shallow clone). They are also useful if all you’re interested in is examining the history of the whole project.</p>
<p class="calibre3">The treeless clone is a special case of the <code>--filter=tree:&lt;depth&gt;</code>. In this case, the clone omits all blobs and trees whose depth <a id="_idIndexMarker1107" class="pcalibre1 pcalibre calibre6"/>from the root tree (from the top directory of the project) is greater than or equal to the specified limit. It can be easily seen that with <code>&lt;depth&gt;</code> being equal to 0 (that is, <code>--filter=tree:0</code>), the clone will not include any trees or blobs (except for those required for initial checkout).</p>
<h3 class="calibre9">Omitting large file contents with sparse clone</h3>
<p class="calibre3">The partial clone can also work as a tool to help<a id="_idIndexMarker1108" class="pcalibre1 pcalibre calibre6"/> you work with large files. This requires that the Git server (the Git hosting site) supports the specific <a id="_idIndexMarker1109" class="pcalibre1 pcalibre calibre6"/>type of filter. It also doesn’t remove the requirement that at least one remote repository must include those large files and their history so that you can download them on demand.</p>
<p class="calibre3">You can do this by providing the <code>--filter=blob:limit=&lt;size&gt;</code> option when you’re cloning, where <code>&lt;size&gt;</code> can include the <code>&lt;size&gt;</code> bytes, be it KiB, MiB, or GiB (depending on the suffix). For example, <code>blob:limit=1k</code> is the same as <code>blob:limit=1024</code>.</p>
<h3 class="calibre9">Matching clone sparsity to checkout sparsity</h3>
<p class="calibre3">Modern Git includes basic support for the<a id="_idIndexMarker1110" class="pcalibre1 pcalibre calibre6"/> sparse clone filter, which makes it omit all blobs that would be not required for sparse checkout. For security reasons, however, support for the easier-to-use form of <code>--filter=sparse:path=&lt;path&gt;</code> was dropped from Git. The supported form is <code>--filter=sparse:oid=&lt;blob-ish&gt;</code>. This form is safe against the time of check to time of use problem, as opposed to the path-based form, because <code>&lt;blob-ish&gt;</code> (that is, a reference to a blob object) ultimately resolves to the object ID that defines its contents.</p>
<p class="calibre3">At the time of writing, you would be hard to find a Git server that supports this filter and doesn’t respond with <strong class="bold">warning: filtering not recognized by server, ignoring</strong>. But when it starts getting widely<a id="_idIndexMarker1111" class="pcalibre1 pcalibre calibre6"/> supported, one possible solution would be to create a tag for every sparse checkout pattern of interest, and then<a id="_idIndexMarker1112" class="pcalibre1 pcalibre calibre6"/> use the selected tag for blob-ish:</p>
<pre class="console">
$ git sparse-checkout set &lt;subdir&gt;
$ git hash-object -t blob -w .git/info/sparse-checkout
bd177ff9527327c67f50c644c421d280bb8b55f5
$ git tag -a -m 'sparse-checkout pattern for &lt;subdir&gt;'
  sparse/&lt;subdir&gt; bd177ff9527327c67f50c644c421d280bb8b55f5
$ git push origin tag sparse-checkout/&lt;subdirectory&gt;
To &lt;repository url&gt;
* [new tag]         sparse/&lt;subdir&gt; -&gt; sparse/&lt;subdir&gt;</pre>
<p class="calibre3">Of course, in the third step, you need to use the SHA-1 that’s output from the previous command.</p>
<p class="calibre3">In this case, cloning would use the <code>--filter=sparse:oid=sparse/&lt;subdir&gt;^{blob}</code> option (where you would need to use the name of the tag that was created by the sequence of commands shown previously).</p>
<h2 id="_idParaDest-286" class="calibre7"><a id="_idTextAnchor315" class="pcalibre1 pcalibre calibre6"/>Faster checking for file changes with filesystem monitor</h2>
<p class="calibre3">When you run a Git command that operates<a id="_idIndexMarker1113" class="pcalibre1 pcalibre calibre6"/> on the worktree, such as <code>git status</code> or <code>git diff</code>, Git has to discover <a id="_idIndexMarker1114" class="pcalibre1 pcalibre calibre6"/>what changed relative to the index, or relative to the specified revision. It does that by searching the entire worktree, which for repositories with a large number of files can take a long time. It also has to rediscover the same information from scratch every time you run such a command.</p>
<p class="calibre3">Filesystem monitor is a long-running daemon or a service process that does the following:</p>
<ul class="calibre16">
<li class="calibre15">Registers with the operating system to watch specified directories and receive change notification events for directories and files of interest</li>
<li class="calibre15">Keeps the pathnames of those changed watched files and directories in some (in-memory) data structure that<a id="_idIndexMarker1115" class="pcalibre1 pcalibre calibre6"/> can be queried quickly</li>
<li class="calibre15">Responds to client requests <a id="_idIndexMarker1116" class="pcalibre1 pcalibre calibre6"/>for a list of files and directories that have been modified recently</li>
</ul>
<p class="calibre3">Since version 2.37, Git includes the <code>git fsmonitor--daemon</code>. It is currently<a id="_idIndexMarker1117" class="pcalibre1 pcalibre calibre6"/> available on macOS and Windows. This daemon<a id="_idIndexMarker1118" class="pcalibre1 pcalibre calibre6"/> listens for IPC connections from client processes, such as <code>git status</code>, and sends a list of changed files over a Unix domain socket or a named pipe.</p>
<p class="calibre3">Turning it on is very simple; you just need to configure Git to use it. This can be done with the following command:</p>
<pre class="console">
$ git config core.fsmonitor true</pre>
<p class="calibre3">This monitor works well with <code>core.untrackedCache</code>, so it is recommended to set this configuration option to <code>true</code> as well.</p>
<p class="calibre3">You can query this daemon for the list of watched repositories:</p>
<pre class="console">
$ git fsmonitor--daemon status
fsmonitor-daemon is watching 'C:/work/chromium'</pre>
<p class="calibre3">If either the operating system or the filesystem the repository is on does not allow you to use this monitor, there is an option to use the <code>core.fsmonitor</code> config option to the path to the filesystem monitor hook. The hook must support the <code>fsmonitor-watchman</code> hook protocol, and when run return the list of changed files on standard output.</p>
<p class="calibre3">Git comes with the <code>fsmonitor-watchman.sample</code> file, which is installed inside the <code>.git/hooks/</code> directory. Before turning it on, as described in the previous paragraph, rename it by removing the <code>*.sample</code> suffix. If the file is missing, you can download it from <a href="https://github.com/git/git/tree/master/templates" class="pcalibre1 pcalibre calibre6">https://github.com/git/git/tree/master/templates</a>. This hook requires the <strong class="bold">Watchman</strong> file’s watching <a id="_idIndexMarker1120" class="pcalibre1 pcalibre calibre6"/>service (<a href="https://facebook.github.io/watchman/" class="pcalibre1 pcalibre calibre6">https://facebook.github.io/watchman/</a>) to be installed.</p>
<h1 id="_idParaDest-287" class="calibre5"><a id="_idTextAnchor316" class="pcalibre1 pcalibre calibre6"/>Summary</h1>
<p class="calibre3">This chapter provided solutions to handling large Git repositories, from the use of the Scalar tool to specialized solutions.</p>
<p class="calibre3">First, you learned how to use shallow clone to download and operate on the selected shallow subset of the project history.</p>
<p class="calibre3">Then, you learned how to handle large files by storing them outside the repository or separating them into submodules. The problem of large data in data science projects was briefly mentioned, as were specialized solutions to this problem.</p>
<p class="calibre3">Finally, you learned how to manage large monorepos with sparse checkout, sparse clone, and filesystem monitor.</p>
<p class="calibre3">The next chapter will help you make Git easier to use and better fit it to your specific circumstances. This includes configuring repository maintenance, which is particularly important for making working with large repositories smooth.</p>
<h1 id="_idParaDest-288" class="calibre5"><a id="_idTextAnchor317" class="pcalibre1 pcalibre calibre6"/>Questions</h1>
<p class="calibre3">Answer the following questions to test your knowledge of this chapter:</p>
<ol class="calibre14">
<li class="calibre15">What is the simplest solution to handling large repositories?</li>
<li class="calibre15">How you can make cloning faster for repositories with a long history?</li>
<li class="calibre15">How can you handle large files that are needed only by some developers?</li>
<li class="calibre15">What techniques make working with repositories with large numbers of files faster?</li>
<li class="calibre15">What’s the difference between shallow clone, sparse clone, and sparse checkout?</li>
</ol>
<h1 id="_idParaDest-289" class="calibre5"><a id="_idTextAnchor318" class="pcalibre1 pcalibre calibre6"/>Answers</h1>
<p class="calibre3">Here are the answers to this chapter’s questions:</p>
<ol class="calibre14">
<li class="calibre15">Use the built-in <strong class="source-inline1">scalar</strong> tool, either using it to clone the repository or to register the given repository with the tool.</li>
<li class="calibre15">You can use shallow clone or blobless sparse clone. In the first case, you would get a shortened history, while in the second case, the repository’s size will be smaller but some operations will require network access to download additional data.</li>
<li class="calibre15">You can store large files outside the repository with Git-LFS or git-annex (or a similar solution). You can clone the repository without downloading large file data with the sparse clone feature.</li>
<li class="calibre15">Use the sparse checkout feature if you’re only working inside a specific subdirectory, use sparse clone to reduce repository size, and use filesystem monitor (if possible) to make operations faster.</li>
<li class="calibre15">Shallow clone only downloads selected part of the repository history, and all local operations are limited to this selection, though it is easy to change the depth of the history. Sparse clone reduces repository size by downloading only a selected subset of objects, fetching those objects on demand, as their presence becomes necessary to perform operations. Sparse checkout reduces the number of checked-out files, making the working directory smaller (and operations faster).</li>
</ol>
<h1 id="_idParaDest-290" class="calibre5"><a id="_idTextAnchor319" class="pcalibre1 pcalibre calibre6"/>Further reading</h1>
<p class="calibre3">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
<ul class="calibre16">
<li class="calibre15"><em class="italic">Introducing Scalar: Git at scale for everyone</em>, by Derrick Stolee (2020): <a href="https://devblogs.microsoft.com/devops/introducing-scalar/" class="pcalibre1 pcalibre calibre6">https://devblogs.microsoft.com/devops/introducing-scalar/</a></li>
<li class="calibre15"><em class="italic">The Story of Scalar</em>, by Derrick Stolee and Victoria Dye (2022): <a href="https://github.blog/2022-10-13-the-story-of-scalar/" class="pcalibre1 pcalibre calibre6">https://github.blog/2022-10-13-the-story-of-scalar/</a></li>
<li class="calibre15"><em class="italic">scalar(1) - A tool for managing large Git </em><em class="italic">repositories</em>: <a href="https://git-scm.com/docs/scalar" class="pcalibre1 pcalibre calibre6">https://git-scm.com/docs/scalar</a></li>
<li class="calibre15"><em class="italic">Supercharging the Git Commit Graph</em>, by Derrick Stolee (2018): <a href="https://devblogs.microsoft.com/devops/supercharging-the-git-commit-graph/" class="pcalibre1 pcalibre calibre6">https://devblogs.microsoft.com/devops/supercharging-the-git-commit-graph/</a></li>
<li class="calibre15"><em class="italic">git-commit-graph(1) - Write and verify Git commit-graph </em><em class="italic">files</em>: <a href="https://git-scm.com/docs/git-commit-graph" class="pcalibre1 pcalibre calibre6">https://git-scm.com/docs/git-commit-graph</a></li>
<li class="calibre15"><em class="italic">Git LFS - Git Large File </em><em class="italic">Storage</em>: <a href="https://git-lfs.com/" class="pcalibre1 pcalibre calibre6">https://git-lfs.com/</a></li>
<li class="calibre15"><em class="italic">git-annex</em>: <a href="https://git-annex.branchable.com/" class="pcalibre1 pcalibre calibre6">https://git-annex.branchable.com/</a></li>
<li class="calibre15"><em class="italic">Get up to speed with partial clone and shallow clone</em>, by Derrick Stolee (2020): <a href="https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/" class="pcalibre1 pcalibre calibre6">https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/</a></li>
<li class="calibre15"><em class="italic">Bring your monorepo down to size with sparse-checkout</em>, by Derrick Stolee (2020): <a href="https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/" class="pcalibre1 pcalibre calibre6">https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/</a></li>
<li class="calibre15"><em class="italic">git-sparse-checkout(1) - Reduce your working tree to a subset of tracked </em><em class="italic">files</em>: <a href="https://git-scm.com/docs/git-sparse-checkout" class="pcalibre1 pcalibre calibre6">https://git-scm.com/docs/git-sparse-checkout</a></li>
<li class="calibre15"><em class="italic">git-clone(1) - Clone a repository into a new </em><em class="italic">directory</em>: <a href="https://git-scm.com/docs/git-clone" class="pcalibre1 pcalibre calibre6">https://git-scm.com/docs/git-clone</a></li>
<li class="calibre15"><em class="italic">Improve Git monorepo performance with a file system monitor</em>, Jeff Hostetler (2022): <a href="https://github.blog/2022-06-29-improve-git-monorepo-performance-with-a-file-system-monitor/" class="pcalibre1 pcalibre calibre6">https://github.blog/2022-06-29-improve-git-monorepo-performance-with-a-file-system-monitor/</a></li>
<li class="calibre15"><em class="italic">git-fsmonitor--daemon - A Built-in Filesystem </em><em class="italic">Monitor</em>: <a href="https://git-scm.com/docs/git-fsmonitor--daemon" class="pcalibre1 pcalibre calibre6">https://git-scm.com/docs/git-fsmonitor--daemon</a></li>
<li class="calibre15"><em class="italic">githooks - Hooks used by Git: </em><em class="italic">fsmonitor-watchman</em>: <a href="https://git-scm.com/docs/githooks#_fsmonitor_watchman" class="pcalibre1 pcalibre calibre6">https://git-scm.com/docs/githooks#_fsmonitor_watchman</a></li>
</ul>
</div>
</body></html>