- en: Self-Adaptation Applied to Instrumented Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An [instrumented service](https://prometheus.io/docs/practices/instrumentation/)
    provides more detailed metrics then what we can scrape from [exporters](https://prometheus.io/docs/instrumenting/exporters/).
    The ability to add all the metrics we might need opens the doors that are often
    closed with exporters. That does not mean that they are any less useful but that
    we need to think of the nature of the resource we are observing.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware metrics should be scraped from exporters. After all, we cannot instrument
    CPU. Third-party services are another good example of a use-case where exporters
    are often a better option. If we use a database, we should look for an exporter
    that fetches metrics from it and transforms them into the Prometheus-friendly
    format. The same goes for proxies, gateways, and just about almost any other service
    that we did not develop.
  prefs: []
  type: TYPE_NORMAL
- en: We might choose to [write an exporter](https://prometheus.io/docs/instrumenting/writing_exporters/)
    even for services we are in control of if we already invested a lot of time implementing
    metrics that are not in Prometheus format.
  prefs: []
  type: TYPE_NORMAL
- en: Exporters can get us only half-way through. We can instruct the system to scale
    based on, for example, memory utilization. [cAdvisor](https://github.com/google/cadvisor)
    provides information about the containers running inside the cluster, but the
    metrics it provides are too generic. We cannot get service-specific data. Inability
    to fine-tune metrics on per-service basis leaves us with insufficient information
    that can be used only for basic alerts. Instrumentation provides the missing piece
    of the puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: In cases we are willing to invest time to instrument our services, the results
    are impressive. We can get everything we need without compromises. We can accomplish
    almost any level of details and instrument services in a way that we can write
    reliable alerts that will notify the system with all the information it needs.
    The result is a step closer towards self-healing and, more importantly, self-adaptation.
    The reason I’m putting self-adaptation into the “more important” group lies in
    the fact that self-healing is already mostly solved by other tools. Schedulers
    (e.g. Docker Swarm) already do a decent job at self-healing services. If we exclude
    hardware from the scope, we are left with self-adaptation of services as the major
    obstacle left to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up The Objectives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to define the scope of what we want to accomplish through instrumentation.
    We’ll keep it small by limiting ourselves to a single goal. We’ll scale services
    if their response times are over an upper limit and de-scale them if they’re below
    a lower limit. Any other alert will lead to a notification to Slack. That does
    not mean that Slack notifications should exist forever. Instead, they should be
    treated as a temporary solution until we find a way to translate manual corrective
    actions into automated responses performed by the system.
  prefs: []
  type: TYPE_NORMAL
- en: A good example of alerts that are often treated manually are responses with
    errors (status codes 500 and above). We’ll send alerts whenever they reach a threshold
    over a specified period. They will result in Slack notifications that will become
    pending tasks for humans. An internal rule should be to fix the problem first,
    evaluate why it happened, and write a script that will repeat the same set of
    steps. With such a script, we’ll be able to instruct the system to do the same
    if the same alert is fired again. With such an approach, we (humans) can spend
    our time solving unexpected problems and leave machines to remedy those that are
    reoccurring.
  prefs: []
  type: TYPE_NORMAL
- en: The summary of the objectives we’ll try to accomplish is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Define maximum response time of a service and create the flow that will scale
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define minimum response time of a service and create the flow that will de-scale
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define thresholds based on responses with status codes 500 and above and send
    Slack notifications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that response time thresholds cannot rely only on milliseconds.
    We must define quantiles, rates, and a few other things. Also, we need to set
    the minimum and the maximum number of replicas of a service. Otherwise, we risk
    scaling to infinity or de-scaling to zero replicas. Once we start implementing
    the system, we’ll see whether those additional requirements are enough or we’ll
    need to extend the scope further.
  prefs: []
  type: TYPE_NORMAL
- en: That was more than enough talk. Let’s move to the practical exploration of the
    subject.
  prefs: []
  type: TYPE_NORMAL
- en: As always, the first step is to create a cluster and deploy a few services.
  prefs: []
  type: TYPE_NORMAL
- en: Creating The Cluster And Deploying Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You know the drill. We’ll create a Swarm cluster and deploy a few stacks we
    are already familiar with. Once we’re done, we’ll have the base required for the
    exploration of the tasks at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
