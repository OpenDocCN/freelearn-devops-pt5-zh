<html><head></head><body>
		<div><h1 id="_idParaDest-35"><em class="italic"><a id="_idTextAnchor034"/>Chapter 2</em>: Introducing Docker</h1>
			<p>In this chapter, we will discuss how the modern <strong class="bold">continuous delivery</strong> (<strong class="bold">CD</strong>) process looks by introducing Docker, the technology that changed the <strong class="bold">information technology</strong> (<strong class="bold">IT</strong>) industry and the way servers are used.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>What is Docker?</li>
				<li>Installing Docker</li>
				<li>Running Docker hello-world</li>
				<li>Docker components</li>
				<li>Docker applications</li>
				<li>Building Docker images</li>
				<li>Docker container states</li>
				<li>Docker networking</li>
				<li>Using Docker volumes</li>
				<li>Using names in Docker</li>
				<li>Docker cleanup</li>
				<li>Docker commands overview</li>
			</ul>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Technical requirements</h1>
			<p>To complete this chapter, you'll need to meet the following hardware/software requirements:</p>
			<ul>
				<li>At least 4 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) of <strong class="bold">random-access memory</strong> (<strong class="bold">RAM</strong>)</li>
				<li>macOS 10.15+, Windows 10/11 Pro 64-bit, Ubuntu 20.04+, or other Linux operating systems</li>
			</ul>
			<p>All the examples and solutions to the exercises can be found at <a href="https://github.com/PacktPublishing/Continuous-Delivery-With-Docker-and-Jenkins-3rd-Edition/tree/main/Chapter02">https://github.com/PacktPublishing/Continuous-Delivery-With-Docker-and-Jenkins-3rd-Edition/tree/main/Chapter02</a>.</p>
			<p>Code in Action videos for this chapter can be viewed at <a href="https://bit.ly/3LJv1n6">https://bit.ly/3LJv1n6</a>.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>What is Docker?</h1>
			<p>Docker is an<a id="_idIndexMarker114"/> open source project designed to help with application deployment using software containers. This approach means running applications together with the complete environment (files, code libraries, tools, and so on). Therefore, Docker—similar to virtualization—allows an application to be packaged into an image that can be run everywhere.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Containerization versus virtualization</h2>
			<p>Without Docker, isolation and other benefits can be achieved with the use of hardware virtualization, often<a id="_idIndexMarker115"/> called <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>). The <a id="_idIndexMarker116"/>most popular solutions are VirtualBox, VMware, and parallels. A VM emulates a computer architecture and <a id="_idIndexMarker117"/>provides the functionality of a physical computer. We can achieve complete isolation of applications if each of them is delivered and run as a separate VM image.</p>
			<p>The following diagram presents the <a id="_idIndexMarker118"/>concept of virtualization:</p>
			<p class="figure-caption"><img src="img/B18223_02_01.png" alt="Figure 2.1 – Virtualization&#13;&#10;"/></p>
			<p class="figure-caption">Figure 2.1 – Virtualization</p>
			<p>Each<a id="_idIndexMarker119"/> application is launched as a separate image with all dependencies and a guest operating system. Images are<a id="_idIndexMarker120"/> run by the <strong class="bold">hypervisor</strong>, which emulates the physical computer architecture. This <a id="_idIndexMarker121"/>method of deployment is widely supported by many tools (such as Vagrant) and dedicated to development and testing environments. Virtualization, however, has<a id="_idIndexMarker122"/> three significant drawbacks, as outlined here:</p>
			<ul>
				<li><strong class="bold">Low performance</strong>: The VM emulates the whole computer architecture to run the guest operating system, so there is a significant overhead associated with executing each operation.</li>
				<li><strong class="bold">High resource consumption</strong>: Emulation requires a lot of resources and has to be done separately for each application. This is why, on a standard desktop machine, only a few applications can be run simultaneously.</li>
				<li><strong class="bold">Large image size</strong>: Each application is delivered with a full operating system, so deployment on a server implies sending and storing a large amount of data.</li>
			</ul>
			<p>The concept of containerization <a id="_idIndexMarker123"/>presents a different solution, as we can see here:</p>
			<div><div><img src="img/B18223_02_02.jpg" alt="Figure 2.2 – Containerization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Containerization</p>
			<p>Each <a id="_idIndexMarker124"/>application<a id="_idIndexMarker125"/> is delivered together with its dependencies, but without the operating system. Applications interface directly with the host operating system, so there is no additional layer of the guest operating system. This results in better performance and no wasted resources. Moreover, shipped Docker images are significantly smaller.</p>
			<p>Notice that, in <a id="_idIndexMarker126"/>the case of containerization, isolation happens at the level of the host operating system's processes. This doesn't mean, however, that the containers share their dependencies. Each of them has its own libraries in the right version, and if any of them is updated, it has no impact on the others. To achieve this, Docker Engine creates a set of Linux namespaces and control groups for the container. This is why Docker security is based on Linux kernel process isolation. This solution, although mature enough, could be considered slightly less secure than the complete operating system-based isolation offered by VMs.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>The need for Docker</h2>
			<p>Docker containerization<a id="_idIndexMarker127"/> solves a number of problems seen in traditional software delivery. Let's take a closer look.</p>
			<h3>Environment</h3>
			<p>Installing and<a id="_idIndexMarker128"/> running software is complex. You need to make decisions about the operating system, resources, libraries, services, permissions, other software, and everything your application depends on. Then, you need to know how to install it. What's more, there may be some conflicting dependencies. <em class="italic">What do you do then?</em> <em class="italic">What if your software needs an upgrade of a library, but the other resources do not?</em> In some companies, such issues are solved by having <em class="italic">classes of applications</em>, and each<a id="_idIndexMarker129"/> class is served by a dedicated server, such as a server for web services with Java 7, and another one for batch jobs with Java 8. This solution, however, is not balanced in terms of resources and requires an army of IT operations teams to take care of all the production and test servers.</p>
			<p>Another problem with the environment's complexity is that it often requires a specialist to run an application. A less technical person may have a hard time setting up MySQL, <strong class="bold">Open Database Connectivity</strong> (<strong class="bold">ODBC</strong>), or <a id="_idIndexMarker130"/>any other slightly more sophisticated tool. This is particularly true for applications not delivered as an operating system-specific binary but that require source code compilation or any other environment-specific configuration.</p>
			<h3>Isolation</h3>
			<p>Keep the <a id="_idIndexMarker131"/>workspace tidy. One application can change the behavior of another one. Imagine what could happen. Applications share one filesystem, so if application <em class="italic">A</em> writes something to the wrong directory, application <em class="italic">B</em> reads the incorrect data. They share resources, so if there is a memory leak in application <em class="italic">A</em>, it can freeze not only itself but also application <em class="italic">B</em>. They share network interfaces, so if applications <em class="italic">A</em> and <em class="italic">B</em> both use port <code>8080</code>, one of them will crash. Isolation concerns the security aspects, too. Running a buggy application or malicious software can cause damage to other applications. This is why it is a much safer approach to keep each application inside a separate sandbox, which limits the scope of possible damage to the application itself.</p>
			<h3>Organizing applications</h3>
			<p>Servers<a id="_idIndexMarker132"/> often end up looking messy, with a ton of running applications nobody knows anything about. <em class="italic">How will you check which applications are running on the server and which dependencies each of them is using?</em> They could depend on libraries, other applications, or tools. Without the exhaustive documentation, all we can do is look at the running processes and start guessing. Docker keeps things organized by having each application as a separate container that can be listed, searched, and monitored.</p>
			<h3>Portability</h3>
			<p><em class="italic">Write once, run anywhere</em>, said the slogan while advertising the earliest versions of Java. Indeed, Java<a id="_idIndexMarker133"/> addresses the portability issue quite well. However, I can still think of a few cases where it fails; for example, the incompatible native dependencies or the older version of the Java Runtime. Moreover, not all software is written in Java.</p>
			<p>Docker moves the concept of portability one level higher; if the Docker version is compatible, the shipped software works correctly, regardless of the programming language, operating system, or environment configuration. Docker, then, can be expressed by the following slogan: <em class="italic">Ship the entire environment instead of just code</em>.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Kittens and cattle</h2>
			<p>The <a id="_idIndexMarker134"/>difference between traditional software deployment and Docker-based deployment is often expressed with an analogy of kittens and cattle. Everybody likes kittens. Kittens are unique. Each has its own name and needs special treatment. Kittens are treated with emotion. We cry when they die. On the contrary, cattle exist only to satisfy our needs. Even the form <em class="italic">cattle</em> is singular since it's just a pack of animals treated together—no naming, no uniqueness. Surely, they are unique (the same as each server is unique), but this is irrelevant. This is why the most straightforward explanation of the idea behind Docker is <em class="italic">treat your servers like cattle, not pets</em>.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Alternative containerization technologies</h2>
			<p>Docker <a id="_idIndexMarker135"/>is not the only containerization system available on the market. Actually, the first versions of Docker were based on<a id="_idIndexMarker136"/> the open source <strong class="bold">Linux Containers</strong> (<strong class="bold">LXC</strong>) system, which is an alternative platform for containers. Other known solutions<a id="_idIndexMarker137"/> are <strong class="bold">Windows Server containers</strong>, <strong class="bold">OpenVZ</strong>, and <strong class="bold">Linux Server</strong>. Docker, however, overtook all other systems <a id="_idIndexMarker138"/>because<a id="_idIndexMarker139"/> of its simplicity, good marketing, and startup approach. It works under most operating systems, allows you to do something useful in less than 15 minutes, and has a lot of simple-to-use features, good tutorials, a great community, and probably the best logo in the IT industry!</p>
			<p>We already understand the idea of Docker, so let's move on to the practical part and start from the beginning: Docker installation.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Installing Docker</h1>
			<p>Docker's installation<a id="_idIndexMarker140"/> process is quick and simple. Currently, it's supported on most Linux operating systems, and a wide range of them have dedicated binaries provided. macOS and Windows are also well supported with native applications. However, it's important to understand that Docker is internally based on the Linux kernel and its specifics, and this is why, in the case of macOS and Windows, it uses VMs (HyperKit for macOS and Hyper-V for Windows) to run the Docker Engine environment.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Prerequisites for Docker</h2>
			<p>The Docker Community Edition requirements are <a id="_idIndexMarker141"/>specific for each operating system, as outlined here:</p>
			<ul>
				<li><strong class="bold">macOS</strong>:<ul><li>macOS 10.15 or newer</li><li>At least 4 GB of RAM</li><li>No VirtualBox prior to version 4.3.30 installed</li></ul></li>
				<li><strong class="bold">Windows</strong>:<ul><li>64-bit Windows 10/11</li><li>The Hyper-V package enabled</li><li>At least 4 GB of RAM</li></ul></li>
				<li><strong class="bold">Linux</strong>:<ul><li>64-bit architecture</li><li>Linux kernel 3.10 or later</li></ul></li>
			</ul>
			<p>If your machine does not meet these requirements, the solution is to use <strong class="bold">VirtualBox</strong> with the Ubuntu operating system installed. This workaround, even though it sounds complicated, is not necessarily the worst method, especially considering that the <a id="_idIndexMarker142"/>Docker Engine environment is virtualized anyway in the case of macOS and Windows. Furthermore, Ubuntu is one of the best-supported systems for using Docker.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">All examples in this book have been tested on the Ubuntu 20.04 operating system.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Installing on a local machine</h2>
			<p>The <a id="_idIndexMarker143"/>Docker installation process is straightforward and<a id="_idIndexMarker144"/> is described in detail on its official page: <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</p>
			<h3>Docker Desktop</h3>
			<p>The<a id="_idIndexMarker145"/> simplest way to use Docker in your local environment is to install Docker Desktop. This way, in just a few minutes, you have a complete Docker development environment all set up and running. For Windows and macOS users, Docker Desktop provides a native application that hides all the setup difficulties behind the scenes. Technically, Docker Engine is installed inside a VM because Docker requires the Linux kernel to operate. Nevertheless, as a user, you don't even need to think about this—you install Docker Desktop and you are ready to start using the <code>docker</code> command. You can see an overview of Docker Desktop in the following screenshot:</p>
			<div><div><img src="img/B18223_02_03.jpg" alt="Figure 2.3 – Docker Desktop&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Docker Desktop</p>
			<p>Apart from Docker Engine, Docker Desktop <a id="_idIndexMarker146"/>provides a number of additional features, as follows:</p>
			<ul>
				<li>A <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) to display images, containers, and volumes</li>
				<li>A local Kubernetes cluster</li>
				<li>Automatic Docker updates</li>
				<li>Volume mounting with the local filesystem integration</li>
				<li>(Windows) Support for Windows containers</li>
				<li>(Windows) Integration with <strong class="bold">Windows Subsystem for Linux</strong> (<strong class="bold">WSL</strong>)/<strong class="bold">WSL version 2</strong> (<strong class="bold">WSL2</strong>)<p class="callout-heading">Note</p><p class="callout">Please <a id="_idIndexMarker147"/>visit <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a> for Docker Desktop installation guides.</p></li>
			</ul>
			<h3>Docker for Ubuntu</h3>
			<p>Visit <a href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a> to find a guide on how to install Docker <a id="_idIndexMarker148"/>on an Ubuntu machine.</p>
			<p>In the case<a id="_idIndexMarker149"/> of Ubuntu 20.04, I've executed the following commands:</p>
			<pre>$ sudo apt-get update
$ sudo apt-get -y install ca-certificates curl gnupg lsb-release
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
$ echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
$ sudo apt-get update
$ sudo apt-get -y install docker-ce docker-ce-cli containerd.io</pre>
			<p>After all operations are completed, Docker should be installed. However, at the moment, the only user allowed to use Docker commands is <code>root</code>. This means that the <code>sudo</code> keyword must precede every Docker command.</p>
			<p>We can enable other users to use Docker by adding them to the <code>docker</code> group, as follows:</p>
			<pre>$ sudo usermod -aG docker &lt;username&gt;</pre>
			<p>After a successful logout, everything is set up. With the latest command, however, we need to take some precautions not to give the Docker permissions to an unwanted user and thereby create a vulnerability in the Docker Engine environment. This is particularly important in the case of installation on the server machine.</p>
			<h3>Docker for other Linux distributions</h3>
			<p>Docker supports most <a id="_idIndexMarker150"/><a id="_idIndexMarker151"/>Linux distributions and architectures. For details, please check the official page at <a href="https://docs.docker.com/engine/install/">https://docs.docker.com/engine/install/</a>.</p>
			<h3>Testing the Docker installation</h3>
			<p>No matter which installation you've chosen (macOS, Windows, Ubuntu, Linux, or something else), Docker<a id="_idIndexMarker152"/><a id="_idIndexMarker153"/> should be set up and ready. The best way to test it is to run the <code>docker info</code> command. The output message should be similar to the following:</p>
			<pre>$ docker info
Containers: 0
   Running: 0
    Paused: 0
   Stopped: 0
    Images: 0
…</pre>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Installing on a server</h2>
			<p>In order to use Docker over the network, it's possible to either take advantage of cloud platform providers or manually install Docker on a dedicated<a id="_idIndexMarker154"/><a id="_idIndexMarker155"/> server.</p>
			<p>In the first case, the Docker configuration differs from one platform to another, but it is always very well described in dedicated tutorials. Most cloud platforms enable Docker hosts to be created through user-friendly web interfaces or describe exact commands to execute on their servers.</p>
			<p>The second case (installing Docker manually) does require a few words, however.</p>
			<h3>Dedicated server</h3>
			<p>Installing Docker<a id="_idIndexMarker156"/><a id="_idIndexMarker157"/> manually on a server does not differ much from the local installation.</p>
			<p>Two additional steps are required, which include setting the Docker daemon to listen on the network socket and setting security certificates. These steps are described in more detail here:</p>
			<ol>
				<li>By default, due to security reasons, Docker runs through a non-networked Unix socket that only allows local communication. It's necessary to add listening on the chosen network interface socket so that external clients can connect. In the case of Ubuntu, the Docker daemon is configured by <code>systemd</code>, so, in order to change the configuration of how it's started, we need to modify one line in the <code>/lib/systemd/system/docker.service</code> file, as follows:<pre>ExecStart=/usr/bin/dockerd -H &lt;server_ip&gt;:2375</pre></li>
			</ol>
			<p>By changing this line, we enabled access to the Docker daemon through the specified <code>systemd</code> configuration<a id="_idIndexMarker158"/> can be found at <a href="https://docs.docker.com/config/daemon/systemd/">https://docs.docker.com/config/daemon/systemd/</a>.</p>
			<ol>
				<li value="2">This step of server configuration concerns Docker security certificates. This enables only clients authenticated by a certificate to access the server. A comprehensive description of the Docker certificate configuration can be found at <a href="https://docs.docker.com/engine/security/protect-access/">https://docs.docker.com/engine/security/protect-access/</a>. This step isn't strictly required; however, unless your Docker daemon server is inside a firewalled network, it is essential.<p class="callout-heading">Information </p><p class="callout">If your Docker daemon is run inside a corporate network, you<a id="_idIndexMarker159"/> have to configure the <strong class="bold">HyperText Transfer Protocol</strong> (<strong class="bold">HTTP</strong>) proxy. A detailed description can be found at <a href="https://docs.docker.com/config/daemon/systemd/">https://docs.docker.com/config/daemon/systemd/</a>.</p></li>
			</ol>
			<p>The Docker environment is set up and ready, so we can start the first example.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Running Docker hello-world</h1>
			<p>Enter <a id="_idIndexMarker160"/>the following command into your console:</p>
			<pre>$ docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
1b930d010525: Pull complete
Digest: sha256:2557e3c07ed1e38f26e389462d03ed943586f744621577a99efb77324b0fe535
Status: Downloaded newer image for hello-world:latest
Hello from Docker!
This message shows that your installation appears to be working correctly.
...</pre>
			<p>Congratulations! You've just run your first Docker container. I hope you can already see how simple Docker is. Let's examine what happened under the hood, as follows:</p>
			<ol>
				<li value="1">You ran the Docker client with the <code>run</code> command.</li>
				<li>The Docker client contacted the Docker daemon and asked to create a container from the image called <code>hello-world</code>.</li>
				<li>The Docker daemon checked whether it contained the <code>hello-world</code> image locally and, since it didn't, requested the <code>hello-world</code> image from the remote Docker Hub registry.</li>
				<li>The Docker Hub registry contained the <code>hello-world</code> image, so it was pulled into the Docker daemon.</li>
				<li>The Docker daemon created a new container from the <code>hello-world</code> image that started the executable producing the output.</li>
				<li>The Docker daemon streamed this output to the Docker client.</li>
				<li>The<a id="_idIndexMarker161"/> Docker client sent it to your Terminal.</li>
			</ol>
			<p>The projected flow is represented in the following diagram:</p>
			<div><div><img src="img/B18223_02_04.jpg" alt="Figure 2.4 – Steps of the docker run command execution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Steps of the docker run command execution</p>
			<p>Let's now look at each Docker component that was illustrated in this section.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Docker components</h1>
			<p>Docker is actually an ecosystem that includes a number of components. Let's describe all of them, starting with a closer look at the Docker client-server architecture.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Docker client and server</h2>
			<p>Let's look at the<a id="_idIndexMarker162"/> following diagram, which presents the Docker Engine architecture:</p>
			<div><div><img src="img/B18223_02_05.jpg" alt="Figure 2.5 – Docker client-server architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Docker client-server architecture</p>
			<p>Docker Engine consists of<a id="_idIndexMarker163"/> the following three components:</p>
			<ul>
				<li>A <strong class="bold">Docker daemon</strong> (server) running in the background</li>
				<li>A <strong class="bold">Docker Client</strong> running as a command tool</li>
				<li>A <strong class="bold">Docker REpresentational State Transfer (REST) application programming interface (API)</strong></li>
			</ul>
			<p>Installing Docker means installing all the components so that the Docker daemon runs on our computer all the time as a service. In the case of the <code>hello-world</code> example, we used the Docker client to interact with the Docker daemon; however, we could do exactly the same thing using the REST API. Also, in the case of the <code>hello-world</code> example, we connected to the local Docker daemon. However, we could use the same client to interact with the Docker daemon running on a remote machine.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">To run the Docker container on a remote machine, you can use the <code>-H</code> option: <code>docker -H &lt;server_ip&gt;:2375 run hello-world</code>.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Docker images and containers</h2>
			<p>An <strong class="bold">image</strong> is a <a id="_idIndexMarker164"/>stateless building block in the Docker world. You can think of an image as a collection of all the files <a id="_idIndexMarker165"/>necessary to run your application, together with the recipe on how to run it. An image is stateless, so you can send it over the network, store it in the registry, name it, version it, and save it as a file. Images are layered, which means that you can build an image on top of another image.</p>
			<p>A container is <a id="_idIndexMarker166"/>a running instance of an image. We can create many containers from the same image if we want to have many instances of the same application. Since containers are stateful, this means we can interact with them and make changes to their states.</p>
			<p>Let's look at the following example of a <strong class="bold">container</strong> and <strong class="bold">image</strong> layered structure:</p>
			<div><div><img src="img/B18223_02_06.jpg" alt="Figure 2.6 – Layered structure of Docker images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Layered structure of Docker images</p>
			<p>At the bottom, there<a id="_idIndexMarker167"/> is always the base image. In most cases, this represents an operating system, and we build our images on top of the existing base images. It's technically possible to create your own base images; however, this is rarely needed.</p>
			<p>In our example, the <code>ubuntu</code> base image provides all the capabilities of the Ubuntu operating system. The <code>add git</code> image adds the Git toolkit. Then, there is an image that adds<a id="_idIndexMarker168"/> the <code>add JDK</code> image. Such a container is able, for example, to download a Java project from the GitHub repository and compile it<a id="_idIndexMarker169"/> to a <strong class="bold">Java ARchive</strong> (<strong class="bold">JAR</strong>) file. As a result, we can use this container to compile and run Java projects without installing any tools on our operating system.</p>
			<p>It is important to note that layering is a very smart mechanism to save bandwidth and storage. Imagine that we have the following application that is also based on Ubuntu:</p>
			<div><div><img src="img/B18223_02_07.jpg" alt="Figure 2.7 – Reusing layers of Docker images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Reusing layers of Docker images</p>
			<p>This time, we files <a id="_idIndexMarker170"/>will use the Python interpreter. While installing the <code>add python</code> image, the Docker daemon will note that the <code>ubuntu</code> image is already installed, and what it needs to do is only to add the Python layer, which is very small. So, the <code>ubuntu</code> image is a dependency that is reused. The same applies if we would like to deploy our image in the network. When we deploy the Git and JDK application, we need to send the whole <code>ubuntu</code> image. However, while subsequently deploying the Python application, we need to send just the small <code>add python</code> layer.</p>
			<p>Now that we understand what the Docker ecosystem consists of, let's describe how we can run applications packaged as Docker images.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Docker applications</h1>
			<p>A lot of <a id="_idIndexMarker171"/>applications are provided in the form of Docker images that can be downloaded from the internet. If we know the image name, it would be enough to run it in the same way we did with the hello-world example. <em class="italic">How can we find the desired application image on Docker Hub?</em> Let's take <strong class="bold">MongoDB</strong> as an example. These are the steps we need to follow:</p>
			<ol>
				<li value="1">If we want to find it on Docker Hub, we have two options, a<a href="https://hub.docker.com/search/">s follows:</a><ul><li><a href="https://hub.docker.com/search/">Search on the Docke</a>r Hub <code>docker search</code> command.</li></ul></li>
			</ol>
			<p>In the<a id="_idIndexMarker172"/> second case, we can perform the following operation:</p>
			<pre><strong class="bold">$ docker search mongo</strong>
<strong class="bold">NAME   DESCRIPTION                    STARS   OFFICIAL AUTOMATED</strong>
<strong class="bold">mongo  MongoDB document databases...  8293    [OK] </strong>
<strong class="bold">...</strong></pre>
			<ol>
				<li value="2">There are many interesting options. <em class="italic">How do we choose the best image?</em> Usually, the most appealing one is the one without any prefix, since it means that it's an official Docker Hub image and should therefore be stable and maintained. The images with prefixes are unofficial, usually maintained as open source projects. In our case, the best choice seems to be <code>mongo</code>, so in order to run the MongoDB server, we can run the following command:<pre><strong class="bold">$ docker run mongo</strong>
<strong class="bold">Unable to find image 'mongo:latest' locally</strong>
<strong class="bold">latest: Pulling from library/mongo</strong>
<strong class="bold">7b722c1070cd: Pull complete</strong>
<strong class="bold">...</strong>
<strong class="bold">Digest: sha256:a7c1784c83536a3c686ec6f0a1c570ad5756b94a1183af88c07df82c5b64663c</strong>
<strong class="bold">{"t":{"$date":"2021-11-17T12:23:12.379+00:00"},"s":"I",  "c":"CONTROL",  "id":23285,   "ctx":"-","msg":"Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'"}</strong>
<strong class="bold">...</strong></pre></li>
			</ol>
			<p>That's all we need to do. MongoDB has started. Running applications as Docker containers is that <a id="_idIndexMarker173"/>simple because we don't need to think of any dependencies; they are all delivered together with the image. Docker can be treated as a useful tool to run applications; however, the real power lies in building your own Docker images that wrap the programs together with the environment.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">On the Docker Hub service, you can find a lot of applications; they store millions of different images.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Building Docker images</h1>
			<p>In <a id="_idIndexMarker174"/>this section, we will see how to build Docker images using two different methods: the <code>docker</code> <code>commit</code> command and a Dockerfile automated build.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>docker commit</h2>
			<p>Let's start <a id="_idIndexMarker175"/>with an example and prepare an image with the Git and JDK toolkits. We will use Ubuntu 20.04 as a base image. There is no need to create it; most base images are available in the Docker Hub registry. Proceed as follows:</p>
			<ol>
				<li value="1">Run a container from <code>ubuntu:20.04</code> and connect it to its command line, like this:<pre><strong class="bold">$ docker run -i -t ubuntu:20.04 /bin/bash</strong></pre></li>
			</ol>
			<p>We've pulled the <code>ubuntu:20.04</code> image, run it as a container, and then called the <code>/bin/bash</code> command in an interactive way (<code>-i</code> flag). You should see the Terminal of the container. Since containers are stateful and writable, we can do anything we want in its Terminal.</p>
			<ol>
				<li value="2">Install the Git toolkit, as follows:<pre><strong class="bold">root@dee2cb192c6c:/# apt-get update</strong>
<strong class="bold">root@dee2cb192c6c:/# apt-get install -y git</strong></pre></li>
				<li>Check whether the Git toolkit is installed by running the following command:<pre><strong class="bold">root@dee2cb192c6c:/# which git</strong>
<strong class="bold">/usr/bin/git</strong></pre></li>
				<li>Exit the container, like this:<pre><strong class="bold">root@dee2cb192c6c:/# exit</strong></pre></li>
				<li>Check <a id="_idIndexMarker176"/>what has changed in the container by comparing its unique container <code>ubuntu</code> image, as follows:<pre><strong class="bold">$ docker diff dee2cb192c6c</strong></pre></li>
			</ol>
			<p>The preceding command should print a list of all files changed in the container.</p>
			<ol>
				<li value="6">Commit the container to the image, like this:<pre><strong class="bold">$ docker commit dee2cb192c6c ubuntu_with_git</strong></pre></li>
			</ol>
			<p>We've just created our first Docker image. Let's list all the images of our Docker host to see whether the image is present, as follows:</p>
			<pre>$ docker images
REPOSITORY       TAG      IMAGE ID      CREATED            SIZE
ubuntu_with_git  latest   f3d674114fe2  About a minute ago 205 MB
ubuntu           20.04    20bb25d32758  7 days ago         87.5 MB
mongo            latest   4a3b93a299a7  10 days ago        394 MB
hello-world      latest   fce289e99eb9  2 weeks ago        1.84 kB</pre>
			<p>As expected, we see <code>hello-world</code>, <code>mongo</code> (installed before), <code>ubuntu</code> (the base image pulled from Docker Hub), and the freshly built <code>ubuntu_with_git</code> image. By the way, we can observe that the size of each image corresponds to what we've installed on the image.</p>
			<p>Now, if we <a id="_idIndexMarker177"/>create a container from the image, it will have the Git tool installed, as illustrated in the following code snippet:</p>
			<pre>$ docker run -i -t ubuntu_with_git /bin/bash
root@3b0d1ff457d4:/# which git
/usr/bin/git
root@3b0d1ff457d4:/# exit</pre>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Dockerfile</h2>
			<p>Creating <a id="_idIndexMarker178"/>each Docker image manually with the <code>commit</code> command could be laborious, especially in the case of build automation and the CD process. Luckily, there is a built-in language to specify all the instructions that should be executed to build a Docker image.</p>
			<p>Let's start with an example similar to the one with Git. This time, we will prepare an <code>ubuntu_with_python</code> image, as follows:</p>
			<ol>
				<li value="1">Create a new directory and a file called <code>Dockerfile</code> with the following content:<pre><strong class="bold">FROM ubuntu:20.04</strong>
<strong class="bold">RUN apt-get update &amp;&amp; \</strong>
<strong class="bold">    apt-get install -y python</strong></pre></li>
				<li>Run the following command to create an <code>ubuntu_with_python</code> image:<pre><strong class="bold">$ docker build -t ubuntu_with_python .</strong></pre></li>
				<li>Check that the image was created by running the following command:<pre><strong class="bold">$ docker images</strong>
<strong class="bold">REPOSITORY              TAG     IMAGE ID       CREATED SIZE</strong>
<strong class="bold">ubuntu_with_python      latest  d6e85f39f5b7  About a minute ago  147 MB</strong>
<strong class="bold">ubuntu_with_git_and_jdk latest  8464dc10abbb  3 minutes ago       580 MB</strong>
<strong class="bold">ubuntu_with_git         latest  f3d674114fe2  9 minutes ago       205 MB</strong>
<strong class="bold">ubuntu                  20.04   20bb25d32758  7 days ago          87.5 MB</strong>
<strong class="bold">mongo                   latest  4a3b93a299a7  10 days ago         394 MB</strong>
<strong class="bold">hello-world             latest  fce289e99eb9  2 weeks ago         1.84 kB </strong></pre></li>
			</ol>
			<p>We can now <a id="_idIndexMarker179"/>create a container from the image and check that the Python interpreter exists in exactly the same way we did after executing the <code>docker commit</code> command. Note that the <code>ubuntu</code> image is listed only once even though it's the base image for both <code>ubuntu_with_git</code> and <code>ubuntu_with_python</code>.</p>
			<p>In this example, we used the first two Dockerfile instructions, as outlined here:</p>
			<ul>
				<li><code>FROM</code> defines an image on top of which the new image will be built</li>
				<li><code>RUN</code> specifies the commands to run inside the container.</li>
			</ul>
			<p>The other widely used instructions are detailed as follows:.</p>
			<ul>
				<li><code>COPY/ADD</code> copies a file or a directory into the filesystem of the image.</li>
				<li><code>ENTRYPOINT</code> defines which application should be run in the executable container.</li>
			</ul>
			<p>A complete guide of all <a id="_idIndexMarker180"/>Dockerfile instructions can be found on the official Docker page at <a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Complete Docker application</h2>
			<p>We <a id="_idIndexMarker181"/>already have all the information necessary to build a fully working application as a Docker image. As an example, we will prepare, step by step, a simple Python <code>hello-world</code> program. The steps are always the same, no matter which environment or programming language we use.</p>
			<h3>Writing the application</h3>
			<p>Create<a id="_idIndexMarker182"/> a new directory and, inside this directory, create a <code>hello.py</code> file with the following content:</p>
			<pre>print "Hello World from Python!"</pre>
			<p>Close the file. This is the source code of our application.</p>
			<h3>Preparing the environment</h3>
			<p>Our <a id="_idIndexMarker183"/>environment will be expressed in the Dockerfile. We need instructions to define the following:</p>
			<ul>
				<li>Which base image should be used</li>
				<li>How to install the Python interpreter</li>
				<li>How to include <code>hello.py</code> in the image</li>
				<li>How to start the application</li>
			</ul>
			<p>In the same directory, create the Dockerfile, like this:</p>
			<pre>FROM ubuntu:20.04
RUN apt-get update &amp;&amp; \
    apt-get install -y python
COPY hello.py .
ENTRYPOINT ["python", "hello.py"]</pre>
			<h3>Building the image</h3>
			<p>Now, we can <a id="_idIndexMarker184"/>build the image exactly the same way we did before, as follows:</p>
			<pre>$ docker build -t hello_world_python .</pre>
			<h3>Running the application</h3>
			<p>We run the<a id="_idIndexMarker185"/> application by running the container, like this:</p>
			<pre>$ docker run hello_world_python</pre>
			<p>You should see a friendly <strong class="bold">Hello World from Python!</strong> message. The most interesting thing in this example is that we are able to run the application written in Python without having the Python interpreter installed in our host system. This is possible because the application packed as an image has the environment already included.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">An image with the Python interpreter already exists in the Docker Hub service, so in a real-life scenario, it would be enough to use it.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Environment variables</h2>
			<p>We've run <a id="_idIndexMarker186"/>our first homemade Docker application. However, <em class="italic">what if the execution of the application depends on some conditions?</em></p>
			<p>For example, in the case of the production server, we would like to print <code>Hello</code> to the logs, not to the console, or we may want to have different dependent services during the testing phase and the production phase. One solution would be to prepare a separate Dockerfile for each case; however, there is a better way: environment variables.</p>
			<p>Let's change our <code>hello-world</code> application to print <code>Hello World from &lt;name_passed_as_environment_variable&gt; !</code>. In order to do this, we need to proceed with the following steps:</p>
			<ol>
				<li value="1">Change the <code>hello.py</code> Python script to use the environment variable, as follows:<pre>import os
print "Hello World from %s !" % os.environ['NAME']</pre></li>
				<li>Build the image, like this:<pre><strong class="bold">$ docker build -t hello_world_python_name .</strong></pre></li>
				<li>Run the container passing the environment variable, like this:<pre><strong class="bold">$ docker run -e NAME=Rafal hello_world_python_name</strong>
<strong class="bold">Hello World from Rafal !</strong></pre></li>
				<li>Alternatively, we can define an environment variable value in Dockerfile, such as the following:<pre>ENV NAME Rafal</pre></li>
				<li>Run the<a id="_idIndexMarker187"/> container without specifying the <code>-e</code> option, as follows:<pre><strong class="bold">$ docker build -t hello_world_python_name_default .</strong>
<strong class="bold">$ docker run hello_world_python_name_default</strong>
<strong class="bold">Hello World from Rafal !</strong></pre></li>
			</ol>
			<p>Environment variables are especially useful when we need to have different versions of the Docker container depending on its purpose; for example, to have separate profiles for production and testing servers.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">If an environment variable is defined both in the Dockerfile and as a flag, then the flag takes precedence.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Docker container states</h1>
			<p>Every <a id="_idIndexMarker188"/>application we've run so far was supposed to do some work and stop—for example, we've printed <code>Hello from Docker!</code> and exited. There are, however, applications that should run continuously, such as services.</p>
			<p>To run a container in the <a id="_idIndexMarker189"/>background, we can use the <code>-d</code> (<code>--detach</code>) option. Let's try it with the <code>ubuntu</code> image, as follows:</p>
			<pre>$ docker run -d -t ubuntu:20.04</pre>
			<p>This command started the Ubuntu container but did not attach the console to it. We can see that it's running by using the following command:</p>
			<pre>$ docker ps
CONTAINER ID   IMAGE           COMMAND       STATUS         PORTS 
NAMES
95f29bfbaadc   ubuntu:20.04    "/bin/bash"   Up 5 seconds kickass_stonebraker</pre>
			<p>This <a id="_idIndexMarker190"/>command prints all containers that are in a <strong class="bold">running</strong> state. <em class="italic">What about our old, already exited containers?</em> We can find them by printing all<a id="_idIndexMarker191"/> containers, like this:</p>
			<pre>$ docker ps -a
CONTAINER ID   IMAGE          COMMAND       STATUS       PORTS  
NAMES
95f29bfbaadc   ubuntu:20.04   "/bin/bash"    Up 33 seconds kickass_stonebraker
34080d914613   hello_world_python_name_default "python hello.py" Exited lonely_newton
7ba49e8ee677 hello_world_python_name "python hello.py" Exited mad_turing
dd5eb1ed81c3 hello_world_python "python hello.py" Exited thirsty_bardeen
...</pre>
			<p>Note that all the old containers are<a id="_idIndexMarker192"/> in an <strong class="bold">exited</strong> state. There are <a id="_idIndexMarker193"/>two more states we haven't observed <a id="_idIndexMarker194"/>yet: <strong class="bold">paused</strong> and <strong class="bold">restarting</strong>.</p>
			<p>All of the states and the transitions between them are presented in the following diagram:</p>
			<div><div><img src="img/B18223_02_08.jpg" alt="Figure 2.8 – Docker container states&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Docker container states</p>
			<p>Pausing<a id="_idIndexMarker195"/> Docker containers is very rare, and technically, it's done by freezing the processes using the <code>SIGSTOP</code> signal. Restarting is a temporary state when the container is run with the <code>--restart</code> option to define a restarting strategy (the Docker daemon is able to automatically restart the container in case of failure).</p>
			<p>The preceding diagram also shows the Docker commands used to change the Docker container state from one state to another.</p>
			<p>For example, we can stop running the Ubuntu container, as shown here:</p>
			<pre>$ docker stop 95f29bfbaadc    
$ docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</pre>
			<p class="callout-heading">Information</p>
			<p class="callout">We've always used the <code>docker run</code> command to create and start a container. However, it's possible to just create a container without starting it (with <code>docker create</code>).</p>
			<p>Having grasped the details of Docker states, let's describe the networking basics within the Docker world.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/>Docker networking</h1>
			<p>Most <a id="_idIndexMarker196"/>applications these days do not run in isolation; they need to communicate with other systems over the network. If we want to run a website, web service, database, or cache server inside a Docker container, we need to first understand how to run a service and expose its port to other applications.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Running services</h2>
			<p>Let's start with a <a id="_idIndexMarker197"/>simple example and run a Tomcat server directly from Docker Hub, as follows:</p>
			<pre>$ docker run -d tomcat</pre>
			<p>Tomcat is a <a id="_idIndexMarker198"/>web application server whose UI can be accessed by port <code>8080</code>. Therefore, if we installed Tomcat on our machine, we could browse it at <code>http://localhost:8080</code>. In our case, however, Tomcat is running inside the Docker container.</p>
			<p>We started it the same way we did with the first <code>Hello World</code> example. We can see that it's running, as follows:</p>
			<pre>$ docker ps
CONTAINER ID IMAGE  COMMAND           STATUS            PORTS    NAMES
d51ad8634fac tomcat "catalina.sh run" Up About a minute 8080/tcp jovial_kare</pre>
			<p>Since it's run as a daemon (with the <code>-d</code> option), we don't see the logs in the console right away. We can, however, access it by executing the following code:</p>
			<pre>$ docker logs d51ad8634fac</pre>
			<p>If there are no errors, we should see a lot of logs, which indicates that Tomcat has been started and is accessible through port <code>8080</code>. We can try going to <code>http://localhost:8080</code>, but we won't be able to connect. This is because Tomcat has been started inside the container and we're trying to reach it from the outside. In other words, we can reach it only if we connect with the command to the console in the container and check it there. <em class="italic">How do we make running Tomcat accessible from outside?</em></p>
			<p>We need to start the container, specifying the port mapping with the <code>-p</code> (<code>--publish</code>) flag, as follows:</p>
			<pre>-p, --publish &lt;host_port&gt;:&lt;container_port&gt;</pre>
			<p>So, let's first stop<a id="_idIndexMarker199"/> the running container and start a new one, like this:</p>
			<pre>$ docker stop d51ad8634fac
$ docker run -d -p 8080:8080 tomcat</pre>
			<p>After waiting a few seconds, Tomcat should have started, and we should be able to open its page—<code>http://localhost:8080</code>.</p>
			<p>The following screenshot illustrates how Docker container ports are published:</p>
			<div><div><img src="img/B18223_02_09.jpg" alt="Figure 2.9 – Publishing Docker container ports&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – Publishing Docker container ports</p>
			<p>Such a simple port mapping command is sufficient in most common Docker use cases. We are able to deploy (micro) services as Docker containers and expose their ports to facilitate communication. However, let's dive a little deeper into what happened under the hood.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">Docker also allows us to publish to the specific host network interface with <code>-p &lt;ip&gt;:&lt;host_port&gt;:&lt;container_port&gt;</code>.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Container networks</h2>
			<p>We <a id="_idIndexMarker200"/>have connected to the application that is running inside the container. In fact, the connection is two-way because, if you <a id="_idIndexMarker201"/>remember from our previous examples, we executed the <code>apt-get install</code> commands from inside and the packages were downloaded from the internet. <em class="italic">How is this possible?</em></p>
			<p>If you check the network interfaces on your machine, you can see that one of the interfaces is called <code>docker0</code>, as illustrated here:</p>
			<pre>$ ifconfig docker0
docker0 Link encap:Ethernet HWaddr 02:42:db:d0:47:db 
     inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0
...</pre>
			<p>The <code>docker0</code> interface is created by the Docker daemon in order to connect with the Docker container. Now, we can see which interfaces are created inside the Tomcat Docker container created with the <code>docker inspect</code> command, as follows:</p>
			<pre>$ docker inspect 03d1e6dc4d9e</pre>
			<p>This prints all the information about the container configuration in <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) format. Among<a id="_idIndexMarker202"/> other things, we can find the part related to the network settings, as illustrated in the following code snippet:</p>
			<pre>"NetworkSettings": {
     "Bridge": "",
     "Ports": {
          "8080/tcp": [
               {
                    "HostIp": "0.0.0.0",
                    "HostPort": "8080"
               }
          ]
          },
     "Gateway": "172.17.0.1",
     "IPAddress": "172.17.0.2",
     "IPPrefixLen": 16,
}</pre>
			<p class="callout-heading">Information</p>
			<p class="callout">In order to filter the <code>docker inspect</code> response, we can use the <code>--format</code> option—for example, <code>docker inspect --format '{{ .NetworkSettings.IPAddress }}' &lt;container_id&gt;</code>.</p>
			<p>We can <a id="_idIndexMarker203"/>observe that the Docker container has an IP address of <code>172.17.0.2</code> and it communicates with the Docker host with an IP address of <code>172.17.0.1</code>. This means that in our previous example, we could access the Tomcat server even without the port forwarding, using <code>http://172.17.0.2:8080</code>. Nevertheless, in most cases, we run the Docker container on a server machine and want to expose it outside, so we need to use the <code>-p</code> option.</p>
			<p>Note that, by default, the containers don't open any routes from external systems. We can change this default behavior by <a id="_idIndexMarker204"/>playing with the <code>--network</code> flag and setting it as follows:</p>
			<ul>
				<li><code>bridge</code> (default): Network through the default Docker bridge</li>
				<li><code>none</code>: No network</li>
				<li><code>container</code>: Network joined with the other (specified) container</li>
				<li><code>host</code>: Host's network stack</li>
				<li><code>NETWORK</code>: User-created network (using the <code>docker network create</code> command)</li>
			</ul>
			<p>The different networks can be listed and managed by the <code>docker network</code> command, as follows:</p>
			<pre>$ docker network ls
NETWORK ID    NAME    DRIVER  SCOPE
b3326cb44121  bridge  bridge  local 
84136027df04  host    host    local 
80c26af0351c  none    null    local</pre>
			<p>If we <a id="_idIndexMarker205"/>specify <code>none</code> as the network, we will not be able to connect to the container, and vice versa; the container has no network access to the external world. The <code>host</code> option makes the <code>bridge</code>) because it lets us define explicitly which ports should be published and is both secure and accessible.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Exposing container ports</h2>
			<p>We <a id="_idIndexMarker207"/>mentioned a few times that the container exposes the port. In fact, if we dig deeper into the Tomcat image on GitHub (<a href="https://github.com/docker-library/tomcat">https://github.com/docker-library/tomcat</a>), we can see the following line in the Dockerfile:</p>
			<pre>EXPOSE 8080</pre>
			<p>This Dockerfile instruction stipulates that port <code>8080</code> should be exposed from the container. However, as we have already seen, this doesn't mean that the port is automatically published. The <code>EXPOSE</code> instruction only informs users which ports they should publish.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Automatic port assignment</h2>
			<p>Let's try to <a id="_idIndexMarker208"/>run the second Tomcat container without stopping the first one, as follows:</p>
			<pre>$ docker run -d -p 8080:8080 tomcat
0835c95538aeca79e0305b5f19a5f96cb00c5d1c50bed87584cfca8ec790f241
docker: Error response from daemon: driver failed programming external connectivity on endpoint distracted_heyrovsky (1b1cee9896ed99b9b804e4c944a3d9544adf72f1ef3f9c9f37bc985e9c30f452): Bind for 0.0.0.0:8080 failed: port is already allocated.</pre>
			<p>This error<a id="_idIndexMarker209"/> may be common. In such cases, we have to either take care of the uniqueness of the ports on our own or let Docker assign the ports automatically using one of the following versions of the <code>publish</code> command:</p>
			<ul>
				<li><code>-p &lt;container_port&gt;</code>: Publishes the container port to the unused host port</li>
				<li><code>-p</code> (<code>--publish-all</code>): Publishes all ports exposed by the container to the unused host ports, as follows:<pre><strong class="bold">$ docker run -d -P tomcat</strong>
<strong class="bold">078e9d12a1c8724f8aa27510a6390473c1789aa49e7f8b14ddfaaa328c8f737b</strong>
<strong class="bold">$ docker port 078e9d12a1c8</strong>
<strong class="bold">8080/tcp -&gt; 0.0.0.0:32772</strong></pre></li>
			</ul>
			<p>We can see that the second Tomcat instance has been published to port <code>32772</code>, so it can be browsed at <code>http://localhost:32772</code>.</p>
			<p>After understanding Docker network basics, let's see how to provide a persistence layer for Docker containers using Docker volumes.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Using Docker volumes</h1>
			<p>Imagine that <a id="_idIndexMarker210"/>you would like to run a database as a container. You can start such a container and enter data. <em class="italic">Where is it stored?</em> <em class="italic">What happens when you stop the container or remove it?</em> You can start a new one, but the database will be empty again. Unless it's your testing environment, you'd expect to have your data persisted permanently.</p>
			<p>A Docker volume <a id="_idIndexMarker211"/>is the Docker host's directory mounted inside the container. It allows the container to write to the host's filesystem as if it were writing to its own. The mechanism is presented in the following diagram:</p>
			<div><div><img src="img/B18223_02_10.jpg" alt="Figure 2.10 – Using a Docker volume&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Using a Docker volume</p>
			<p>Docker volumes enable the persistence and sharing of a container's data. Volumes also clearly separate the processing from the data. Let's start with the following example:</p>
			<ol>
				<li value="1">Specify a volume with the <code>-v &lt;host_path&gt;:&lt;container_path&gt;</code> option and then connect to the container, as follows:<pre><strong class="bold">$ docker run -i -t -v ~/docker_ubuntu:/host_directory ubuntu:20.04 /bin/bash</strong></pre></li>
				<li>Create an empty file in <code>host_directory</code> in the container, like this:<pre><strong class="bold">root@01bf73826624:/# touch /host_directory/file.txt</strong></pre></li>
				<li>Check whether the file was created in the Docker host's filesystem by running the following command:<pre><strong class="bold">root@01bf73826624:/# exit</strong>
<strong class="bold">exit</strong>
<strong class="bold">$ ls ~/docker_ubuntu/</strong>
<strong class="bold">file.txt</strong></pre></li>
				<li>We can see that the filesystem was shared and the data was therefore persisted permanently. Stop the container and run a new one to see if our file will still be there, as follows:<pre><strong class="bold">$ docker run -i -t -v ~/docker_ubuntu:/host_directory ubuntu:20.04 /bin/bash</strong>
<strong class="bold">root@a9e0df194f1f:/# ls /host_directory/</strong>
<strong class="bold">file.txt</strong>
<strong class="bold">root@a9e0df194f1f:/# exit</strong></pre></li>
				<li>Instead <a id="_idIndexMarker212"/>of specifying a volume with the <code>-v</code> flag, it's possible to specify it as an instruction in the Dockerfile, as in the following example:<pre>VOLUME /host_directory</pre></li>
			</ol>
			<p>In this case, if we run the Docker container without the <code>-v</code> flag, the container's <code>/host_directory</code> path will be mapped into the host's default directory for volumes, <code>/var/lib/docker/vfs/</code>. This is a good solution if you deliver an application as an image and you know it requires permanent storage for some reason (for example, storing application logs).</p>
			<p class="callout-heading">Information</p>
			<p class="callout">If a volume is defined both in a Dockerfile and as a flag, the <code>flag</code> command takes precedence.</p>
			<p>Docker volumes can be much more complicated, especially in the case of databases. More complex use cases of Docker volumes are, however, outside the scope of this book.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">A very common approach to data management with Docker is to introduce an additional layer, in the form of data volume containers. A data volume container is a Docker container whose only purpose is to declare a volume. Then, other containers can use it (with the <code>--volumes-from &lt;container&gt;</code> option) instead of declaring the volume directly. Read <a id="_idIndexMarker213"/>more at <a href="https://docs.docker.com/storage/volumes/">https://docs.docker.com/storage/volumes/</a>.</p>
			<p>After understanding Docker volumes, let's see how we can use names to make working with Docker images/containers more convenient.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Using names in Docker</h1>
			<p>So far, when we've operated on containers, we've always used autogenerated names. This approach has some advantages, such as the names being unique (no naming conflicts) and automatic (no need to do anything). In many cases, however, it's better to give a user-friendly name to a container or an image.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>Naming containers</h2>
			<p>There are two<a id="_idIndexMarker214"/> good reasons to name a container: convenience and the possibility of automation. Let's look at why, as follows:</p>
			<ul>
				<li><strong class="bold">Convenience</strong>: It's <a id="_idIndexMarker215"/>simpler to make any operations on a container when addressing it by name than by checking the hashes or the autogenerated name.</li>
				<li><strong class="bold">Automation</strong>: Sometimes, we <a id="_idIndexMarker216"/>would like to depend on the specific naming of a container.</li>
			</ul>
			<p>For example, we would like to have containers that depend on each other and to have one linked to another. Therefore, we need to know their names.</p>
			<p>To name a container, we use the <code>--name</code> parameter, as follows:</p>
			<pre>$ docker run -d --name tomcat tomcat</pre>
			<p>We can check (with <code>docker ps</code>) that the container has a meaningful name. Also, as a result, any operation can be performed using the container's name, as in the following example:</p>
			<pre>$ docker logs tomcat</pre>
			<p>Please note that when a container is named, it does not lose its identity. We can still address the container by its autogenerated hash ID, just as we did before.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">A container always has both an ID and a name. It can be addressed by either of them, and both are unique.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Tagging images</h2>
			<p>Images <a id="_idIndexMarker217"/>can be tagged. We already did this when creating our own images—for example, in the case of building the <code>hello_world_python</code> image, as illustrated here:</p>
			<pre>$ docker build -t hello_world_python .</pre>
			<p>The <code>-t</code> flag describes the tag of the image. If we don't use it, the image will be built without any tags and, as a result, we would have to address it by its ID (hash) in order to run the container.</p>
			<p>The image can have multiple tags, and they should follow this naming convention:</p>
			<pre>&lt;registry_address&gt;/&lt;image_name&gt;:&lt;version&gt;</pre>
			<p>A tag consists of the following parts:</p>
			<ul>
				<li><code>registry_address</code>: IP and port of the registry or the alias name</li>
				<li><code>image_name</code>: Name of the image that is built—for example, <code>ubuntu</code></li>
				<li><code>version</code>: A version of the image in any form—for example, <code>20.04</code>, <code>20170310</code></li>
			</ul>
			<p>We will cover Docker registries in <a href="B18223_05_ePub.xhtml#_idTextAnchor133"><em class="italic">Chapter 5</em></a>, <em class="italic">Automated Acceptance Testing</em>. If an image is kept on the official Docker Hub registry, we can skip the registry address. This is why we've run the <code>tomcat</code> image without any prefix. The last version is always tagged as the latest and it can also be skipped, so we've run the <code>tomcat</code> image without any suffix.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">Images usually have multiple tags; for example, all these three tags are the same image: <code>ubuntu:18.04</code>, <code>ubuntu:bionic-20190122</code>, and <code>ubuntu:bionic</code>.</p>
			<p>Last but not least, we need to learn how to clean up after playing with Docker.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Docker cleanup</h1>
			<p>Throughout this chapter, we have created a number of containers and images. This is, however, only a small part of what you will see in real-life scenarios. Even when containers are not running, they need to be stored on the Docker host. This can quickly result in exceeding the storage space and stopping the machine. How can we approach this concern?</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Cleaning up containers</h2>
			<p>First, let's look<a id="_idIndexMarker218"/> at the containers that are stored on our machine. Here are the steps we need to follow:</p>
			<ol>
				<li value="1">To print all the containers (irrespective of their state), we can use the <code>docker ps -a</code> command, as follows:<pre><strong class="bold">$ docker ps -a</strong>
<strong class="bold">CONTAINER ID IMAGE  COMMAND           STATUS  PORTS  NAMES</strong>
<strong class="bold">95c2d6c4424e tomcat "catalina.sh run" Up 5 minutes 8080/tcp tomcat</strong>
<strong class="bold">a9e0df194f1f ubuntu:20.04 "/bin/bash" Exited         jolly_archimedes</strong>
<strong class="bold">01bf73826624 ubuntu:20.04 "/bin/bash" Exited         suspicious_feynman</strong>
<strong class="bold">...</strong></pre></li>
				<li>In order to delete a stopped container, we can use the <code>docker rm</code> command (if a container is running, we need to stop it first), as follows:<pre><strong class="bold">$ docker rm 47ba1c0ba90e</strong></pre></li>
				<li>If we want to remove all stopped containers, we can use the following command:<pre><strong class="bold">$ docker container prune</strong></pre></li>
				<li>We <a id="_idIndexMarker219"/>can also adopt a different approach and ask the container to remove itself as soon as it has stopped by using the <code>--rm</code> flag, as in the following example:<pre><strong class="bold">$ docker run --rm hello-world</strong></pre></li>
			</ol>
			<p>In most real-life scenarios, we don't use stopped containers, and they are left only for debugging purposes.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Cleaning up images</h2>
			<p>Cleaning up images<a id="_idIndexMarker220"/> is just as important as cleaning up containers. They can occupy a lot of space, especially in the case of the CD process, when each build ends up in a new Docker image. This can quickly result in a <em class="italic">no space left on device</em> error. The steps are as outlined here:</p>
			<ol>
				<li value="1">To check all the images in the Docker container, we can use the <code>docker images</code> command, as follows:<pre><strong class="bold">$ docker images</strong>
<strong class="bold">REPOSITORY TAG                         IMAGE ID     CREATED     SIZE</strong>
<strong class="bold">hello_world_python_name_default latest 9a056ca92841 2 hours ago 202.6 MB</strong>
<strong class="bold">hello_world_python_name latest         72c8c50ffa89 2 hours ago 202.6 MB</strong>
<strong class="bold">...</strong></pre></li>
				<li>To remove an image, we can call the following command:<pre><strong class="bold">$ docker rmi 48b5124b2768</strong></pre></li>
				<li>In the case of images, the automatic cleanup process is slightly more complex. Images don't have states, so we cannot ask them to remove themselves when not used. A <a id="_idIndexMarker221"/>common strategy would be to set up a cron cleanup job, which removes all old and unused images. We could do this using the following command:<pre><code>docker volume prune</code> command.</p></li>
			</ol>
			<p>With the cleaning up section, we've come to the end of the main Docker description. Now, let's do a short wrap-up and walk through the most important Docker commands.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Use the <code>docker system prune</code> command to remove all unused containers, images, and networks. Additionally, you can add the <code>–volumes</code> parameter to clean up volumes.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Docker commands overview</h1>
			<p>All Docker commands<a id="_idIndexMarker222"/> can be found by executing the following <code>help</code> command:</p>
			<pre>$ docker help</pre>
			<p>To see all the options of any particular Docker command, we can use <code>docker help &lt;command&gt;</code>, as in the following example:</p>
			<pre>$ docker help run</pre>
			<p>There is also a very good explanation of all Docker commands on the official Docker page at <a href="https://docs.docker.com/engine/reference/commandline/docker/">https://docs.docker.com/engine/reference/commandline/docker/</a>. It's worth reading, or at least skimming, through.</p>
			<p>In this chapter, we've covered the most useful commands and their options. As a quick reminder, let's <a id="_idIndexMarker223"/>walk through them, as follows:</p>
			<div><div><img src="img/B18223_02_Table_01.jpg" alt=""/>
				</div>
			</div>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Summary</h1>
			<p>In this chapter, we covered the Docker basics, which is enough for building images and running applications as containers. Here are the key takeaways.</p>
			<p>The containerization technology addresses the issues of isolation and environment dependencies using Linux kernel features. This is based on a process separation mechanism, so, therefore, no real performance drop is observed. Docker can be installed on most systems but is supported natively only on Linux. Docker allows us to run applications from images available on the internet and to build our own images. An image is an application packed together with all the dependencies.</p>
			<p>Docker provides two methods for building images—a Dockerfile or committing a container. In most cases, the first option is used. Docker containers can communicate over the network by publishing the ports they expose. Docker containers can share persistent storage using volumes. For the purpose of convenience, Docker containers should be named, and Docker images should be tagged. In the Docker world, there is a specific convention for how to tag images. Docker images and containers should be cleaned from time to time in order to save on server space and avoid <em class="italic">no space left on device</em> errors.</p>
			<p>In the next chapter, we will look at the Jenkins configuration and find out how Jenkins can be used in conjunction with Docker.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Exercises</h1>
			<p>We've covered a lot of material in this chapter. To consolidate what we have learned, we recommend the following two exercises:</p>
			<ol>
				<li value="1">Run <code>CouchDB</code> as a Docker container and publish its port, as follows:<p class="callout-heading">Tip</p><p class="callout">You can use the <code>docker search</code> command to find the <code>CouchDB</code> image.</p><ol><li>Run the container.</li><li>Publish the <code>CouchDB</code> port.</li><li>Open the browser and check that <code>CouchDB</code> is available.</li></ol></li>
				<li>Create a Docker image with a REST service, replying <code>Hello World</code> to <code>localhost:8080/hello</code>. Use any language and framework you prefer. Here are the steps you need to follow:<p class="callout-heading">Tip</p><p class="callout">The easiest way to create a REST service is to use Python with the Flask framework (<a href="https://flask.palletsprojects.com/">https://flask.palletsprojects.com/</a>). Note that a lot of web frameworks, by default, start an application only on the localhost interface. In order to publish a port, it's necessary to start it on all interfaces (<code>app.run(host='0.0.0.0')</code> in the case of a Flask framework).</p><ol><li>Create a web service application.</li><li>Create a Dockerfile to install dependencies and libraries.</li><li>Build the image.</li><li>Run the container that is publishing the port.</li><li>Check that it's running correctly by using the browser (or <code>curl</code>).</li></ol></li>
			</ol>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Questions</h1>
			<p>To verify the knowledge acquired from this chapter, please answer the following questionsUse L-numbering for this list</p>
			<ol>
				<li value="1">What is the main difference between containerization (such as with Docker) and virtualization (such as with VirtualBox)?</li>
				<li>What are the benefits of providing an application as a Docker image? Name at least two.</li>
				<li>Can the Docker daemon be run natively on Windows and macOS?</li>
				<li>What is the difference between a Docker image and a Docker container?</li>
				<li>What does it mean when saying that Docker images have layers?</li>
				<li>What are two methods of creating a Docker image?</li>
				<li>Which command is used to create a Docker image from a Dockerfile?</li>
				<li>Which command is used to run a Docker container from a Docker image?</li>
				<li>In Docker terminology, what does it mean to publish a port?</li>
				<li>What is a Docker volume?</li>
			</ol>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Further reading</h1>
			<p>If you're interested in getting a deeper understanding of Docker and related technologies, please have a look at the following resources:</p>
			<ul>
				<li>Docker documentation—<em class="italic">Get started</em>: <a href="https://docs.docker.com/get-started/">https://docs.docker.com/get-started/</a></li>
				<li><em class="italic">The Docker Book</em> by <em class="italic">James Turnbull</em>: <a href="https://dockerbook.com/">https://dockerbook.com/</a></li>
			</ul>
		</div>
	</body></html>