<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Container Schedulers</h1></div></div></div><p>Now, we are at the business end of the book. There is a lot of buzz at the moment around this topic. This is where containers are going to go in future, and schedulers solve a lot of problems, for instance, spreading the load of our application across multiple hosts for us pending on load and starting our containers on another instance if the original host fails. In this chapter, we will look at three different schedulers. First, we will look at Docker Swarm. This is a Docker open source scheduler. We will build five servers and look at how to create a replicated master. We will then run a few containers and look at how Swarm will schedule them across nodes. The next scheduler we will look at is Docker <strong>UCP</strong> (<strong>Universal Control Plane</strong>). This<a id="id201" class="indexterm"/> is a Docker enterprise solution that is integrated with Docker Compose. We will build a three-node cluster and deploy our Consul module. As UCP has a graphical interface, we will look at how UCP is scheduled from there. The final scheduler we will look at is Kubernetes. This is Google's offering and is also open source. For Kubernetes, we will build a single node using containers and use Puppet to define the more complex types. As you can see, we are going to look at each one differently, as they all have their individual strengths and weaknesses. Depending on your use case, you might decide on one or all of them to solve a problem that you may face.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec30"/>Docker Swarm</h1></div></div></div><p>For our first <a id="id202" class="indexterm"/>scheduler, we are going to look at Docker Swarm. This is a really solid product and in my opinion is a bit underrated compared to Kubernetes. It has really come on in leaps and bounds in the last few releases. It now supports replicated masters, rescheduling containers on failed hosts. So, let's look at the architecture of what we are building. Then, we will get into the coding.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec41"/>The Docker Swarm architecture</h2></div></div></div><p>In this example, we <a id="id203" class="indexterm"/>are going to build five servers, where two will be replicated masters and the other three nodes will be in the swarm cluster. As Docker Swarm needs a key/value store backend, we will use Consul. In this instance, we are not going to use our Consul modules; instead, we are going to use <a class="ulink" href="https://forge.puppetlabs.com/KyleAnderson/consul">https://forge.puppetlabs.com/KyleAnderson/consul</a>. The reason for this is that in all three<a id="id204" class="indexterm"/> examples, we are going to use different design choices. So, when you are trying to <a id="id205" class="indexterm"/>build a solution, you are exposed to more than one way to skin the cat. In this example, we are going to install Swarm and Consul onto the OS using Puppet and then run containers on top of it.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec42"/>Coding</h2></div></div></div><p>In this example, we<a id="id206" class="indexterm"/> are going create a new Vagrant repo. So, we will Git clone <a class="ulink" href="https://github.com/scotty-c/vagrant-template.git">https://github.com/scotty-c/vagrant-template.git</a> into the directory of our choice. The first thing that we will edit is the Puppetfile. This can be found in the root of our Vagrant repo. We will add the following changes to the file:</p><div><img src="img/B05201_07_02.jpg" alt="Coding"/></div><p>The next file we will edit is <code class="literal">servers.yaml</code>. Again, this is located in the root of our Vagrant repo. We are going to add five servers to it. So, I will break down this file into five parts, one for each server.</p><p>First, let's look at the code for server 1:</p><div><img src="img/B05201_07_03.jpg" alt="Coding"/></div><p>Now, let's look at the code for server 2:</p><div><img src="img/B05201_07_04.jpg" alt="Coding"/></div><p>The following <a id="id207" class="indexterm"/>screenshot shows the code for server 3:</p><div><img src="img/B05201_07_05.jpg" alt="Coding"/></div><p>The following screenshot shows the code for server 4:</p><div><img src="img/B05201_07_06.jpg" alt="Coding"/></div><p>Finally, the<a id="id208" class="indexterm"/> code for server 5 is as follows:</p><div><img src="img/B05201_07_07.jpg" alt="Coding"/></div><p>All this should look fairly familiar to you. The one call out is that we have used servers one through three as our cluster nodes. Four and five will be our masters.</p><p>The next thing we are going to do is add some values to Hiera. This is the first time we have used Hiera, so let's look at our <code class="literal">hiera.yaml</code> file to see our configuration in the root of our Vagrant repo, which is as follows:</p><div><img src="img/B05201_07_08.jpg" alt="Coding"/></div><p>As you can see, we have a basic hierarchy. We just have one file to look at, which is our <code class="literal">global.yaml</code> file. We can see that the file lives in our <code class="literal">hieradata</code> folder as it is declared as our <code class="literal">data</code> directory. So, if <a id="id209" class="indexterm"/>we open the <code class="literal">global.yaml</code> file, we are going to add the following values to it:</p><div><img src="img/B05201_07_09.jpg" alt="Coding"/></div><p>The first value will tell Swarm what version we want to use. The last value sets the version of Consul that we will be using, which is <code class="literal">0.6.3</code>.</p><p>The next thing we need to do is write our module that will deploy both our Consul and Swarm clusters. We have created quite a few modules so far in the book, so we won't cover it again. In this instance, we will create a module called <code class="literal">&lt;AUTHOR&gt;-config</code>. We will then move the module into our <code class="literal">modules</code> folder at the root of our Vagrant repo. Now, let's look at what we are going to add to our <code class="literal">init.pp</code> file:</p><div><img src="img/B05201_07_10.jpg" alt="Coding"/></div><p>As you can see, this sets up our module. We will need to create a <code class="literal">.pp</code> file for each of our entries in our <code class="literal">init.pp</code> file, for example, <code class="literal">consul_config.pp</code>, <code class="literal">swarm.pp</code>, and so on. We have also declared one variable called <code class="literal">consul_ip</code>, whose value will actually come from Hiera, as that is where we set the value earlier. We will look at our files in alphabetical order. So, we will start with <code class="literal">config::compose.pp</code>, which is as follows:</p><div><img src="img/B05201_07_11.jpg" alt="Coding"/></div><p>In this class, we <a id="id210" class="indexterm"/>are setting up a registrator. We are going to use Docker Compose for this, as it is a familiar configuration and something that we have already covered. You will note that there is an <code class="literal">if</code> statement in the code. This is a logic to run the container only on a cluster member. We don't want to run container applications on our masters. The next file we will look at is <code class="literal">consul_config.pp</code>, which is as follows:</p><div><img src="img/B05201_07_12.jpg" alt="Coding"/></div><p>In this class, we will configure Consul, something that we have already covered in this book. I always like to look at multiple ways to do the same job, as you never know the solution that you might need to produce tomorrow and you always want the right tool for the right job. So, in this instance, we will not configure Consul in a container but natively on the host OS.</p><p>You can see <a id="id211" class="indexterm"/>that the code is broken into three blocks. The first block bootstraps our Consul cluster. You will note that the configurations are familiar, as they are the same as those we used to set up our container in an earlier chapter. The second block of code sets the members of the cluster after the cluster is bootstrapped. We have not seen this third block before. This sets up a service for Consul to monitor. This is just a taste of what we can manage, but we will look at this in anger in the next chapter. Let's look at our next file, that is, <code class="literal">dns.pp</code>:</p><div><img src="img/B05201_07_13.jpg" alt="Coding"/></div><p>You will note that<a id="id212" class="indexterm"/> this file is exactly the same as we used in our <code class="literal">consul</code> module. So, we can move on to the next file, which is <code class="literal">run_containers.pp</code>:</p><div><img src="img/B05201_07_14.jpg" alt="Coding"/></div><p>This is the class<a id="id213" class="indexterm"/> that will run our containers on our cluster. At the top, we are declaring that we want master number two to initiate the call to the cluster. We are going to deploy three container applications. The first is <code class="literal">jenkins</code>, and as you can see, we will expose Jenkins web port <code class="literal">8080</code> to the host that the container runs on. The next container is <code class="literal">nginx</code>, and we are going to forward both <code class="literal">80</code> and <code class="literal">443</code> to the host, but also connect <code class="literal">nginx</code> to our private network across our <code class="literal">swarm-private</code> cluster. To get our logs <a id="id214" class="indexterm"/>from <code class="literal">nginx</code>, we will tell our container to use syslog as the logging driver. The last application that we will run is <code class="literal">redis</code>. This will be the backend of <code class="literal">nginx</code>. You will note that we are not forwarding any ports, as we are hiding Redis on our internal network. Now that we have our containers sorted, we have one file left, <code class="literal">swarm.pp</code>. This will configure our Swarm cluster and internal Swarm network as follows:</p><div><img src="img/B05201_07_15.jpg" alt="Coding"/></div><p>The first resource declaration will install the Swarm binaries. The next resource will configure our<a id="id215" class="indexterm"/> Docker network on each host. The next <code class="literal">if</code> state will define if the Swarm node is a master or part of the cluster via <code class="literal">hostname</code>. As you can see, we are declaring a few defaults that are the first values that tell Swarm what backend to use Consul with. The second value tells Swarm the IP address of the Consul backend which is <code class="literal">172.17.8.101</code>. The third value tells Swarm about the port it can access Swarm on, <code class="literal">8500</code>. The fourth value tells Swarm which interface to advertise the cluster on, which in our case is <code class="literal">enp0s8</code>. The last value sets the root of the key value store Swarm will use. Now, let's create our <code class="literal">templates</code> folder in the root of our module.</p><p>We will create three files there. The first file will be <code class="literal">consul.conf.erb</code> and the contents will be as follows:</p><div><img src="img/B05201_07_16.jpg" alt="Coding"/></div><p>The next file will be <code class="literal">named.conf.erb</code> and its contents will be as follows:</p><div><img src="img/B05201_07_17.jpg" alt="Coding"/></div><p>The last file <a id="id216" class="indexterm"/>will be <code class="literal">registrator.yml.erb</code>, and the file's content will be as follows:</p><div><img src="img/B05201_07_18.jpg" alt="Coding"/></div><p>The next thing that we need to add is our <code class="literal">config</code> class to our <code class="literal">default.pp</code> manifest file in the <code class="literal">manifest</code> folder in the root of our Vagrant repo:</p><div><img src="img/B05201_07_23.jpg" alt="Coding"/></div><p>Now, <a id="id217" class="indexterm"/>our module is complete and we are ready to run our cluster. So, let's open our terminal and change the directory to the root of our Vagrant repo and issue the <code class="literal">vagrant up</code> command. Now, we are building five servers, so be patient. Once our last server is built, our terminal output should look like that shown in this screenshot:</p><div><img src="img/B05201_07_19.jpg" alt="Coding"/><div><p>Terminal output</p></div></div><p>We can now look at our Consul cluster's web UI at <code class="literal">127.0.0.1:9501</code> (remember that we changed our <a id="id218" class="indexterm"/>ports on our <code class="literal">servers.yaml</code> file) as follows:</p><div><img src="img/B05201_07_20.jpg" alt="Coding"/></div><p>Now, let's see what host our Jenkins service is running on:</p><div><img src="img/B05201_07_21.jpg" alt="Coding"/></div><p>In this example, my service has come up on cluster node 101. You could get a different host for your cluster. So, we need to check what port <code class="literal">8080</code> forwards to our <code class="literal">servers.yaml</code> file guest. In my example, it's <code class="literal">8081</code>. So, if I go to my web browser and open a new tab and browse to <code class="literal">127.0.0.1:8081</code>, we will get the Jenkins page, which is as follows:</p><div><img src="img/B05201_07_22.jpg" alt="Coding"/></div><p>You can do the<a id="id219" class="indexterm"/> same with nginx, and I will leave that up to you as a challenge.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec31"/>Docker UCP</h1></div></div></div><p>In this topic, we <a id="id220" class="indexterm"/>will be looking at Docker's new offering, UCP. This product is not open sourced, so there is a licensing cost. You can get a trial license for 30 days (<a class="ulink" href="https://www.docker.com/products/docker-universal-control-plane">https://www.docker.com/products/docker-universal-control-plane</a>), and that is what we will be using. Docker UCP takes out the complexity of managing all the moving parts of a scheduler. This could be a massive plus pending your use case. Docker UCP also brings with it a web UI for administration. So, if container schedulers seem daunting, this could be a perfect solution for you.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec43"/>The Docker UCP architecture</h2></div></div></div><p>So, in this <a id="id221" class="indexterm"/>example, we are going to build three nodes. The first will be our UCP controller and the other two nodes will be UCP HA replicas that give the design some fault tolerance. As UCP is a wrapped product, I am not going to go into all the moving parts.</p><p>Refer to the following diagram to get a visualization of the main components:</p><div><img src="img/B05201_07_24.jpg" alt="The Docker UCP architecture"/></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec44"/>Coding</h2></div></div></div><p>We <a id="id222" class="indexterm"/>are going to do something different in this module, just to show you that our modules are portable to any OS. We will build this module with the Ubuntu 14.04 server. Again, for this environment, we are going to create a new Vagrant repo. So, let's Git clone our Vagrant repo (<a class="ulink" href="https://github.com/scotty-c/vagrant-template.git">https://github.com/scotty-c/vagrant-template.git</a>). As in the last topic, we will look at the plumbing first, before we write our module. The first thing we are going to do is create a file called <code class="literal">config.json</code>. This will have your Docker Hub auth in the file:</p><p> </p><div><img src="img/B05201_07_25.jpg" alt="Coding"/></div><p>The next will be <code class="literal">docker_subscription.lic</code>. This will contain your trial license.</p><p>Now, let's look at our <code class="literal">servers.yaml</code> file in the root of our Vagrant repo, which is as follows:</p><div><img src="img/B05201_07_26.jpg" alt="Coding"/></div><p>The main <a id="id223" class="indexterm"/>call out here is that now we are using the <code class="literal">puppetlabs/ubuntu-14.04-64-puppet-enterprise</code> vagrant box. We have changed <code class="literal">yum</code> to <code class="literal">apt-get</code>. Then, we are copying both our <code class="literal">config.json</code> and <code class="literal">docker_subscription.lic</code> files to their correct place on our vagrant box.</p><p>Now, we will look at the changes we need to make to our Puppetfile:</p><div><img src="img/B05201_07_27.jpg" alt="Coding"/></div><p>You will see<a id="id224" class="indexterm"/> that we need a few new modules from the Forge. The Docker module is familiar, as is stdlib. We will need Puppetlab's <code class="literal">apt</code> module to control our repos that Ubuntu will use to pull Docker. The last module is the Puppetlabs module for UCP itself. To find out more information about this module, you can read all about it at <a class="ulink" href="https://forge.puppetlabs.com/puppetlabs/docker_ucp">https://forge.puppetlabs.com/puppetlabs/docker_ucp</a>. We will write a module that wraps this class and configures it for our environment.</p><p>Now, let's look at our Hiera file in <code class="literal">hieradata/global.yaml</code>:</p><div><img src="img/B05201_07_28.jpg" alt="Coding"/></div><p>As you can see, we have added two values. The first is <code class="literal">ucpconfig::ucp_url:</code>, which we will set to our first vagrant box. The next is the value for <code class="literal">ucpconfig::ucp_fingerprint:</code>, which we will leave blank for the moment. But remember it, as we will come back to this later in the topic.</p><p>Now, we will create a module called <code class="literal">&lt;AUTHOR&gt;-ucpconfig</code>. We have done this a few times now, so once you have created a module, create a folder in the root of our Vagrant repo called <code class="literal">modules</code> and move <code class="literal">ucpconfig</code> into that folder.</p><p>We will then create three manifests files in the module's <code class="literal">manifest</code> directory. The first will be <code class="literal">master.pp</code>, the second will be <code class="literal">node.pp</code>, and the last file will be <code class="literal">params.pp</code>.</p><p>Now, let's add our code to the <code class="literal">params.pp</code> file, as follows:</p><div><img src="img/B05201_07_29.jpg" alt="Coding"/></div><p>As you can see, we have four values: <code class="literal">Ucp_url</code>, which comes from Hiera; <code class="literal">ucp_username</code>, whose <a id="id225" class="indexterm"/>default value is set to <code class="literal">admin</code>; we then have <code class="literal">ucp_password</code> whose default value is set to <code class="literal">orca</code>. The last value is <code class="literal">ucp_fingerprint</code>, which again comes from Hiera. Now, in a production environment, I would set both the username and password in Hiera and overwrite the defaults, which we have set in <code class="literal">params.pp</code>. In this case, as it is a test lab, we will just use the defaults.</p><p>The next file we will look at is our <code class="literal">init.pp</code> file, which is as follows:</p><div><img src="img/B05201_07_30.jpg" alt="Coding"/></div><p>You can see<a id="id226" class="indexterm"/> that at the top of the class, we are mapping our <code class="literal">params.pp</code> file. The next declaration installs the <code class="literal">docker</code> class and sets the <code class="literal">socket_bind</code> parameter for the daemon. Now, the next bit of logic defines whether the node is a master or a node depending on the hostname. As you can see, we are only setting <code class="literal">ucp-01</code> as our master.</p><p>Now, let's look at <code class="literal">master.pp</code>:</p><div><img src="img/B05201_07_31.jpg" alt="Coding"/></div><p>In this class, we<a id="id227" class="indexterm"/> have the logic to install the UCP controller or master. At the top of the class, we are mapping our parameters to our <code class="literal">init.pp</code> file. The next block of code calls the <code class="literal">docker_ucp</code> class. As you can see, we are setting the value of controller to <code class="literal">true</code>, the host address to our second interface, the alternate name of the cluster to our first interface, and the version to <code class="literal">1.0.1</code> (which is the latest at the time of writing this book). We will then set the ports for both the controller and Swarm. Then, we will tell UCP about the Docker socket location and also the location of the license file.</p><p>Now, let's look at our last file, <code class="literal">node.pp</code>:</p><div><img src="img/B05201_07_32.jpg" alt="Coding"/></div><p>As you can see, most of the settings might look familiar. The call out for a node is that we need to point it to <a id="id228" class="indexterm"/>the controller URL (which we set in Hiera). We will get to know about the admin username and password and the cluster fingerprint just a little bit later. So, that completes our module. We now need to add our class to our nodes which we will do by adding the <code class="literal">default.pp</code> manifest file in the <code class="literal">manifests/default.pp</code> location from the root of our Vagrant repo, as follows:</p><div><img src="img/B05201_07_38.jpg" alt="Coding"/></div><p>Let's go to our <a id="id229" class="indexterm"/>terminal and change the directory to the root of our Vagrant repo. This time, we are going to do something different. We are going to issue the <code class="literal">vagrant up ucp-01</code> command. This will bring up only the first node. We do this as we need to get the fingerprint that is generated as UCP comes up.</p><p>Our terminal output should look like that shown in the following screenshot:</p><div><img src="img/B05201_07_33.jpg" alt="Coding"/><div><p>Terminal output</p></div></div><p>You will note <a id="id230" class="indexterm"/>that the fingerprint has been displayed on your terminal output. For my example, the fingerprint is <code class="literal">INFO[0031] UCP Server SSL: SHA1 Fingerprint=C2:7C:BB:C8:CF:26:59:0F:DB:BB:11:BC:02:18:C4:A4:18:C4:05:4E</code>. So, we will add this to our Hiera file, which is <code class="literal">global.yaml</code>:</p><div><img src="img/B05201_07_34.jpg" alt="Coding"/></div><p>Now that we have our first node up, we should be able to log in to the web UI. We do this in our browser. We will go to <code class="literal">https:127.0.0.1:8443</code> and get the login page as follows:</p><div><img src="img/B05201_07_35.jpg" alt="Coding"/></div><p>We will then add the<a id="id231" class="indexterm"/> username and password that we set in our <code class="literal">params.pp</code> file:</p><div><img src="img/B05201_07_36.jpg" alt="Coding"/></div><p>Then, after we <a id="id232" class="indexterm"/>log in, you can see that we have a health cluster, as follows:</p><div><img src="img/B05201_07_37.jpg" alt="Coding"/><div><p>Health cluster after logging in</p></div></div><p>Now, let's return <a id="id233" class="indexterm"/>to our terminal and issue the <code class="literal">vagrant up ucp-02 &amp;&amp; vagrant up ucp-03</code> command.</p><p>Once that is complete, if we look at our web UI, we can see that we have three nodes in our cluster, which are as follows:</p><div><img src="img/B05201_07_39.jpg" alt="Coding"/></div><p>In this book, we <a id="id234" class="indexterm"/>are not going to go into how to administer the cluster through the web UI. I would definitely recommend that you explore this product, as it has some really cool features. All the documentation is available at <a class="ulink" href="https://docs.docker.com/ucp/overview/">https://docs.docker.com/ucp/overview/</a>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec32"/>Kubernetes</h1></div></div></div><p>There is massive <a id="id235" class="indexterm"/>buzz around Kubernetes at the moment. This is Google's offering to the container world. Kubernetes has some very heavy backers such as Google, CoreOS, and Netflix. Out of all the schedulers that we have looked at, Kubernetes is the most complex and is highly driven by APIs. If you are new to Kubernetes, I would <a id="id236" class="indexterm"/>suggest that you read further about the product (<a class="ulink" href="http://kubernetes.io/">http://kubernetes.io/</a>). We are going to first look at the architecture of Kubernetes, as there are a few moving parts. Then, we will write our module, and we are going to build Kubernetes completely with containers.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec45"/>The architecture</h2></div></div></div><p>We will <a id="id237" class="indexterm"/>be building Kubernetes on a single node. The reason for this is that it will cut out some of the complexity on how to use flannel over the Docker bridge. This module will give you a good understanding of how Kubernetes works using more advanced Puppet techniques. If you want to take your knowledge further after you have mastered this chapter, I would recommend that you check out the module on the forge at <a class="ulink" href="https://forge.puppetlabs.com/garethr/kubernetes">https://forge.puppetlabs.com/garethr/kubernetes</a>. This module really <a id="id238" class="indexterm"/>takes Puppet and Kubernetes to the next level.</p><p>So what we are going to code looks like the following diagram:</p><div><img src="img/B05201_07_40.jpg" alt="The architecture"/></div><p>As you can see, we have a container that runs <a id="id239" class="indexterm"/>
<strong>etcd</strong> (to read more about etcd, go to <a class="ulink" href="https://coreos.com/etcd/docs/latest/">https://coreos.com/etcd/docs/latest/</a>). etcd is similar to Consul, which <a id="id240" class="indexterm"/>we are familiar with. In the next few containers, we are going to use hyperkube (<a class="ulink" href="https://godoc.org/k8s.io/kubernetes/pkg/hyperkube">https://godoc.org/k8s.io/kubernetes/pkg/hyperkube</a>). This will load balance the required Kubernetes components across multiple containers for us. It seems pretty easy, right? Let's get into the code so that we get a better perspective on all the moving parts.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec46"/>Coding</h2></div></div></div><p>We are going<a id="id241" class="indexterm"/> to create a new Vagrant repo again. We won't cover how to do that as we have covered it twice in this chapter. If you are unsure, just take a look at the initial part of this chapter.</p><p>Once we have created our Vagrant repo, let's open our <code class="literal">servers.yaml</code> file, as follows:</p><div><img src="img/B05201_07_41.jpg" alt="Coding"/></div><p>As you can <a id="id242" class="indexterm"/>see, there is nothing special there that we have not covered in this book. There's just a single node that we mentioned earlier, <code class="literal">kubernetes</code>. The next file we will look at is our Puppetfile. We will, of course, need our Docker module, <code class="literal">stdlib</code>, and lastly, <code class="literal">wget</code>. We need <code class="literal">wget</code> to get <code class="literal">kubectl</code>:</p><div><img src="img/B05201_07_42.jpg" alt="Coding"/></div><p>That is all the plumbing that we need to set up for our repo. Let's create a new module called <code class="literal">&lt;AUTHOR&gt;-kubernetes_docker</code>. Once it is created, we will move our module to the <code class="literal">modules</code> directory in the root of our Vagrant repo.</p><p>We are going to create two new folders in our module. The first will be the <code class="literal">templates</code> folder, and the other folder will be the <code class="literal">lib</code> directory. We will get to the <code class="literal">lib</code> directory toward the end of our coding. The first file we will create and edit is <code class="literal">docker-compose.yml.erb</code>. The reason for this is that it is the foundation of our module. We will add the following code to it:</p><div><img src="img/B05201_07_43.jpg" alt="Coding"/></div><p>Let's break <a id="id243" class="indexterm"/>this file down into three chunks, as there is a lot going on there. The first block of code is going to set up our etcd cluster. You can see from the screenshot name that we are using Google's official images, and we are using etcd version 2.2.1. We are giving the container access to the host network. Then, in the command resource, we pass some arguments to etcd as it starts.</p><p>The next container we create is hyperkube. Again, it is an official Google image. Now, we are giving this container access to a lot of host volumes, the host network, and the host process, making the container privileged. This is because the first container will bootstrap Kubernetes and it will spawn more containers running the various Kubernertes components. Now, in the command resource, we are again passing some arguments for hyperkube. The two major ones we need to worry about are the API server address and config manifests. You will note that we have a mapped folder from <code class="literal">/kubeconfig:/etc/kubernetes/manifests:ro</code>. We are going to modify our manifest file to make our Kubernetes environment available to the outside world. We will get to that next. But, we will finish looking at the code in this file first.</p><p>The last container and the third block of code is going to set up our service proxy. We are going to give this container access to the host network and process. In the command resource, we are going to specify that this container is a proxy. The next thing to take notice of is that we specify where the proxy can find the API. Now, let's create the next file, <code class="literal">master.json.erb</code>. This is the file that hyperkube will use to schedule all the Kubernetes components, which are as follows:</p><div><img src="img/B05201_07_44.jpg" alt="Coding"/></div><p>As you <a id="id244" class="indexterm"/>can see, we have defined three more containers. This is the first time we will define a Kubernetes pod (<a class="ulink" href="http://kubernetes.io/docs/user-guide/pods/">http://kubernetes.io/docs/user-guide/pods/</a>). A <a id="id245" class="indexterm"/>pod is a group of containers that creates an application. It is <a id="id246" class="indexterm"/>similar to what we have done with Docker Compose. As you can see, we have changed all the IP addresses to the <code class="literal">&lt;%= @master_ip %&gt;</code> parameter. We will create four new files: <code class="literal">apps.pp</code>, <code class="literal">config.pp</code>, <code class="literal">install.pp</code>, and <code class="literal">params.pp</code>.</p><p>We will now move on to our files in the <code class="literal">modules</code> manifest directory. Now, strap yourselves in, as this is where the magic happens. Well, that's not true. The magic happens here and in our <code class="literal">lib</code> directory. We will need to write some custom types and providers for Puppet to be able to control Kubernertes as <code class="literal">kubectl</code> is the user interface (for types, visit <a class="ulink" href="https://docs.puppetlabs.com/guides/custom_types.html">https://docs.puppetlabs.com/guides/custom_types.html</a>, and for providers, visit <a class="ulink" href="https://docs.puppetlabs.com/guides/provider_development.html">https://docs.puppetlabs.com/guides/provider_development.html</a>).</p><p>Let's start with our <code class="literal">init,pp</code> file, which is as follows:</p><div><img src="img/B05201_07_45.jpg" alt="Coding"/></div><p>As you can see, there<a id="id247" class="indexterm"/> is not much in this file. We are going to use our <code class="literal">init.pp</code> file to control the order in which classes are executed. We are also declaring <code class="literal">param &lt;%= @master_ip %&gt;</code>. We will now move on to our <code class="literal">install.pp</code> file, as follows:</p><div><img src="img/B05201_07_46.jpg" alt="Coding"/></div><p>In this file, we <a id="id248" class="indexterm"/>install Docker as we did before. We will place our two templates that we created earlier. Then, we will run Docker Compose to bring up our cluster. Now, we will move on to <code class="literal">config.pp</code>, as follows:</p><div><img src="img/B05201_07_47.jpg" alt="Coding"/></div><p>The first thing that we declare is that we want <code class="literal">wget</code> to be <a id="id249" class="indexterm"/>our <code class="literal">kubectl</code> client, and we place it at <code class="literal">/usr/bin/</code> (<a class="ulink" href="http://kubernetes.io/docs/user-guide/kubectl/kubectl/">http://kubernetes.io/docs/user-guide/kubectl/kubectl/</a>). You really need to understand what this interface does; otherwise, you might get a bit lost from here. So, I suggest that you have a fairly good idea of what <a id="id250" class="indexterm"/>kubectl is and what it is capable of. Next, we will make it executable and available for all our users. Now, this last piece of code does not make sense, as we have not called the <code class="literal">kubectl_config</code> class yet:</p><div><img src="img/B05201_07_48.jpg" alt="Coding"/></div><p>We now need to jump to our <code class="literal">lib</code> directory. This first thing we will do is create all our folders that we need. The first folder that we will create is <code class="literal">puppet</code> under the <code class="literal">lib</code> directory. We will look at our custom types first. We will create a folder called <code class="literal">type</code> under <code class="literal">puppet</code>. The following screenshot will give you a visualization of the structure:</p><div><img src="img/B05201_07_49.jpg" alt="Coding"/></div><p>Under the <code class="literal">type</code> folder, we <a id="id251" class="indexterm"/>will create a file called <code class="literal">kubectl_config.rb</code>. In that file, we will add the new parameters of <code class="literal">type</code> as follows:</p><div><img src="img/B05201_07_50.jpg" alt="Coding"/></div><p>Let me explain what is happening here. In the first line, we are going to declare our new type, <code class="literal">kubectl_config</code>. We are then going to set the default value of the new type when it is declared as <code class="literal">present</code>. We are going to declare three values to our type, <code class="literal">name</code>, <code class="literal">cluster</code>, and <code class="literal">kube_context</code>. These <a id="id252" class="indexterm"/>are all settings that we will add to our <code class="literal">config</code> file that <code class="literal">kubectl</code> will use when we interface with it. Now, we will create a folder under the <code class="literal">lib</code> directory called <code class="literal">provider</code>. Then, under that, we will create a folder with the same name as our custom type, <code class="literal">kubectl_config</code>. Inside that folder, we will create a file called <code class="literal">ruby.rb</code>. In this file, we will put the Ruby code that provides logic to our type as follows:</p><div><img src="img/B05201_07_51.jpg" alt="Coding"/></div><p>A provider needs to have three methods for Puppet to be able to run the code. They are <code class="literal">exsists?</code>, <code class="literal">create</code>, and <code class="literal">destroy</code>. These are pretty easy to understand. The <code class="literal">exists?</code> method checks whether the type has already been executed by Puppet, <code class="literal">create</code> runs the type, and <code class="literal">destroy</code> is invoked when the type is set to <code class="literal">absent</code>.</p><p>We are now going to work through the file, from top to bottom. We need to first load a few Ruby libraries for some of our methods. We will then tie this provider to our type. The next thing that we need to declare is the <code class="literal">kubectl</code> executable.</p><p>Now we will <a id="id253" class="indexterm"/>write our first method, <code class="literal">interface</code>. This will get the IP address from the hostname of the box. We will then create three more methods. We will also create an array and add all our configuration to them. You will note that we are mapping our parameters from our type in the arrays.</p><p>In our <code class="literal">exists?</code> method, we will check for our <code class="literal">kubectl</code> config file.</p><p>In our <code class="literal">create</code> method, we are calling our <code class="literal">kubectl</code> executable and then passing our arrays as arguments. We will then link our <code class="literal">config</code> file to roots' home directory (this is fine for our lab. In a production environment, I would use a named user account).</p><p>Lastly, we will remove the <code class="literal">config</code> file if the type is set to <code class="literal">absent</code>. We will now go back to our <code class="literal">manifests</code> directory and look at our last file, which is <code class="literal">apps.pp</code>:</p><div><img src="img/B05201_07_52.jpg" alt="Coding"/></div><p>In this file, we are going to run a container application on our Kubernetes cluster. Again, we will write another custom type and provider. Before we get to that, we should look at the code in this class. As you can see, our type is called <code class="literal">kubernetes_run</code>. We can see that our service is named <code class="literal">nginx</code>, the Docker image we will pull is <code class="literal">nginx</code>, and we will then expose port <code class="literal">80</code>.</p><p>Let's go back to our <code class="literal">lib</code> directory. We will then create a file in our <code class="literal">type</code> folder called <code class="literal">kubernetes_run.rb</code>. In this file, we will set up our custom type as we did earlier:</p><div><img src="img/B05201_07_53.jpg" alt="Coding"/></div><p>As you can see, we <a id="id254" class="indexterm"/>are mapping the same parameters that we had in our <code class="literal">apps.pp</code> file. We will then create a folder under the <code class="literal">provider</code> folder with the same name as our <code class="literal">kubernetes_run</code> type. Again, in the newly created directory, we will create a file called <code class="literal">ruby.rb</code>. It will have the code shown in the following screenshot:</p><div><img src="img/B05201_07_54.jpg" alt="Coding"/></div><p>In this file, we <a id="id255" class="indexterm"/>are going to add two commands this time: the first is <code class="literal">kubectl</code> and the second is <code class="literal">docker</code>. We will create two methods, again with arrays that map the values from our type.</p><p>Now, let's look at our <code class="literal">exists?</code> method. We are going to pass an array as an argument to <code class="literal">kubectl</code> to check whether the service exists. We are then going to catch the error if <code class="literal">kubectl</code> throws an error with the request and returns <code class="literal">false</code>. This is used if there are no services deployed on the cluster.</p><p>In our <code class="literal">create</code> method, we will first pass an array to <code class="literal">kubectl</code> to get the nodes in the cluster. We are using this as an arbitrary command to make sure that the cluster is up. Under that, we will capture the error and retry the command until it is successful. Once it is successful, we will deploy our container with the <code class="literal">ensure</code> resource.</p><p>In the <code class="literal">destroy</code> method, we will use <code class="literal">docker</code> to remove our container.</p><p>Now we have all our coding done. We just need to add our class to our node by editing our <code class="literal">default.pp</code> file in the <code class="literal">manifests</code> folder in the root of our Vagrant repo as follows:</p><div><img src="img/B05201_07_55.jpg" alt="Coding"/></div><p>Now, let's open our <a id="id256" class="indexterm"/>terminal and change the directory to the root of our Vagrant repo and issue the <code class="literal">vagrant up</code> command. Once Puppet has completed its run, our terminal should look like the following screenshot:</p><div><img src="img/B05201_07_56.jpg" alt="Coding"/></div><p>We will now log in to our vagrant box by issuing the <code class="literal">vagrant ssh</code> command and then <code class="literal">sudo -i</code> and change to root. Once we are root, we will look at our service on our cluster. We will do this by issuing the <code class="literal">kubectl get svc</code> command as follows:</p><div><img src="img/B05201_07_57.jpg" alt="Coding"/></div><p>As you can see, we <a id="id257" class="indexterm"/>have two services running our cluster: <code class="literal">Kubernetes</code> and <code class="literal">nginx</code>. If we go to our web browser and go to the address we gave to the second network interface, <code class="literal">http://172.17.9.101</code>, we will get the following nginx default page:</p><div><img src="img/B05201_07_58.jpg" alt="Coding"/></div><p>Now, our cluster is running successfully with our <code class="literal">nginx</code> service.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec33"/>Summary</h1></div></div></div><p>We covered my favorite three container schedulers; each one of them has their own pros and cons. Now, you have the knowledge and the required code to give all three a really good test run. I would suggest that you do so so that you can make the right choice when choosing the design in your environment.</p></div></body></html>