- en: Creating and Managing a Docker Swarm Cluster in Amazon Web Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In fast moving markets, adaptation is significantly more important than optimization.
  prefs: []
  type: TYPE_NORMAL
- en: –Larry Constantine
  prefs: []
  type: TYPE_NORMAL
- en: 'The time has finally come to set up a Swarm cluster that is closer to what
    we should have in production. The reason I added the word "closer" lies in a few
    subjects that we''ll explore in later chapters. Later on, once we go through a
    few hosting providers, we''ll work on the missing pieces (example: persistent
    storage).'
  prefs: []
  type: TYPE_NORMAL
- en: For now, we'll limit ourselves to the creation of a production-like cluster
    and exploration of different tools we can choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Since AWS holds by far the biggest share of the hosting market, it is the natural
    choice as the first provider we'll explore.
  prefs: []
  type: TYPE_NORMAL
- en: I'm sure that AWS does not need much of an introduction. Even if you haven't
    used it, I'm sure you are aware of its existence and a general gist.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) were created in 2006 with the offering of
    IT infrastructure services. The type of services AWS offered later on became commonly
    known as cloud computing. With the cloud, companies and individuals alike no longer
    need to plan for and procure servers and other IT infrastructure weeks or months
    in advance. Instead, they can instantly spin up hundreds or even thousands of
    servers in minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: I will assume that you already have an AWS account. If that's not the case,
    please head over to Amazon Web Services and sign-up. Even if you already made
    up a firm decision to use a different cloud computing provider or in-house servers,
    I highly recommend going through this chapter. You will be introduced to a few
    tools you might not have in your tool belt and be able to compare AWS with other
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into practical exercises, we'll need to install the AWS CLI,
    get the access keys and decide the region and the zone where we'll run the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing AWS CLI and setting up the environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we should do is get the AWS credentials.
  prefs: []
  type: TYPE_NORMAL
- en: Please open the *Amazon EC2 Console *([https://console.aws.amazon.com/ec2/](https://console.aws.amazon.com/ec2/)),
    click on your name from the top-right menu, and select My Security Credentials.
    You will see the screen with different types of credentials. Expand the Access
    Keys *(Access Key ID* and *Secret Access Key)* section and click the Create New
    Access Key button. Expand the Show Access Key section to see the keys.
  prefs: []
  type: TYPE_NORMAL
- en: You will not be able to view the keys later on, so this is the only chance you'll
    have to Download Key File.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `11-aws.sh` ([https://gist.github.com/vfarcic/03931d011324431f211c4523941979f8](https://gist.github.com/vfarcic/03931d011324431f211c4523941979f8))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll place the keys as environment variables that will be used by the tools
    we''ll explore in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: We'll need to install AWS **Command Line Interface** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))and
    gather info about your account.
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: I found the most convienent way to get awscli installed on Windows is to use
    *Chocolatey *([https://chocolatey.org/](https://chocolatey.org/)). Download and
    install Chocolatey, then run `choco install awscli` from an Administrator Command
    Prompt. Later in the chapter, Chocolatey will be used to `install jq`, `packer`,
    and terraform.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't already, please open the *Installing the AWS Command Line Interface *([http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html))
    page and follow the installation method best suited for your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''re done, we should confirm that the installation was successful by
    outputting the version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output (from my laptop) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: You might need to reopen your *GitBash* terminal for the changes to the environment
    variable `path` to take effect.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the CLI is installed, we can get the region and the availability zones
    our cluster will run in.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EC2 is hosted in multiple locations worldwide. These locations are composed
    of regions and availability zones. Each region is a separate geographic area composed
    of multiple isolated locations known as availability zones. Amazon EC2 provides
    you the ability to place resources, such as instances, and data in multiple locations.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the currently available regions from the *Available Regions* ([http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions))
    section of the *Regions and Availability Zones* ([http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html))
    page.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, I will be using `us-east-1` (US East (N. Virginia))
    region. Feel free to change it to the region closest to your location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the region inside the environment variable `AWS_DEFAULT_REGION`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the region selected, we can decide which availability zone we'll select
    to run our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Each region is completely independent and consists of multiple availability
    zones. Zones within a region are isolated and connected through low-latency links.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, you should place all your nodes of a cluster inside one region
    and benefit from low-latency links. Those nodes should be distributed across multiple
    availability zones so that failure of one of them does not imply failure of the
    whole cluster. If you have a need to operate across multiple regions, the best
    option is to set up multiple clusters (one for each region). Otherwise, if you’d
    set up a single cluster that spans multiple regions, you might experience latency
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to first time AWS users**'
  prefs: []
  type: TYPE_NORMAL
- en: If this is the first time you are executing `aws`, you will receive a message
    asking you to configure credentials. Please run the `aws configure` command and
    follow the instructions. You will be asked for credentials. Use those we generated
    earlier. Feel free to answer the rest of the questions with the enter key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the AWS CLI to see the available zones within the selected region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since I selected `us-east-1` as the region, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are four zones (`a`, `b`, `d`, and `e`) available for
    the `us-east-1` region. In your case, depending on the selected region, the output
    might be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please choose the zones and put them inside environment variables, one for
    each of the five servers that will form our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to choose any combination of availability zones. In my case, I made
    a decision to distribute the cluster across zones `b`, `d`, and `e`.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are all set with the prerequisites that will allow us to create the first
    Swarm cluster in AWS. Since we used Docker Machine throughout the most of the
    book, it will be our first choice.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster with Docker Machine and AWS CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll continue using the `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository. It contains configurations and scripts that''ll help us out. You already
    have it cloned. To be on the safe side, we''ll pull the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the first EC2 instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We specified that the *Docker Machine* should use the `amazonec2` driver to
    create an instance in the zone we defined as the environment variable `AWS_ZONE_1`.
  prefs: []
  type: TYPE_NORMAL
- en: We made a tag with the key `type` and the value manager. Tag are mostly for
    informational purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we specified the name of the instance to be `swarm-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Docker machine launched an AWS EC2 instance, provisioned it with Ubuntu, and
    installed and configured Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can initialize the cluster. We should use private IPs for all communication
    between nodes. Unfortunately, the command `docker-machine ip` returns only the
    public IP, so we'll have to resort to a different method to get the private IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `aws ec2 describe-instances` command to retrieve all the information
    about our EC2 instances. We also filter only running instances by adding `Name=instance-state-name`, `Values=running`.
    Doing so excludes instances that are being terminated or that are terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `describe-instances` command lists all the EC2 instances. We combined it
    with `--filter` to limit the output to the instance tagged with the name `swarm-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relevant sample of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though we got all the information related to the `swarm-1` EC2 instance,
    we still need to limit the output to the `PrivateIpAddress` value. We''ll use
    `jq` ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))to filter
    the output and get what we need. Please download and install the distribution
    suited for your OS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: Using Chocolatey, install `jq` from an Administrator Command Prompt via `choco
    install jq`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We used `jq` to retrieve the first element of the Reservations array. Within
    it, we got the first entry of the Instances, followed with the `PrivateIpAddress`.
    The `-r` returns the value in its raw format (in this case without double quotes
    that surround the IP). The result of the command is stored in the environment
    variable `MANAGER_IP`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be on the safe side, we can echo the value of the newly created variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can execute the `swarm init` command in the same way as we did in the
    previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that the cluster is indeed initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Apart from creating an EC2 instance, `docker-machine` created a security group
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: A security group acts as a virtual firewall that controls the traffic. When
    you launch an instance, you associate one or more security groups with the instance.
    You add rules to each security group that allows traffic to or from its associated
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, Docker Machine was not yet adapted to support Swarm
    Mode out of the box. As a result, it created an AWS security group named `docker-machine`
    and opened only the ingress (inbound) ports `22` and `2376`. Egress (output) is
    opened for all ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Swarm Mode to function correctly, the ingress ports that should be opened
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: TCP port 2377 for cluster management communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP and UDP port 7946 for communication among nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP and UDP port 4789 for overlay network traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To modify the security group, we need to get its ID. We can see the info of
    a security group with the `aws ec2 describe-security-groups` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The command that will assign the ID to the `SECURITY_GROUP_ID` environment
    variable is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We requested the information about the security group `docker-machine` and filtered
    the JSON output to get the `GroupId` key located in the first element of the `SecurityGroups`
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use the `aws ec2 authorize-security-group-ingress` command to open
    TCP ports `2377`, `7946`, and `4789`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We should execute a similar command to open UDP ports `7946` and `4789`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Please note that, in all the cases, we specified that the `source-group` should
    be the same as the security group. That means that the ports will be opened only
    to instances that belong to the same group. In other words, those ports will not
    be available to the public. Since they will be used only for internal communication
    within the cluster, there is no reason to put our security at risk by exposing
    those ports further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please repeat the `aws ec2 describe-security-groups` command to confirm that
    the ports were indeed opened:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can add more nodes to the cluster. We''ll start by creating two new
    instances and joining them as managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: There's no need to explain the commands we just executed since they are the
    combination of those we used before.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll add a few worker nodes as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that all five nodes are indeed forming the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Our cluster is ready. The only thing left is to deploy a few services
    and confirm that the cluster is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already created the services quite a few times, we''ll speed up the
    process with the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    Compose stacks. They''ll create the `proxy`, `swarm-listener`, `go-demo-db`, and
    `go-demo` services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Non-Windows users do not need to enter the `swarm-1` machine and can accomplish
    the same result by deploying the stacks directly from their laptops.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''ll take a few moments until all the images are downloaded. After a while,
    the output of the `service ls` command should be as follows (IDs are removed for
    brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that the `go-demo` service is accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: That's embarrassing. Even though all the services are running and we used the
    same commands as in the previous chapters, we cannot access the `proxy` and, through
    it, the `go-demo` service.
  prefs: []
  type: TYPE_NORMAL
- en: The explanation is quite simple. We never opened ports `80` and `443`. By default,
    all incoming traffic to AWS EC2 instances is closed, and we opened only the ports
    required for Swarm to operate properly. They are open inside EC2 instances attached
    to the `docker-machine security` group, but not outside our AWS VPC.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `aws ec2 authorize-security-group-ingress` command to open the
    ports `80` and `443`. This time we''ll specify `cidr` instead `source-group` as
    the source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `aws ec2 authorize-security-group-ingress` command was executed twice; once
    for port `80` and the second time for `443`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s send the request one more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This time the output is as expected. We got our response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We set up the whole Swarm cluster in AWS using Docker Machine and AWS CLI. Is
    that everything we need? That depends on the requirements we might define for
    our cluster. We should probably add a few Elastic IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: An Elastic IP address is a static IP address designed for dynamic cloud computing.
    It is associated with your AWS account. With an Elastic IP address, you can mask
    the failure of an instance or software by rapidly remapping the address to another
    instance in your account. An Elastic IP address is a public IP address which is
    reachable from the Internet. If your instance does not have a public IP address,
    you can associate an Elastic IP address with it to enable communication with the
    Internet; for example, to connect to your instance from your local computer.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we should probably set at least two Elastic IPs and map them
    to two of the EC2 instances in the cluster. Those two (or more) IPs would be set
    as our DNS records. That way, when an instance fails, and we replace it with a
    new one, we can remap the Elastic IP without affecting our users.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few other improvements we could do. However, that would put
    us in an awkward position. We would be using a tool that is not meant for setting
    up a complicated cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The creation of VMs was quite slow. Docker Machine spent too much time provisioning
    it with Ubuntu and installing Docker Engine. We can reduce that time by creating
    our own **Amazon Machine Image** (**AMI**) with Docker Engine pre-installed. However,
    with such an action, the main reason for using Docker Machine would be gone. Its
    primary usefulness is simplicity. Once we start complicating the setup with other
    AWS resources, we'll realize that the simplicity is being replaced with too many
    ad-hoc commands.
  prefs: []
  type: TYPE_NORMAL
- en: Running `docker-machine` and `aws` commands works great when we are dealing
    with a small cluster, especially when we want to create something fast and potentially
    not very durable. The biggest problem is that everything we've done so far has
    been ad-hoc commands. Chances are that we would not be able to reproduce the same
    steps the second time. Our infrastructure is not documented, so our team would
    not know what constitutes our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: My recommendation is to use `docker-machine` and `aws` as a quick and dirty
    way to create a cluster mostly for demo purposes. It can be useful for production
    as well, as long as the cluster is relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: We should look at alternatives if we'd like to set up a complicated, bigger,
    and potentially more permanent solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us delete the cluster we created and explore the alternatives with a clean
    slate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing left is to remove the security group `docker-machine` created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The last command might fail if the instances are not yet terminated. If that's
    the case, please wait a few moments and repeat the command.
  prefs: []
  type: TYPE_NORMAL
- en: Let us move on and explore *Docker for AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster with Docker for AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we create a Swarm cluster using *Docker for AWS*, we'll need to generate
    a Key Pair that we'll use to SSH into the EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new `key-pair`, please execute the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We executed `aws ec2 create-key-pair` command and passed `devops21` as the name.
    The output was filtered with `jq` so that only the actual value is returned. Finally,
    we sent the output to the `devops21.pem` file.
  prefs: []
  type: TYPE_NORMAL
- en: If someone gets a hold of your key file, your instances would be exposed. Therefore,
    we should move the key somewhere safe.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common location for SSH keys on Linux/OSX systems is `$HOME/.ssh`. If you
    are a Windows user, feel free to change the command that follows to any destination
    you think is appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We should also change permissions by giving the current user only the read
    access and removing all permissions from other users or groups. Feel free to skip
    the command that follows if you are a Windows user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll put the path to the key inside the environment variable `KEY_PATH`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to create a Swarm stack using *Docker for AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: Please open the *Docker for AWS Release Notes *([https://docs.docker.com/docker-for-aws/release-notes/](https://docs.docker.com/docker-for-aws/release-notes/))
    page and click the Deploy Docker Community Edition for AWS button.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you log into the *AWS Console*, you will be presented with the Select
    Template screen. It is a generic CloudFormation screen with the Docker for AWS
    template already selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-select-template.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-1: Docker For AWS Select Template screen'
  prefs: []
  type: TYPE_NORMAL
- en: There's not much to do here, so please click the Next button.
  prefs: []
  type: TYPE_NORMAL
- en: The next screen allows us to specify details of the stack we are about to launch.
    The fields should be self-explanatory. The only modification we'll make is the
    reduction of Swarm workers from *5* to *1*. The exercises in this section won't
    need more than four servers, so three managers and one worker should suffice.
    We'll leave instance types with their default values `t2.micro`. By creating only
    four micro nodes, the whole exercise should have almost negligible cost, and you
    won't be able to complain to your friends that you went bankrupt because of me.
    The total cost will probably be smaller than the price of a can of soda or a cup
    of coffee you're drinking while reading this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Which SSH key to use? field should hold the `devops21` key we just created.
    Please select it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-specify-details.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-2: Docker For AWS Specify Details screen'
  prefs: []
  type: TYPE_NORMAL
- en: Click the Next button.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t change anything from the Options screen. Later on, when you get comfortable
    with *Docker for AWS*, you might want to go back to this screen and fiddle with
    the additional options. For now, we''ll just ignore its existence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-options.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-3: Docker For AWS Options screen'
  prefs: []
  type: TYPE_NORMAL
- en: Click the Next button.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reached the last screen. Feel free to review the information about the stack.
    Once you''re done, click the I acknowledge that AWS CloudFormation might create
    IAM resources*.* checkbox followed by the Create button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-review.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-4: Docker For AWS Review Screen'
  prefs: []
  type: TYPE_NORMAL
- en: You will see the screen that allows you to create a new stack. Please click
    the refresh button located in the top-right corner. You'll see the *Docker* stack
    with the status `CREATE_IN_PROGRESS`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take a while until all the resources are created. If you''d like to
    see the progress, please select the *Docker* stack and click the restore button
    located in the bottom-right part of the screen. You''ll see the list of all the
    events generated by the *Docker for AWS* template. Feel free to explore the content
    of the tabs while waiting for the stack creation to finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-stack-status.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-5: Docker For AWS stack status screen'
  prefs: []
  type: TYPE_NORMAL
- en: We can proceed once the *Docker* stack status is `CREATE_COMPLETE`.
  prefs: []
  type: TYPE_NORMAL
- en: Our cluster is ready. We can enter one of the manager instances and explore
    the cluster in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find information about Swarm manager instances, please click the Outputs
    tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-stack-outputs.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-6: Docker For AWS Stack outputs screen'
  prefs: []
  type: TYPE_NORMAL
- en: You'll see two rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll store the value of the DefaultDNSTarget in an environment variable.
    It''ll come in handy soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual DefaultDNSTarget value.
  prefs: []
  type: TYPE_NORMAL
- en: If this were a "real" production cluster, you'd use it to update your DNS entries.
    It is the public entry to your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the link next to the Managers column. You will be presented with the
    EC2 Instances screen that contains the results filtered by manager nodes. The
    workers are hidden:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-managers.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-7: Docker For AWS EC2 instances filtered by manager nodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select one of the managers and find its Public IP. As with the DNS, we''ll
    store it as an environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual Public IP value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are, finally, ready to SSH into one of the managers and explore the cluster
    we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once inside the server, you will be presented with a welcome message. The OS
    specifically made for this stack is very minimalistic, and the message reflects
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we''ll start by listing the nodes that form the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The only thing left is to create a few services that will confirm that the cluster
    works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll deploy the same `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    stacks we used with the cluster we created with Docker Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We downloaded the Compose file and deployed the stacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s confirm that the services are indeed up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'After a while, the output should be as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get out of the server and confirm that the `go-demo` service is accessible
    to the public:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, we received the response confirming that the cluster is operational
    and accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'What happens if our servers become too crowded and we need to expand the capacity?
    How can we increase (or decrease) the number of nodes that form our cluster? The
    answer lies in AWS Auto Scaling Groups. Please click the Auto Scaling Groups link
    from the AUTO SCALING group in the left-hand menu of the EC2 console and select
    the row with the group name that starts with `Docker-NodeAsg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-auto-scaling-groups.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-8: Docker For AWS Auto Scaling Groups'
  prefs: []
  type: TYPE_NORMAL
- en: 'To scale or de-scale the number of nodes, all we have to do is click the Edit
    button from the Actions menu, change the value of the Desired field from `1` to
    `*2*`, and click the Save button. The number of Desired instances will immediately
    change to `2`. However, it might take a while until the actual number of instances
    aligns with the desire. Let''s go back to one of the manager servers and confirm
    that the desire we expressed is indeed fulfilled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'It might take a while until the new instance is created and joined to the cluster.
    The end result should be as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: What happens in case one of the servers fails? After all, everything fails sooner
    or later. We can test that by removing one of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please click the Instances link from the left-hand menu of the EC2 console,
    select one of the `Docker-worker` nodes, click Actions, and change the Instance
    State to Terminate. Confirm the termination by clicking the Yes*,* Terminate button:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'After a while, the output of the `node ls` command should be as follows (IDs
    are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the auto scaling group realizes that the node is down, it''ll start the
    process of creating a new one and joining it to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Before long, the output of the `node ls` command should be as follows (IDs
    are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The server we terminated is still marked as `Down`, and a new one is created
    and added to the cluster in its place.
  prefs: []
  type: TYPE_NORMAL
- en: There's much more to the *Docker for AWS* stack than we explored. I hope that
    what you learned during this brief exploration gave you enough base information
    to expand the knowledge by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of exploring more details about the stack, we'll see how we can accomplish
    the same result without the UI. By this time you should know me well enough to
    understand that I prefer automatable and reproducible ways of running tasks. I
    broke my "no UIs unless necessary" rule only to give you a better understanding
    how the *Docker for AWS* stack works. A fully automated way to do the same is
    coming up next.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, we'll delete the stack and, with it, remove the cluster.
    That will be the last time you'll see any UI in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please click the Services link from the top menu, followed by the CloudFormation
    link. Select the Docker stack, and click the Delete Stack option from the Actions
    menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/docker-for-aws-delete-stack.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-9: Docker For AWS Delete Stack screen'
  prefs: []
  type: TYPE_NORMAL
- en: Confirm the destruction by clicking the Yes*,* Delete button that appears after
    you click Delete Stack.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically setting up a Swarm cluster with Docker for AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a *Docker for AWS* stack from the UI was a great exercise. It helped
    us understand better how things work. However, our mission is to automate as many
    processes as possible. With automation, we gain speed, reliability, and higher
    quality. When we run some manual tasks, like going through a UI and selecting
    different options, we are increasing the chance that something will go wrong due
    to human error. We are slow. We are much slower than machines when we need to
    execute repeatable steps.
  prefs: []
  type: TYPE_NORMAL
- en: Due to my mistrust in manual operations of repeatable tasks, it's only natural
    to seek a more automated way to create a *Docker for AWS* stack. All we did through
    the AWS console was to fill in a few fields which, in the background, generate
    parameters which are later used to execute a `CloudFormation` process. We can
    do the same without a UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by defining a few environment variables. They will be the same
    as those you already created in this chapter. Feel free to skip the next set of
    commands if you still have the same terminal session open:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Please change `us-east-1` to the region of your choice and replace `[...]` with
    the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall the first screen that allowed us to select a template, you'll
    remember that there was a field with pre-populated URL of a CloudFormation template.
    At the time of this writing, the template is `Docker.tmpl` ([https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl](https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl))
    Please note that the address differs from one region to another. I'll be using
    `us-east-1` edition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inspect the template by retrieving its content with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Please spend some time exploring the output. Even if you are not familiar with
    `CloudFormation` syntax, you should recognize the AWS resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The part of the template that we are most interested in is Metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `ParameterLabels` to customize the result of the template.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that would create the same stack like the one we generated using
    the AWS console is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The command should be self-explanatory. We used `aws` to create a `CloudFormation`
    stack with all the required parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can monitor the status of the stack resources by executing the `cloudformation
    describe-stack-resources` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'After a while, three manager instances should be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Now we can enter one of the managers and start creating services. I'll skip
    the examples that create services and validate that they are working correctly.
    The end-result is the same cluster as the one we created previously with AWS console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to explore the cluster on your own and `delete` the stack once you''re
    finished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: For anything but small clusters, *Docker for AWS* is a much better option than
    using the combination with `docker-machine` and `aws` commands. It is a more robust
    and reliable solution. However, it comes with its downsides.
  prefs: []
  type: TYPE_NORMAL
- en: '*Docker for AWS* is still young and prone to frequent changes. Moreover, it
    is still so new that the documentation is close to non-existent.'
  prefs: []
  type: TYPE_NORMAL
- en: By being an out-of-the-box solution, it is very easy to use and requires little
    effort. That is both a blessing and a curse. It is a good choice if what you need
    is, more or less, what *Docker for AWS* offers. However, if your needs are different,
    you might experience quite a few problems when trying to adapt the template to
    your needs. The solution is based on a custom OS, `CloudFormation` template, and
    containers built specifically for this purpose. Changing anything but the template
    is highly discouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, I think that *Docker for AWS* has a very bright future and is, in most
    cases, a better solution than `docker-machine`. If those two were the only choices,
    my vote would be to use *Docker for AWS*. Fortunately, there are many other options
    we can choose from; much more than could fit into a single chapter. You might
    be reading the printed edition of the book, and I don't feel comfortable sacrificing
    too many trees. Therefore, I'll present only one more set of tools we can use
    to create a Swarm (or any other type of) cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster with Packer and Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This time we'll use a set of tools completely unrelated to Docker. It'll be
    *Packer*([https://www.packer.io/](https://www.packer.io/))and *Terraform *([https://www.terraform.io/](https://www.terraform.io/)).
    Both are coming from *HashiCorp *([https://www.hashicorp.com/](https://www.hashicorp.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: Using Chocolatey, install packer from an administrator command prompt via `choco
    install packer`. For terraform, execute `choco install terraform` in an administrator
    command prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Packer allows us to create machine images. With Terraform we can create, change,
    and improve cluster infrastructure. Both tools support almost all the major providers.
    They can be used with Amazon EC2, CloudStack, DigitalOcean, Google Compute Engine
    (GCE), Microsoft Azure, VMware, VirtualBox, and quite a few others. The ability
    to be infrastructure agnostic allows us to avoid vendor lock-in. With a minimal
    change in configuration, we can easily transfer our cluster from one provider
    to another. Swarm is designed to work seamlessly no matter which hosting provider
    we use, as long as the infrastructure is properly defined. With Packer and Terraform
    we can define infrastructure in such a way that transitioning from one to another
    is as painless as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Using Packer to create Amazon Machine Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository already has the Packer and Terraform configurations we''ll use. They
    are located in the `terraform/aws` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to use Packer to create an **Amazon Machine Image** (**AMI**).
    To do that, we''ll need AWS access keys set as environment variables. They will
    be the same as those you already created in this chapter. Feel free to skip the
    next set of commands if you still have the same terminal session open:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: We'll instantiate all Swarm nodes from the same AMI. It'll be based on Ubuntu
    and have the latest Docker engine installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON definition of the image we are about to build is in `terraform/aws/packer-ubuntu-docker.json` 
     ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/packer-ubuntu-docker.json)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration consists of two sections: `builders` and `provisioners`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The builders section defines all the information Packer needs to build an image.
    The `provisioners` section describes the commands that will be used to install
    and configure software for the machines created by builders. The only required
    section is builders.
  prefs: []
  type: TYPE_NORMAL
- en: Builders are responsible for creating machines and generating images from them
    for various platforms. For example, there are separate builders for EC2, VMware,
    VirtualBox, and so on. Packer comes with many builders by default, and can also
    be extended to add new builders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `builders` section we''ll use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Each type of builder has specific arguments that can be used. We specified that
    the `type` is `amazon-ebs`. Besides `amazon-ebs`, we can also use `amazon-instance`
    and `amazon-chroot` builders for AMIs. In most cases, `amazon-ebs` is what we
    should use. Please visit the *Amazon AMI Builder *([https://www.packer.io/docs/builders/amazon.html](https://www.packer.io/docs/builders/amazon.html))
    page for more info.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, when using `amazon-ebs` type, we have to provide AWS keys.
    We could have specified them through the access_key and secret_key fields. However,
    there is an alternative. If those fields are not specified, Packer will try to
    get the values from environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
    Since we already exported them, there's no need to repeat ourselves by setting
    them inside the Packer configuration. Moreover, those keys should be secret. Putting
    them inside the config would risk exposure.
  prefs: []
  type: TYPE_NORMAL
- en: The region is critical since an AMI can be created only within one region. If
    we wanted to share the same machine across multiple regions, each would need to
    be specified as a separate builder.
  prefs: []
  type: TYPE_NORMAL
- en: We could have specified `source_ami` with the ID of the initial AMI that would
    serve as a base for the newly created machine. However, since AMIs are unique
    to a particular region, that would make it unusable if we decide to change the
    region. Instead, we took a different approach and specified the `source_ami_filter`
    which will populate the s`ource_ami` field. It'll filter AMIs and find an Ubuntu
    `16.04` image with `hvm` virtualization type with Root Device Type set to `ebs`.
    The owners field will limit the results to a trusted AMI provider. Since the filter
    would fail if more than one AMI is returned, the `most_recent` field will limit
    the result by selecting the newest image.
  prefs: []
  type: TYPE_NORMAL
- en: The `instance_type` field defines which EC2 instance type will be used to build
    the AMI. Please note that this will not prevent us from instantiating VMs based
    on this image using any other instance type supported, in this case, by Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other fields we used, the `ssh_username` is not specific to the `amazon-ebs`
    builder. It specifies the user Packer will use while creating the image. Just
    like the instance type, it will not prevent us from specifying any other user
    when instantiating VMs based on this image.
  prefs: []
  type: TYPE_NORMAL
- en: The `ami_name` field is the name we'll give to this AMI.
  prefs: []
  type: TYPE_NORMAL
- en: In case we already created an AMI with the same, the `force_deregister` field
    will remove it before creating the new one.
  prefs: []
  type: TYPE_NORMAL
- en: Please visit the *AMI Builder (EBS Backed)* ([https://www.packer.io/docs/builders/amazon-ebs.html](https://www.packer.io/docs/builders/amazon-ebs.html))
    page for more information.
  prefs: []
  type: TYPE_NORMAL
- en: The second section is provisioners. It contains an array of all the provisioners
    that Packer should use to install and configure software within running machines
    before turning them into Machine Images.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few provisioner types we can use. If you read *The DevOps
    2.0 Toolkit*, you know that I advocated Ansible as the provisioner of choice.
    Should we use it here as well? In most cases, when building images meant to run
    Docker containers, I opt for a simple shell. The reasons for a change from Ansible
    to Shell lies in the different objectives provisioners should fulfill when running
    on live servers, as opposed to building images.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Shell, Ansible (and most other provisioners) are idempotent. They verify
    the actual state and execute one action or another depending on what should be
    done for the desired state to be fulfilled. That's a great approach since we can
    run Ansible as many times as we want and the result will always be the same. For
    example, if we specify that we want to have JDK 8, Ansible will SSH into a destination
    server, discover that the JDK is not present and install it. The next time we
    run it, it'll discover that the JDK is already there and do nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach allows us to run Ansible playbooks as often as we want and
    it'll always result in JDK being installed. If we'd try to accomplish the same
    through a Shell script, we'd need to write lengthy `if/else` statements. If JDK
    is installed, do nothing, if it's not installed, install it, if it's installed,
    but the version is not correct, upgrade it, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: So, why not use it with Packer? The answer is simple. We do not need idempotency
    since we'll run it only once while creating an image. We won't use it on running
    instances. Do you remember the "pets vs cattle" discussion? Our VMs will be instantiated
    from an image that already has everything we need. If the state of that VM changes,
    we'll terminate it and create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: If we need to do an upgrade or install additional software, we won't do it inside
    the running instance, but create a new image, destroy running instances, and instantiate
    new ones based on the updated image.
  prefs: []
  type: TYPE_NORMAL
- en: Is idempotency the only reason we would use Ansible? Definitely not! It is a
    very handy tool when we need to define a complicated server setup. However, in
    our case the setup is simple. We need Docker Engine and not much more. Almost
    everything will be running inside containers. Writing a few Shell commands to
    install Docker is easier and faster than defining Ansible playbooks. It would
    probably take the same number of commands to install Ansible as to install Docker.
  prefs: []
  type: TYPE_NORMAL
- en: To make a Long story short, we'll use `shell` as our provisioner of choice for
    building AMIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `provisioners` section we''ll use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The `shell type` is followed by a set of commands. They are the same as the
    commands we can find in the *Install Docker on Ubuntu* ([https://docs.docker.com/engine/installation/linux/ubuntulinux/](https://docs.docker.com/engine/installation/linux/ubuntulinux/))
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a general idea how Packer configuration works, we can proceed
    and build an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: We ran the `packer` build of the `packer-ubuntu-docker.json` with the `machine-readable`
    output sent to the `packer-ubuntu-docker.log` file. Machine readable output will
    allow us to parse it easily and retrieve the ID of the AMI we just created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the confirmation that the build was successful, the relevant part
    of the output is the line id, `us-east-1:ami-02ebd915`. It contains the AMI ID
    we'll need to instantiate VMs based on the image.
  prefs: []
  type: TYPE_NORMAL
- en: You might want to store the `packer-ubuntu-docker.log` in your code repository
    in case you need to get the ID from a different server.
  prefs: []
  type: TYPE_NORMAL
- en: The flow of the process we executed can be described through *figure 11-10:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cloud-architecture-images.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-10: The flow of the Packer process'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to create a Swarm cluster with VMs based on the image we built.
  prefs: []
  type: TYPE_NORMAL
- en: Using Terraform to create a Swarm cluster in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll start by redefining the environment variables we used with Packer in
    case you start this section in a new terminal session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform does not force us to have any particular file structure. We can define
    everything in a single file. However, that does not mean that we should. Terraform
    configs can get big, and separation of logical sections into separate files is
    often a good idea. In our case, we'll have three `tf` files. The `terraform/aws/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf))
    holds all the variables.
  prefs: []
  type: TYPE_NORMAL
- en: If we need to change any parameter, we'll know where to find it. The `terraform/aws/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf))
    file contains definitions of the elements that might be potentially reusable on
    other occasions. Finally, the `terraform/aws/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf))
    file has the `Swarm-specific` resources.
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore each of the Terraform configuration files separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of the `terraform/aws/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf))
    file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The `swarm_manager_token` and `swarm_worker_token` will be required to join
    the nodes to the cluster. The `swarm_ami_id` will hold the ID of the image we
    created with Packer. The `swarm_manager_ip` variable is the IP of one of the managers
    that we'll need to provide for the nodes to join the cluster. The `swarm_managers`
    and `swarm_workers` define how many nodes we want of each. The `swarm_instance_typ`e
    is the type of the instance we want to create. If defaults to the smallest and
    cheapest (often free) instance. Feel free to change it to a more potent type later
    on if you start using this Terraform config to create a "real" cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '>Finally, the `swarm_init` variable allows us to specify whether this is the
    first run and the node should initialize the cluster. We''ll see its usage very
    soon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of the `terraform/aws/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf))
    file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Each resource is defined with a type (example: `aws_security_group`) and a
    name (example: `docker`). The type determines which resource should be created
    and must be one of those currently supported.'
  prefs: []
  type: TYPE_NORMAL
- en: The first resource `aws_security_group` contains the list of all ingress ports
    that should be opened. Port `22` is required for SSH. Ports `80` and `443` will
    be used for HTTP and HTTPS access to the `proxy`. The rest of the ports will be
    used for Swarms internal communication. TCP port `2377` is for cluster management
    communications, TCP and UDP port `7946` for communication among nodes, and TCP
    and UDP port `4789` for overlay network traffic. Those are the same ports we had
    to open when we created the cluster using Docker Machine. Please note that all
    but ports `22`, `80` and `443` are assigned to self. That means that they will
    be available only to other servers that belong to the same group. Any outside
    access will be blocked.
  prefs: []
  type: TYPE_NORMAL
- en: The last entry in the `aws_security_group` is `egress` allowing communication
    from the cluster to the outside world without any restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult the `AWS_SECURITY_GROUP` ([https://www.terraform.io/docs/providers/aws/d/security_group.html](https://www.terraform.io/docs/providers/aws/d/security_group.html))
    page for more info.
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the "real deal". The `terraform/aws/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf))
    file contains the definition of all the instances we'll create. Since the content
    of this file is a bit bigger than the others, we'll examine each resource separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first resource in line is the `aws_instance` type named `swarm-manager`.
    Its purpose is to create Swarm manager nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: The resource contains the `ami` that references the image we created with Packer.
    The actual value is a variable that we'll define at runtime. The `instance_type`
    specifies the type of the instance we want to create. The default value is fetched
    from the variable `swarm_instance_type`. By default, it is set to `t2.micro`.
    Just as any other variable, it can be overwritten at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The count field defines how many managers we want to create. The first time
    we run `terraform`, the value should be 1 since we want to start with one manager
    that will initialize the cluster. Afterward, the value should be whatever is defined
    in variables. We'll see the use case of both combinations soon.
  prefs: []
  type: TYPE_NORMAL
- en: The tags are for informational purposes only.
  prefs: []
  type: TYPE_NORMAL
- en: The `vpc_security_group_id`s field contains the list of all groups we want to
    attach to the server. In our case, we are using only the group docker defined
    in `terraform/aws/common.tf`.
  prefs: []
  type: TYPE_NORMAL
- en: The `key_name` is the name of the key we have stored in AWS. We created the
    `devops21` key at the beginning of the chapter. Please double check that you did
    not remove it. Without it, you won't be able to SSH into the machine.
  prefs: []
  type: TYPE_NORMAL
- en: The connection field defines the SSH connection details. The user will be `ubuntu`.
    Instead of a password, we'll use the `devops21.pem` key.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the provisioner is defined. The idea is to do as much provisioning
    as possible during the creation of the images. That way, instances are created
    much faster since the only action is to create a VM out of an image. However,
    there is often a part of provisioning that cannot be done when creating an image.
    The `swarm init` command is one of those. We cannot initialize the first Swarm
    node until we get the IP of the server. In other words, the server needs to be
    running (and therefore has an IP) before the `swarm init command` is executed.
  prefs: []
  type: TYPE_NORMAL
- en: Since the first node has to initialize the cluster while any other should join,
    we're using if statements to distinguish one case from the other. If the variable
    `swarm_init` is true, the docker swarm init command will be executed. On the other
    hand, if the variable `swarm_init` is set to false, the command will be docker
    swarm join. In that case, we are using another variable `swarm_manager_ip` to
    tell the node which manager to use to join the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the IP is obtained using the special syntax `self.private_ip`.
    We are referencing oneself and getting the `private_ip`. There are many other
    attributes we can get from a resource.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult the *AWS_INSTANCE* ([https://www.terraform.io/docs/providers/aws/r/instance.html](https://www.terraform.io/docs/providers/aws/r/instance.html)*)*
    page for more info.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `aws_instance` resource named `swarm-worker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The `swarm-worker` resource is almost identical to `swarm-manager`. The only
    difference is in the count field that uses the `swarm_workers` variable and the
    provisioner. Since a worker cannot initialize a cluster, there was no need for
    if statements, so the only command we want to execute is `docker swarm join`
  prefs: []
  type: TYPE_NORMAL
- en: Terraform uses a naming convention that allows us to specify values as environment
    variables by adding the `TF_VAR_ prefix`. For example, we can specify the value
    of the variable `swarm_ami_id` by setting the environment variable `TF_VAR_swarm_ami_id`.
    The alternative is to use the `-var` argument. I prefer environment variables
    since they allow me to specify them once instead of adding `-var` to every command.
  prefs: []
  type: TYPE_NORMAL
- en: The last part of the `terraform/aws/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf))
    specification are outputs.
  prefs: []
  type: TYPE_NORMAL
- en: When building potentially complex infrastructure, Terraform stores hundreds
    or thousands of attribute values for all resources. But, as a user, we may only
    be interested in a few values of importance, such as manager IPs. Outputs are
    a way to tell Terraform what data is relevant. This data is outputted when apply
    is called, and can be queried using the `terraform output` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outputs we defined are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'They are public and private IPs of the managers. Since there are only a few
    (if any) reasons to know worker IPs, we did not define them as outputs. Please
    consult the *Output Configuration* ([https://www.terraform.io/docs/configuration/outputs.html](https://www.terraform.io/docs/configuration/outputs.html))
    page for more info. Since we''ll use the AMI we created with Packer, we need to
    retrieve the ID from the `packer-ubuntu-docker.log`. The command that follows
    parses the output and extracts the ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we create our cluster and the infrastructure around it, we should ask
    Terraform to show us the execution plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an extensive list of resources and their properties. Since the
    output is too big to be printed, I''ll limit the output only to the resource types
    and names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Since this is the first execution, all the resources would be created if we
    were to execute terraform apply. We would get five EC2 instances; three managers
    and two workers. That would be accompanied by one security group.
  prefs: []
  type: TYPE_NORMAL
- en: If you see the complete output, you'll notice that some of the property values
    are set to `<computed>`. That means that Terraform cannot know what will be the
    actual values until it creates the resources. A good example are IPs. They do
    not exist until the EC2 instance is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also output the plan using the `graph` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: That, in itself, is not very useful.
  prefs: []
  type: TYPE_NORMAL
- en: The `graph` command is used to generate a visual representation of either a
    configuration or an execution plan. The output is in the DOT format, which can
    be used by GraphViz to make graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Please open *Graphviz Download* ([http://www.graphviz.org/Download..php](http://www.graphviz.org/Download..php))
    page and download and install the distribution compatible with your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can combine the `graph` command with dot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The output should be the same as in *Figure 11-11:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/terraform-graph.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-11: The image generated by Graphviz from the output of the terraform
    graph command'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization of the plan allows us to see the dependencies between different
    resources. In our case, all resources will use the `aws` provider. Both instance
    types will depend on the security group docker.
  prefs: []
  type: TYPE_NORMAL
- en: When dependencies are defined, we don't need to explicitly specify all the resources
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s take a look at the plan Terraform will generate when
    we limit it to only one Swarm manager node, so that we can initialize the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The runtime variables `swarm_init` and `swarm_managers` will be used to tell
    Terraform that we want to initialize the cluster with one manager. The plan command
    takes those variables into account and outputs the execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output, limited only to resource types and names, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Even though we specified that we only want the plan for the `swarm-manager`
    resource, Terraform noticed that it depends on the security group `docker`, and
    included it in the execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing missing before we start creating the AWS resources is to copy
    the SSH key `devops21.pem` to the current directory. The configuration expects
    it to be there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Please change the `KEY_PATH` value to the correct path before copying it.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start small and create only one manager instance that will initialize
    the cluster. As we saw from the plan, it depends on the security group, so Terraform
    will create it as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The output is too big to be presented in the book. If you look at it from your
    terminal, you'll notice that the security group is created first since `swarm-manager`
    depends on it. Please note that we did not specify the dependency explicitly.
    However, since the resource has it specified in the `vpc_security_group_ids` field,
    Terraform understood that it is the dependency.
  prefs: []
  type: TYPE_NORMAL
- en: Once the `swarm-manager` instance is created, Terraform waited until SSH access
    is available. After it had managed to connect to the new instance, it executed
    `provisioning` commands that initialized the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: The outputs are defined at the bottom of the `terraform/aws/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf))
    file. Please note that not all outputs are listed but only those of the resources
    that were created.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the public IP of the newly created EC2 instance and SSH into it.
  prefs: []
  type: TYPE_NORMAL
- en: You might be inclined to copy the IP. There's no need for that. Terraform has
    a command that can be used to retrieve any information we defined as the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that retrieves the public IP of the first, and currently the only
    manager is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We can leverage the `output` command to construct `SSH` commands. As an example,
    the command that follows will SSH into the machine and retrieve the list of Swarm
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'From now on, we won''t be limited to a single manager node that initialized
    the cluster. We can create all the rest of the nodes. However, before we do that,
    we need to discover the `manager` and `worker` tokens. For security reasons, it
    is better that they are not stored anywhere, so we''ll create environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also need to set the environment variable `swarm_manager_ip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Even though we could use `aws_instance.swarm-manager.0.private_ip` inside the
    `terraform/aws/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf)),
    it is a good idea to have it defined as an environment variable. That way, if
    the first manager fails, we can easily change it to `swarm_manager_2_private_ip`
    without modifying the `tf` files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us see the plan for the creation of all the missing resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: The was no need to specify targets since, this time, we want to create all the
    resources that are missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last line of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the plan is to create four new resources. Since we already have
    one manager running and specified that the desired number is three, two additional
    managers will be created together with two workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply the execution plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'The last lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: All four resources were created, and we got the output of the manager public
    and private IPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s enter into one of the managers and confirm that the cluster indeed works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `node ls` command is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: All the nodes are present, and the cluster seems to be working.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be fully confident that everything works as expected, we''ll deploy a few
    services. Those will be the same services we were creating throughout the book,
    so we''ll save ourselves some time and deploy the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.ym](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.ym)l)
    stacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We downloaded the script from the repository, gave it executable permissions,
    and executed it. At the end, we listed all the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a while, the output of the `service ls` command should be as follows
    (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s send a request to the `go-demo` service through the `proxy`.
    If it returns the correct response, we''ll know that everything works correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: It works!
  prefs: []
  type: TYPE_NORMAL
- en: 'Are we finished? We probably are. As a last check, let''s validate that the
    `proxy` is accessible from outside the security group. We can confirm that by
    exiting the server and sending a request from our laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Let's see what happens if we simulate a failure of an instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll delete an instance using AWS CLI. We could use Terraform to remove an
    instance. However, removing it with the AWS CLI will more closely simulate an
    unexpected failure of a node. To remove an instance, we need to find its ID. We
    can do that with the `terraform show` command. Let''s say that we want to remove
    the second worker. The command to find all its information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Among other pieces of data, we got the ID. In my case, it is `i-6a3a1964`.Before
    running the command that follows, please change the ID to the one you got from
    the `terraform state show` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: AWS changed the state of the instance from `running` to `shutting-down`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `terraform plan` command one more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Terraform deduced that one resource `swarm-worker.1` needs to be added to reconcile
    the discrepancy between the state it has stored locally and the actual state of
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we have to do to restore the cluster to the desirable state is to run `terraform
    apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'The last lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: We can see that one resource was added. The terminated worker has been recreated,
    and the cluster continues operating at its full capacity.
  prefs: []
  type: TYPE_NORMAL
- en: The state of the cluster is stored in the `terraform.tfstate`  file. If you
    are not running it always from the same computer, you might want to store that
    file in your repository together with the rest of configuration files. The alternative
    is to use *Remote State *([https://www.terraform.io/docs/state/remote/index.html](https://www.terraform.io/docs/state/remote/index.html))and,
    for example, store it in Consul.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the desired state of the cluster is easy as well. All we have to to
    is add more resources and rerun `terraform apply`.
  prefs: []
  type: TYPE_NORMAL
- en: We are finished with the brief introduction to Terraform for AWS.
  prefs: []
  type: TYPE_NORMAL
- en: The flow of the process we executed can be described through *figure 11-12:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cloud-architecture-instances.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-12: The flow of the Terraform process'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s destroy what we did before we compare the different approaches we took
    to create and manage a Swarm cluster in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: The cluster is gone as if it never existed, saving us from unnecessary expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right tools to create and manage Swarm clusters in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We tried three different combinations to create a Swarm cluster in AWS. We used
    *Docker Machine* with the *AWS CLI*, *Docker for AWS* with a CloudFormation template,
    and *Packer* with *Terraform*. That is, by no means, the final list of the tools
    we can use. The time is limited, and I promised myself that this book will be
    shorter than *War and Peace* so I had to draw the line somewhere. Those three
    combinations are, in my opinion, the best candidates as your tools of choice.
    Even if you do choose something else, this chapter, hopefully, gave you an insight
    into the direction you might want to take.
  prefs: []
  type: TYPE_NORMAL
- en: Most likely you won't use all three combinations so the million dollar question
    is which one should it be?
  prefs: []
  type: TYPE_NORMAL
- en: Only you can answer that question. Now you have the practical experience that
    should be combined with the knowledge of what you want to accomplish. Each use
    case is different, and no combination would be the best fit for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I will provide a brief overview and some of the use-cases that
    might be a good fit for each combination.
  prefs: []
  type: TYPE_NORMAL
- en: To Docker Machine or not to Docker Machine?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Machine is the weakest solution we explored. It is based on ad-hoc commands
    and provides little more than a way to create an EC2 instance and install Docker
    Engine. It uses *Ubuntu 15.10* as the base AMI. Not only that it is old but is
    a temporary release. If we choose to use Ubuntu, the correct choice is *16.04*
     **Long Term Support** (**LTS**).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Docker Machine still does not support Swarm Mode so we need to manually
    open the port before executing `docker swarm init` and `docker swarm join` commands.
    To do that, we need to combine Docker Machine with AWS Console, AWS CLI, or CloudFormation.
  prefs: []
  type: TYPE_NORMAL
- en: If Docker Machine would, at least, provide the minimum setup for Swarm Mode
    (as it did with the old Standalone Swarm), it could be a good choice for a small
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As it is now, the only true benefit Docker Machine provides when working with
    a Swarm cluster in AWS is Docker Engine installation on a remote node and the
    ability to use the `docker-machine env` command to make our local Docker client
    seamlessly communicate with the remote cluster. Docker Engine installation is
    simple so that alone is not enough. On the other hand, the `docker-machine env`
    command should not be used in a production environment. Both benefits are too
    weak.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the current problems with Docker Machine can be fixed with some extra
    arguments (example: `--amazonec2-ami`) and in combination with other tools. However,
    that only diminishes the primary benefit behind Docker Machine. It was supposed
    to be simple and work out of the box. That was partly true before *Docker 1.12*.
    Now, at least in AWS, it is lagging behind.'
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean we should discard Docker Machine when working with AWS? Not always.
    It is still useful when we want to create an ad-hoc cluster for demo purposes
    or maybe experiment with some new features. Also, if you don't want to spend time
    learning other tools and just want something you're familiar with, Docker Machine
    might be the right choice. I doubt that's your case.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that you reached this far in this book tells me that you do want to
    explore better ways of managing a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The final recommendation is to keep Docker Machine as the tool of choice when
    you want to simulate a Swarm cluster locally as we did before this chapter. There
    are better choices for AWS.
  prefs: []
  type: TYPE_NORMAL
- en: To Docker for AWS or not to Docker for AWS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Docker for AWS* ([https://docs.docker.com/docker-for-aws/release-notes/](https://docs.docker.com/docker-for-aws/release-notes/))
    is opposite from Docker Machine. It is a complete solution for your Swarm cluster.
    While Docker Machine does not do much more than to create EC2 instances and install
    Docker Engine, *Docker for AWS* sets up many of the things we might have a hard
    time setting up ourselves. Autoscaling groups, VPCs, subnets, and ELB are only
    a few of the things we get with it.'
  prefs: []
  type: TYPE_NORMAL
- en: There is almost nothing we need to do to create and manage a Swarm cluster with
    *Docker for AWS*. Choose how many managers and how many workers you need, click
    the Create Stack button, and wait a few minutes. That's all there is to it.
  prefs: []
  type: TYPE_NORMAL
- en: There's even more. *Docker for AWS* comes with a new OS specifically designed
    to run containers.
  prefs: []
  type: TYPE_NORMAL
- en: Does so much praise for *Docker for AWS* inevitably mean it's the best choice?
    Not necessarily. It depends on your needs and the use case. If what *Docker for
    AWS* offers is what you need, the choice is simple. Go for it. On the other hand,
    if you'd like to change some of its aspects or add things that are not included,
    you might have a hard time. It is not easy to modify or extend it.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, *Docker for AWS* will output all the logs to *Amazon CloudWatch*
    ([https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/)). That's
    great as long as CloudWatch is where you want to have your logs. On the other
    hand, if you prefer the ELK stack, DataDog, or any other logging solution, you
    will discover that changing the default setup is not that trivial.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see another example. What if you'd like to add persistent storage. You
    might mount an EFS volume on all servers, but that's not an optimum solution.
    You might want to experiment with RexRay or Flocker. If that's the case, you will
    discover that, again, it's not that simple to extend the system. You'd probably
    end up modifying the CloudFormation template and risk not being able to upgrade
    to a new *Docker for AWS* version.
  prefs: []
  type: TYPE_NORMAL
- en: Did I mention that *Docker for AWS* is still young? At the time of this writing,
    it is, more or less, stable, but it still has its problems. More than problems,
    it lacks some features like, for example, persistent storage. All this negativity
    does not mean that you should give up on *Docker for AWS*. It is a great solution
    that will only become better with time.
  prefs: []
  type: TYPE_NORMAL
- en: The final recommendation is to use *Docker for AWS* if it provides (almost)
    everything you need or if you do not want to start working on your solution from
    scratch. The biggest show stopper would be if you already have a set of requirements
    that need to be fulfilled no matter the tool you'll use.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to host your cluster in AWS and you do not want to spend time
    learning how all its services work, read no more. *Docker for AWS* is what you
    need. It saves you from learning about security groups, VPCs, elastic IPs, and
    a myriad of other services that you might, or might not need.
  prefs: []
  type: TYPE_NORMAL
- en: To Terraform or not to Terraform?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Terraform, when combined with Packer, is an excellent choice. HashiCorp managed
    to make yet another tool that changes the way we configure and provision servers.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration management tools have as their primary objective the task of making
    a server always be in the desired state. If a web server stops, it will be started
    again. If a configuration file is changed, it will be restored. No matter what
    happens to a server, its desired state will be restored. Except, when there is
    no fix to the issue. If a hard disk fails, there's nothing configuration management
    can do.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with configuration management tools is that they were designed to
    work with physical, not virtual servers. Why would we fix a faulty virtual server
    when we can create a new one in a matter of seconds? Terraform understands how
    cloud computing works better than anyone and embraces the idea that our servers
    are not pets anymore. They are cattle. It'll make sure that all your resources
    are available. When something is wrong on a server, it will not try to fix it.
    Instead, it will destroy it and create a new one based on the image we choose.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean that there is no place for Puppet, Chef, Ansible, and other similar
    tools? Are they obsolete when operating in the cloud? Some are more outdated than
    others. Puppet and Chef are designed to run an agent on each server continuously
    monitoring its state and modifying it if things go astray. There is no place for
    such tools when we start treating our servers as cattle. Ansible is in a bit better
    position since it is more useful than others as a tool designed to configure a
    server instead to monitor it. As such, it could be very helpful when creating
    images.
  prefs: []
  type: TYPE_NORMAL
- en: We can combine Ansible with Packer. Packer would create a new VM, Ansible would
    provision that VM with everything we need, and leave it to Packer to create an
    image out of it. If the server setup is complicated, that makes a lot of sense.
    The question is how complex a server setup should be? With AWS, many of the resources
    that would traditionally run on a server are now services. We do not set up a
    firewall on each server but use VPC and security group services. We don't create
    a lot of system users since we do not log into a machine to deploy software. Swarm
    does that for us. We do not install web servers and runtime dependencies anymore.
    They are inside containers. Is there a true benefit from using configuration management
    tools to install a few things into VMs that will be converted into images? More
    often than not, the answer is no. The few things we need can be just as easily
    installed and configured with a few Shell commands. Configuration management of
    our cattle can, and often should, be done with bash.
  prefs: []
  type: TYPE_NORMAL
- en: I might have been too harsh. Ansible is still a great tool if you know when
    to use it and for what purpose. If you prefer it over bash to install and configure
    a server before it becomes an image, go for it. If you try to use it to control
    your nodes and create AWS resources, you're on a wrong path. Terraform does that
    much better. If you think that it is better to provision a running node instead
    instantiating images that already have everything inside, you must have much more
    patience than I do.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we established my bias towards tools that are designed from the ground
    up to work with cloud (as opposed to on-premise physical servers), you might be
    wondering whether to use CloudFormation instead of Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: The major problem with CloudFormation is that it is designed to lock you into
    AWS. It manages Amazon services and not much more. Personally, I think that vendor
    lock-in is unacceptable if there is a good alternative. If you are already using
    AWS services to their fullest, feel free to disregard my opinion on that subject.
    I prefer the freedom of choice. I'm usually trying to design systems that have
    as little dependency on the provider as it makes sense. I'll use a service in
    AWS if it's truly better or easier to setup than some other. In some cases, that's
    true, while in many others it isn't. AWS VPCs and security groups are a good example
    of services that provide a lot of value. I don't see a reason not to use them,
    especially since they are easy to replace if I move to a different provider.
  prefs: []
  type: TYPE_NORMAL
- en: CloudWatch would be an opposite example. ELK is a better solution than CloudWatch,
    it's free, and it can be ported to any provider. The same can be said, for example,
    for ELB. It is mostly obsolete with Docker Networking. If you need a proxy, choose
    HAProxy or nginx.
  prefs: []
  type: TYPE_NORMAL
- en: For you, the vendor lock-in argument might be irrelevant. You might have chosen
    AWS and will stick with it for some time to come. Fair enough. However, Terraform's
    ability to work with a myriad of hosting providers is not the only advantage it
    has.
  prefs: []
  type: TYPE_NORMAL
- en: When compared with CloudFormation, its configuration is easier to understand,
    it works well with other types of resources like *DNSimple *([https://www.terraform.io/docs/providers/dnsimple/](https://www.terraform.io/docs/providers/dnsimple/)),
    and its ability to display a plan before applying it can save us from a lot of
    painful errors. When combined with Packer, it is, in my opinion, the best combination
    for managing a cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get back to the original discussion. Should we use *Docker for AWS* or
    *Terraform* with *Packer*?
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Docker Machine that was easy to reject for most cases, the dilemma whether
    to use *Terraform* or *Docker for AWS* is harder to resolve. It might take you
    a while to reach with Terraform the state where the cluster has everything you
    need. It is not an out-of-the-box solution. You have to write the configs yourself.
    If you are experienced with AWS, such a feat should not cause much trouble. On
    the other hand, if AWS is not your strongest virtue, it might take you quite a
    while to define everything.
  prefs: []
  type: TYPE_NORMAL
- en: Still, I would discard learning AWS as the reason to choose one over the other.
    Even if you go with an out-of-the-box solution like *Docker for AWS*, you should
    still know AWS. Otherwise, you're running a risk of failing to react to infrastructure
    problems when they come. Don't think that anything can save you from understanding
    AWS intricacies. The question is only whether you'll learn the details before
    or after you create your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The final recommendation is to use Terraform with Packer if you want to have
    a control of all the pieces that constitute your cluster or if you already have
    a set of rules that need to be followed. Be ready to spend some time tuning the
    configs until you reach the optimum setup. Unlike with Docker for AWS, you will
    not have a definition of a fully functioning cluster in an hour. If that's what
    you want, choose Docker for AWS. On the other hand, when you do configure Terraform
    to do everything you need, the result will be beautiful.
  prefs: []
  type: TYPE_NORMAL
- en: The final verdict
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What should we use? How do we make a decision? Fully functioning cluster made
    by people who know what they're doing *Docker for AWS* versus fully operational
    cluster made by you Terraform. Docker for AWS versus whatever you'd like to label
    your own solution Terraform. More things than you need (Docker for AWS) versus
    just the resources you want Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Making the choice is hard. *Docker for AWS* is still too young and might be
    an immature solution. Docker folks will continue developing it and it will almost
    certainly become much better in the not so distant future. Terraform gives you
    freedom at a price.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I will closely watch the improvements in *Docker for AWS* and reserve
    the right to make the verdict later. Until that time, I am slightly more inclined
    towards Terraform. I like building things. It's a very narrow victory that should
    be revisited soon.
  prefs: []
  type: TYPE_NORMAL
