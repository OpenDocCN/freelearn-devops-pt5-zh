<html><head></head><body>
        

                            
                    <h1 class="header-title">Docker and Swarm Clusters</h1>
                
            
            
                
<p class="mce-root">In this chapter, we will review container technology and introduce Docker and its orchestration engine, as well as Docker Swarm mode. We will then discuss why we need a Docker infrastructure to deploy and run serverless and FaaS applications. The topics covered in this chapter are as follows:</p>
<ul>
<li>Containers and Docker</li>
<li>Setting up a Docker Swarm cluster</li>
<li>Performing container networking with Docker</li>
<li>Why Docker fits into the serverless and FaaS infrastructure</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">What is a container?</h1>
                
            
            
                
<p>Before talking about Docker, it would be better to discuss the technology behind the software container.</p>
<p>Virtual machines are a common virtualization technology and have been widely adopted by cloud providers and enterprise companies. Actually, a software container (or container for short) is also a kind of virtualization technology, but there is something different about them. The key difference is that every container shares the same kernel on the host machine, while each virtual machine has its own kernel. Basically, a container uses virtualization techniques at the level of the operating system, not the <em>hypervisor</em>. The following diagram shows a comparison between container and VM stacks:</p>
<div><img src="img/bc35c9d4-51bf-44b9-8e41-6e1635c6142b.png" style="width:42.50em;height:30.00em;"/></div>
<p>Figure 2.1: Containers versus virtual machines</p>
<p>Linux's container technology heavily relies on two important kernel capabilities, <strong>namespace</strong> and <strong>cgroups</strong>. Namespace puts a process into isolation so it has its own of set of global resources, such as PIDs and networks. Cgroups or control groups provide a mechanism for metering and limiting resources, such as CPU usage, memory, block I/O, and network bandwidth:</p>
<div><img src="img/ba561123-d240-45cd-9872-c528f52bd817.png"/></div>
<p>Figure 2.2: Linux capabilities—namespaces and cgroups used by a container</p>
<p>The core engine that uses the <strong>namespaces</strong> and <strong>cgroups</strong> capabilities of Linux is called <strong>runC</strong>. It is a tool for spawning and running containers in the <strong>Open Container Initiative</strong> (<strong>OCI</strong>) format. Docker plays a major role in drafting this spec, so the Docker container image is compatible with OCI specifications and therefore runnable by runC. The Docker Engine itself uses <em>runC</em> underneath to start each container.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is Docker?</h1>
                
            
            
                
<p>Containers in the past were quite difficult to manage and use. Docker is basically a set of technologies to help us prepare, manage, and execute containers. In the world of virtual machines, we need a hypervisor to take care of all VM instances. Similarly, in the world of containers, we use Docker as the <em>container engine</em> to take care of everything to do with containers.</p>
<p>Undeniably, Docker is the most popular container engine to date. When using Docker, we follow the three concepts build, ship, and run, recommended by Docker itself:</p>
<ul>
<li>The workflow of <strong>Build</strong>-<strong>Ship</strong>-<strong>Run</strong> is optimized by the philosophy of Docker. In the <strong>Build</strong> step, we are allowed to build and destroy container images rapidly. As developers, we can include the container building steps as a part of our development cycle.</li>
<li>In the <strong>Ship</strong> step, we ship container images to places, from our development laptops to the QA servers and to the staging servers. We send the container images to be stored in the public hub or to our private registry hub inside our company. Ultimately, we send our container images to run in the production environment.</li>
<li>In the <strong>Run</strong> step, Docker helps us prepare the production environment with Swarm clusters. We start containers from the container images. We may schedule containers to run at a specific part of the cluster with a certain set of constraints. We manage a container's life cycle using Docker commands:</li>
</ul>
<div><img src="img/415de6bb-86cd-47ac-893e-8972cd6a4187.png" style="width:39.17em;height:19.58em;"/></div>
<p>Figure 2.3: Build-ship-run</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Docker</h1>
                
            
            
                
<p>Before we follow the build-ship-run steps, we need to install Docker on our machine. On Linux, we use the classic installation method, <strong>Docker Community Edition</strong> (<strong>CE</strong> or <strong>Docker-CE</strong>):</p>
<pre><strong>$ curl -sSL https://get.docker.com | sudo bash</strong></pre>
<p>Throughout the book, we will use a Debian or Ubuntu machine to demonstrate Docker. On a Debian/Ubuntu machine, we will get the most stable version of Docker (at the time of writing) via <kbd>apt-get</kbd> Docker back to version 17.06.2. If we already have a newer version of Docker, such as 17.12 or 18.03, it will be downgraded to 17.06.2:</p>
<pre><strong>$ sudo apt-get install docker-ce=17.06.2~ce-0~ubuntu</strong></pre>
<p>For macOS and Windows systems, we can download Docker from the Docker website:</p>
<ul>
<li>Docker for Mac: <a href="https://www.docker.com/docker-mac">https://www.docker.com/docker-mac</a></li>
<li>Docker for Windows: <a href="https://www.docker.com/docker-windows">https://www.docker.com/docker-windows</a></li>
</ul>
<p>To check the installed version of Docker, we can use the <kbd>docker version</kbd> command:</p>
<pre><br/><strong>$ docker version</strong><br/><br/><strong>Client:</strong><br/><strong> Version: 17.06.2-ce</strong><br/><strong> API version: 1.30</strong><br/><strong> Go version: go1.8.3</strong><br/><strong> Git commit: cec0b72</strong><br/><strong> Built: Tue Sep 5 20:00:33 2017</strong><br/><strong> OS/Arch: linux/amd64</strong><br/><br/><strong>Server:</strong><br/><strong> Version: 17.06.2-ce</strong><br/><strong> API version: 1.30 (minimum version 1.12)</strong><br/><strong> Go version: go1.8.3</strong><br/><strong> Git commit: cec0b72</strong><br/><strong> Built: Tue Sep 5 19:59:26 2017</strong><br/><strong> OS/Arch: linux/amd64</strong><br/><strong> Experimental: true</strong></pre>
<p>The information printed out from the <kbd>docker version</kbd> is separated into two sections, client and server. The client section tells us information about the <kbd>docker</kbd> binary used to issue commands. The server section tells us the version of <kbd>dockerd</kbd>, the Docker Engine.</p>
<p>What we can see from the previous snippet is that both client and server are of version 17.06.2-ce<em>,</em> the second update of the stable 17.06 Community Edition. The server allows Docker client 1.12 as the minimum version to connect to. The <em>API version</em> tells us that <kbd>dockerd</kbd> implements remote API version 1.30.</p>
<p>If we expect to use the next stable version of Docker, we should go for the upcoming 17.06.3, 17.09.x, or 17.12.x versions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building a container image</h1>
                
            
            
                
<p>We use Docker to prepare our software and its execution environment by packing them onto a file system. We call this step building a container image. OK, let's do this. We will build our own version of an NGINX server on Ubuntu, <kbd>my-nginx</kbd>, as a Docker image. Please note that the terms container image and Docker image will be used interchangeably throughout this book.</p>
<p>We create a directory called <kbd>my-nginx</kbd> and change to it:</p>
<pre><strong>$ mkdir my-nginx</strong><br/><strong>$ cd my-nginx</strong><br/></pre>
<p>Then, we create a file named Dockerfile with the following content:</p>
<pre>FROM ubuntu<br/>RUN apt-get update &amp;&amp; apt-get install -y nginx<br/>EXPOSE 80<br/>ENTRYPOINT ["nginx", "-g", "daemon off;"]</pre>
<p>We will explain the contents of Dockerfile line by line:</p>
<ul>
<li>First, it says that we want to use the image named <kbd>ubuntu</kbd> as our base image. This <kbd>ubuntu</kbd> image is stored on the Docker Hub, a central image registry server hosted by Docker Inc.</li>
<li>Second, it says that we want to install NGINX and related packages using the <kbd>apt-get</kbd> command. The trick here is that <kbd>ubuntu</kbd> is a plain Ubuntu image without any package information, so we need to run <kbd>apt-get update</kbd> before installing packages.</li>
<li>Third, we want this image to open port <kbd>80</kbd>, <em>inside the container</em>, for our NGINX server.</li>
<li>Finally, when we start a container from this image, Docker will run the <kbd>nginx -g daemon off;</kbd> command inside the container for us.</li>
</ul>
<p>We are now ready to build our first Docker image. Type the following command to start building an image. Please note that there is <em>dot</em> at the end of the command:</p>
<pre><strong>$ docker build -t my-nginx .</strong></pre>
<p>You will now see something similar to the following output with different hash numbers, so don't worry. Steps 2 to 4 will take a couple of minutes to finish, as it will download and install NGINX packages into the image filesystem. Just make sure that there are four steps and it ends with the message <kbd>Successfully tagged my-nginx:latest</kbd><em>:</em></p>
<pre><strong>Sending build context to Docker daemon 2.048kB</strong><br/><strong>Step 1/4 : FROM ubuntu</strong><br/><strong> ---&gt; ccc7a11d65b1</strong><br/><strong>Step 2/4 : RUN apt-get update &amp;&amp; apt-get install -y nginx</strong><br/><strong> ---&gt; Running in 1f95e93426d3</strong><br/><strong>...</strong><br/><strong>Step 3/4 : EXPOSE 8080</strong><br/><strong> ---&gt; Running in 4f84a2dc1b28</strong><br/><strong> ---&gt; 8b89cae986b0</strong><br/><strong>Removing intermediate container 4f84a2dc1b28</strong><br/><strong>Step 4/4 : ENTRYPOINT nginx -g daemon off;</strong><br/><strong> ---&gt; Running in d0701d02a092</strong><br/><strong> ---&gt; 0a393c45ed34</strong><br/><strong>Removing intermediate container d0701d02a092</strong><br/><strong>Successfully built 0a393c45ed34</strong><br/><strong>Successfully tagged my-nginx:latest</strong></pre>
<p>We now have a Docker image called <kbd>my-nginx:latest</kbd> locally on our machine. We can check that the image is really there using the <kbd>docker image ls</kbd> command (or <kbd>docker images</kbd> for the old-style, top-level command):</p>
<pre><strong>$ docker image ls</strong><br/><strong>REPOSITORY  TAG     IMAGE ID      CREATED         SIZE</strong><br/><strong>my-nginx    latest  0a393c45ed34  18 minutes ago  216MB</strong></pre>
<p>Basically, this is the <em>build</em> concept of Docker. Next, we continue with shipping images.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shipping an image</h1>
                
            
            
                
<p>We usually ship Docker images via a Docker registry. The public registry hosted by Docker Inc. is called <strong>Docker Hub</strong>. To ship a Docker image to a registry, we use the <kbd>docker push</kbd> command. When we start a container, its image will be automatically checked and downloaded to the host before running. The process of downloading can be explicitly done using the <kbd>docker pull</kbd> command. The following diagram illustrates the push/pull behavior among different environments and registries:</p>
<div><img src="img/307800e2-0f55-48a1-832e-0af3bdae7871.png"/></div>
<p>Figure 2.4: Push and pull image workflow</p>
<p>In the previous diagram, developers pull images from the Docker public registry (Docker Hub) then push and pull images from their own Docker private registry. In the development environment, each environment will be triggered by a mechanism to pull images there and run them.</p>
<p>To check that our Docker daemon is allowed to interact with a Docker registry insecurely over the non-encrypted HTTP, we do <kbd>docker info</kbd> then <kbd>grep</kbd> for the <kbd>Registries</kbd> keyword.</p>
<p>Please note that the insecure Docker registry is not recommended for a production environment. You have been warned!</p>
<pre><strong>$ docker info | grep -A3 Registries</strong><br/><strong>Insecure Registries:</strong><br/><strong> 127.0.0.0/8</strong><br/><strong>Live Restore Enabled: false</strong></pre>
<p>OK, seeing <kbd>127.0.0.0/8</kbd> means that we are allowed to do so. We will have a local Docker registry running at <kbd>127.0.0.1:5000</kbd>. Let's set it up. </p>
<p>To have a local Docker registry running, just run it from the Docker registry V2 image:</p>
<pre><strong>$ docker container run --name=registry -d -p 5000:5000 registry:2</strong><br/><strong>6f7dc5ef89f070397b93895527ec2571f77e86b8d2beea2d8513fb30294e3d10</strong></pre>
<p>We should check if it is now up and running:</p>
<pre><strong>$ docker container ls --filter name=registry</strong><br/><strong>CONTAINER ID  IMAGE       COMMAND                CREATED        STATUS</strong><br/><strong>6f7dc5ef89f0  registry:2  "/entrypoint.sh /e.."  8 seconds ago  Up</strong></pre>
<p>The details of <kbd>container run</kbd> and other commands will be discussed again in the <em>Running a container</em> section.</p>
<p>Recall that we have built an image named <kbd>my-nginx</kbd>. We can check if it is still there; this time we use <kbd>--filter reference</kbd> to select only an image name ending with <kbd>nginx</kbd>: </p>
<pre><strong>$ docker image ls --filter reference=*nginx</strong><br/><strong>REPOSITORY  TAG     IMAGE ID      CREATED       SIZE</strong><br/><strong>my-nginx    latest  a773a4303694  1 days ago    216MB</strong><br/><strong>nginx       latest  b8efb18f159b  2 months ago  107MB</strong></pre>
<p>We can also shorten the command to <kbd>docker image ls *nginx</kbd>. It yields the same result.</p>
<p>Let's tag the image. We will tag <kbd>my-nginx</kbd> to <kbd>127.0.0.1:5000/my-nginx</kbd> so it can be pushed into our private Docker registry. We can do this using the <kbd>docker image tag</kbd> command (<kbd>docker tag</kbd> for the old-style, top-level command):</p>
<pre><strong>$ docker image tag my-nginx 127.0.0.1:5000/my-nginx</strong></pre>
<p>We can check using <kbd>image ls</kbd> again to see that the <kbd>tag</kbd> command is done successfully:</p>
<pre><strong>$ docker image ls 127.0.0.1:5000/my-nginx</strong><br/><strong>REPOSITORY               TAG     IMAGE ID      CREATED     SIZE</strong><br/><strong>127.0.0.1:5000/my-nginx  latest  a773a4303694  1 days ago  216MB</strong></pre>
<p>OK, that looks great! We can now push the <kbd>my-nginx</kbd> image to the local repository, of course with <kbd>docker image push</kbd>, and the process will be very quick because the Docker repository is locally here on our machine.</p>
<p>Again, you will find that the hash number is not the same as in the following listing when you try the commands. It is harmless; please just ignore it.</p>
<p>Now, execute the following command to push the <kbd>my-nginx</kbd> image onto the local private repository:</p>
<pre><strong>$ docker image push 127.0.0.1:5000/my-nginx</strong><br/><strong>The push refers to a repository [127.0.0.1:5000/my-nginx]</strong><br/><strong>b3c96f2520ad: Pushed </strong><br/><strong>a09947e71dc0: Pushed </strong><br/><strong>9c42c2077cde: Pushed </strong><br/><strong>625c7a2a783b: Pushed </strong><br/><strong>25e0901a71b8: Pushed </strong><br/><strong>8aa4fcad5eeb: Pushed </strong><br/><strong>latest: digest: sha256:c69c400a56b43db695 ... size: 1569</strong></pre>
<p>The hard part has already been done beautifully. We now go back to the simple part: pushing an image to Docker Hub. Before we continue, please sign up for your Docker ID at <a href="https://hub.docker.com/">https://hub.docker.com/</a> if you don't have one yet.</p>
<p>To store an image there, we have to tag the image with the <kbd>&lt;docker id&gt;/&lt;image name&gt;</kbd> format. For pushing <kbd>my-nginx</kbd> to the Docker Hub, we will <kbd>image tag</kbd> it to <kbd>&lt;docker id&gt;/my-nginx</kbd>. I'll use my Docker ID there. Replace <kbd>&lt;docker id&gt;</kbd> with your registered Docker ID:</p>
<pre><strong>$ docker image tag my-nginx chanwit/my-nginx</strong></pre>
<p>Before pushing, we need to log in to the Docker Hub first using the <kbd>docker login</kbd> command. Please use <kbd>-u</kbd> and your Docker ID to specify the account. We will be asked for a password; if everything is OK, the command will say <kbd>Login Succeeded</kbd>:</p>
<pre><strong>$ docker login -u chanwit</strong><br/><strong>Password: </strong><br/><strong>Login Succeeded</strong></pre>
<p>Please note that our username and password are insecurely stored in <kbd>~/.docker/config.json</kbd>, so please do not forget to type <kbd>docker logout</kbd> whenever possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running a container</h1>
                
            
            
                
<p>Now, let us run a container from our <kbd>my-nginx</kbd> image. We will use the <kbd>docker container run</kbd> command (the old, top-level command is <kbd>docker run</kbd>). This is done to run our container as a background process with <kbd>-d</kbd> and bind port <kbd>8080</kbd> of the host to port <kbd>80</kbd> of the container (<kbd>-p 8080:80</kbd>). We specified the container name with <kbd>--name</kbd>. If we run the container successfully, we will get a hash number, starting with <kbd>4382d778bcc9</kbd> in this example. It is the ID of our running container:</p>
<pre><strong>$ docker container run --name=my-nginx -d -p 8080:80 my-nginx</strong><br/><strong>4382d778bcc96f70dd290e8ef9454d5a260e87366eadbd1060c7b6e087b3df26</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">Open the web browser and point it to <kbd>http://localhost:8080</kbd>; we will see the NGINX server running:</p>
<div><img src="img/e9bd1da1-2c23-42fe-b61b-75c173ba1dca.png" style="width:28.33em;height:14.92em;"/></div>
<p>Figure 2.5: Example of NGINX running inside a container</p>
<p>Now our NGINX server is running as a background container serving on the host's <kbd>8080</kbd> port. We can use the <kbd>docker container ls</kbd> command (or the old-style, top-level <kbd>docker ps</kbd>) to list all running containers:</p>
<pre>$ docker container ls<br/>CONTAINER ID  IMAGE       COMMAND                 CREATED         ...<br/>4382d778bcc9  my-nginx    "nginx -g 'daemon ..."  2 seconds ago   ...<br/>6f7dc5ef89f0  registry:2  "/entrypoint.sh /e..."  2 hours ago     ...</pre>
<p>We can control the life cycle of the container using the commands <kbd>docker container start</kbd>, <kbd>stop</kbd>, <kbd>pause</kbd>, or <kbd>kill</kbd>, for example.</p>
<p>If we would like to force removal of running containers, we can use <kbd>docker container rm -f &lt;container id or name&gt;</kbd> to do so. Let's remove all running instances of <kbd>my-nginx</kbd> and the private registry before continuing to play around with a Docker Swarm cluster:</p>
<pre>$ docker container rm -f my-nginx registry<br/>my-nginx<br/>registry</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Docker Swarm clusters</h1>
                
            
            
                
<p>A cluster is a group of machines connecting together to do work. A Docker host is a physical or virtual machine with the Docker Engine installed. We create a Docker Swarm cluster by connecting many Docker hosts together. We refer to each Docker host as a Docker Swarm node, simply a node.</p>
<p>In version 1.12, Docker introduced Swarm mode, a new orchestration engine to replace the old Swarm cluster, which is now referred to as <strong>Swarm classic</strong>. The main difference between Swarm classic and Swarm mode is that Swarm classic uses an external service, such as Consul, etcd, or Apache ZooKeeper as its key/value store, but Swarm mode has this key/value built in. With this, Swarm mode keeps orchestration latency at a minimum, and is more robust than Swarm classic because it does not need to interact with an external store. The monolithic nature of Swarm mode is good for making changes to its algorithms. For example, one of my research works implemented the Ant Colony optimization to improve how Swarm placing containers ran on non-uniform clusters.</p>
<p>From experiments at our laboratory, we have found that Swarm classic has limitations when scaling to 100–200 nodes. With Swarm mode, we have done experiments with the Docker community to show that it can scale to at least 4,700 nodes.</p>
<p>The results are publicly available at project Swarm2K (<a href="https://github.com/swarmzilla/swarm2k">https://github.com/swarmzilla/swarm2k</a>) and Swarm3K (<a href="https://github.com/swarmzilla/swarm3k">https://github.com/swarmzilla/swarm3k</a>) on GitHub.</p>
<p>The key to the performance of Swarm mode is that it is built on top of the embedded <em>etcd</em> library. The embedded etcd library provides a mechanism for storing the state of a cluster in a distributed fashion. All state information is maintained in the Raft logs database with the Raft consensus algorithm. </p>
<p>In this section, we discuss how to set up a cluster in Swarm mode.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up a cluster</h1>
                
            
            
                
<p>To create a fully functional single-node Swarm cluster, we just type the following command:</p>
<pre><strong>$ docker swarm init<br/></strong><strong>Swarm initialized: current node (jbl2cz9gkilvu5i6ahtxlkypa) is now a manager.</strong><br/><strong>To add a worker to this swarm, run the following command:</strong><br/><strong>    docker swarm join --token SWMTKN-1-470wlqyqbsxhk6gps0o9597izmsjx4xeht5cy3df5sc9nu5n6u-9vlvcxjv5jjrcps4trjcocaae 192.168.1.4:2377</strong><br/><strong>To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.</strong></pre>
<p>We call this process Swarm cluster initialization. This process initializes the new cluster by preparing the <kbd>/var/lib/docker/swarm</kbd> directory to store all states related to the cluster. Here's the contents of <kbd>/var/lib/docker/swarm</kbd>, which could be backed up if needed:</p>
<pre><strong>$ sudo ls -al /var/lib/docker/swarm</strong><br/><br/><strong>total 28</strong><br/><strong>drwx------ 5  root root 4096 Sep 30 23:31 .</strong><br/><strong>drwx--x--x 12 root root 4096 Sep 29 15:23 ..</strong><br/><strong>drwxr-xr-x 2  root root 4096 Sep 30 23:31 certificates</strong><br/><strong>-rw------- 1  root root 124  Sep 30 23:31 docker-state.json</strong><br/><strong>drwx------ 4  root root 4096 Sep 30 23:31 raft</strong><br/><strong>-rw------- 1  root root 67   Sep 30 23:31 state.json</strong><br/><strong>drwxr-xr-x 2  root root 4096 Sep 30 23:31 worker</strong></pre>
<p>If we have many network interfaces on the host, the previous command will fail as Docker Swarm requires us to specify an advertised address using an IP address, or a certain network interface.</p>
<p>In the following example, I use my <kbd>wlan0</kbd> IP address as the advertised address of the cluster. This means that any machine on the Wi-Fi network can try to join this cluster:</p>
<pre><strong>$ docker swarm init --advertise-addr=192.168.1.4:2377</strong></pre>
<p>Similarly, we may advertise using the name of a network interface, for example, <kbd>eth0</kbd>:</p>
<pre><strong>$ docker swarm init --advertise-addr=eth0</strong></pre>
<p>Choose the style that works best for your working environment.</p>
<p>After initialization, we get a fully working, single-node cluster. To force a node to leave the current cluster, we use the following command:</p>
<pre><strong>$ docker swarm leave --force</strong><br/><strong>Node left the swarm.</strong></pre>
<p>If we run this command on a single-node cluster, the cluster will be destroyed. If you run the preceding command here, please do not forget to initialize the cluster again with <kbd>docker swarm init</kbd> before proceeding to the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Masters and workers</h1>
                
            
            
                
<p>Recall that we used the term Docker host to refer to a machine with Docker installed. When we join these hosts together to form a cluster, sometimes we call each of them a Docker node.</p>
<p>A Swarm cluster consists of two kinds of Docker nodes, a master and a worker. We say node <kbd>mg0</kbd> has the master role, and node <kbd>w01</kbd> has the worker role, for example. We form a cluster by joining other nodes to a master, usually the first master. The <kbd>docker swarm join</kbd> command requires the security tokens to be different, to allow a node to join as the master or as the worker. Please note that we must run the <kbd>docker swarm join</kbd> command on each node, not on the master node:</p>
<pre># Login to each node<br/><strong>$ docker swarm join --token SWMTKN-1-27uhz2azpesmsxu0tlli2e2uhdr2hudn3e2x5afilc02x1zicc-9wd3glqr5i92xmxvpnzdwz2j9 192.168.1.4:2377</strong></pre>
<p>A master node is responsible for controlling the cluster. The best practice recommended by Docker is that odd numbers of master nodes are the best configurations. We should have an odd number of master nodes starting from three. If we have three masters, one of them is allowed to fail and the cluster will still work.</p>
<p>The following table shows the possible configurations, from one to six master nodes. For example, a cluster of three master nodes allows one master to fail and it still maintains the cluster. If two masters fail, the cluster will not be allowed to operate, starting or stopping services. However, in that state, the running containers will not die and continue to run:</p>
<table style="width: 704px;height: 572px">
<tbody>
<tr>
<td>
<p><strong>Master nodes</strong></p>
</td>
<td>
<p><strong>Number of masters to maintain cluster</strong></p>
</td>
<td>
<p><strong>Failed masters allowed</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>3 (best)</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>5 (best)</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>2</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The best option to recover the cluster after losing the majority of master nodes is to bring the failed master nodes back online as fast as possible.</p>
<p>In the production cluster, we usually do not schedule running <em>tasks</em> on master nodes. A master node needs to have enough CPU, memory, and network bandwidth to properly handle node information and Raft logs. We control the cluster by commanding one of the master nodes. For example, we can list all nodes of a cluster by sending the following command to a master:</p>
<pre><strong>$ docker node ls</strong><br/><br/><strong>ID            HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS</strong><br/><strong>wbb8rb0xob *  mg0       Ready   Active        Leader</strong></pre>
<p>What we see in the result is the list of all nodes in the current cluster. We can tell that the <kbd>mg0</kbd> node is a manager by looking at the <kbd>MANAGER STATUS</kbd> column. If a manager node is the primary manager of the cluster, <kbd>MANAGER STATUS</kbd> will say it is a <kbd>Leader</kbd>. If we have two more manager nodes here, the status will tell us they are a <kbd>Follower</kbd>. Here's how this leader/follower mechanism works. When we issue a command to the leader, the leader performs the command and the state of the cluster is changed. The cluster state is then updated by also sending this change to other manager nodes, that is, followers. If we issue a command to a follower, it will forward the command to the leader instead of doing that itself. Basically, all commands for the cluster will be performed by the leader, and the followers will update the changes to their internal Raft logs only.</p>
<p>If a new manager node would like to join, we require a master token for it. Type the <kbd>docker swarm join-token manager</kbd> command to obtain a security token to join a cluster in a manager role: </p>
<pre><strong>$ docker swarm join-token manager</strong><br/><br/><strong>To add a manager to this swarm, run the following command:</strong><br/><br/><strong>    docker swarm join --token SWMTKN-1-2c6finlm9d97q075kpwxcn59q93vbpfaf5qp13awjin3s3jopw-5hex62dfsd3360zxds46i6s56 192.168.1.4:2377</strong></pre>
<p>Although a task as a container can be running on both kinds of nodes, we usually do not submit tasks to run on master nodes. We only use worker nodes to run tasks in production. To join worker nodes to the cluster, we pass the worker token to the join command. Use <kbd>docker swarm join-token worker</kbd> to obtain a worker token.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Services and tasks</h1>
                
            
            
                
<p>Along with the new orchestration engine, Docker introduced the new abstraction of services and tasks in version 1.12. A service may consist of many instances of a task. We call each instance a replica. Each instance of a task runs on a Docker node in the form of a container.</p>
<p>A service can be created using the following command:</p>
<pre><strong>$ docker service create \</strong><br/><strong>   --replicas 3 \</strong><br/><strong>   --name web \</strong><br/><strong>   -p 80:80 \</strong><br/><strong>   --constraint node.role==worker \</strong><br/><strong>   nginx</strong></pre>
<p>This web service consists of three tasks, specified with <kbd>--replicas</kbd>. These tasks are submitted by the orchestration engine to run on selected nodes. The service's name, web, can be resolved using a virtual IP address. Other services on the same network, in this case maybe a reverse proxy service, can refer to it. We use <kbd>--name</kbd> to specify the name of the service.</p>
<p>We continue the discussion of the details of this command in the following diagram:</p>
<div><img src="img/84e841b0-684c-4ebd-bd57-9b4ebb6fcad2.png" style="color: #333333;font-size: 1em;border: 1em solid black;text-align: center;width:42.00em;height:33.75em;"/></div>
<p>Figure 2.6: Swarm cluster in action</p>
<p>We assume that our cluster consists of one manager node and five worker nodes. There is no high availability setup for the manager; this will be left as an exercise for the reader.</p>
<p>We start at the manager. The manager is set to be <em>drained </em>because we do not want it to accept any scheduled tasks. This is the best practice, and we can drain a node as follows:</p>
<pre><strong>$ docker node update --availability drain mg0</strong></pre>
<p>This service will be published to port <kbd>80</kbd> on the routing mesh. The routing mesh is a mechanism to perform load balancing inside the Swarm mode. Port <kbd>80</kbd> will be opened on every worker node to serve this service. When a request comes in, the routing mesh will route the request to a certain container (a task) on a certain node, automatically.</p>
<p>The routing mesh relies on a Docker network with the overlay driver, namely <kbd>ingress</kbd>. We can use <kbd>docker network ls</kbd> to list all active networks:</p>
<pre><strong>$ docker network ls</strong><br/><strong>NETWORK ID    NAME             DRIVER    SCOPE</strong><br/><strong>c32139129f45  bridge           bridge    local</strong><br/><strong>3315d809348e  docker_gwbridge  bridge    local</strong><br/><strong>90103ae1188f  host             host      local</strong><br/><strong>ve7fj61ifakr  ingress          overlay   swarm</strong><br/><strong>489d441af28d  none             null      local</strong></pre>
<p>We find a network with ID <kbd>ve7fj61ifakr</kbd> which is an <kbd>overlay</kbd> network of the <kbd>swarm</kbd> scope. As the information implies, this kind of network is working only in Docker Swarm mode. To see the details of this network, we use the <kbd>docker network inspect ingress</kbd> command:</p>
<pre><strong>$ docker network inspect ingress</strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "Name": "ingress",</strong><br/><strong>        "Id": "ve7fj61ifakr8ybux1icawwbr",</strong><br/><strong>        "Created": "2017-10-02T23:22:46.72494239+07:00",</strong><br/><strong>        "Scope": "swarm",</strong><br/><strong>        "Driver": "overlay",</strong><br/><strong>        "EnableIPv6": false,</strong><br/><strong>        "IPAM": {</strong><br/><strong>            "Driver": "default",</strong><br/><strong>            "Options": null,</strong><br/><strong>            "Config": [</strong><br/><strong>                {</strong><br/><strong>                    "Subnet": "10.255.0.0/16",</strong><br/><strong>                    "Gateway": "10.255.0.1"</strong><br/><strong>                }</strong><br/><strong>            ]</strong><br/><strong>        },</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>We can see that the <kbd>ingress</kbd> network has a subnet of <kbd>10.255.0.0/16</kbd><em>,</em> which means that we are allowed to use 65,536 IP addresses in this network by default. This number is the maximum number of tasks (containers) created by <kbd>docker service create -p</kbd> on a single Swarm mode cluster. This number is not affected when we use <kbd>docker container run -p</kbd> outside the Swarm.</p>
<p>To create a Swarm scoped overlay network, we use the <kbd>docker network create</kbd> command:</p>
<pre><strong>$ docker network create  --driver overlay appnet</strong><br/><strong>lu29kfat35xph3beilupcw4m2</strong><br/><br/><strong>$ docker network ls</strong><br/><strong>NETWORK ID    NAME             DRIVER    SCOPE</strong><br/><strong>lu29kfat35xp  appnet           overlay   swarm</strong><br/><strong>c32139129f45  bridge           bridge    local</strong><br/><strong>3315d809348e  docker_gwbridge  bridge    local</strong><br/><strong>90103ae1188f  host             host      local</strong><br/><strong>ve7fj61ifakr  ingress          overlay   swarm</strong><br/><strong>489d441af28d  none             null      local</strong></pre>
<p>We can check again with the <kbd>docker network ls</kbd> command and see the <kbd>appnet</kbd> network with the <kbd>overlay</kbd> driver and <kbd>swarm</kbd> scope there. Your network's ID will be different. To attach a service to a specific network, we can pass the network name to the <kbd>docker service create</kbd> command. For example:</p>
<pre><strong>$ docker service create --name web --network appnet -p 80:80 nginx</strong></pre>
<p class="mce-root">The preceding example creates the <kbd>web</kbd> service and attaches it to the <kbd>appnet</kbd> network. This command works if, and only if, the appnet is Swarm-scoped.</p>
<p>We can dynamically detach or re-attach net networks to the current running service using the <kbd>docker service update</kbd> command with <kbd>--network-add</kbd> or <kbd>--network-rm</kbd>, respectively.  Try the following command:</p>
<pre><strong>$ docker service update --network-add appnet web</strong><br/><strong>web</strong></pre>
<p>Here, we can observe the result with <kbd>docker inspect web</kbd>. You will find a chunk of JSON printed out with the last block looking as follows:</p>
<pre><strong>$ docker inspect web</strong><br/><br/><strong>...</strong><br/><br/><strong>        "UpdateStatus": {</strong><br/><strong>            "State": "completed",</strong><br/><strong>            "StartedAt": "2017-10-09T15:45:03.413491944Z",</strong><br/><strong>            "CompletedAt": "2017-10-09T15:45:21.155296293Z",</strong><br/><strong>            "Message": "update completed"</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>It means that the service has been updated and the process of updating has been completed. We will now have the <kbd>web</kbd> service attaching to the <kbd>appnet</kbd> network:</p>
<div><img src="img/1ad6e3d1-fe1b-4f0f-b7c1-06c518a949c4.png" style="color: #333333;font-size: 1em;border: 1em solid black;text-align: center;width:28.33em;height:28.08em;"/></div>
<p>Figure 2.7: The Gossip communication mechanism for Swarm-scope overlay networks</p>
<p>Overlay networks rely on the <strong>gossip</strong> protocol implementation over port <kbd>7946</kbd>, for both TCP and UDP, accompanied by Linux's VXLAN over UDP port <kbd>4789</kbd>. The overlay network is implemented with performance in mind. A network will cover only the necessary hosts and gradually expand when needed.</p>
<p>We can scale a service by increasing or decreasing the number of its replicas. Scaling the service can be done using the <kbd>docker service scale</kbd> command. For example, if we would like to scale the <kbd>web</kbd> service to five replicas, we could issue the following command:</p>
<pre><strong>$ docker service scale web=5</strong></pre>
<p>When the service is scaled, and its task is scheduled on a new node, all related networks bound to this service will be expanded to cover the new node automatically. In the following diagram, we have two replicas of the app service, and we would like to scale it from two to three with the command <kbd>docker service scale app=3</kbd>. The new replica <strong>app.3</strong> will be scheduled on the worker node <strong>w03</strong>. Then the overlay network bound to this app service will be expanded to cover node <strong>w03</strong> too. The network-scoped gossip communication is responsible for the network expansion mechanism:</p>
<div><img src="img/64ed8f48-8d3e-40e3-98fd-1605048c9fb8.png"/></div>
<p>Figure 2.8: Swarm-scoped network expansion</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Docker and serverless</h1>
                
            
            
                
<p>How will Docker benefit us? When dealing with application development, Docker can be used to simplify the development toolchain. We can pack everything we need to write serverless applications into a single container image and let the whole team use it. This ensures consistency of tool versions and ensures they will not mess up our development machines.</p>
<p>We will then use Docker to prepare our infrastructure. Actually, the term serverless means developers should not maintain their own infrastructure. However, in cases where the public cloud is not an option, we can use Docker to simplify infrastructure provisioning. Using the same architecture as the third-party serverless platforms on our company's infrastructure, we can minimize the operation and maintenance costs. Later chapters will discuss how we can operate our own Docker-based FaaS infrastructure.</p>
<p>For the serverless application itself, we use Docker as a wrapper for serverless functions. We use Docker as a unit of work, so that any kind of binary can be integrated into our serverless platform, ranging from the legacy COBOL, C, or Pascal programs to the programs written in modern languages, such as Node.js, Kotlin, or Crystal. In the 17.06+ versions of Docker, it is also possible to form a Swarm cluster across multi-hardware architecture. We can even host Windows-based C# functions on the same cluster as mainframe-based COBOL programs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>To help you better remember and understand the concepts and practices of Docker described in this chapter, try answering the following questions without going back to the chapter's contents. Let's get started:</p>
<ol>
<li>What are containers? What's the key difference between containers and virtual machines?</li>
<li>What are the main features inside the Linux kernel to enable container technology? Please name at least two of them.</li>
<li>What are the key concepts of the Docker workflow?</li>
<li>What is a Dockerfile for? Which Docker command do you use to interact with it?</li>
<li>What is the ENTRYPOINT instruction inside a Dockerfile?</li>
<li>Which command do we use to list all Docker images?</li>
</ol>
<ol start="7">
<li>Which command do we use to form a Docker Swarm cluster?</li>
<li>What is the key difference between Swarm classic and Swarm mode?</li>
<li>Please explain the relationship between services and tasks.</li>
<li>How can we create an NGINX service with five replicas?</li>
<li>How can we scale down the number of the NGINX services to two?</li>
<li>What is the minimum number of nodes required to form a Swarm cluster with the high-availability property? Why?</li>
<li>What is the name for a network that is part of the routing mesh? How large is it?</li>
<li>Which port numbers are used by a Swarm cluster? What are they for?</li>
<li>What is the main benefit of network-scoped Gossip communication?</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter started off by discussing the concepts of containers. Then we reviewed what Docker is, how to install it, and the Docker build, ship, and run workflow. We then learnt how to form a Docker Swarm cluster and Swarm master and worker nodes. We learnt how to properly set up a robust Swarm cluster with an odd number of master nodes. We then learnt the service and task concepts of Docker Swarm. Finally, we learnt how Docker fits into serverless application development.</p>
<p>In the next chapter, we will review serverless frameworks and platforms to understand the overall architecture and the limitations of them.</p>


            

            
        
    </body></html>