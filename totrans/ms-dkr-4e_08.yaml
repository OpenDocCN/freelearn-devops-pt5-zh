- en: '*Chapter 8*'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be taking a look at Docker Swarm. With Docker Swarm,
    you can create and manage Docker clusters. Swarm can be used to distribute containers
    across multiple hosts and also has the ability to scale containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and managing a swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm services and stacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing, overlays, and scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in previous chapters, we will continue to use our local Docker installations.
    Again, the screenshots in this chapter will be from my preferred operating system,
    macOS. As before, the Docker commands we will be running will work on all three
    of the operating systems on which we have installed Docker so far. However, some
    of the supporting commands, which will be few and far between, may only apply
    to macOS- and Linux-based operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/334RE0A](https://bit.ly/334RE0A)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we go any further, I should mention that there are two very different
    versions of Docker Swarm. There was a standalone version of Docker Swarm—this
    was supported up until Docker `1.12` and is no longer being actively developed;
    however, you may find some old documentation mentions it. Installation of the
    standalone Docker Swarm is not recommended as Docker ended support for version
    `1.11.x` in the first quarter of 2017.
  prefs: []
  type: TYPE_NORMAL
- en: Docker version `1.12` introduced Docker Swarm mode. This introduced all of the
    functionality that was available in the standalone Docker Swarm version into the
    core Docker Engine, along with a significant number of additional features. As
    we are covering Docker 19.03 and higher in this book, we will be using Docker
    Swarm mode, which, for the remainder of the chapter, we will refer to as Docker
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you are already running a version of Docker with in-built support for Docker
    Swarm, there isn''t anything you need to do in order to install Docker Swarm.
    You can verify that Docker Swarm is available on your installation by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something that looks like the following Terminal output when
    running the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Viewing the help'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.01_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Viewing the help
  prefs: []
  type: TYPE_NORMAL
- en: If you get an error, ensure that you are running Docker 19.03 or higher, the
    installation of which we covered in [*Chapter 1*](B15659_01_Final_JM_ePub.xhtml#_idTextAnchor046),
    *Docker Overview*. Now that we know that our Docker client supports Docker Swarm,
    what do we mean by a Swarm?
  prefs: []
  type: TYPE_NORMAL
- en: A **Swarm** is a collection of hosts, all running Docker, which have been set
    up to interact with each other in a clustered configuration. Once configured,
    you will be able to use all of the commands we have been running so far when targeting
    a single host, and let Docker Swarm decide the placement of your containers by
    using a deployment strategy to decide the most appropriate host on which to launch
    your container. Docker Swarms are made up of two types of host. Let's take a look
    at these now.
  prefs: []
  type: TYPE_NORMAL
- en: Roles within a Docker Swarm cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Which roles are involved with Docker Swarm? Let's take a look at the two roles
    a host can assume when running within a Docker Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm manager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Swarm manager** is a host that is the central management point for all
    Swarm hosts. The Swarm manager is where you issue all your commands to control
    those nodes. You can switch between the nodes, join nodes, remove nodes, and manipulate
    those hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Each cluster can run several Swarm managers. For production, it is recommended
    that you run a minimum of five Swarm managers; this would mean that our cluster
    can take a maximum of two Swarm manager node failures before you start to encounter
    any errors. Swarm managers use the *Raft consensus algorithm* (see the *Further
    reading* section for more details) to maintain a consistent state across all of
    the manager nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm workers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Swarm workers, which we have seen referred to earlier as Docker hosts,
    are those that run the Docker containers. Swarm workers are managed from the Swarm
    manager, and are depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – An overview of Swarm workers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.02_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – An overview of Swarm workers
  prefs: []
  type: TYPE_NORMAL
- en: This is an illustration of all the Docker Swarm components. We see that the
    Docker Swarm manager talks to each Swarm host that has a Docker Swarm worker role.
    The workers do have some level of connectivity, which we will look at shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and managing a Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now take a look at using Swarm and how we can perform the following
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joining workers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the cluster hosts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by creating a cluster of three machines. Since we are going to
    be creating a multi-node cluster on our local machine, we are going to use Multipass,
    which we covered in [*Chapter 6*](B15659_06_Final_JM_ePub.xhtml#_idTextAnchor187)*,
    Docker Machine, Vagrant, and Multipass*, to launch the hosts by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give us three nodes; you can check this by just running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Launching the nodes using Multipass'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.03_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Launching the nodes using Multipass
  prefs: []
  type: TYPE_NORMAL
- en: 'You may remember from when we last used Multipass that bringing up a host doesn''t
    mean that Docker is installed; so, now, let''s install Docker and add the `ubuntu`
    user to the `docker` group so that when we use `multipass exec`, we don''t have
    to change user. To do this, run the following three commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our three cluster nodes ready, we can move on to the next step,
    which is adding a Swarm manager to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Swarm manager to the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s bootstrap our Swarm manager. To do this, we will pass the results of
    a few Docker Machine commands to our host. Before we create the Swarm manager,
    we need to get the IP address of `node1` as this is going to be our Swarm manager.
    If you are using macOS or Linux, then you can set an environment variable by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you are using Windows 10, then run `multipass list` and make a note of the
    IP address for `node1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to run in order to create our manager is shown in the following
    code snippet (if you are running Windows, replace `$IP` with the IP address you
    made a note of):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive a message similar to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the output, once your manager is initialized, you are given
    a unique token.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, the full token is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This token will be needed for the worker nodes to authenticate themselves and
    join our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Joining Swarm workers to the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it is time to add our two workers (`node2` and `node3`) to the cluster.
    First, let''s set an environment variable to hold our token, making sure that
    you replace the token with the one you received when initializing your own manager,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Again, Windows users need to make a note of the token and will have to replace
    both `$SWARM_TOKEN` and `$IP` in the command shown next with their respective
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run the following command to add `node2` to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For `node3`, you need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Both times, you should get confirmation that your node has joined the cluster,
    as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Adding the workers to the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.04_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Adding the workers to the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Listing nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can check the Swarm by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This will connect to `node1`, which we have configured as the Swarm master,
    and query all of the nodes that form our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see that all three of our nodes are listed, as illustrated in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Listing the cluster nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.05_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Listing the cluster nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to move from the Docker client on our local machine to that
    on `node1`. To connect to the shell on `node1`, we just need to run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will leave us at a prompt on `node1`, where we are ready to start using
    our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Managing a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can perform some management of all of these cluster nodes that
    we are creating.
  prefs: []
  type: TYPE_NORMAL
- en: There are only two ways in which you can go about managing the containers within
    your cluster—these are by using the `docker service` and `docker stack` commands,
    which we are going to be covering in the next section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at launching containers in our cluster, let's have a look at
    managing the cluster itself, starting with how you can find out more information
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: Finding information on the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have already seen, we can list the nodes within the cluster using the
    Docker client installed on `node1`. To find out more information, we can simply
    type this to the command line of `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us lots of information about the host, as you can see from the
    following output, which I have truncated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is information about the cluster in the `Swarm` section;
    however, we are only able to run the `docker info` command against the host with
    which our client is currently configured to communicate. Luckily, the `docker
    node` command is cluster-aware, so we can use that to get information on each
    node within our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: TIP
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the `--pretty` flag with the `docker node inspect` command will render
    the output in the easy-to-read format you see next. If `--pretty` is left out,
    Docker will return the raw `JSON` object containing the results of the query the
    `inspect` command runs against the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what we would need to run to get information on `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This should provide the following information on our Swarm manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the same command, but this time targeting one of the worker nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us similar information, as can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: But as you can see, information about the state of the manager functionality
    is missing. This is because the worker nodes do not need to know about the status
    of the manager nodes; they just need to know that they are allowed to receive
    instructions from the managers.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we can see details about this host, such as the number of containers,
    the number of images on the host, and information about the **central processing
    unit** (**CPU**) and memory, along with other interesting information.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to get information on the nodes that go to make up our
    cluster, let's take a look at how we can promote a node's role within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting a worker node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Say you wanted to perform some maintenance on your single manager node, but
    you wanted to maintain the availability of your cluster. No problem—you can promote
    a worker node to a manager node.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we have our local three-node cluster up and running, let''s promote `node2`
    to be a new manager. To do this, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive a message confirming that your node has been promoted immediately
    after executing the command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'List the nodes by running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you that you now have two nodes that display something in
    the `MANAGER STATUS` column, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Checking the status of the nodes in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.06_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Checking the status of the nodes in the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Our `node1` node is still the primary manager node, though. Let's look at doing
    something about that, and switch its role from manager to worker.
  prefs: []
  type: TYPE_NORMAL
- en: Demoting a manager node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have already put two and two together, but to demote a manager node
    to a worker node, you simply need to run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, you will receive immediate feedback stating the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have demoted our node, you can check the status of the nodes within
    the cluster by running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As we are connected to `node1`, which is the newly demoted node, you will receive
    a message stating the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To connect to our new Swarm manager, we need to be SSHd into `node2`. To do
    this, we simply need to disconnect from `node1` and connect to `node2` by running
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that are connected to a manager node again, rerun this, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It should list the nodes as expected, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Checking the status of the nodes in the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.07_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – Checking the status of the nodes in the cluster
  prefs: []
  type: TYPE_NORMAL
- en: This is all well and good, but how would we take a node out of the cluster so
    that we could perform maintenance on it? Let's now take a look at how we would
    drain a node.
  prefs: []
  type: TYPE_NORMAL
- en: Draining a node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To temporarily remove a node from our cluster so that we can perform maintenance,
    we need to set the status of the node to `Drain`. Let''s look at draining our
    former manager node. To do this, we need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This will stop any new tasks, such as new containers launching or being executed
    against the node we are draining. Once new tasks have been blocked, all running
    tasks will be migrated from the node we are draining to nodes with an `Active`
    status.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the following Terminal output, listing the nodes now shows
    that `node1` is listed with a status of `Drain` in the `AVAILABILITY` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Checking the status of our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.08_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Checking the status of our cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our node is no longer accepting new tasks and all running tasks have
    been migrated to our two remaining nodes, we can safely perform our maintenance,
    such as rebooting the host. To reboot `node1`, run the following two commands
    on your main host in a second window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the host has been rebooted, run this command on `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It should show that the node has an `AVAILABILITY` status of `Drain`. To add
    the node back into the cluster, simply change the `AVAILABILITY` status to `Active`
    by running the following on `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the following Terminal output, our node is now active,
    meaning new tasks can be executed against it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Checking the status of our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.09_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Checking the status of our cluster
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at how to create and manage a Docker Swarm cluster,
    we should look at how to run a task such as creating and scaling a service or
    launching a stack.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm services and stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have looked at the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'These two commands allow us to bootstrap and manage our Docker Swarm cluster
    from a collection of existing Docker hosts. The next two commands we are going
    to look at are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `service` and `stack` commands allow us to execute tasks that, in turn,
    launch, scale, and manage containers within our Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `service` command is a way of launching containers that take advantage of
    the Swarm cluster. Let's look at launching a really basic single-container service
    on our Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: TIP
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget that the `docker` commands here need to be executed from your current
    Swarm manager. If you are following, that should be `node2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This will create a service called `cluster` that consists of a single container
    with port `80` mapped from the container to the host machine, and it will only
    be running on nodes that have the role of `worker`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at doing more with the service, we can check whether it worked
    on our browser. To do this, we will need the IP address of our two worker nodes.
    First of all, we need to double-check which are the worker nodes by running this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we know which node has which role, you can find the IP addresses of your
    nodes by running this command in a second Terminal window on your host machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the following Terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Creating a service and checking the IP addresses of the nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.10_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Creating a service and checking the IP addresses of the nodes
  prefs: []
  type: TYPE_NORMAL
- en: My worker nodes are `node1` and `node3`, whose IP addresses are `192.168.64.9`
    and `192.168.64.11`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going to either of the IP addresses of your worker nodes, such as http://192.168.64.9/
    or http://192.168.64.11/, in a browser will show the output of the `russmckendrick/cluster`
    application, which is the Docker Swarm graphic and the hostname of the container
    the page is being served from. This is illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Our cluster application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.11_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – Our cluster application
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our service running on our cluster, we can start to find out
    more information about it. First of all, we can list the services again by running
    this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, this should return the single service we launched, called `cluster`,
    as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Listing the services'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.12_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12 – Listing the services
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it is a `inspect` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return detailed information about the service, as illustrated in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Grabbing information on a service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.13_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.13 – Grabbing information on a service
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that so far, we haven't had to bother about which of our
    two worker nodes the service is currently running on. This is quite an important
    feature of Docker Swarm, as it completely removes the need for you to worry about
    the placement of individual containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at scaling our service, we can take a quick look at which host
    our single container is running on by executing these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This will list the containers running on each of our hosts. By default, it
    will list the host the command is being targeted against, which in my case is
    `node1`, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Finding the node our service is running on'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.14_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.14 – Finding the node our service is running on
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at scaling our service to six instances of our application container.
    Run the following commands to scale and check our service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We are only checking two of the nodes since we originally told our service
    to launch on worker nodes. As you can see from the following Terminal output,
    we now have three containers running on each of our worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Checking the distribution of containers on our nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.15_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.15 – Checking the distribution of containers on our nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to look at stacks, let''s remove our service. To do this,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This will remove all of the containers, while leaving the downloaded image on
    the hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is more than possible to create quite complex, highly available multi-container
    applications using Swarm services. In a non-Swarm cluster, manually launching
    each set of containers for a part of the application can start to become a little
    laborious and also makes it difficult to share services. To this end, Docker has
    created a functionality that allows you to define your services in Docker Compose
    files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Docker Compose file, which is named `docker-compose.yml`, will
    create the same service we launched in the `services` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the stack can be made up of multiple services, each defined
    under the `services` section of the Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the normal Docker Compose commands, you can add a `deploy` section;
    this is where you define everything relating to the Swarm element of your stack.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we said we would like six replicas, which should be
    distributed across our two worker nodes. Also, we updated the default restart
    policy, which you saw when we inspected the service from the previous section,
    and it showed up as `paused`, so that if a container becomes unresponsive, it
    is always restarted.
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch our stack, copy the previous content into the `docker-compose.yml`
    file, and then run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker will—as when launching containers with Docker Compose—create a new network
    and then launch your services on it. You can check the status of your stack by
    running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show that a single service has been created. You can get details
    of the service created by the `stack` by running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, running the following command will show where the containers within
    the stack are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following Terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Deploying our stack'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.16_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.16 – Deploying our stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, you will be able to access the stack using the IP addresses of your
    nodes, and you will be routed to one of the running containers. To remove a stack,
    simply run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This will remove all services and networks created by the stack when it is launched.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a Swarm cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before moving on, as we no longer require it for the next section, you can
    delete your Swarm cluster by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Should you need to relaunch the Swarm cluster for any reason, simply follow
    the instructions from the start of the chapter to recreate a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing, overlays, and scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few sections, we looked at launching services and stacks. To access
    the applications we launched, we were able to use any of the host IP addresses
    in our cluster; how was this possible?
  prefs: []
  type: TYPE_NORMAL
- en: Ingress load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker Swarm has an ingress load balancer built in, making it easy to distribute
    traffic to our public-facing containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that you can expose applications within your Swarm cluster to services—for
    example, an external load balancer such as Amazon **Elastic Load Balancer** (**ELB**)—knowing
    that your request will be routed to the correct container(s) no matter which host
    happens to be currently hosting it, as demonstrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – An overview of load balancing in a Swarm cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.17_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.17 – An overview of load balancing in a Swarm cluster
  prefs: []
  type: TYPE_NORMAL
- en: This means that our application can be scaled up or down, fail, or be updated,
    all without the need to have the external load balancer reconfigured to talk to
    the individual containers, as Docker Swarm is handling that for us.
  prefs: []
  type: TYPE_NORMAL
- en: Network overlays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our example, we launched a simple service running a single application. Say
    we wanted to add a database layer in our application, which is typically a fixed
    point within the network; how could we do this?
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm's network overlay layer extends the network you launch your containers
    in across multiple hosts, meaning that each service or stack can be launched in
    its own isolated network. This means that our database container, running MongoDB,
    will be accessible to all other containers running on the same overlay network
    on port `27017`, no matter which of the hosts the containers are running on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be thinking to yourself: *Hang on a minute. Does this mean I have to
    hardcode an IP address into my application''s configuration?* Well, that wouldn''t
    fit well with the problems Docker Swarm is trying to resolve, so no, you don''t.'
  prefs: []
  type: TYPE_NORMAL
- en: Each overlay network has its own inbuilt `mongodb:27017`, and it will connect
    to our MongoDB container.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will make our diagram appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – An overview of overlay networks in a Docker Swarm cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.18_B15659.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.18 – An overview of overlay networks in a Docker Swarm cluster
  prefs: []
  type: TYPE_NORMAL
- en: There are some other considerations you will need to take into account when
    adopting this pattern, but we will cover those in [*Chapter 15*](B15659_15_Final_JM_ePub.xhtml#_idTextAnchor823)*,*
    *Docker Workflows*.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, there is only a single scheduling strategy available
    within Docker Swarm, called **spread**. What this strategy does is schedule tasks
    to be run against the least loaded node that meets any of the constraints you
    defined when launching the service or stack. For the most part, you should not
    need to add too many constraints to your services.
  prefs: []
  type: TYPE_NORMAL
- en: One feature that is not currently supported by Docker Swarm is affinity and
    anti-affinity rules. While it is easy to get around using this constraint, I urge
    you not to overcomplicate things, as it is very easy to end up overloading hosts
    or creating single points of failure if you put too many constraints in place
    when defining your services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored Docker Swarm. We took a look at how to install
    Docker Swarm and the Docker Swarm components that make up Docker Swarm. We took
    a look at how to use Docker Swarm, joining, listing, and managing Swarm manager
    and worker nodes. We reviewed the `service` and `stack` commands and how to use
    them, and spoke about the Swarm inbuilt ingress load balancer, overlay networks,
    and scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Now, Docker Swarm is in an interesting state at the time of writing, as Docker
    Swarm was one of the technologies acquired by Mirantis as part of the Docker Enterprise
    sale, and while Mirantis have said that they will offer support for existing Docker
    Swarm clusters for 2 years (that was in November 2019), they haven't given much
    information on the future of Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't surprising as Mirantis do a lot of work with another container cluster
    called Kubernetes, which we are going to be looking at in [*Chapter 11*](B15659_11_Final_JM_ePub.xhtml#_idTextAnchor294),
    *Docker and Kubernetes*. Before then, in the next chapter, we are going to take
    a look at a **graphical user interface** (**GUI**) for Docker called **Portainer**.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True or false: You should be running your Docker Swarm using the standalone
    Docker Swarm rather than the in-built Docker Swarm mode.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which two things do you need after initiating your Docker Swarm manager to add
    your workers to your Docker Swarm cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which command would you use to find out the status of each of the nodes within
    your Docker Swarm cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which flag would you add to `docker node inspect` on Swarm manager to make it
    more readable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you promote a node to be a manager?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which command can you use to scale your service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a detailed explanation of the Raft consensus algorithm, I recommend working
    through the excellent presentation entitled *The Secret Lives of Data*, which
    can be found at [http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/).
    It explains all the processes taking place in the background on the manager nodes
    via an easy-to-follow animation.
  prefs: []
  type: TYPE_NORMAL
