<html><head></head><body>
		<div><h1 id="_idParaDest-50"><em class="italic"><a id="_idTextAnchor049"/>Chapter 2</em>: Setting Up Your Kubernetes Cluster</h1>
			<p>This chapter contains a review of some of the possibilities for creating a Kubernetes cluster, which we'll need to be able to learn the rest of the concepts in this book. We'll start with minikube, a tool to create a simple local cluster, then touch on some additional, more advanced (and production-ready) tools and review the major managed Kubernetes services from public cloud providers, before we finally introduce the strategies for creating a cluster from scratch.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Options for creating your first cluster</li>
				<li>minikube – an easy way to start </li>
				<li>Managed services – EKS, GKE, AKS, and more</li>
				<li>Kubeadm – simple conformance</li>
				<li>Kops – infrastructure bootstrapping</li>
				<li>Kubespray – Ansible-powered cluster creation</li>
				<li>Creating a cluster completely from scratch</li>
			</ul>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Technical requirements</h1>
			<p>In order to run the commands in this chapter, you will need to have the kubectl tool installed. Installation instructions are available in <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>.</p>
			<p>If you are actually going to create a cluster using any of the methods in this chapter, you will need to review the specific technical requirements for each method in the relevant project's documentation. For minikube specifically, most machines running Linux, macOS, or Windows will work. For large clusters, please review the specific documentation of the tool you plan <a id="_idTextAnchor051"/>to use.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository at the following link: </p>
			<p><a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter2">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter2</a></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Options for creating a cluster</h1>
			<p>There<a id="_idIndexMarker057"/> are many ways to create a Kubernetes cluster, ranging from simple local tools all the way to fully creating a cluster from scratch.</p>
			<p>If you're just getting started with learning Kubernetes, you'll probably want to spin up a simple local cluster with a tool such as minikube.</p>
			<p>If you're looking to build a production cluster for an application, you have several options:</p>
			<ul>
				<li>You can use a tool such as Kops, Kubespray, or Kubeadm to create the cluster programmatically.</li>
				<li>You can use a managed Kubernetes service.</li>
				<li>You can create a cluster completely from scratch on VMs or physical hardware.</li>
			</ul>
			<p>Unless you have extremely specific demands in terms of cluster configuration (and even then), it is not usually recommended to create your cluster completely from scratch without using a bootstrapping tool.</p>
			<p>For most use cases, the decision will be between using a managed Kubernetes service on a cloud provider and using a bootstrapping tool.</p>
			<p>In air-gapped systems, using a bootstrapping tool is the only way to go – but some are better than others for particular use cases. In particular, Kops is aimed at making it easier to create and manage clusters on cloud providers such as AWS.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Not included in this section is a discussion of alternative third-party managed services or cluster creation and administration tools such as Rancher or OpenShift. When making a selection for running clusters in production, it is important to take into account a large variety of factors including the current infrastructure, business requirements, and much more. To keep things simple, in this book we will focus on production clusters, assuming no other infrastructure or hyper-specific business needs – a "clean slate," so to speak.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>minikube – an easy way to start</h1>
			<p>minikube is<a id="_idIndexMarker058"/> the easiest way to get started with a simple local cluster. This cluster won't be set up for high availability, and is not aimed at production uses, but it is a great way to get started running workloads on Kubernetes in minutes.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Installing minikube</h2>
			<p>minikube can be <a id="_idIndexMarker059"/>installed on<a id="_idIndexMarker060"/> Windows, macOS, and Linux. What follows is the installation instructions for all three platforms, which you can also find by navigating to <a href="https://minikube.sigs.k8s.io/docs/start">https://minikube.sigs.k8s.io/docs/start</a>.</p>
			<h3>Installing on Windows</h3>
			<p>The easiest installation <a id="_idIndexMarker061"/>method on Windows is to download and run<a id="_idIndexMarker062"/> the minikube installer from <a href="https://storage.googleapis.com/minikube/releases/latest/minikube-installer.exe">https://storage.googleapis.com/minikube/releases/latest/minikube-installer.exe</a>.</p>
			<h3>Installing on macOS</h3>
			<p>Use the following <a id="_idIndexMarker063"/>command to download and install the binary. You can <a id="_idIndexMarker064"/>find it in the code repository as well:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Minikube-install-mac.sh</p>
			<pre>     curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 \
&amp;&amp; sudo install minikube-darwin-amd64 /usr/local/bin/minikube</pre>
			<h3>Installing on Linux</h3>
			<p>Use the following <a id="_idIndexMarker065"/>command to <a id="_idIndexMarker066"/>download and install the binary:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Minikube-install-linux.sh</p>
			<pre>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \
&amp;&amp; sudo install minikube-linux-amd64 /usr/local/bin/minikube</pre>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>Creating a cluster on minikube</h2>
			<p>To get started with a cluster on<a id="_idIndexMarker067"/> minikube, simply run <code>minikube start</code>, which will <a id="_idIndexMarker068"/>create a simple local cluster with the default VirtualBox VM driver. minikube also has several additional configuration options that can be reviewed at the documentation site.</p>
			<p>Running the <code>minikube</code> <code>start</code> command will automatically configure your <code>kubeconfig</code> file so you can run <code>kubectl</code> commands without any further configuration on your newly created cluster.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Managed Kubernetes services</h1>
			<p>The number of <a id="_idIndexMarker069"/>managed cloud providers that offer a managed Kubernetes service is always growing. However, for the purposes of this book, we will focus on the major public clouds and their particular Kubernetes offerings. This includes the following:</p>
			<ul>
				<li><strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) – <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>)</li>
				<li>Google Cloud – <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>)</li>
				<li>Microsoft Azure – <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>)<p class="callout-heading">Important note</p><p class="callout">The number and implementation of managed Kubernetes services is always changing. AWS, Google Cloud, and Azure were selected for this section of the book because they are very likely to continue working in the same manner. Whatever managed service you use, make sure to check the official documentation provided with the service to ensure that the cluster creation procedure is still the same as what is presented in this book.</p></li>
			</ul>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>Benefits of managed Kubernetes services</h2>
			<p>Generally, the <a id="_idIndexMarker070"/>major managed Kubernetes service offerings provide a few benefits. Firstly, all three of the managed service offerings we're reviewing provide a completely managed Kubernetes control plane.</p>
			<p>This means that when you use one of these managed Kubernetes services, you do not need to worry about your master nodes. They are abstracted away and may as well not exist. All three of these managed clusters allow you to choose the number of worker nodes when creating a cluster.</p>
			<p>Another benefit of a managed cluster is seamless upgrades from one version of Kubernetes to another. Generally, once a new version of Kubernetes (not always the newest version) is validated for the managed service, you should be able to upgrade using a push button or a reasonably simple procedure.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Drawbacks of managed Kubernetes services</h2>
			<p>Although a <a id="_idIndexMarker071"/>managed Kubernetes cluster can make operations easier in many respects, there are also some downsides.</p>
			<p>For many of the managed Kubernetes services available, the minimum cost for a managed cluster far exceeds the cost of a minimal cluster created manually or with a tool such as Kops. For production use cases, this is generally not as much of an issue because a production cluster should contain a minimum amount of nodes anyway, but for development environments or test clusters, the additional cost may not be worth the ease of operations depending on the budget.</p>
			<p>Additionally, though abstracting away master nodes makes operations easier, it also prevents fine tuning or advanced master node functionality that may otherwise be available on clusters with defined masters.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>AWS – Elastic Kubernetes Service</h1>
			<p>AWS' managed Kubernetes service is <a id="_idIndexMarker072"/>called EKS, or Elastic Kubernetes Service. There are a few different ways to get started with EKS, but we'll cover the simplest way.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Getting started</h2>
			<p>In order to create an EKS cluster, you must<a id="_idIndexMarker073"/> provision the proper <strong class="bold">Virtual Private Cloud (VPC)</strong> and <strong class="bold">Identity and Access Management (IAM)</strong> role settings – at which point you can create a cluster through the console. These settings can be created manually through the console, or through infrastructure provisioning tools such as CloudFormation and Terraform. Full instructions for creating a cluster through the console can be found at <a href="https://docs.aws.amazon.com/en_pv/eks/latest/userguide/getting-started-console.html">https://docs.aws.amazon.com/en_pv/eks/latest/userguide/getting-started-console.html</a>.</p>
			<p>Assuming you're creating a cluster and VPC from scratch, however, you can instead use a tool called <code>eksctl</code> to provision your cluster.</p>
			<p>To install <code>eksctl</code>, you can find <a id="_idIndexMarker074"/>installation instructions for macOS, Linux, and Windows at <a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html">https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html</a>.</p>
			<p>Once you have <code>eksctl</code> installed, creating a cluster is as simple as using the <code>eksctl create cluster</code> command:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Eks-create-cluster.sh</p>
			<pre>eksctl create cluster \
--name prod \
--version 1.17 \
--nodegroup-name standard-workers \
--node-type t2.small \
--nodes 3 \
--nodes-min 1 \
--nodes-max 4 \
--node-ami auto</pre>
			<p>This will create a cluster of three <code>t2.small</code> instances as worker nodes set up in an autoscaling group with a minimum of one node and a maximum of four. The Kubernetes version that is used will be <code>1.17</code>. Importantly, <code>eksctl</code> starts with a default region, and depending on the number of nodes chosen, they will be spread throughout multiple availability zones in that region.</p>
			<p><code>eksctl</code> will also automatically update your <code>kubeconfig</code> file, so you should be able to run <code>kubectl</code> commands<a id="_idIndexMarker075"/> immediately after the cluster creation process is finished.</p>
			<p>Test the configuration with the following code: </p>
			<pre>kubectl get nodes</pre>
			<p>You should see a list of your nodes and their associated IPs. Your cluster is ready! Next, let's take a look at Google's GKE setup process.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/>Google Cloud – Google Kubernetes Engine</h1>
			<p>GKE is Google Cloud's<a id="_idIndexMarker076"/> managed Kubernetes service. With the gcloud command-line tool, it is very easy to quickly spin up a GKE cluster.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>Getting started</h2>
			<p>To create a cluster on <a id="_idIndexMarker077"/>GKE using gcloud, you can either use Google Cloud's Cloud Shell service, or run the commands locally. If you want to run the commands locally, you must install the gcloud CLI via the Google Cloud SDK. See <a href="https://cloud.google.com/sdk/docs/quickstarts">https://cloud.google.com/sdk/docs/quickstarts</a> for installation instructions.</p>
			<p>Once you have gcloud installed, you need to ensure that you have activated the GKE API in your Google Cloud account.</p>
			<p>To easily accomplish this, navigate to <a href="https://console.cloud.google.com/apis/library">https://console.cloud.google.com/apis/library</a>, then search for <code>kubernetes</code> in the search bar. Click on <strong class="bold">Kubernetes Engine API</strong> and then click <strong class="bold">Enable</strong>.</p>
			<p>Now that the API is activated, set your project and compute zone in Google Cloud by using the following commands:</p>
			<pre>gcloud config set project proj_id
gcloud config set compute/zone compute_zone</pre>
			<p>In the commands, <code>proj_id</code> corresponds to the project ID in Google Cloud that you want to create your <a id="_idIndexMarker078"/>cluster in, and <code>compute_zone</code> corresponds to your desired compute zone in Google Cloud.</p>
			<p>There are actually three types of clusters on GKE, each with different (increasing) levels of reliability and fault tolerance:</p>
			<ul>
				<li>Single-zone clusters</li>
				<li>Multi-zonal clusters</li>
				<li>Regional clusters</li>
			</ul>
			<p>A <strong class="bold">single-zone</strong> cluster in<a id="_idIndexMarker079"/> GKE means a cluster that has a single control plane replica and one or more worker nodes running in the same Google Cloud zone. If something happens to the zone, both the control plane and the workers (and thus the workloads) will go down.</p>
			<p>A <strong class="bold">multi-zonal</strong> cluster<a id="_idIndexMarker080"/> in GKE means a cluster that has a single control plane replica and two or more worker nodes running in different Google Cloud zones. This means that if a single zone (even the zone containing the control plane) goes down, the workloads running in the cluster will still persist, but the Kubernetes API will be unavailable until the control plane zone comes back up.</p>
			<p>Finally, a <strong class="bold">regional cluster</strong> in GKE<a id="_idIndexMarker081"/> means a cluster that has both a multi-zonal control plane and multi-zonal worker nodes. If any zone goes down, both the control plane and the workloads on the worker nodes will persist. This is the most expensive and reliable option.</p>
			<p>Now, to actually create your cluster, you can run the following command to create a cluster named <code>dev</code> with the default settings: </p>
			<pre>gcloud container clusters create dev \
    --zone [compute_zone]</pre>
			<p>This command will create a single-zone cluster in your chosen compute zone.</p>
			<p>In order to create a multi-zonal cluster, you can run the following command:</p>
			<pre>gcloud container clusters create dev \
    --zone [compute_zone_1]
    --node-locations [compute_zone_1],[compute_zone_2],[etc]</pre>
			<p>Here, <code>compute_zone_1</code> and <code>compute_zone_2</code> are disparate Google Cloud zones. In addition, more zones can be added via the <code>node-locations</code> flag.</p>
			<p>Finally, to create a regional cluster, you can run the following command:</p>
			<pre>gcloud container clusters create dev \
    --region [region] \
    --node-locations [compute_zone_1],[compute_zone_2],[etc]</pre>
			<p>In this case, the <code>node-locations</code> flag is actually optional. If left out, the cluster will be created with worker nodes in all the zones within the region. If you'd like to change this default behavior, you can override it using the <code>node-locations</code> flag.</p>
			<p>Now that you have a<a id="_idIndexMarker082"/> cluster running, you need to configure your <code>kubeconfig</code> file to communicate with the cluster. To do this, simply pass the cluster name into the following command:</p>
			<pre>gcloud container clusters get-credentials [cluster_name]</pre>
			<p>Finally, test the configuration with the following command: </p>
			<pre>kubectl get nodes</pre>
			<p>As with EKS, you should see a list of all your provisioned nodes. Success! Finally, let's take a look at Azure's managed offering.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Microsoft Azure – Azure Kubernetes Service</h1>
			<p>Microsoft Azure's managed <a id="_idIndexMarker083"/>Kubernetes service is called AKS. Creating a cluster on AKS can be done via the Azure CLI.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>Getting started</h2>
			<p>To create a cluster<a id="_idIndexMarker084"/> on AKS, you can use the Azure CLI tool and run the following command to create a service principal (a role that the cluster will use to access Azure resources):</p>
			<pre>az ad sp create-for-rbac --skip-assignment --name myClusterPrincipal</pre>
			<p>The result of this command will be a JSON object with information on the service principal, which we will use in the next step. This JSON object looks like the following:</p>
			<pre>{
  "appId": "559513bd-0d99-4c1a-87cd-851a26afgf88",
  "displayName": "myClusterPrincipal",
  "name": "http://myClusterPrincipal",
  "password": "e763725a-5eee-892o-a466-dc88d980f415",
  "tenant": "72f988bf-90jj-41af-91ab-2d7cd011db48"
}</pre>
			<p>Now, you can use the values from the previous JSON command to actually create your AKS cluster:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Aks-create-cluster.sh</p>
			<pre>az aks create \
    --resource-group devResourceGroup \
    --name myCluster \
    --node-count 2 \
    --service-principal &lt;appId&gt; \
    --client-secret &lt;password&gt; \
    --generate-ssh-keys</pre>
			<p>This command assumes a resource group named <code>devResourceGroup</code>, and a cluster named <code>devCluster</code>. For <code>appId</code> and <code>password</code>, use the values from the service principal creation step.</p>
			<p>Finally, to generate the proper <code>kubectl</code> configuration on your machine, you can run the following command:</p>
			<pre>az aks get-credentials --resource-group devResourceGroup --name myCluster</pre>
			<p>At this point, you <a id="_idIndexMarker085"/>should be able to properly run <code>kubectl</code> commands. Test the configuration with the <code>kubectl get nodes</code> command.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor065"/>Programmatic cluster creation tools</h1>
			<p>There are several tools available that will bootstrap a Kubernetes cluster in various non-managed environments. We'll focus on three of the most popular: Kubeadm, Kops, and Kubespray. Each tool is aimed at a different use case and generally works by a different method.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Kubeadm</h2>
			<p>Kubeadm is <a id="_idIndexMarker086"/>a tool created by the Kubernetes community to simplify cluster creation on<a id="_idIndexMarker087"/> infrastructure that is already provisioned. Unlike Kops, Kubeadm does not have the ability to provision infrastructure on cloud services. It simply creates a best-practices cluster that will pass Kubernetes conformance tests. Kubeadm is agnostic to infrastructure – it should work anywhere you can run Linux VMs.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Kops </h2>
			<p>Kops is a<a id="_idIndexMarker088"/> popular cluster provisioning tool. It provisions the underlying infrastructure for<a id="_idIndexMarker089"/> your cluster, installs all cluster components, and validates the functionality of your cluster. It can also be used to perform various cluster operations such as upgrades, node rotations, and more. Kops currently supports AWS, with (as of the time of writing this book) beta support for Google Compute Engine and OpenStack, and alpha support for VMware vSphere and DigitalOcean.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>Kubespray</h2>
			<p>Kubespray is <a id="_idIndexMarker090"/>different to both Kops and Kubeadm. Unlike Kops, Kubespray does not inherently provision cluster resources. Instead, Kubespray allows you to choose between Ansible and Vagrant in order to perform provisioning, orchestration, and node setup.</p>
			<p>When compared to <a id="_idIndexMarker091"/>Kubeadm, Kubespray has far fewer integrated cluster creation and life cycle processes. Newer versions of Kubespray allow you to use Kubeadm specifically for cluster creation after node setup.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Since creating a cluster with Kubespray requires some Ansible-specific domain knowledge, we will keep that discussion out of this book – but a guide to all things Kubespray can<a id="_idIndexMarker092"/> be found at <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md</a>. </p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor069"/>Creating a cluster with Kubeadm</h1>
			<p>To create a cluster with Kubeadm, you will <a id="_idIndexMarker093"/>need your nodes provisioned ahead of time. As with any other Kubernetes cluster, we'll need VMs or bare-metal servers running Linux.</p>
			<p>For the purposes of this book, we <a id="_idIndexMarker094"/>will show how to bootstrap a Kubeadm cluster with only a single master node. For highly available setups, you'll need to run additional join commands on the other master nodes, which you can find at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</a>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Installing Kubeadm</h2>
			<p>First things first – you'll need to install <a id="_idIndexMarker095"/>Kubeadm on all nodes. The installation instructions for each supported operating system can be found at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm</a>.</p>
			<p>For each node, also make sure to check that all the required ports are open, and that you've installed your intended container runtime.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>Starting the master nodes</h2>
			<p>To quickly start master nodes<a id="_idIndexMarker096"/> with Kubeadm, you only need to run a single command: </p>
			<pre>kubeadm init</pre>
			<p>This initialization command can take in several optional arguments – depending on your preferred cluster setup, networking, and so on, you may need to use them.</p>
			<p>In the output of the <code>init</code> command, you'll see a <code>kubeadm join</code> command. Make sure to save this command.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>Starting the worker nodes</h2>
			<p>In order to bootstrap the <a id="_idIndexMarker097"/>worker nodes, you need to run the <code>join</code> command you saved. The command will be of the following form:</p>
			<pre>kubeadm join --token [TOKEN] [IP ON MASTER]:[PORT ON MASTER] --discovery-token-ca-cert-hash sha256:[HASH VALUE]</pre>
			<p>The token in this command is a bootstrap token. It is used to authenticate nodes with each other and join new nodes to the cluster. With access to this token comes the power to join new nodes to the cluster, so treat it as such.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/>Setting up kubectl</h2>
			<p>With Kubeadm, kubectl<a id="_idIndexMarker098"/> will already be properly set up on the master node. However, to use kubectl from any other machine or outside the cluster, you can copy the config from the master to your local machine:</p>
			<pre>scp root@[IP OF MASTER]:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes </pre>
			<p>This <code>kubeconfig</code> will be the cluster administrator config – in order to specify other users (and permissions), you will need to add new service accounts and generate <code>kubeconfig</code> files for them.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/>Creating a cluster with Kops</h1>
			<p>Since Kops will provision<a id="_idIndexMarker099"/> infrastructure for you, there is no need to pre-create any nodes. All <a id="_idIndexMarker100"/>you need to do is install Kops, ensure your cloud platform credentials are working, and create your cluster all at once. Kops can be installed on Linux, macOS, and Windows.</p>
			<p>For this tutorial, we will go through creating a cluster on AWS, but you can find instructions for other supported Kops platforms in the Kops documentation at <a href="https://github.com/kubernetes/kops/tree/master/docs">https://github.com/kubernetes/kops/tree/master/docs</a>.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Installing on macOS</h2>
			<p>On OS X, the easiest way to<a id="_idIndexMarker101"/> install Kops is <a id="_idIndexMarker102"/>using Homebrew:</p>
			<pre>brew update &amp;&amp; brew install kops</pre>
			<p>Alternatively, you can grab the newest stable Kops binary from the Kops GitHub page at <a href="https://github.com/kubernetes/kops/releases/tag/1.12.3">https://github.com/kubernetes/kops/releases/tag/1.12.3</a>.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>Installing on Linux</h2>
			<p>On Linux, you can<a id="_idIndexMarker103"/> install Kops <a id="_idIndexMarker104"/>via the following command:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Kops-linux-install.sh</p>
			<pre>curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops</pre>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>Installing on Windows </h2>
			<p>To install Kops<a id="_idIndexMarker105"/> on Windows, you'll need to download the newest <a id="_idIndexMarker106"/>Windows release from <a href="https://github.com/kubernetes/kops/releases/latest">https://github.com/kubernetes/kops/releases/latest</a>, rename it to <code>kops.exe</code>, and add it to your <code>path</code> variable.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>Setting up credentials for Kops </h2>
			<p>In order for<a id="_idIndexMarker107"/> Kops to work, you'll need AWS credentials on your <a id="_idIndexMarker108"/>machine with a few required IAM permissions. To do this safely, you will want to create an IAM user specifically for Kops.</p>
			<p>First, create an IAM group for the <code>kops</code> user:</p>
			<pre>aws iam create-group --group-name kops_users</pre>
			<p>Then, attach the required roles for the <code>kops_users</code> group. To function properly, Kops will need <code>AmazonEC2FullAccess</code>, <code>AmazonRoute53FullAccess</code>, <code>AmazonS3FullAccess</code>, <code>IAMFullAccess</code>, and <code>AmazonVPCFullAccess</code>. We can accomplish this by running the following commands:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Provide-aws-policies-to-kops.sh</p>
			<pre>aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops</pre>
			<p>Finally, create the <code>kops</code> user, add it to the <code>kops_users</code> group, and create programmatic access keys, which you should save:</p>
			<pre>aws iam create-user --user-name kops
aws iam add-user-to-group --user-name kops --group-name kops_users
aws iam create-access-key --user-name kops</pre>
			<p>To allow Kops<a id="_idIndexMarker109"/> to access your new IAM credentials, you can <a id="_idIndexMarker110"/>use the following commands to configure your AWS CLI with the access key and secret from the previous command (<code>create-access-key</code>):</p>
			<pre>aws configure
export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)</pre>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>Setting up state storage</h2>
			<p>With the proper<a id="_idIndexMarker111"/> credentials set up, we can start creating our cluster. In this case, we're going to build a simple gossip-based cluster so we won't need to mess around with DNS. To see the possible DNS setups, you can look at the Kops documentation (<a href="https://github.com/kubernetes/kops/tree/master/docs">https://github.com/kubernetes/kops/tree/master/docs</a>).</p>
			<p>First, we'll need a location to store our cluster spec. S3 is perfect for this since we're on AWS.</p>
			<p>As usual with S3, bucket names need to be unique. You can easily create a bucket using the AWS SDK (make sure to replace <code>my-domain-dev-state-store</code> with your desired S3 bucket name):</p>
			<pre>aws s3api create-bucket \
    --bucket my-domain-dev-state-store \
    --region us-east-1</pre>
			<p>It's a best practice to enable bucket encryption and versioning as well:</p>
			<pre>aws s3api put-bucket-versioning --bucket prefix-example-com-state-store  --versioning-configuration Status=Enabled
aws s3api put-bucket-encryption --bucket prefix-example-com-state-store --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'</pre>
			<p>Finally, to set up<a id="_idIndexMarker112"/> variables for Kops, use the following commands:</p>
			<pre>export NAME=devcluster.k8s.local
export KOPS_STATE_STORE=s3://my-domain-dev-cluster-state-store</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">Kops supports several state storage locations such as AWS S3, Google Cloud Storage, Kubernetes, DigitalOcean, OpenStack Swift, Alibaba Cloud, and memfs. However, you can just save the Kops state to a local file and use that instead. The benefit of having a cloud-based state store is the ability for multiple infrastructure developers to access and update it with versioning controls.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor080"/>Creating clusters</h2>
			<p>With Kops, we can deploy <a id="_idIndexMarker113"/>clusters of any size. For the purposes of this guide, we<a id="_idIndexMarker114"/> will deploy a production-ready cluster by having both worker and master nodes span three availability zones. We're going to use the US-East-1 region, and both the masters and workers will be <code>t2.medium</code> instances.</p>
			<p>To create the config for this cluster, you can run the following <code>kops create</code> command:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Kops-create-cluster.sh</p>
			<pre>kops create cluster \
    --node-count 3 \
    --zones us-east-1a,us-east-1b,us-east-1c \
    --master-zones us-east-1a,us-east-1b,us-east-1c \
    --node-size t2.medium \
    --master-size t2.medium \
    ${NAME}</pre>
			<p>To see the config that has been created, use the following command: </p>
			<pre>kops edit cluster ${NAME}</pre>
			<p>Finally, to create our<a id="_idIndexMarker115"/> cluster, run the following command:</p>
			<pre>kops update cluster ${NAME} --yes</pre>
			<p>The cluster creation <a id="_idIndexMarker116"/>process may take some time, but once it is complete, your <code>kubeconfig</code> should be properly configured to use kubectl with your new cluster.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor081"/>Creating a cluster completely from scratch</h1>
			<p>Creating a <a id="_idIndexMarker117"/>Kubernetes cluster entirely from scratch is a multi-step endeavor that could likely span multiple chapters of this book. However, since our purpose is to get you up and running with Kubernetes as quickly as possible, we will refrain from describing the entire process.</p>
			<p>If you are interested in creating a cluster from scratch, either for educational reasons or a need to finely customize your cluster, a great guide is <em class="italic">Kubernetes The Hard Way</em>, which is a full cluster creation tutorial written by <em class="italic">Kelsey Hightower</em>. It can be found at <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a>.</p>
			<p>Now that we've gotten that out of the way, we can proceed with an overview of the manual cluster creation process.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor082"/>Provisioning your nodes</h2>
			<p>First things first – you'll need<a id="_idIndexMarker118"/> some infrastructure to run Kubernetes on. Generally, VMs are a good candidate for this, though Kubernetes can be run on bare metal as well. If you're working in an environment where you cannot easily add nodes (which removes many of the scaling benefits of the cloud, but is definitely possible in enterprise settings), you'll need enough nodes to meet your application demands. This is more likely to be an issue in air-gapped environments. </p>
			<p>Some of your nodes will be used for the master control plane, while others will solely be used as workers. There is no need to make the master and worker nodes identical from a memory or CPU perspective – you could even have some weaker and some more powerful workers. This pattern results in a non-homogeneous cluster, in which certain nodes are better suited to particular workloads.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor083"/>Creating the Kubernetes certificate authority for TLS</h2>
			<p>In order to <a id="_idIndexMarker119"/>function properly, all major control plane components will need a TLS certificate. To create these, a <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong>) needs to<a id="_idIndexMarker120"/> be created, which will in turn create the TLS certificates. </p>
			<p>To create the CA, a <strong class="bold">Public Key Infrastructure</strong> (<strong class="bold">PKI</strong>) needs <a id="_idIndexMarker121"/>to be bootstrapped. For this task, you can use any PKI tool, but the one used in the Kubernetes docs is cfssl.</p>
			<p>Once the PKI, CA, and TLS certificates have been created for all components, the next step is to create config files for the control plane and worker node components.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor084"/>Creating config files </h2>
			<p>Config files <a id="_idIndexMarker122"/>need to be created for the <code>kubelet</code>, <code>kube-proxy</code>, <code>kube-controller-manager</code>, and <code>kube-scheduler</code> components. They will use the certificates in these config files to authenticate with <code>kube-apiserver</code>.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor085"/>Creating an etcd cluster and configuring encryption</h2>
			<p>Creating the <a id="_idIndexMarker123"/>data encryption<a id="_idIndexMarker124"/> config is handled via a YAML file with a data encryption secret. At this point, it is required to start the <code>etcd</code> cluster.</p>
			<p>To do this, <code>systemd</code> files are created on each node with the <code>etcd</code> process config. Then <code>systemctl</code> is used on each node to start the <code>etcd</code> servers.</p>
			<p>Here is a sample <code>systemd</code> file for <code>etcd</code>. The <code>systemd</code> files for the other control plane components will be similar to this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Example-systemd-control-plane</p>
			<pre>[Unit]
Description=etcd
Documentation=https://github.com/coreos
[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target</pre>
			<p>This service file<a id="_idIndexMarker125"/> provides a runtime definition for our <code>etcd</code> component, which <a id="_idIndexMarker126"/>will be started on each master node. To actually start <code>etcd</code> on our node, we run the following command:</p>
			<pre>{
  sudo systemctl daemon-reload
  sudo systemctl enable etcd
  sudo systemctl start etcd
}</pre>
			<p>This enables the <code>etcd</code> service along with automatic restarts when the node is restarted.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor086"/>Bootstrapping the control plane component</h2>
			<p>Bootstrapping the control<a id="_idIndexMarker127"/> plane components on the master nodes is similar to the process used to create the <code>etcd</code> cluster. <code>systemd</code> files are created for each component – the API server, the controller manager, and the scheduler – and then a <code>systemctl</code> command is used to start each component. </p>
			<p>The previously created config files and certificates also need to be included on each master node.</p>
			<p>Let's take a look at our service file definition for the <code>kube-apiserver</code> component, broken down into its sections as follows. The <code>Unit</code> section is just a quick description of our <code>systemd</code> file:</p>
			<pre>[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes</pre>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Api-server-systemd-example</p>
			<p>This second piece is <a id="_idIndexMarker128"/>the actual start command for the services, along with any variables to be passed to the services:</p>
			<pre>[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.10.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2</pre>
			<p>Finally, the <code>Install</code> section allows us to specify a <code>WantedBy</code> target:</p>
			<pre>Restart=on-failure
RestartSec=5
 [Install]
WantedBy=multi-user.target</pre>
			<p>The service files<a id="_idIndexMarker129"/> for <code>kube-scheduler</code> and <code>kube-controller-manager</code> will be very similar to the <code>kube-apiserver</code> definition, and once we're ready to start the components on the node, the process is easy:</p>
			<pre>{
  sudo systemctl daemon-reload
  sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
  sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
}</pre>
			<p>Similarly to <code>etcd</code>, we want to ensure the services restart on a node shutdown.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor087"/>Bootstrapping the worker node</h2>
			<p>It's a similar story on the worker<a id="_idIndexMarker130"/> nodes. Service specs for <code>kubelet</code>, the container runtime, <code>cni</code>, and <code>kube-proxy</code> need to be created and run using <code>systemctl</code>. The <code>kubelet</code> config will specify the aforementioned TLS certificate so that it can communicate with the control plane via the API server.</p>
			<p>Let's take a look at what our <code>kubelet</code> service definition looks like:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Kubelet-systemd-example</p>
			<pre>[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service
[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target</pre>
			<p>As you can see, this<a id="_idIndexMarker131"/> service definition references <code>cni</code>, the container runtime, and the <code>kubelet-config</code> file. The <code>kubelet-config</code> file contains the TLS information we need for our workers.</p>
			<p>After bootstrapping the workers and master, the cluster should be functional via the use of the admin <code>kubeconfig</code> file that was created as part of the TLS setup.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor088"/>Summary</h1>
			<p>In this chapter, we reviewed several methods for creating a Kubernetes cluster. We looked at minimal local cluster creation using minikube, setting up clusters on managed Kubernetes services on Azure, AWS, and Google Cloud, creating clusters using the Kops provisioning tool, and finally, manually creating a cluster from scratch.</p>
			<p>Now that we have the skills to create a Kubernetes cluster in several different environments, we can move on to using Kubernetes to run applications. </p>
			<p>In the next chapter, we will learn how to start running applications on Kubernetes. The knowledge you've gained about how Kubernetes works at the architectural level should make it much easier to understand the concepts in the next few chapters.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor089"/>Questions </h1>
			<ol>
				<li>What purpose does minikube serve?</li>
				<li>What are some downsides to using a managed Kubernetes service?</li>
				<li>How does Kops compare to Kubeadm? What are the major differences?</li>
				<li>Which platforms does Kops support?</li>
				<li>When manually creating a cluster, how are the major cluster components specified? How are they run on each node?</li>
			</ol>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor090"/>Further reading</h1>
			<ul>
				<li>The official Kubernetes documentation: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
				<li><em class="italic">Kubernetes The Hard Way</em>: <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a></li>
			</ul>
		</div>
	</body></html>