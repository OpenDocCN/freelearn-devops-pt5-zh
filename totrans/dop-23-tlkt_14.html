<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating a Production-Ready Kubernetes Cluster</h1>
                </header>
            
            <article>
                
<div class="packt_tip">Creating a Kubernetes cluster is not trivial. We have to make many choices, and we can easily get lost in the myriad of options. The number of permutations is getting close to infinite and, yet, our clusters need to be configured consistently. Experience from the first attempt to set up a cluster can easily convert into a nightmare that will haunt you for the rest of your life.</div>
<p>Unlike Docker Swarm that packs almost everything into a single binary, Kubernetes clusters require quite a few separate components running across the nodes. Setting them up can be very easy, or it can become a challenge. It all depends on the choices we make initially. One of the first things we need to do is choose a tool that we'll use to create a Kubernetes cluster.</p>
<p>If we'd decide to install a Docker Swarm cluster, all we'd need to do is to install Docker engine on all the servers, and execute <kbd>docker swarm init</kbd> or <kbd>docker swarm join</kbd> command on each of the nodes. That's it. Docker packs everything into a single binary. Docker Swarm setup process is as simple as it can get. The same cannot be said for Kubernetes. Unlike Swarm that is highly opinionated, Kubernetes provides much higher freedom of choice. It is designed around extensibility. We need to choose among many different components. Some of them are maintained by the core Kubernetes project, while others are provided by third-parties. Extensibility is probably one of the main reasons behind Kubernetes' rapid growth. Almost every software vendor today is either building components for Kubernetes or providing a service that sits on top of it.</p>
<p>Besides the intelligent design and the fact that it solves problems related to distributed, scalable, fault-tolerant, and highly available systems, Kubernetes' power comes from adoption and support from a myriad of individuals and companies. You can use that power, as long as you understand that it comes with responsibilities. It's up to you, dear reader, to choose how will your Kubernetes cluster look like, and which components it'll host. You can decide to build it from scratch, or you can use one of the hosted solutions like <strong>Google Cloud Platform</strong> (<strong>GCE</strong>) Kubernetes Engine. There is a third option though. We can choose to use one of the installation tools. Most of them are highly opinionated with a limited amount of arguments we can use to tweak the outcome.</p>
<p>You might be thinking that creating a cluster from scratch using <kbd>kubeadm</kbd> cannot be that hard. You'd be right if running Kubernetes is all we need. But, it isn't. We need to make it fault tolerant and highly available. It needs to stand the test of time. Constructing a robust solution would require a combination of Kubernetes core and third-party components, AWS know-how, and quite a lot of custom scripts that would tie the two together. We won't go down that road. At least, not now.</p>
<p>We'll use <strong>Kubernetes Operations</strong> (<strong>kops</strong>) to create a cluster. It is somewhere in the middle between do-it-yourself-from-scratch and hosted solutions (for example, GCE). It's an excellent fit for both newbies and veterans. You'll learn which components are required for running a Kubernetes cluster. You'll be able to make some choices. And, yet, we won't go down the rabbit hole of setting up the cluster from scratch. Believe me, that hole is very deep, and it might take us a very long time to get out of it.</p>
<p>Typically, this would be a great place to explain the most significant components of a Kubernetes cluster. Heck, you were probably wondering why we didn't do that early on when we began the journey. Still, we'll postpone the discussion for a while longer. I believe it'll be better to create a cluster first and discuss the components through live examples. I feel that it's easier to understand something we can see and touch, instead of keeping it purely on the theoretical level.</p>
<p>All in all, we'll create a cluster first, and discuss its components later.</p>
<p>Since I already mentioned that we'll use <strong>kops</strong> to create a cluster, we'll start with a very brief introduction to the project behind it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is kubernetes operations (kops) project?</h1>
                </header>
            
            <article>
                
<p>If you visit <strong>Kubernetes Operations</strong> (<strong>kops</strong>) (<a href="https://github.com/kubernetes/kops" target="_blank"><span class="URLPACKT">https://github.com/kubernetes/kops</span></a>) project, the first sentence you'll read is that it is "the easiest way to get a production-grade Kubernetes cluster up and running." In my humble opinion, that sentence is accurate only if we exclude <strong>Google Kubernetes Engine</strong> (<strong>GKE</strong>). Today (February 2018), other hosting vendors did not yet release their Kubernetes-as-a-service solutions. Amazon's <strong>Elastic Container Service for Kubernetes</strong> (<strong>EKS</strong>) (<a href="https://aws.amazon.com/eks/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/eks/</span></a>) is still not open to the public. <strong>Azure Container Service</strong> (<strong>AKS</strong>) (<a href="https://azure.microsoft.com/en-us/services/kubernetes-service/" target="_blank"><span class="URLPACKT">https://azure.microsoft.com/en-us/services/kubernetes-service/</span></a>) is also a new addition that still has a few pain points. By the time you read this, all major hosting providers might have their solutions. Still, I prefer kops since it provides almost the same level of simplicity without taking away the control of the process. It allows us to tweak the cluster more than we would be permitted with hosted solutions. It is entirely open source, it can be stored in version control, and it is not designed to lock you into a vendor.</p>
<p>If your hosting vendor is AWS, kops is, in my opinion, the best way to create a Kubernetes cluster. Whether that's true for GCE, is open for debate since GKE works great. We can expect kops to be extended in the future to other vendors. For example, at the time of this writing, VMWare is in alpha and should be stable soon. Azure and Digital Ocean support are being added as I write this.</p>
<p>We'll use kops to create a Kubernetes cluster in AWS. This is the part of the story that might get you disappointed. You might have chosen to run Kubernetes somewhere else. Don't be depressed. Almost all Kubernetes clusters follow the same principles even though the method of setting them up might differ. The principles are what truly matters, and I'm confident that, once you set it up successfully on AWS, you'll be able to transfer that knowledge anywhere else.</p>
<p>The reason for choosing AWS lies in its adoption. It is the hosting vendor with, by far, the biggest user-base. If I'd have to place a blind bet on your choice, it would be AWS solely because that is statistically the most likely choice. I could not explore all the options in a single chapter. If I am to go through all hosting vendors and different projects that might help with the installation, we'd need to dedicate a whole book to that. Instead, I invite you to explore the subject further once you're finished with installing Kubernetes in AWS with kops. As an alternative, ping me on <kbd>slack.devops20toolkit.com</kbd> or send me an email to <kbd>viktor@farcic.com</kbd> and I'll give you a hand. If I receive enough messages, I might even dedicate a whole book to Kubernetes installations.</p>
<p>I went astray from kops...</p>
<p>Kops lets us create a production-grade Kubernetes cluster. That means that we can use it not only to create a cluster, but also to upgrade it (without downtime), update it, or destroy it if we don't need it anymore. A cluster cannot be called "production grade" unless it is highly available and fault tolerant. We should be able to execute it entirely from the command line if we'd like it to be automated. Those and quite a few other things are what kops provides, and what makes it great.</p>
<p>Kops follows the same philosophy as Kubernetes. We create a set of JSON or YAML objects which are sent to controllers that create a cluster.</p>
<p>We'll discuss what kops can and cannot do in more detail soon. For now, we'll jump into the hands-on part of this chapter and ensure that all the prerequisites for the installation are set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing for the cluster setup</h1>
                </header>
            
            <article>
                
<p>We'll continue using the specifications from the <kbd>vfarcic/k8s-specs</kbd> repository, so the first thing we'll do is to go inside the directory where we cloned it, and pull the latest version.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>14-aws.sh</kbd> (<a href="https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134" target="_blank"><span class="URLPACKT">https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134</span></a>) Gist.</div>
<pre><strong>cd k8s-specs</strong>
    
<strong>git pull</strong> </pre>
<p>I will assume that you already have an AWS account. If that's not the case, please head over to Amazon Web Services (<a href="https://aws.amazon.com/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/</span></a>) and sign-up.</p>
<div class="packt_infobox">If you are already proficient with AWS, you might want to skim through the text that follows and only execute the commands.</div>
<p>The first thing we should do is get the AWS credentials.</p>
<p>Please open Amazon EC2 Console (<a href="https://console.aws.amazon.com/ec2/v2/home" target="_blank"><span class="URLPACKT">https://console.aws.amazon.com/ec2/v2/home</span></a>), click on your name from the top-right menu and select <span class="packt_screen">My Security Credentials</span>. You will see the screen with different types of credentials. Expand the <span class="packt_screen">Access Keys (Access Key ID and Secret Access Key)</span> section and click the <span class="packt_screen">Create New Access Key</span> button. Expand the <span class="packt_screen">Show Access Key</span> section to see the keys.</p>
<p>You will not be able to view the keys later on, so this is the only chance you'll have to <em>Download Key File</em>.</p>
<p>We'll put the keys as environment variables that will be used by the <strong>AWS Command Line Interface</strong> (<strong>AWS CLI</strong>) (<a href="https://aws.amazon.com/cli/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/cli/</span></a>).</p>
<p>Please replace <kbd>[...]</kbd> with your keys before executing the commands that follow:</p>
<pre>export AWS_ACCESS_KEY_ID=[...] 
 
export AWS_SECRET_ACCESS_KEY=[...] </pre>
<p>We'll need to install AWS <strong>Command Line Interface</strong> (<strong>CLI</strong>) (<a href="https://aws.amazon.com/cli/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/cli/</span></a>) and gather info about your account.</p>
<p>If you haven't already, please open the Installing the AWS Command Line Interface (<a href="https://docs.aws.amazon.com/cli/latest/userguide/installing.html" target="_blank"><span class="URLPACKT">https://docs.aws.amazon.com/cli/latest/userguide/installing.html</span></a>) page, and follow the installation method best suited for your OS.</p>
<div class="packt_tip">A note to Windows users: I found the most convenient way to get AWS CLI installed on Windows is to use Chocolatey (<a href="https://chocolatey.org/" target="_blank"><span class="URLPACKT">https://chocolatey.org/</span></a>). Download and install Chocolatey, then run <kbd>choco install awscli</kbd> from an Administrator Command Prompt. Later on in the chapter, Chocolatey will be used to install jq.</div>
<p>Once you're done, we'll confirm that the installation was successful by outputting the version.</p>
<div class="packt_infobox">A note to Windows users: You might need to reopen your <em>GitBash</em> terminal for the changes to the environment variable <kbd>PATH</kbd> to take effect.</div>
<pre><strong>aws --version</strong></pre>
<p>The output (from my laptop) is as follows:</p>
<pre><strong>aws-cli/1.11.15 Python/2.7.10 Darwin/16.0.0 botocore/1.4.72</strong></pre>
<p>Amazon EC2 is hosted in multiple locations worldwide. These locations are composed of regions and availability zones. Each region is a separate geographic area composed of multiple isolated locations known as availability zones. Amazon EC2 provides you the ability to place resources, such as instances, and data in multiple locations.</p>
<p>Next, we'll define the environment variable <kbd>AWS_DEFAULT_REGION</kbd> that will tell AWS CLI which region we'd like to use by default.</p>
<pre>export AWS_DEFAULT_REGION=us-east-2 </pre>
<p>For now, please note that you can change the value of the variable to any other region, as long as it has at least three availability zones. We'll discuss the reasons for using <kbd>us-east-2</kbd> region and the need for multiple availability zones soon.</p>
<p>Next, we'll create a few <strong>Identity and Access Management</strong> (<strong>IAM</strong>) resources. Even though we could create a cluster with the user you used to register to AWS, it is a good practice to create a separate account that contains only the privileges we'll need for the exercises that follow:</p>
<p>First, we'll create an IAM group called <kbd>kops</kbd>:</p>
<pre><strong>aws iam create-group \</strong>
<strong>    --group-name kops</strong>
  </pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>    "Group": {</strong>
<strong>        "Path": "/",</strong>
<strong>        "CreateDate": "2018-02-21T12:58:47.853Z",</strong>
<strong>        "GroupId": "AGPAIF2Y6HJF7YFYQBQK2",</strong>
<strong>        "Arn": "arn:aws:iam::036548781187:group/kops",</strong>
<strong>        "GroupName": "kops"</strong>
<strong>    }</strong>
<strong>}</strong></pre>
<p>We don't care much for any of the information from the output except that it does not contain an error message thus confirming that the group was created successfully.</p>
<p>Next, we'll assign a few policies to the group thus providing the future users of the group with sufficient permissions to create the objects we'll need.</p>
<p>Since our cluster will consist of EC2 (<a href="https://aws.amazon.com/ec2/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/ec2/</span></a>) instances, the group will need to have the permissions to create and manage them. We'll need a place to store the state of the cluster so we'll need access to S3 (<a href="https://aws.amazon.com/s3/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/s3/</span></a>). Furthermore, we need to add VPCs (<a href="https://aws.amazon.com/vpc/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/vpc/</span></a>) to the mix so that our cluster is isolated from prying eyes. Finally, we'll need to be able to create additional IAMs.</p>
<p>In AWS, user permissions are granted by creating policies. We'll need <em>AmazonEC2FullAccess</em>, <em>AmazonS3FullAccess</em>, <em>AmazonVPCFullAccess</em>, and <em>IAMFullAccess</em>.</p>
<p>The commands that attach the required policies to the <kbd>kops</kbd> group are as follows:</p>
<pre><strong>aws iam attach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess \</strong>
<strong>    --group-name kops</strong>
    
<strong>aws iam attach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \</strong>
<strong>    --group-name kops</strong>
    
<strong>aws iam attach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess \</strong>
<strong>    --group-name kops</strong>
   
<strong>aws iam attach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/IAMFullAccess \</strong>
<strong>    --group-name kops</strong>  </pre>
<p>Now that we have a group with the sufficient permissions, we should create a user as well.</p>
<pre><strong>aws iam create-user \</strong>
<strong>    --user-name kops</strong></pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>    "User": {</strong>
<strong>        "UserName": "kops",</strong>
<strong>        "Path": "/",</strong>
<strong>        "CreateDate": "2018-02-21T12:59:28.836Z",</strong>
<strong>        "UserId": "AIDAJ22UOS7JVYQIAVMWA",</strong>
<strong>        "Arn": "arn:aws:iam::036548781187:user/kops"</strong>
<strong>    }</strong>
<strong>}</strong>  </pre>
<p>Just as when we created the group, the contents of the output are not important, except as a confirmation that the command was executed successfully.</p>
<p>The user we created does not yet belong to the <kbd>kops</kbd> group. We'll fix that next:</p>
<pre><strong>aws iam add-user-to-group \</strong>
<strong>    --user-name kops \</strong>
<strong>    --group-name kops</strong>  </pre>
<p>Finally, we'll need access keys for the newly created user. Without them, we would not be able to act on its behalf.</p>
<pre><strong>aws iam create-access-key \</strong>
<strong>    --user-name kops &gt;kops-creds</strong>  </pre>
<p>We created access keys and stored the output in the <kbd>kops-creds</kbd> file. Let's take a quick look at its content.</p>
<pre><strong>cat kops-creds</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>    "AccessKey": {</strong>
<strong>        "UserName": "kops",</strong>
<strong>        "Status": "Active",</strong>
<strong>        "CreateDate": "2018-02-21T13:00:24.733Z",</strong>
<strong>        "SecretAccessKey": "...",</strong>
<strong>        "AccessKeyId": "..."</strong>
<strong>    }</strong>
<strong>}</strong>  </pre>
<p>Please note that I removed the values of the keys. I do not yet trust you enough with the keys of my AWS account.</p>
<p>We need the <kbd>SecretAccessKey</kbd> and <kbd>AccessKeyId</kbd> entries. So, the next step is to parse the content of the <kbd>kops-creds</kbd> file and store those two values as the environment variables <kbd>AWS_ACCESS_KEY_ID</kbd> and <kbd>AWS_SECRET_ACCESS_KEY</kbd>.</p>
<p>In the spirit of full automation, we'll use <kbd>jq</kbd> (<a href="https://stedolan.github.io/jq/" target="_blank"><span class="URLPACKT">https://stedolan.github.io/jq/</span></a>) to parse the contents of the <kbd>kops-creds</kbd> file. Please download and install the distribution suited for your OS.</p>
<div class="packt_tip">A note to Windows users: Using Chocolatey, install <kbd>jq</kbd> from an Administrator Command Prompt via <kbd>choco install jq</kbd>.</div>
<pre><strong>export AWS_ACCESS_KEY_ID=$(\</strong>
<strong>    cat kops-creds | jq -r \</strong>
<strong>    '.AccessKey.AccessKeyId')</strong>
    
<strong>export AWS_SECRET_ACCESS_KEY=$(</strong>
<strong>    cat kops-creds | jq -r \</strong>
<strong>    '.AccessKey.SecretAccessKey')</strong>  </pre>
<p>We used <kbd>cat</kbd> to output contents of the file and combined it with <kbd>jq</kbd> to filter the input so that only the field we need is retrieved.</p>
<p>From now on, all the AWS CLI commands will not be executed by the administrative user you used to register to AWS, but as <kbd>kops</kbd>.</p>
<div class="packt_infobox">It is imperative that the <kbd>kops-creds</kbd> file is secured and not accessible to anyone but people you trust. The best method to secure it depends from one organization to another. No matter what you do, do not write it on a post-it and stick it to your monitor. Storing it in one of your GitHub repositories is even worse.</div>
<p>Next, we should decide which availability zones we'll use. So, let's take a look at what's available in the <kbd>us-east-2</kbd> region.</p>
<pre><strong>aws ec2 describe-availability-zones \</strong>
    <strong>--region $AWS_DEFAULT_REGION</strong></pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>    "AvailabilityZones": [</strong>
<strong>        {</strong>
<strong>            "State": "available", </strong>
<strong>            "RegionName": "us-east-2", </strong>
<strong>            "Messages": [], </strong>
<strong>            "ZoneName": "us-east-2a"</strong>
<strong>        }, </strong>
<strong>        {</strong>
<strong>            "State": "available", </strong>
<strong>            "RegionName": "us-east-2", </strong>
<strong>            "Messages": [], </strong>
<strong>            "ZoneName": "us-east-2b"</strong>
<strong>        }, </strong>
<strong>        {</strong>
<strong>            "State": "available", </strong>
<strong>            "RegionName": "us-east-2", </strong>
<strong>            "Messages": [], </strong>
<strong>            "ZoneName": "us-east-2c"</strong>
<strong>        }</strong>
<strong>    ]</strong>
<strong>}</strong></pre>
<p>As we can see, the region has three availability zones. We'll store them in an environment variable.</p>
<div class="packt_infobox">A note to Windows users: Please use <kbd>tr '\r\n' ', '</kbd> instead of <kbd>tr '\n' ','</kbd> in the command that follows.</div>
<pre><strong>export ZONES=$(aws ec2 \</strong>
<strong>    describe-availability-zones \</strong>
<strong>    --region $AWS_DEFAULT_REGION \</strong>
<strong>    | jq -r \</strong>
<strong>    '.AvailabilityZones[].ZoneName' \</strong>
<strong>    | tr '\n' ',' | tr -d ' ')</strong>
  
<strong>ZONES=${ZONES%?}</strong>
    
<strong>echo $ZONES</strong>  </pre>
<p>Just as with the access keys, we used <kbd>jq</kbd> to limit the results only to the zone names, and we combined that with <kbd>tr</kbd> that replaced new lines with commas. The second command removes the trailing comma.</p>
<p>The output of the last command that echoed the values of the environment variable is as follows:</p>
<pre><strong>us-east-2a,us-east-2b,us-east-2c</strong>  </pre>
<p>We'll discuss the reasons behind the usage of three availability zones later on. For now, just remember that they are stored in the environment variable <kbd>ZONES</kbd>.</p>
<p>The last preparation step is to create SSH keys required for the setup. Since we might create some other artifacts during the process, we'll create a directory dedicated to the creation of the cluster.</p>
<pre><strong>mkdir -p cluster</strong>
    
<strong>cd cluster</strong>  </pre>
<p>SSH keys can be created through the <kbd>aws ec2</kbd> command <kbd>create-key-pair</kbd>:</p>
<pre><strong>aws ec2 create-key-pair \</strong>
<strong>    --key-name devops23 \</strong>
<strong>    | jq -r '.KeyMaterial' \</strong>
<strong>    &gt;devops23.pem</strong></pre>
<p>We created a new key pair, filtered the output so that only the <kbd>KeyMaterial</kbd> is returned, and stored it in the <kbd>devops23.pem</kbd> file.</p>
<p>For security reasons, we should change the permissions of the <kbd>devops23.pem</kbd> file so that only the current user can read it.</p>
<pre><strong>chmod 400 devops23.pem</strong> \ </pre>
<p>Finally, we'll need only the public segment of the newly generated SSH key, so we'll use <kbd>ssh-keygen</kbd> to extract it.</p>
<pre><strong>ssh-keygen -y -f devops23.pem </strong>
<strong>    &gt;devops23.pub</strong>  </pre>
<p>All those steps might look a bit daunting if this is your first contact with AWS. Nevertheless, they are pretty standard. No matter what you do in AWS, you'd need to perform, more or less, the same actions. Not all of them are mandatory, but they are good practice. Having a dedicated (non-admin) user and a group with only required policies is always a good idea. Access keys are necessary for any <kbd>aws</kbd> command. Without SSH keys, no one can enter into a server.</p>
<p>The good news is that we're finished with the prerequisites, and we can turn our attention towards creating a Kubernetes cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a kubernetes cluster in AWS</h1>
                </header>
            
            <article>
                
<p>We'll start by deciding the name of our soon to be created cluster. We'll choose to call it <kbd>devops23.k8s.local</kbd>. The latter part of the name (<kbd>.k8s.local</kbd>) is mandatory if we do not have a DNS at hand. It's a naming convention kops uses to decide whether to create a gossip-based cluster or to rely on a publicly available domain. If this would be a "real" production cluster, you would probably have a DNS for it. However, since I cannot be sure whether you do have one for the exercises in this book, we'll play it safe, and proceed with the gossip mode.</p>
<p>We'll store the name into an environment variable so that it is easily accessible.</p>
<pre><strong>export NAME=devops23.k8s.local</strong>  </pre>
<p>When we create the cluster, kops will store its state in a location we're about to configure. If you used Terraform, you'll notice that kops uses a very similar approach. It uses the state it generates when creating the cluster for all subsequent operations. If we want to change any aspect of a cluster, we'll have to change the desired state first, and then apply those changes to the cluster.</p>
<p>At the moment, when creating a cluster in AWS, the only option for storing the state are <kbd>Amazon S3</kbd> (<a href="https://aws.amazon.com/s3/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/s3/</span></a>) buckets. We can expect availability of additional stores soon. For now, S3 is our only option.</p>
<p>The command that creates an S3 bucket in our region is as follows:</p>
<pre><strong>export BUCKET_NAME=devops23-$(date +%s)</strong>
    
<strong>aws s3api create-bucket \</strong>
<strong>    --bucket $BUCKET_NAME \</strong>
<strong>    --create-bucket-configuration \ </strong>
<strong>    LocationConstraint=$AWS_DEFAULT_REGION</strong>  </pre>
<p>We created a bucket with a unique name and the output is as follows:</p>
<pre><strong>{</strong>
<strong>    "Location": http://devops23-1519993212.s3.amazonaws.com/</strong>
<strong>}</strong>  </pre>
<p>For simplicity, we'll define the environment variable <kbd>KOPS_STATE_STORE</kbd>. Kops will use it to know where we store the state. Otherwise, we'd need to use <kbd>--store</kbd> argument with every <kbd>kops</kbd> command.</p>
<pre><strong>export KOPS_STATE_STORE=s3://$BUCKET_NAME</strong>  </pre>
<p>There's only one thing missing before we create the cluster. We need to install kops.</p>
<p>If you are a <strong>MacOS user</strong>, the easiest way to install <kbd>kops</kbd> is through <kbd>Homebrew</kbd> <span class="MsoCommentReference">(</span><a href="https://brew.sh/" target="_blank"><span class="URLPACKT">https://brew.sh/</span></a><span class="MsoCommentReference">)</span>.</p>
<pre><strong>brew update &amp;&amp; brew install kops</strong>  </pre>
<p>As an alternative, we can download a release from GitHub.</p>
<pre><strong>curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s <br/>https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-darwin-amd64</strong>
    
<strong>chmod +x ./kops</strong>
    
<strong>sudo mv ./kops /usr/local/bin/</strong>  </pre>
<p>If, on the other hand, you're a <strong>Linux user</strong>, the commands that will install <kbd>kops</kbd> are as follows:</p>
<pre><strong>wget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s <br/>https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | <br/>cut -d '"' -f 4)/kops-linux-amd64</strong>
    
<strong>chmod +x ./kops</strong>
    
<strong>sudo mv ./kops /usr/local/bin/</strong>  </pre>
<p>Finally, if you are a <strong>Windows user</strong>, you cannot install <em>kops</em>. At the time of this writing, its releases do not include Windows binaries. Don't worry. I am not giving up on you, dear <em>Windows user</em>. We'll manage to overcome the problem soon by exploiting Docker's ability to run any Linux application. The only requirement is that you have Docker for Windows (<a href="https://www.docker.com/docker-windows" target="_blank"><span class="URLPACKT">https://www.docker.com/docker-windows</span></a>) installed.</p>
<p>I already created a Docker image that contains <kbd>kops</kbd> and its dependencies. So, we'll create an alias <kbd>kops</kbd> that will create a container instead running a binary. The result will be the same.</p>
<p>The command that creates the <kbd>kops</kbd> alias is as follows. Execute it only if you are a <strong>Windows user</strong>:</p>
<pre><strong>mkdir config</strong>
   
<strong>alias kops="docker run -it --rm \</strong>
<strong>    -v $PWD/devops23.pub:/devops23.pub \ </strong>
<strong>    -v $PWD/config:/config \</strong>
<strong>    -e KUBECONFIG=/config/kubecfg.yaml \ </strong>
<strong>    -e NAME=$NAME -e ZONES=$ZONES \</strong>
<strong>    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \ </strong>
<strong>    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \ </strong>
<strong>    -e KOPS_STATE_STORE=$KOPS_STATE_STORE \</strong>
<strong>    vfarcic/kops"</strong>  </pre>
<p>We won't go into details of all the arguments the <kbd>docker run</kbd> command uses. Their usage will become clear when we start using <kbd>kops</kbd>. Just remember that we are passing all the environment variables we might use as well as mounting the SSH key and the directory where <kbd>kops</kbd> will store <kbd>kubectl</kbd> configuration.</p>
<p>We are, finally, ready to create a cluster. But, before we do that, we'll spend a bit of time discussing the requirements we might have. After all, not all clusters are created equal, and the choices we are about to make might severely impact our ability to accomplish the goals we might have.</p>
<p>The first question we might ask ourselves is whether we want to have high-availability. It would be strange if anyone would answer no. Who doesn't want to have a cluster that is (almost) always available? Instead, we'll ask ourselves what the things that might bring our cluster down are.</p>
<p>When a node is destroyed, Kubernetes will reschedule all the applications that were running inside it into the healthy nodes. All we have to do is to make sure that, later on, a new server is created and joined the cluster, so that its capacity is back to the desired values. We'll discuss later how are new nodes created as a reaction to failures of a server. For now, we'll assume that will happen somehow.</p>
<p>Still, there is a catch. Given that new nodes need to join the cluster, if the failed server was the only master, there is no cluster to join. All is lost. The part is where master servers are. They host the critical components without which Kubernetes cannot operate.</p>
<p>So, we need more than one master node. How about two? If one fails, we still have the other one. Still, that would not work.</p>
<p>Every piece of information that enters one of the master nodes is propagated to the others, and only after the majority agrees, that information is committed. If we lose majority (50%+1), masters cannot establish a quorum and cease to operate. If one out of two masters is down, we can get only half of the votes, and we would lose the ability to establish the quorum. Therefore, we need three masters or more. Odd numbers greater than one are "magic" numbers. Given that we won't create a big cluster, three should do.</p>
<p>With three masters, we are safe from a failure of any single one of them. Given that failed servers will be replaced with new ones, as long as only one master fails at the time, we should be fault tolerant and have high availability.</p>
<div class="packt_tip">Always set an odd number greater than one for master nodes.</div>
<p>The whole idea of having multiple masters does not mean much if an entire data center goes down.</p>
<p>Attempts to prevent a data center from failing are commendable. Still, no matter how well a data center is designed, there is always a scenario that might cause its disruption. So, we need more than one data center. Following the logic behind master nodes, we need at least three. But, as with almost anything else, we cannot have any three (or more) data centers. If they are too far apart, the latency between them might be too high. Since every piece of information is propagated to all the masters in a cluster, slow communication between data centers would severely impact the cluster as a whole.</p>
<p>All in all, we need three data centers that are close enough to provide low latency, and yet physically separated, so that failure of one does not impact the others. Since we are about to create the cluster in AWS, we'll use <strong>availability zones</strong> (<strong>AZs</strong>) which are physically separated data centers with low latency.</p>
<div class="packt_tip">Always spread your cluster between at least three data centers which are close enough to warrant low latency.</div>
<p>There's more to high-availability to running multiple masters and spreading a cluster across multiple availability zones. We'll get back to this subject later. For now, we'll continue exploring the other decisions we have to make.</p>
<p>Which networking shall we use? We can choose between <em>kubenet</em>, <em>CNI</em>, <em>classic</em>, and <em>external</em> networking.</p>
<p>The classic Kubernetes native networking is deprecated in favor of kubenet, so we can discard it right away.</p>
<p>The external networking is used in some custom implementations and for particular use cases, so we'll discard that one as well.</p>
<p>That leaves us with kubenet and CNI.</p>
<p><strong>Container Network Interface</strong> (<strong>CNI</strong>) allows us to plug in a third-party networking driver. Kops supports Calico (<a href="https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/" target="_blank"><span class="URLPACKT">https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/</span></a>), flannel (<a href="https://github.com/coreos/flannel" target="_blank"><span class="URLPACKT">https://github.com/coreos/flannel</span></a>), Canal (Flannel + Calico) (<a href="https://github.com/projectcalico/canal" target="_blank"><span class="URLPACKT">https://github.com/projectcalico/canal</span></a>), kopeio-vxlan (<a href="https://github.com/kopeio/networking" target="_blank"><span class="URLPACKT">https://github.com/kopeio/networking</span></a>), kube-router (<a href="https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer" target="_blank"><span class="URLPACKT">https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer</span></a>), romana (<a href="https://github.com/romana/romana" target="_blank"><span class="URLPACKT">https://github.com/romana/romana</span></a>), weave (<a href="https://github.com/weaveworks-experiments/weave-kube" target="_blank"><span class="URLPACKT">https://github.com/weaveworks-experiments/weave-kube</span></a>), and <kbd>amazon-vpc-routed-eni</kbd> (<a href="https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend" target="_blank"><span class="URLPACKT">https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend</span></a>) networks. Each of those networks comes with pros and cons and differs in its implementation and primary objectives. Choosing between them would require a detailed analysis of each. We'll leave a comparison of all those for some other time and place. Instead, we'll focus on <kbd>kubenet</kbd>.</p>
<p>Kubenet is kops' default networking solution. It is Kubernetes native networking, and it is considered battle tested and very reliable. However, it comes with a limitation. On AWS, routes for each node are configured in AWS VPC routing tables. Since those tables cannot have more than fifty entries, kubenet can be used in clusters with up to fifty nodes. If you're planning to have a cluster bigger than that, you'll have to switch to one of the previously mentioned CNIs.</p>
<div class="packt_tip">Use kubenet networking if your cluster is smaller than fifty nodes.</div>
<p>The good news is that using any of the networking solutions is easy. All we have to do is specify the <kbd>--networking</kbd> argument followed with the name of the network.</p>
<p>Given that we won't have the time and space to evaluate all the CNIs, we'll use kubenet as the networking solution for the cluster we're about to create. I encourage you to explore the other options on your own (or wait until I write a post or a new book).</p>
<p>Finally, we are left with only one more choice we need to make. What will be the size of our nodes? Since we won't run many applications, <kbd>t2.small</kbd> should be more than enough and will keep AWS costs to a minimum. <kbd>t2.micro</kbd> is too small, so we elected the second smallest among those AWS offers.</p>
<div class="packt_infobox">You might have noticed that we did not mention persistent volumes. We'll explore them in the next chapter.</div>
<p>The command that creates a cluster using the specifications we discussed is as follows:</p>
<pre><strong>kops create cluster \</strong>
<strong>    --name $NAME \</strong>
<strong>    --master-count 3 \</strong>
<strong>    --node-count 1 \</strong>
<strong>    --node-size t2.small \</strong>
<strong>    --master-size t2.small \</strong>
<strong>    --zones $ZONES \</strong>
<strong>    --master-zones $ZONES \</strong>
<strong>    --ssh-public-key devops23.pub \</strong>
<strong>    --networking kubenet \</strong>
<strong>    --kubernetes-version v1.8.4 \</strong>
<strong>    --yes</strong>  </pre>
<p>We specified that the cluster should have three masters and one worker node. Remember, we can always increase the number of workers, so there's no need to start with more than what we need at the moment.</p>
<p>The sizes of both worker nodes and masters are set to <kbd>t2.small</kbd>. Both types of nodes will be spread across the three availability zones we specified through the environment variable <kbd>ZONES</kbd>. Further on, we defined the public key and the type of networking.</p>
<p>We used <kbd>--kubernetes-version</kbd> to specify that we prefer to run version <kbd>v1.8.4</kbd>. Otherwise, we'd get a cluster with the latest version considered stable by kops. Even though running latest stable version is probably a good idea, we'll need to be a few versions behind to demonstrate some of the features kops has to offer.</p>
<p>By default, kops sets <kbd>authorization</kbd> to <kbd>AlwaysAllow</kbd>. Since this is a simulation of a production-ready cluster, we changed it to <kbd>RBAC</kbd>, which we already explored in one of the previous chapters.</p>
<p>The <kbd>--yes</kbd> argument specifies that the cluster should be created right away. Without it, <kbd>kops</kbd> would only update the state in the S3 bucket, and we'd need to execute <kbd>kops apply</kbd> to create the cluster. Such two-step approach is preferable, but I got impatient and would like to see the cluster in all its glory as soon as possible.</p>
<p>The output of the command is as follows:</p>
<pre><strong>...</strong>
<strong>kops has set your kubectl context to devops23.k8s.local</strong>
    
<strong>Cluster is starting.  It should be ready in a few minutes.</strong>
    
<strong>Suggestions:</strong>
<strong> * validate cluster: kops validate cluster</strong>
<strong> * list nodes: kubectl get nodes --show-labels</strong>
<strong> * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.devops23.k8s.local</strong>
<strong>The admin user is specific to Debian. If not using Debian please use the appropriate user based on your OS.</strong>
<strong> * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs/addons.md</strong>  </pre>
<p>We can see that the <kbd>kubectl</kbd> context was changed to point to the new cluster which is starting, and will be ready soon. Further down are a few suggestions of the next actions. We'll skip them, for now.</p>
<div class="packt_infobox"><span class="packt_screen">A note to Windows users<br/></span>Kops was executed inside a container. It changed the context inside the container that is now gone. As a result, your local <kbd>kubectl</kbd> context was left intact. We'll fix that by executing <kbd>kops export kubecfg --name ${NAME}</kbd> and <kbd>export KUBECONFIG=$PWD/config/kubecfg.yaml</kbd>. The first command exported the config to <kbd>/config/kubecfg.yaml</kbd>. That path was specified through the environment variable <kbd>KUBECONFIG</kbd> and is mounted as <kbd>config/kubecfg.yaml</kbd> on local hard disk. The latter command exports <kbd>KUBECONFIG</kbd> locally. Through that variable, kubectl is now instructed to use the configuration in <kbd>config/kubecfg.yaml</kbd> instead of the default one. Before you run those commands, please give AWS a few minutes to create all the EC2 instances and for them to join the cluster. After waiting and executing those commands, you'll be all set.</div>
<p>We'll use kops to retrieve the information about the newly created cluster.</p>
<pre><strong>kops get cluster</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME               CLOUD ZONES</strong>
<strong>devops23.k8s.local aws   us-east-2a,us-east-2b,us-east-2c</strong>  </pre>
<p>This information does not tell us anything new. We already knew the name of the cluster and the zones it runs in.</p>
<p>How about <kbd>kubectl cluster-info</kbd>?</p>
<pre><strong>kubectl cluster-info</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>Kubernetes master is running at https://api-devops23-k8s-local-ivnbim-6094461<br/>90.us-east-2.elb.amazonaws.com</strong>
<strong>KubeDNS is running at https://api-devops23-k8s-local-ivnbim-609446190.us-east<br/>-2.elb.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</strong>
    
<strong>To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</strong>  </pre>
<p>We can see that the master is running as well as KubeDNS. The cluster is probably ready. If in your case KubeDNS did not appear in the output, you might need to wait for a few more minutes.</p>
<p>We can get more reliable information about the readiness of our new cluster through the <kbd>kops validate</kbd> command.</p>
<pre><strong>kops validate cluster</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>Using cluster from kubectl context: devops23.k8s.local</strong>
   
<strong>Validating cluster devops23.k8s.local</strong>
    
<strong>INSTANCE GROUPS</strong>
<strong>NAME              ROLE   MACHINETYPE MIN MAX SUBNETS</strong>
<strong>master-us-east-2a Master t2.small    1   1   us-east-2a</strong>
<strong>master-us-east-2b Master t2.small    1   1   us-east-2b</strong>
<strong>master-us-east-2c Master t2.small    1   1   us-east-2c</strong>
<strong>nodes             Node   t2.small    1   1   us-east-2a,us-east-2b,us-east-2c</strong>
    
<strong>NODE STATUS</strong>
<strong>NAME                 ROLE   READY</strong>
<strong>ip-172-20-120-133... master True</strong>
<strong>ip-172-20-34-249...  master True</strong>
<strong>ip-172-20-65-28...   master True</strong>
<strong>ip-172-20-95-101...  node   True</strong>
    
<strong>Your cluster devops23.k8s.local is ready</strong>  </pre>
<p>That is useful. We can see that the cluster uses four instance groups or, to use AWS terms, four <strong>auto-scaling groups</strong> (<strong>ASGs</strong>). There's one for each master, and there's one for all the (worker) nodes.</p>
<p>The reason each master has a separate ASG lies in need to ensure that each is running in its own <strong>availability zone</strong> (<strong>AZ</strong>). That way we can guarantee that failure of the whole AZ will affect only one master. Nodes (workers), on the other hand, are not restricted to any specific AZ. AWS is free to schedule nodes in any AZ that is available.</p>
<p>We'll discuss ASGs in more detail later on.</p>
<p>Further down the output, we can see that there are four servers, three with masters, and one with worker node. All are ready.</p>
<p>Finally, we got the confirmation that our <kbd>cluster devops23.k8s.local is ready</kbd>.</p>
<p>Using the information we got so far, we can describe the cluster through the <em>figure 14-1</em>.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/41026644-f0e5-4485-94ef-86b733e7c2e7.png" style="width:24.75em;height:11.25em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-1: The servers that form the Kubernetes cluster</div>
<p>There's apparently much more to the cluster than what is depicted in the <em>figure 14-1</em>. So, let's try to discover the goodies kops created for us.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the components that constitute the cluster</h1>
                </header>
            
            <article>
                
<p>When kops created the VMs (EC2 instances), the first thing it did was to execute <em>nodeup</em>. It, in turn, installed a few packages. It made sure that Docker, Kubelet, and Protokube are up and running.</p>
<p><strong>Docker</strong> runs containers. It would be hard for me to imagine that you don't know what Docker does, so we'll skip to the next in line.</p>
<p><strong>Kubelet</strong> is Kubernetes' node agent. It runs on every node of a cluster, and its primary purpose is to run Pods. Or, to be more precise, it ensures that the containers described in PodSpecs are running as long as they are healthy. It primarily gets the information about the Pods it should run through Kubernetes' API server. As an alternative, it can get the info through files, HTTP endpoints, and HTTP servers.</p>
<p>Unlike Docker and Kubelet, <strong>Protokube</strong> is specific to kops. Its primary responsibilities are to discover master disks, to mount them, and to create manifests. Some of those manifests are used by Kubelet to create system-level Pods and to make sure that they are always running.</p>
<p>Besides starting the containers defined through Pods in the manifests (created by Protokube), Kubelet also tries to contact the API server which, eventually, is also started by it. Once the connection is established, Kubelet registers the node where it is running.</p>
<p>All three packages are running on all the nodes, no matter whether they are masters or workers:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/895f0fd4-538c-4178-886e-beefe7b0362b.png" style="width:29.25em;height:19.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-2: The servers that form the Kubernetes cluster</div>
<p>Let's take a look at the system-level Pods currently running in our cluster:</p>
<pre><strong>kubectl --namespace kube-system get pods</strong> </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                                         READY STATUS  RESTARTS AGE</strong>
<strong>dns-controller-...                           1/1   Running 0        5m</strong>
<strong>etcd-server-events-ip-172-20-120-133...      1/1   Running 0        5m</strong>
<strong>etcd-server-events-ip-172-20-34-249...       1/1   Running 1        4m</strong>
<strong>etcd-server-events-ip-172-20-65-28...        1/1   Running 0        4m</strong>
<strong>etcd-server-ip-172-20-120-133...             1/1   Running 0        4m</strong>
<strong>etcd-server-ip-172-20-34-249...              1/1   Running 1        3m</strong>
<strong>etcd-server-ip-172-20-65-28...               1/1   Running 0        4m</strong>
<strong>kube-apiserver-ip-172-20-120-133...          1/1   Running 0        4m</strong>
<strong>kube-apiserver-ip-172-20-34-249...           1/1   Running 3        3m</strong>
<strong>kube-apiserver-ip-172-20-65-28...            1/1   Running 1        4m</strong>
<strong>kube-controller-manager-ip-172-20-120-133... 1/1   Running 0        4m</strong>
<strong>kube-controller-manager-ip-172-20-34-249...  1/1   Running 0        4m</strong>
<strong>kube-controller-manager-ip-172-20-65-28...   1/1   Running 0        4m</strong>
<strong>kube-dns-7f56f9f8c7-...                      3/3   Running 0        5m</strong>
<strong>kube-dns-7f56f9f8c7-...                      3/3   Running 0        2m</strong>
<strong>kube-dns-autoscaler-f4c47db64-...            1/1   Running 0        5m</strong>
<strong>kube-proxy-ip-172-20-120-133...              1/1   Running 0        4m</strong>
<strong>kube-proxy-ip-172-20-34-249...               1/1   Running 0        4m</strong>
<strong>kube-proxy-ip-172-20-65-28...                1/1   Running 0        4m</strong>
<strong>kube-proxy-ip-172-20-95-101...               1/1   Running 0        3m</strong>
<strong>kube-scheduler-ip-172-20-120-133...          1/1   Running 0        4m</strong>
<strong>kube-scheduler-ip-172-20-34-249...           1/1   Running 0        4m</strong>
<strong>kube-scheduler-ip-172-20-65-28...            1/1   Running 0        4m</strong>  </pre>
<p>As you can see, quite a few core components are running.</p>
<p>We can divide core (or system-level) components into two groups. Master components run only on masters. In our case, they are <kbd>kube-apiserver</kbd>, <kbd>kube-controller-manager</kbd>, <kbd>kube-scheduler</kbd>, <kbd>etcd</kbd>, and <kbd>dns-controller</kbd>. Node components run on all the nodes, both masters and workers. We already discussed a few of those. In addition to Protokube, Docker, and Kubelet, we got <kbd>kube-proxy</kbd>, as one more node component. Since this might be the first time you heard about those core components, we'll briefly explain each of their functions.</p>
<p><strong>Kubernetes API Server</strong> (<kbd>kube-apiserver</kbd>) accepts requests to create, update, or remove Kubernetes resources. It listens on ports <kbd>8080</kbd> and <kbd>443</kbd>. The former is insecure and is only reachable from the same server. Through it, the other components can register themselves without requiring a token. The former port (<kbd>443</kbd>) is used for all external communications with the API Server. That communication can be user-facing like, for example, when we send a <kbd>kubectl</kbd> command. Kubelet also uses <kbd>443</kbd> port to reach the API server and register itself as a node.</p>
<p>No matter who initiates communication with the API Server, its purpose is to validate and configure API object. Among others, those can be Pods, Services, ReplicaSets, and others. Its usage is not limited to user-facing interactions. All the components in the cluster interact with the API Server for the operations that require a cluster-wide shared state.</p>
<p>The shared state of the cluster is stored in <kbd>etcd</kbd> (<a href="https://github.com/coreos/etcd" target="_blank"><span class="URLPACKT">https://github.com/coreos/etcd</span></a>). It is a key/value store where all cluster data is kept, and it is highly available through consistent data replication. It is split into two Pods, where <kbd>etcd-server</kbd> holds the state of the cluster and <kbd>etcd-server-events</kbd> stores the events.</p>
<p>Kops creates an <strong>EBS volume</strong> for each <kbd>etcd</kbd> instance. It serves as its storage.</p>
<p><strong>Kubernetes Controller Manager</strong> (<kbd>kube-controller-manager</kbd>) is in charge of running controllers. You already saw a few controllers in action like <kbd>ReplicaSets</kbd> and <kbd>Deployments</kbd>. Apart from object controllers like those, <kbd>kube-controller-manager</kbd> is also in charge of Node Controllers responsible for monitoring servers and responding when one becomes unavailable.</p>
<p><strong>Kubernetes Scheduler</strong> (<kbd>kube-scheduler</kbd>) watches the API Server for new Pods and assigns them to a node. From there on, those Pods are run by Kubelet on the allocated node.</p>
<p><strong>DNS Controller</strong> (<kbd>dns-controller</kbd>) allows nodes and users to discover the API Server.</p>
<p><strong>Kubernetes Proxy</strong> (<kbd>kube-proxy</kbd>) reflects Services defined through the API Server. It is in charge of TCP and UDP forwarding. It runs on all nodes of the cluster (both masters and workers).</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/3b27529b-a97b-4ebd-b2a1-52e9976c3280.png" style="width:46.50em;height:28.58em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-3: The core components of the cluster</div>
<p>There's much more going on in our new cluster. For now, we explored only the major components.</p>
<p>Next, we'll try to update our cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the cluster</h1>
                </header>
            
            <article>
                
<p>No matter how much we plan, we will never manage to have a cluster with capacity that should serve us equally well today as tomorrow. Things change, and we'll need to be able to adapt to those changes. Ideally, our cluster should increase and decrease its capacity automatically by evaluating metrics and firing alerts that would interact with kops or directly with AWS. However, that is an advanced topic that we won't be able to cover. For now, we'll limit the scope to manual cluster updates.</p>
<p>With kops, we cannot update the cluster directly. Instead, we edit the desired state of the cluster stored, in our case, in the S3 bucket. Once the state is changed, kops will make the necessary changes to comply with the new desire.</p>
<p>We'll try to update the cluster so that the number of worker nodes is increased from one to two. In other words, we want to add one more server to the cluster.</p>
<p>Let's see the sub-commands provided through <kbd>kops edit</kbd>.</p>
<pre><strong>kops edit --help</strong>  </pre>
<p>The output, limited to the available commands, is as follows:</p>
<pre><strong>...</strong>
<strong>Available Commands:</strong>
<strong>  cluster       Edit cluster.</strong>
<strong>  federation    Edit federation.</strong>
<strong>  instancegroup Edit instancegroup.</strong>
<strong>...</strong>  </pre>
<p>We have three types of edits we can make. We did not set up federation, so that one is out of the game. You might think that <kbd>cluster</kbd> would provide the possibility to create a new worker node. However, that is not the case. If you execute <kbd>kops edit cluster --name $NAME</kbd>, you'll see that nothing in the configuration indicates how many nodes we should have. That is normal considering that we should not create servers in AWS directly. Just as Kubernetes, AWS also prefers declarative approach over imperative. At least, when dealing with EC2 instances.</p>
<p>Instead of sending an imperative instruction to create a new node, we'll change the value of the <strong>Auto-Scaling Group</strong> (<strong>ASG</strong>) related to worker nodes. Once we change ASG values, AWS will make sure that it complies with the new desire. It'll not only create a new server to comply with the new ASG sizes, but it will also monitor EC2 instances and maintain the desired number in case one of them fails.</p>
<p>So, we'll choose the third <kbd>kops edit</kbd> option.</p>
<pre><strong>kops edit ig --name $NAME nodes</strong>  </pre>
<p>We executed <kbd>kops edit ig</kbd> command, where <kbd>ig</kbd> is one of the aliases of <kbd>instancegroup</kbd>. We specified the name of the cluster with the <kbd>--name</kbd> argument. Finally, we set the type of the servers to <kbd>nodes</kbd>. As a result, we are presented with the <kbd>InstanceGroup</kbd> config for the Auto-Scaling Group associated with worker nodes.</p>
<p>The output is as follows.</p>
<pre><strong>apiVersion: kops/v1alpha2</strong>
<strong>kind: InstanceGroup</strong>
<strong>metadata:</strong>
<strong>  creationTimestamp: 2018-02-23T00:04:50Z</strong>
<strong>  labels:</strong>
<strong>    kops.k8s.io/cluster: devops23.k8s.local</strong>
<strong>  name: nodes</strong>
<strong>spec:</strong>
<strong>  image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-01-14</strong>
<strong>  machineType: t2.small</strong>
<strong>  maxSize: 1</strong>
<strong>  minSize: 1</strong>
<strong>  nodeLabels:</strong>
<strong>    kops.k8s.io/instancegroup: nodes</strong>
<strong>  role: Node</strong>
<strong>  subnets:</strong>
<strong>  - us-east-2a</strong>
<strong>  - us-east-2b</strong>
<strong>  - us-east-2c</strong>  </pre>
<p>Bear in mind that what you're seeing on the screen is not the standard output (<kbd>stdout</kbd>). Instead, the configuration is opened in your default editor. In my case, that is <kbd>vi</kbd>.</p>
<p>We can see some useful information from this config. For example, the <kbd>image</kbd> used to create EC2 instances is based on Debian. It is custom made for kops. The <kbd>machineType</kbd> represents EC2 size which is set to <kbd>t2.small</kbd>. Further down, you can see that we're running the VMs in three subnets or, since we're in AWS, three availability zones.</p>
<p>The parts of the config we care about are the <kbd>spec</kbd> entries <kbd>maxSize</kbd> and <kbd>minSize</kbd>. Both are set to <kbd>1</kbd> since that is the number of worker nodes we specified when we created the cluster. Please change the values of those two entries to <kbd>2</kbd>, save, and exit.</p>
<div class="packt_tip">If you're using <kbd>vi</kbd> as your default editor, you'll need to press <em>I</em> to enter into the <kbd>insert</kbd> mode. From there on, you can change the values. Once you're finished editing, please press the <em>ESC</em> key, followed by <kbd>:wq</kbd>. Colon (<kbd>:</kbd>) allows us to enter commands, <kbd>w</kbd> is translated to save, and <kbd>q</kbd> to quit. Don't forget to press the enter key. If, on the other hand, you are not using <kbd>vi</kbd>, you're on your own. I'm sure that you'll know how to operate your default editor. If not, Google is your friend.</div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/2ce16a02-cba2-4136-b00d-e4282ebd6cc0.png" style="width:43.92em;height:31.33em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-4: The process behind the <kbd>kops edit</kbd> command</div>
<p>Now that we changed the configuration, we need to tell kops that we want it to update the cluster to comply with the new desired state.</p>
<pre><strong>kops update cluster --name $NAME --yes</strong>  </pre>
<p>The output, limited to the last few lines, is as follows:</p>
<pre><strong>...</strong>
<strong>kops has set your kubectl context to devops23.k8s.local</strong>
   
<strong>Cluster changes have been applied to the cloud.</strong>
    
    
<strong>Changes may require instances to restart: kops rolling-update cluster</strong>  </pre>
<p>We can see that kops set our <kbd>kubectl</kbd> context to the cluster we updated. There was no need for that since that was already our context, but it did that anyway. Further on, we got the confirmation that the changes <kbd>have been applied to the cloud</kbd>.</p>
<p>The last sentence is interesting. It informed us that we can use <kbd>kops rolling-update</kbd>. The <kbd>kops update</kbd> command applies all the changes to the cluster at once. That can result in downtime. For example, if we wanted to change the image to a newer version, running <kbd>kops update</kbd> would recreate all the worker nodes at once. As a result, we'd have downtime from the moment instances are shut down until the new ones are created, and Kubernetes schedules the Pods in them. Kops knows that such an action should not be allowed so, if the update requires that servers are replaced, it does nothing expecting that you'll execute <kbd>kops rolling-update</kbd> afterward. That is not our case. Adding new nodes does not require restarts or replacement of the existing servers.</p>
<p>The <kbd>kops rolling-update</kbd> command intends to apply the changes without downtime. It would apply them to one server at the time so that most of the servers are always running. In parallel, Kubernetes would be rescheduling the Pods that were running on the servers that were brought down.</p>
<div class="packt_tip">As long as our applications are scaled, <kbd>kops rolling-update</kbd> should not produce downtime.</div>
<p>Let's see what happened when we executed the <kbd>kops update</kbd> command.</p>
<ol>
<li>Kops retrieved the desired state from the S3 bucket.</li>
<li>Kops sent requests to AWS API to change the values of the workers ASG.</li>
<li>AWS modified the values of the workers ASG by increasing them by 1.</li>
<li>ASG created a new EC2 instance to comply with the new sizing.</li>
<li>Protokube installed Kubelet and Docker and created the manifest file with the list of Pods.</li>
<li>Kubelet read the manifest file and run the container that forms the <kbd>kube-proxy</kbd> Pod (the only Pod on the worker nodes).</li>
</ol>
<ol start="7">
<li>Kubelet sent a request to the <kbd>kube-apiserver</kbd> (through the <kbd>dns-controller</kbd>) to register the new node and join it to the cluster. The information about the new node is stored in <kbd>etcd</kbd>.</li>
</ol>
<p>This process is almost identical to the one used to create the nodes of the cluster.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a24d8159-561d-4817-b0ea-89d5683c7081.png" style="width:40.83em;height:31.75em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-5: The process behind the <kbd>kops update</kbd> command</div>
<p>Unless you are a very slow reader, ASG created a new EC2 instance, and Kubelet joined it to the cluster. We can confirm that through the <kbd>kops validate</kbd> command.</p>
<pre><strong>kops validate cluster</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>Validating cluster devops23.k8s.local</strong>
    
<strong>INSTANCE GROUPS</strong>
<strong>NAME              ROLE   MACHINETYPE MIN MAX SUBNETS</strong>
<strong>master-us-east-2a Master t2.small    1   1   us-east-2a</strong>
<strong>master-us-east-2b Master t2.small    1   1   us-east-2b</strong>
<strong>master-us-east-2c Master t2.small    1   1   us-east-2c</strong>
<strong>nodes             Node   t2.small    2   2   us-east-2a,us-east-2b,us-east-2c</strong>
  
<strong>NODE STATUS</strong>
<strong>NAME                 ROLE   READY</strong>
<strong>ip-172-20-120-133... master True</strong>
<strong>ip-172-20-33-237...  node   True</strong>
<strong>ip-172-20-34-249...  master True</strong>
<strong>ip-172-20-65-28...   master True</strong>
<strong>ip-172-20-95-101...  node   True</strong>
    
<strong>Your cluster devops23.k8s.local is ready</strong>  </pre>
<p>We can see that now we have two nodes (there was one before) and that they are located somewhere inside the three <kbd>us-east-2</kbd> availability zones.</p>
<p>Similarly, we can use <kbd>kubectl</kbd> to confirm that Kubernetes indeed added the new worker node to the cluster.</p>
<pre><strong>kubectl get nodes</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                 STATUS ROLES  AGE VERSION</strong>
<strong>ip-172-20-120-133... Ready  master 13m v1.8.4</strong>
<strong>ip-172-20-33-237...  Ready  node   1m  v1.8.4</strong>
<strong>ip-172-20-34-249...  Ready  master 13m v1.8.4</strong>
<strong>ip-172-20-65-28...   Ready  master 13m v1.8.4</strong>
<strong>ip-172-20-95-101...  Ready  node   12m v1.8.4</strong>  </pre>
<p>That was easy, wasn't it? From now on, we can effortlessly add or remove nodes.</p>
<p>How about upgrading?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upgrading the cluster manually</h1>
                </header>
            
            <article>
                
<p>The process to upgrade the cluster depends on what we want to do.</p>
<p>If we'd like to upgrade it to a specific Kubernetes version, we can execute a similar process like the one we used to add a new worker node.</p>
<pre><strong>kops edit cluster $NAME</strong>  </pre>
<p>Just as before, we are about to edit the cluster definition. The only difference is that this time we're not editing a specific instance group, but the cluster as a whole.</p>
<p>If you explore the YAML file in front of you, you'll see that it contains the information we specified when we created the cluster, combined with the kops default values that we omitted to set.</p>
<p>For now, we're interested in <kbd>kubernetesVersion</kbd>. Please find it and change the version from <kbd>v1.8.4</kbd> to <kbd>v1.8.5</kbd>. Save and exit.</p>
<p>Now that we modified the desired state of the cluster, we can proceed with <kbd>kops update</kbd>.</p>
<pre><strong>kops update cluster $NAME</strong>  </pre>
<p>The last line of the output indicates that we <em>must specify</em> <kbd>--yes</kbd> <em>to apply changes</em>. Unlike the previous time we executed <kbd>kops update</kbd>, now we did not specify the argument <kbd>--yes</kbd>. As a result, we got a preview, or a dry-run, of what would happen if we apply the change. Previously, we added a new worker node, and that operation did not affect the existing servers. We were brave enough to update the cluster without previewing which resources will be created, updated, or destroyed. However, this time we are upgrading the servers in the cluster. Existing nodes will be replaced with new ones, and that is potentially dangerous operation. Later on, we might trust kops to do what's right and skip the preview altogether. But, for now, we should evaluate what will happen if we proceed.</p>
<p>Please go through the output. You'll see a git-like diff of the changes that will be applied to some of the resources that form the cluster. Take your time.</p>
<p>Now that you are confident with the changes, we can apply them.</p>
<pre><strong>kops update cluster $NAME --yes</strong>  </pre>
<p>The last line of the output states that <kbd>changes may require instances to restart: kops rolling-update cluster</kbd>. We already saw that message before but, this time, the update was not performed. The reason is simple, even though not necessarily very intuitive. We can update auto-scaling groups since that results in creation or destruction of nodes. But, when we need to replace them, as in this case, it would be disastrous to execute a simple update. Updating everything at once would, at best, produce a downtime. In our case, it's even worse. Destroying all the masters at once would likely result in a loss of quorum. A new cluster might not be able to recuperate.</p>
<p>All in all, kops requires an extra step when "big bang" updating of the cluster might result in undesirable results. So, we need to execute the <kbd>kops rolling-update</kbd> command. Since we're still insecure, we'll run a preview first.</p>
<pre><strong>kops rolling-update cluster $NAME</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME              STATUS      NEEDUPDATE READY MIN MAX NODES</strong>
<strong>master-us-east-2a NeedsUpdate 1          0     1   1   1</strong>
<strong>master-us-east-2b NeedsUpdate 1          0     1   1   1</strong>
<strong>master-us-east-2c NeedsUpdate 1          0     1   1   1</strong>
<strong>nodes             NeedsUpdate 2          0     2   2   2</strong>
    
<strong>Must specify --yes to rolling-update.</strong>  </pre>
<p>We can see that all the nodes require an update. Since we already evaluated the changes through the output of the <kbd>kops update</kbd> command, we'll proceed and apply rolling updates.</p>
<pre><strong>kops rolling-update cluster $NAME --yes</strong>  </pre>
<p>The rolling update process started, and it will take approximately 30 minutes to complete.</p>
<p>We'll explore the output as it comes:</p>
<pre><strong>NAME              STATUS      NEEDUPDATE READY MIN MAX NODES</strong>
<strong>master-us-east-2a NeedsUpdate 1          0     1   1   1</strong>
<strong>master-us-east-2b NeedsUpdate 1          0     1   1   1</strong>
<strong>master-us-east-2c NeedsUpdate 1          0     1   1   1</strong>
<strong>nodes             NeedsUpdate 2          0     2   2   2</strong>  </pre>
<p>The output starts with the same information we got when we asked for a preview, so there's not much to comment:</p>
<pre><strong>I0225 23:03:03.993068       1 instancegroups.go:130] Draining the node: "ip-1<br/> 72-20-40-167...".</strong>
<strong>node "ip-172-20-40-167..." cordoned</strong>
<strong>node "ip-172-20-40-167..." cordoned</strong>
<strong>WARNING: Deleting pods not managed by ReplicationController, <br/> ReplicaSet, Job, DaemonSet or StatefulSet: etcd-server-events-<br/> ip-172-20-40-167..., etcd-server-ip-172-20-40-167..., kube-apiserver-ip-172-20-40-167..., kube-controller-manager-ip-172-20-40-167..., <br/> kube-proxy-ip-172-20-40-167..., kube-scheduler-ip-172-20-40-167...</strong>
<strong>node "ip-172-20-40-167..." drained</strong></pre>
<p>Instead of destroying the first node, kops picked one masters and drained it. That way, the applications running on it can shut down gracefully. We can see that it drained <kbd>etcd-server-events</kbd>, <kbd>etcd-server-ip</kbd>, <kbd>kube-apiserver</kbd>, <kbd>kube-controller-manager</kbd>, <kbd>kube-proxy</kbd>, <kbd>kube-scheduler</kbd> Pods running on the server <kbd>ip-172-20-40-167</kbd>. As a result, Kubernetes rescheduled them to one of the healthy nodes. That might not be true for all the Pods but only for those that can be rescheduled.</p>
<pre><strong>I0225 23:04:37.479407 1 instancegroups.go:237] Stopping instance "i-<br/> 06d40d6ff583fe10b", node "ip-172-20-40-167...", in group "master-us-east-<br/> 2a.masters.devops23.k8s.local".</strong>  </pre>
<p>We can see that after draining finished, the master node was stopped. Since each master is associated with an auto-scaling group, AWS will detect that the node is no more, and start a new one. Once the new server is initialized, <kbd>nodeup</kbd> will execute and install Docker, Kubelet, and Protokube. The latter will create the manifest that will be used by Kubelet to run the Pods required for a master node. Kubelet will also register the new node with one of the healthy masters.</p>
<p>That part of the process is the same as the one executed when creating a new cluster or when adding new servers. It is the part that takes longest to complete (around five minutes).</p>
<pre><strong>I0225 23:09:38.218945 1 instancegroups.go:161] Validating the cluster.</strong>
<strong>I0225 23:09:39.437456 1 instancegroups.go:212] Cluster validated.</strong>  </pre>
<p>We can see that, after waiting for everything to settle, kops validated the cluster, thus confirming that upgrade of the first master node finished successfully.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/cf42f01c-d9a5-4b8a-8e49-38cee4dd01d1.png" style="width:28.33em;height:29.00em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-6: Rolling upgrade of one of the master nodes</div>
<p>As soon as it validated the upgrade of the first master, kops proceeded with the next node. During next ten to fifteen minutes, the same process will be repeated with the other two masters. Once all three are upgraded, kops will execute the same process with the worker nodes, and we'll have to wait for another ten to fifteen minutes.</p>
<pre><strong>I0225 23:34:01.148318 1 rollingupdate.go:191] Rolling update <br/> completed for cluster "devops23.k8s.local"!</strong>
  </pre>
<p>Finally, once all the servers were upgraded, we can see that rolling update was completed.</p>
<p>The experience was positive but long. Auto-scaling groups need a bit of time to detect that a server is down. It takes a minute or two for a new VM to be created and initialized. Docker, Kubelet, and Protokube need to be installed. Containers that form core Pods need to be pulled. All in all, quite a few things need to happen.</p>
<p>The upgrade process would be faster if kops would use immutable approach and bake everything into images (AMIs). However, the choice was made to decouple OS with packages and core Pods, so the installation needs to be done at runtime. Also, the default distribution is Debian, which is not as light as, let's say, CoreOS. Due to those, and a few other design choices, the process is somehow lengthy. When combined with inevitable time AWS needs to do its part of the process, we're looking at over five minutes of upgrade duration for each node in a cluster. Even with only five nodes, the whole process is around thirty minutes. If we'd have a bigger cluster, it could take hours, or even days to upgrade.</p>
<p>Even though it takes considerable time to upgrade, the process is hands-free. If we are brave enough, we can let kops do its job and spend our time working on something more exciting. Assuming that our applications are designed to be scalable and fault-tolerant, we won't experience downtime. That is what matters much more than whether we'll be able to watch the process unfold. If we trust the system, we can just as well run it in the background and ignore it. However, earning trust is hard. We need to successfully run the process a few times before we put our fate in it. Even then, we should build a robust monitoring and alerting system that will notify us if things go wrong. Unfortunately, we won't cover those subjects in this book. You'll have to wait for the next one or explore it yourself.</p>
<p>Let's go back to our cluster and verify that Kubernetes was indeed upgraded.</p>
<pre><strong>kubectl get nodes</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                 STATUS ROLES  AGE VERSION</strong>
<strong>ip-172-20-107-172... Ready  node   4m  v1.8.5</strong>
<strong>ip-172-20-124-177... Ready  master 16m v1.8.5</strong>
<strong>ip-172-20-44-126...  Ready  master 28m v1.8.5</strong>
<strong>ip-172-20-56-244...  Ready  node   10m v1.8.5</strong>
<strong>ip-172-20-67-40...   Ready  master 22m v1.8.5</strong>  </pre>
<p>Judging by versions of each of the nodes, all were upgraded to <kbd>v1.8.5</kbd>. The process worked.</p>
<div class="packt_infobox">Try to upgrade often. As a rule of thumb, you should upgrade one minor release at a time.</div>
<p>Even if you are a couple of minor releases behind the stable kops-recommended release, it's better if you execute multiple rolling upgrades (one for each minor release) than to jump to the latest at once. By upgrading to the next minor release, you'll minimize potential problems and simplify rollback if required.</p>
<p>Even though kops is fairly reliable, you should not trust it blindly. It's relatively easy to create a small testing cluster running the same release as production, execute the upgrade process, and validate that everything works as expected. Once finished, you can destroy the test cluster and avoid unnecessary expenses.</p>
<div class="packt_infobox">Don't trust anyone. Test upgrades in a separate cluster.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upgrading the cluster automatically</h1>
                </header>
            
            <article>
                
<p>We edited cluster's desired state before we started the rolling update process. While that worked well, we're likely to always upgrade to the latest stable version. In those cases, we can execute the <kbd>kops upgrade</kbd> command.</p>
<pre><strong>kops upgrade cluster $NAME --yes</strong>  </pre>
<p>Please note that this time we skipped the preview by setting the <kbd>--yes</kbd> argument. The output is as follows:</p>
<pre><strong>ITEM    PROPERTY          OLD    NEW</strong>
<strong>Cluster KubernetesVersion v1.8.5 1.8.6</strong>
    
<strong>Updates applied to configuration.</strong>
<strong>You can now apply these changes, using 'kops update cluster <br/>devops23.k8s.local'</strong></pre>
<p>We can see that the current Kubernetes version is <kbd>v1.8.5</kbd> and, in case we choose to proceed, it will be upgraded to the latest which, at the time of this writing, is <kbd>v1.8.6</kbd>.</p>
<pre><strong>kops update cluster $NAME --yes</strong>  </pre>
<p>Just as before, we can see from the last entry that <kbd>changes may require instances to restart: kops rolling-update cluster</kbd>.</p>
<p>Let's proceed:</p>
<pre><strong>kops rolling-update cluster $NAME --yes</strong>  </pre>
<p>I'll skip commenting on the output since it is the same as the previous time we upgraded the cluster. The only significant difference, from the process perspective, is that we did not edit cluster's desired state by specifying the version we want, but initiated the process through the <kbd>kops upgrade</kbd> command. Everything else was the same in both cases.</p>
<p>If we are to create a test cluster and write a set of tests that verify the upgrade process, we could execute the upgrade process periodically. We could, for example, create a job in Jenkins that would upgrade every month. If there isn't new Kubernetes release, it would do nothing. If there is, it would create a new cluster with the same release as production, upgrade it, validate that everything works as expected, destroy the testing cluster, upgrade the production cluster, and run another round of test. However, it takes time and experience to get to that point. Until then, manually executed upgrades are the way to go.</p>
<p>We are missing one more thing before we can deploy applications to our simulation of a production cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accessing the cluster</h1>
                </header>
            
            <article>
                
<p>We need a way to access the cluster. So far, we saw that we can, at least, interact with the Kubernetes API. Every time we executed <kbd>kubectl</kbd>, it communicated with the cluster through the API server. That communication is established through AWS Elastic Load Balancer (ELB). Let's take a quick look at it.</p>
<pre><strong>aws elb describe-load-balancers</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>{</strong>
<strong>  "LoadBalancerDescriptions": [</strong>
<strong>    {</strong>
<strong>      ...</strong>
<strong>      "ListenerDescriptions": [</strong>
<strong>        {</strong>
<strong>          "Listener": {</strong>
<strong>            "InstancePort": 443, </strong>
<strong>            "LoadBalancerPort": 443, </strong>
<strong>            "Protocol": "TCP", </strong>
<strong>            "InstanceProtocol": "TCP"</strong>
<strong>          }, </strong>
<strong>         ...</strong>
<strong>      "Instances": [</strong>
<strong>        {</strong>
<strong>          "InstanceId": "i-01f5c2ca47168b248"</strong>
<strong>        }, </strong>
<strong>        {</strong>
<strong>          "InstanceId": "i-0305e3b2d3da6e1ce"</strong>
<strong>        }, </strong>
<strong>        {</strong>
<strong>          "InstanceId": "i-04291ef2432b462f2"</strong>
<strong>        }</strong>
<strong>      ], </strong>
    
<strong>      "DNSName": "api-devops23-k8s-local-ivnbim-1190013982.us-east-2.elb.amazonaws.com", </strong>
<strong>      ...</strong>
<strong>      "LoadBalancerName": "api-devops23-k8s-local-ivnbim", </strong>
<strong>      ...</strong>  </pre>
<p>Judging from the <kbd>Listener</kbd> section, we can see that only port <kbd>443</kbd> is opened, thus allowing only SSL requests. The three instances belong to managers. We can safely assume that this load balancer is used only for the access to Kubernetes API. In other words, we are still missing access to worker nodes through which we'll be able to communicate with our applications. We'll come back to this issue in a moment.</p>
<p>The entry that matters, from user's perspective, is <kbd>DNSName</kbd>. That is the address we need to use if we want to communicate with Kubernetes' API Server. Load Balancer is there to ensure that we have a fixed address and that requests will be forwarded to one of the healthy masters.</p>
<p>Finally, the name of the load balancer is <kbd>api-devops23-k8s-local-ivnbim</kbd>. It is important that you remember that it starts with <kbd>api-devops23</kbd>. You'll see soon why the name matters.</p>
<p>We can confirm that the <kbd>DNSName</kbd> is indeed the door to the API by examining <kbd>kubectl</kbd> configuration:</p>
<pre><strong>kubectl config view</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>clusters:</strong>
<strong>- cluster:</strong>
<strong>    certificate-authority-data: REDACTED</strong>
<strong>    server: https://api-devops23-k8s-local-ivnbim-1190013982.us-east-2.elb.am<br/> azonaws.com</strong>
<strong>  name: devops23.k8s.local</strong>
<strong>...</strong>
<strong>current-context: devops23.k8s.local</strong>
<strong>...</strong>  </pre>
<p>We can see that the <kbd>devops23.k8s.local</kbd> is set to use <kbd>amazonaws.com</kbd> subdomain as the server address and that it is the current context. That is the DNS of the ELB.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/df4178fe-c7a7-4479-927a-9e9f757388d1.png" style="width:31.42em;height:29.83em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-7: Load balancer behind Kubernetes API Server</div>
<p>The fact that we can access the API does not get us much closer to having a way to access applications we are soon to deploy. We already learned that we can use Ingress to channel requests to a set of ports (usually <kbd>80</kbd> and <kbd>443</kbd>). However, even if we deploy Ingress, we still need an entry point to the worker nodes. We need another load balancer sitting above the nodes.</p>
<p>Fortunately, kops has a solution.</p>
<p>We can use kops' add-ons to deploy additional core services. You can get the list of those currently available by exploring directories in <a href="https://github.com/kubernetes/kops/tree/master/addons" target="_blank"><span class="URLPACKT">https://github.com/kubernetes/kops/tree/master/addons</span></a>. Even though most of them are useful, we'll focus only on the task at hand.</p>
<p>Add-ons are, in most cases, Kubernetes resources defined in a YAML file. All we have to do is pick the addon we want, choose the version we prefer, and execute <kbd>kubectl create</kbd>. We'll create the resources defined in <kbd>ingress-nginx</kbd> version <kbd>v1.6.0</kbd>.</p>
<p>We won't go into details behind the definition YAML file we are about to use to create the resources kops assembled for us. I'll leave that up to you. Instead, we'll proceed with <kbd>kubectl create</kbd>.</p>
<pre><strong>kubectl create \</strong>
<strong>    -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx/v1.6.0.yaml</strong></pre>
<p>The output is as follows:</p>
<pre><strong>namespace "kube-ingress" created</strong>
<strong>serviceaccount "nginx-ingress-controller" created</strong>
<strong>clusterrole "nginx-ingress-controller" created</strong>
<strong>role "nginx-ingress-controller" created</strong>
<strong>clusterrolebinding "nginx-ingress-controller" created</strong>
<strong>rolebinding "nginx-ingress-controller" created</strong>
<strong>service "nginx-default-backend" created</strong>
<strong>deployment "nginx-default-backend" created</strong>
<strong>configmap "ingress-nginx" created</strong>
<strong>service "ingress-nginx" created</strong>
<strong>deployment "ingress-nginx" created</strong>  </pre>
<p>We can see that quite a few resources were created in the Namespace <kbd>kube-ingress</kbd>. Let's take a look what's inside.</p>
<pre><strong>kubectl --namespace kube-ingress \</strong>
<strong>   get all</strong>
  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                         DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/ingress-nginx         3       3       3          3         1m</strong>
<strong>deploy/nginx-default-backend 1       1       1          1         1m</strong>
<strong>NAME                                DESIRED CURRENT READY AGE</strong>
<strong>rs/ingress-nginx-768fc7997b         3       3       3     1m</strong>
<strong>rs/nginx-default-backend-74f9cd546d 1       1       1     1m</strong>
<strong>NAME                                      READY STATUS  RESTARTS AGE</strong>
<strong>po/ingress-nginx-768fc7997b-4xfq8         1/1   Running 0        1m</strong>
<strong>po/ingress-nginx-768fc7997b-c7zvx         1/1   Running 0        1m</strong>
<strong>po/ingress-nginx-768fc7997b-clr5m         1/1   Running 0        1m</strong>
<strong>po/nginx-default-backend-74f9cd546d-mtct8 1/1   Running 0        1m</strong>
<strong>NAME                      TYPE         CLUSTER-IP     EXTERNAL-IP      PORT(S)                    AGE</strong>
<strong>svc/ingress-nginx         LoadBalancer 100.66.190.165 abb5117871831... 80:301</strong>
    <strong>07/TCP,443:30430/TCP 1m</strong>
<strong>svc/nginx-default-backend ClusterIP    100.70.227.240 &lt;none&gt;           80/TCP</strong>
    <strong>                     1m</strong></pre>
<p>We can see that it created two deployments, which created two <kbd>ReplicaSets</kbd>, which created Pods. In addition, we got two Services as well. As a result, Ingress is running inside our cluster and are a step closer to being able to test it. Still, we need to figure out how to access the cluster.</p>
<p>One of the two Services (<kbd>ingress-nginx</kbd>) is <kbd>LoadBalancer</kbd>. We did not explore that type when we discussed Services.</p>
<p><kbd>LoadBalancer</kbd> Service type exposes the service externally using a cloud provider's load balancer. <kbd>NodePort</kbd> and <kbd>ClusterIP</kbd> services, to which the external load balancer will route, are automatically created. Ingress is "intelligent" enough to know how to create and configure an AWS ELB. All it needed is an annotation <kbd>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</kbd> (defined in the YAML file).</p>
<p>You'll notice that the <kbd>ingress-nginx</kbd> Service published port <kbd>30107</kbd> and mapped it to <kbd>80</kbd>. <kbd>30430</kbd> was mapped to <kbd>443</kbd>. This means that, from inside the cluster, we should be able to send HTTP requests to <kbd>30107</kbd> and HTTPS to <kbd>30430</kbd>. However, that is only part of the story. Since the Service is the <kbd>LoadBalancer</kbd> type, we should expect some changes to AWS <strong>Elastic Load Balancers</strong> (<strong>ELBs</strong>) as well.</p>
<p>Let's check the state of the load balancers in our cluster.</p>
<pre><strong>aws elb describe-load-balancers</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>{</strong>
<strong>  "LoadBalancerDescriptions": [</strong>
<strong>    {</strong>
<strong>      ...</strong>
<strong>      "LoadBalancerName": "api-devops23-k8s-local-ivnbim",</strong>
<strong>      ...</strong>
<strong>    }, </strong>
<strong>    {</strong>
<strong>      ...</strong>
<strong>      "ListenerDescriptions": [</strong>
<strong>        {</strong>
<strong>          "Listener": {</strong>
<strong>            "InstancePort": 30107, </strong>
<strong>            "LoadBalancerPort": 80, </strong>
<strong>            "Protocol": "TCP", </strong>
<strong>            "InstanceProtocol": "TCP"</strong>
<strong>          }, </strong>
<strong>          "PolicyNames": []</strong>
<strong>        }, </strong>
<strong>        {</strong>
<strong>          "Listener": {</strong>
<strong>            "InstancePort": 30430, </strong>
<strong>            "LoadBalancerPort": 443, </strong>
<strong>            "Protocol": "TCP", </strong>
<strong>            "InstanceProtocol": "TCP"</strong>
<strong>          }, </strong>
    <strong>      "PolicyNames": []</strong>
    <strong>    }</strong>
    <strong>  ], </strong>
    <strong>  ...</strong>
    <strong>  "Instances": [</strong>
    <strong>    {</strong>
    <strong>      "InstanceId": "i-063fabc7ad5935db5"</strong>
    <strong>    },</strong>
    <strong>    {</strong>
    <strong>      "InstanceId": "i-04d32c91cfc084369"</strong>
    <strong>    }</strong>
    <strong>  ], </strong>
    <strong>  "DNSName": "a1c431cef1bfa11e88b600650be36f73-2136831960.us-east-2.elb.amazonaws.com", </strong>
   <strong>   ...</strong>
   <strong>   "LoadBalancerName": "a1c431cef1bfa11e88b600650be36f73", </strong>
   <strong>   ...</strong></pre>
<p>We can observe from the output that a new load balancer was added.</p>
<p>The new load balancer publishes port <kbd>80</kbd> (HTTP) and maps it to <kbd>30107</kbd>. This port is the same as the <kbd>ingress-nginx</kbd> Service published. Similarly, the LB published port <kbd>443</kbd> (HTTPS) and mapped it to <kbd>30430</kbd>. From the <kbd>Instances</kbd> section, we can see that it currently maps to the two worker nodes.</p>
<p>Further down, we can see the <kbd>DNSName</kbd>. We should retrieve it but, unfortunately, <kbd>LoadBalancerName</kbd> does not follow any format. However, we do know that now there are two load balancers and that the one dedicated to masters has a name that starts with <kbd>api-devops23</kbd>. So, we can retrieve the other LB by specifying that it should not contain that prefix. We'll use <kbd>jq</kbd> instruction <kbd>not</kbd> for that.</p>
<p>The command that retrieves DNS from the new load balancer is as follows:</p>
<pre><strong>CLUSTER_DNS=$(aws elb \</strong>
<strong>    describe-load-balancers | jq -r \</strong>
<strong>    ".LoadBalancerDescriptions[] \</strong>
<strong>    | select(.DNSName \</strong>
<strong>    | contains (\"api-devops23\") \</strong>
<strong>    | not).DNSName")</strong>  </pre>
<p>We'll come back to the newly created Ingress and the load balancer soon. For now, we'll move on and deploy the <kbd>go-demo-2</kbd> application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying applications</h1>
                </header>
            
            <article>
                
<p>Deploying resources to a Kubernetes cluster running in AWS is no different from deployments anywhere else, including Minikube. That's one of the big advantages of Kubernetes, or of any other container scheduler. We have a layer of abstraction between hosting providers and our applications. As a result, we can deploy (almost) any YAML definition to any Kubernetes cluster, no matter where it is. That's huge. It gives up a very high level of freedom and allows us to avoid vendor locking. Sure, we cannot effortlessly switch from one scheduler to another, meaning that we are "locked" into the scheduler we chose. Still, it's better to depend on an open source project than on a commercial hosting vendor like AWS, GCE, or Azure.</p>
<div class="packt_infobox">We need to spend time setting up a Kubernetes cluster, and the steps will differ from one hosting provider to another. However, once a cluster is up-and-running, we can create any Kubernetes resource (almost) entirely ignoring what's underneath it. The result is the same no matter whether our cluster is AWS, GCE, Azure, on-prem, or anywhere else.</div>
<p>Let's get back to the task at hand and create <kbd>go-demo-2</kbd> resources:</p>
<pre><strong>cd ..</strong>
    
<strong>kubectl create \</strong>
<strong>    -f aws/go-demo-2.yml \</strong>
<strong>    --record --save-config</strong></pre>
<p>We moved back to the repository's root directory, and created the resources defined in <kbd>aws/go-demo-2.yml</kbd>. The output is as follows:</p>
<pre><strong>ingress "go-demo-2" created</strong>
<strong>deployment "go-demo-2-db" created</strong>
<strong>service "go-demo-2-db" created</strong>
<strong>deployment "go-demo-2-api" created</strong>
<strong>service "go-demo-2-api" created</strong>  </pre>
<p>Next, we should wait until <kbd>go-demo-2-api</kbd> Deployment is rolled out.</p>
<pre><strong>kubectl rollout status \</strong>
<strong>    deployment go-demo-2-api</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>deployment "go-demo-2-api" successfully rolled out</strong>  </pre>
<p>Finally, we can validate that the application is running and is accessible through the DNS provided by the AWS <strong>Elastic Load Balancer</strong> (<strong>ELB</strong>):</p>
<pre><strong>curl -i "http://$CLUSTER_DNS/demo/hello"</strong>  </pre>
<p>We got response code <kbd>200</kbd> and the message <kbd>hello, world!</kbd>. The Kubernetes cluster we set up in AWS works!</p>
<p>When we sent the request to the ELB dedicated to workers, it performed round-robin and forwarded it to one of the healthy nodes. Once inside the worker, the request was picked by the <kbd>nginx</kbd> Service, forwarded to Ingress, and, from there, to one of the containers that form the replicas of the <kbd>go-demo-2-api</kbd> ReplicaSet.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/93b6e8a9-7415-46fe-b8c6-3ca283045100.png" style="width:29.58em;height:31.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 14-8: Load balancer behind Kubernetes worker nodes</div>
<p>It might be worth pointing out that containers that form our applications are always running in worker nodes. Master servers, on the other hand, are entirely dedicated to running Kubernetes system. That does not mean that we couldn't create a cluster in the way that masters and workers are combined into the same servers, just as we did with Minikube. However, that is risky, and we're better off separating the two types of nodes. Masters are more reliable when they are running on dedicated servers. Kops knows that, and it does not even allow us to mix the two.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring high-availability and fault-tolerance</h1>
                </header>
            
            <article>
                
<p>The cluster would not be reliable if it wouldn't be fault tolerant. Kops intents to make it so, but we're going to validate that anyways.</p>
<p>Let's retrieve the list of worker node instances:</p>
<pre><strong>aws ec2 \</strong>
<strong>    describe-instances | jq -r \</strong>
<strong>    ".Reservations[].Instances[] \</strong>
<strong>    | select(.SecurityGroups[]\</strong>
<strong>    .GroupName==\"nodes.$NAME\")\</strong>
<strong>    .InstanceId"</strong>  </pre>
<p>We used <kbd>aws ec2 describe-instances</kbd> to retrieve all the instances (five in total). The output was sent to <kbd>jq</kbd>, which filtered them by the security group dedicated to worker nodes.</p>
<p>The output is as follows:</p>
<pre><strong>i-063fabc7ad5935db5</strong>
<strong>i-04d32c91cfc084369</strong>  </pre>
<p>We'll terminate one of the worker nodes. To do that, we'll pick a random one, and retrieve its ID.</p>
<pre><strong>INSTANCE_ID=$(aws ec2 \</strong>
<strong>    describe-instances | jq -r \</strong>
<strong>    ".Reservations[].Instances[] \ </strong>
<strong>    | select(.SecurityGroups[]\</strong>
<strong>    .GroupName==\"nodes.$NAME\")\</strong>
<strong>    .InstanceId" | tail -n 1)</strong>  </pre>
<p>We used the same command as before and added <kbd>tail -n 1</kbd>, so that the output is limited to a single line (entry). We stored the result in the <kbd>INSTANCE_ID</kbd> variable. Now we know which instance to terminate.</p>
<pre><strong>aws ec2 terminate-instances \</strong>
<strong>    --instance-ids $INSTANCE_ID</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>  "TerminatingInstances": [</strong>
<strong>    {</strong>
<strong>      "InstanceId": "i-063fabc7ad5935db5",</strong>
<strong>      "CurrentState": {</strong>
<strong>        "Code": 32,</strong>
<strong>        "Name": "shutting-down"</strong>
<strong>     },</strong>
<strong>     "PreviousState": {</strong>
<strong>        "Code": 16,</strong>
<strong>        "Name": "running"</strong>
<strong>     }</strong>
<strong>    }</strong>
<strong>  ]</strong>
<strong>}</strong>  </pre>
<p>We can see from the output that the instance is shutting down. We can confirm that by listing all the instances from the security group <kbd>nodes.devops23.k8s.local</kbd>.</p>
<pre>aws ec2 describe-instances | jq -r \<br/>    ".Reservations[].Instances[] \<br/>    | select(\<br/>    .SecurityGroups[].GroupName \<br/>    ==\"nodes.$NAME\").InstanceId"<br/> </pre>
<p>The output is as follows:</p>
<pre><strong>i-04d32c91cfc084369</strong>  </pre>
<p>As expected, we are now running only one instance. All that's left is to wait for a minute, and repeat the same command.</p>
<pre><strong>aws ec2 \ </strong>
<strong>    describe-instances | jq -r \</strong>
<strong>    ".Reservations[].Instances[] \ </strong>
<strong>    | select(.SecurityGroups[]\</strong>
<strong>    .GroupName==\"nodes.$NAME\")\</strong>
<strong>    .InstanceId"</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>i-003b4b1934d85641a</strong>
<strong>i-04d32c91cfc084369</strong>  </pre>
<p>This time, we can see that there are again two instances. The only difference is that this time one of the instance IDs is different.</p>
<p>AWS auto-scaling group discovered that the instances do not match the desired number, and it created a new one.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The fact that AWS created a node to replace the one we terminated does not mean that the new server joined the Kubernetes cluster. Let's verify that:</p>
<pre><strong>kubectl get nodes</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                                        STATUS ROLES  AGE VERSION</strong>
<strong>ip-172-20-55-183.us-east-2.compute.internal Ready  master 30m v1.<br/> 8.6</strong>
<strong>ip-172-20-61-82.us-east-2.compute.internal  Ready  node   13m v1.<br/> 8.6</strong>
<strong>ip-172-20-71-53.us-east-2.compute.internal  Ready  master 30m v1.<br/> 8.6</strong>
<strong>ip-172-20-97-39.us-east-2.compute.internal  Ready  master 30m v1.<br/> 8.6</strong> </pre>
<p>If you were fast enough, your output should also show that there is only one (worker) <kbd>node</kbd>. Once AWS created a new server, it takes a bit of time until Docker, Kubelet, and Protokube are installed, containers are pulled and run, and the node is registered through one of the masters.</p>
<p>Let's try it again.</p>
<pre><strong>kubectl get nodes</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                                        STATUS ROLES  AGE VERSION</strong>
<strong>ip-172-20-55-183.us-east-2.compute.internal Ready  master 32m v1.<br/> 8.6</strong>
<strong>ip-172-20-61-82.us-east-2.compute.internal  Ready  node   15m v1.<br/> 8.6</strong>
<strong>ip-172-20-71-53.us-east-2.compute.internal  Ready  master 32m v1.<br/> 8.6</strong>
<strong>ip-172-20-79-161.us-east-2.compute.internal Ready  node   2m  v1.<br/> 8.6</strong>
<strong>ip-172-20-97-39.us-east-2.compute.internal  Ready  master 32m v1.<br/> 8.6</strong>  </pre>
<p>This time, the number of (worker) nodes is back to two. Our cluster is back in the desired state.</p>
<p>What we just experienced is, basically, the same as when we executed the rolling upgrade. The only difference is that we terminated an instance as a way to simulate a failure. During the upgrade process, kops does the same. It shuts down one instance at a time and waits until the cluster goes back to the desired state.</p>
<p>Feel free to do a similar test with master nodes. The only difference is that you'll have to use <kbd>masters</kbd> instead of <kbd>nodes</kbd> as the prefix of the security group name. Since everything else is the same, I trust you won't need instructions and explanations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Giving others access to the cluster</h1>
                </header>
            
            <article>
                
<p>Unless you're planning to be the only person in your organization with the access to the cluster, you'll need to create a <kbd>kubectl</kbd> configuration that you can distribute to your coworkers. Let's see the steps:</p>
<pre><strong>cd cluster</strong>
    
<strong>mkdir -p config</strong>
    
<strong>export KUBECONFIG=$PWD/config/kubecfg.yaml</strong>  </pre>
<p>We went back to the <kbd>cluster</kbd> directory, created the sub-directory <kbd>config</kbd>, and exported <kbd>KUBECONFIG</kbd> variable with the path to the file where we'd like to store the configuration. Now we can execute <kbd>kops export</kbd>:</p>
<pre><strong>kops export kubecfg --name ${NAME}</strong>
    
<strong>cat $KUBECONFIG</strong>  </pre>
<p>The output of the latter command is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>clusters:</strong>
<strong>- cluster:</strong>
<strong>    certificate-authority-data: ...</strong>
<strong>    server: https://api-devops23-k8s-local-ivnbim-609446190.us-east-2.elb.amazonaws.com</strong>
  <strong>name: devops23.k8s.local</strong>
<strong>contexts:</strong>
<strong>- context:</strong>
<strong>    cluster: devops23.k8s.local</strong>
<strong>    user: devops23.k8s.local</strong>
<strong>  name: devops23.k8s.local</strong>
<strong>current-context: devops23.k8s.local</strong>
<strong>kind: Config</strong>
<strong>preferences: {}</strong>
<strong>users:</strong>
<strong>- name: devops23.k8s.local</strong>
<strong>  user:</strong>
<strong>    as-user-extra: {}</strong>
<strong>    client-certificate-data: ...</strong>
<strong>    client-key-data: ...</strong>
<strong>    password: oeezRbhG4yz3oBUO5kf7DSWcOwvjKZ6l</strong>
<strong>    username: admin</strong>
<strong>- name: devops23.k8s.local-basic-auth</strong>
<strong>  user:</strong>
<strong>    as-user-extra: {}</strong>
    <strong>password: oeezRbhG4yz3oBUO5kf7DSWcOwvjKZ6l</strong>
    <strong>username: admin</strong></pre>
<p>Now you can pass that configuration to one of your coworkers, and he'll have the same access as you.</p>
<p>Truth be told, you should create a new user and a password or, even better, an SSH key and let each user in your organization access the cluster with their own authentication. You should also create RBAC permissions for each user or a group of users. We won't go into the steps how to do that since they are already explained in the <a href="36d9d538-dc85-4366-bd95-72076b27cb28.xhtml">Chapter 12</a>,<em>Securing Kubernetes Clusters</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Destroying the cluster</h1>
                </header>
            
            <article>
                
<p>The chapter is almost finished, and we do not need the cluster anymore. We want to destroy it as soon as possible. There's no good reason to keep it running when we're not using it. But, before we proceed with the destructive actions, we'll create a file that will hold all the environment variables we used in this chapter. That will help us the next time we want to recreate the cluster.</p>
<pre>echo "export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID 
export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY 
export AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION 
export ZONES=$ZONES 
export NAME=$NAME 
export KOPS_STATE_STORE=$KOPS_STATE_STORE" \ 
    &gt;kops </pre>
<p>We echoed the variables with the values into the <kbd>kops</kbd> file, and now we can delete the cluster:</p>
<pre><strong>kops delete cluster \</strong>
<strong>    --name $NAME \</strong>
<strong>    --yes</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>...</strong>
<strong>Deleted kubectl config for devops23.k8s.local</strong>
    
<strong>Deleted cluster: "devops23.k8s.local"</strong>  </pre>
<p>Kops removed references of the cluster from our <kbd>kubectl</kbd> configuration and proceeded to delete all the AWS resources it created. Our cluster is no more. We can proceed and delete the S3 bucket as well.</p>
<pre><strong>aws s3api delete-bucket \</strong>
<strong>    --bucket $BUCKET_NAME</strong>  </pre>
<p>We will not remove the IAM resources (group, user, access key, and policies). It does not cost to keep them in AWS, and we'll save ourselves from re-running the commands that create them. However, I will list the commands as a reference.</p>
<div class="packt_infobox"><span class="packt_screen">Do NOT execute following commands</span>. They are only a reference. We'll need those resources in the next chapter.</div>
<pre><strong># Replace `[...]` with the administrative access key ID.</strong>
<strong>export AWS_ACCESS_KEY_ID=[...]</strong>
   
<strong># Replace `[...]` with the administrative secret access key.</strong>
<strong>export AWS_SECRET_ACCESS_KEY=[...]</strong>
    
<strong>aws iam remove-user-from-group \</strong>
<strong>    --user-name kops \</strong>
<strong>    --group-name kops</strong>
    
<strong>aws iam delete-access-key \ </strong>
<strong>    --user-name kops \</strong>
<strong>    --access-key-id $(\</strong>
<strong>    cat kops-creds | jq -r\ </strong>
<strong>    '.AccessKey.AccessKeyId')</strong>
    
<strong>aws iam delete-user \</strong>
<strong>    --user-name kops</strong>
    
<strong>aws iam detach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess \</strong>
<strong>    --group-name kops</strong>
    
<strong>aws iam detach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \</strong>
<strong>    --group-name kops</strong>
 
<strong>aws iam detach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess \</strong>
<strong>    --group-name kops</strong>
    
<strong>aws iam detach-group-policy \</strong>
<strong>    --policy-arn arn:aws:iam::aws:policy/IAMFullAccess \</strong>
<strong>    --group-name kops</strong>
    
<strong>aws iam delete-group \</strong>
<strong>    --group-name kops</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What now?</h1>
                </header>
            
            <article>
                
<p>We have a production-ready Kubernetes cluster running in AWS. Isn't that something worthwhile a celebration?</p>
<p>Kops proved to be relatively easy to use. We executed more <kbd>aws</kbd> than <kbd>kops</kbd> commands. If we exclude them, the whole cluster can be created with a single <kbd>kops</kbd> command. We can easily add or remove worker nodes. Upgrades are simple and reliable, if a bit long. The important part is that through rolling upgrades we can avoid downtime.</p>
<p>There are a few <kbd>kops</kbd> command we did not explore. I feel that now you know the important parts and that you will be able to figure out the rest through the documentation.</p>
<p>You might be inclined to think that you are ready to apply everything you learned so far. Do not open that champagne bottle you've been saving for special occasions. There's still one significant topic we need to explore. We postponed the discussion about stateful services since we did not have the ability to use external drives. We did use volumes, but they were all local, and do not qualify as persistent. Failure of a single server would prove that. Now that we are running a cluster in AWS, we can explore how to deploy stateful applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes operations (kops) compared to Docker for AWS</h1>
                </header>
            
            <article>
                
<p>Docker for AWS (D4AWS) quickly became the preferable way to create a Docker Swarm cluster in AWS (and Azure). Similarly, kops is the most commonly used tool to create Kubernetes clusters in AWS. At least, at the time of this writing.</p>
<p>The result, with both tools, is more or less the same. Both create Security Groups, VPCs, Auto-Scaling Groups, Elastic Load Balancers, and everything else a cluster needs. In both cases, Auto-Scaling Groups are in charge of creating EC2 instances. Both rely on external storage to keep the state of the cluster (kops in S3 and D4AWS in DynamoDB). In both cases, EC2 instances brought to life by Auto-Scaling Groups know how to run system-level services and join the cluster. If we exclude the fact that one solution runs Docker Swarm and that the other uses Kubernetes, there is no significant functional difference if we observe only the result (the cluster). So, we'll focus on user experience instead.</p>
<p>Both tools can be executed from the command line and that's where we can spot the first difference.</p>
<p>Docker for AWS relies on CloudFormation templates, so we need to execute <kbd>aws cloudformation</kbd> command. Docker provides a template and we should use parameters to customize it. In my humble opinion, the way CloudFormation expects us to pass parameters is just silly.</p>
<p>Let's take a look at an example:</p>
<pre><strong>aws cloudformation create-stack \</strong>
<strong>    --template-url https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl \</strong>
<strong>    --capabilities CAPABILITY_IAM \</strong>
<strong>    --stack-name devops22 \</strong>
 <strong>   --parameters \</strong>
<strong>    ParameterKey=ManagerSize,ParameterValue=3 \</strong>
<strong>    ParameterKey=ClusterSize,ParameterValue=2 \</strong>
<strong>    ParameterKey=KeyName,ParameterValue=workshop \ </strong>
<strong>    ParameterKey=EnableSystemPrune,ParameterValue=yes \</strong>
<strong>    ParameterKey=EnableCloudWatchLogs,ParameterValue=no \ </strong>
<strong>    ParameterKey=EnableCloudStorEfs,ParameterValue=yes \</strong>
<strong>    ParameterKey=ManagerInstanceType,ParameterValue=t2.small \</strong>
 <strong>   ParameterKey=InstanceType,ParameterValue=t2.small</strong>  </pre>
<p>Having to write something like <kbd>ParameterKey=ManagerSize,ParameterValue=3</kbd> instead of <kbd>ManagerSize=3</kbd> is annoying at best.</p>
<p>A sample command that creates Kubernetes cluster using <kbd>kops</kbd> is as follows:</p>
<pre><strong>kops create cluster \</strong>
<strong>    --name $NAME \</strong>
<strong>    --master-count 3 \</strong>
<strong>    --node-count 1 \</strong>
<strong>    --node-size t2.small \</strong>
<strong>    --master-size t2.small \ </strong>
<strong>    --zones $ZONES \</strong>
<strong>    --master-zones $ZONES \</strong>
<strong>    --ssh-public-key devops23.pub \</strong>
<strong>    --networking kubenet \</strong>
<strong>    --kubernetes-version v1.8.4 \</strong>
<strong>    --yes</strong> </pre>
<p>Isn't that easier and more intuitive?</p>
<p>Moreover, kops is a binary with everything we would expect. We can, for example, execute <kbd>kops --help</kbd> and see the available options and a few examples. If we'd like to know which parameters are available with Docker For AWS, we'd need to go through the template. That's definitely less intuitive and more difficult than running <kbd>kops create cluster --help</kbd>. Even if we don't mind browsing through the Docker For AWS template, we still don't have examples at hand (from the command line, not browser). From user experience perspective, kops wins over Docker For AWS if we restrict the comparison only to command line interface. Simply put, executing a well-defined binary dedicated to managing a cluster is better than executing <kbd>aws cloudformation</kbd> commands with remote templates.</p>
<p>Did Docker make a mistake for choosing CloudFormation? I don't think so. Even if command line experience is suboptimal, it is apparent that they wanted to provide an experience native to hosting vendor. In our case that's AWS, but the same can be said for Azure. If you will always operate cluster from the command line (as I think you should), this is where the story ends and kops is a winner with a very narrow margin.</p>
<p>The fact that we can create Docker For AWS cluster using CloudFormation means that we can take advantage of it from AWS Console. That translates into UI experience. We can use AWS Console UI to create, update, or delete a cluster. We can see the events as they progress, explore the resources that were created, roll back to the previous version, and so on. By choosing CloudFormation template, Docker decided to provide not only command line but also a visual experience.</p>
<p>Personally, I think that UIs are evil and that we should do everything from the command line. That being said, I'm fully aware that not everybody feels the same. Even if you do choose never to use UI for "real" work, it is very helpful, at least at the beginning, as a learning experience of what can and what cannot be done, and how all the steps tie together.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/dc704627-7611-4e26-81cf-b8efabc6ceb8.png" style="width:48.67em;height:39.17em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Figure 14-9: Docker For AWS UI</div>
<p>It's a tough call. What matters is that both tools are creating reliable clusters. Kops is more user-friendly from the command line, but it has no UI. Docker For AWS, on the other hand, works as native AWS solution through CloudFormation. That gives it the UI, but at the cost of suboptimal command line experience.</p>
<p>You won't have to choose one over the other since the choice will not depend on which one you like more, but whether you want to use Docker Swarm or Kubernetes.</p>


            </article>

            
        </section>
    </body></html>