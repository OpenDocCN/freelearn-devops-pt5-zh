["```\n$ docker swarm --help\n```", "```\n$ multipass launch -n node1\n$ multipass launch -n node2\n$ multipass launch -n node3\n```", "```\n$ multipass list\n```", "```\n$ multipass exec node1 -- \\\n\t/bin/bash -c 'curl -s https://get.docker.com | sh - && sudo usermod -aG docker ubuntu'\n$ multipass exec node2 -- \\\n\t/bin/bash -c 'curl -s https://get.docker.com | sh - && sudo usermod -aG docker ubuntu'\n$ multipass exec node3 -- \\\n\t/bin/bash -c 'curl -s https://get.docker.com | sh - && sudo usermod -aG docker ubuntu'\n```", "```\n$ IP=$(multipass info node1 | grep IPv4 | awk '{print $2}')\n```", "```\n$ multipass exec node1 -- \\\n\t/bin/bash -c 'docker swarm init --advertise-addr $IP:2377 --listen-addr $IP:2377'\n```", "```\nSwarm initialized: current node (92kts1c9x17gbqv3in9t1w4qm) is now a manager.\nTo add a worker to this swarm, run the following command:\n    docker swarm join --token SWMTKN-1-4s8vpkileg2l2sicpyay5fojhis9jygb8mv04tsy9jmeqmzhk8-4cp4hp0i1qjqpuln6q3ytprtc 192.168.64.9:2377\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n```", "```\nSWMTKN-1-4s8vpkileg2l2sicpyay5fojhis9jygb8mv04tsy9jmeqmzhk8-4cp4hp0i1qjqpuln6q3ytprtc.\n```", "```\n$ SWARM_TOKEN=$(multipass exec node1 -- /bin/bash -c 'docker swarm join-token --quiet worker')\n```", "```\n$ multipass exec node2 -- \\\n\t/bin/bash -c 'docker swarm join --token $SWARM_TOKEN $IP:2377'\n```", "```\n$ multipass exec node3 -- \\\n\t/bin/bash -c 'docker swarm join --token $SWARM_TOKEN $IP:2377'\n```", "```\n$ multipass exec node1 -- /bin/bash -c 'docker node ls'\n```", "```\n$ multipass shell node1\n```", "```\n$ docker info\n```", "```\nServer:\n Containers: 0\n Images: 0\n Server Version: 19.03.8\n Swarm: active\n  NodeID: 92kts1c9x17gbqv3in9t1w4qm\n  Is Manager: true\n  ClusterID: k65y4ke5rmup1n74z9lb9gerx\n  Managers: 1\n  Nodes: 3\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n```", "```\n$ docker node inspect node1 –pretty\n```", "```\nID:\t\t\t92kts1c9x17gbqv3in9t1w4qm\nHostname:              \tnode1\nJoined at:             \t2020-04-12 14:00:02.218330889 +0000 utc\nStatus:\n State:\t\t\tReady\n Availability:         \tActive\n Address:\t\t192.168.64.9\nManager Status:\n Address:\t\t192.168.64.9:2377\n Raft Status:\t\tReachable\n Leader:\t\tYes\nPlatform:\n Operating System:\tlinux\n Architecture:\t\tx86_64\nResources:\n CPUs:\t\t\t1\n Memory:\t\t985.7MiB\nPlugins:\n Log:\t\tawslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog\n Network:\t\tbridge, host, ipvlan, macvlan, null, overlay\n Volume:\t\tlocal\nEngine Version:\t\t19.03.8\n```", "```\n$ docker node inspect node2 --pretty\n```", "```\nID:\t\t\tx5qbl7j7qp07amffbps56p562\nHostname:              \tnode2\nJoined at:             \t2020-04-12 14:10:15.08034922 +0000 utc\nStatus:\n State:\t\t\tReady\n Availability:         \tActive\n Address:\t\t192.168.64.10\nPlatform:\n Operating System:\tlinux\n Architecture:\t\tx86_64\nResources:\n CPUs:\t\t\t1\n Memory:\t\t985.7MiB\nPlugins:\n Log:\t\tawslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog\n Network:\t\tbridge, host, ipvlan, macvlan, null, overlay\n Volume:\t\tlocal\nEngine Version:\t\t19.03.8\n```", "```\n$ docker node promote node2\n```", "```\nNode node2 promoted to a manager in the swarm.\n```", "```\n$ docker node ls\n```", "```\n$ docker node demote node1\n```", "```\nManager node1 demoted in the swarm.\n```", "```\n$ docker node ls\n```", "```\nError response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.\n```", "```\n$ exit\n$ multipass shell node2\n```", "```\n$ docker node ls\n```", "```\n$ docker node update --availability drain node1\n```", "```\n$ multipass shell node1\n$ sudo reboot\n```", "```\n$ docker node ls\n```", "```\n$ docker node update --availability active node1\n```", "```\n$ docker swarm <command>\n$ docker node <command>\n```", "```\n$ docker service <command>\n$ docker stack <command>\n```", "```\n$ docker service create \\\n    --name cluster \\\n    --constraint 'node.role == worker' \\\n    -p:80:80/tcp \\\n    russmckendrick/cluster\n```", "```\n$ docker node ls\n```", "```\n$ multipass list\n```", "```\n$ docker service ls\n```", "```\n$ docker service inspect cluster --pretty\n```", "```\n$ docker node ps\n$ docker node ps node1\n$ docker node ps node3\n```", "```\n$ docker service scale cluster=6\n$ docker service ls\n$ docker node ps node1\n$ docker node ps node3\n```", "```\n$ docker service rm cluster\n```", "```\nversion: '3'\nservices:\n  cluster:\n    image: russmckendrick/cluster\n    ports:\n      - '80:80'\n    deploy:\n      replicas: 6\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints:\n          - node.role == worker\n```", "```\n$ docker stack deploy --compose-file=docker-compose.yml cluster\n```", "```\n$ docker stack ls\n```", "```\n$ docker stack services cluster\n```", "```\n$ docker stack ps cluster\n```", "```\n$ docker stack rm cluster\n```", "```\n$ multipass delete --purge node1\n$ multipass delete --purge node2\n$ multipass delete --purge node3\n```"]