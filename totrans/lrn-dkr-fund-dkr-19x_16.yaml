- en: Introduction to Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we introduced orchestrators. Like a conductor in an orchestra,
    an orchestrator makes sure that all of our containerized application services
    play together nicely and contribute harmoniously to a common goal. Such orchestrators
    have quite a few responsibilities, which we discussed in detail. Finally, we provided
    a short overview of the most important container orchestrators on the market.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces Docker's native orchestrator, SwarmKit. It elaborates
    on all of the concepts and objects SwarmKit uses to deploy and run distributed,
    resilient, robust, and highly available applications in a cluster on premises
    or in the cloud. This chapter also introduces how SwarmKit ensures secure applications by
    using a **Software-Defined Network** (**SDN**) to isolate containers. Additionally,
    this chapter demonstrates how to install a highly available Docker Swarm in the
    cloud. It introduces the routing mesh, which provides layer-4 routing and load
    balancing. Finally, it demonstrates how to deploy a first application consisting
    of multiple services onto the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the topics we are going to discuss in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Swarm architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacks, services, and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-host networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a first application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Swarm routing mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After completing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sketch the essential parts of a highly available Docker Swarm on a whiteboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain in two or three simple sentences to an interested layman what a (swarm)
    service is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a highly available Docker Swarm in AWS, Azure, or GCP consisting of three
    manager and two worker nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Successfully deploy a replicated service such as Nginx on a Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale a running Docker Swarm service up and down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the aggregated log of a replicated Docker Swarm service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a simple stack file for a sample application consisting of at least two
    interacting services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a stack into a Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker Swarm architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of a Docker Swarm from a 30,000-foot view consists of two
    main parts—a raft consensus group of an odd number of manager nodes, and a group
    of worker nodes that communicate with each other over a gossip network, also called
    the control plane. The following diagram illustrates this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/185aff48-e453-4420-a5c7-35859f604904.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level architecture of a Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: The **manager** nodes manage the swarm while the **worker** nodes execute the
    applications deployed into the swarm. Each **manager** has a complete copy of
    the full state of the Swarm in its local raft store. Managers synchronously communicate
    with each other and their raft stores are always in sync.
  prefs: []
  type: TYPE_NORMAL
- en: The **workers**, on the other hand, communicate with each other asynchronously
    for scalability reasons. There can be hundreds if not thousands of **worker**
    nodes in a Swarm. Now that we have a high-level overview of what a Docker Swarm
    is, let's describe all of the individual elements of a Docker Swarm in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Swarm is a collection of nodes. We can classify a node as a physical computer or **Virtual
    Machine** (**VM**). Physical computers these days are often referred to as *bare
    metal*. People say *we're running on bare metal* to distinguish from running on
    a VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we install Docker on such a node, we call this node a Docker host. The
    following diagram illustrates a bit better what a node and what a Docker host
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45b82a12-5b32-4290-8970-944fe94532eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Bare metal and VM types of Docker Swarm nodes
  prefs: []
  type: TYPE_NORMAL
- en: To become a member of a Docker Swarm, a node must be a Docker host. A node in
    a Docker Swarm can have one of two roles. It can be a manager or it can be a worker.
    Manager nodes do what their name implies; they manage the Swarm. The worker nodes,
    in turn, execute the application workload.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, a manager node can also be a worker node and hence run application
    workload—although that is not recommended, especially if the Swarm is a production
    system running mission-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm managers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each Docker Swarm needs to include at least one manager node. For high availability
    reasons, we should have more than one manager node in a Swarm. This is especially
    true for production or production-like environments. If we have more than one
    manager node, then these nodes work together using the Raft consensus protocol.
    The Raft consensus protocol is a standard protocol that is often used when multiple
    entities need to work together and always need to agree with each other as to
    which activity to execute next.
  prefs: []
  type: TYPE_NORMAL
- en: To work well, the Raft consensus protocol asks for an odd number of members in
    what is called the consensus group. Hence, we should always have 1, 3, 5, 7, and
    so on manager nodes. In such a consensus group, there is always a leader. In the
    case of Docker Swarm, the first node that starts the Swarm initially becomes the
    leader. If the leader goes away then the remaining manager nodes elect a new leader.
    The other nodes in the consensus group are called followers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's assume that we shut down the current leader node for maintenance
    reasons. The remaining manager nodes will elect a new leader. When the previous
    leader node comes back online, it will now become a follower. The new leader remains
    the leader.
  prefs: []
  type: TYPE_NORMAL
- en: All of the members of the consensus group communicate synchronously with each
    other. Whenever the consensus group needs to make a decision, the leader asks
    all followers for agreement. If a majority of the manager nodes give a positive
    answer, then the leader executes the task. That means if we have three manager
    nodes, then at least one of the followers has to agree with the leader. If we
    have five manager nodes, then at least two followers have to agree.
  prefs: []
  type: TYPE_NORMAL
- en: Since all manager follower nodes have to communicate synchronously with the
    leader node to make a decision in the cluster, the decision-making process gets
    slower and slower the more manager nodes we have forming the consensus group.
    The recommendation of Docker is to use one manager for development, demo, or test
    environments. Use three managers nodes in small to medium size Swarms, and use
    five managers in large to extra large Swarms. To use more than five managers in
    a Swarm is hardly ever justified.
  prefs: []
  type: TYPE_NORMAL
- en: 'The manager nodes are not only responsible for managing the Swarm but also
    for maintaining the state of the Swarm. *What do we mean by that?* When we talk
    about the state of the Swarm we mean all of the information about it—for example, *how
    many nodes are in the Swarm and* *what are the properties of each node, such as
    name or IP address*. We also mean what containers are running on which node in
    the Swarm and more. What, on the other hand, is not included in the state of the
    Swarm is data produced by the application services running in containers on the
    Swarm. This is called application data and is definitely not part of the state
    that is managed by the manager nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf1b7c95-25a4-43a4-aa20-168b1b938059.png)'
  prefs: []
  type: TYPE_IMG
- en: A Swarm manager consensus group
  prefs: []
  type: TYPE_NORMAL
- en: All of the Swarm states are stored in a high-performance key-value store (**kv-store**)
    on each **manager** node. That's right, each **manager** node stores a complete replica
    of the whole Swarm state. This redundancy makes the Swarm highly available. If
    a **manager** node goes down, the remaining **managers** all have the complete
    state at hand.
  prefs: []
  type: TYPE_NORMAL
- en: If a new **manager** joins the consensus group, then it synchronizes the Swarm
    state with the existing members of the group until it has a complete replica.
    This replication is usually pretty fast in typical Swarms but can take a while
    if the Swarm is big and many applications are running on it.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm workers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, a Swarm worker node is meant to host and run containers that
    contain the actual application services we're interested in running on our cluster.
    They are the workhorses of the Swarm. In theory, a manager node can also be a
    worker. But, as we already said, this is not recommended on a production system.
    On a production system, we should let managers be managers.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes communicate with each other over the so-called control plane. They
    use the gossip protocol for their communication. This communication is asynchronous,
    which means that, at any given time, it is likely that not all worker nodes are
    in perfect sync.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might ask—*what information do worker nodes exchange?* It is mostly
    information that is needed for service discovery and routing, that is, information
    about which containers are running on with nodes and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8049bdd7-03d8-405a-9ed4-e48e4d2216b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Worker nodes communicating with each other
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you can see how workers communicate with each other.
    To make sure the gossiping scales well in a large Swarm, each **worker** node
    only synchronizes its own state with three random neighbors. For those who are
    familiar with Big O notation, that means that the synchronization of the **worker**
    nodes using the gossip protocol scales with O(0).
  prefs: []
  type: TYPE_NORMAL
- en: '**Worker** nodes are kind of passive. They never actively do something other
    than run the workloads that they get assigned by the manager nodes. The **worker** makes sure,
    though, that it runs these workloads to the best of its capabilities. Later on
    in this chapter, we will get to know more about exactly what workloads the worker
    nodes are assigned by the manager nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacks, services, and tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using a Docker Swarm versus a single Docker host, there is a paradigm
    change. Instead of talking of individual containers that run processes, we are
    abstracting away to services that represent a set of replicas of each process,
    and, in this way, become highly available. We also do not speak anymore of individual
    Docker hosts with well-known names and IP addresses to which we deploy containers;
    we''ll now be referring to clusters of hosts to which we deploy services. We don''t
    care about an individual host or node anymore. We don''t give it a meaningful
    name; each node rather becomes a number to us. We also don''t care about individual
    containers and where they are deployed any longer—we just care about having a
    desired state defined through a service. We can try to depict that as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1ff1173-269e-4448-a409-8279610fc9be.png)'
  prefs: []
  type: TYPE_IMG
- en: Containers are deployed to well-known servers
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of deploying individual containers to well-known servers like in the
    preceding diagram, where we deploy the **web** container to the **alpha** server with
    the IP address `52.120.12.1`, and the **payments** container to the **beta** server with
    the IP `52.121.24.33`, we switch to this new paradigm of services and Swarms (or,
    more generally, clusters):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6dd5f3db-4cb9-4052-9071-a8cb8256586f.png)'
  prefs: []
  type: TYPE_IMG
- en: Services are deployed to Swarms
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we see that a **web** service and an **inventory** service are
    both deployed to a **Swarm** that consists of many nodes. Each of the services
    has a certain number of replicas: six for **web** and five for **inventory**.
    We don''t really care on which node the replicas will run; we only care that the
    requested number of replicas is always running on whatever nodes the **Swarm**
    scheduler decides to put them on.'
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Swarm service is an abstract thing. It is a description of the desired state
    of an application or application service that we want to run in a Swarm. The Swarm
    service is like a manifest describing such things as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image from which to create the containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of replicas to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network(s) that the containers of the service are attached to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ports that should be mapped
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having this service manifest, the Swarm manager then makes sure that the described
    desired state is always reconciled if ever the actual state should deviate from
    it. So, if for example, one instance of the service crashes, then the scheduler
    on the Swarm manager schedules a new instance of this particular service on a
    node with free resources so that the desired state is reestablished.
  prefs: []
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned that a service corresponds to a description of the desired state
    in which an application service should be at all times. Part of that description
    was the number of replicas the service should be running. Each replica is represented
    by a task. In this regard, a Swarm service contains a collection of tasks. On
    Docker Swarm, a task is the atomic unit of deployment. Each task of a service
    is deployed by the Swarm scheduler to a worker node. The task contains all of
    the necessary information that the worker node needs to run a container based
    on the image, which is part of the service description. Between a task and a container,
    there is a one-to-one relation. The container is the instance that runs on the
    worker node, while the task is the description of this container as a part of
    a Swarm service.
  prefs: []
  type: TYPE_NORMAL
- en: Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a good idea about what a Swarm service is and what tasks are,
    we can introduce the stack. A stack is used to describe a collection of Swarm
    services that are related, most probably because they are part of the same application.
    In that sense, we could also say that a stack describes an application that consists
    of one to many services that we want to run on the Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, we describe a stack declaratively in a text file that is formatted
    using the YAML format and that uses the same syntax as the already-known Docker
    Compose file. This leads to a situation where people sometimes say that a stack
    is described by a `docker-compose` file. A better wording would be: a stack is
    described in a stack file that uses similar syntax to a `docker-compose` file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to illustrate the relationship between the stack, services, and
    tasks in the following diagram and connect it with the typical content of a stack
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34c40ca5-56fa-4ce6-a61c-aa6d5ded8230.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram showing the relationship between stack, services, and tasks
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we see on the right-hand side a declarative description
    of a sample **Stack**. The **Stack** consists of three services called **web**, **payments**,
    and **inventory**. We also see that the **web** service uses the **example/web:1.0** image and has
    four replicas.
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of the diagram, we see that the **Stack** embraces the
    three services mentioned. Each service, in turn, contains a collection of **Tasks**,
    as many as there are replicas. In the case of the **web** service, we have a collection
    of four **Tasks**. Each **Task** contains the name of the **Image** from which
    it will instantiate a container once the **Task** is scheduled on a Swarm node.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-host networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 10](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml), *Single-Host Networking,* we
    discussed how containers communicate on a single Docker host. Now, we have a Swarm
    that consists of a cluster of nodes or Docker hosts. Containers that are located
    on different nodes need to be able to communicate with each other. Many techniques
    can help us to achieve this goal. Docker has chosen to implement an **overlay
    network** driver for Docker Swarm. This **overlay network** allows containers
    attached to the same **overlay network** to discover each other and freely communicate
    with each other. The following is a schema for how an **overlay network** works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1731fa28-e8f4-459e-b4c2-40e63b876032.png)'
  prefs: []
  type: TYPE_IMG
- en: Overlay network
  prefs: []
  type: TYPE_NORMAL
- en: We have two nodes or Docker hosts with the IP addresses `172.10.0.15` and `172.10.0.16`.
    The values we have chosen for the IP addresses are not important; what is important
    is that both hosts have a distinct IP address and are connected by a physical
    network (a network cable), which is called the **underlay network**.
  prefs: []
  type: TYPE_NORMAL
- en: On the node on the left-hand side we have a container running with the IP address `10.3.0.2`, and
    on the node on the right-hand side another container with the IP address `10.3.0.5`.
    Now, the former container wants to communicate with the latter. *How can this
    happen?* In [Chapter 10](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml), *Single-Host
    Networking,* we saw how this works when both containers are located on the same
    node—by using a Linux bridge. But Linux bridges only operate locally and cannot
    span across nodes. So, we need another mechanism. Linux VXLAN comes to the rescue.
    VXLAN has been available on Linux since way before containers were a thing.
  prefs: []
  type: TYPE_NORMAL
- en: When the left-hand container sends a data packet, the **bridge** realizes that
    the target of the packet is not on this host. Now, each node participating in
    an overlay network gets a so-called **VXLAN Tunnel Endpoint** (**VTEP**) object,
    which intercepts the packet (the packet at that moment is an OSI layer 2 data
    packet), wraps it with a header containing the target IP address of the host that
    runs the destination container (this makes it now an OSI layer 3 data packet),
    and sends it over the **VXLAN tunnel**. The **VTEP** on the other side of the
    tunnel unpacks the data packet and forwards it to the local bridge, which in turn
    forwards it to the destination container.
  prefs: []
  type: TYPE_NORMAL
- en: The overlay driver is included in SwarmKit and is in most cases the recommended
    network driver for Docker Swarm. There are other multi-node-capable network drivers
    available from third parties that can be installed as plugins to each participating
    Docker host. Certified network plugins are available from the Docker store.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a Docker Swarm is almost trivial. It is so easy that it seems unreal
    if you know what an orchestrator is all about. But it is true, Docker has done
    a fantastic job in making Swarms simple and elegant to use. At the same time,
    Docker Swarm has been proven in use by large enterprises to be very robust and
    scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a local single node swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, enough imagining — let's demonstrate how we can create a Swarm. In its most
    simple form, a fully functioning Docker Swarm consists only of a single node.
    If you're using Docker for Mac or Windows, or even if you're using Docker Toolbox,
    then your personal computer or laptop is such a node. Hence, we can start right
    there and demonstrate some of the most important features of a Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s initialize a Swarm. On the command line, just enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And after an incredibly short time, you should see something like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39cd7eb9-5916-422b-a23f-a158857f0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of the Docker Swarm init command
  prefs: []
  type: TYPE_NORMAL
- en: 'Our computer is now a Swarm node. Its role is that of a manager and it is the
    leader (of the managers, which makes sense since there is only one manager at
    this time). Although it took only a very short time to finish `docker swarm init`,
    the command did a lot of things during that time. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It created a root **Certificate Authority** (**CA**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It created a key-value store that is used to store the state of the whole Swarm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, in the preceding output, we can see a command that can be used to join
    other nodes to the Swarm that we just created. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<join-token>` is a token generated by the Swarm leader at the time the Swarm
    was initialized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<IP address>` is the IP address of the leader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although our cluster remains simple, as it consists of only one member, we
    can still ask the Docker CLI to list all of the nodes of the Swarm. This will
    look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4519a489-42e8-42d8-9ef0-8b791ce51d14.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing the nodes of the Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: In this output, we first see `ID` that was given to the node. The star (`*`)
    that follows `ID` indicates that this is the node on which `docker node ls` was
    executed—basically saying that this is the active node. Then, we have the (human-readable)
    name of the node, its status, availability, and manager status. As mentioned earlier,
    this very first node of the Swarm automatically became the leader, which is indicated
    in the preceding screenshot. Lastly, we see which version of the Docker engine
    we're using.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get even more information about a node, we can use the `docker node inspect` command,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9182c97-d23d-4bdc-a365-38f1897f4b63.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated output of the docker node inspect command
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of information generated by this command, so we only present
    a truncated version of the output. This output can be useful, for example, when
    you need to troubleshoot a misbehaving cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a local Swarm in VirtualBox or Hyper-V
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, a single node Swarm is not enough, but we don't have or don't want to
    use an account to create a Swarm in the cloud. In this case, we can create a local
    Swarm in either VirtualBox or Hyper-V. Creating the Swarm in VirtualBox is slightly
    easier than creating it in Hyper-V, but if you're using Windows 10 and have Docker
    for Windows running, then you cannot use VirtualBox at the same time. The two
    hypervisors are mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume we have VirtualBox and `docker-machine` installed on our laptop.
    We can then use `docker-machine` to list all Docker hosts that are currently defined
    and may be running in VirtualBox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In my case, I have one VM called `default` defined, which is currently stopped.
    I can easily start the VM by issuing the `docker-machine start default` command.
    This command takes a while and will result in the following (shortened) output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if I list my VMs again, I should see the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e64986fa-4614-4968-9d15-b62e7e873916.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all VMs running in Hyper-V
  prefs: []
  type: TYPE_NORMAL
- en: 'If we do not have a VM called `default` yet, we can easily create one using
    the `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a6258ef-8613-41e5-95cb-b86bfe858b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of docker-machine create
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the preceding output how `docker-machine`creates the VM from an
    ISO image, defines SSH keys and certificates, and copies them to the VM and to
    the local `~/.docker/machine` directory, where we will use it later when we want
    to remotely access this VM through the Docker CLI. It also provisions an IP address
    for the new VM.
  prefs: []
  type: TYPE_NORMAL
- en: We're using the `docker-machine create` command with the `--driver virtualbox` parameter.
    The docker-machine can also work with other drivers such as Hyper-V, AWS, Azure,
    DigitalOcean, and many more. Please see the documentation of `docker-machine`
    for more information. By default, a new VM gets 1 GB of memory associated, which
    is enough to use this VM as a node for a development or test Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: If you're on Windows 10 with Docker for Desktop, use the `hyperv` driver instead.
    To be successful though, you need to run as Administrator. Furthermore, you need
    to have an external virtual switch defined on Hyper-V first. You can use the Hyper-V
    Manager to do so. The output of the command will look very similar to the one
    for the `virtualbox` driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create five VMs for a five-node Swarm. We can use a bit of scripting
    to reduce the manual work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `docker-machine` will now create five VMs with the names `node-1` to `node-5`.
    This might take a few moments, so this is a good time to get yourself a hot cup
    of tea. After the VMs are created, we can list them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2820423c-f815-4247-a15e-251dd7791c3d.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all the VMs we need for the Swarm
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re ready to build a Swarm. Technically, we could SSH into the first
    VM `node-1` and initialize a Swarm and then SSH into all the other VMs and join
    them to the Swarm leader. But this is not efficient. Let''s again use a script
    that does all of the hard work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the join token and the IP address of the Swarm leader, we
    can ask the other nodes to join the Swarm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the Swarm highly available, we can now promote, for example, `node-2` and `node-3 `to
    become managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can list all of the nodes of the Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfb95f3f-4cbd-4275-b620-f5a6b5493e88.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all of the nodes of the Docker Swarm on VirtualBox
  prefs: []
  type: TYPE_NORMAL
- en: 'This is proof that we have just created a highly available Docker Swarm locally
    on our laptop or workstation. Let''s pull all of our code snippets together and
    make the whole thing a bit more robust. The script will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script first deletes (if present) and then recreates five VMs
    called `node-1` to `node-5`, and then initializes a Swarm on `node-1`. After that,
    the remaining four VMs are added to the Swarm, and finally, `node-2` and `node-3` are
    promoted to manager status to make the Swarm highly available. The whole script
    will take less than 5 minutes to execute and can be repeated as many times as
    desired. The complete script can be found in the repository, in the `docker-swarm` subfolder;
    it is called `create-swarm.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: It is a highly recommended best practice to always script and hence automate
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Using Play with Docker to generate a Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To experiment with Docker Swarm without having to install or configure anything locally
    on our computer, we can use **Play with Docker** (**PWD**). PWD is a website that
    can be accessed with a browser and that offers us the ability to create a Docker
    Swarm consisting of up to five nodes. It is definitely a playground, as the name
    implies, and the time for which we can use it is limited to four hours per session.
    We can open as many sessions as we want, but each session automatically ends after
    four hours. Other than that, it is a fully functional Docker environment that
    is ideal for tinkering with Docker or to demonstrate some features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s access the site now. In your browser, navigate to the website [https://labs.play-with-docker.com](https://labs.play-with-docker.com).
    You will be presented with a welcome and login screen. Use your Docker ID to log
    in. After successfully going so, you will be presented with a screen that looks
    like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3172277e-9a56-44e6-8651-cfe1d23cf525.png)'
  prefs: []
  type: TYPE_IMG
- en: Play with Docker window
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see immediately, there is a big timer counting down from four hours.
    That''s how much time we have left to play in this session. Furthermore, we see
    a + ADD NEW INSTANCE link. Click it to create a new Docker host. When you do that,
    your screen should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07316581-3443-4328-ba3b-5c57af1e85c8.png)'
  prefs: []
  type: TYPE_IMG
- en: PWD with one new node
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side, we see the newly created node with its IP address (`192.168.0.48`)
    and its name (`node1`). On the right-hand side, we have some additional information
    about this new node in the upper half of the screen and a Terminal in the lower
    half. Yes, this Terminal is used to execute commands on this node that we just
    created. This node has the Docker CLI installed, and hence we can execute all
    of the familiar Docker commands on it such as `docker version`. Try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now we want to create a Docker Swarm. Execute the following command in
    the Terminal in your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The output generated by the preceding command corresponds to what we already
    know from our previous trials with the one-node cluster on our workstation and
    the local cluster using VirtualBox or Hyper-V. The important information, once
    again, is the `join` command that we want to use to join additional nodes to the
    cluster we just created.
  prefs: []
  type: TYPE_NORMAL
- en: You might have noted that this time we specified the `--advertise-addr` parameter in
    the Swarm `init` command. *Why is that necessary here?* The reason is that the
    nodes generated by PWD have more than one IP address associated with them. We
    can easily verify that by executing the `ip a` command on the node. This command
    will show us that there are indeed two endpoints, `eth0` and `eth1`, present.
    We hence have to specify explicitly to the new to-be swarm manager which one we
    want to use. In our case, it is `eth0`.
  prefs: []
  type: TYPE_NORMAL
- en: Create four additional nodes in PWD by clicking four times on the + ADD NEW
    INSTANCE link. The new nodes will be called `node2`, `node3`, `node4`, and `node5` and
    will all be listed on the left-hand side. If you click on one of the nodes on
    the left-hand side, then the right-hand side shows the details of the respective
    node and a Terminal window for that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select each node (2 to 5) and execute the `docker swarm join` command that you
    have copied from the leader node (`node1`) in the respective Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22724adb-5938-4487-a1f2-9d491800bb51.png)'
  prefs: []
  type: TYPE_IMG
- en: Joining a node to the Swarm in PWD
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have joined all four nodes to the Swarm, switch back to `node1` and
    list all nodes, which, unsurprisingly, results in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e270d3d8-8783-4eec-9ba2-2a66f9a43a08.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all of the nodes of the swarm in PWD
  prefs: []
  type: TYPE_NORMAL
- en: 'Still on `node1`, we can now promote, say, `node2` and `node3`, to make the
    Swarm highly available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With this, our Swarm on PWD is ready to accept a workload. We have created a
    highly available Docker Swarm with three manager nodes that form a Raft consensus
    group and two worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker Swarm in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the Docker Swarms we have created so far are wonderful to use in development or
    to experiment or to use for demonstration purposes. If we want to create a Swarm
    that can be used as a production environment where we run our mission-critical
    applications, though, then we need to create a—I'm tempted to say—real Swarm in
    the cloud or on premises. In this book, we are going to demonstrate how to create
    a Docker Swarm in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to create a Swarm is by using **d****ocker-machine** (**DM**). DM has
    a driver for AWS. If we have an account on AWS, we need the AWS access key ID
    and the AWS secret access key. We can add those two values to a file called `~/.aws/configuration`.
    It should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Every time we run `docker-machine create`, DM will look up those values in that
    file. For more in-depth information on how to get an AWS account and how to obtain
    the two secret keys, please consult this link: [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have an AWS account in place and have stored the access keys in the
    configuration file, we can start building our Swarm. The necessary code looks
    exactly the same as the one we used to create a Swarm on our local machine in
    VirtualBox. Let''s start with the first node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create an EC2 instance called `aws-node-1` in the requested region
    (`us-east-1` in my case). The output of the preceding command looks like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca6013e9-a419-478f-8967-e86ac6defaab.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a swarm node on AWS with DM
  prefs: []
  type: TYPE_NORMAL
- en: 'It looks very similar to the output we already know from working with VirtualBox.
    We can now configure our Terminal for remote access to that EC2 instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will configure the environment variables used by the Docker CLI accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2776b997-08d5-4993-8f68-e047a054969e.png)'
  prefs: []
  type: TYPE_IMG
- en: Environment variables used by Docker to enable remote access to the AWS EC2
    node
  prefs: []
  type: TYPE_NORMAL
- en: For security reasons, **Transport Layer Security** (**TLS**) is used for the communication between
    our CLI and the remote node. The certificates necessary for that were copied by
    DM to the path we assigned to the environment variable `DOCKER_CERT_PATH`.
  prefs: []
  type: TYPE_NORMAL
- en: 'All Docker commands that we now execute in our Terminal will be remotely executed
    in AWS on our EC2 instance. Let''s try to run Nginx on this node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `docker container ls` to verify that the container is running. If
    so, then let''s test it using `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `<IP address>` is the public IP address of the AWS node; in my case,
    it would be `35.172.240.127`. Sadly, this doesn''t work; the preceding command
    times out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f6eb2e9-c813-4c67-b0a8-3d4e623caa0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing Nginx on the AWS node times out
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for this is that our node is part of an AWS **Security Group** (**SG**).
    By default, access to objects inside this SG is denied. Hence, we have to find
    out to which SG our instance belongs and configure access explicitly. For this,
    we typically use the AWS console. Go to the EC2 Dashboard and select Instances
    on the left-hand side. Locate the EC2 instance called `aws-node-1` and select
    it. In the details view, under Security groups, click on the docker-machine link, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daacdfa4-bfb7-4c54-8333-4f42a645ed2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Locating the SG to which our Swarm node belongs
  prefs: []
  type: TYPE_NORMAL
- en: 'This will lead us to the SG page with the `docker-machine` SG pre-selected.
    In the details section under the Inbound tab, add a new rule for your IP address
    (the IP address of workstation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d0ee48e-dd73-4b8e-87ee-e5069e77183b.png)'
  prefs: []
  type: TYPE_IMG
- en: Open access to SG for our computer
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, the IP address `70.113.114.234` happens to be the
    one assigned to my personal workstation. I have enabled all inbound traffic coming
    from this IP address to the `docker-machine` SG. Note that in a production system
    you should be very careful about which ports of the SG to open to the public.
    Usually, it is ports `80` and `443` for HTTP and HTTPS access. Everything else
    is a potential invitation to hackers.
  prefs: []
  type: TYPE_NORMAL
- en: You can get your own IP address through a service such as [https://www.whatismyip.com/](https://www.whatismyip.com/).
    Now, if we execute the `curl` command again, the greeting page of Nginx is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we leave the SG, we should add another rule to it. The Swarm nodes need
    to be able to freely communicate on ports `7946` and `4789` through TCP and UDP
    and on port `2377` through TCP. We could now add five rules with these requirements
    where the source is the SG itself, or we just define a crude rule that allows
    all inbound traffic inside the SG (`sg-c14f4db3` in my case):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a51e4f76-db2c-4513-8dc2-d878d5e0ba30.png)'
  prefs: []
  type: TYPE_IMG
- en: SG rule to enable intra-Swarm communication
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s continue with the creation of the remaining four nodes. Once again,
    we can use a script to ease the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After the provisioning of the nodes is done, we can list all nodes with DM.
    In my case, I see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fe33d9f-01b8-4fc8-9bf7-c24f2cfaa1a3.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all the nodes created by DM
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see the five nodes that we originally created
    in VirtualBox and the five new nodes that we created in AWS. Apparently, the nodes on
    AWS are using a new version of Docker; here, the version is `18.02.0-ce`. The
    IP addresses we see in the `URL` column are the public IP addresses of my EC2
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our CLI is still configured for remote access to the `aws-node-1` node,
    we can just run the `swarm init` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the join token do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the IP address of the leader use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'With this information, we can now join the other four nodes to the Swarm leader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative way to achieve the same without needing to SSH into the individual
    nodes would be to reconfigure our client CLI every time we want to access a different
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As a last step, we want to promote nodes `2` and `3` to manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then list all of the Swarm nodes, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0038c9cb-fc2f-4a9d-ac15-af1133dbf6b8.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all nodes of our swarm in the cloud
  prefs: []
  type: TYPE_NORMAL
- en: 'And hence we have a highly available Docker Swarm running in the cloud. To
    clean up the Swarm in the cloud and avoid incurring unnecessary costs, we can
    use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Deploying a first application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have created a few Docker Swarms on various platforms. Once created, a Swarm
    behaves the same way on any platform. The way we deploy and update applications
    on a Swarm is not platform-dependent. It has been one of Docker's main goals to
    avoid vendor lock-in when using a Swarm. Swarm-ready applications can be effortlessly
    migrated from, say, a Swarm running on premises to a cloud-based Swarm. It is
    even technically possible to run part of a Swarm on premises and another part
    in the cloud. It works, yet we have, of course, to consider possible side effects
    due to the higher latency between nodes in geographically distant areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a highly available Docker Swarm up and running, it is time
    to run some workloads on it. I''m using a local Swarm created with docker-machine.
    We''ll start by first creating a single service. For this, we need to SSH into
    one of the manager nodes. I select `node-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Creating a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A service can be either created as part of a stack or directly using the Docker
    CLI. Let''s first look at a sample stack file that defines a single service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we see what the desired state of a service called `whoami` is:'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the `training/whoami:latest` image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers of the service are attached to the `test-net` network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container port `8000` is published to port `81`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is running with six replicas (or tasks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During a rolling update, the individual tasks are updated in batches of two,
    with a delay of 10 seconds between each successful batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service (and its tasks and containers) is assigned the two labels `app` and `environment `with
    the values `sample-app` and `prod-south`, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more settings that we could define for a service, but the preceding
    ones are some of the more important ones. Most settings have meaningful default
    values. If, for example, we do not specify the number of replicas, then Docker
    defaults it to `1`. The name and image of a service are, of course, mandatory.
    Note that the name of the service must be unique in the Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the preceding service, we use the `docker stack deploy` command.
    Assuming that the file in which the preceding content is stored is called `stack.yaml`, we
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have created a stack called `sample-stack` that consists of one service, `whoami`.
    We can list all stacks on our Swarm, whereupon we should get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If we list the services defined in our Swarm, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2598bb40-3f60-44b3-8bb8-2d23d4e5a2df.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all services running in the Swarm
  prefs: []
  type: TYPE_NORMAL
- en: In the output, we can see that currently, we have only one service running,
    which was to be expected. The service has an `ID`. The format of `ID`, contrary
    to what you have used so far for containers, networks, or volumes, is alphanumeric
    (in the latter cases it was always `sha256`). We can also see that `NAME` of the
    service is a combination of the service name we defined in the stack file and
    the name of the stack, which is used as a prefix. This makes sense since we want
    to be able to deploy multiple stacks (with different names) using the same stack
    file into our Swarm. To make sure that service names are unique, Docker decided
    to combine service name and stack name.
  prefs: []
  type: TYPE_NORMAL
- en: In the third column, we see the mode, which is `replicated`. The number of `REPLICAS`
    is shown as `6/6`. This tells us that six out of the six requested `REPLICAS`
    are running. This corresponds to the desired state. In the output, we also see
    the image that the service uses and the port mappings of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the service and its tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding output, we cannot see the details of the `6` replicas that have been
    created. To get some deeper insight into that, we can use the `docker service
    ps` command. If we execute this command for our service, we will get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/407ffc09-b33c-459b-a73c-fb17de789245.png)'
  prefs: []
  type: TYPE_IMG
- en: Details of the whoami service
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see the list of six tasks that correspond to
    the requested six replicas of our `whoami` service. In the `NODE` column, we can
    also see the node to which each task has been deployed. The name of each task
    is a combination of the service name plus an increasing index. Also note that,
    similar to the service itself, each task gets an alphanumeric ID assigned.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, apparently task 2, with the name `sample-stack_whoami.2`, has been
    deployed to `node-1`, which is the leader of our Swarm. Hence, I should find a
    container running on this node. Let''s see what we get if we list all containers
    running on `node-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18d77691-b017-4ff3-9ea0-d07b10cd107a.png)'
  prefs: []
  type: TYPE_IMG
- en: List of containers on node-1
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, we find a container running from the `training/whoami:latest` image
    with a name that is a combination of its parent task name and ID. We can try to
    visualize the whole hierarchy of objects that we generated when deploying our
    sample stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/376c4e6d-f65b-4c82-b74b-02f320141812.png)'
  prefs: []
  type: TYPE_IMG
- en: Object hierarchy of a Docker Swarm stack
  prefs: []
  type: TYPE_NORMAL
- en: 'A **stack** can consist of one to many services. Each service has a collection
    of tasks. Each task has a one-to-one association with a container. Stacks and
    services are created and stored on the Swarm manager nodes. Tasks are then scheduled
    to Swarm worker nodes, where the worker node creates the corresponding container.
    We can also get some more information about our service by inspecting it. Execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This provides a wealth of information about all of the relevant settings of
    the service. This includes those we have explicitly defined in our `stack.yaml` file,
    but also those that we didn't specify and that therefore got their default values
    assigned. We're not going to list the whole output here, as it is too long, but
    I encourage the reader to inspect it on their own machine. We will discuss part
    of the information in more detail in the *The swarm routing mesh* section.
  prefs: []
  type: TYPE_NORMAL
- en: Logs of a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an earlier chapter, we worked with the logs produced by a container. Here,
    we''re concentrating on a service. Remember that, ultimately, a service with many
    replicas has many containers running. Hence, we would expect that, if we ask the
    service for its logs, Docker returns an aggregate of all logs of those containers
    belonging to the service. And indeed, that''s what we get if we use the `docker
    service logs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbe0a89f-0434-42a8-ae44-c1abdf29515f.png)'
  prefs: []
  type: TYPE_IMG
- en: Logs of the whoami service
  prefs: []
  type: TYPE_NORMAL
- en: There is not much information in the logs at this point, but it is enough to
    discuss what we get. The first part of each line in the log always contains the
    name of the container combined with the node name from which the log entry originates.
    Then, separated by the vertical bar (`|`), we get the actual log entry. So, if
    we would, say, ask for the logs of the first container in the list directly, we
    would only get a single entry, and the value we would see in this case would be `Listening
    on :8000`.
  prefs: []
  type: TYPE_NORMAL
- en: The aggregated logs that we get with the `docker service logs` command are not
    sorted in any particular way. So, if the correlation of events is happening in
    different containers, you should add information to your log output that makes
    this correlation possible. Typically, this is a timestamp for each log entry.
    But this has to be done at the source; for example, the application that produces
    a log entry needs to also make sure a timestamp is added.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can as well query the logs of an individual task of the service by providing
    the task ID instead of the service ID or name. So, querying the logs from task
    2 gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edd0bb6b-3bfc-4cb8-97f1-ddab8836e1c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Logs of an individual task of the whoami service
  prefs: []
  type: TYPE_NORMAL
- en: Reconciling the desired state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned that a Swarm service is a description or manifest of the desired
    state that we want an application or application service to run in. Now, let's
    see how Docker Swarm reconciles this desired state if we do something that causes
    the actual state of the service to be different from the desired state. The easiest
    way to do this is to forcibly kill one of the tasks or containers of the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do this with the container that has been scheduled on `node-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do that and then do `docker service ps` right afterward, we will see
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b7a9616-8a86-467d-8e73-eafd14dd9f97.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker Swarm reconciling the desired state after one task failed
  prefs: []
  type: TYPE_NORMAL
- en: We see that task 2 failed with exit code `137` and that the Swarm immediately
    reconciled the desired state by rescheduling the failed task on a node with free
    resources. In this case, the scheduler selected the same node as the failed tasks,
    but this is not always the case. So, without us intervening, the Swarm completely
    fixed the problem, and since the service is running in multiple replicas, at no
    time was the service down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try another failure scenario. This time we''re going to shut down an
    entire node and are going to see how the Swarm reacts. Let''s take `node-2` for
    this, as it has two tasks (tasks 3 and 4) running on it. For this, we need to
    open a new Terminal window and use `docker-machine` to stop `node-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Back on `node-1`, we can now again run `docker service ps` to see what happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9b37aa0-d5c0-4666-8a11-f57874d4d283.png)'
  prefs: []
  type: TYPE_IMG
- en: Swarm reschedules all tasks of a failed node
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see that immediately task 3 was rescheduled
    on `node-1` while task 4 was rescheduled on `node-3`. Even this more radical failure
    is handled gracefully by Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note though that if `node-2` ever comes back online in the
    Swarm, the tasks that had previously been running on it will not automatically
    be transferred back to it. But the node is now ready for a new workload.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a service or a stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to remove a particular service from the Swarm, we can use the `docker
    service rm` command. If, on the other hand, we want to remove a stack from the
    Swarm, we analogously use the `docker stack rm` command. This command removes
    all services that are part of the stack definition. In the case of the `whoami` service,
    it was created by using a stack file and hence we''re going to use the latter
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61fca437-abee-41f3-8cea-6a8a534a58fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Removing a stack
  prefs: []
  type: TYPE_NORMAL
- en: The preceding command will make sure that all tasks of each service of the stack
    are terminated, and the corresponding containers are stopped by first sending `SIGTERM`,
    and then, if not successful, `SIGKILL` after 10 seconds of timeout.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the stopped containers are not removed from the
    Docker host. Hence, it is advised to purge containers from time to time on worker
    nodes to reclaim unused resources. Use `docker container purge -f` for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Why does it make sense to leave stopped or crashed containers on
    the worker node and not automatically remove them?'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a multi-service stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml), *Docker Compose*, we
    used an application consisting of two services that were declaratively described
    in a Docker compose file. We can use this compose file as a template to create
    a stack file that allows us to deploy the same application into a Swarm. The content
    of our stack file, called `pet-stack.yaml`, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We request that the `web` service has three replicas, and both services are
    attached to the overlay network, `pets-net`. We can deploy this application using
    the `docker stack deploy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4920e7c3-dc61-4bcf-9960-cc64dcb42e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploy the pets stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker creates the `pets_pets-net` overlay network and then the two services `pets_web` and `pets_db`.
    We can then list all of the tasks in the `pets` stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2af11ff2-9cb7-4056-aa02-409bce27fed2.png)'
  prefs: []
  type: TYPE_IMG
- en: List of all of the tasks in the pets stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s test the application using `curl`. And, indeed, the application
    works as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e7fac0b-5781-4038-a0f8-8f53d933da3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing the pets application using curl
  prefs: []
  type: TYPE_NORMAL
- en: The container ID is in the output, where it says `Delivered to you by container 8b906b509a7e`.
    If you run the `curl` command multiple times, the ID should cycle between three
    different values. These are the IDs of the three containers (or replicas) that
    we have requested for the `web` service.
  prefs: []
  type: TYPE_NORMAL
- en: Once we're done, we can remove the stack with `docker stack rm pets`.
  prefs: []
  type: TYPE_NORMAL
- en: The swarm routing mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have been paying attention, then you might have noticed something interesting
    in the last section. We had the `pets` application deployed and it resulted in
    the fact that an instance of the `web` service was installed on the three nodes, `node-1`, `node-2`,
    and `node-3`. Yet, we were able to access the `web` service on `node-1` with `localhost` and
    we reached each container from there. *How is that possible?* Well, this is due
    to the so-called Swarm routing mesh. The routing mesh makes sure that when we
    publish a port of a service; that port is then published on all nodes of the Swarm.
    Hence, network traffic that hits any node of the Swarm and requests to use a specific
    port will be forwarded to one of the service containers by the routing mesh. Let''s
    look at the following diagram to see how that works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ceca300-992c-4267-a309-239315df3cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker Swarm routing mesh
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, we have three nodes, called **Host A** to **Host C**, with
    the IP addresses `172.10.0.15`, `172.10.0.17`, and `172.10.0.33`. In the lower-left
    corner of the diagram, we see the command that created a **web** service with
    two replicas. The corresponding tasks have been scheduled on **Host B** and **Host
    C**. Task 1 landed on **Host B** while task 2 landed on **Host C**.
  prefs: []
  type: TYPE_NORMAL
- en: When a service is created on Docker Swarm, it automatically gets a **V****irtual
    IP** (**VIP**) address assigned. This IP address is stable and reserved during
    the whole life cycle of the service. Let's assume that in our case the VIP is `10.2.0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: If now a request for port `8080` coming from an external **Load Balancer** (**LB**)
    is targeted at one of the nodes of our Swarm, then this request is handled by
    the Linux **IP Virtual Server** (**IPVS**) service on that node. This service
    makes a lookup with the given port `8080` in the IP table and will find that this
    corresponds to the VIP of the **web** service. Now, since the VIP is not a real
    target, the IPVS service will load balance the IP addresses of the tasks that
    are associated with this service. In our case, it picked task 2, with the IP address, `10.2.0.3`.
    Finally, the **ingress**** Network** (**O****verlay**) is used to forward the
    request to the target container on **Host C**.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that it doesn't matter which Swarm node the external
    request is forwarded to by the **External LB**. The routing mesh will always handle
    the request correctly and forward it to one of the tasks of the targeted service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced Docker Swarm, which, next to Kubernetes, is the
    second most popular orchestrator for containers. We have looked into the architecture
    of a Swarm, discussed all of the types of resources running in a Swarm, such as
    services, tasks, and more, and we have created services in the Swarm and deployed
    an application that consists of multiple related services.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore how to deploy services or applications
    onto a Docker Swarm with zero downtime and automatic rollback capabilities. We
    are also going to introduce secrets as a means to protect sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your learning progress, please answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do you initialize a new Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. `docker init swarm`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. `docker swarm init --advertise-addr <IP address>`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. `docker swarm join --token <join token>`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You want to remove a worker node from a Docker Swarm. What steps are necessary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you create an overlay network called `front-tier`? Make the network attachable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you create a service called `web` from the `nginx:alpine` image with
    five replicas, which exposes port `3000` on the ingress network and is attached
    to the `front-tier` network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you scale the web service down to three instances?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please consult the following link for more in-depth information about selected
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS EC2 example at [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Raft Consensus Algorithm at [https://raft.github.io/](https://raft.github.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gossip Protocol at [https://en.wikipedia.org/wiki/Gossip_protocol](https://en.wikipedia.org/wiki/Gossip_protocol)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VXLAN and Linux at [https://vincent.bernat.ch/en/blog/2017-vxlan-linux](https://vincent.bernat.ch/en/blog/2017-vxlan-linux)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
