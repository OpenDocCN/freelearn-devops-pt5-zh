- en: Introduction to Docker Swarm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm 介绍
- en: In the last chapter, we introduced orchestrators. Like a conductor in an orchestra,
    an orchestrator makes sure that all of our containerized application services
    play together nicely and contribute harmoniously to a common goal. Such orchestrators
    have quite a few responsibilities, which we discussed in detail. Finally, we provided
    a short overview of the most important container orchestrators on the market.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了编排工具。就像乐队指挥一样，编排工具确保我们的容器化应用服务能够和谐地协同工作，并共同为一个共同的目标作出贡献。这些编排工具承担着许多责任，我们已进行了详细讨论。最后，我们简要概述了市场上最重要的容器编排工具。
- en: This chapter introduces Docker's native orchestrator, SwarmKit. It elaborates
    on all of the concepts and objects SwarmKit uses to deploy and run distributed,
    resilient, robust, and highly available applications in a cluster on premises
    or in the cloud. This chapter also introduces how SwarmKit ensures secure applications by
    using a **Software-Defined Network** (**SDN**) to isolate containers. Additionally,
    this chapter demonstrates how to install a highly available Docker Swarm in the
    cloud. It introduces the routing mesh, which provides layer-4 routing and load
    balancing. Finally, it demonstrates how to deploy a first application consisting
    of multiple services onto the swarm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Docker 的原生编排工具 SwarmKit。它详细阐述了 SwarmKit 用于在本地或云中集群中部署和运行分布式、弹性、强健且高度可用的应用程序的所有概念和对象。本章还介绍了
    SwarmKit 如何通过使用 **软件定义网络** (**SDN**) 来隔离容器，从而确保应用程序的安全性。此外，本章演示了如何在云中安装一个高可用的
    Docker Swarm。它介绍了路由网格，该网格提供第 4 层路由和负载均衡。最后，它演示了如何将由多个服务组成的第一个应用程序部署到 Swarm 上。
- en: 'These are the topics we are going to discuss in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: The Docker Swarm architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm 架构
- en: Swarm nodes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 节点
- en: Stacks, services, and tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆栈、服务和任务
- en: Multi-host networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多主机网络
- en: Creating a Docker Swarm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 Docker Swarm
- en: Deploying a first application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署第一个应用程序
- en: The Swarm routing mesh
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 路由网格
- en: 'After completing this chapter, you will be able to do the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将能够做到以下几点：
- en: Sketch the essential parts of a highly available Docker Swarm on a whiteboard
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在白板上勾画出高可用 Docker Swarm 的关键部分
- en: Explain in two or three simple sentences to an interested layman what a (swarm)
    service is
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用两三句话向感兴趣的外行解释什么是（Swarm）服务
- en: Create a highly available Docker Swarm in AWS, Azure, or GCP consisting of three
    manager and two worker nodes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS、Azure 或 GCP 中创建一个高可用的 Docker Swarm，包括三个管理节点和两个工作节点
- en: Successfully deploy a replicated service such as Nginx on a Docker Swarm
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功地在 Docker Swarm 上部署一个复制服务，例如 Nginx
- en: Scale a running Docker Swarm service up and down
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展和缩减运行中的 Docker Swarm 服务
- en: Retrieve the aggregated log of a replicated Docker Swarm service
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取复制的 Docker Swarm 服务的聚合日志
- en: Write a simple stack file for a sample application consisting of at least two
    interacting services
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为至少包含两个交互服务的示例应用程序编写一个简单的堆栈文件
- en: Deploy a stack into a Docker Swarm
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个堆栈部署到 Docker Swarm 中
- en: The Docker Swarm architecture
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm 架构
- en: 'The architecture of a Docker Swarm from a 30,000-foot view consists of two
    main parts—a raft consensus group of an odd number of manager nodes, and a group
    of worker nodes that communicate with each other over a gossip network, also called
    the control plane. The following diagram illustrates this architecture:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从 30,000 英尺的高度看，Docker Swarm 的架构由两个主要部分组成——一个由奇数个管理节点组成的 Raft 共识组，以及一个通过 Gossip
    网络相互通信的工作节点组，也叫做控制平面。下图展示了这一架构：
- en: '![](img/185aff48-e453-4420-a5c7-35859f604904.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/185aff48-e453-4420-a5c7-35859f604904.png)'
- en: High-level architecture of a Docker Swarm
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 的高层架构
- en: The **manager** nodes manage the swarm while the **worker** nodes execute the
    applications deployed into the swarm. Each **manager** has a complete copy of
    the full state of the Swarm in its local raft store. Managers synchronously communicate
    with each other and their raft stores are always in sync.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**管理**节点管理 Swarm，而 **工作**节点执行部署到 Swarm 中的应用程序。每个 **管理**节点在其本地的 Raft 存储中都有完整的
    Swarm 状态副本。管理节点之间同步通信，且它们的 Raft 存储始终保持同步。'
- en: The **workers**, on the other hand, communicate with each other asynchronously
    for scalability reasons. There can be hundreds if not thousands of **worker**
    nodes in a Swarm. Now that we have a high-level overview of what a Docker Swarm
    is, let's describe all of the individual elements of a Docker Swarm in more detail.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，工作节点为了可扩展性原因异步地彼此通信。在一个 Swarm 中可以有数百甚至数千个工作节点。现在，我们已经对 Docker Swarm 是什么有了高层次的概述，让我们更详细地描述
    Docker Swarm 的所有单个元素。
- en: Swarm nodes
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm 节点
- en: A Swarm is a collection of nodes. We can classify a node as a physical computer or **Virtual
    Machine** (**VM**). Physical computers these days are often referred to as *bare
    metal*. People say *we're running on bare metal* to distinguish from running on
    a VM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Swarm 是一组节点。我们可以将一个节点分类为物理计算机或虚拟机（**VM**）。如今，物理计算机通常被称为裸机。人们说“我们在裸机上运行”以区分于在虚拟机上运行。
- en: 'When we install Docker on such a node, we call this node a Docker host. The
    following diagram illustrates a bit better what a node and what a Docker host
    is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这样的节点上安装 Docker 时，我们称此节点为 Docker 主机。以下图示更清楚地说明了节点和 Docker 主机的区别：
- en: '![](img/45b82a12-5b32-4290-8970-944fe94532eb.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45b82a12-5b32-4290-8970-944fe94532eb.png)'
- en: Bare metal and VM types of Docker Swarm nodes
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 节点的裸机和虚拟机类型
- en: To become a member of a Docker Swarm, a node must be a Docker host. A node in
    a Docker Swarm can have one of two roles. It can be a manager or it can be a worker.
    Manager nodes do what their name implies; they manage the Swarm. The worker nodes,
    in turn, execute the application workload.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为 Docker Swarm 的成员，一个节点必须是 Docker 主机。Docker Swarm 中的节点可以担任两种角色之一。它可以是管理节点，或者可以是工作节点。管理节点如其名，管理
    Swarm。工作节点则执行应用程序工作负载。
- en: Technically, a manager node can also be a worker node and hence run application
    workload—although that is not recommended, especially if the Swarm is a production
    system running mission-critical applications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，管理节点也可以是工作节点，因此可以运行应用工作负载，尽管这不被推荐，特别是如果 Swarm 是运行关键应用程序的生产系统。
- en: Swarm managers
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm 管理员
- en: Each Docker Swarm needs to include at least one manager node. For high availability
    reasons, we should have more than one manager node in a Swarm. This is especially
    true for production or production-like environments. If we have more than one
    manager node, then these nodes work together using the Raft consensus protocol.
    The Raft consensus protocol is a standard protocol that is often used when multiple
    entities need to work together and always need to agree with each other as to
    which activity to execute next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Docker Swarm 至少需要包括一个管理节点。出于高可用性的考虑，我们在 Swarm 中应该有多个管理节点。对于生产或类似生产环境尤其如此。如果有多个管理节点，则这些节点使用
    Raft 共识协议协同工作。Raft 共识协议是一种标准协议，当多个实体需要一起工作并始终需要达成一致时，经常使用这种协议来决定下一步执行的活动。
- en: To work well, the Raft consensus protocol asks for an odd number of members in
    what is called the consensus group. Hence, we should always have 1, 3, 5, 7, and
    so on manager nodes. In such a consensus group, there is always a leader. In the
    case of Docker Swarm, the first node that starts the Swarm initially becomes the
    leader. If the leader goes away then the remaining manager nodes elect a new leader.
    The other nodes in the consensus group are called followers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了良好运作，Raft 共识协议要求在所谓的共识组中有奇数个成员。因此，我们应该始终有 1、3、5、7 等管理节点。在这样一个共识组中，总会有一个领导者。在
    Docker Swarm 的情况下，首个启动 Swarm 的节点最初成为领导者。如果领导者离开，剩余的管理节点将选举新的领导者。共识组中的其他节点称为跟随者。
- en: Now, let's assume that we shut down the current leader node for maintenance
    reasons. The remaining manager nodes will elect a new leader. When the previous
    leader node comes back online, it will now become a follower. The new leader remains
    the leader.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设由于维护原因关闭当前的领导者节点。剩余的管理节点将选举新的领导者。当先前的领导节点重新上线时，它现在将成为跟随者。新的领导者仍然保持领导地位。
- en: All of the members of the consensus group communicate synchronously with each
    other. Whenever the consensus group needs to make a decision, the leader asks
    all followers for agreement. If a majority of the manager nodes give a positive
    answer, then the leader executes the task. That means if we have three manager
    nodes, then at least one of the followers has to agree with the leader. If we
    have five manager nodes, then at least two followers have to agree.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 共识组的所有成员都同步地相互通信。每当共识组需要做出决策时，领导节点会要求所有跟随节点达成一致。如果大多数管理节点给出肯定的答案，那么领导节点就会执行任务。这意味着，如果我们有三个管理节点，那么至少有一个跟随节点必须同意领导节点的决策。如果我们有五个管理节点，那么至少有两个跟随节点必须同意。
- en: Since all manager follower nodes have to communicate synchronously with the
    leader node to make a decision in the cluster, the decision-making process gets
    slower and slower the more manager nodes we have forming the consensus group.
    The recommendation of Docker is to use one manager for development, demo, or test
    environments. Use three managers nodes in small to medium size Swarms, and use
    five managers in large to extra large Swarms. To use more than five managers in
    a Swarm is hardly ever justified.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有的管理节点跟随节点必须与领导节点同步通信以做出集群决策，因此随着管理节点数量的增加，决策过程会变得越来越慢。Docker的推荐做法是在开发、演示或测试环境中使用一个管理节点。在小型到中型的Swarm中使用三个管理节点，在大型到超大规模的Swarm中使用五个管理节点。使用超过五个管理节点的Swarm几乎从未被证明是合理的。
- en: 'The manager nodes are not only responsible for managing the Swarm but also
    for maintaining the state of the Swarm. *What do we mean by that?* When we talk
    about the state of the Swarm we mean all of the information about it—for example, *how
    many nodes are in the Swarm and* *what are the properties of each node, such as
    name or IP address*. We also mean what containers are running on which node in
    the Swarm and more. What, on the other hand, is not included in the state of the
    Swarm is data produced by the application services running in containers on the
    Swarm. This is called application data and is definitely not part of the state
    that is managed by the manager nodes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 管理节点不仅负责管理Swarm，还负责维护Swarm的状态。*那我们到底是什么意思呢？* 当我们谈论Swarm的状态时，我们指的是有关它的所有信息——例如，*Swarm中有多少个节点，以及*
    *每个节点的属性是什么，比如名称或IP地址*。我们还指的是哪些容器正在Swarm中的哪些节点上运行，等等。另一方面，Swarm的状态中不包括由运行在Swarm容器中的应用服务生成的数据。这被称为应用数据，并且绝对不属于由管理节点管理的状态：
- en: '![](img/cf1b7c95-25a4-43a4-aa20-168b1b938059.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf1b7c95-25a4-43a4-aa20-168b1b938059.png)'
- en: A Swarm manager consensus group
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Swarm管理节点共识组
- en: All of the Swarm states are stored in a high-performance key-value store (**kv-store**)
    on each **manager** node. That's right, each **manager** node stores a complete replica
    of the whole Swarm state. This redundancy makes the Swarm highly available. If
    a **manager** node goes down, the remaining **managers** all have the complete
    state at hand.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Swarm的状态都存储在每个**管理节点**上的高性能键值存储（**kv-store**）中。没错，每个**管理节点**都存储整个Swarm状态的完整副本。这种冗余使得Swarm具有高度可用性。如果一个**管理节点**宕机，剩余的**管理节点**都能随时访问完整的状态。
- en: If a new **manager** joins the consensus group, then it synchronizes the Swarm
    state with the existing members of the group until it has a complete replica.
    This replication is usually pretty fast in typical Swarms but can take a while
    if the Swarm is big and many applications are running on it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个新的**管理节点**加入共识组，那么它会将Swarm状态与现有的组成员同步，直到它拥有完整的副本。在典型的Swarm中，这种复制通常非常快速，但如果Swarm很大，并且上面运行着许多应用程序，这可能会需要一些时间。
- en: Swarm workers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm工作节点
- en: As we mentioned earlier, a Swarm worker node is meant to host and run containers that
    contain the actual application services we're interested in running on our cluster.
    They are the workhorses of the Swarm. In theory, a manager node can also be a
    worker. But, as we already said, this is not recommended on a production system.
    On a production system, we should let managers be managers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Swarm工作节点是用来托管和运行包含实际应用服务的容器的，这些服务是我们希望在集群中运行的。它们是Swarm的主力军。从理论上讲，一个管理节点也可以是工作节点。但是，正如我们已经说过的那样，这在生产系统中并不推荐。在生产系统中，我们应该让管理节点保持管理节点的角色。
- en: Worker nodes communicate with each other over the so-called control plane. They
    use the gossip protocol for their communication. This communication is asynchronous,
    which means that, at any given time, it is likely that not all worker nodes are
    in perfect sync.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点通过所谓的控制平面进行通信。它们使用 gossip 协议进行通信，这种通信是异步的，这意味着在任何给定的时刻，不一定所有工作节点都能完美同步。
- en: 'Now, you might ask—*what information do worker nodes exchange?* It is mostly
    information that is needed for service discovery and routing, that is, information
    about which containers are running on with nodes and more:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能会问——*工作节点交换哪些信息？* 这些信息大多数是用于服务发现和路由的信息，也就是说，关于哪些容器在节点上运行等信息：
- en: '![](img/8049bdd7-03d8-405a-9ed4-e48e4d2216b4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8049bdd7-03d8-405a-9ed4-e48e4d2216b4.png)'
- en: Worker nodes communicating with each other
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点之间的通信
- en: In the preceding diagram, you can see how workers communicate with each other.
    To make sure the gossiping scales well in a large Swarm, each **worker** node
    only synchronizes its own state with three random neighbors. For those who are
    familiar with Big O notation, that means that the synchronization of the **worker**
    nodes using the gossip protocol scales with O(0).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，您可以看到工作节点是如何相互通信的。为了确保在大规模 Swarm 中，gossip 协议能够良好扩展，每个**工作**节点只与三个随机邻居同步其自身状态。对于熟悉大
    O 表示法的人来说，这意味着使用 gossip 协议同步**工作**节点的过程具有 O(0) 级别的扩展性。
- en: '**Worker** nodes are kind of passive. They never actively do something other
    than run the workloads that they get assigned by the manager nodes. The **worker** makes sure,
    though, that it runs these workloads to the best of its capabilities. Later on
    in this chapter, we will get to know more about exactly what workloads the worker
    nodes are assigned by the manager nodes.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作**节点可以说是被动的。它们除了运行由管理节点分配的工作负载外，几乎不会主动执行其他操作。尽管如此，**工作**节点会确保以最佳能力运行这些工作负载。在本章后续部分，我们将进一步了解管理节点分配给工作节点的具体工作负载。'
- en: Stacks, services, and tasks
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆栈、服务和任务
- en: 'When using a Docker Swarm versus a single Docker host, there is a paradigm
    change. Instead of talking of individual containers that run processes, we are
    abstracting away to services that represent a set of replicas of each process,
    and, in this way, become highly available. We also do not speak anymore of individual
    Docker hosts with well-known names and IP addresses to which we deploy containers;
    we''ll now be referring to clusters of hosts to which we deploy services. We don''t
    care about an individual host or node anymore. We don''t give it a meaningful
    name; each node rather becomes a number to us. We also don''t care about individual
    containers and where they are deployed any longer—we just care about having a
    desired state defined through a service. We can try to depict that as shown in
    the following diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 相较于单一 Docker 主机时，会有范式上的变化。我们不再讨论运行进程的单个容器，而是抽象成服务，代表每个进程的副本集，通过这种方式，服务可以实现高可用性。我们也不再讨论带有固定名称和
    IP 地址的单个 Docker 主机了；现在我们谈论的是将服务部署到的主机集群。我们不再关心单一主机或节点，不再给它赋予有意义的名称；每个节点对我们来说只是一个数字。我们也不再关心单个容器以及它们部署的位置，我们只关心通过服务定义的期望状态。我们可以通过以下图示来描述这一点：
- en: '![](img/f1ff1173-269e-4448-a409-8279610fc9be.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1ff1173-269e-4448-a409-8279610fc9be.png)'
- en: Containers are deployed to well-known servers
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 容器部署到知名服务器
- en: 'Instead of deploying individual containers to well-known servers like in the
    preceding diagram, where we deploy the **web** container to the **alpha** server with
    the IP address `52.120.12.1`, and the **payments** container to the **beta** server with
    the IP `52.121.24.33`, we switch to this new paradigm of services and Swarms (or,
    more generally, clusters):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面图示中将**web**容器部署到IP地址为`52.120.12.1`的**alpha**服务器，将**payments**容器部署到IP为`52.121.24.33`的**beta**服务器不同，在新的服务和
    Swarm（或更广泛地说，集群）范式下，我们进行了转变：
- en: '![](img/6dd5f3db-4cb9-4052-9071-a8cb8256586f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6dd5f3db-4cb9-4052-9071-a8cb8256586f.png)'
- en: Services are deployed to Swarms
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 服务部署到 Swarm 集群
- en: 'In the preceding diagram, we see that a **web** service and an **inventory** service are
    both deployed to a **Swarm** that consists of many nodes. Each of the services
    has a certain number of replicas: six for **web** and five for **inventory**.
    We don''t really care on which node the replicas will run; we only care that the
    requested number of replicas is always running on whatever nodes the **Swarm**
    scheduler decides to put them on.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们看到一个**web**服务和一个**inventory**服务都部署到了一个由多个节点组成的**Swarm**中。每个服务都有一定数量的副本：**web**有六个副本，**inventory**有五个副本。我们并不关心副本运行在哪个节点上；我们只关心请求的副本数量总是能在**Swarm**调度器决定的任何节点上运行。
- en: Services
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务
- en: 'A Swarm service is an abstract thing. It is a description of the desired state
    of an application or application service that we want to run in a Swarm. The Swarm
    service is like a manifest describing such things as the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm服务是一个抽象的概念。它是我们希望在Swarm中运行的应用或应用服务的期望状态的描述。Swarm服务就像一个清单，描述了以下内容：
- en: Name of the service
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的名称
- en: Image from which to create the containers
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于创建容器的镜像
- en: Number of replicas to run
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要运行的副本数量
- en: Network(s) that the containers of the service are attached to
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务容器所附加的网络
- en: Ports that should be mapped
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该映射的端口
- en: Having this service manifest, the Swarm manager then makes sure that the described
    desired state is always reconciled if ever the actual state should deviate from
    it. So, if for example, one instance of the service crashes, then the scheduler
    on the Swarm manager schedules a new instance of this particular service on a
    node with free resources so that the desired state is reestablished.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个服务清单后，Swarm管理器会确保当实际状态偏离期望状态时，始终将其协调一致。因此，如果例如，某个服务的一个实例崩溃，那么Swarm管理器上的调度器会在一个有空闲资源的节点上调度该服务的新实例，从而重新建立期望的状态。
- en: Task
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: We have learned that a service corresponds to a description of the desired state
    in which an application service should be at all times. Part of that description
    was the number of replicas the service should be running. Each replica is represented
    by a task. In this regard, a Swarm service contains a collection of tasks. On
    Docker Swarm, a task is the atomic unit of deployment. Each task of a service
    is deployed by the Swarm scheduler to a worker node. The task contains all of
    the necessary information that the worker node needs to run a container based
    on the image, which is part of the service description. Between a task and a container,
    there is a one-to-one relation. The container is the instance that runs on the
    worker node, while the task is the description of this container as a part of
    a Swarm service.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，一个服务对应于应用服务应始终处于的期望状态的描述。该描述的一部分是服务应该运行的副本数量。每个副本由一个任务表示。在这方面，Swarm服务包含一组任务。在Docker
    Swarm中，任务是部署的最小单位。每个服务的任务都由Swarm调度器部署到一个工作节点上。任务包含工作节点需要的所有信息，用于根据镜像（这是服务描述的一部分）运行容器。在任务和容器之间，存在一一对应的关系。容器是运行在工作节点上的实例，而任务是作为Swarm服务一部分的容器描述。
- en: Stack
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 栈
- en: Now that we have a good idea about what a Swarm service is and what tasks are,
    we can introduce the stack. A stack is used to describe a collection of Swarm
    services that are related, most probably because they are part of the same application.
    In that sense, we could also say that a stack describes an application that consists
    of one to many services that we want to run on the Swarm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对**Swarm**服务是什么以及任务是什么有了很好的了解，我们可以介绍栈的概念。栈用于描述一组相关的Swarm服务，这些服务通常因为它们是同一个应用程序的一部分而相关。从这个意义上说，我们也可以说栈描述了一个由一个或多个我们希望在Swarm上运行的服务组成的应用程序。
- en: 'Typically, we describe a stack declaratively in a text file that is formatted
    using the YAML format and that uses the same syntax as the already-known Docker
    Compose file. This leads to a situation where people sometimes say that a stack
    is described by a `docker-compose` file. A better wording would be: a stack is
    described in a stack file that uses similar syntax to a `docker-compose` file.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在一个文本文件中以声明性方式描述一个栈，该文件使用YAML格式并且采用与已知的Docker Compose文件相同的语法。由此产生了一种情况，人们有时会说栈是由一个`docker-compose`文件描述的。更准确的说法应该是：栈是在一个栈文件中描述的，该文件使用与`docker-compose`文件类似的语法。
- en: 'Let''s try to illustrate the relationship between the stack, services, and
    tasks in the following diagram and connect it with the typical content of a stack
    file:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过下面的图示来说明Stack、服务和任务之间的关系，并将其与典型的Stack文件内容连接起来：
- en: '![](img/34c40ca5-56fa-4ce6-a61c-aa6d5ded8230.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34c40ca5-56fa-4ce6-a61c-aa6d5ded8230.png)'
- en: Diagram showing the relationship between stack, services, and tasks
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 显示Stack、服务和任务之间关系的图表
- en: In the preceding diagram, we see on the right-hand side a declarative description
    of a sample **Stack**. The **Stack** consists of three services called **web**, **payments**,
    and **inventory**. We also see that the **web** service uses the **example/web:1.0** image and has
    four replicas.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，右侧展示了一个示例**Stack**的声明式描述。**Stack**由三个服务组成，分别是**web**、**payments**和**inventory**。我们还可以看到，**web**服务使用的是**example/web:1.0**镜像，并且有四个副本。
- en: On the left-hand side of the diagram, we see that the **Stack** embraces the
    three services mentioned. Each service, in turn, contains a collection of **Tasks**,
    as many as there are replicas. In the case of the **web** service, we have a collection
    of four **Tasks**. Each **Task** contains the name of the **Image** from which
    it will instantiate a container once the **Task** is scheduled on a Swarm node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的左侧，我们可以看到**Stack**包含了三个提到的服务。每个服务依次包含了一组**Tasks**，数量与副本数相同。以**web**服务为例，我们有四个**Tasks**。每个**Task**包含一个**Image**的名称，容器将在该**Task**被调度到Swarm节点时从中实例化。
- en: Multi-host networking
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多主机网络
- en: 'In [Chapter 10](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml), *Single-Host Networking,* we
    discussed how containers communicate on a single Docker host. Now, we have a Swarm
    that consists of a cluster of nodes or Docker hosts. Containers that are located
    on different nodes need to be able to communicate with each other. Many techniques
    can help us to achieve this goal. Docker has chosen to implement an **overlay
    network** driver for Docker Swarm. This **overlay network** allows containers
    attached to the same **overlay network** to discover each other and freely communicate
    with each other. The following is a schema for how an **overlay network** works:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml)，*单主机网络*中，我们讨论了容器如何在单个Docker主机上进行通信。现在，我们有一个由多个节点或Docker主机组成的Swarm。位于不同节点上的容器需要能够互相通信。有许多技术可以帮助我们实现这一目标。Docker选择为Docker
    Swarm实现一个**覆盖网络**驱动程序。这个**覆盖网络**使得附加到同一**覆盖网络**的容器可以相互发现并自由通信。以下是**覆盖网络**工作原理的示意图：
- en: '![](img/1731fa28-e8f4-459e-b4c2-40e63b876032.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1731fa28-e8f4-459e-b4c2-40e63b876032.png)'
- en: Overlay network
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: We have two nodes or Docker hosts with the IP addresses `172.10.0.15` and `172.10.0.16`.
    The values we have chosen for the IP addresses are not important; what is important
    is that both hosts have a distinct IP address and are connected by a physical
    network (a network cable), which is called the **underlay network**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个节点或Docker主机，IP地址分别为`172.10.0.15`和`172.10.0.16`。我们选择的IP地址值并不重要；重要的是这两个主机有不同的IP地址，并通过物理网络（网络电缆）连接在一起，这个网络被称为**underlay
    network**。
- en: On the node on the left-hand side we have a container running with the IP address `10.3.0.2`, and
    on the node on the right-hand side another container with the IP address `10.3.0.5`.
    Now, the former container wants to communicate with the latter. *How can this
    happen?* In [Chapter 10](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml), *Single-Host
    Networking,* we saw how this works when both containers are located on the same
    node—by using a Linux bridge. But Linux bridges only operate locally and cannot
    span across nodes. So, we need another mechanism. Linux VXLAN comes to the rescue.
    VXLAN has been available on Linux since way before containers were a thing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧节点上，我们有一个容器，IP地址为`10.3.0.2`，在右侧节点上，则有另一个容器，IP地址为`10.3.0.5`。现在，前一个容器想要与后一个容器通信。*这怎么实现呢？*
    在[第10章](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml)，*单主机网络*中，我们已经看到了当两个容器位于同一节点时，如何通过Linux桥接实现通信。但Linux桥接仅限于本地操作，不能跨节点工作。所以，我们需要另一个机制，Linux
    VXLAN来解决这个问题。VXLAN在Linux中早在容器出现之前就已经可用了。
- en: When the left-hand container sends a data packet, the **bridge** realizes that
    the target of the packet is not on this host. Now, each node participating in
    an overlay network gets a so-called **VXLAN Tunnel Endpoint** (**VTEP**) object,
    which intercepts the packet (the packet at that moment is an OSI layer 2 data
    packet), wraps it with a header containing the target IP address of the host that
    runs the destination container (this makes it now an OSI layer 3 data packet),
    and sends it over the **VXLAN tunnel**. The **VTEP** on the other side of the
    tunnel unpacks the data packet and forwards it to the local bridge, which in turn
    forwards it to the destination container.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当左侧容器发送数据包时，**bridge**会意识到数据包的目标不在此主机上。现在，每个参与覆盖网络的节点都获得一个所谓的**VXLAN隧道端点**（**VTEP**）对象，它拦截数据包（此时数据包是OSI第2层数据包），并将其包装在一个包含目标容器所在主机IP地址的头信息中（这使它成为OSI第3层数据包），然后通过**VXLAN隧道**发送。隧道另一侧的**VTEP**会解包数据包并将其转发到本地桥接器，桥接器再将其转发给目标容器。
- en: The overlay driver is included in SwarmKit and is in most cases the recommended
    network driver for Docker Swarm. There are other multi-node-capable network drivers
    available from third parties that can be installed as plugins to each participating
    Docker host. Certified network plugins are available from the Docker store.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖驱动程序包含在SwarmKit中，在大多数情况下，它是Docker Swarm推荐的网络驱动程序。还有其他由第三方提供的支持多节点的网络驱动程序，可以作为插件安装到每个参与的Docker主机上。经过认证的网络插件可以从Docker商店获取。
- en: Creating a Docker Swarm
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个Docker Swarm
- en: Creating a Docker Swarm is almost trivial. It is so easy that it seems unreal
    if you know what an orchestrator is all about. But it is true, Docker has done
    a fantastic job in making Swarms simple and elegant to use. At the same time,
    Docker Swarm has been proven in use by large enterprises to be very robust and
    scalable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Docker Swarm几乎是微不足道的。它非常简单，以至于如果你知道什么是编排工具，似乎不真实。但这确实是真的，Docker做得非常出色，使得Swarm的使用既简单又优雅。与此同时，Docker
    Swarm已经在大型企业的使用中证明了它的稳健性和可扩展性。
- en: Creating a local single node swarm
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建本地单节点Swarm
- en: So, enough imagining — let's demonstrate how we can create a Swarm. In its most
    simple form, a fully functioning Docker Swarm consists only of a single node.
    If you're using Docker for Mac or Windows, or even if you're using Docker Toolbox,
    then your personal computer or laptop is such a node. Hence, we can start right
    there and demonstrate some of the most important features of a Swarm.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，不再需要想象——让我们展示如何创建一个Swarm。在最简单的形式下，一个完全功能的Docker Swarm仅由一个节点组成。如果你使用的是Docker
    for Mac或Windows，甚至是Docker Toolbox，那么你的个人电脑或笔记本电脑就是这样的一个节点。因此，我们可以从这里开始，并展示Swarm的一些最重要的功能。
- en: 'Let''s initialize a Swarm. On the command line, just enter the following command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一个Swarm。在命令行中，只需输入以下命令：
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And after an incredibly short time, you should see something like the following
    screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在短短的时间后，你应该能看到类似于以下截图的内容：
- en: '![](img/39cd7eb9-5916-422b-a23f-a158857f0401.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39cd7eb9-5916-422b-a23f-a158857f0401.png)'
- en: Output of the Docker Swarm init command
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm初始化命令的输出
- en: 'Our computer is now a Swarm node. Its role is that of a manager and it is the
    leader (of the managers, which makes sense since there is only one manager at
    this time). Although it took only a very short time to finish `docker swarm init`,
    the command did a lot of things during that time. Some of them are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的计算机是一个Swarm节点。它的角色是管理者，并且是领导者（在管理者中是领导者，这很合理，因为目前只有一个管理者）。虽然执行`docker swarm
    init`只用了非常短的时间，但这个命令在这段时间内做了很多事情。以下是其中的一些：
- en: It created a root **Certificate Authority** (**CA**).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个根**证书授权机构**（**CA**）。
- en: It created a key-value store that is used to store the state of the whole Swarm.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个键值存储，用于存储整个Swarm的状态。
- en: 'Now, in the preceding output, we can see a command that can be used to join
    other nodes to the Swarm that we just created. The command is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在前面的输出中，我们可以看到一个可以用来将其他节点加入我们刚创建的Swarm的命令。该命令如下：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we have the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '`<join-token>` is a token generated by the Swarm leader at the time the Swarm
    was initialized.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<join-token>` 是Swarm领导者在初始化Swarm时生成的令牌。'
- en: '`<IP address>` is the IP address of the leader.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<IP地址>` 是领导者的IP地址。'
- en: 'Although our cluster remains simple, as it consists of only one member, we
    can still ask the Docker CLI to list all of the nodes of the Swarm. This will
    look similar to the following screenshot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的集群保持简单，因为它只有一个成员，但我们仍然可以要求Docker CLI列出Swarm中的所有节点。这看起来类似于以下截图：
- en: '![](img/4519a489-42e8-42d8-9ef0-8b791ce51d14.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4519a489-42e8-42d8-9ef0-8b791ce51d14.png)'
- en: Listing the nodes of the Docker Swarm
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列出Docker Swarm中的节点
- en: In this output, we first see `ID` that was given to the node. The star (`*`)
    that follows `ID` indicates that this is the node on which `docker node ls` was
    executed—basically saying that this is the active node. Then, we have the (human-readable)
    name of the node, its status, availability, and manager status. As mentioned earlier,
    this very first node of the Swarm automatically became the leader, which is indicated
    in the preceding screenshot. Lastly, we see which version of the Docker engine
    we're using.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，我们首先看到分配给节点的`ID`。紧随其后的星号（`*`）表示这是执行`docker node ls`命令的节点——基本上说明这是活动节点。接下来，我们可以看到节点的（人类可读的）名称、状态、可用性和管理状态。如前所述，Swarm的第一个节点自动成为领导者，在前面的截图中有标示。最后，我们看到正在使用的Docker引擎版本。
- en: 'To get even more information about a node, we can use the `docker node inspect` command,
    as shown in the following screenshot:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取更多关于节点的信息，我们可以使用`docker node inspect`命令，如下图所示：
- en: '![](img/c9182c97-d23d-4bdc-a365-38f1897f4b63.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9182c97-d23d-4bdc-a365-38f1897f4b63.png)'
- en: Truncated output of the docker node inspect command
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: docker node inspect命令的截断输出
- en: There is a lot of information generated by this command, so we only present
    a truncated version of the output. This output can be useful, for example, when
    you need to troubleshoot a misbehaving cluster node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令生成了大量信息，因此我们只展示了截断后的输出。例如，当你需要排查故障节点时，这些输出信息可能会很有用。
- en: Creating a local Swarm in VirtualBox or Hyper-V
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在VirtualBox或Hyper-V中创建本地Swarm
- en: Sometimes, a single node Swarm is not enough, but we don't have or don't want to
    use an account to create a Swarm in the cloud. In this case, we can create a local
    Swarm in either VirtualBox or Hyper-V. Creating the Swarm in VirtualBox is slightly
    easier than creating it in Hyper-V, but if you're using Windows 10 and have Docker
    for Windows running, then you cannot use VirtualBox at the same time. The two
    hypervisors are mutually exclusive.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，一个单节点的Swarm是不够的，但我们没有或不想使用账户在云中创建Swarm。在这种情况下，我们可以在VirtualBox或Hyper-V中创建本地Swarm。在VirtualBox中创建Swarm稍微简单一些，但如果你使用的是Windows
    10并且运行Docker for Windows，那么你不能同时使用VirtualBox。这两个虚拟机管理程序是互斥的。
- en: 'Let''s assume we have VirtualBox and `docker-machine` installed on our laptop.
    We can then use `docker-machine` to list all Docker hosts that are currently defined
    and may be running in VirtualBox:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在笔记本电脑上安装了VirtualBox和`docker-machine`。然后，我们可以使用`docker-machine`列出所有当前已定义且可能在VirtualBox中运行的Docker主机：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In my case, I have one VM called `default` defined, which is currently stopped.
    I can easily start the VM by issuing the `docker-machine start default` command.
    This command takes a while and will result in the following (shortened) output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我已经定义了一个名为`default`的虚拟机，它当前处于停止状态。我可以通过执行`docker-machine start default`命令轻松启动虚拟机。这个命令需要一些时间，并将产生以下（简化）输出：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, if I list my VMs again, I should see the following screenshot:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我再次列出我的虚拟机，我应该能看到以下截图：
- en: '![](img/e64986fa-4614-4968-9d15-b62e7e873916.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e64986fa-4614-4968-9d15-b62e7e873916.png)'
- en: List of all VMs running in Hyper-V
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Hyper-V中运行的所有虚拟机列表
- en: 'If we do not have a VM called `default` yet, we can easily create one using
    the `create` command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还没有名为`default`的虚拟机，可以使用`create`命令轻松创建一个：
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/9a6258ef-8613-41e5-95cb-b86bfe858b8e.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a6258ef-8613-41e5-95cb-b86bfe858b8e.png)'
- en: Output of docker-machine create
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: docker-machine create的输出
- en: We can see in the preceding output how `docker-machine`creates the VM from an
    ISO image, defines SSH keys and certificates, and copies them to the VM and to
    the local `~/.docker/machine` directory, where we will use it later when we want
    to remotely access this VM through the Docker CLI. It also provisions an IP address
    for the new VM.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从上述输出中看到，`docker-machine`是如何从ISO镜像创建虚拟机，定义SSH密钥和证书，并将其复制到虚拟机和本地`~/.docker/machine`目录中，稍后我们将使用它，当我们想通过Docker
    CLI远程访问这个虚拟机时。同时，它还为新虚拟机配置了一个IP地址。
- en: We're using the `docker-machine create` command with the `--driver virtualbox` parameter.
    The docker-machine can also work with other drivers such as Hyper-V, AWS, Azure,
    DigitalOcean, and many more. Please see the documentation of `docker-machine`
    for more information. By default, a new VM gets 1 GB of memory associated, which
    is enough to use this VM as a node for a development or test Swarm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`docker-machine create`命令，并带有`--driver virtualbox`参数。docker-machine也可以与其他驱动程序一起使用，比如Hyper-V、AWS、Azure、DigitalOcean等。有关更多信息，请参阅`docker-machine`的文档。默认情况下，新虚拟机将分配1
    GB的内存，这足够将此虚拟机用作开发或测试Swarm的节点。
- en: If you're on Windows 10 with Docker for Desktop, use the `hyperv` driver instead.
    To be successful though, you need to run as Administrator. Furthermore, you need
    to have an external virtual switch defined on Hyper-V first. You can use the Hyper-V
    Manager to do so. The output of the command will look very similar to the one
    for the `virtualbox` driver.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Windows 10并且安装了Docker Desktop，可以改用`hyperv`驱动程序。为了成功运行，你需要以管理员身份运行。此外，你还需要先在Hyper-V中定义一个外部虚拟交换机。你可以使用Hyper-V管理器来完成这个操作。命令的输出将与`virtualbox`驱动程序的输出非常相似。
- en: 'Now, let''s create five VMs for a five-node Swarm. We can use a bit of scripting
    to reduce the manual work:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为五节点Swarm创建五个虚拟机。我们可以使用一些脚本来减少手动操作：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `docker-machine` will now create five VMs with the names `node-1` to `node-5`.
    This might take a few moments, so this is a good time to get yourself a hot cup
    of tea. After the VMs are created, we can list them:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine`现在将创建五个虚拟机，命名为`node-1`到`node-5`。这可能需要几分钟时间，所以这是一个很好的机会去泡一杯热茶。在虚拟机创建完成后，我们可以列出它们：'
- en: '![](img/2820423c-f815-4247-a15e-251dd7791c3d.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2820423c-f815-4247-a15e-251dd7791c3d.png)'
- en: List of all the VMs we need for the Swarm
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的所有虚拟机列表
- en: 'Now, we''re ready to build a Swarm. Technically, we could SSH into the first
    VM `node-1` and initialize a Swarm and then SSH into all the other VMs and join
    them to the Swarm leader. But this is not efficient. Let''s again use a script
    that does all of the hard work:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好构建Swarm了。从技术上讲，我们可以SSH进入第一个虚拟机`node-1`并初始化Swarm，然后再SSH进入所有其他虚拟机，将它们加入Swarm领导节点。但这样效率不高。让我们再次使用一个脚本来完成所有繁重的工作：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have the join token and the IP address of the Swarm leader, we
    can ask the other nodes to join the Swarm as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了Swarm领导者的加入令牌和IP地址，我们可以让其他节点加入Swarm，如下所示：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To make the Swarm highly available, we can now promote, for example, `node-2` and `node-3 `to
    become managers:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Swarm具备高可用性，我们现在可以将`node-2`和`node-3`等提升为管理节点：
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can list all of the nodes of the Swarm:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以列出Swarm的所有节点：
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We should see the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该会看到如下内容：
- en: '![](img/dfb95f3f-4cbd-4275-b620-f5a6b5493e88.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfb95f3f-4cbd-4275-b620-f5a6b5493e88.png)'
- en: List of all of the nodes of the Docker Swarm on VirtualBox
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: VirtualBox上Docker Swarm的所有节点列表
- en: 'This is proof that we have just created a highly available Docker Swarm locally
    on our laptop or workstation. Let''s pull all of our code snippets together and
    make the whole thing a bit more robust. The script will look as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明我们刚刚在本地笔记本电脑或工作站上创建了一个高可用的Docker Swarm。现在让我们将所有代码片段组合起来，使整个过程更加稳健。脚本将如下所示：
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding script first deletes (if present) and then recreates five VMs
    called `node-1` to `node-5`, and then initializes a Swarm on `node-1`. After that,
    the remaining four VMs are added to the Swarm, and finally, `node-2` and `node-3` are
    promoted to manager status to make the Swarm highly available. The whole script
    will take less than 5 minutes to execute and can be repeated as many times as
    desired. The complete script can be found in the repository, in the `docker-swarm` subfolder;
    it is called `create-swarm.sh`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本首先删除（如果存在）并重新创建五个名为`node-1`到`node-5`的虚拟机，然后在`node-1`上初始化Swarm。之后，剩下的四个虚拟机将加入Swarm，最后，`node-2`和`node-3`被提升为管理节点，以使Swarm具备高可用性。整个脚本执行时间不到5分钟，并且可以根据需要重复运行。完整脚本可以在仓库中的`docker-swarm`子文件夹里找到，名为`create-swarm.sh`。
- en: It is a highly recommended best practice to always script and hence automate
    operations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈推荐的一种最佳实践是始终编写脚本，从而实现操作的自动化。
- en: Using Play with Docker to generate a Swarm
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Play with Docker生成Swarm
- en: To experiment with Docker Swarm without having to install or configure anything locally
    on our computer, we can use **Play with Docker** (**PWD**). PWD is a website that
    can be accessed with a browser and that offers us the ability to create a Docker
    Swarm consisting of up to five nodes. It is definitely a playground, as the name
    implies, and the time for which we can use it is limited to four hours per session.
    We can open as many sessions as we want, but each session automatically ends after
    four hours. Other than that, it is a fully functional Docker environment that
    is ideal for tinkering with Docker or to demonstrate some features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了无需在本地计算机上安装或配置任何东西即可实验Docker Swarm，我们可以使用**Play with Docker**（**PWD**）。PWD是一个可以通过浏览器访问的网站，它让我们能够创建一个由最多五个节点组成的Docker
    Swarm。正如名字所暗示的，它确实是一个游乐场，我们可以在其中玩耍，而且每次会话的使用时间限制为四小时。我们可以开启任意数量的会话，但每个会话都会在四小时后自动结束。除此之外，它是一个功能齐全的Docker环境，适合用来摆弄Docker或演示某些功能。
- en: 'Let''s access the site now. In your browser, navigate to the website [https://labs.play-with-docker.com](https://labs.play-with-docker.com).
    You will be presented with a welcome and login screen. Use your Docker ID to log
    in. After successfully going so, you will be presented with a screen that looks
    like the following screenshot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们访问该站点。在浏览器中，进入网站[https://labs.play-with-docker.com](https://labs.play-with-docker.com)。你将看到一个欢迎和登录界面。使用你的Docker
    ID登录。成功登录后，你将看到如下截图所示的界面：
- en: '![](img/3172277e-9a56-44e6-8651-cfe1d23cf525.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3172277e-9a56-44e6-8651-cfe1d23cf525.png)'
- en: Play with Docker window
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Docker操作窗口
- en: 'As we can see immediately, there is a big timer counting down from four hours.
    That''s how much time we have left to play in this session. Furthermore, we see
    a + ADD NEW INSTANCE link. Click it to create a new Docker host. When you do that,
    your screen should look like the following screenshot:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们立刻看到的那样，有一个大计时器从四小时开始倒计时。这是我们在本次会话中剩余的时间。此外，我们看到一个+ ADD NEW INSTANCE链接。点击它可以创建一个新的Docker主机。完成后，你的屏幕应该看起来像下图：
- en: '![](img/07316581-3443-4328-ba3b-5c57af1e85c8.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07316581-3443-4328-ba3b-5c57af1e85c8.png)'
- en: PWD with one new node
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 带有一个新节点的PWD
- en: On the left-hand side, we see the newly created node with its IP address (`192.168.0.48`)
    and its name (`node1`). On the right-hand side, we have some additional information
    about this new node in the upper half of the screen and a Terminal in the lower
    half. Yes, this Terminal is used to execute commands on this node that we just
    created. This node has the Docker CLI installed, and hence we can execute all
    of the familiar Docker commands on it such as `docker version`. Try it out.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们可以看到新创建的节点，它的IP地址是(`192.168.0.48`)，节点名称是(`node1`)。右侧显示了有关这个新节点的一些附加信息，屏幕的上半部分是这些信息，下半部分是一个终端。是的，这个终端用于在我们刚创建的节点上执行命令。这个节点已经安装了Docker
    CLI，因此我们可以在其上执行所有熟悉的Docker命令，例如`docker version`。试试看吧。
- en: 'But now we want to create a Docker Swarm. Execute the following command in
    the Terminal in your browser:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们想要创建一个Docker Swarm。请在浏览器的终端中执行以下命令：
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output generated by the preceding command corresponds to what we already
    know from our previous trials with the one-node cluster on our workstation and
    the local cluster using VirtualBox or Hyper-V. The important information, once
    again, is the `join` command that we want to use to join additional nodes to the
    cluster we just created.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令生成的输出与我们之前在工作站上使用单节点集群以及在使用VirtualBox或Hyper-V的本地集群中进行的实验所得到的结果相同。重要的信息再次是我们要使用的`join`命令，用来将额外的节点加入到我们刚刚创建的集群中。
- en: You might have noted that this time we specified the `--advertise-addr` parameter in
    the Swarm `init` command. *Why is that necessary here?* The reason is that the
    nodes generated by PWD have more than one IP address associated with them. We
    can easily verify that by executing the `ip a` command on the node. This command
    will show us that there are indeed two endpoints, `eth0` and `eth1`, present.
    We hence have to specify explicitly to the new to-be swarm manager which one we
    want to use. In our case, it is `eth0`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，这次我们在Swarm的`init`命令中指定了`--advertise-addr`参数。*为什么这里需要这样做？*原因是，PWD生成的节点有多个IP地址与之关联。我们可以通过在节点上执行`ip
    a`命令轻松验证这一点。该命令会显示出确实存在`eth0`和`eth1`两个端点。因此，我们必须明确指定新Swarm管理节点要使用哪一个。在我们的例子中，是`eth0`。
- en: Create four additional nodes in PWD by clicking four times on the + ADD NEW
    INSTANCE link. The new nodes will be called `node2`, `node3`, `node4`, and `node5` and
    will all be listed on the left-hand side. If you click on one of the nodes on
    the left-hand side, then the right-hand side shows the details of the respective
    node and a Terminal window for that node.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击`+ ADD NEW INSTANCE`链接四次，在PWD中创建另外四个节点。新的节点将被命名为`node2`、`node3`、`node4`和`node5`，并会在左侧列出。如果你点击左侧的一个节点，右侧将显示该节点的详细信息以及该节点的终端窗口。
- en: 'Select each node (2 to 5) and execute the `docker swarm join` command that you
    have copied from the leader node (`node1`) in the respective Terminal:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 选择每个节点（2到5），并在相应的终端中执行从领导节点（`node1`）复制的`docker swarm join`命令：
- en: '![](img/22724adb-5938-4487-a1f2-9d491800bb51.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22724adb-5938-4487-a1f2-9d491800bb51.png)'
- en: Joining a node to the Swarm in PWD
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将节点加入PWD中的Swarm
- en: 'Once you have joined all four nodes to the Swarm, switch back to `node1` and
    list all nodes, which, unsurprisingly, results in this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将所有四个节点加入Swarm，切换回`node1`并列出所有节点，结果不出所料如下所示：
- en: '![](img/e270d3d8-8783-4eec-9ba2-2a66f9a43a08.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e270d3d8-8783-4eec-9ba2-2a66f9a43a08.png)'
- en: List of all of the nodes of the swarm in PWD
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在PWD中列出Swarm的所有节点
- en: 'Still on `node1`, we can now promote, say, `node2` and `node3`, to make the
    Swarm highly available:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在`node1`上，我们现在可以提升，例如，将`node2`和`node3`提升，以使Swarm具有高可用性：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this, our Swarm on PWD is ready to accept a workload. We have created a
    highly available Docker Swarm with three manager nodes that form a Raft consensus
    group and two worker nodes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们在PWD上的Swarm已经准备好接受工作负载。我们已经创建了一个高度可用的Docker Swarm，包含三个管理节点，组成一个Raft共识组，以及两个工作节点。
- en: Creating a Docker Swarm in the cloud
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云中创建Docker Swarm
- en: All of the Docker Swarms we have created so far are wonderful to use in development or
    to experiment or to use for demonstration purposes. If we want to create a Swarm
    that can be used as a production environment where we run our mission-critical
    applications, though, then we need to create a—I'm tempted to say—real Swarm in
    the cloud or on premises. In this book, we are going to demonstrate how to create
    a Docker Swarm in AWS.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们创建的所有Docker Swarm都非常适合用于开发、实验或演示。如果我们想要创建一个可以作为生产环境使用的Swarm，在这个环境中我们运行我们的关键任务应用程序，那么我们就需要在云中或本地创建一个——我很想说——真正的Swarm。本书将演示如何在AWS上创建Docker
    Swarm。
- en: 'One way to create a Swarm is by using **d****ocker-machine** (**DM**). DM has
    a driver for AWS. If we have an account on AWS, we need the AWS access key ID
    and the AWS secret access key. We can add those two values to a file called `~/.aws/configuration`.
    It should look like the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Swarm的一种方式是使用**docker-machine**（**DM**）。DM为AWS提供了一个驱动程序。如果我们有AWS账户，我们需要AWS访问密钥ID和AWS秘密访问密钥。我们可以将这两个值添加到名为`~/.aws/configuration`的文件中。它应该如下所示：
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Every time we run `docker-machine create`, DM will look up those values in that
    file. For more in-depth information on how to get an AWS account and how to obtain
    the two secret keys, please consult this link: [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们运行`docker-machine create`时，DM都会在该文件中查找这些值。有关如何获得AWS账户以及如何获取两个密钥的详细信息，请参阅此链接：[http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)。
- en: 'Once we have an AWS account in place and have stored the access keys in the
    configuration file, we can start building our Swarm. The necessary code looks
    exactly the same as the one we used to create a Swarm on our local machine in
    VirtualBox. Let''s start with the first node:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了AWS账户，并且将访问密钥存储在配置文件中，就可以开始构建我们的Swarm。所需的代码与我们在VirtualBox上为本地机器创建Swarm时使用的代码完全相同。让我们从第一个节点开始：
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will create an EC2 instance called `aws-node-1` in the requested region
    (`us-east-1` in my case). The output of the preceding command looks like the following
    screenshot:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会在请求的区域（在我的例子中是`us-east-1`）创建一个名为`aws-node-1`的EC2实例。前面命令的输出如下图所示：
- en: '![](img/ca6013e9-a419-478f-8967-e86ac6defaab.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca6013e9-a419-478f-8967-e86ac6defaab.png)'
- en: Creating a swarm node on AWS with DM
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上使用DM创建一个Swarm节点
- en: 'It looks very similar to the output we already know from working with VirtualBox.
    We can now configure our Terminal for remote access to that EC2 instance:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来与我们在使用VirtualBox时已经熟悉的输出非常相似。现在，我们可以配置我们的终端以便远程访问这个EC2实例：
- en: '[PRE15]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will configure the environment variables used by the Docker CLI accordingly:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将根据Docker CLI配置相应的环境变量：
- en: '![](img/2776b997-08d5-4993-8f68-e047a054969e.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2776b997-08d5-4993-8f68-e047a054969e.png)'
- en: Environment variables used by Docker to enable remote access to the AWS EC2
    node
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 用于启用对 AWS EC2 节点远程访问的环境变量
- en: For security reasons, **Transport Layer Security** (**TLS**) is used for the communication between
    our CLI and the remote node. The certificates necessary for that were copied by
    DM to the path we assigned to the environment variable `DOCKER_CERT_PATH`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 出于安全原因，**传输层安全性**（**TLS**）用于我们的 CLI 和远程节点之间的通信。为此所需的证书已由 DM 复制到我们为环境变量 `DOCKER_CERT_PATH`
    指定的路径中。
- en: 'All Docker commands that we now execute in our Terminal will be remotely executed
    in AWS on our EC2 instance. Let''s try to run Nginx on this node:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在终端中执行的所有 Docker 命令，将会在 AWS 上的 EC2 实例中远程执行。让我们尝试在这个节点上运行 Nginx：
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can use `docker container ls` to verify that the container is running. If
    so, then let''s test it using `curl`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `docker container ls` 来验证容器是否正在运行。如果是的话，那我们可以使用 `curl` 来测试它：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, `<IP address>` is the public IP address of the AWS node; in my case,
    it would be `35.172.240.127`. Sadly, this doesn''t work; the preceding command
    times out:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`<IP 地址>` 是 AWS 节点的公网 IP 地址；在我的例子中是 `35.172.240.127`。可惜，这个方法不起作用；前面的命令超时了：
- en: '![](img/0f6eb2e9-c813-4c67-b0a8-3d4e623caa0d.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f6eb2e9-c813-4c67-b0a8-3d4e623caa0d.png)'
- en: Accessing Nginx on the AWS node times out
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 AWS 节点上的 Nginx 时超时
- en: 'The reason for this is that our node is part of an AWS **Security Group** (**SG**).
    By default, access to objects inside this SG is denied. Hence, we have to find
    out to which SG our instance belongs and configure access explicitly. For this,
    we typically use the AWS console. Go to the EC2 Dashboard and select Instances
    on the left-hand side. Locate the EC2 instance called `aws-node-1` and select
    it. In the details view, under Security groups, click on the docker-machine link, as
    shown in the following screenshot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以如此，是因为我们的节点属于 AWS **安全组**（**SG**）。默认情况下，SG 内部的对象访问是被拒绝的。因此，我们必须找出我们的实例属于哪个
    SG，并显式地配置访问权限。通常，我们会使用 AWS 控制台来完成这项工作。进入 EC2 控制面板，并在左侧选择实例。找到名为 `aws-node-1` 的
    EC2 实例并选择它。在详情视图中，在“安全组”下，点击名为 docker-machine 的链接，如下图所示：
- en: '![](img/daacdfa4-bfb7-4c54-8333-4f42a645ed2d.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daacdfa4-bfb7-4c54-8333-4f42a645ed2d.png)'
- en: Locating the SG to which our Swarm node belongs
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 定位我们的 Swarm 节点所属的 SG
- en: 'This will lead us to the SG page with the `docker-machine` SG pre-selected.
    In the details section under the Inbound tab, add a new rule for your IP address
    (the IP address of workstation):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把我们带到 SG 页面，默认选中 `docker-machine` SG。在详情部分的“入站”选项卡下，为你的 IP 地址（工作站的 IP 地址）添加一条新规则：
- en: '![](img/0d0ee48e-dd73-4b8e-87ee-e5069e77183b.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d0ee48e-dd73-4b8e-87ee-e5069e77183b.png)'
- en: Open access to SG for our computer
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 开放 SG 访问权限给我们的计算机
- en: In the preceding screenshot, the IP address `70.113.114.234` happens to be the
    one assigned to my personal workstation. I have enabled all inbound traffic coming
    from this IP address to the `docker-machine` SG. Note that in a production system
    you should be very careful about which ports of the SG to open to the public.
    Usually, it is ports `80` and `443` for HTTP and HTTPS access. Everything else
    is a potential invitation to hackers.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，IP 地址 `70.113.114.234` 恰好是分配给我个人工作站的 IP 地址。我已经启用了来自这个 IP 地址的所有入站流量到
    `docker-machine` SG。请注意，在生产系统中，你应该非常小心开放哪些 SG 端口给公众访问。通常，只有 `80` 和 `443` 端口用于
    HTTP 和 HTTPS 访问，其他的端口都是潜在的黑客入侵点。
- en: You can get your own IP address through a service such as [https://www.whatismyip.com/](https://www.whatismyip.com/).
    Now, if we execute the `curl` command again, the greeting page of Nginx is returned.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过像 [https://www.whatismyip.com/](https://www.whatismyip.com/) 这样的服务获取你自己的
    IP 地址。现在，如果我们再次执行 `curl` 命令，将会返回 Nginx 的欢迎页面。
- en: 'Before we leave the SG, we should add another rule to it. The Swarm nodes need
    to be able to freely communicate on ports `7946` and `4789` through TCP and UDP
    and on port `2377` through TCP. We could now add five rules with these requirements
    where the source is the SG itself, or we just define a crude rule that allows
    all inbound traffic inside the SG (`sg-c14f4db3` in my case):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开 SG 之前，我们应该向其添加另一条规则。Swarm 节点需要能够通过 TCP 和 UDP 在端口 `7946` 和 `4789` 上自由通信，并通过
    TCP 在端口 `2377` 上通信。我们现在可以添加五条符合这些要求的规则，其中源是 SG 本身，或者我们只定义一条粗略的规则，允许 SG 内部的所有入站流量（在我的情况下是
    `sg-c14f4db3`）：
- en: '![](img/a51e4f76-db2c-4513-8dc2-d878d5e0ba30.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a51e4f76-db2c-4513-8dc2-d878d5e0ba30.png)'
- en: SG rule to enable intra-Swarm communication
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Swarm 内部通信的 SG 规则
- en: 'Now, let''s continue with the creation of the remaining four nodes. Once again,
    we can use a script to ease the process:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续创建其余的四个节点。我们可以再次使用脚本来简化这个过程：
- en: '[PRE18]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After the provisioning of the nodes is done, we can list all nodes with DM.
    In my case, I see this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 节点配置完成后，我们可以通过DM列出所有节点。在我的情况下，我看到的是：
- en: '![](img/9fe33d9f-01b8-4fc8-9bf7-c24f2cfaa1a3.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fe33d9f-01b8-4fc8-9bf7-c24f2cfaa1a3.png)'
- en: List of all the nodes created by DM
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: DM创建的所有节点列表
- en: In the preceding screenshot, we can see the five nodes that we originally created
    in VirtualBox and the five new nodes that we created in AWS. Apparently, the nodes on
    AWS are using a new version of Docker; here, the version is `18.02.0-ce`. The
    IP addresses we see in the `URL` column are the public IP addresses of my EC2
    instances.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的截图中，我们可以看到最初在VirtualBox中创建的五个节点和在AWS上创建的五个新节点。显然，AWS上的节点正在使用新版Docker；此处版本为`18.02.0-ce`。我们在`URL`列中看到的IP地址是我EC2实例的公共IP地址。
- en: 'Because our CLI is still configured for remote access to the `aws-node-1` node,
    we can just run the `swarm init` command as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的CLI仍然配置为远程访问`aws-node-1`节点，我们可以像下面这样运行`swarm init`命令：
- en: '[PRE19]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To get the join token do the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取加入令牌，请执行以下操作：
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To get the IP address of the leader use the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取领导者的IP地址，请使用以下命令：
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With this information, we can now join the other four nodes to the Swarm leader:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们现在可以将其他四个节点加入Swarm领导者：
- en: '[PRE22]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'An alternative way to achieve the same without needing to SSH into the individual
    nodes would be to reconfigure our client CLI every time we want to access a different
    node:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 实现相同目标的另一种方式是，无需SSH进入单个节点，而是每次访问不同的节点时重新配置我们的客户端CLI：
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As a last step, we want to promote nodes `2` and `3` to manager:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，我们希望将节点`2`和`3`提升为管理节点：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can then list all of the Swarm nodes, as shown in the following screenshot:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以列出所有Swarm节点，如下图所示：
- en: '![](img/0038c9cb-fc2f-4a9d-ac15-af1133dbf6b8.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0038c9cb-fc2f-4a9d-ac15-af1133dbf6b8.png)'
- en: List of all nodes of our swarm in the cloud
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们云中Swarm的所有节点列表
- en: 'And hence we have a highly available Docker Swarm running in the cloud. To
    clean up the Swarm in the cloud and avoid incurring unnecessary costs, we can
    use the following command:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在云中运行了一个高可用的Docker Swarm。为了清理云中的Swarm并避免不必要的费用，我们可以使用以下命令：
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Deploying a first application
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署第一个应用
- en: We have created a few Docker Swarms on various platforms. Once created, a Swarm
    behaves the same way on any platform. The way we deploy and update applications
    on a Swarm is not platform-dependent. It has been one of Docker's main goals to
    avoid vendor lock-in when using a Swarm. Swarm-ready applications can be effortlessly
    migrated from, say, a Swarm running on premises to a cloud-based Swarm. It is
    even technically possible to run part of a Swarm on premises and another part
    in the cloud. It works, yet we have, of course, to consider possible side effects
    due to the higher latency between nodes in geographically distant areas.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在不同的平台上创建了一些Docker Swarm。创建之后，Swarm在任何平台上表现相同。我们在Swarm上部署和更新应用的方式不依赖于平台。避免在使用Swarm时被厂商锁定一直是Docker的主要目标之一。准备好Swarm的应用可以轻松地从本地Swarm迁移到基于云的Swarm。甚至在技术上，也可以将部分Swarm部署在本地，另一部分在云中运行。它是可行的，但我们当然需要考虑到地理上距离较远的节点之间可能会因为较高延迟带来的副作用。
- en: 'Now that we have a highly available Docker Swarm up and running, it is time
    to run some workloads on it. I''m using a local Swarm created with docker-machine.
    We''ll start by first creating a single service. For this, we need to SSH into
    one of the manager nodes. I select `node-1`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经搭建了一个高可用的Docker Swarm，接下来是时候在其上运行一些工作负载了。我使用的是通过docker-machine创建的本地Swarm。我们将首先创建一个单一的服务。为此，我们需要通过SSH连接到其中一个管理节点。我选择了`node-1`：
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Creating a service
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个服务
- en: 'A service can be either created as part of a stack or directly using the Docker
    CLI. Let''s first look at a sample stack file that defines a single service:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 服务可以作为堆栈的一部分创建，也可以直接使用Docker CLI创建。我们首先来看一个定义单个服务的示例堆栈文件：
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding example, we see what the desired state of a service called `whoami` is:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们可以看到名为`whoami`的服务的期望状态：
- en: It is based on the `training/whoami:latest` image.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于`training/whoami:latest`镜像。
- en: Containers of the service are attached to the `test-net` network.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的容器附加到`test-net`网络。
- en: The container port `8000` is published to port `81`.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器端口`8000`映射到端口`81`。
- en: It is running with six replicas (or tasks)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它运行着六个副本（或任务）
- en: During a rolling update, the individual tasks are updated in batches of two,
    with a delay of 10 seconds between each successful batch.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在滚动更新过程中，单独的任务按两两个批次进行更新，每个成功批次之间有10秒的延迟。
- en: The service (and its tasks and containers) is assigned the two labels `app` and `environment `with
    the values `sample-app` and `prod-south`, respectively
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该服务（及其任务和容器）被分配了两个标签`app`和`environment`，分别对应值`sample-app`和`prod-south`
- en: There are many more settings that we could define for a service, but the preceding
    ones are some of the more important ones. Most settings have meaningful default
    values. If, for example, we do not specify the number of replicas, then Docker
    defaults it to `1`. The name and image of a service are, of course, mandatory.
    Note that the name of the service must be unique in the Swarm.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为一个服务定义更多的设置，但前面提到的那些是一些比较重要的设置。大多数设置都有有意义的默认值。例如，如果我们没有指定副本数，Docker 会默认为`1`。服务的名称和镜像当然是必填项。请注意，服务的名称在
    Swarm 中必须是唯一的。
- en: 'To create the preceding service, we use the `docker stack deploy` command.
    Assuming that the file in which the preceding content is stored is called `stack.yaml`, we
    have the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建上述服务，我们使用`docker stack deploy`命令。假设前述内容存储的文件名为`stack.yaml`，我们有以下内容：
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we have created a stack called `sample-stack` that consists of one service, `whoami`.
    We can list all stacks on our Swarm, whereupon we should get this:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个名为`sample-stack`的堆栈，其中包含一个服务`whoami`。我们可以列出 Swarm 中的所有堆栈，应该会得到如下输出：
- en: '[PRE29]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If we list the services defined in our Swarm, we get the following output:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们列出在 Swarm 中定义的服务，将得到以下输出：
- en: '![](img/2598bb40-3f60-44b3-8bb8-2d23d4e5a2df.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2598bb40-3f60-44b3-8bb8-2d23d4e5a2df.png)'
- en: List of all services running in the Swarm
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Swarm 中运行的所有服务列表
- en: In the output, we can see that currently, we have only one service running,
    which was to be expected. The service has an `ID`. The format of `ID`, contrary
    to what you have used so far for containers, networks, or volumes, is alphanumeric
    (in the latter cases it was always `sha256`). We can also see that `NAME` of the
    service is a combination of the service name we defined in the stack file and
    the name of the stack, which is used as a prefix. This makes sense since we want
    to be able to deploy multiple stacks (with different names) using the same stack
    file into our Swarm. To make sure that service names are unique, Docker decided
    to combine service name and stack name.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到目前只有一个服务在运行，这是预期中的情况。该服务有一个`ID`。与您迄今为止用于容器、网络或卷的 ID 格式不同，`ID` 是字母数字形式的（在后者情况下，它通常是`sha256`）。我们还可以看到服务的`NAME`是我们在堆栈文件中定义的服务名称和堆栈名称的组合，堆栈名称作为前缀使用。这是有道理的，因为我们希望能够使用相同的堆栈文件，将多个不同名称的堆栈部署到我们的
    Swarm 中。为了确保服务名称的唯一性，Docker 决定将服务名称和堆栈名称结合起来。
- en: In the third column, we see the mode, which is `replicated`. The number of `REPLICAS`
    is shown as `6/6`. This tells us that six out of the six requested `REPLICAS`
    are running. This corresponds to the desired state. In the output, we also see
    the image that the service uses and the port mappings of the service.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三列中，我们可以看到模式是`replicated`。`REPLICAS`的数量显示为`6/6`，这告诉我们，六个请求的`REPLICAS`都在运行。这与期望状态相符。在输出中，我们还可以看到服务使用的镜像和服务的端口映射。
- en: Inspecting the service and its tasks
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查服务及其任务
- en: 'In the preceding output, we cannot see the details of the `6` replicas that have been
    created. To get some deeper insight into that, we can use the `docker service
    ps` command. If we execute this command for our service, we will get the following
    output:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的输出中，我们无法看到已创建的`6`个副本的详细信息。为了深入了解，我们可以使用`docker service ps`命令。如果我们对我们的服务执行这个命令，将得到以下输出：
- en: '![](img/407ffc09-b33c-459b-a73c-fb17de789245.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/407ffc09-b33c-459b-a73c-fb17de789245.png)'
- en: Details of the whoami service
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: whoami 服务的详细信息
- en: In the preceding output, we can see the list of six tasks that correspond to
    the requested six replicas of our `whoami` service. In the `NODE` column, we can
    also see the node to which each task has been deployed. The name of each task
    is a combination of the service name plus an increasing index. Also note that,
    similar to the service itself, each task gets an alphanumeric ID assigned.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的输出中，我们可以看到六个任务的列表，这些任务对应于我们`whoami`服务请求的六个副本。在`NODE`列中，我们还可以看到每个任务已部署到哪个节点。每个任务的名称是服务名称和递增索引的组合。还请注意，类似于服务本身，每个任务都会被分配一个字母数字
    ID。
- en: 'In my case, apparently task 2, with the name `sample-stack_whoami.2`, has been
    deployed to `node-1`, which is the leader of our Swarm. Hence, I should find a
    container running on this node. Let''s see what we get if we list all containers
    running on `node-1`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，显然任务2，名称为`sample-stack_whoami.2`，已部署到`node-1`，这是我们Swarm的领导节点。因此，我应该能在该节点上找到一个正在运行的容器。让我们看看如果列出`node-1`上所有运行的容器会得到什么：
- en: '![](img/18d77691-b017-4ff3-9ea0-d07b10cd107a.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18d77691-b017-4ff3-9ea0-d07b10cd107a.png)'
- en: List of containers on node-1
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: node-1上的容器列表
- en: 'As expected, we find a container running from the `training/whoami:latest` image
    with a name that is a combination of its parent task name and ID. We can try to
    visualize the whole hierarchy of objects that we generated when deploying our
    sample stack:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们找到了一个来自`training/whoami:latest`镜像的容器，容器的名称是其父任务名称和ID的组合。我们可以尝试可视化在部署示例堆栈时生成的所有对象的层级结构：
- en: '![](img/376c4e6d-f65b-4c82-b74b-02f320141812.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/376c4e6d-f65b-4c82-b74b-02f320141812.png)'
- en: Object hierarchy of a Docker Swarm stack
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm堆栈的对象层级结构
- en: 'A **stack** can consist of one to many services. Each service has a collection
    of tasks. Each task has a one-to-one association with a container. Stacks and
    services are created and stored on the Swarm manager nodes. Tasks are then scheduled
    to Swarm worker nodes, where the worker node creates the corresponding container.
    We can also get some more information about our service by inspecting it. Execute
    the following command:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆栈**可以由一个或多个服务组成。每个服务都有一组任务。每个任务与一个容器一一对应。堆栈和服务在Swarm管理节点上创建并存储。然后，任务会调度到Swarm工作节点，工作节点在其上创建相应的容器。我们还可以通过检查服务获取更多信息。执行以下命令：'
- en: '[PRE30]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This provides a wealth of information about all of the relevant settings of
    the service. This includes those we have explicitly defined in our `stack.yaml` file,
    but also those that we didn't specify and that therefore got their default values
    assigned. We're not going to list the whole output here, as it is too long, but
    I encourage the reader to inspect it on their own machine. We will discuss part
    of the information in more detail in the *The swarm routing mesh* section.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了关于服务所有相关设置的丰富信息。这包括我们在`stack.yaml`文件中明确定义的设置，也包括那些我们未指定的，因此被分配了默认值的设置。我们不会在这里列出完整的输出，因为它太长，但我鼓励读者在自己的机器上查看。我们将在*The
    swarm routing mesh*部分更详细地讨论其中的一部分信息。
- en: Logs of a service
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务日志
- en: 'In an earlier chapter, we worked with the logs produced by a container. Here,
    we''re concentrating on a service. Remember that, ultimately, a service with many
    replicas has many containers running. Hence, we would expect that, if we ask the
    service for its logs, Docker returns an aggregate of all logs of those containers
    belonging to the service. And indeed, that''s what we get if we use the `docker
    service logs` command:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们处理了容器生成的日志。在这里，我们专注于服务。请记住，最终，一个具有多个副本的服务有多个容器在运行。因此，如果我们要求服务返回日志，Docker会返回属于该服务的所有容器日志的汇总。事实上，如果我们使用`docker
    service logs`命令，正是这样得到的：
- en: '![](img/cbe0a89f-0434-42a8-ae44-c1abdf29515f.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbe0a89f-0434-42a8-ae44-c1abdf29515f.png)'
- en: Logs of the whoami service
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: whoami服务的日志
- en: There is not much information in the logs at this point, but it is enough to
    discuss what we get. The first part of each line in the log always contains the
    name of the container combined with the node name from which the log entry originates.
    Then, separated by the vertical bar (`|`), we get the actual log entry. So, if
    we would, say, ask for the logs of the first container in the list directly, we
    would only get a single entry, and the value we would see in this case would be `Listening
    on :8000`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 目前日志中没有太多信息，但足以讨论我们得到的内容。每行日志的第一部分总是包含容器的名称和该日志条目来源的节点名称。然后，使用竖线（`|`）分隔，我们得到了实际的日志条目。因此，如果我们直接要求查看列表中第一个容器的日志，我们只会得到一条日志条目，这时看到的值将是`Listening
    on :8000`。
- en: The aggregated logs that we get with the `docker service logs` command are not
    sorted in any particular way. So, if the correlation of events is happening in
    different containers, you should add information to your log output that makes
    this correlation possible. Typically, this is a timestamp for each log entry.
    But this has to be done at the source; for example, the application that produces
    a log entry needs to also make sure a timestamp is added.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`docker service logs`命令获得的汇总日志并没有按照特定的顺序排列。所以，如果事件的关联发生在不同的容器中，你应该在日志输出中添加一些信息以使这种关联成为可能。通常，这对于每个日志条目来说是一个时间戳。但这必须在源头完成；例如，生成日志条目的应用程序需要确保添加时间戳。
- en: 'We can as well query the logs of an individual task of the service by providing
    the task ID instead of the service ID or name. So, querying the logs from task
    2 gives us the following output:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过提供任务ID而不是服务ID或名称，查询服务中单个任务的日志。所以，查询任务2的日志给我们以下输出：
- en: '![](img/edd0bb6b-3bfc-4cb8-97f1-ddab8836e1c6.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edd0bb6b-3bfc-4cb8-97f1-ddab8836e1c6.png)'
- en: Logs of an individual task of the whoami service
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 单个任务的whoami服务日志
- en: Reconciling the desired state
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整期望状态
- en: We have learned that a Swarm service is a description or manifest of the desired
    state that we want an application or application service to run in. Now, let's
    see how Docker Swarm reconciles this desired state if we do something that causes
    the actual state of the service to be different from the desired state. The easiest
    way to do this is to forcibly kill one of the tasks or containers of the service.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，Swarm服务是我们希望应用程序或应用程序服务运行的期望状态的描述或清单。现在，让我们看看如果我们做一些导致服务的实际状态与期望状态不同的操作，Docker
    Swarm如何调整这个期望状态。最简单的方法是强制终止服务的某个任务或容器。
- en: 'Let''s do this with the container that has been scheduled on `node-1`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对在`node-1`上调度的容器执行此操作：
- en: '[PRE31]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we do that and then do `docker service ps` right afterward, we will see
    the following output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做，然后紧接着运行`docker service ps`，我们将看到以下输出：
- en: '![](img/2b7a9616-8a86-467d-8e73-eafd14dd9f97.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b7a9616-8a86-467d-8e73-eafd14dd9f97.png)'
- en: Docker Swarm reconciling the desired state after one task failed
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm在一个任务失败后调整期望状态
- en: We see that task 2 failed with exit code `137` and that the Swarm immediately
    reconciled the desired state by rescheduling the failed task on a node with free
    resources. In this case, the scheduler selected the same node as the failed tasks,
    but this is not always the case. So, without us intervening, the Swarm completely
    fixed the problem, and since the service is running in multiple replicas, at no
    time was the service down.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到任务2因退出代码`137`失败，并且Swarm立即通过在具有空闲资源的节点上重新调度失败的任务来调整期望状态。在这种情况下，调度程序选择了与失败任务相同的节点，但这并不总是如此。因此，在没有我们干预的情况下，Swarm完全解决了问题，并且由于服务以多个副本运行，服务始终没有中断。
- en: 'Let''s try another failure scenario. This time we''re going to shut down an
    entire node and are going to see how the Swarm reacts. Let''s take `node-2` for
    this, as it has two tasks (tasks 3 and 4) running on it. For this, we need to
    open a new Terminal window and use `docker-machine` to stop `node-2`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个故障场景。这一次我们将关闭整个节点，并观察Swarm如何反应。我们选择`node-2`，因为它上面运行着两个任务（任务3和任务4）。为此，我们需要打开一个新的终端窗口，并使用`docker-machine`停止`node-2`：
- en: '[PRE32]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Back on `node-1`, we can now again run `docker service ps` to see what happened:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`node-1`，我们现在可以再次运行`docker service ps`来查看发生了什么：
- en: '![](img/e9b37aa0-d5c0-4666-8a11-f57874d4d283.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9b37aa0-d5c0-4666-8a11-f57874d4d283.png)'
- en: Swarm reschedules all tasks of a failed node
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm重新调度失败节点的所有任务
- en: In the preceding screenshot, we can see that immediately task 3 was rescheduled
    on `node-1` while task 4 was rescheduled on `node-3`. Even this more radical failure
    is handled gracefully by Docker Swarm.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到，任务3立即被重新调度到`node-1`，而任务4则被重新调度到`node-3`。即使是这种更为剧烈的故障，Docker Swarm也能优雅地处理。
- en: It is important to note though that if `node-2` ever comes back online in the
    Swarm, the tasks that had previously been running on it will not automatically
    be transferred back to it. But the node is now ready for a new workload.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果`node-2`在Swarm中重新上线，之前在其上运行的任务不会自动转移回该节点。但该节点现在已准备好处理新的工作负载。
- en: Deleting a service or a stack
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除服务或堆栈
- en: 'If we want to remove a particular service from the Swarm, we can use the `docker
    service rm` command. If, on the other hand, we want to remove a stack from the
    Swarm, we analogously use the `docker stack rm` command. This command removes
    all services that are part of the stack definition. In the case of the `whoami` service,
    it was created by using a stack file and hence we''re going to use the latter
    command:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想从Swarm中删除特定的服务，可以使用`docker service rm`命令。另一方面，如果我们想从Swarm中移除一个堆栈，我们类似地使用`docker
    stack rm`命令。此命令将删除堆栈定义的所有服务。对于`whoami`服务的情况，它是通过使用堆栈文件创建的，因此我们将使用后者命令：
- en: '![](img/61fca437-abee-41f3-8cea-6a8a534a58fc.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61fca437-abee-41f3-8cea-6a8a534a58fc.png)'
- en: Removing a stack
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 移除堆栈
- en: The preceding command will make sure that all tasks of each service of the stack
    are terminated, and the corresponding containers are stopped by first sending `SIGTERM`,
    and then, if not successful, `SIGKILL` after 10 seconds of timeout.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将确保终止堆栈每个服务的所有任务，并通过首先发送`SIGTERM`，然后如果不成功，在超时10秒后发送`SIGKILL`来停止相应的容器。
- en: It is important to note that the stopped containers are not removed from the
    Docker host. Hence, it is advised to purge containers from time to time on worker
    nodes to reclaim unused resources. Use `docker container purge -f` for this purpose.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，已停止的容器未从Docker主机中移除。因此，建议定期在工作节点上清理容器以回收未使用的资源。用`docker container purge
    -f`来实现这一目的。
- en: 'Question: Why does it make sense to leave stopped or crashed containers on
    the worker node and not automatically remove them?'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么将已停止或崩溃的容器留在工作节点上而不自动删除它们是有意义的？
- en: Deploying a multi-service stack
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署多服务堆栈
- en: 'In [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml), *Docker Compose*, we
    used an application consisting of two services that were declaratively described
    in a Docker compose file. We can use this compose file as a template to create
    a stack file that allows us to deploy the same application into a Swarm. The content
    of our stack file, called `pet-stack.yaml`, looks like this:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](412c6f55-a00b-447f-b22a-47b305453507.xhtml)，*Docker Compose*，我们使用了一个由Docker
    Compose文件声明描述的由两个服务组成的应用程序。我们可以使用这个Compose文件作为模板创建一个堆栈文件，允许我们将相同的应用程序部署到Swarm中。我们的堆栈文件`pet-stack.yaml`的内容如下：
- en: '[PRE33]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We request that the `web` service has three replicas, and both services are
    attached to the overlay network, `pets-net`. We can deploy this application using
    the `docker stack deploy` command:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求`web`服务有三个副本，并且两个服务都连接到覆盖网络`pets-net`。我们可以使用`docker stack deploy`命令部署此应用程序：
- en: '![](img/4920e7c3-dc61-4bcf-9960-cc64dcb42e3f.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4920e7c3-dc61-4bcf-9960-cc64dcb42e3f.png)'
- en: Deploy the pets stack
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 部署宠物堆栈
- en: 'Docker creates the `pets_pets-net` overlay network and then the two services `pets_web` and `pets_db`.
    We can then list all of the tasks in the `pets` stack:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Docker创建了`pets_pets-net`覆盖网络，然后是两个服务`pets_web`和`pets_db`。然后我们可以列出`pets`堆栈中的所有任务：
- en: '![](img/2af11ff2-9cb7-4056-aa02-409bce27fed2.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2af11ff2-9cb7-4056-aa02-409bce27fed2.png)'
- en: List of all of the tasks in the pets stack
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列出宠物堆栈中的所有任务
- en: 'Finally, let''s test the application using `curl`. And, indeed, the application
    works as expected:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用`curl`测试应用程序。确实，应用程序按预期工作：
- en: '![](img/7e7fac0b-5781-4038-a0f8-8f53d933da3f.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e7fac0b-5781-4038-a0f8-8f53d933da3f.png)'
- en: Testing the pets application using curl
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用curl测试宠物应用程序
- en: The container ID is in the output, where it says `Delivered to you by container 8b906b509a7e`.
    If you run the `curl` command multiple times, the ID should cycle between three
    different values. These are the IDs of the three containers (or replicas) that
    we have requested for the `web` service.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 容器ID在输出中，显示为`Delivered to you by container 8b906b509a7e`。如果多次运行`curl`命令，ID应在三个不同的值之间循环。这些是我们请求`web`服务的三个容器（或副本）的ID。
- en: Once we're done, we can remove the stack with `docker stack rm pets`.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以使用`docker stack rm pets`删除堆栈。
- en: The swarm routing mesh
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm路由网格
- en: 'If you have been paying attention, then you might have noticed something interesting
    in the last section. We had the `pets` application deployed and it resulted in
    the fact that an instance of the `web` service was installed on the three nodes, `node-1`, `node-2`,
    and `node-3`. Yet, we were able to access the `web` service on `node-1` with `localhost` and
    we reached each container from there. *How is that possible?* Well, this is due
    to the so-called Swarm routing mesh. The routing mesh makes sure that when we
    publish a port of a service; that port is then published on all nodes of the Swarm.
    Hence, network traffic that hits any node of the Swarm and requests to use a specific
    port will be forwarded to one of the service containers by the routing mesh. Let''s
    look at the following diagram to see how that works:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在关注，那么你可能会注意到上一部分有一个有趣的现象。我们部署了`pets`应用程序，结果在`node-1`、`node-2`和`node-3`三个节点上安装了`web`服务的实例。然而，我们能够通过`localhost`访问`node-1`上的`web`服务，并且从那里访问每个容器。*这怎么可能？*
    其实这是由于所谓的Swarm路由网格。路由网格确保当我们发布一个服务的端口时，这个端口会在Swarm的所有节点上发布。因此，任何节点上的网络流量，只要请求使用特定的端口，都会通过路由网格转发到服务的某个容器上。我们来看一下下面的图，看看它是如何工作的：
- en: '![](img/6ceca300-992c-4267-a309-239315df3cd9.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ceca300-992c-4267-a309-239315df3cd9.png)'
- en: Docker Swarm routing mesh
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 路由网格
- en: In this situation, we have three nodes, called **Host A** to **Host C**, with
    the IP addresses `172.10.0.15`, `172.10.0.17`, and `172.10.0.33`. In the lower-left
    corner of the diagram, we see the command that created a **web** service with
    two replicas. The corresponding tasks have been scheduled on **Host B** and **Host
    C**. Task 1 landed on **Host B** while task 2 landed on **Host C**.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有三个节点，分别称为**Host A**、**Host B**和**Host C**，它们的IP地址分别是`172.10.0.15`、`172.10.0.17`和`172.10.0.33`。在图的左下角，我们看到创建**web**服务的命令，服务包含两个副本。相应的任务已被调度到**Host
    B**和**Host C**上。任务1被分配到**Host B**，而任务2被分配到**Host C**。
- en: When a service is created on Docker Swarm, it automatically gets a **V****irtual
    IP** (**VIP**) address assigned. This IP address is stable and reserved during
    the whole life cycle of the service. Let's assume that in our case the VIP is `10.2.0.1`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Docker Swarm上创建一个服务时，它会自动分配一个**虚拟IP**（**VIP**）地址。这个IP地址在服务的整个生命周期内都是稳定且保留的。假设在我们的案例中，VIP为`10.2.0.1`。
- en: If now a request for port `8080` coming from an external **Load Balancer** (**LB**)
    is targeted at one of the nodes of our Swarm, then this request is handled by
    the Linux **IP Virtual Server** (**IPVS**) service on that node. This service
    makes a lookup with the given port `8080` in the IP table and will find that this
    corresponds to the VIP of the **web** service. Now, since the VIP is not a real
    target, the IPVS service will load balance the IP addresses of the tasks that
    are associated with this service. In our case, it picked task 2, with the IP address, `10.2.0.3`.
    Finally, the **ingress**** Network** (**O****verlay**) is used to forward the
    request to the target container on **Host C**.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在来自外部**负载均衡器**（**LB**）的请求针对我们的Swarm节点的端口`8080`，那么该请求将由该节点上的Linux**IP虚拟服务器**（**IPVS**）服务处理。该服务会在IP表中查找端口`8080`，并找到它对应的是**web**服务的VIP。现在，由于VIP并不是实际的目标，IPVS服务将进行负载均衡，将请求转发到与该服务关联的任务的IP地址。在我们的案例中，它选择了任务2，对应的IP地址是`10.2.0.3`。最后，**入口**网络（**Overlay**）被用来将请求转发到**Host
    C**上的目标容器。
- en: It is important to note that it doesn't matter which Swarm node the external
    request is forwarded to by the **External LB**. The routing mesh will always handle
    the request correctly and forward it to one of the tasks of the targeted service.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，外部请求被**外部负载均衡器**（**External LB**）转发到哪个Swarm节点并不重要。路由网格总是会正确处理请求，并将其转发到目标服务的其中一个任务。
- en: Summary
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced Docker Swarm, which, next to Kubernetes, is the
    second most popular orchestrator for containers. We have looked into the architecture
    of a Swarm, discussed all of the types of resources running in a Swarm, such as
    services, tasks, and more, and we have created services in the Swarm and deployed
    an application that consists of multiple related services.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Docker Swarm，它是仅次于Kubernetes的第二大最流行的容器编排工具。我们探讨了Swarm的架构，讨论了Swarm中运行的各种资源类型，如服务、任务等，并且我们在Swarm中创建了服务并部署了由多个相关服务组成的应用程序。
- en: In the next chapter, we are going to explore how to deploy services or applications
    onto a Docker Swarm with zero downtime and automatic rollback capabilities. We
    are also going to introduce secrets as a means to protect sensitive information.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨如何将服务或应用程序部署到 Docker Swarm 中，并实现零停机时间和自动回滚功能。我们还将介绍如何使用密钥来保护敏感信息。
- en: Questions
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'To assess your learning progress, please answer the following questions:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估您的学习进度，请回答以下问题：
- en: How do you initialize a new Docker Swarm?
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何初始化一个新的 Docker Swarm？
- en: A. `docker init swarm`
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. `docker init swarm`
- en: B. `docker swarm init --advertise-addr <IP address>`
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. `docker swarm init --advertise-addr <IP 地址>`
- en: C. `docker swarm join --token <join token>`
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. `docker swarm join --token <join token>`
- en: You want to remove a worker node from a Docker Swarm. What steps are necessary?
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想从 Docker Swarm 中移除一个工作节点，需要哪些步骤？
- en: How do you create an overlay network called `front-tier`? Make the network attachable.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何创建一个名为 `front-tier` 的覆盖网络？使该网络可连接。
- en: How would you create a service called `web` from the `nginx:alpine` image with
    five replicas, which exposes port `3000` on the ingress network and is attached
    to the `front-tier` network?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用 `nginx:alpine` 镜像创建一个名为 `web` 的服务，包含五个副本，在入口网络上暴露端口 `3000`，并连接到 `front-tier`
    网络？
- en: How would you scale the web service down to three instances?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何将网页服务缩放至三个实例？
- en: Further reading
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Please consult the following link for more in-depth information about selected
    topics:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 请查阅以下链接，获取有关选定主题的更深入信息：
- en: AWS EC2 example at [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS EC2 示例 [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)
- en: The Raft Consensus Algorithm at [https://raft.github.io/](https://raft.github.io/)
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raft 共识算法 [https://raft.github.io/](https://raft.github.io/)
- en: The Gossip Protocol at [https://en.wikipedia.org/wiki/Gossip_protocol](https://en.wikipedia.org/wiki/Gossip_protocol)
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谣言协议 [https://en.wikipedia.org/wiki/Gossip_protocol](https://en.wikipedia.org/wiki/Gossip_protocol)
- en: VXLAN and Linux at [https://vincent.bernat.ch/en/blog/2017-vxlan-linux](https://vincent.bernat.ch/en/blog/2017-vxlan-linux)
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN 和 Linux [https://vincent.bernat.ch/en/blog/2017-vxlan-linux](https://vincent.bernat.ch/en/blog/2017-vxlan-linux)
