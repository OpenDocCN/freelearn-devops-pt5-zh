<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-328"><a id="_idTextAnchor328"/>15</h1>
<h1 id="_idParaDest-329"><a id="_idTextAnchor329"/>Deploying and Running a Distributed Application on Docker Swarm</h1>
<p>In the last chapter, we got a detailed introduction to Docker’s native orchestrator called SwarmKit. SwarmKit is part of Docker Engine, and no extra installation is needed once you have Docker installed on your system. We learned about the concepts and objects SwarmKit uses to deploy and run distributed, resilient, robust, and highly available applications in a cluster, which can either run on-premises or in the cloud. We also showed how Docker’s orchestrator secures applications using SDNs. We learned how to create a Docker Swarm locally, in a special environment called Play with Docker, and also in the cloud. Finally, we discovered how to deploy an application that consists of multiple related services to Docker Swarm.</p>
<p>In this chapter, we are going to introduce the routing mesh, which provides layer-4 routing and load balancing. Next, we are going to demonstrate how to deploy a first application consisting of multiple services onto the Swarm. We are also learning how to achieve zero downtime when updating an application in the swarm and finally how to store configuration data in the swarm and how to protect sensitive data using Docker secrets.</p>
<p>These are the topics we are going to discuss in this chapter:</p>
<ul>
<li>The Swarm routing mesh</li>
<li>Zero-downtime deployment</li>
<li>Storing configuration data in the swarm</li>
<li>Protecting sensitive data with Docker Secrets</li>
</ul>
<p>After completing this chapter, you will be able to do the following:</p>
<ul>
<li>List two to three different deployment strategies commonly used to update a service without downtime</li>
<li>Update a service in batches without causing a service interruption</li>
<li>Define a rollback strategy for a service that is used if an update fails</li>
<li>Store non-sensitive configuration data using Docker configs</li>
<li>Use a Docker secret with a service</li>
<li>Update the value of a secret without causing downtime</li>
</ul>
<p>Let’s get started!</p>
<h1 id="_idParaDest-330"><a id="_idTextAnchor330"/>The swarm routing mesh</h1>
<p>If you have <a id="_idIndexMarker1275"/>paid attention, then you might have noticed something interesting in the last chapter. We had the <code>pets</code> application deployed and it resulted in an instance of the web service being installed on the three nodes – <code>node-1</code>, <code>node-2</code>, and <code>node-3</code>.</p>
<p>Yet, we were able to access the web service on <code>node-1</code> with <code>localhost</code> and we reached each container from there. How is that possible? Well, this is due to the so-called Swarm routing mesh. The routing mesh makes sure that when we publish a port of a service, that port is then published on all nodes of the Swarm. Hence, network traffic that hits any node of the Swarm and requests to use a specific port will be forwarded to one of the service containers by routing the mesh. Let’s look at the following diagram to see how that works:</p>
<div><div><img alt="Figure 15.1 – Docker Swarm routing mesh" height="481" src="img/Image98324.jpg" width="871"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.1 – Docker Swarm routing mesh</p>
<p>In this situation, we have three nodes, called <code>172.10.0.15</code>, <code>172.10.0.17</code>, and <code>172.10.0.33</code>. In the lower-left corner of the diagram, we see the command that created a web service with two replicas. The corresponding tasks have been scheduled on <strong class="bold">Host B</strong> and <strong class="bold">Host C</strong>. <strong class="bold">task1</strong> landed on <strong class="bold">Host B</strong> while <strong class="bold">task2</strong> landed on <strong class="bold">Host C</strong>.</p>
<p>When a service is <a id="_idIndexMarker1276"/>created in Docker Swarm, it automatically gets a <code>10.2.0.1</code>.</p>
<p>If now a request for port <code>8080</code> coming from <a id="_idIndexMarker1278"/>an external <code>8080</code> in the IP table and will find that this corresponds to the VIP of the web service.</p>
<p>Now, since the VIP is not a real target, the IPVS service will load-balance the IP addresses of the tasks that are associated with this service. In our case, it picked <code>10.2.0.3</code>. Finally, <strong class="bold">Ingress Network (Overlay)</strong> is used to forward the request to the target container on <strong class="bold">Host C</strong>.</p>
<p>It is important to note that it doesn’t matter which Swarm node the external request is forwarded to by <strong class="bold">External LB</strong>. The routing mesh will always handle the request correctly and forward it to one of the tasks of the targeted service.</p>
<p>We have learned a <a id="_idIndexMarker1280"/>lot about networking in a Docker swarm. The next topic that we are going to learn about is how can we deploy an application without causing any system downtime.</p>
<h1 id="_idParaDest-331"><a id="_idTextAnchor331"/>Zero-downtime deployment</h1>
<p>One of the most<a id="_idIndexMarker1281"/> important aspects of a mission-critical application that needs frequent updates is the ability to do updates in a fashion that requires no outage at all. We call this a zero-downtime deployment. At all times, the application that is updated must be fully operational.</p>
<h2 id="_idParaDest-332"><a id="_idTextAnchor332"/>Popular deployment strategies</h2>
<p>There are various<a id="_idIndexMarker1282"/> ways to achieve this. Some of them are as follows:</p>
<ul>
<li>Rolling updates</li>
<li>Blue-green deployments</li>
<li>Canary releases</li>
</ul>
<p>Docker Swarm supports rolling updates out of the box. The other two types of deployments can be achieved with some extra effort on our part.</p>
<h2 id="_idParaDest-333"><a id="_idTextAnchor333"/>Rolling updates</h2>
<p>In a mission-critical <a id="_idIndexMarker1283"/>application, each application service has to run in multiple replicas. Depending on the load, that can be as few as two to three instances and as many as dozens, hundreds, or thousands of instances. At any given time, we want to have a clear majority when it comes to all the service instances running. So, if we have three replicas, we want to have at least two of them up and running at all times. If we have 100 replicas, we can be content with a minimum of, say, 90 replicas, available. By doing this, we can define the batch size of replicas that we may take down to upgrade. In the first case, the batch size would be 1, and in the second case, it would be 10.</p>
<p>When we take replicas down, Docker Swarm will automatically take those instances out of the load-balancing pool and all traffic will be load-balanced across the remaining active instances. Those remaining instances will thus experience a slight increase in traffic. In the following diagram, prior to the start of the rolling update, if <strong class="bold">Task A3</strong> wanted to access <strong class="bold">Service B</strong>, it could be load-balanced to any of the three tasks of <strong class="bold">Service B</strong> by SwarmKit. Once the rolling update started, SwarmKit took down <strong class="bold">Task B1</strong> for updates.</p>
<p>Automatically, this task is then taken out of the pool of targets. So, if <strong class="bold">Task A3</strong> now requests to<a id="_idIndexMarker1284"/> connect to <strong class="bold">Service B</strong>, load balancing will only select from the remaining tasks, that is, <strong class="bold">Task B2</strong> and <strong class="bold">Task B3</strong>. Thus, those two tasks might experience a higher load temporarily:</p>
<div><div><img alt="Figure 15.2 – Task B1 is taken down to be updated" height="420" src="img/Image98334.jpg" width="651"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.2 – Task B1 is taken down to be updated</p>
<p>The stopped instances are then replaced by an equivalent number of new instances of the new version of the application service. Once the new instances are up and running, we can have the Swarm observe them for a given period of time and make sure they’re healthy. If all is well, then we can continue by taking down the next batch of instances and replacing them with instances of the new version. This process is repeated until all the instances of the application service have been replaced.</p>
<p>In the following diagram, we can see that <strong class="bold">Task B1</strong> of <strong class="bold">Service B</strong> has been updated to version 2. The container of <strong class="bold">Task B1</strong> was assigned a new IP address, and it was deployed to<a id="_idIndexMarker1285"/> another worker node with free resources:</p>
<div><div><img alt="Figure 15.3 – The first batch being updated in a rolling update" height="420" src="img/Image98343.jpg" width="651"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.3 – The first batch being updated in a rolling update</p>
<p>It is important to understand that when the task of a service is updated, in most cases, it gets deployed to a worker node other than the one it used to live on, but that should be fine as long as the corresponding service is stateless. If we have a stateful service that is location- or node-aware and we’d like to update it, then we have to adjust our approach, but this is outside of the scope of this book.</p>
<p>Now, let’s look at how we can actually instruct the Swarm to perform a rolling update of an application service. When we declare a service in a <code>stack</code> file, we can define multiple options that are relevant in this context. Let’s look at a snippet of a typical <code>stack</code> file:</p>
<pre class="console">
version: "3.5"services:
  web:
    image: nginx:alpine
    deploy:
      replicas: 10
      update_config:
        parallelism: 2
        delay: 10s
...</pre>
<p>In this snippet, we can see a section, <code>update_config</code>, with <code>parallelism</code> and <code>delay</code> properties. <code>parallelism</code> defines the batch size of how many replicas are going to be updated at a time during a rolling update. <code>delay</code> defines how long Docker Swarm is going to wait between updating individual batches. In the preceding case, we have 10 replicas that are being updated in two instances at a time and, between each successful update, Docker Swarm waits for 10 seconds.</p>
<p>Let’s test such a <a id="_idIndexMarker1286"/>rolling update. Navigate to the <code>ch14</code> subfolder of our <code>sample-solutions</code> folder and use the <code>web-stack.yaml</code> file to create a web service that’s been configured for a rolling update. The service uses an Alpine-based Nginx image whose version is <code>1.12-alpine</code>. We will update the service to a newer version, that is, <code>1.13-alpine</code>.</p>
<p>To start, we will deploy this service to the swarm that we created in AWS.</p>
<p>Let’s take a look:</p>
<ol>
<li>SSH into the <code>master1</code> instance of your Docker swarm on AWS:<pre class="source-code">
$ ssh -i "aws-docker-demo.pem" &lt;public-dns-of-manager1-instance&gt;</pre></li> <li>Create a file called <code>web-stack.yml</code> using <code>vi</code> or <code>nano</code> with this content:<pre class="source-code">
version: "3.7"services:  whoami:    image: nginx:1.12-alpine    ports:    - 81:80    deploy:      replicas: 10      update_config:        parallelism: 2        delay: 10s</pre></li> <li>Now, we can <a id="_idIndexMarker1287"/>deploy the service using the <code>stack</code> file:<pre class="source-code">
$ docker stack deploy -c web-stack.yaml web</pre></li> </ol>
<p>The output of the preceding command looks like this:</p>
<pre class="source-code">
Creating network web_defaultCreating service web_web</pre>
<ol>
<li value="4">Once the service has been deployed, we can monitor it using the following command:<pre class="source-code">
$ watch docker stack ps web</pre></li> </ol>
<p>We will see the following output:</p>
<div><div><img alt="Figure 15.4 – Service web of the web stack running in Swarm with 10 replicas" height="278" src="img/Image98354.jpg" width="1103"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.4 – Service web of the web stack running in Swarm with 10 replicas</p>
<p>The previous command will continuously update the output and provide us with a good overview of what happens during the rolling update.</p>
<ol>
<li value="5">Now, we need to open a second Terminal and configure it for remote access for the manager node of our swarm. Once we have done that, we can execute the <code>docker</code> command, which will update the image of the web service of the <code>stack</code>, also called <code>web</code>:<pre class="source-code">
$ docker service update --image nginx:1.13-alpine web_web</pre></li> </ol>
<p>The preceding command leads to the following output, indicating the progress of the rolling<a id="_idIndexMarker1288"/> update:</p>
<div><div><img alt="Figure 15.5 – Screen showing the progress of the rolling update" height="389" src="img/Image98364.jpg" width="1160"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.5 – Screen showing the progress of the rolling update</p>
<p>The preceding output indicates that the first two batches, each with two tasks, have been successful and that the third batch is about to be prepared.</p>
<p>In the first Terminal window, where we’re watching the <code>stack</code>, we should now see how Docker Swarm updates the service batch by batch with an interval of 10 seconds. After the first batch, it should look like the following screenshot:</p>
<div><div><img alt="Figure 15.6 – Rolling update for a service in Docker Swarm" height="335" src="img/Image98374.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.6 – Rolling update for a service in Docker Swarm</p>
<p>In the preceding screenshot, we can see that the first batch of the two tasks, 2 and 10, has been updated. Docker Swarm is waiting for 10 seconds to proceed with the next batch.</p>
<p>It is interesting to note that in this particular case, SwarmKit deploys the new version of the task to the same node as the previous version. This is accidental since we have five nodes and two tasks on each node. SwarmKit always tries to balance the workload evenly across the nodes.</p>
<p>So, when SwarmKit takes down a task, the corresponding node has a smaller workload than all the <a id="_idIndexMarker1289"/>others, so the new instance is scheduled to it. Normally, you cannot expect to find a new instance of a task on the same node. Just try it out yourself by deleting the <code>stack</code> with <code>docker stack rm web</code> and changing the number of replicas to say, seven, and then redeploy and update it.</p>
<p>Once all the tasks have been updated, the output of our <code>docker stack ps web</code> command will look similar to the following screenshot:</p>
<div><div><img alt="Figure 15.7 – All tasks have been updated successfully" height="449" src="img/Image98382.jpg" width="1100"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.7 – All tasks have been updated successfully</p>
<p>Please note that SwarmKit does not immediately remove the containers of the previous versions of the tasks from the corresponding nodes. This makes sense as we might want to, for example, retrieve the logs from those containers for debugging purposes, or we might want to retrieve their metadata using <code>docker container inspect</code>. SwarmKit keeps the four latest terminated task instances around before it purges older ones so that it doesn’t clog the system with unused resources.</p>
<p>We can use the <code>--update-order</code> parameter to instruct Docker to start the new container replica before stopping the old one. This can improve application availability. Valid values are <code>start-first</code> and <code>stop-first</code>.</p>
<p>The latter is the default.</p>
<p>Once we’re done, we can tear down the <code>stack</code> using the following command:</p>
<pre class="console">
$ docker stack rm web</pre> <p>Although using <code>stack</code> files to define and deploy applications is the recommended best practice, we can also define the update behavior in a <code>service create</code> statement. If we just want to deploy a single service, this might be the preferred way of doing things. Let’s look at such a <code>create</code> command:</p>
<pre class="console">
$ docker service create --name web \    --replicas 10 \
    --update-parallelism 2 \
    --update-delay 10s \
    nginx:alpine</pre>
<p>This command defines the same desired state as the preceding <code>stack</code> file. We want the service to run<a id="_idIndexMarker1290"/> with 10 replicas and we want a rolling update to happen in batches of two tasks at a time, with a 10-second interval between consecutive batches.</p>
<h2 id="_idParaDest-334"><a id="_idTextAnchor334"/>Health checks</h2>
<p>To make informed<a id="_idIndexMarker1291"/> decisions, for example, during a rolling update of a Swarm service regarding whether or not the just-installed batch of new service instances is running OK or whether a rollback is needed, SwarmKit needs a way to know about the overall health of the system. On its own, SwarmKit (and Docker) can collect quite a bit of information, but there is a limit. Imagine a container containing an application. The container, as seen from the outside, can look absolutely healthy and carry on just fine, but that doesn’t necessarily mean that the application running inside the container is also doing well. The application could, for example, be in an infinite loop or be in a corrupt state, yet still be running. However, as long as the application runs, the container runs, and, from the outside, everything looks perfect.</p>
<p>Thus, SwarmKit provides a seam where we can provide it with some help. We, the authors of the application services running inside the containers in the swarm, know best whether or not our service is in a healthy state. SwarmKit gives us the opportunity to define a command that is executed against our application service to test its health. What exactly this command does is not important to Swarm; the command just needs to return <em class="italic">OK</em>, <em class="italic">NOT OK</em>, or <em class="italic">time out</em>. The latter two situations, namely NOT OK or timeout, will tell SwarmKit that the task it is investigating is potentially unhealthy.</p>
<p>Here, I am writing potentially on purpose, and we will see why later:</p>
<pre class="console">
FROM alpine:3.6…
HEALTHCHECK --interval=30s \
    --timeout=10s
    --retries=3
    --start-period=60s
    CMD curl -f http://localhost:3000/health || exit 1
...</pre>
<p>In the preceding<a id="_idIndexMarker1292"/> snippet from a Dockerfile, we can see the <code>HEALTHCHECK</code> keyword. It has a few options or parameters and an actual command, that is, <code>CMD</code>. Let’s discuss the options:</p>
<ul>
<li><code>--interval</code>: Defines the wait time between health checks. Thus, in our case, the orchestrator executes a check every 30 seconds.</li>
<li><code>--timeout</code>: This parameter defines how long Docker should wait if the health check does not respond until it times out with an error. In our sample, this is 10 seconds. Now, if one health check fails, SwarmKit retries a couple of times until it gives up and declares the corresponding task as unhealthy and opens the door for Docker to kill this task and replace it with a new instance.</li>
<li>The number of retries is defined by the <code>--retries</code> parameter. In the preceding code, we want to have three retries.</li>
<li>Next, we have the start period. Some containers take some time to start up (not that this is a recommended pattern, but sometimes it is inevitable). During this startup time, the service instance might not be able to respond to health checks. With the start period, we can define how long SwarmKit should wait before it executes the very first health check and thus give the application time to initialize. To define the startup time, we use the <code>--start-period</code> parameter. In our case, we do the first check after 60 seconds. How long this start period needs to be depends on the application and its startup behavior. The recommendation is to start with a relatively low value and, if you have a lot of false positives and tasks that are restarted many times, you might want to increase the time interval.</li>
<li>Finally, we define the actual probing command on the last line with the <code>CMD</code> keyword. In our case, we are defining a request to the <code>/health</code> endpoint of <code>localhost</code> at port <code>3000</code> as a probing command. This call is expected to have three possible outcomes:<ul><li>The command succeeds</li><li>The command fails</li><li>The command times out</li></ul></li>
</ul>
<p>The latter two are<a id="_idIndexMarker1293"/> treated the same way by SwarmKit. This is the orchestrator telling us that the corresponding task might be unhealthy. I did say <em class="italic">might</em> with intent since SwarmKit does not immediately assume the worst-case scenario but assumes that this might just be a temporary fluke of the task and that it will recover from it. This is the reason why we have a <code>--retries</code> parameter. There, we can define how many times SwarmKit should retry before it can assume that the task is indeed unhealthy, and consequently kill it and reschedule another instance of this task on another free node to reconcile the desired state of the service.</p>
<p>Why can we use <code>localhost</code> in our probing command? This is a very good question, and the reason is that SwarmKit, when probing a container running in the Swarm, executes this probing command inside the container (that is, it does something such as <code>docker container exec &lt;containerID&gt; &lt;probing command&gt;</code>). Thus, the command executes in the same network namespace as the application running inside the container. In the following diagram, we can see the life cycle of a service task from its beginning:</p>
<div><div><img alt="Figure 15.8 – Service task with transient health failure" height="376" src="img/Image98391.jpg" width="722"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.8 – Service task with transient health failure</p>
<p>First, SwarmKit <a id="_idIndexMarker1294"/>waits to probe until the start period is over. Then, we have our first health check. Shortly thereafter, the task fails when probed. It fails two consecutive times but then it recovers. Thus, <strong class="bold">health check 4</strong> is successful and SwarmKit leaves the task running.</p>
<p>Here, we can see a task that is permanently failing:</p>
<div><div><img alt="Figure 15.9 – Permanent failure of a task" height="376" src="img/Image98401.jpg" width="721"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.9 – Permanent failure of a task</p>
<p>We have just learned how we can define a health check for a service in the Dockerfile of its image, but this is not the only way we can do this. We can also define the health check in the <code>stack</code> <a id="_idIndexMarker1295"/>file that we use to deploy our application into Docker Swarm. Here is a short snippet of what such a <code>stack</code> file would look like:</p>
<pre class="console">
version: "3.8"services:
  web:
    image: example/web:1.0
    healthcheck:
      test: ["CMD", "curl", "-f", http://localhost:3000/health]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
...</pre>
<p>In the preceding snippet, we can see how the health check-related information is defined in the <code>stack</code> file. First and foremost, it is important to realize that we have to define a health check for every service individually. There is no health check at an application or global level.</p>
<p>Similar to what we defined previously in the Dockerfile, the command that is used to execute the health check by SwarmKit is <code>curl -f http://localhost:3000/health</code>. We also have definitions for <code>interval</code>,<code> timeout</code>,<code> retries</code>, and <code>start_period</code>. These four key-value pairs have the same meaning as the corresponding parameters we used in the Dockerfile. If there are health check-related settings defined in the image, then the ones defined in the <code>stack</code> file override the<a id="_idIndexMarker1296"/> ones from the Dockerfile.</p>
<p>Now, let’s try to use a service that has a health check defined:</p>
<ol>
<li value="1">Use <code>vi</code> or <code>nano</code> to create a file called <code>stack-health.yml</code> with the following content:<pre class="source-code">
version: "3.8"services:  web:    image: nginx:alpine    deploy:      replicas: 3    healthcheck:      test: ["CMD", "wget", "-qO", "-", "http://localhost"]      interval: 5s      timeout: 2s      retries: 3      start_period: 15s</pre></li> <li>Let’s deploy this:<pre class="source-code">
$ docker stack deploy -c stack-health.yml myapp</pre></li> <li>We can find out where the single task was deployed to each cluster node using <code>docker stack ps myapp</code>. Thus, on any particular node, we can list all the containers to find one of our stacks. In my example, task 3 had been deployed to node <code>ip-172-31-32-21</code>, which happens to be the master.</li>
<li>Now, list the containers on that node:</li>
</ol>
<div><div><img alt="Figure 15.10 – Displaying the health status of a running task instance" height="64" src="img/Image98410.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.10 – Displaying the health status of a running task instance</p>
<p>The interesting thing in this screenshot is the <strong class="bold">STATUS</strong> column. Docker, or more precisely, SwarmKit, has recognized that the service has a health check function <a id="_idIndexMarker1297"/>defined and is using it to determine the health of each task of the service.</p>
<p>Next, let’s see what happens if something goes wrong.</p>
<h2 id="_idParaDest-335"><a id="_idTextAnchor335"/>Rolling back</h2>
<p>Sometimes, things <a id="_idIndexMarker1298"/>don’t go as expected. A last-minute fix to an application release may have inadvertently introduced a new bug, or the new version significantly may have significantly decreased the throughput of the component, and so on. In such cases, we need to have a plan B, which in most cases means the ability to roll back the update to the previous good version.</p>
<p>As with the update, the rollback has to happen so that it does not cause any outages in terms of the application; it needs to cause zero downtime. In that sense, a rollback can be looked at as a reverse update. We are installing a new version, yet this new version is actually the previous version.</p>
<p>As with the update behavior, we can declare, either in our <code>stack</code> files or in the Docker <code>service create</code> command, how the system should behave in case it needs to execute a rollback. Here, we have the <code>stack</code> file that we used previously, but this time with some rollback-relevant attributes:</p>
<pre class="console">
version: "3.8"services:
  web:
    image: nginx:1.12-alpine
    ports:
    - 80:80
    deploy:
      replicas: 10
    update_config:
      parallelism: 2
      delay: 10s
      failure_action: rollback
      monitor: 10s
    healthcheck:
      test: ["CMD", "wget", "-qO", "-", http://localhost]
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s</pre>
<p>We can create a stack file named <code>stack-rollback.yaml</code>, and add the preceding content to it. In this <a id="_idIndexMarker1299"/>content, we define the details of the rolling update, the health checks, and the behavior during rollback. The health check is defined so that after an initial wait time of 2 seconds, the orchestrator starts to poll the service on <code>http://localhost</code> every 2 seconds and retries 3 times before it considers a task unhealthy.</p>
<p>If we do the math, then it takes at least 8 seconds until a task will be stopped if it is unhealthy due to a bug. So, now under <code>deploy</code>, we have a new entry called <code>monitor</code>. This entry defines how long newly deployed tasks should be monitored for health and whether or not to continue with the next batch in the rolling update. Here, in this sample, we have given it 10 seconds. This is slightly more than the 8 seconds we calculated it takes to discover that a defective service has been deployed, so this is good.</p>
<p>We also have a new entry, <code>failure_action</code>, which defines what the orchestrator will do if it encounters a failure during the rolling update, such as the service being unhealthy. By default, the action is just to stop the whole update process and leave the system in an intermediate state. The system is not down since it is a rolling update and at least some healthy instances of the service are still operational, but an operations engineer would be better at taking a look and fixing the problem.</p>
<p>In our case, we<a id="_idIndexMarker1300"/> have defined the action to be a rollback. Thus, in case of failure, SwarmKit will automatically revert all tasks that have been updated back to their previous version.</p>
<h2 id="_idParaDest-336"><a id="_idTextAnchor336"/>Blue-green deployments</h2>
<p>In <a href="B19199_09.xhtml#_idTextAnchor194"><em class="italic">Chapter 9</em></a>, <em class="italic">Learning about </em><em class="italic">Distributed Application Architecture</em>, we discussed what blue-green deployments are, in <a id="_idIndexMarker1301"/>an abstract way. It turns<a id="_idIndexMarker1302"/> out that, on Docker Swarm, we cannot really implement blue-green deployments for arbitrary services. The service discovery and load balancing between two services running in Docker Swarm are part of the Swarm routing mesh and cannot be (easily) customized.</p>
<p>If <strong class="bold">Service A</strong> wants to call <strong class="bold">Service B</strong>, then Docker does this implicitly. Docker, given the name of the target service, will use the Docker DNS service to resolve this name to a VIP address. When the request is then targeted at the VIP, the Linux IPVS service will do another lookup in the Linux kernel IP tables with the VIP and load-balance the request to one of the physical IP addresses of the tasks of the service represented by the VIP, as shown in the following diagram:</p>
<div><div><img alt="Figure 15.11 – How service discovery and load balancing work in Docker Swarm" height="312" src="img/Image98420.jpg" width="571"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.11 – How service discovery and load balancing work in Docker Swarm</p>
<p>Unfortunately, there is no easy way to intercept this mechanism and replace it with custom behavior, but this would be needed to allow for a true blue-green deployment of <strong class="bold">Service B</strong>, which is the target service in our example. As we will see in <a href="B19199_17.xhtml#_idTextAnchor374"><em class="italic">Chapter 17</em></a>, <em class="italic">Deploying, Updating, and Securing an Application with Kubernetes</em>, Kubernetes is more flexible in this area.</p>
<p>That being said, we<a id="_idIndexMarker1303"/> can always deploy the public-facing services in a blue-green fashion. We can use the <strong class="bold">interlock 2</strong> product and its layer-7 routing mechanism to allow for a true blue-green deployment.</p>
<h2 id="_idParaDest-337"><a id="_idTextAnchor337"/>Canary releases</h2>
<p>Technically <a id="_idIndexMarker1304"/>speaking, rolling updates are a kind of canary release, but due to their lack of seams, where you can plug customized logic into the system, rolling updates are only a very limited version of canary releases.</p>
<p>True canary releases require us to have more fine-grained control over the update process. Also, true canary releases do not take down the old version of the service until 100% of the traffic has been funneled through the new version. In that regard, they are treated like blue-green deployments.</p>
<p>In a canary release scenario, we don’t just want to use things such as health checks as deciding factors regarding whether or not to funnel more and more traffic through the new version of the service; we also want to consider external input in the decision-making process, such as metrics that are collected and aggregated by a log aggregator or tracing information. An example that could be used as a decision-maker includes conformance<a id="_idIndexMarker1305"/> to <strong class="bold">Service Level Agreements</strong> (<strong class="bold">SLAs</strong>), namely whether the new version of the service shows response times that are outside of the tolerance band. This can happen if <a id="_idIndexMarker1306"/>we add new functionality to an existing service yet this new functionality degrades the response time.</p>
<p>Now that we know how to deploy an application causing zero downtime, we want to discuss how we can store configuration data used by the applications in the swarm.</p>
<h1 id="_idParaDest-338"><a id="_idTextAnchor338"/>Storing configuration data in the swarm</h1>
<p>If we want to store <a id="_idIndexMarker1307"/>non-sensitive data such as <a id="_idIndexMarker1308"/>configuration files in Docker Swarm, then we can use Docker configs. Docker configs are very similar to Docker secrets, which we will discuss in the next section. The main difference is that config values are not encrypted at rest, while secrets are. Like Docker secrets, Docker configs can only be used in Docker Swarm – that is, they cannot be used in your non-Swarm development environment. Docker configs are mounted directly into the container’s filesystem. Configuration values can either be strings or binary values up to a size of 500 KB.</p>
<p>With the use of Docker configs, you can separate the configuration from Docker images and containers. This way, your services can easily be configured with environment-specific values. The production swarm environment has different configuration values from the staging swarm, which in turn has different config values from the development or integration environment.</p>
<p>We can add configs to services and also remove them from running services. Configs can even be shared among different services running in the swarm.</p>
<p>Now, let’s create some Docker configs:</p>
<ol>
<li value="1">First, we start with a simple string value:<pre class="source-code">
$ echo "Hello world" | docker config create hello-config –</pre></li> </ol>
<p>Please note the hyphen at the end of the <code>docker config create</code> command. This means that Docker expects the value of the config from standard input. This is exactly what we’re doing by piping the <code>Hello world</code> value into the <code>create</code> command.</p>
<p>The preceding command results in an output like this:</p>
<pre class="source-code">
941xbaen80tdycup0wm01nspr</pre> <p>The preceding command creates a config named <code>hello-config</code> with the value “<code>Hello world</code>.” The output of this command is the unique ID of this new config<a id="_idIndexMarker1309"/> that’s<a id="_idIndexMarker1310"/> being stored in the swarm.</p>
<ol>
<li value="2">Let’s see what we got and use the <code>list</code> command to do so:<pre class="source-code">
$ docker config ls</pre></li> </ol>
<p>This will output the following (which has been shortened):</p>
<pre class="source-code">
ID       NAME        CREATED            UPDATEDrrin36..  hello-config  About a minute ago   About a minute ago</pre>
<p>The output of the <code>list</code> command shows the <code>ID</code> and <code>NAME</code> information for the config we just created, as well as its <code>CREATED</code> and (last) updated time. However, configs are non-confidential.</p>
<ol>
<li value="3">For that reason, we can do more and even output the content of a config, like so:<pre class="source-code">
$ docker config inspect hello-config</pre></li> </ol>
<p>The output looks like this:</p>
<pre class="source-code">
[    {
        "ID": "941xbaen80tdycup0wm01nspr",
        "Version": {
            "Index": 557
        },
        "CreatedAt": "2023-05-01T15:58:15.873515031Z",
        "UpdatedAt": "2023-05-01T15:58:15.873515031Z",
        "Spec": {
            "Name": "hello-config",
            "Labels": {},
            "Data": "SGVsbG8gd29ybGQK"
        }
    }
]</pre>
<p>Hmmm, interesting. In the <code>Spec</code> subnode of the preceding JSON-formatted output, we have the <code>Data</code> key with a value of <code>SGVsbG8gd29ybGQK</code>. Didn’t we just say <a id="_idIndexMarker1311"/>that <a id="_idIndexMarker1312"/>the config data is not encrypted at rest?</p>
<ol>
<li value="4">It turns out that the value is just our string encoded as <code>base64</code>, as we can easily verify:<pre class="source-code">
$ echo 'SGVsbG8gd29ybGQK' | base64 --decode</pre></li> </ol>
<p>We get the following:</p>
<pre class="source-code">
Hello world</pre> <p>So far, so good.</p>
<p>Now, let’s define a somewhat more complicated Docker config. Let’s assume we are developing a Java application. Java’s preferred way of passing configuration data to the application is the use of so-called <code>properties</code> files. A <code>properties</code> file is just a text file containing a list of key-value pairs. Let’s take a look:</p>
<ol>
<li value="1">Let’s create a file called <code>my-app.properties</code> and add the following content to it:<pre class="source-code">
username=pguserdatabase=productsport=5432dbhost=postgres.acme.com</pre></li> <li>Save the file and create a Docker config called <code>app.properties</code> from it:<pre class="source-code">
$ docker config create app.properties ./my-app.properties</pre></li> </ol>
<p>This gives us an output like this:</p>
<pre class="source-code">
2yzl73cg4cwny95hyft7fj80u</pre> <ol>
<li value="3">To prepare the next command, first, install the <code>jq</code> tool:<pre class="source-code">
$ sudo apt install –y jq</pre></li> <li>Now, we can<a id="_idIndexMarker1313"/> use<a id="_idIndexMarker1314"/> this (somewhat contrived) command to get the cleartext value of the config we just created:<pre class="source-code">
$ docker config inspect app.properties | jq .[].Spec.Data | xargs echo | base64 --decode</pre></li> </ol>
<p>We get this output:</p>
<pre class="source-code">
username=pguserdatabase=products
port=5432
dbhost=postgres.acme.com</pre>
<p>This is exactly what we expected.</p>
<ol>
<li value="5">Now, let’s create a Docker service that uses the preceding config. For simplicity, we will be using the <code>nginx</code> image to do so:<pre class="source-code">
$ docker service create \     --name nginx \    --config source=app.properties,target=/etc/myapp/conf/app.properties,mode=0440 \    nginx:1.13-alpine</pre></li> </ol>
<p>This results in an output similar to the following:</p>
<pre class="source-code">
svf9vmsjdttq4tx0cuy83hpgfoverall progress: 1 out of 1 tasks
1/1: running [==================================================&gt;]
verify: Service converged</pre>
<p>The interesting part in the preceding <code>service create</code> command is the line that contains the <code>--config</code> parameter. With this line, we’re telling Docker to use the config named <code>app.properties</code> and mount it as a file at <code>/etc/myapp/conf/app.properties</code> inside the container. Furthermore, we want that file to have <code>mode 0440</code> assigned <a id="_idIndexMarker1315"/>to <a id="_idIndexMarker1316"/>it to give the owner (root) and the group read permission.</p>
<p>Let’s see what we got:</p>
<pre class="console">
$ docker service ps nginxID   NAME     IMAGE              NODE            DESIRED STATE   CURRENT STATE           ERROR      PORTS
pvj  nginx.1  nginx:1.13-alpine  ip-172-31-32-21   Running  Running 2 minutes ago</pre>
<p>In the preceding output, we can see that the only instance of the service is running on node <code>ip-172-31-32-21</code>. On this node, I can now list the containers to get the ID of the <code>nginx</code> instance:</p>
<pre class="console">
$ docker container lsCONTAINER ID    IMAGE                COMMAND                 CREATED         STATUS               PORTS …
44417e1a70a1    nginx:1.13-alpine    "nginx -g 'daemon of…"   5 minutes ago   Up 5 minutes         80/tcp …</pre>
<p>Finally, we can <code>exec</code> into that container and output the value of the <code>/</code><code>etc/myapp/conf/app.properties</code> file:</p>
<pre class="console">
$ docker exec 44417 cat /etc/my-app/conf/app.properties</pre> <p>Note that <code>44417</code> in the preceding command represents the first part of the container hash.</p>
<p>This then will give us the expected values:</p>
<pre class="console">
username=pguserdatabase=products
port=5432
dbhost=postgres.acme.com</pre>
<p>No surprise here; this is exactly what we expected.</p>
<p>Docker configs can, of course, also be removed from the swarm, but only if they are not being used. If we try<a id="_idIndexMarker1317"/> to <a id="_idIndexMarker1318"/>remove the config we were just using previously, without first stopping and removing the service, we would get the following output:</p>
<pre class="console">
$ docker config rm app.properties</pre> <p>Oh no, that did not work, as we can see from the following output:</p>
<pre class="console">
Error response from daemon: rpc error: code = InvalidArgument desc = config 'app.properties' is in use by the following service: nginx</pre> <p>We get an error message in which Docker is nice enough to tell us that the config is being used by our service called nginx. This behavior is somewhat similar to what we are used to when working with Docker volumes.</p>
<p>Thus, first, we need to remove the service and then we can remove the config:</p>
<pre class="console">
$ docker service rm nginxnginx</pre>
<p>And now it should work:</p>
<pre class="console">
$ docker config rm app.propertiesapp.properties</pre>
<p>It is important to note once more that Docker configs should never be used to store confidential data such <a id="_idIndexMarker1319"/>as <a id="_idIndexMarker1320"/>secrets, passwords, or access keys and key secrets.</p>
<p>In the next section, we will discuss how to handle confidential data.</p>
<h1 id="_idParaDest-339"><a id="_idTextAnchor339"/>Protecting sensitive data with Docker secrets</h1>
<p>Secrets are used to<a id="_idIndexMarker1321"/> work with confidential data in a secure way. Swarm secrets are secure at rest and in transit. That is, when a new secret is created on a manager node, and it can only be created on a manager node, its value is encrypted and stored in the raft consensus storage. This is why it is secure at rest. If a service gets a secret assigned to it, then the manager reads the secret from storage, decrypts it, and forwards it to all the containers that are instances of the swarm service that requested the secret. Since node-to-node communication in Docker Swarm uses <code>tmpFS</code> into the container. By default, secrets are mounted into the container at <code>/run/secrets</code>, but you can change that to any custom folder.</p>
<p>It is important to note that secrets will not be encrypted on Windows nodes since there is no concept similar to <code>tmpfs</code>. To achieve the same level of security that you would get on a Linux node, the administrator should encrypt the disk of the respective Windows node.</p>
<h2 id="_idParaDest-340"><a id="_idTextAnchor340"/>Creating secrets</h2>
<p>First, let’s see<a id="_idIndexMarker1323"/> how we can actually create a secret:</p>
<pre class="console">
$ echo "sample secret value" | docker secret create sample-secret -</pre> <p>This command creates a secret called <code>sample-secret</code> with a value of <code>sample secret value</code>. Please note the hyphen at the end of the <code>docker secret create</code> command. This means that Docker expects the value of the secret from standard input. This is exactly what we’re doing by piping <code>sample secret value</code> into the <code>create</code> command.</p>
<p>Alternatively, we can use a file as the source for the secret value:</p>
<ol>
<li value="1">Create a <code>secret-value.txt</code> file as follows:<pre class="source-code">
$ echo "other secret" &gt; secret-value.txt</pre></li> <li>Create the Docker secret from this file with the following:<pre class="source-code">
$ docker secret create other-secret ./secret-value.txt</pre></li> </ol>
<p>Here, the value of the secret with the name <code>other-secret</code> is read from a file called <code>./secret-value.txt</code>.</p>
<ol>
<li value="3">Once a secret has been created, there is no way to access the value of it. We can, for example, list all our secrets to get the following output:</li>
</ol>
<div><div><img alt="Figure 15.12 – List of all secrets" height="119" src="img/Image98428.jpg" width="1215"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.12 – List of all secrets</p>
<p>In this list, we can only see the <code>ID</code> and <code>NAME</code> info for the secret, plus some other metadata, but the actual value of the secret is not visible.</p>
<ol>
<li value="4">We can also use <code>inspect</code> on a secret, for example, to get more information about <code>other-secret</code>:</li>
</ol>
<div><div><img alt="Figure 15.13 – Inspecting a swarm secret" height="448" src="img/Image98437.jpg" width="867"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.13 – Inspecting a swarm secret</p>
<p>Even here, we do not get the value of the secret back. This is, of course, intentional: a secret is a secret and thus needs to remain confidential. We can assign labels to secrets if we want and <a id="_idIndexMarker1324"/>we can even use a different driver to encrypt and decrypt the secret if we’re not happy with what Docker delivers out of the box.</p>
<h2 id="_idParaDest-341"><a id="_idTextAnchor341"/>Using a secret</h2>
<p>Secrets are <a id="_idIndexMarker1325"/>used by services that run in the swarm. Usually, secrets are assigned to a service at creation time. Thus, if we want to run a service called <code>web</code> and assign it a secret, say, <code>api-secret-key</code>, the syntax would look as follows:</p>
<pre class="console">
$ docker service create --name web \    --secret api-secret-key \
    --publish 8000:8000 \
    training/whoami:latest</pre>
<p>This command creates a service called <code>web</code> based on the <code>fundamentalsofdocker/whoami:latest</code> image, publishes the container port <code>8000</code> to port <code>8000</code> on all swarm nodes, and assigns it the secret called <code>api-secret-key</code>.</p>
<p>This will only work if the secret called <code>api-secret-key</code> is defined in the swarm; otherwise, an error will be generated with the following text:</p>
<pre class="console">
secret not found: api-secret-key.</pre> <p>Thus, let’s create this secret now:</p>
<pre class="console">
$ echo "my secret key" | docker secret create api-secret-key -</pre> <p>Now, if we rerun the <code>service create</code> command, it will succeed.</p>
<p>Now, we can use <code>docker service ps web</code> to find out on which node the sole service instance has been deployed, and then <code>exec</code> into this container. In my case, the instance has been deployed to node <code>ip-172-31-32-21</code>, which coincidentally happens to be the <code>manager1</code> EC2 instance on which I am already working. Otherwise, I would have to SSH into the other node first.</p>
<p>Then, I list all my containers on that node with <code>docker container ls</code> to find the one instance belonging to my service and copy its container ID. We can then run the following<a id="_idIndexMarker1326"/> command to make sure that the secret is indeed available inside the container under the expected filename containing the secret value in cleartext:</p>
<pre class="console">
$ docker exec -it &lt;container ID&gt; cat /run/secrets/api-secret-key</pre> <p>Once again, in my case, the output generated is as follows:</p>
<pre class="console">
my secret key</pre> <p>This is evidently what we expected. We can see the secret in cleartext.</p>
<p>If, for some reason, the default location where Docker mounts the secrets inside the container is not acceptable to you, you can define a custom location. In the following command, we mount the secret to <code>/app/my-secrets</code>:</p>
<pre class="console">
$ docker service create --name web \    --name web \
    -p 8000:8000 \
    --secret source=api-secret-key,target=/run/my-secrets/api-secret-key \
    fundamentalsofdocker/whoami:latest</pre>
<p>In this command, we <a id="_idIndexMarker1327"/>are using the extended syntax to define a secret that includes the destination folder.</p>
<h2 id="_idParaDest-342"><a id="_idTextAnchor342"/>Simulating secrets in a development environment</h2>
<p>When working<a id="_idIndexMarker1328"/> in development, we usually don’t have a local swarm on our machine. However, secrets only work in a swarm. So, what can we do? Well, luckily, this answer is really simple.</p>
<p>Since secrets are treated as files, we can easily mount a volume that contains the secrets into the container to the expected location, which by default is at <code>/run/secrets</code>.</p>
<p>Let’s assume that we have a folder called <code>./dev-secrets</code> on our local workstation. For each secret, we have a file named the same as the secret name and with the unencrypted value of the secret as the content of the file. For example, we can simulate a secret called <code>demo-secret</code> with a secret value of <code>demo secret value</code> by executing the following command on our workstation:</p>
<pre class="console">
$ echo "demo secret value" &gt; ./dev-secrets/sample-secret</pre> <p>Then, we can create a container that mounts this folder, like this:</p>
<pre class="console">
$ docker container run -d --name whoami \    -p 8000:8000 \
    -v $(pwd)/dev-secrets:/run/secrets \
    fundamentalsofdocker/whoami:latest</pre>
<p>The process running inside the container will be unable to distinguish these mounted files from the ones originating from a secret. So, for example, <code>demo-secret</code> is available as a file called <code>/run/secrets/demo-secret</code> inside the container and has the expected value, <code>demo secret value</code>. Let’s take a look at this in more detail in the following steps:</p>
<ol>
<li value="1">To test this, we can <code>exec</code> a shell inside the preceding container:<pre class="source-code">
$ docker container exec -it whoami /bin/bash</pre></li> <li>Now, we can navigate to the <code>/run/secrets</code> folder and display the content of the <code>demo-secret</code> file:<pre class="source-code">
/# cd /run/secrets/# cat demo-secretdemo secret value</pre></li> </ol>
<p>Next, we will <a id="_idIndexMarker1329"/>look at secrets and legacy applications.</p>
<h2 id="_idParaDest-343"><a id="_idTextAnchor343"/>Secrets and legacy applications</h2>
<p>Sometimes, we want<a id="_idIndexMarker1330"/> to containerize a legacy application that we cannot easily, or do not want to, change. This legacy application might expect a secret value to be available as an environment variable. How are we going to deal with this now? Docker presents us with the secrets as files, but the application is expecting them in the form of environment variables.</p>
<p>In this situation, it is helpful to define a script that runs when the container is started (a so-called entry point or startup script). This script will read the secret value from the respective file and define an environment variable with the same name as the file, assigning the new variable the value read from the file. In the case of a secret called <code>demo-secret</code> whose value should be available in an environment variable called <code>DEMO_SECRET</code>, the necessary code snippet in this startup script could look like this:</p>
<pre class="console">
export DEMO_SECRET=$(cat /run/secrets/demo-secret)</pre> <p>Similarly, let’s say we have a legacy application that expects the secret values to be present as an entry in, say, a YAML configuration file located in the <code>/app/bin</code> folder and called <code>app.config</code>, whose relevant part looks like this:</p>
<pre class="console">
…secrets:
demo-secret: "&lt;&lt;demo-secret-value&gt;&gt;"
other-secret: "&lt;&lt;other-secret-value&gt;&gt;"
yet-another-secret: "&lt;&lt;yet-another-secret-value&gt;&gt;"
…</pre>
<p>Our initialization script now needs to read the secret value from the secret file and replace the corresponding placeholder in the config file with the secret value. For <code>demo_secret</code>, this could look like this:</p>
<pre class="console">
file=/app/bin/app.confdemo_secret=$(cat /run/secret/demo-secret)
sed -i "s/&lt;&lt;demo-secret-value&gt;&gt;/$demo_secret/g" "$file"</pre>
<p>In the preceding snippet, we’re using the <code>sed</code> tool to replace a placeholder with a value in place. We can use the same technique for the other two secrets in the config file.</p>
<p>We put all the initialization logic into a file called <code>entrypoint.sh</code>, make this file executable and, for example, add it to the root of the container’s filesystem. Then, we define this file as <code>ENTRYPOINT</code> in the Dockerfile, or we can override the existing <code>ENTRYPOINT</code> of an image in the <code>docker container </code><code>run</code> command.</p>
<p>Let’s make a sample. Let’s assume that we have a legacy application running inside a container <a id="_idIndexMarker1331"/>defined by the <code>fundamentalsofdocker/whoami:latest</code> image that expects a secret called <code>db_password</code> to be defined in a file, <code>whoami.conf</code>, in the application folder.</p>
<p>Let’s take a look at these steps:</p>
<ol>
<li value="1">We can define a file, <code>whoami.conf</code>, on our local machine that contains the following content:<pre class="source-code">
database:    name: demo    db_password: "&lt;&lt;db_password_value&gt;&gt;"others:    val1=123    val2="hello world"</pre></li> </ol>
<p>The important part is line 3 of this snippet. It defines where the secret value has to be put by the startup script.</p>
<ol>
<li value="2">Let’s add a file called <code>entrypoint.sh</code> to the local folder that contains the following content:<pre class="source-code">
file=/app/whoami.confdb_pwd=$(cat /run/secret/db-password)sed -i "s/&lt;&lt;db_password_value&gt;&gt;/$db_pwd/g" "$file" /app/http</pre></li> </ol>
<p>The last line in the preceding script stems from the fact that this is the start command that was used in the original Dockerfile.</p>
<ol>
<li value="3">Now, change <a id="_idIndexMarker1332"/>the mode of this file to an executable:<pre class="source-code">
$ sudo chmod +x ./entrypoint.sh</pre></li> <li>Now, we define a Dockerfile that inherits from the <code>fundamentalsofdocker/whoami:latest</code> image. Add a file called <code>Dockerfile</code> to the current folder that contains the following content:<pre class="source-code">
FROM fundamentalsofdocker/whoami:latestCOPY ./whoami.conf /app/COPY ./entrypoint.sh /CMD ["/entrypoint.sh"]</pre></li> <li>Let’s build the image from this Dockerfile:<pre class="source-code">
$ docker image build -t secrets-demo:1.0 .</pre></li> <li>Once the image has been built, we can run a service from it, but before we can do that, we need to define the secret in Swarm:<pre class="source-code">
$ echo "passw0rD123" | docker secret create demo-secret -</pre></li> <li>Now, we can create a service that uses the following secret:<pre class="source-code">
$ docker service create --name demo \--secret demo-secret \secrets-demo:1.0</pre></li> </ol>
<h2 id="_idParaDest-344"><a id="_idTextAnchor344"/>Updating secrets</h2>
<p>At times, we<a id="_idIndexMarker1333"/> need to update a secret in a running service since secrets could be leaked out to the public or be stolen by malicious people, such as hackers. In this case, we need to change our confidential data since the moment it is leaked to a non-trusted entity, it has to be considered insecure.</p>
<p>Updating secrets, like any other update, requires zero downtime. Docker SwarmKit supports us in this regard.</p>
<p>First, we create a new secret in the swarm. It is recommended to use a versioning strategy when doing so. In our example, we use a version as a postfix of the secret name. We originally started with the secret named <code>db-password</code> and now the new version of this secret is called <code>db-password-v2</code>:</p>
<pre class="console">
$ echo "newPassw0rD" | docker secret create db-password-v2 -</pre> <p>Let’s assume that the original service that used the secret had been created like this:</p>
<pre class="console">
$ docker service create --name web \    --publish 80:80
    --secret db-password
    nginx:alpine</pre>
<p>The application running inside the container was able to access the secret at <code>/run/secrets/db-password</code>. Now, SwarmKit does not allow us to update an existing secret in a running service, so we have to remove the now obsolete version of the secret and then add the new one. Let’s start with removal with the following command:</p>
<pre class="console">
$ docker service update --secret-rm db-password web</pre> <p>Now, we can add the new secret with the following command:</p>
<pre class="console">
$ docker service update \    --secret-add source=db-password-v2,target=db-password \
    web</pre>
<p>Please note the<a id="_idIndexMarker1334"/> extended syntax of <code>--secret-add</code> with the source and target parameters.</p>
<h1 id="_idParaDest-345"><a id="_idTextAnchor345"/>Summary</h1>
<p>In this chapter, we introduced the routing mesh, which provides layer-4 routing and load balancing to a Docker Swarm. We then learned how SwarmKit allows us to update services without requiring downtime. Furthermore, we discussed the current limits of SwarmKit in regard to zero-downtime deployments. Then, we showed how to store configuration data in the Swarm, and in the last part of this chapter, we introduced secrets as a means to provide confidential data to services in a highly secure way.</p>
<p>In the next chapter, we will introduce the currently most popular container orchestrator, Kubernetes. We’ll discuss the objects that are used to define and run a distributed, resilient, robust, and highly available application in a Kubernetes cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that’s used to locally deploy a Kubernetes application, and also demonstrate the integration of Kubernetes with Docker Desktop.</p>
<h1 id="_idParaDest-346"><a id="_idTextAnchor346"/>Questions</h1>
<p>To assess your learning progress, please try to answer the following questions:</p>
<ol>
<li value="1">In a few simple sentences, explain to an interested lay person what zero-downtime deployment means.</li>
<li>How does SwarmKit achieve zero-downtime deployments?</li>
<li>Contrary to traditional (non-containerized) systems, why does a rollback in Docker Swarm just work? Explain this in a few short sentences.</li>
<li>Describe two to three characteristics of a Docker secret.</li>
<li>You need to roll out a new version of the inventory service. What does your command look like? Here is some more information:<ul><li>The new image is called <code>acme/inventory:2.1</code></li><li>We want to use a rolling update strategy with a batch size of two tasks</li><li>We want the system to wait for one minute after each batch</li></ul></li>
<li>You need to update an existing service named <code>inventory</code> with a new password that is provided through a Docker secret. The new secret is called <code>MYSQL_PASSWORD_V2</code>. The code in the service expects the secret to be called <code>MYSQL_PASSWORD</code>. What does the update command look like? (Note that we do not want the code of the service to be changed!)</li>
</ol>
<h1 id="_idParaDest-347"><a id="_idTextAnchor347"/>Answers</h1>
<p>Here are sample answers to the preceding questions:</p>
<ol>
<li value="1">Zero-downtime deployment means that a new version of a service in a distributed application is updated to a new version without the application needing to stop working. Usually, with Docker SwarmKit or Kubernetes (as we will see), this is done in a rolling fashion. A service consists of multiple instances and those are updated in batches so that the majority of the instances are up and running at all times.</li>
<li>By default, Docker SwarmKit uses a rolling updated strategy to achieve zero-downtime deployments.</li>
<li>Containers are self-contained units of deployment. If a new version of a service is deployed and does not work as expected, we (or the system) only need to roll back to the previous version. The previous version of the service is also deployed in the form of self-contained containers. Conceptually, there is no difference between rolling forward (an update) or backward (a rollback). One version of a container is replaced by another one. The host itself is not affected by such changes in any way.</li>
<li>Docker secrets are encrypted at rest. They are only transferred to the services and containers that use the secrets. Secrets are transferred encrypted due to the fact that the communication between swarm nodes uses mTLS. Secrets are never physically stored on a worker node.</li>
<li>The command to achieve this is as follows:<pre class="source-code">
$ docker service update \    --image acme/inventory:2.1 \    --update-parallelism 2 \    --update-delay 60s \    inventory</pre></li> <li>First, we need to remove the old secret from the service, and then we need to add the new version to it (directly updating a secret is not possible):<pre class="source-code">
$ docker service update \    --secret-rm MYSQL_PASSWORD \    inventory$ docker service update \    --secret-add source=MYSQL_PASSWORD_V2, target=MYSQL_PASSWORD \    inventory</pre></li> </ol>
</div>
</div>

<div><div><h1 id="_idParaDest-348"><a id="_idTextAnchor348"/>Part 4:Docker, Kubernetes, and the Cloud</h1>
<p>This part introduces the currently most popular container orchestrator. It introduces the core Kubernetes objects that are used to define and run a distributed, resilient, robust, and highly available application in a cluster. Finally, it introduces minikube as a way to locally deploy a Kubernetes application and also covers the integration of Kubernetes with Docker for Mac and Docker Desktop.</p>
<ul>
<li><a href="B19199_16.xhtml#_idTextAnchor349"><em class="italic">Chapter 16</em></a>, <em class="italic">Introducing Kubernetes</em></li>
<li><a href="B19199_17.xhtml#_idTextAnchor374"><em class="italic">Chapter 17</em></a>, <em class="italic">Deploying, Updating, and Securing an Application with Kubernetes</em></li>
<li><a href="B19199_18.xhtml#_idTextAnchor396"><em class="italic">Chapter 18</em></a>, <em class="italic">Running a Containerized Application in the Cloud</em></li>
<li><a href="B19199_19.xhtml#_idTextAnchor412"><em class="italic">Chapter 19</em></a>, <em class="italic">Monitoring and Troubleshooting an Application Running in Production</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</div></body></html>