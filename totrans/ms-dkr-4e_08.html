<html><head></head><body><div><div><p id="_idParaDest-143" class="chapter-number"><a id="_idTextAnchor230"/><em class="italic">Chapter 8</em></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor231"/>Clustering with Docker Swarm</h1>
			<p>In this chapter, we will be taking a look at Docker Swarm. With Docker Swarm, you can create and manage Docker clusters. Swarm can be used to distribute containers across multiple hosts and also has the ability to scale containers. </p>
			<p>We will cover the following topics:</p>
			<ul>
				<li>Introducing Docker Swarm</li>
				<li>Creating and managing a swarm</li>
				<li>Docker Swarm services and stacks</li>
				<li>Load balancing, overlays, and scheduling </li>
			</ul>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor232"/>Technical requirements</h1>
			<p>As in previous chapters, we will continue to use our local Docker installations. Again, the screenshots in this chapter will be from my preferred operating system, macOS. As before, the Docker commands we will be running will work on all three of the operating systems on which we have installed Docker so far. However, some of the supporting commands, which will be few and far between, may only apply to macOS- and Linux-based operating systems.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/334RE0A">https://bit.ly/334RE0A</a></p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor233"/><a id="_idTextAnchor234"/>Introducing Docker Swarm</h1>
			<p>Before we go any<a id="_idIndexMarker589"/> further, I should mention that there are two very different versions of Docker Swarm. There was a standalone version of Docker Swarm—this was supported up until Docker <code>1.12</code> and is no longer being actively developed; however, you may find some old documentation mentions it. Installation of the standalone Docker Swarm is not recommended as Docker ended support for version <code>1.11.x</code> in the first quarter of 2017.</p>
			<p>Docker version <code>1.12</code> introduced Docker Swarm mode. This introduced all of the functionality that was available in the standalone Docker Swarm version into the core Docker Engine, along with a significant number of additional features. As we are covering Docker 19.03 and higher in this book, we will be using Docker Swarm mode, which, for the remainder of the chapter, we will refer to as Docker Swarm.</p>
			<p>As you are already running a version of Docker with in-built support for Docker Swarm, there isn't anything you need to do in order to install Docker Swarm. You can verify that Docker Swarm is available on your installation by running the following command:</p>
			<pre>$ docker swarm --help</pre>
			<p>You should see something<a id="_idIndexMarker590"/> that looks like the following Terminal output when running the command:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.01_B15659.jpg" alt="Figure 8.1 – Viewing the help&#13;&#10;" width="1387" height="851"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Viewing the help</p>
			<p>If you get an error, ensure that you are running Docker 19.03 or higher, the installation of which we covered in <a href="B15659_01_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 1</em></a>, <em class="italic">Docker Overview</em>. Now that we know that our Docker client supports Docker Swarm, what do we mean by a Swarm?</p>
			<p>A <strong class="bold">Swarm</strong> is a collection of hosts, all running Docker, which have been set up to interact with each other in a clustered configuration. Once configured, you will be able to use all of the commands we have been running so far when targeting a single host, and let Docker Swarm decide the placement of your containers by using a deployment strategy to decide the most <a id="_idIndexMarker591"/>appropriate host on which to launch your container. Docker Swarms are made up of two types of host. Let's take a look at these now.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor235"/>Roles within a Docker Swarm cluster</h2>
			<p>Which roles are <a id="_idIndexMarker592"/>involved with Docker Swarm? Let's take a look at the two roles a host can assume when running within a Docker Swarm cluster.</p>
			<h3>Swarm manager</h3>
			<p>The <strong class="bold">Swarm manager</strong> is a host<a id="_idIndexMarker593"/> that is the central <a id="_idIndexMarker594"/>management point for all Swarm hosts. The Swarm manager is where you issue all your commands to control those nodes. You can switch between the nodes, join nodes, remove nodes, and manipulate those hosts.</p>
			<p>Each cluster can run several Swarm managers. For production, it is recommended that you run a minimum of five Swarm managers; this would mean that our cluster can take a maximum of two Swarm manager node failures before you start to encounter any errors. Swarm managers use the <em class="italic">Raft consensus algorithm</em> (see the <em class="italic">Further reading</em> section for more details) to maintain a consistent state across all of the manager nodes.</p>
			<h3>Swarm workers</h3>
			<p>The Swarm <a id="_idIndexMarker595"/>workers, which we have seen referred to earlier as Docker hosts, are those that run the Docker containers. Swarm workers are managed from the Swarm manager, and are depicted in the following diagram:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.02_B15659.jpg" alt="Figure 8.2 – An overview of Swarm workers&#13;&#10;" width="1037" height="539"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – An overview of Swarm workers</p>
			<p>This is an illustration of all the Docker Swarm components. We see that the Docker Swarm manager<a id="_idIndexMarker596"/> talks to each Swarm host that has a Docker Swarm worker role. The workers do have some level of connectivity, which we will look at sho<a id="_idTextAnchor236"/><a id="_idTextAnchor237"/>rtly.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor238"/>Creating and managing a Swarm</h1>
			<p>Let's now <a id="_idIndexMarker597"/>take<a id="_idIndexMarker598"/> a look at using Swarm and how we can perform the following tasks:</p>
			<ul>
				<li>Creating a cluster</li>
				<li>Joining workers</li>
				<li>Listing nodes</li>
				<li>Managing a cluster</li>
			</ul>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor239"/>Creating the cluster hosts</h2>
			<p>Let's start by <a id="_idIndexMarker599"/>creating a cluster of three machines. Since we are going to be creating a multi-node cluster on our local machine, we are going to use Multipass, which we covered in <a href="B15659_06_Final_JM_ePub.xhtml#_idTextAnchor187"><em class="italic">Chapter 6</em></a><em class="italic">, Docker Machine, Vagrant, and Multipass</em>, to launch the hosts by running the following commands:</p>
			<pre>$ multipass launch -n node1
$ multipass launch -n node2
$ multipass launch -n node3</pre>
			<p>This should give us three nodes; you can check this by just running the following command:</p>
			<pre>$ multipass list</pre>
			<p>You should see something similar to the following output:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.03_B15659.jpg" alt="Figure 8.3 – Launching the nodes using Multipass&#13;&#10;" width="1299" height="582"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Launching the nodes using Multipass</p>
			<p>You may remember from when we last used Multipass that bringing up a host doesn't mean that Docker is installed; so, now, let's install Docker and add the <code>ubuntu</code> user to the <code>docker</code> group so that when we use <code>multipass exec</code>, we don't have to change user. To do this, run the following three commands:</p>
			<pre>$ multipass exec node1 -- \
	/bin/bash -c 'curl -s https://get.docker.com | sh - &amp;&amp; sudo usermod -aG docker ubuntu'
$ multipass exec node2 -- \
	/bin/bash -c 'curl -s https://get.docker.com | sh - &amp;&amp; sudo usermod -aG docker ubuntu'
$ multipass exec node3 -- \
	/bin/bash -c 'curl -s https://get.docker.com | sh - &amp;&amp; sudo usermod -aG docker ubuntu'</pre>
			<p>Now that we have <a id="_idIndexMarker600"/>our three cluster nodes ready, we can move on to the next step, which is adding a Swarm manager to the cluster.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor240"/>Adding a Swarm manager to the cluster</h2>
			<p>Let's bootstrap <a id="_idIndexMarker601"/>our Swarm manager. To do this, we will pass the results of a few Docker Machine commands to our host. Before we create the Swarm manager, we need to get the IP address of <code>node1</code> as this is going to be our Swarm manager. If you are using macOS or Linux, then you can set an environment variable by running the following command:</p>
			<pre>$ IP=$(multipass info node1 | grep IPv4 | awk '{print $2}')</pre>
			<p>If you are using Windows 10, then run <code>multipass list</code> and make a note of the IP address for <code>node1</code>.</p>
			<p>The command to run in order to create our manager is shown in the following code snippet (if you are running Windows, replace <code>$IP</code> with the IP address you made a note of):</p>
			<pre>$ multipass exec node1 -- \
	/bin/bash -c 'docker swarm init --advertise-addr $IP:2377 --listen-addr $IP:2377'</pre>
			<p>You should receive a message similar to this one:</p>
			<pre>Swarm initialized: current node (92kts1c9x17gbqv3in9t1w4qm) is now a manager.
To add a worker to this swarm, run the following command:
    docker swarm join --token SWMTKN-1-4s8vpkileg2l2sicpyay5fojhis9jygb8mv04tsy9jmeqmzhk8-4cp4hp0i1qjqpuln6q3ytprtc 192.168.64.9:2377
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.</pre>
			<p>As you can see from <a id="_idIndexMarker602"/>the output, once your manager is initialized, you are given a unique token. </p>
			<p>In the preceding example, the full token is this:</p>
			<pre><code>SWMTKN-1-4s8vpkileg2l2sicpyay5fojhis9jygb8mv04tsy9jmeqmzhk8-4cp4hp0i1qjqpuln6q3ytprtc</code>.</pre>
			<p>This token will be needed for the worker nodes to authenticate themselves and join our cluster.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor241"/>Joining Swarm workers to the cluster</h2>
			<p>Now it is time to <a id="_idIndexMarker603"/>add our two workers (<code>node2</code> and <code>node3</code>) to the cluster. First, let's set an environment variable to hold our token, making sure that you replace the token with the one you received when initializing your own manager, as follows:</p>
			<pre>$ SWARM_TOKEN=$(multipass exec node1 -- /bin/bash -c 'docker swarm join-token --quiet worker')</pre>
			<p>Again, Windows users need to make a note of the token and will have to replace both <code>$SWARM_TOKEN</code> and <code>$IP</code> in the command shown next with their respective values. </p>
			<p>Now, we can run the following command to add <code>node2</code> to the cluster:</p>
			<pre>$ multipass exec node2 -- \
	/bin/bash -c 'docker swarm join --token $SWARM_TOKEN $IP:2377'</pre>
			<p>For <code>node3</code>, you need to run the following command:</p>
			<pre>$ multipass exec node3 -- \
	/bin/bash -c 'docker swarm join --token $SWARM_TOKEN $IP:2377'</pre>
			<p>Both times, you<a id="_idIndexMarker604"/> should get confirmation that your node has joined the cluster, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.04_B15659.jpg" alt="Figure 8.4 – Adding the workers to the cluster&#13;&#10;" width="1415" height="362"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Adding the workers to the cluster</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor242"/>Listing nodes</h2>
			<p>You can check the<a id="_idIndexMarker605"/> Swarm by running the following command:</p>
			<pre>$ multipass exec node1 -- /bin/bash -c 'docker node ls'</pre>
			<p>This will connect to <code>node1</code>, which we have configured as the Swarm master, and query all of the nodes that form our cluster. </p>
			<p>You should see that all three of our nodes are listed, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.05_B15659.jpg" alt="Figure 8.5 – Listing the cluster nodes&#13;&#10;" width="1638" height="484"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Listing the cluster nodes</p>
			<p>Now, we are going to <a id="_idIndexMarker606"/>move from the Docker client on our local machine to that on <code>node1</code>. To connect to the shell on <code>node1</code>, we just need to run the following command:</p>
			<pre>$ multipass shell node1</pre>
			<p>This will leave us at a prompt on <code>node1</code>, where we are ready to start using our cluster.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor243"/>Managing a cluster</h1>
			<p>Let's see how we can<a id="_idIndexMarker607"/> perform some management of all of these cluster nodes that we are creating.</p>
			<p>There are only two ways in which you can go about managing the containers within your cluster—these are by using the <code>docker service</code> and <code>docker stack</code> commands, which we are going to be covering in the next section of the chapter.</p>
			<p>Before we look at launching containers in our cluster, let's have a look at managing the cluster itself, starting with how you can find out more information on it.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor244"/>Finding information on the cluster</h2>
			<p>As we have already seen, we<a id="_idIndexMarker608"/> can list the nodes within the cluster using the Docker client installed on <code>node1</code>. To find out more information, we can simply type this to the command line of <code>node1</code>:</p>
			<pre>$ docker info</pre>
			<p>This will give us lots of information about the host, as you can see from the following output, which I have truncated:</p>
			<pre>Server:
 Containers: 0
 Images: 0
 Server Version: 19.03.8
 Swarm: active
  NodeID: 92kts1c9x17gbqv3in9t1w4qm
  Is Manager: true
  ClusterID: k65y4ke5rmup1n74z9lb9gerx
  Managers: 1
  Nodes: 3
  Default Address Pool: 10.0.0.0/8
  SubnetSize: 24</pre>
			<p>As you can see, there is information about the cluster in the <code>Swarm</code> section; however, we are only able to run the <code>docker info</code> command against the host with which our client is currently configured to communicate. Luckily, the <code>docker node</code> command is cluster-aware, so we can use that to get information on each node within our cluster.</p>
			<p class="callout-heading">TIP</p>
			<p class="callout">Assessing the <code>--pretty</code> flag with the <code>docker node inspect</code> command will render the output in the easy-to-read format you see next. If <code>--pretty</code> is left out, Docker will return the raw <code>JSON</code> object containing the results of the query the <code>inspect</code> command runs against the cluster.</p>
			<p>Here is what we would need to run to get information on <code>node1</code>:</p>
			<pre>$ docker node inspect node1 –pretty</pre>
			<p>This should provide <a id="_idIndexMarker609"/>the following information on our Swarm manager:</p>
			<pre>ID:			92kts1c9x17gbqv3in9t1w4qm
Hostname:              	node1
Joined at:             	2020-04-12 14:00:02.218330889 +0000 utc
Status:
 State:			Ready
 Availability:         	Active
 Address:		192.168.64.9
Manager Status:
 Address:		192.168.64.9:2377
 Raft Status:		Reachable
 Leader:		Yes
Platform:
 Operating System:	linux
 Architecture:		x86_64
Resources:
 CPUs:			1
 Memory:		985.7MiB
Plugins:
 Log:		awslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog
 Network:		bridge, host, ipvlan, macvlan, null, overlay
 Volume:		local
Engine Version:		19.03.8</pre>
			<p>Run the same command, but this time targeting one of the worker nodes, as follows:</p>
			<pre>$ docker node inspect node2 --pretty</pre>
			<p>This gives us similar<a id="_idIndexMarker610"/> information, as can be seen here:</p>
			<pre>ID:			x5qbl7j7qp07amffbps56p562
Hostname:              	node2
Joined at:             	2020-04-12 14:10:15.08034922 +0000 utc
Status:
 State:			Ready
 Availability:         	Active
 Address:		192.168.64.10
Platform:
 Operating System:	linux
 Architecture:		x86_64
Resources:
 CPUs:			1
 Memory:		985.7MiB
Plugins:
 Log:		awslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog
 Network:		bridge, host, ipvlan, macvlan, null, overlay
 Volume:		local
Engine Version:		19.03.8</pre>
			<p>But as you can see, information about the state of the manager functionality is missing. This is because the worker nodes do not need to know about the status of the manager nodes; they just need to know that they are allowed to receive instructions from the managers.</p>
			<p>In this way, we can see details about this host, such as the number of containers, the number of images <a id="_idIndexMarker611"/>on the host, and information about the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) and<a id="_idIndexMarker612"/> memory, along with other interesting information.</p>
			<p>Now that we know how to get information on the nodes that go to make up our cluster, let's take a look at how we can promote a node's role within the cluster.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor245"/>Promoting a worker node</h2>
			<p>Say you wanted to <a id="_idIndexMarker613"/>perform some maintenance on your single manager node, but you wanted to maintain the availability of your cluster. No problem—you can promote a worker node to a manager node.</p>
			<p>While we have our local three-node cluster up and running, let's promote <code>node2</code> to be a new manager. To do this, run the following command:</p>
			<pre>$ docker node promote node2</pre>
			<p>You should receive a message confirming that your node has been promoted immediately after executing the command, as follows:</p>
			<pre>Node node2 promoted to a manager in the swarm.</pre>
			<p>List the nodes by running this command:</p>
			<pre>$ docker node ls</pre>
			<p>This should show you that you now have two nodes that display something in the <code>MANAGER STATUS</code> column, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.06_B15659.jpg" alt="Figure 8.6 – Checking the status of the nodes in the cluster&#13;&#10;" width="1617" height="488"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Checking the status of the nodes in the cluster</p>
			<p>Our <code>node1</code> node is still the primary manager node, though. Let's look at doing something about <a id="_idIndexMarker614"/>that, and switch its role from manager to worker.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor246"/>Demoting a manager node</h2>
			<p>You may have <a id="_idIndexMarker615"/>already put two and two together, but to demote a manager node to a worker node, you simply need to run this command:</p>
			<pre>$ docker node demote node1</pre>
			<p>Again, you will receive immediate feedback stating the following:</p>
			<pre>Manager node1 demoted in the swarm.</pre>
			<p>Now that we have demoted our node, you can check the status of the nodes within the cluster by running this command:</p>
			<pre>$ docker node ls</pre>
			<p>As we are connected to <code>node1</code>, which is the newly demoted node, you will receive a message stating the following:</p>
			<pre>Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.</pre>
			<p>To connect to our new Swarm manager, we need to be SSHd into <code>node2</code>. To do this, we simply need to disconnect from <code>node1</code> and connect to <code>node2</code> by running the following:</p>
			<pre>$ exit
$ multipass shell node2</pre>
			<p>Now that are connected to a manager node again, rerun this, as follows:</p>
			<pre>$ docker node ls</pre>
			<p>It should list the nodes <a id="_idIndexMarker616"/>as expected, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.07_B15659.jpg" alt="Figure 8.7 – Checking the status of the nodes in the cluster&#13;&#10;" width="1573" height="494"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Checking the status of the nodes in the cluster</p>
			<p>This is all well and good, but how would we take a node out of the cluster so that we could perform maintenance on it? Let's now take a look at how we would drain a node.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor247"/>Draining a node</h2>
			<p>To temporarily<a id="_idIndexMarker617"/> remove a node from our cluster so that we can perform maintenance, we need to set the status of the node to <code>Drain</code>. Let's look at draining our former manager node. To do this, we need to run the following command:</p>
			<pre>$ docker node update --availability drain node1</pre>
			<p>This will stop any new tasks, such as new containers launching or being executed against the node we are draining. Once new tasks have been blocked, all running tasks will be migrated from the node we are draining to nodes with an <code>Active</code> status.</p>
			<p>As you can see from the following Terminal output, listing the nodes now shows that <code>node1</code> is listed with <a id="_idIndexMarker618"/>a status of <code>Drain</code> in the <code>AVAILABILITY</code> column:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.08_B15659.jpg" alt="Figure 8.8 – Checking the status of our cluster&#13;&#10;" width="1605" height="582"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Checking the status of our cluster</p>
			<p>Now that our node is no longer accepting new tasks and all running tasks have been migrated to our two remaining nodes, we can safely perform our maintenance, such as rebooting the host. To reboot <code>node1</code>, run the following two commands on your main host in a second window:</p>
			<pre>$ multipass shell node1
$ sudo reboot</pre>
			<p>Once the host has been rebooted, run this command on <code>node2</code>:</p>
			<pre>$ docker node ls</pre>
			<p>It should show that the node has an <code>AVAILABILITY</code> status of <code>Drain</code>. To add the node back into the cluster, simply change the <code>AVAILABILITY</code> status to <code>Active</code> by running the following on <code>node2</code>:</p>
			<pre>$ docker node update --availability active node1</pre>
			<p>As you can see from the following Terminal output, our node is now active, meaning new tasks can be executed against it:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.09_B15659.jpg" alt="Figure 8.9 – Checking the status of our cluster&#13;&#10;" width="1628" height="582"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Checking the status of our cluster</p>
			<p>Now that we have looked at how to create and manage a Docker Swarm cluster, we should look at how <a id="_idIndexMarker619"/>to run a task such as creating and scaling a service or launching a stack.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor248"/>Docker Swarm services and stacks</h1>
			<p>So far, we<a id="_idIndexMarker620"/> have <a id="_idIndexMarker621"/>looked at the following commands:</p>
			<pre>$ docker swarm &lt;command&gt;
$ docker node &lt;command&gt;</pre>
			<p>These two commands allow us to bootstrap and manage our Docker Swarm cluster from a collection of existing Docker hosts. The next two commands we are going to lo<a id="_idTextAnchor249"/><a id="_idTextAnchor250"/>ok at are as follows:</p>
			<pre>$ docker service &lt;command&gt;
$ docker stack &lt;command&gt;</pre>
			<p>The <code>service</code> and <code>stack</code> commands allow us to execute tasks that, in turn, launch, scale, and manage containers within our Swarm cluster.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor251"/>Services</h2>
			<p>The <code>service</code> command<a id="_idIndexMarker622"/> is a way of launching containers that take advantage of the Swarm cluster. Let's look at launching a really basic single-container service on our Swarm cluster. </p>
			<p class="callout-heading">TIP</p>
			<p class="callout">Don't forget that the <code>docker</code> commands here need to be executed from your current Swarm manager. If you are following, that should be <code>node2</code>.</p>
			<p>To do this, run the following command:</p>
			<pre>$ docker service create \
    --name cluster \
    --constraint 'node.role == worker' \
    -p:80:80/tcp \
    russmckendrick/cluster</pre>
			<p>This will create a service called <code>cluster</code> that consists of a single container with port <code>80</code> mapped from the container to the host machine, and it will only be running on nodes that have the role of <code>worker</code>.</p>
			<p>Before we look at doing more with the service, we can check whether it worked on our browser. To do this, we will need the IP address of our two worker nodes. First of all, we need to double-check which are the worker nodes by running this command:</p>
			<pre>$ docker node ls</pre>
			<p>Once we know which node has which role, you can find the IP addresses of your nodes by running this command in a second Terminal window on your host machine:</p>
			<pre>$ multipass list</pre>
			<p>Look at the following Terminal output:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.10_B15659.jpg" alt="Figure 8.10 – Creating a service and checking the IP addresses of the nodes&#13;&#10;" width="1650" height="1000"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Creating a service and checking the IP addresses of the nodes</p>
			<p>My worker nodes <a id="_idIndexMarker623"/>are <code>node1</code> and <code>node3</code>, whose IP addresses are <code>192.168.64.9</code> and <code>192.168.64.11</code>, respectively.</p>
			<p>Going to either of the IP addresses of your worker nodes, such as http://192.168.64.9/ or http://192.168.64.11/, in a browser will show the output of the <code>russmckendrick/cluster</code> application, which is the Docker Swarm graphic and the hostname of the container the page is being served from. This is illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.11_B15659.jpg" alt="Figure 8.11 – Our cluster application&#13;&#10;" width="1481" height="982"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – Our cluster application</p>
			<p>Now that we have our<a id="_idIndexMarker624"/> service running on our cluster, we can start to find out more information about it. First of all, we can list the services again by running this command:</p>
			<pre>$ docker service ls</pre>
			<p>In our case, this should return the single service we launched, called <code>cluster</code>, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.12_B15659.jpg" alt="Figure 8.12 – Listing the services&#13;&#10;" width="1650" height="317"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Listing the services</p>
			<p>As you can see, it <a id="_idIndexMarker625"/>is a <code>inspect</code> command, as follows:</p>
			<pre>$ docker service inspect cluster --pretty</pre>
			<p>This will return detailed information about the service, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.13_B15659.jpg" alt="Figure 8.13 – Grabbing information on a service&#13;&#10;" width="1650" height="1235"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Grabbing information on a service</p>
			<p>You may have noticed <a id="_idIndexMarker626"/>that so far, we haven't had to bother about which of our two worker nodes the service is currently running on. This is quite an important feature of Docker Swarm, as it completely removes the need for you to worry about the placement of individual containers.</p>
			<p>Before we look at scaling our service, we can take a quick look at which host our single container is running on by executing these commands:</p>
			<pre>$ docker node ps
$ docker node ps node1
$ docker node ps node3</pre>
			<p>This will list the containers running on each of our hosts. By default, it will list the host the command is being targeted against, which in my case is <code>node1</code>, as illustrated in the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.14_B15659.jpg" alt="Figure 8.14 – Finding the node our service is running on&#13;&#10;" width="1650" height="475"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Finding the node our service is running on</p>
			<p>Let's look at scaling our <a id="_idIndexMarker627"/>service to six instances of our application container. Run the following commands to scale and check our service:</p>
			<pre>$ docker service scale cluster=6
$ docker service ls
$ docker node ps node1
$ docker node ps node3</pre>
			<p>We are only checking two of the nodes since we originally told our service to launch on worker nodes. As you can see from the following Terminal output, we now have three containers running on each of our worker nodes:</p>
			<p> </p>
			<div><div><img src="img/Figure_8.15_B15659.jpg" alt="Figure 8.15 – Checking the distribution of containers on our nodes&#13;&#10;" width="1642" height="1286"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Checking the distribution of containers on our nodes</p>
			<p>Before we move on to <a id="_idIndexMarker628"/>look at stacks, let's remove our service. To do this, run the following command:</p>
			<pre>$ docker service rm cluster</pre>
			<p>This will remove all of the containers, while leaving the downloaded image on the hosts.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor252"/>Stacks</h2>
			<p>It is more than possible <a id="_idIndexMarker629"/>to create quite complex, highly available multi-container applications using Swarm services. In a non-Swarm cluster, manually launching each set of containers for a part of the application can start to become a little laborious and also makes it difficult to share services. To this end, Docker has created a functionality that allows you to define your services in Docker Compose files.</p>
			<p>The following Docker Compose file, which is named <code>docker-compose.yml</code>, will create the same service we launched in the <code>services</code> section:</p>
			<pre>version: '3'
services:
  cluster:
    image: russmckendrick/cluster
    ports:
      - '80:80'
    deploy:
      replicas: 6
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == worker</pre>
			<p>As you can see, the stack can be made up of multiple services, each defined under the <code>services</code> section of the Docker Compose file.</p>
			<p>In addition to the normal Docker Compose commands, you can add a <code>deploy</code> section; this is where you define everything relating to the Swarm element of your stack.</p>
			<p>In the previous example, we said we would like six replicas, which should be distributed across our two worker nodes. Also, we updated the default restart policy, which you saw when we inspected the service from the previous section, and it showed up as <code>paused</code>, so that if a container becomes unresponsive, it is always restarted.</p>
			<p>To launch our stack, copy the previous content into the <code>docker-compose.yml</code> file, and then run the following command:</p>
			<pre>$ docker stack deploy --compose-file=docker-compose.yml cluster</pre>
			<p>Docker will—as when launching containers with Docker Compose—create a new network and then launch <a id="_idIndexMarker630"/>your services on it. You can check the status of your stack by running this command:</p>
			<pre>$ docker stack ls</pre>
			<p>This will show that a single service has been created. You can get details of the service created by the <code>stack</code> by running this command:</p>
			<pre>$ docker stack services cluster</pre>
			<p>Finally, running the following command will show where the containers within the stack are running:</p>
			<pre>$ docker stack ps cluster</pre>
			<p>Take a look at the following Terminal output:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.16_B15659.jpg" alt="Figure 8.16 – Deploying our stack&#13;&#10;" width="1650" height="1011"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – Deploying our stack</p>
			<p>Again, you will be able<a id="_idIndexMarker631"/> to access the stack using the IP addresses of your nodes, and you will be routed to one of the running containers. To remove a stack, simply run this command:</p>
			<pre>$ docker stack rm cluster</pre>
			<p>This will remove all services and networks created by the stack when it is launched.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor253"/>Deleting a Swarm cluster</h2>
			<p>Before moving on, as <a id="_idIndexMarker632"/>we no longer require it for the next section, you can delete your Swarm cluster by running the following command:</p>
			<pre>$ multipass delete --purge node1
$ multipass delete --purge node2
$ multipass delete --purge node3</pre>
			<p>Should you need to relaunch the Swarm cluster for any reason, simply follow the instructions from<a id="_idIndexMarker633"/> the start of the chapter to recreate a cluster.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor254"/>Load balancing, overlays, and scheduling</h1>
			<p>In the last few sections, we looked at launching services and stacks. To access the applications we launched, we were able to use any of the host IP addresses in our cluster; how was this possible?</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor255"/>Ingress load balancing</h2>
			<p>Docker Swarm <a id="_idIndexMarker634"/>has an ingress load balancer built in, making<a id="_idIndexMarker635"/> it easy to distribute traffic to our public-facing containers.</p>
			<p>This means that you can expose applications within your Swarm cluster to services—for example, an external load balancer <a id="_idIndexMarker636"/>such as Amazon <strong class="bold">Elastic Load Balancer</strong> (<strong class="bold">ELB</strong>)—knowing that your request will be routed to the correct container(s) no matter which host happens to be currently hosting it, as demonstrated by the following diagram:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.17_B15659.jpg" alt="Figure 8.17 – An overview of load balancing in a Swarm cluster&#13;&#10;" width="1153" height="559"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.17 – An overview of load balancing in a Swarm cluster</p>
			<p>This means that our application can be scaled up or down, fail, or be updated, all without the need<a id="_idIndexMarker637"/> to have the external load balancer reconfigured to talk to the individual containers, as Docker Swarm is handling that for us.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor256"/>Network overlays</h2>
			<p>In our example, we<a id="_idIndexMarker638"/> launched a simple service running a single<a id="_idIndexMarker639"/> application. Say we wanted to add a database layer in our application, which is typically a fixed point within the network; how could we do this?</p>
			<p>Docker Swarm's network overlay layer extends the network you launch your containers in across multiple hosts, meaning that each service or stack can be launched in its own isolated network. This means that our database container, running MongoDB, will be accessible to all other containers running on the same overlay network on port <code>27017</code>, no matter which of the hosts the containers are running on.</p>
			<p>You may be thinking to yourself: <em class="italic">Hang on a minute. Does this mean I have to hardcode an IP address into my application's configuration?</em> Well, that wouldn't fit well with the problems Docker Swarm is trying to resolve, so no, you don't.</p>
			<p>Each overlay network<a id="_idIndexMarker640"/> has its own inbuilt <code>mongodb:27017</code>, and<a id="_idIndexMarker642"/> it will connect to our MongoDB container.</p>
			<p>This will make our diagram appear as follows:</p>
			<p class="figure-caption"> </p>
			<div><div><img src="img/Figure_8.18_B15659.jpg" alt="Figure 8.18 – An overview of overlay networks in a Docker Swarm cluster&#13;&#10;" width="1221" height="695"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – An overview of overlay networks in a Docker Swarm cluster</p>
			<p>There are some other considerations you will need to take into account when adopting this pattern, but we will cover those in <a href="B15659_15_Final_JM_ePub.xhtml#_idTextAnchor823"><em class="italic">Chapter 15</em></a><em class="italic">,</em> <em class="italic">Docker Workflows</em>.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor257"/>Scheduling</h2>
			<p>At the time of writing, there<a id="_idIndexMarker643"/> is only a single scheduling strategy available within Docker Swarm, called <strong class="bold">spread</strong>. What this<a id="_idIndexMarker644"/> strategy does is schedule tasks to be run against the least loaded node that meets any of the constraints you defined when launching the service or stack. For the most part, you should not need to add too many constraints to your services.</p>
			<p>One feature that is not currently supported by Docker Swarm is affinity and anti-affinity rules. While it is easy to get around using this constraint, I urge you not to overcomplicate things, as it is very easy to end up overloading hosts or creating single points of failure if you put too many constraints in place when defining your services.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor258"/>Summary</h1>
			<p>In this chapter, we explored Docker Swarm. We took a look at how to install Docker Swarm and the Docker Swarm components that make up Docker Swarm. We took a look at how to use Docker Swarm, joining, listing, and managing Swarm manager and worker nodes. We reviewed the <code>service</code> and <code>stack</code> commands and how to use them, and spoke about the Swarm inbuilt ingress load balancer, overlay networks, and scheduler.</p>
			<p>Now, Docker Swarm is in an interesting state at the time of writing, as Docker Swarm was one of the technologies acquired by Mirantis as part of the Docker Enterprise sale, and while Mirantis have said that they will offer support for existing Docker Swarm clusters for 2 years (that was in November 2019), they haven't given much information on the future of Docker Swarm.</p>
			<p>This isn't surprising as Mirantis do a lot of work with another container cluster called Kubernetes, which we are going to be looking at in <a href="B15659_11_Final_JM_ePub.xhtml#_idTextAnchor294"><em class="italic">Chapter 11</em></a>, <em class="italic">Docker and Kubernetes</em>. Before then, in the next chapter, we are going to take a look at a <strong class="bold">graphical user interface</strong> (<strong class="bold">GUI</strong>) for Docker called <strong class="bold">Portainer</strong>.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor259"/>Questions</h1>
			<ol>
				<li>True or false: You should be running your Docker Swarm using the standalone Docker Swarm rather than the in-built Docker Swarm mode.</li>
				<li>Which two things do you need after initiating your Docker Swarm manager to add your workers to your Docker Swarm cluster?</li>
				<li>Which command would you use to find out the status of each of the nodes within your Docker Swarm cluster?</li>
				<li>Which flag would you add to <code>docker node inspect</code> on Swarm manager to make it more readable?</li>
				<li>How do you promote a node to be a manager?</li>
				<li>Which command can you use to scale your service?</li>
			</ol>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor260"/>Further reading</h1>
			<p>For a detailed explanation of the Raft consensus algorithm, I recommend working through the excellent presentation entitled <em class="italic">The Secret Lives of Data</em>, which can be found at <a href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a>. It explains all the processes taking place in the background on the manager nodes via an easy-to-follow animation.</p>
		</div>
	</div>



  </body></html>