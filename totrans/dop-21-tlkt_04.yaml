- en: Service Discovery inside a Swarm Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm 集群中的服务发现
- en: It does not take much strength to do things, but it requires a great deal of
    strength to decide what to do.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 做事并不需要多大力气，但决定做什么却需要极大的力量。
- en: -Elbert Hubbard
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 埃尔伯特·哈伯德'
- en: If you used the old Swarm, the one shipped as a standalone product before *Docker
    1.12*, you were forced to set up a service registry alongside it. You might have
    chosen Consul, etcd, or Zookeper. The standalone Swarm could not work without
    one of them. Why is that? What was the reason for such a strong dependency?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用过旧版的 Swarm，即 *Docker 1.12* 之前作为独立产品发布的版本，你必须为其设置一个服务注册表。你可能选择了 Consul、etcd
    或 Zookeeper。没有它们，独立版的 Swarm 无法工作。为什么会这样呢？为什么会有这么强的依赖关系？
- en: Before we discuss reasons behind using an external service registry with the
    old Swarm, let's discuss how would Swarm behave without it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论使用外部服务注册表的原因之前，先来讨论一下没有它的情况下，Swarm 会是什么表现。
- en: What would Docker Swarm look like without?
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果没有服务注册表，Docker Swarm 会是什么样子？
- en: Let's say we have a cluster with three nodes. Two of them run **Swarm managers**,
    and one is a worker. Managers accept our requests, decide what should be done,
    and send tasks to **Swarm workers**. In turn, workers translate those tasks into
    commands that are sent to the local **Docker Engine**. Managers act as workers
    as well.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由三个节点组成的集群。其中两个节点运行 **Swarm 管理器**，一个节点是工作节点。管理器接受我们的请求，决定应该做什么，并将任务发送给
    **Swarm 工作节点**。反过来，工作节点将这些任务转换为命令，并将其发送到本地的 **Docker Engine**。管理器本身也充当工作节点。
- en: If we describe the flow we did earlier with the `go-demo` service, and imagine
    there is no service discovery associated with Swarm, it would be as follows.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用之前的 `go-demo` 服务描述流程，并假设 Swarm 没有服务发现功能，情况会是这样的：
- en: 'A user sends a request to one of the managers. The request is not a declarative
    instruction but an expression of the desired state. For example, I want to have
    two instances of the `go-demo` service and one instance of the `DB` running inside
    the cluster:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 用户向其中一个管理器发送请求。这个请求不是一个声明式指令，而是对期望状态的表达。例如，我希望在集群中运行两个 `go-demo` 服务实例和一个 `DB`
    实例：
- en: '![](img/swarm-without-service-registry-user.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-without-service-registry-user.png)'
- en: 'Figure 4-1: User sends a request to one of the managers'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-1：用户向其中一个管理器发送请求
- en: 'Once **Swarm manager** receives our request for the desired state, it compares
    it with the current state of the cluster, generates tasks, and sends them to **Swarm
    workers**. The tasks might be to run an instance of the `go-demo` service on **node-1**
    and **node-2**, and an instance of the `go-demo-db` service on **node-3**:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 **Swarm 管理器** 接收到我们对期望状态的请求，它会将其与集群的当前状态进行比较，生成任务并将其发送给 **Swarm 工作节点**。这些任务可能是将
    `go-demo` 服务的一个实例运行在 **node-1** 和 **node-2** 上，并将 `go-demo-db` 服务的一个实例运行在 **node-3**
    上：
- en: '![](img/swarm-without-service-registry-manager.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-without-service-registry-manager.png)'
- en: 'Figure 4-2: Swarm manager compares the current state of the cluster with the
    desired state, generates tasks, and sends them to Swarm workers.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-2：Swarm 管理器将集群的当前状态与期望状态进行比较，生成任务并将其发送到 Swarm 工作节点。
- en: '**Swarm workers** receive tasks from the managers, translate them into Docker
    Engine commands, and send them to their local **Docker Engine** instances:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Swarm 工作节点** 接收来自管理器的任务，将其转换为 Docker Engine 命令，并将其发送到本地的 **Docker Engine**
    实例：'
- en: '![](img/swarm-without-service-registry-node.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-without-service-registry-node.png)'
- en: 'Figure 4-3: Swarm nodes translate received tasks to Docker Engine commands'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-3：Swarm 节点将接收到的任务转换为 Docker Engine 命令
- en: 'Docker Engine receives a command from the Swarm worker and executes it:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Engine 接收到来自 Swarm 工作节点的命令并执行：
- en: '![](img/swarm-without-service-registry-engine.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-without-service-registry-engine.png)'
- en: 'Figure 4-4: Docker Engine manages local containers.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-4：Docker Engine 管理本地容器。
- en: 'Next, let''s say that we send a new desired state to the manager. For example,
    we might want to scale the number of the **go-demo** instances to **node-3**.
    We would send a request to the **Swarm manager** on **node-1**, it would consult
    the cluster state it stored internally and make a decision to, for example, run
    a new instance on **node-2**. Once the decision is made, the manager would create
    a new task and send it to the **Swarm worker** on **node-2**. In turn, the worker
    would translate the task into a Docker command, and send it to the local engine.
    Once the command is executed, we would have the third instance of the **go-demo**
    service running on **node-2**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设我们向管理器发送一个新的期望状态。例如，我们可能想要将**go-demo**实例的数量扩展到**node-3**。我们将向**node-1**上的**Swarm
    管理器**发送请求，它会查询内部存储的集群状态，并做出决定，比如在**node-2**上运行一个新实例。一旦做出决定，管理器会创建一个新任务并将其发送到**node-2**上的**Swarm
    worker**。然后，worker 会将任务转换为 Docker 命令，并发送到本地引擎。命令执行后，我们将在**node-2**上运行第三个**go-demo**服务实例：
- en: '![](img/swarm-without-service-registry-scale.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-without-service-registry-scale.png)'
- en: 'Figure 4-5: A scale request is sent to the Swarm manager'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-5：扩展请求被发送到 Swarm 管理器
- en: If the flow were as described, we would have quite a lot of problems that would
    make such a solution almost useless.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果流程按照描述进行，我们将面临许多问题，这将使这种解决方案几乎毫无用处。
- en: Let's try to list some of the issues we would face.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着列举一些我们可能面临的问题。
- en: A Docker manager uses the information we sent to it. That would work as long
    as we always use the same manager and the state of the cluster does not change
    due to factors outside the control of the manager. The important thing to understand
    is that the information about the cluster is not stored in one place, nor it is
    complete. Each manager knows only about the things it did. Why is that such a
    problem?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 管理器使用我们发送给它的信息。这在我们始终使用相同的管理器并且集群状态未因管理器控制之外的因素而变化时有效。需要理解的关键是，关于集群的信息并非存储在一个地方，也并不完整。每个管理器只知道它自己做过的事情。这为什么会是个问题呢？
- en: Let's explore a few alternative (but not uncommon) paths.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些替代的（但并不罕见的）路径。
- en: What would happen if we sent the request to scale to three instances to the
    manager on **node-2**? That manager would be oblivious of the tasks created by
    the manager in **node-1**. As a result, it would try to run three new instances
    of the **go-demo** service resulting in five instances in total. We’d have two
    instances created by the manager in **node-1** and three by the manager in **node-2**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向**node-2**上的管理器发送扩展到三个实例的请求，会发生什么？该管理器对**node-1**上创建的任务一无所知。因此，它会尝试运行三个新的**go-demo**服务实例，导致集群中总共有五个实例。我们将有两个实例由**node-1**上的管理器创建，三个实例由**node-2**上的管理器创建。
- en: It would be tempting always to use the same manager, but, in that case, we would
    have a single point of failure. What would happen if the whole **node-1** fails?
    We would have no managers available or would be forced to use the manager on **node-2**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然始终使用相同的管理器看起来很有吸引力，但在这种情况下，我们将面临单点故障的问题。如果整个**node-1**故障会发生什么？我们将无法使用管理器，或者只能强制使用**node-2**上的管理器。
- en: Many other factors might produce such discrepancies. Maybe one of the containers
    stopped unexpectedly. In such a case, when we decide to scale to three instances,
    the manager on **node-1** would think that two instances are running and would
    create a task to run one more. However, that would not result in three but two
    instances running inside the cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有许多其他因素会导致这种差异。也许其中一个容器意外停止。在这种情况下，当我们决定扩展到三个实例时，**node-1**上的管理器会认为有两个实例在运行，并会创建一个任务来运行另一个实例。然而，这样做不会导致集群中运行三个实例，而是两个实例。
- en: The list of things that might go wrong is infinite, and we won't go into more
    examples.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可能出错的情况是无穷无尽的，我们不会再举更多的例子。
- en: The important thing to note is that it is unacceptable for any single manager
    to be stateful in isolation. Every manager needs to have the same information
    as any other. On the other hand, every node needs to monitor events generated
    by Docker Engine and make sure that any change to its server is propagated to
    all managers. Finally, we need to oversee the state of each server in case one
    of them fails. In other words, each manager needs to have an up-to-date picture
    of the entire cluster. Only then it can translate our requests for the desired
    state into tasks that will be dispatched to the Swarm nodes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的重要事项是，任何单一的管理器都不应当在孤立状态下保持有状态。每个管理器都需要拥有与其他管理器相同的信息。另一方面，每个节点需要监控Docker引擎生成的事件，并确保任何对其服务器的更改都会传播到所有管理器。最后，我们需要监督每台服务器的状态，以防其中一台出现故障。换句话说，每个管理器都需要有整个集群的最新状态。只有这样，它才能将我们请求的目标状态转化为任务，并将任务分发给Swarm节点。
- en: How can all the managers have a complete view of the whole cluster no matter
    who made a change to it?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确保所有管理器都能够全面了解整个集群的状态，无论是谁对其进行了更改？
- en: The answer to that question depends on the requirements we set. We need a place
    where all the information is stored. Such a place need to be distributed so that
    the failure of one server does not affect the correct functioning of the tool.
    Being distributed provides fault tolerance, but that, by itself, does not mean
    data is synchronized across the cluster. The tool needs to maintain data replicated
    across all the instances. Replication is not anything new except that, in this
    case, it needs to be very fast so that the services that would consult it can
    receive data in (near) real-time. Moreover, we need a system that will monitor
    each server inside the cluster and update the data if anything changes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案取决于我们设定的要求。我们需要一个存储所有信息的地方。这个地方需要是分布式的，以便一个服务器的故障不会影响工具的正常运行。分布式提供了容错能力，但仅此并不意味着数据会在集群中同步。该工具需要保持数据在所有实例之间的复制。复制并不是什么新鲜事，唯一不同的是，在这种情况下，它需要非常快速，以便咨询它的服务能够实时（或接近实时）接收数据。此外，我们还需要一个系统来监控集群内的每个服务器，并在任何变化发生时更新数据。
- en: To summarize, we need a distributed service registry and a monitoring system
    in place. The first requirement is best accomplished with one of the service registries
    or key-value stores. The old Swarm (standalone version before Docker 1.12) supports
    *Consul* ([https://www.consul.io/](https://www.consul.io/)), *etcd* ([https://github.com/coreos/etcd](https://github.com/coreos/etcd)),
    and *Zookeeper* ([https://zookeeper.apache.org/](https://zookeeper.apache.org/)).
    My preference is towards Consul, but any of the three should do.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们需要一个分布式的服务注册表和监控系统。第一个需求最好通过使用服务注册表或键值存储来实现。旧版Swarm（Docker 1.12之前的独立版）支持*Consul*
    ([https://www.consul.io/](https://www.consul.io/))、*etcd* ([https://github.com/coreos/etcd](https://github.com/coreos/etcd))
    和 *Zookeeper* ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))。我个人偏好Consul，但三者中的任何一个都可以。
- en: 'For a more detailed discussion about service discovery and the comparison of
    the major service registries, please consult the service discovery: The Key to
    Distributed services chapter of *The DevOps 2.0 Toolkit*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于服务发现以及主要服务注册表比较的更详细讨论，请参考《DevOps 2.0工具包》中的《服务发现：分布式服务的关键》章节。
- en: What does standalone Docker Swarm look like with service discovery?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立版Docker Swarm在服务发现方面是什么样子的？
- en: Now that we have a better understanding of the requirements and the reasons
    behind the usage of service discovery, we can define the (real) flow of a request
    to a Docker Swarm manager.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对需求和使用服务发现的原因有了更清晰的理解，我们可以定义请求到Docker Swarm管理器的（实际）流程。
- en: 'Please note that we are still exploring how the old (standalone) Swarm is working:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们仍在探索旧版（独立版）Swarm的工作原理：
- en: A user sends a request with the desired state to one of the Swarm managers.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户向其中一台Swarm管理器发送带有目标状态的请求。
- en: The Swarm manager gets the cluster information from the service registry, creates
    a set of tasks, and dispatches them to Swarm workers.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Swarm管理器从服务注册表中获取集群信息，创建一组任务，并将其分发给Swarm工作节点。
- en: Swarm workers translate the tasks into commands and send them to the local Docker
    Engine which, in turn, runs or stops containers**.**
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Swarm工作节点将任务转换为命令并发送到本地的Docker引擎，后者随后运行或停止容器**。**
- en: Swarm workers continuously monitor Docker events and update the **service registry**.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Swarm工作节点持续监控Docker事件，并更新**服务注册表**。
- en: 'That way, information about the whole cluster is always up-to-date. The exception
    is when one of the managers or workers fails. Since managers are monitoring each
    other, the failure of a manager or a worker is considered a failure of the whole
    node. After all, without a worker, containers cannot be scheduled on that node:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，整个集群的信息始终是最新的。例外情况是当某个管理节点或工作节点失败时。由于管理节点相互监控，一个管理节点或工作节点的失败被视为整个节点的失败。毕竟，没了工作节点，容器就无法在该节点上调度：
- en: '![](img/swarm-standalone.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/swarm-standalone.png)'
- en: 'Figure 4-6: Docker Swarm (standalone) flow'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-6：Docker Swarm（独立）流程
- en: Now that we established that service discovery is an essential tool for managing
    a cluster, the natural question is what happened to it in Swarm Mode (*Docker
    1.12*)?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定服务发现是管理集群的必备工具，那么接下来自然的问题是它在 Swarm 模式（*Docker 1.12*）中发生了什么变化？
- en: Service discovery in the Swarm cluster
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm 集群中的服务发现
- en: The old (standalone) Swarm required a service registry so that all its managers
    can have the same view of the cluster state. When instantiating the old Swarm
    nodes, we had to specify the address of a service registry. However, if you take
    a look at setup instructions of the new Swarm (Swarm Mode introduced in *Docker
    1.12*), you'll notice that we did not set up anything beyond Docker Engines. You
    will not find any mention of an external service registry or a key-value store.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 旧版（独立）Swarm 需要一个服务注册中心，以便所有管理节点都能看到相同的集群状态。在实例化旧版 Swarm 节点时，我们必须指定服务注册中心的地址。然而，如果你查看新
    Swarm（在 *Docker 1.12* 中引入的 Swarm 模式）的设置说明，你会注意到我们没有设置除了 Docker 引擎以外的任何东西。你不会发现有提到外部服务注册中心或键值存储。
- en: Does that mean that Swarm does not need service discovery? Quite the contrary.
    The need for service discovery is as strong as ever, and Docker decided to incorporate
    it inside Docker Engine. It is bundled inside just as Swarm is. The internal process
    is, essentially, still very similar to the one used by the standalone Swarm, only
    with less moving parts. Docker Engine now acts as a Swarm manager, Swarm worker,
    and service registry.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着 Swarm 不需要服务发现？恰恰相反。对服务发现的需求依旧强烈，Docker 决定将其集成到 Docker 引擎中。它与 Swarm 一样被捆绑在内部。其内部过程本质上仍然与独立的
    Swarm 使用的方式非常相似，只是部件更少。现在，Docker 引擎充当着 Swarm 管理节点、Swarm 工作节点和服务注册中心的角色。
- en: The decision to bundle everything inside the engine provoked a mixed response.
    Some thought that such a decision creates too much coupling and increases Docker
    Engine's level of instability. Others think that such a bundle makes the engine
    more robust and opens the door to some new possibilities. While both sides have
    valid arguments, I am more inclined towards the opinion of the latter group. Docker
    Swarm Mode is a huge step forward, and it is questionable whether the same result
    could be accomplished without bundling service registry inside the engine.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容捆绑到引擎中的决定引起了不同的反应。一些人认为这样的决策会导致过度耦合，增加 Docker 引擎的不稳定性。另一些人认为这种捆绑使得引擎更加强大，并为一些新可能性打开了大门。虽然双方都有有效的论点，但我更倾向于后者的观点。Docker
    Swarm 模式是一次巨大的进步，是否能够在不将服务注册中心捆绑到引擎中的情况下实现同样的效果，仍然值得质疑。
- en: Knowing how Docker Swarm works, especially networking, the question that might
    be on your mind is whether we need service discovery (beyond Swarms internal usage).
    In *The DevOps 2.0 Toolkit*, I argued that service discovery is a must and urged
    everyone to set up *Consul *([https://www.consul.io/](https://www.consul.io/))
    or *etcd (*[https://gith](https://github.com/coreos/etcd)[ub.com/coreos/etcd](https://github.com/coreos/etcd))
    as service registries, Registrator as a mechanism to register changes inside the
    cluster, and Consul Template or confd ([https://github.com/kelseyhightower/confd](https://github.com/kelseyhightower/confd))
    as a templating solution. Do we still need those tools?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 了解 Docker Swarm 的工作原理，尤其是其网络功能后，你可能会问，我们是否仍然需要服务发现（超出 Swarm 内部使用的范围）？在《DevOps
    2.0 工具包》中，我曾认为服务发现是必须的，并建议大家设置 *Consul*（[https://www.consul.io/](https://www.consul.io/)）或
    *etcd*（[https://github.com/coreos/etcd](https://github.com/coreos/etcd)）作为服务注册中心，使用
    Registrator 作为在集群内注册更改的机制，并用 Consul Template 或 confd（[https://github.com/kelseyhightower/confd](https://github.com/kelseyhightower/confd)）作为模板解决方案。那么我们现在还需要这些工具吗？
- en: Do we need service discovery?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们需要服务发现吗？
- en: It is hard to provide a general recommendation whether service discovery tools
    are needed when working inside a Swarm cluster. If we look at the need to find
    services as the main use case for those tools, the answer is usually no. We don't
    need external service discovery for that. As long as all services that should
    communicate with each other are inside the same network, all we need is the name
    of the destination service. For example, for the go-demo ([https://github.com/vfarcic/go-demo](https://github.com/vfarcic/go-demo))
    service to find the related database, it only needs to know its DNS `go-demo-db`.
    The [Chapter 3](fc49e3b5-55fc-4ebe-8f43-9b15cdf924ba.xhtml), *Docker Swarm Networking
    and Reverse Proxy*  proved that proper networking usage is enough for most use
    cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 很难提供一个通用的建议，说明在Swarm集群中工作时是否需要服务发现工具。如果我们把寻找服务作为这些工具的主要用途，答案通常是否定的。我们不需要外部的服务发现来解决这个问题。只要所有需要互相通信的服务在同一个网络中，我们只需要知道目标服务的名称。例如，对于go-demo（[https://github.com/vfarcic/go-demo](https://github.com/vfarcic/go-demo)）服务，它只需要知道数据库的DNS
    `go-demo-db` 就可以找到相关的数据库。[第3章](fc49e3b5-55fc-4ebe-8f43-9b15cdf924ba.xhtml)，*Docker
    Swarm 网络与反向代理* 已证明，正确的网络使用对于大多数用例来说已经足够。
- en: However, finding services and load balancing requests among them is not the
    only reason for service discovery. We might have other uses for service registries
    or key-value stores. We might need to store some information such that it is distributed
    and fault tolerant.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，寻找服务和在它们之间进行负载均衡请求并不是服务发现的唯一原因。我们可能还有其他对服务注册表或键值存储的需求。我们可能需要存储一些信息，以便它是分布式的并且具有容错性。
- en: An example of the need for a key-value store can be seen inside the *Docker
    Flow Proxy* ([https://github.com/vfarcic/docker-flow-p](https://github.com/vfarcic/docker-flow-proxy)[roxy](https://github.com/vfarcic/docker-flow-proxy))
    project. It is based on HAProxy which is a stateful service. It loads the information
    from a configuration file into memory. Having stateful services inside a dynamic
    cluster represents a challenge that needs to be solved. Otherwise, we might lose
    state when a service is scaled, rescheduled after a failure, and so on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 需要键值存储的一个例子可以在*Docker Flow Proxy*（[https://github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy)）项目中看到。它基于HAProxy，这是一个有状态的服务。它将配置信息加载到内存中。将有状态的服务放入动态集群中会带来挑战，必须解决这个问题。否则，当服务被扩展、在失败后重新调度等情况下，我们可能会丢失状态。
- en: Before we go into more details and problems related with stateful services,
    let's see how we could set up Consul as our key-value store of choice and go through
    its basic features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨有状态服务的更多细节和相关问题之前，先来看一下如何将Consul设置为我们选择的键值存储，并了解它的基本功能。
- en: Setting up Consul as service registry inside a Swarm cluster
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Swarm集群中设置Consul作为服务注册表
- en: As before, we'll start by setting up a Swarm cluster. From there on, we'll proceed
    with the Consul setup and a quick overview of the basic operations we can do with
    it. That will give us the knowledge necessary for the rest of this chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 像以前一样，我们将首先设置一个Swarm集群。从那里开始，我们将继续进行Consul的设置，并快速概述我们可以用它做的基本操作。这将为本章的其余部分提供必要的知识。
- en: '**A note to The DevOps 2.0 Toolkit readers**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**《DevOps 2.0工具包》读者注意**'
- en: You might be tempted to skip this sub-chapter since you already learned how
    to set up Consul. I recommend you read on. We'll use the official Consul image
    that was not available at the time I wrote the previous book. At the same time,
    I promise to keep this sub-chapter as brief as possible without confusing the
    new readers too much.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得可以跳过这一小节，因为你已经学会了如何设置Consul。但我建议你继续阅读。我们将使用官方的Consul镜像，而在我写前一本书时，这个镜像还不可用。同时，我保证会尽量简洁地讲解这一小节，不会让新读者感到困惑。
- en: Practice makes perfect, but there is a limit after which there is no reason
    to repeat the same commands over and over. I'm sure that, by now, you got tired
    of writing the commands that create a Swarm cluster. So, I prepared the `scripts/dm-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh))
    script that will create Docker Machine nodes and join them into a Swarm cluster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 熟能生巧，但也有一个限度，超出了这个限度，就没有理由一遍遍地重复相同的命令。我敢肯定，到目前为止，你已经厌倦了编写创建Swarm集群的命令。所以，我准备了`scripts/dm-swarm.sh`（[https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh)）脚本，它会创建Docker
    Machine节点并将它们加入到一个Swarm集群中。
- en: All the commands from this chapter are available in the `04-service-discovery.sh` ([https://gist.github.com/vfarcic/fa57e88faf09651c9a7e9e46c8950ef5](https://gist.github.com/vfarcic/fa57e88faf09651c9a7e9e46c8950ef5))
    Gist.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在`04-service-discovery.sh`（[https://gist.github.com/vfarcic/fa57e88faf09651c9a7e9e46c8950ef5](https://gist.github.com/vfarcic/fa57e88faf09651c9a7e9e46c8950ef5)）Gist中找到。
- en: 'Let''s clone the code and run the script:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们克隆代码并运行脚本：
- en: Some of the files will be shared between the host file system and Docker Machines
    we'll create soon. Docker Machine makes the whole directory that belongs to the
    current user available inside the VM. Therefore, please make sure that the code
    is cloned inside one of the user's sub-folders.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文件将在主机文件系统和我们将很快创建的Docker机器之间共享。Docker Machine会将当前用户所属的整个目录在虚拟机内共享。因此，请确保代码被克隆到用户的子文件夹中。
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the `node ls` command is as follows (IDs are removed for brevity):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`node ls`命令的输出如下（为了简洁，ID已移除）：'
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please note that this time there was a slight change in the commands. We used
    the `manager` token so that all three nodes are set up as managers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这次命令有所变化。我们使用了`manager`令牌，因此所有三个节点都设置为管理节点。
- en: As a general rule, we should have a least three Swarm managers. That way, if
    one of them fails, the others will reschedule the failed containers and can be
    used as our access points to the system. As is often the case with solutions that
    require a quorum, an odd number is usually the best. Hence, we have three.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们应该至少有三个Swarm管理节点。这样，如果其中一个节点失败，其他节点会重新调度失败的容器，并且可以作为我们访问系统的入口点。正如许多需要法定人数的解决方案一样，通常奇数是最好的。因此，我们有三个管理节点。
- en: You might be tempted to run all nodes as managers. I advise you against that.
    Managers synchronize data between themselves. The more manager instances are running,
    the more time the synchronization might last. While that is not even noticeable
    when there are only a few, if, for example, you'd run a hundred managers there
    would be some lag. After all, that's why we have workers. Managers are our entry
    points to the system and coordinators of the tasks, while workers do the actual
    work.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想要将所有节点都运行为管理节点。我建议你不要这样做。管理节点之间会同步数据。运行的管理节点越多，同步所需的时间可能越长。当管理节点不多时，这种延迟几乎是察觉不到的，但如果你运行一百个管理节点，可能会有一些延迟。毕竟，这就是我们需要工作节点的原因。管理节点是我们进入系统的入口点和任务的协调者，而工作节点则执行实际的工作。
- en: With that out of the way, we can proceed and set up Consul.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一点后，我们可以继续设置Consul。
- en: We'll start by downloading the `docker-compose.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml))
    file from the *Docker Flow Proxy* ([https://github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    project. It already contains Consul defined as Compose services.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始下载`docker-compose.yml`文件（[https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml)）来自*Docker
    Flow Proxy*（[https://github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy)）项目。该文件已经包含了定义为Compose服务的Consul。
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Just as Docker Swarm node can act as a manager or a worker, Consul can be run
    as a server or an agent. We'll start with the server.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Docker Swarm节点可以作为管理节点或工作节点一样，Consul也可以作为服务器或代理运行。我们从服务器开始。
- en: 'The Compose definition of the Consul service that acts as a server is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作为服务器运行的Consul服务的Compose定义如下：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The important thing to note is that we set up the network mode as `host`. That
    means that the container will share the same network as the host it is running
    on. This is followed by an environment variable and the command.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的重要事项是，我们将网络模式设置为`host`。这意味着容器将与运行它的主机共享相同的网络。接下来是环境变量和命令。
- en: The command will run the agent in server mode and, initially, it expects to
    be the only one in the cluster `-bootstrap-expect=1`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将以服务器模式运行代理，并且最初预计它是集群中唯一的一个`-bootstrap-expect=1`。
- en: You'll notice the usage of the `DOCKER_IP` environment variable. Consul expects
    the information about the binding and the client address. Since we don't know
    the IP of the servers in advance, it had to be a variable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到使用了`DOCKER_IP`环境变量。Consul需要绑定信息和客户端地址。由于我们无法提前知道服务器的IP地址，因此必须使用变量。
- en: 'At this moment you might be wondering why are we talking about Docker Compose
    services inside a Swarm cluster. Shouldn''t we run `docker service create` command?
    The truth is, at the time of this writing, the official consul image is still
    not adapted to the "Swarm way" of running things. Most images do not require any
    changes before launching them inside a Swarm cluster. Consul is one of the very
    few exceptions. I will do my best to update the instructions as soon as the situation
    changes. Until then, the good old Compose should do:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此时你可能会想，为什么我们在 Swarm 集群中讨论 Docker Compose 服务？我们不是应该运行 `docker service create`
    命令吗？事实上，在写这篇文章时，官方的 Consul 镜像仍然没有适应“Swarm 方式”的运行方式。大多数镜像在启动到 Swarm 集群中时不需要任何更改。Consul
    是少数几个例外之一。我会尽力在情况发生变化时及时更新说明。在此之前，经典的 Compose 方式应该没问题：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You''ll notice `WARNING: The Docker Engine you''re using is running in swarm
    mode` message in the output. It is only a friendly reminder that we are not running
    this as Docker service. Feel free to ignore it.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '你会在输出中看到 `WARNING: The Docker Engine you''re using is running in swarm mode`
    消息。这只是一个友好的提醒，表明我们并没有以 Docker 服务的方式运行它。可以忽略这个警告。'
- en: Now that we have a Consul instance running, we can go through the basic operations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个 Consul 实例在运行，我们可以进行一些基本操作。
- en: 'We can, for example, put some information into the key-value store:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将一些信息放入键值存储中：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `curl` command put this is a test value as the `msg1` key inside Consul.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 命令将一个测试值作为 `msg1` 键放入 Consul 中。'
- en: 'We can confirm that the key-value combination is indeed stored by sending a
    `GET` request:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过发送 `GET` 请求来确认键值组合确实已经存储：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows (formatted for readability):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（已格式化以提高可读性）：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You''ll notice that the value is encoded. If we add the `raw` parameter to
    the request, Consul will return only the value in its raw format:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到值被编码了。如果我们在请求中添加 `raw` 参数，Consul 将仅返回原始格式的值：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Right now, we have only one Consul instance. If the node it is running in fails
    `swarm-1`, all the data will be lost and service registry will be unavailable.
    That's not a good situation to be in.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只有一个 Consul 实例。如果它运行的节点 `swarm-1` 失败，所有数据将丢失，服务注册将不可用。这种情况并不好。
- en: We can create fault tolerance by running a few more Consul instances. This time,
    we'll run agents.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行更多的 Consul 实例来实现容错。这次，我们将运行代理。
- en: 'Just as the Consul server instance, the agent is also defined in the `docker-compose.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml))
    file in the *Docker Flow Proxy* ([https://github.com/vfarc](https://github.com/vfarcic/docker-flow-proxy)[ic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    project. Remember, we downloaded it with the name `docker-compose-proxy.yml`.
    Let''s take a look at the service definition:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Consul 服务器实例一样，代理也在 `docker-compose.yml` 文件中定义（[https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml)）文件中，属于
    *Docker Flow Proxy* 项目（[https://github.com/vfarc](https://github.com/vfarcic/docker-flow-proxy)[ic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy)）。记住，我们下载了名为
    `docker-compose-proxy.yml` 的文件。让我们来看一下服务定义：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The part of the output that defines the `Consul-agent` service is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中定义 `Consul-agent` 服务的部分如下：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It is almost the same as the definition we used to run the Consul server instance.
    The only important difference is that the `-server` is missing and that we have
    the `-retry-join` argument. We're using the latter to specify the address of another
    instance. Consul uses the gossip protocol. As long as every instance is aware
    of at least one other instance, the protocol will propagate the information across
    all of them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎与我们用于运行 Consul 服务器实例的定义相同。唯一的重要区别是缺少 `-server`，并且我们有了 `-retry-join` 参数。我们使用后者来指定另一个实例的地址。Consul
    使用 gossip 协议。只要每个实例至少知道一个其他实例，协议就会将信息传播到所有实例中。
- en: 'Let''s run agents on the other two nodes `swarm-2` and `swarm-3`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在其他两个节点 `swarm-2` 和 `swarm-3` 上运行代理：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have three Consul instances running inside the cluster (one on each
    node), we can confirm that gossip indeed works.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在集群中运行了三个 Consul 实例（每个节点上一个），我们可以确认 gossip 协议确实有效。
- en: 'Let''s request the value of the `msg1` key. This time, we''ll request it from
    the Consul instance running on `swarm-2`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们请求 `msg1` 键的值。这次，我们将从 `swarm-2` 上运行的 Consul 实例请求：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see from the output, even though we put the information to the instance
    running on `swarm-1`, it is available from the instance in `swarm-2`. The information
    is propagated through all the instances.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，尽管我们将信息存储到运行在 `swarm-1` 上的实例中，但它也可以从运行在 `swarm-2` 上的实例中访问。这些信息会在所有实例中传播。
- en: 'We can give the gossip protocol one more round of testing:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再给 Gossip 协议进行一轮测试：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We sent one `PUT` request to the instance running in `swarm-2` and another to
    the instance in `swarm-3`. When we requested all the keys from the instance running
    in `swarm-1`, all three were returned. In other words, no matter what we do with
    data, it is always in sync in all of the instances.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向运行在 `swarm-2` 上的实例发送了一个 `PUT` 请求，并向运行在 `swarm-3` 上的实例发送了另一个请求。当我们从运行在 `swarm-1`
    上的实例请求所有键时，所有三个键都被返回。换句话说，无论我们对数据做什么，它都会在所有实例中保持同步。
- en: 'Similarly, we can delete information:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们也可以删除信息：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We sent the request to the `swarm-2` to delete all keys. When we queried the
    instance running in `swarm-3`, we got an empty response meaning that everything
    is, indeed, gone.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向 `swarm-2` 发送了删除所有键的请求。当我们查询运行在 `swarm-3` 上的实例时，得到了一个空的响应，意味着所有的东西确实都消失了。
- en: With a setup similar to the one we explored, we can have a reliable, distributed,
    and fault-tolerant way for storing and retrieving any information our services
    might need.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过类似我们探讨过的设置，我们可以拥有一种可靠、分布式且容错的方式来存储和检索我们的服务可能需要的任何信息。
- en: We'll use this knowledge to explore a possible solution for some of the problems
    that might arise when running stateful services inside a Swarm cluster. But before
    we start discussing the solution, let's see what the problem is with stateful
    services.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用这些知识来探索在 Swarm 集群中运行有状态服务时可能出现的一些问题的解决方案。但在开始讨论解决方案之前，让我们先看看有状态服务的问题是什么。
- en: Problems when scaling stateful instances
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展有状态实例时的问题
- en: Scaling services inside a Swarm cluster is easy, isn't it? Just execute `docker
    service scale <SERVICE_NAME>=<NUMBER_OF_INSTANCES>` and, all of a sudden, the
    service is running multiple copies.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Swarm 集群中扩展服务很容易，不是吗？只需执行 `docker service scale <SERVICE_NAME>=<NUMBER_OF_INSTANCES>`，突然间，服务就运行了多个副本。
- en: The previous statement is only partly true. The more precise wording would be
    that "scaling stateless services inside a Swarm cluster is easy".
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的说法只是部分正确。更准确的表述应该是：“在 Swarm 集群中扩展无状态服务很容易”。
- en: The reason that scaling stateless services is easy lies in the fact that there
    is no state to think about. An instance is the same no matter how long it runs.
    There is no difference between a new instance and one that run for a week. Since
    the state does not change over time, we can create new copies at any given moment,
    and they will all be exactly the same.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态服务容易扩展的原因在于不需要考虑状态。无论实例运行多久，它都是一样的。新实例和运行一周的实例没有区别。由于状态不会随时间变化，我们可以在任何时刻创建新的副本，它们都会完全相同。
- en: 'However, the world is not stateless. State is an unavoidable part of our industry.
    As soon as the first piece of information is created, it needs to be stored somewhere.
    The place we store data must be stateful. It has a state that changes over time.
    If we want to scale such a stateful service, there are at least two things we
    need to consider:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，世界并非无状态的。状态是我们行业中不可避免的一部分。一旦第一条信息被创建，就需要存储在某个地方。我们存储数据的地方必须是有状态的。它有一个随时间变化的状态。如果我们想要扩展这种有状态的服务，至少有两件事需要考虑：
- en: How do we propagate a change of state of one instance to the rest of the instances?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何将一个实例的状态变化传播到其余实例？
- en: How do we create a copy (a new instance) of a stateful service, and make sure
    that the state is copied as well?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何创建一个有状态服务的副本（一个新实例），并确保状态也被复制？
- en: We usually combine stateless and stateful services into one logical entity.
    A back-end service could be stateless and rely on a database service as an external
    data storage. That way, there is a clear separation of concerns and a different
    lifecycle of each of those services.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将无状态和有状态服务结合成一个逻辑实体。一个后端服务可以是无状态的，并依赖于数据库服务作为外部数据存储。这样，服务之间就有了明确的职责分离，每个服务的生命周期也不同。
- en: Before we proceed, I must state that there is no silver bullet that makes stateful
    services scalable and fault-tolerant. Throughout the book, I will go through a
    couple of examples that might, or might not, apply to your use case. An obvious,
    and very typical example of a stateful service is a database. While there are
    some common patterns, almost every database provides a different mechanism for
    data replication. That, in itself, is enough to prevent us from having a definitive
    answer that would apply to all. We'll explore scalability of a MongoDB later on
    in the book. We'll also see an example with Jenkins that uses a file system for
    its state.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我必须声明，没有一个“灵丹妙药”能让有状态的服务具备可扩展性和容错性。在本书中，我将通过几个示例来讲解，这些示例可能适用于您的使用场景，也可能不适用。一个显而易见且非常典型的有状态服务示例是数据库。尽管有一些常见的模式，但几乎每个数据库都提供不同的数据复制机制。这本身就足以阻止我们给出一个适用于所有情况的最终答案。我们将在本书后面探讨
    MongoDB 的可扩展性。我们还会看到一个使用文件系统存储其状态的 Jenkins 示例。
- en: The first case we'll tackle will be of a different type. We'll discuss scalability
    of a service that has its state stored in a configuration file. To make things
    more complicated, the configuration is dynamic. It changes over time, throughout
    the lifetime of the service. We'll explore ways to make HAProxy scalable.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先处理一个不同类型的情况。我们将讨论将状态存储在配置文件中的服务的可扩展性。为了使事情更复杂，配置是动态的。它随着时间的推移在服务的生命周期中不断变化。我们将探索使
    HAProxy 可扩展的方法。
- en: If we use the official *HAProxy* ([https://hub.doc](https://hub.docker.com/_/haproxy/)[ker.com/_/haproxy/](https://hub.docker.com/_/haproxy/))
    image, one of the challenges we would face is deciding how to update the state
    of all the instances. We'd have to change the configuration and reload each copy
    of the `proxy`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用官方的*HAProxy*（[https://hub.doc](https://hub.docker.com/_/haproxy/)[ker.com/_/haproxy/](https://hub.docker.com/_/haproxy/)）镜像，我们面临的挑战之一是决定如何更新所有实例的状态。我们需要更改配置，并重新加载每个`proxy`副本。
- en: We can, for example, mount an NFS volume on each node in the cluster and make
    sure that the same host volume is mounted inside all HAProxy containers. At first,
    it might seem that that would solve the problem with the state since all instances
    would share the same configuration file. Any change to the config on the host
    would be available inside all the instances we would have. However, that, in itself,
    would not change the state of the service.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在集群中的每个节点上挂载一个 NFS 卷，并确保相同的主机卷在所有 HAProxy 容器内都被挂载。起初，这似乎能解决与状态相关的问题，因为所有实例都会共享相同的配置文件。对主机上配置文件的任何更改都会在所有实例中生效。然而，这本身并不会改变服务的状态。
- en: HAProxy loads the configuration file during initialization, and it is oblivious
    to any changes we might make to the configuration afterward. For the change of
    the state of the file to be reflected in the state of the service, we'd need to
    reload it. The problem is that instances can run on any of the nodes inside the
    cluster. On top of that, if we adopt dynamic scaling (more on that later on),
    we might not even know how many instances are running. So we'd need to discover
    how many instances we have, find out on which nodes they are running, get IDs
    of each of the containers, and, only then, send a signal to reload the `proxy`.
    While all this can be scripted, it is far from an optimum solution. Moreover,
    mounting an NFS volume is a single point of failure. If the server that hosts
    the volume fails, data is lost. Sure, we can create backups, but they would only
    provide a way to restore lost data partially. That is, we can restore a backup,
    but the data generated between the moment the last backup was created, and the
    node failure would be lost.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 在初始化时加载配置文件，之后对配置文件所做的任何更改它都无法察觉。为了让配置文件状态的更改反映到服务的状态中，我们需要重新加载配置文件。问题在于，实例可能在集群内的任何节点上运行。除此之外，如果我们采用动态扩展（稍后会详细讨论），我们可能甚至不知道有多少实例在运行。所以，我们需要发现我们有多少个实例，了解它们在哪些节点上运行，获取每个容器的
    ID，只有这样，我们才能发送信号重新加载`proxy`。虽然所有这些可以通过脚本实现，但这远不是一个最优的解决方案。此外，挂载 NFS 卷是一个单点故障。如果托管卷的服务器发生故障，数据就会丢失。当然，我们可以创建备份，但这些备份只能部分恢复丢失的数据。也就是说，我们可以恢复备份，但从上次备份创建到节点故障之间生成的数据会丢失。
- en: An alternative would be to embed the configuration into HAProxy images. We could
    create a new Dockerfile that would be based on `haproxy` and add the `COPY` instruction
    that would add the configuration. That would mean that every time we want to reconfigure
    the proxy, we'd need to change the config, build a new set of images (a new release),
    and update the `proxy` service currently running inside the cluster. As you can
    imagine, this is also not practical. It's too big of a process for a simple proxy
    reconfiguration.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方式是将配置嵌入到 HAProxy 镜像中。我们可以创建一个新的 Dockerfile，该文件基于 `haproxy`，并添加 `COPY` 指令来添加配置。这样，每次我们想重新配置代理时，都需要更改配置、构建一组新的镜像（即新的版本），并更新当前在集群内运行的
    `proxy` 服务。正如你能想象的，这同样是不现实的。对于简单的代理重新配置而言，这个过程太复杂了。
- en: '*Docker Flow Proxy* uses a different, less conventional, approach to the problem.
    It stores a replica of its state in Consul. It also uses an undocumented Swarm
    networking feature (at least at the time of this writing).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*Docker Flow Proxy* 使用一种不同的、不太传统的方法来解决这个问题。它将其状态的副本存储在 Consul 中。它还使用了一个未记录的
    Swarm 网络功能（至少在写这篇文章时是如此）。'
- en: Using service registry to store the state
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用服务注册表存储状态
- en: Now that we have Consul instances set up let us explore how to exploit them
    to our own benefit. We'll study the design of the *Docker Flow Proxy* as a way
    to demonstrate some of the challenges and solutions you might want to apply to
    your own services.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了 Consul 实例，让我们探讨如何利用它们为我们所用。我们将研究 *Docker Flow Proxy* 的设计，作为展示一些你可能希望应用到自己服务中的挑战和解决方案的一种方式。
- en: 'Let us create the `proxy` network and the service:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 `proxy` 网络和服务：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The command we used to create the **proxy** service is slightly different than
    before. Namely, now we have the `CONSUL_ADDRESS` variable with the comma separated
    addresses of all three **Consul** instances. The **proxy** is made in a way that
    it will try the first address. If it does not respond, it will try the next one,
    and so on. That way, as long as at least one **Consul** instance is running, the
    **proxy** will be able to fetch and put data. We would not need to do this loop
    if **Consul** would run as a Swarm service. In that case, all we'd need to do
    is put both inside the same network and use the service name as the address.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来创建 **proxy** 服务的命令与之前稍有不同。具体来说，现在我们有一个 `CONSUL_ADDRESS` 变量，其中包含所有三个 **Consul**
    实例的用逗号分隔的地址。**proxy** 的设计方式是它会尝试第一个地址。如果该地址没有响应，它会尝试下一个，依此类推。这样，只要至少有一个 **Consul**
    实例在运行，**proxy** 就能够获取和存储数据。如果 **Consul** 能作为 Swarm 服务运行，我们就不需要做这个循环了。在那种情况下，我们只需要将两者放在同一网络中，并使用服务名作为地址。
- en: 'Unfortunately, **Consul** cannot, yet, run as a Swarm service, so we are forced
    to specify all addresses, refer to the following diagram:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，**Consul** 目前还不能作为 Swarm 服务运行，因此我们不得不指定所有的地址，请参考下面的图示：
- en: '![](img/proxy-scaled-service-view.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/proxy-scaled-service-view.png)'
- en: 'Figure 4-7: The proxy scaled to three instances'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-7：代理扩展到三个实例
- en: 'Before we proceed, we should make sure that all instances of the `proxy` are
    running:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们应该确保所有的 `proxy` 实例都在运行：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Please wait until the current state of all the instances is set to `Running`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请等待直到所有实例的当前状态设置为 `Running`。
- en: 'Let''s create the `go-demo` service. It will act as a catalyst for a discussion
    around challenges we might face with a scaled reverse `proxy`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 `go-demo` 服务。它将作为一个催化剂，讨论我们在处理扩展的反向 `proxy` 时可能遇到的挑战：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: There's no reason to explain the commands in detail. They are the same as those
    we've run in the previous chapters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要详细解释这些命令。它们与我们在前几章中运行的命令是相同的。
- en: Please wait until the current state of the `go-demo` service is Running. Feel
    free to use `docker service ps go-demo` command to check the status.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请等待直到 `go-demo` 服务的当前状态为 Running。你可以随时使用 `docker service ps go-demo` 命令检查状态。
- en: If we would repeat the same process we used in the [Chapter 3](fc49e3b5-55fc-4ebe-8f43-9b15cdf924ba.xhtml), *Docker
    Swarm Networking and Reverse Proxy* the request to reconfigure the proxy would
    be as follows (please do not run it).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重复在 [第 3 章](fc49e3b5-55fc-4ebe-8f43-9b15cdf924ba.xhtml) 中使用的相同过程，*Docker
    Swarm 网络和反向代理*，重新配置代理的请求将如下所示（请不要执行它）。
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We would send a reconfigure request to the `proxy` service. Can you guess what
    would be the result?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会向 `proxy` 服务发送一个重新配置的请求。你能猜到结果是什么吗？
- en: A user sends a request to reconfigure the **proxy**. The request is picked by
    the routing mesh and load balanced across all the instances of the **proxy**.
    The request is forwarded to one of the instances. Since the **proxy** is using
    **Consul** to store its configuration, it sends the info to one of the **Consul**
    instances which, in turn, synchronizes the data across all others.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 用户发送请求来重新配置**代理**。请求由路由网格接收并在所有**代理**实例之间进行负载均衡。请求被转发到其中一个实例。由于**代理**使用**Consul**来存储其配置，它将信息发送到一个**Consul**实例，然后该实例会将数据同步到其他所有实例。
- en: 'As a result, we have **proxy** instances with different states. The one that
    received the request is reconfigured to use the `go-demo` service. The other two
    are, still, oblivious to it. If we try to ping the `go-demo` service through the
    **proxy**, we will get mixed responses. One out of three times, the response would
    be status `200`. The rest of the time, we would get `404`, not found:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是我们得到了具有不同状态的**代理**实例。接收到请求的那个被重新配置为使用`go-demo`服务。其他两个实例依然对其一无所知。如果我们尝试通过**代理**来
    ping `go-demo` 服务，我们将得到混合的响应。三次请求中一次会返回状态`200`，其余时间我们会收到`404`，未找到：
- en: '![](img/proxy-scaled-service-view-without-distribute.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/proxy-scaled-service-view-without-distribute.png)'
- en: 'Figure 4-8: A request to reconfigure the proxy'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-8：重新配置代理的请求
- en: We would experience a similar result if we scale MongoDB. The **routing mesh**
    would load balance across all instances, and their states would start to diverge.
    We could solve the problem with MongoDB by using replica sets. That's the mechanism
    that allows us to replicate data across all `DB` instances. However, HAProxy does
    not have such a feature. So, I had to add it myself.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们扩展 MongoDB，应该会得到类似的结果。**路由网格**会在所有实例之间进行负载均衡，它们的状态开始分歧。我们可以通过使用副本集来解决 MongoDB
    的问题。这是一个允许我们在所有 `DB` 实例之间复制数据的机制。然而，HAProxy 并没有这样的功能。所以，我必须自己添加它。
- en: 'The correct request to reconfigure the proxy running multiple instances is
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的请求来重新配置运行多个实例的代理如下：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Please note the new parameter `distribute=true`. When specified, the **proxy**
    will accept the request, reconfigure itself, and resend the request to all other
    instances:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意新的参数`distribute=true`。当指定该参数时，**代理**将接受请求，重新配置自身，并将请求重新发送到所有其他实例：
- en: '![](img/proxy-scaled-service-view-distribute-1.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/proxy-scaled-service-view-distribute-1.png)'
- en: 'Figure 4-9: The proxy instance that received the request and resent it to all
    others'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-9：接收请求并将其转发给所有其他实例的代理实例
- en: That way, the **proxy** implements a mechanism similar to replica sets in MongoDB.
    A change to one of the instances is propagated to all others.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，**代理**实现了类似于 MongoDB 中副本集的机制。对其中一个实例的更改会传播到所有其他实例。
- en: 'Let us confirm that it indeed works as expected:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认它确实按预期工作：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The response is `200` meaning that the `go-demo` service received the request
    forwarded by the `proxy` service. Since the routing mesh is in play, the request
    entered the system, was load balanced and resent to one of the proxy instances.
    The proxy instance that received the request evaluated the path and decided that
    it should go to the `go-demo` service. As a result, the request is resent to the
    `go-demo` network, load balanced again and forwarded to one of the `go-demo` instances.
    In other words, any of the `proxy` and `go-demo` instances could have received
    the request. If the proxy state was not synchronized across all the instances,
    two out of three requests would fail.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 响应为`200`，这意味着`go-demo`服务收到了**代理**服务转发的请求。由于路由网格的作用，请求进入系统后，进行了负载均衡，并再次转发到某个代理实例。接收到请求的代理实例评估了路径，决定将其转发到`go-demo`服务。因此，请求被重新发送到`go-demo`网络，再次进行负载均衡，并转发到某个`go-demo`实例。换句话说，任何一个`proxy`和`go-demo`实例都有可能收到请求。如果代理状态没有在所有实例之间同步，三次请求中有两次会失败。
- en: Feel free to repeat the `curl -i $(docker-machine ip swarm-1)/demo/hello` command.
    The result should always be the same.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可以随意重复`curl -i $(docker-machine ip swarm-1)/demo/hello`命令。结果应该始终相同。
- en: We can double check that the configuration is indeed synchronized by taking
    a peek into one of the containers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看其中一个容器来再次确认配置确实已同步。
- en: Let's take a look at, let's say, proxy instance number three.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下，比如说，代理实例三。
- en: 'The first thing we should do is find out the node the instance is running in:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该做的是找出实例运行所在的节点：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We listed all `proxy` service processes `docker service ps proxy`, filtered
    the result with the third instance `grep "proxy.3"`, and returned the name of
    the node stored in the fourth column of the output `awk '{print $4}'`. The result
    was stored in the environment variable `NODE`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了所有的`proxy`服务进程`docker service ps proxy`，通过第三个实例`grep "proxy.3"`过滤结果，并返回输出的第四列中存储的节点名称`awk
    '{print $4}'`。结果被存储在环境变量`NODE`中。
- en: 'Now that we know the server this instance is running in, we can enter the container
    and display the contents of the configuration file:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道这个实例运行的服务器，我们可以进入容器并显示配置文件的内容：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We changed the Docker client to point to the node. That was followed with the
    command that lists all running processes `docker ps`, filters out the third instance
    `grep "proxy.3"`, and outputs the container ID stored in the first column `awk
    '{print $1}'`. The result was stored in the environment variable ID.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Docker客户端指向节点。接着执行列出所有正在运行的进程的命令`docker ps`，过滤出第三个实例`grep "proxy.3"`，并输出存储在第一列的容器ID`awk
    '{print $1}'`。结果被存储在环境变量ID中。
- en: 'With the client pointing to the correct node and the ID stored as the environment
    variable ID, we can, finally, enter the container and display the configuration:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端指向正确的节点，ID也被存储为环境变量ID，我们最终可以进入容器并显示配置：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The relevant part of the output is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, the third instance of the `proxy` is indeed configured correctly
    with the `go-demo` service. Feel free to repeat the process with the other two
    instances. The result should be exactly the same proving that synchronization
    works.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`proxy`的第三个实例确实已正确配置了`go-demo`服务。你可以随意重复这个过程，检查其他两个实例，结果应该完全相同，从而证明同步有效。
- en: How was it done? How did the `proxy` instance that received the request discover
    the IPs of all the other instances? After all, there is no Registrator that would
    provide the IPs to Consul, and we cannot access Swarms internal service discovery
    API.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何做到的？`proxy`实例是如何发现所有其他实例的IP的？毕竟，并没有一个Registrator提供IP给Consul，而且我们也无法访问Swarm的内部服务发现API。
- en: Discovering addresses of all instances that form a service
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现组成服务的所有实例的地址
- en: If you browse through the official Docker documentation, you will not find any
    reference to addresses of individual instances that form a service.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你浏览官方Docker文档，你将找不到任何提及组成服务的各个实例的地址。
- en: The previous sentence might not be true at the time you're reading this. Someone
    might have updated the documentation. However, at the time I'm writing this chapter,
    there is not a trace of such information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这段话时，前述句子可能不再成立。有人可能已经更新了文档。然而，在我写这章的时候，确实没有任何此类信息的痕迹。
- en: The fact that something is not documented does not mean that it does not exist.
    Indeed, there is a special DNS that will return all IPs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 事情没有被记录并不意味着它不存在。事实上，有一个特殊的DNS，它会返回所有的IP。
- en: 'To see it in action, we''ll create the global service called util and attach
    it to the `proxy` network:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到实际效果，我们将创建一个名为util的全局服务并将其附加到`proxy`网络：
- en: '[PRE27]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Before proceeding, please wait until the current state is set to running.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请等待直到当前状态设置为运行中。
- en: 'Next, we''ll find the ID of one of the util instances and install drill that
    will show us the information related to DNS entries:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将找到其中一个util实例的ID并安装drill工具，它将展示与DNS条目相关的信息：
- en: '[PRE28]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s start by drilling the DNS proxy:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从钻取DNS代理开始：
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, even though we are running three instances of the service, only
    one IP is returned `10.0.0.2`. That is the IP of the service, not an individual
    instance. To be more concrete, it is the IP of the `proxy` service network end-point.
    When a request reaches that end-point, Docker network performs load balancing
    across all the instances.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，尽管我们运行了三个实例，但只返回了一个IP`10.0.0.2`。那是服务的IP，而不是单个实例的IP。更具体地说，它是`proxy`服务网络端点的IP。当请求到达该端点时，Docker网络会对所有实例进行负载均衡。
- en: In most cases, we do not need anything else. All we have to know is the name
    of the service and Docker will do the rest of the work for us. However, in a few
    cases, we might need more. We might need to know the IPs of every single instance
    of a service. That is the problem *Docker Flow Proxy* faced.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们不需要其他任何东西。我们只需要知道服务的名称，Docker 就会为我们完成剩下的工作。然而，在某些情况下，我们可能需要更多信息。我们可能需要知道每个服务实例的
    IP。这正是 *Docker Flow Proxy* 面临的问题。
- en: To find the IPs of all the instances of a service we can use the "undocumented"
    feature. We need to add the tasks prefix to the service name.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找服务所有实例的 IP，我们可以使用“未文档化”的功能。我们需要在服务名称前添加 `tasks` 前缀。
- en: 'Let''s drill again:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再深入了解一下：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This time, the output is different:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，输出有所不同：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We got three answers, each with a different IP `10.0.0.4, 10.0.0.3, 10.0.0.5`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了三个答案，每个答案有一个不同的 IP：`10.0.0.4, 10.0.0.3, 10.0.0.5`。
- en: Knowing the IPs of all the instances solved the problem of having to synchronize
    data. With tasks.`<SERVICE_NAME>` we have all the info we need. The rest is only
    a bit of coding that will utilize those IPs. It is a similar mechanism used when
    synchronizing databases (more on that later).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 知道所有实例的 IP 解决了数据同步的问题。通过 tasks.`<SERVICE_NAME>` 我们得到了所有需要的信息。剩下的只是一些代码，利用这些
    IP。这个机制类似于同步数据库时使用的机制（稍后会详细介绍）。
- en: We are not done yet. The fact that we can synchronize data on demand (or events)
    does not mean that the service is fault tolerant. What should we do if we need
    to create a new instance? What happens if an instance fails and Swarm reschedules
    it somewhere else?
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。我们能够按需同步数据（或事件）并不意味着服务是容错的。如果我们需要创建一个新的实例该怎么办？如果一个实例失败，Swarm 将它调度到其他地方会发生什么？
- en: Using service registry or key value store to store service state
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用服务注册表或键值存储来存储服务状态
- en: We'll continue using *Docker Flow Proxy* as a playground to explore some of
    the mechanisms and decisions we might make when dealing with stateful services.
    Please note that, in this chapter, we are concentrating on services with a relatively
    small state. We'll explore other use cases in the chapters that follow.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用 *Docker Flow Proxy* 作为游乐场，探索在处理有状态服务时可能采取的一些机制和决策。请注意，在本章中，我们重点讨论的是状态相对较小的服务。在接下来的章节中，我们会探讨其他的使用案例。
- en: Imagine that the proxy does not use Consul to store data and that we do not
    use volumes. What would happen if we were to scale it up? The new instances would
    be out of sync. Their state would be the same as the initial state of the first
    instance we created. In other words, there would be no state, even though the
    instances that are already running changed over time and generated data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 假设代理没有使用 Consul 来存储数据，并且我们没有使用卷。如果我们要扩展它，会发生什么呢？新的实例将会不同步。它们的状态将与我们创建的第一个实例的初始状态相同。换句话说，虽然已经运行的实例随着时间变化并生成了数据，但它们将没有状态。
- en: That is where Consul comes into play. Every time an instance of the proxy receives
    a request that results in the change of its state, it propagates that change to
    other instances, as well as to Consul. On the other hand, the first action the
    proxy performs when initialized is to consult Consul, and create the configuration
    from its data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，Consul 就派上了用场。每当代理的某个实例接收到一个请求并导致其状态发生变化时，它会将该变化传播到其他实例，以及 Consul。另一方面，代理在初始化时执行的第一步就是查询
    Consul，并从其中的数据创建配置。
- en: 'We can observe the state stored in Consul by sending a request for all the
    data with keys starting with `docker-flow`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过发送请求获取所有以 `docker-flow` 开头的键的数据，来观察存储在 Consul 中的状态：
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A part of the output is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding example shows that the path and the port we specified when we
    reconfigured the proxy for the `go-demo` service, is stored in Consul. If we instruct
    Swarm manager to scale the `proxy` service, new instances will be created. Those
    instances will query Consul and use the information to generate their configurations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子显示了我们在重新配置 `go-demo` 服务的代理时指定的路径和端口，已被存储在 Consul 中。如果我们指示 Swarm 管理器扩展 `proxy`
    服务，新的实例将会被创建。这些实例会查询 Consul 并使用其中的信息来生成它们的配置。
- en: 'Let''s give it a try:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We increased the number of instances from three to six.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实例数量从三增加到六。
- en: 'Let''s take a sneak peek into the instance number six:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们偷偷看看第六个实例：
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'A part of the output of the `exec` command is as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`exec` 命令输出的一部分如下：'
- en: '[PRE37]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see, the new instance recuperated all the information from Consul.
    As a result, its state became the same as the state of any other `proxy` instance
    running inside the cluster.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，新实例从 Consul 恢复了所有信息。因此，它的状态与集群内运行的任何其他 `proxy` 实例的状态相同。
- en: 'If we destroy an instance, the result will, again, be the same. Swarm will
    detect that an instance crashed and schedule a new one. The new instance will
    repeat the same process of querying Consul and create the same state as the other
    instances:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们销毁一个实例，结果将再次是相同的。Swarm 会检测到实例崩溃并调度一个新的实例。新实例将重复相同的过程，查询 Consul 并创建与其他实例相同的状态：
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We should wait for a few moments until Swarm detects the failure and creates
    a new instance.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该稍等片刻，直到 Swarm 检测到故障并创建一个新实例。
- en: 'Once it''s running, we can take a look at the configuration of the new instance.
    It will be the same as before:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦它运行起来，我们可以查看新实例的配置。它将与之前相同：
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The explanation of *Docker Flow Proxy* inner workings is mostly for educational
    purposes. I wanted to show you one of the possible solutions when dealing with
    stateful services. The methods we discussed are applicable only when the state
    is relatively small. When it is bigger, as is the case with databases, we should
    employ different mechanisms to accomplish the same goals.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*Docker Flow Proxy* 内部工作原理的解释主要是出于教育目的。我想向你展示在处理有状态服务时可能的一种解决方案。我们讨论的方法仅适用于状态相对较小的情况。当状态变得更大时，例如数据库的情况，我们应该使用不同的机制来实现相同的目标。'
- en: 'If we go one level higher, the primary requirements, or prerequisites, when
    running stateful services inside a cluster are as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们往上一层看，在集群内运行有状态服务时，主要的要求或前提条件如下：
- en: Ability to synchronize the state across all instances of the service.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有实例之间同步状态的能力。
- en: Ability to recuperate the state during initialization.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化期间恢复状态的能力。
- en: If we manage to fulfill those two requirements, we are on the right path towards
    solving one of the major bottlenecks when operating stateful services inside the
    cluster.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够满足这两个要求，我们就走在了正确的道路上，朝着解决在集群内操作有状态服务时的主要瓶颈之一迈进。
- en: What now?
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: That concludes the exploration of basic concepts behind the usage of service
    discovery inside a Swarm cluster.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了关于在 Swarm 集群内使用服务发现的基本概念的探索。
- en: Are we done learning about Swarm features? We are far from knowing everything
    there is to know about Docker Swarm. However, at this point, we have enough knowledge
    to go back to the end of [Chapter 1](44df5a4c-1e47-4de0-9442-660034287e66.xhtml), *Continuous
    Integration with Docker Containers* and make the next logical step. We can design
    a Continuous Delivery flow.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学完 Swarm 特性了吗？我们远未掌握 Docker Swarm 的所有知识。然而，到目前为止，我们已经拥有足够的知识，可以回到[第一章](44df5a4c-1e47-4de0-9442-660034287e66.xhtml)的末尾，*使用
    Docker 容器进行持续集成*，并迈出下一步。我们可以设计一个持续交付流程。
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候休息一下，再深入下一章了。和之前一样，我们将销毁创建的机器，重新开始：
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
