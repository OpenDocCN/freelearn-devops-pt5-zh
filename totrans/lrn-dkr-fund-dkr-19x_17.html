<html><head></head><body>
        

                            
                    <h1 class="header-title">Zero-Downtime Deployments and Secrets</h1>
                
            
            
                
<p>In the previous chapter, we explored Docker Swarm and its resources in detail. We learned how to build a highly available swarm locally and in the cloud. Then, we discussed Swarm services and stacks in depth. Finally, we created services and stacks in the swarm.</p>
<p>In this chapter, we will show you how we can update services and stacks running in Docker Swarm without interrupting their availability. This is called zero-downtime deployment. We are also going to introduce swarm secrets as a means to securely provide sensitive information to containers of a service using those secrets.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Zero-downtime deployment</li>
<li>Storing configuration data in the swarm</li>
<li>Protecting sensitive data with Docker Secrets</li>
</ul>
<p>After finishing this chapter, you will be able to do the following:</p>
<ul>
<li>List two to three different deployment strategies commonly used to update a service without downtime.</li>
<li>Update a service in batches without causing a service interruption.</li>
<li>Define a rollback strategy for a service that is used if an update fails.</li>
<li>Store non-sensitive configuration data using Docker configs.</li>
<li>Use a Docker secret with a service.</li>
<li>Update the value of a secret without causing downtime.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p class="mce-root">The code files for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition" target="_blank">https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition</a>. If you have checked out the repository as indicated in <a href="99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml" target="_blank">Chapter 2</a>, <em>Setting up a Working Environment</em>, then you'll find the code at <kbd>~/fod-solution/ch14</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Zero-downtime deployment</h1>
                
            
            
                
<p>One of the most important aspects of a mission-critical application that needs frequent updates is the ability to do updates in a fashion that requires no outage at all. We call this a zero-downtime deployment. At all times, the application that is updated must be fully operational.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Popular deployment strategies</h1>
                
            
            
                
<p>There are various ways to achieve this. Some of them are as follows:</p>
<ul>
<li style="font-weight: 400">Rolling updates</li>
<li style="font-weight: 400">Blue-green deployments</li>
<li style="font-weight: 400">Canary releases</li>
</ul>
<p>Docker Swarm supports rolling updates out of the box. The other two types of deployments can be achieved with some extra effort from our side.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Rolling updates</h1>
                
            
            
                
<p>In a mission-critical application, each application service has to run in multiple replicas. Depending on the load, that can be as few as two to three instances and as many as dozens, hundreds, or thousands of instances. At any given time, we want to have a clear majority when it comes to all the service instances running. So, if we have three replicas, we want to have at least two of them up and running at all times. If we have 100 replicas, we can be content with a minimum of, say, 90 replicas, being available. By doing this, we can define a batch size of replicas that we may take down to upgrade. In the first case, the batch size would be 1 and in the second case, it would be 10.</p>
<p>When we take replicas down, Docker Swarm will automatically take those instances out of the load balancing pool and all traffic will be load balanced across the remaining active instances. Those remaining instances will thus experience a slight increase in traffic. In the following diagram, prior to the start of the rolling update, if <strong>Task A3</strong> wanted to access <strong>Service B</strong>, it could have been load balanced to any of the three tasks of <strong>Service B</strong> by SwarmKit. Once the rolling update started, SwarmKit took down <strong>Task B1</strong> for updates. Automatically, this task is then taken out of the pool of targets. So, if <strong>Task A3</strong> now requests to connect to <strong>Service B</strong>, load balancing will only select from the remaining tasks, that is, <strong>B2</strong> and <strong>B3</strong>. Thus, those two tasks might experience a higher load temporarily:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b5692dbe-f8b2-4050-bc4b-04147a063825.png" style="width:39.08em;height:23.67em;"/></p>
<p>Task B1 is taken down to be updated</p>
<p>The stopped instances are then replaced by an equivalent number of new instances of the new version of the application service. Once the new instances are up and running, we can have the Swarm observe them for a given period of time and make sure they're healthy. If all is well, then we can continue by taking down the next batch of instances and replacing them with instances of the new version. This process is repeated until all the instances of the application service have been replaced.</p>
<p>In the following diagram, we can see that <strong>Task B1</strong> of <strong>Service B</strong> has been updated to version 2. The container of <strong>Task B1</strong> was assigned a new <strong>IP</strong> address, and it was deployed to another worker node with free resources:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2e0094c4-5dce-4763-8401-394a87cc79b3.png" style="width:38.75em;height:22.75em;"/></p>
<p>The first batch being updated in a rolling update</p>
<p>It is important to understand that when the task of a service is updated, in most cases, it gets deployed to a different worker node than the one it used to live on. But that should be fine as long as the corresponding service is stateless. If we have a stateful service that is location- or node-aware and we'd like to update it, then we have to adjust our approach, but this is outside of the scope of this book.</p>
<p>Now, let's look at how we can actually instruct the Swarm to perform a rolling update of an application service. When we declare a service in a stack file, we can define multiple options that are relevant in this context. Let's look at a snippet of a typical stack file:</p>
<pre>version: "3.5"<br/>services:<br/> web:<br/>   image: nginx:alpine<br/>   deploy:<br/>     replicas: 10<br/>     update_config:<br/>       parallelism: 2<br/>       delay: 10s<br/>...</pre>
<p>In this snippet, we can see a section, <kbd>update_config</kbd>, with the <kbd>parallelism</kbd> and <kbd>delay</kbd> properties. <kbd>parallelism</kbd> defines the batch size of how many replicas are going to be updated at a time during a rolling update. <kbd>delay</kbd> defines how long Docker Swarm is going to wait between updating individual batches. In the preceding case, we have <kbd>10</kbd> replicas that are being updated in two instances at a time and, between each successful update, Docker Swarm waits for <kbd>10</kbd> seconds.</p>
<p>Let's test such a rolling update. Navigate to the <kbd>ch14</kbd> subfolder of our <kbd>labs</kbd> folder and use the <kbd>stack.yaml</kbd> file to create a web service that's been configured for a rolling update. The service uses an Alpine-based Nginx image whose version is <kbd>1.12-alpine</kbd>. We will update the service to a newer version, that is, <kbd>1.13-alpine</kbd>.</p>
<p>To start, we will deploy this service to our swarm that we created locally in VirtualBox. Let's take a look:</p>
<ol>
<li>First, we need to make sure that we have our Terminal window configured so that we can access one of the master nodes of our cluster. Let's take the leader, that is, <kbd>node-1</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ eval $(docker-machine env node-1)</strong></pre>
<ol start="2">
<li>Now, we can deploy the service using the stack file:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker stack deploy -c stack.yaml web</strong></pre>
<p style="padding-left: 60px">The output of the preceding command looks like this:</p>
<div><img src="img/1f326e7f-883f-4cc7-b643-3844164cc739.png" style="width:23.67em;height:6.17em;"/></div>
<p>Deployment of the web stack</p>
<ol start="3">
<li>Once the service has been deployed, we can monitor it using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ watch docker stack ps web</strong></pre>
<p style="padding-left: 60px">We will see the following output:</p>
<div><img src="img/909a831e-a9a3-4ae8-98b1-addeb1ac75a7.png"/></div>
<p>Service web of the web stack running in Swarm with 10 replicas</p>
<p>If you're working on a macOS machine, you need to make sure your watch tool is installed. Use the <kbd>brew install watch</kbd> command to do so.</p>
<p>The previous command will continuously update the output and provide us with a good overview of what happens during the rolling update.</p>
<p>Now, we need to open a second Terminal and configure it for remote access for the manager node of our swarm. Once we have done that, we can execute the <kbd>docker</kbd> command, which will update the image of the <kbd>web</kbd> service of the stack, also called <kbd>web</kbd>:</p>
<pre><strong>$ docker service update --image nginx:1.13-alpine web_web</strong></pre>
<p>The preceding command leads to the following output, indicating the progress of the rolling update:</p>
<div><img src="img/80e17241-6cbe-414b-b393-b874ba9f475a.png" style="width:37.58em;height:15.42em;"/></div>
<p>Screen showing the progress of the rolling update</p>
<p>The preceding output indicates that the first two batches, each with two tasks, have been successful and that the third batch is preparing.</p>
<p>In the first Terminal window, where we're watching the stack, we should now see how Docker Swarm updates the service batch by batch with an interval of <kbd>10 seconds</kbd>. After the first batch, it should look like the following screenshot:</p>
<div><img src="img/0c203143-48d9-4eb6-8207-bf098224f2d8.png"/></div>
<p>Rolling update for a service in Docker Swarm</p>
<p>In the preceding screenshot, we can see that the first batch of the two tasks, <kbd>8</kbd> and <kbd>9</kbd>, has been updated. Docker Swarm is waiting for <kbd>10 seconds</kbd> to proceed with the next batch.</p>
<p>It is interesting to note that in this particular case, SwarmKit deploys the new version of the task to the same node as the previous version. This is accidental since we have five nodes and two tasks on each node. SwarmKit always tries to balance the workload evenly across the nodes. So, when SwarmKit takes down a task, the corresponding node has a smaller workload than all the others, so the new instance is scheduled to it. Normally, you cannot expect to find the new instance of a task on the same node. Just try it out yourself by deleting the stack with <kbd>docker stack rm web</kbd> and changing the number of replicas to say, seven, and then redeploy and update it.</p>
<p>Once all the tasks have been updated, the output of our <kbd>docker stack ps web</kbd> command will look similar to the following screenshot:</p>
<div><img src="img/4def2ed1-1be6-4416-9e98-041b0ec8d8d6.png"/></div>
<p>All tasks have been updated successfully</p>
<p>Please note that SwarmKit does not immediately remove the containers of the previous versions of the tasks from the corresponding nodes. This makes sense as we might want to, for example, retrieve the logs from those containers for debugging purposes, or we might want to retrieve their metadata using <kbd>docker container inspect</kbd>. SwarmKit keeps the four latest terminated task instances around before it purges older ones so that it doesn't clog the system with unused resources.</p>
<p>We can use the <kbd>--update-order</kbd> parameter to instruct Docker to start the new container replica before stopping the old one. This can improve application availability. Valid values are <kbd>"start-first"</kbd> and <kbd>"stop-first"</kbd>. The latter is the default.</p>
<p>Once we're done, we can tear down the stack using the following command:</p>
<pre><strong>$ docker stack rm web</strong></pre>
<p>Although using stack files to define and deploy applications is the recommended best practice, we can also define the update behavior in a service <kbd>create</kbd> statement. If we just want to deploy a single service, this might be the preferred way of doing things. Let's look at such a <kbd>create</kbd> command:</p>
<pre><strong>$ docker service create --name web \</strong><br/><strong>    --replicas 10 \</strong><br/><strong>    --update-parallelism 2 \</strong><br/><strong>    --update-delay 10s \</strong><br/><strong>    nginx:alpine</strong></pre>
<p>This command defines the same desired state as the preceding stack file. We want the service to run with <kbd>10</kbd> replicas and we want a rolling update to happen in batches of two tasks at a time, with a 10-second interval between consecutive batches.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Health checks</h1>
                
            
            
                
<p>To make informed decisions, for example, during a rolling update of a Swarm service regarding whether or not the just-installed batch of new service instances is running OK or if a rollback is needed, the SwarmKit needs a way to know about the overall health of the system. On its own, SwarmKit (and Docker) can collect quite a bit of information. But there is a limit. Imagine a container containing an application. The container, as seen from the outside, can look absolutely healthy and carry on just fine. But that doesn't necessarily mean that the application running inside the container is also doing well. The application could, for example, be in an infinite loop or be in a corrupt state, yet still running. However, as long as the application runs, the container runs and from outside, everything looks perfect.</p>
<p>Thus, SwarmKit provides a seam where we can provide it with some help. We, the authors of the application services running inside the containers in the swarm, know best as to whether or not our service is in a healthy state. SwarmKit gives us the opportunity to define a command that is executed against our application service to test its health. What exactly this command does is not important to Swarm; the command just needs to return <kbd>OK</kbd>, <kbd>NOT OK</kbd>, or <kbd>time out</kbd>. The latter two situations, namely <kbd>NOT OK</kbd> or <kbd>timeout</kbd>, will tell SwarmKit that the task it is investigating is potentially unhealthy.</p>
<p>Here, I am writing potentially on purpose and later, we will see why:</p>
<pre>FROM alpine:3.6<br/>...<br/>HEALTHCHECK --interval=30s \<br/>    --timeout=10s<br/>    --retries=3<br/>    --start-period=60s<br/>    CMD curl -f http://localhost:3000/health || exit 1<br/>...</pre>
<p>In the preceding snippet from a <kbd>Dockerfile</kbd>, we can see the keyword <kbd>HEALTHCHECK</kbd>. It has a few options or parameters and an actual command, that is, <kbd>CMD</kbd>. Let's discuss the options:</p>
<ul>
<li><kbd>--interval</kbd>: Defines the wait time between health checks. Thus, in our case, the orchestrator executes a check every <kbd>30</kbd> seconds.</li>
<li><kbd>--timeout</kbd>: This parameter defines how long Docker should wait if the health check does not respond until it times out with an error. In our sample, this is <kbd>10</kbd> seconds. Now, if one health check fails, SwarmKit retries a couple of times until it gives up and declares the corresponding task as unhealthy and opens the door for Docker to kill this task and replace it with a new instance.</li>
<li>The number of retries is defined with the <kbd>--retries</kbd> parameter. In the preceding code, we want to have three retries.</li>
<li>Next, we have the start period. Some containers take some time to start up (not that this is a recommended pattern, but sometimes it is inevitable). During this startup time, the service instance might not be able to respond to health checks. With the start period, we can define how long SwarmKit should wait before it executes the very first health check and thus give the application time to initialize. To define the startup time, we use the <kbd>--start-period</kbd> parameter. In our case, we do the first check after <kbd>60</kbd> seconds. How long this start period needs to be depends on the application and its startup behavior. The recommendation is to start with a relatively low value and if you have a lot of false positives and tasks that are restarted many times, you might want to increase the time interval.</li>
<li>Finally, we define the actual probing command on the last line with the <kbd>CMD</kbd> keyword. In our case, we are defining a request to the <kbd>/health</kbd> endpoint of <kbd>localhost</kbd> at port <kbd>3000</kbd> as a probing command. This call is expected to have three possible outcomes:
<ul>
<li>The command succeeds.</li>
<li>The command fails.</li>
<li>The command times out.</li>
</ul>
</li>
</ul>
<p>The latter two are treated the same way by SwarmKit. This is the orchestrator telling us that the corresponding task might be unhealthy. I did say <em>might </em>with intent since SwarmKit does not immediately assume the worst-case scenario but assumes that this might just be a temporary fluke of the task and that it will recover from it. This is the reason why we have a <kbd>--retries</kbd> parameter. There, we can define how many times SwarmKit should retry before it can assume that the task is indeed unhealthy, and consequently kill it and reschedule another instance of this task on another free node to reconcile the desired state of the service.</p>
<p><em>Why can we use localhost in our probing command?</em> This is a very good question, and the reason is because SwarmKit, when probing a container running in the Swarm, executes this <kbd>probing</kbd> command inside the container (that is, it does something like <kbd>docker container exec &lt;containerID&gt; &lt;probing command&gt;</kbd>). Thus, the command executes in the same network namespace as the application running inside the container. In the following diagram, we can see the life cycle of a service task from its beginning:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/04607fa9-4a95-4188-9437-5db991b5d3b1.png"/></p>
<p>Service task with transient health failure</p>
<p>First, SwarmKit waits to probe until the start period is over. Then, we have our first health check. Shortly thereafter, the task fails when probed. It fails two consecutive times but then it recovers. Thus, <strong>health check 4</strong> is successful and SwarmKit leaves the task running.</p>
<p>Here, we can see a task that is permanently failing:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/69eb3697-f61f-446c-b5cc-3c5d36bbe6d0.png"/></p>
<p>Permanent failure of a task</p>
<p>We have just learned how we can define a health check for a service in the <kbd>Dockerfile</kbd> of its image. But this is not the only way we can do this. We can also define the health check in the stack file that we use to deploy our application into Docker Swarm. Here is a short snippet of what such a stack file would look like:</p>
<pre>version: "3.5"<br/>services:<br/>  web:<br/>    image: example/web:1.0<br/>    healthcheck:<br/>      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]<br/>      interval: 30s<br/>      timeout: 10s<br/>      retries: 3<br/>      start_period: 60s<br/>...</pre>
<p>In the preceding snippet, we can see how the health check-related information is defined in the stack file. First and foremost, it is important to realize that we have to define a health check for every service individually. There is no health check at an application or global level.</p>
<p>Similar to what we defined previously in the <kbd>Dockerfile</kbd>, the command that is used to execute the health check by SwarmKit is <kbd>curl -f http://localhost:3000/health</kbd>. We also have definitions for <kbd>interval</kbd>, <kbd>timeout</kbd>, <kbd>retries</kbd>, and <kbd>start_period</kbd>. These four key-value pairs have the same meaning as the corresponding parameters we used in the <kbd>Dockerfile</kbd>. If there are health check-related settings defined in the image, then the ones defined in the stack file override the ones from the <kbd>Dockerfile</kbd>.</p>
<p>Now, let's try to use a service that has a health check defined. In our <kbd>lab</kbd> folder, we have a file called <kbd>stack-health.yaml</kbd> with the following content:</p>
<pre>version: "3.5"<br/>services:<br/>  web:<br/>    image: nginx:alpine<br/>    healthcheck:<br/>      test: ["CMD", "wget", "-qO", "-", "http://localhost"]<br/>      interval: 5s<br/>      timeout: 2s<br/>      retries: 3<br/>      start_period: 15s</pre>
<p>Let's deploy this:</p>
<pre><strong>$ docker stack deploy -c stack-health.yaml myapp</strong></pre>
<p>We can find out where the single task was deployed to using <kbd>docker stack ps myapp</kbd>. On that particular node, we can list all the containers to find one of our stacks. In my example, the task had been deployed to <kbd>node-3</kbd>:</p>
<div><img src="img/b39744ac-ae71-456f-b8d1-3b34c99837e2.png"/></div>
<p>Displaying the health status of a running task instance</p>
<p>The interesting thing in this screenshot is the <kbd>STATUS</kbd> column. Docker, or more precisely, SwarmKit, has recognized that the service has a health check function defined and is using it to determine the health of each task of the service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Rollback</h1>
                
            
            
                
<p>Sometimes, things don't go as expected. A last-minute fix in an application release may have inadvertently introduced a new bug, or the new version significantly decreases the throughput of the component, and so on. In such cases, we need to have a plan B, which in most cases means the ability to roll back the update to the previous good version.</p>
<p>As with the update, the rollback has to happen in such a way that it does not cause any outages in terms of the application; it needs to cause zero-downtime. In that sense, a rollback can be looked at as a reverse update. We are installing a new version, yet this new version is actually the previous version.</p>
<p>As with the update behavior, we can declare, either in our stack files or in the Docker service <kbd>create</kbd> command, how the system should behave in case it needs to execute a rollback. Here, we have the stack file that we used previously, but this time with some rollback-relevant attributes:</p>
<pre>version: "3.5"<br/>services:<br/>  web:<br/>    image: nginx:1.12-alpine<br/>    ports:<br/>      - 80:80<br/>    deploy:<br/>      replicas: 10<br/>      update_config:<br/>        parallelism: 2<br/>        delay: 10s<br/><br/>        failure_action: rollback<br/>        monitor: 10s<br/><br/>    healthcheck:<br/>      test: ["CMD", "wget", "-qO", "-", "http://localhost"]<br/>      interval: 2s<br/>      timeout: 2s<br/>      retries: 3<br/>      start_period: 2s</pre>
<p>In this stack file, which is available in our lab as <kbd>stack-rollback.yaml</kbd>, we defined the details about the rolling update, the health checks, and the behavior during rollback. The health check is defined so that after an initial wait time of <kbd>2</kbd> seconds, the orchestrator starts to poll the service on <kbd>http://localhost</kbd> every <kbd>2</kbd> seconds and it retries <kbd>3</kbd> times before it considers a task as unhealthy.</p>
<p>If we do the math, then it takes at least 8 seconds until a task will be stopped if it is unhealthy due to a bug. So, now under deploy, we have a new entry called <kbd>monitor</kbd>. This entry defines how long newly deployed tasks should be monitored for health and whether or not to continue with the next batch in the rolling update. Here, in this sample, we have given it <kbd>10</kbd> seconds. This is slightly more than the 8 seconds we calculated it takes to discover that a defective service has been deployed, so this is good.</p>
<p>We also have a new entry, <kbd>failure_action</kbd>, which defines what the orchestrator will do if it encounters a failure during the rolling update, such as that the service is unhealthy. By default, the action is just to stop the whole update process and leave the system in an intermediate state. The system is not down since it is a rolling update and at least some healthy instances of the service are still operational, but an operations engineer would be better at taking a look and fixing the problem.</p>
<p>In our case, we have defined the action to be a <kbd>rollback</kbd>. Thus, in case of failure, SwarmKit will automatically revert all tasks that have been updated back to their previous version.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Blue–green deployments</h1>
                
            
            
                
<p>In <a href="bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml" target="_blank">Chapter 9</a>, <em>Distributed Application Architecture</em>, we discussed what blue-green deployments are, in an abstract way. It turns out that, on Docker Swarm, we cannot really implement blue-green deployments for arbitrary services. The service discovery and load balancing between two services running in Docker Swarm are part of the Swarm routing mesh and cannot be (easily) customized.</p>
<p>If <strong>Service A</strong> wants to call <strong>Service B</strong>, then Docker does this implicitly. Docker, given the name of the target service, will use the Docker <strong>DNS</strong> service to resolve this name to a <strong>virtual IP</strong> (<strong>VIP</strong>) address. When the request is then targeted at the <strong>VIP</strong>, the Linux <strong>IPVS</strong> service will do another lookup in the Linux kernel IP tables with the <strong>VIP</strong> and load balance the request to one of the physical IP addresses of the tasks of the service represented by the <strong>VIP</strong>, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f7103312-f96e-4301-8f61-6a75c5c74a43.png" style="width:34.25em;height:20.33em;"/></p>
<p>How service discovery and load balancing work in Docker Swarm</p>
<p>Unfortunately, there is no easy way to intercept this mechanism and replace it with a custom behavior. But this would be needed to allow for a true blue-green deployment of <strong>Service B</strong>, which is the target service in our example. As we will see in <a href="cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml" target="_blank">Chapter 16</a>, <em>Deploying, Updating, and Securing an Application with Kubernetes,</em> Kubernetes is more flexible in this area.</p>
<p>That being said, we can always deploy the public-facing services in a blue-green fashion. We can use interlock 2 and its layer 7 routing mechanism to allow for a true blue-green deployment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Canary releases</h1>
                
            
            
                
<p>Technically speaking, rolling updates are a kind of canary release. But due to their lack of seams, where you could plug customized logic into the system, rolling updates are only a very limited version of canary releases.</p>
<p>True canary releases require us to have more fine-grained control over the update process. Also, true canary releases do not take down the old version of the service until 100% of the traffic has been funneled through the new version. In that regard, they are treated like blue-green deployments.</p>
<p>In a canary release scenario, we don't just want to use things such as health checks as deciding factors regarding whether or not to funnel more and more traffic through the new version of the service; we also want to consider external input in the decision-making process, such as metrics that are collected and aggregated by a log aggregator or tracing information. An example that could be used as a decision-maker includes conformance to <strong>service-level agreements</strong> (<strong>SLAs</strong>), namely if the new version of the service shows response times that are outside of the tolerance band. This can happen if we add new functionality to an existing service, yet this new functionality degrades the response time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Storing configuration data in the swarm</h1>
                
            
            
                
<p>If we want to store non-sensitive data such as configuration files in Docker Swarm, then we can use Docker configs. Docker configs are very similar to Docker secrets, which we will discuss in the next section. The main difference is that config values are not encrypted at rest, while secrets are. Docker configs can only be used in Docker Swarm, that is, they cannot be used in your non-Swarm development environment. Docker configs are mounted directly into the container's filesystem. Configuration values can either be strings or binary values up to a size of 500 KB.</p>
<p>With the use of Docker configs, you can separate the configuration from Docker images and containers. This way, your services can easily be configured with environment-specific values. The production swarm environment has different configuration values than the staging swarm, which in turn has different config values than the development or integration environment.</p>
<p>We can add configs to services and also remove them from running services. Configs can even be shared among different services running in the swarm.</p>
<p>Now, let's create some Docker configs:</p>
<ol>
<li>First, we start with a simple string value:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>echo "Hello world" | docker config create hello-config -<br/></strong>rrin36epd63pu6w3gqcmlpbz0</pre>
<p style="padding-left: 60px">The preceding command creates the <kbd>Hello world</kbd> configuration value and uses it as input to the config named <kbd>hello-config</kbd>. The output of this command is the unique <kbd>ID</kbd> of this new config that's being stored in the swarm.</p>
<ol start="2">
<li>Let's see what we got and use the list command to do so:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker config ls<br/></strong>ID                         NAME           CREATED              UPDATED<br/>rrin36epd63pu6w3gqcmlpbz0  hello-config   About a minute ago   About a minute ago<strong><br/></strong></pre>
<p style="padding-left: 60px">The output of the list command shows the <kbd>ID</kbd> and the <kbd>NAME</kbd> of the config we just created, as well as its <kbd>CREATED</kbd> and (last) updated time. But since configs are non-confidential, we can do more and even output the content of a config, like so:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker config docker config inspect hello-config</strong><br/>[<br/>    {<br/>        "ID": "rrin36epd63pu6w3gqcmlpbz0",<br/>        "Version": {<br/>            "Index": 11<br/>        },<br/>        "CreatedAt": "2019-11-30T07:59:20.6340015Z",<br/>        "UpdatedAt": "2019-11-30T07:59:20.6340015Z",<br/>        "Spec": {<br/>            "Name": "hello-config",<br/>            "Labels": {},<br/>            "Data": "SGVsbG8gd29ybGQK"<br/>        }<br/>    }<br/>]</pre>
<p style="padding-left: 60px">Hmmm, interesting. In the <kbd>Spec</kbd> subnode of the preceding JSON-formatted output, we have the <kbd>Data</kbd> key with a value of <kbd>SGVsbG8gd29ybGQK</kbd>. Didn't we just say that the config data is not encrypted at rest? It turns out that the value is just our string encoded as <kbd>base64</kbd>, as we can easily verify:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>echo 'SGVsbG8gd29ybGQK' | base64 -d</strong><br/>Hello world</pre>
<p>So far, so good.</p>
<p>Now, let's define a somewhat more complicated Docker config. Let's assume we are developing a Java application. Java's preferred way of passing configuration data to the application is the use of so-called <kbd>properties</kbd> files. A <kbd>properties</kbd> file is just a text file containing a list of key-value pairs. Let's take a look:</p>
<ol>
<li>Let's create a file called <kbd>my-app.properties</kbd> and add the following content to it:</li>
</ol>
<pre style="padding-left: 60px">username=pguser<br/>database=products<br/>port=5432<br/>dbhost=postgres.acme.com</pre>
<ol start="2">
<li>Save the file and create a Docker config called <kbd>app.properties</kbd> from it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker config create app.properties ./my-app.properties</strong><br/>2yzl73cg4cwny95hyft7fj80u</pre>
<p style="padding-left: 60px">Now, we can use this (somewhat contrived) command to get the clear text value of the config we just created:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker config inspect app.properties | jq .[].Spec.Data | xargs echo | base64 -d<br/></strong>username=pguser<br/>database=products<br/>port=5432<br/>dbhost=postgres.acme.com</pre>
<p style="padding-left: 60px">This is exactly what we expected.</p>
<ol start="3">
<li>Now, let's create a Docker service that uses the preceding config. For simplicity, we will be using the nginx image to do so:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker service create \</strong><br/><strong>    --name nginx \</strong><br/><strong>    --config source=app.properties,target=/etc/my-app/conf/app.properties,mode=0440 \</strong><br/><strong>    nginx:1.13-alpine</strong><br/><br/>p3f686vinibdhlnrllnspqpr0<br/>overall progress: 1 out of 1 tasks<br/>1/1: running [==================================================&gt;]<br/>verify: Service converged</pre>
<p style="padding-left: 60px">The interesting part in the preceding service <kbd>create</kbd> command is the line that contains <kbd>--config</kbd>. With this line, we're telling Docker to use the config named <kbd>app.properties</kbd> and mount it as a file at <kbd>/etc/my-app/conf/app.properties</kbd> inside the container. Furthermore, we want that file to have the mode <kbd>0440</kbd> assigned to it.</p>
<p style="padding-left: 60px">Let's see what we got:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker service ps nginx</strong><br/>ID            NAME     IMAGE              NODE DESIRED    STATE    CURRENT STATE ...<br/>b8lzzwl3eg6y  nginx.1  nginx:1.13-alpine  node-1  Running  Running 2 minutes ago</pre>
<p style="padding-left: 60px">In the preceding output, we can see that the only instance of the service is running on node <kbd>node-1</kbd>. On this node, I can now list the containers to get the <kbd>ID</kbd> of the nginx instance:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker container ls</strong><br/>CONTAINER ID   IMAGE               COMMAND                  CREATED         STATUS         PORTS ...<br/>bde33d92cca7   nginx:1.13-alpine   "nginx -g 'daemon of…"   5 minutes ago   Up 5 minutes   80/tcp ...</pre>
<p style="padding-left: 60px">Finally, we can <kbd>exec</kbd> into that container and output the value of the <kbd>/etc/my-app/conf/app.properties</kbd> file:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker exec bde33 cat /etc/my-app/conf/app.properties</strong><br/>username=pguser<br/>database=products<br/>port=5432<br/>dbhost=postgres.acme.com</pre>
<p style="padding-left: 60px">No surprise here; this is exactly what we expected.</p>
<p style="padding-left: 60px">Docker configs can, of course, also be removed from the swarm, but only if they are not being used. If we try to remove the config we were just using previously, without first stopping and removing the service, we would get the following output:</p>
<pre style="padding-left: 60px"><strong>$ docker config rm app.properties</strong><br/>Error response from daemon: rpc error: code = InvalidArgument desc = config 'app.properties' is in use by the following service: nginx</pre>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">We get an error message in which Docker is nice enough to tell us that the config is being used by our service called <kbd>nginx</kbd>. This behavior is somewhat similar to what we are used to when working with Docker volumes.</p>
<p style="padding-left: 60px">Thus, first, we need to remove the service and then we can remove the config:</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker service rm nginx</strong><br/>nginx<br/><strong>$</strong> <strong>docker config rm app.properties</strong><br/>app.properties</pre>
<p>It is important to note once more that Docker configs should never be used to store confidential data such as secrets, passwords, or access keys and key secrets.</p>
<p>In the next section, we will discuss how to handle confidential data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Protecting sensitive data with Docker secrets</h1>
                
            
            
                
<p>Secrets are used to work with confidential data in a secure way. Swarm secrets are secure at rest and in transit. That is, when a new secret is created on a manager node, and it can only be created on a manager node, its value is encrypted and stored in the raft consensus storage. This is why it is secure at rest. If a service gets a secret assigned to it, then the manager reads the secret from storage, decrypts it, and forwards it to all the containers who are instances of the swarm service that requested the secret. Since node-to-node communication in Docker Swarm uses mutual <strong>transport layer security</strong> (<strong>TLS</strong>), the secret value, although decrypted, is still secure in transit. The manager forwards the secret only to the worker nodes that a service instance is running on. Secrets are then mounted as files into the target container. Each secret corresponds to a file. The name of the secret will be the name of the file inside the container, and the value of the secret is the content of the respective file. Secrets are never stored on the filesystem of a worker node and are instead mounted using <kbd>tmpFS</kbd> into the container. By default, secrets are mounted into the container at <kbd>/run/secrets</kbd>, but you can change that to any custom folder.</p>
<p>It is important to note that secrets will not be encrypted on Windows nodes since there is no concept similar to <kbd>tmpfs</kbd>. To achieve the same level of security that you would get on a Linux node, the administrator should encrypt the disk of the respective Windows node.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating secrets</h1>
                
            
            
                
<p>First, let's see how we can actually create a secret:</p>
<pre><strong>$ echo "sample secret value" | docker secret create sample-secret -</strong> </pre>
<p>This command creates a secret called <kbd>sample-secret</kbd> with the <kbd>sample secret value</kbd> value. Please note the hyphen at the end of the <kbd>docker secret create</kbd> command. This means that Docker expects the value of the secret from standard input. This is exactly what we're doing by piping the <kbd>sample secret value</kbd> value into the <kbd>create</kbd> command.</p>
<p>Alternatively, we can use a file as the source for the secret value:</p>
<pre><strong>$ docker secret create other-secret ~/my-secrets/secret-value.txt</strong></pre>
<p>Here, the value of the secret with the name <kbd>other-secret</kbd> is read from a file called <kbd>~/my-secrets/secret-value.txt</kbd>. Once a secret has been created, there is no way to access the value of it. We can, for example, list all our secrets to get the following output:</p>
<div><img src="img/2b31dcdd-9f1d-44eb-ac20-1d8263bf8f1c.png" style="width:48.42em;height:6.50em;"/></div>
<p>List of all secrets</p>
<p>In this list, we can only see the <kbd>ID</kbd> and <kbd>NAME</kbd> of the secret, plus some other metadata, but the actual value of the secret is not visible. We can also use <kbd>inspect</kbd> on a secret, for example, to get more information about <kbd>other-secret</kbd>:</p>
<div><img src="img/82cad794-8a35-44db-81d7-51a9bbd67b29.png" style="width:29.58em;height:20.83em;"/></div>
<p>Inspecting a swarm secret</p>
<p>Even here, we do not get the value of the secret back. This is, of course, intentional: a secret is a secret and thus needs to remain confidential. We can assign labels to secrets if we want and we can even use a different driver to encrypt and decrypt the secret if we're not happy with what Docker delivers out of the box.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using a secret</h1>
                
            
            
                
<p>Secrets are used by services that run in the swarm. Usually, secrets are assigned to a service at creation time. Thus, if we want to run a service called <kbd>web</kbd> and assign it a secret, say, <kbd>api-secret-key</kbd>, the syntax would look as follows:</p>
<pre><strong>$ docker service create --name web \</strong><br/><strong>    --secret api-secret-key \</strong><br/><strong>    --publish 8000:8000 \</strong><br/><strong>    fundamentalsofdocker/whoami:latest</strong></pre>
<p>This command creates a service called <kbd>web</kbd> based on the <kbd>fundamentalsofdocker/whoami:latest</kbd> image, publishes the container port <kbd>8000</kbd> to port <kbd>8000</kbd> on all swarm nodes, and assigns it the secret called <kbd>api-secret-key</kbd>.</p>
<p class="mce-root"/>
<p>This will only work if the secret called <kbd>api-secret-key</kbd> is defined in the swarm; otherwise, an error will be generated with the text <kbd>secret not found: api-secret-key</kbd>. Thus, let's create this secret now:</p>
<pre><strong>$ echo "my secret key" | docker secret create api-secret-key -</strong></pre>
<p>Now, if we rerun the service <kbd>create</kbd> command, it will succeed:</p>
<div><img src="img/4dd798d2-4a33-41b6-ae25-39f71d55386e.png" style="width:36.25em;height:12.25em;"/></div>
<p>Creating a service with a secret</p>
<p>Now, we can use <kbd>docker service ps web</kbd> to find out on which node the sole service instance has been deployed, and then <kbd>exec</kbd> into this container. In my case, the instance has been deployed to <kbd>node-3</kbd>, so I need to <kbd>SSH</kbd> into that node:</p>
<pre><strong>$ docker-machine ssh node-3</strong></pre>
<p>Then, I list all my containers on that node to find the one instance belonging to my service and copy its <kbd>container ID</kbd>. We can then run the following command to make sure that the secret is indeed available inside the container under the expected filename containing the secret value in clear text:</p>
<pre><strong>$ docker exec -it &lt;container ID&gt; cat /run/secrets/api-secret-key</strong></pre>
<p>Once again, in my case, this looks like this:</p>
<div><img src="img/397cb02b-4760-4f21-9a10-0364294c20b9.png" style="width:44.67em;height:4.33em;"/></div>
<p>A secret as a container sees it</p>
<p>If, for some reason, the default location where Docker mounts the secrets inside the container is not acceptable to you, you can define a custom location. In the following command, we mount the secret to <kbd>/app/my-secrets</kbd>:</p>
<pre><strong>$ docker service create --name web \</strong><br/><strong>    --name web \</strong><br/><strong>    -p 8000:8000 \</strong><br/><strong>    --secret source=api-secret-key,target=/run/my-secrets/api-secret-key \</strong><br/><strong>    fundamentalsofdocker/whoami:latest</strong></pre>
<p>In this command, we are using the extended syntax to define a secret that includes the destination folder.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simulating secrets in a development environment</h1>
                
            
            
                
<p>When working in development, we usually don't have a local swarm on our machine. But secrets only work in a swarm. So, <em>what can we do</em>? Well, luckily, this answer is really simple. Due to the fact that secrets are treated as files, we can easily mount a volume that contains the secrets into the container to the expected location, which by default is at <kbd>/run/secrets</kbd>.</p>
<p>Let's assume that we have a folder called <kbd>./dev-secrets</kbd> on our local workstation. For each secret, we have a file named the same as the secret name and with the unencrypted value of the secret as the content of the file. For example, we can simulate a secret called <kbd>demo-secret</kbd> with a secret value of <kbd>demo secret value</kbd> by executing the following command on our workstation:</p>
<pre><strong>$ echo "demo secret value" &gt; ./dev-secrets/sample-secret</strong></pre>
<p>Then, we can create a container that mounts this folder, like this:</p>
<pre><strong>$ docker container run -d --name whoami \</strong><br/><strong>    -p 8000:8000 \</strong><br/><strong>    -v $(pwd)/dev-secrets:/run/secrets \</strong><br/><strong>    fundamentalsofdocker/whoami:latest</strong></pre>
<p>The process running inside the container will be unable to distinguish these mounted files from the ones originating from a secret. So, for example, <kbd>demo-secret</kbd> is available as a file called <kbd>/run/secrets/demo-secret</kbd> inside the container and has the expected value <kbd>demo secret value</kbd>. Let's take a look at this in more detail in the following steps:</p>
<ol>
<li>To test this, we can <kbd>exec</kbd> a shell inside the preceding container:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container exec -it whoami /bin/bash</strong></pre>
<ol start="2">
<li>Now, we can navigate to the <kbd>/run/secrets</kbd> folder and display the content of the <kbd>demo-secret</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>/# cd /run/secrets</strong><br/><strong>/# cat demo-secret</strong><br/>demo secret value<strong><br/></strong></pre>
<p>Next, we will be looking at secrets and legacy applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Secrets and legacy applications</h1>
                
            
            
                
<p>Sometimes, we want to containerize a legacy application that we cannot easily, or do not want to, change. This legacy application might expect a secret value to be available as an environment variable. <em>How are we going to deal with this now?</em> Docker presents us with the secrets as files but the application is expecting them in the form of environment variables.</p>
<p>In this situation, it is helpful to define a script that runs when the container is started (a so-called entry point or startup script). This script will read the secret value from the respective file and define an environment variable with the same name as the file, assigning the new variable the value read from the file. In the case of a secret called <kbd>demo-secret</kbd> whose value should be available in an environment variable called <kbd>DEMO_SECRET,</kbd> the necessary code snippet in this startup script could look like this:</p>
<pre><strong>export DEMO_SECRET=$(cat /run/secrets/demo-secret)</strong></pre>
<p>Similarly, let's say we have a legacy application that expects the secret values to be present as an entry in, say, a YAML configuration file located in the <kbd>/app/bin</kbd> folder and called <kbd>app.config</kbd>, whose relevant part looks like this:</p>
<pre>...
<br/>secrets:<br/>  demo-secret: "&lt;&lt;demo-secret-value&gt;&gt;"<br/>  other-secret: "&lt;&lt;other-secret-value&gt;&gt;"<br/>  yet-another-secret: "&lt;&lt;yet-another-secret-value&gt;&gt;"<br/>...</pre>
<p>Our initialization script now needs to read the secret value from the <kbd>secret</kbd> file and replace the corresponding placeholder in the config file with the <kbd>secret</kbd> value. For <kbd>demo-secret</kbd>, this could look like this:</p>
<pre>file=/app/bin/app.conf<br/>demo_secret=$(cat /run/secret/demo-secret)<br/>sed -i "s/&lt;&lt;demo-secret-value&gt;&gt;/$demo_secret/g" "$file"</pre>
<p>In the preceding snippet, we're using the <kbd>sed</kbd> tool to replace a placeholder with a value in place. We can use the same technique for the other two secrets in the config file.</p>
<p>We put all the initialization logic into a file called <kbd>entrypoint.sh</kbd>, make this file executable and, for example, add it to the root of the container's filesystem. Then, we define this file as <kbd>ENTRYPOINT</kbd> in the <kbd>Dockerfile</kbd>, or we can override the existing <kbd>ENTRYPOINT</kbd> of an image in the <kbd>docker container run</kbd> command.</p>
<p>Let's make a sample. Let's assume that we have a legacy application running inside a container defined by the <kbd>fundamentalsofdocker/whoami:latest</kbd> image that expects a secret called <kbd>db_password</kbd> to be defined in a file, <kbd>whoami.conf</kbd>, in the application folder. Let's take a look at these steps:</p>
<ol>
<li>We can define a file, <kbd>whoami.conf</kbd>, on our local machine that contains the following content:</li>
</ol>
<pre style="padding-left: 60px">database:<br/>  name: demo<br/>  db_password: "&lt;&lt;db_password_value&gt;&gt;"<br/>others:<br/>  val1=123<br/>  val2="hello world"</pre>
<p style="padding-left: 60px">The important part is line 3 of this snippet. It defines where the secret value has to be put by the startup script.</p>
<ol start="2">
<li>Let's add a file called <kbd>entrypoint.sh</kbd> to the local folder that contains the following content:</li>
</ol>
<pre style="padding-left: 60px">file=/app/whoami.conf<br/>db_pwd=$(cat /run/secret/db-password)<br/>sed -i "s/&lt;&lt;db_password_value&gt;&gt;/$db_pwd/g" "$file"<br/><br/>/app/http</pre>
<p style="padding-left: 60px">The last line in the preceding script stems from the fact that this is the start command that was used in the original <kbd>Dockerfile</kbd>.</p>
<ol start="3">
<li>Now, change the mode of this file to an executable:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo chmod +x ./entrypoint.sh</strong></pre>
<p style="padding-left: 60px">Now, we define a <kbd>Dockerfile</kbd> that inherits from the <kbd>fundamentalsofdocker/whoami:latest</kbd> image.</p>
<ol start="4">
<li>Add a file called <kbd>Dockerfile</kbd> to the current folder that contains the following content:</li>
</ol>
<pre style="padding-left: 60px">FROM fundamentalsofdocker/whoami:latest<br/>COPY ./whoami.conf /app/<br/>COPY ./entrypoint.sh /<br/>CMD ["/entrypoint.sh"]</pre>
<ol start="5">
<li>Let's build the image from this <kbd>Dockerfile</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker image build -t secrets-demo:1.0 .</strong></pre>
<ol start="6">
<li>Once the image has been built, we can run a service from it. But before we can do that, we need to define the secret in Swarm:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ echo "passw0rD123" | docker secret create demo-secret -</strong></pre>
<ol start="7">
<li>Now, we can create a service that uses the following secret:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker service create --name demo \</strong><br/><strong>    --secret demo-secret \</strong><br/><strong>    secrets-demo:1.0</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Updating secrets</h1>
                
            
            
                
<p>At times, we need to update a secret in a running service since secrets could be leaked out to the public or be stolen by malicious people, such as hackers. In this case, we need to change our confidential data since the moment it is leaked to a non-trusted entity, it has to be considered as insecure.</p>
<p>Updating secrets, like any other update, has to happen in a way that requires zero-downtime. Docker SwarmKit supports us in this regard.</p>
<p>First, we create a new secret in the swarm. It is recommended to use a versioning strategy when doing so. In our example, we use a version as a postfix of the secret name. We originally started with the secret named <kbd>db-password</kbd> and now the new version of this secret is called <kbd>db-password-v2</kbd>:</p>
<pre><strong>$ echo "newPassw0rD" | docker secret create db-password-v2 -</strong></pre>
<p>Let's assume that the original service that used the secret had been created like this:</p>
<pre><strong>$ docker service create --name web \</strong><br/><strong>    --publish 80:80</strong><br/><strong>    --secret db-password</strong><br/><strong>    nginx:alpine</strong></pre>
<p>The application running inside the container was able to access the secret at <kbd>/run/secrets/db-password</kbd>. Now, SwarmKit does not allow us to update an existing secret in a running service, so we have to remove the now obsolete version of the secret and then add the new one. Let's start with removal with the following command:</p>
<pre><strong>$ docker service update --secret-rm db-password web</strong></pre>
<p>Now, we can add the new secret with the following command:</p>
<pre><strong>$ docker service update \</strong><br/><strong>    --secret-add source=db-password-v2,target=db-password \</strong><br/><strong>    web</strong></pre>
<p>Please note the extended syntax of <kbd>--secret-add</kbd> with the <kbd>source</kbd> and <kbd>target</kbd> parameters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned how SwarmKit allows us to update services without requiring downtime. We also discussed the current limits of SwarmKit in regard to zero-downtime deployments. In the second part of this chapter, we introduced secrets as a means to provide confidential data to services in a highly secure way.</p>
<p>In the next chapter, we will introduce the currently most popular container orchestrator, Kubernetes. We'll discuss the objects that are used to define and run a distributed, resilient, robust, and highly available application in a Kubernetes cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that's used to locally deploy a Kubernetes application, and also demonstrate the integration of Kubernetes with Docker for macOS and Docker for Windows.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<p>To assess your understanding of the topics that were discussed in this chapter, please answer the following questions:</p>
<ol start="6">
<li style="font-weight: 400">In a few simple sentences, explain to an interested layman what zero-downtime deployment means.</li>
<li style="font-weight: 400">How does SwarmKit achieve zero-downtime deployments?</li>
<li style="font-weight: 400">Contrary to traditional (non-containerized) systems, why does a rollback in Docker Swarm just work? Explain this in a few short sentences.</li>
<li style="font-weight: 400">Describe two to three characteristics of a Docker secret.</li>
<li>You need to roll out a new version of the <kbd>inventory</kbd> service. What does your command look like? Here is some more information:
<ul>
<li>The new image is called <kbd>acme/inventory:2.1</kbd>.</li>
<li>We want to use a rolling update strategy with a batch size of two tasks.</li>
<li>We want the system to wait for one minute after each batch.</li>
</ul>
</li>
<li style="font-weight: 400">You need to update an existing service named <kbd>inventory</kbd> with a new password that is provided through a Docker secret. The new secret is called <kbd>MYSQL_PASSWORD_V2</kbd>. The code in the service expects the secret to be called <kbd>MYSQL_PASSWORD</kbd>. What does the update command look like? (Note that we do not want the code of the service to be changed!)</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>Here are some links to external sources:</p>
<ul>
<li>Apply rolling updates to a service, at <a href="https://dockr.ly/2HfGjlD" target="_blank">https://dockr.ly/2HfGjlD</a></li>
<li>Managing sensitive data with Docker secrets, at <a href="https://dockr.ly/2vUNbuH" target="_blank">https://dockr.ly/2vUNbuH</a></li>
<li>Introducing Docker secrets management, at <a href="https://dockr.ly/2k7zwzE" target="_blank">https://dockr.ly/2k7zwzE</a></li>
<li>From env variables to Docker secrets, at <a href="https://bit.ly/2GY3UUB" target="_blank">https://bit.ly/2GY3UUB</a></li>
</ul>


            

            
        
    </body></html>