- en: Defining Logging Strategy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义日志策略
- en: Most software today is very much like an Egyptian pyramid with millions of bricks
    piled on top of each other, with no structural integrity, but just done by brute
    force and thousands of slaves.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的大多数软件就像埃及金字塔一样，数百万块砖头堆叠在一起，毫无结构性，只是靠蛮力和成千上万的奴隶完成的。
- en: –Alan Kay
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: –艾伦·凯
- en: We've reached the point where we have a fully operating Swarm cluster and a
    defined Continuous Deployment Pipeline that'll update our services on each commit.
    Now we can spend time coding and pushing commits to our repository knowing that
    the rest of the process is automated. We can, finally, spend our time on tasks
    that bring real value to the organization we're working for. We can dedicate our
    time to producing new features for the services we're working on. However, when
    something goes wrong, we need to stop churning new features and investigate the
    problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经达到了拥有一个完全运行的 Swarm 集群和一个定义好的持续部署管道的阶段，管道会在每次提交时更新我们的服务。现在，我们可以专注于编写代码并推送提交到我们的代码库，知道其余的流程是自动化的。我们最终可以将时间花在对组织真正有价值的任务上。我们可以把时间投入到为我们所工作的服务开发新功能。然而，当发生问题时，我们需要停止开发新功能并调查问题。
- en: The first thing we tend to do when we detect an issue is to check logs. They
    are, by no means, not the only source of data we can use for debugging problem.
    We'll also need a lot of metrics (more on that in the next chapter). However,
    even if logs are not the only thing we should look at, they are often a good start.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在发现问题时通常做的第一件事是查看日志。日志并不是我们调试问题时唯一能用的数据来源。我们还需要大量的指标（下一章会详细讲解）。然而，即使日志不是唯一需要查看的内容，它们通常是一个很好的起点。
- en: '**A note to The DevOps 2.0 Toolkit readers**'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**《The DevOps 2.0 Toolkit》读者的提示**'
- en: The text that follows is identical to the one published in *The DevOps 2.0 Toolkit*.
    If it is still fresh in your mind, feel free to jump to the *Setting up LogStash
    and ElasticSearch as the logging Database *(`#logging-es`) sub-chapter. Since
    I wrote the *2.0*, I discovered a few better ways to treat logs, especially inside
    a Swarm cluster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容与《The DevOps 2.0 Toolkit》一书中的内容相同。如果它仍然记忆犹新，可以直接跳到*设置 LogStash 和 ElasticSearch
    作为日志数据库*（`#logging-es`）子章节。自从我编写了*2.0*版本后，我发现了一些更好的处理日志的方法，特别是在 Swarm 集群内部。
- en: Our exploration of DevOps practices and tools led us towards clustering and
    scaling. As a result, we developed a system that allows us to deploy services
    to a cluster, in an easy and efficient way. The result is an ever increasing number
    of containers running inside a cluster consisting of, potentially, many servers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 DevOps 实践和工具的探索引导我们走向了集群化和扩展。因此，我们开发了一种系统，使我们能够以简单高效的方式将服务部署到集群中。结果是，集群内的容器数量不断增加，这些容器可能运行在多台服务器上。
- en: Monitoring one server is easy. Monitoring many services on a single server poses
    some difficulties. Monitoring many services on many servers requires a whole new
    way of thinking and a new set of tools. As you start embracing microservices,
    containers, and clusters, the number of created services and their instances will
    begin increasing rapidly. The same holds true for servers that form the cluster.
    We cannot, anymore, log into a node and look at logs. There are too many to look
    at. On top of that, they are distributed among many servers. While yesterday we
    had two instances of a service deployed on a single server, tomorrow we might
    have eight instances deployed to six servers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 监控一个服务器很简单，但在单台服务器上监控多个服务会带来一些困难。在多台服务器上监控多个服务需要全新的思维方式和一套新的工具。随着你开始拥抱微服务、容器和集群，创建的服务及其实例数量将迅速增加。对于构成集群的服务器而言，也是如此。我们不能再像以前那样登录到一个节点并查看日志了，因为日志实在是太多了。而且，它们分布在多个服务器上。昨天我们可能在单台服务器上部署了两个实例的服务，但明天我们可能会在六台服务器上部署八个实例。
- en: We need historical and (near) real time information about our system. That information
    can be in the form of logs, hardware utilization, health checking, network traffic,
    and many other things. The need to store historical data is not new and has been
    in use for a long time. However, the direction that information travels has changed
    over time. While, in the past, most solutions were based on centralized data collectors,
    today, due to the very dynamic nature of services and servers, we tend to have
    them decentralized.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要系统的历史信息和（近）实时信息。这些信息可以是日志、硬件利用率、健康检查、网络流量等多种形式。存储历史数据的需求并不新鲜，早已在使用中。然而，信息传输的方向随着时间发生了变化。过去，大多数解决方案依赖于集中式数据收集器，而今天，由于服务和服务器的动态特性，我们倾向于将其去中心化。
- en: What we need for cluster logging and monitoring is a combination of decentralized
    data collectors that are sending information to a centralized parsing service
    and data storage. There are plenty of products specially designed to fulfill this
    requirement, ranging from on-premise to cloud solutions, and everything in between.
    *FluentD* ([http://www.fluentd.org/](http://www.fluentd.org/)), *Loggly* ([https://www.loggly.com/](https://www.loggly.com/)),
    *GrayLog* ([https://www.graylog.org/](https://www.graylog.org/)), *Splunk* ([http://www.splunk.com/](http://www.splunk.com/)),
    and *DataDog* ([https://www.datadoghq.com/](https://www.datadoghq.com/)) are only
    a few of the solutions we can employ. I chose to show you the concepts through
    the ELK stack (*ElasticSearch* ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)),
    *LogStash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)),
    and *Kibana* ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana))).
    The stack has the advantage of being free, well documented, efficient, and widely
    used. *ElasticSearch* ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch))
    established itself as one of the best databases for real-time search and analytics.
    It is distributed, scalable, highly available, and provides a sophisticated API.
    *LogStash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash))
    allows us to centralize data processing. It can be easily extended to custom data
    formats and offers a lot of plugins that can fulfill almost any need. Finally,
    *Kibana* ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana))
    is an analytics and visualization platform with intuitive interface sitting on
    top of ElasticSearch.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对集群日志和监控的需求是一个结合了去中心化数据收集器的系统，这些收集器将信息发送到一个集中式的解析服务和数据存储。有许多专门设计用于满足这一需求的产品，从本地解决方案到云解决方案应有尽有。*FluentD* ([http://www.fluentd.org/](http://www.fluentd.org/))、*Loggly* ([https://www.loggly.com/](https://www.loggly.com/))、*GrayLog* ([https://www.graylog.org/](https://www.graylog.org/))、*Splunk* ([http://www.splunk.com/](http://www.splunk.com/))
    和 *DataDog* ([https://www.datadoghq.com/](https://www.datadoghq.com/)) 只是我们可以使用的一些解决方案。我选择通过
    ELK 堆栈来展示这些概念（*ElasticSearch* ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch))、*LogStash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash))
    和 *Kibana* ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana))）。该堆栈的优势在于它是免费的、文档完善、高效且广泛使用。*ElasticSearch* ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch))
    已经证明自己是实时搜索和分析的最佳数据库之一。它是分布式的、可扩展的、高可用的，并提供了一个复杂的 API。*LogStash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash))
    让我们能够集中处理数据。它可以轻松扩展到自定义数据格式，并提供了许多插件，几乎可以满足任何需求。最后，*Kibana* ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana))
    是一个分析和可视化平台，具有直观的界面，基于 ElasticSearch。
- en: The fact that we'll use the ELK stack does not mean that it is better than the
    other solutions. It all depends on specific use cases and particular needs. I'll
    walk you through the principles of centralized logging and monitoring using the
    ELK stack. Once those principles are understood, you should have no problem applying
    them to a different stack if you choose to do so.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 ELK 堆栈并不意味着它比其他解决方案更好。一切都取决于特定的使用场景和需求。我将通过 ELK 堆栈的集中式日志和监控原则向您展示。一旦理解了这些原则，您应该能够轻松地将其应用到其他堆栈中，若您选择这样做的话。
- en: We switched the order of things and chose the tools before discussing the need
    for centralized logging. Let's remedy that.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调换了事务的顺序，并在讨论集中式日志需求之前就选择了工具。让我们来纠正这个问题。
- en: The need for centralized logging
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集中式日志的需求
- en: In most cases, log messages are written to files. That is not to say that files
    are the only, nor the most efficient way of storing logs. However, since most
    teams are using file-based logs in one form or another, for the time being, I'll
    assume that is your case as well. If it is, we identified the first thing we should
    fix. Containers expect us to send logs to `stdout` and `stderr`. Only log entries
    forwarded to the standard output are retrievable with `docker logs` command. Moreover,
    tools designed to work with container logs will expect just that. They'll assume
    that entries are not written to a file but sent to the output. Even without containers,
    I believe that `stdout` and `stderr` are where our services should log things.
    However, that's a story for some other time. For now, we'll concentrate on containers
    and assume that you are outputting your logs to `stdout` and `stderr.` If you're
    not, most logging libraries will allow you to change your logging destination
    to standard output and error.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，日志消息是写入文件的。这并不是说文件是唯一的方式，或者是存储日志最有效的方式。然而，由于大多数团队以某种形式使用基于文件的日志，因此暂时假设你也是如此。如果是这样，我们已经确定了需要修复的第一件事。容器期望我们将日志发送到`stdout`和`stderr`。只有转发到标准输出的日志条目才能通过`docker
    logs`命令检索。此外，专为容器日志设计的工具将只期待这一点。它们假设日志条目不是写入文件，而是发送到输出。即便没有容器，我相信`stdout`和`stderr`应该是我们服务记录日志的地方。不过，这是另一个话题。现在，我们将集中讨论容器，并假设你已经将日志输出到`stdout`和`stderr`。如果没有，大多数日志库会允许你将日志目标更改为标准输出和错误。
- en: Most of the time, we do not care what is written in logs. When things are working
    well, there is not much need to spend valuable time browsing through them. A log
    is not a novel we read to pass the time with, nor it is a technical book we read
    as a way to improve our knowledge. Logs are there to provide valuable info when
    something, somewhere, went wrong.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，我们并不关心日志中写了什么。当一切正常时，没有太多必要花费宝贵的时间去浏览它们。日志不是我们用来打发时间的小说，也不是我们用来提升知识的技术书籍。日志存在的目的是在某些地方发生错误时，提供有价值的信息。
- en: The situation seems to be simple. We write information to logs that we ignore
    most of the time, and when something goes wrong, we consult them and find the
    cause of the problem in no time. At least, that's what many are hoping for. The
    reality is far more complicated than that. In all but the most trivial systems,
    the debugging process is much more challenging. Applications and services are
    almost always interconnected, and it is often not easy to know which one caused
    the problem. While it might manifest in one application, investigation often shows
    that the cause is in another. For example, a service might have failed to instantiate.
    After some time spent browsing its logs, we might discover that the cause is in
    the database. The service could not connect to it and failed to launch. We got
    the symptom, but not the cause. We need to switch to the database log to find
    it out. With this simple example, we've already gotten to the point where looking
    at one log is not enough.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 情况看起来很简单。我们把信息写入日志，而大多数时候忽略它们，当出现问题时，我们查阅日志，迅速找到问题的根源。至少，许多人是这样希望的。然而，现实远比这复杂。在除最简单的系统外，调试过程往往更加具有挑战性。应用程序和服务几乎总是相互关联的，而确定究竟是哪个服务引发了问题常常并不容易。虽然问题可能在某个应用程序中显现，但调查通常会发现，问题的根源在另一个地方。例如，一个服务可能未能实例化。经过一段时间浏览其日志后，我们可能发现根源在数据库中。该服务无法连接到数据库，从而导致启动失败。我们得到了症状，却没有得到原因。我们需要切换到数据库日志才能找出真相。通过这个简单的例子，我们已经得出结论：仅查看一个日志是远远不够的。
- en: With distributed services running on a cluster, the situation complicates exponentially.
    Which instance of the service is failing? Which server is it running on? What
    are the upstream services that initiated the request? What are the memory and
    hard disk usage in the node where the culprit resides? As you might have guessed,
    finding, gathering, and filtering the information needed for the successful discovery
    of the cause is often very complicated. The bigger the system, the harder it gets.
    Even with monolithic applications, things can easily get out of hand.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着分布式服务在集群上运行，情况变得更加复杂。是哪个实例的服务出现故障？它运行在哪台服务器上？请求是由哪些上游服务发起的？故障所在节点的内存和硬盘使用情况如何？正如你可能猜到的，找到、收集并过滤出成功发现问题根源所需的信息，通常是非常复杂的。系统越大，问题就越复杂。即使是单体应用，也很容易陷入困境。
- en: If a microservices approach is adopted, those problems are multiplied. Centralized
    logging is a must for all but the simplest and smallest systems. Instead, many
    of us, when things go wrong, start running from one server to another and jumping
    from one file to the other. Like a chicken with its head cut off - running around
    with no direction. We tend to accept the chaos logging creates, and consider it
    part of our profession.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果采用微服务架构，这些问题会成倍增加。集中式日志记录对所有系统都是必须的，除了最简单和最小的系统。相反，当问题发生时，我们中的许多人会开始从一台服务器跑到另一台服务器，从一个文件跳到另一个文件。就像一只丧失了头的鸡——到处乱跑，没有方向。我们往往接受日志所带来的混乱，并认为这是我们职业的一部分。
- en: 'What do we look for in centralized logging? As it happens, many things, but
    the most important are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在集中式日志记录中寻找什么？实际上有很多东西，但最重要的有以下几点：
- en: A way to parse data and send them to a central database in near real-time
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种解析数据并将其发送到中央数据库的近实时方式
- en: The capacity of the database to handle near real-time data querying and analytics
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库处理近实时数据查询和分析的能力
- en: A visual representation of the data through filtered tables, dashboards, and
    so on
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过过滤后的表格、仪表板等方式展示数据的可视化表示
- en: We already chose the tools that will be able to fulfill all those requirements
    (and more). The ELK stack (ElasticSearch, LogStash, and Kibana) can do all that.
    As in the case of all other tools we explored, this stack can easily be extended
    to satisfy the particular needs we'll set in front of us.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经选择了能够满足所有这些要求（甚至更多）工具。ELK 堆栈（ElasticSearch、LogStash 和 Kibana）可以做所有这些。与我们探索的其他工具一样，这个堆栈也可以轻松扩展，以满足我们设定的特定需求。
- en: Now that we have a vague idea what we want to accomplish, and have the tools
    to do that, let us explore a few of the logging strategies we can use. We'll start
    with the most commonly used scenario and, slowly, move towards more complicated
    and more efficient ways to define our logging strategy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对自己想要达成的目标有了一个模糊的了解，并且也拥有了实现目标的工具，让我们来探索一下可以使用的一些日志策略。我们将从最常见的场景开始，逐步过渡到更复杂、更高效的日志策略定义方法。
- en: Without further ado, let's create the environments we'll use to experiment with
    centralized logging and, later on, monitoring.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 话不多说，让我们创建将用于实验集中式日志记录和后续监控的环境。
- en: Setting up ElasticSearch as the logging database
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 ElasticSearch 设置为日志数据库
- en: 'As in quite a few cases before, we''ll start by creating the already familiar
    nodes (`swarm-1`, `swarm-2`, and `swarm-3`):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前许多情况一样，我们将从创建已经熟悉的节点（`swarm-1`、`swarm-2` 和 `swarm-3`）开始：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All the commands from this chapter are available in the `08-logging.sh` ([https://gist.github.com/vfarcic/c89b73ebd32dbf8f849531a842739c4d](https://gist.github.com/vfarcic/c89b73ebd32dbf8f849531a842739c4d))
    Gist.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 `08-logging.sh`（[https://gist.github.com/vfarcic/c89b73ebd32dbf8f849531a842739c4d](https://gist.github.com/vfarcic/c89b73ebd32dbf8f849531a842739c4d)）Gist中找到。
- en: 'The first service we''ll create is *Elastic Search* ([https://hub.docker.com/_/elasticsearch](https://hub.docker.com/_/elasticsearch)).
    Since we''ll need it to be accessible from a few other services, we''ll also create
    a network called `elk`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建的第一个服务是*Elastic Search*（[https://hub.docker.com/_/elasticsearch](https://hub.docker.com/_/elasticsearch)）。由于我们需要让它能够从其他一些服务访问，我们还将创建一个名为`elk`的网络：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After a few moments, the `elasticsearch` service will be up and running.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，`elasticsearch` 服务将启动并运行。
- en: 'We can check the status using the `service ps` command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`service ps`命令检查状态：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows (IDs and ERROR PORTS columns are removed for brevity):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为简洁起见，ID 和 ERROR PORTS 列已被删除）：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If `elasticsearch` is still not running, please wait a few moments before proceeding.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `elasticsearch` 仍未启动，请稍等片刻再继续操作。
- en: Now that we have a database where we can store our logs, the next step is to
    create a service that will parse log entries and forward the results to ElasticSearch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以存储日志的数据库，下一步是创建一个服务来解析日志条目，并将结果转发到 ElasticSearch。
- en: Setting up LogStash as the logs parser and forwarder
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 LogStash 设置为日志解析器和转发器
- en: 'We did *E* from the *ELK* stack. Now let''s move to *L*. *LogStash* requires
    a configuration file. We''ll use one that is already available inside the `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository. We’ll create a new directory, copy the `conf/logstash.conf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/conf/logstash.conf](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/logstash.conf))
    configuration, and use it inside the `logstash` service:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了*ELK*堆栈中的*E*部分。现在我们开始处理*L*部分。*LogStash*需要一个配置文件。我们将使用一个已经存在于`vfarcic/cloud-provisioning`（[https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning)）仓库中的配置文件。我们将创建一个新目录，复制`conf/logstash.conf`（[https://github.com/vfarcic/cloud-provisioning/blob/master/conf/logstash.conf](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/logstash.conf)）配置，并在`logstash`服务中使用它：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The content of the `logstash.conf` file is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`logstash.conf`文件的内容如下：'
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is a very simple *LogStash* configuration. If will listen on port `51415`
    for `syslog` entries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的*LogStash*配置。它将监听端口`51415`以接收`syslog`条目。
- en: Each entry will be sent to two outputs; `elasticsearch` and `stdout`. Since
    both `logstash` and `elasticsearch` will be attached to the same network, all
    we had to do is put the service name as the host.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个条目将被发送到两个输出：`elasticsearch`和`stdout`。由于`logstash`和`elasticsearch`将连接到同一网络，我们所要做的就是将服务名称作为主机名。
- en: The second output will send everything to `stdout`. Please note that this entry
    should be removed before running *LogStash* in production. It creates an unnecessary
    overhead that, if there are many services, can be substantial. The only reason
    we have it is to show you how logs are passing through LogStash. In production,
    you'll have no need to look at its output. Instead, you'll use Kibana to explore
    the logs from the whole system.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出将把所有内容发送到`stdout`。请注意，在生产环境中运行*LogStash*之前，应该删除这个条目。它会产生不必要的开销，如果有很多服务，可能会相当可观。我们之所以保留它，是为了展示日志如何通过LogStash传递。在生产环境中，你无需查看其输出，而是使用Kibana来浏览整个系统的日志。
- en: 'Let''s move on and create the second service:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续并创建第二个服务：
- en: '**A note to Windows users**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意**'
- en: 'For mounts used in the next command to work, you have to stop Git Bash from
    altering file system paths. Set this environment variable:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让下一个命令中的挂载生效，你需要阻止Git Bash更改文件系统路径。请设置此环境变量：
- en: '`export MSYS_NO_PATHCONV=1`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`export MSYS_NO_PATHCONV=1`'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We created a service called `logstash` and mounted the host volume `docker/logstash
    as /conf` inside the container. That way we'll have the configuration file currently
    residing on the host available inside the container.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`logstash`的服务，并将主机卷`docker/logstash`挂载到容器中的`/conf`。这样，我们可以在容器内访问当前位于主机上的配置文件。
- en: 'Please note that mounting a volume is not the best way to put the configuration
    inside the container. Instead, we should have built our own image with the configuration
    inside. We should have created a Dockerfile. It could be as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，挂载卷并不是将配置文件放入容器中的最佳方式。相反，我们应该构建一个包含配置文件的镜像。我们应该创建一个Dockerfile，示例如下：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This configuration file should not change often (if ever), so the option of
    creating a new image based on `logstash` is much better than mounting a volume.
    However, for simplicity reasons, we used the mount. Just remember to build your
    own image once you start applying what you learned from this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置文件不应频繁更改（如果有更改的话），因此基于`logstash`创建新镜像要比挂载卷更好。然而，为了简单起见，我们使用了挂载。只需记住，一旦开始应用本章所学内容，务必构建你自己的镜像。
- en: We also defined the environment variable `LOGSPOUT`. It is not relevant right
    now. We'll comment on it later on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了环境变量`LOGSPOUT`。目前它并不相关，我们稍后会解释它。
- en: 'The `logStash` service should be up and running by now. Let''s double check
    it:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`logStash`服务现在应该已启动并运行。让我们再检查一下：'
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output should be as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If the current state is still not running, please wait a few moments and repeat
    the `service ps` command. We can proceed only after `logstash` is operational.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前状态仍未运行，请稍等片刻并重复执行`service ps`命令。只有当`logstash`正常运行后，我们才能继续。
- en: 'Now we can confirm that `logStash` was initialized correctly. We''ll need to
    find out which node it is running in, get the `ID` of the container, and output
    the logs:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以确认`logStash`已正确初始化。我们需要找出它运行在哪个节点，获取容器的`ID`，并输出日志：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the previous command `logs` is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上一条命令`logs`的输出如下：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Pipeline main started` means that LogStash is running and waiting for input.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline main started` 表示 LogStash 正在运行并等待输入。'
- en: 'Before we set up a solution that will ship logs from all the containers inside
    the cluster, we''ll make an intermediary step and confirm that LogStash can indeed
    accept `syslog` entries on port `51415`. We''ll create a temporary service called
    `logger-test`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置一个解决方案来转发集群中所有容器的日志之前，我们将进行一个中间步骤，确认 LogStash 是否能在端口 `51415` 上接受 `syslog`
    条目。我们将创建一个名为 `logger-test` 的临时服务：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The service is attached to the `elk` network so that it can communicate with
    the `logstash` service.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务连接到 `elk` 网络，以便能够与 `logstash` 服务进行通信。
- en: We had to specify `restart-condition` as `none`. Otherwise, when the process
    is finished, the container would stop, Swarm would detect it as a failure and
    reschedule it. In other words, without the restart condition set to none, Swarm
    would enter into an endless loop trying to reschedule containers that almost immediately
    stop.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指定 `restart-condition` 为 `none`。否则，当进程完成时，容器会停止，Swarm 会将其视为故障并重新调度。换句话说，如果没有将重启条件设置为
    none，Swarm 会进入一个无限循环，试图重新调度几乎立即停止的容器。
- en: The command we're executing sends a `syslog` message `logger`, to `logstash`
    running on port `51415`. The message is `hello world`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的命令发送了一个 `syslog` 消息 `logger`，目标是运行在端口 `51415` 上的 `logstash`。消息内容是 `hello
    world`。
- en: 'Let''s output LogStash logs one more time:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再一次输出 LogStash 日志：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First Swarm had to download the debian image and, once the logger message was
    sent, LogStash had to start accepting entries. It takes a bit of time until LogStash
    processes the first entry. All subsequent entries will be processed almost instantly.
    If your output is not similar to the one above, please wait a moment and repeat
    the logs command.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Swarm 需要下载 debian 镜像，并且一旦发送了 logger 消息，LogStash 就必须开始接收日志条目。在 LogStash 处理第一个条目之前会稍微花一些时间，后续的条目几乎会立即处理。如果你的输出与上述不相同，请稍等片刻，并重新执行日志命令。
- en: As you can see, LogStash received the message hello world. It also recorded
    a few other fields like the `timestamp` and `host`. Ignore the error message `_grokparsefailure_sysloginput`.
    We could configure LogStash to parse `logger` messages correctly but, since we
    won't be using it anymore, it would be a waste of time. Soon we'll see a much
    better way to forward logs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，LogStash 收到了消息 hello world。它还记录了一些其他字段，如 `timestamp` 和 `host`。忽略错误信息
    `_grokparsefailure_sysloginput`。我们可以配置 LogStash 正确解析 `logger` 消息，但由于我们以后不再使用它，配置会浪费时间。很快我们将看到一种更好的方式来转发日志。
- en: LogStash acted as a parser of the message and forwarded it to ElasticSearch.
    At the moment, you'll have to take my word for it. Soon we'll see how are those
    messages stored and how we can explore them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LogStash 充当了消息的解析器，并将其转发到 ElasticSearch。目前，你只能相信我说的这些。很快我们将看到这些消息是如何存储的，以及我们如何浏览它们。
- en: 'We’ll remove the `logger-test` service. Its purpose was only to demonstrate
    that we have a LogStash instance that accepts `syslog` messages:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将移除 `logger-test` 服务。它的目的是仅仅演示我们有一个接受 `syslog` 消息的 LogStash 实例：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Sending messages by invoking logger is great but is not what we're trying to
    accomplish. The goal is to forward the logs from all the containers running anywhere
    inside the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 logger 发送消息是很好的，但这并不是我们想要实现的目标。我们的目标是转发来自集群中任何地方运行的所有容器的日志。
- en: Forwarding logs from all containers running anywhere inside a Swarm cluster
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转发来自 Swarm 集群中任何地方运行的所有容器的日志
- en: How can we forward logs from all the containers no matter where they’re running?
    One possible solution would be to *configure logging drivers* ([https://docs.docker.com/engine/admin/logging/overview/](https://docs.docker.com/engine/admin/logging/overview/)).
    We could use the `--log-driver` argument to specify a driver for each service.
    The driver could be `syslog` or any other supported option. That would solve our
    log shipping problem. However, using the argument for each service is tedious
    and, more importantly, we could easily forget to specify it for a service or two
    and discover the omission only after we encounter a problem and are in need of
    logs. Let's see if there is another option to accomplish the same result.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将所有容器的日志转发到指定位置，无论它们在哪个环境中运行？一种可能的解决方案是*配置日志驱动程序*（[https://docs.docker.com/engine/admin/logging/overview/](https://docs.docker.com/engine/admin/logging/overview/)）。我们可以使用`--log-driver`参数为每个服务指定一个驱动程序。驱动程序可以是`syslog`或任何其他支持的选项。这样可以解决我们的日志转发问题。然而，为每个服务使用这个参数会很繁琐，更重要的是，我们可能会忘记为某些服务指定它，直到我们遇到问题并需要日志时才发现遗漏。让我们看看是否有其他方法可以实现相同的结果。
- en: We could specify a log driver as a configuration option of the Docker daemon
    on each node. That would certainly make the setup easier. After all, there are
    probably fewer servers than services. If we were to choose between setting a driver
    when creating a service or as the daemon configuration, I'd choose the latter.
    However, we managed to get thus far without changing the default daemon configuration
    and I'd prefer if we can continue working without involving any special provisioning
    tools. Luckily, we still did not exhaust all our options.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将日志驱动程序作为每个节点上Docker守护进程的配置选项进行指定。这样肯定能简化设置。毕竟，服务器数量可能少于服务数量。如果我们必须在创建服务时设置驱动程序与在守护进程配置中设置之间做选择，我会选择后者。然而，我们到目前为止都没有更改默认的守护进程配置，我更希望能继续在不涉及特殊配置工具的情况下进行工作。幸运的是，我们还没有用尽所有的选项。
- en: We can ship logs from all our containers with the project called `logspout` ([https://github.com/gliderlabs/logspout](https://github.com/gliderlabs/logspout))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过名为`logspout`的项目来转发所有容器的日志（[https://github.com/gliderlabs/logspout](https://github.com/gliderlabs/logspout))
- en: LogSpout is a log router for Docker containers that runs inside Docker. It attaches
    to all containers on a host, then routes their logs wherever we want. It also
    has an extensible module system. It's a mostly stateless log appliance. It's not
    meant for managing log files or looking at history. It is just a tool to get your
    logs out to live somewhere else, where they belong.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LogSpout 是一个用于Docker容器的日志路由器，运行在Docker内部。它附加到主机上的所有容器，然后将它们的日志路由到我们想要的地方。它还有一个可扩展的模块系统。它是一个几乎无状态的日志设备，设计并非用于管理日志文件或查看历史记录。它只是一个将日志发送到其他位置的工具，那是它们应该存在的地方。
- en: If you go through the project documentation, you'll notice that there are no
    instructions on how to run it as a Docker service. That should not matter since,
    by this time, you can consider yourself an expert in creating services.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你浏览项目文档，你会注意到没有关于如何将其作为Docker服务运行的说明。这个问题不应该影响你，因为到目前为止，你应该已经是创建服务的专家了。
- en: What do we need from a service that should forward logs from all the containers
    running inside all the nodes that form a cluster? Since we want to forward them
    to LogStash that is already attached to the `elk` network, we should attach LogSpout
    to it as well. We need it to ship logs from all the nodes so the service should
    be global. It needs to know that the destination is the service called `logstash`
    and that it listens on port `51415`. Finally, one of the requirement for LogSpout
    is that the Docker socket from the host is mounted inside the service containers.
    That’s what it'll use to monitor the logs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要什么样的服务来转发所有节点上运行的容器日志？由于我们希望将它们转发到已连接到`elk`网络的LogStash，因此我们也应该将LogSpout连接到它。我们需要它从所有节点转发日志，所以服务应该是全局的。它需要知道目标是名为`logstash`的服务，并且该服务监听端口`51415`。最后，LogSpout的一个要求是Docker主机的套接字必须挂载到服务容器内。这是它监控日志所需要的。
- en: 'The command that creates the service that fulfills all those objectives and
    requirements is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 创建满足所有这些目标和要求的服务的命令如下：
- en: '**A note to Windows users**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows 用户注意**'
- en: 'For mounts used in the next command to work, you have to stop Git Bash from
    altering file system paths. Set this environment variable:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要使下一条命令中使用的挂载点生效，你必须停止Git Bash修改文件系统路径。设置这个环境变量：
- en: '`export MSYS_NO_PATHCONV=1`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`export MSYS_NO_PATHCONV=1`'
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We created a service called `logspout`, attached it to the `elk` network, set
    it to be global, and mounted the Docker socket. The command that will be executed
    once containers are created is `syslog://logstash:51415`. This tells LogSpout
    that we want to use `syslog` protocol to send logs to `logstash` running on port
    `51415`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`logspout`的服务，将其连接到`elk`网络，设置为全局模式，并挂载了Docker套接字。容器创建后将执行的命令是`syslog://logstash:51415`。这告诉LogSpout，我们希望使用`syslog`协议将日志发送到运行在`51415`端口的`logstash`。
- en: This project is an example of the usefulness behind the Docker Remote API. The
    `logspout` containers will use it to retrieve the list of all currently running
    containers and stream their logs. This is already the second product inside our
    cluster that uses the API (the first being *Docker Flow Swarm Listener* ([https://github.com/vfarcic/docker-flow-swarm-listener](https://github.com/vfarcic/docker-flow-swarm-listener))).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目展示了Docker远程API的有用性。`logspout`容器将使用该API来检索当前所有正在运行的容器的列表，并流式传输它们的日志。这已经是我们集群中第二个使用该API的产品（第一个是*Docker
    Flow Swarm Listener* ([https://github.com/vfarcic/docker-flow-swarm-listener](https://github.com/vfarcic/docker-flow-swarm-listener))）。
- en: 'Let''s see the status of the service we just created:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下刚创建的服务的状态：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows (IDs & ERROR PORTS column are removed for brevity):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，移除了IDs & ERROR PORTS列）：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The service is running in global mode resulting in an instance inside each node.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 服务以全局模式运行，导致每个节点内部都有一个实例。
- en: 'Let''s test whether the `logspout` service is indeed sending all the logs to
    LogStash. All we have to do is create a service that generates some logs and observe
    them from the output of LogStash . We''ll use the registry to test the setup we
    have made so far:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下`logspout`服务是否确实将所有日志发送到了LogStash。我们只需要创建一个生成日志的服务，并从LogStash的输出中观察它们。我们将使用注册表来测试到目前为止我们所做的配置：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before we check the LogStash logs, we should wait until the registry is running:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查LogStash日志之前，我们应该等到注册表正在运行：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If the current state is still not running, please wait a few moments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前状态仍然没有运行，请稍等片刻。
- en: 'Now we can take a look at `logstash` logs and confirm that `logspout` sent
    it log entries generated by the `registry`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以查看`logstash`日志，确认`logspout`是否成功发送了由`registry`生成的日志条目：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'One of the entries from the output is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的一条记录如下：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As before when we tested LogStash input with logger, we have the message, `timestamp`,
    `host`, and a few other `syslog` fields. We also got `logsource` that holds the
    `ID` of the container that produced the log as well as program that holds the
    container name. Both will be useful when debugging which service and container
    produced a bug.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前用logger测试LogStash输入时，我们会看到`timestamp`、`host`和一些其他`syslog`字段。我们还会看到`logsource`，它保存了生成日志的容器的`ID`，以及`program`，它保存了容器的名称。这两个字段在调试哪个服务和容器产生了故障时非常有用。
- en: If you go back to the command we used to create the `logstash` service, you'll
    notice the environment variable `LOGSPOUT=ignore`. It tells LogSpout that the
    service or, to be more precise, all containers that form the service, should be
    ignored. If we did not define it, LogSpout would forward all `logstash` logs to
    `logstash` thus creating an infinite loop. As we already discussed, in production
    we should not output LogStash entries to `stdout`. We did it only to get a better
    understanding of how it works. If `stdout` output is removed from the logstash
    configuration, there would be no need for the environment variable `LOGSPOUT=ignore`.
    As a result `logstash` logs would also be stored in ElasticSearch.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回顾一下我们用于创建`logstash`服务的命令，你会注意到环境变量`LOGSPOUT=ignore`。它告诉LogSpout忽略该服务，或者更准确地说，忽略组成该服务的所有容器。如果我们没有定义它，LogSpout会将所有`logstash`日志转发到`logstash`，从而形成一个无限循环。正如我们之前讨论的那样，在生产环境中，我们不应该将LogStash条目输出到`stdout`。我们这样做仅仅是为了更好地理解它是如何工作的。如果从logstash配置中移除`stdout`输出，就不再需要环境变量`LOGSPOUT=ignore`。结果，`logstash`日志也会被存储在ElasticSearch中。
- en: Now that we are shipping all the logs to LogStash and from there to ElasticSearch,
    we should explore the ways to consult them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将所有日志发送到LogStash，并从那里发送到ElasticSearch，我们应该探索如何查询这些日志。
- en: Exploring logs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索日志
- en: Having all the logs in a central database is a good start, but it does not allow
    us to explore them in an easy and user-friendly way. We cannot expect developers
    to start issuing requests to the ElasticSearch API whenever they want to explore
    what went wrong. We need a UI that allows us to visualize and filter logs. We
    need *K* from the *ELK* stack.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有日志集中存储在一个数据库中是一个好的开始，但它不能让我们以简单和用户友好的方式进行探索。我们不能指望开发人员每次想要探索发生了什么错误时都去调用ElasticSearch
    API。我们需要一个UI，允许我们可视化和筛选日志。我们需要*ELK*堆栈中的*K*。
- en: '**A note to Windows users**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意事项**'
- en: 'You might experience a problem with volumes not being mapped correctly with
    Docker Compose. If you may see an *Invalid volume specification* error, please
    export the environment variable `COMPOSE_CONVERT_WINDOWS_PATHS set` to `0`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到Docker Compose无法正确映射卷的问题。如果你看到*Invalid volume specification*错误，请将环境变量`COMPOSE_CONVERT_WINDOWS_PATHS`设置为`0`：
- en: '`export COMPOSE_CONVERT_WINDOWS_PATHS=0`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`export COMPOSE_CONVERT_WINDOWS_PATHS=0`'
- en: Please make sure that the variable is exported every time you run `docker-compose`
    or `docker stack deploy`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保每次运行`docker-compose`或`docker stack deploy`时都导出该变量。
- en: 'Let''s create one more service. This time, it''ll be Kibana. Besides the need
    for this service to communicate with `logspout` and `elasticsearch` services,
    we want to expose it through the proxy, so we''ll create swarm-listener and `proxy`
    services as well. Let''s get to it:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个服务。这一次是Kibana。除了需要此服务与`logspout`和`elasticsearch`服务进行通信外，我们还希望通过代理暴露它，因此我们还将创建`swarm-listener`和`proxy`服务。让我们开始吧：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We created the `proxy` network, downloaded Compose file with the service definitions,
    and deployed the proxy stack which consists of `swarm-listener` and `proxy` services.
    They are the same commands as those we executed in the [Chapter 8](c47e8687-ec20-4f84-9a79-ea2c6f9eb185.xhtml), *Using
    Docker Stack and Compose YAML Files to Deploy Swarm Services*, so there’s no need
    to explain them again.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了`proxy`网络，下载了包含服务定义的Compose文件，并部署了由`swarm-listener`和`proxy`服务组成的代理堆栈。这些命令与我们在[第8章](c47e8687-ec20-4f84-9a79-ea2c6f9eb185.xhtml)中执行的相同，*使用Docker
    Stack和Compose YAML文件部署Swarm服务*，因此无需再重复解释。
- en: The only thing missing before we create the `kibana` service is to wait until
    both swarm-listener and proxy are up and running.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建`kibana`服务之前，唯一缺少的就是等待`swarm-listener`和`proxy`两个服务启动并运行。
- en: Please execute `docker service ls` command to confirm that both services have
    their replicas running.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行`docker service ls`命令，确认两个服务的副本已在运行。
- en: 'Now we''re ready to create the `kibana` service:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建`kibana`服务：
- en: '**A note to Windows users**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意事项**'
- en: 'For mounts used in the next command to work, you have to stop Git Bash from
    altering file system paths. Set this environment variable:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要使下一个命令中使用的挂载点工作，必须阻止Git Bash更改文件系统路径。设置此环境变量：
- en: '`export MSYS_NO_PATHCONV=1`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`export MSYS_NO_PATHCONV=1`'
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We attached it to both the elk and `proxy` networks. The first is needed so
    that it can communicate with the `elasticsearch` service, while the second is
    required for communication with the proxy. We also set up the `ELASTICSEARCH_URL`
    environment variable that tells Kibana the address of the database, and reserved
    `50m` of memory. Finally, we defined a few labels that will be used by the `swarm-listener`
    to notify the proxy about the services existence. This time, the `com.df.servicePath`
    label has three paths that match those used by Kibana.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它连接到了elk和`proxy`两个网络。第一个网络是为了使其能够与`elasticsearch`服务通信，第二个网络则是为了与代理进行通信。我们还设置了`ELASTICSEARCH_URL`环境变量，用于告知Kibana数据库的地址，并预留了`50m`的内存。最后，我们定义了一些标签，这些标签将由`swarm-listener`用于通知代理服务的存在。这一次，`com.df.servicePath`标签有三个路径，与Kibana使用的路径相匹配。
- en: 'Le''s confirm that `kibana` is running before opening its UI:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开其UI之前，让我们确认`kibana`是否正在运行：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The UI can be opened through the command that follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: UI可以通过以下命令打开：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**A note to Windows users**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意事项**'
- en: 'Git Bash might not be able to use the open command. If that’s the case, execute
                            `docker-machine ip <SERVER_NAME>` to find out the IP of
    the machine and open the URL directly in your browser of choice. For example,
    the command above should be replaced with the command that follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash可能无法使用open命令。如果是这种情况，请执行`docker-machine ip <SERVER_NAME>`来找出机器的IP地址，并直接在你选择的浏览器中打开URL。例如，上面的命令应该替换为以下命令：
- en: '`docker-machine ip swarm-1`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine ip swarm-1`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4:8082/jenkins`
    in your browser.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是`1.2.3.4`，请在浏览器中打开`http://1.2.3.4:8082/jenkins`。
- en: You should see the screen that lets you configure ElasticSearch indexes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个屏幕，让您配置ElasticSearch索引。
- en: Now we can explore the logs by clicking the Discover button from the top menu.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过单击顶部菜单中的Discover按钮来探索日志。
- en: Kibana, by default, displays the logs generated during the last fifteen minutes.
    Depending on the time that passed since we produced the logs, fifteen minutes
    might be less than the actual time that passed. We'll increase the duration to
    twenty-four hours.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kibana显示在最近十五分钟内生成的日志。根据生成日志以来的时间，十五分钟可能少于实际经过的时间。我们将持续时间增加到二十四小时。
- en: 'Please select `@timestamp` as Time-field name and click the *Create* button
    to generate LogStash indexes in ElasticSearch:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请选择`@timestamp`作为时间字段名称，然后点击*Create*按钮在ElasticSearch中生成LogStash索引：
- en: '![](img/kibana-index-config.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/kibana-index-config.png)'
- en: 'Figure 8-1: Configure an index pattern Kibana screen'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-1：配置Kibana屏幕的索引模式
- en: Please click the Last 15 minutes from the top-right menu. You'll see a plethora
    of options we can use to filter the results based on time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请单击右上角菜单中的“Last 15 minutes”。您将看到我们可以使用大量选项来基于时间过滤结果。
- en: Please click the Last 24 hours link and observe that the time from the top-right
    menu changed accordingly. Now click the Last 24 hours button to hide the filters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请单击“Last 24 hours”链接，并观察右上角菜单中的时间变化。现在点击“Last 24 hours”按钮以隐藏过滤器。
- en: 'More information can be found in the *Setting the Time Filter* ([https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter](https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter))
    section of the documentation for Kibana :'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 可在Kibana文档的*Setting the Time Filter*（[https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter](https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter)）部分找到更多信息：
- en: '![](img/kibana-time-filters.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/kibana-time-filters.png)'
- en: 'Figure 8-2: Time filters in Kibanas Discover screen'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-2：Kibana 的发现屏幕中的时间过滤器
- en: At the moment, the central part of the screen displays all the logs that match
    the given `time-span`. Most of the time, on a "real" production system, we would
    not be interested in all the logs produced inside the cluster. Instead, we'd filter
    them based on some criteria.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，屏幕中央显示与给定的`time-span`匹配的所有日志。在“真实”的生产系统上，我们通常不会对集群中生成的所有日志感兴趣。相反，我们会根据某些标准对它们进行过滤。
- en: Let's say we want to see all the logs generated by the `proxy` service. We often
    don't need to know the exact name of the program that generated them. This is
    true because Swarm adds an instance number and a hashtag into container names,
    and we are often unsure what the exact name is, or which instance produced a problem.
    Instead, we'll filter the logs to display all those that have a program containing
    the word `proxy`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要查看由`proxy`服务生成的所有日志。我们通常不需要知道生成它们的程序的确切名称。这是因为Swarm会向容器名称添加实例编号和哈希标记，我们经常不确定确切的名称或哪个实例引起了问题。因此，我们将过滤日志以显示所有包含`proxy`一词的程序。
- en: 'Please type `program: "proxy_proxy"` in the Search field located in the upper
    part of the screen and press enter. The result will be that only logs that contain
    `proxy_proxy` in the program name field are displayed in the main part of the
    screen. Similarly, we can change the search to the previous state and list all
    the logs that match the given time frame. All we have to do is type `*` in the
    Search field and press Enter.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '请在屏幕上部的搜索字段中键入`program: "proxy_proxy"`，然后按Enter。结果将只显示主屏幕中包含程序名称字段中`proxy_proxy`的日志。类似地，我们可以更改搜索到先前状态并列出所有与给定时间范围匹配的日志。我们只需在搜索字段中键入`*`，然后按Enter。'
- en: More information can be found in the *Searching Your Data* ([https://www.elastic.](https://www.elastic.co/guide/en/kibana/current/discover.html#search)[co/guide/en/kibana/current/discover.html#search](https://www.elastic.co/guide/en/kibana/current/discover.html#search)) section
    of documentation for Kibanas .
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可在*Kibanas*文档的*Searching Your Data*（[https://www.elastic.co/guide/en/kibana/current/discover.html#search](https://www.elastic.co/guide/en/kibana/current/discover.html#search)）部分找到更多信息。
- en: 'The list of all fields that match the current query is located in the left-hand
    menu. We can see the top values of one of those fields by clicking on it. For
    example, we can click on the *program* field and see all the programs that produced
    logs during the specified time. We can use those values as another way to filter
    the results. Please click the + sign next to `proxy.1.4psvagyv4bky2lftjg4a` (in
    your case the hash will be different). We just accomplished the same result as
    if we typed `program: "proxy.1.4psvagyv4bky2lftjg4a:` in the Search field.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '当前查询匹配的所有字段列表位于左侧菜单中。我们可以通过点击某个字段来查看该字段的顶部值。例如，我们可以点击 *program* 字段，查看在指定时间内产生日志的所有程序。我们可以将这些值作为另一种过滤结果的方式。请点击
    `proxy.1.4psvagyv4bky2lftjg4a`（在你的情况下，哈希值会不同）旁边的 + 号。我们刚刚达成的结果就相当于在搜索字段中输入了 `program:
    "proxy.1.4psvagyv4bky2lftjg4a:`。'
- en: More information can be found in the *Filtering by Field* ([https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter](https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter))
    section of documentation for Kibana .
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以在 Kibana 文档中的 *按字段过滤* ([https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter](https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter))
    部分找到。
- en: The main body of the screen displays the selected fields in each row, with the
    option to drill down and show all the information. The truth is that the default
    fields (Time and _source) are not very helpful so we'll change them.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕的主体部分显示了每一行中所选的字段，并且可以深入查看并显示所有信息。事实上，默认字段（时间和 _source）并不是非常有用，因此我们将更改它们。
- en: Please click the Add button next to program in the left-hand menu. You'll see
    that the program column was added to the Time column. Let's add a few more. Please
    repeat the process with the host, and @timestamp fields as well.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击左侧菜单中程序旁边的“添加”按钮。你会看到程序列已经添加到时间列。让我们再添加一些字段。请重复此操作，添加主机和 @timestamp 字段。
- en: To see more information about a particular entry, please click the arrow pointing
    to the right. A table with all the fields will appear below it, and you will be
    able to explore all the details related to the particular logs entry.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看特定条目的更多信息，请点击指向右边的箭头。它下面会出现一个包含所有字段的表格，你可以浏览与特定日志条目相关的所有详细信息。
- en: More information can be found in the *Filtering by Field* ([https://www.elastic.co/guide/en/kibana/current/discover.html#document-data](https://www.elastic.co/guide/en/kibana/current/discover.html#document-data))
    section of documentation for Kibana .
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以在 Kibana 文档中的 *按字段过滤* ([https://www.elastic.co/guide/en/kibana/current/discover.html#document-data](https://www.elastic.co/guide/en/kibana/current/discover.html#document-data))
    部分找到。
- en: 'The only thing left in this short tour around Kibana is to save the filter
    we just created. Please click the Save Search button in the top menu to save what
    we created by now. Type a name for your search and click the Save button. Your
    filters are now saved and can be accessed through the Load Saved Search button
    located in the top menu:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次简短的 Kibana 之旅中剩下的唯一任务是保存我们刚刚创建的过滤器。请点击顶部菜单中的“保存搜索”按钮，将目前创建的内容保存。为你的搜索输入一个名称并点击“保存”按钮。你的过滤器现在已保存，可以通过顶部菜单中的“加载已保存搜索”按钮访问：
- en: '![](img/kibana-discover.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/kibana-discover.png)'
- en: 'Figure 8-3: Discover screen in Kibana'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-3：Kibana 中的 Discover 屏幕
- en: 'That''s it. Now you know the basics how to explore your logs stored in ElasticSearch.
    If you''re wondering what can be done with Visualize and Dashboard screens, I''ll
    only state that they are not very useful for logs. They become much more interesting,
    however, if we start adding other types of information like resource usage (example:
    memory, CPU, network traffic, and so on).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。现在你已经了解了如何浏览存储在 ElasticSearch 中的日志。如果你想知道在可视化和仪表板屏幕上能做什么，我只想说它们对日志不太有用。不过，如果我们开始添加其他类型的信息，比如资源使用情况（例如：内存、CPU、网络流量等），它们会变得更有趣。
- en: Discussing other logging solutions
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论其他日志记录解决方案
- en: Is ELK the solution you should choose for your logging purposes? That a hard
    question to answer. There are a plethora of similar tools in the market, and it
    would be close to impossible to give a universal answer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ELK 是你应该选择的日志记录解决方案吗？这是一个很难回答的问题。市场上有大量类似的工具，要给出一个普适的答案几乎是不可能的。
- en: Do you prefer a free solution? If you do, then ELK ( *ElasticSearch* ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)),
    *LogStash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)),
    and *Kibana* ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)))
    is an excellent choice. If you’re looking for an equally cheap (free) alternative,
    *FluentD* ([http://www.fluentd.org/](http://www.fluentd.org/)) is something worth
    trying out. Many other solutions might fit your needs. A simple Google search
    will reveal a plethora of options.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你偏好使用免费解决方案吗？如果是，那么 ELK（*ElasticSearch*（[https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)），*LogStash*（[https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)），和
    *Kibana*（[https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)））是一个极好的选择。如果你在寻找一个同样便宜（免费的）替代方案，*FluentD*（[http://www.fluentd.org/](http://www.fluentd.org/)）是值得尝试的东西。还有许多其他的解决方案可能适合你的需求。一个简单的
    Google 搜索会揭示大量的选择。
- en: Are you more interested in a solution provided as a service? Would you like
    someone else taking care of your logging infrastructure? If you do, many services
    offer, for a fee, to host your logs in their database and provide nice interfaces
    you can use to explore them. I won't list examples since I decided to base this
    book fully on open source solutions you can run yourself. Again, Google is your
    friend if you'd prefer a service maintained by someone else.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否对作为服务提供的解决方案更感兴趣？你希望有人为你管理日志基础设施吗？如果是，许多服务提供商会收费为你托管日志，并提供良好的界面供你查看。由于本书完全基于你可以自行运行的开源解决方案，因此我不会列举例子。如果你更倾向于使用由他人维护的服务，再次提醒，Google
    是你的好帮手。
- en: What now?
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: We touched only the surface of what the ELK stack can do. ElasticSearch is a
    very powerful database that can be scaled easily and store vast amounts of data.
    LogStash provides almost unlimited possibilities that allow us to use virtually
    any data source as input (in our case `syslog`), transform it into any form we
    find useful, and output to many different destinations (in our case ElasticSearch).
    When a need occurs, you can use Kibana to go through the logs generated by your
    system. Finally, the tool that made all that happen is LogSpout. It ensured that
    all the logs produced by any of the containers running inside our cluster are
    collected and shipped to LogStash.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅触及了 ELK 栈能做的皮毛。ElasticSearch 是一个非常强大的数据库，能够轻松扩展并存储大量数据。LogStash 提供几乎无限的可能性，使我们能够使用几乎任何数据源作为输入（在我们的案例中是
    `syslog`），将其转换为我们认为有用的任何形式，并输出到许多不同的目标（在我们的案例中是 ElasticSearch）。当有需求时，你可以使用 Kibana
    来浏览系统生成的日志。最后，促成这一切的工具是 LogSpout。它确保了在我们集群中运行的任何容器产生的所有日志都能被收集并发送到 LogStash。
- en: This goal of the chapter was to explore a potential solution to deal with massive
    quantities of logs and give you a base understanding how to collect them from
    services running inside a Swarm cluster. Do you know everything you should know
    about logging? You probably don't. However, I hope you have a good base to explore
    the subject in more details.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是探索一个潜在的解决方案，用以处理海量日志，并让你对如何从在 Swarm 集群中运行的服务中收集日志有一个基础的理解。你知道关于日志记录的所有知识吗？你可能并不知道。然而，我希望你能有一个良好的基础，来更深入地探索这个主题。
- en: Even if you choose to use a different set of tools, the process will still be
    the same. Use a tool to collect logs from your services, ship them to some database,
    use a UI to explore them when needed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你选择使用不同的工具集，过程依然是相同的。使用一个工具从你的服务中收集日志，将其发送到某个数据库，在需要时使用用户界面进行查看。
- en: Now we have logs that provide only part of the information we'll need to find
    a cause of an issue. Logs by themselves are often not enough. We need metrics
    from the system. Maybe our services use more memory than our cluster provides.
    Maybe the system takes too much time to respond. Or maybe we have a memory leak
    in one of our services. Those things would be very hard to find out through logs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了日志，但它们只能提供我们找到问题原因所需信息的一部分。日志本身往往不足够。我们还需要来自系统的指标。也许我们的服务使用的内存超出了集群提供的范围。也许系统响应时间过长。或者可能我们在某个服务中有内存泄漏。这些问题通过日志很难发现。
- en: We need to know not only current metrics of the system but also how it behaved
    in the past. Even if we do have those metrics, we need a process that will notify
    us of problems. Looking at logs and metrics provides a lot of information we can
    use to debug issues, but we wouldn't know that a problem exists in the first place.
    We need a process that will notify us when something goes wrong or, even better,
    before the actual problem happens. Even with such a system in place, we should
    go even further and try to prevent problems from happening. Such prevention can
    often be automated. After all, why should we fix all the problems manually when
    some of them can be fixed automatically by the system itself? The ultimate goal
    is to make a self-healing system and involve humans only when unexpected things
    happen.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅需要知道系统的当前指标，还需要了解其过去的表现。即使我们拥有这些指标，我们仍然需要一个能够通知我们问题的流程。查看日志和指标提供了大量的信息，帮助我们调试问题，但如果我们不知道问题的存在，根本无法开始调试。我们需要一个能够在问题发生时通知我们，或者更好的是，在问题真正发生之前就发出警告的流程。即便有了这样的系统，我们还应该进一步采取措施，尝试防止问题的发生。这种预防措施通常可以自动化。毕竟，为什么我们要手动修复所有问题，而其中一些问题可以由系统自己自动修复呢？最终目标是创建一个自我修复的系统，只有在出现意外情况时才需要人工介入。
- en: Metrics, notifications, self-healing systems, and other pending tasks in front
    of us are too much for a single chapter so we'll do one step at a time. For now,
    we’re finished with logs and will jump into a discussion about different ways
    to collect metrics and use them to monitor our cluster and services running inside
    it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 指标、通知、自我修复系统以及我们面前的其他待办任务对于单独一章来说实在太多了，因此我们将一步步来。目前，我们已经完成了日志部分，接下来将讨论收集指标的不同方式，并使用它们来监控我们的集群及其内部运行的服务。
- en: 'As always, we''ll end with a destructive note:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们将以一个破坏性的结尾结束：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
