<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Defining Logging Strategy</h1>
            </header>

            <article>
                
<div class="packt_quote">Most software today is very much like an Egyptian pyramid with millions of bricks piled on top of each other, with no structural integrity, but just done by brute force and thousands of slaves.<br/>
                                                                                                                             –Alan Kay</div>
<p>We've reached the point where we have a fully operating Swarm cluster and a defined Continuous Deployment Pipeline that'll update our services on each commit. Now we can spend time coding and pushing commits to our repository knowing that the rest of the process is automated. We can, finally, spend our time on tasks that bring real value to the organization we're working for. We can dedicate our time to producing new features for the services we're working on. However, when something goes wrong, we need to stop churning new features and investigate the problem.</p>
<p>The first thing we tend to do when we detect an issue is to check logs. They are, by no means, not the only source of data we can use for debugging problem. We'll also need a lot of metrics (more on that in the next chapter). However, even if logs are not the only thing we should look at, they are often a good start.</p>
<div class="packt_tip"><strong>A note to The DevOps 2.0 Toolkit readers</strong><br/>
The text that follows is identical to the one published in <em>The DevOps 2.0 Toolkit</em>. If it is still fresh in your mind, feel free to jump to the <em>Setting up LogStash and ElasticSearch as the logging Database </em>(<kbd>#logging-es</kbd>) sub-chapter. Since I wrote the <em>2.0</em>, I discovered a few better ways to treat logs, especially inside a Swarm cluster.</div>
<p>Our exploration of DevOps practices and tools led us towards clustering and scaling. As a result, we developed a system that allows us to deploy services to a cluster, in an easy and efficient way. The result is an ever increasing number of containers running inside a cluster consisting of, potentially, many servers.</p>
<p>Monitoring one server is easy. Monitoring many services on a single server poses some difficulties. Monitoring many services on many servers requires a whole new way of thinking and a new set of tools. As you start embracing microservices, containers, and clusters, the number of created services and their instances will begin increasing rapidly. The same holds true for servers that form the cluster. We cannot, anymore, log into a node and look at logs. There are too many to look at. On top of that, they are distributed among many servers. While yesterday we had two instances of a service deployed on a single server, tomorrow we might have eight instances deployed to six servers.</p>
<p>We need historical and (near) real time information about our system. That information can be in the form of logs, hardware utilization, health checking, network traffic, and many other things. The need to store historical data is not new and has been in use for a long time. However, the direction that information travels has changed over time. While, in the past, most solutions were based on centralized data collectors, today, due to the very dynamic nature of services and servers, we tend to have them decentralized.</p>
<p>What we need for cluster logging and monitoring is a combination of decentralized data collectors that are sending information to a centralized parsing service and data storage. There are plenty of products specially designed to fulfill this requirement, ranging from on-premise to cloud solutions, and everything in between. <em>FluentD</em> (<a href="http://www.fluentd.org/">http://www.fluentd.org/</a>), <em>Loggly</em> (<a href="https://www.loggly.com/">https://www.loggly.com/</a>), <em>GrayLog</em> (<a href="https://www.graylog.org/">https://www.graylog.org/</a>), <em>Splunk</em> (<a href="http://www.splunk.com/">http://www.splunk.com/</a>), and <em>DataDog</em> (<a href="https://www.datadoghq.com/">https://www.datadoghq.com/</a>) are only a few of the solutions we can employ. I chose to show you the concepts through the ELK stack (<em>ElasticSearch</em> (<a href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a>), <em>LogStash</em> (<a href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a>), and <em>Kibana</em> (<a href="https://www.elastic.co/products/kibana">https://www.elastic.co/products/kibana</a>)). The stack has the advantage of being free, well documented, efficient, and widely used. <em>ElasticSearch</em> (<a href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a>) established itself as one of the best databases for real-time search and analytics. It is distributed, scalable, highly available, and provides a sophisticated API. <em>LogStash</em> (<a href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a>) allows us to centralize data processing. It can be easily extended to custom data formats and offers a lot of plugins that can fulfill almost any need. Finally, <em>Kibana</em> (<a href="https://www.elastic.co/products/kibana">https://www.elastic.co/products/kibana</a>) is an analytics and visualization platform with intuitive interface sitting on top of ElasticSearch.</p>
<p>The fact that we'll use the ELK stack does not mean that it is better than the other solutions. It all depends on specific use cases and particular needs. I'll walk you through the principles of centralized logging and monitoring using the ELK stack. Once those principles are understood, you should have no problem applying them to a different stack if you choose to do so.</p>
<p>We switched the order of things and chose the tools before discussing the need for centralized logging. Let's remedy that.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The need for centralized logging</h1>
            </header>

            <article>
                
<p>In most cases, log messages are written to files. That is not to say that files are the only, nor the most efficient way of storing logs. However, since most teams are using file-based logs in one form or another, for the time being, I'll assume that is your case as well. If it is, we identified the first thing we should fix. Containers expect us to send logs to <kbd>stdout</kbd> and <kbd>stderr</kbd>. Only log entries forwarded to the standard output are retrievable with <kbd>docker logs</kbd> command. Moreover, tools designed to work with container logs will expect just that. They'll assume that entries are not written to a file but sent to the output. Even without containers, I believe that <kbd>stdout</kbd> and <kbd>stderr</kbd> are where our services should log things. However, that's a story for some other time. For now, we'll concentrate on containers and assume that you are outputting your logs to <kbd>stdout</kbd> and <kbd>stderr.</kbd> If you're not, most logging libraries will allow you to change your logging destination to standard output and error.</p>
<p>Most of the time, we do not care what is written in logs. When things are working well, there is not much need to spend valuable time browsing through them. A log is not a novel we read to pass the time with, nor it is a technical book we read as a way to improve our knowledge. Logs are there to provide valuable info when something, somewhere, went wrong.</p>
<p>The situation seems to be simple. We write information to logs that we ignore most of the time, and when something goes wrong, we consult them and find the cause of the problem in no time. At least, that's what many are hoping for. The reality is far more complicated than that. In all but the most trivial systems, the debugging process is much more challenging. Applications and services are almost always interconnected, and it is often not easy to know which one caused the problem. While it might manifest in one application, investigation often shows that the cause is in another. For example, a service might have failed to instantiate. After some time spent browsing its logs, we might discover that the cause is in the database. The service could not connect to it and failed to launch. We got the symptom, but not the cause. We need to switch to the database log to find it out. With this simple example, we've already gotten to the point where looking at one log is not enough.</p>
<p>With distributed services running on a cluster, the situation complicates exponentially. Which instance of the service is failing? Which server is it running on? What are the upstream services that initiated the request? What are the memory and hard disk usage in the node where the culprit resides? As you might have guessed, finding, gathering, and filtering the information needed for the successful discovery of the cause is often very complicated. The bigger the system, the harder it gets. Even with monolithic applications, things can easily get out of hand.<br/>
If a microservices approach is adopted, those problems are multiplied. Centralized logging is a must for all but the simplest and smallest systems. Instead, many of us, when things go wrong, start running from one server to another and jumping from one file to the other. Like a chicken with its head cut off - running around with no direction. We tend to accept the chaos logging creates, and consider it part of our profession.</p>
<p>What do we look for in centralized logging? As it happens, many things, but the most important are as follows:</p>
<ul>
<li>A way to parse data and send them to a central database in near real-time</li>
<li>The capacity of the database to handle near real-time data querying and analytics</li>
<li>A visual representation of the data through filtered tables, dashboards, and so on</li>
</ul>
<p>We already chose the tools that will be able to fulfill all those requirements (and more). The ELK stack (ElasticSearch, LogStash, and Kibana) can do all that. As in the case of all other tools we explored, this stack can easily be extended to satisfy the particular needs we'll set in front of us.</p>
<p>Now that we have a vague idea what we want to accomplish, and have the tools to do that, let us explore a few of the logging strategies we can use. We'll start with the most commonly used scenario and, slowly, move towards more complicated and more efficient ways to define our logging strategy.</p>
<p>Without further ado, let's create the environments we'll use to experiment with centralized logging and, later on, monitoring.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up ElasticSearch as the logging database</h1>
            </header>

            <article>
                
<p>As in quite a few cases before, we'll start by creating the already familiar nodes (<kbd>swarm-1</kbd>, <kbd>swarm-2</kbd>, and <kbd>swarm-3</kbd>):</p>
<pre>
<strong><span class="hljs-built_in">cd</span> cloud-provisioning<br/><br/>git pull<br/><br/>scripts/dm-swarm.sh</strong>
</pre>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>08-logging.sh</kbd> (<a href="https://gist.github.com/vfarcic/c89b73ebd32dbf8f849531a842739c4d">https://gist.github.com/vfarcic/c89b73ebd32dbf8f849531a842739c4d</a>) Gist.</div>
<p>The first service we'll create is <em>Elastic Search</em> (<a href="https://hub.docker.com/_/elasticsearch">https://hub.docker.com/_/elasticsearch</a>). Since we'll need it to be accessible from a few other services, we'll also create a network called <kbd>elk</kbd>:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)</strong><br/><br/><strong>docker network create --driver overlay elk</strong><br/><br/><strong>docker service create \</strong><br/><strong>    --name elasticsearch \</strong><br/><strong>    --network elk \</strong><br/><strong>    --reserve-memory <span class="hljs-number">500</span>m \</strong><br/><strong>    elasticsearch:<span class="hljs-number">2.4</span></strong>
</pre>
<p>After a few moments, the <kbd>elasticsearch</kbd> service will be up and running.</p>
<p>We can check the status using the <kbd>service ps</kbd> command:</p>
<pre>
<strong>docker service ps elasticsearch</strong>
</pre>
<p>The output is as follows (IDs and ERROR PORTS columns are removed for brevity):</p>
<pre>
<strong><span class="hljs-tag">NAME</span>            <span class="hljs-tag">IMAGE</span>             <span class="hljs-tag">NODE</span>    <span class="hljs-tag">DESIRED</span> <span class="hljs-tag">STATE</span> </strong><br/><strong><span class="hljs-tag">elasticsearch</span><span class="hljs-class">.1</span> <span class="hljs-tag">elasticsearch</span><span class="hljs-pseudo">:2</span><span class="hljs-class">.4</span> <span class="hljs-tag">swarm-1</span> <span class="hljs-tag">Running</span>       <span class="hljs-tag"><br/>------------------------------------------------------<br/>CURRENT STATE<br/>Running 19 seconds ago<br/></span></strong>
</pre>
<p>If <kbd>elasticsearch</kbd> is still not running, please wait a few moments before proceeding.<br/>
Now that we have a database where we can store our logs, the next step is to create a service that will parse log entries and forward the results to ElasticSearch.<br/></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up LogStash as the logs parser and forwarder</h1>
            </header>

            <article>
                
<p>We did <em>E</em> from the <em>ELK</em> stack. Now let's move to <em>L</em>. <em>LogStash</em> requires a configuration file. We'll use one that is already available inside the <kbd>vfarcic/cloud-provisioning</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning">https://github.com/vfarcic/cloud-provisioning</a>) repository. We’ll create a new directory, copy the <kbd>conf/logstash.conf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/conf/logstash.conf">https://github.com/vfarcic/cloud-provisioning/blob/master/conf/logstash.conf</a>) configuration, and use it inside the <kbd>logstash</kbd> service:</p>
<pre>
<strong>mkdir -p docker/logstash<br/><br/>cp conf/logstash.conf \<br/>    docker/logstash/logstash.conf<br/><br/>cat docker/logstash/logstash.conf</strong>
</pre>
<p>The content of the <kbd>logstash.conf</kbd> file is as follows:</p>
<pre>
<strong>input {<br/>  syslog { port =&gt; <span class="hljs-number">51415</span> }<br/>}<br/><br/>output {<br/>  elasticsearch {<br/>    hosts =&gt; [<span class="hljs-string">"elasticsearch:9200"</span>]<br/>  }<br/><span class="hljs-comment">  # Remove in production</span><br/>  stdout {<br/>    codec =&gt; rubydebug<br/>  }<br/>}</strong>
</pre>
<p>This is a very simple <em>LogStash</em> configuration. If will listen on port <kbd>51415</kbd> for <kbd>syslog</kbd> entries.</p>
<p>Each entry will be sent to two outputs; <kbd>elasticsearch</kbd> and <kbd>stdout</kbd>. Since both <kbd>logstash</kbd> and <kbd>elasticsearch</kbd> will be attached to the same network, all we had to do is put the service name as the host.</p>
<p>The second output will send everything to <kbd>stdout</kbd>. Please note that this entry should be removed before running <em>LogStash</em> in production. It creates an unnecessary overhead that, if there are many services, can be substantial. The only reason we have it is to show you how logs are passing through LogStash. In production, you'll have no need to look at its output. Instead, you'll use Kibana to explore the logs from the whole system.</p>
<p>Let's move on and create the second service:</p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
For mounts used in the next command to work, you have to stop Git Bash from altering file system paths. Set this environment variable:<br/>
<kbd>export MSYS_NO_PATHCONV=1</kbd></div>
<pre>
<strong>docker service create --name logstash \<br/>    --mount <span class="hljs-string">"type=bind,source=<span class="hljs-variable">$PWD</span>/docker/logstash,target=/conf"</span> \<br/>    --network elk \<br/><span class="hljs-operator">    -e</span> LOGSPOUT=ignore \<br/>    --reserve-memory <span class="hljs-number">100</span>m \<br/>    logstash:<span class="hljs-number">2.4</span> logstash <span class="hljs-operator">-f</span> /conf/logstash.conf</strong>
</pre>
<p>We created a service called <kbd>logstash</kbd> and mounted the host volume <kbd>docker/logstash as /conf</kbd> inside the container. That way we'll have the configuration file currently residing on the host available inside the container.</p>
<p>Please note that mounting a volume is not the best way to put the configuration inside the container. Instead, we should have built our own image with the configuration inside. We should have created a Dockerfile. It could be as follows:</p>
<pre>
<strong>FROM logstash<br/><br/>RUN mkdir <span class="hljs-regexp">/config/</span><br/>COPY conf<span class="hljs-regexp">/logstash.conf /config/</span><br/><br/>CMD [<span class="hljs-string">"-f"</span>, <span class="hljs-string">"/config/logstash.conf"</span>]</strong>
</pre>
<p>This configuration file should not change often (if ever), so the option of creating a new image based on <kbd>logstash</kbd> is much better than mounting a volume. However, for simplicity reasons, we used the mount. Just remember to build your own image once you start applying what you learned from this chapter.</p>
<p>We also defined the environment variable <kbd>LOGSPOUT</kbd>. It is not relevant right now. We'll comment on it later on.</p>
<p>The <kbd>logStash</kbd> service should be up and running by now. Let's double check it:</p>
<pre>
<strong>docker service ps logstash</strong>
</pre>
<p>The output should be as follows:</p>
<pre>
<strong><span class="hljs-tag">NAME</span>       <span class="hljs-tag">IMAGE</span>        <span class="hljs-tag">NODE</span>    <span class="hljs-tag">DESIRED</span> <span class="hljs-tag">STATE</span> <span class="hljs-tag">CURRENT</span> <span class="hljs-tag">STATE</span><br/><span class="hljs-tag">logstash</span><span class="hljs-class">.1</span> <span class="hljs-tag">logstash</span><span class="hljs-pseudo">:2</span><span class="hljs-class">.4</span> <span class="hljs-tag">swarm-1</span> <span class="hljs-tag">Running</span>       <span class="hljs-tag">Running</span> 2 <span class="hljs-tag">seconds</span> <span class="hljs-tag">ago</span></strong>
</pre>
<p>If the current state is still not running, please wait a few moments and repeat the <kbd>service ps</kbd> command. We can proceed only after <kbd>logstash</kbd> is operational.</p>
<p>Now we can confirm that <kbd>logStash</kbd> was initialized correctly. We'll need to find out which node it is running in, get the <kbd>ID</kbd> of the container, and output the logs:</p>
<pre>
<strong>LOGSTASH_NODE=$(docker service ps logstash | tail -n +<span class="hljs-number">2</span> | awk <span class="hljs-string">'{print $4}'</span>)<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env <span class="hljs-variable">$LOGSTASH_NODE</span>)<br/><br/>LOGSTASH_ID=$(docker ps -q \<br/>    --filter label=com.docker.swarm.service.name=logstash)<br/><br/>docker logs <span class="hljs-variable">$LOGSTASH_ID</span><br/></strong>
</pre>
<p>The output of the previous command <kbd>logs</kbd> is as follows:</p>
<pre>
<strong>{<span class="hljs-symbol">:timestamp=&gt;<span class="hljs-string">"2016-10-19T23:08:06.358000+0000"</span></span>, <span class="hljs-symbol">:message=&gt;<span class="hljs-string">"Pipeline \<br/>main started"</span></span>}</strong>
</pre>
<p><kbd>Pipeline main started</kbd> means that LogStash is running and waiting for input.</p>
<p>Before we set up a solution that will ship logs from all the containers inside the cluster, we'll make an intermediary step and confirm that LogStash can indeed accept <kbd>syslog</kbd> entries on port <kbd>51415</kbd>. We'll create a temporary service called <kbd>logger-test</kbd>:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker service create \<br/>    --name logger-test \<br/>    --network elk \<br/>    --restart-condition none \<br/>    debian \<br/>    logger -n logstash -P <span class="hljs-number">51415</span> hello world</strong>
</pre>
<p>The service is attached to the <kbd>elk</kbd> network so that it can communicate with the <kbd>logstash</kbd> service.</p>
<p>We had to specify <kbd>restart-condition</kbd> as <kbd>none</kbd>. Otherwise, when the process is finished, the container would stop, Swarm would detect it as a failure and reschedule it. In other words, without the restart condition set to none, Swarm would enter into an endless loop trying to reschedule containers that almost immediately stop.</p>
<p>The command we're executing sends a <kbd>syslog</kbd> message <kbd>logger</kbd>, to <kbd>logstash</kbd> running on port <kbd>51415</kbd>. The message is <kbd>hello world</kbd>.</p>
<p>Let's output LogStash logs one more time:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env <span class="hljs-variable">$LOGSTASH_NODE</span>)<br/><br/>docker logs <span class="hljs-variable">$LOGSTASH_ID</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>{<br/><span class="hljs-string">    "message"</span> =&gt; <span class="hljs-string">"&lt;5&gt;Oct 19 23:11:47 &lt;someone&gt;: hello world\u0000"</span>,<br/><span class="hljs-string">    "@version"</span> =&gt; <span class="hljs-string">"1"</span>,<br/><span class="hljs-string">    "@timestamp"</span> =&gt; <span class="hljs-string">"2016-10-19T23:11:47.882Z"</span>,<br/><span class="hljs-string">        "host"</span> =&gt; <span class="hljs-string">"10.0.0.7"</span>,<br/><span class="hljs-string">        "tags"</span> =&gt; [<br/>        [<span class="hljs-number">0</span>] <span class="hljs-string">"_grokparsefailure_sysloginput"</span><br/>        ],<br/><span class="hljs-string">           "priority"</span> =&gt; <span class="hljs-number">0</span>,<br/><span class="hljs-string">           "severity"</span> =&gt; <span class="hljs-number">0</span>,<br/><span class="hljs-string">           "facility"</span> =&gt; <span class="hljs-number">0</span>,<br/><span class="hljs-string">  "facility_label"</span> =&gt; <span class="hljs-string">"kernel"</span>,<br/><span class="hljs-string">  "severity_label"</span> =&gt; <span class="hljs-string">"Emergency"</span><br/>}</strong>
</pre>
<p>First Swarm had to download the debian image and, once the logger message was sent, LogStash had to start accepting entries. It takes a bit of time until LogStash processes the first entry. All subsequent entries will be processed almost instantly. If your output is not similar to the one above, please wait a moment and repeat the logs command.</p>
<p>As you can see, LogStash received the message hello world. It also recorded a few other fields like the <kbd>timestamp</kbd> and <kbd>host</kbd>. Ignore the error message <kbd>_grokparsefailure_sysloginput</kbd>. We could configure LogStash to parse <kbd>logger</kbd> messages correctly but, since we won't be using it anymore, it would be a waste of time. Soon we'll see a much better way to forward logs.</p>
<p>LogStash acted as a parser of the message and forwarded it to ElasticSearch. At the moment, you'll have to take my word for it. Soon we'll see how are those messages stored and how we can explore them.</p>
<p>We’ll remove the <kbd>logger-test</kbd> service. Its purpose was only to demonstrate that we have a LogStash instance that accepts <kbd>syslog</kbd> messages:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker service rm logger-test</strong>
</pre>
<p>Sending messages by invoking logger is great but is not what we're trying to accomplish. The goal is to forward the logs from all the containers running anywhere inside the cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Forwarding logs from all containers running anywhere inside a Swarm cluster</h1>
            </header>

            <article>
                
<p>How can we forward logs from all the containers no matter where they’re running? One possible solution would be to <em>configure logging drivers</em> (<a href="https://docs.docker.com/engine/admin/logging/overview/">https://docs.docker.com/engine/admin/logging/overview/</a>). We could use the <kbd>--log-driver</kbd> argument to specify a driver for each service. The driver could be <kbd>syslog</kbd> or any other supported option. That would solve our log shipping problem. However, using the argument for each service is tedious and, more importantly, we could easily forget to specify it for a service or two and discover the omission only after we encounter a problem and are in need of logs. Let's see if there is another option to accomplish the same result.</p>
<p>We could specify a log driver as a configuration option of the Docker daemon on each node. That would certainly make the setup easier. After all, there are probably fewer servers than services. If we were to choose between setting a driver when creating a service or as the daemon configuration, I'd choose the latter. However, we managed to get thus far without changing the default daemon configuration and I'd prefer if we can continue working without involving any special provisioning tools. Luckily, we still did not exhaust all our options.</p>
<p>We can ship logs from all our containers with the project called <kbd>logspout</kbd> (<a href="https://github.com/gliderlabs/logspout">https://github.com/gliderlabs/logspout</a>)</p>
<p>LogSpout is a log router for Docker containers that runs inside Docker. It attaches to all containers on a host, then routes their logs wherever we want. It also has an extensible module system. It's a mostly stateless log appliance. It's not meant for managing log files or looking at history. It is just a tool to get your logs out to live somewhere else, where they belong.</p>
<p>If you go through the project documentation, you'll notice that there are no instructions on how to run it as a Docker service. That should not matter since, by this time, you can consider yourself an expert in creating services.<br/>
What do we need from a service that should forward logs from all the containers running inside all the nodes that form a cluster? Since we want to forward them to LogStash that is already attached to the <kbd>elk</kbd> network, we should attach LogSpout to it as well. We need it to ship logs from all the nodes so the service should be global. It needs to know that the destination is the service called <kbd>logstash</kbd> and that it listens on port <kbd>51415</kbd>. Finally, one of the requirement for LogSpout is that the Docker socket from the host is mounted inside the service containers. That’s what it'll use to monitor the logs.</p>
<p>The command that creates the service that fulfills all those objectives and requirements is as follows:</p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
For mounts used in the next command to work, you have to stop Git Bash from altering file system paths. Set this environment variable:<br/>
<kbd>export MSYS_NO_PATHCONV=1</kbd></div>
<pre>
<strong>docker service create --name logspout \<br/>    --network elk \<br/>    --mode global \<br/>    --mount \<br/><span class="hljs-string">"type=bind,source=/var/run/docker.sock,target=/var/run/\<br/>    docker.sock"</span> <br/><span class="hljs-operator">    -e</span> SYSLOG_FORMAT=rfc3164 \<br/>    gliderlabs/logspout syslog://logstash:<span class="hljs-number">51415</span></strong>
</pre>
<p>We created a service called <kbd>logspout</kbd>, attached it to the <kbd>elk</kbd> network, set it to be global, and mounted the Docker socket. The command that will be executed once containers are created is <kbd>syslog://logstash:51415</kbd>. This tells LogSpout that we want to use <kbd>syslog</kbd> protocol to send logs to <kbd>logstash</kbd> running on port <kbd>51415</kbd>.</p>
<p>This project is an example of the usefulness behind the Docker Remote API. The <kbd>logspout</kbd> containers will use it to retrieve the list of all currently running containers and stream their logs. This is already the second product inside our cluster that uses the API (the first being <em>Docker Flow Swarm Listener</em> (<a href="https://github.com/vfarcic/docker-flow-swarm-listener">https://github.com/vfarcic/docker-flow-swarm-listener</a>)).</p>
<p>Let's see the status of the service we just created:</p>
<pre>
<strong>docker service ps logspout</strong>
</pre>
<p>The output is as follows (IDs &amp; ERROR PORTS column are removed for brevity):</p>
<pre>
<strong>NAME        IMAGE                      NODE    DESIRED STATE           <br/>logspout... gliderlabs/logspout:latest swarm-<span class="hljs-number">3</span> Running       <br/>logspout... gliderlabs/logspout:latest swarm-<span class="hljs-number">2</span> Running       <br/>logspout... gliderlabs/logspout:latest swarm-<span class="hljs-number">1</span> Running       <br/>------------------------------------------------------<br/>CURRENT STATE<br/>Running <span class="hljs-number">11</span> <span class="hljs-built_in">seconds</span> ago<br/>Running <span class="hljs-number">10</span> <span class="hljs-built_in">seconds</span> ago<br/>Running <span class="hljs-number">10</span> <span class="hljs-built_in">seconds</span> ago<br/></strong>
</pre>
<p>The service is running in global mode resulting in an instance inside each node.<br/>
Let's test whether the <kbd>logspout</kbd> service is indeed sending all the logs to LogStash. All we have to do is create a service that generates some logs and observe them from the output of LogStash . We'll use the registry to test the setup we have made so far:</p>
<pre>
<strong>docker service create --name registry \<br/>    -p <span class="hljs-number">5000</span>:<span class="hljs-number">5000</span> \<br/>    --reserve-memory <span class="hljs-number">100</span>m \<br/>    registry</strong>
</pre>
<p>Before we check the LogStash logs, we should wait until the registry is running:</p>
<pre>
<strong>docker service ps registry</strong>
</pre>
<p>If the current state is still not running, please wait a few moments.</p>
<p>Now we can take a look at <kbd>logstash</kbd> logs and confirm that <kbd>logspout</kbd> sent it log entries generated by the <kbd>registry</kbd>:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env <span class="hljs-variable">$LOGSTASH_NODE</span>)<br/><br/>docker logs <span class="hljs-variable">$LOGSTASH_ID</span></strong>
</pre>
<p>One of the entries from the output is as follows:</p>
<pre>
<strong>{<br/><span class="hljs-string">   "message"</span> =&gt; <span class="hljs-string">"time=\"2016-10-19T23:14:19Z\" level=info \<br/>msg=\"listening on [::]:5000\" go.version=go1.6.3 \<br/>instance.id=87c31e30-a747-4f70-b7c2-396dd80eb47b version=v2.5.1 \n"</span>,<br/><span class="hljs-string">         "@version"</span> =&gt; <span class="hljs-string">"1"</span>,<br/><span class="hljs-string">     "@timestamp"</span> =&gt; <span class="hljs-string">"2016-10-19T23:14:19.000Z"</span>,<br/><span class="hljs-string">         "host"</span> =&gt; <span class="hljs-string">"10.0.0.7"</span>,<br/><span class="hljs-string">         "priority"</span> =&gt; <span class="hljs-number">14</span>,<br/><span class="hljs-string">   "timestamp8601"</span> =&gt; <span class="hljs-string">"2016-10-19T23:14:19Z"</span>,<br/><span class="hljs-string">      "logsource"</span> =&gt; <span class="hljs-string">"c51c177bd308"</span>,<br/><span class="hljs-string">         "program"</span> =&gt; <span class="hljs-string">"registry.1.abszmuwq8k3d7comu504lz2mc"</span>,<br/><span class="hljs-string">             "pid"</span> =&gt; <span class="hljs-string">"4833"</span>,<br/><span class="hljs-string">        "severity"</span> =&gt; <span class="hljs-number">6</span>,<br/><span class="hljs-string">        "facility"</span> =&gt; <span class="hljs-number">1</span>,<br/><span class="hljs-string">       "timestamp"</span> =&gt; <span class="hljs-string">"2016-10-19T23:14:19Z"</span>,<br/><span class="hljs-string">   "facility_label"</span> =&gt; <span class="hljs-string">"user-level"</span>,<br/><span class="hljs-string">   "severity_label"</span> =&gt; <span class="hljs-string">"Informational"</span><br/>}</strong>
</pre>
<p>As before when we tested LogStash input with logger, we have the message, <kbd>timestamp</kbd>, <kbd>host</kbd>, and a few other <kbd>syslog</kbd> fields. We also got <kbd>logsource</kbd> that holds the <kbd>ID</kbd> of the container that produced the log as well as program that holds the container name. Both will be useful when debugging which service and container produced a bug.</p>
<p>If you go back to the command we used to create the <kbd>logstash</kbd> service, you'll notice the environment variable <kbd>LOGSPOUT=ignore</kbd>. It tells LogSpout that the service or, to be more precise, all containers that form the service, should be ignored. If we did not define it, LogSpout would forward all <kbd>logstash</kbd> logs to <kbd>logstash</kbd> thus creating an infinite loop. As we already discussed, in production we should not output LogStash entries to <kbd>stdout</kbd>. We did it only to get a better understanding of how it works. If <kbd>stdout</kbd> output is removed from the logstash configuration, there would be no need for the environment variable <kbd>LOGSPOUT=ignore</kbd>. As a result <kbd>logstash</kbd> logs would also be stored in ElasticSearch.</p>
<p>Now that we are shipping all the logs to LogStash and from there to ElasticSearch, we should explore the ways to consult them.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Exploring logs</h1>
            </header>

            <article>
                
<p>Having all the logs in a central database is a good start, but it does not allow us to explore them in an easy and user-friendly way. We cannot expect developers to start issuing requests to the ElasticSearch API whenever they want to explore what went wrong. We need a UI that allows us to visualize and filter logs. We need <em>K</em> from the <em>ELK</em> stack.</p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
You might experience a problem with volumes not being mapped correctly with Docker Compose. If you may see an <em>Invalid volume specification</em> error, please export the environment variable <kbd>COMPOSE_CONVERT_WINDOWS_PATHS set</kbd> to <kbd>0</kbd>:<br/>
<kbd>export COMPOSE_CONVERT_WINDOWS_PATHS=0</kbd><br/>
Please make sure that the variable is exported every time you run <kbd>docker-compose</kbd> or <kbd>docker stack deploy</kbd>.</div>
<p>Let's create one more service. This time, it'll be Kibana. Besides the need for this service to communicate with <kbd>logspout</kbd> and <kbd>elasticsearch</kbd> services, we want to expose it through the proxy, so we'll create swarm-listener and <kbd>proxy</kbd> services as well. Let's get to it:</p>
<pre>
<strong>docker network create --driver overlay proxy<br/><br/>curl -o docker-compose-stack.yml \<br/>    https://raw.githubusercontent.com/\<br/>vfarcic/docker-flow-proxy/master/docker-compose-stack.yml<br/><br/>docker stack deploy \<br/>    -c docker-compose-stack.yml proxy</strong>
</pre>
<p>We created the <kbd>proxy</kbd> network, downloaded Compose file with the service definitions, and deployed the proxy stack which consists of <kbd>swarm-listener</kbd> and <kbd>proxy</kbd> services. They are the same commands as those we executed in the <a href="c47e8687-ec20-4f84-9a79-ea2c6f9eb185.xhtml">Chapter 8</a>, <em>Using Docker Stack and Compose YAML Files to Deploy Swarm Services</em>, so there’s no need to explain them again.</p>
<p>The only thing missing before we create the <kbd>kibana</kbd> service is to wait until both swarm-listener and proxy are up and running.</p>
<p>Please execute <kbd>docker service ls</kbd> command to confirm that both services have their replicas running.</p>
<p>Now we're ready to create the <kbd>kibana</kbd> service:</p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
For mounts used in the next command to work, you have to stop Git Bash from altering file system paths. Set this environment variable:<br/>
<kbd>export MSYS_NO_PATHCONV=1</kbd></div>
<pre>
<strong>docker service create --name kibana \<br/>    --network elk \<br/>    --network proxy \ <br/><span class="hljs-operator">-e</span> ELASTICSEARCH_URL=http://elasticsearch:<span class="hljs-number">9200</span> \<br/>    --reserve-memory <span class="hljs-number">50</span>m \<br/>    --label com.df.notify=<span class="hljs-literal">true</span> \<br/>    --label com.df.distribute=<span class="hljs-literal">true</span> \<br/>    --label com.df.servicePath=/app/kibana,/bundles,/elasticsearch \<br/>    --label com.df.port=<span class="hljs-number">5601</span> \<br/>    kibana:<span class="hljs-number">4.6</span></strong>
</pre>
<p>We attached it to both the elk and <kbd>proxy</kbd> networks. The first is needed so that it can communicate with the <kbd>elasticsearch</kbd> service, while the second is required for communication with the proxy. We also set up the <kbd>ELASTICSEARCH_URL</kbd> environment variable that tells Kibana the address of the database, and reserved <kbd>50m</kbd> of memory. Finally, we defined a few labels that will be used by the <kbd>swarm-listener</kbd> to notify the proxy about the services existence. This time, the <kbd>com.df.servicePath</kbd> label has three paths that match those used by Kibana.</p>
<p>Le's confirm that <kbd>kibana</kbd> is running before opening its UI:</p>
<pre>
<strong>docker service ps kibana</strong>
</pre>
<p>The UI can be opened through the command that follows:</p>
<pre>
<strong>open <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-1)</span>/app/kibana"</span></strong>
</pre>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
Git Bash might not be able to use the open command. If that’s the case, execute                         <kbd>docker-machine ip &lt;SERVER_NAME&gt;</kbd> to find out the IP of the machine and open the URL directly in your browser of choice. For example, the command above should be replaced with the command that follows:<br/>
<kbd>docker-machine ip swarm-1</kbd><br/>
If the output would be <kbd>1.2.3.4</kbd>, you should open <kbd>http://1.2.3.4:8082/jenkins</kbd> in your browser.</div>
<p>You should see the screen that lets you configure ElasticSearch indexes.</p>
<p>Now we can explore the logs by clicking the <span class="packt_screen">Discover</span> button from the top menu.</p>
<p>Kibana, by default, displays the logs generated during the last fifteen minutes. Depending on the time that passed since we produced the logs, fifteen minutes might be less than the actual time that passed. We'll increase the duration to twenty-four hours.</p>
<p>Please select <kbd>@timestamp</kbd> <span class="packt_screen">as Time-field name</span> and click the <em><span class="packt_screen">Create</span></em> button to generate LogStash indexes in ElasticSearch:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="327" src="assets/kibana-index-config.png" width="375"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-1: Configure an index pattern Kibana screen</div>
<p>Please click the <span class="packt_screen">Last 15 minutes</span> from the top-right menu. You'll see a plethora of options we can use to filter the results based on time.</p>
<p>Please click the <span class="packt_screen">Last 24 hours</span> link and observe that the time from the top-right menu changed accordingly. Now click the <span class="packt_screen">Last 24 hours</span> button to hide the filters.</p>
<p>More information can be found in the <em>Setting the Time Filter</em> (<a href="https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter">https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter</a>) section of the documentation for Kibana :</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="215" src="assets/kibana-time-filters.png" width="444"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-2: Time filters in Kibanas Discover screen</div>
<p>At the moment, the central part of the screen displays all the logs that match the given <kbd>time-span</kbd>. Most of the time, on a "real" production system, we would not be interested in all the logs produced inside the cluster. Instead, we'd filter them based on some criteria.</p>
<p>Let's say we want to see all the logs generated by the <kbd>proxy</kbd> service. We often don't need to know the exact name of the program that generated them. This is true because Swarm adds an instance number and a hashtag into container names, and we are often unsure what the exact name is, or which instance produced a problem. Instead, we'll filter the logs to display all those that have a program containing the word <kbd>proxy</kbd>.</p>
<p>Please type <kbd>program: "proxy_proxy"</kbd> in the Search field located in the upper part of the screen and press enter. The result will be that only logs that contain <kbd>proxy_proxy</kbd> in the program name field are displayed in the main part of the screen. Similarly, we can change the search to the previous state and list all the logs that match the given time frame. All we have to do is type <kbd>*</kbd> in the Search field and press Enter.</p>
<p>More information can be found in the <em>Searching Your Data</em> (<a href="https://www.elastic.co/guide/en/kibana/current/discover.html#search">https://www.elastic.</a><a href="https://www.elastic.co/guide/en/kibana/current/discover.html#search">co/guide/en/kibana/current/discover.html#search</a>) section of documentation for Kibanas .</p>
<p>The list of all fields that match the current query is located in the left-hand menu. We can see the top values of one of those fields by clicking on it. For example, we can click on the <em>program</em> field and see all the programs that produced logs during the specified time. We can use those values as another way to filter the results. Please click the + sign next to <kbd>proxy.1.4psvagyv4bky2lftjg4a</kbd> (in your case the hash will be different). We just accomplished the same result as if we typed <kbd>program: "proxy.1.4psvagyv4bky2lftjg4a:</kbd> in the <span class="packt_screen">Search</span> field.</p>
<p>More information can be found in the <em>Filtering by Field</em> (<a href="https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter">https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter</a>) section of documentation for Kibana .</p>
<p>The main body of the screen displays the selected fields in each row, with the option to drill down and show all the information. The truth is that the default fields (<span class="packt_screen">Time</span> and <span class="packt_screen">_source</span>) are not very helpful so we'll change them.</p>
<p>Please click the <span class="packt_screen">Add</span> button next to <span class="packt_screen">program</span> in the left-hand menu. You'll see that the <span class="packt_screen">program</span> column was added to the <span class="packt_screen">Time</span> column. Let's add a few more. Please repeat the process with the <span class="packt_screen">host</span>, and <span class="packt_screen">@timestamp</span> fields as well.</p>
<p>To see more information about a particular entry, please click the arrow pointing to the right. A table with all the fields will appear below it, and you will be able to explore all the details related to the particular logs entry.</p>
<p>More information can be found in the <em>Filtering by Field</em> (<a href="https://www.elastic.co/guide/en/kibana/current/discover.html#document-data">https://www.elastic.co/guide/en/kibana/current/discover.html#document-data</a>) section of documentation for Kibana .<br/>
The only thing left in this short tour around Kibana is to save the filter we just created. Please click the <span class="packt_screen">Save Search</span> button in the top menu to save what we created by now. Type a name for your search and click the <span class="packt_screen">Save</span> button. Your filters are now saved and can be accessed through the <span class="packt_screen">Load Saved Search</span> button located in the top menu:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/kibana-discover.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-3: Discover screen in Kibana</div>
<p>That's it. Now you know the basics how to explore your logs stored in ElasticSearch. If you're wondering what can be done with <span class="packt_screen">Visualize</span> and <span class="packt_screen">Dashboard</span> screens, I'll only state that they are not very useful for logs. They become much more interesting, however, if we start adding other types of information like resource usage (example: memory, CPU, network traffic, and so on).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Discussing other logging solutions</h1>
            </header>

            <article>
                
<p>Is ELK the solution you should choose for your logging purposes? That a hard question to answer. There are a plethora of similar tools in the market, and it would be close to impossible to give a universal answer.<br/>
Do you prefer a free solution? If you do, then ELK ( <em>ElasticSearch</em> (<a href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a>), <em>LogStash</em> (<a href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a>), and <em>Kibana</em> (<a href="https://www.elastic.co/products/kibana">https://www.elastic.co/products/kibana</a>)) is an excellent choice. If you’re looking for an equally cheap (free) alternative, <em>FluentD</em> (<a href="http://www.fluentd.org/">http://www.fluentd.org/</a>) is something worth trying out. Many other solutions might fit your needs. A simple Google search will reveal a plethora of options.</p>
<p>Are you more interested in a solution provided as a service? Would you like someone else taking care of your logging infrastructure? If you do, many services offer, for a fee, to host your logs in their database and provide nice interfaces you can use to explore them. I won't list examples since I decided to base this book fully on open source solutions you can run yourself. Again, Google is your friend if you'd prefer a service maintained by someone else.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What now?</h1>
            </header>

            <article>
                
<p>We touched only the surface of what the ELK stack can do. ElasticSearch is a very powerful database that can be scaled easily and store vast amounts of data. LogStash provides almost unlimited possibilities that allow us to use virtually any data source as input (in our case <kbd>syslog</kbd>), transform it into any form we find useful, and output to many different destinations (in our case ElasticSearch). When a need occurs, you can use Kibana to go through the logs generated by your system. Finally, the tool that made all that happen is LogSpout. It ensured that all the logs produced by any of the containers running inside our cluster are collected and shipped to LogStash.</p>
<p>This goal of the chapter was to explore a potential solution to deal with massive quantities of logs and give you a base understanding how to collect them from services running inside a Swarm cluster. Do you know everything you should know about logging? You probably don't. However, I hope you have a good base to explore the subject in more details.</p>
<p>Even if you choose to use a different set of tools, the process will still be the same. Use a tool to collect logs from your services, ship them to some database, use a UI to explore them when needed.</p>
<p>Now we have logs that provide only part of the information we'll need to find a cause of an issue. Logs by themselves are often not enough. We need metrics from the system. Maybe our services use more memory than our cluster provides. Maybe the system takes too much time to respond. Or maybe we have a memory leak in one of our services. Those things would be very hard to find out through logs.</p>
<p>We need to know not only current metrics of the system but also how it behaved in the past. Even if we do have those metrics, we need a process that will notify us of problems. Looking at logs and metrics provides a lot of information we can use to debug issues, but we wouldn't know that a problem exists in the first place. We need a process that will notify us when something goes wrong or, even better, before the actual problem happens. Even with such a system in place, we should go even further and try to prevent problems from happening. Such prevention can often be automated. After all, why should we fix all the problems manually when some of them can be fixed automatically by the system itself? The ultimate goal is to make a self-healing system and involve humans only when unexpected things happen.</p>
<p>Metrics, notifications, self-healing systems, and other pending tasks in front of us are too much for a single chapter so we'll do one step at a time. For now, we’re finished with logs and will jump into a discussion about different ways to collect metrics and use them to monitor our cluster and services running inside it.</p>
<p>As always, we'll end with a destructive note:</p>
<pre>
<strong>docker-machine rm <span class="hljs-operator">-f</span> swarm-<span class="hljs-number">1</span> swarm-<span class="hljs-number">2</span> swarm-<span class="hljs-number">3</span></strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>