<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-instrumenting-services" class="calibre15">Instrumenting Services</h2>

<p class="calibre3">In the previous chapters, we used data from <em class="calibre21">cAdvisor</em> to scale services automatically. Specifically, Prometheus was firing alerts if memory limits were reached. When memory utilization was over the limit, we were scaling the service associated with the data. While that approach is a good start, it is far from enough for the type of the system we’re building. As a minimum, we need to measure response times of our services. Should we look for an exporter that would provide that information?</p>

<p class="calibre3">The chances are that your first thought would be to use <a href="https://github.com/prometheus/haproxy_exporter">haproxy_exporter</a>. If all public requests are going through it, it makes sense to scrape response times and set some alerts based on collected data. That model would be in line with the most of the other monitoring systems. The only problem with that approach is that it would be almost useless.</p>

<p class="calibre3">Not all requests are going through the proxy. Services that do not need to be accessed publicly are not hooked into the proxy. For example, <em class="calibre21">Docker Flow Swarm Listener</em> cannot be accessed. It does not publish any port, nor it has an API. It listens to Docker Socket and sends information to other services (e.g. <em class="calibre21">Docker Flow Proxy</em>, <em class="calibre21">Docker Flow Monitor</em>, and so on). It is entirely invisible to the proxy. We could overlook this lack of information if that were the only problem behind the idea of monitoring the proxy.</p>

<p class="calibre3">When a request enters the proxy, it is forwarded to a service based on request paths, domains, and a few other criteria. By scraping metrics from the proxy, we would know only response times of those requests. In many cases, a service that receives a request forwarded from the proxy is making other requests. For example, <em class="calibre21">go-demo</em> communicates with <em class="calibre21">MongoDB</em>. A service that receives a request from the proxy might make many requests to other services. The proxy does not know about any of those. It receives a request, forwards it, waits for a response, and re-sends it to the client that initiated the communication. It is oblivious of any other processes or requests happening in the middle. As a result, we would know the duration of a request that enters the proxy but would be oblivious what are response times of each service involved in serving those requests.</p>

<p class="calibre3">Without the knowledge about response times of each service, we cannot deduce which one needs to be scaled. If a response time of a backend is high, should we scale that same backend or the database it uses?</p>

<p class="calibre3">To make things more complicated, response times are not the only metrics we need. We might be interested in failure rates, paths, methods, and a few other pieces of additional data. And all that needs to be related to a particular service, or even a concrete replica of a service</p>

<p class="calibre3">If your memory serves you well, you might remember that I said that <strong class="calibre16">my advice is to always start with exporters, and instrument your services only if you require metrics that are not provided by one of the existing exporters</strong>. Well… We reached that point when exporters are not enough. We need to instrument our services and gather more detailed metrics.</p>

<p class="calibre3">We’ll limit the focus to only a few metrics. Specifically, we’ll explore ways to collect error counts, response times, status codes, and a few other metrics. Do not take that as a sign that other types are not needed. They are. However, we need to keep the scope limited and produce tangible results within a decent number of pages. Otherwise, we could just as well start competing with <a href="https://en.wikipedia.org/wiki/Encyclop%C3%A6dia_Britannica">Encyclopedia Britannica</a>. I will assume that you will take those examples for what they are and use them as a base for your own system. Error rates, response times, and status codes might be the most common types of metrics, but they are almost certainly not the only ones you need.</p>

<p class="calibre3">With the scope limited to only a few metrics, we should spend a bit of time discussing the data we would need.</p>

<h3 id="leanpub-auto-defining-requirements-behind-service-specific-metrics" class="calibre20">Defining Requirements Behind Service Specific Metrics</h3>

<p class="calibre3">We might need different types of metrics. Some of them could be simple counters. A good example is errors. We might want to count them and react when their numbers reach certain thresholds. That in itself might not be enough, and we should be able to differentiate errors depending on a function or part of the service that produced them.</p>

<p class="calibre3">How about more complex metrics? Response times are another good example.</p>

<p class="calibre3">We might need a metric that will provide request response times. That might lead us towards having something like <code class="calibre19">resp_time 0.043</code> as the metric. It has a name (<code class="calibre19">resp_time</code>) and value in seconds (<code class="calibre19">0.043</code>). If we’d implement a metric like that, we’d soon discover that we need more. Having the information that the system responses are slow does not give us a clue which part of it is misbehaving. We need to know the name of the service.</p>

<p class="calibre3">We might not be able to instrument all the services in our clusters. If we take <code class="calibre19">go-demo</code> stack as an example, it consists of two services. It has a backend and a MongoDB. The backend is in our control, and we can easily extend it by adding instrumentation. The database is a different story. While we can (and should) use <a href="https://github.com/dcu/mongodb_exporter">MongoDB Exporter</a>, it provides data related to the server status. What we need are metrics that we can relate to the backend service. We need to know whether a request sent to the <code class="calibre19">go-demo</code> stack is slow because of an issue in the backend or the database. Assuming that we are not going to “adapt” MongoDB to our own needs, we should try to answer that and few other questions by extending metrics inside services we’re controlling.</p>

<p class="calibre3">We can use request path and method. If we add it to our metric, it should give us fairly good granularity of information. Depending on the path and the method, we can know whether the metric is related to the database or is limited to the internal processes of the service. We could also add query, but that would go too far. It would record almost each request separately and might result in too much memory and CPU usage when stored in Prometheus. Our updated metric could be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>resp_time{method="GET",path="/demo/hello",service="go-demo"} 0.611446292
</pre></div>

</figure>

<p class="calibre3">Through those labels, we would know which service the metric belongs to, the path of the request, and the method.</p>

<p class="calibre3">In the sea of possible additional labels we could add, there is one more that could be considered critical. We should know the status code. If we adopt standard HTTP response codes, the same ones our backend provides with the rest of the response, we can easily filter metrics and, for example, retrieve only those that are related to server errors.</p>

<p class="calibre3">Our updated metric could be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>resp_time{code="200",method="GET",path="/demo/hello",service="go-demo"} 0.611446\
<code class="lineno">2 </code>292
</pre></div>

</figure>

<p class="calibre3">That is surely it. Right? Well, that’s not quite what we truly need. A few other critical things are missing but we will not comment on them just yet. Since the libraries used to instrument code add a few additional features, we’ll comment on them once we reach the hands-on part. For now, it is enough to know that we can instrument services to generate metrics to count (e.g. errors) or observe (e.g. response times). Any additional information can be provided through labels.</p>

<h3 id="leanpub-auto-differentiating-services-based-on-their-types" class="calibre20">Differentiating Services Based On Their Types</h3>

<p class="calibre3">Before we start instrumenting our services, we should discuss services we’re deploying. They can be divided into three categories: online services, offline services, and batch processes. While there is overlap between each of those types and it is often not that easy to place a service into only one of them, such a division will provide us with a good understanding of the types of metrics we should implement.</p>

<p class="calibre3">We can define online services as those that accept requests from another service, a human, or a client (e.g. browser). Those who send requests to online services often expect an immediate response. Front-end, APIs, and databases are only a few of the examples of such services. Due to the expectations we have from them, the key metrics we are interested in are the number of requests they served, the number of errors they produced, and latency.</p>

<p class="calibre3">Offline services are those that do not have a client that is waiting for a response. Something, or someone, instructs those services to do some tasks without waiting until they are finished. A good example of such a service would be Jenkins. Even though it does have it’s UI and API and can fall in the category of online services, most of the work it does is offline. An example would be builds triggered by a webhook from a code repository. Those webhooks do not wait until Jenkins finishes building the jobs initiated by them. Instead, they announce that there is a new commit and trust that Jenkins will know what to do and will do it well. With offline services, we usually want to track the number of tasks being executed, the last time something was processed, and the length of queues.</p>

<p class="calibre3">Finally, the last group of services is batch processes. The significant difference when compared with offline services is that batch jobs do not run continuously. They start execution of a task or a group of tasks, terminate, and disappear. That makes them very difficult to scrape. Prometheus would not know when a batch job started nor when it should end. We cannot expect a system (Prometheus or any other) to pull metrics from a batch job. Our best bet is to push them instead. With such services, we usually track how long it takes to complete them, how long each stage of a job lasted, the time a job finished executing, and whether it produced an error or it was successful.</p>

<p class="calibre3">I prefer avoiding batch jobs since they are very hard to track and measure. Instead, when possible, we should consider converting them into offline services. A good example is, again, Jenkins. It allows us to schedule execution of a job thus providing a similar functionality as a batch process while still providing easy to scrape metrics and health checks.</p>

<p class="calibre3">Now that we divided services into groups, we can discuss different types of metrics we can define when instrumenting our services.</p>

<h3 id="leanpub-auto-choosing-instrumentation-type" class="calibre20">Choosing Instrumentation Type</h3>

<p class="calibre3">Prometheus supports four major metric types. We can make a choice between counters, gauges, summaries, and histograms. We will see them in action soon. For now, we’ll limit the discussion to a brief overview of each.</p>

<p class="calibre3">A <em class="calibre21">counter</em> can only go up. We cannot decrease its value. It is useful for accumulating values. An example would be errors. Each error in the system should increase the counter by one.</p>

<p class="calibre3">Unlike counters, <em class="calibre21">gauge</em> values can go both up and down. A good example of a gauge is memory usage. It can increase, only to decrease a few moments later.</p>

<p class="calibre3"><em class="calibre21">Histograms</em> and <em class="calibre21">summaries</em> are more complex types. They are often used to measure durations of requests and sizes of responses. They track both summaries and counts. When those two are combined, we can measure averages over time. Their data is usually placed in buckets that form quantiles.</p>

<p class="calibre3">We’ll go deeper into each of the metric types through practical examples starting with a <em class="calibre21">counter</em> as the simplest of all. But, before we do that, we need to create a cluster that will serve as our playground.</p>

<h3 id="leanpub-auto-creating-the-cluster-and-deploying-services-5" class="calibre20">Creating The Cluster And Deploying Services</h3>

<p class="calibre3">All hands-on parts of the past chapters started with the execution of a script that creates a cluster and deploys the services we’ll need. This chapter is no exception. You know the drill so let’s get to it.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/85bd6824032fb2a05d7fe624516548a7">11-instrumentation.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod +x scripts/dm-swarm-11.sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>./scripts/dm-swarm-11.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env swarm-1<code class="k">)</code>
<code class="lineno">6 </code>
<code class="lineno">7 </code>docker stack ls
</pre></div>

</figure>

<p class="calibre3">We executed the <code class="calibre19">dm-swarm-11.sh</code> script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks, and deployed only one stack. The last command listed all the stacks in the cluster and showed that we are running only the <code class="calibre19">proxy</code> stack.</p>

<p class="calibre3">Let’s move into the <em class="calibre21">counter</em> metric.</p>

<h3 id="leanpub-auto-instrumenting-services-using-counters" class="calibre20">Instrumenting Services Using Counters</h3>

<p class="calibre3">There are many usages of the <em class="calibre21">counter</em> metric. We can measure the number of requests entering the system, the number of bytes sent through the network, the number of errors, and so on. Whenever we want to record an incremental value, a counter is a good choice.</p>

<p class="calibre3">We’ll use counter to track errors produced by a service. With such a goal, a counter is usually put around code that handles errors.</p>

<p class="calibre3">The examples that follow are taken from <a href="http://swarmlistener.dockerflow.com/">Docker Flow Swarm Listener</a>. The code is written in Go. Do not be afraid if that is not your language of choice. As you will see, examples are straightforward and can be easily extrapolated to any programming language.</p>

<p class="calibre3">Prometheus provides <a href="https://prometheus.io/docs/instrumenting/clientlibs/">client libraries</a> for a myriad of languages. Even if your favorite language is not one of them, it should be relatively easy to roll-out your own solution based on <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">exposition formats</a>.</p>

<p class="calibre3">The reason for choosing <em class="calibre21">Docker Flow Swarm Listener</em> lies in its type. It is (mostly) an offline service. Most of its objectives are to listen to Docker events through its socket and propagate Swarm events to other services like <em class="calibre21">Docker Flow Proxy</em> and <em class="calibre21">Docker Flow Monitor</em>. It does have an API but, since it is not its primary function, we’ll ignore it. As such, it is not a good candidate for more complex types of metrics thus making it suitable for a counter. That does not mean that the counter is the only metric it implements. However, we need to start from something simple, so we’ll ignore the others.</p>

<p class="calibre3">There are a few essential things code needs to do to start producing metrics.</p>

<p class="calibre3">We must define a variable that determines the type of the metric. Since we want to count errors, the code can be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">var</code> <code class="calibre19">errorCounter</code> <code class="calibre19">=</code> <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">NewCounterVec</code><code class="calibre19">(</code>
<code class="lineno">2 </code>  <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">CounterOpts</code><code class="calibre19">{</code>
<code class="lineno">3 </code>    <code class="calibre19">Subsystem</code><code class="calibre19">:</code> <code class="s">"docker_flow"</code><code class="calibre19">,</code>
<code class="lineno">4 </code>    <code class="calibre19">Name</code><code class="calibre19">:</code> <code class="s">"error"</code><code class="calibre19">,</code>
<code class="lineno">5 </code>    <code class="calibre19">Help</code><code class="calibre19">:</code> <code class="s">"Error counter"</code><code class="calibre19">,</code>
<code class="lineno">6 </code>  <code class="calibre19">},</code>
<code class="lineno">7 </code>  <code class="calibre19">[]</code><code class="kt">string</code><code class="calibre19">{</code><code class="s">"service"</code><code class="calibre19">,</code> <code class="s">"operation"</code><code class="calibre19">},</code>
<code class="lineno">8 </code><code class="calibre19">)</code>
</pre></div>

</figure>

<p class="calibre3">The snippet defines a variable <code class="calibre19">errorCounter</code> based on <code class="calibre19">CounterVec</code> structure provided through the <code class="calibre19">NewCounterVec</code> function. The function requires two arguments. The first one (<code class="calibre19">CounterOpts</code>) defines options of the counter. In our case, we set the subsytem to <code class="calibre19">docker_flow</code> and the name to <code class="calibre19">error</code>. The fully qualified metric name consists of the namespace (we’re not using it today), subsystem, and name. When combined, the metric we are creating will be called <code class="calibre19">docker_flow_error</code>. Help is only for informative purposes and should help users of our metrics understand better its purpose. As you can see, I was not very descriptive with the help. Hopefully, it is clear what it does without a more detailed explanation.</p>

<p class="calibre3">The second argument is the list of labels. They are critical since they allow us to filter metrics. In our case, we want to know which service generated metrics. That way, we can have the same instrumentation across many services and choose whether to explore them all at once or filter by the service name.</p>

<p class="calibre3">Knowing which service produced errors is often not enough. We should be able to pinpoint a particular operation that caused a problem. The second label called <code class="calibre19">operation</code> provides that additional info.</p>

<p class="calibre3">It is important to specify all the labels we might need when filtering metrics, but not more. Each label requires extra resources. While that is in most cases negligible overhead, it could still have a negative impact when dealing with big systems. Just follow the rule of “everything you need, but not more” and you should be on the right track.</p>

<p class="calibre3">Please read the <a href="https://prometheus.io/docs/practices/instrumentation/#use-labels">Use labels</a> section of the instrumentation page for a discussion about dos and don’ts.</p>

<p class="calibre3">The <code class="calibre19">errorCounter</code> variable is, in Prometheus terms, called collector. Each collector needs to be registered. We’ll do that inside <code class="calibre19">init</code> function that is executed automatically, thus saving us from worrying about it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">func</code> <code class="calibre19">init</code><code class="calibre19">()</code> <code class="calibre19">{</code>
<code class="lineno">2 </code>  <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">MustRegister</code><code class="calibre19">(</code><code class="calibre19">errorCounter</code><code class="calibre19">)</code>
<code class="lineno">3 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">Now we are ready to start incrementing the <code class="calibre19">errorCounter</code>. Since I do not like repeated code, the code that increments the metric is wrapped into another function. It is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">func</code> <code class="calibre19">recordError</code><code class="calibre19">(</code><code class="calibre19">operation</code> <code class="kt">string</code><code class="calibre19">,</code> <code class="calibre19">err</code> <code class="kt">error</code><code class="calibre19">)</code> <code class="calibre19">{</code>
<code class="lineno">2 </code>  <code class="calibre19">metrics</code><code class="calibre19">.</code><code class="calibre19">errorCounter</code><code class="calibre19">.</code><code class="calibre19">With</code><code class="calibre19">(</code><code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">Labels</code><code class="calibre19">{</code>
<code class="lineno">3 </code>    <code class="s">"service"</code><code class="calibre19">:</code>   <code class="calibre19">metrics</code><code class="calibre19">.</code><code class="calibre19">serviceName</code><code class="calibre19">,</code>
<code class="lineno">4 </code>    <code class="s">"operation"</code><code class="calibre19">:</code> <code class="calibre19">operation</code><code class="calibre19">,</code>
<code class="lineno">5 </code>  <code class="calibre19">}).</code><code class="calibre19">Inc</code><code class="calibre19">()</code>
<code class="lineno">6 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">Whenever this function is called, <code class="calibre19">errorCounter</code> will be incremented by one (<code class="calibre19">Inc()</code>). Each time that happens, the name of the service and the operation that produced the error will be recorded as labels.</p>

<p class="calibre3">An example invocation of the <code class="calibre19">recordError</code> function is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="o">...</code>
<code class="lineno"> 2 </code><code class="calibre19">err</code> <code class="calibre19">=</code> <code class="calibre19">n</code><code class="calibre19">.</code><code class="calibre19">ServicesCreate</code><code class="calibre19">(</code>
<code class="lineno"> 3 </code>  <code class="calibre19">newServices</code><code class="calibre19">,</code>
<code class="lineno"> 4 </code>  <code class="calibre19">args</code><code class="calibre19">.</code><code class="calibre19">Retry</code><code class="calibre19">,</code>
<code class="lineno"> 5 </code>  <code class="calibre19">args</code><code class="calibre19">.</code><code class="calibre19">RetryInterval</code><code class="calibre19">,</code>
<code class="lineno"> 6 </code><code class="calibre19">)</code>
<code class="lineno"> 7 </code><code class="k">if</code> <code class="calibre19">err</code> <code class="o">!=</code> <code class="k">nil</code> <code class="calibre19">{</code>
<code class="lineno"> 8 </code>  <code class="calibre19">metrics</code><code class="calibre19">.</code><code class="calibre19">RecordError</code><code class="calibre19">(</code><code class="s">"ServicesCreate"</code><code class="calibre19">)</code>
<code class="lineno"> 9 </code><code class="calibre19">}</code>
<code class="lineno">10 </code><code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">The function <code class="calibre19">ServicesCreate</code> returns an <code class="calibre19">err</code> (short for <code class="calibre19">error</code>). If the <code class="calibre19">err</code> is not <code class="calibre19">nil</code>, the <code class="calibre19">recordError</code> is called passing <code class="calibre19">GetServices</code> as operation and thus incrementing the counter.</p>

<p class="calibre3">The last piece missing is to enable <code class="calibre19">/metrics</code> as the endpoint Prometheus can use to scrape metrics from out service.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">func</code> <code class="calibre19">(</code><code class="calibre19">m</code> <code class="o">*</code><code class="calibre19">Serve</code><code class="calibre19">)</code> <code class="calibre19">Run</code><code class="calibre19">()</code> <code class="kt">error</code> <code class="calibre19">{</code>
<code class="lineno">2 </code>  <code class="calibre19">mux</code> <code class="o">:=</code> <code class="calibre19">http</code><code class="calibre19">.</code><code class="calibre19">NewServeMux</code><code class="calibre19">()</code>
<code class="lineno">3 </code>  <code class="o">...</code>
<code class="lineno">4 </code>  <code class="calibre19">mux</code><code class="calibre19">.</code><code class="calibre19">Handle</code><code class="calibre19">(</code><code class="s">"/metrics"</code><code class="calibre19">,</code> <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">Handler</code><code class="calibre19">())</code>
<code class="lineno">5 </code>  <code class="o">...</code>
<code class="lineno">6 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">That code snippet should be self-explanatory. We registered <code class="calibre19">/metrics</code> as the address that is handled by Prometheus handler provided with the GoLang client library we’re using.</p>

<p class="calibre3">I hope that those few snippets of Go code were not scary. Even if you never worked with Go, you probably managed to understand the gist of it and will be able to create something similar in your favorite language. Remember to visit <a href="https://prometheus.io/docs/instrumenting/clientlibs/">Client Libraries</a> page, choose the preferred language, and follow the instructions.</p>

<p class="calibre3">If you’re interested in the full source code behind the snippets, please visit <a href="https://github.com/vfarcic/docker-flow-swarm-listener">vfarcic/docker-flow-swarm-listener</a> GitHub repository.</p>

<p class="calibre3">Let’s see those metrics in action.</p>

<p class="calibre3">Since <code class="calibre19">swarm-listener</code> deployed through the <code class="calibre19">proxy</code> stack does not publish port <code class="calibre19">8080</code>, we’ll create a new service attached to the <code class="calibre19">proxy</code> network. It will be global so that it is guaranteed to run on each node. That way it’ll be easier to find the container, enter into it, and send requests to <code class="calibre19">swarm-listener</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service create --name util <code class="se">\</code>
<code class="lineno">2 </code>    --mode global <code class="se">\</code>
<code class="lineno">3 </code>    --network proxy <code class="se">\</code>
<code class="lineno">4 </code>    alpine sleep <code class="o">1000000</code>
</pre></div>

</figure>

<p class="calibre3">We created the <code class="calibre19">util</code> service based on the <code class="calibre19">alpine</code> image and made it sleep for a very long time. Please confirm that it is up-and-running by executing <code class="calibre19">docker service ps util</code>.</p>

<p class="calibre3">Let’s find the ID of the container running on the node our Docker client points to and enter inside it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">ID</code><code class="o">=</code><code class="k">$(</code>docker container ls -q <code class="se">\</code>
<code class="lineno">2 </code>    -f <code class="s">"label=com.docker.swarm.service.name=util"</code><code class="k">)</code>
<code class="lineno">3 </code>
<code class="lineno">4 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
</pre></div>

</figure>

<p class="calibre3">The only thing missing is to install <code class="calibre19">curl</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>apk add --update curl
</pre></div>

</figure>

<p class="calibre3">Now we can send a request to <code class="calibre19">swarm-listener</code> and retrieve metrics.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl <code class="s">"http://swarm-listener:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">You’ll see a lot of metrics that come out of the box when using Prometheus clients. In this case, most of the metrics are very particular to Go, so we’ll skip them. What you won’t be able to see is <code class="calibre19">docker_flow_error</code>. Since the service did not produce any errors, that metric does not show.</p>

<p class="calibre3">Let’s get out of the container we’re in.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
</pre></div>

</figure>

<p class="calibre3">My guess is that you would not be delighted reaching this far without seeing the metric we discussed so let us generate a situation in which <code class="calibre19">swarm-listener</code> will produce errors.</p>

<p class="calibre3"><em class="calibre21">Docker Flow Swarm Listener</em> discovers services by communicating with Docker Engine through its socket. Typically, the service mounts the socket to the host and, in that way, Docker client inside the container communicates with Docker Engine running on the node. If we remove that mount, the communication will be broken, and <em class="calibre21">Docker Flow Swarm Listener</em> will start reporting errors.</p>

<p class="calibre3">Let’s test it out.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --mount-rm /var/run/docker.sock <code class="se">\</code>
<code class="lineno">3 </code>    proxy_swarm-listener
</pre></div>

</figure>

<p class="calibre3">We removed the <code class="calibre19">/var/run/docker.sock</code> mount and the communication between Docker client inside the container and Docker engine on the host was cut. We should wait a few moments until Docker reschedules a new replica. If you want to confirm that the update was finished, please execute <code class="calibre19">docker stack ps proxy</code>.</p>

<p class="calibre3">Let’s check the logs and confirm that the service is indeed generating errors.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service logs proxy_swarm-listener
</pre></div>

</figure>

<p class="calibre3">One of the output entries should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docke\
<code class="lineno">3 </code>r daemon running?
</pre></div>

</figure>

<p class="calibre3">Now that the service is generating errors, we can take another look at the metrics and confirm that <code class="calibre19">docker_flow_error</code> is indeed added to the list of metrics.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl <code class="s">"http://swarm-listener:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">We entered the <code class="calibre19">util</code> replica and sent a request to <code class="calibre19">swarm-listener</code> endpoint <code class="calibre19">/metrics</code>. The output, limited to the relevant parts, should be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code># HELP docker_flow_error Error counter
<code class="lineno">3 </code># TYPE docker_flow_error counter
<code class="lineno">4 </code>docker_flow_error{operation="GetServices",service="swarm_listener"} 10
<code class="lineno">5 </code>...
</pre></div>

</figure>

<p class="calibre3">Please note that metrics are ordered alphabetically, so <code class="calibre19">docker_flow_error</code> should be somewhere around the top.</p>

<p class="calibre3">As you can see, <code class="calibre19">docker_flow_error</code> generated <code class="calibre19">10</code> errors. By inspecting labels, we can see that the operation that causes errors is <code class="calibre19">GetServices</code> and that the service is <code class="calibre19">swarm_listener</code>. If this would be a production system, we’d know not only that there is a problem with the service but also which part of it caused the issue. That is very important since the actions the system should take are rarely the same for the whole service. Knowing that the problem is related to a particular operation or a function, lets us fine tune the actions the system should take when certain thresholds are reached.</p>

<p class="calibre3">Before we continue, let us exit the container we’re in and restore the <code class="calibre19">swarm-listener</code> to its original state.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">4 </code>    -c stacks/docker-flow-proxy-mem.yml <code class="se">\</code>
<code class="lineno">5 </code>    proxy
</pre></div>

</figure>

<p class="calibre3">We redeployed the stack and thus restored the mount we removed.</p>

<p class="calibre3">Let’s try to generate the same metric with a different value. We can, for example, remove <code class="calibre19">proxy</code> service and deploy <code class="calibre19">go-demo</code> stack. <em class="calibre21">Docker Flow Swarm Listener</em> will detect a new service and try to send the information to the <code class="calibre19">proxy</code>. If it fails to do so, Prometheus client will increase <code class="calibre19">docker_flow_error</code> by one.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service rm proxy_proxy
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">4 </code>    -c stacks/go-demo-scale.yml <code class="se">\</code>
<code class="lineno">5 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">We removed the proxy and deployed <code class="calibre19">go-demo</code> stack. <em class="calibre21">Docker Flow Swarm Listener</em> will try to send service information to the proxy and, since we removed it, fail to do so. By default, if <code class="calibre19">swarm-listener</code> fails to deliver information, it retries for fifty times with five seconds delay in between. That means that we need to wait a bit over 4 minutes for <code class="calibre19">swarm-listener</code> to give up and throw an error.</p>

<p class="calibre3">After a while, we can check the logs.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service logs proxy_swarm-listener
</pre></div>

</figure>

<p class="calibre3">After fifty retries, you should see log entries similar to the ones that follow.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Retrying service created notification to ...
<code class="lineno">3 </code>ERROR: Get ...: dial tcp: lookup proxy on 127.0.0.11:53: no such host
</pre></div>

</figure>

<p class="calibre3">Now we can go back to the <code class="calibre19">util</code> container and take another look at the metrics.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl <code class="s">"http://swarm-listener:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">This time, <code class="calibre19">docker_flow_error</code> metric is slightly different.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code># HELP docker_flow_error Error counter
<code class="lineno">2 </code># TYPE docker_flow_error counter
<code class="lineno">3 </code>docker_flow_error{operation="notificationSendCreateServiceRequest",service="swar\
<code class="lineno">4 </code>m_listener"} 1
<code class="lineno">5 </code>...
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">operation</code> label has the value <code class="calibre19">notificationSendCreateServiceRequest</code> clearly indicating that it comes from a different place than the previous error.</p>

<p class="calibre3">The two errors we explored are of quite a different nature and should be treated differently. The one associated with the label <code class="calibre19">GetServices</code> means that there is no communication with the Docker socket. That could be caused by a faulty manager and the action that should remedy that could be to reschedule the service to a different node or maybe even to remove that node altogether. The code of the service will retry establishing socket connection so we should probably not react on the first occupancy of the metric but wait until, for example, it reaches twenty failed attempts over the timespan of five minutes or less.</p>

<p class="calibre3">The error related to the <code class="calibre19">notificationSendCreateServiceRequest</code> means that there is no communication with the services that should receive notifications. In this case, that destination is the proxy. The problem might be related to networking, or the proxy is not running. Our action might be to check whether the proxy is running and, if it isn’t, deploy it again. Or maybe there should be no action at all. The proxy itself should have its own alerts that will remedy the situation. Moreover, the service does not throw an error when the connection with the proxy fails. Instead, it retries it for a while and errors only if all attempts failed. That means that we should react on the first occurrence of the error.</p>

<p class="calibre3">As you can see, even though those two errors come from the same service, the causes and the actions associated with them are entirely different. For that reason, we are using the <code class="calibre19">operation</code> label to distinguish them. Later on, it should be relatively easy to filter them in Prometheus and define different alerts.</p>

<p class="calibre3">Instrumenting our service with counters was easy. Let’s see whether gauge is any different.</p>

<p class="calibre3">Since we removed the proxy service, we should exit the <code class="calibre19">util</code> container and restore the stack to its original state before we proceed further.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">4 </code>    -c stacks/docker-flow-proxy-mem.yml <code class="se">\</code>
<code class="lineno">5 </code>    proxy
</pre></div>

</figure>

<h3 id="leanpub-auto-instrumenting-services-using-gauges" class="calibre20">Instrumenting Services Using Gauges</h3>

<p class="calibre3">Gauges are very similar to counters. The only significant difference is that we can not only increment, but also decrease their values. We’ll continue exploring <a href="https://github.com/vfarcic/docker-flow-swarm-listener">vfarcic/docker-flow-swarm-listener</a> GitHub repository for an example of a gauge.</p>

<p class="calibre3">Since <code class="calibre19">gauge</code> is almost identical to <code class="calibre19">counter</code>, we won’t go into many details but only briefly explore a few snippets.</p>

<p class="calibre3">Just as with a counter, we need to declare a variable that defines the type of the metric. A simple example is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">var</code> <code class="calibre19">serviceGauge</code> <code class="calibre19">=</code> <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">NewGaugeVec</code><code class="calibre19">(</code>
<code class="lineno">2 </code>	<code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">GaugeOpts</code><code class="calibre19">{</code>
<code class="lineno">3 </code>		<code class="calibre19">Subsystem</code><code class="calibre19">:</code> <code class="s">"docker_flow"</code><code class="calibre19">,</code>
<code class="lineno">4 </code>		<code class="calibre19">Name</code><code class="calibre19">:</code> <code class="s">"service_count"</code><code class="calibre19">,</code>
<code class="lineno">5 </code>		<code class="calibre19">Help</code><code class="calibre19">:</code> <code class="s">"Service gauge"</code><code class="calibre19">,</code>
<code class="lineno">6 </code>	<code class="calibre19">},</code>
<code class="lineno">7 </code>	<code class="calibre19">[]</code><code class="kt">string</code><code class="calibre19">{</code><code class="s">"service"</code><code class="calibre19">},</code>
<code class="lineno">8 </code><code class="calibre19">)</code>
</pre></div>

</figure>

<p class="calibre3">Next, we need to register it with Prometheus. We’ll reuse the code from the <code class="calibre19">init</code> function where we defined the <code class="calibre19">errorCounter</code> and add <code class="calibre19">serviceGauge</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">func</code> <code class="calibre19">init</code><code class="calibre19">()</code> <code class="calibre19">{</code>
<code class="lineno">2 </code>	<code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">MustRegister</code><code class="calibre19">(</code><code class="calibre19">errorCounter</code><code class="calibre19">,</code> <code class="calibre19">serviceGauge</code><code class="calibre19">)</code>
<code class="lineno">3 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">There’s also a function that simplifies the usage of the metric.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>func RecordService(count int) {
<code class="lineno">2 </code>	serviceGauge.With(prometheus.Labels{
<code class="lineno">3 </code>		"service":   serviceName,
<code class="lineno">4 </code>	}).Set(float64(count))
<code class="lineno">5 </code>}
</pre></div>

</figure>

<p class="calibre3">We’re setting the value of the gauge using the <code class="calibre19">Set</code> function. Alternatively, we could have used <code class="calibre19">Add</code> or <code class="calibre19">Sub</code> functions to add or subtract the value. <code class="calibre19">Inc</code> or <code class="calibre19">Dec</code> can be used to increase of decrease the value by one.</p>

<p class="calibre3">Finally, on every iteration of the <code class="calibre19">swarm-listener</code>, we are setting the gauge to the number of services retrieved by <code class="calibre19">swarm-listener</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="calibre19">metrics</code><code class="calibre19">.</code><code class="calibre19">RecordService</code><code class="calibre19">(</code><code class="nb">len</code><code class="calibre19">(</code><code class="calibre19">service</code><code class="calibre19">.</code><code class="calibre19">Services</code><code class="calibre19">))</code>
</pre></div>

</figure>

<p class="calibre3">Let’s take another look at the <code class="calibre19">/metrics</code> endpoint.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl <code class="s">"http://swarm-listener:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">One of the metric entries is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code># HELP docker_flow_service_count Service gauge
<code class="lineno">2 </code># TYPE docker_flow_service_count gauge
<code class="lineno">3 </code>docker_flow_service_count{service="swarm_listener"} 1
</pre></div>

</figure>

<p class="calibre3">It might look confusing that the value of the metric is one since we are running a few other services. <em class="calibre21">Docker Flow Swarm Listener</em> fetches only services with the <code class="calibre19">com.df.notify</code> label. Among the services we’re currently running, only <code class="calibre19">go-demo_main</code> has that label, hence being the only one included in the metric.</p>

<p class="calibre3">Let’s see what happens if we remove <code class="calibre19">go-demo_main</code> service.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker service rm go-demo_main
<code class="lineno">4 </code>
<code class="lineno">5 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">6 </code>
<code class="lineno">7 </code>curl <code class="s">"http://swarm-listener:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">The output of the <code class="calibre19">/metrics</code> API is as follows (limited to the relevant parts).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code># HELP docker_flow_service_count Service gauge
<code class="lineno">2 </code># TYPE docker_flow_service_count gauge
<code class="lineno">3 </code>docker_flow_service_count{service="swarm_listener"} 0
<code class="lineno">4 </code>...
</pre></div>

</figure>

<p class="calibre3">As you can see, the <code class="calibre19">docker_flow_service_count</code> metric is now set to zero thus accurately representing the number of services discovered by <code class="calibre19">swarm-listener</code>. If, in your case, the number is still one, please wait a few moments and try again. <em class="calibre21">Docker Swarm Listener</em> has five seconds iterations, and you might have requested metrics too soon.</p>

<p class="calibre3">Let us exit the <code class="calibre19">util</code> container and restore the <code class="calibre19">go-demo</code> stack before we proceed into histograms.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">4 </code>    -c stacks/go-demo-scale.yml <code class="se">\</code>
<code class="lineno">5 </code>    go-demo
</pre></div>

</figure>

<h3 id="leanpub-auto-instrumenting-services-using-histograms-and-summaries" class="calibre20">Instrumenting Services Using Histograms And Summaries</h3>

<p class="calibre3">When compared with counters and gauges, histograms are much more complex. That does not mean that they are harder to implement but that the data they provide is less simple when compared with the other metric types we explored. We’ll comment on them by studying a sample code and the output it provides.</p>

<p class="calibre3">We’ll switch from the <a href="https://github.com/vfarcic/docker-flow-swarm-listener">vfarcic/docker-flow-swarm-listener</a> repository to <a href="https://github.com/vfarcic/go-demo">vfarcic/go-demo</a> since it provides a simple example of a histogram.</p>

<p class="calibre3">Just as with the other types of metrics, histogram also needs to be declared as a variable of the particular type.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="k">var</code> <code class="calibre19">(</code>
<code class="lineno"> 2 </code>  <code class="calibre19">histogram</code> <code class="calibre19">=</code> <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">NewHistogramVec</code><code class="calibre19">(</code><code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">HistogramOpts</code><code class="calibre19">{</code>
<code class="lineno"> 3 </code>    <code class="calibre19">Subsystem</code><code class="calibre19">:</code> <code class="s">"http_server"</code><code class="calibre19">,</code>
<code class="lineno"> 4 </code>    <code class="calibre19">Name</code><code class="calibre19">:</code> <code class="s">"resp_time"</code><code class="calibre19">,</code>
<code class="lineno"> 5 </code>    <code class="calibre19">Help</code><code class="calibre19">:</code> <code class="s">"Request response time"</code><code class="calibre19">,</code>
<code class="lineno"> 6 </code>  <code class="calibre19">},</code> <code class="calibre19">[]</code><code class="kt">string</code><code class="calibre19">{</code>
<code class="lineno"> 7 </code>    <code class="s">"service"</code><code class="calibre19">,</code>
<code class="lineno"> 8 </code>    <code class="s">"code"</code><code class="calibre19">,</code>
<code class="lineno"> 9 </code>    <code class="s">"method"</code><code class="calibre19">,</code>
<code class="lineno">10 </code>    <code class="s">"path"</code><code class="calibre19">,</code>
<code class="lineno">11 </code>  <code class="calibre19">})</code>
<code class="lineno">12 </code><code class="calibre19">)</code>
</pre></div>

</figure>

<p class="calibre3">The objective of the metric is to record information about response times. Its labels provide additional information like the name of the service (<code class="calibre19">service</code>), the response code (<code class="calibre19">code</code>), the method of the request (<code class="calibre19">method</code>), and the path (<code class="calibre19">path</code>). All those labels together should give us a fairly accurate picture of the response times of the service, and we’ll be able to filter the results using any combination of the labels.</p>

<p class="calibre3">Next is a helper function that will allow us to record metrics easily.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="k">func</code> <code class="calibre19">recordMetrics</code><code class="calibre19">(</code><code class="calibre19">start</code> <code class="calibre19">time</code><code class="calibre19">.</code><code class="calibre19">Time</code><code class="calibre19">,</code> <code class="calibre19">req</code> <code class="o">*</code><code class="calibre19">http</code><code class="calibre19">.</code><code class="calibre19">Request</code><code class="calibre19">,</code> <code class="calibre19">code</code> <code class="kt">int</code><code class="calibre19">)</code> <code class="calibre19">{</code>
<code class="lineno"> 2 </code>  <code class="calibre19">duration</code> <code class="o">:=</code> <code class="calibre19">time</code><code class="calibre19">.</code><code class="calibre19">Since</code><code class="calibre19">(</code><code class="calibre19">start</code><code class="calibre19">)</code>
<code class="lineno"> 3 </code>  <code class="calibre19">histogram</code><code class="calibre19">.</code><code class="calibre19">With</code><code class="calibre19">(</code>
<code class="lineno"> 4 </code>    <code class="calibre19">prometheus</code><code class="calibre19">.</code><code class="calibre19">Labels</code><code class="calibre19">{</code>
<code class="lineno"> 5 </code>      <code class="s">"service"</code><code class="calibre19">:</code> <code class="calibre19">serviceName</code><code class="calibre19">,</code>
<code class="lineno"> 6 </code>      <code class="s">"code"</code><code class="calibre19">:</code> <code class="calibre19">fmt</code><code class="calibre19">.</code><code class="calibre19">Sprintf</code><code class="calibre19">(</code><code class="s">"%d"</code><code class="calibre19">,</code> <code class="calibre19">code</code><code class="calibre19">),</code>
<code class="lineno"> 7 </code>      <code class="s">"method"</code><code class="calibre19">:</code> <code class="calibre19">req</code><code class="calibre19">.</code><code class="calibre19">Method</code><code class="calibre19">,</code>
<code class="lineno"> 8 </code>      <code class="s">"path"</code><code class="calibre19">:</code> <code class="calibre19">req</code><code class="calibre19">.</code><code class="calibre19">URL</code><code class="calibre19">.</code><code class="calibre19">Path</code><code class="calibre19">,</code>
<code class="lineno"> 9 </code>    <code class="calibre19">},</code>
<code class="lineno">10 </code>  <code class="calibre19">).</code><code class="calibre19">Observe</code><code class="calibre19">(</code><code class="calibre19">duration</code><code class="calibre19">.</code><code class="calibre19">Seconds</code><code class="calibre19">())</code>
<code class="lineno">11 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">recordMetrics</code> function accepts argument that defines the time when a request started (<code class="calibre19">start</code>), the request itself (<code class="calibre19">req</code>), and the response code (<code class="calibre19">code</code>). We’re calling histogram’s’ <code class="calibre19">Observe</code> function with the duration of the request expressed in seconds. The duration is obtained by calculating the time passed since the value of the <code class="calibre19">start</code> variable.</p>

<p class="calibre3">Let’s take a look at one of the functions that invokes <code class="calibre19">recordMetrics</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">func</code> <code class="calibre19">HelloServer</code><code class="calibre19">(</code><code class="calibre19">w</code> <code class="calibre19">http</code><code class="calibre19">.</code><code class="calibre19">ResponseWriter</code><code class="calibre19">,</code> <code class="calibre19">req</code> <code class="o">*</code><code class="calibre19">http</code><code class="calibre19">.</code><code class="calibre19">Request</code><code class="calibre19">)</code> <code class="calibre19">{</code>
<code class="lineno">2 </code>  <code class="calibre19">start</code> <code class="o">:=</code> <code class="calibre19">time</code><code class="calibre19">.</code><code class="calibre19">Now</code><code class="calibre19">()</code>
<code class="lineno">3 </code>  <code class="k">defer</code> <code class="k">func</code><code class="calibre19">()</code> <code class="calibre19">{</code> <code class="calibre19">recordMetrics</code><code class="calibre19">(</code><code class="calibre19">start</code><code class="calibre19">,</code> <code class="calibre19">req</code><code class="calibre19">,</code> <code class="calibre19">http</code><code class="calibre19">.</code><code class="calibre19">StatusOK</code><code class="calibre19">)</code> <code class="calibre19">}()</code>
<code class="lineno">4 </code>
<code class="lineno">5 </code>  <code class="c">// The rest of the code that processes the request.</code>
<code class="lineno">6 </code><code class="calibre19">}</code>
</pre></div>

</figure>

<p class="calibre3">Whenever a request is made to a particular path, the web server invokes the <code class="calibre19">HelloServer</code> function. That function starts by recording the current time and storing it in the <code class="calibre19">start</code> variable. Go has a special statement that defers execution of a function. In this case, we defined that the invocation of the <code class="calibre19">recordMetrics</code> should be deferred. As a result, it will be executed before the <code class="calibre19">HelloServer</code> function exists, thus giving us an (almost) exact duration of the requests.</p>

<p class="calibre3">A similar logic is applied to all endpoints of the service thus providing us with the response times of the whole service.</p>

<p class="calibre3">If you’re interested in the full source code behind the snippets, please visit <a href="https://github.com/vfarcic/go-demo">vfarcic/go-demo</a> GitHub repository.</p>

<p class="calibre3">Let us send some traffic to the <code class="calibre19">go-demo</code> service before we explore the histogram metrics.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..100<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello"</code>
<code class="lineno">3 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">We’ll repeat the already familiar process of entering the <code class="calibre19">util</code> container and retrieving the metrics. The only difference is that this time we’ll explore <code class="calibre19">go-demo_main</code> metrics instead of those from the <code class="calibre19">swarm-listener</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl <code class="s">"http://go-demo_main:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">The output, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code># HELP resp_time Request response time
<code class="lineno"> 3 </code># TYPE resp_time histogram
<code class="lineno"> 4 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno"> 5 </code>="0.005"} 69
<code class="lineno"> 6 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno"> 7 </code>="0.01"} 69
<code class="lineno"> 8 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno"> 9 </code>="0.025"} 69
<code class="lineno">10 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">11 </code>="0.05"} 69
<code class="lineno">12 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">13 </code>="0.1"} 69
<code class="lineno">14 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">15 </code>="0.25"} 69
<code class="lineno">16 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">17 </code>="0.5"} 69
<code class="lineno">18 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">19 </code>="1"} 69
<code class="lineno">20 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">21 </code>="2.5"} 69
<code class="lineno">22 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">23 </code>="5"} 69
<code class="lineno">24 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">25 </code>="10"} 69
<code class="lineno">26 </code>resp_time_bucket{code="200",method="GET",path="/demo/hello",service="go-demo",le\
<code class="lineno">27 </code>="+Inf"} 69
<code class="lineno">28 </code>resp_time_sum{code="200",method="GET",path="/demo/hello",service="go-demo"} 0.00\
<code class="lineno">29 </code>3403602
<code class="lineno">30 </code>resp_time_count{code="200",method="GET",path="/demo/hello",service="go-demo"} 69
<code class="lineno">31 </code>...
</pre></div>

</figure>

<p class="calibre3">Unlike counters and gauges, each histogram produces quite a few metrics. The major one is <code class="calibre19">resp_time_sum</code> that provides a summary of all the recorded responses. Below it is <code class="calibre19">resp_time_counter</code> with the number of responses. Based on those two, we can see that <code class="calibre19">69</code> responses took <code class="calibre19">0.0034</code> seconds. If we’d like to get the average time of the responses, we’d need to divide <code class="calibre19">sum</code> with <code class="calibre19">count</code>.</p>

<p class="calibre3">In addition to <code class="calibre19">sum</code> and <code class="calibre19">count</code>, we can observe the number of responses grouped into different buckets called quantiles. At the moment, all sixty-nine requests fall into all of the quantiles, so we’ll postpone discussion about them until we reach the examples with more differencing response times.</p>

<p class="calibre3">One thing worth noting is that the metrics come from only one of the three replicas, so our current examples do not paint the full picture. Later on, when we start scraping the metrics with Prometheus, we’ll see that they are aggregated from all the replicas.</p>

<p class="calibre3">Finally, you might have expected around thirty-three responses since we sent a hundred requests that were distributed across three replicas. However, the service continuously pings itself, so the final number was quite higher.</p>

<p class="calibre3">Let’s get out of the <code class="calibre19">util</code> container and try to generate some requests that will end with errored responses.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..100<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">4 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/random-error"</code>
<code class="lineno">5 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">/demo/random-error</code> endpoint produces response code <code class="calibre19">500</code> in approximately ten percent of cases. The rest should be “normal” responses with status code <code class="calibre19">200</code>.</p>

<p class="calibre3">The output should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>...
<code class="lineno">2 </code>Everything is still OK
<code class="lineno">3 </code>Everything is still OK
<code class="lineno">4 </code>Everything is still OK
<code class="lineno">5 </code>Everything is still OK
<code class="lineno">6 </code>ERROR: Something, somewhere, went wrong!
<code class="lineno">7 </code>...
</pre></div>

</figure>

<p class="calibre3">Let’s see how do metrics look like now.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl <code class="s">"http://go-demo_main:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">The output limited to the relevant parts is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code># HELP http_server_resp_time Request response time
<code class="lineno"> 3 </code># TYPE http_server_resp_time histogram
<code class="lineno"> 4 </code>...
<code class="lineno"> 5 </code>http_server_resp_time_sum{code="200",method="GET",path="/demo/random-error",serv\
<code class="lineno"> 6 </code>ice="go-demo"} 0.001033751
<code class="lineno"> 7 </code>http_server_resp_time_count{code="200",method="GET",path="/demo/random-error",se\
<code class="lineno"> 8 </code>rvice="go-demo"} 32
<code class="lineno"> 9 </code>...
<code class="lineno">10 </code>http_server_resp_time_sum{code="500",method="GET",path="/demo/random-error",serv\
<code class="lineno">11 </code>ice="go-demo"} 7.033700000000001e-05
<code class="lineno">12 </code>http_server_resp_time_count{code="500",method="GET",path="/demo/random-error",se\
<code class="lineno">13 </code>rvice="go-demo"} 2
<code class="lineno">14 </code>...
</pre></div>

</figure>

<p class="calibre3">Since the response code is one of the labels, we got two metrics; one for the code <code class="calibre19">200</code>, and the other for <code class="calibre19">500</code>. Since those hundred requests were load balanced across three replicas, the one that produced this output got approximately one-third of them (32+2). We can see that the requests that produce errors take considerably longer time with the total of seven seconds for only two requests.</p>

<p class="calibre3">You might have been “unlucky” and did not get a single response with the code <code class="calibre19">500</code>. If that was the case, feel free to send another hundred requests.</p>

<p class="calibre3">Now that we confirmed that our response metrics are separated by different labels, we should explore quantiles. For that, we need to simulate queries with varying response times. Fortunately, <code class="calibre19">go-demo</code> has such an endpoint.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..30<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">4 </code>    <code class="nv">DELAY</code><code class="o">=</code>$<code class="o">[</code> <code class="nv">$RANDOM</code> % <code class="o">6000</code> <code class="o">]</code>
<code class="lineno">5 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello?delay=</code><code class="nv">$DELAY</code><code class="s">"</code>
<code class="lineno">6 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">When <code class="calibre19">delay</code> query parameter is set, <code class="calibre19">go-demo</code> goes to sleep for the specified number of milliseconds. We made thirty iterations. Each generated a random number between 0 and 6000 and sent that number as the <code class="calibre19">delay</code> parameter. As a result, the service should have received requests with a wide range of response times.</p>

<p class="calibre3">Let’s take another look at the metrics.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl <code class="s">"http://go-demo_main:8080/metrics"</code>
</pre></div>

</figure>

<p class="calibre3">The output, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code># HELP http_server_resp_time Request response time
<code class="lineno"> 3 </code># TYPE http_server_resp_time histogram
<code class="lineno"> 4 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno"> 5 </code>"go-demo",le="0.005"} 78
<code class="lineno"> 6 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno"> 7 </code>"go-demo",le="0.01"} 78
<code class="lineno"> 8 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno"> 9 </code>"go-demo",le="0.025"} 78
<code class="lineno">10 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">11 </code>"go-demo",le="0.05"} 78
<code class="lineno">12 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">13 </code>"go-demo",le="0.1"} 78
<code class="lineno">14 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">15 </code>"go-demo",le="0.25"} 78
<code class="lineno">16 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">17 </code>"go-demo",le="0.5"} 79
<code class="lineno">18 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">19 </code>"go-demo",le="1"} 80
<code class="lineno">20 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">21 </code>"go-demo",le="2.5"} 83
<code class="lineno">22 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">23 </code>"go-demo",le="5"} 87
<code class="lineno">24 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">25 </code>"go-demo",le="10"} 88
<code class="lineno">26 </code>http_server_resp_time_bucket{code="200",method="GET",path="/demo/hello",service=\
<code class="lineno">27 </code>"go-demo",le="+Inf"} 88
<code class="lineno">28 </code>http_server_resp_time_sum{code="200",method="GET",path="/demo/hello",service="go\
<code class="lineno">29 </code>-demo"} 29.430902277
<code class="lineno">30 </code>http_server_resp_time_count{code="200",method="GET",path="/demo/hello",service="\
<code class="lineno">31 </code>go-demo"} 88
<code class="lineno">32 </code>...
</pre></div>

</figure>

<p class="calibre3">Now we have the combination of the fast responses from before combined with those with a delay of up to six seconds. If we focus only on the last two lines, we can see that there are <code class="calibre19">88</code> responses in total with the summed time of <code class="calibre19">29.43</code> seconds. The average time of responses is around <code class="calibre19">0.33</code> seconds. That, in itself, does not give us enough information. Maybe two requests lasted for <code class="calibre19">10</code> seconds each, and all of the rest were lightning fast. Or, perhaps, all of the requests were below <code class="calibre19">0.5</code> seconds. We cannot know that by just looking at the sum of all response times and dividing them with the count. We need quantiles.</p>

<p class="calibre3">The histogram used in <code class="calibre19">go-demo</code> did not specify buckets, so the quantiles are those defined by default. They range from as low as <code class="calibre19">0.005</code> to as high as <code class="calibre19">10</code> seconds. If you pay closer attention to the numbers beside each of those buckets, you’ll see that <code class="calibre19">78</code> requests were below <code class="calibre19">0.25</code> seconds, <code class="calibre19">79</code> below <code class="calibre19">0.5</code>, and so on all the way until all of the <code class="calibre19">88</code> requests being below <code class="calibre19">10</code> seconds. All the requests from a smaller bucket belong to the larger one. That might be confusing the first time we look at the metrics, but it makes perfect sense. A request that lasted less than, for example, <code class="calibre19">0.5</code> seconds, definitely lasted less than, <code class="calibre19">1</code> seconds, and so on.</p>

<p class="calibre3">Using quantiles (or buckets) will be essential when we start defining Prometheus alerts based on those metrics, so we’ll postpone further discussion until we reach that part.</p>

<p class="calibre3">As you can see, unlike counters and gauges, histograms go beyond simple additions and subtractions. They provide observations over a period. They track the number of observations and their summaries thus allowing us to calculate average values. The number of observations behaves like a counter. It can only be increased. The sum, on the other hand, is similar to a gauge. It can be both increased and decreased depending on the values we observe. If it is negative, the sum will decrease. We did not explore such an example since response times are always positive.</p>

<p class="calibre3">The most common usage of histrograms is to record request durations and response times. We explored one of those two through our examples.</p>

<p class="calibre3">How about summaries? They are the only metric type we did not explore.</p>

<p class="calibre3"><em class="calibre21">Summary</em> is similar to <em class="calibre21">histogram</em> metric type. Both sample observations. The major difference is that summary calculates quantiles based on a sliding time frame. We won’t go deeper into summaries. Instead, please read the <a href="https://prometheus.io/docs/practices/histograms/">Histograms And Summaries</a> page that explains both in more detail and provides a comparison of the two.</p>

<h3 id="leanpub-auto-what-now-9" class="calibre20">What Now?</h3>

<p class="calibre3">We explored, through a few examples, how to instrument our services and provide more detailed metrics than what we would be able to do through exporters. Early in the book, I said that we should use exporters instead instrumentation unless they do not provide enough information. It turned out that they do not. If, for example, we used an exporter, we would get metrics based on requests coming through the proxy. We would not be aware of internal communication between services nor would we be able to obtain response times of certain parts of the services we’re deploying. Actually, <a href="https://github.com/prometheus/haproxy_exporter">HAProxy Exporter</a> does not even provide response times since the internal metrics it exposes is not entirely compatible with Prometheus and cannot be exported without sacrificing accuracy. That does not mean that HAProxy metrics are not accurate but that they use a different logic. Instead of having a counter, HAProxy exposes response as exponentially decaying value. It cannot be transformed into a histogram.</p>

<p class="calibre3">If you’re interested in the discussion about <em class="calibre21">HAProxy Exporter</em> response time, please visit <a href="https://github.com/prometheus/haproxy_exporter/issues/37">issue 37</a>.</p>

<p class="calibre3">Without accurate response times, we cannot instruct our system to scale and de-scale them effectively.  We need to obtain more information if we want to get closer to building a truly <em class="calibre21">self-adapting</em> system.</p>

<p class="calibre3">While instrumentation we explored through examples is by no means all the instrumentation we should add, it does provide a step forward. Even though response times are not the only metric we’re missing, it is probably the most important one. Counting errors is useful as well but does not provide clear guidance. Some errors will need a different set of actions, and many cannot even be hooked into the system that auto-corrects itself. Generally speaking, errors often (but not always) require human intervention. Response times, on the other hand, are easy to grasp. They do provide clear guidance for the system. If it goes over a certain threshold within a predefined period, scale up. If it goes down, scale down.</p>

<p class="calibre3">The next chapter will continue exploring response times. We’ll see what we can do with them in Prometheus and how we can improve our current alerts by incorporating this new data.</p>

<p class="calibre3">And now we need a break. Take a rest, go to sleep, recharge your batteries. Before you do any of that, remember that your computer needs a rest too. Get out of the <code class="calibre19">util</code> container and remove the machines we created.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker-machine rm -f swarm-1 swarm-2 swarm-3
</pre></div>

</figure>



</div>
</body></html>