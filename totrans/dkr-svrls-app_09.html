<html><head></head><body>
        

                            
                    <h1 class="header-title">The Future of Serverless</h1>
                
            
            
                
<p>This chapter discusses what lies ahead beyond FaaS. We will start by discussing a new experimental technique to restore the speed of the container runtime by introducing RunF, a libcontainer-based runtime designed for invoking immutable function containers. This chapter will continue the discussion of the possibility of using LinuxKit to prepare immutable infrastructure for FaaS platforms in general. We conclude this chapter by exploring a new architecture to hybrid the FaaS architecture on-premises with the serverless architecture on the public cloud.</p>
<p>Before going to these topics, let's start by summarizing what we have learned so far.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>FaaS and Docker reviews</li>
<li>Runtime for function containers</li>
<li>LinuxKit – immutable infrastructure for FaaS</li>
<li>Beyond serverless</li>
<li>Declarative containers</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">FaaS and Docker reviews</h1>
                
            
            
                
<p>In this book, we introduced serverless, the FaaS platforms, and how Docker is relevant to this technology. We learned together about how to set up Docker Swarm clusters on production.</p>
<p>The book discussed three well-known FaaS platforms, which are <em>OpenFaaS</em>, <em>OpenWhisk</em>, and the <em>Fn Project</em>. OpenFaaS uses the Swarm-based orchestrator, while OpenWhisk and Fn used their own scheduling techniques on plain Docker.</p>
<p>Then, we demonstrated a project in <a href="0d30ef75-34b4-4a72-9b0a-71a8e335d494.xhtml" target="_blank">Chapter 8</a>, <em>Putting Them All Together</em>, to present how we can link all of these three platforms together, by running them on the same network of a Docker cluster. The project was demonstrated on how we could invoke the services of other FaaS platforms. Functions written in several programming languages were presented including Java, Go, and JavaScript (Node.js).</p>
<p>We used Java to write a simple function. For modern programming models, we could use the RxJava library to help writing Java programs in the reactive style, which fit very nicely for the event-driven programming.</p>
<p>With JavaScript, we wrote a Chrome-based scripting to connect through. We also deployed a blockchain to demonstrate that it works nicely with the FaaS computing model.</p>
<p>In the following sections, we will discuss some advanced, experimental topics that go deeply or beyond the current scope of serverless and FaaS. However, some of them may be going to be mainstream in the near future.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Runtime for function containers</h1>
                
            
            
                
<p>One of the most important components of the container ecosystem is the <strong>container runtime</strong>. During the early days of Docker, the runtime was LXC, then it changed to be the Docker-owned libcontainer. The libcontainer was later donated to <strong>OCI</strong>, the <strong>Open Container Initiative</strong> project under the Linux Foundation. Later, Project RunC was started. RunC is a command-line wrapper around libcontainer to enable developers to start containers from a Terminal. A developer could start a container by invoking the RunC binary and passing a root filesystem and a container specification to it.</p>
<p>RunC is an extremely stable piece of software. It has been with Docker since version 1.12 and is already used by millions of users. The <kbd>docker run</kbd> command actually sends its parameters to another daemon, containerd, which converts that information into a configuration file for RunC.</p>
<p>RunC makes the dependencies simpler as we need only a single binary, a root filesystem, and a configuration file to start a container.</p>
<p>As RunC is a thin wrapper around libcontainer, its code is straightforward. It is relatively easy to directly make use of libcontainer with some Go programming knowledge. The only drawback of RunC is that it is designed and built to run containers in general. In the next section, we will introduce <em>RunF,</em> a minimal runtime designed specially for running function containers efficiently.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Meet RunF</h1>
                
            
            
                
<p>This section introduces RunF. It is a RunC counterpart that is designed for running immutable function containers. RunF is an experimental project that uses <em>libcontainer</em> to implement a new runtime to run containers in the read-only and rootless environment. Containers started with RunF are expected to be running efficiently, even inside other containers. RunF allows rootless container execution by mapping a non-root user from the host to the root user's ID inside the container.</p>
<p>How can we use it? The following diagram illustrates the scenario. We have a FaaS platform, and the <strong>Gateway</strong> accepts the incoming request and forwards it to the function <strong>Initiator</strong>. Through the <strong>Event Bus</strong>, a <strong>Function Executor</strong> then uses it rather than Docker to invoke the function container. With this architecture, we can improve the overall performance of the platform:</p>
<div><img src="img/ba5ab83d-cca2-4693-b9fd-8c6162aab03c.png" style="width:45.42em;height:28.08em;"/></div>
<p>Figure 9.1: The block diagram illustrating a FaaS architecture with RunF as its runtime</p>
<p class="mce-root">A rootless container is a container allowed to run without the root user, such as in AWS Lambda. We want an immutable version of a function with read-only and rootless, because rootless containers make the system and infrastructure more secure.</p>
<p class="mce-root">Then there is a network constraint. A function should not be aware of any network-related configuration. All current FaaS platforms we have implemented so far have this limitation. Say we need to attach a running function to a certain network in order to make it work correctly, and be able to resolve names of other dependent services.</p>
<p class="mce-root">We found during <a href="0d30ef75-34b4-4a72-9b0a-71a8e335d494.xhtml" target="_blank">Chapter 8</a>, <em>Putting Them All Together</em>, that it is tricky to make a function container work virtually with any network provided by the platform. <em>RunF</em> is designed to solve this issue by letting the function container use the outer container network namespace. With this execution model, the <em>function proxy</em> is responsible for attaching itself to the networks, and the function container will also use these networks to access other services. If the function container runs inside the container of the function proxy, all network configuration could be eliminated.</p>
<p class="mce-root">Performance-wise with a special container runtime such as <em>RunF</em>, it is possible to cache all necessary filesystems inside each function proxy, and make them immutable. With this, we can achieve the highest possible performance similar to the way the mechanism of hot functions work.</p>
<p class="mce-root">Now let's see what's inside the implementation to make it meet all requirements:</p>
<ul>
<li class="mce-root">Immutable</li>
<li class="mce-root">Rootless</li>
<li class="mce-root">Host networking by default</li>
<li class="mce-root">Zero configuration.</li>
</ul>
<p>We mostly use the libcontainer APIs directly. Here, we explain the details to show how RunF uses libcontainer to achieve the mutable runtime for function containers.</p>
<p>The program starts by initializing the libcontainer, with the <kbd>Cgroupfs</kbd> configuration, to say that libcontainer will use <kbd>Cgroup</kbd> to control the resources of the process:</p>
<pre>func main() {<br/><br/>  containerId := namesgenerator.GetRandomName(0)<br/><br/>  factory, err := libcontainer.New("/tmp/runf",<br/>    libcontainer.Cgroupfs,<br/>    libcontainer.InitArgs(os.Args[0], "init"))<br/>  if err != nil {<br/>    logrus.Fatal(err)<br/>    return<br/>  }</pre>
<p>The following snippet creates a config. The default location of the <kbd>rootfs</kbd> is <kbd>./rootfs</kbd> under the current directory. We set the flag <kbd>Readonlyfs</kbd> to be <kbd>true</kbd> for the immutable filesystem. <kbd>NoNewPrivileges</kbd> is set to <kbd>true</kbd> so as to not allow the process to gain any new privilege. <kbd>Rootless</kbd> being <kbd>true</kbd> is designed to tell us that we will map non-root UID and GID to the container's root ID. After the initial flags, we then set the capability of the process. Here's the list:</p>
<ul>
<li><kbd>CAP_AUDIT_WRITE</kbd> is the ability to write to the kernel's audit logs</li>
<li><kbd>CAP_KILL</kbd> is the ability for the process to send the signals</li>
<li><kbd>CAP_NET_BIND_SERVICE</kbd> is the ability to bind a socket to the privileged ports</li>
</ul>
<pre>  defaultMountFlags := unix.MS_NOEXEC | unix.MS_NOSUID | unix.MS_NODEV<br/><br/>  cwd, err := os.Getwd()<br/>  currentUser, err := user.Current()<br/>  uid, err := strconv.Atoi(currentUser.Uid)<br/>  gid, err := strconv.Atoi(currentUser.Gid)<br/>  caps := []string{<br/>    "CAP_AUDIT_WRITE",<br/>    "CAP_KILL",<br/>    "CAP_NET_BIND_SERVICE",<br/>  }<br/><br/>  config := &amp;configs.Config{<br/>    Rootfs:          cwd + "/rootfs",<br/>    Readonlyfs:      true,<br/>    NoNewPrivileges: true,<br/>    Rootless:        true,<br/>    Capabilities: &amp;configs.Capabilities{<br/>      Bounding:    caps,<br/>      Permitted:   caps,<br/>      Inheritable: caps,<br/>      Ambient:     caps,<br/>      Effective:   caps,<br/>    },</pre>
<p>The <kbd>Namespaces</kbd> property is one of the most important settings of the container runtime. Within this block of configuration, we set it to use the following namespaces, <kbd>NS</kbd>, <kbd>UTS</kbd> (hostname and domain name), <kbd>IPC</kbd>, <kbd>PID</kbd>, and <kbd>USER</kbd>. The user namespace, <kbd>NSUSER</kbd>, is the key setting to allow running containers in the rootless mode. We left out the <kbd>NET</kbd> namespace. The reason is that <kbd>runf</kbd> will start a function container inside another container, the <em>function executor</em>. Without the <kbd>NET</kbd> namespace isolation, the function container will use the same network namespace as the outside container, so it will be able to access any service attached to the network of the function executor.</p>
<p>Another setting is the <kbd>Cgroup</kbd> setting. This setting allows hierarchical control resources of the process. This is mostly the default configuration:</p>
<pre>Namespaces: configs.Namespaces([]configs.Namespace{<br/>  {Type: configs.NEWNS},<br/>  {Type: configs.NEWUTS},<br/>  {Type: configs.NEWIPC},<br/>  {Type: configs.NEWPID},<br/>  {Type: configs.NEWUSER},<br/>}),<br/>Cgroups: &amp;configs.Cgroup{<br/>  Name:      "runf",<br/>  Parent:    "system",<br/>  Resources: &amp;configs.Resources{<br/>    MemorySwappiness: nil,<br/>    AllowAllDevices:  nil,<br/>    AllowedDevices:   configs.DefaultAllowedDevices,<br/>  },<br/>},</pre>
<p><kbd>MaskPaths</kbd> and <kbd>ReadonlyPaths</kbd> are set as the following. This setting is mainly to prevent the changes made by the running process to the system:</p>
<pre>MaskPaths: []string{<br/>  "/proc/kcore",<br/>  "/proc/latency_stats",<br/>  "/proc/timer_list",<br/>  "/proc/timer_stats",<br/>  "/proc/sched_debug",<br/>  "/sys/firmware",<br/>  "/proc/scsi",<br/>},<br/>ReadonlyPaths: []string{<br/>  "/proc/asound",<br/>  "/proc/bus",<br/>  "/proc/fs",<br/>  "/proc/irq",<br/>  "/proc/sys",<br/>  "/proc/sysrq-trigger",<br/>},</pre>
<p>All devices are set to be auto created. Then, the <kbd>Mount</kbd> setting defines a set of filesystems required to mount from the host into the container. In the case of RunF, it is a nested mounted from the function executor to the function container:</p>
<pre>Devices: configs.DefaultAutoCreatedDevices,<br/>Hostname: containerId,<br/>Mounts: []*configs.Mount{<br/>  {<br/>    Source:      "proc",<br/>    Destination: "/proc",<br/>    Device:      "proc",<br/>    Flags:       defaultMountFlags,<br/>  },<br/>  {<br/>    Source:      "tmpfs",<br/>    Destination: "/dev",<br/>    Device:      "tmpfs",<br/>    Flags:       unix.MS_NOSUID | unix.MS_STRICTATIME,<br/>    Data:        "mode=755",<br/>  },<br/>  {<br/>    Device:      "devpts",<br/>    Source:      "devpts",<br/>    Destination: "/dev/pts",<br/>    Flags:       unix.MS_NOSUID | unix.MS_NOEXEC,<br/>    Data:        "newinstance,ptmxmode=0666,mode=0620",<br/>  },<br/>  {<br/>    Device:      "tmpfs",<br/>    Source:      "shm",<br/>    Destination: "/dev/shm",<br/>    Flags:       defaultMountFlags,<br/>    Data:        "mode=1777,size=65536k",<br/>  },<br/>},</pre>
<p>Here's the UID and GID mapping from the host ID (<kbd>HostID</kbd>) to the ID inside the container (<kbd>ContainerID</kbd>). In the following example, we map the current user ID to the ID of the <kbd>root</kbd> user inside the container:</p>
<pre>    Rlimits: []configs.Rlimit{<br/>      {<br/>        Type: unix.RLIMIT_NOFILE,<br/>        Hard: uint64(1024),<br/>        Soft: uint64(1024),<br/>      },<br/>    },<br/>    UidMappings: []configs.IDMap{<br/>      {<br/>        ContainerID: 0,<br/>        HostID:      uid,<br/>        Size:        1,<br/>      },<br/>    },<br/>    GidMappings: []configs.IDMap{<br/>      {<br/>        ContainerID: 0,<br/>        HostID:      gid,<br/>        Size:        1,<br/>      },<br/>    },<br/>  }</pre>
<p>We use libcontainer's factor to create a container with the generated ID and the <kbd>config</kbd> we have set:</p>
<pre>  container, err := factory.Create(containerId, config)<br/>  if err != nil {<br/>    logrus.Fatal(err)<br/>    return<br/>  }</pre>
<p>Then, we prepare environment variables. They are simply an <em>array of strings</em>. Each element is a <em>key=value</em> pair of each variable that we'd like to set for the process. We prepare a process to run using <kbd>libcontainer.Process</kbd>. Process input, output, and error are redirected to the default standard counterparts:</p>
<pre>  environmentVars := []string{<br/>    "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",<br/>    "HOSTNAME=" + containerId,<br/>    "TERM=xterm",<br/>  }<br/>  process := &amp;libcontainer.Process{<br/>    Args:   os.Args[1:],<br/>    Env:    environmentVars,<br/>    User:   "root",<br/>    Cwd:    "/",<br/>    Stdin:  os.Stdin,<br/>    Stdout: os.Stdout,<br/>    Stderr: os.Stderr,<br/>  }<br/><br/>  err = container.Run(process)<br/>  if err != nil {<br/>    container.Destroy()<br/>    logrus.Fatal(err)<br/>    return<br/>  }<br/><br/>  _, err = process.Wait()<br/>  if err != nil {<br/>    logrus.Fatal(err)<br/>  }<br/><br/>  defer container.Destroy()<br/>}</pre>
<p>We will then prepare and build the <kbd>runf</kbd> binary. This requires <em>libcontainer</em> and other few to build. We normally use the <kbd>go get</kbd> command to do so. After that, just simply build with the <kbd>go build</kbd> command:</p>
<pre class="mce-root"><strong>$ go get golang.org/x/sys/unix</strong><br/><strong>$ go get github.com/Sirupsen/logrus</strong><br/><strong>$ go get github.com/docker/docker/pkg/namesgenerator</strong><br/><strong>$ go get github.com/opencontainers/runc/libcontainer</strong><br/><br/><strong>$ go build runf.go</strong></pre>
<p>To prepare a root filesystem, we use <kbd>undocker.py</kbd> together with the <kbd>docker save</kbd> command. The <kbd>undocker.py</kbd> script can be downloaded from <a href="https://github.com/larsks/undocker">https://github.com/larsks/undocker</a>.</p>
<p>Here's the command to prepare a root filesystem to the <kbd>rootfs</kbd> directory from the <kbd>busybox</kbd> image:</p>
<pre><strong>$ docker save busybox | ./undocker.py --output rootfs -W -i busybox</strong></pre>
<p class="mce-root">Now, let's test running some containers. We will see that the <kbd>ls</kbd> command lists files inside a container:</p>
<pre><strong>$ ./runf ls</strong><br/><strong>bin dev etc home proc root sys tmp usr var</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Within a Docker network</h1>
                
            
            
                
<p>Next, we will try something a bit advanced by preparing a small system that looks similar to the following diagram. The scenario is that we would like a container started by <strong>runf</strong> inside another container, <strong>wrapper-runf</strong> (which is, in reality, a function executor), to connect to some network services running on the same Docker network, <strong>test_net</strong>:</p>
<div><img src="img/215f30db-9eb2-4691-ac17-cd519153647c.png" style="width:46.25em;height:30.58em;"/></div>
<p>Figure 9.2: An example of using RunF inside a Docker network</p>
<p>The trick is that we put <kbd>resolv.conf</kbd> from the standard Docker Swarm mode as <kbd>./rootfs/etc/resolv.conf</kbd> to make the process inside the nested container be able to resolve all service names on the attached Docker network. Here's the content of <kbd>resolv.conf</kbd>:</p>
<pre>search domain.name<br/>nameserver 127.0.0.11<br/>options ndots:0</pre>
<p>Then we prepare a Dockerfile for the <kbd>wrapper-runf</kbd> container:</p>
<pre>FROM ubuntu:latest<br/><br/>RUN apt-get update &amp;&amp; apt-get install -y curl<br/><br/>WORKDIR /root<br/><br/>COPY ./runf /usr/bin/runf<br/>COPY rootfs /root/rootfs<br/>COPY resolv.conf /root/rootfs/etc/resolv.conf</pre>
<p>We can build it normally with the <kbd>docker build</kbd> command:</p>
<pre><strong>$ docker build -t wrapper-runf .</strong></pre>
<p>The following snippet is the preparation for creating a Docker network, attaching <kbd>nginx</kbd> to the network, then running a <kbd>wrapper-runf</kbd> container with <kbd>/bin/bash</kbd> there.</p>
<p>Finally, we start a nested container with <kbd>runf</kbd> that connects to <kbd>nginx</kbd>:</p>
<pre><strong>$ docker network create -d overlay --attachable test_net</strong><br/><br/><strong>$ docker run -d \</strong><br/><strong>  --network=test_net \</strong><br/><strong>  --network-alias=nginx \</strong><br/><strong>  nginx</strong><br/><br/><strong>$ docker run --rm -it \</strong><br/><strong>  --network=test_net \</strong><br/><strong>  --privileged \</strong><br/><strong>  -v /sys/fs/cgroup:/sys/fs/cgroup \</strong><br/><strong>  wrapper-runf /bin/bash</strong><br/><br/><strong>/ # runf wget http://nginx</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">What's next?</h1>
                
            
            
                
<p>With <kbd>runf</kbd>, it is potentially a way to move towards another step of fast and immutable functions with a special runtime. What you can try is to implement a proxy container wrapping around <kbd>runf</kbd> and make it run functions inside the real platform. This is left as an (a bit advanced) exercise.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">LinuxKit – immutable infrastructure for FaaS</h1>
                
            
            
                
<p>LinuxKit is a set of tools for preparing immutable sets of infrastructure. It is designed to compose containers into a ready-to-use OS. Of course, an OS produced by LinuxKit is for running containers. To make an immutable and scalable infrastructure for FaaS platforms, LinuxKit is one of the best choices out there.</p>
<p>Here's a sample of a LinuxKit YAML file to build an immutable OS for Docker. The kernel block is saying that this OS will boot with Linux Kernel 4.14.23. The <kbd>boot</kbd> command, <kbd>cmdline</kbd>, says that the kernel will be starting with consoles on four different TTYs:</p>
<pre>kernel:<br/>  image: linuxkit/kernel:4.14.23<br/>  cmdline: "console=tty0 console=ttyS0 console=ttyAMA0 console=ttysclp0"</pre>
<p>The four next containers declared inside the <kbd>init</kbd> block are the base programs that will be unpacked directly onto the filesystem. All the <kbd>init</kbd> level programs include <kbd>runc</kbd> and containerd. Also, the CA certificates will be installed directly onto the filesystem before the programs declared in the next, <kbd>onboot</kbd>, block can proceed:</p>
<pre>init:<br/>  - linuxkit/init:b212cfeb4bb6330e0a7547d8010fe2e8489b677a<br/>  - linuxkit/runc:7c39a68490a12cde830e1922f171c451fb08e731<br/>  - linuxkit/containerd:37e397ebfc6bd5d8e18695b121166ffd0cbfd9f0<br/>  - linuxkit/ca-certificates:v0.2</pre>
<p>The <kbd>onboot</kbd> block and the <kbd>mountie</kbd> command will automatically mount the first available partition to <kbd>/var/lib/docker</kbd>. Please note that LinuxKit only allows you to mount to the directory under the <kbd>/var</kbd> directory:</p>
<pre>onboot:<br/>  - name: sysctl<br/>    image: linuxkit/sysctl:v0.2<br/>  - name: sysfs<br/>    image: linuxkit/sysfs:v0.2<br/>  - name: format<br/>    image: linuxkit/format:v0.2<br/>  - name: mount<br/>    image: linuxkit/mount:v0.2<br/>    command: ["/usr/bin/mountie", "/var/lib/docker"]</pre>
<p>The <kbd>services</kbd> block declares system containers, which serve as long running services. All these services are run and maintained by containers, started by the <kbd>init</kbd> process in the <kbd>init</kbd> block.</p>
<p>A service declared in this block can be started in any order.</p>
<p>In the following example, <kbd>docker</kbd> is one of the services. Docker image, <kbd>docker:17.09.0-ce-dind</kbd>, is used for running this Docker service. This service runs on the host network. This is basically the same concept as RancherOS. This instance of <kbd>dockerd</kbd> run by the <kbd>docker</kbd> service is the user-level container management system, while containerd from the <kbd>init</kbd> block is the system-level container management system. Other system containers here are <kbd>rngd</kbd>—a random number generator daemon, <kbd>dhcpd</kbd>—an DHCP service, and <kbd>ntpd</kbd>—the OpenNTPD daemon for syncing the machine clock, for example:</p>
<pre>services:<br/>  - name: getty<br/>    image: linuxkit/getty:v0.2<br/>    env:<br/>     - INSECURE=true<br/>  - name: rngd<br/>    image: linuxkit/rngd:v0.2<br/>  - name: dhcpcd<br/>    image: linuxkit/dhcpcd:v0.2<br/>  - name: ntpd<br/>    image: linuxkit/openntpd:v0.2<br/>  - <strong>name: docker</strong><br/>    <strong>image: docker:17.09.0-ce-dind</strong><br/>    capabilities:<br/>     - all<br/>    net: host<br/>    mounts:<br/>     - type: cgroup<br/>       options: ["rw","nosuid","noexec","nodev","relatime"]<br/>    binds:<br/>     - /etc/resolv.conf:/etc/resolv.conf<br/>     - /var/lib/docker:/var/lib/docker<br/>     - /lib/modules:/lib/modules<br/>     - /etc/docker/daemon.json:/etc/docker/daemon.json<br/>    command: ["/usr/local/bin/docker-init", "/usr/local/bin/dockerd"]</pre>
<p>The file block is for declaring <em>files</em> or <em>directories</em> that we would like to have on our immutable filesystem. In the following example, we declare <kbd>/var/lib/docker</kbd> and create a Docker's daemon configuration <kbd>/etc/docker/daemon.json</kbd> with the content <kbd>{"debug": true}</kbd> inside it. These files are created during the image's build phase:</p>
<pre>files:<br/>  - path: var/lib/docker<br/>    directory: true<br/>  - path: etc/docker/daemon.json<br/>    contents: '{"debug": true}'<br/>trust:<br/>  org:<br/>    - linuxkit<br/>    - library</pre>
<p>We have another example of the <kbd>files</kbd> block. This is the standard way to put our public key into the filesystem image. The attribute <kbd>mode</kbd> is for setting the file mode when copying the file to the final image. In this example, we require the public key file to be <kbd>0600</kbd>. With this configuration and the running <kbd>sshd</kbd> service, we will be allowed to remotely SSH into the machine:</p>
<pre>files:<br/>  - path: root/.ssh/authorized_keys<br/>    source: ~/.ssh/id_rsa.pub<br/>    mode: "0600"<br/>    optional: true</pre>
<p>Here's the step to build the LinuxKit command line:</p>
<pre><strong>$ go get -u github.com/linuxkit/linuxkit/src/cmd/linuxkit</strong></pre>
<p>If we have already installed the Go programming language using GVM, the binary will be available to run.</p>
<p>We'll build a Docker OS, available at <a href="https://github.com/linuxkit/linuxkit/blob/master/examples/docker.yml">https://github.com/linuxkit/linuxkit/blob/master/examples/docker.yml</a>:</p>
<pre><strong>$ linuxkit build docker.yml </strong><br/><strong>Extract kernel image: linuxkit/kernel:4.14.26</strong><br/><strong>Pull image: docker.io/linuxkit/kernel:4.14.26@sha256:9368a ...</strong><br/><strong>...</strong><br/><strong>Add files:</strong><br/><strong>  var/lib/docker</strong><br/><strong>  etc/docker/daemon.json</strong><br/><strong>Create outputs:</strong><br/><strong>  docker-kernel docker-initrd.img docker-cmdline</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Beyond serverless</h1>
                
            
            
                
<p>Hybrid serverless would be a deployment model that links hybrid cloud to the serverless deployment model. It is already started by IT vendors offering hardware rental services in the form of private clouds, putting them to customer's organizations, and charging at the rate of pay-as-you-go.</p>
<p>When the serverless and FaaS computing platforms are deployed on top of that kind of hybrid infrastructure, they become <strong>hybrid serverless</strong>. This could be the next generation of computing platform that allows you to store sensitive data inside the organization, having some important FaaS functions running on the local system, while leveraging some extra computing resources as pay-per-request. It will be in the scope of the definition of serverless, if the customer's organization does not need to maintain or administer any of the hardware servers. Fortunately, when mixing this model with what we have discussed throughout this book, using Docker as our infrastructure would still be applied to this kind of infrastructure. Docker is still a good choice for balancing between maintaining infrastructure on our own and making the serverless platforms do the rest of the work for us.</p>
<p>In the following diagram, the overall system shows a hybrid architecture. In the case of using a FaaS platform only from inside the organization, requests would be made firstly to the on-premises infrastructure. When loads become large, instances of the function executors would be scaled out and eventually burst to a public cloud infrastructure. However, the data stores are usually placed inside the organization. So, the outside function executors must be able to access them just as if they were running on-premises:</p>
<div><img src="img/e6768fae-af48-4305-b6bc-136a4c2c085f.png"/></div>
<p>Figure 9.3: A hybrid architecture for FaaS</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Declarative containers</h1>
                
            
            
                
<p>Declarative containers could be considered as a technology in between a normal container and a container running on FaaS. Let's look at the following imaginary Dockerfile:</p>
<pre>FROM openjdk:8<br/><br/>COPY app.jar /app/app.jar<br/><br/>CMD ["/opt/jdk/bin/java", "-Xmx2G", "-jar", "/app/app.jar"]</pre>
<p>What do we see here? At the first time of reading, it would look like a normal Dockerfile. Yes, it is. But it's not a declarative way to define an application container. Why?</p>
<p>We already know that this is a Java application doing some work for us. But it has hardcoded some important and brittle configurations, for example, when <kbd>openjdk:8</kbd> pinned the app to use only that implementation, while <kbd>-Xmx2G</kbd> limits the memory usage of the app.</p>
<p>All FaaS platforms these days use containers in the same way. They tie some specific configurations into function containers but actually people need a very neutral and portable way of deploying functions.</p>
<p>So what does a declarative container look like?</p>
<p>It looks something like this:</p>
<pre>FROM scratch<br/><br/>COPY app.jar /app/app.jar<br/><br/>CMD ["java", "-jar", "/app/app.jar"]</pre>
<p>You might think it is impossible to run this container on any runtime at the moment. The answer is you are correct. But I still argue that the application should be declared in the same way. We should remove all brittle configuration out of the <kbd>Dockerfile</kbd> as much as possible. Then we should let a new entity, maybe inside a container engine, manage the environment around the application for us.</p>
<p>For example, it is relatively easy to intercept the container creation process then compute the limitation of the memory allowed by the container (via <kbd>docker run -m</kbd> for example) and put that value into the command line of <kbd>java</kbd> to cap the memory limit at the application level. The entity responsible for this kind of work inside a container engine would be called the <strong>Application Profile Manager</strong>, as shown in the following diagram:</p>
<div><img src="img/61d4cc6a-a41b-470a-b849-92fb0861c78f.png" style="width:40.50em;height:30.17em;"/></div>
<p>Figure 9.4: A container engine with the application profile manager</p>
<p>The crosscutting concept like this is nothing new. We already have a similar concept applied to Docker. Guess what? It's the security concern. Docker already has the AppArmor default profile applied to each running container with the AppArmor subsystem enabled. That's about security concerns. This is at the more application-specific level of concern, so why don't we have a similar concept to help make our life easier?</p>
<p>With this concept, container images would become declarative containers as there is no specific environment or configuration hardcoded for them. And it's the responsibility of the <strong>Application Profile Manager</strong> to selectively apply an appropriate profile for the container and make it work nicely.</p>
<p>What is the practical benefit of declarative containers? Here's a concrete explanation for the Java app we discussed earlier.</p>
<p>In the world of Java, the application architecture has been designed to decouple between the application and the runtime. With the very strong specification of the JVM, the JVM for running applications is always swappable and replaceable. For example, if we start running an application with an OpenJDK, and we are not happy with its performance, we can safely swap the JVM to be the Zulu JVM or the IBM J9.</p>
<p>With the declarative approach to containers, the Java runtime would be easily swappable on-the-fly without rebuilding the Docker image. It also allows you to apply JVM hot fixes to the running system.</p>
<p>We can download the modified version of Docker with this declarative feature for Java from <a href="http://github.com/joconen/engine">http://github.com/joconen/engine</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>Now it's time to review all the concepts in this chapter:</p>
<ol>
<li>What do you think is lying ahead after the serverless era?</li>
<li>What is the next generation of computing you might probably be thinking of?</li>
<li>What is the feature of libcontainer that allows rootless execution?</li>
<li>What are the namespaces provided by Linux?</li>
<li>Explain why RunF is able to access the network services when running inside other containers.</li>
<li>What is the benefit of using LinuxKit to prepare an infrastructure.</li>
<li>What is a declarative approach to containers? How could it apply to other application platforms, beside Java?</li>
<li>How could we design a hybrid serverless architecture when we would like to access services from outside the organization?</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter ends this book with a discussion of what we could use to make the FaaS moving forward. We reviewed what we have been through with Docker and three major FaaS platforms running on it.</p>
<p>Docker is a great infrastructure when it's considered that all these three FaaS platforms are actually using its direct feature, rather than solely relying on its orchestrator functionalities. Why? Maybe because the FaaS computing model fits this simple kind of infrastructure instead of complex ones.</p>
<p>What if we could simply do <kbd>docker run</kbd>, then the container is transformed into a FaaS function serving its functionality somewhere on the cluster? Function wrapper, action proxy, or function watchdog could be injected into a simple container that processes input and output via standard I/O and turns it into an online function. Then a kind of magical infrastructure will be taking care of everything for us. We are gradually moving toward to that reality.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">References</h1>
                
            
            
                
<ol>
<li>Apache Foundation. <em>Apache OpenWhisk</em>. Available at: <a href="https://openwhisk.apache.org/.">https://openwhisk.apache.org/.</a> (Accessed: March 28, 2018).</li>
<li>Microsoft Corp. Azure functions—serverless architecture | Microsoft Azure. Available at: <a href="https://azure.microsoft.com/en-us/services/functions/.">https://azure.microsoft.com/en-us/services/functions/.</a> (Accessed: March 28, 2018).</li>
<li>Burns, B., Grant, B., Oppenheimer, D., Brewer, E. &amp; Wilkes, J. Borg, Omega, and Kubernetes. Queue 14, 10:70–10:93 (2016).</li>
<li>Schickling, J., Lüthy, M., Suchanek, T. &amp; et al. <em>chromeless: Chrome automation made simple.</em> (Graphcool, 2018).</li>
<li>Google Inc. Concepts | Cloud Functions. <em>Google Cloud</em> Available at: <a href="https://cloud.google.com/functions/docs/concepts.">https://cloud.google.com/functions/docs/concepts.</a> (Accessed: 28th March 2018).</li>
<li>Crosby, M., Day, S., Laventure, K.-M., McGowan, D. &amp; et al. <em>containerd: An open and reliable container runtime.</em> (containerd, 2018).</li>
<li>Docker Inc. Docker. (2018). Available at: <a href="https://www.docker.com/.">https://www.docker.com/.</a> (Accessed: 28th March 2018).</li>
<li>Smith, R. <em>Docker Orchestration</em>. (Packt Publishing Ltd, 2017).</li>
<li>Merkel, D. Docker: Lightweight Linux Containers for Consistent Development and Deployment. <em>Linux J.</em> <strong>2014</strong>, (2014).</li>
</ol>
<ol start="10">
<li>The Go Community. Documentation - The Go Programming Language. Available at: <a href="https://golang.org/doc/.">https://golang.org/doc/.</a> (Accessed: 30th March 2018).</li>
<li>The Linux Foundation. Envoy Proxy - Home. Available at: <a href="https://www.envoyproxy.io/.">https://www.envoyproxy.io/.</a> (Accessed: 1st April 2018).</li>
<li>The Ethereum Foundation. Ethereum Project. Available at: <a href="https://www.ethereum.org/.">https://www.ethereum.org/.</a> (Accessed: 30th March 2018).</li>
<li>Avram, A. FaaS, PaaS, and the Benefits of the Serverless Architecture. <em>Retrieved from’InfoQ’https://www. infoq. com/news/2016/06/faasserverless-architecture</em> on <strong>28</strong>, (2016).</li>
<li>Oracle Inc. Fn Project - The Container Native Serverless Framework. Available at: <a href="https://fnproject.io/.">https://fnproject.io/.</a> (Accessed: 28th March 2018).</li>
<li>Arimura, C., Reeder, T. &amp; et al. <em>Fn: The container native, cloud agnostic serverless platform.</em> (Oracle inc., 2018).</li>
<li>Google Inc. Google Cloud Functions Documentation | Cloud Functions. <em>Google Cloud</em> Available at: <a href="https://cloud.google.com/functions/docs/.">https://cloud.google.com/functions/docs/.</a> (Accessed: 28th March 2018). Kaewkasi, C. &amp; Chuenmuneewong, K. Improvement of container scheduling for docker using ant colony optimization. in <em>Knowledge and Smart Technology (KST), 2017 9th International Conference on</em> 254–259 (IEEE, 2017).</li>
<li>Apache Foundation. incubator-openwhisk: <em>Apache OpenWhisk is a serverless event-based programming service and an Apache Incubator project.</em> (The Apache Software Foundation, 2018).</li>
<li>Cormack, J. &amp; et al. linuxkit: <em>A toolkit for building secure, portable and lean operating systems for containers.</em> (LinuxKit, 2018).</li>
<li>Janakiraman, B. Martin Fowler’s bliki: Serverless. <em>martinfowler.com</em> (2016). Available at: <a href="https://martinfowler.com/bliki/Serverless.html.">https://martinfowler.com/bliki/Serverless.html.</a> (Accessed: 28th March 2018).</li>
<li>Sharma, S. <em>Mastering Microservices with Java 9</em>. (Packt Publishing Ltd, 2017).</li>
<li>Moby Community,The. Moby. <em>GitHub</em> Available at: <a href="https://github.com/moby.">https://github.com/moby.</a> (Accessed: 30th March 2018).</li>
<li>Moby Community,The. moby: <em>Moby Project</em> - a collaborative project for the container ecosystem to assemble container-based systems. (Moby, 2018).</li>
<li>Jones, D. E. &amp; et al. Moqui Ecosystem. Available at: <a href="https://www.moqui.org/.">https://www.moqui.org/.</a> (Accessed: 30th March 2018).</li>
<li>Soppelsa, F. &amp; Kaewkasi, C. <em>Native Docker Clustering with Swarm.</em> (Packt Publishing - ebooks Account, 2017).</li>
<li>Marmol, V., Jnagal, R. &amp; Hockin, T. Networking in containers and container clusters. <em>Proceedings of netdev 0.1, February</em> (2015).</li>
</ol>
<ol start="26">
<li>Ellis, A. <em>OpenFaaS - Serverless Functions Made Simple for Docker &amp; Kubernetes.</em> (OpenFaaS, 2018).</li>
<li>Amazon Web Services, Inc. Optimizing Enterprise Economics with Serverless Architectures. (2017).</li>
<li>Parse Community,The. Parse + Open Source. <em>Parse Open Source Hub</em> Available at: <a href="http://parseplatform.org/.">http://parseplatform.org/.</a> (Accessed: 30th March 2018).</li>
<li>Vilmart, F. &amp; et al. parse-server: <em>Parse-compatible API server module for Node/Express</em>. (Parse, 2018).</li>
<li>Linux Foundation,The. <em>runc: CLI tool for spawning and running containers according to the OCI specification</em>. (Open Container Initiative, 2018).</li>
<li>Christensen, B., Karnok, D. &amp; et al. <em>RxJava – Reactive Extensions for the JVM – a library for composing asynchronous and event-based programs using observable sequences for the Java VM</em>. (ReactiveX, 2018).</li>
<li>Roberts, M. Serverless Architectures. <em>martinfowler.com</em> (2016). Available at: <a href="https://martinfowler.com/articles/serverless.html.">https://martinfowler.com/articles/serverless.html.</a> (Accessed: 28th March 2018).</li>
<li>Baldini, I. et al. Serverless computing: Current trends and open problems. in <em>Research Advances in Cloud Computing</em> 1–20 (Springer, 2017).</li>
<li>GOTO Conferences. <em>Serverless: the Future of Software Architecture by Peter Sbarski.</em> (2017).</li>
<li>Fox, G. C., Ishakian, V., Muthusamy, V. &amp; Slominski, A. Status of Serverless Computing and Function-as-a-Service (FaaS) in Industry and Research. <em>arXiv preprint arXiv:1708.08028</em> (2017).</li>
<li>Docker Inc. Swarm mode overview. <em>Docker Documentation (2018)</em>. Available at: <a href="https://docs.docker.com/engine/swarm/.">https://docs.docker.com/engine/swarm/.</a> (Accessed: 28th March 2018).</li>
<li>Kaewkasi, C. &amp; et al. <em>The Docker Swarm 2000 Collaborative Project.</em> (SwarmZilla Collaborative Project, 2016).</li>
<li>Containous. Træfik. Available at: <a href="https://traefik.io/.">https://traefik.io/.</a> (Accessed: 1st April 2018).</li>
<li>Lubin, J. &amp; et al. Truffle Suite - Your Ethereum Swiss Army Knife. <em>Truffle Suite</em> Available at: <a href="http://truffleframework.com./">http://truffleframework.com.</a> (Accessed: 30th March 2018).</li>
<li>Weaveworks Inc. Weave Net: Network Containers Across Environments | Weaveworks. Available at: <a href="https://www.weave.works/oss/net/.">https://www.weave.works/oss/net/.</a> (Accessed:30th March 2018).</li>
</ol>


            

            
        
    </body></html>