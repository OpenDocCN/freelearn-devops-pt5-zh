- en: Chapter 8. Service Discovery – The Key to Distributed Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *It does not take much strength to do things, but it requires a great
    deal of strength to decide what to do.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Elbert Hubbard* |'
  prefs: []
  type: TYPE_TB
- en: 'The more services we have, the bigger the chance for a conflict to occur if
    we are using predefined ports. After all, there can be no two services listening
    on the same port. Managing an accurate list of all the ports used by, let''s say,
    a hundred services is a challenge in itself. Add to that list the databases those
    services need and the number grows even more. For that reason, we should deploy
    services without specifying ports and letting Docker assign random ones for us.
    The only problem is that we need to discover the port number and let others know
    about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Service Discovery – The Key to Distributed Services](img/B05848_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1 – Single node with services deployed as Docker containers
  prefs: []
  type: TYPE_NORMAL
- en: 'Things will get even more complicated later on when we start working on a distributed
    system with services deployed into one of the multiple servers. We can choose
    to define in advance which service goes to which server, but that would cause
    a lot of problems. We should try to utilize server resources as best we can, and
    that is hardly possible if we define in advance where to deploy each service.
    Another problem is that automatic scaling of services would be difficult at best,
    and not to mention automatic recuperation from, let''s say, server failure. On
    the other hand, if we deploy services to the server that has, for example, least
    number of containers running, we need to add the IP to the list of data needed
    to be discovered and stored somewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Service Discovery – The Key to Distributed Services](img/B05848_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2 – Multiple nodes with services deployed as Docker containers
  prefs: []
  type: TYPE_NORMAL
- en: There are many other examples of cases when we need to store and retrieve (discover)
    some information related to the services we are working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to locate our services, we need at least the following two processes
    to be available for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service registration** process that will store, as a minimum, the host and
    the port service is running on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Service discovery** process that will allow others to be able to discover
    the information we stored during the registration process:![Service Discovery
    – The Key to Distributed Services](img/B05848_08_03.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8-3 – Service registration and discovery
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides those processes, we need to consider several other aspects. Should we
    unregister the service if it stops working and deploy/register a new instance?
    What happens when there are multiple copies of the same service? How do we balance
    the load among them? What happens if a server goes down? Those and many other
    questions are tightly related to the registration and discovery processes and
    will be the subject of the next chapters. For now, we'll limit the scope only
    to the *service discovery* (the common name that envelops both processes mentioned
    above) and the tools we might use for such a task. Most of them feature highly
    available distributed key/value storage.
  prefs: []
  type: TYPE_NORMAL
- en: Service Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the service registry is simple. Provide capabilities to store service
    information, be fast, persistent, fault-tolerant, and so on. In its essence, service
    registry is a database with a very limited scope. While other databases might
    need to deal with a vast amount of data, service registry expects a relatively
    small data load. Due to the nature of the task, it should expose some API so that
    those in need of it's data can access it easily.
  prefs: []
  type: TYPE_NORMAL
- en: There's not much more to be said (until we start evaluating different tools)
    so we'll move on to service registration.
  prefs: []
  type: TYPE_NORMAL
- en: Service Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices tend to be very dynamic. They are created and destroyed, deployed
    to one server and then moved to another. They are always changing and evolving.
    Whenever there is any change in service properties, information about those changes
    needs to be stored in some database (we'll call it *service registry* or simply
    *registry*). The logic behind service registration is simple even though the implementation
    of that logic might become complicated. Whenever a service is deployed, its data
    (IP and port as a minimum) should be stored in the service registry. Things are
    a bit more complicated when a service is destroyed or stopped. If that is a result
    of a purposeful action, service data should be removed from the registry. However,
    there are cases when service is stopped due to a failure and in such a situation
    we might choose to do additional actions meant to restore the correct functioning
    of that service. We'll speak about such a situation in more details when we reach
    the self-healing chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few ways service registration can be performed.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Registration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Self-registration* is a common way to register service information. When a
    service is deployed it notifies the registry about its existence and sends its
    data. Since each service needs to be capable of sending its data to the registry,
    this can be considered an anti-pattern. By using this approach, we are breaking
    *single concern* and *bounded context* principles that we are trying to enforce
    inside our microservices. We''d need to add the registration code to each service
    and, therefore, increase the development complexity. More importantly, that would
    couple services to a specific registry service. Once their number increases, modifying
    all of them to, for example, change the registry would be a very cumbersome work.
    Besides, that was one of the reasons we moved away from monolithic applications;
    freedom to modify any service without affecting the whole system. The alternative
    would be to create a library that would do that for us and include it in each
    service. However, this approach would severally limit our ability to create entirely
    self-sufficient microservices. We''d increase their dependency on external resources
    (in this case the registration library).'
  prefs: []
  type: TYPE_NORMAL
- en: 'De-registration is, even more, problematic and can quickly become quite complicated
    with the self-registration concept. When a service is stopped purposely, it should
    be relatively easy to remove its data from the registry. However, services are
    not always stopped on purpose. They might fail in unexpected ways and the process
    they''re running in might stop. In such a case it might be difficult (if not impossible)
    to always be able to de-register the service from itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Registration](img/B05848_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4 – Self-registration
  prefs: []
  type: TYPE_NORMAL
- en: While self-registration might be common, it is not an optimum nor productive
    way to perform this type of operations. We should look at alternative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Registration Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Registration service or third party registration is a process that manages
    registration and de-registration of all services. The service is in charge of
    checking which microservices are running and should update the registry accordingly.
    A similar process is applied when services are stopped. The registration service
    should detect the absence of a microservice and remove its data from the registry.
    As an additional function, it can notify some other process of the absence of
    the microservice that would, in turn, perform some corrective actions like re-deployment
    of the absent microservice, email notifications, and so on. We''ll call this registration
    and de-registration process *service registrator* or simply *registrator* (actually,
    as you''ll soon see, there is a product with the same name):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Registration Service](img/B05848_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5 – Registration service
  prefs: []
  type: TYPE_NORMAL
- en: A separate registration service is a much better option than self-registration.
    It tends to be more reliable and, at the same time, does not introduce unnecessary
    coupling inside our microservices code.
  prefs: []
  type: TYPE_NORMAL
- en: Since we established what will be the underlying logic behind the services registration
    process, it is time to discuss the discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Service discovery is the opposite of service registration. When a client wants
    to access a service (the client might also be another service), it must know,
    as a minimum, where that service is. One approach we can take is self-discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Discovery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-discovery uses the same principles as self-registration. Every client or
    a service that wants to access other services would need to consult the registry.
    Unlike self-registration that posed problems mostly related to our internal ways
    to connect services, self-discovery might be used by clients and services outside
    our control. One example would be a front-end running in user browsers. That front-end
    might need to send requests to many separate back-end services running on different
    ports or even different IPs. The fact that we do have the information stored in
    the registry does not mean that others can, should, or know how to use it. Self-discovery
    can be effectively used only for the communication between internal services.
    Even such a limited scope poses a lot of additional problems many of which are
    the same as those created by self-registration. Due to what we know by now, this
    option should be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Proxy Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proxy services have been around for a while and proved their worth many times
    over. The next chapter will explore them in more depth so we'll go through them
    only briefly. The idea is that each service should be accessible through one or
    more fixed addresses. For example, the list of books from our `books-ms` service
    should be available only through the `[DOMAIN]/api/v1/books` address. Notice that
    there is no IP, port nor any other deployment-specific detail. Since there will
    be no service with that exact address, something will have to detect such a request
    and redirect it to the IP and port of the actual service. Proxy services tend
    to be the best type of tools that can fulfill this task.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a general, and hopefully clear, idea of what we're trying to
    accomplish, let's take a look at some of the tools that can help us out.
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary objective of *service discovery tools* is to help services find
    and talk to one another. To perform their duty, they need to know where each service
    is. The concept is not new, and many tools existed long before Docker was born.
    However, containers brought the need for such tools to a whole new level.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind *service discovery* is for each new instance of a service
    (or application) to be able to identify its current environment and store that
    information. Storage itself is performed in a registry usually in key/value format.
    Since the discovery is often used in distributed system, registry needs to be
    scalable, fault-tolerant and distributed among all nodes in the cluster. The primary
    usage of such a storage is to provide, as a minimum, IP and port of a service
    to all interested parties that might need to communicate with it. This data is
    often extended with other types of information.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery tools tend to provide some API that can be used by a service to register
    itself as well as by others to find the information about that service.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we have two services. One is a provider, and the other one is
    its consumer. Once we deploy the provider, we need to store its information in
    the *service registry* of choice. Later on, when the consumer tries to access
    the provider, it would first query the registry and call the provider using the
    IP and port obtained from the registry. To decouple the consumer from a particular
    implementation of the registry, we often employ some *proxy service*. That way
    the consumer would always request information from the fixed address that would
    reside inside the proxy that, in turn, would use the discovery service to find
    out the provider information and redirect the request. Actually, in many cases,
    there is no need for the proxy to query the service registry if there is a process
    that updates its configuration every time data in the registry changes. We'll
    go through *reverse proxy* later on in the book. For now, it is important to understand
    that the flow that is based on three actors; consumer, proxy, and provider.
  prefs: []
  type: TYPE_NORMAL
- en: What we are looking for in the service discovery tools is data. As a minimum,
    we should be able to find out where the service is, whether it is healthy and
    available, and what is its configuration. Since we are building a distributed
    system with multiple servers, the tool needs to be robust, and failure of one
    node should not jeopardize data. Also, each of the nodes should have the same
    data replica. Further on, we want to be able to start services in any order, be
    able to destroy them, or to replace them with newer versions. We should also be
    able to reconfigure our services and see the data change accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at a few of the tools we can use to accomplish the goals we
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Manual Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the services are still managed manually. We decide in advance where
    to deploy the service, what is its configuration and hope beyond reason that it
    will continue working properly until the end of days. Such approach is not easily
    scalable. Deploying a second instance of the service means that we need to start
    the manual process all over. We have to bring up a new server or find out which
    one has low utilization of resources, create a new set of configurations and deploy
    it. The situation is even more complicated in the case of, let's say, a hardware
    failure since the reaction time is usually slow when things are managed manually.
    Visibility is another sore point. We know what the static configuration is. After
    all, we prepared it in advance. However, most of the services have a lot of information
    generated dynamically. That information is not easily visible. There is no single
    location we can consult when we are in need of that data.
  prefs: []
  type: TYPE_NORMAL
- en: Reaction time is inevitably slow, failure resilience questionable at best and
    monitoring difficult to manage due to a lot of manually handled moving parts.
  prefs: []
  type: TYPE_NORMAL
- en: While there was an excuse to do this job manually in the past or when the number
    of services and/or servers is small, with the emergence of service discovery tools,
    this excuse quickly evaporated.
  prefs: []
  type: TYPE_NORMAL
- en: Zookeeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zookeeper is one of the oldest projects of this type. It originated out of the
    Hadoop world, where it was built to help the maintenance of various components
    in a Hadoop cluster. It is mature, reliable and used by many big companies (YouTube,
    eBay, Yahoo, and so on). The format of the data it stores is similar to the organization
    of the file system. If run on a server cluster, Zookeeper will share the state
    of the configuration across all of the nodes. Each cluster elects a leader and
    clients can connect to any of the servers to retrieve data.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantages Zookeeper brings to the table is its maturity, robustness,
    and feature richness. However, it comes with its set of disadvantages, with Java
    and complexity being main culprits. While Java is great for many use cases, it
    is massive for this type of work. Zookeeper's usage of Java, together with a considerable
    number of dependencies, makes Zookeeper much more resource hungry that its competition.
    On top of those problems, Zookeeper is complex. Maintaining it requires considerably
    more knowledge than we should expect from an application of this type. That is
    the part where feature richness converts itself from an advantage to a liability.
    The more features an application has, the bigger the chances that we won't need
    all of them. Thus, we end up paying the price in the form of complexity for something
    we do not fully need.
  prefs: []
  type: TYPE_NORMAL
- en: Zookeeper paved the way that others followed with considerable improvements.
    "Big players" are using it because there were no better alternatives at the time.
    Today, Zookeeper shows its age, and we are better off with alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: We'll skip Zookeeper examples and skip straight into better options.
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: etcd is a key/value store accessible through HTTP. It is distributed and features
    hierarchical configuration system that can be used to build service discovery.
    It is very easy to deploy, setup and use, provides reliable data persistence,
    it's secure and with excellent documentation.
  prefs: []
  type: TYPE_NORMAL
- en: etcd is a better option than Zookeeper due to its simplicity. However, it needs
    to be combined with a few third-party tools before it can serve service discovery
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up etcd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us set up the *etcd*. First, we should create the first node in the cluster
    (*serv-disc-01*) together with the, already familiar, *cd* VM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With the cluster node *serv-disc-01* up and running, we can install `etcd` and
    `etcdctl` (etcd command line client).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We downloaded, uncompressed and moved the executables to `/usr/local/bin` so
    that they are easily accessible. Then, we removed unneeded files and, finally,
    run the `etcd` with output redirected to `/tmp/etcd.log`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what we can do with etcd.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic operations are `set` and `get`. Please note that we can set a key/value
    inside a directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first command put the key `port` with the value `1234` into the directory
    `myService`. The second did the same with the key `ip`, and the last two commands
    were used to output values of those two keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also list all the keys in the specified directory or delete a key with
    its value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The last command output only the `/myService/ip` value since previous command
    removed the port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides `etcdctl`, we can also run all commands through HTTP API. Before we
    try it out, let''s install `jq` so that we can see the formatted output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can, for example, put a value into `etcd` through its HTTP API and retrieve
    it through a `GET` request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `jq ''.''` is not required, but I tend to use it often to format JSON.
    The output should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: HTTP API is especially useful when we need to query etcd remotely. In most,
    I prefer the `etcdctl`, when running ad-hoc commands while HTTP is a preferred
    way to interact with `etcd` through some code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve seen (briefly) how etcd works on a single server, let us try
    it inside a cluster. The cluster setup requires a few additional arguments to
    be passed to `etcd`. Let''s say that we''ll have a cluster of three nodes with
    IPs `10.100.197.201` (`serv-disc-01`), `10.100.197.202` (`serv-disc-02`) and `10.100.197.203`
    (`serv-disc-03`). The etcd command that should be run on the first server would
    be the following (please don''t run it yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: I extracted parts that would change from one server (or a cluster) to another
    into variables so that you can see them clearly. We won't go into details of what
    each argument means. You can find more information in the [https://coreos.com/etcd/docs/latest/clustering.html](https://coreos.com/etcd/docs/latest/clustering.html).
    Suffice to say that we specified the IP and the name of the server where this
    command should run as well as the list of all the servers in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start working on the `etcd` deployment to the cluster, let us kill
    the currently running instance and create the rest of servers (there should be
    three in total):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Doing the same set of tasks manually across multiple servers is tedious and
    error prone. Since we already worked with Ansible, we can use it to set up etcd
    across the cluster. This should be a fairly easy task since we already have all
    the commands, and all we have to do is translate those we already run into the
    Ansible format. We can create the `etcd` role and add it to the playbook with
    the same name. The role is fairly simple. It copies the executables to the `/usr/local/bin`
    directory and runs etcd with the cluster arguments (the very long command we examined
    above). Let us take a look at it before running the playbook.
  prefs: []
  type: TYPE_NORMAL
- en: The first task in the `roles/etcd/tasks/main.yml` is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The name is purely descriptive and followed with the copy module. Then, we
    are specifying few of the module options. The copy option `src` indicates the
    name of the local file we want to copy and is relative to the `files` directory
    inside the role. The second copy option (`dest`) is the destination path on the
    remote server. Finally, we are setting the mode to be `755`. The user that runs
    with roles will have `read/write/execute` permissions, and those belonging to
    the same group and everyone else will be assigned `read/execute` permissions.
    Next is the `with_items` declaration that allows us to use a list of values. In
    this case, the values are specified in the `roles/etcd/defaults/main.yml` file
    and are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Externalizing variables is a good way to keep things that might change in the
    future separated from the tasks. If, for example, we are to copy another file
    through this role, we'd add it here and avoid even opening the tasks file. The
    task that uses the `files` variable will iterate for each value in the list and,
    in this case, run twice; once for `etcd` and the second time for `etcdctl`. Values
    from variables are represented with the variable key surrounded with `{{` and
    `}}` and use the Jinja2 format. Finally, we set `etcd` to be the tag associated
    with this task. Tags can be used to filter tasks when running playbooks and are
    very handy when we want to run only a subset of them or when we want to exclude
    something.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Shell module is often the last resort since it does not work with states. In
    most cases, commands run as through shell will not check whether something is
    in the correct state or not and run every time we execute Ansible playbook. However,
    etcd always runs only a single instance and there is no risk that multiple executions
    of this command will produce multiple instances. we have a lot of arguments and
    all those that might change are put as variables. Some of them, like ansible_hostname,
    are discovered by Ansible. Others were defined by us and placed in the `roles/etcd/defaults/main.yml`.
    With all the tasks defined, we can take a look at the playbook `etcd.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When this playbook is run, Ansible will configure all the servers defined in
    an inventory, use `vagrant` as the remote user, run commands as `sudo` and execute
    the `common` and `etcd` roles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at the `hosts/serv-disc` file. It is our inventory that
    contains the list of all hosts we''re using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, you can a different way to define hosts. The second line is
    Ansible's way of saying that all addresses between `10.100.194.201` and `10.100.194.203`
    should be used. In total, we have three IPs specified for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `etcd` playbook and see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check whether etcd cluster was correctly set by putting a value through
    one server and getting it from the another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of those commands should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We sent the HTTP PUT request to the `serv-disc-01` server (`10.100.197.201`)
    and retrieved the stored value through the HTTP GET request from the `serv-disc-03`
    (`10.100.197.203`) node. In other words, data set through any of the servers in
    the cluster is available in all of them. Isn't that neat?
  prefs: []
  type: TYPE_NORMAL
- en: Our cluster (after we deploy few containers), would look as presented in the
    Figure 8-6.
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up etcd](img/B05848_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6 – Multiple nodes with Docker containers and etcd
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a place to store the information related to our services, we
    need a tool that will send that information to etcd automatically. After all,
    why would we put data to etcd manually if that can be done automatically? Even
    if we would want to put the information manually to etcd, we often don't know
    what that information is. Remember, services might be deployed to a server with
    least containers running and it might have a random port assigned. Ideally, that
    tool should monitor Docker on all nodes and update etcd whenever a new container
    is run, or an existing one is stopped. One of the tools that can help us with
    this goal is *Registrator*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Registrator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Registrator automatically registers and deregisters services by inspecting containers
    as they are brought online or stopped. It currently supports **etcd**, **Consul**
    and **SkyDNS 2**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up Registrator with etcd registry is easy. We can simply run the Docker
    container as follows (please do not run it yourself):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With this command we are sharing `/var/run/docker.sock` as Docker volume. Registrator
    will monitor and intercept Docker events and, depending on the event type, put
    or remove service information to/from etcd. With the `-h` argument we are specifying
    the hostname. Finally, we are passing two arguments to Registrator. The first
    one is the `-ip` and represents the IP of the host and the second one is the protocol
    (`etcd`), the IP (`serv-disc-01`) and the port (`2379`) of the registration service.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, let's create a new Ansible role called *registrator* and
    deploy it to all nodes inside the cluster. The `roles/registrator/tasks/main.yml`
    file is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This Ansible role is equivalent to the manual command we saw earlier. Please
    note that we changed the hard-coded `etcd` protocol with a variable. That way
    we can reuse this role with other registries as well. Keep in mind that having
    quotes is not mandatory in Ansible except when value starts with `{{` as in the
    case of the `hos` `tname` value.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the `registrator-etcd.yml` playbook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Most of the playbook is similar to those we used before except the `vars` key.
    In this case, we're using it to define the Registrator protocol as `etcd` and
    port of the registry as `2379`.
  prefs: []
  type: TYPE_NORMAL
- en: With everything in place, we can run the playbook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Once the playbook is finished executing, Registrator will be running on all
    three nodes of our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give Registrator a spin and run one container inside one of the three
    cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We exported the `DOCKER_HOST` variable so that Docker commands are sent to the
    cluster node 2 (`serv-disc-02`) and run the `nginx` container exposing port `1234`.
    We'll use `nginx` later on, and there will be plenty of opportunities to get familiar
    with it. For now, we are not interested in what nginx does, but that Registrator
    detected it and stored the information in etcd. In this case, we put a few environment
    variables (`SERVICE_NAME` and `SERVICE_ID`) that Registrator can use to identify
    better the service.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a look at Registrator's log.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can see that Registrator detected nginx container with the ID `5cf7dd974939`.
    We can also see that it ignored the port `443`. The `nginx` container internally
    exposes ports 80 and `443`. However, we exposed only `80` to the outside world,
    so Registrator decided to ignore the port `443`. After all, why would we store
    the information about the port not accessible to anyone?
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us take a look at data stored in etcd:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the last command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The first command listed all keys at the root, the second listed all those inside
    `nginx-80` and the last one retrieved the final value. Registrator stored values
    in the format `/` that matches environment variables we used when running the
    container. Please note that in case more that one port is defined for a service,
    Registrator adds it as a suffix (e.g. nginx-`80`). The value that Registrator
    put corresponds with the IP of the host where the container is running and the
    port that we exposed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that even though the container is run on the node 2, we queried
    etcd running on the node 1\. It was yet another demonstration that data is replicated
    across all nodes etcd is running on.
  prefs: []
  type: TYPE_NORMAL
- en: What happens when we remove the container?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of Registrator logs should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Registrator detected that we removed the container and sent a request to etcd
    to remove corresponding values. We can confirm that with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The service with the ID `nginx/nginx` disappeared.
  prefs: []
  type: TYPE_NORMAL
- en: Registrator combined with `etcd` is a powerful, yet simple, combination that
    will allow us to practice many advanced techniques. Whenever we bring up a container,
    data will be stored in etcd and propagated to all nodes in the cluster. What we'll
    do with that information will be the subject of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Registrator](img/B05848_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7 – Multiple nodes with Docker containers, etcd and Registrator
  prefs: []
  type: TYPE_NORMAL
- en: There is one more piece of the puzzle missing. We need a way to create configuration
    files with data stored in `etcd` as well as run some commands when those files
    are created.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up confd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The confd is a lightweight tool that can be used to maintain configuration files.
    The most common usage of the tool is keeping configuration files up-to-date using
    data stored in `etcd`, `consul`, and few other data registries. It can also be
    used to reload applications when configuration files change. In other words, we
    can use it as a way to reconfigure services with the information stored in etcd
    (or few other registries).
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing `confd` is straightforward. The commands are as follows (please
    don''t run them yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In order for `confd` to work, we need a configuration file located in the `/etc/confd/conf.d/`
    directory and a template in the `/etc/confd/templates`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example configuration file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As a minimum, we need to specify template source, destination file, and keys
    that will be fetched from the registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Templates use GoLang text templates format. An example template is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: When this template is processed, it will substitute `{{getv "/nginx/nginx"}}`
    with the value from the registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, `confd` can be run in two modes. In the Daemon mode, it polls a registry
    and updates destination configuration whenever relevant values change. The `onetime`
    mode is run once. An example of the `onetime` mode is as follows (please do not
    run it yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This command would run in the `onetime` mode, would use `etcd` as the backend
    running on the specified node. When executed, destination configuration would
    be updated with values from the `etcd` registry.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know basics of how confd works, let's take a look at the Ansible
    role `confd` that will make sure that it is installed on all servers in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `roles/confd/tasks/main.yml` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This Ansible role is even simpler than the one we created for `etcd` since
    we are not even running the binary. It makes sure that directories are created
    and that files are copied to the destination servers. Since there are multiple
    directories and files involved, we defined them as variables in the `roles/confd/defaults/main.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We defined directories where we'll put configurations and templates. We also
    defined files that need to be copied; one binary, one configuration, and one template
    file that we'll use to try out confd.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need `confd.yml` file that will act as the Ansible playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: There's nothing new to discuss since this file is almost the same the other
    playbooks we worked with.
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything set up, we can deploy confd to all the cluster servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With `confd` installed on all nodes in the cluster, we can try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the nginx container again so that Registrator can put some data
    to etcd:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the nginx container on the `serv-disc-01` node and exposed the port
    `4321`. Since *Registrator* is already running on that server, it put data to
    `etcd`. Finally, we run the local instance of `confd` that checked all its configuration
    files and compared keys with those stored in etcd. Since `nginx/nginx` key has
    been changed in etcd, it processed the template and updated the destination config.
    That can be seen from the output that should be similar to the following (timestamp
    has been removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'It found that the `/tmp/example.conf` is out of sync and updated it. Let us
    confirm that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'If any of the changes in templates or `etcd` data is updated, running `confd`
    will make sure that all destination configurations are updated accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up confd](img/B05848_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8 – Multiple nodes with Docker containers, etcd, Registrator and confd
  prefs: []
  type: TYPE_NORMAL
- en: Combining etcd, Registrator, and confd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When etcd, Registrator, and confd are combined, we get a simple yet powerful
    way to automate all our service discovery and configuration needs. That will come
    in handy when we start working on more advanced deployment strategies. The combination
    also demonstrates the effectiveness of having the right mix of small tools. Those
    three do what we need them to do. Less than this and we would not be able to accomplish
    the goals set in front of us. If, on the other hand, they were designed with bigger
    scope in mind, we would introduce unnecessary complexity and overhead on server
    resources and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Before we make the final verdict, let's take a look at another combination of
    tools with similar goals. After all, we should never settle for some solution
    without investigating alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Consul
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consul is strongly consistent datastore that uses gossip to form dynamic clusters.
    It features hierarchical key/value store that can be used not only to store data
    but also to register watches that can be used for a variety of tasks, from sending
    notifications about data changes, to running health checks and custom commands
    depending on their output.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Zookeeper and etcd, Consul implements service discovery system embedded,
    so there is no need to build your own or use a third-party one. This discovery
    includes, among other things, health checks of nodes and services running on top
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: ZooKeeper and etcd provide only a primitive K/V store and require that application
    developers build their own system to provide service discovery. Consul, on the
    other hand, provides a built-in framework for service discovery. Clients only
    need to register services and perform discovery using the DNS or HTTP interface.
    The other two tools require either a hand-made solution or the usage of third-party
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Consul offers out of the box native support for multiple data centers and the
    gossip system that works not only with nodes in the same cluster but across data
    centers as well.
  prefs: []
  type: TYPE_NORMAL
- en: Consul has another nice feature that distinguishes it from the others. Not only
    that it can be used to discover information about deployed services and nodes
    they reside on, but it also provides easy to extend health checks through HTTP
    and TCP requests, TTLs (time-to-live), custom scripts and even Docker commands.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Consul
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As before, we''ll start by exploring manual installation commands and, later
    on, automate them with Ansible. We''ll configure it on the `cd` node as an exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We started by installing `unzip` since it is not included in default Ubuntu
    distribution. Then we downloaded the Consul ZIP, unpacked it, moved it to the
    `/usr/local/bin` directory, removed the ZIP file since we won't need it anymore
    and, finally, created few directories. Consul will place its information to the
    `data` directory and configuration files into `config`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we can run `consul`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Running Consul was very straight forward. We specified that it should run the
    `agent` as a `server` and that there will be only one server instance `(-bootstrap-expect
    1`). That is followed by locations of key directories; `ui`, `data` and `config`.
    Then we specified the name of the `node`, address it will `bind` to and which
    `client` can connect to it (`0.0.0.0` refers to all). Finally, we redirected the
    output and made sure that it's running in the background (`&`).
  prefs: []
  type: TYPE_NORMAL
- en: Let's verify that Consul started correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The output of the log file should be similar to the following (timestamps are
    removed for brevity).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the Consul agent we run in server mode elected itself as the
    leader (which is to be expected since it's the only one).
  prefs: []
  type: TYPE_NORMAL
- en: With Consul up and running, let's see how we can put some data into it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The first command created the `msg1` key with the value `this is a test`. The
    second had nested the key `msg2` into a parent key `messages`. Finally, the last
    command added the `flag` with the value `1234`. Flags can be used to store version
    number or any other information that can be expressed as an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look how to retrieve the information we just stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the command is as follows (order is not guaranteed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Since we used the `recurse` query, keys were returned from the root recursively.
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see all the keys we inserted. However, the value is base64 encoded.
    Consul can store more than text and, in fact, it stores everything as binary under
    the hood. Since not everything can be represented as text, you can store anything
    in Consul's K/V, but there are size limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also retrieve a single key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the same as before but limited to the key `msg1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can request only the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we put the `raw` query parameter and the result is only the value
    of the requested key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might have guessed, Consul keys can easily be deleted. The command to,
    for example, delete the `messages/msg2` key is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also delete recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The Consul agent we deployed was set up to be the server. However, most agents
    do not need to run in the server mode. Depending on the number of nodes, we might
    opt for three Consul agents running in the server mode and many non-server agents
    joining it. If, on the other hand, the number of nodes is indeed big, we might
    increase the number of agents running in the server mode to five. If only one
    server is running, there will be data loss in case of its failure. In our case,
    since the cluster consists of only three nodes and this is a demo environment,
    one Consul agent running in the server mode is more than enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to run an agent on the `serv-disc-02` node and make it join the
    cluster is as follows (please don''t run it yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The only difference we did when compared with the previous execution is the
    removal of arguments `-server` and `-bootstrap-expect 1`. However, running Consul
    in one of the cluster servers is not enough. We need to join it with the Consul
    agent running on the other server. The command to accomplish that is as follows
    (please don't run it yet).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The effect of running this command is that agents of both servers would be clustered
    and data synchronized between them. If we continued adding Consul agents to other
    servers and joining them, the effect would be an increased number of cluster nodes
    registered in Consul. There is no need to join more than one agent since Consul
    uses a gossip protocol to manage membership and broadcast messages to the cluster.
    That is one of the useful improvements when compared to `etcd` that requires us
    to specify the list of all servers in the cluster. Managing such a list tends
    to be more complicated when the number of servers increases. With the gossip protocol,
    Consul is capable of discovering nodes in the cluster without us telling it where
    they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Consul basics covered, let''s see how we can automate its configuration
    across all servers in the cluster. Since we are already committed to Ansible,
    we''ll create a new role for Consul. While the configuration we''re about to explore
    is very similar to those we did by now, there are few new details we have not
    yet seen.tasks from the Ansible role `roles/consul/tasks/main.yml` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We started by creating directories and copying files. Both tasks use variables
    array specified in the `with_items` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at those variables. They are defined in the `roles/consul/defaults/main.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though we could specify all those variables inside the `roles/consul/tasks/main.yml`
    file, having them separated allows us to change their values more easily. In this
    case, have a simple list of directories and the list of files in JSON format with
    source, destination and mode.tinue with the tasks in the `roles/consul/tasks/main.yml`.
    The third one is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Since Consul makes sure that there is only one process running at the time,
    there is no danger running this task multiple times. It is equivalent to the command
    we run manually with an addition of a few variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember the manual execution of Consul, one node should run Consul
    in the server node and the rest should join at least one node so that Consul can
    gossip that information to the whole cluster. We defined those differences as
    the (`consul_extra`) variable. Unlike those we used before that are defined in
    `roles/consul/defaults/main.y` `ml` file inside the role, `consul_extra` is defined
    in the `hosts/serv-disc` inventory file. Let''s take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We defined variables to the right of the server IPs. In this case, the `.201`
    is acting as a server. The rest is defining the `consul_server_ip` variables that
    we'll discuss very soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s jump into the fourth (and last) task defined in the `roles/consul/tasks/main.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This task makes sure that every Consul agent, except the one running in the
    server mode, joins the cluster. The task runs the same command like the one we
    executed manually, with the addition of the `consul_server_ip` variable that has
    a double usage. The first usage is to provide value for the `shell` command. The
    second usage is to decide whether this task is run at all. We accomplished that
    using the `when: consu` `l_server_i` `p is defined` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have the `consul.yml` playbook, that is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: There's not much to say about it since it follows the same structure as the
    playbooks we used before.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the playbook, let us execute it and take a look at Consul nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We can confirm whether Consul is indeed running on all nodes by sending the
    *nodes* request to one of its agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The output of the command is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: All three nodes in the cluster are now running Consul. With that out of the
    way, we can move back to Registrator and see how it behaves when combined with
    Consul.
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul](img/B05848_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9 – Multiple nodes with Docker containers and Consul
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Registrator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Registrator has two Consul protocols. We''ll take a look at `consulkv` first
    since its results should be very similar to those obtained with the etcd protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the Registrator log and check whether everything seems
    to be working correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following (timestamps were removed for
    brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the same as when we run Registrator with the etcd protocol. It
    found the nginx container running (the one that we started previously while practicing
    `etcd`) and published the exposed port `4321` to Consul. We can confirm that by
    querying Consul:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the output is the IP and the port exposed through the nginx container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: However, Registrator has another protocol called `consul` (the one we just used
    is `consulkv`) that utilizes Consul's format for storing service information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what information Registrator sent to Consul this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the data is a bit more complete yet still in a very simple format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the IP and the port that is normally stored with `etcd` or `consulkv`
    protocols, this time, we got more information. We know the node the service is
    running on, service ID and the name. We can do even better than that with few
    additional environment variables. Let''s bring up another nginx container and
    see the data stored in Consul:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The output of the last command is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The second container (`nginx2`) was registered and, this time, Consul got tags
    that we might find useful later on. Since both containers are listed under the
    same name Consul considers them to be two instances of the same service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how Registrator works in conjunction with Consul, let''s configure
    it in all nodes of the cluster. The good news is that the role is already created,
    and we set the protocol to be defined with the variable `protocol`. We also put
    the name of the container as the `registrator_name` variable so that we can bring
    the Registrator container with the consul protocol without getting in conflict
    with the etcd one we configured earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: The playbook `registrator.yml` is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The `registrator-etcd.yml` has the `registrator_protocol` variable set to `etcd`
    and `registrator_port` to `2379`. We didn''t need it in this case since we already
    had default values set to `consul` and `8500` in the `roles/registrator/defaults/main.yml`
    file. On the other hand, we did overwrite the default value of the `registrator_name`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything ready, we can run the playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the execution of this playbook is finished, Registrator with the consul
    protocol will be configured on all nodes in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Registrator](img/B05848_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10 – Multiple nodes with Docker containers, Consul and Registrator
  prefs: []
  type: TYPE_NORMAL
- en: How about templating? Should we use confd or something else?
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Consul Template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use confd with Consul in the same way as we used it with etcd. However,
    Consul has its own templating service with features more in line with what Consul
    offers.
  prefs: []
  type: TYPE_NORMAL
- en: Consul Template is a very convenient way to create files with values obtained
    from Consul. As a bonus, it can also run arbitrary commands after the files have
    been updated. Just as confd, Consul Template also uses Go Template format.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you're probably accustomed to the routine. First we'll try Consul Template
    manually. As with all other tools, we set up in this chapter, installation consists
    of downloading the release, unpacking it and making sure that the executable is
    in the system path.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'With Consul Template available on the node, we should create one template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: When this template is processed, it will iterate (`range`) over all services
    with the name `nginx-80`. Each iteration will produce the text with service `Address`
    and `Port`. Template has been created as `/tmp/nginx.ctmpl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we run the Consul Template, let''s take another look at what we have
    stored in Consul for the nginx services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two nginx services up and running and registered in Consul. Let''s
    see the result of applying the template we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the second command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: The Consul Template command we executed found both services and generated the
    output in the format we specified. We specified that it should run only once.
    The alternative is to run it in daemon mode. In such a case, it would monitor
    the registry for changes and apply them to specified configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: We will go into details of how Consul Template works later on when we start
    using it in our deployment pipeline. Until then, please consult [https://www.consul.io/docs/](https://www.consul.io/docs/)
    yourself. For now, it is important to understand that it can obtain any information
    stored in Consul and apply it to the template we specify. Besides creating the
    file, it can also run custom commands. That will come in handy with `reverse proxy`,
    that is the subject of our next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We didn't try Consul Template applied to Consul's key/value format. In that
    combination, there is no significant difference when compared to confd.
  prefs: []
  type: TYPE_NORMAL
- en: The major downside Consul Template has is its tight coupling with Consul. Unlike
    confd that can be used with many different registries, Consul Template is created
    as a templating engine tightly integrated with Consul. That is, at the same time,
    an advantage, since it understands Consul's service format. If you choose to use
    Consul, Consul Template is a great fit.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next subject, let's create Consul Template role and
    configure it on all nodes. The `roles/consul-template/tasks/main.yml` file is
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s nothing exciting with this role. It''s probably the simplest one we
    did by now. The same holds true for the `consul-template.yml` playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'And, finally, we can configure it on all nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The end result is very similar to the etcd/Registrator combination with the
    difference in data format sent to Consul:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Template](img/B05848_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11 – Multiple nodes with Docker containers, Consul, Registrator and
    Consul Template
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we covered Consul's features that are, somewhat, similar to
    the etcd/registrator/confd combination. It's time to take a look at the characteristics
    that make Consul indeed stand up from the crowd.
  prefs: []
  type: TYPE_NORMAL
- en: Consul Health Checks, Web UI, and Data Centers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring health of cluster nodes and services is as important as testing and
    deployment itself. While we should aim towards having stable environments that
    never fail, we should also acknowledge that unexpected failures happen and be
    prepared to act accordingly. We can, for example, monitor memory usage and, if
    it reaches a certain threshold, move some services to a different node in the
    cluster. That would be an example of preventive actions performed before the "disaster"
    would happen. On the other hand, not all potential failures can be detected in
    time for us to act on time. A single service can fail. A whole node can stop working
    due to a hardware failure. In such cases, we should be prepared to act as fast
    as possible by, for example, replacing a node with a new one and moving failed
    services. We won't go into details how Consul can help us in this task since there
    is a whole chapter dedicated to `self-healing systems` and Consul will play a
    major role in it. For now, suffice to say that Consul has a simple, elegant and,
    yet, powerful way to perform health checks that can help us define what actions
    should be performed when health thresholds are reached.
  prefs: []
  type: TYPE_NORMAL
- en: If you googled `etcd ui` or `etcd dashboard` you probably saw that there are
    a few solutions available, and you might be asking why we haven't presented them.
    The reason is simple; etcd is a key/value store and not much more. Having a UI
    to present data is not of much use since we can easily obtain it through the etcdctl.
    That does not mean that etcd UI is of no use but that it does not make much difference
    due to its limited scope.
  prefs: []
  type: TYPE_NORMAL
- en: Consul is much more than a simple key/value store. As we've already seen, besides
    storing key/value pairs, it has a notion of a service together with data that
    belong to it. It can also perform health checks, thus becoming a good candidate
    for a dashboard that can be used to see the status of our nodes and services running
    on top of them. Finally, it understands the concept of multiple data centers.
    All those features combined, let us see the need for a dashboard in a different
    light.
  prefs: []
  type: TYPE_NORMAL
- en: With the Consul Web UI, we can view all services and nodes, monitor health checks
    and their statuses, read and set key/value data as well as switch from one data
    center to another. To see it in action, please open `http://10.100.194.201:8500/ui`
    in your favorite browser. You'll see items in the top menu that correspond to
    the steps we performed earlier through the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Services` menu item lists all the services we registered. There''s not
    much at the moment since only Consul server, Docker UI and two instances of the
    nginx service are up and running. We can filter them by name or status and see
    details by clicking on one of the registered services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Consul Health Checks, Web UI, and Data Centers](img/B05848_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12 – Consul Web UI services
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes show us the list of all nodes belonging to the selected data center.
    In our case, we have three nodes. The first one has three registered services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Consul Health Checks, Web UI, and Data Centers](img/B05848_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13 – Consul Web UI nodes
  prefs: []
  type: TYPE_NORMAL
- en: The `Key/Value` screen can be used to both display and modify data. In it, you
    can see data put to Consul by the Registrator instance set to use `consulkv` as
    the protocol. Please feel free to add data yourself and see how they are visualized
    in the UI. Besides working with Consul key/value data with the API we used before,
    you can also manage them through the UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![Consul Health Checks, Web UI, and Data Centers](img/B05848_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14 – Consul Web UI key/value
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that Consul allows us to group nodes into data centers. We haven't
    used this feature since we are running only three nodes. When nodes in the cluster
    start increasing, splitting them into data centers is often a good idea and Consul
    helps us to visualize them through its UI.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Consul, Registrator, Template, Health Checks and WEB UI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consul, together with the tools we explored, is in many cases a better solution
    than what etcd offers. It was designed with services architecture and discovery
    in mind. It is simple, yet powerful. It provides a complete solution without sacrificing
    simplicity and, in many cases, it is the best tool for service discovery and health
    checking needs (at least among those we evaluated).
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery Tools Compared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the tools are based on similar principles and architecture. They run
    on nodes, require a quorum to operate and are strongly consistent. They all provide
    some form of key/value storage.
  prefs: []
  type: TYPE_NORMAL
- en: Zookeeper is the oldest of the three, and the age shows in its complexity, utilization
    of resources and goals it's trying to accomplish. It was designed in a different
    age than the rest of the tools we evaluated (even though it's not much older).
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` with Registrator and `confd` is a very simple, yet very powerful combination
    that can solve most, if not all, of our service discovery needs. It showcases
    the power we can obtain when we combine simple and very specific tools. Each of
    them performs a very specific task, communicates through well-established API
    and is capable of working with relative autonomy. They are `microservices` both
    in their architectural as well as their functional approach.'
  prefs: []
  type: TYPE_NORMAL
- en: What distinguishes `Consul` is the support for multiple data centers and health
    checking without the usage of third-party tools. That does not mean that the usage
    of third-party tools is wrong. Actually, throughout this book we are trying to
    combine different tools by choosing those that are performing better than others
    without introducing unnecessary features overhead. The best results are obtained
    when we use right tools for the job. If the tool does more than the job we require,
    its efficiency drops. On the other hand, a tool that doesn't do what we need it
    to do is useless. Consul strikes the right balance. It does very few things, and
    it does them well.
  prefs: []
  type: TYPE_NORMAL
- en: The way Consul uses the gossip protocol to propagate knowledge about the cluster
    makes it easier to set up than etcd, especially in the case of a big data center.
    The ability to store data as a service makes it more complete and useful than
    key/value storage used in etcd (even though Consul has that option as well). While
    we could accomplish the same by inserting multiple keys in etcd, Consul's service
    achieves a more compact result that often requires a single query to retrieve
    all the data related to the service. On top of that, Registrator has quite a good
    implementation of the Consul protocol making the two an excellent combination,
    especially when Consul Template is added to this mixture. Consul's Web UI is like
    a cherry on top of a cake and provides a good way to visualize your services and
    their health.
  prefs: []
  type: TYPE_NORMAL
- en: I can't say that Consul is a clear winner. Instead, it has a slight edge when
    compared with etcd. Service discovery as a concept, as well as the tools we can
    use, is so new that we can expect a lot of changes in this field. By the time
    you read this book, it's likely that new tools will come, or those we evaluated
    will change enough that some of the exercises we did will become obsolete. Have
    an open mind and try to take bits of advice from this chapter with a grain of
    salt. The logic we employed is solid and is not likely to change anytime soon.
    The same can not be said for tools. They are bound to evolve rapidly soon.
  prefs: []
  type: TYPE_NORMAL
- en: We are left with one more subject before we can get back to our deployment procedure.
    The integration step will require that we go through `reverse proxy`.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, let's destroy the virtual machines we created for the purpose
    of service discovery practice and free some resources for the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
