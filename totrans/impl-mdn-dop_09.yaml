- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen a large number of tools that we can use as DevOps engineers
    in our company to enhance our capabilities. Now we are able to provision servers
    with Ansible, create Kubernetes clusters on Google Cloud Platform, and set up
    a delivery pipeline for our microservices. We have also dived deep into how Docker
    works and how we should organize our company to be a successful delivering software.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to take a look at the **missing piece of the
    puzzle**: monitoring. Usually overlooked, monitoring is, in my opinion, a key
    component of a successful DevOps company. Monitoring is the first line of defense
    against problems. In [Chapter 8](127a7b5f-4bd7-4290-bea0-3e8db867e4af.xhtml),
    *Release Management â€“ Continuous Delivery*, we talked about how we should shift
    our focus toward being able to fix the arising problems rather than spending a
    huge amount of resources in trying to prevent them:'
  prefs: []
  type: TYPE_NORMAL
- en: 20 percent of your time will create 80 percent of the functionality. The other
    20 percent is going to cost you 80 percent of your time.
  prefs: []
  type: TYPE_NORMAL
- en: This non-written rule dominates the world. With monitoring, we can bend this
    rule to live comfortably with 20 percent of unknown outcomes as we are able to
    identify the problems quite quickly.
  prefs: []
  type: TYPE_NORMAL
- en: We will review some of the tools to monitor software, but our focus will be
    on Stackdriver, as it is the monitoring solution from Google Cloud Platform that,
    out of the box, provides us with a fairly comprehensive set of tools to deal with
    the flaws in our systems.
  prefs: []
  type: TYPE_NORMAL
- en: Types of monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the SRE book from Google, there are two types of monitoring defined:'
  prefs: []
  type: TYPE_NORMAL
- en: BlackBox monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WhiteBox monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is generally accepted by everyone, leading to a solid amount of tools that
    are clearly differentiated around whitebox and blackbox monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the best comparisons I''ve ever heard on whitebox versus blackbox monitoring
    is the diagnosis of a bone fracture. When you first go to the doctor, he/she only
    has access to your blackbox metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the area have any bump?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it painful on movement?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, once the initial diagnoses has been pronounced, the next step is getting
    X-rays from the area. Now we can confirm whether the bone is broken and, if it
    is, what is the impact in the system. The X-rays are the WhiteBox monitoring that
    the doctor is using.
  prefs: []
  type: TYPE_NORMAL
- en: Blackbox monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whitebox monitoring is the type of monitoring that observes a system from outside
    without having to look into how the system is built. These metrics are the first
    ones that impact the users and the first external symptoms that something is going
    wrong in our application or server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the metrics that can be used for blackbox monitoring, we can find the
    following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two metrics are the holy grail of blackbox monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latency is, by definition, how long takes our system to respond. If we
    are looking at an HTTP server, from the very first time that we sent the request
    to the time when the server on the other side of the line replies is what we understand
    as latency. This metric is a fairly interesting one because it is the absolute
    truth about how the users see our system: the bigger the latency is, the worse
    the experience they get.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughput is extremely related to the latency. Basically, it is the number
    of requests that our software can serve per time unit, usually per second. This
    measure is a critical one for capacity planning, and you are discouraged to measure
    it in real time in a running system, as it pushes a lot of load through the system,
    which is surely going to affect the response time for live users. In general,
    throughput is measured at the performance testing stage of our application, which
    might be tricky:'
  prefs: []
  type: TYPE_NORMAL
- en: The hardware for testing has to match production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset for the database has to be similar to production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance testing step is usually overlooked by many of the companies
    as it is fairly expensive. That is why preproduction is usually used for capacity
    testing in order to guess the amount of hardware needed in production. Nowadays,
    this is less problematic, as with auto scaling groups in the cloud infrastructure,
    it becomes less of a problem as the infrastructure is going to scale on its own
    when needed.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, these metrics are fairly simple to understand, and even though
    they play a key role in the error response time, they might not be the first indicators
    of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Whitebox monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whitebox monitoring, as the name indicates, is the monitoring that needs to
    know about how the system is built in order to raise alerts on certain events
    happening inside of our application or infrastructure. These metrics are quite
    fine-grained (unlike blackbox monitoring), and once we have been alerted, they
    are the answer to the main questions of a postmortem analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Where is the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is causing the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which flows are affected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can we do to avoid this in future?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Among other metrics, these are a fairly interesting set of examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Function execution time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Errors per time unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests per time unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard drive usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O operations per time unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there is an endless number of whitebox metrics to ensure the
    stability of our system. There are almost too many, so we usually need to pick
    the right ones in order to avoid the noise.
  prefs: []
  type: TYPE_NORMAL
- en: An important detail here is the fact that when a blackbox monitoring metric
    gives an abnormal reading, there is always a whitebox metric that can be used
    to diagnose, but it is not true the other way around. A server can have a spike
    in the memory usage due to an internal problem without impacting the users.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important artifacts in whitebox monitoring are the logging files.
    These files are the ordered chain of events happening in our software, and usually,
    they are the first line of attack to diagnose problems related to our software.
    The main problem with log files is the fact that they are stored on production
    servers and we should not access them on a regular basis just to check the log
    files as it is a security threat on its own. It only takes an open terminal to
    a server forgotten by someone to give access rights to the wrong person.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring third-party tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring is usually a good candidate to involve third-party companies. It
    requires a fair amount of redundancy in the systems in order to keep the monitoring
    active, which is primordial in order to ensure that we are not blind to what is
    happening in our system.
  prefs: []
  type: TYPE_NORMAL
- en: Another positive aspect of using third-party apps for monitoring is the fact
    that they don't live in the same data center, and if they do (usually AWS), their
    redundancy is enough to ensure stability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to take a look at three tools in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: Pingdom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logentries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AppDynamics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That doesn''t mean that they are the best or the only tools in the market.
    There are other interesting alternatives (for example, New Relic instead of AppDynamics)
    that are worth exploring, but in this case, we are going to focus on Stackdriver,
    the monitoring solution for Google Cloud Platform, due to a number of factors:'
  prefs: []
  type: TYPE_NORMAL
- en: It integrates very well with Google Cloud Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a very interesting free tier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The alerting systems are one of the most advanced systems you can find in the
    market
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pingdom
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pingdom is a tool used to measure the latency of our servers from different
    sides of the world. As you can see, if you have worked in a 24/7 company, latency
    across the globe varies a lot depending on where our customers are in relation
    to our data centers. As a matter of curiosity, if our server is in Europe, someone
    from Australia will have around 2-3 seconds extra on the latency.
  prefs: []
  type: TYPE_NORMAL
- en: Pingdom has servers spread across the globe to monitor how our users see the
    system and take adequate measures to solve the problem (for example, spawning
    a new data center closer to them).
  prefs: []
  type: TYPE_NORMAL
- en: You can register in Pingdom for free with a 14-days trial, but you need to enter
    a credit card (don't worry; they will advise you when your trial is over so you
    can cancel the plan if you don't want to continue with it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71c60795-f39a-4827-ae2a-ca8d4cc59ef0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, after specifying hosts, Pingdom will start issuing requests
    to the specified URL and measuring the response time from different parts of the
    world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lately, Pingdom has included fairly interesting capabilities: now it can read
    custom metrics through an endpoint in order to monitor an endless amount of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Free space on the disks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used amount of RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stock levels (yes, you can send any number of items you have left in your warehouse
    to Pingdom)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, I have used Pingdom quite successfully in the past to measure the
    latency in my servers and improve the experience of the users by distributing
    the data centers strategically across the globe to mitigate this problem. One
    of the most interesting insights that Pingdom (and similar tools) can give you
    is that your site might be down due to network splits on the internet or failures
    in some DNS servers (in the latter case, it is not really down but Pingdom and
    users won't be able to reach it).
  prefs: []
  type: TYPE_NORMAL
- en: Logentries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logentries is one of these companies that makes your life much easier when
    dealing with a large number of logs. It basically solves one problem that was
    an impediment for few years: it aggregates all the logs from your system in a
    common place with access controls and a more than decent interface that allows
    you to quickly search through big datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an account is free, and it provides 30 days of usage with some limits
    that are more than enough for testing and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to [https://logentries.com/](https://logentries.com/) and create an account.
    Once you are logged in, the first screen should be similar to what is shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9206f600-80f6-49c8-baeb-25f49c96e4bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there are explanations for how to configure the log aggregation
    in an endless number of platforms: you can monitor from systems to libraries going
    through a number of platforms (AWS, Docker, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agents are usually a good choice for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: They do not create coupling in your application (the agent reads the log files
    and sends them to the Logentries servers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They push the complexity to a third-party software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are also other interesting options, such as manual log aggregation.
    In this case, we are going to demonstrate how to use a custom logger to send logs
    from a very simple Node.js application to Logentries. Create a folder called `logentries`
    and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This assumes that Node.js is installed on your system, so if it is not, download
    any version of Node.js from [https://nodejs.org/en/](https://nodejs.org/en/) and
    install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to install the Logentries library for Node.js. Logentries provides
    support for a number of platforms, but support for Node.js is particularly good.
    Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it is finished, we should have the required library installed. Now it
    is time to create a simple `Node.js` program to demonstrate how it works, but
    first, we need to create a service token. On the following screen, click on Manual
    and fill in the form, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3e83f22-b897-479e-aec9-3ac93bc81282.png)'
  prefs: []
  type: TYPE_IMG
- en: Logentries is able to understand many different types of logs, but it really
    shines on JSON logs. We don't need to specify any type of logs for it to catch
    them, so leave this option empty and give a name to the log and the set. Once
    you click on Create Log Token, the token should be displayed after the button.
    Save it for later; you are going to need it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if we go to the main dashboard, we should be able to see our log set called
    Testing Set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e34f1ba0-b38d-4145-89d9-0362d8b130bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now it is time to send some data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This script is enough to send data to Logentries. Be aware that the token specified
    has to be replaced by the token obtained in the previous step. Save it as `index.js`
    and execute it a few times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have executed it a few times, head back to Logentries and open Test
    log inside Testing Set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a75d0030-4bea-4b68-8eed-069b93254b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now you can see our logs in Logentries being aggregated. There are some interesting
    things in Logentries that have been improving with time:'
  prefs: []
  type: TYPE_NORMAL
- en: The UI is quite slick
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The search mechanisms are very powerful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logentries is able to live stream the logs in real time (more or less)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding search mechanisms, Logentries has developed something called **LEQL**,
    which is basically a language designed by Logentries to search for certain events
    using JSON fields or just plain text searching. You can find more information
    about it at [https://docs.logentries.com/v1.0/docs/search/](https://docs.logentries.com/v1.0/docs/search/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The other interesting feature is the live tailing of the logs. Let''s test
    that feature. Create another file in the project called `livetail.js` and add
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It does not need explanation: a function that gets executed every 500 milliseconds
    and sends a log line to Logentries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It might look like nothing is happening, but things are actually happening.
    Go back to Logentries and click on the Start live tail button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92fdd2ef-b97f-4e1f-8a25-4ada23391abc.png)'
  prefs: []
  type: TYPE_IMG
- en: After a couple of seconds (or less), the logs will start flowing. This can be
    done on any log file that is stored in Logentries, and it is a fairly interesting
    mechanism to debug problems in our servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logentries is also able to send alerts to a certain email. You can configure
    it to alert your team on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exceptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This alert is usually the first indicator of problems on a system, so if you
    want an early response to errors, the best practice is to try to reduce the noise
    up to a point where alerts are not missed and the false positives are reduced
    to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: AppDynamics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AppDynamics was the king for a while (as it was the only real option on monitoring).
    It is a very curated software that allows you to explore what is going on in your
    software and servers: exceptions, requests per time unit, and CPU usage are among
    many other metrics that AppDynamics can capture for us.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It also captures interactions with the third-party endpoints: if our software
    is consuming a third-party API, AppDynamics will know about it and display the
    calls in a dashboard similar to the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/907bb61f-fb55-430f-96c0-bf5544feb3e1.png)'
  prefs: []
  type: TYPE_IMG
- en: AppDynamics is quite advanced regarding proactive measures. One of these measures
    is automated actions, such as restarting a Tomcat server or reloading a service
    running on the server on certain events. For example, if we deploy a new version
    of our Java application that is having a problem with the PermGen space (this
    is not the case anymore in Java 8+), it is usually very tricky to fix as the problem
    comes from many classes loaded by the JVM and only manifests a few hours after
    the deployment. In some cases, we can instruct AppDynamics to restart the application
    when the usage reaches 80 percent and more of the assigned total so that instead
    of crashing badly and not being able to serve to any customers, we only have a
    few dropouts every few hour getting an air balloon to act and fix the problem.
  prefs: []
  type: TYPE_NORMAL
- en: AppDynamics works with what is known an agent model. An application needs to
    be installed on your server (for example, Tomcat) in order to collect metrics
    that are sent to a centralized server to process and create the pertinent dashboards
    and trigger the workflows. The interesting part of AppDynamics is that if you
    don't feel comfortable sending data to a third-party (which is usually a security
    requirement for companies handling high-profile data), they provide an on-premises
    version of the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Stackdriver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have visited a set of tools from different third-parties but
    now we are going to take a look at Stackdriver. Stackdriver was a cloud monitoring
    solution acquired by Google and integrated (not fully) into Google Cloud Platform.
    This is an important step for GCP, as being able to provide an integrated monitoring
    solution is something that's pretty much mandatory nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: With Stackdriver, we are not only able to monitor applications, but also Kubernetes
    clusters or even standalone VMs. As we will see, the integration is not yet as
    seamless as we would desire (it might be completed by the time you are reading
    this), but it is good enough to be considered a big player in the market.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stackdriver can monitor standalone applications by capturing metrics and logs.
    It has support for major platforms and libraries, so our technology choices should
    not be a concern. In this case, we are going to create a Node.js application for
    several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The official examples are well documented for the Node.js version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node.js is increasingly becoming a big platform for enterprise and startups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first thing we need to do is write a small Node.js application. Create
    a new folder and execute this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the instructions on the screen and you should now have `package.json`
    in the folder that you just created. Now it is time to install the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use four libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**express**: To handle the HTTP requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bunyan**: To log our application activity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two libraries from Google are for interacting with Stackdriver:'
  prefs: []
  type: TYPE_NORMAL
- en: '**logging-bunyan**: This will send the logs from bunyan to Stackdriver'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**trace-agent**: This will trace the requests through our application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s create a simple application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it is time to explain what the interesting parts of the code do:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line enables the tracing for Stackdriver. It is very important that
    this line happen before anything else; otherwise, the tracing won't work. We'll
    see how amazing the tracing is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to let Stackdriver collect logs, we need to add a stream to the bunyan
    logger, as shown in the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Everything else is quite normal: an `Express.js` Node.js application that has
    a handler for the URL/replying with the classic Hello World.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one thing missing: there are no credentials to access the remote APIs.
    This is done on purpose as Google Cloud Platform has a very sophisticated system
    for handling credentials: basically, it will be handled for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to deploy our application. First, create a VM in Google Cloud
    Platform, as we have seen a few times in the previous chapters. A small one will
    suffice, but make sure that you allow HTTP traffic. Debian Stretch is a good choice
    as an operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Once the machine is up, install Node.js, as shown in [http://nodejs.org](http://nodejs.org.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to copy the code into our newly created machine. The best solution
    is to create a GitHub repository or use mine: [https://github.com/dgonzalez/stackdriver](https://github.com/dgonzalez/stackdriver).'
  prefs: []
  type: TYPE_NORMAL
- en: 'By cloning it in our VM (don''t forget to install Git first via `apt`), we
    just need to install the dependencies with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And we are good to go. Just run the application with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now go to the external IP of the machine (shown in Google Compute Engine) on
    port `3000`. In my case, this is [http://35.195.151.10:3000/](http://35.195.151.10:3000/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have done it, we should see Hello World in the browser and something
    similar to the following in the logs of our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If there are no errors, everything worked. In order to verify this, go to [http://console.cloud.google.com](http://console.cloud.google.com)
    and open the Logging section of Stackdriver.
  prefs: []
  type: TYPE_NORMAL
- en: Stackdriver is a different system from Google Cloud Platform; it might ask you
    to log in using a Google account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are there, you should see something similar to what is shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03f49c27-f2be-4088-a08b-4ee15ef61622.png)'
  prefs: []
  type: TYPE_IMG
- en: Be aware that you have to select the section on the logs, in my case, GCE VM
    Instance, Instance-3.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly the log from your app uploaded to Google Cloud Platform with
    a bunch of very interesting information. You can play around by having different
    handlers for other URLs and different logging events, but the result is the same:
    all your logs will be aggregated here.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can do this with Trace as well. Open the trace section of Google Cloud
    Platform under Stackdriver.
  prefs: []
  type: TYPE_NORMAL
- en: 'The screen should look similar to what is shown in the following screenshot
    (select the traces list option):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f941972-0f9c-4d51-a960-6f139d553aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a lot of useful information:'
  prefs: []
  type: TYPE_NORMAL
- en: The call stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start and finish time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also play around, issue multiple calls, and get familiar yourself with
    how it works. Now we are going to modify our program to call a third-party API
    and see how Stackdriver is able to trace it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are listing all the available APIs on Google by executing an HTTP GET
    into the [https://www.googleapis.com/discovery/v1/apis](https://www.googleapis.com/discovery/v1/apis.)
    URL. Redeploy it into your VM in Google Cloud Platform and go to the endpoint/discovery
    of your VM. A big JSON payload will be presented on your screen, but the interesting
    part is happening under the hood. Go back to Stackdriver in the Trace list section,
    and you''ll see that there is a new trace being captured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e204ee46-fdff-4a4f-9bbf-85d13fad6481.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can see how our program contacted the remote API and that it took
    68 seconds to reply.
  prefs: []
  type: TYPE_NORMAL
- en: Getting this type of information in real time is very powerful--if customers
    are getting a very large response time, we can immediately see what is happening
    inside of our application pretty much real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Kubernetes clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes covers 99 percent of the needs for any software company, but one
    part where it does not really shine is in embedded monitoring, leaving a space
    to be filled by third-parties. The main problem with Kubernetes comes from Docker:
    containers are ephemeral, so a common practice is to dump the logs into the standard
    output/error and use *syslogd* to gather them in a centralized location.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With Kubernetes, we have an added problem: the orchestrator on top of Docker
    needs to know how to fetch logs in order to make them available via the API or
    dashboard, so it is possible for the user to access them when required. But then
    there is another problem. Usually, logs are rotated on the basis of time and archived
    in order to avoid a log sprawl that can consume all the free space in our servers,
    preventing the application (and the OS) from functioning normally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best solution is to use an external system to aggregate logs and events
    inside the cluster so that we push the complexity to the side, allowing Kubernetes
    to focus on what it does best: orchestrate containers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, in order to integrate our cluster with Stackdriver in Google
    Cloud Platform, the only thing that we need to do is mark the two checkboxes in
    the cluster creation screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/baf7c352-db8f-4c39-8c9b-536c8cdc520d.png)'
  prefs: []
  type: TYPE_IMG
- en: This will enable the monitoring across the different nodes in our cluster and
    improve the way in which we tackle problems happening in our applications. Click
    on Create and allow the cluster to be provisioned (might take a few seconds or
    even minutes). It does not need to be a big cluster; just use the small size for
    the machines, and two VMs will be enough. In fairness, we will probably need to
    reduce the size during the load testing in order to speed up the alerting part.
  prefs: []
  type: TYPE_NORMAL
- en: As we've seen in the previous section, with GKE monitoring active, Kubernetes
    also sends the logs to the Logging capabilities of Stackdriver, so you don't need
    to connect to the nodes to fetch the logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it is created, we are going to add monitoring from Stackdriver. The first
    thing that we need to do is open Monitoring in the Stackdriver section of Google
    Cloud Platform. This will open a new site, the original Stackdriver site, which
    looks very similar to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19caa285-4659-4d30-bf1f-6a762404e29f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even though the UI might be a bit confusing in the beginning, after a bit of
    usage, it becomes clear that it is really hard to pack the huge amount of functionality
    that Stackdriver offers in a better UI. By default, we can see some metrics about
    our cluster (the bottom-right section of the image), but they are not very useful:
    we don''t want to have someone looking at the metrics the full day in order to
    raise alerts. Let''s automate it. The first thing that we need to do is create
    a group. A group is basically a set of resources that work together, in this case,
    our cluster. Click on Groups and create a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5ed7da1-5fe7-4c21-904f-a7d485473a22.png)'
  prefs: []
  type: TYPE_IMG
- en: By default, Stackdriver will suggest groupings to you. In this case, in the
    suggested Groups section, we can see that Stackdriver has suggested our cluster.
    It is possible to add more sophisticated criteria, but in this case, matching
    the start of the name of our machines will work as the GKE names them according
    to some criteria, including the name of the cluster in the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a group (call it GKE, the default suggested name). Once the group is
    created, you can navigate to it and see different metrics such as CPU or even
    configure them, adding others such as disk I/O and similar. Get yourself familiar
    with the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83a4112e-e61b-4e53-a521-96307731700f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In my case, I have added a metric for capturing the used memory in the cluster.
    Even though this amount of available data is quite interesting, there is an even
    more powerful tool: the alerting policies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The alerting policies are criteria in which we should get alerted: high memory
    usage, low disk space, or high CPU utilization, among others, are events that
    we want to know about in order to take actions as soon as possible. The beauty
    of the alerting policies is that if we configure them as appropriated, we enter
    a state that I call the *autopilot* mode: we don''t need to worry about the performance
    of the system unless we get alerted, which drastically reduces the number of people
    required to operate a system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create an alerting policy by clicking on the Create Alerting Policy
    button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e07ab7d-4293-47e0-af8f-e49942c6999b.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding screen, we need to select a metric. We are going to
    use the type Metric Threshold as it is included in the basic package of Stackdriver,
    so we don't need to upgrade to a Premium subscription. Our alert is going to be
    raised if any of the members of our cluster has a CPU usage of more than 30 percent
    for one minute.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to configure the notification. In the basic package, only email
    is included, but it is sufficient to test how the system works. Stackdriver also
    allows you to include text to be sent across with the notification. Just enter
    something like `Test alert` alongside your email and save the alert policy with
    the name of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, creating alerts is very simple in Stackdriver. This is a very
    simplistic example, but once you have set up your cluster, the next step is to
    set up the acceptable set of metrics where it should operate normally and get
    alerted if any of them is violated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to set off the alarm to see what happens. In order to do that,
    we need to overload the cluster with several replicas of the same image, and we
    are going to use a tool called Apache benchmark to generate a load on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And now expose the deployment called `my-nginx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Be aware that you first need to configure `kubectl` to point to your cluster,
    as we've seen in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once nginx is deployed, it is time to stress it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ab` tool is a very powerful benchmark tool called Apache Benchmark. We
    are going to create 350 concurrent consumers, and they will issue 4 million requests
    in total. It might be possible that you need to reduce the size of your cluster
    in order to stress the CPU: if you reduce the size while the benchmark is running,
    Kubernetes will need to reschedule containers to reorganize the resources, adding
    more load to the system.'
  prefs: []
  type: TYPE_NORMAL
- en: I would recommend that you further explore Apache Benchmark, as it is very useful
    for load testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the CPU has gone beyond the threshold for any of our nodes for over a
    minute, we should receive an alert by email, and it should be displayed in the
    Stackdriver interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54f0b210-4fd8-4c97-864e-8f60fca91024.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, I have received two alerts as two of the nodes went beyond the
    limits. These alerts follow a workflow. You can acknowledge them if the rule is
    still broken and they will go to resolved once they are acknowledged and the condition
    has disappeared. In this case, if you stop Apache Benchmark and acknowledge the
    raised alerts, they will go straight into the resolved state.
  prefs: []
  type: TYPE_NORMAL
- en: In the Premium version, there are more advanced policies, such as Slack messages
    or SMS, which allows your team to set up a rota for acknowledging incidents and
    managing the actions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the final chapter of the book. Through the entire book, we visited
    the most important tools used by DevOps engineers, with a strong focus on the
    Google Cloud Platform. In this chapter, we experimented with what, in my opinion,
    is a very important aspect of any system: monitoring. I am of the opinion that
    monitoring is the best way to tackle problems once they have hit your production
    systems, which, no matter how much effort you put in it, will eventually happen.'
  prefs: []
  type: TYPE_NORMAL
- en: What is next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This book did not go particularly in deep into any subject. This is intended.
    This book is meant to plant the seed of a big tree: the culture of DevOps, hoping
    that you have enough information to keep growing your knowledge of DevOps tools
    and processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping yourself up to date with the latest tools is a full-time job on its
    own, but it's very necessary if you want to be on the top of the wave. My opinion
    is that we are very lucky to be able to participate in the rise and shine of the
    DevOps culture, and the future is bright. Automation and better languages (Golang,
    Kotlin, Node.js, and so on) will allow us to reduce human intervention, improving
    the overall resilience of our systems.
  prefs: []
  type: TYPE_NORMAL
- en: If you look five years back and compare it with what it is there in the market
    today, can you imagine how our jobs are going to be in 15 years?
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to follow up about any question or check what I am working on nowadays,
    you can always get in touch with me in LinkedIn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.linkedin.com/in/david-gonzalez-microservices/](https://www.linkedin.com/in/david-gonzalez-microservices/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
