<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Managing Resources</h1>
                </header>
            
            <article>
                
<div class="packt_tip">Without an indication how much CPU and memory a container needs, Kubernetes has no other option than to treat all containers equally. That often produces a very uneven distribution of resource usage. Asking Kubernetes to schedule containers without resource specifications is like entering a taxi driven by a blind person.</div>
<p>We have come a long way, from humble beginnings, towards understanding many of the essential Kubernetes object types and principles. One of the most important things we're missing is resource management. Kubernetes was blindly scheduling the applications we deployed so far. We never gave it any indication how much resources we expect those applications to use, nor established any limits. Without them, Kubernetes was carrying out its tasks in a very myopic fashion. Kubernetes could see a lot, but not enough. We'll change that soon. We'll give Kubernetes a pair of glasses that will provide it a much better vision.</p>
<p>Once we learn how to define resources, we'll go further and make sure that certain limitations are set, that some defaults are determined, and that there are quotas that will prevent applications from overloading the cluster.</p>
<p>This chapter is the last piece of the puzzle. Once we solve it, we'll be ready to start thinking about using Kubernetes in production. You won't know everything you should know about operating Kubernetes. No one does. But, you will know just enough to get you going in the right direction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a cluster</h1>
                </header>
            
            <article>
                
<p>We'll go through almost the same routine as we did in the previous chapters. We'll enter the directory where we cloned the <kbd>vfarcic/k8s-specs</kbd> repository, pull the latest code, start a Minikube cluster, and so on and so forth. The only new thing we'll do this time is to enable one more addon. We'll add Heapster to the cluster. It's too soon to explain what it does and why we'll need it. That will come later. For now, just remember that there will soon be something in your cluster called Heapster. If you do not already know what it is, consider this a teaser meant to build suspense.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <a href="https://gist.github.com/cc8c44e1e84446dccde3d377c131a5cd"><kbd>13-resource.sh</kbd></a> (<a href="https://gist.github.com/vfarcic/cc8c44e1e84446dccde3d377c131a5cd" target="_blank"><span class="URLPACKT">https://gist.github.com/vfarcic/cc8c44e1e84446dccde3d377c131a5cd</span></a>) Gist.</div>
<pre><strong>cd k8s-specs</strong>
    
<strong>git pull</strong>
    
<strong>minikube start --vm-driver=virtualbox</strong>
    
<strong>kubectl config current-context</strong>
    
<strong>minikube addons enable ingress</strong>
    
<strong>minikube addons enable heapster</strong>  </pre>
<p>Now that the latest code is pulled, the cluster is running, and the add-ons are enabled, we can proceed and explore how to define container memory and CPU resources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining container memory and CPU resources</h1>
                </header>
            
            <article>
                
<p>So far, we did not specify how much memory and CPU containers should use, nor what their limits should be. If we do that, Kubernetes' scheduler will have a much better idea about the needs of those containers, and it'll make much better decisions on which nodes to place the Pods and what to do if they start "misbehaving".</p>
<p>Let's take a look at a modified <kbd>go-demo-2</kbd> definition:</p>
<pre><strong>cat res/go-demo-2-random.yml</strong>  </pre>
<p>The specification is almost the same as those we used before. The only new entries are in the <kbd>resources</kbd> section.</p>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-db 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: db 
        image: mongo:3.3 
        resources: 
          limits: 
            memory: 200Mi 
            cpu: 0.5 
          requests: 
            memory: 100Mi 
            cpu: 0.3 
... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-api 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: api 
        image: vfarcic/go-demo-2 
        ... 
        resources: 
          limits: 
            memory: 100Mi 
            cpu: 200m 
          requests: 
            memory: 50Mi 
            cpu: 100m 
... </pre>
<p>We specified <kbd>limits</kbd> and <kbd>requests</kbd> entries in the <kbd>resources</kbd> section.</p>
<p>CPU resources are measured in <kbd>cpu</kbd> units. The exact meaning of a <kbd>cpu</kbd> unit depends on where we host our cluster. If servers are virtualized, one <kbd>cpu</kbd> unit is equivalent to one <strong>virtualized processor</strong> (<strong>vCPU</strong>). When running on bare-metal with Hyperthreading, one <kbd>cpu</kbd> equals one Hyperthread. For the sake of simplification, we'll assume that one <kbd>cpu</kbd> resource is one CPU processor (even though that is not entirely true).</p>
<p>If one container is set to use <kbd>two</kbd> CPU, and the other is set to <kbd>one</kbd> CPU, the later is guaranteed half as much processing power.</p>
<p>CPU values can be fractioned. In our example, the <kbd>db</kbd> container has the CPU requests set to <kbd>0.5</kbd> which is equivalent to half CPU. The same value could be expressed as <kbd>500m</kbd>, which translates to five hundred millicpu. If you take another look at the CPU specs of the <kbd>api</kbd> container, you'll see that its CPU limit is set to <kbd>400m</kbd> and the requests to <kbd>200m</kbd>. They are equivalent to <kbd>0.4</kbd> and <kbd>0.2</kbd> CPUs.</p>
<p>Memory resources follow a similar pattern as CPU. The significant difference is in the units. Memory can be expressed as <strong>K</strong> (<strong>kilobyte</strong>), <strong>M</strong> (<strong>Megabyte</strong>), <strong>G</strong> (<strong>Gigabyte</strong>), <strong>T</strong> (<strong>Terabyte</strong>), <strong>P</strong> (<strong>Petabyte</strong>), and <strong>E</strong> (<strong>Exabyte</strong>). We can also use the power-of-two equivalents <kbd>Ki</kbd>, <kbd>Mi</kbd>, <kbd>Gi</kbd>, <kbd>Ti</kbd>, <kbd>Pi</kbd>, and <kbd>Ei</kbd>.</p>
<p>If we go back to the <kbd>go-demo-2-random.yml</kbd> definition, we'll see that the <kbd>db</kbd> container has the limit set to <strong>200Mi</strong> (<strong>two hundred megabytes</strong>) and the requests to <strong>100Mi</strong> (<strong>one hundred megabytes</strong>).</p>
<p>We have already mentioned <kbd>limits</kbd> and <kbd>requests</kbd> quite a few times and yet we have not explained what each of them mean.</p>
<p>A limit represents the amount of resources that a container should not pass. The assumption is that we define limits as upper boundaries which, when reached, indicate that something went wrong, as well as a way to guard our resources from being overtaken by a single rouge container due to memory leaks or similar problems.</p>
<p>If a container is restartable, Kubernetes will restart a container that exceeds its memory limit. Otherwise, it might terminate it. Bear in mind that a terminated container will be recreated if it belongs to a Pod (as all Kubernetes-controlled containers do).</p>
<p>Unlike memory, CPU limits never result in termination or restarts. Instead, a container will not be allowed to consume more than the CPU limit for an extended period.</p>
<p>Requests represent the expected resource utilization. They are used by Kubernetes to decide where to place Pods depending on actual resource utilization of the nodes that form the cluster.</p>
<p>If a container exceeds its memory requests, the Pod it resides in might be evicted if a node runs out of memory. Such eviction usually results in the Pod being scheduled on a different node, as long as there is one with enough available memory. If a Pod cannot be scheduled to any of the nodes due to lack of available resources, it enters the pending state waiting until resources on one of the nodes are freed, or a new node is added to the cluster.</p>
<p>Simply discussing the theory of <kbd>resources</kbd> might be confusing if not followed by practical examples. Therefore, we'll move on and create the resources defined in the <kbd>go-demo-2-random.yml</kbd> file:</p>
<pre><strong>kubectl create \</strong>
<strong>    -f res/go-demo-2-random.yml \</strong>
<strong>    --record --save-config</strong>
    
<strong>kubectl rollout status \</strong>
<strong>    deployment go-demo-2-api</strong>  </pre>
<p>We created the resources and waited until the <kbd>go-demo-2-api</kbd> Deployment was rolled out. The output of the later command should be as follows:</p>
<pre><strong>deployment "go-demo-2-api" successfully rolled out</strong>  </pre>
<p>Let's describe the <kbd>go-demo-2-api</kbd> Deployment and see its <kbd>limits</kbd> and <kbd>requests</kbd>:</p>
<pre><strong>kubectl describe deploy go-demo-2-api</strong>  </pre>
<p>The output, limited to the <kbd>limits</kbd> and the <kbd>requests</kbd>, is as follows:</p>
<pre>... 
Pod Template: 
  ... 
  Containers: 
    ... 
   Limits: 
      cpu:    200m 
      memory: 100Mi 
    Requests: 
      cpu:    100m 
      memory: 50Mi 
... </pre>
<p>We can see that the <kbd>limits</kbd> and the <kbd>requests</kbd> correspond to those we defined in the <kbd>go-demo-2-random.yml</kbd> file. That should come as no surprise.</p>
<p>Let's describe the nodes that form the cluster (even though there's only one).</p>
<pre><strong>kubectl describe nodes</strong>  </pre>
<p>The output, limited to the resource-related entries, is as follows:</p>
<pre><strong>... 
Capacity: 
 cpu:     2 
 memory:  2048052Ki 
 pods:    110 
... 
Non-terminated Pods: (12 in total) 
  Namespace          Name                         CPU Requests CPU Limits Memory Requests Memory Limits 
  ---------          ----                         ------------ ---------- --------------- ------------- 
  default            go-demo-2-api-...            100m (5%)    200m (10%) 50Mi (2%)       100Mi (5%) 
  default            go-demo-2-api-...            100m (5%)    200m (10%) 50Mi (2%)       100Mi (5%) 
  default            go-demo-2-api-...            100m (5%)    200m (10%) 50Mi (2%)       100Mi (5%) 
  default            go-demo-2-db-...             300m (15%)   500m (25%) 100Mi (5%)      200Mi (10%) 
  kube-system        default-http-...             10m (0%)     10m (0%)   20Mi (1%)       20Mi (1%) 
  kube-system        heapster-...                 0 (0%)       0 (0%)     0 (0%)          0 (0%) 
  kube-system        influxdb-grafana-...         0 (0%)       0 (0%)     0 (0%)          0 (0%) 
  kube-system        kube-addon-manager-minikube  5m (0%)      0 (0%)     50Mi (2%)       0 (0%) 
  kube-system        kube-dns-54cccfbdf8-...      260m (13%)   0 (0%)     110Mi (5%)      170Mi (8%) 
  kube-system        kubernetes-dashboard-...     0 (0%)       0 (0%)     0 (0%)          0 (0%) 
  kube-system        nginx-ingress-controller-... 0 (0%)       0 (0%)     0 (0%)          0 (0%) 
  kube-system        storage-provisioner          0 (0%)       0 (0%)     0 (0%)          0 (0%) 
Allocated resources: 
  (Total limits may be over 100 percent, i.e., overcommitted.) 
  CPU Requests CPU Limits  Memory Requests Memory Limits 
  ------------ ----------  --------------- ------------- 
  875m (43%)   1110m (55%) 430Mi (22%)     690Mi (36%) 
...</strong> </pre>
<p>The <kbd>Capacity</kbd> represents the overall capacity of a node. In our case, the <kbd>minikube</kbd> node has 2 CPUs, 2GB of RAM, and can run up to one hundred and ten Pods. Those are the upper limits imposed by the hardware or, in our case, the size of the VM created by Minikube.</p>
<p>Further down is the <kbd>Non-terminated Pods</kbd> section. It lists all the Pods with the CPU and memory limits and requests. We can, for example, see that the <kbd>go-demo-2-db</kbd> Pod has the memory limit set to <kbd>100Mi</kbd>, which is <kbd>5%</kbd> of the capacity. Similarly, we can see that not all Pods have specified resources. For example, the <kbd>heapster-snq2f</kbd> Pod has all the values set to <kbd>0</kbd>. Kubernetes will not be able to handle those Pods appropriately. However, since this is a demo cluster, we'll give the Minikube authors a pass and ignore the lack of resource specification.</p>
<p>Finally, the <kbd>Allocated resources</kbd> section provides summed values from all the Pods. We can, for example, see that the CPU limits are <kbd>55%</kbd>. Limits can be even higher than <kbd>100%</kbd>, and that would not necessarily be a thing to worry about. Not all the containers will have memory and CPU bursts over the requested values. Even if that happens, Kubernetes will know what to do.</p>
<p>What truly matters is that the total amount of requested memory and CPU is within the limits of the capacity. That, however, leads us to an interesting question. What is the basis for the resources we defined so far?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring actual memory and CPU consumption</h1>
                </header>
            
            <article>
                
<p>How did we come up with the current memory and CPU values? Why did we set the memory of the MongoDB to <kbd>100Mi</kbd>? Why not <kbd>50Mi</kbd> or <kbd>1Gi</kbd>? It is embarrassing to admit that the values we have right now are random. I guessed that the containers based on the <kbd>vfarcic/go-demo-2</kbd> image require less resources than Mongo database, so their values are comparatively smaller. That was the only criteria I used to define the resources.</p>
<p>Before you frown upon my decision to put random values for resources, you should know that we do not have any metrics to back us up. Anybody's guess is as good as mine.</p>
<p>The only way to truly know how much memory and CPU an application uses is by retrieving metrics. We'll use Heapster (<a href="https://github.com/kubernetes/heapster" target="_blank"><span class="URLPACKT">https://github.com/kubernetes/heapster</span></a>) for that purpose.</p>
<p>Heapster collects and interprets various signals like compute resource usage, lifecycle events, and so on. In our case, we're interested only in CPU and memory consumption of the containers we're running in our cluster.</p>
<p>When we created the cluster, we enabled the <kbd>heapster</kbd> addon and Minikube deployed it as a system application. Not only that, but it also deployed InfluxDB (<a href="https://github.com/influxdata/influxdb" target="_blank"><span class="URLPACKT">https://github.com/influxdata/influxdb</span></a>) and Grafana (<a href="https://grafana.com/" target="_blank"><span class="URLPACKT">https://grafana.com/</span></a>). The former is the database where Heapster stores data and the latter can be used to visualize it through dashboards.</p>
<p>You might be inclined to think that Heapster, InfluxDB, and Grafana might be the solution for your monitoring needs. I advise against such a decision. We're using Heapster only because it's readily available as a Minikube addon. The idea to develop Heapster as a tool for monitoring needs is mostly abandoned. Its primary focus is to serve as an internal tool required for some of the Kubernetes features. Instead, I'd suggest a combination of Prometheus (<a href="https://prometheus.io/" target="_blank"><span class="URLPACKT">https://prometheus.io/</span></a>) combined with the Kubernetes API as the source of metrics and Alertmanager (<a href="https://prometheus.io/docs/alerting/alertmanager/" target="_blank"><span class="URLPACKT">https://prometheus.io/docs/alerting/alertmanager/</span></a>) for your alerting needs. However, those tools are not in the scope of this chapter, so you might need to educate yourself from their documentation, or wait until the sequel to this book is published (the tentative name is <em>Advanced Kubernetes</em>).</p>
<div class="packt_tip">Use Heapster only as a quick-and-dirty way to retrieve metrics. Explore the combination of Prometheus and Alertmanager for your monitoring and alerting needs.</div>
<p>Now that we clarified what Heapster is good for, as well as what it isn't, we can proceed and confirm that it is indeed running inside our cluster.</p>
<pre>kubectl --namespace kube-system \
    get pods </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                         READY STATUS  RESTARTS AGE</strong>
<strong>default-http-backend-...     1/1   Running 0        59m</strong>
<strong>heapster-...                 1/1   Running 0        59m</strong>
<strong>influxdb-grafana-...         2/2   Running 0        59m</strong>
<strong>kube-addon-manager-minikube  1/1   Running 0        59m</strong>
<strong>kube-dns-54cccfbdf8-...      3/3   Running 0        59m</strong>
<strong>kubernetes-dashboard-...     1/1   Running 0        59m</strong>
<strong>nginx-ingress-controller-... 1/1   Running 0        59m</strong>
<strong>storage-provisioner          1/1   Running 0        59m</strong>  </pre>
<p>As you can see, the <kbd>heapster</kbd> and <kbd>influxdb-grafana</kbd> Pods are running.</p>
<p>We'll explore Heapster just enough to retrieve the data we need. For that, we'll need access to its API. However, Minikube didn't expose its port so that'll be the first thing we'll do:</p>
<pre>kubectl --namespace kube-system \ 
    expose rc heapster \ 
    --name heapster-api \ 
    --port 8082 \ 
    --type NodePort </pre>
<p>We'll need to find out which <kbd>NodePort</kbd> was created for us. To do that, we need to get familiar with the JSON definition of the service:</p>
<pre>kubectl --namespace kube-system \ 
    get svc heapster-api \  
    -o json </pre>
<p>We are looking for the <kbd>nodePort</kbd> entry inside the <kbd>spec.ports</kbd> array. The command that retrieves it and assigns the output to the <kbd>PORT</kbd> variable is as follows:</p>
<pre>PORT=$(kubectl --namespace kube-system \ 
    get svc heapster-api \ 
    -o jsonpath="{.spec.ports[0].nodePort}") </pre>
<p>We used the <kbd>jsonpath</kbd> output to retrieve only <kbd>nodePort</kbd> of the first (and the only) entry of the <kbd>spec.ports</kbd> array.</p>
<p>Let's try a very simple query of the Heapster API.</p>
<pre>BASE_URL="http://$(minikube ip):$PORT/api/v1/model/namespaces/default/pods" 
 
curl "$BASE_URL" </pre>
<p>The output of the <kbd>curl</kbd> request is as follows:</p>
<pre>[ 
  "go-demo-2-api-796db5987d-dm69g", 
  "go-demo-2-db-bf6f5b486-p9vhj", 
  "go-demo-2-api-796db5987d-5t84b", 
  "go-demo-2-api-796db5987d-99nh6" 
 ] </pre>
<p>We don't really need Heapster to retrieve the list of Pods. What we do need are metrics of one of the Pods. For that, we need it's name.</p>
<p>We'll use a similar command we used to retrieve Heapster's service port.</p>
<pre>DB_POD_NAME=$(kubectl get pods \ 
    -l service=go-demo-2 \ 
    -l type=db \ 
    -o jsonpath="{.items[0].metadata.name}") </pre>
<p>We retrieved all the Pods with the labels <kbd>service=go-demo-2</kbd> and <kbd>type=db</kbd>, and formatted the output so that <kbd>metadata.name</kbd> from the first item is retrieved. The value is stored as the <kbd>DB_POD_NAME</kbd> variable.</p>
<p>Now we can take a look at the available metrics of the <kbd>db</kbd> container inside the Pod.</p>
<pre><strong>curl "$BASE_URL/$DB_POD_NAME/containers/db/metrics"</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>[</strong>
<strong>  "memory/rss",</strong>
<strong>  "cpu/usage_rate",</strong>
<strong>  "cpu/request",</strong>
<strong>  "memory/usage",</strong>
<strong>  "memory/major_page_faults_rate",</strong>
<strong>  "cpu/limit",</strong>
<strong>  "memory/page_faults",</strong>
<strong>  "memory/major_page_faults",</strong>
<strong>  "uptime",</strong>
<strong>  "memory/limit",</strong>
<strong>  "cpu/usage",</strong>
<strong>  "memory/page_faults_rate",</strong>
<strong>  "memory/working_set",</strong>
<strong>  "restart_count",</strong>
<strong>  "memory/request"</strong>
<strong>]</strong>
  </pre>
<p>As you can see, most of the available metrics are related to memory and CPU.</p>
<p>Let's see whether memory usage indeed corresponds with the memory resources we defined for the <kbd>go-demo-2-db</kbd> Deployment. As a reminder, we set memory request to <kbd>100Mi</kbd> and memory limit to <kbd>200Mi</kbd>.</p>
<p>A request that retrieves memory usage of the <kbd>db</kbd> container is as follows.</p>
<pre><strong>curl "$BASE_URL/$DB_POD_NAME/containers/db/metrics/memory/usage"</strong>  </pre>
<p>The output, limited only to a few entries, is as follows:</p>
<pre>{ 
  "metrics": [ 
   ... 
   { 
    "timestamp": "2018-02-01T20:24:00Z", 
    "value": 38334464 
   }, 
   { 
    "timestamp": "2018-02-01T20:25:00Z", 
    "value": 38342656 
   } 
  ], 
  "latestTimestamp": "2018-02-01T20:25:00Z" 
 } </pre>
<p>We can see that memory usage is somewhere around 38 megabytes. That's quite a big difference from <kbd>100Mi</kbd> we set. Sure, this service is not under real production load but, since we're simulating a "real" cluster, we'll pretend that <kbd>38Mi</kbd> is indeed memory usage under "real" conditions. That means that we overestimated the requests by assigning a value almost three times larger than the actual usage.</p>
<p>How about CPU? Did we make such a colossal mistake with it as well? As a reminder, we set the CPU request to <kbd>0.3</kbd> and the limit to <kbd>0.5</kbd>.</p>
<pre><strong>curl "$BASE_URL/$DB_POD_NAME/containers/db/metrics/cpu/usage_rate"</strong>  </pre>
<p>The output, limited to only a few entries, is as follows.</p>
<pre>{ 
  "metrics": [ 
   ... 
   { 
    "timestamp": "2018-02-01T20:25:00Z", 
    "value": 5 
   }, 
   { 
    "timestamp": "2018-02-01T20:26:00Z", 
    "value": 4 
   } 
  ], 
  "latestTimestamp": "2018-02-01T20:26:00Z" 
 } </pre>
<p>As we can see, the CPU usage is around <kbd>5m</kbd> or <kbd>0.005</kbd> CPU. We, again, made a huge mistake with resource specification. Our value is around sixty times higher.</p>
<p>Such deviations between our expectations (resource requests and limits) and the actual usage can lead to very unbalanced scheduling with undesirable effects. We'll correct the resources soon. For now, we'll explore what happens if the amount of resources is below the actual usage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the effects of discrepancies between resource specifications and resource usage</h1>
                </header>
            
            <article>
                
<p>Let's take a look at a slightly modified version of the <kbd>go-demo-2</kbd> definition:</p>
<pre><strong>cat res/go-demo-2-insuf-mem.yml</strong>  </pre>
<p>When compared with the previous definition, the difference is only in <kbd>resources</kbd> of the <kbd>db</kbd> container in the <kbd>go-demo-2-db</kbd> Deployment.</p>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-db 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: db 
        image: mongo:3.3 
        resources: 
          limits: 
            memory: 10Mi 
            cpu: 0.5 
          requests: 
            memory: 5Mi 
            cpu: 0.3 </pre>
<p>The memory limit is set to <kbd>10Mi</kbd> and the request to <kbd>5Mi</kbd>. Since we already know from Heapster's data that MongoDB requires around <kbd>38Mi</kbd>, memory resources are, this time, much lower than the actual usage.</p>
<p>Let's see what will happen when we apply the new configuration:</p>
<pre>kubectl apply \  
    -f res/go-demo-2-insuf-mem.yml \
    --record 
 
kubectl get pods </pre>
<p>We applied the new configuration and retrieved the Pods. The output is as follows:</p>
<pre>    <strong>NAME              READY STATUS    RESTARTS AGE</strong>
    <strong>go-demo-2-api-... 1/1   Running   0        1m</strong>
    <strong>go-demo-2-api-... 1/1   Running   0        1m</strong>
    <strong>go-demo-2-api-... 1/1   Running   0        1m</strong>
    <strong>go-demo-2-db-...  0/1   OOMKilled 2        17s</strong>
  </pre>
<p>In your case, the status might not be <kbd>OOMKilled</kbd>. If so, wait for a while longer and retrieve the Pods again. The status should eventually change to <kbd>CrashLoopBackOff</kbd>.</p>
<p>As you can see, the status of the <kbd>go-demo-2-db</kbd> Pod is <kbd>OOMKilled</kbd> (<strong>Out Of Memory Killed</strong>). Kubernetes detected that the actual usage is way above the limit and it declared the Pod as a candidate for termination. The container was terminated shortly afterwards. Kubernetes will recreate the terminated container a while later only to discover that the memory usage is still above the limit. And so on, and so forth. The loop will continue.</p>
<div class="packt_infobox">A container can exceed its memory request if the node has enough available memory. On the other hand, a container is not allowed to use more memory than the limit. When that happens, it becomes a candidate for termination.</div>
<p>Let's describe the Deployment and see the status of the <kbd>db</kbd> container:</p>
<pre><strong>kubectl describe pod go-demo-2-db</strong>  </pre>
<p>The output, limited to relevant parts, is as follows:</p>
<pre><strong>...</strong>
<strong>Containers:</strong>
<strong>  db:</strong>
<strong>    ...</strong>
<strong>    Last State:     Terminated</strong>
<strong>      Reason:       OOMKilled</strong>
<strong>      Exit Code:    137</strong>
<strong>      ...</strong>
<strong>Events:</strong>
<strong>  Type    Reason  Age             From              Message</strong>
<strong>  ----    ------  ----            ----              -------</strong>
<strong>  ...</strong>
<strong>  Warning BackOff 3s (x8 over 1m) kubelet, minikube Back-off restarting failed container</strong>
  </pre>
<p>We can see that the last state of the <kbd>db</kbd> container is <kbd>OOMKilled</kbd>. When we explore the events, we can see that, so far, the container was restarted eight times with the reason <kbd>BackOff</kbd>.</p>
<p>Let's explore another possible situation through yet another updated definition:</p>
<pre><strong>cat res/go-demo-2-insuf-node.yml</strong>  </pre>
<p>Just as before, the change is only in the <kbd>resources</kbd> of the <kbd>go-demo-2-db</kbd> Deployment. The output, limited to the relevant parts, is as follows:</p>
<pre>apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-db 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: db 
        image: mongo:3.3 
        resources: 
          limits: 
            memory: 8Gi 
            cpu: 0.5 
          requests: 
            memory: 4Gi 
            cpu: 0.3 </pre>
<p>This time, we specified that the requested memory is twice as much as the total memory of the node (2GB). The memory limit is even higher.</p>
<p>Let's apply the change and observe what happens:</p>
<pre>kubectl apply \  
    -f res/go-demo-2-insuf-node.yml \ 
    --record 
 
kubectl get pods </pre>
<p>The output of the latter command is as follows:</p>
<pre><strong>NAME                           READY STATUS  RESTARTS AGE</strong>
<strong>go-demo-2-api-796db5987d-8wbk4 1/1   Running 0        8m</strong>
<strong>go-demo-2-api-796db5987d-w6mnx 1/1   Running 0        8m</strong>
<strong>go-demo-2-api-796db5987d-wtz4q 1/1   Running 0        9m</strong>
<strong>go-demo-2-db-5d5c46bc7c-d676j  0/1   Pending 0        13s</strong>  </pre>
<p>This time, the status of the Pod is <kbd>Pending</kbd>. Kubernetes could not place it anywhere in the cluster and is waiting until the situation changes.</p>
<p>Even though memory requests are associated with containers, it often makes sense to translate them into Pods requirements. We can say that the requested memory of a Pod is the sum of the requests of all the containers that form it. In our case, the Pod has only one container, so the requests of the two are equal. The same can be said for limits.</p>
<p>During the scheduling process, Kubernetes sums the requests of a Pod and looks for a node that has enough available memory and CPU. If Pod's request cannot be satisfied, it is placed in the pending state in the hope that resources will be freed on one of the nodes, or that a new server will be added to the cluster. Since such a thing will not happen in our case, the Pod created through the <kbd>go-demo-2-db</kbd> Deployment will be pending forever, unless we change the memory request again.</p>
<div class="packt_infobox">When Kubernetes cannot find enough free resources to satisfy the resource requests of all the containers that form a Pod, it changes its state to <kbd>Pending</kbd>. Such Pods will remain in this state until requested resources become available.</div>
<p>Let's describe the <kbd>go-demo-2-db</kbd> Deployment and see whether there is some additional useful information in it.</p>
<pre><strong>kubectl describe pod go-demo-2-db</strong>  </pre>
<p>The output, limited to the events section, is as follows:</p>
<pre><strong>...</strong>
<strong>Events:</strong>
<strong>  Type    Reason           Age               From              Message</strong>
<strong> ----    ------           ----              ----              -------</strong>
  <strong>Warning FailedScheduling 11s (x7 over 42s) default-scheduler 0/<br/> 1 nodes are available: 1 Insufficient memory.</strong>
  </pre>
<p>We can see that it has already <kbd>FailedScheduling</kbd> seven times and that the message clearly indicates that there is <kbd>Insufficient memory</kbd>.</p>
<p>We'll revert to the initial definition. Even though we know that its resources are incorrect, we know that it satisfies all the requirements and that all the Pods will be scheduled successfully:</p>
<pre>kubectl apply \  
    -f res/go-demo-2-random.yml \  
    --record 
 
kubectl rollout status \  
    deployment go-demo-2-db 
 
kubectl rollout status \ 
    deployment go-demo-2-api </pre>
<p>Now that all the Pods are running, we should try to write a better definition. For that, we need to observe memory and CPU usage and use that information to decide the requests and the limits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting resources based on actual usage</h1>
                </header>
            
            <article>
                
<p>We saw some of the effects that can be caused by a discrepancy between resource usage and resource specification. It's only natural that we should adjust our specification to reflect the actual memory and CPU usage better.</p>
<p>Let's start with the database:</p>
<pre>DB_POD_NAME=$(kubectl get pods \  
    -l service=go-demo-2 \  
    -l type=db \ 
    -o jsonpath="{.items[0].metadata.name}") 
 
curl "$BASE_URL/$DB_POD_NAME/containers/db/metrics/memory/usage" 
 
curl "$BASE_URL/$DB_POD_NAME/containers/db/metrics/cpu/usage_rate" </pre>
<p>We retrieved the name of the database Pod and used it to obtain memory and CPU usage of the <kbd>db</kbd> container. As a result, we now know that memory usage is somewhere between <kbd>30Mi</kbd> and <kbd>40Mi</kbd>. Similarly, we know that the CPU consumption is somewhere around <kbd>5m</kbd></p>
<p>Let's take the same metrics for the <kbd>api</kbd> container.</p>
<pre>API_POD_NAME=$(kubectl get pods \ 
    -l service=go-demo-2 \ 
    -l type=api \ 
    -o jsonpath="{.items[0].metadata.name}") 
 
curl "$BASE_URL/$API_POD_NAME/containers/api/metrics/memory/usage" 
 
curl <br/> "$BASE_URL/$API_POD_NAME/containers/api/metrics/cpu/usage_rate" </pre>
<p>As expected, an <kbd>api</kbd> container uses even less resources than MongoDB. Its memory is somewhere between <kbd>3Mi</kbd> and <kbd>7Mi</kbd>. Its CPU usage is so low that Heapster rounded it to <kbd>0m</kbd>.</p>
<p>Equipped with this knowledge, we can proceed to update our YAML definition. Still, before we do that, I need to clarify a few things.</p>
<p>The metrics we collected are based on applications that do nothing. Once they start getting real load and start hosting production size data, the metrics would change drastically. What you need is a way to predict how much resources an application will use in production, not in a simple test environment. You might be inclined to run stress tests that would simulate production setup. Do that. It's significant, but it does not necessarily result in real production-like behavior.</p>
<p>Replicating production and behavior of real users is tough. Stress tests will get you half-way. For the other half, you'll have to monitor your applications in production and, among other things, adjust resources accordingly. There are many additional things you should take into account but, for now, I wanted to stress that applications that do nothing are not a good measure of resource usage. Still, we're going to imagine that the applications we're currently running are under production-like load and that the metrics we retrieved represent how the applications would behave in production.</p>
<div class="packt_infobox">Simple test environments do not reflect production usage of resources. Stress tests are a good start, but not a complete solution. Only production provides real metrics.</div>
<p>Let's take a look at a new definition that better represents resource usage of the applications.</p>
<pre><strong>cat res/go-demo-2.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-db 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: db 
        image: mongo:3.3 
        resources: 
          limits: 
            memory: "100Mi" 
            cpu: 0.1 
          requests: 
            memory: "50Mi" 
            cpu: 0.01 
... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-api 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: api 
        image: vfarcic/go-demo-2 
        ... 
        resources: 
          limits: 
            memory: "10Mi" 
            cpu: 0.1 
          requests: 
            memory: "5Mi" 
            cpu: 0.01 </pre>
<p>That is much better. The resource requests are only slightly higher than the current usage. We set the memory limits value to double that of the requests so that the applications have ample resources for occasional (and short-lived) bursts of additional memory consumption. CPU limits are much higher than requests mostly because I was too embarrassed to put anything less than a tenth of a CPU as the limit. Anyways, the point is that requests are close to the observed usage and limits are higher so that applications have some space to breathe in case of a temporary spike in resource usage.</p>
<p>All that's left is to apply the new definition:</p>
<pre>kubectl apply \  
    -f res/go-demo-2.yml \  
    --record 
 
kubectl rollout status \ 
    deployment go-demo-2-api </pre>
<p>The <kbd>deployment "go-demo-2-api"</kbd> was successfully rolled out, and we can move onto the next subject.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring quality of service (QoS) contracts</h1>
                </header>
            
            <article>
                
<p>When we send a request to Kubernetes API to create a Pod (directly or through one of the Controllers), it initiates the scheduling process. What happens next or, to be more precise, where it will decide to run a Pod, depends hugely on the resources we defined for the containers that form the Pod. In a nutshell, Kubernetes will decide to deploy a Pod, whenever it is possible, inside one of the nodes that has enough available memory.</p>
<p>When memory requests are defined, Pods will get the memory they requested. If memory usage of one of the containers exceeds the requested amount, or if some other Pod needs that memory, the Pod hosting it might be killed. Please note that I wrote that a Pod <em>might</em> be killed. Whether that will happen depends on the requests from other Pods and the available memory in the cluster. On the other hand, containers that exceed their memory limits are always killed (unless it was a temporary situation).</p>
<p>CPU requests and limits work a bit differently. Containers that exceed specified CPU resources are not killed. Instead, they are throttled.</p>
<p>Now that we shed a bit of light around Kubernetes' killing activities, we should note that (almost) nothing happens randomly. When there aren't enough resources to serve the needs of all the Pods, Kubernetes will destroy one or more containers. The decision which one it will be is anything but random. Who will be the unlucky one depends on the assigned <strong>Quality of Service</strong> (<strong>QoS</strong>). Those with the lowest priority are killed first.</p>
<p>Since this might be the first time you heard about QoS, we'll spend some time explaining what they are and how they work.</p>
<p>Pods are the smallest units in Kubernetes. Since almost everything ends up as a Pod (one way or another), it is no wonder that Kubernetes promises specific guarantees to all the Pods running inside the cluster. Whenever we send a request to the API to create or update a Pod, it gets assigned one of the QoS classes. They are used to make decisions such as where to schedule a Pod or whether to evict it.</p>
<p>We do not specify QoS directly. Instead, they are assigned based on the decisions we make with resource requests and limits.</p>
<p>At the moment, three QoS classes are available. Each Pod can have the <em>Guaranteed</em>, the <em>Burstable</em>, or the <em>BestEffort</em> QoS.</p>
<p><strong>Guaranteed QoS</strong> is assigned only to Pods which have set both CPU requests and limits, and memory requests and limits for all of their containers. The Pods we created with the last definition match that criteria. However, there's one more necessary condition that must be met. The requests and limits values must be the same per container. Still, there is a catch. When a container specifies only limits, requests are automatically set to the same values. In other words, containers without requests will have Guaranteed QoS if their limits are defined.</p>
<p>We can summarize criteria for Guaranteed QoS as follows:</p>
<ul>
<li>Both memory and CPU limits must be set</li>
<li>Memory and CPU requests must be set to the same values as the limits, or they can be left empty, in which case they default to the limits (we'll explore them soon)</li>
</ul>
<p>Pods with Guaranteed QoS assigned are the top priority and will never be killed unless they exceed their limits or are unhealthy. They are the last to go when things go wrong. As long as their resource usage is within limits, Kubernetes will always choose to kill Pods with other QoS assignments when resource usage is over the capacity.</p>
<p>Let's move to the next QoS.</p>
<p><strong>Burstable QoS</strong> is assigned to Pods that do not meet the criteria for Guaranteed QoS but have at least one container with memory or CPU requests defined.</p>
<p>Pods with the Burstable QoS are guaranteed minimal (requested) memory usage. They might be able to use more resources if they are available. If the system is under pressure and needs more available memory, containers belonging to the Pods with the Burstable QoS are more likely to be killed than those with Guaranteed QoS when there are no Pods with the BestEffort QoS. You can consider the Pods with this QoS as medium priority.</p>
<p>Finally, we reached the last QoS.</p>
<p><strong>BestEffort QoS</strong> is given to the Pods that do not qualify as Guaranteed or Burstable. They are Pods that consist of containers that have none of the resources defined. Containers in Pods qualified as BestEffort can use any available memory they need.</p>
<p>When in need of more resources, Kubernetes will start killing containers residing in the Pods with BestEffort QoS. They are the lowest priority, and they are the first to disappear when more memory is needed.</p>
<p>Let's take a look which QoS our <kbd>go-demo-2-db</kbd> Pod got assigned.</p>
<pre><strong>kubectl describe pod go-demo-2-db</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>... 
Containers: 
  db: 
    ... 
    Limits: 
      cpu:    100m 
      memory: 100Mi 
    Requests: 
      cpu:    10m 
      memory: 50Mi 
... 
QoS Class:       Burstable 
... </pre>
<p>The Pod was assigned Burstable QoS. Its limits are different from requests, so it did not qualify for Guaranteed QoS. Since its resources are set, and it is not eligible for Guaranteed QoS, Kubernetes assigned it the second best QoS.</p>
<p>Now, let's take a look at a slightly modified definition:</p>
<pre><strong>cat res/go-demo-2-qos.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-db 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: db 
        image: mongo:3.3 
        resources: 
          limits: 
            memory: "50Mi" 
            cpu: 0.1 
          requests: 
            memory: "50Mi" 
            cpu: 0.1 
... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-api 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: api 
        image: vfarcic/go-demo-2 
... </pre>
<p>This time, we specified that both <kbd>cpu</kbd> and <kbd>memory</kbd> should have the same values for both the <kbd>requests</kbd> and the <kbd>limits</kbd> for the containers that will be created with the <kbd>go-demo-2-db</kbd> Deployment. As a result, it should be assigned Guaranteed QoS.</p>
<p>The containers of the <kbd>go-demo-2-api</kbd> Deployment are void of any <kbd>resources</kbd> definitions and, therefore, will be assigned BestEffort QoS.</p>
<p>Let's confirm that both assumptions (not to say guesses) are indeed correct.</p>
<pre>kubectl apply \  
    -f res/go-demo-2-qos.yml \ 
    --record 
 
kubectl rollout status \ 
    deployment go-demo-2-db </pre>
<p>We applied the new definition and output the rollout status of the <kbd>go-demo-2-db</kbd> Deployment.</p>
<p>Now we can describe the Pod created thought the <kbd>go-demo-2-db</kbd> Deployment and check its QoS.</p>
<pre><strong>kubectl describe pod go-demo-2-db</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>Containers: 
  db: 
    ... 
    Limits: 
      cpu:    100m 
      memory: 50Mi 
    Requests: 
      cpu:    100m 
      memory: 50Mi 
... 
QoS Class: Guaranteed 
... </pre>
<p>Memory and CPU limits and requests are the same and, as a result, the QoS is <kbd>Guaranteed</kbd>.</p>
<p>Let's check the QoS of the Pods created through the <kbd>go-demo-2-api</kbd> Deployment.</p>
<pre><strong>kubectl describe pod go-demo-2-api</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>... 
QoS Class:       BestEffort 
... 
QoS Class:       BestEffort 
... 
QoS Class:       BestEffort 
... </pre>
<p>The three Pods created through the <kbd>go-demo-2-api</kbd> Deployment are without any resources definitions and, therefore, their QoS is set to <kbd>BestEffort</kbd>.</p>
<p>We won't be needing the objects we created so far so we'll remove them before moving onto the next subject.</p>
<pre><strong>kubectl delete \</strong>
<strong>    -f res/go-demo-2-qos.yml</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining resource defaults and limitations within a namespace</h1>
                </header>
            
            <article>
                
<p>We already learned how to leverage Kubernetes namespaces to create clusters within a cluster. When combined with RBAC, we can create namespaces and give users permissions to use them without exposing the whole cluster. Still, one thing is missing.</p>
<p>We can, let's say, create a <kbd>test</kbd> namespace and allow users to create objects without permitting them to access other namespaces. Even though that is better than allowing everyone full access to the cluster, such a strategy would not prevent people from bringing the whole cluster down or affecting the performance of applications running in other namespaces. The piece of the puzzle we're missing is resource control on the namespace level.</p>
<p>We already discussed that every container should have resource <kbd>limits</kbd> and <kbd>requests</kbd> defined. That information helps Kubernetes schedule Pods more efficiently. It also provides it with the information it can use to decide whether a Pod should be evicted or restarted. Still, the fact that we can specify <kbd>resources</kbd> does not mean that we are forced to define them. We should have the ability to set default <kbd>resources</kbd> that will be applied when we forget to specify them explicitly.</p>
<p>Even if we define default <kbd>resources</kbd>, we also need a way to set limits. Otherwise, everyone with permissions to deploy a Pod can potentially run an application that requests more resources than we're willing to give.</p>
<p>All in all, our next task is to define default requests and limits as well as to specify minimum and maximum values someone can define for a Pod.</p>
<p>We'll start by creating a <kbd>test</kbd> Namespace.</p>
<pre><strong>kubectl create namespace test</strong>  </pre>
<p>With a playground namespace created, we can take a look at a new definition.</p>
<pre><strong>cat res/limit-range.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre>apiVersion: v1 
kind: LimitRange 
metadata: 
  name: limit-range 
spec: 
  limits: 
  - default: 
      memory: 50Mi 
      cpu: 0.2 
    defaultRequest: 
      memory: 30Mi 
      cpu: 0.05 
    max: 
      memory: 80Mi 
      cpu: 0.5 
    min: 
      memory: 10Mi 
      cpu: 0.01 
    type: Container </pre>
<p>We specified that the resource should be of <kbd>LimitRange</kbd> kind. It's <kbd>spec</kbd> has four <kbd>limits</kbd>.</p>
<p>The <kbd>default</kbd> limit and <kbd>defaultRequest</kbd> entries will be applied to the containers that do not specify resources. If a container does not have memory or CPU limits, it'll be assigned the values set in the <kbd>LimitRange</kbd>. The <kbd>default</kbd> entries are used as limits, and the <kbd>defaultRequest</kbd> entries are used as requests.</p>
<p>When a container does have the resources defined, they will be evaluated against <kbd>LimitRange</kbd> thresholds specified as <kbd>max</kbd> and <kbd>min</kbd>. If a container does not meet the criteria, the Pod that hosts the containers will not be created.</p>
<p>We'll see a practical implementation of the four <kbd>limits</kbd> soon. For now, the next step is to create the <kbd>limit-range</kbd> resource:</p>
<pre>kubectl --namespace test create \  
    -f res/limit-range.yml \  
    --save-config --record </pre>
<p>We created the <kbd>LimitRange</kbd> resource.</p>
<p>Let's describe the <kbd>test</kbd> namespace where the resource was created.</p>
<pre><strong>kubectl describe namespace test</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows.</p>
<pre><strong>...</strong>
<strong>Resource Limits</strong>
 <strong>Type      Resource Min  Max  Default Request Default Limit  Max  Limit/Request Ratio</strong>
<strong> ----      -------- ---  ---  --------------- ------------- ----- ------------------</strong>
 <strong>Container cpu      10m  500m 50m             200m          -</strong>
 <strong>Container memory   10Mi 80Mi 30Mi            50Mi          -</strong> </pre>
<p>We can see that the <kbd>test</kbd> namespace has the resource limits we specified. We set four out of five possible values. The <kbd>maxLimitRequestRatio</kbd> is missing and we'll describe it only briefly. When <kbd>MaxLimitRequestRatio</kbd> is set, container request and limit resources must both be non-zero, and the limit divided by the request must be less than or equal to the enumerated value.</p>
<p>Let's take a look at yet another variation of the <kbd>go-demo</kbd> definition:</p>
<pre><strong>cat res/go-demo-2-no-res.yml</strong>  </pre>
<p>The only thing to note is that none of the containers have any resources defined.</p>
<p>Next, we'll create the objects defined in the <kbd>go-demo-2-no-res.yml</kbd> file.</p>
<pre>kubectl --namespace test create \  
    -f res/go-demo-2-no-res.yml \ 
    --save-config --record 
 
kubectl --namespace test \ 
    rollout status \ 
    deployment go-demo-2-api </pre>
<p>We created the objects inside the <kbd>test</kbd> namespace and waited until the <kbd>deployment "go-demo-2-api"</kbd> was successfully rolled out.</p>
<p>Let's describe one of the Pods we created:</p>
<pre>kubectl --namespace test describe \  
    pod go-demo-2-db </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>... 
Containers: 
  db: 
    ... 
    Limits: 
      cpu:     200m 
      memory:  50Mi 
    Requests: 
      cpu:        50m 
      memory:     30Mi 
... </pre>
<p>Even though we did not specify the resources of the <kbd>db</kbd> container inside the <kbd>go-demo-2-db</kbd> Pod, the resources are set. The <kbd>db</kbd> container was assigned the <kbd>default</kbd> limits of the <kbd>test</kbd> Namespace as the container limit. Similarly, the <kbd>defaultRequest</kbd> limits were used as container requests.</p>
<p>As we can see, any attempt to create Pods hosting containers without resources will result in the namespace limits applied.</p>
<div class="packt_infobox">We should still define container resources instead of relying on namespace default limits. They are, after all, only a fallback in case someone forgot to define resources.</div>
<p>Let's see what happens when resources are defined, but they do not match the namespace <kbd>min</kbd> and <kbd>max</kbd> limits.</p>
<p>We'll use the same <kbd>go-demo-2.yml</kbd> we used before.</p>
<pre><strong>cat res/go-demo-2.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-db 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: db 
        image: mongo:3.3 
        resources: 
          limits: 
            memory: "100Mi" 
            cpu: 0.1 
          requests: 
            memory: "50Mi" 
            cpu: 0.01 
... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-api 
spec: 
  ... 
  template: 
    ... 
    spec: 
      containers: 
      - name: api 
        ... 
        resources: 
          limits: 
            memory: "10Mi" 
            cpu: 0.1 
          requests: 
            memory: "5Mi" 
            cpu: 0.01 
... </pre>
<p>What matters is that the <kbd>resources</kbd> for both Deployments are defined.</p>
<p>Let's create the objects and retrieve the events. They will help us understand better what is happening.</p>
<pre><strong>kubectl --namespace test apply \</strong>
<strong>    -f res/go-demo-2.yml \</strong>
<strong>    --record</strong>
    
<strong>kubectl --namespace test get events -w</strong>  </pre>
<p>The output of the latter command, limited to the relevant parts, is as follows:</p>
<pre>    <strong>... Error creating: pods "go-demo-2-db-868dbbc488-s92nm" is forbi<br/> dden: maximum memory usage per Container is 80Mi, but limit is 100Mi.</strong>
    <strong>...</strong>
    <strong>... Error creating: pods "go-demo-2-api-6bd767ffb6-96mbl" is <br/> forbidden: minimum memory usage per Container is 10Mi, but request is <br/> 5Mi.</strong>
    <strong>...</strong>
  </pre>
<p>We can see that we are forbidden from creating either of the two Pods. The difference between those events is in what caused Kubernetes to reject our request.</p>
<p>The <kbd>go-demo-2-db-*</kbd> Pod could not be created because its <kbd>maximum memory usage per Container is 80Mi, but limit is 100Mi</kbd>. On the other hand, we are forbidden from creating the <kbd>go-demo-2-api-*</kbd> Pods because the <kbd>minimum memory usage per Container is 10Mi, but request is 5Mi</kbd>.</p>
<p>All the containers within the <kbd>test</kbd> namespace will have to comply with the <kbd>min</kbd> and <kbd>max</kbd> limits. Otherwise, we are forbidden from creating them. Container limits cannot be higher than the namespace <kbd>max</kbd> limits. On the other hand, container resource requests cannot be smaller than namespace <kbd>min</kbd> limits.</p>
<p>If we think about namespace limits as lower and upper thresholds, we can say that container requests cannot be below them, and that container limits can't be above.</p>
<p>Press the <em>Ctrl</em> + <em>C</em> keys to stop watching the events.</p>
<p>It might be easier to observe the effects of the <kbd>max</kbd> and <kbd>min</kbd> limits if we create Pods directly, instead of through Deployments.</p>
<pre>kubectl --namespace test run test \  
    --image alpine \ 
    --requests memory=100Mi \ 
    --restart Never \  
    sleep 10000 </pre>
<p>We tried to create a Pod with the memory request set to <kbd>100Mi</kbd>. Since the namespace limit is <kbd>80Mi</kbd>, the API returned the error message stating that the <kbd>Pod "test" is invalid</kbd>. Even though the <kbd>max</kbd> limit refers to container <kbd>limit</kbd>, memory request was used in its absence.</p>
<p>We'll run a similar exercise but, this time, with only <kbd>1Mi</kbd> set as memory request.</p>
<pre>kubectl --namespace test run test \ 
    --image alpine \ 
    --requests memory=1Mi \  
    --restart Never \ 
    sleep 10000 </pre>
<p>This time, the error is slightly different. We can see that <kbd>pods "test" is forbidden: minimum memory usage per Container is 10Mi, but request is 1Mi</kbd>. What we requested is below the <kbd>min</kbd> limit of the <kbd>test</kbd> namespace and, therefore, we are forbidden from creating the Pod.</p>
<p>We'll delete the <kbd>test</kbd> namespace before we move into the next subject.</p>
<pre><strong>kubectl delete namespace test</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining resource quotas for a namespace</h1>
                </header>
            
            <article>
                
<p>Resource defaults and limitations are a good first step towards preventing malicious or accidental deployment of Pods that can potentially produce adverse effects on the cluster. Still, any user with the permissions to create Pods in a namespace can overload the system. Even if <kbd>max</kbd> values are set to some reasonably small amount of memory and CPU, a user could deploy thousands, or even millions of Pods, and "eat" all the available cluster resources. Such an effect might not be even produced out of malice but accidentally. A Pod might be attached to a system that scales it automatically without defining upper bounds and, before we know it, it might scale to too many replicas. There are also many other ways things might get out of control.</p>
<p>What we need is to define namespace boundaries through quotas.</p>
<p>With quotas, we can guarantee that each namespace gets its fair share of resources. Unlike <kbd>LimitRange</kbd> rules that are applied to each container, <kbd>ResourceQuota</kbd> defines namespace limits based on aggregate resource consumption.</p>
<p>We can use <kbd>ResourceQuota</kbd> objects to define the total amount of compute resources (memory and CPU) that can be spent in a namespace. We can also use it to limit storage utilization or the number of objects of a certain type that can be created in a namespace.</p>
<p>Let's take a look at the cluster resources we have in our Minikube cluster. It is small, and it's not even a real cluster. However, it's the only one we have (for now), so please use your imagination and pretend that it's "real".</p>
<p>Our cluster has 2 CPUs and 2 GB of memory. Now, let's say that this cluster serves only development and production purposes. We can use the <kbd>default</kbd> namespace for production and create a <kbd>dev</kbd> namespace for development. We can assume that the production should consume all the resources of the cluster minus those given to the <kbd>dev</kbd> namespace which, on the other hand, should not exceed a specific limit.</p>
<p>The truth is that with 2 CPUs and 2 GB of memory, there isn't much we can give to developers. Still, we'll try to be generous. We'll give them 500 MB and 0.8 CPUs for requests. We'll allow occasional bursts in resource usage by defining limits of 1 CPU and 1 GB of memory. Furthermore, we might want to limit the number of Pods to ten. Finally, as a way to reduce risks, we will deny developers the right to expose node ports.</p>
<p>Isn't that a decent plan? I'll imagine that, at this moment, you are nodding as a sign of approval so we'll move on and create the quotas we discussed.</p>
<p>Let's take a look at the <kbd>dev.yaml</kbd> definition:</p>
<pre><strong>cat res/dev.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>kind: Namespace</strong>
<strong>metadata:</strong>
<strong>  name: dev</strong>
    
<strong>---</strong>
    
<strong>apiVersion: v1</strong>
<strong>kind: ResourceQuota</strong>
<strong>metadata:</strong>
  <strong>name: dev</strong>
  <strong>namespace: dev</strong>
<strong>spec:</strong>
  <strong>hard:</strong>
    <strong>requests.cpu: 0.8</strong>
    <strong>requests.memory: 500Mi</strong>
    <strong>limits.cpu: 1</strong>
    <strong>limits.memory: 1Gi</strong>
    <strong>pods: 10</strong>
    <strong>services.nodeports: "0"</strong>  </pre>
<p>Besides creating the <kbd>dev</kbd> namespace, we're also creating a <kbd>ResourceQuota</kbd>. It specifies a set of <kbd>hard</kbd> limits. Remember, they are based on aggregated data, and not on per-container basis like LimitRanges.</p>
<p>We set requests quotas to <kbd>0.8</kbd> CPUs and <kbd>500Mi</kbd> of RAM. Similarly, limit quotas as set to <kbd>1</kbd> CPU and <kbd>1Gi</kbd> of memory. Finally, we specified that the <kbd>dev</kbd> namespace can have only <kbd>10</kbd> Pods and that there can be no NodePorts. That's the plan we formulated and defined. Now let's create the objects and explore the effects.</p>
<pre><strong>kubectl create \ </strong>
<strong>    -f res/dev.yml \ </strong>
<strong>    --record --save-config</strong>  </pre>
<p>We can see from the output that the <kbd>namespace "dev"</kbd> was created as well as the <kbd>resourcequota "dev"</kbd>. To be on the safe side, we'll describe the newly created <kbd>devquota</kbd>.</p>
<pre>kubectl --namespace dev describe \ 
    quota dev </pre>
<p>The output is as follows:</p>
<pre><strong>Name:               dev</strong>
<strong>Namespace:          dev</strong>
<strong>Resource            Used  Hard</strong>
<strong>--------            ----  ----</strong>
<strong>limits.cpu          0     1</strong>
<strong>limits.memory       0     1Gi</strong>
<strong>pods                0     10</strong>
<strong>requests.cpu        0     800m</strong>
<strong>requests.memory     0     500Mi</strong>
<strong>services.nodeports  0     0</strong></pre>
<p>We can see that the hard limits are set and that there's currently no usage. That was to be expected since we're not running any objects in the <kbd>dev</kbd> namespace. Let's spice it up a bit by creating the already too familiar <kbd>go-demo-2</kbd> objects.</p>
<pre><strong>kubectl --namespace dev create \</strong>
<strong>    -f res/go-demo-2.yml \ </strong>
<strong>    --save-config --record</strong>
    
<strong>kubectl --namespace dev \ </strong>
<strong>    rollout status \ </strong>
<strong>    deployment go-demo-2-api</strong>  </pre>
<p>We created the objects from the <kbd>go-demo-2.yml</kbd> file and waited until the <kbd>go-demo-2-api</kbd> Deployment rolled out. Now we can revisit the values of the <kbd>dev</kbd> quota:</p>
<pre>kubectl --namespace dev describe \ 
    quota dev </pre>
<p>The output is as follows:</p>
<pre><strong>Name:              dev</strong>
<strong>Namespace:         dev</strong>
<strong>Resource           Used  Hard</strong>
<strong>--------           ----  ----</strong>
<strong>limits.cpu         400m  1</strong>
<strong>limits.memory      130Mi 1Gi</strong>
<strong>pods               4     10</strong>
<strong>requests.cpu       40m   800m</strong>
<strong>requests.memory    65Mi  500Mi</strong>
<strong>services.nodeports 0     0</strong>  </pre>
<p>Judging from the <kbd>Used</kbd> column, we can see that we are, for example, currently running <kbd>4</kbd> Pods and that we are still below the limit of <kbd>10</kbd>. One of those Pods was created through the <kbd>go-demo-2-db</kbd> Deployment, and the other three with the <kbd>go-demo-2-api</kbd>. If you summarize resources we specified for the containers that form those Pods, you'll see that the values match the used <kbd>limits</kbd> and <kbd>requests</kbd>.</p>
<p>So far, we did not reach any of the quotas. Let's try to break at least one of them.</p>
<pre><strong>cat res/go-demo-2-scaled.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre>... 
apiVersion: apps/v1beta2 
kind: Deployment 
metadata: 
  name: go-demo-2-api 
spec: 
  replicas: 15 
  ... </pre>
<p>The definition of the <kbd>go-demo-2-scaled.yml</kbd> is almost the same as the one in <kbd>go-demo-2.yml</kbd>. The only difference is that the number of replicas of the <kbd>go-demo-2-api</kbd> Deployment is increased to fifteen. As you already know, that should result in fifteen Pods created through that Deployment.</p>
<p>I'm sure you can guess what will happen if we apply the new definition. We'll do it anyway.</p>
<pre><strong>kubectl --namespace dev apply \</strong>
<strong>    -f res/go-demo-2-scaled.yml \</strong>
<strong>    --record</strong></pre>
<p>We applied the new definition. We'll give Kubernetes a few moments to do the work before we take a look at the events it'll generate. So, take a deep breath and count from one to the number of processors in your Laptop. In my case, it's one Mississippi, two Mississippi, three Mississippi, all the way until sixteen Mississippi.</p>
<pre><strong>kubectl --namespace dev get events</strong>  </pre>
<p>The output of a few of the events generated inside the <kbd>dev</kbd> namespace is as follows:</p>
<pre><strong>...</strong>
<strong>... Error creating: pods "go-demo-2-api-..." is forbidden: exceeded quota: dev, requested: limits.cpu=100m,pods=1, used: limits.cpu=1,pods=10, limited: limits.cpu=1,pods=10</strong>
    <strong>13s         13s          1         go-demo-2-api-6bd767ffb6.150f5   1f4b3a7ed3f         ReplicaSet                          Warning .<br/> .. Error creating: pods "go-demo-2-api-..." is forbidden: exceeded quota: dev, requested: limits.cpu=100m,pods=1, used: limits.cpu=1,pods=10, limited: limits.cpu=1,pods=10</strong>
<strong>...</strong> </pre>
<p>We can see that we reached two of the limits imposed by the namespace quota. We reached the maximum amount of CPU (<kbd>1</kbd>) and Pods (<kbd>10</kbd>). As a result, ReplicaSet controller was forbidden from creating new Pods.</p>
<p>We should be able to confirm which hard limits were reached by describing the <kbd>dev</kbd> namespace.</p>
<pre><strong>kubectl describe namespace dev</strong>  </pre>
<p>The output, limited to the <kbd>Resource Quotas</kbd> section, is as follows:</p>
<pre>... 
Resource Quotas 
 Name:               dev 
 Resource            Used   Hard 
 --------            ---    --- 
 limits.cpu          1      1 
 limits.memory       190Mi  1Gi 
 pods                10     10 
 requests.cpu        100m   800m 
 requests.memory     95Mi   500Mi 
 services.nodeports  0      0 
... </pre>
<p>As the events showed us, the values of <kbd>limits.cpu</kbd> and <kbd>pods</kbd> resources are the same in both <kbd>User</kbd> and <kbd>Hard</kbd> columns. As a result, we won't be able to create any more Pods, nor will we be allowed to increase CPU limits for those that are already running.</p>
<p>Finally, let's take a look at the Pods inside the <kbd>dev</kbd> namespace.</p>
<pre><strong>kubectl get pods --namespace dev</strong>  </pre>
<p>Following is the output of the preceding command:</p>
<pre><strong>NAME              READY STATUS  RESTARTS AGE</strong>
<strong>go-demo-2-api-... 1/1   Running 0        3m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        3m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        5m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        3m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        5m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        3m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        3m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        3m</strong>
<strong>go-demo-2-api-... 1/1   Running 0        5m</strong>
<strong>go-demo-2-db-...  1/1   Running 0        5m</strong>  </pre>
<p>The <kbd>go-demo-2-api</kbd> Deployment managed to create nine Pods. Together with the Pod created through the <kbd>go-demo-2-db</kbd>, we reached the limit of ten.</p>
<p>We confirmed that the limit and the Pod quotas work. We'll revert to the previous definition (the one that does not reach any of the quotas) before we move onto the next verification.</p>
<pre><strong>kubectl --namespace dev apply \</strong>
<strong>    -f res/go-demo-2.yml \</strong>
<strong>    --record</strong>
    
<strong>kubectl --namespace dev \</strong>
<strong>    rollout status \</strong>
<strong>    deployment go-demo-2-api</strong>  </pre>
<p>The output of the latter command should indicate that the <kbd>deployment "go-demo-2-api" was successfully rolled out</kbd>.</p>
<p>Let's take a look at yet another slightly modified definition of the <kbd>go-demo-2</kbd> objects:</p>
<pre><strong>cat res/go-demo-2-mem.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>...</strong>
<strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-db</strong>
<strong>spec:</strong>
<strong>  ...</strong>
<strong>  template:</strong>
<strong>    ...</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: db</strong>
<strong>        image: mongo:3.3</strong>
<strong>        resources:</strong>
<strong>          limits:</strong>
<strong>            memory: "100Mi"</strong>
<strong>            cpu: 0.1</strong>
<strong>          requests:</strong>
<strong>            memory: "50Mi"</strong>
<strong>            cpu: 0.01</strong>
<strong>...</strong>
<strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-api</strong>
<strong>spec:</strong>
<strong>  replicas: 3</strong>
<strong>  ...</strong>
<strong>  template:</strong>
<strong>    ...</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: api</strong>
<strong>        ...</strong>
<strong>        resources:</strong>
<strong>          limits:</strong>
<strong>            memory: "200Mi"</strong>
<strong>            cpu: 0.1</strong>
<strong>          requests:</strong>
<strong>            memory: "200Mi"</strong>
<strong>            cpu: 0.01</strong>
<strong>...</strong>  </pre>
<p>Both memory request and limit of the <kbd>api</kbd> container of the <kbd>go-demo-2-api</kbd> Deployment is set to <kbd>200Mi</kbd> while the database remains with the memory request of <kbd>50Mi</kbd>. Knowing that the <kbd>requests.memory</kbd> quota of the <kbd>dev</kbd> namespace is <kbd>500Mi</kbd>, it's enough to do simple math and come to the conclusion that we won't be able to run all three replicas of the <kbd>go-demo-2-api</kbd> Deployment.</p>
<pre><strong>kubectl --namespace dev apply \</strong>
<strong>    -f res/go-demo-2-mem.yml \</strong>
<strong>    --record</strong>  </pre>
<p>Just as before, we should wait for a while before taking a look at the events of the <kbd>dev</kbd> namespace.</p>
<pre><strong>kubectl --namespace dev get events \</strong>
<strong>    | grep mem</strong>  </pre>
<p>The output, limited to one of the entries, is as follows:</p>
<pre><strong>... Error creating: pods "go-demo-2-api-..." is forbidden: exceeded quota: dev, requested: requests.memory=200Mi, used: requests.memory=455Mi, limited: requests.memory=500Mi</strong></pre>
<p>We reached the quota of the <kbd>requests.memory</kbd>. As a result, creation of at least one of the Pods is forbidden. We can see that we requested creation of a Pod that requests <kbd>200Mi</kbd> of memory. Since the current summary of the memory requests is <kbd>455Mi</kbd>, creating that Pod would exceed the allocated <kbd>500Mi</kbd>.</p>
<p>Let's take a closer look at the namespace.</p>
<pre><strong>kubectl describe namespace dev</strong>  </pre>
<p>The output, limited to the <kbd>Resource Quotas</kbd> section, is as follows:</p>
<pre><strong>...</strong>
<strong>Resource Quotas</strong>
<strong> Name:               dev</strong>
<strong> Resource            Used   Hard</strong>
<strong> --------            ---    ---</strong>
<strong> limits.cpu          400m   1</strong>
<strong> limits.memory       510Mi  1Gi</strong>
<strong> pods                4      10</strong>
<strong> requests.cpu        40m    800m</strong>
<strong> requests.memory     455Mi  500Mi</strong>
<strong> services.nodeports  0      0</strong>
<strong>...</strong>  </pre>
<p>Indeed, the amount of used memory requests is <kbd>455Mi</kbd>, meaning that we could create additional Pods with up to <kbd>45Mi</kbd>, not <kbd>200Mi</kbd>.</p>
<p>We'll revert to the <kbd>go-demo-2.yml</kbd> one more time before we explore the last quota we defined.</p>
<pre><strong>kubectl --namespace dev apply \ </strong>
<strong>    -f res/go-demo-2.yml \ </strong>
<strong>    --record</strong>
    
<strong>kubectl --namespace dev \</strong>
<strong>    rollout status \</strong>
<strong>    deployment go-demo-2-api</strong>  </pre>
<p>The only quota we did not yet verify is <kbd>services.nodeports</kbd>. We set it to <kbd>0</kbd> and, as a result, we should not be allowed to expose any node ports. Let's confirm that is indeed true.</p>
<pre><strong>kubectl expose deployment go-demo-2-api \</strong>
<strong>    --namespace dev \</strong>
<strong>    --name go-demo-2-api \ </strong>
<strong>    --port 8080 \</strong>
<strong>    --type NodePort</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>Error from server (Forbidden): services "go-demo-2-api" is forbidden: exceeded quota: dev, requested: services.nodeports=1, used: services.nodeports=0, limited: services.nodeports=0</strong></pre>
<p>All our quotas work as expected. But, there are others. We won't have time to explore examples of all the quotas we can use. Instead, we'll list them all for future reference.</p>
<p>We can divide quotas into several groups:</p>
<p><strong>Compute resource quotas</strong> limit the total sum of the compute resources. They are as follows:</p>
<table border="1" class="MsoTableGrid" style="width: 642px;height: 797px">
<tbody>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>Resource name</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>Description</strong></div>
</td>
</tr>
<tr>
<td>
<p><kbd>cpu</kbd></p>
</td>
<td>
<p>Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>limits.cpu</kbd></p>
</td>
<td>
<p>Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>limits.memory</kbd></p>
</td>
<td>
<p>Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>memory</kbd></p>
</td>
<td>
<p>Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.cpu</kbd></p>
</td>
<td>
<p>Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.memory</kbd></p>
</td>
<td>
<p>Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><strong>Storage resource quotas</strong> limit the total sum of the storage resources. We did not yet explore storage (beyond a few local examples) so you might want to keep the list that follows for future reference:</p>
<table border="1" class="MsoTableGrid" style="width: 642px;height: 797px">
<tbody>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>Resource name</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>Description</strong></div>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.storage</kbd></p>
</td>
<td>
<p>Across all persistent volume claims, the sum of storage requests cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>persistentvolumeclaims</kbd></p>
</td>
<td>
<p>The total number of persistent volume claims that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>[PREFIX]/requests.storage</kbd></p>
</td>
<td>
<p>Across all persistent volume claims associated with the storage-class-name, the sum of storage requests cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>[PREFIX]/persistentvolumeclaims</kbd></p>
</td>
<td>
<p>Across all persistent volume claims associated with the storage-class-name, the total number of persistent volume claims that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>requests.ephemeral-storage</kbd></p>
</td>
<td>
<p>Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>limits.ephemeral-storage</kbd></p>
</td>
<td>
<p>Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Please note that <kbd>[PREFIX]</kbd> should be replaced with <kbd>&lt;storage-class-name&gt;.storageclass.storage.k8s.io</kbd>.</p>
<p><strong>Object count quotas</strong> limit the number of objects of a given type. They are as follows:</p>
<table border="1" class="MsoTableGrid" style="width: 642px;height: 797px">
<tbody>
<tr>
<td style="width: 230px">
<div class="CDPAlignCenter CDPAlign"><strong>Resource name</strong></div>
</td>
<td style="width: 396px">
<div class="CDPAlignCenter CDPAlign"><strong>Description</strong></div>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>configmaps</kbd></p>
</td>
<td style="width: 396px">
<p class="CDPAlignLeft CDPAlign">The total number of config maps that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>persistentvolumeclaims</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of persistent volume claims that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>pods</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if status.phase in (Failed, Succeeded) is true.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>replicationcontrollers</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of replication controllers that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>resourcequotas</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of resource quotas that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>services</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of services that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>services.loadbalancers</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of services of type load balancer that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>services.nodeports</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of services of type node port that can exist in the namespace.</p>
</td>
</tr>
<tr>
<td style="width: 230px">
<p><kbd>secrets</kbd></p>
</td>
<td style="width: 396px">
<p>The total number of secrets that can exist in the namespace.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What now?</h1>
                </header>
            
            <article>
                
<p>Wasn't that a ride?</p>
<p>Kubernetes relies heavily on available resources spread throughout the cluster. Still, it cannot do magic. We need to help it out by defining resources we expect our containers will consume.</p>
<p>Even though Heapster is not the best solution for collecting metrics, it is already available in our Minikube cluster, and we used it to learn how much resources our applications use and, through that information, we refined our resource definitions. Without metrics, our definitions are pure guesses. When we guess, Kubernetes needs to guess as well. A stable system is a predictable system based on facts, not someone's imagination. Heapster helped us transform our assumptions into measurable facts which we fed into Kubernetes which, in turn, used them in its scheduling algorithms.</p>
<p>Exploration of resource definitions led us to <strong>Quality Of Service</strong> (<strong>QoS</strong>). Even though Kubernetes decides which QoS will be used, knowing the rules used in the decision process is essential if we are to prioritize applications and their availability.</p>
<p>All that leads us to the culmination of the strategies that make our clusters secure, stable, and robust. Dividing a cluster into Namespaces and employing RBAC is not enough. RBAC prevents unauthorized users from accessing the cluster and provides permissions to those we trust. However, RBAC does not prevent users from accidentally (or intentionally) putting the cluster in danger through too many deployments, too big applications, or inaccurate sizing. Only by combining RBAC with resource defaults, limitations, and quotas can we hope for a fault tolerant and robust cluster capable of reliably hosting our applications.</p>
<p>We learned almost all the essential Kubernetes objects and principles. The time has come to move to a "real" cluster. We are about to delete the Minikube cluster for the last time (at least in this book).</p>
<pre><strong>minikube delete</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes resource management compared to docker swarm equivalent</h1>
                </header>
            
            <article>
                
<p>Resource management can be divided into a few categories. We need to define how much memory and CPU we except a container will use and what are the limits. This information is crucial for a scheduler to make "intelligent" decisions when calculating where to place containers. In this aspect, there is no essential difference between Kubernetes and Docker Swarm. Both are using requested resources to decide where to deploy containers and limits when to evict them. Both of them are, more or less, the same in this aspect.</p>
<p>How can we know how much memory and CPU to dedicate to each of our containers? That's one of the questions I heard way too many times. The answer is simple. Collect metrics, evaluate them, adjust resources, take a break, repeat. Where do we collect metrics? Wherever you want. Prometheus is a good choice. Where will it get metrics? Well, it depends which scheduler you use. If it's Docker Swarm, you'll need to run a bunch of exporters. Or, you might be brave enough and try the experimental feature that exposes Docker's internal metrics in Prometheus format. You might even be enthusiastic enough to think that they will be enough for all your monitoring and alerting needs. Maybe, by the time you read this, the feature is not experimental anymore. On the other hand, Kubernetes has it all, and so much more. You can use Heapster, or you might discover that it is too limiting and configure Prometheus to scrape metrics directly from Kubernetes API. Kubernetes exposes a vast amount of data. More than you'll probably ever need. You will be able to fetch memory, CPU, IO, network, and a myriad of other metrics and make intelligent decisions not only about the resources your containers require but about so much more.</p>
<p>To make things clear, you can get the same metrics no matter whether you're running Kubernetes or Docker Swarm. The major difference is that Kubernetes exposes them through its API, while with Swarm you'll have to struggle between the decisions whether to use its limited metrics or go into the trouble of setting up the exporters like cAdvisor and Node Exporter. Most likely, you'll discover that you'll need both the metrics from Swarm's API and those from the exporters. Kubernetes has a more robust solution, even though you might still need an exporter or two. Still, having most, if not all, of the metrics you'll need from its API is a handy thing to have.</p>
<p>Frankly, the differences in the way we retrieve metrics from the two schedulers are not of great importance. If this would be where the story about resources ends, I'd conclude that both solutions are, more or less, equally good. But, the narrative continues. This is where similarities stop. Or, to be more precise, this is where Docker Swarm ends, and Kubernetes only just began.</p>
<p>Kubernetes allows us to define resource defaults and limitations that are applied to containers that do not specify resources. It allows us to specify resource quotas that prevent accidental or malicious over-usage of resources. The two combined with Namespaces provide very powerful safeguards. They give us some of the means with which we can design the system that is truly fault tolerant by preventing rogue containers, uncontrolled scaling, and human errors from bringing our clusters to a grinding halt. Don't think, even for a second, that quotas are the only thing required for building a robust system. It isn't the only piece of the puzzle, but it is a significant one never the less.</p>
<p>Namespaces combined with quotas are important. I'd even say that they are crucial. Without them, we would be forced to create a cluster for every group, team, or a department in our organizations. Or, we might have to resort to further tightening of the processes that prevent our teams from exploiting the benefits behind container orchestrators. If the goal is to provide freedom to our teams without sacrificing cluster stability, Kubernetes has a clear edge over Docker Swarm.</p>
<p>This battle is won by Kubernetes, but the war still rages.</p>
<p>OK. I exaggerated a bit with the words <em>battle</em> and <em>war</em>. It's not a conflict, and both communities are increasing collaboration and sharing ideas and solutions. Both platforms are merging. Still, for now, Kubernetes has a clear edge over Docker Swarm on the subject of resource management.</p>


            </article>

            
        </section>
    </body></html>