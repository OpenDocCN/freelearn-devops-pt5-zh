- en: Chapter 8. Testing Network Changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on an important part of the software development lifecycle
    as well as DevOps, testing, and quality assurance. This chapter will describe
    why it is essential to incorporate network changes as part of the continuous integration
    process and test them thoroughly. It will then go on to look at open source test
    tooling that is available to facilitate the creation of tests suites for network
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on the overall quality assurance process, outlining
    some of the best-practice approaches that can be adopted by network teams or teams
    implementing network operations.
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at the benefits of implementing feedback loops, quality reporting,
    and what checks can be implemented to make sure that the network is functioning
    as expected. These are all essential topics as network teams move toward code-driven
    network operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality assurance best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available test tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways to ensure quality when making operational or development
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'When combining quality checks and ordering them, they can be used to form a
    set of quality gates that development, infrastructure, or even network changes
    should flow through before they reach production. We will briefly touch upon some
    of the more popular testing strategies that are used to ensure that any changes
    to a system or application are operating effectively, comprised of the following
    phases of testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing overview](img/B05559_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unit testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most popular types of quality assurance is the unit test. A **unit
    test** will test each isolated code operation and make sure that each method or
    function exhibits the desired behavior with different inputs.
  prefs: []
  type: TYPE_NORMAL
- en: One or more unit tests will be required to make sure that a method or function
    works as desired. So multiple unit tests may need to be written to test any basic
    operation asserting either a pass or failure based on one isolated operation.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests can normally be carried out against compiled binaries, as opposed
    to requiring a fully-fledged test environment. Utilizing popular test frameworks,
    unit tests can be used to assert a pass or failure based on input.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a unit test for an Apache Tomcat web server could involve making
    sure that the code can serve traffic on HTTP port 8080:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unit testing](img/B05559_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Component testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Component testing** involves testing a single component in isolation and
    making sure that it behaves the way it should as a self-contained entity.'
  prefs: []
  type: TYPE_NORMAL
- en: Component testing normally involves deploying an application to a test environment
    and executing a suite of tests against the component that tests all its features
    and functionality. Microservice applications are small components which need to
    be tested each time they are released.
  prefs: []
  type: TYPE_NORMAL
- en: This may involve making sure a banking application can process transactions
    correctly based on a specific type of account.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Integration testing** involves more than one microservice component, so if
    two different components are integrated, a set of integration tests needs to be
    written to make sure they both integrate and exhibit the desired behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing normally requires a simulation of a database schema or multiple
    components to be deployed in an environment and tested together. While a unit
    test can assert the behavior of the build binaries, an integration test is slightly
    more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking or stubbing can be carried out in order to simulate another application's
    endpoint behavior and assert if it is operating as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integration testing could test that two different microservice endpoints can
    be connected and that a transaction such as a TCP handshake can be completed correctly
    between the initiator service and the receiver service, with the reception of
    ACK making sure that the two-way TCP handshake is working correctly between the
    two microservice applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integration testing](img/B05559_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: System testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**System testing** is normally carried out on a full-blown environment with
    a set of fully deployed components. System testing will test the whole system
    and is normally utilized as a final step before production. Some of the tests
    that can be carried out are user journey tests such as setting up a full transaction.
    This tests that the fully integrated system can pass all the end-to-end testing
    as a customer would use it in production.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This may result in integrating multiple microservice applications together
    such as microservice **A**, **B**, **C**, and **D** and making sure they all integrate
    functionally and work as a single entity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![System testing](img/B05559_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Performance testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Performance testing** is fairly self-explanatory, it will baseline the application''s
    performance on the first execution. It will then use that baseline to check for
    any performance degradations in the application every time a new release takes
    place.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests will be used to check performance metrics, this is useful
    to see if a code commit causes performance issues in the overall system. Performance
    testing can be incorporated into the system test phase.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, performance testing can also mean **Stress Testing or Load Testing**
    the application, network, or infrastructure to its absolute limit and by writing
    tests to see to find if the system can cope with the desired traffic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '**Endurance Testing** means setting a time period for testing and see how long
    the infrastructure, network, or application can cope with stress for a fixed period
    of time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spike Testing** is making sure a system can cope with a sudden spike in traffic
    from a dormant traffic pattern, which tests if the system can cope with a high
    degree of variance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability Testing** on the other hand can mean horizontally scaling out
    infrastructure or scaling up more applications to the point it makes no performance
    benefit. This identifies the scaling limits a system has.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume Testing** can be used to see the volume of transactions or data a
    system can process over a given period of time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the different types of testing that fall under
    the performance testing umbrella:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance testing](img/B05559_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: User acceptance testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**User acceptance testing** involves having end users test new features or
    functionality. User acceptance testing is normally utilized to make sure customers
    or product managers are happy with the development changes that have been made.
    This type of testing is normally exploratory and fairly manual. It is often used
    to test the look and feel of a website or graphical user interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Why is testing relevant to network teams?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quality assurance is a huge part of network or infrastructure changes, it is
    not just solely a software development concern. If the network or infrastructure,
    which software is installed upon, is not operating as desired, then this will
    have the same customer impact as a software bug.
  prefs: []
  type: TYPE_NORMAL
- en: The customer doesn't differentiate between software bugs, infrastructure, or
    networking issues. All a customer knows is that they can't utilize products and
    as far as they are concerned the business is not meeting their needs or providing
    a good and reliable service.
  prefs: []
  type: TYPE_NORMAL
- en: Not having adequate testing can be very harmful to a business, as its very reputation
    can be damaged, and the rise of social media means that if websites are down or
    not operational, within a blink of the eye an outage can be all over social media
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: If one user notices an issue they can send a tweet, which alerts other customers
    to the issue, one tweet becomes many and before the company knows it, the outage
    is trending on Twitter or other social media and now everyone across the world
    is aware that the business is having problems.
  prefs: []
  type: TYPE_NORMAL
- en: This situation is the worst fear for many online businesses, if the site is
    not up, operational, and providing a good user experience, then the business is
    no longer making money and customers may go to a competitor.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key objectives for any development, infrastructure, or networking
    team is to provide a good service to end users and prevent downtime or outages.
    Typically, a set of **Key Performance Indicators** (**KPIs**) are used to quantify
    performance and set targets to decipher if a business is meeting customer needs.
  prefs: []
  type: TYPE_NORMAL
- en: So making the delivery of network changes less prone to error should be the
    aim of any network team. In [Chapters 4](ch04.html "Chapter 4. Configuring Network
    Devices Using Ansible"), *Configuring Network Devices Using Ansible*, [Chapter
    5](ch05.html "Chapter 5. Orchestrating Load Balancers Using Ansible"), *Orchestrating
    Load Balancers Using Ansible*, and [Chapter 6](ch06.html "Chapter 6. Orchestrating
    SDN Controllers Using Ansible"), *Orchestrating SDN Controllers Using Ansible,*
    we looked at ways to automate network devices, load balancers, and SDN controllers
    using configuration management tooling. At the same time, having a set of repeatable
    tests for any network change should also be something that network teams are striving
    for. The ideal scenario being that a network team knows that a change is going
    to fail, before it has a customer impact. This means testing changes sufficiently
    before giving them a seal of approval and failing as fast as possible in test
    environments so breaking changes are not pushed directly to production environments.
  prefs: []
  type: TYPE_NORMAL
- en: One of the common concerns from network engineers when initially moving to an
    automated process is a lack of trust in the automation. Network engineers are
    used to going through due diligence and a subset of checks prior to releasing
    network changes. Just because automation is in place doesn't mean that the manual
    check-list network engineers used to validate network changes goes away.
  prefs: []
  type: TYPE_NORMAL
- en: However, when considering software delivery, the overall process is only as
    fast as the slowest component, so if those network validation checks remain manual,
    then the whole process will be slowed down. This will result in manual stops being
    placed in the automation process, which will inevitably slow down the delivery
    of a new product to market.
  prefs: []
  type: TYPE_NORMAL
- en: The simple solution is to automate each of the networking check-lists, so any
    validation that was carried out manually by a network engineer instead becomes
    an automated check or test as part of an automated test suite which is run alongside
    the automation.
  prefs: []
  type: TYPE_NORMAL
- en: These checks or tests are then written and built up over a period of time. So
    if a situation occurs when an edge case is found and it doesn't have test coverage
    that causes a failure. Rather than using the argument that the automation doesn't
    work and making a case to revert to tried and tested manual approaches, network
    engineers need to instead create a new test or check and add it to the automated
    validation pack which will catch the issue and fail in a test environment before
    it reaches the end user.
  prefs: []
  type: TYPE_NORMAL
- en: Network changes and testing today
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network teams still remain in the main work with a waterfall methodology, so
    they need to align and adopt a more agile approach. This will allow network teams
    to better integrate with the rest of the IT team and become a participant in the
    Continuous Delivery processes rather an observer.
  prefs: []
  type: TYPE_NORMAL
- en: When the waterfall methodology was the de-facto way of delivering software development
    projects to market, then a very rigid process lifecycle would be followed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Waterfall processes stipulated every new feature would traverse the following
    phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the main implementations of the waterfall method was known as the **V-Model**,
    which was initially used to simplify projects into deliverable chunks. This meant
    that all stakeholders could identify progress and look for potential delays.
  prefs: []
  type: TYPE_NORMAL
- en: This simplification made project managers and senior management happy as they
    had an easy way of tracking projects and whether they were on time or going over
    budget.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the V-Model is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network changes and testing today](img/B05559_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In waterfall terms, the analysis and design phases took place on the left-hand
    side of the V-Model and would happen at the start of the process. The left-hand
    side of the V-Model in simplistic terms is used to interact with stakeholders,
    do necessary research, and gather all necessary high-level and low-level requirements
    to work out what is required to implement a new product or change. The left side
    of the V-Model was also about documenting the overall process at an architectural
    level.
  prefs: []
  type: TYPE_NORMAL
- en: After the initial requirement gathering as part of the analysis phase, the analysis
    phase was signed off, which meant that the architectural design phase could begin
    and some meat could be put around the requirements. As part of the design phase,
    high-level and low-level design documents would be created to document the proposed
    changes which would have an associated review, sign off, and approval process.
  prefs: []
  type: TYPE_NORMAL
- en: Once the design was completed, then the left side of the V-Model was complete
    and implementation of the change or product would take place. The implementation
    phase could span numerous weeks, or even months, to deliver the desired result
    and this lifecycle phase sits at the bottom of the V-Model.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the requirements were implemented, the implementation phase would be
    signed off and the project would move to the right-hand side of the V-Model where
    the Test phase would commence.
  prefs: []
  type: TYPE_NORMAL
- en: A test team would then carry out unit, integration, and then finally system
    testing on any change or new product feature. Any issues found with the implementation
    phase would result in a change request. This would mean that the high- or low-level
    design would need to be updated, re-work on the implementation would need to be
    done, and then tests would need to be repeated or re-written in order for the
    product to be refined.
  prefs: []
  type: TYPE_NORMAL
- en: With the move to agile development covered in [Chapter 3](ch03.html "Chapter 3. Bringing
    DevOps to Network Operations"), *Bringing DevOps to Network Operations,* the V-Model
    has been seen to be a sub-optimal delivery mechanism. For reporting purposes,
    the V-Model is ideal and transparent, but it means that the implementation process
    suffers from the rigid restrictions enforced on engineers.
  prefs: []
  type: TYPE_NORMAL
- en: The V-Model doesn't take into account that any engineer likes to iterate processes
    and the actual implementation they write down at the start of a process may not
    be the final design they implement. The V-Model doesn't align well to prototyping
    as engineers typically like to spend time with the system and try, fail, iterate,
    and then improve the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Not accounting for prototyping leads to multiple change requests which have
    cost implications to businesses, so using two week sprints in an agile methodology
    to plan in iterative development has proved much more realistic. Although it is
    still something that senior managers struggle with as they are indoctrinated with
    having the need to report due dates and milestones, the due date is, by all intents
    and purposes, a made-up date.
  prefs: []
  type: TYPE_NORMAL
- en: An engineer in the waterfall process will still do the same amount of prototyping
    to deliver implementations, and work takes *x* amount of time regardless of how
    a plan is structured. Agile development is just structured to accept prototyping
    and time-boxed spikes.
  prefs: []
  type: TYPE_NORMAL
- en: So the age-old question from a project manager to an engineer is always; *when
    will this be done by?* The engineer that replies; *I don't know* isn't an acceptable
    answer in a waterfall methodology. What is expected is an estimate, or in engineering
    circles a made-up date, which the project manager will likely change later when,
    inevitably, said date isn't met.
  prefs: []
  type: TYPE_NORMAL
- en: So how does any of this have any relevance to network changes and testing overall?
    Well, network teams today typically implement a mini V-Model when they think about
    making network changes. Network managers will act as a project manager that will
    plan out a design, implementation, and test phase cycle and report this back to
    senior management teams as network changes are seen as long pieces of work that
    need massive planning and testing before implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Network managers may not split out testing into test, integration, and system
    testing as traditionally, network testing is not as sophisticated as this, but
    it doesn't allow network engineers the freedom to prototype.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead network engineers, like infrastructure engineers or any operational
    team before them, will be pressured into making changes to a rigid plan. The plan
    will be indicative of the following criteria being met:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the implemented change work as desired?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did the change break anything?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the documentation updated to reflect change?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all these points are met, they would have deemed a successful change by a
    network team.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this doesn''t tell the complete and whole story as other points have
    to be considered when making network changes within the remit of a Continuous
    Delivery model:'
  prefs: []
  type: TYPE_NORMAL
- en: Will the change break anything that isn't immediately visible?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Was the same change implemented to pre-production environments?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Was the same change tested and validated on pre-production environments?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial three points are mandatory requirements when doing any change, but
    the remaining three points should be considered mandatory too, in order to maintain
    a successful Continuous Delivery model. This is something of a mind-set change
    for network engineers, that they need to take this into consideration when making
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'All network changes from a network engineer need to be committed to the source
    control management system and propagated through all the necessary environments
    before being pushed to production, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network changes and testing today](img/B05559_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If a change is made directly to production manually by network engineers and
    not implemented first on test, pre-production and production environments via
    an automated, then the network configuration will be forever misaligned on test
    environments. This will have dire consequences as the configuration of test, pre-production
    environments, and production will have drifted apart.
  prefs: []
  type: TYPE_NORMAL
- en: This means that any developer, infrastructure, or network engineer using test
    and pre-production environments expecting a copy of production for mission-critical
    changes will be sadly disappointed. This can, in turn, compromise any tests run
    on those environments as they are no longer a proper reflection of the production
    estate.
  prefs: []
  type: TYPE_NORMAL
- en: So what does this mean in practice? The system test box on the V-Model may pass
    in pre-production environments but fail in production. This does not build confidence
    in the Continuous Delivery process which is now a mission critical part of the
    business.
  prefs: []
  type: TYPE_NORMAL
- en: It cannot be highlighted how important it is to make sure all network, code,
    or infrastructure changes are pushed to pre-production environments prior to production
    to maintain the validity of all environments. Any team deviating from this process
    can compromise the whole system and negate the testing.
  prefs: []
  type: TYPE_NORMAL
- en: This is not only used to test the changes, but also to keep the pre-production
    environments as a scaled-down mirror image of production that avoids the scenario
    of all tests pass in test environments but when they are deployed to production
    they cause outages to customers. So it means it is important that all validation
    tests are completed in associated test environments before a change is released
    to production.
  prefs: []
  type: TYPE_NORMAL
- en: If manual changes are pushed directly into production, even in the event of
    emergencies, then the changes need to be immediately put back into the **Source
    Control Management** (**SCM**) system, the SCM system should be the single source
    of truth for all configuration at all times.
  prefs: []
  type: TYPE_NORMAL
- en: 'If any manual changes are applied, snowflake environments will become common,
    which are shown below. This is where an engineer has made a manual change to production
    outside the process and not pushed the change to any of the other environments
    using the deployment pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network changes and testing today](img/B05559_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In order for network changes to be delivered at the desired rate, network changes
    and testing cannot continue to be done in a mini V-Model strategy. If network
    teams and managers are serious about being collaborators in Continuous Delivery
    models and DevOps models, they need to keep pace with the rest of the agile changes
    being made in development and infrastructure teams.
  prefs: []
  type: TYPE_NORMAL
- en: The solution though is not to stop validating changes and paying due diligence,
    lessons can be learned from formulaic quality assurance processes that have been
    successfully applied on development and infrastructure changes for years, these
    processes can also help test network changes.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quality assurance teams, when utilizing a waterfall V-Model structure of delivery,
    worked in silos that retrospectively tested development changes once they were
    completed by a development team.
  prefs: []
  type: TYPE_NORMAL
- en: 'This led to quality assurance teams having to react to every development change,
    as it was an impossible task having to write tests for a feature they had not
    yet seen, or understand how it fully operated. Situations would often arise where
    developers without warning would commit features into source control management
    systems and then quality assurance teams would have to react to them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Quality assurance best practices](img/B05559_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This method of working provided lots of challenges such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Developers changing user interfaces, so the quality assurance team's automated
    tests broke as test engineers were not aware of the user interface changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test engineers not understanding new features meaning appropriate tests weren't
    written to test functionality properly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers having to spend lots of time explaining how features worked to quality
    assurance testers so they could write tests post-commit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delays associated with fixing broken regression tests that were not down to
    bugs but test issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality assurance teams were acting in a completely reactive fashion as they
    could not see what new developer changes were coming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality assurance packs never passing, or being green, meant that actual software
    issues slipped through the process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, when considering network testing, the solution to this problem is not to
    hire a separate test team. Instead, it is about incorporating and integrating
    network testing into a Continuous Delivery model.
  prefs: []
  type: TYPE_NORMAL
- en: Agile development has shown that as code changes were being written, embedding
    quality assurance test engineers in the development team meant that tests could
    be written pre-commit. It is a far more productive method of working.
  prefs: []
  type: TYPE_NORMAL
- en: Moving quality assurance engineers out of the siloed quality assurance team
    and allowing them to work together in the same scrum team means that individuals
    that work together can collaborate and make sure that the submitted commit will
    work at every phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The associated regression, integration, or system tests then form a set of
    automated quality gates that the change will propagate through:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Quality assurance best practices](img/B05559_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The main benefits of the agile testing approach over using a siloed waterfall
    approach is:'
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance testers were no longer working reactively and have complete
    visibility of what developers are creating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each agile user story could have proper acceptance criteria written that included
    automated testing, allowing quality assurance engineers to work on test tasks
    as developers code new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When new features were coded, relevant tests are written for a new feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New feature tests can be added to the regression pack, so that every time a
    code commit is made the feature is tested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process change removes the inhibitor, which is simply the team structure,
    and joins two teams together so they become more productive, which is in essence
    the DevOps way.
  prefs: []
  type: TYPE_NORMAL
- en: Creating testing feedback loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we think back to the continuous integration process in [Chapter 7](ch07.html
    "Chapter 7. Using Continuous Integration Builds for Network Configuration"), *Using
    Continuous Integration Builds for Network Configuration*, then we had the commit
    process. The commit essentially starting the whole Continuous Delivery process.
    Once a commit has taken place, the change is already on the road to production.
    Any commit to the trunk/mainline/master branch is a final change, so if a network
    commit is made, it is already on its way to production.
  prefs: []
  type: TYPE_NORMAL
- en: If no validation engine or tests exist post check-in, then changes will flow
    all the way through test environments reaching production environments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating testing feedback loops](img/B05559_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that utilizing feedback loops with proper test gates is essential,
    so once a code commit has taken place, it will be adequately tested and provide
    an immediate indicator that a change has failed. Once all the quality gates have
    completed successfully only then should the change be promoted to production,
    this model promotes continuous improvement and failing fast. The further to the
    left a change fails, the less cost it incurs to a business.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 7,](ch07.html "Chapter 7. Using Continuous Integration Builds for
    Network Configuration") *Using Continuous Integration Builds for Network Configuration*,
    we focused on the process of continuous integration and how multiple different
    checks can be applied as part of the validation engine for a user commit. This
    makes sure that the user commit is always properly validated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuous integration provides a set of feedback loops where a code commit
    is submitted to the SCM and the validation engine will return either a pass or
    a failure. All testing can form the validation engine for changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration testing](img/B05559_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The continuous integration process when applied to development, takes the approach
    that all changes are committed to the **Trunk/Mainline/Master** branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration testing](img/B05559_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The new development feature will be committed to the **Trunk/Mainline/Master**
    branch. This new commit will be compiled and be immediately integrated with the
    rest of the code base, and then subsequent unit tests will then be executed to
    determine a pass or failure against the build binaries, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration testing](img/B05559_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using the commit to **Trunk/Mainline/Master** continuous integration approach
    relies on a degree of discipline from teams. If a commit fails and the **CI Build
    Server** returns a failed build, then the team member that made the failed commit
    has a duty to fix the build immediately, by either reverting or fixing the broken
    commit.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration builds should under no circumstances ever be left in
    a failed state as it means the **Trunk/Mainline/Master** is not in a clean state
    and all subsequent code commits will not have valid continuous integration performed
    until the build is fixed. This slows down a team's productivity so continuous
    integration is a collaborative process and failure should be seen as a learning
    opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: Gated builds on branches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another popular method is using **feature branches** and **gated builds**. Every
    time a developer makes a change, they will raise a merge request which will be
    peer-reviewed by other members of the team and then subsequently merged.
  prefs: []
  type: TYPE_NORMAL
- en: Each merge request, when accepted, will start the merge process, but as part
    of the merge process something known as a gated build will execute.
  prefs: []
  type: TYPE_NORMAL
- en: '![Gated builds on branches](img/B05559_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A gated build will be invoked when a merge occurs prior to integration with
    the **Trunk/Mainline/Master**. It will run the equivalent of a continuous integration
    build as a pre-commit, but only if the build and unit testing associated with
    the pre-commit build passes, will the contents of the merge request be merged
    to the **Trunk/Mainline/Master** branch.
  prefs: []
  type: TYPE_NORMAL
- en: The gated build process means that the **Trunk/Mainline/Master** branch is always
    kept completely clean and functional. Where pure continuous integration can have
    developers break the continuous integration build, gated builds prevent this from
    happening, as long as the tests are good.
  prefs: []
  type: TYPE_NORMAL
- en: Applying quality assurance best practices to networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network teams can greatly benefit from adopting some of the best practices and
    tried and tested methodologies that have been implemented to test development
    or infrastructure changes.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance is all about principles and processes, so test methodologies
    are fairly agnostic and the tools used to implement the process come secondary.
  prefs: []
  type: TYPE_NORMAL
- en: When teams are working within a Continuous Delivery model, any changes to network
    devices, load balancers, or even SDN controllers should be defined in source control
    using orchestration and configuration management tools such as Ansible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Continuous Delivery model, network changes need to propagate through environments
    and at all times be governed by source control management systems, with the state
    of the SCM system being the state of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying quality assurance best practices to networking](img/B05559_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Network changes can be treated much like code changes and adequate testing could
    be created as a network team initiative or by collaborating with the quality assurance
    team. The type of testing at each phase may vary slightly from the set of tests
    that a development or infrastructure team would run as part of their deployment
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: However, equivalent network-specific testing can be derived to create a set
    of robust tests that network changes have to traverse before being deployed to
    production by associating particular network tests with each quality gate on the
    deployment pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Network teams, such as development and infrastructure teams need to create a
    set of feedback loops to govern network changes, so that different test categories
    can be executed in the deployment pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'All testing should ideally be automated as part of each network change, in
    a proactive manner and written at the same time as the network change is being
    lined up. This then allows network changes to be tested in an automated manner
    at the point of inception:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying quality assurance best practices to networking](img/B05559_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When setting up continuous integration, selecting either continuous integration
    or a gated build strategy is down to the preference of the network team or engineers
    that commit changes.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing should be integrated with the network continuous integration process.
    A network operator will first check in a code change or change the state of the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: The **CI Build Server** will check the Ansible `var` YAML files using Lint,
    which will make sure that the YAML files are valid syntax.
  prefs: []
  type: TYPE_NORMAL
- en: If valid, the same playbook that would be executed on any downstream environment
    will be executed against a CI test environment to make sure the playbook is successful
    in terms of syntax and execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a set of unit tests will be executed against the environment to validate
    its functional and desired state of the environment after the playbook has been
    executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying quality assurance best practices to networking](img/B05559_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The important thing to note is that unit tests are executed as part of the continuous
    integration process and that these tests can either be part of the merge request
    validation or executed on commit to the **Trunk/Mainline/Master** branch.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning network testing to quality gates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When looking at what type of testing network teams can carry out to validate
    network changes, they can be broken into different test categories and assigned
    to different quality gates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the main test environments covered in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before considering where to put tests, we should first look at the network team's
    needs. With a blank canvas what would be a beneficial set of tests that could
    help with network operations?
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the following tests spring to mind, but any check or validation that
    is valid to a particular team is applicable and should be included:'
  prefs: []
  type: TYPE_NORMAL
- en: Network checklist that network engineers carry out manually when making changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit testing against network automation to make sure the network device is in
    the desired state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing performance of the network to see what the desired throughput is and
    test when parts of the network become oversubscribed and need to be scaled out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing failover of network devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing network code quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing different user journeys through the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Quality of Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these types of tests can then be assigned to particular test environments
    and quality gates created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assigning network testing to quality gates](img/B05559_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Available test tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test tools, like all tools, should be used to facilitate test processes and
    outcomes. So. for every single test quality gate, tools are required to wrap processes,
    schedule, and execute tests.
  prefs: []
  type: TYPE_NORMAL
- en: There are various test tools available on the market today that network engineers
    could greatly benefit from using.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network unit testing as said many times before will form part of the continuous
    integration build process and scheduled by a continuous integration build server.
  prefs: []
  type: TYPE_NORMAL
- en: One open source tool that can help with unit testing network changes is Test
    Kitchen. **Test Kitchen** is a unit testing tool which utilizes the Busser framework
    and can be used to carry out infrastructure testing. Test Kitchen supports many
    test frameworks such as **Bats** and **RSpec**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Test Kitchens Busser** framework is comprised of the following architectural
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: Driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test Kitchen defines all its plugins using a `kitchen.yml` file, which outlines
    the Driver, Provisioner, Platform, and Suites to use for the testing.
  prefs: []
  type: TYPE_NORMAL
- en: A **Driver** can be any platform that can be used to provision a virtual machine
    or container. Test Kitchen has support for Vagrant, Amazon, OpenStack, and Docker,
    and so can be used to test infrastructure changes.
  prefs: []
  type: TYPE_NORMAL
- en: A **Provisioner** is a configuration management tool such as Ansible, Chef,
    Puppet, or Salt and is used to configure the server into the state that needs
    to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: The **Platform** is the operating system that the Provisioner will execute on.
    Multiple Platforms can be specified for cross-operating system testing. This could
    be very useful when testing new versions of network operating systems operate
    in the same way as their predecessors when doing software upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: '**Suites** are used to create a test suite in combination with the Platform
    definition, so if two different Platforms are defined, then unit tests will be
    executed against each different platform in a consistent manner.'
  prefs: []
  type: TYPE_NORMAL
- en: Test Kitchen example using OpenStack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `test kitchen` gem will need to be pre-installed on the Ansible controller
    host. Then perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: From an Ansible controller node, in the folder containing the top player file
    structure, as shown in the following screenshot:![Test Kitchen example using OpenStack](img/B05559_08_20.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, execute the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This creates a `kitchen.yml` file and a test subdirectory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the `folder test` folder needs to be created which will store the unit
    tests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `test kitchen` file will then need to be populated with the Driver, Platform,
    Provisioner, and Suites.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Driver is specified as OpenStack, with a `cumulus-vx` image and Platform
    being created.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the image is `m1.large` which specifies the CPU, RAM, and disk for
    the server.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The instance will be created within the `network_team` tenant and `qa` availability
    zone.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once spun up, the `configure_device.yml` playbook will be executed to configure
    the network device before the default folder under `test/integration` which was
    defined in step 2\. This tells Test Kitchen the location of the Bats tests that
    will be executed to test the state of the device:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Test Kitchen example using OpenStack](img/B05559_08_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Each test can be given a unique name and the `.bats` file extension to define
    each unit test under the `bats` directory that was created in step 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An example of a test that can be written using Bats is as follows:![Test Kitchen
    example using OpenStack](img/B05559_08_22.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This checks that the `eth0` interface is in a good working state when executed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, to execute `test kitchen`, execute the command shown in the following
    screenshot:![Test Kitchen example using OpenStack](img/B05559_08_23.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test Kitchen will then carry out the following workflow:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create instance in OpenStack.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Run playbook.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Busser plugin.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Run unit tests.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Destroy instance if all tests passed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Network checklist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network engineers, as discussed, often have a set of manual checklists that
    they use to validate if a network change has been successful or not.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, this could involve validating whether a user interface has the desired
    configuration that checks if the automation has worked as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of doing these checks manually, Selenium can be used to carry out graphical
    user interface checks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Selenium''s workflow can be summarized as test scripts invoking the Selenium
    web driver which then creates a browser session to test a website or web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network checklist](img/B05559_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Test scripts can be written in multiple languages such as Java, Python, or Ruby.
  prefs: []
  type: TYPE_NORMAL
- en: Selenium can be installed in Python form by doing a pip install when using Python
    for authoring scripts.
  prefs: []
  type: TYPE_NORMAL
- en: As Selenium is browser-based, it works with multiple browsers such as Internet
    Explorer, Firefox, Chrome, and Safari and tests cross-browser support.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Selenium test sample is shown in the following screenshot; this script will
    launch [google.co.uk](http://google.co.uk) in Chrome, type `DevOps For Networking`
    and finally click the **Search** button on Google:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network checklist](img/B05559_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, any graphical interface can be screen scraped such as a load balancer or
    network device interface to assert that the correct information has been entered
    and returned. This can also be useful if older network devices don't have an API.
  prefs: []
  type: TYPE_NORMAL
- en: Network user journey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good test methodology is to test user journeys throughout the network. This
    can be done by doing point-to-point testing in the network.
  prefs: []
  type: TYPE_NORMAL
- en: A good example of network user journeys may be testing **Equal Cost Multipath**
    (**ECMP**) on Leaf-Spine architecture to make sure it is performing as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Another test may be setting up point-to-point tests across data centers to make
    sure links are performing as desired and do not suddenly depreciate.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up user journey testing means that if a baseline performance drops,
    then it can be tracked back to specific network changes as part of the network
    deployment pipeline. This is done in much the same way as baselining application
    performance and making sure a new release doesn't cause a drop in performance
    that will impact end users.
  prefs: []
  type: TYPE_NORMAL
- en: Network user journey testing mean that if an ill-performing path through the
    network is found, then it can be localized and fixed quickly so it improves mean
    time to resolution when issues occur. Network engineers can use a tool such as
    **iPerf** to send large amounts of packets through points in the network. This
    can be useful to see where the bottlenecks are in the network and make sure the
    performance is as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of network tools now offer **Quality of Service** (**QoS**), which allows
    network operators to limit the amount of network bandwidth that particular tenants
    utilize in a network.
  prefs: []
  type: TYPE_NORMAL
- en: This prevents noisy test environments from impacting a production environment.
    This is possible as network devices can set guarantees on performance on particular
    tenant networks. This means that certain application workloads are always guaranteed
    a certain network throughput, while other less crucial tenant networks can be
    capped at peak times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different thresholds and alerting can be set up on network devices and faults
    in network hardware can be detected if the QoS drops at a random time. It also
    guards network engineers against the age-old: *I think we have a network problem*.
    Instead they can prove it is an application issue, as the network service is stable
    and performing as desired and can be easily displayed.'
  prefs: []
  type: TYPE_NORMAL
- en: It is good to simulate and test QoS away from production environments and have
    network teams come up with different scenarios to design the best fit for the
    network, based on the applications that they are hosting.
  prefs: []
  type: TYPE_NORMAL
- en: Failover testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Failover testing** should ideally be tested regularly by network teams, as
    modern networks should be disaster recovery-aware and designed for failure.'
  prefs: []
  type: TYPE_NORMAL
- en: Network failover tests can be simulated by writing an Ansible playbook or role
    that disables a service or reboots a switch to make sure that the system adequately
    fails over.
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilizing `delegate_to: localhost`, API commands can be issued to network devices
    such as switches to disable them programmatically using the API. Alternatively,
    Ansible can SSH onto a network device''s operating system and issue an impromptu
    hard reboot.'
  prefs: []
  type: TYPE_NORMAL
- en: Supplementary monitoring should be set up while doing failover testing to make
    sure the network does not drop packets and test the speed at which the network
    device fails over after the initial primary device is disabled.
  prefs: []
  type: TYPE_NORMAL
- en: Network code quality tooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When defining the desired state of the network as code, make sure the Python
    code that is written to create Ansible modules, as well as any other code that
    is used is of a high standard and good quality.
  prefs: []
  type: TYPE_NORMAL
- en: '**SonarQube** is an open source code quality tool which allows teams to analyze
    their codes quality. Its architecture is comprised of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: SonarQube Runner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SonarQube database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SonarQube web interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sonar has a range of plug-ins that can be configured to provide unit test reporting,
    code coverage, or code quality rules and can be set-up for any language be it
    Python, Java, or C#.
  prefs: []
  type: TYPE_NORMAL
- en: SonarQube will snapshot a code repository every time it is run and store the
    history of a project in terms of code quality. This can be trended over time showing
    quality improvements or drops in the code quality. Sonar can be used to define
    specific best practice or rules, which show up as violations when broken by commits.
  prefs: []
  type: TYPE_NORMAL
- en: The **SonarQube Runner** uses a `sonar.properties` file at runtime that can
    be included as part of the source control management system. This can be pulled
    down as part of the continuous integration process. This means that after a new
    code commit on a custom Ansible module the SonarQube Runner can be executed against
    the code to test the new commit and see the impact.
  prefs: []
  type: TYPE_NORMAL
- en: The SonarQube Runner will execute a code quality check using one of the plug-ins
    stipulated in the `sonar.properties` file. In the case of a new or changed Ansible
    module that will invoke the Python-specific group of code quality tests. Information
    will subsequently be displayed on the Sonar web-interface once the analysis is
    complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for this process is shown in the following screenshot with the
    SonarQube Runner triggering the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network code quality tooling](img/B05559_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An example of the sonar Python SonarQube project dashboard is shown in the
    following screenshot, outlining the bugs, vulnerabilities, and tech debt to fix
    all the issues in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network code quality tooling](img/B05559_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tracking code quality and metrics is very important when implementing a continuous
    improvement model in any company. So adequately measuring and analyzing where
    improvements can be made in the code that drives all processes is important in
    order to have engineers engage and write tests.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have looked at why testing network changes are necessary.
    We focused on the benefits of utilizing feedback loops to continuously improve
    network operations. We then explored some of the challenges associated with the
    way network teams approach network changes and testing and how they will need
    to adapt and adopt quality assurance best practices to keep up when companies
    are running a Continuous Delivery model supplemented by a DevOps methodology.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how network teams could set up quality gates for testing and
    looked at some of the tests that could be mapped at each stage of testing. Finally
    we looked at some available tools that could be used to carry out network testing
    to implement unit testing, check-lists, and code quality checks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter you have learned about different types of test strategies such
    as unit, component, integration, performance, system, and user acceptance testing.
    Key takeaways also include quality assurance best practices, and why they are
    applicable to networking and different types of network validations that could
    help assert automated network changes.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has also delved into test tools that can be used to help test networking
    such as Test Kitchen ([http://kitchen.ci/](http://kitchen.ci/)), SonarQube ([http://www.sonarqube.org/](http://www.sonarqube.org/)),
    and iPerf ([https://iperf.fr/](https://iperf.fr/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will focus on deployment pipelines, look at the tooling
    that can be used to automatically deploy network changes. We will also look at
    the difference between Continuous Delivery and deployment and when each approach
    should be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following blogs and presentations may be useful for further understanding
    microservice test strategies in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://martinfowler.com/articles/microservice-testing/](http://martinfowler.com/articles/microservice-testing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=FotoHYyY8Bo](https://www.youtube.com/watch?v=FotoHYyY8Bo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
