- en: 'Painting The Big Picture: The Self-Sufficient System Thus Far'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描绘全貌：到目前为止的自给自足系统
- en: A self-sufficient system is a system capable of healing and adaptation. Healing
    means that the cluster will always be in the designed state. As an example, if
    a replica of a service goes down, the system needs to bring it back up again.
    Adaptation, on the other hand, is about modifications of the desired state so
    that the system can deal with changed conditions. A simple example would be increased
    traffic. When it happens, services need to be scaled up. When healing and adaptation
    are automated, we get self-healing and self-adaptation. Together, they both a
    self-sufficient system that can operate without human intervention.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自给自足系统是一个能够自我修复和自我适应的系统。修复意味着集群始终保持在设计的状态。例如，如果某个服务的副本宕机，系统需要重新启动它。适应性则是指修改期望状态，以便系统能够应对变化的条件。一个简单的例子是流量增加。当流量增加时，服务需要扩展。当修复和适应自动化时，我们就得到了自我修复和自我适应。两者共同构成了一个无需人工干预即可运行的自给自足系统。
- en: How does a self-sufficient system look? What are its principal parts? Who are
    the actors?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自给自足的系统是什么样的？它的主要组成部分有哪些？谁是其中的参与者？
- en: We’ll limit the scope of the discussion to services and ignore the fact that
    hardware is equally important. With such a limitation in mind, we’ll paint a high-level
    picture that describes a (mostly) autonomous system from the services point of
    view. We’ll elevate ourselves from the details and have a birds-view of the system.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论范围限制为服务，并忽略硬件同样重要的事实。考虑到这一限制，我们将描绘一个高层次的图景，描述从服务角度来看（大多数）自主系统。我们将从细节中抽身，俯瞰整个系统。
- en: In case you are know-it-all type of person and want to see everything at once,
    the system is summarized in the figure 10-1.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是那种什么都想一次性看全的人，系统已在图 10-1 中做了总结。
- en: '![Figure 10-1: The system with self-healing and self-adapting services](img/00052.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-1：具有自我修复和自我适应服务的系统](img/00052.jpeg)'
- en: 'Figure 10-1: The system with self-healing and self-adapting services'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-1：具有自我修复和自我适应服务的系统
- en: A diagram like that one is probably too much to process at once. Throwing it
    into your face might make you think that empathy is not one of my strengths. If
    that’s the case, you’re not alone. My wife shares that impression even without
    any diagrams. This time I’ll do my best to change your opinion and start over
    with a blank slate.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的图表可能一下子让人难以消化。直接展示给你可能会让你觉得同理心不是我的强项。如果你有这种感觉，你并不孤单。我的妻子也有同样的印象，即使没有任何图表。这次我会尽力改变你的看法，从头开始，重新整理一下。
- en: We can separate the system into two major domains; human and machine. Think
    of them as [Matrix](http://www.imdb.com/title/tt0133093/). If you haven’t seen
    the movie, stop reading the book right away, make some popcorn, and watch it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将系统分为两个主要领域：人类和机器。可以把它们想象成[黑客帝国](http://www.imdb.com/title/tt0133093/)。如果你还没看过这部电影，立刻停下阅读这本书，准备些爆米花，去看一看吧。
- en: In Matrix, the world is overtaken by machines. Humans don’t do much except the
    few that realized what’s going on. Most are living in a dream that reflects past
    events of human history. They are physically in the present, but their minds are
    in the past. The same situation can be observed with modern clusters. Most people
    still operate them as if it’s 1999\. Almost everything is manual, the processes
    are cumbersome, and the system is surviving due to a brute force and wasted energy.
    Some understood that the year is 2017 (at least at the time of this writing) and
    that a well-designed system is a system that does most of the work autonomously.
    Almost everything is run by machines, not human operators.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在《黑客帝国》中，世界被机器控制。人类几乎不做什么，只有那些意识到发生了什么的人例外。大多数人生活在一个反映人类历史过去事件的梦境中。他们身体上处于现在，但思想却停留在过去。现代集群也呈现出类似的情况。大多数人仍然像1999年那样操作它们。几乎所有的操作都是手动的，过程繁琐，系统依靠蛮力和浪费的能量勉强存活下来。少数人意识到现在已经是2017年（至少在本文写作时是这样），而一个设计良好的系统是一个大部分工作由机器自动完成的系统。几乎所有的操作都是由机器而非人工操控的。
- en: That does not mean that there is no place for us (humans). There is, but it
    is more related to creative and non-repetitive tasks. Therefore, if we focus only
    on cluster operations, the human domain is shrinking and being taken over by the
    machine domain.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着我们（人类）没有角色。我们有，但它更与创造性和非重复性任务相关。因此，如果我们仅关注集群操作，人类领域正在缩小，并被机器领域取代。
- en: The system can be divided into different roles. As you will see, a tool or a
    person can be very specialized and perform only a single role, or it can be in
    charge of multiple aspects of the operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 系统可以分为不同的角色。正如你将看到的那样，一个工具或一个人可以非常专业化，只执行单一的角色，也可以负责操作的多个方面。
- en: Developer’s Role In The System
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发者在系统中的角色
- en: The human domain consists of processes and tools that are operated manually.
    We are trying to move away from that domain all the actions that are repeatable.
    That does not mean that the goal is for that domain to disappear. Quite the contrary.
    By pushing repetitive tasks away from it, we are freeing ourselves from mundane
    tasks and increasing the time we spend with those that bring real value. The less
    we do the tasks that can be delegated to machines, the more time we can spend
    with those that require creativity. This philosophy is in line with strengths
    and weaknesses of each actor in this drama. Machines are good at crunching numbers.
    They know how to execute predefined operations very fast. They are much better
    and more reliable at that than us. Unlike machines, we are capable of critical
    thinking. We can be creative. We can program those machines. We can tell them
    what to do and when.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类领域包括那些由人工操作的流程和工具。我们试图将所有可以重复的操作从这个领域移除。这并不意味着这个领域的目标是消失，恰恰相反。通过将重复的任务移出人类领域，我们解放了自己，使我们能花更多时间在那些带来真正价值的事情上。我们越少做那些可以委托给机器的任务，我们就能有更多时间去做那些需要创造力的工作。这种哲学符合每个角色在这场戏中的优势和劣势。机器擅长处理数据，它们知道如何快速执行预定义的操作，且比我们更快、更可靠。与机器不同，我们能够进行批判性思维，我们可以富有创造力。我们可以编程这些机器，告诉它们做什么以及何时做。
- en: I designated a developer as the leading actor of a human domain. I intentionally
    avoided using the word coder. A developer is everyone working on a software development
    project. It does not matter whether you’re a coder, a tester, an operations guru,
    or a scrum master. I’m putting you all in the group labeled as developer. The
    result of your work is to push something to a code repository. Until it gets there,
    it’s as if it does not exist. It does not matter whether it sits on your laptop,
    in a notebook, on your desk, or on a tiny piece of paper attached to a pigeon
    messenger. From the point of view of the system, it does not exist until it gets
    into a code repository. That repository is hopefully Git but, for the sake of
    argument, it can be any other place where you can store and version something.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将开发者指定为人类领域的主要角色。我有意避免使用“编码员”一词。开发者是指参与软件开发项目的所有人。无论你是编码员、测试员、运维专家，还是敏捷教练，都归为开发者这个群体。你们的工作成果是将某些内容提交到代码库中。在它到达之前，它就像不存在一样。无论它是在你的笔记本电脑上、笔记本中、桌面上，还是在一张小纸条上附着在信鸽上，都不重要。从系统的角度看，直到它进入代码库，它才算存在。那个代码库希望是
    Git，但为了便于讨论，它可以是任何一个可以存储和版本管理代码的地方。
- en: That code repository is also part of the human domain. Even though it is a piece
    of software, it belongs to us. We operate it. We are pushing commits, pulling
    code, merging, and, sometimes, staring at it out of despair produced by too many
    merge conflicts. That does not mean that it does not have automated operations,
    nor that some parts of the machine domain are not operating it without any human
    involvement. Still, as long as something is mostly hands-on, we’ll consider it
    being part of the human domain. Code repository definitely qualifies as a piece
    of the system that requires a lot of human intervention.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码库也是人类领域的一部分。尽管它是一款软件，它仍然属于我们。我们来操作它。我们提交代码、拉取代码、合并代码，有时还会因太多的合并冲突而无奈地盯着它看。这并不意味着它没有自动化操作，也不意味着机器领域的某些部分在没有任何人为干预的情况下操作它。尽管如此，只要某件事大多是人工操作的，我们就会认为它属于人类领域。代码库绝对算是需要大量人工干预的系统的一部分。
- en: '![Figure 10-2: A developer commits code to a code repository](img/00053.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-2: 开发者将代码提交到代码库](img/00053.jpeg)'
- en: 'Figure 10-2: A developer commits code to a code repository'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10-2: 开发者将代码提交到代码库'
- en: Let’s see what happens when a commit is pushed to a code repository.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当代码提交到代码库时发生了什么。
- en: Continuous Deployment Role In The System
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统中持续部署的角色
- en: The continuous deployment process is fully automated. No exceptions. If your
    pipeline is not automated, it is not continuous deployment. You might require
    a manual action to deploy to production. If that action consists of pressing a
    single button that says, in bold letters, **deploy**, your process is continuous
    delivery. I can accept that. There might be business reasons for having such a
    button. Still, the level of automation is the same as with continuous deployment.
    You are only a decision maker. If there are any other manual operations, you are
    either doing continuous integration or, more likely, something that should not
    have a word continuous in its name.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 持续部署过程是完全自动化的。没有例外。如果你的流水线没有自动化，那就不是持续部署。你可能需要手动操作才能部署到生产环境。如果这个操作仅仅是按下一个写着**deploy**的按钮，那么你的过程是持续交付。我可以接受这种情况。可能出于业务原因需要这样一个按钮。尽管如此，自动化的程度和持续部署是一样的。你只是一个决策者。如果有任何其他手动操作，那你要么是在做持续集成，要么，更可能是在做一些不该带有“持续”字眼的工作。
- en: No matter whether it is continuous deployment or delivery, the process is fully
    automated. You are excused from having manual parts of the process only if your
    system is a legacy system that your organization choose not to touch (typically
    a Cobol application). It just sits on top of a server and does something. I’m
    very fond of “nobody knows what it does, do not touch it” type of rules. It is
    a way to show utmost respect while still keeping the safe distance. However, I
    will assume that’s not your case. You want to touch it. The desire is burning
    within you. If that’s not the case and you are unfortunate enough to work on one
    of those stay-away-from-it types of systems, you are reading the wrong book, and
    I’m surprised you did not realize that yourself.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是持续部署还是持续交付，过程都是完全自动化的。只有在你的系统是一个遗留系统，并且你的组织选择不去触碰它时，你才可以免于手动操作（通常是一个Cobol应用）。它仅仅是坐在服务器上做一些事情。我非常喜欢“没人知道它在做什么，不要碰它”类型的规则。这是一种在保持安全距离的同时，表现出极高尊重的方式。然而，我假设这不是你的情况。你想要去触碰它。你内心的渴望在燃烧。如果不是这样，而你不幸正在一个那种“远离它”的系统上工作，那么你读错了书，我很惊讶你自己没有意识到这一点。
- en: Once a code repository receives a commit or a pull request, it triggers a Web
    hook that sends a request to a CD tool which initiates the continuous deployment
    process. In our case, that tool is [Jenkins](https://jenkins.io/). The request
    starts a build of the pipeline that performs all sorts of continuous deployment
    tasks. It checks out the code and runs unit tests. It builds an image and pushes
    it to a registry. It runs functional, integration, performance, and other types
    of tests that require a live service. The very end of the process (excluding production
    tests) is a request to a scheduler to deploy or update the service in the production
    cluster. Our choice for a scheduler is Docker Swarm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码仓库接收到提交或拉取请求，它会触发一个Web hook，发送请求给CD工具，启动持续部署过程。在我们的案例中，这个工具是[Jenkins](https://jenkins.io/)。该请求会启动流水线构建，执行各种持续部署任务。它会检出代码并运行单元测试。它构建一个镜像并将其推送到注册表。它运行功能测试、集成测试、性能测试以及其他需要实时服务的测试。流程的最后阶段（不包括生产环境测试）是向调度器发送请求，在生产集群中部署或更新服务。我们选择的调度器是Docker
    Swarm。
- en: '![Figure 10-3: Deployment of a service through Jenkins](img/00054.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图10-3：通过Jenkins部署服务](img/00054.jpeg)'
- en: 'Figure 10-3: Deployment of a service through Jenkins'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-3：通过Jenkins部署服务
- en: In parallel with continuous deployment, another set of processes is running
    and trying to keep the configurations of the system up-to-date.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与持续部署并行，另一组进程正在运行并试图保持系统配置的最新状态。
- en: Service Configuration Role In The System
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统中服务配置的角色
- en: Parts of the system needs to be reconfigured whenever any aspect of the cluster
    changes. A proxy might need an update of its configuration, metrics collector
    might require new targets, logs parser might need an update to it’s rules.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的某些部分需要在集群的任何方面发生变化时重新配置。代理可能需要更新其配置，指标收集器可能需要新的目标，日志解析器可能需要更新它的规则。
- en: No matter which parts of the system require changes, those changes need to be
    applied automatically. Hardly anyone disputes that. The bigger question is where
    to find those pieces of information that should be incorporated into the system.
    The most optimum place is in the service itself. Since almost all schedulers use
    Docker, the most logical place for the information about a service is inside it,
    in the form of labels. Setting the information anywhere else would prevent us
    from having a single source of truth and would make auto-discovery a hard thing
    to accomplish.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无论系统的哪些部分需要更改，这些更改都需要自动应用。几乎没有人对此有异议。更大的问题是，如何找到那些应当被纳入系统的信息。最理想的地方是服务本身。由于几乎所有的调度程序都使用
    Docker，因此关于服务的信息最合理的位置就是它内部，以标签的形式存在。将信息放在其他地方会阻碍我们拥有单一的信息源，并使自动发现变得难以实现。
- en: Having information about a service inside it does not mean that the same information
    should not reside in other places inside the cluster. It should. However, the
    service is where master information must be and, from there on, it should be propagated
    towards other services. Docker makes that very easy. It already has an API that
    anyone can hook into and discover any information about any service.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将服务信息放在服务内部，并不意味着同样的信息不应该存在于集群中的其他地方。它应该存在。然而，服务是主信息必须存在的地方，从那里开始，信息应当传播到其他服务。Docker
    使这一过程变得非常简单。它已经有一个 API，任何人都可以接入，并发现任何服务的所有信息。
- en: The choice of a tool that discovers service information and propagates it to
    the rest of the system is [Docker Flow Swarm Listener (DFSL)](http://swarmlistener.dockerflow.com/).
    You might choose something else or build your own solution. The goal of such a
    tool, and *Docker Flow Swarm Listener* in particular, is to listen to Docker Swarm
    events. If a service contains a specific set of labels, the listener will fetch
    the information as soon as a service is deployed or updated and pass it to all
    interested parties. In this case, that is *Docker Flow Proxy (DFP)* (with HAProxy
    inside) and *Docker Flow Monitor (DFM)* (with Prometheus inside). As a result,
    both are having configurations that are always up-to-date. The proxy has the routes
    of all the publicly available services while Prometheus has the information about
    the exporters, alerts, the address of *Alertmanager*, and quite a few other things.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用来发现服务信息并将其传播到系统其他部分的工具是 [Docker Flow Swarm Listener (DFSL)](http://swarmlistener.dockerflow.com/)。你可以选择其他工具或构建自己的解决方案。这类工具的目标，特别是
    *Docker Flow Swarm Listener*，是监听 Docker Swarm 事件。如果服务包含特定的标签集，监听器将在服务部署或更新时立即获取信息，并将其传递给所有相关方。在这种情况下，相关方是
    *Docker Flow Proxy (DFP)*（内部包含 HAProxy）和 *Docker Flow Monitor (DFM)*（内部包含 Prometheus）。最终，二者都拥有始终最新的配置。代理拥有所有公开服务的路由，而
    Prometheus 则拥有有关导出器、警报、*Alertmanager* 的地址以及其他许多信息。
- en: '![Figure 10-4: Reconfiguration of the system through Docker Flow Swarm Listener](img/00055.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-4：通过 Docker Flow Swarm Listener 重新配置系统](img/00055.jpeg)'
- en: 'Figure 10-4: Reconfiguration of the system through Docker Flow Swarm Listener'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-4：通过 Docker Flow Swarm Listener 重新配置系统
- en: While deployments and reconfigurations are going on, users must be able to access
    our services without downtime.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署和重新配置进行时，用户必须能够在不中断服务的情况下访问我们的服务。
- en: Proxy Role In The System
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理在系统中的角色
- en: Every cluster needs a proxy that will receive requests coming to a single port
    and forward them to destination services. The only exception is when we have only
    one public-facing service. In that case, it is questionable not only whether we
    need a proxy but whether we need a cluster at all.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群都需要一个代理，代理将接收发送到单个端口的请求并将其转发到目标服务。唯一的例外是当我们只有一个面向公众的服务时。在这种情况下，值得质疑的不仅是我们是否需要代理，甚至是否根本需要集群。
- en: When a request comes to the proxy, it is evaluated and, depending on its path,
    domain, or a few other headers, forwarded to one of the services.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求到达代理时，它会被评估，并根据其路径、域名或其他几个头信息，转发到其中一个服务。
- en: Docker made quite a few aspects of proxies obsolete. There is no reason for
    load balancing. Docker’s Overlay network does that for us. There’s no need to
    maintain IPs of the nodes where services are hosted. Service discovery does that
    for us. Evaluation of headers and forwarding is pretty much everything that a
    proxy should do.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Docker使得代理的许多方面变得过时。负载均衡已经没有必要，Docker的Overlay网络会为我们完成这项工作。我们也不需要维护托管服务的节点的IP地址，服务发现系统会为我们处理这一切。对头信息的评估和转发基本上就是代理应当执行的所有工作。
- en: Since Docker Swarm utilizes rolling updates whenever an aspect of a service
    is changed, the continuous deployment (CD) process should not produce any downtime.
    For that statement to be true, a few requirements need to be fulfilled. Among
    others, a service needs to run at least two replicas, preferably more. Otherwise,
    any update of a service with a single replica will, unavoidably, create downtime.
    It does not matter whether that is a minute, a second, or a millisecond.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Docker Swarm 在服务的每个方面发生变化时都会利用滚动更新，因此持续部署（CD）过程不应产生任何停机时间。为了确保这一点成立，需要满足一些要求。除了其他要求外，服务至少需要运行两个副本，最好更多。否则，任何单副本的服务更新都会不可避免地造成停机。无论是几分钟、几秒钟还是毫秒，都没有区别。
- en: Downtime is not always disastrous. It all depends on the type of a service.
    If Prometheus is updated to a newer release, there will be downtime since it cannot
    scale. But, it is not a public facing service unless you count a few operators.
    A few seconds of downtime is not a big deal.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 停机并不总是灾难性的，这取决于服务的类型。如果 Prometheus 被更新为新版本，由于它无法扩展，肯定会有停机时间。但它不是一个面向公众的服务，除非你算上几个操作员。几秒钟的停机对它来说并不算大问题。
- en: A public facing service like an online retail store where thousands or even
    millions of users are shopping can quickly lose good reputation if it goes down.
    We are so spoiled as consumers that a single glitch can change our mind and make
    us go to the competition. If that “glitch” is repeated over and over, loss of
    business is almost guaranteed. Continuous deployment has many advantages but,
    since it is executed fairly often, it also amplifies potential deployment problems,
    downtime being one of them. One second downtime produced many times a day is,
    indeed, not acceptable.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个面向公众的服务，例如一个在线零售商店，成千上万甚至数百万用户在其中购物，如果服务出现故障，迅速就会失去良好的声誉。作为消费者，我们已经非常惯坏了，哪怕是一个小小的故障，也能让我们改变主意，转而去竞争对手那里。如果这个“故障”一而再、再而三地发生，业务损失几乎是注定的。持续部署有很多优点，但由于它执行得比较频繁，它也放大了潜在的部署问题，其中停机就是其中之一。每天多次出现的“一秒钟停机”，实际上是不可接受的。
- en: The good news is that rolling updates combined with multiple replicas will allow
    us to avoid downtime, as long as the proxy is always up-to-date.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，结合滚动更新和多个副本，我们可以避免停机，只要代理始终保持最新。
- en: The combination of rolling updates with a proxy that dynamically reconfigures
    itself results in a situation where a user can send a request to a service at
    any time without being affected by continuous deployment, failures, and other
    changes to the state of the cluster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新与能够动态重新配置自己的代理相结合，结果是用户可以在任何时候向服务发送请求，而不受持续部署、故障和集群状态变化的影响。
- en: When a user sends a request to a domain, that request enters a cluster through
    any of the healthy nodes and is taken over by Docker’s *Ingress* network. The
    network, in turn, detects that a request uses a port published by the proxy and
    forwards it. The proxy, on the other hand, evaluates the path, domain, or some
    other aspect of the request and forwards it to the destination service.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户向某个域发送请求时，请求通过任何一个健康节点进入集群，并被 Docker 的*Ingress*网络接管。该网络会检测到请求使用的是代理发布的端口并进行转发。代理则会评估请求的路径、域名或其他某个方面，并将请求转发到目标服务。
- en: We’re using [Docker Flow Proxy (DFP)](http://proxy.dockerflow.com/) that adds
    the required level of dynamism on top of HAProxy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是[Docker Flow Proxy (DFP)](http://proxy.dockerflow.com/)，它在 HAProxy 上增加了所需的动态性。
- en: '![Figure 10-5: The flow of a request to the destination service](img/00056.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-5：请求流向目标服务的过程](img/00056.jpeg)'
- en: 'Figure 10-5: The flow of a request to the destination service'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-5：请求流向目标服务的过程
- en: The next role we’ll discuss is about collecting metrics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的角色是关于收集度量指标的。
- en: Metrics Role In The System
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统中的度量指标角色
- en: The crucial part of any cluster, especially those that are aiming towards self-adaptation,
    is data. Hardly anyone will dispute the need to have the past and present metrics.
    Without them, we’d run like a headless chicken when things go wrong. The central
    question is not whether they are required but what we do with them. Traditionally,
    operators would spend endless hours watching dashboards. That is far from efficient.
    Watch Netflix instead. It is, at least, more entertaining. The system should use
    metrics. The system generates them, it collects them, and it should decide what
    actions to perform when they reach some thresholds. Only then, the system can
    be self-adaptive. Only when it acts without human intervention can it be self-sufficient.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任何集群中至关重要的部分，尤其是那些朝着自适应方向发展的集群，是数据。几乎没有人会争辩过去和当前指标的必要性。如果没有它们，当出现问题时我们就像无头的苍蝇一样乱跑。关键问题不在于是否需要它们，而在于我们如何使用它们。传统上，运维人员会花费无数小时盯着仪表板看。这远远低效，不如看
    Netflix。至少，后者更具娱乐性。系统应该使用这些指标。系统生成它们，收集它们，并且应该决定在它们达到某些阈值时执行哪些操作。只有这样，系统才能自适应。只有在没有人工干预的情况下执行操作，系统才能自给自足。
- en: A system that implements self-adaptation needs to collect data, store them,
    and act upon them. I will skip the discussion of pros and cons between pushing
    and scraping data. Since we chose to use [Prometheus](https://prometheus.io/)
    as a place where data is stored and evaluated and as the service that generates
    and fires alerts, the choice is to scrape data. That data is available in the
    form of exporters. They can be generic (e.g. Node Exporter, cAdvisor, and so on),
    or specific to a service. In the latter case, services must expose metrics in
    a simple format Prometheus expects.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实现自适应的系统需要收集数据、存储数据并对数据采取行动。我将跳过推送和抓取数据的利弊讨论。由于我们选择使用[Prometheus](https://prometheus.io/)作为数据存储和评估的地方，以及生成和触发告警的服务，因此选择了抓取数据。这些数据通过导出器的形式提供。它们可以是通用的（例如
    Node Exporter、cAdvisor 等），也可以是特定于某个服务的。在后者情况下，服务必须以 Prometheus 期望的简单格式暴露指标。
- en: Independently of the flows we described earlier, exporters are exposing different
    types of metrics. Prometheus periodically scrapes them and stores them in its
    database. In parallel with scraping, Prometheus is continuously evaluating the
    thresholds set by alerts and, if any of them is reached, it is propagated to [Alertmanager](https://prometheus.io/docs/alerting/alertmanager/).
    Under most circumstances, those limits are reached as a result of changed conditions
    (e.g. increased load on the system).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 独立于我们之前描述的流程，导出器暴露了不同类型的指标。Prometheus 会定期抓取这些指标并将其存储在数据库中。与抓取数据并行，Prometheus
    会持续评估由告警设置的阈值，如果达到任何一个阈值，它会被传播到[Alertmanager](https://prometheus.io/docs/alerting/alertmanager/)。在大多数情况下，这些阈值的触发是由于条件发生变化（例如，系统负载增加）。
- en: '![Figure 10-6: Data collection and alerting](img/00057.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-6：数据收集与告警](img/00057.jpeg)'
- en: 'Figure 10-6: Data collection and alerting'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-6：数据收集与告警
- en: Alert receivers are what makes the difference.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 告警接收器是决定差异的关键。
- en: Alerting Role In The System
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统中的告警角色
- en: The alerts are split into two general groups depending on alert receivers. It
    can be forwarded to the system or to humans. When an alert qualifies as the type
    that should be sent to the system, a request is usually forwarded to a service
    that is capable of evaluating the situation and executing tasks that will adapt
    the system. In our case, that service is *Jenkins* which executes one of the predefined
    jobs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 告警根据接收器的类型分为两大类。它可以转发到系统或人类。当某个告警被认定为应该发送到系统时，通常会转发一个请求到一个能够评估情况并执行适应系统任务的服务。在我们的案例中，这个服务是
    *Jenkins*，它会执行预定义的某个作业。
- en: The most common set of tasks Jenkins performs is to scale (or de-scale) a service.
    However, before it attempts to scale, it needs to discover the current number
    of replicas and compare it with the upper and lower limits we set through service
    labels. If scaling would result in a number of replicas that is outside those
    boundaries, it sends a notification to Slack so that a human can decide what should
    be the correct set of actions that will remedy the problem. On the other hand,
    when scaling would keep the number of replicas within the limits, Jenkins sends
    a request to one of Swarm managers which, in turn, increases (or decreases) the
    number of replicas of a service. We’re calling the process self-adaptation because
    the system is adapting to changed conditions without human intervention.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins执行的最常见任务集是调整（或反向调整）服务。然而，在尝试扩展之前，它需要发现当前副本数量，并将其与我们通过服务标签设置的上限和下限进行比较。如果扩展会导致副本数量超出这些边界，它会向Slack发送通知，以便人类决定应采取什么正确的操作来解决问题。另一方面，当扩展会保持副本数量在限制范围内时，Jenkins会向其中一个Swarm管理器发送请求，后者会增加（或减少）服务的副本数量。我们称这个过程为自适应，因为系统在没有人类干预的情况下适应了变化的条件。
- en: '![Figure 10-7: A notification to the system to self-adapt](img/00058.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-7：系统自适应的通知](img/00058.jpeg)'
- en: 'Figure 10-7: A notification to the system to self-adapt'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-7：系统自适应的通知
- en: Even though the goal is to make the system fully autonomous, it is almost sure
    that in some cases human intervention is needed. The cases are, in their essence,
    those that could not be predicted. When something expected happens, let the system
    fix it. On the other hand, call humans when unexpected occurs. In those cases,
    Alertmanager sends a message to the human domain. In our case, that is a [Slack](https://slack.com/)
    message, but it could be any other communication service.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 即使目标是使系统完全自动化，几乎可以肯定在某些情况下需要人工干预。这些情况本质上是无法预测的。当发生预期之外的情况时，让系统修复它。另一方面，当出现意外情况时，呼叫人类。在这些情况下，Alertmanager向人类领域发送消息。在我们的情况下，这是一条[Slack](https://slack.com/)消息，但也可以是任何其他通讯服务。
- en: When you start designing a self-healing system, most of the alerts will fall
    into the “unexpected” category. You cannot predict all the situations that can
    happen to the system. What you can do is make sure that each of those cases is
    unexpected only once. When you receive an alert, your first set of tasks should
    be to adapt the system manually. The second, and equally important, group of actions
    would be to improve the rules in Alertmanager and Jenkins so that the next time
    the same thing happens, the system can handle it automatically.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始设计自愈系统时，大多数警报将属于“意外”类别。您无法预测系统可能发生的所有情况。您可以做的是确保每个这类情况只被视为意外一次。当您收到警报时，您的第一组任务应该是手动调整系统。第二组同样重要的行动是改进Alertmanager和Jenkins的规则，以便在下次发生相同情况时，系统可以自动处理。
- en: '![Figure 10-8: A notification to a human when something unexpected happens](img/00059.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-8：当发生意外情况时向人类发送通知](img/00059.jpeg)'
- en: 'Figure 10-8: A notification to a human when something unexpected happens'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-8：当发生意外情况时向人类发送通知
- en: Setting up a self-adapting system is hard, and it is something that never truly
    ends. It will need continuous improvements. How about self-healing? Is that equally
    hard to accomplish?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 设置自适应系统是困难的，它是永远不会真正结束的事情。它将需要持续改进。那么自愈系统呢？它同样难以实现吗？
- en: Scheduler Role In The System
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统中的调度程序角色
- en: Unlike self-adaptation, self-healing is relatively easy to accomplish. As long
    as there are available resources, a scheduler will make sure that the specified
    number of replicas is always running. In our case, that scheduler is [Docker Swarm](https://docs.docker.com/engine/swarm/).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与自适应不同，自愈相对容易实现。只要有可用资源，调度程序将确保指定数量的副本始终在运行。在我们的情况下，该调度程序是[Docker Swarm](https://docs.docker.com/engine/swarm/)。
- en: Replicas can fail, they can be killed, and they can reside inside an unhealthy
    node. It does not really matter since Swarm will make sure that they are rescheduled
    when needed and (almost) always up-and-running. If all our services are scalable
    and we are running at least a few replicas of each, there will never be downtime.
    Self-healing processes inside Docker will make sure of that while our own self-adaptation
    processes aim to provide high-availability. The combination of the two is what
    makes the system almost fully autonomous and self-sufficient.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 副本可能会失败，可能会被杀死，也可能会驻留在不健康的节点上。但这并不重要，因为 Swarm 会确保在需要时重新调度它们，并且（几乎）总是保持运行。如果我们的所有服务都是可扩展的，并且每个服务至少运行几个副本，就永远不会出现停机时间。Docker
    内部的自我修复过程将确保这一点，而我们的自适应过程旨在提供高可用性。两者的结合使得系统几乎完全自治，且自给自足。
- en: Problems begin piling up when a service is not scalable. If we cannot have multiple
    replicas of a service, Swarm cannot guarantee that there will be no downtime.
    If a replica fails, it will be rescheduled. However, if that replica is the only
    one, the period between a failure and until it is up and running results in downtime.
    It’s a similar situation like with us. We get sick, stay in bed, and, after a
    while, return to work. The problem is if we’re the only employee in the company
    and there’s no one to take over the business while we’re out. The same holds true
    for services. Two replicas is a minimum for any service that hopes to avoid any
    downtime.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务不可扩展时，问题就开始堆积。如果我们无法拥有多个副本，Swarm 就无法保证没有停机时间。如果一个副本失败，它会被重新调度。但是，如果那个副本是唯一的副本，那么从失败到恢复运行之间的这段时间就会导致停机。这就像我们自己一样：我们生病了，躺在床上，过一段时间后才回到工作岗位。问题是，如果我们是公司里唯一的员工，而在我们离开时没有人来接手工作，那么就会造成问题。服务也是如此。两个副本是任何希望避免停机的服务的最小要求。
- en: '![Figure 10-9: Docker Swarm ensures no downtime](img/00060.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-9：Docker Swarm 确保无停机时间](img/00060.jpeg)'
- en: 'Figure 10-9: Docker Swarm ensures no downtime'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-9：Docker Swarm 确保无停机时间
- en: Unfortunately, your services might not be designed with scalability in mind.
    Even when they are, the chances are that some of the third-party services you’re
    using are not. Scalability is an important design decision, and it is an essential
    requirement we should evaluate whenever we’re choosing the next tool we’ll use.
    We need to make a clear distinction between services that must never have downtime
    and those that would not put the system at risk when they are not available for
    a few seconds. Once you make that distinction, you will know which ones must be
    scalable. Scalability is a requirement for no-downtime services.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你的服务可能没有考虑到可扩展性。即使考虑到了，可你所使用的某些第三方服务可能并未做到这一点。可扩展性是一个重要的设计决策，也是我们在选择下一个工具时必须评估的必要条件。我们需要清楚地区分那些绝不能有停机时间的服务和那些在几秒钟不可用时不会对系统造成风险的服务。一旦做出这种区分，你就能知道哪些服务必须具备可扩展性。可扩展性是零停机服务的必要要求。
- en: Cluster Role In The System
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群角色在系统中的作用
- en: Finally, everything we do is inside one or more clusters. There are no individual
    servers anymore. We do not decide what goes where. Schedulers do. From our (human)
    perspective, the smallest entity is a cluster which is a collection of resources
    like memory and CPU.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们所做的一切都在一个或多个集群内。现在不再有独立的服务器。我们不决定什么东西放在哪里。调度器来做决定。从我们的（人类的）角度来看，最小的实体是集群，它是由资源（如内存和
    CPU）组成的集合。
- en: '![Figure 10-10: Everything is a cluster](img/00061.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 10-10：一切都是集群](img/00061.jpeg)'
- en: 'Figure 10-10: Everything is a cluster'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-10：一切都是集群
- en: What Now?
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: We made a short break. I hope it was useful to get to a higher level and see
    what we did from afar. I hope this intermezzo made the picture clearer and that
    you recharged your batteries. There’s still a lot to do, and I hope you’re ready
    for new challenges.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们休息了一下。希望这次短暂的停顿对提升你的层次有所帮助，并且能从更远的角度看我们所做的事情。希望这段插曲能让整体思路更加清晰，同时你也已经重新充电了。还有很多工作要做，我希望你已准备好迎接新的挑战。
