<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0; Secure Your AWS Environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9.  Secure Your AWS Environment </h1></div></div></div><p>Security is unsurprisingly a very hot topic in <span class="emphasis"><em>The Cloud Computing - should you be doing it? debate.</em></span></p><p>On one side we have the <span class="emphasis"><em>my-hardware-is-my-castle</em></span> group of people, who find it deeply unnatural to even think of delegating your compute environment to some abstract entity that assures you that you own the capacity of <span class="emphasis"><em>X</em></span> number of machines at any given time, but which you cannot see or touch. Not to mention the question of your data.</p><p>On the other, we find the people who do not really mind the mystical concept of the cloud at all. Their main interest is in having instant access to somewhat unlimited amount of compute resources at a reasonable cost. Unfortunately, they might occasionally concentrate too much on getting a job done quickly, ignoring some valid, healthy concerns that the former group puts forward.</p><p>Then there is the middle ground - those of us who recognize the sacrifices one has to accept when moving to the cloud as well as the various solutions to make up for those. That is to say, with well-designed applications plus carefully planned-out architecture, your environment can remain adequately secure regardless of the underlying type of hosting platform.</p><p>We are going to examine a few of these solutions and practices in attempt to make our AWS environment more secure.</p><p>We shall cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Managing access using IAM</li><li class="listitem" style="list-style-type: disc">VPC security</li><li class="listitem" style="list-style-type: disc">EC2 security</li><li class="listitem" style="list-style-type: disc">Security auditing</li></ul></div><p>Let us begin.</p><div class="section" title="Managing access using IAM"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec34"/>Managing access using IAM</h1></div></div></div><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization).</em></span></p><p>
<span class="emphasis"><em>ref: <a class="ulink" href="http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html</a></em></span></p></blockquote></div><p>We will be using IAM for managing access (be it user or application) to services under our AWS account.</p><div class="section" title="Securing the root account"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec60"/>Securing the root account</h2></div></div></div><p>When a new AWS account is opened, it comes with a single user (the account owner) also referred to as the <span class="strong"><strong>root login</strong></span>. This almighty user has all the powers, including the option of terminating the AWS account. For this reason, it is often advised that the root login is only used for high-level account management purposes while any day-to-day operations are done via IAM user accounts.</p><p>We shall follow this recommendation, so the very first thing we do after registering an AWS account is to login as <span class="strong"><strong>root</strong></span>, disable any unnecessary authentication mechanisms and create ourselves a lower-privileged IAM user account.</p><p>Let us browse to the AWS Console (ref: <a class="ulink" href="https://console.aws.amazon.com/console/home">https://console.aws.amazon.com/console/home</a>):</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_001.jpg" alt="Securing the root account"/></div><p>
</p><p>Notice the small print underneath the <span class="strong"><strong>Sign In</strong></span> button. This is the link we need to follow in order to access the root account, which takes us to a slightly different login page as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_002.jpg" alt="Securing the root account"/></div><p>
</p><p>Here, use your main Amazon credentials; you should see the familiar Console page. click on the name in the top-right corner:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_003.jpg" alt="Securing the root account"/></div><p>
</p><p>Choosing <span class="strong"><strong>Security Credentials</strong></span> takes us to our root account security options:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_004.jpg" alt="Securing the root account"/></div><p>
</p><p>Enable <span class="strong"><strong>Multi-Factor Authentication (MFA);</strong></span> there really isn't a good reason not to. You could purchase a hardware token device or simply use an app on your phone such as the <span class="strong"><strong>Google Authenticator</strong></span>.</p><p>Delete the keys under <span class="strong"><strong>Access Keys</strong></span>. These are used for API access, which you are very likely not going to need for account management tasks.</p><p>Next, click on the <span class="strong"><strong>Account Settings</strong></span> link on the left, to update the current password policy. With the various password management tools available today, choosing a complex password and changing it often is no longer an inconvenience, so go crazy:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_005.jpg" alt="Securing the root account"/></div><p>
</p><p>On the same page, we can disable any regions we are not going to be using:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_006.jpg" alt="Securing the root account"/></div><p>
</p><p>Now we proceed to create the IAM accounts for daily AWS usage. We will organize our users into groups. We start with a user in a group which has administrator privileges, which can then be used to manage almost all aspects of the AWS account.</p><p>On the left, select <span class="strong"><strong>Groups</strong></span> and create a new group, granting it administrator access. Then under <span class="strong"><strong>Users</strong></span>, create an account for yourself and make it a member of that group.</p><p>During the user creation process you would have had the option to create API access keys (you could also do it at a later stage too), which are useful if you are planning to use the AWS CLI or programmatic access in general. Once created, select the user and switch to the <span class="strong"><strong>Security Credentials</strong></span> tab:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_007.jpg" alt="Securing the root account"/></div><p>
</p><p>Here you have the option to create an <span class="strong"><strong>Access Keys</strong></span> pair, if you did not do so earlier, as well as set a password for using the AWS Console. As mentioned earlier, you should take the opportunity to enable <span class="strong"><strong>MFA</strong></span> (to take this a step further, have a look at <a class="ulink" href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html</a>). Also if you are planning to use the CodeCommit service over SSH, this is where you upload your public key.</p><p>This is it, from now on you can login to the AWS Console using the username and password of the IAM account you just created, keeping the root for special occasions.</p><p>As a side note for those who might already maintain a user database external to AWS, there are ways to integrate it using <span class="strong"><strong>Federation</strong></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note80"/>Note</h3><p>For more details, see either of these links:
<a class="ulink" href="https://aws.amazon.com/iam/details/manage-federation">https://aws.amazon.com/iam/details/manage-federation
</a> <a class="ulink" href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html</a>
</p></div></div></div></div></div>
<div class="section" title="VPC security"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec35"/>VPC security</h1></div></div></div><p>If you have deployed your resources in a VPC, you are already moving in the right direction. Here we are mostly going to concern ourselves with network security and the tools or features a VPC provides for enhancing it.</p><div class="section" title="Security Groups"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec61"/>Security Groups</h2></div></div></div><p>These represent our first layer of defense as stated in the AWS documentation. <span class="strong"><strong>Security Groups</strong></span> (<span class="strong"><strong>SG</strong></span>) get assigned to EC2 instances (generally speaking) and provide a type of stateful firewall, which supports allow rules only.</p><p>They are very flexible and an EC instance can have multiple such groups assigned to it. The rules can be based on host IP addresses, CIDRs or even on other Security Groups, for example, allow inbound <code class="literal">HTTP:80</code> from group ID <code class="literal">sg-12345</code>.</p><p>Usually, within a VPC we would create an SG per role, such as <span class="strong"><strong>web server</strong></span>, <span class="strong"><strong>db</strong></span>, <span class="strong"><strong>cache</strong></span>. Instances of the same component would then be assigned the respective SG, thus regulating traffic between the different components of a platform.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip81"/>Tip</h3><p>It is often tempting to allow traffic based on the VPC CIDR address, resting on the fact that the VPC is largely an isolated environment. Resist that as much as possible and limit access to components that actually need it.</p><p>The db SG should allow traffic from/to the web server SG, but possibly not from the cache one.</p></div></div></div><div class="section" title="Network ACLs"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec62"/>Network ACLs</h2></div></div></div><p>The second layer comes in the form of Network ACLs.</p><p>The ACLs are stateless, they apply to the underlying subnet that an instance lives in and their rules are evaluated based on priority, just like an old fashioned firewall. As a bonus, you can also set deny policies.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip82"/>Tip</h3><p>Network ACLs sit at the edge of the VPC, hence are evaluated before traffic reaches any Security Groups. This feature plus the ability to set deny rules make them very suitable for reacting to potential DDOS threats.</p></div></div><p>Overall, both types of traffic management have their place in our VPC security design. ACLs should store a set of broader, less frequently changing rules, complemented by flexible Security Groups for fine-grained control.</p></div><div class="section" title="VPN gateway"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec63"/>VPN gateway</h2></div></div></div><p>If it so happens that you are using a VPC as an extension to your on-premise infrastructure, it would make a lot of sense to have the two sides more tightly connected.</p><p>Instead of restricting external access via Security Groups or ACLs, you could create a secure VPN channel, benefiting from the implied encryption.</p><p>You can connect your VPC to your office network using either a hardware or a software VPN solution (ref: <a class="ulink" href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a>).</p><p>For more demanding use-cases, one could even route their VPN traffic over a high-speed direct link to AWS using the AWS Direct Connect service (ref: <a class="ulink" href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html">http://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html</a>).</p></div><div class="section" title="VPC peering"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec64"/>VPC peering</h2></div></div></div><p>In a similar situation, where instead of your office network you have another VPC which needs to communicate with your, let us call it primary one, you could use VPC peering:</p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single region.</em></span></p><p><span class="emphasis"><em>AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.</em></span>
</p><p><span class="emphasis"><em>ref: <a class="ulink" href="http://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/vpc-peering-overview.html">http://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/vpc-peering-overview.html</a>
</em></span>
</p></blockquote></div><p>Your VPCs will be able to communicate directly (within the same region) so you will not need to expose any services that do not explicitly need to be exposed. In addition, you can conveniently keep using private addresses for communication.</p></div></div>
<div class="section" title="EC2 security"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec36"/>EC2 security</h1></div></div></div><p>Diving deeper into our VPC, we are now going to look at ways to enhance the security around our EC2 instances.</p><div class="section" title="IAM Roles"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec65"/>IAM Roles</h2></div></div></div><p>
<span class="strong"><strong>IAM EC2 Roles</strong></span> are the recommended way to grant your application access to AWS services.</p><p>As an example, let us assume we had a web app running on our web server EC2 instance and it needs to be able to upload assets to S3.</p><p>A quick way of satisfying that requirement would be to create a set of IAM access keys and hardcode those into the application or its configuration. This however means that from that moment on it might not be very easy to update those keys unless we perform an app/config deployment. Furthermore, we might for one reason or another end up re-using the same set of keys with other applications.</p><p>The security implications are evident: reusing keys increases our exposure if those get compromised and having them hardcoded greatly increases our reaction time (it takes more effort to rotate such keys).</p><p>An alternative to the preceding method would be to use Roles. We would create an EC2 Role, grant it write access to the S3 bucket and assign it to the web server EC2 instance. Once the instance has booted, it is given temporary credentials which can be found in its metadata and which get changed at regular intervals. We can now instruct our web app to retrieve the current set of credentials from the instance metadata and use those to carry out the S3 operations. If we were to use the AWS CLI on that instance, we would notice that it fetches the said metadata credentials by default.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip83"/>Tip</h3><p>Roles can be associated with instances only at launch time, so it is a good habit to assign one to all your hosts even if they do not need it right away.</p></div></div><p>Roles can be used to assume other roles, making it possible for your instances to temporarily escalate their privileges by assuming a different role within your account or even across AWS accounts (ref: <a class="ulink" href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html">http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a>).</p></div><div class="section" title="SSH access"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec66"/>SSH access</h2></div></div></div><p>The most common way to interact with an EC2 instance would be over SSH. Here are a couple of ideas to make our SSH sessions even more secure.</p><div class="section" title="Individual keys"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec34"/>Individual keys</h3></div></div></div><p>When a vanilla EC2 instance is launched it usually has a set of PEM keys associated with it to allow initial SSH access. If you also work within a team, my recommendation would be not to share that same key pair with your colleagues.</p><p>Instead, as soon as you, or ideally your configuration management tool, gain access to the instance, individual user accounts should be created and public keys uploaded for the team members (plus <code class="literal">sudo</code> access where needed). Then the default <code class="literal">ec2-user</code> account (on Amazon Linux) and PEM key can be removed.</p></div><div class="section" title="Entrypoint"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec35"/>Entrypoint</h3></div></div></div><p>Regardless of the purpose that an EC2 instance serves, it is rarely the case that you must have direct external SSH access to it.</p><p>Assigning public IP addresses and opening ports on EC2 instances is often an unnecessary exposure in the name of convenience and somewhat contradicts the idea of using a VPC in the first place.</p><p>SSH can unarguably be useful however. So, to maintain the balance between the forces, one could setup an SSH gateway host with a public address. You would then restrict access to it to your home and/or office network and permit SSH connections from that host towards the rest of the VPC estate.</p><p>The chosen node becomes the administrative entry point of the VPC.</p></div></div><div class="section" title="ELBs everywhere"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec67"/>ELBs everywhere</h2></div></div></div><p>Latency is of importance. You will find brilliant engineering articles online from expert AWS users who have put time and effort into benchmarking ELB performance and side-effects.</p><p>Perhaps not surprisingly their findings show that there is a given latency penalty with using an ELB, as opposed to serving requests directly off of a backend web server farm. The other side to this however is the fact that such an additional layer, be it an ELB or a cluster of custom HAProxy instances, acts as a shield in front of those web servers.</p><p>With a balancer at the edge of the VPC, web server nodes can remain within the private subnet which is not a small advantage if you can afford the said latency trade-off.</p></div><div class="section" title="HTTPS by default"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec68"/>HTTPS by default</h2></div></div></div><p>Services like the <span class="strong"><strong>AWS Certificate Manager</strong></span>, make using SSL/TLS encryption even easier and more affordable. You get the certificates plus automatic renewals for free (within AWS).</p><p>Whether traffic between an ELB and the backend instances within a VPC should be encrypted is another good question, but for now please do add a certificate to your ELBs and enforce HTTPS where possible.</p></div><div class="section" title="Encrypted storage"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec69"/>Encrypted storage</h2></div></div></div><p>Logically, since we are concerned with encrypting our HTTP traffic, we should not ignore our data at rest.</p><p>The most common type of storage on AWS must be the EBS volume with S3 right behind it. Each of the two services supports a strong and effortless implementation of encryption.</p><div class="section" title="EBS volumes"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec36"/>EBS volumes</h3></div></div></div><p>First, it should be noted that not all EC2 instance types support encrypted volumes. Before going any further, please consult this table: <a class="ulink" href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_supported_instances">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_supported_instances</a>
</p><p>Also, let us see what does get encrypted and how:</p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</em></span></p><p><span class="emphasis"><em>- Data at rest inside the volume</em></span></p><p><span class="emphasis"><em>- All data moving between the volume and the instance</em></span></p><p><span class="emphasis"><em>- All snapshots created from the volume</em></span></p><p><span class="emphasis"><em>The encryption occurs on the servers that host EC2 instances, providing encryption of data-in-transit from EC2 instances to EBS storage.</em></span></p><p><span class="emphasis"><em>ref: <a class="ulink" href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a>
</em></span>
</p></blockquote></div><p>Note that the data gets encrypted on the servers that host EC2 instances, that is to say the hypervisors.</p><p>Naturally, if you wanted to go the extra mile you could manage your own encryption on the instance itself. Otherwise, you can be reasonably at peace knowing that each volume gets encrypted with an individual key which is in turn encrypted by a master key associated with the given AWS account.</p><p>In terms of key management, AWS recommends that you create a custom key to replace the one which gets generated for you by default. Let us create a key and put it to use.</p><p>On the IAM dashboard, select <span class="strong"><strong>Encryption Keys</strong></span> on the left:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_008.jpg" alt="EBS volumes"/></div><p>
</p><p>Choose to <span class="strong"><strong>Create Key</strong></span> and fill in the details:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_009.jpg" alt="EBS volumes"/></div><p>
</p><p>Then you can define who can manage the key:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_010.jpg" alt="EBS volumes"/></div><p>
</p><p>As well as who can use it:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_011.jpg" alt="EBS volumes"/></div><p>
</p><p>And the result should be visible back on that dashboard among the list of keys:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_012.jpg" alt="EBS volumes"/></div><p>
</p><p>Now if you were to switch to the EC2 Console and choose to create a new EBS volume, the custom encryption key should be available as an option:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_013.jpg" alt="EBS volumes"/></div><p>
</p><p>You can now proceed to attach the new encrypted volume to an EC2 instance as per the usual process.</p></div><div class="section" title="S3 objects"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec37"/>S3 objects</h3></div></div></div><p>S3 allows the encryption of all, or a selection of objects within a bucket with the same <span class="strong"><strong>AES-256</strong></span> algorithm as EBS here.</p><p>A few methods of key management are available (ref: <a class="ulink" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a>):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">You can import your own, external set of keys</li><li class="listitem" style="list-style-type: disc">You can use the KMS service to generate custom keys within AWS</li><li class="listitem" style="list-style-type: disc">You can use the S3 service default (unique) key</li></ul></div><p>Encrypting existing data can be done on the folder level:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_014.jpg" alt="S3 objects"/></div><p>
</p><p>or by selecting individual files:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_015.jpg" alt="S3 objects"/></div><p>
</p><p>New data is encrypted on demand by either specifying a header (<code class="literal">x-amz-server-side-encryption</code>) in the <code class="literal">PUT</code> request or by passing any of the <code class="literal">--sse</code> options if using the AWS S3 CLI.</p><p>It is also possible to deny any upload attempts which do not specify encryption by using a bucket policy (ref: <a class="ulink" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a>).</p></div></div><div class="section" title="OS updates"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec70"/>OS updates</h2></div></div></div><p>If you follow any security bulletins, you would have noticed the frequency with which new security flaws are being published. So, it is probably not much of an exaggeration to state that OS packages become obsolete days if not hours after a fully up-to-date EC2 instance has been provisioned. And unless the latest vulnerability is affecting BASH or OpenSSL, we tend to take comfort in the fact that most of our hosts reside within an isolated environment (such as a VPC), postponing updates over and over again.</p><p>I believe we all agree this is a scary practice, which likely exists due to the anxiety that accompanies the thought of updating live, production systems. There is also a legitimate degree of complication brought about by services such as <span class="strong"><strong>Auto Scaling</strong></span>, but this can be turned to an advantage. Let us see how.</p><p>We'll separate a typical EC2 deployment into two groups of instances: <span class="emphasis"><em>static(non-autoscaled)</em></span> and <span class="emphasis"><em>autoscaled</em></span>. Our task is to deploy the latest OS updates to both.</p><p>In the case of static instances, where scaling is not an option due to some application specific or other type of limitation, we will have to resort to the well-known approach of first testing the updates in a completely separate environment then updating our static production hosts (usually one at a time).</p><p>With Auto Scaling however, OS patching can be a much more pleasant experience. You will recall Packer and Serverspec from previous chapters, where we used these tools to produce and test AMIs. A similar Jenkins pipeline can also be used for performing OS updates:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Launch the source AMI.</li><li class="listitem">Perform a package update.</li><li class="listitem">Run tests.</li><li class="listitem">Package a new AMI.</li><li class="listitem">Proceed with a phased deployment in production.</li></ol></div><p>To be comfortable with this process, we certainly need to put a decent amount of effort into ensuring that tests, deployment and rollback procedures are as reliable as practically possible, but then the end justifies the means.</p></div></div>
<div class="section" title="Security auditing"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec37"/>Security auditing</h1></div></div></div><p>AWS offers some good tools to help you keep your security policies in shape. Those will provide you with detailed audit reports including advice on how to improve any potential risk areas. In addition, you can configure service logs, so you get a better understanding what goes on within your deployment or AWS account as a whole.</p><div class="section" title="VPC Flow Logs"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec71"/>VPC Flow Logs</h2></div></div></div><p>This service lets you capture information about the network traffic flowing through a VPC. The generated logs (unfortunately not quite real-time yet) contain src/dst port, src/dst address, protocol and other related details (for a full list please see: <a class="ulink" href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#flow-log-records">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#flow-log-records</a>). Apart from making for some pretty cool graphs to help identify network bottlenecks, the data can also be used for spotting unusual behavior. You could, for example, devise an in-house IDS by parsing the <span class="strong"><strong>Flow Logs</strong></span> and forwarding any suspicious entries to your monitoring solution.</p><p>In the VPC Console, select a VPC and switch to the <span class="strong"><strong>Flow Logs</strong></span> tab:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_016.jpg" alt="VPC Flow Logs"/></div><p>
</p><p>Click on <span class="strong"><strong>Create Flow Log</strong></span>: you will need to fill a few parameters such as the IAM Role to be used (click on <span class="strong"><strong>Set Up Permissions</strong></span> to create one) and the desired name of the <span class="strong"><strong>Destination Log Group</strong></span>.</p><p>In a few minutes, the said log group should appear under the <span class="strong"><strong>Logs</strong></span> section in the <span class="strong"><strong>CloudWatch</strong></span> dashboard:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_017.jpg" alt="VPC Flow Logs"/></div><p>
</p><p>Within that group, you will find a log stream per EC2 instance (per network interface to be more precise) containing the captured traffic details.</p></div><div class="section" title="CloudTrail"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec72"/>CloudTrail</h2></div></div></div><p>The <span class="strong"><strong>CloudTrail</strong></span> service is used for recording API activity within an AWS account. This includes requests done via the AWS Console, the CLI, the SDK or other services which issue calls on your behalf. The trail can be helpful for both security auditing and troubleshooting. Collected data is stored in S3 as encrypted objects, along with signed hashes to help ensure no tampering has occurred.</p><p>To enable the service, we go to the <span class="strong"><strong>CloudTrail</strong></span> dashboard looking for a <span class="strong"><strong>Get Started</strong></span> or an <span class="strong"><strong>Add new trail</strong></span> button:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_018.jpg" alt="CloudTrail"/></div><p>
</p><p>We have chosen to collect data from all regions, storing it in a new S3 bucket with validation turned on. It is also possible to receive notifications on each log delivery, which can be useful for any further processing jobs.</p><p>Back on the dashboard, we click on the new trail to review its settings:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_019.jpg" alt="CloudTrail"/></div><p>
</p><p>We enable encryption, then enter a name for the new KMS key. After approximately 15 minutes, we should see events appearing under the API activity history dashboard tab:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_020.jpg" alt="CloudTrail"/></div><p>
</p><p>Expanding any of these entries would provide additional information such as the <code class="literal">access_key</code> used for the given API call and source IP.</p><p>In the S3 bucket we would find two subfolders: <span class="strong"><strong>CloudTrail</strong></span> which holds the API logs and <span class="strong"><strong>CloudTrail-Digest</strong></span> for the file hashes.</p></div><div class="section" title="Trusted Advisor"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec73"/>Trusted Advisor</h2></div></div></div><p>The <span class="strong"><strong>Advisor</strong></span> is enabled by default and periodically reviews your AWS account in order to identify any risk or areas of improvement.</p><p>It provides insights about cost, performance, security and HA as seen on the dashboard:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_021.jpg" alt="Trusted Advisor"/></div><p>
</p><p>We are mainly interested in the security tips at this time:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_022.jpg" alt="Trusted Advisor"/></div><p>
</p><p>Things appear to be green, following the steps we took to secure the root account earlier in the chapter.</p><p>In addition to this view, weekly e-mail reports can be configured under the <span class="strong"><strong>Preferences</strong></span> tab.</p></div><div class="section" title="AWS Config"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec74"/>AWS Config</h2></div></div></div><p>With <span class="strong"><strong>Config</strong></span> we can track, inspect, and alert on resource changes that have occurred within our deployment.</p><p>When first enabled, the service performs an inventory of the resources found within the region and starts recording any changes.</p><p>Once a resource change is detected, for example a new rule is added to a security group, Config allows us to view a timeline with details about the current and any previous changes to that resource.</p><p>Another powerful feature is change inspection. Within Config we can define a set of rules to be evaluated against each resource change and alerts generated where necessary.</p><p>Let us try both use-cases.</p><p>Click on <span class="strong"><strong>Get Started</strong></span> on the Config dashboard, then choose a <span class="strong"><strong>Bucket name</strong></span> and a <span class="strong"><strong>Role name</strong></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_023.jpg" alt="AWS Config"/></div><p>
</p><p>One the next page we can choose a few rules to get us started:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_024.jpg" alt="AWS Config"/></div><p>
</p><p>We have chosen to monitor CloudTrail, EBS volumes and MFA settings. Finalize the setup and go back to the <span class="strong"><strong>Rules</strong></span> tab in the dashboard where we can add some more.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note84"/>Note</h3><p>Please note that at the time of writing, there is a cost of $2 per active rule per month.</p></div></div><p>Click on <span class="strong"><strong>Add rule</strong></span> and look for the <span class="strong"><strong>restricted-ssh</strong></span> rule which will monitor security groups for open SSH access. With the new rule in place, we can make a few resource changes and see how Config reacts to these. As an example, disable CloudTrail and create a temporary security group which allows incoming SSH from anywhere.</p><p>After a short while we can see the effect on the <span class="strong"><strong>AWS Config</strong></span> dashboard:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_025.jpg" alt="AWS Config"/></div><p>
</p><p>We can click on the <span class="strong"><strong>restricted-ssh</strong></span> entry for more details. Locate the noncompliant entry in the list and click the <span class="strong"><strong>AWS Config</strong></span> timeline icon:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_026.jpg" alt="AWS Config"/></div><p>
</p><p>We can see the two recorded states of the resource. Clicking on the <span class="strong"><strong>Change</strong></span> shows us what has happened:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_027.jpg" alt="AWS Config"/></div><p>
</p><p>Here we see the reason why our security group resource has been flagged as <span class="strong"><strong>noncompliant</strong></span>.</p><p>In addition to the AWS-provided Config rules, you could write your own in the form of Lambda functions (ref: <a class="ulink" href="http://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules.html">http://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_develop-rules.html</a>).</p></div><div class="section" title="Self pen testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec75"/>Self pen testing</h2></div></div></div><p>Here we examine self pen testing as an inexpensive alternative or as a preparation step prior to you hiring a third party for the official test (considering that each penetration testing iteration is usually chargeable).</p><p>The goal is a system which allows for on-demand and/or regular vulnerability scanning against our VPC deployment both internally and externally.</p><p>Two community projects that can help us with this task are <span class="strong"><strong>OpenVAS</strong></span> (ref: <a class="ulink" href="http://www.openvas.org">http://www.openvas.org</a>) and <span class="strong"><strong>OpenSCAP</strong></span> (ref: <a class="ulink" href="https://www.open-scap.org">https://www.open-scap.org</a>).</p><p>A relatively easy way of setting up such an automated scanner would be to use a prebaked AMI and some user data. In essence, you would install either or both of the preceding frameworks on a vanilla EC2 instance and create an AMI out of it. Then launch a new instance of that AMI (perhaps per schedule) and, using user data, you would start the scanner, pass it the destination URI to be scanned, then e-mail any scan reports or save to S3.</p><p>Scheduling is achieved using an Auto Scale Group, which simply launches a node, then terminates it after N hours (however long it takes to perform the scan). Alternatively, you could use CloudWatch events together with some Lambda functions (ref: <a class="ulink" href="https://aws.amazon.com/premiumsupport/knowledge-center/start-stop-lambda-cloudwatch">https://aws.amazon.com/premiumsupport/knowledge-center/start-stop-lambda-cloudwatch</a>).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note85"/>Note</h3><p>Please note that vulnerability scanning or similar activity needs to be approved by AWS Support first (ref: <a class="ulink" href="https://aws.amazon.com/forms/penetration-testing-request">https://aws.amazon.com/forms/penetration-testing-request</a>).</p></div></div><p>Following the advice throughout this  chapter is one step towards creating a more secure environment, but we can by no means consider the job done. It has been said that security is a process, not a product and as such it should perhaps be a daily task on one's list.</p><p>It is recommended that you subscribe to relevant security feeds or mailing lists.</p><p>AWS maintains a few of its own:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://aws.amazon.com/blogs/security">https://aws.amazon.com/blogs/security</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://aws.amazon.com/security/security-bulletins/">https://aws.amazon.com/security/security-bulletins/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://alas.aws.amazon.com/">https://alas.aws.amazon.com/</a></li></ul></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec38"/>Summary</h1></div></div></div><p>In this chapter we covered some ideas on how to improve the overall security of an AWS account.</p><p>We looked at AWS services which can be used for auditing and alerting on suspicious activity plus open-source tools that can be useful for regular vulnerability scanning.</p><p>In the next chapter we will look at a list of popular (and less so) AWS tips and tricks.</p></div></body></html>