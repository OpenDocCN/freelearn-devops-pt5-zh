<html><head></head><body>
        

                            
                    <h1 class="header-title">Containers at Scale with Kubernetes</h1>
                
            
            
                
<p>Kubernetes is by far one of the most popular open source projects to take the IT world by storm. It seems like almost everywhere you go, every blog you read, or news article you encounter, tells the tale of how Kubernetes has revolutionized the way DevOps and IT infrastructure are handled. With good reason, Kubernetes has truly taken a firm grasp of the IT landscape and introduced new concepts and ways of looking at infrastructure like no other platform before it. You might be in the camp of IT professionals who have heard of Kubernetes, but you have no idea what it is or how it can really benefit your infrastructure. Or, you could be where most of us are today, in the process of containerizing applications and workloads but don’t want to dabble in the additional complexity and murky water of Kubernetes just yet. Finally, you could be one of those lucky DevOps engineers or IT administrators who have successfully adopted containers and Kubernetes and is able to really reap the reliability and flexibility that Kubernetes provides.</p>
<p>The purpose of this chapter is to provide an overview of what Kubernetes is, how it works (at a high level), and how to deploy your containerized applications to Kubernetes clusters using Ansible Container. Before we dive in too deep, you might ask, what is Kubernetes exactly? Kubernetes is a platform developed by Google for deploying, managing, configuring, and orchestrating containers at both small and large scales. Kubernetes was started as an internal project at Google, known as <strong>Borg</strong>, for managing the automatic deployment and scaling of containers across the vast Google infrastructure footprint. Based on some real-world lessons learned with Borg, Google released Kubernetes as an open source project so that other users and organizations could leverage the same flexibility to deploy containers at scale. Using Kubernetes, one can easily run containerized applications across multiple clustered nodes, automatically maintaining the desired number of replicas, service endpoints, and loadbalancing across the cluster.</p>
<p>Throughout this book, we have looked closely at how we can use the Ansible Container platform to quickly and reliably build container images using Ansible Playbooks and run those containers on our local workstation. Since we now understand quite well how to build version control and configuration management inside of our containers, the next step is using configuration management to declare how our applications should run outside of our container. This is the gap that Kubernetes fills. And, yes, it is just as awesome as it sounds. Ready? Let’s get started.</p>
<p>Throughout this chapter, we will cover:</p>
<ul>
<li class="mce-root">A brief overview of Kubernetes</li>
<li class="mce-root">Getting started using Google Cloud Engine</li>
<li class="mce-root">Deploying an application in Kubernetes using kubectl</li>
<li class="mce-root">Writing a Kubernetes manifest</li>
<li class="mce-root">Deploying and updating containers in Kubernetes</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">A brief overview of Kubernetes</h1>
                
            
            
                
<p>Admittedly, when one thinks of Kubernetes, one might immediately think of the complexity and multifaceted hierarchy of concepts associated with Kubernetes and be quick to think that this chapter will be over the reader's head in terms of how to understand and apply these concepts. Most users who have unsuccessfully attempted to venture into Kubernetes in the past may still feel the scars and be wary about moving forward with Kubernetes. Container automation using Kubernetes can quickly get quite complicated, but the rewards for learning and using Kubernetes are vast. Before we go forward, I must stress to the reader that Kubernetes is quite a complex platform. Attempting to explain in detail every feature and function of Kubernetes would take an entire book, if not longer. In fact, there has been a multitude of books written on container orchestration using Kubernetes that I would direct the reader's attention to should you would want to dig deeper into your understanding of these concepts. The point of this chapter is to introduce the reader to a basic understanding of what Kubernetes is, the primary functionality, and how the reader can quickly get started using it to optimize the deployment of containers. There is a lot more to be said about Kubernetes than the scope of this book has the time to go into, so if the reader wants to learn more about Kubernetes, I would strongly suggest checking out the documentation on the Kubernetes website at <a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a>.</p>
<p>Throughout the book so far, we have looked at using Ansible Container to build Docker containers that run on our local workstation or a remote server that has Docker installed and running on it. Docker provides us with a usable container runtime environment that has the functionality to start containers, expose ports, mount system volumes, and provide basic network connectivity using a bridged interface and IP Network Address Translation, or NAT. Docker does a very good job at running containers but does not provide the user with very much functionality beyond that. What happens when your container crashes? What do you do when you need to scale out your application to more nodes, racks, or data centers? What happens when a container on one host needs to access resources in a container running on a separate host? This is the exact type of use case that tools such as Kubernetes address. Think of Kubernetes essentially as a service that uses a scheduler and API to proactively monitor the current state of containers running in Docker (or another container runtime) and continuously attempts to drive it towards the desired state specified by the operator. For example, say you have a 4-node Kubernetes cluster running 3 instances of your application container. If the operator (you) instructs the Kubernetes API that you want the fourth instance of your application container running, Kubernetes will identify that you currently have three running instances and immediately schedule a fourth container to be created. Using a bin-packing algorithm, Kubernetes intrinsically understands that containers should be scheduled to run on separate hosts to provide high availability and make the most use of cluster resources. In the example above, the fourth container scheduled will most likely be scheduled to run on the fourth cluster host, provided no outstanding configuration has been put into place that would prevent new container workloads from running on that host. Furthermore, if one of the hosts in our cluster goes down, Kubernetes is intelligent enough to recognize the disparity and reschedule those workloads to run on different hosts until the downed node has been restored.</p>
<p>In addition to the flexible configuration management capabilities Kubernetes provides, it is also known for its unique ability to provide resilient networking resources to containers such as service discovery, DNS resolution, and load balancing across containers. In other words, Kubernetes has the innate ability to provide internal DNS resolution based on the services running in the cluster. When new pods are added to the service, Kubernetes will automatically see the new containers and update the DNS endpoints so that the new containers can be served by calling the internal service domain name within the cluster. This ensures that other containers can talk directly to other containerized services by calling internal domain names and cluster IP addresses within the Kubernetes overlay network.</p>
<p>Kubernetes incorporates many new concepts that might be somewhat foreign if you come from a background of working with static container deployments. Throughout this chapter, these concepts will be referred to as we learn more about Kubernetes, so it is important to have a grasp of what these terms mean as we go forward:</p>
<ul>
<li class="mce-root"><strong>Pod</strong>: a pod represents one or more application containers running in the Kubernetes cluster. By default, a pod definition names at least one container that the user wishes to run in the cluster, including any additional environment variables, command or entrypoint configuration the user wants the pod to run with. If the pod includes more than one container definition, all containers running in the pod share the pod network and storage resources. For example, you could run a pod that consisted of a web server container as well a caching server. From the perspective of the pod, the web server might run on the localhost port 80, and the cache would likewise run on localhost port <kbd>11211</kbd>. From the perspective of Kubernetes, the pod itself would have a single IP address internal to the cluster the services would be exposed on, but in reality, would consist of two entirely separate containers.</li>
<li class="mce-root"><strong>Deployment</strong>: A deployment is an object in Kubernetes that defines pods which will be running in the cluster. Deployments consist of a variety of parameters, such as the name of the container image, volume mounts, and the number of replicas to run. In order to delete pods from a Kubernetes cluster, the deployment must be deleted. If you simply attempt to delete pods, you will see that Kubernetes attempts to recreate those pods. This happens due to the fact that the deployment object is informing the cluster that those pods should be running, and the controller manager (more on this later) will attempt to bring the cluster back into the desired state.</li>
<li class="mce-root"><strong>Labels</strong>: Labels are key-value pairs that can be assigned to almost any object in Kubernetes. Labels can be assigned to objects to organize subsets of resources in the cluster. For example, if you have a cluster that runs multiple deployments of the same pods, they can be labeled differently to indicate different uses. Labels can even be leveraged to by the scheduler to determine where and when pods should be running in across the cluster.</li>
<li class="mce-root"><strong>Service</strong>:<strong> </strong>A service defines a logical subgrouping of pods (usually by a label selector) and the methods by which they should be accessed by other resources in the cluster. For example, you could create a service that exposes a set of pods to the outside world. A selector such as a label could be used to determine which pods should be exposed. Later, if pods are added or removed from the cluster, Kubernetes will automatically scale the service, provided the new pods are running with the same selector.</li>
</ul>
<p>To make this functionality transparent, Kubernetes provides multiple services running in the cluster that work in conjunction to ensure that the cluster and applications are continuously in the desired state. Collectively, these services are known as the Kubernetes Control Plane. The control plane is what allows the function, manage running containers, and maintain the state of  nodes and resources across the cluster. Let’s take a quick look at those now:</p>
<ul>
<li class="mce-root"><strong>KubeCTL</strong>:<strong> </strong><kbd>kubectl</kbd>, (pronounced kube-control), is the command-line tool for interacting with Kubernetes. <kbd>Kubectl</kbd> gives you direct access to the Kubernetes API to schedule new deployments, interact with Pods, expose deployments, and more. The <kbd>kubectl</kbd> tool requires a set of credentials in order to access the Kubernetes API.</li>
<li class="mce-root"><strong>Kubernetes API Server</strong>: The Kubernetes API server is responsible for accepting input from the operator, either from the <kbd>kubectl</kbd> command-line tool or by direct access to the API itself. The Kubernetes API is responsible for coordinating information to the rest of the cluster to execute the desired state. It should also be noted that the Kubernetes API server depends on the ETCD service to store and retrieve information about the cluster nodes and services running in the cluster.</li>
<li class="mce-root"><strong>Kubernetes Scheduler</strong>: The Kubernetes scheduler is responsible for scheduling new workloads across the cluster nodes. Core to this responsibility is monitoring the cluster to ensure that available resources are present in the cluster to run pods, as well as ensuring that servers are available and reachable.</li>
<li class="mce-root"><strong>Kubernetes Controller Manager</strong>: The controller manager is primarily concerned with desired state compliance across the cluster. The controller manager service interacts with the ETCD service and watches for new jobs and requests coming in through the API server. When a new request is received and stored in ETCD, the controller manager kicks off a new job in tandem with the scheduler to ensure that the cluster is in the desired state defined by the operator. The controller manager accomplishes this by using control loops to continuously monitor the state of the cluster and immediately correct any discrepancies it sees between the current and desired state. When you delete a Kubernetes pod and a new one automatically gets created, you have the controller manager to thank for that.</li>
<li class="mce-root"><strong>ETCD</strong>: ETCD is a distributed key-value store created by CoreOS, which is used to store configuration information across the Kubernetes cluster. As stated previously, ETCD is primarily written to by the Kubernetes API server.</li>
<li class="mce-root"><strong>Container Networking Interface</strong>: The Container Network Interface project, or CNI, attempts to bring additional network functionality then what comes out of the box with Kubernetes. CNI provides interfaces and plugin support to allow various network plugins to be deployed within Kubernetes clusters. This allows Kubernetes to provide overlay network connectivity to containers distributed across hosts so that containers do not have to rely on the relatively limited networking space provided on the Kubernetes hosts. Common third-party plugins that implement the CNI standard are Flannel, Weave, and Calico.</li>
<li class="mce-root"><strong>Kubelet</strong>: Kubelet is a service which runs on every host in the Kubernetes cluster. The Kubelet’s primary responsibility is to leverage the underlying container runtime (Docker or rkt) to create and manage pods on cluster nodes according to the instructions received by the API, the scheduler, and controller manager. The Kubelet service does not manage containers or pods running on the host that were not created by Kubernetes. Think of the Kubelet as the translation layer between Docker and Kubernetes.</li>
</ul>
<p>Now that we have an understanding of the Kubernetes platform and how it works, we can start using Kubernetes to run some of the containers we built earlier in this book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting started with the Google Cloud platform</h1>
                
            
            
                
<p>Throughout the many chapters in this book, we have worked primarily in a single-node Vagrant lab that comes preloaded with most of the tools and utilities you need to get started using Docker and Ansible Container to initialize, build, run, and destroy containers through the various examples and lab exercises. Unfortunately, due to the complexity of Kubernetes, it is very difficult to run a Kubernetes environment within the Vagrant lab we have used so far. There are methods, but they would require more computing power and explanation that extends beyond the scope of this book. As a solution, I would suggest that the reader signs up for a free-tier account on Google Cloud Platform to quickly spin up a three-node Kubernetes cluster in only a few minutes, which can be used using <kbd>kubectl</kbd> command-line agent from the single-node Vagrant lab. At the time of writing, Google is offering a free $300.00 credit for signing up for a free-tier Google Cloud account. Once the $300.00 allowance has expired, Google will not charge you for further use without explicit authorization. In and of itself, this is more than enough to run our simple cluster and cover many of the major Kubernetes concepts.</p>
<p>If you are unable to sign up for a Google Cloud Platform account, you can spin up a local Kubernetes node on your workstation absolutely free of charge using the Minikube project. Configuring Minikube to work on your laptop with proper reachability for <kbd>kubectl</kbd> commands to work is fairly straightforward if you are using the Virtualbox hypervisor. If you are interested, you can find more information on Minikube at <a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>.</p>
<p>Before we can proceed with creating our Google Cloud Kubernetes cluster, we need to first sign up for an account at <a href="https://cloud.google.com/free/">https://cloud.google.com/free/</a>.</p>
<p>Once you have created a free account, it will prompt you to create a new project. You can name your project anything you like, as Google Cloud will assign a unique identifier to it within the console. I named mine <kbd>AC-Kubernetes-Demo</kbd>. If the signup process does not prompt you to create a new project, from the main console you can select: Projects and click the + sign button to create a new project:</p>
<div><img height="131" width="462" src="img/8d4b6058-b7cf-49f2-a96e-1ee79f563f02.png"/></div>
<p>Once a project has been created, we can create a Kubernetes cluster using the Google Container Engine. From the main console window, on the left-hand side menu, select Container Engine<strong> </strong>| Container Clusters from the submenu:</p>
<div><img height="126" width="397" src="img/77686a44-c8ad-403c-a4ea-c73236d4a3fa.png"/></div>
<p>For the purposes of this example, and also to make the most of the free allowance provided to use the Google Container Engine, we will create a three-node container cluster using the minimum specifications. To do this, from the Container clusters dashboard, select the button Create Cluster. This will drop you into a form that will allow you to select your cluster specifications. I created mine to the following specifications:</p>
<ul>
<li class="mce-root">Name: Cluster-1</li>
<li class="mce-root">Cluster Version: 1.6.9</li>
<li class="mce-root">1 vCPU per Node (3 total vCPUs)</li>
<li class="mce-root">Container Optimized OS</li>
<li class="mce-root">Disabled Automatic Upgrades</li>
</ul>
<p>Once the cluster has been created, you should see a cluster that resembles the following screenshot:</p>
<div><img height="69" width="551" src="img/6c791484-ef85-4c0b-aa30-f1c5ca681b9f.png"/></div>
<p>The most recent versions of the Google Cloud interface may have changed since the time of writing. You may have to set up your Kubernetes cluster using a slightly different set of steps, or customization options. The default settings should be sufficient to create a cluster that isn’t so expensive that it quickly burns through your $300.00 allowance. Remember, the more resources you allocate to your cluster, the faster you will use up your credit!</p>
<p>Once our cluster has been fully deployed, we can connect to it from our Vagrant development lab. To do this, we need to first initialize the <kbd>kubectl</kbd> tool using the <kbd>Gcloud</kbd> interface. By default, these packages are not installed in the Vagrant lab to save on time and complexity when provisioning the VM. To enable this functionality, we need to modify the Vagrantfile, located in the <kbd>root</kbd> directory of the official Git repository for this book. Towards the bottom of the Vagrant file, you will see a section titled: <kbd>#Un-Comment this section to Install the Google Cloud SDK</kbd>. Un-commenting this section should result in the following changes to the Vagrantfile:</p>
<pre><strong>##Un-Comment this to Install the Google Cloud SDK:</strong><br/><strong>export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)"</strong><br/><strong>echo "deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main" |     sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list</strong><br/><strong>curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</strong><br/><strong>apt-get update &amp;&amp; sudo apt-get install -y google-cloud-sdk</strong><br/><strong>apt-get install kubectl</strong><br/><strong>SHELL</strong><br/><strong>end</strong></pre>
<p>After making these changes, save the file, and launch the lab VM using the <kbd>vagrant up</kbd> command. If the lab VM is already running, you can use the <kbd>vagrant provision</kbd> command to re-provision the running VM, or simply destroy and re-create the VM as follows:</p>
<pre><strong>user@localhost:~/$ vagrant destroy -f</strong><br/><strong>==&gt; node01: Forcing shutdown of VM...</strong><br/><strong>==&gt; node01: Destroying VM and associated drives...</strong><br/><br/><strong>user@localhost:~/$ vagrant up</strong><br/><strong>Bringing machine 'node01' up with 'virtualbox' provider...</strong><br/><strong>==&gt; node01: Checking if box 'ubuntu/xenial64' is up to date..</strong></pre>
<p>Once the Vagrant lab VM has the <kbd>Google Cloud SDK</kbd> and <kbd>kubectl</kbd> installed, Execute the command <kbd>gcloud init</kbd> and, when prompted to log in, enter <kbd>Y</kbd> to confirm and continue logging in.</p>
<pre class="western"><strong>ubuntu@node01:~$ gcloud init
Welcome! This command will take you through the configuration of gcloud.

Your current configuration has been set to: [default]

You can skip diagnostics next time by using the following flag:
  gcloud init --skip-diagnostics

Network diagnostic detects and fixes local network connection issues.
Checking network connection...done.
Reachability Check passed.
Network diagnostic (1/1 checks) passed.

You must log in to continue. Would you like to log in (Y/n)? Y</strong></pre>
<p>The <kbd>Gcloud</kbd> CLI tool should then return a hyperlink that will allow you to authorize your Vagrant lab with your Google Cloud account. Once you have granted permission to use your Google Cloud account, your web browser should return a code you can enter on the command line to complete the authorization process.</p>
<p>The CLI wizard should then prompt you to select a project. The project you just created should be displayed with a list of options. Select the project we just created:</p>
<pre class="western"><strong>Pick cloud project to use: 
 [1] ac-kubernetes-demo
 [2] api-project-815655054520
 [3] Create a new project
Please enter numeric choice or text value (must exactly match list 
item): 1</strong></pre>
<p>It will then prompt you if you wish to configure the Google Compute Engine. This is not a necessary step, but if you opt to perform it, you will be presented with a list of geographic regions to select from. Select the one closest to you. Finally, your Google Cloud account should be connected to your Vagrant lab.</p>
<p>Now, we can set up connectivity to our Kubernetes cluster using the <kbd>kubectl</kbd> tool. This can be accomplished by selecting the Connect button on the Container Engine dashboard, next to our cluster. A screen should pop up displaying details on how to connect to our cluster from our initialized Vagrant lab:</p>
<div><img height="174" width="447" src="img/7520e02d-5b01-48cf-8c72-854ae11710ab.png"/></div>
<p>Copy and paste that command into your Vagrant environment:</p>
<pre class="western"><strong>ubuntu@node01:~$ gcloud container clusters get-credentials cluster-1 --zone us-central1-a --project ac-kubernetes-demo

WARNING: Accessing a Container Engine cluster requires the kubernetes commandline
client [kubectl]. To install, run
  $ gcloud components install kubectl

Fetching cluster endpoint and auth data.
kubeconfig entry generated for cluster-1.</strong></pre>
<p>This should cache the default Kubernetes credentials required for access to our cluster from the <kbd>kubectl</kbd> command-line tool. <kbd>kubectl</kbd> will already be installed in the Vagrant lab VM due to the changes made to the Vagrantfile earlier in the chapter.</p>
<p>Since <kbd>kubectl</kbd> is already installed, we can validate the connectivity to your Kubernetes cluster by executing <kbd>kubectl cluster-info</kbd> to view details about our running cluster. I censored the IP details for my cluster environment. However, your output will show all the relevant addresses for the core services:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl cluster-info
Kubernetes master is running at https://IPADDRESS
GLBCDefaultBackend is running at https://IPADDRESS/api/v1/namespaces/kube-system/services/default-http-backend/proxy
Heapster is running at https://IPADDRESS/api/v1/namespaces/kube-system/services/heapster/proxy
KubeDNS is running at https://IPADDRESS/api/v1/namespaces/kube-system/services/kube-dns/proxy
kubernetes-dashboard is running at https://IPADDRESS/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy</strong></pre>
<p>You can also run <kbd>kubectl get nodes</kbd> to see an output of the nodes the cluster consists of:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get nodes
NAME                                       STATUS    AGE       VERSION
gke-cluster-1-default-pool-ca63b897-7pwx   Ready     2d       v1.6.9
gke-cluster-1-default-pool-ca63b897-d9cf   Ready     2d       v1.6.9
gke-cluster-1-default-pool-ca63b897-fnnt   Ready     2d       v1.6.9</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying an application in Kubernetes using kubectl</h1>
                
            
            
                
<p>KubeCTL or Kube Control is the official command line interface into the Kubernetes API Server and the rest of the Kubernetes Control Plane. Using the <kbd>kubectl</kbd> tool, you can view the status of pods, access cluster resources, and even exec into running pods for troubleshooting purposes. In this portion of the chapter, we will look at the basics of using <kbd>kubectl</kbd> to manually describe deployments, scale pods, and create services to access the pods. This is beneficial to understanding the basic concepts of Kubernetes to understand how Ansible Container is able to automate many of these tasks using the native Kubernetes modules available to it.</p>
<p>Let’s take a look at some of the most common <kbd>kubectl</kbd> options and syntax you are likely to run into working with Kubernetes:</p>
<ul>
<li><kbd>kubectl get</kbd>: <kbd>kubectl get</kbd> is used to return resources that currently exist in the Kubernetes cluster. Commonly, this command is used to get a list of pods currently running or nodes in the cluster. Think of this command as being similar to the docker ps command. Examples of <kbd>get</kbd> commands are: <kbd>kubectl get pods</kbd> and <kbd>kubectl get deployments</kbd>.</li>
<li><kbd>kubectl describe</kbd>: <kbd>describe</kbd> is used to view more verbose details about a particular cluster resource. If you want to know the latest state of a resource or current details about how the resource is running you can use the describe command. <kbd>describe</kbd> is very helpful since you can call out a specific cluster resource, such as a pod, service, deployment, or replication controller to view the details pertaining directly to that instance. <kbd>describe</kbd> is also very useful for troubleshooting issues across Kubernetes environments. Examples of <kbd>describe</kbd> are: <kbd>kubectl describe pod</kbd>, and <kbd>kubectl describe node</kbd>.<br/></li>
<li><kbd>kubectl run</kbd>: <kbd>kubectl run</kbd> functions quite similar to the <kbd>docker run</kbd> command we explored earlier in this book. Run is primarily used to start a new deployment and get pods up and running quickly in the Kubernetes cluster. The use case for run is rather limited, since more complex deployments are better suited for the <kbd>create</kbd> and <kbd>apply</kbd> commands. However, for testing or getting containers running quickly and efficiently, <kbd>run</kbd> is a fantastic tool.</li>
<li><kbd>kubectl create</kbd>: Create is used to create new cluster resources such as pods, deployments, namespaces, or services. Create functions very similar to apply and run, with the caveat that it is used solely for launching new objects. Using create, you can use the <kbd>-f</kbd> flag to pass in a manifest file or direct URL to launch more complex items than you could with <kbd>kubectl</kbd> <kbd>run</kbd>.</li>
<li><kbd>kubectl apply</kbd>: Apply is often confused with <kbd>create</kbd> since the syntax and functionality is so similar. Apply is used to update Kubernetes resources that exist in the cluster, whereas <kbd>create</kbd> is used to create new resources. For example, if you created a series of pods based on a Kubernetes manifest that you launched using <kbd>kubectl</kbd> create, you could use <kbd>kubectl</kbd> apply to update any changes you may have made to the manifests. The Kubernetes Control Plane will analyze the manifest an attempt to make the changes necessary to bring the cluster resources into the desired state.</li>
</ul>
<ul>
<li><kbd>kubectl delete: delete</kbd> is rather self-explanatory since the primary function is used to delete objects from the Kubernetes cluster. Similar to create and apply, you can use the <kbd>-f</kbd> flag to pass in a Kubernetes manifest file that was created or updated previously and use that as an identifier to delete those resources from the cluster.</li>
</ul>
<p>As you will notice, the <kbd>kubectl</kbd> uses a verb/noun syntax that is quite easy to remember. Everything you do with <kbd>kubectl</kbd> will take a verb argument (get, describe, create, apply), followed by the objects in kubernetes you wish to act on: (pods, namespaces, nodes, and other specific resources). There are a lot more command options available to you using the <kbd>kubectl</kbd> tool, but these are by far some of the most common options you are likely to use when starting with Kubernetes.</p>
<p>To view all of the possible parameters that Kubectl takes, you can use <kbd>kubectl --help or kubectl</kbd> subcommand <kbd>--help</kbd> to get help on a particular function or subcommand.</p>
<p>Since we now have access to the Kubernetes cluster from our Vagrant lab, we can use the <kbd>kubectl</kbd> tool to explore the cluster resources and objects. The first command that we will look at is the <kbd>kubectl get pods</kbd> command. We will use this to return a list of pods that exist in all namespaces across the cluster. Simply typing in <kbd>kubectl get pods</kbd> will return nothing since Kubernetes resources are separated by namespaces. Namespaces provide a logical separation of Kubernetes resources based on DNS and networking rules, which allow users to have fine-grained control over multiple deployments that simultaneously exist in the same cluster. Currently, everything that exists in the Kubernetes cluster exists as running processes critical to the functionality of the Kubernetes control plane and exist in the <kbd>kube-system</kbd> namespace. To see a list of everything running in all namespaces, we can use the <kbd>kubectl get pods</kbd> command, passing in the <kbd>--all-namespaces</kbd> flag:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get pods --all-namespaces
NAME                                                  READY     STATUS    RESTARTS   AGE
fluentd-gcp-v2.0-k8nrl                                2/2       Running   0          17m
fluentd-gcp-v2.0-l05dw                                2/2       Running   0          17m
fluentd-gcp-v2.0-svnfw                                2/2       Running   0          17m
heapster-v1.3.0-1288166888-cqpd3                      2/2       Running   0          16m
kube-dns-3664836949-sl69q                             3/3       Running   0          17m
kube-dns-3664836949-tbmvl                             3/3       Running   0          17m
kube-dns-autoscaler-2667913178-vdjc5                  1/1       Running   0          17m
kube-proxy-gke-cluster-1-default-pool-ca63b897-7pwx   1/1       Running   0          17m</strong>
<strong>kube-proxy-gke-cluster-1-default-pool-ca63b897-d9cf   1/1       Running   0          17m
kube-proxy-gke-cluster-1-default-pool-ca63b897-fnnt   1/1       Running   0          17m
kubernetes-dashboard-2917854236-sctqd                 1/1       Running   0          17m
l7-default-backend-1044750973-68fx0                   1/1 Running   0          17m</strong></pre>
<p>Your list may look slightly different to the output I have provided here, based on the version of Kubernetes your cluster is running and any changes the Google Container Engine platform may have introduced since the time of writing. However, what you will see is a list of containers that are running to support the Kubernetes Control Plane, such as the <kbd>kube-proxy</kbd>, <kbd>kube-dns</kbd>, and logging mechanisms using <kbd>fluentd</kbd>. The default output will show the name of the pods, how long they have been running (the age), the number of running replicas, and the number of times the pods have restarted.</p>
<p>You can use the <kbd>-o wide</kbd> flag to see more details, such as the namespace and overlay network IP addresses assigned to the pods. For example, <kbd>kubectl get pods -o wide --all-namespaces</kbd>.</p>
<p>Now that we have a firm understanding of listing pods, we can use the <kbd>kubectl run</kbd> command to start our first deployment. In <a href="4b15cefb-8d9c-48b7-8927-126501886315.xhtml" target="_blank">Chapter 3</a>, <em>Your First Ansible Container Project</em> we learned how to build an NGINX container using a community-developed container-enabled role and uploaded it to our personal Docker Hub repository. We can use the <kbd>kubectl run</kbd> command to download our container, quickly create a new Kubernetes deployment called <kbd>nginx-web</kbd> and get this pod running in our cluster. In order to pull the pod from our repository, we will need to provide the fully qualified container name in the format: <kbd>image-repository/username/containername</kbd>. Furthermore, we need to map the port to port <kbd>8000</kbd> since the community-developed role leveraged that port by default. Finally, we will be launching this deployment in the <kbd>default</kbd> namespace, so no additional namespace configuration needs to be applied:</p>
<pre class="western"><strong>kubectl run nginx-web --image=docker.io/username/nginx_demo-webserver  --port=8000</strong></pre>
<p>Now, if we try running <kbd>kubectl get pods</kbd>, we will see a single NGINX pod running the default namespace:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get pods -o wide
NAME                         READY     STATUS    RESTARTS   AGE
nginx-web-1202329523-qjkwp   1/1       Running   0          3m </strong> </pre>
<p>Similarly, we can use the <kbd>kubctl get deployments</kbd> function to see what the current state of deployments for the default namespace looks like:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get deployments
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-web   1         1         1            1           12m</strong></pre>
<p>As you can see from the get pods and get deployments output, we have a single deployment called <kbd>nginx-web</kbd>, which consists of a single pod and a single container within that pod. This is in full agreement with the input we provided into the Kubernetes API server using the <kbd>kubectl run</kbd> command. If we attempt to delete this pod, there will be a delta between the desired state and the current status of our deployment. Kubernetes will then attempt to bring the cluster back into the desired state by recreating the deleted cluster resource. Let’s try doing a delete on the NGINX pod we created and see what happens. Usually, this happens so quickly, you will need to pay attention to the name of the pod as well as the age to see that the change has occurred:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl delete pod nginx-web-1202329523-qjkwp
pod "nginx-web-498735019-6llvp" deleted</strong><br/><strong>ubuntu@node01:~$ kubectl get pods
NAME                        READY     STATUS    RESTARTS   AGE
nginx-web-498735019-xcp21   1/1       Running   0          15s</strong></pre>
<p>If we wanted to actually delete the pods from the cluster permanently, we could use the delete command on the deployment itself, using the syntax: <kbd>kubectl delete deployment nginx-web </kbd> This would declare a new desired state, namely that we no longer want the deployment <kbd>nginx-web</kbd> present and all pods in that deployment should likewise be deleted.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Describing Kubernetes resources</h1>
                
            
            
                
<p>Kubernetes can also be used to view detailed information about the pods or other objects we have instantiated in our cluster. We can do this using the <kbd>kubectl describe</kbd> command. Describe can be used to see a detailed view of almost any resource in our cluster.</p>
<p>Let’s take a moment to describe our NGINX pod and ensure that it is running as expected:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl describe deployment nginx-web
Name:                   nginx-web
Namespace:              default
CreationTimestamp:      Wed, 13 Sep 2017 19:36:48 +0000
Labels:                 run=nginx-web
Annotations:            deployment.kubernetes.io/revision=1
Selector:               run=nginx-web
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:       run=nginx-web
  Containers:
   nginx-web:
    Image:              docker.io/aric49/nginx_demo-webserver</strong>
   <strong> Port:               8000/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
OldReplicaSets: &lt;none&gt;
NewReplicaSet:  nginx-web-498735019 (1/1 replicas created)
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath   Type            Reason                  Message
  ---------     --------        -----   ----                    -------------   --------        ------                  -------
  59s           59s             1       deployment-controller Normal          ScalingReplicaSet       Scaled up replica set nginx-web-498735019 to 1</strong></pre>
<p>As you can see describe displays a lot of pertinent information about our cluster, including details such as the namespace the pod is running in, any labels our pod is configured with, the name and location of the container image that is running, as well as the most recent events that have occurred to our pod. The describe output shows us a wealth of information that can help us to troubleshoot or optimize the deployments and containers in our cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exposing Kubernetes services</h1>
                
            
            
                
<p>Since we now have a functional NGINX web server running in our cluster, we can expose this service to the outside world so that others can use our shiny new service. In order to do this, we can create a Kubernetes abstraction known as a service to control how our pod is granted outside access. By default, Kubernetes pods are assigned a cluster IP address by the overlay network fabric, which is only reachable within the cluster by other containers and across nodes. This is useful if you have a deployment that should never be directly exposed to the outside world. However, Kubernetes also supports exposing deployments using the service abstraction. Services can expose pods in a variety of ways, from allocating publicly routable IP addresses to the services and load balancing across the cluster to opening a simple node port on the master nodes, from which the service can listen. Google Container Engine provides native support for the <kbd>LoadBalancer</kbd> service type which can allocate a public virtual IP address to our deployment, making services extremely easy to expose. In order to allow our service to see the outside world, we can use the <kbd>kubectl expose deployment</kbd> command, providing the service type as <kbd>LoadBalancer</kbd>. Upon successful completion, you should see the message service <kbd>nginx-web</kbd> exposed.</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl expose deployment nginx-web –type=LoadBalancer
service "nginx-web" exposed</strong></pre>
<p>We can see our newly created service by running the <kbd>kubectl get services</kbd> command. You may notice that the <kbd>EXTERNAL IP</kbd> column may be in the pending state for a moment or two while Kubernetes allocates a public IP for the cluster. If you execute the <kbd>kubectl get services</kbd> command after a few minutes, you should notice it has an external IP and is ready to be accessed:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get services
NAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes   10.11.240.1     &lt;none&gt;        443/TCP          31m
nginx-web    10.11.255.240   &lt;pending&gt;     8000:32567/TCP   6s</strong></pre>
<p>After a minute or two:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get services
NAME         CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
kubernetes   10.11.240.1     &lt;none&gt;          443/TCP          1h
nginx-web    10.11.241.144   35.202.165.54   8000:32567/TCP   1m</strong></pre>
<p>In this example, the external IP of <kbd>35.202.165.54</kbd> has been allocated to our deployment. You can access this IP address in a web browser to actually see the NGINX default web page in action. Remember, you have to access this service on TCP port <kbd>8000</kbd> since that is how the container-enabled role is configured out of the box. Bonus points if you want to go back and reconfigure your NGINX container to run on port <kbd>80</kbd>!</p>
<p>Google Cloud Platform has native integration with the Google Cloud virtual load balancer resources, which allow Kubernetes to assign external IP addresses to services.  In baremetal environments or clusters running on other clouds, an extra configuration will be required to allow Kubernetes to seamlessly allocate publicly routed IP addresses.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scaling Kubernetes pods</h1>
                
            
            
                
<p>Now that we have pods running in our cluster, we can use the powerful Kubernetes primitives to scale out containers and running services across nodes for high availability. As mentioned previously, as soon as a desire state is declared that involves running more than one replica of a pod, Kubernetes will apply a bin-packing algorithm to the deployment in an effort to determine which nodes the service will run on. If you declare the same number of replicas as you have nodes in your cluster, Kubernetes will run one replica on each node by default. If you have more replicas declared then nodes, Kubernetes will run more than one replica on some of the nodes, and on others, it will run a single replica. This provides us with native high availability out of the box. One of the benefits of using Kubernetes is that, by leveraging these features and functionality, operators no longer worry as much about the underlying infrastructure the containers are running on as much as the cluster abstractions themselves.</p>
<p>NOTE: Kubernetes can also use labels to control where certain deployments should run. For example, if you have a high compute capacity node, you could label that node as a compute node. You can customize your deployment so that those pods will only run on nodes with that particular label.</p>
<p>To demonstrate how powerful of a functionality this is, we use <kbd>kubectl</kbd> to scale out our existing web server deployment. Since we are currently running a three-node cluster, let’s scale out our NGINX deployment to four replicas. This way, we can best illustrate what decisions Kubernetes is making on where to place our containers. In order to scale our current deployment, we can use the <kbd>kubectl scale deployment</kbd> command to increase our replica count from one to four:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl scale deployment nginx-web --replicas=4
deployment "nginx-web" scaled</strong></pre>
<p>Using <kbd>kubectl get deployments</kbd>, we can see that Kubernetes is actively reconfiguring our cluster towards the desired state. It might take a few seconds for Kubernetes to get all four pods running, depending on the configuration you have chosen for your cluster. Following we can see the desired number of pods, the current number of pods running in the cluster, the number of pods that update, and the pods that are ready and available to accept traffic. It looks like our cluster is in our desired state:</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get deployments
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-web   4         4         4            4           14m</strong></pre>
<p>Running <kbd>kubectl get pods</kbd> with the <kbd>-o wide</kbd> flag, we can see that all four NGINX pods are running with different IP addresses allocated and on different cluster nodes. It is important to note that, since we specified four replicas and only have three nodes, Kubernetes made the decision to have two pod replicas running on the same host. Keep in mind that these are two separate and distinct pods with a different IP address, even though they are running the same host.</p>
<pre class="western"><strong>ubuntu@node01:~$ kubectl get pods -o wide
NAME   READY     STATUS    RESTARTS   AGE       IP          NODE
nginx  1/1       Running   0          2m        10.8.2.5    7pwx
nginx  1/1       Running   0          2m        10.8.1.10   fnnt
nginx  1/1       Running   0          15m       10.8.1.9    fnnt
nginx  1/1       Running   0          2m        10.8.0.5    d9cf</strong></pre>
<p>The preceding output is slightly truncated since the <kbd>-o</kbd> wide output is difficult to read properly in the context of a book page. Your output will be slightly more verbose than mine.</p>
<p>Accessing the public IP address again will result in the service now load balancing traffic across the pods in the cluster. Since we specified the service type as <kbd>LoadBalancer</kbd>, Kubernetes will use a round-robin algorithm to pass traffic to our pods with high availability. Unfortunately, this will not be obvious to the reader, since each pod is running the same NGINX test web page. One of the major benefits of Kubernetes is that services are usually tied to deployments. When you scale a deployment, the service will automatically scale and start passing traffic to the new pods!</p>
<p>Before we move forward to the next exercise, let’s delete the deployment we just created, as well as the exposed service. This will return our cluster to a fresh state:</p>
<pre class="western"><br/><strong>ubuntu@node01:~$ kubectl delete deployment nginx-web
deployment "nginx-web" deleted

ubuntu@node01:~$ kubectl delete services nginx-web
service "nginx-web" deleted</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating deployments using Kubernetes manifests</h1>
                
            
            
                
<p>Along with the ability to create services and other objects directly from the command line, Kubernetes also gives you the ability to describe desired states using a manifest document. Kubernetes manifests give you the freedom to provide more customization options in an easier to read, understand, and repeatable format, as opposed to the command line, which is rather limited in its format. Since this chapter is not designed to be a deep dive into Kubernetes, we will not spend a lot of time going into all of the various configuration options that can be used in a Kubernetes manifest. Rather, my intention is to show the reader what manifests look like and how they work at a basic level.</p>
<p>Since you are already familiar with creating deployments using the <kbd>kubectl</kbd> command-line tool, let’s demonstrate what our <kbd>nginx-web</kbd> deployment would look like using a Kubernetes manifest. These examples are available in the official git repository for the book, under the <kbd>Kubernetes/nginx-demo</kbd> directory. Open your text editor and create a file: <kbd>webserver-deployment.yml</kbd>. The content of this file should resemble the following. In this example, we are going to continue to use our previously created NGINX container. However, feel free to use other container URLs if you wish to experiment with using other types of services and ports.</p>
<pre class="western">---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: webserver-deployment
spec:
  replicas: 4
  template:
    metadata:
      labels:
        app: http-webserver

    spec:
      containers:
        - name: webserver-container
          image: docker.io/<strong>USERNAME</strong>/nginx_demo-webserver
          ports:
            - containerPort: 8000</pre>
<p>Like all the YAML documents we have looked at thus far, Kubernetes manifest documents begin with three dashes to indicate the start of a YAML file. Kubernetes manifests always begin by specifying the API version, object kind, and metadata. This is colloquially known as the header data and indicates to the Kubernetes API the type of objects this document is going to create. Since we are creating a deployment, we will specify the <kbd>kind</kbd> parameter as <kbd>Deployment</kbd> and provide the name of the deployment as the metadata name. Everything listed underneath the <kbd>spec</kbd> section provides configuration option parameters that are specific to the pod object the document is creating. Since we are basically reverse engineering our previous deployment, we are specifying the number of replicas as <kbd>4</kbd>. The next few lines specify metadata we are going to configure our pods with, specifically a key-value pair label called, <kbd>app:http-webserver</kbd>. Keep this label in mind, as we are going to use it when we create the service resource next.</p>
<p>Finally, we have another <kbd>spec</kbd> section, which lists the containers that are going to run inside our pod. Earlier in the chapter, I mentioned that a pod can be one or more containers running using shared network and cluster resources. Containers in a pod share a pod IP address and localhost namespace. Kubernetes deployments allow you to specify more than one pod under the <kbd>containers:</kbd> section, passing them in as listed key-value pair items. This example, however, will create a single-pod container known as <kbd>webserver-container</kbd>. It is here that we will specify the container image version, as well as the container port (<kbd>80</kbd>).</p>
<p>This manifest can be applied using the <kbd>kubectl create</kbd> command, passing in the <kbd>-f</kbd> flag, which indicates a manifest object, as well as the path to the manifest:</p>
<pre><strong>kubectl create -f webserver-deployment.yml</strong></pre>
<p>Upon successful completion, you should see the pods getting created using <kbd>kubectl get pods</kbd>:</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating services using Kubernetes manifests</h1>
                
            
            
                
<p>In a similar way to creating our deployment using Kubernetes manifest, we can create other Kubernetes objects using manifests. The service we created earlier can be described using the following Kubernetes manifest:</p>
<pre class="mce-root">---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: webserver-service<br/>spec:<br/>  type: LoadBalancer<br/>  selector:<br/>    app: http-webserver<br/>  ports:<br/>    - protocol: TCP<br/>      port: 80<br/>      targetPort: 8000</pre>
<p>Notice in this example, we are specifying a different <kbd>kind</kbd> parameter to be <kbd>Service</kbd> as opposed to our previous example, which used <kbd>Deployment</kbd>. This tells the Kubernetes API to expect that the rest of the document will contain specifications that describe services instead of other types of Kubernetes objects. In the metadata section, we will name our service <kbd>webserver-service</kbd> (creative, no?). For the specification section, we will provide the type of service we are exposing, <kbd>LoadBalancer</kbd>, and provide the label we assigned to our deployment: <kbd>app: http-webserver</kbd>. When using <kbd>kubectl</kbd> to expose deployments, the service you create is inherently tied to the deployment you are exposing. When that deployment scales out, the service will be aware and will adjust according to how many backend pods are running. However, when creating a service using Kubernetes manifests, we can get more creative with how services are tied to the services they are exposing. In this example, we are creating a service that is associated with any pod that has the label <kbd>app: http-webserver</kbd>. In theory, this could be any number of pods across different namespaces and deployments. This allows for a lot of flexibility when designing applications around a Kubernetes architecture.</p>
<p>The final section of our manifest describes the ports we will perform load balancing across. Remember how our NGINX container uses the fixed port <kbd>8000</kbd> by virtue of the fact we built this container using the community-written role? Using the load balancer service, we can expose any port we want on the frontend to forward traffic to any port on the backend pods. The protocol we will use will be TCP. The port we want to expose on the virtual IP address will be <kbd>80</kbd> for standard HTTP requests. Finally, we will list the port that NGINX is listening to internally on our pods to forward traffic to. In this case, <kbd>8000</kbd>.</p>
<p>Using the <kbd>kubectl create</kbd> command, we can create our service very similar to how we created our initial deployment:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/nginx-demo$ kubectl create -f webserver-service.yml
service "webserver-service" created</strong></pre>
<p>Using <kbd>kubectl get services</kbd>, we can see which external virtual IP address gets allocated to our cluster:</p>
<pre class="western"><strong>ubuntu@node01:$ kubectl get services
NAME                CLUSTER-IP      EXTERNAL-IP      PORT(S)
webserver-service   10.11.251.238   104.197.85.153   80:30600/TCP</strong></pre>
<p>Looking at the <kbd>PORTS</kbd> column, we can see that TCP port <kbd>80</kbd> is exposed in our cluster. Using a web browser again, we can access our new public IP on port <kbd>80</kbd> and see whether it’s working:</p>
<div><img height="216" width="497" src="img/7dc64e1a-eb4f-49b6-88eb-e2a40805e508.png"/></div>
<p>Using Kubernetes manifests, we can describe in greater detail the ways we want our containerized applications to function. Manifests can easily be modified as well and reapplied using the <kbd>kubectl apply -f manifest.yml</kbd> command. If at any time, we wanted to update our application to a different version of the container image, or modify exposed ports on the service, <kbd>kubectl</kbd> apply would only make the changes necessary to bring our application into the desired state. Feel free to tweak these manifests on your own and reapply them to see in what ways you can configure the services to run.</p>
<p>Next, we will look at deploying containers to Kubernetes using Ansible Container. Before we move forward, let’s remove the pods in your Kubernetes cluster using the <kbd>kubectl delete</kbd> command, specifying the Kubernetes manifests we used to create or modify the deployment and service:</p>
<pre class="western"><strong>ubuntu@node01:$ kubectl delete -f webserver-deployment.yml
deployment "webserver-deployment" deleted

ubuntu@node01:$ kubectl delete -f webserver-service.yml
service "webserver-service" deleted</strong></pre>
<p>For now, we have finished working work the Kubernets cluster.  If you wish to delete the cluster from Google Cloud, you can do so now.  However, it is important to note that <a href="ef89f30f-00a9-4f4c-93b9-009474fc3022.xhtml" target="_blank">Chapter 7</a>, <em>Deploying Your First Project</em> covers deploying projects to Kubernetes. I would suggest you keep your cluster active until you have finished working on the material in <a href="ef89f30f-00a9-4f4c-93b9-009474fc3022.xhtml" target="_blank">Chapter 7</a><em>, Deploying Your First Project</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">References</h1>
                
            
            
                
<ul>
<li><strong>Kubernetes Documentation</strong>: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
<li><strong>Google Cloud Platform</strong>: <a href="https://cloud.google.com">https://cloud.google.com</a></li>
<li><strong>Running Kubernetes locally with Minikube</strong>:  <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">https://kubernetes.io/docs/getting-started-guides/minikube/</a></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Kubernetes is quickly becoming one of the most robust, flexible, and popular container deployment and orchestration platforms that is taking the IT industry by storm. Throughout this chapter, we have taken a close look at Kubernetes, learning about how it works as a platform and some of the key features that make it so useful and versatile. If you have worked in or around containers for very long, it will be clear to you that Kubernetes is rapidly being adopted by organizations throughout the world due to the extremely sophisticated mechanisms it uses to deploy and manage containers at scale.</p>
<p>Due to the native support for Kubernetes in Ansible Container, we can use the same workflow to build, run, test, and destroy containerized applications that we can deploy to robust services such as Kubernetes. Ansible Container truly provides the right tools to help drive complex deployments using a unified and reliable framework.</p>
<p>However, Google Cloud and the Kubernetes framework are not the only cloud-based container orchestration solutions on the market in today’s world. OpenShift is quickly gaining popularity as a managed solution built by Red Hat that functions on top of the Kubernetes platform. Next, we will apply the Kubernetes concepts we learned in this chapter to deploy applications to the OpenShift software stack, using the powerful tools offered to us by the Ansible Container platform to drive large-scale application workloads.</p>


            

            
        
    </body></html>