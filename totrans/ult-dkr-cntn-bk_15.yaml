- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying and Running a Distributed Application on Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we got a detailed introduction to Docker’s native orchestrator
    called SwarmKit. SwarmKit is part of Docker Engine, and no extra installation
    is needed once you have Docker installed on your system. We learned about the
    concepts and objects SwarmKit uses to deploy and run distributed, resilient, robust,
    and highly available applications in a cluster, which can either run on-premises
    or in the cloud. We also showed how Docker’s orchestrator secures applications
    using SDNs. We learned how to create a Docker Swarm locally, in a special environment
    called Play with Docker, and also in the cloud. Finally, we discovered how to
    deploy an application that consists of multiple related services to Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce the routing mesh, which provides
    layer-4 routing and load balancing. Next, we are going to demonstrate how to deploy
    a first application consisting of multiple services onto the Swarm. We are also
    learning how to achieve zero downtime when updating an application in the swarm
    and finally how to store configuration data in the swarm and how to protect sensitive
    data using Docker secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the topics we are going to discuss in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Swarm routing mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-downtime deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing configuration data in the swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting sensitive data with Docker Secrets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After completing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: List two to three different deployment strategies commonly used to update a
    service without downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update a service in batches without causing a service interruption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a rollback strategy for a service that is used if an update fails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store non-sensitive configuration data using Docker configs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a Docker secret with a service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the value of a secret without causing downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: The swarm routing mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have paid attention, then you might have noticed something interesting
    in the last chapter. We had the `pets` application deployed and it resulted in
    an instance of the web service being installed on the three nodes – `node-1`,
    `node-2`, and `node-3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, we were able to access the web service on `node-1` with `localhost` and
    we reached each container from there. How is that possible? Well, this is due
    to the so-called Swarm routing mesh. The routing mesh makes sure that when we
    publish a port of a service, that port is then published on all nodes of the Swarm.
    Hence, network traffic that hits any node of the Swarm and requests to use a specific
    port will be forwarded to one of the service containers by routing the mesh. Let’s
    look at the following diagram to see how that works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Docker Swarm routing mesh](img/Image98324.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Docker Swarm routing mesh
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, we have three nodes, called `172.10.0.15`, `172.10.0.17`,
    and `172.10.0.33`. In the lower-left corner of the diagram, we see the command
    that created a web service with two replicas. The corresponding tasks have been
    scheduled on **Host B** and **Host C**. **task1** landed on **Host B** while **task2**
    landed on **Host C**.
  prefs: []
  type: TYPE_NORMAL
- en: When a service is created in Docker Swarm, it automatically gets a `10.2.0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: If now a request for port `8080` coming from an external `8080` in the IP table
    and will find that this corresponds to the VIP of the web service.
  prefs: []
  type: TYPE_NORMAL
- en: Now, since the VIP is not a real target, the IPVS service will load-balance
    the IP addresses of the tasks that are associated with this service. In our case,
    it picked `10.2.0.3`. Finally, **Ingress Network (Overlay)** is used to forward
    the request to the target container on **Host C**.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that it doesn’t matter which Swarm node the external
    request is forwarded to by **External LB**. The routing mesh will always handle
    the request correctly and forward it to one of the tasks of the targeted service.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned a lot about networking in a Docker swarm. The next topic that
    we are going to learn about is how can we deploy an application without causing
    any system downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important aspects of a mission-critical application that needs
    frequent updates is the ability to do updates in a fashion that requires no outage
    at all. We call this a zero-downtime deployment. At all times, the application
    that is updated must be fully operational.
  prefs: []
  type: TYPE_NORMAL
- en: Popular deployment strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are various ways to achieve this. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm supports rolling updates out of the box. The other two types of
    deployments can be achieved with some extra effort on our part.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a mission-critical application, each application service has to run in multiple
    replicas. Depending on the load, that can be as few as two to three instances
    and as many as dozens, hundreds, or thousands of instances. At any given time,
    we want to have a clear majority when it comes to all the service instances running.
    So, if we have three replicas, we want to have at least two of them up and running
    at all times. If we have 100 replicas, we can be content with a minimum of, say,
    90 replicas, available. By doing this, we can define the batch size of replicas
    that we may take down to upgrade. In the first case, the batch size would be 1,
    and in the second case, it would be 10.
  prefs: []
  type: TYPE_NORMAL
- en: When we take replicas down, Docker Swarm will automatically take those instances
    out of the load-balancing pool and all traffic will be load-balanced across the
    remaining active instances. Those remaining instances will thus experience a slight
    increase in traffic. In the following diagram, prior to the start of the rolling
    update, if **Task A3** wanted to access **Service B**, it could be load-balanced
    to any of the three tasks of **Service B** by SwarmKit. Once the rolling update
    started, SwarmKit took down **Task B1** for updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatically, this task is then taken out of the pool of targets. So, if **Task
    A3** now requests to connect to **Service B**, load balancing will only select
    from the remaining tasks, that is, **Task B2** and **Task B3**. Thus, those two
    tasks might experience a higher load temporarily:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – Task B1 is taken down to be updated](img/Image98334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – Task B1 is taken down to be updated
  prefs: []
  type: TYPE_NORMAL
- en: The stopped instances are then replaced by an equivalent number of new instances
    of the new version of the application service. Once the new instances are up and
    running, we can have the Swarm observe them for a given period of time and make
    sure they’re healthy. If all is well, then we can continue by taking down the
    next batch of instances and replacing them with instances of the new version.
    This process is repeated until all the instances of the application service have
    been replaced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see that **Task B1** of **Service B** has
    been updated to version 2\. The container of **Task B1** was assigned a new IP
    address, and it was deployed to another worker node with free resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – The first batch being updated in a rolling update](img/Image98343.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – The first batch being updated in a rolling update
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that when the task of a service is updated, in
    most cases, it gets deployed to a worker node other than the one it used to live
    on, but that should be fine as long as the corresponding service is stateless.
    If we have a stateful service that is location- or node-aware and we’d like to
    update it, then we have to adjust our approach, but this is outside of the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at how we can actually instruct the Swarm to perform a rolling
    update of an application service. When we declare a service in a `stack` file,
    we can define multiple options that are relevant in this context. Let’s look at
    a snippet of a typical `stack` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we can see a section, `update_config`, with `parallelism` and
    `delay` properties. `parallelism` defines the batch size of how many replicas
    are going to be updated at a time during a rolling update. `delay` defines how
    long Docker Swarm is going to wait between updating individual batches. In the
    preceding case, we have 10 replicas that are being updated in two instances at
    a time and, between each successful update, Docker Swarm waits for 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test such a rolling update. Navigate to the `ch14` subfolder of our `sample-solutions`
    folder and use the `web-stack.yaml` file to create a web service that’s been configured
    for a rolling update. The service uses an Alpine-based Nginx image whose version
    is `1.12-alpine`. We will update the service to a newer version, that is, `1.13-alpine`.
  prefs: []
  type: TYPE_NORMAL
- en: To start, we will deploy this service to the swarm that we created in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSH into the `master1` instance of your Docker swarm on AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a file called `web-stack.yml` using `vi` or `nano` with this content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can deploy the service using the `stack` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding command looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the service has been deployed, we can monitor it using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – Service web of the web stack running in Swarm with 10 replicas](img/Image98354.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – Service web of the web stack running in Swarm with 10 replicas
  prefs: []
  type: TYPE_NORMAL
- en: The previous command will continuously update the output and provide us with
    a good overview of what happens during the rolling update.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to open a second Terminal and configure it for remote access for
    the manager node of our swarm. Once we have done that, we can execute the `docker`
    command, which will update the image of the web service of the `stack`, also called
    `web`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command leads to the following output, indicating the progress
    of the rolling update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5 – Screen showing the progress of the rolling update](img/Image98364.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.5 – Screen showing the progress of the rolling update
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output indicates that the first two batches, each with two tasks,
    have been successful and that the third batch is about to be prepared.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first Terminal window, where we’re watching the `stack`, we should now
    see how Docker Swarm updates the service batch by batch with an interval of 10
    seconds. After the first batch, it should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.6 – Rolling update for a service in Docker Swarm](img/Image98374.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.6 – Rolling update for a service in Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see that the first batch of the two tasks,
    2 and 10, has been updated. Docker Swarm is waiting for 10 seconds to proceed
    with the next batch.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to note that in this particular case, SwarmKit deploys the
    new version of the task to the same node as the previous version. This is accidental
    since we have five nodes and two tasks on each node. SwarmKit always tries to
    balance the workload evenly across the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: So, when SwarmKit takes down a task, the corresponding node has a smaller workload
    than all the others, so the new instance is scheduled to it. Normally, you cannot
    expect to find a new instance of a task on the same node. Just try it out yourself
    by deleting the `stack` with `docker stack rm web` and changing the number of
    replicas to say, seven, and then redeploy and update it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the tasks have been updated, the output of our `docker stack ps web`
    command will look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.7 – All tasks have been updated successfully](img/Image98382.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.7 – All tasks have been updated successfully
  prefs: []
  type: TYPE_NORMAL
- en: Please note that SwarmKit does not immediately remove the containers of the
    previous versions of the tasks from the corresponding nodes. This makes sense
    as we might want to, for example, retrieve the logs from those containers for
    debugging purposes, or we might want to retrieve their metadata using `docker
    container inspect`. SwarmKit keeps the four latest terminated task instances around
    before it purges older ones so that it doesn’t clog the system with unused resources.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `--update-order` parameter to instruct Docker to start the new
    container replica before stopping the old one. This can improve application availability.
    Valid values are `start-first` and `stop-first`.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is the default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’re done, we can tear down the `stack` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Although using `stack` files to define and deploy applications is the recommended
    best practice, we can also define the update behavior in a `service create` statement.
    If we just want to deploy a single service, this might be the preferred way of
    doing things. Let’s look at such a `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This command defines the same desired state as the preceding `stack` file. We
    want the service to run with 10 replicas and we want a rolling update to happen
    in batches of two tasks at a time, with a 10-second interval between consecutive
    batches.
  prefs: []
  type: TYPE_NORMAL
- en: Health checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make informed decisions, for example, during a rolling update of a Swarm
    service regarding whether or not the just-installed batch of new service instances
    is running OK or whether a rollback is needed, SwarmKit needs a way to know about
    the overall health of the system. On its own, SwarmKit (and Docker) can collect
    quite a bit of information, but there is a limit. Imagine a container containing
    an application. The container, as seen from the outside, can look absolutely healthy
    and carry on just fine, but that doesn’t necessarily mean that the application
    running inside the container is also doing well. The application could, for example,
    be in an infinite loop or be in a corrupt state, yet still be running. However,
    as long as the application runs, the container runs, and, from the outside, everything
    looks perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, SwarmKit provides a seam where we can provide it with some help. We, the
    authors of the application services running inside the containers in the swarm,
    know best whether or not our service is in a healthy state. SwarmKit gives us
    the opportunity to define a command that is executed against our application service
    to test its health. What exactly this command does is not important to Swarm;
    the command just needs to return *OK*, *NOT OK*, or *time out*. The latter two
    situations, namely NOT OK or timeout, will tell SwarmKit that the task it is investigating
    is potentially unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I am writing potentially on purpose, and we will see why later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet from a Dockerfile, we can see the `HEALTHCHECK` keyword.
    It has a few options or parameters and an actual command, that is, `CMD`. Let’s
    discuss the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--interval`: Defines the wait time between health checks. Thus, in our case,
    the orchestrator executes a check every 30 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--timeout`: This parameter defines how long Docker should wait if the health
    check does not respond until it times out with an error. In our sample, this is
    10 seconds. Now, if one health check fails, SwarmKit retries a couple of times
    until it gives up and declares the corresponding task as unhealthy and opens the
    door for Docker to kill this task and replace it with a new instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of retries is defined by the `--retries` parameter. In the preceding
    code, we want to have three retries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have the start period. Some containers take some time to start up (not
    that this is a recommended pattern, but sometimes it is inevitable). During this
    startup time, the service instance might not be able to respond to health checks.
    With the start period, we can define how long SwarmKit should wait before it executes
    the very first health check and thus give the application time to initialize.
    To define the startup time, we use the `--start-period` parameter. In our case,
    we do the first check after 60 seconds. How long this start period needs to be
    depends on the application and its startup behavior. The recommendation is to
    start with a relatively low value and, if you have a lot of false positives and
    tasks that are restarted many times, you might want to increase the time interval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we define the actual probing command on the last line with the `CMD`
    keyword. In our case, we are defining a request to the `/health` endpoint of `localhost`
    at port `3000` as a probing command. This call is expected to have three possible
    outcomes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command succeeds
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The command fails
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The command times out
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The latter two are treated the same way by SwarmKit. This is the orchestrator
    telling us that the corresponding task might be unhealthy. I did say *might* with
    intent since SwarmKit does not immediately assume the worst-case scenario but
    assumes that this might just be a temporary fluke of the task and that it will
    recover from it. This is the reason why we have a `--retries` parameter. There,
    we can define how many times SwarmKit should retry before it can assume that the
    task is indeed unhealthy, and consequently kill it and reschedule another instance
    of this task on another free node to reconcile the desired state of the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why can we use `localhost` in our probing command? This is a very good question,
    and the reason is that SwarmKit, when probing a container running in the Swarm,
    executes this probing command inside the container (that is, it does something
    such as `docker container exec <containerID> <probing command>`). Thus, the command
    executes in the same network namespace as the application running inside the container.
    In the following diagram, we can see the life cycle of a service task from its
    beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.8 – Service task with transient health failure](img/Image98391.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.8 – Service task with transient health failure
  prefs: []
  type: TYPE_NORMAL
- en: First, SwarmKit waits to probe until the start period is over. Then, we have
    our first health check. Shortly thereafter, the task fails when probed. It fails
    two consecutive times but then it recovers. Thus, **health check 4** is successful
    and SwarmKit leaves the task running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see a task that is permanently failing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.9 – Permanent failure of a task](img/Image98401.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.9 – Permanent failure of a task
  prefs: []
  type: TYPE_NORMAL
- en: 'We have just learned how we can define a health check for a service in the
    Dockerfile of its image, but this is not the only way we can do this. We can also
    define the health check in the `stack` file that we use to deploy our application
    into Docker Swarm. Here is a short snippet of what such a `stack` file would look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we can see how the health check-related information
    is defined in the `stack` file. First and foremost, it is important to realize
    that we have to define a health check for every service individually. There is
    no health check at an application or global level.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to what we defined previously in the Dockerfile, the command that is
    used to execute the health check by SwarmKit is `curl -f http://localhost:3000/health`.
    We also have definitions for `interval`, `timeout`, `retries`, and `start_period`.
    These four key-value pairs have the same meaning as the corresponding parameters
    we used in the Dockerfile. If there are health check-related settings defined
    in the image, then the ones defined in the `stack` file override the ones from
    the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try to use a service that has a health check defined:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `vi` or `nano` to create a file called `stack-health.yml` with the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s deploy this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can find out where the single task was deployed to each cluster node using
    `docker stack ps myapp`. Thus, on any particular node, we can list all the containers
    to find one of our stacks. In my example, task 3 had been deployed to node `ip-172-31-32-21`,
    which happens to be the master.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, list the containers on that node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 15.10 – Displaying the health status of a running task instance](img/Image98410.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.10 – Displaying the health status of a running task instance
  prefs: []
  type: TYPE_NORMAL
- en: The interesting thing in this screenshot is the **STATUS** column. Docker, or
    more precisely, SwarmKit, has recognized that the service has a health check function
    defined and is using it to determine the health of each task of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see what happens if something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, things don’t go as expected. A last-minute fix to an application
    release may have inadvertently introduced a new bug, or the new version significantly
    may have significantly decreased the throughput of the component, and so on. In
    such cases, we need to have a plan B, which in most cases means the ability to
    roll back the update to the previous good version.
  prefs: []
  type: TYPE_NORMAL
- en: As with the update, the rollback has to happen so that it does not cause any
    outages in terms of the application; it needs to cause zero downtime. In that
    sense, a rollback can be looked at as a reverse update. We are installing a new
    version, yet this new version is actually the previous version.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the update behavior, we can declare, either in our `stack` files or
    in the Docker `service create` command, how the system should behave in case it
    needs to execute a rollback. Here, we have the `stack` file that we used previously,
    but this time with some rollback-relevant attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can create a stack file named `stack-rollback.yaml`, and add the preceding
    content to it. In this content, we define the details of the rolling update, the
    health checks, and the behavior during rollback. The health check is defined so
    that after an initial wait time of 2 seconds, the orchestrator starts to poll
    the service on `http://localhost` every 2 seconds and retries 3 times before it
    considers a task unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: If we do the math, then it takes at least 8 seconds until a task will be stopped
    if it is unhealthy due to a bug. So, now under `deploy`, we have a new entry called
    `monitor`. This entry defines how long newly deployed tasks should be monitored
    for health and whether or not to continue with the next batch in the rolling update.
    Here, in this sample, we have given it 10 seconds. This is slightly more than
    the 8 seconds we calculated it takes to discover that a defective service has
    been deployed, so this is good.
  prefs: []
  type: TYPE_NORMAL
- en: We also have a new entry, `failure_action`, which defines what the orchestrator
    will do if it encounters a failure during the rolling update, such as the service
    being unhealthy. By default, the action is just to stop the whole update process
    and leave the system in an intermediate state. The system is not down since it
    is a rolling update and at least some healthy instances of the service are still
    operational, but an operations engineer would be better at taking a look and fixing
    the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have defined the action to be a rollback. Thus, in case of failure,
    SwarmKit will automatically revert all tasks that have been updated back to their
    previous version.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B19199_09.xhtml#_idTextAnchor194), *Learning about* *Distributed
    Application Architecture*, we discussed what blue-green deployments are, in an
    abstract way. It turns out that, on Docker Swarm, we cannot really implement blue-green
    deployments for arbitrary services. The service discovery and load balancing between
    two services running in Docker Swarm are part of the Swarm routing mesh and cannot
    be (easily) customized.
  prefs: []
  type: TYPE_NORMAL
- en: 'If **Service A** wants to call **Service B**, then Docker does this implicitly.
    Docker, given the name of the target service, will use the Docker DNS service
    to resolve this name to a VIP address. When the request is then targeted at the
    VIP, the Linux IPVS service will do another lookup in the Linux kernel IP tables
    with the VIP and load-balance the request to one of the physical IP addresses
    of the tasks of the service represented by the VIP, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.11 – How service discovery and load balancing work in Docker Swarm](img/Image98420.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.11 – How service discovery and load balancing work in Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no easy way to intercept this mechanism and replace
    it with custom behavior, but this would be needed to allow for a true blue-green
    deployment of **Service B**, which is the target service in our example. As we
    will see in [*Chapter 17*](B19199_17.xhtml#_idTextAnchor374), *Deploying, Updating,
    and Securing an Application with Kubernetes*, Kubernetes is more flexible in this
    area.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, we can always deploy the public-facing services in a blue-green
    fashion. We can use the **interlock 2** product and its layer-7 routing mechanism
    to allow for a true blue-green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Technically speaking, rolling updates are a kind of canary release, but due
    to their lack of seams, where you can plug customized logic into the system, rolling
    updates are only a very limited version of canary releases.
  prefs: []
  type: TYPE_NORMAL
- en: True canary releases require us to have more fine-grained control over the update
    process. Also, true canary releases do not take down the old version of the service
    until 100% of the traffic has been funneled through the new version. In that regard,
    they are treated like blue-green deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In a canary release scenario, we don’t just want to use things such as health
    checks as deciding factors regarding whether or not to funnel more and more traffic
    through the new version of the service; we also want to consider external input
    in the decision-making process, such as metrics that are collected and aggregated
    by a log aggregator or tracing information. An example that could be used as a
    decision-maker includes conformance to **Service Level Agreements** (**SLAs**),
    namely whether the new version of the service shows response times that are outside
    of the tolerance band. This can happen if we add new functionality to an existing
    service yet this new functionality degrades the response time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to deploy an application causing zero downtime, we want
    to discuss how we can store configuration data used by the applications in the
    swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Storing configuration data in the swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to store non-sensitive data such as configuration files in Docker
    Swarm, then we can use Docker configs. Docker configs are very similar to Docker
    secrets, which we will discuss in the next section. The main difference is that
    config values are not encrypted at rest, while secrets are. Like Docker secrets,
    Docker configs can only be used in Docker Swarm – that is, they cannot be used
    in your non-Swarm development environment. Docker configs are mounted directly
    into the container’s filesystem. Configuration values can either be strings or
    binary values up to a size of 500 KB.
  prefs: []
  type: TYPE_NORMAL
- en: With the use of Docker configs, you can separate the configuration from Docker
    images and containers. This way, your services can easily be configured with environment-specific
    values. The production swarm environment has different configuration values from
    the staging swarm, which in turn has different config values from the development
    or integration environment.
  prefs: []
  type: TYPE_NORMAL
- en: We can add configs to services and also remove them from running services. Configs
    can even be shared among different services running in the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create some Docker configs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start with a simple string value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note the hyphen at the end of the `docker config create` command. This
    means that Docker expects the value of the config from standard input. This is
    exactly what we’re doing by piping the `Hello world` value into the `create` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command results in an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command creates a config named `hello-config` with the value “`Hello
    world`.” The output of this command is the unique ID of this new config that’s
    being stored in the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we got and use the `list` command to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will output the following (which has been shortened):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `list` command shows the `ID` and `NAME` information for the
    config we just created, as well as its `CREATED` and (last) updated time. However,
    configs are non-confidential.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that reason, we can do more and even output the content of a config, like
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Hmmm, interesting. In the `Spec` subnode of the preceding JSON-formatted output,
    we have the `Data` key with a value of `SGVsbG8gd29ybGQK`. Didn’t we just say
    that the config data is not encrypted at rest?
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that the value is just our string encoded as `base64`, as we can
    easily verify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: So far, so good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s define a somewhat more complicated Docker config. Let’s assume we
    are developing a Java application. Java’s preferred way of passing configuration
    data to the application is the use of so-called `properties` files. A `properties`
    file is just a text file containing a list of key-value pairs. Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a file called `my-app.properties` and add the following content
    to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the file and create a Docker config called `app.properties` from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To prepare the next command, first, install the `jq` tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use this (somewhat contrived) command to get the cleartext value
    of the config we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a Docker service that uses the preceding config. For simplicity,
    we will be using the `nginx` image to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The interesting part in the preceding `service create` command is the line that
    contains the `--config` parameter. With this line, we’re telling Docker to use
    the config named `app.properties` and mount it as a file at `/etc/myapp/conf/app.properties`
    inside the container. Furthermore, we want that file to have `mode 0440` assigned
    to it to give the owner (root) and the group read permission.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we can see that the only instance of the service is
    running on node `ip-172-31-32-21`. On this node, I can now list the containers
    to get the ID of the `nginx` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can `exec` into that container and output the value of the `/``etc/myapp/conf/app.properties`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that `44417` in the preceding command represents the first part of the
    container hash.
  prefs: []
  type: TYPE_NORMAL
- en: 'This then will give us the expected values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: No surprise here; this is exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker configs can, of course, also be removed from the swarm, but only if
    they are not being used. If we try to remove the config we were just using previously,
    without first stopping and removing the service, we would get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Oh no, that did not work, as we can see from the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We get an error message in which Docker is nice enough to tell us that the config
    is being used by our service called nginx. This behavior is somewhat similar to
    what we are used to when working with Docker volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, first, we need to remove the service and then we can remove the config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And now it should work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note once more that Docker configs should never be used to
    store confidential data such as secrets, passwords, or access keys and key secrets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how to handle confidential data.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting sensitive data with Docker secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Secrets are used to work with confidential data in a secure way. Swarm secrets
    are secure at rest and in transit. That is, when a new secret is created on a
    manager node, and it can only be created on a manager node, its value is encrypted
    and stored in the raft consensus storage. This is why it is secure at rest. If
    a service gets a secret assigned to it, then the manager reads the secret from
    storage, decrypts it, and forwards it to all the containers that are instances
    of the swarm service that requested the secret. Since node-to-node communication
    in Docker Swarm uses `tmpFS` into the container. By default, secrets are mounted
    into the container at `/run/secrets`, but you can change that to any custom folder.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that secrets will not be encrypted on Windows nodes
    since there is no concept similar to `tmpfs`. To achieve the same level of security
    that you would get on a Linux node, the administrator should encrypt the disk
    of the respective Windows node.
  prefs: []
  type: TYPE_NORMAL
- en: Creating secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s see how we can actually create a secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a secret called `sample-secret` with a value of `sample
    secret value`. Please note the hyphen at the end of the `docker secret create`
    command. This means that Docker expects the value of the secret from standard
    input. This is exactly what we’re doing by piping `sample secret value` into the
    `create` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use a file as the source for the secret value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `secret-value.txt` file as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the Docker secret from this file with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the value of the secret with the name `other-secret` is read from a file
    called `./secret-value.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a secret has been created, there is no way to access the value of it.
    We can, for example, list all our secrets to get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 15.12 – List of all secrets](img/Image98428.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.12 – List of all secrets
  prefs: []
  type: TYPE_NORMAL
- en: In this list, we can only see the `ID` and `NAME` info for the secret, plus
    some other metadata, but the actual value of the secret is not visible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use `inspect` on a secret, for example, to get more information
    about `other-secret`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 15.13 – Inspecting a swarm secret](img/Image98437.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.13 – Inspecting a swarm secret
  prefs: []
  type: TYPE_NORMAL
- en: 'Even here, we do not get the value of the secret back. This is, of course,
    intentional: a secret is a secret and thus needs to remain confidential. We can
    assign labels to secrets if we want and we can even use a different driver to
    encrypt and decrypt the secret if we’re not happy with what Docker delivers out
    of the box.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a secret
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Secrets are used by services that run in the swarm. Usually, secrets are assigned
    to a service at creation time. Thus, if we want to run a service called `web`
    and assign it a secret, say, `api-secret-key`, the syntax would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a service called `web` based on the `fundamentalsofdocker/whoami:latest`
    image, publishes the container port `8000` to port `8000` on all swarm nodes,
    and assigns it the secret called `api-secret-key`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will only work if the secret called `api-secret-key` is defined in the
    swarm; otherwise, an error will be generated with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, let’s create this secret now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we rerun the `service create` command, it will succeed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use `docker service ps web` to find out on which node the sole service
    instance has been deployed, and then `exec` into this container. In my case, the
    instance has been deployed to node `ip-172-31-32-21`, which coincidentally happens
    to be the `manager1` EC2 instance on which I am already working. Otherwise, I
    would have to SSH into the other node first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, I list all my containers on that node with `docker container ls` to find
    the one instance belonging to my service and copy its container ID. We can then
    run the following command to make sure that the secret is indeed available inside
    the container under the expected filename containing the secret value in cleartext:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, in my case, the output generated is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This is evidently what we expected. We can see the secret in cleartext.
  prefs: []
  type: TYPE_NORMAL
- en: 'If, for some reason, the default location where Docker mounts the secrets inside
    the container is not acceptable to you, you can define a custom location. In the
    following command, we mount the secret to `/app/my-secrets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In this command, we are using the extended syntax to define a secret that includes
    the destination folder.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating secrets in a development environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working in development, we usually don’t have a local swarm on our machine.
    However, secrets only work in a swarm. So, what can we do? Well, luckily, this
    answer is really simple.
  prefs: []
  type: TYPE_NORMAL
- en: Since secrets are treated as files, we can easily mount a volume that contains
    the secrets into the container to the expected location, which by default is at
    `/run/secrets`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that we have a folder called `./dev-secrets` on our local workstation.
    For each secret, we have a file named the same as the secret name and with the
    unencrypted value of the secret as the content of the file. For example, we can
    simulate a secret called `demo-secret` with a secret value of `demo secret value`
    by executing the following command on our workstation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create a container that mounts this folder, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The process running inside the container will be unable to distinguish these
    mounted files from the ones originating from a secret. So, for example, `demo-secret`
    is available as a file called `/run/secrets/demo-secret` inside the container
    and has the expected value, `demo secret value`. Let’s take a look at this in
    more detail in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this, we can `exec` a shell inside the preceding container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can navigate to the `/run/secrets` folder and display the content of
    the `demo-secret` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will look at secrets and legacy applications.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets and legacy applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we want to containerize a legacy application that we cannot easily,
    or do not want to, change. This legacy application might expect a secret value
    to be available as an environment variable. How are we going to deal with this
    now? Docker presents us with the secrets as files, but the application is expecting
    them in the form of environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this situation, it is helpful to define a script that runs when the container
    is started (a so-called entry point or startup script). This script will read
    the secret value from the respective file and define an environment variable with
    the same name as the file, assigning the new variable the value read from the
    file. In the case of a secret called `demo-secret` whose value should be available
    in an environment variable called `DEMO_SECRET`, the necessary code snippet in
    this startup script could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, let’s say we have a legacy application that expects the secret values
    to be present as an entry in, say, a YAML configuration file located in the `/app/bin`
    folder and called `app.config`, whose relevant part looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Our initialization script now needs to read the secret value from the secret
    file and replace the corresponding placeholder in the config file with the secret
    value. For `demo_secret`, this could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we’re using the `sed` tool to replace a placeholder
    with a value in place. We can use the same technique for the other two secrets
    in the config file.
  prefs: []
  type: TYPE_NORMAL
- en: We put all the initialization logic into a file called `entrypoint.sh`, make
    this file executable and, for example, add it to the root of the container’s filesystem.
    Then, we define this file as `ENTRYPOINT` in the Dockerfile, or we can override
    the existing `ENTRYPOINT` of an image in the `docker container` `run` command.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make a sample. Let’s assume that we have a legacy application running
    inside a container defined by the `fundamentalsofdocker/whoami:latest` image that
    expects a secret called `db_password` to be defined in a file, `whoami.conf`,
    in the application folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a file, `whoami.conf`, on our local machine that contains the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The important part is line 3 of this snippet. It defines where the secret value
    has to be put by the startup script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add a file called `entrypoint.sh` to the local folder that contains the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last line in the preceding script stems from the fact that this is the start
    command that was used in the original Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, change the mode of this file to an executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define a Dockerfile that inherits from the `fundamentalsofdocker/whoami:latest`
    image. Add a file called `Dockerfile` to the current folder that contains the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s build the image from this Dockerfile:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the image has been built, we can run a service from it, but before we
    can do that, we need to define the secret in Swarm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create a service that uses the following secret:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Updating secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At times, we need to update a secret in a running service since secrets could
    be leaked out to the public or be stolen by malicious people, such as hackers.
    In this case, we need to change our confidential data since the moment it is leaked
    to a non-trusted entity, it has to be considered insecure.
  prefs: []
  type: TYPE_NORMAL
- en: Updating secrets, like any other update, requires zero downtime. Docker SwarmKit
    supports us in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new secret in the swarm. It is recommended to use a versioning
    strategy when doing so. In our example, we use a version as a postfix of the secret
    name. We originally started with the secret named `db-password` and now the new
    version of this secret is called `db-password-v2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s assume that the original service that used the secret had been created
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The application running inside the container was able to access the secret
    at `/run/secrets/db-password`. Now, SwarmKit does not allow us to update an existing
    secret in a running service, so we have to remove the now obsolete version of
    the secret and then add the new one. Let’s start with removal with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can add the new secret with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Please note the extended syntax of `--secret-add` with the source and target
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the routing mesh, which provides layer-4 routing
    and load balancing to a Docker Swarm. We then learned how SwarmKit allows us to
    update services without requiring downtime. Furthermore, we discussed the current
    limits of SwarmKit in regard to zero-downtime deployments. Then, we showed how
    to store configuration data in the Swarm, and in the last part of this chapter,
    we introduced secrets as a means to provide confidential data to services in a
    highly secure way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the currently most popular container
    orchestrator, Kubernetes. We’ll discuss the objects that are used to define and
    run a distributed, resilient, robust, and highly available application in a Kubernetes
    cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that’s
    used to locally deploy a Kubernetes application, and also demonstrate the integration
    of Kubernetes with Docker Desktop.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your learning progress, please try to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: In a few simple sentences, explain to an interested lay person what zero-downtime
    deployment means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does SwarmKit achieve zero-downtime deployments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contrary to traditional (non-containerized) systems, why does a rollback in
    Docker Swarm just work? Explain this in a few short sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe two to three characteristics of a Docker secret.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You need to roll out a new version of the inventory service. What does your
    command look like? Here is some more information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new image is called `acme/inventory:2.1`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to use a rolling update strategy with a batch size of two tasks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We want the system to wait for one minute after each batch
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to update an existing service named `inventory` with a new password
    that is provided through a Docker secret. The new secret is called `MYSQL_PASSWORD_V2`.
    The code in the service expects the secret to be called `MYSQL_PASSWORD`. What
    does the update command look like? (Note that we do not want the code of the service
    to be changed!)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are sample answers to the preceding questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime deployment means that a new version of a service in a distributed
    application is updated to a new version without the application needing to stop
    working. Usually, with Docker SwarmKit or Kubernetes (as we will see), this is
    done in a rolling fashion. A service consists of multiple instances and those
    are updated in batches so that the majority of the instances are up and running
    at all times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, Docker SwarmKit uses a rolling updated strategy to achieve zero-downtime
    deployments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Containers are self-contained units of deployment. If a new version of a service
    is deployed and does not work as expected, we (or the system) only need to roll
    back to the previous version. The previous version of the service is also deployed
    in the form of self-contained containers. Conceptually, there is no difference
    between rolling forward (an update) or backward (a rollback). One version of a
    container is replaced by another one. The host itself is not affected by such
    changes in any way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker secrets are encrypted at rest. They are only transferred to the services
    and containers that use the secrets. Secrets are transferred encrypted due to
    the fact that the communication between swarm nodes uses mTLS. Secrets are never
    physically stored on a worker node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The command to achieve this is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we need to remove the old secret from the service, and then we need
    to add the new version to it (directly updating a secret is not possible):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Part 4:Docker, Kubernetes, and the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part introduces the currently most popular container orchestrator. It introduces
    the core Kubernetes objects that are used to define and run a distributed, resilient,
    robust, and highly available application in a cluster. Finally, it introduces
    minikube as a way to locally deploy a Kubernetes application and also covers the
    integration of Kubernetes with Docker for Mac and Docker Desktop.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B19199_16.xhtml#_idTextAnchor349), *Introducing Kubernetes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B19199_17.xhtml#_idTextAnchor374), *Deploying, Updating, and
    Securing an Application with Kubernetes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 18*](B19199_18.xhtml#_idTextAnchor396), *Running a Containerized
    Application in the Cloud*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 19*](B19199_19.xhtml#_idTextAnchor412), *Monitoring and Troubleshooting
    an Application Running in Production*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
