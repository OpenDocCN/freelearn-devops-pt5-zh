- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Deploying and Running a Distributed Application on Docker Swarm
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Docker Swarm 上部署和运行分布式应用程序
- en: In the last chapter, we got a detailed introduction to Docker’s native orchestrator
    called SwarmKit. SwarmKit is part of Docker Engine, and no extra installation
    is needed once you have Docker installed on your system. We learned about the
    concepts and objects SwarmKit uses to deploy and run distributed, resilient, robust,
    and highly available applications in a cluster, which can either run on-premises
    or in the cloud. We also showed how Docker’s orchestrator secures applications
    using SDNs. We learned how to create a Docker Swarm locally, in a special environment
    called Play with Docker, and also in the cloud. Finally, we discovered how to
    deploy an application that consists of multiple related services to Docker Swarm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们详细介绍了 Docker 的原生调度器 SwarmKit。SwarmKit 是 Docker 引擎的一部分，一旦在系统中安装了 Docker，无需额外安装。我们了解了
    SwarmKit 用于在集群中部署和运行分布式、弹性、稳健且高可用的应用程序的概念和对象，这些应用程序可以运行在本地或云端。我们还展示了 Docker 的调度器如何使用软件定义网络（SDN）保护应用程序。我们学习了如何在名为“Play
    with Docker”的特殊环境中以及在云端本地创建 Docker Swarm。最后，我们发现如何将一个由多个相关服务组成的应用程序部署到 Docker
    Swarm 上。
- en: In this chapter, we are going to introduce the routing mesh, which provides
    layer-4 routing and load balancing. Next, we are going to demonstrate how to deploy
    a first application consisting of multiple services onto the Swarm. We are also
    learning how to achieve zero downtime when updating an application in the swarm
    and finally how to store configuration data in the swarm and how to protect sensitive
    data using Docker secrets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍路由网格，它提供第4层路由和负载均衡。接下来，我们将演示如何将一个由多个服务组成的第一个应用程序部署到 Swarm 中。我们还将学习如何在
    Swarm 中更新应用程序时实现零停机，最后将介绍如何在 Swarm 中存储配置数据，以及如何使用 Docker 秘密保护敏感数据。
- en: 'These are the topics we are going to discuss in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: The Swarm routing mesh
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 路由网格
- en: Zero-downtime deployment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零停机部署
- en: Storing configuration data in the swarm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Swarm 中存储配置数据
- en: Protecting sensitive data with Docker Secrets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker Secrets 保护敏感数据
- en: 'After completing this chapter, you will be able to do the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够做到以下几点：
- en: List two to three different deployment strategies commonly used to update a
    service without downtime
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出两到三种常见的部署策略，用于在不造成停机的情况下更新服务
- en: Update a service in batches without causing a service interruption
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不引起服务中断的情况下批量更新服务
- en: Define a rollback strategy for a service that is used if an update fails
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个回滚策略，以防更新失败时用于恢复服务
- en: Store non-sensitive configuration data using Docker configs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 配置存储非敏感配置数据
- en: Use a Docker secret with a service
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 秘密与服务配合使用
- en: Update the value of a secret without causing downtime
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新秘密的值而不引起停机
- en: Let’s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: The swarm routing mesh
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm 路由网格
- en: If you have paid attention, then you might have noticed something interesting
    in the last chapter. We had the `pets` application deployed and it resulted in
    an instance of the web service being installed on the three nodes – `node-1`,
    `node-2`, and `node-3`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到的话，可能会在上一章中看到一些有趣的现象。我们已经部署了 `pets` 应用程序，并且该应用程序在三个节点——`node-1`、`node-2`
    和 `node-3` 上安装了 Web 服务的实例。
- en: 'Yet, we were able to access the web service on `node-1` with `localhost` and
    we reached each container from there. How is that possible? Well, this is due
    to the so-called Swarm routing mesh. The routing mesh makes sure that when we
    publish a port of a service, that port is then published on all nodes of the Swarm.
    Hence, network traffic that hits any node of the Swarm and requests to use a specific
    port will be forwarded to one of the service containers by routing the mesh. Let’s
    look at the following diagram to see how that works:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们能够通过 `localhost` 访问 `node-1` 上的 Web 服务，并且从那里访问每个容器。这是如何实现的呢？这正是所谓的 Swarm
    路由网格的功劳。路由网格确保，当我们发布服务的端口时，该端口会在 Swarm 的所有节点上发布。因此，任何访问 Swarm 节点并请求使用特定端口的网络流量，都将通过路由网格转发到其中一个服务容器。让我们看一下下面的图示，了解它是如何工作的：
- en: '![Figure 15.1 – Docker Swarm routing mesh](img/Image98324.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.1 – Docker Swarm 路由网格](img/Image98324.jpg)'
- en: Figure 15.1 – Docker Swarm routing mesh
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1 – Docker Swarm 路由网格
- en: In this situation, we have three nodes, called `172.10.0.15`, `172.10.0.17`,
    and `172.10.0.33`. In the lower-left corner of the diagram, we see the command
    that created a web service with two replicas. The corresponding tasks have been
    scheduled on **Host B** and **Host C**. **task1** landed on **Host B** while **task2**
    landed on **Host C**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有三个节点，分别为`172.10.0.15`、`172.10.0.17`和`172.10.0.33`。在图的左下角，我们看到创建了一个包含两个副本的网页服务的命令。相应的任务已经被调度到**Host
    B**和**Host C**上。**task1** 被安排在**Host B**，而**task2** 被安排在**Host C**。
- en: When a service is created in Docker Swarm, it automatically gets a `10.2.0.1`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Docker Swarm 中创建服务时，它会自动获得一个`10.2.0.1`。
- en: If now a request for port `8080` coming from an external `8080` in the IP table
    and will find that this corresponds to the VIP of the web service.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在有一个来自外部的`8080`端口的请求，IP 表中会发现这个请求对应的是网页服务的 VIP。
- en: Now, since the VIP is not a real target, the IPVS service will load-balance
    the IP addresses of the tasks that are associated with this service. In our case,
    it picked `10.2.0.3`. Finally, **Ingress Network (Overlay)** is used to forward
    the request to the target container on **Host C**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于 VIP 并非真实目标，IPVS 服务将对与该服务关联的任务的 IP 地址进行负载均衡。在我们的案例中，它选择了`10.2.0.3`。最后，**Ingress
    网络（Overlay）** 被用来将请求转发到**Host C**上的目标容器。
- en: It is important to note that it doesn’t matter which Swarm node the external
    request is forwarded to by **External LB**. The routing mesh will always handle
    the request correctly and forward it to one of the tasks of the targeted service.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，**外部负载均衡器**将外部请求转发到哪个 Swarm 节点并不重要。路由网格始终会正确处理请求并将其转发到目标服务的某个任务。
- en: We have learned a lot about networking in a Docker swarm. The next topic that
    we are going to learn about is how can we deploy an application without causing
    any system downtime.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到了很多关于 Docker swarm 网络的知识。接下来我们要学习的主题是如何在不造成系统停机的情况下部署应用程序。
- en: Zero-downtime deployment
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署
- en: One of the most important aspects of a mission-critical application that needs
    frequent updates is the ability to do updates in a fashion that requires no outage
    at all. We call this a zero-downtime deployment. At all times, the application
    that is updated must be fully operational.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要频繁更新的关键任务应用程序中，最重要的方面之一就是能够以不产生任何停机时间的方式进行更新。我们称之为零停机部署。更新中的应用程序必须始终保持完全可用。
- en: Popular deployment strategies
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的部署策略
- en: 'There are various ways to achieve this. Some of them are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标有多种方法，其中一些方法如下：
- en: Rolling updates
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新
- en: Blue-green deployments
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Canary releases
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Docker Swarm supports rolling updates out of the box. The other two types of
    deployments can be achieved with some extra effort on our part.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 开箱即支持滚动更新。其他两种部署方式需要我们付出额外的努力才能实现。
- en: Rolling updates
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滚动更新
- en: In a mission-critical application, each application service has to run in multiple
    replicas. Depending on the load, that can be as few as two to three instances
    and as many as dozens, hundreds, or thousands of instances. At any given time,
    we want to have a clear majority when it comes to all the service instances running.
    So, if we have three replicas, we want to have at least two of them up and running
    at all times. If we have 100 replicas, we can be content with a minimum of, say,
    90 replicas, available. By doing this, we can define the batch size of replicas
    that we may take down to upgrade. In the first case, the batch size would be 1,
    and in the second case, it would be 10.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键任务应用中，每个应用服务必须运行多个副本。根据负载，这个数量可以少至两到三个实例，也可以多至几十、几百甚至上千个实例。在任何给定时刻，我们希望所有运行中的服务实例中有明确的大多数。所以，如果我们有三个副本，我们希望至少有两个副本始终运行。如果我们有
    100 个副本，我们可以接受最少 90 个副本可用。通过这种方式，我们可以定义在升级时可以停机的副本的批次大小。在第一个案例中，批次大小为 1，而在第二个案例中，批次大小为
    10。
- en: When we take replicas down, Docker Swarm will automatically take those instances
    out of the load-balancing pool and all traffic will be load-balanced across the
    remaining active instances. Those remaining instances will thus experience a slight
    increase in traffic. In the following diagram, prior to the start of the rolling
    update, if **Task A3** wanted to access **Service B**, it could be load-balanced
    to any of the three tasks of **Service B** by SwarmKit. Once the rolling update
    started, SwarmKit took down **Task B1** for updates.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们停用副本时，Docker Swarm 会自动将这些实例从负载均衡池中移除，所有流量将会被负载均衡到剩余的活动实例上。因此，这些剩余的实例将会经历流量的轻微增加。在下面的图示中，在滚动更新开始之前，如果**任务
    A3**想要访问**服务 B**，它可以通过 SwarmKit 将流量负载均衡到**服务 B**的任意一个三个任务中。一旦滚动更新开始，SwarmKit 就会停用**任务
    B1**进行更新。
- en: 'Automatically, this task is then taken out of the pool of targets. So, if **Task
    A3** now requests to connect to **Service B**, load balancing will only select
    from the remaining tasks, that is, **Task B2** and **Task B3**. Thus, those two
    tasks might experience a higher load temporarily:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自动地，这个任务会被从目标池中移除。所以，如果**任务 A3**现在请求连接到**服务 B**，负载均衡将只从剩余的任务中选择，也就是**任务 B2**和**任务
    B3**。因此，这两个任务可能会暂时经历更高的负载：
- en: '![Figure 15.2 – Task B1 is taken down to be updated](img/Image98334.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.2 – 任务 B1 被停用以进行更新](img/Image98334.jpg)'
- en: Figure 15.2 – Task B1 is taken down to be updated
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2 – 任务 B1 被停用以进行更新
- en: The stopped instances are then replaced by an equivalent number of new instances
    of the new version of the application service. Once the new instances are up and
    running, we can have the Swarm observe them for a given period of time and make
    sure they’re healthy. If all is well, then we can continue by taking down the
    next batch of instances and replacing them with instances of the new version.
    This process is repeated until all the instances of the application service have
    been replaced.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 停止的实例随后会被等量的新版本应用服务实例替代。一旦新实例启动并运行，我们可以让 Swarm 在给定时间内监控它们，确保它们是健康的。如果一切正常，那么我们可以继续通过停用下一批实例，并用新版本的实例替换它们。这个过程会一直重复，直到所有的应用服务实例都被替换。
- en: 'In the following diagram, we can see that **Task B1** of **Service B** has
    been updated to version 2\. The container of **Task B1** was assigned a new IP
    address, and it was deployed to another worker node with free resources:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图示中，我们可以看到**服务 B**的**任务 B1**已经更新到版本 2。**任务 B1**的容器被分配了一个新的 IP 地址，并且它被部署到了一个具有空闲资源的另一个工作节点：
- en: '![Figure 15.3 – The first batch being updated in a rolling update](img/Image98343.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.3 – 在滚动更新中，第一批任务正在更新](img/Image98343.jpg)'
- en: Figure 15.3 – The first batch being updated in a rolling update
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3 – 在滚动更新中，第一批任务正在更新
- en: It is important to understand that when the task of a service is updated, in
    most cases, it gets deployed to a worker node other than the one it used to live
    on, but that should be fine as long as the corresponding service is stateless.
    If we have a stateful service that is location- or node-aware and we’d like to
    update it, then we have to adjust our approach, but this is outside of the scope
    of this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，当服务的任务被更新时，在大多数情况下，它会被部署到与原来不同的工作节点上，但只要相应的服务是无状态的，这应该是没问题的。如果我们有一个有状态的服务，它是位置或节点感知的，并且我们想要更新它，那么我们就必须调整方法，但这超出了本书的范围。
- en: 'Now, let’s look at how we can actually instruct the Swarm to perform a rolling
    update of an application service. When we declare a service in a `stack` file,
    we can define multiple options that are relevant in this context. Let’s look at
    a snippet of a typical `stack` file:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何实际指示 Swarm 执行应用服务的滚动更新。当我们在`stack`文件中声明服务时，我们可以定义多个与此上下文相关的选项。让我们来看一个典型的`stack`文件片段：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this snippet, we can see a section, `update_config`, with `parallelism` and
    `delay` properties. `parallelism` defines the batch size of how many replicas
    are going to be updated at a time during a rolling update. `delay` defines how
    long Docker Swarm is going to wait between updating individual batches. In the
    preceding case, we have 10 replicas that are being updated in two instances at
    a time and, between each successful update, Docker Swarm waits for 10 seconds.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，我们可以看到一个名为`update_config`的部分，其中包含`parallelism`和`delay`属性。`parallelism`定义了滚动更新时每次更新多少副本的批量大小。`delay`定义了
    Docker Swarm 在更新单个批次之间将等待多长时间。在前面的例子中，我们有 10 个副本，每次更新 2 个实例，并且在每次成功更新后，Docker
    Swarm 会等待 10 秒。
- en: Let’s test such a rolling update. Navigate to the `ch14` subfolder of our `sample-solutions`
    folder and use the `web-stack.yaml` file to create a web service that’s been configured
    for a rolling update. The service uses an Alpine-based Nginx image whose version
    is `1.12-alpine`. We will update the service to a newer version, that is, `1.13-alpine`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下这样的滚动更新。导航到我们 `sample-solutions` 文件夹下的 `ch14` 子文件夹，使用 `web-stack.yaml`
    文件创建一个已配置滚动更新的 web 服务。该服务使用基于 Alpine 的 Nginx 镜像，版本为 `1.12-alpine`。我们将把服务更新到更新版本，即
    `1.13-alpine`。
- en: To start, we will deploy this service to the swarm that we created in AWS.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将把这个服务部署到我们在 AWS 上创建的 Swarm 集群中。
- en: 'Let’s take a look:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下：
- en: 'SSH into the `master1` instance of your Docker swarm on AWS:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 SSH 登录到 AWS 上 Docker Swarm 的 `master1` 实例：
- en: '[PRE1]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a file called `web-stack.yml` using `vi` or `nano` with this content:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `vi` 或 `nano` 创建一个名为 `web-stack.yml` 的文件，并包含以下内容：
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we can deploy the service using the `stack` file:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `stack` 文件来部署服务：
- en: '[PRE3]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the preceding command looks like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the service has been deployed, we can monitor it using the following command:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务部署完成，我们可以使用以下命令来监控它：
- en: '[PRE5]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will see the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![Figure 15.4 – Service web of the web stack running in Swarm with 10 replicas](img/Image98354.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.4 – 在 Swarm 中运行的 web 堆栈的 web 服务，包含 10 个副本](img/Image98354.jpg)'
- en: Figure 15.4 – Service web of the web stack running in Swarm with 10 replicas
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.4 – 在 Swarm 中运行的 web 堆栈的 web 服务，包含 10 个副本
- en: The previous command will continuously update the output and provide us with
    a good overview of what happens during the rolling update.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将持续更新输出，并为我们提供滚动更新过程中发生的情况的概览。
- en: 'Now, we need to open a second Terminal and configure it for remote access for
    the manager node of our swarm. Once we have done that, we can execute the `docker`
    command, which will update the image of the web service of the `stack`, also called
    `web`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要打开第二个终端并为我们 Swarm 的管理节点配置远程访问。完成后，我们就可以执行 `docker` 命令，更新 `stack` 的 web
    服务镜像，也叫 `web`：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding command leads to the following output, indicating the progress
    of the rolling update:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令会产生以下输出，表示滚动更新的进度：
- en: '![Figure 15.5 – Screen showing the progress of the rolling update](img/Image98364.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.5 – 显示滚动更新进度的屏幕](img/Image98364.jpg)'
- en: Figure 15.5 – Screen showing the progress of the rolling update
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5 – 显示滚动更新进度的屏幕
- en: The preceding output indicates that the first two batches, each with two tasks,
    have been successful and that the third batch is about to be prepared.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出表示前两批任务（每批有两个任务）已经成功，第三批即将准备好。
- en: 'In the first Terminal window, where we’re watching the `stack`, we should now
    see how Docker Swarm updates the service batch by batch with an interval of 10
    seconds. After the first batch, it should look like the following screenshot:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个终端窗口中，我们正在查看 `stack`，现在我们应该能看到 Docker Swarm 如何每 10 秒更新一次服务，每批更新后看起来应该像以下截图所示：
- en: '![Figure 15.6 – Rolling update for a service in Docker Swarm](img/Image98374.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.6 – Docker Swarm 中服务的滚动更新](img/Image98374.jpg)'
- en: Figure 15.6 – Rolling update for a service in Docker Swarm
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6 – Docker Swarm 中服务的滚动更新
- en: In the preceding screenshot, we can see that the first batch of the two tasks,
    2 and 10, has been updated. Docker Swarm is waiting for 10 seconds to proceed
    with the next batch.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到任务 2 和任务 10 的第一批已更新。Docker Swarm 正在等待 10 秒钟，以便继续进行下一批任务。
- en: It is interesting to note that in this particular case, SwarmKit deploys the
    new version of the task to the same node as the previous version. This is accidental
    since we have five nodes and two tasks on each node. SwarmKit always tries to
    balance the workload evenly across the nodes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在这个特定案例中，SwarmKit 将新版本的任务部署到与之前版本相同的节点上。这是偶然的，因为我们有五个节点，每个节点上有两个任务。SwarmKit
    总是尽力平衡节点之间的工作负载。
- en: So, when SwarmKit takes down a task, the corresponding node has a smaller workload
    than all the others, so the new instance is scheduled to it. Normally, you cannot
    expect to find a new instance of a task on the same node. Just try it out yourself
    by deleting the `stack` with `docker stack rm web` and changing the number of
    replicas to say, seven, and then redeploy and update it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当 SwarmKit 停止一个任务时，相应的节点的工作负载会比其他节点小，因此新的实例会被调度到该节点。通常，你无法期望在同一个节点上找到一个新的任务实例。你可以通过删除
    `stack`（命令：`docker stack rm web`）并将副本数更改为七来尝试一下，然后重新部署并更新它。
- en: 'Once all the tasks have been updated, the output of our `docker stack ps web`
    command will look similar to the following screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有任务更新完成，我们的 `docker stack ps web` 命令的输出将类似于以下截图：
- en: '![Figure 15.7 – All tasks have been updated successfully](img/Image98382.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.7 – 所有任务已成功更新](img/Image98382.jpg)'
- en: Figure 15.7 – All tasks have been updated successfully
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7 – 所有任务已成功更新
- en: Please note that SwarmKit does not immediately remove the containers of the
    previous versions of the tasks from the corresponding nodes. This makes sense
    as we might want to, for example, retrieve the logs from those containers for
    debugging purposes, or we might want to retrieve their metadata using `docker
    container inspect`. SwarmKit keeps the four latest terminated task instances around
    before it purges older ones so that it doesn’t clog the system with unused resources.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SwarmKit 不会立即从相应节点中移除前版本任务的容器。这是有道理的，因为我们可能需要从这些容器中获取日志进行调试，或者可能需要使用 `docker
    container inspect` 获取它们的元数据。SwarmKit 会保留最近四个终止的任务实例，在清除较旧的实例之前，以避免系统被未使用的资源阻塞。
- en: We can use the `--update-order` parameter to instruct Docker to start the new
    container replica before stopping the old one. This can improve application availability.
    Valid values are `start-first` and `stop-first`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `--update-order` 参数指示 Docker 在停止旧容器之前先启动新容器副本。这可以提高应用程序的可用性。有效值为 `start-first`
    和 `stop-first`。
- en: The latter is the default.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 后者是默认设置。
- en: 'Once we’re done, we can tear down the `stack` using the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以使用以下命令拆除 `stack`：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Although using `stack` files to define and deploy applications is the recommended
    best practice, we can also define the update behavior in a `service create` statement.
    If we just want to deploy a single service, this might be the preferred way of
    doing things. Let’s look at such a `create` command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用 `stack` 文件来定义和部署应用程序是推荐的最佳实践，但我们也可以在 `service create` 语句中定义更新行为。如果我们只想部署一个单独的服务，这可能是更优的做法。让我们看看这样的
    `create` 命令：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command defines the same desired state as the preceding `stack` file. We
    want the service to run with 10 replicas and we want a rolling update to happen
    in batches of two tasks at a time, with a 10-second interval between consecutive
    batches.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令定义了与前面的`stack`文件相同的期望状态。我们希望服务运行 10 个副本，并且希望滚动更新一次更新两项任务，并且每批任务之间有 10 秒的间隔。
- en: Health checks
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 健康检查
- en: To make informed decisions, for example, during a rolling update of a Swarm
    service regarding whether or not the just-installed batch of new service instances
    is running OK or whether a rollback is needed, SwarmKit needs a way to know about
    the overall health of the system. On its own, SwarmKit (and Docker) can collect
    quite a bit of information, but there is a limit. Imagine a container containing
    an application. The container, as seen from the outside, can look absolutely healthy
    and carry on just fine, but that doesn’t necessarily mean that the application
    running inside the container is also doing well. The application could, for example,
    be in an infinite loop or be in a corrupt state, yet still be running. However,
    as long as the application runs, the container runs, and, from the outside, everything
    looks perfect.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出明智的决策，例如在 Swarm 服务的滚动更新过程中，判断刚安装的新服务实例批次是否正常运行，或者是否需要回滚，SwarmKit 需要一种方式来了解系统的整体健康状况。就其本身而言，SwarmKit（和
    Docker）能够收集相当多的信息，但也有一定的限制。想象一下，一个包含应用程序的容器。从外部看，容器可能看起来非常健康并且运行良好，但这并不意味着容器内部运行的应用程序也同样运行良好。应用程序可能处于无限循环或损坏状态，但仍然在运行。然而，只要应用程序还在运行，容器就会继续运行，从外部看一切看起来都很完美。
- en: Thus, SwarmKit provides a seam where we can provide it with some help. We, the
    authors of the application services running inside the containers in the swarm,
    know best whether or not our service is in a healthy state. SwarmKit gives us
    the opportunity to define a command that is executed against our application service
    to test its health. What exactly this command does is not important to Swarm;
    the command just needs to return *OK*, *NOT OK*, or *time out*. The latter two
    situations, namely NOT OK or timeout, will tell SwarmKit that the task it is investigating
    is potentially unhealthy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SwarmKit 提供了一个接口，我们可以通过它为 SwarmKit 提供一些帮助。我们，作为在 Swarm 中运行的容器应用服务的开发者，最清楚我们的服务是否处于健康状态。SwarmKit
    让我们有机会定义一个命令来测试我们的应用服务的健康状态。这个命令具体执行什么操作对 Swarm 来说并不重要；它只需要返回 *OK*、*NOT OK* 或
    *time out*。后两种情况，即 *NOT OK* 或超时，会告诉 SwarmKit 正在调查的任务可能不健康。
- en: 'Here, I am writing potentially on purpose, and we will see why later:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我故意用了“可能”这个词，稍后我们会看到为什么：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding snippet from a Dockerfile, we can see the `HEALTHCHECK` keyword.
    It has a few options or parameters and an actual command, that is, `CMD`. Let’s
    discuss the options:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Dockerfile 代码片段中，我们可以看到 `HEALTHCHECK` 关键字。它有几个选项或参数以及一个实际的命令，即 `CMD`。我们来讨论一下这些选项：
- en: '`--interval`: Defines the wait time between health checks. Thus, in our case,
    the orchestrator executes a check every 30 seconds.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--interval`：定义健康检查之间的等待时间。因此，在我们的案例中，调度器每 30 秒执行一次检查。'
- en: '`--timeout`: This parameter defines how long Docker should wait if the health
    check does not respond until it times out with an error. In our sample, this is
    10 seconds. Now, if one health check fails, SwarmKit retries a couple of times
    until it gives up and declares the corresponding task as unhealthy and opens the
    door for Docker to kill this task and replace it with a new instance.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--timeout`：此参数定义了如果健康检查未响应，Docker 应该等待多久才会超时并返回错误。在我们的示例中，设置为 10 秒。现在，如果某个健康检查失败，SwarmKit
    会重试几次，直到放弃并将相应的任务标记为不健康，并允许 Docker 终止该任务并用新实例替换它。'
- en: The number of retries is defined by the `--retries` parameter. In the preceding
    code, we want to have three retries.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重试次数由 `--retries` 参数定义。在前面的代码中，我们希望进行三次重试。
- en: Next, we have the start period. Some containers take some time to start up (not
    that this is a recommended pattern, but sometimes it is inevitable). During this
    startup time, the service instance might not be able to respond to health checks.
    With the start period, we can define how long SwarmKit should wait before it executes
    the very first health check and thus give the application time to initialize.
    To define the startup time, we use the `--start-period` parameter. In our case,
    we do the first check after 60 seconds. How long this start period needs to be
    depends on the application and its startup behavior. The recommendation is to
    start with a relatively low value and, if you have a lot of false positives and
    tasks that are restarted many times, you might want to increase the time interval.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是启动周期。某些容器启动需要一些时间（虽然这不是推荐的模式，但有时是不可避免的）。在此启动时间内，服务实例可能无法响应健康检查。通过启动周期，我们可以定义
    SwarmKit 在执行第一次健康检查之前应等待多长时间，从而为应用程序初始化提供时间。为了定义启动时间，我们使用 `--start-period` 参数。在我们的例子中，我们在
    60 秒后进行第一次检查。启动周期需要多长时间取决于应用程序及其启动行为。建议从一个较小的值开始，如果出现大量假阳性并且任务被多次重启，您可能希望增加时间间隔。
- en: 'Finally, we define the actual probing command on the last line with the `CMD`
    keyword. In our case, we are defining a request to the `/health` endpoint of `localhost`
    at port `3000` as a probing command. This call is expected to have three possible
    outcomes:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在最后一行使用 `CMD` 关键字定义实际的探测命令。在我们的例子中，我们定义了一个向 `localhost` 上的 `/health` 端点（端口为
    `3000`）发送请求的探测命令。这个调用预计会有三种可能的结果：
- en: The command succeeds
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令成功
- en: The command fails
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令失败
- en: The command times out
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令超时
- en: The latter two are treated the same way by SwarmKit. This is the orchestrator
    telling us that the corresponding task might be unhealthy. I did say *might* with
    intent since SwarmKit does not immediately assume the worst-case scenario but
    assumes that this might just be a temporary fluke of the task and that it will
    recover from it. This is the reason why we have a `--retries` parameter. There,
    we can define how many times SwarmKit should retry before it can assume that the
    task is indeed unhealthy, and consequently kill it and reschedule another instance
    of this task on another free node to reconcile the desired state of the service.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 后两个任务会被SwarmKit以相同的方式处理。这是调度器在告诉我们相应的任务可能处于不健康状态。我故意用了*可能*这个词，因为SwarmKit并不会立刻假设最坏的情况，而是认为这可能只是任务的暂时异常，任务会从中恢复。这也是我们需要`--retries`参数的原因。在这里，我们可以定义SwarmKit在假设任务确实不健康之前应尝试多少次，然后它会终止该任务并重新调度一个新实例到另一个空闲节点，以便恢复服务的期望状态。
- en: 'Why can we use `localhost` in our probing command? This is a very good question,
    and the reason is that SwarmKit, when probing a container running in the Swarm,
    executes this probing command inside the container (that is, it does something
    such as `docker container exec <containerID> <probing command>`). Thus, the command
    executes in the same network namespace as the application running inside the container.
    In the following diagram, we can see the life cycle of a service task from its
    beginning:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们可以在探测命令中使用`localhost`？这是一个非常好的问题，原因是SwarmKit在探测一个运行在Swarm中的容器时，会在容器内部执行该探测命令（也就是说，它会执行类似`docker
    container exec <containerID> <probing command>`的操作）。因此，命令会在与容器内部应用程序相同的网络命名空间中执行。在下面的示意图中，我们可以看到一个服务任务从开始到结束的生命周期：
- en: '![Figure 15.8 – Service task with transient health failure](img/Image98391.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图15.8 – 服务任务与暂时性健康失败](img/Image98391.jpg)'
- en: Figure 15.8 – Service task with transient health failure
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8 – 服务任务与暂时性健康失败
- en: First, SwarmKit waits to probe until the start period is over. Then, we have
    our first health check. Shortly thereafter, the task fails when probed. It fails
    two consecutive times but then it recovers. Thus, **health check 4** is successful
    and SwarmKit leaves the task running.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，SwarmKit会等到启动期结束后才开始探测。然后，我们进行第一次健康检查。不久之后，任务在探测时失败。它连续两次失败，但随后恢复。因此，**健康检查4**是成功的，SwarmKit保持任务运行。
- en: 'Here, we can see a task that is permanently failing:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到一个任务永久失败的情况：
- en: '![Figure 15.9 – Permanent failure of a task](img/Image98401.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图15.9 – 任务的永久失败](img/Image98401.jpg)'
- en: Figure 15.9 – Permanent failure of a task
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9 – 任务的永久失败
- en: 'We have just learned how we can define a health check for a service in the
    Dockerfile of its image, but this is not the only way we can do this. We can also
    define the health check in the `stack` file that we use to deploy our application
    into Docker Swarm. Here is a short snippet of what such a `stack` file would look
    like:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了如何在其镜像的Dockerfile中定义服务的健康检查，但这并不是我们能做到的唯一方式。我们还可以在我们用来将应用程序部署到Docker
    Swarm中的`stack`文件中定义健康检查。以下是一个`stack`文件的简短示例：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding snippet, we can see how the health check-related information
    is defined in the `stack` file. First and foremost, it is important to realize
    that we have to define a health check for every service individually. There is
    no health check at an application or global level.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们可以看到如何在`stack`文件中定义与健康检查相关的信息。首先，要明确的一点是，我们需要为每个服务单独定义健康检查。在应用程序层面或全局层面并没有健康检查。
- en: Similar to what we defined previously in the Dockerfile, the command that is
    used to execute the health check by SwarmKit is `curl -f http://localhost:3000/health`.
    We also have definitions for `interval`, `timeout`, `retries`, and `start_period`.
    These four key-value pairs have the same meaning as the corresponding parameters
    we used in the Dockerfile. If there are health check-related settings defined
    in the image, then the ones defined in the `stack` file override the ones from
    the Dockerfile.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前在Dockerfile中定义的相似，SwarmKit用来执行健康检查的命令是`curl -f http://localhost:3000/health`。我们还定义了`interval`、`timeout`、`retries`和`start_period`，这四个键值对与我们在Dockerfile中使用的对应参数意义相同。如果在镜像中定义了与健康检查相关的设置，那么在`stack`文件中定义的设置将覆盖Dockerfile中的设置。
- en: 'Now, let’s try to use a service that has a health check defined:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用一个已定义健康检查的服务：
- en: 'Use `vi` or `nano` to create a file called `stack-health.yml` with the following
    content:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`vi`或`nano`创建一个名为`stack-health.yml`的文件，内容如下：
- en: '[PRE11]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s deploy this:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们部署这个：
- en: '[PRE12]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can find out where the single task was deployed to each cluster node using
    `docker stack ps myapp`. Thus, on any particular node, we can list all the containers
    to find one of our stacks. In my example, task 3 had been deployed to node `ip-172-31-32-21`,
    which happens to be the master.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`docker stack ps myapp`查看每个集群节点上单个任务的部署情况。因此，在任何特定节点上，我们可以列出所有容器，找到我们堆栈中的一个。在我的示例中，任务
    3 被部署到了节点`ip-172-31-32-21`，该节点恰好是主节点。
- en: 'Now, list the containers on that node:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，列出该节点上的容器：
- en: '![Figure 15.10 – Displaying the health status of a running task instance](img/Image98410.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.10 – 显示正在运行的任务实例的健康状态](img/Image98410.jpg)'
- en: Figure 15.10 – Displaying the health status of a running task instance
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10 – 显示正在运行的任务实例的健康状态
- en: The interesting thing in this screenshot is the **STATUS** column. Docker, or
    more precisely, SwarmKit, has recognized that the service has a health check function
    defined and is using it to determine the health of each task of the service.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个截图中有趣的地方是**状态（STATUS）**列。Docker，或者更准确地说，SwarmKit，已经识别到该服务定义了健康检查功能，并正在使用它来确定服务中每个任务的健康状况。
- en: Next, let’s see what happens if something goes wrong.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如果发生问题会怎么样。
- en: Rolling back
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回滚
- en: Sometimes, things don’t go as expected. A last-minute fix to an application
    release may have inadvertently introduced a new bug, or the new version significantly
    may have significantly decreased the throughput of the component, and so on. In
    such cases, we need to have a plan B, which in most cases means the ability to
    roll back the update to the previous good version.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，事情并不会按预期进行。一个临时修复可能无意中引入了一个新 bug，或者新版本可能显著降低了组件的吞吐量，等等。在这种情况下，我们需要有一个计划 B，这通常意味着能够将更新回滚到先前的良好版本。
- en: As with the update, the rollback has to happen so that it does not cause any
    outages in terms of the application; it needs to cause zero downtime. In that
    sense, a rollback can be looked at as a reverse update. We are installing a new
    version, yet this new version is actually the previous version.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新一样，回滚必须发生，以确保应用程序没有中断；它需要实现零停机时间。从这个意义上说，回滚可以看作是一个反向更新。我们正在安装一个新版本，但这个新版本实际上是之前的版本。
- en: 'As with the update behavior, we can declare, either in our `stack` files or
    in the Docker `service create` command, how the system should behave in case it
    needs to execute a rollback. Here, we have the `stack` file that we used previously,
    but this time with some rollback-relevant attributes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新行为一样，我们可以在`stack`文件中或者在 Docker `service create`命令中声明，系统在需要执行回滚时应该如何行为。在这里，我们使用了之前的`stack`文件，不过这次添加了一些与回滚相关的属性：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can create a stack file named `stack-rollback.yaml`, and add the preceding
    content to it. In this content, we define the details of the rolling update, the
    health checks, and the behavior during rollback. The health check is defined so
    that after an initial wait time of 2 seconds, the orchestrator starts to poll
    the service on `http://localhost` every 2 seconds and retries 3 times before it
    considers a task unhealthy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个名为`stack-rollback.yaml`的堆栈文件，并将前面的内容添加到其中。在这个内容中，我们定义了滚动更新的细节、健康检查和回滚时的行为。健康检查被定义为在初始等待时间
    2 秒后，调度器开始每 2 秒轮询一次`http://localhost`上的服务，并在认为任务不健康之前重试 3 次。
- en: If we do the math, then it takes at least 8 seconds until a task will be stopped
    if it is unhealthy due to a bug. So, now under `deploy`, we have a new entry called
    `monitor`. This entry defines how long newly deployed tasks should be monitored
    for health and whether or not to continue with the next batch in the rolling update.
    Here, in this sample, we have given it 10 seconds. This is slightly more than
    the 8 seconds we calculated it takes to discover that a defective service has
    been deployed, so this is good.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们算一下时间，那么至少需要 8 秒钟，如果任务因为 bug 而不健康，它才会被停止。所以，在`deploy`下，我们有一个新的条目叫做`monitor`。该条目定义了新部署的任务应该监控健康状况多长时间，以及是否继续进行滚动更新的下一个批次。在这个示例中，我们设定了
    10 秒钟。这比我们计算出来的发现一个缺陷服务已被部署需要的 8 秒钟稍多一些，所以这是合适的。
- en: We also have a new entry, `failure_action`, which defines what the orchestrator
    will do if it encounters a failure during the rolling update, such as the service
    being unhealthy. By default, the action is just to stop the whole update process
    and leave the system in an intermediate state. The system is not down since it
    is a rolling update and at least some healthy instances of the service are still
    operational, but an operations engineer would be better at taking a look and fixing
    the problem.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个新的条目，`failure_action`，它定义了在滚动更新过程中遇到故障（例如服务不健康）时编排器将采取的措施。默认情况下，该操作是停止整个更新过程，并将系统置于中间状态。系统并不会完全宕机，因为这是滚动更新，至少一些健康的实例仍然在运行，但运维工程师会更适合检查并修复问题。
- en: In our case, we have defined the action to be a rollback. Thus, in case of failure,
    SwarmKit will automatically revert all tasks that have been updated back to their
    previous version.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们已将该操作定义为回滚。因此，如果发生故障，SwarmKit将自动将所有已更新的任务恢复到先前的版本。
- en: Blue-green deployments
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: In [*Chapter 9*](B19199_09.xhtml#_idTextAnchor194), *Learning about* *Distributed
    Application Architecture*, we discussed what blue-green deployments are, in an
    abstract way. It turns out that, on Docker Swarm, we cannot really implement blue-green
    deployments for arbitrary services. The service discovery and load balancing between
    two services running in Docker Swarm are part of the Swarm routing mesh and cannot
    be (easily) customized.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第9章*](B19199_09.xhtml#_idTextAnchor194)，*学习分布式应用架构*中，我们以抽象的方式讨论了蓝绿部署是什么。结果发现，在Docker
    Swarm中，我们无法真正为任意服务实现蓝绿部署。Docker Swarm中两项服务之间的服务发现和负载均衡是Swarm路由网格的一部分，无法（轻松）自定义。
- en: 'If **Service A** wants to call **Service B**, then Docker does this implicitly.
    Docker, given the name of the target service, will use the Docker DNS service
    to resolve this name to a VIP address. When the request is then targeted at the
    VIP, the Linux IPVS service will do another lookup in the Linux kernel IP tables
    with the VIP and load-balance the request to one of the physical IP addresses
    of the tasks of the service represented by the VIP, as shown in the following
    diagram:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果**服务A**想调用**服务B**，Docker会隐式地完成这一操作。Docker会根据目标服务的名称，使用Docker DNS服务将该名称解析为VIP地址。当请求定向到VIP时，Linux
    IPVS服务将再次在Linux内核IP表中查找VIP，并将请求负载均衡到VIP所表示的服务任务的物理IP地址之一，如下图所示：
- en: '![Figure 15.11 – How service discovery and load balancing work in Docker Swarm](img/Image98420.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图15.11 – Docker Swarm中服务发现和负载均衡是如何工作的](img/Image98420.jpg)'
- en: Figure 15.11 – How service discovery and load balancing work in Docker Swarm
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11 – Docker Swarm中服务发现和负载均衡是如何工作的
- en: Unfortunately, there is no easy way to intercept this mechanism and replace
    it with custom behavior, but this would be needed to allow for a true blue-green
    deployment of **Service B**, which is the target service in our example. As we
    will see in [*Chapter 17*](B19199_17.xhtml#_idTextAnchor374), *Deploying, Updating,
    and Securing an Application with Kubernetes*, Kubernetes is more flexible in this
    area.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有简单的方法来拦截这个机制并用自定义行为替换它，但这是实现**服务B**的真正蓝绿部署所需要的，就如我们在[*第17章*](B19199_17.xhtml#_idTextAnchor374)中看到的那样，*使用Kubernetes部署、更新和保护应用程序*，Kubernetes在这方面更为灵活。
- en: That being said, we can always deploy the public-facing services in a blue-green
    fashion. We can use the **interlock 2** product and its layer-7 routing mechanism
    to allow for a true blue-green deployment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们始终可以以蓝绿方式部署面向公众的服务。我们可以使用**interlock 2**产品及其7层路由机制来实现真正的蓝绿部署。
- en: Canary releases
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Technically speaking, rolling updates are a kind of canary release, but due
    to their lack of seams, where you can plug customized logic into the system, rolling
    updates are only a very limited version of canary releases.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，滚动更新是一种金丝雀发布（canary release），但由于它们缺乏插入自定义逻辑的接缝，滚动更新仅仅是金丝雀发布的一个非常有限的版本。
- en: True canary releases require us to have more fine-grained control over the update
    process. Also, true canary releases do not take down the old version of the service
    until 100% of the traffic has been funneled through the new version. In that regard,
    they are treated like blue-green deployments.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的金丝雀发布要求我们对更新过程有更细粒度的控制。此外，真正的金丝雀发布在100%的流量都已经通过新版本时，才会停止使用旧版本的服务。从这个角度来看，它们像蓝绿部署（blue-green
    deployments）一样被处理。
- en: In a canary release scenario, we don’t just want to use things such as health
    checks as deciding factors regarding whether or not to funnel more and more traffic
    through the new version of the service; we also want to consider external input
    in the decision-making process, such as metrics that are collected and aggregated
    by a log aggregator or tracing information. An example that could be used as a
    decision-maker includes conformance to **Service Level Agreements** (**SLAs**),
    namely whether the new version of the service shows response times that are outside
    of the tolerance band. This can happen if we add new functionality to an existing
    service yet this new functionality degrades the response time.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀发布场景中，我们不仅希望使用健康检查等因素作为决定是否将更多流量导向新版本服务的决定因素；我们还希望在决策过程中考虑外部输入，如通过日志聚合器收集和汇总的度量数据或追踪信息。作为决策依据的一个例子是是否符合**服务水平协议**（**SLA**），即新版本的服务响应时间是否超出了容忍带。如果我们向现有服务添加新功能，但这些新功能降低了响应时间，就可能发生这种情况。
- en: Now that we know how to deploy an application causing zero downtime, we want
    to discuss how we can store configuration data used by the applications in the
    swarm.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何实现零停机时间部署应用程序，接下来我们想讨论如何在 Swarm 中存储应用程序使用的配置数据。
- en: Storing configuration data in the swarm
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Swarm 中存储配置数据
- en: If we want to store non-sensitive data such as configuration files in Docker
    Swarm, then we can use Docker configs. Docker configs are very similar to Docker
    secrets, which we will discuss in the next section. The main difference is that
    config values are not encrypted at rest, while secrets are. Like Docker secrets,
    Docker configs can only be used in Docker Swarm – that is, they cannot be used
    in your non-Swarm development environment. Docker configs are mounted directly
    into the container’s filesystem. Configuration values can either be strings or
    binary values up to a size of 500 KB.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要在 Docker Swarm 中存储非敏感数据，比如配置文件，那么我们可以使用 Docker 配置。Docker 配置与 Docker 密钥非常相似，后者我们将在下一节讨论。主要的区别是配置值在静态时不会加密，而密钥会。像
    Docker 密钥一样，Docker 配置只能在 Docker Swarm 中使用——也就是说，它们不能在非 Swarm 开发环境中使用。Docker 配置会直接挂载到容器的文件系统中。配置值可以是字符串或二进制值，最大支持
    500 KB 大小。
- en: With the use of Docker configs, you can separate the configuration from Docker
    images and containers. This way, your services can easily be configured with environment-specific
    values. The production swarm environment has different configuration values from
    the staging swarm, which in turn has different config values from the development
    or integration environment.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker 配置，您可以将配置与 Docker 镜像和容器分离。这样，您的服务就可以轻松地使用特定环境的值进行配置。生产环境的 Swarm 配置与临时环境的
    Swarm 配置不同，临时环境的配置与开发或集成环境的配置也不同。
- en: We can add configs to services and also remove them from running services. Configs
    can even be shared among different services running in the swarm.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将配置添加到服务中，也可以从正在运行的服务中移除它们。配置甚至可以在 Swarm 中运行的不同服务之间共享。
- en: 'Now, let’s create some Docker configs:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一些 Docker 配置：
- en: 'First, we start with a simple string value:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从一个简单的字符串值开始：
- en: '[PRE14]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Please note the hyphen at the end of the `docker config create` command. This
    means that Docker expects the value of the config from standard input. This is
    exactly what we’re doing by piping the `Hello world` value into the `create` command.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 `docker config create` 命令末尾的连字符。这意味着 Docker 期望从标准输入获取配置的值。这正是我们通过将 `Hello
    world` 值通过管道传递给 `create` 命令所做的。
- en: 'The preceding command results in an output like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出结果如下所示：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding command creates a config named `hello-config` with the value “`Hello
    world`.” The output of this command is the unique ID of this new config that’s
    being stored in the swarm.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令创建了一个名为 `hello-config` 的配置，值为“`Hello world`”。该命令的输出是此新配置在 Swarm 中存储的唯一 ID。
- en: 'Let’s see what we got and use the `list` command to do so:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看结果，并使用 `list` 命令查看：
- en: '[PRE16]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will output the following (which has been shortened):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容（已被缩短）：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output of the `list` command shows the `ID` and `NAME` information for the
    config we just created, as well as its `CREATED` and (last) updated time. However,
    configs are non-confidential.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`list` 命令的输出将显示我们刚创建的配置的 `ID` 和 `NAME` 信息，以及其 `CREATED` 和（最后）更新时间。然而，配置是非机密的。'
- en: 'For that reason, we can do more and even output the content of a config, like
    so:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们可以做更多的操作，甚至输出配置的内容，像这样：
- en: '[PRE18]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output looks like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Hmmm, interesting. In the `Spec` subnode of the preceding JSON-formatted output,
    we have the `Data` key with a value of `SGVsbG8gd29ybGQK`. Didn’t we just say
    that the config data is not encrypted at rest?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，有趣。在上述 JSON 格式的输出的`Spec`子节点中，我们有一个`Data`键，其值为`SGVsbG8gd29ybGQK`。我们刚刚说配置数据并没有在静止时加密？
- en: 'It turns out that the value is just our string encoded as `base64`, as we can
    easily verify:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果证明，该值只是我们的字符串编码为`base64`，我们可以轻松验证：
- en: '[PRE20]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So far, so good.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很顺利。
- en: 'Now, let’s define a somewhat more complicated Docker config. Let’s assume we
    are developing a Java application. Java’s preferred way of passing configuration
    data to the application is the use of so-called `properties` files. A `properties`
    file is just a text file containing a list of key-value pairs. Let’s take a look:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个稍微复杂一些的 Docker 配置。假设我们正在开发一个 Java 应用程序。Java 传递配置数据到应用程序的首选方式是使用所谓的`properties`文件。`properties`文件只是一个包含键值对列表的文本文件。让我们看一下：
- en: 'Let’s create a file called `my-app.properties` and add the following content
    to it:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`my-app.properties`的文件，并添加以下内容：
- en: '[PRE22]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Save the file and create a Docker config called `app.properties` from it:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存文件并从中创建名为`app.properties`的 Docker 配置：
- en: '[PRE23]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This gives us an output like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个类似于这样的输出：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To prepare the next command, first, install the `jq` tool:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为准备下一个命令，首先安装`jq`工具：
- en: '[PRE25]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can use this (somewhat contrived) command to get the cleartext value
    of the config we just created:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个（有点牵强的）命令来获取我们刚刚创建的配置的明文值：
- en: '[PRE26]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We get this output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这个输出：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is exactly what we expected.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们预期的。
- en: 'Now, let’s create a Docker service that uses the preceding config. For simplicity,
    we will be using the `nginx` image to do so:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个使用上述配置的 Docker 服务。为了简单起见，我们将使用`nginx`镜像来实现：
- en: '[PRE28]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in an output similar to the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致类似以下的输出：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The interesting part in the preceding `service create` command is the line that
    contains the `--config` parameter. With this line, we’re telling Docker to use
    the config named `app.properties` and mount it as a file at `/etc/myapp/conf/app.properties`
    inside the container. Furthermore, we want that file to have `mode 0440` assigned
    to it to give the owner (root) and the group read permission.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述的`service create`命令中，有趣的部分是包含`--config`参数的那一行。通过这一行，我们告诉 Docker 使用名为`app.properties`的配置，并将其挂载为一个文件到容器内的`/etc/myapp/conf/app.properties`。此外，我们希望该文件被赋予`0440`的权限模式，以赋予所有者（root）和组读取权限。
- en: 'Let’s see what we got:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the preceding output, we can see that the only instance of the service is
    running on node `ip-172-31-32-21`. On this node, I can now list the containers
    to get the ID of the `nginx` instance:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们可以看到唯一一个服务实例正在节点`ip-172-31-32-21`上运行。在这个节点上，我现在可以列出容器以获取`nginx`实例的ID：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we can `exec` into that container and output the value of the `/``etc/myapp/conf/app.properties`
    file:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以`exec`进入该容器，并输出位于`/etc/myapp/conf/app.properties`文件中的值：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that `44417` in the preceding command represents the first part of the
    container hash.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述命令中，`44417`代表容器哈希的第一部分。
- en: 'This then will give us the expected values:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这将给我们预期的值：
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: No surprise here; this is exactly what we expected.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点毫不奇怪；这正是我们预期的。
- en: 'Docker configs can, of course, also be removed from the swarm, but only if
    they are not being used. If we try to remove the config we were just using previously,
    without first stopping and removing the service, we would get the following output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 配置当然也可以从集群中移除，但前提是它们没有被使用。如果我们尝试移除之前正在使用的配置，而没有先停止并移除服务，我们将会得到以下输出：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Oh no, that did not work, as we can see from the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 哦不，这不起作用，我们可以从以下输出看到：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We get an error message in which Docker is nice enough to tell us that the config
    is being used by our service called nginx. This behavior is somewhat similar to
    what we are used to when working with Docker volumes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个错误消息，Docker 告诉我们该配置正在被我们称为 nginx 的服务使用。这种行为在使用 Docker 卷时我们已经习惯了。
- en: 'Thus, first, we need to remove the service and then we can remove the config:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先我们需要移除服务，然后才能移除配置：
- en: '[PRE36]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And now it should work:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该可以工作了：
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: It is important to note once more that Docker configs should never be used to
    store confidential data such as secrets, passwords, or access keys and key secrets.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，非常重要的一点是 Docker 配置不应该用于存储诸如密码、访问密钥或关键秘密等机密数据。
- en: In the next section, we will discuss how to handle confidential data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何处理机密数据。
- en: Protecting sensitive data with Docker secrets
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker 秘密保护敏感数据
- en: Secrets are used to work with confidential data in a secure way. Swarm secrets
    are secure at rest and in transit. That is, when a new secret is created on a
    manager node, and it can only be created on a manager node, its value is encrypted
    and stored in the raft consensus storage. This is why it is secure at rest. If
    a service gets a secret assigned to it, then the manager reads the secret from
    storage, decrypts it, and forwards it to all the containers that are instances
    of the swarm service that requested the secret. Since node-to-node communication
    in Docker Swarm uses `tmpFS` into the container. By default, secrets are mounted
    into the container at `/run/secrets`, but you can change that to any custom folder.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密用于以安全的方式处理机密数据。Swarm 的秘密在静止时和传输中都是安全的。也就是说，当在管理节点上创建一个新秘密时，并且它只能在管理节点上创建，其值会被加密并存储在
    raft 一致性存储中。这就是为什么它在静止时是安全的原因。如果一个服务被分配了秘密，那么管理节点会从存储中读取该秘密，解密后将其转发给所有请求该秘密的 swarm
    服务实例的容器。由于 Docker Swarm 中的节点间通信使用 `tmpFS` 将数据传递到容器内。默认情况下，秘密会挂载到容器中的 `/run/secrets`
    目录，但你可以将其更改为任何自定义文件夹。
- en: It is important to note that secrets will not be encrypted on Windows nodes
    since there is no concept similar to `tmpfs`. To achieve the same level of security
    that you would get on a Linux node, the administrator should encrypt the disk
    of the respective Windows node.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，秘密在 Windows 节点上不会被加密，因为 Windows 上没有类似 `tmpfs` 的概念。为了实现与 Linux 节点相同的安全级别，管理员应该加密相应
    Windows 节点的磁盘。
- en: Creating secrets
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建秘密
- en: 'First, let’s see how we can actually create a secret:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何实际创建一个秘密：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This command creates a secret called `sample-secret` with a value of `sample
    secret value`. Please note the hyphen at the end of the `docker secret create`
    command. This means that Docker expects the value of the secret from standard
    input. This is exactly what we’re doing by piping `sample secret value` into the
    `create` command.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令创建了一个名为 `sample-secret` 的秘密，其值为 `sample secret value`。请注意，`docker secret
    create` 命令末尾有一个连字符。这意味着 Docker 期望从标准输入中获取秘密的值。这正是我们通过将 `sample secret value` 管道传递给
    `create` 命令所做的。
- en: 'Alternatively, we can use a file as the source for the secret value:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用文件作为秘密值的来源：
- en: 'Create a `secret-value.txt` file as follows:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `secret-value.txt` 文件，如下所示：
- en: '[PRE39]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create the Docker secret from this file with the following:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从这个文件创建 Docker 秘密：
- en: '[PRE40]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, the value of the secret with the name `other-secret` is read from a file
    called `./secret-value.txt`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，名为 `other-secret` 的秘密值是从名为 `./secret-value.txt` 的文件中读取的。
- en: 'Once a secret has been created, there is no way to access the value of it.
    We can, for example, list all our secrets to get the following output:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦秘密被创建，就无法访问其值。例如，我们可以列出所有的秘密，得到以下输出：
- en: '![Figure 15.12 – List of all secrets](img/Image98428.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.12 – 所有秘密的列表](img/Image98428.jpg)'
- en: Figure 15.12 – List of all secrets
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12 – 所有秘密的列表
- en: In this list, we can only see the `ID` and `NAME` info for the secret, plus
    some other metadata, but the actual value of the secret is not visible.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，我们只能看到秘密的 `ID` 和 `NAME` 信息，以及一些其他元数据，但秘密的实际值是不可见的。
- en: 'We can also use `inspect` on a secret, for example, to get more information
    about `other-secret`:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用 `inspect` 命令来查看一个秘密的更多信息，例如查看 `other-secret`：
- en: '![Figure 15.13 – Inspecting a swarm secret](img/Image98437.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.13 – 检查 swarm 秘密](img/Image98437.jpg)'
- en: Figure 15.13 – Inspecting a swarm secret
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.13 – 检查 swarm 秘密
- en: 'Even here, we do not get the value of the secret back. This is, of course,
    intentional: a secret is a secret and thus needs to remain confidential. We can
    assign labels to secrets if we want and we can even use a different driver to
    encrypt and decrypt the secret if we’re not happy with what Docker delivers out
    of the box.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这里，我们也无法得到秘密的值。这显然是故意的：秘密就是秘密，因此必须保持机密。如果我们愿意，我们可以为秘密分配标签，甚至可以使用不同的驱动程序来加密和解密秘密，如果我们不满意
    Docker 默认提供的加密方法。
- en: Using a secret
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用秘密
- en: 'Secrets are used by services that run in the swarm. Usually, secrets are assigned
    to a service at creation time. Thus, if we want to run a service called `web`
    and assign it a secret, say, `api-secret-key`, the syntax would look as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密由在 swarm 中运行的服务使用。通常，秘密在创建服务时分配给该服务。因此，如果我们想运行一个名为 `web` 的服务并为其分配一个秘密，比如 `api-secret-key`，语法如下所示：
- en: '[PRE41]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This command creates a service called `web` based on the `fundamentalsofdocker/whoami:latest`
    image, publishes the container port `8000` to port `8000` on all swarm nodes,
    and assigns it the secret called `api-secret-key`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令基于`fundamentalsofdocker/whoami:latest`镜像创建一个名为`web`的服务，将容器端口`8000`发布到所有集群节点的`8000`端口，并分配给它名为`api-secret-key`的机密。
- en: 'This will only work if the secret called `api-secret-key` is defined in the
    swarm; otherwise, an error will be generated with the following text:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这只有在`api-secret-key`机密已在集群中定义的情况下才有效；否则，将生成以下错误信息：
- en: '[PRE42]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Thus, let’s create this secret now:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们现在创建这个机密：
- en: '[PRE43]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now, if we rerun the `service create` command, it will succeed.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们重新运行`service create`命令，它将成功执行。
- en: Now, we can use `docker service ps web` to find out on which node the sole service
    instance has been deployed, and then `exec` into this container. In my case, the
    instance has been deployed to node `ip-172-31-32-21`, which coincidentally happens
    to be the `manager1` EC2 instance on which I am already working. Otherwise, I
    would have to SSH into the other node first.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`docker service ps web`查找唯一的服务实例部署在哪个节点上，然后`exec`进入这个容器。在我的情况下，实例已部署到`ip-172-31-32-21`节点，这恰好是我正在使用的`manager1`
    EC2实例。否则，我需要先SSH到另一个节点。
- en: 'Then, I list all my containers on that node with `docker container ls` to find
    the one instance belonging to my service and copy its container ID. We can then
    run the following command to make sure that the secret is indeed available inside
    the container under the expected filename containing the secret value in cleartext:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我使用`docker container ls`列出该节点上的所有容器，找到属于我的服务的实例并复制其容器ID。接下来，我们可以运行以下命令，确保机密确实在容器内以包含机密值的预期文件名可用：
- en: '[PRE44]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Once again, in my case, the output generated is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，在我的情况下，生成的输出如下所示：
- en: '[PRE45]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This is evidently what we expected. We can see the secret in cleartext.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是我们预期的结果。我们可以看到机密以明文形式显示。
- en: 'If, for some reason, the default location where Docker mounts the secrets inside
    the container is not acceptable to you, you can define a custom location. In the
    following command, we mount the secret to `/app/my-secrets`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某种原因，Docker在容器内挂载机密的默认位置不符合你的要求，你可以定义一个自定义位置。在以下命令中，我们将机密挂载到`/app/my-secrets`：
- en: '[PRE46]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In this command, we are using the extended syntax to define a secret that includes
    the destination folder.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，我们使用了扩展语法来定义一个包括目标文件夹的机密。
- en: Simulating secrets in a development environment
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在开发环境中模拟机密
- en: When working in development, we usually don’t have a local swarm on our machine.
    However, secrets only work in a swarm. So, what can we do? Well, luckily, this
    answer is really simple.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，我们通常在机器上没有本地集群。然而，机密仅在集群中有效。那么，我们该怎么办呢？幸运的是，这个答案非常简单。
- en: Since secrets are treated as files, we can easily mount a volume that contains
    the secrets into the container to the expected location, which by default is at
    `/run/secrets`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机密被当作文件处理，我们可以轻松地将包含机密的卷挂载到容器的预期位置，默认情况下，该位置是`/run/secrets`。
- en: 'Let’s assume that we have a folder called `./dev-secrets` on our local workstation.
    For each secret, we have a file named the same as the secret name and with the
    unencrypted value of the secret as the content of the file. For example, we can
    simulate a secret called `demo-secret` with a secret value of `demo secret value`
    by executing the following command on our workstation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在本地工作站上有一个名为`./dev-secrets`的文件夹。对于每个机密，我们都有一个与机密名称相同的文件，文件内容是该机密的未加密值。例如，我们可以通过在工作站上执行以下命令，模拟一个名为`demo-secret`、值为`demo
    secret value`的机密：
- en: '[PRE47]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we can create a container that mounts this folder, like this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个挂载此文件夹的容器，如下所示：
- en: '[PRE48]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The process running inside the container will be unable to distinguish these
    mounted files from the ones originating from a secret. So, for example, `demo-secret`
    is available as a file called `/run/secrets/demo-secret` inside the container
    and has the expected value, `demo secret value`. Let’s take a look at this in
    more detail in the following steps:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内运行的进程将无法区分这些挂载的文件和来源于机密的文件。因此，例如，`demo-secret`作为文件`/run/secrets/demo-secret`出现在容器内，并具有预期的值`demo
    secret value`。让我们在接下来的步骤中更详细地了解这一点：
- en: 'To test this, we can `exec` a shell inside the preceding container:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试这一点，我们可以在前面的容器中`exec`一个shell：
- en: '[PRE49]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we can navigate to the `/run/secrets` folder and display the content of
    the `demo-secret` file:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以导航到`/run/secrets`文件夹并显示`demo-secret`文件的内容：
- en: '[PRE50]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Next, we will look at secrets and legacy applications.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论秘密和遗留应用程序。
- en: Secrets and legacy applications
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 秘密和遗留应用程序
- en: Sometimes, we want to containerize a legacy application that we cannot easily,
    or do not want to, change. This legacy application might expect a secret value
    to be available as an environment variable. How are we going to deal with this
    now? Docker presents us with the secrets as files, but the application is expecting
    them in the form of environment variables.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们想要容器化一个我们无法轻易更改，或者不想更改的遗留应用程序。这个遗留应用程序可能希望一个秘密值作为环境变量可用。那么现在我们该怎么处理呢？Docker将秘密呈现为文件，但该应用程序期望它们以环境变量的形式存在。
- en: 'In this situation, it is helpful to define a script that runs when the container
    is started (a so-called entry point or startup script). This script will read
    the secret value from the respective file and define an environment variable with
    the same name as the file, assigning the new variable the value read from the
    file. In the case of a secret called `demo-secret` whose value should be available
    in an environment variable called `DEMO_SECRET`, the necessary code snippet in
    this startup script could look like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，定义一个在容器启动时运行的脚本（所谓的入口点或启动脚本）是非常有帮助的。这个脚本将从相应的文件中读取秘密值，并定义一个与文件同名的环境变量，将读取到的值分配给该变量。对于名为`demo-secret`的秘密，其值应作为名为`DEMO_SECRET`的环境变量可用，启动脚本中的必要代码片段可以如下所示：
- en: '[PRE51]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Similarly, let’s say we have a legacy application that expects the secret values
    to be present as an entry in, say, a YAML configuration file located in the `/app/bin`
    folder and called `app.config`, whose relevant part looks like this:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设我们有一个遗留应用程序，期望秘密值作为条目存在于位于`/app/bin`文件夹中的YAML配置文件中，名为`app.config`，其相关部分如下所示：
- en: '[PRE52]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Our initialization script now needs to read the secret value from the secret
    file and replace the corresponding placeholder in the config file with the secret
    value. For `demo_secret`, this could look like this:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的初始化脚本需要从秘密文件中读取秘密值，并将配置文件中相应的占位符替换为秘密值。对于`demo_secret`，可以如下所示：
- en: '[PRE53]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In the preceding snippet, we’re using the `sed` tool to replace a placeholder
    with a value in place. We can use the same technique for the other two secrets
    in the config file.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用`sed`工具来替换占位符，将其替换为实际值。我们可以对配置文件中的另外两个秘密使用相同的技巧。
- en: We put all the initialization logic into a file called `entrypoint.sh`, make
    this file executable and, for example, add it to the root of the container’s filesystem.
    Then, we define this file as `ENTRYPOINT` in the Dockerfile, or we can override
    the existing `ENTRYPOINT` of an image in the `docker container` `run` command.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有初始化逻辑放入一个名为`entrypoint.sh`的文件中，使其可执行，并且例如将其添加到容器文件系统的根目录中。然后，我们在Dockerfile中将此文件定义为`ENTRYPOINT`，或者我们也可以在`docker
    container`的`run`命令中覆盖镜像的现有`ENTRYPOINT`。
- en: Let’s make a sample. Let’s assume that we have a legacy application running
    inside a container defined by the `fundamentalsofdocker/whoami:latest` image that
    expects a secret called `db_password` to be defined in a file, `whoami.conf`,
    in the application folder.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做个示例。假设我们有一个在容器中运行的遗留应用程序，该容器由`fundamentalsofdocker/whoami:latest`镜像定义，并且该应用程序需要在名为`whoami.conf`的文件中定义一个名为`db_password`的秘密。
- en: 'Let’s take a look at these steps:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些步骤：
- en: 'We can define a file, `whoami.conf`, on our local machine that contains the
    following content:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在本地机器上定义一个名为`whoami.conf`的文件，其中包含以下内容：
- en: '[PRE54]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The important part is line 3 of this snippet. It defines where the secret value
    has to be put by the startup script.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是这个代码片段的第3行。它定义了启动脚本必须将秘密值放置的位置。
- en: 'Let’s add a file called `entrypoint.sh` to the local folder that contains the
    following content:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们向本地文件夹添加一个名为`entrypoint.sh`的文件，并包含以下内容：
- en: '[PRE55]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The last line in the preceding script stems from the fact that this is the start
    command that was used in the original Dockerfile.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 前面脚本的最后一行源自原始Dockerfile中使用的启动命令。
- en: 'Now, change the mode of this file to an executable:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，改变该文件的权限，使其可执行：
- en: '[PRE56]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, we define a Dockerfile that inherits from the `fundamentalsofdocker/whoami:latest`
    image. Add a file called `Dockerfile` to the current folder that contains the
    following content:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个从`fundamentalsofdocker/whoami:latest`镜像继承的Dockerfile。向当前文件夹添加一个名为`Dockerfile`的文件，其中包含以下内容：
- en: '[PRE57]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Let’s build the image from this Dockerfile:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从这个 Dockerfile 中构建镜像：
- en: '[PRE58]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Once the image has been built, we can run a service from it, but before we
    can do that, we need to define the secret in Swarm:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦镜像构建完成，我们可以从中运行服务，但在此之前，我们需要在 Swarm 中定义密钥：
- en: '[PRE59]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, we can create a service that uses the following secret:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个使用以下密钥的服务：
- en: '[PRE60]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Updating secrets
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新密钥
- en: At times, we need to update a secret in a running service since secrets could
    be leaked out to the public or be stolen by malicious people, such as hackers.
    In this case, we need to change our confidential data since the moment it is leaked
    to a non-trusted entity, it has to be considered insecure.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要更新正在运行的服务中的密钥，因为密钥可能会被泄露或被恶意人员（如黑客）窃取。在这种情况下，一旦我们的机密数据被泄露给不可信的实体，它就必须被视为不安全的，因此我们需要更改它。
- en: Updating secrets, like any other update, requires zero downtime. Docker SwarmKit
    supports us in this regard.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 更新密钥与任何其他更新一样，都需要实现零停机时间。Docker SwarmKit 在这方面提供了支持。
- en: 'First, we create a new secret in the swarm. It is recommended to use a versioning
    strategy when doing so. In our example, we use a version as a postfix of the secret
    name. We originally started with the secret named `db-password` and now the new
    version of this secret is called `db-password-v2`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在 swarm 中创建一个新的密钥。建议在创建时使用版本控制策略。在我们的示例中，我们将版本作为密钥名称的后缀。我们最初使用名为 `db-password`
    的密钥，现在这个密钥的新版本叫做 `db-password-v2`：
- en: '[PRE61]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let’s assume that the original service that used the secret had been created
    like this:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始服务使用密钥时是这样创建的：
- en: '[PRE62]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The application running inside the container was able to access the secret
    at `/run/secrets/db-password`. Now, SwarmKit does not allow us to update an existing
    secret in a running service, so we have to remove the now obsolete version of
    the secret and then add the new one. Let’s start with removal with the following
    command:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内运行的应用程序可以访问位于 `/run/secrets/db-password` 的密钥。现在，SwarmKit 不允许我们在运行的服务中更新现有密钥，因此我们必须先删除现有的过时版本密钥，然后再添加新的版本。让我们使用以下命令开始删除：
- en: '[PRE63]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, we can add the new secret with the following command:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令添加新密钥：
- en: '[PRE64]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Please note the extended syntax of `--secret-add` with the source and target
    parameters.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 `--secret-add` 命令的扩展语法，其中包含源和目标参数。
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the routing mesh, which provides layer-4 routing
    and load balancing to a Docker Swarm. We then learned how SwarmKit allows us to
    update services without requiring downtime. Furthermore, we discussed the current
    limits of SwarmKit in regard to zero-downtime deployments. Then, we showed how
    to store configuration data in the Swarm, and in the last part of this chapter,
    we introduced secrets as a means to provide confidential data to services in a
    highly secure way.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了路由网格，它为 Docker Swarm 提供了第4层路由和负载均衡。然后，我们学习了 SwarmKit 如何在不需要停机的情况下更新服务。此外，我们还讨论了
    SwarmKit 在零停机部署方面的当前限制。接着，我们展示了如何在 Swarm 中存储配置数据，在本章的最后部分，我们介绍了使用密钥作为提供机密数据给服务的安全方式。
- en: In the next chapter, we will introduce the currently most popular container
    orchestrator, Kubernetes. We’ll discuss the objects that are used to define and
    run a distributed, resilient, robust, and highly available application in a Kubernetes
    cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that’s
    used to locally deploy a Kubernetes application, and also demonstrate the integration
    of Kubernetes with Docker Desktop.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍目前最流行的容器编排工具——Kubernetes。我们将讨论在 Kubernetes 集群中定义和运行分布式、具有韧性、稳健性和高可用性的应用程序所使用的对象。此外，本章还将帮助我们了解
    MiniKube，这是一个用于在本地部署 Kubernetes 应用程序的工具，并展示 Kubernetes 与 Docker Desktop 的集成。
- en: Questions
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'To assess your learning progress, please try to answer the following questions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估你的学习进度，请尝试回答以下问题：
- en: In a few simple sentences, explain to an interested lay person what zero-downtime
    deployment means.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用简洁的几句话向感兴趣的外行解释什么是零停机部署。
- en: How does SwarmKit achieve zero-downtime deployments?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SwarmKit 如何实现零停机部署？
- en: Contrary to traditional (non-containerized) systems, why does a rollback in
    Docker Swarm just work? Explain this in a few short sentences.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与传统（非容器化）系统不同，为什么在 Docker Swarm 中回滚操作能够顺利进行？用简短的几句话解释一下。
- en: Describe two to three characteristics of a Docker secret.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述 Docker 密钥的两到三项特性。
- en: 'You need to roll out a new version of the inventory service. What does your
    command look like? Here is some more information:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要推出一个新的库存服务版本。你的命令会是什么样子？以下是一些额外的信息：
- en: The new image is called `acme/inventory:2.1`
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的镜像名为`acme/inventory:2.1`
- en: We want to use a rolling update strategy with a batch size of two tasks
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用滚动更新策略，批次大小为两个任务
- en: We want the system to wait for one minute after each batch
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望系统在每批次之后等待一分钟
- en: You need to update an existing service named `inventory` with a new password
    that is provided through a Docker secret. The new secret is called `MYSQL_PASSWORD_V2`.
    The code in the service expects the secret to be called `MYSQL_PASSWORD`. What
    does the update command look like? (Note that we do not want the code of the service
    to be changed!)
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要通过Docker机密更新一个名为`inventory`的现有服务，新的密码通过Docker机密提供。新的机密名为`MYSQL_PASSWORD_V2`。服务中的代码期望机密名为`MYSQL_PASSWORD`。更新命令是什么样子的？（请注意，我们不希望更改服务代码！）
- en: Answers
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'Here are sample answers to the preceding questions:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面问题的示例答案：
- en: Zero-downtime deployment means that a new version of a service in a distributed
    application is updated to a new version without the application needing to stop
    working. Usually, with Docker SwarmKit or Kubernetes (as we will see), this is
    done in a rolling fashion. A service consists of multiple instances and those
    are updated in batches so that the majority of the instances are up and running
    at all times.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零停机部署意味着分布式应用中的一个服务的新版本可以更新到新的版本，而不需要应用停止工作。通常，使用Docker SwarmKit或Kubernetes（如我们将看到的那样）是以滚动方式完成的。一个服务由多个实例组成，这些实例按批次更新，以确保大多数实例始终处于运行状态。
- en: By default, Docker SwarmKit uses a rolling updated strategy to achieve zero-downtime
    deployments.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，Docker SwarmKit使用滚动更新策略来实现零停机部署。
- en: Containers are self-contained units of deployment. If a new version of a service
    is deployed and does not work as expected, we (or the system) only need to roll
    back to the previous version. The previous version of the service is also deployed
    in the form of self-contained containers. Conceptually, there is no difference
    between rolling forward (an update) or backward (a rollback). One version of a
    container is replaced by another one. The host itself is not affected by such
    changes in any way.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器是自包含的部署单元。如果部署了一个新的服务版本且未按预期工作，我们（或系统）只需回滚到之前的版本。之前的服务版本也是以自包含的容器形式进行部署的。从概念上讲，向前推进（更新）或向后回滚（撤销）没有区别。一个版本的容器被另一个版本替代。主机本身不会受到这种变化的影响。
- en: Docker secrets are encrypted at rest. They are only transferred to the services
    and containers that use the secrets. Secrets are transferred encrypted due to
    the fact that the communication between swarm nodes uses mTLS. Secrets are never
    physically stored on a worker node.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker机密在静态存储时是加密的。它们仅传输到使用这些机密的服务和容器。由于swarm节点之间的通信使用mTLS，因此机密是加密传输的。机密永远不会物理存储在工作节点上。
- en: 'The command to achieve this is as follows:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现这一目标的命令如下：
- en: '[PRE65]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'First, we need to remove the old secret from the service, and then we need
    to add the new version to it (directly updating a secret is not possible):'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要从服务中删除旧的机密，然后我们需要将新版本的机密添加进去（直接更新机密是不可能的）：
- en: '[PRE66]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Part 4:Docker, Kubernetes, and the Cloud
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分：Docker、Kubernetes和云
- en: This part introduces the currently most popular container orchestrator. It introduces
    the core Kubernetes objects that are used to define and run a distributed, resilient,
    robust, and highly available application in a cluster. Finally, it introduces
    minikube as a way to locally deploy a Kubernetes application and also covers the
    integration of Kubernetes with Docker for Mac and Docker Desktop.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分介绍了当前最流行的容器编排工具。它介绍了用于在集群中定义和运行分布式、弹性、可靠和高可用性应用的核心Kubernetes对象。最后，它介绍了minikube作为一种在本地部署Kubernetes应用的方式，并涵盖了Kubernetes与Mac的Docker和Docker
    Desktop的集成。
- en: '[*Chapter 16*](B19199_16.xhtml#_idTextAnchor349), *Introducing Kubernetes*'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第16章*](B19199_16.xhtml#_idTextAnchor349)，*介绍Kubernetes*'
- en: '[*Chapter 17*](B19199_17.xhtml#_idTextAnchor374), *Deploying, Updating, and
    Securing an Application with Kubernetes*'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第17章*](B19199_17.xhtml#_idTextAnchor374)，*使用Kubernetes部署、更新和保护应用*'
- en: '[*Chapter 18*](B19199_18.xhtml#_idTextAnchor396), *Running a Containerized
    Application in the Cloud*'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第18章*](B19199_18.xhtml#_idTextAnchor396)，*在云中运行容器化应用*'
- en: '[*Chapter 19*](B19199_19.xhtml#_idTextAnchor412), *Monitoring and Troubleshooting
    an Application Running in Production*'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第19章*](B19199_19.xhtml#_idTextAnchor412)，*在生产环境中监控与排查应用程序故障*'
