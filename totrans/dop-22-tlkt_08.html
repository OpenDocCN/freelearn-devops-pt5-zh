<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-scraping-metrics" class="calibre15">Scraping Metrics</h2>

<p class="calibre3">Prometheus is a pull-based system. It requires targets from which it will get metrics. They can be exposed from inside your services or as generic exporters acting as intermediaries between Prometheus and other services or systems.</p>

<p class="calibre3">Services can be instrumented to provide metrics using one of the <a href="https://prometheus.io/docs/instrumenting/clientlibs/">Client Libraries</a>. Many of the languages are supported. If no library is available for your programming language of choice, or if you don’t want to add one more dependency, there is always the option of implementing one of the <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">exposition formats</a>.</p>

<p class="calibre3">The alternative to instrumentation of our services is to use exporters. The <a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters and Integrations</a> page lists quite a few official and community maintained solutions.</p>

<p class="calibre3">Having both options in front of us, we should make a decision which type to use. Should we instrument our services or use exporters? The decision does not have to be binary. We can use both.</p>

<p class="calibre3">In some cases, we do not have a choice. With third-party software like, for instance, HAProxy, exporters might be the only option since it does not natively provide metrics in Prometheus format. On the other hand, if there is a very particular set of metrics that should be scraped from one of our services, instrumentation is the best option unless that service already exposes metrics in a different format.</p>

<p class="calibre3">More often than not, we do have a choice between using an exporter or instrumenting a service. In such a case, I have a strong preference towards exporters. Instrumentation, even though it is sometimes unavoidable, leads to undesirable coupling. Our services should do only what they are designed to do. If, for example, we have a service that acts as a shopping cart, adding instrumentation and, probably, dependency on Prometheus library introduces tight coupling. If we do that, our shopping cart is not focused on solving only that single business domain but has side functions as well. You might argue that adding instrumentation is not a significant effort. Still, keeping services focused exclusively on their business domain has multiple benefits, and we should avoid increasing their scope by adding any additional responsibilities. That is, as long as we can avoid doing that.</p>

<p class="calibre3">My advice is to always start with exporters, and instrument your services only if you require metrics that are not provided by one of the existing exporters. That way, your services will have clear responsibilities and be focused on a business domain while all infrastructure type of tasks will be delegated to vertical services like, in this case, exporters.</p>

<p class="calibre3">In this chapter, we’ll use exporters as the only mean of providing targets that will be utilized by Prometheus to scrape metrics. If you realize that you do have to instrument your services, please consult <a href="https://prometheus.io/docs/instrumenting/clientlibs/">Prometheus documentation</a> for more information.</p>

<p class="calibre3">Later on in the book, we might instrument our demo service if we do realize that it provides a substantial advantage.</p>

<p class="calibre3">Now that we had a brief overview of the different ways to expose metrics, we can proceed towards a hands-on exploration of the subject.</p>

<h3 id="leanpub-auto-creating-the-cluster-and-deploying-services" class="calibre20">Creating The Cluster And Deploying Services</h3>

<p class="calibre3">We’ll start by recreating the cluster and deploying the stacks that we used in the previous chapter.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/955690ba490ce4464fab11823eb61d97">04-exporters.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod +x scripts/dm-swarm-04.sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>./scripts/dm-swarm-04.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env swarm-1<code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">We executed the <code class="calibre19">dm-swarm-04.sh</code> script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks and deployed the stacks. Now we should wait a few moments until all the services in the <code class="calibre19">monitor</code> stack are up and running. Please use <code class="calibre19">docker stack ps monitor</code> command to confirm that the status of all the services in the stack is <em class="calibre21">Running</em>.</p>

<p class="calibre3">Finally, we’ll confirm that everything is deployed correctly by opening Prometheus in a browser.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor"</code>
</pre></div>

</figure>

<p class="calibre3">Now the state of our cluster is the same as it was at the end of the previous chapter and we can proceed towards deploying exporters.</p>

<h3 id="leanpub-auto-deploying-exporters" class="calibre20">Deploying Exporters</h3>

<p class="calibre3">Exporters provide data Prometheus can scrape and put into its database.</p>

<p class="calibre3">The stack we’ll deploy is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nn">version</code><code class="calibre19">:</code> <code class="s">"3"</code>
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="nn">services</code><code class="calibre19">:</code>
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>  <code class="s">ha</code><code class="o">-</code><code class="nn">proxy</code><code class="calibre19">:</code>
<code class="lineno"> 6 </code>    <code class="nn">image</code><code class="calibre19">:</code> <code class="s">quay</code><code class="calibre19">.</code><code class="s">io</code><code class="o">/</code><code class="s">prometheus</code><code class="o">/</code><code class="s">haproxy</code><code class="o">-</code><code class="nn">exporter</code><code class="calibre19">:</code><code class="err">$</code><code class="calibre19">{</code><code class="nv">HA_PROXY_TAG</code><code class="calibre19">:-</code><code class="s">latest</code><code class="calibre19">}</code>
<code class="lineno"> 7 </code>    <code class="nn">networks</code><code class="calibre19">:</code>
<code class="lineno"> 8 </code>      <code class="o">-</code> <code class="s">proxy</code>
<code class="lineno"> 9 </code>      <code class="o">-</code> <code class="s">monitor</code>
<code class="lineno">10 </code>    <code class="nn">deploy</code><code class="calibre19">:</code>
<code class="lineno">11 </code>      <code class="nn">labels</code><code class="calibre19">:</code>
<code class="lineno">12 </code>        <code class="o">-</code> <code class="s">com</code><code class="calibre19">.</code><code class="s">df</code><code class="calibre19">.</code><code class="s">notify</code><code class="o">=</code><code class="s">true</code>
<code class="lineno">13 </code>        <code class="o">-</code> <code class="s">com</code><code class="calibre19">.</code><code class="s">df</code><code class="calibre19">.</code><code class="s">scrapePort</code><code class="o">=</code><code class="o">9101</code>
<code class="lineno">14 </code>    <code class="nn">command</code><code class="calibre19">:</code> <code class="o">-</code><code class="s">haproxy</code><code class="calibre19">.</code><code class="s">scrape</code><code class="o">-</code><code class="s">uri=</code><code class="s">"http://admin:admin@proxy/admin?stats;csv"</code>
<code class="lineno">15 </code>
<code class="lineno">16 </code>  <code class="nn">cadvisor</code><code class="calibre19">:</code>
<code class="lineno">17 </code>    <code class="nn">image</code><code class="calibre19">:</code> <code class="s">google</code><code class="o">/</code><code class="nn">cadvisor</code><code class="calibre19">:</code><code class="err">$</code><code class="calibre19">{</code><code class="nv">CADVISOR_TAG</code><code class="calibre19">:-</code><code class="s">latest</code><code class="calibre19">}</code>
<code class="lineno">18 </code>    <code class="nn">networks</code><code class="calibre19">:</code>
<code class="lineno">19 </code>      <code class="o">-</code> <code class="s">monitor</code>
<code class="lineno">20 </code>    <code class="nn">volumes</code><code class="calibre19">:</code>
<code class="lineno">21 </code>      <code class="o">-</code> <code class="s">/:/rootfs</code>
<code class="lineno">22 </code>      <code class="o">-</code> <code class="o">/</code><code class="s">var</code><code class="o">/</code><code class="nn">run</code><code class="calibre19">:</code><code class="o">/</code><code class="s">var</code><code class="o">/</code><code class="s">run</code>
<code class="lineno">23 </code>      <code class="o">-</code> <code class="o">/</code><code class="nn">sys</code><code class="calibre19">:</code><code class="o">/</code><code class="s">sys</code>
<code class="lineno">24 </code>      <code class="o">-</code> <code class="o">/</code><code class="s">var</code><code class="o">/</code><code class="s">lib</code><code class="o">/</code><code class="nn">docker</code><code class="calibre19">:</code><code class="o">/</code><code class="s">var</code><code class="o">/</code><code class="s">lib</code><code class="o">/</code><code class="s">docker</code>
<code class="lineno">25 </code>    <code class="nn">deploy</code><code class="calibre19">:</code>
<code class="lineno">26 </code>      <code class="nn">mode</code><code class="calibre19">:</code> <code class="s">global</code>
<code class="lineno">27 </code>      <code class="nn">labels</code><code class="calibre19">:</code>
<code class="lineno">28 </code>        <code class="o">-</code> <code class="s">com</code><code class="calibre19">.</code><code class="s">df</code><code class="calibre19">.</code><code class="s">notify</code><code class="o">=</code><code class="s">true</code>
<code class="lineno">29 </code>        <code class="o">-</code> <code class="s">com</code><code class="calibre19">.</code><code class="s">df</code><code class="calibre19">.</code><code class="s">scrapePort</code><code class="o">=</code><code class="o">8080</code>
<code class="lineno">30 </code>
<code class="lineno">31 </code>  <code class="s">node</code><code class="o">-</code><code class="nn">exporter</code><code class="calibre19">:</code>
<code class="lineno">32 </code>    <code class="nn">image</code><code class="calibre19">:</code> <code class="s">basi</code><code class="o">/</code><code class="s">node</code><code class="o">-</code><code class="nn">exporter</code><code class="calibre19">:</code><code class="err">$</code><code class="calibre19">{</code><code class="nv">NODE_EXPORTER_TAG</code><code class="calibre19">:-</code><code class="s">v1</code><code class="o">.13.0</code><code class="calibre19">}</code>
<code class="lineno">33 </code>    <code class="nn">networks</code><code class="calibre19">:</code>
<code class="lineno">34 </code>      <code class="o">-</code> <code class="s">monitor</code>
<code class="lineno">35 </code>    <code class="nn">environment</code><code class="calibre19">:</code>
<code class="lineno">36 </code>      <code class="o">-</code> <code class="nv">HOST_HOSTNAME</code><code class="s">=/etc</code><code class="o">/</code><code class="s">host_hostname</code>
<code class="lineno">37 </code>    <code class="nn">volumes</code><code class="calibre19">:</code>
<code class="lineno">38 </code>      <code class="o">-</code> <code class="o">/</code><code class="nn">proc</code><code class="calibre19">:</code><code class="o">/</code><code class="s">host</code><code class="o">/</code><code class="s">proc</code>
<code class="lineno">39 </code>      <code class="o">-</code> <code class="o">/</code><code class="nn">sys</code><code class="calibre19">:</code><code class="o">/</code><code class="s">host</code><code class="o">/</code><code class="s">sys</code>
<code class="lineno">40 </code>      <code class="o">-</code> <code class="s">/:/rootfs</code>
<code class="lineno">41 </code>      <code class="o">-</code> <code class="o">/</code><code class="s">etc</code><code class="o">/</code><code class="nn">hostname</code><code class="calibre19">:</code><code class="o">/</code><code class="s">etc</code><code class="o">/</code><code class="s">host_hostname</code>
<code class="lineno">42 </code>    <code class="nn">deploy</code><code class="calibre19">:</code>
<code class="lineno">43 </code>      <code class="nn">mode</code><code class="calibre19">:</code> <code class="s">global</code>
<code class="lineno">44 </code>      <code class="nn">labels</code><code class="calibre19">:</code>
<code class="lineno">45 </code>        <code class="o">-</code> <code class="s">com</code><code class="calibre19">.</code><code class="s">df</code><code class="calibre19">.</code><code class="s">notify</code><code class="o">=</code><code class="s">true</code>
<code class="lineno">46 </code>        <code class="o">-</code> <code class="s">com</code><code class="calibre19">.</code><code class="s">df</code><code class="calibre19">.</code><code class="s">scrapePort</code><code class="o">=</code><code class="o">9100</code>
<code class="lineno">47 </code>    <code class="nn">command</code><code class="calibre19">:</code> <code class="s">'-collector.procfs /host/proc -collector.sysfs /host/sys -collector\</code>
<code class="lineno">48 </code><code class="s">.filesystem.ignored-mount-points "^/(sys|proc|dev|host|etc)($$|/)" -collector.te\</code>
<code class="lineno">49 </code><code class="s">xtfile.directory /etc/node-exporter/ -collectors.enabled="conntrack,diskstats,en\</code>
<code class="lineno">50 </code><code class="s">tropy,filefd,filesystem,loadavg,mdadm,meminfo,netdev,netstat,stat,textfile,time,\</code>
<code class="lineno">51 </code><code class="s">vmstat,ipvs"'</code>
<code class="lineno">52 </code>
<code class="lineno">53 </code><code class="nn">networks</code><code class="calibre19">:</code>
<code class="lineno">54 </code>  <code class="nn">monitor</code><code class="calibre19">:</code>
<code class="lineno">55 </code>    <code class="nn">external</code><code class="calibre19">:</code> <code class="s">true</code>
<code class="lineno">56 </code>  <code class="nn">proxy</code><code class="calibre19">:</code>
<code class="lineno">57 </code>    <code class="nn">external</code><code class="calibre19">:</code> <code class="s">true</code>
</pre></div>

</figure>

<p class="calibre3">As you can see, the stack definition contains the <code class="calibre19">node</code> and <code class="calibre19">haproxy</code> exporters as well as <code class="calibre19">cadvisor</code> service. <code class="calibre19">haproxy-exporter</code> provides proxy metrics, <code class="calibre19">node-exporter</code> collects server data, while <code class="calibre19">cadvisor</code> outputs information about containers inside our cluster. You’ll notice that <code class="calibre19">cadvisor</code> and <code class="calibre19">node-exporter</code> are running in the <code class="calibre19">global mode</code>. A replica will run on each server so that we can obtain an accurate picture of all the nodes that form the cluster.</p>

<p class="calibre3">The important parts of the stack definition are <code class="calibre19">com.df.notify</code> and <code class="calibre19">com.df.scrapePort</code> labels. The first one tells <code class="calibre19">swarm-listener</code> that it should notify the monitor when those services are created (or destroyed). The <code class="calibre19">scrapePort</code> labels are defining ports of the exporters.</p>

<p class="calibre3">Please visit <a href="http://monitor.dockerflow.com/usage/#scrape-parameters">Scrape Parameter</a> section of the documentation for more information how to define scrape parameters.</p>

<p class="calibre3">Let’s deploy the stack and see it in action.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/exporters.yml <code class="se">\</code>
<code class="lineno">3 </code>    exporter
</pre></div>

</figure>

<p class="calibre3">Please wait until all the services in the stack and running. You can monitor their status with <code class="calibre19">docker stack ps exporter</code> command.</p>

<p class="calibre3">Once the <code class="calibre19">exporter</code> stack is up-and-running, we can confirm that all the services were added to the <code class="calibre19">monitor</code> config.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/config"</code>
</pre></div>

</figure>


<figure class="image">
  <img src="../images/00015.jpeg" alt="Figure 4-1: Configuration with exporters" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-1: Configuration with exporters</figcaption>
</figure>


<p class="calibre3">We can also confirm that all the targets are indeed working.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/targets"</code>
</pre></div>

</figure>

<p class="calibre3">There should be three targets. If they are still not registered, please wait a few moments and refresh your screen.</p>

<p class="calibre3">Two of the targets (<code class="calibre19">exporter_cadvisor</code> and <code class="calibre19">exporter_node-exporter</code>) are running as global services. As a result, each has three endpoints, one on each node. The last target is <code class="calibre19">exporter_ha-proxy</code>. Since we did not deploy it globally nor specified multiple replicas, in has only one endpoint.</p>


<figure class="image">
  <img src="../images/00016.jpeg" alt="Figure 4-2: Targets and endpoints" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-2: Targets and endpoints</figcaption>
</figure>


<p class="calibre3">If we used the “official” Prometheus image, setting up those targets would require an update of the config file and reload of the service. On top of that, we’d need to persist the configuration. Instead, we let <em class="calibre21">Swarm Listener</em> notify <em class="calibre21">Docker Flow Monitor</em> that there are new services that should, in this case, generate new scraping targets. Instead of splitting the initial information into multiple locations, we specified scraping info as service labels and let the system take care of the distribution of that data.</p>


<figure class="image">
  <img src="../images/00017.jpeg" alt="Figure 4-3: Prometheus scrapes metrics from exporters" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-3: Prometheus scrapes metrics from exporters</figcaption>
</figure>


<p class="calibre3">Let’s take a closer look into the exporters running in our cluster.</p>

<h3 id="leanpub-auto-exploring-exporter-metrics" class="calibre20">Exploring Exporter Metrics</h3>

<p class="calibre3">All the exporters we deployed expose metrics in Prometheus format. We can observe them by sending a simple HTTP request. Since the services do not publish any ports, the only way we can communicate with them is through the <code class="calibre19">monitor</code> network attached to those exporters.</p>

<p class="calibre3">We’ll create a new utility service and attach it to the <code class="calibre19">monitor</code> network.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service create <code class="se">\</code>
<code class="lineno">2 </code>    --name util <code class="se">\</code>
<code class="lineno">3 </code>    --network monitor <code class="se">\</code>
<code class="lineno">4 </code>    --mode global <code class="se">\</code>
<code class="lineno">5 </code>    alpine sleep <code class="o">100000000</code>
</pre></div>

</figure>

<p class="calibre3">We created a service based on the <code class="calibre19">alpine</code> image, named it <code class="calibre19">util</code>, and attached it to the <code class="calibre19">monitor</code> network so that it can communicate with exporters we deployed. We made the service <code class="calibre19">global</code> so that it runs on every node. That guaranteed that a replica runs on the node we’re in. Since <code class="calibre19">alpine</code> does not have a long running process, without <code class="calibre19">sleep</code>, it would stop as soon as it started, Swarm would reschedule it, only to detect that it stopped again, and so on. Without <code class="calibre19">sleep</code> it would enter a never ending loop of failures and rescheduling.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">ID</code><code class="o">=</code><code class="k">$(</code>docker container ls -q <code class="se">\</code>
<code class="lineno">2 </code>    -f <code class="s">"label=com.docker.swarm.service.name=util"</code><code class="k">)</code>
<code class="lineno">3 </code>
<code class="lineno">4 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> <code class="se">\</code>
<code class="lineno">5 </code>    apk add --update curl
</pre></div>

</figure>

<p class="calibre3">Next, we found the <code class="calibre19">ID</code> of the container, entered it, and installed <code class="calibre19">curl</code>.</p>

<p class="calibre3">Now we’re ready to send requests to the exporters.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> <code class="se">\</code>
<code class="lineno">2 </code>    curl node-exporter:9100/metrics
</pre></div>

</figure>

<p class="calibre3">Partial output of the request to the <code class="calibre19">node-exporter</code> is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code># HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
<code class="lineno"> 3 </code># TYPE process_cpu_seconds_total counter
<code class="lineno"> 4 </code>process_cpu_seconds_total 3.05
<code class="lineno"> 5 </code># HELP process_max_fds Maximum number of open file descriptors.
<code class="lineno"> 6 </code># TYPE process_max_fds gauge
<code class="lineno"> 7 </code>process_max_fds 1.048576e+06
<code class="lineno"> 8 </code># HELP process_open_fds Number of open file descriptors.
<code class="lineno"> 9 </code># TYPE process_open_fds gauge
<code class="lineno">10 </code>process_open_fds 7
<code class="lineno">11 </code># HELP process_resident_memory_bytes Resident memory size in bytes.
<code class="lineno">12 </code># TYPE process_resident_memory_bytes gauge
<code class="lineno">13 </code>process_resident_memory_bytes 1.6228352e+07
<code class="lineno">14 </code># HELP process_start_time_seconds Start time of the process since unix epoch in \
<code class="lineno">15 </code>seconds.
<code class="lineno">16 </code># TYPE process_start_time_seconds gauge
<code class="lineno">17 </code>process_start_time_seconds 1.49505618366e+09
<code class="lineno">18 </code># HELP process_virtual_memory_bytes Virtual memory size in bytes.
<code class="lineno">19 </code># TYPE process_virtual_memory_bytes gauge
<code class="lineno">20 </code>process_virtual_memory_bytes 2.07872e+07
<code class="lineno">21 </code>...
</pre></div>

</figure>

<p class="calibre3">As you can see, each metric contains a help entry that describes it, states the type, and displays metric name followed with a value.</p>

<p class="calibre3">We won’t go into details of all the metrics provided by <code class="calibre19">node-exporter</code>. The list is quite big, and it would require a whole chapter (maybe even a book) to go through all of them. The important thing, at this moment, is to know that almost anything hardware and OS related is exposed as a metric.</p>

<p class="calibre3">Please note that Overlay network load-balanced our request and forwarded it to one of the replicas of the exporter. We don’t know what the origin of those metrics is. It could be a replica running on any of the nodes of the cluster. That should not be a problem since, at this moment, we’re interested only in observing how metrics look like. If you go back to the configuration screen, you’ll notice that targets are configured to use <code class="calibre19">tasks.[SERVICE_NAME]</code> format for addresses. When a service name is prefixed with <code class="calibre19">tasks.</code>, Swarm returns the list of all replicas (or tasks) of a service.</p>

<p class="calibre3">Let’s move to <code class="calibre19">cadvisor</code> metrics.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="nv">$ID</code> <code class="se">\</code>
<code class="lineno">2 </code>    curl cadvisor:8080/metrics
</pre></div>

</figure>

<p class="calibre3">Partial output of the request to <code class="calibre19">cadvisor</code> metrics is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code># HELP container_network_receive_bytes_total Cumulative count of bytes received
<code class="lineno"> 3 </code># TYPE container_network_receive_bytes_total counter
<code class="lineno"> 4 </code>container_network_receive_bytes_total{id="/",interface="dummy0"} 0
<code class="lineno"> 5 </code>container_network_receive_bytes_total{id="/",interface="eth0"} 6.6461026e+07
<code class="lineno"> 6 </code>container_network_receive_bytes_total{id="/",interface="eth1"} 1.3054141e+07
<code class="lineno"> 7 </code>...
<code class="lineno"> 8 </code>container_network_receive_bytes_total{container_label_com_docker_stack_namespace\
<code class="lineno"> 9 </code>="proxy",container_label_com_docker_swarm_node_id="zvn1kazstoa12pu3rfre9j4sw",co\
<code class="lineno">10 </code>ntainer_label_com_docker_swarm_service_id="gfoias8w9bf1cve5dujzzlpfh",container_\
<code class="lineno">11 </code>label_com_docker_swarm_service_name="proxy_swarm-listener",container_label_com_d\
<code class="lineno">12 </code>ocker_swarm_task="",container_label_com_docker_swarm_task_id="39hgd75s8vt051smew\
<code class="lineno">13 </code>3ke4imw",container_label_com_docker_swarm_task_name="proxy_swarm-listener.1.39hg\
<code class="lineno">14 </code>d75s8vt051smew3ke4imw",id="/docker/f2232d2ddf801b1ff41120bb1b95213be15767fe0e6d4\
<code class="lineno">15 </code>5266b3b8bba149b3634",image="vfarcic/docker-flow-swarm-listener:latest@sha256:d67\
<code class="lineno">16 </code>494f08aa3efba86d5231adba8ee7281c29fd401a5f67377ee026cc436552b",interface="eth0",\
<code class="lineno">17 </code>name="proxy_swarm-listener.1.39hgd75s8vt051smew3ke4imw"} 112764
<code class="lineno">18 </code>...
</pre></div>

</figure>

<p class="calibre3">The major difference, when compared to <code class="calibre19">node-exporter</code>, is that <code class="calibre19">cadvisor</code> provides a lot of labels. They help a lot when querying metrics, and we’ll use them soon.</p>

<p class="calibre3">Just like with <code class="calibre19">node-exporter</code>, we won’t go into details of each metric exposed through <code class="calibre19">cadvisor</code>. Instead, as we’re progressing towards creating a <em class="calibre21">self-healing</em> system, we’ll gradually increase the number of metrics we’re using and comment on them as they come.</p>

<p class="calibre3">Now that we have the metrics and that Prometheus is scraping and storing them in its database, we can turn our attention to queries we can execute.</p>

<h3 id="leanpub-auto-querying-metrics" class="calibre20">Querying Metrics</h3>

<p class="calibre3">Targets are up and running, and Prometheus is scraping their data. We should generate some traffic that would let us see Prometheus query language in action.</p>

<p class="calibre3">We’ll deploy <em class="calibre21">go-demo</em> stack. It contains a service with an API and a corresponding database. We’ll use it as a demo service that will allow us to explore better some of the metrics we can use.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">We should wait a few moments for the services from the <code class="calibre19">go-demo</code> stack to become operational. Please execute <code class="calibre19">docker stack ps go-demo</code> to confirm that all the replicas are running.</p>

<p class="calibre3">Now that the demo service is running, we can explore some of the metrics we have at our disposal.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/graph"</code>
</pre></div>

</figure>

<p class="calibre3">Please type <code class="calibre19">haproxy_backend_connections_total</code> in the <em class="calibre21">Expression</em> field, and press the <em class="calibre21">Execute</em> button. The result should be zero connections on the backend <code class="calibre19">go-demo_main-be8080</code>. Let’s spice it up by creating a bit of traffic.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> <code class="o">((</code><code class="nv">n</code><code class="o">=</code><code class="o">0</code><code class="calibre19">;</code>n&lt;<code class="o">200</code><code class="calibre19">;</code>n++<code class="o">))</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello"</code>
<code class="lineno">3 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">We sent 200 requests to the <code class="calibre19">go-demo</code> service.</p>

<p class="calibre3">If we go back to the Prometheus UI and repeat the execution of the <code class="calibre19">haproxy_backend_connections_total</code> expression, the result should be different. In my case, there are <em class="calibre21">200</em> backend connections from <em class="calibre21">go-demo_main-be8080</em>.</p>


<figure class="image">
  <img src="../images/00018.jpeg" alt="Figure 4-4: HA Proxy metrics" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-4: HA Proxy metrics</figcaption>
</figure>


<p class="calibre3">We could display the data as a graph by clicking the <em class="calibre21">Graph</em> tab.</p>

<aside class="tip">
    <p class="calibre3">You might be tempted to make a Dashboard (or two) in Prometheus. Don’t! I recommend using Grafana, even though it is out of the scope of this chapter. You’ll need to be patient and wait until we start exploring dashboards.</p>

</aside>

<p class="calibre3">How about memory usage? We have the data through <code class="calibre19">cadvisor</code> so we might just as well use it.</p>

<p class="calibre3">Please type <code class="calibre19">container_memory_usage_bytes{container_label_com_docker_swarm_service_name="go-demo_main"}</code> in the expression field and click the <em class="calibre21">Execute</em> button.</p>

<p class="calibre3">The result is memory usage limited to the Docker service <code class="calibre19">go-demo_main</code>. Depending on the view, you should see three values in <em class="calibre21">Console</em> or three lines in the <em class="calibre21">Graph</em> tab. They represent memory usage of the three replicas of the <code class="calibre19">go-demo_main</code> service.</p>


<figure class="image">
  <img src="../images/00019.jpeg" alt="Figure 4-5: cAdvisor metrics" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-5: cAdvisor metrics</figcaption>
</figure>


<p class="calibre3">Finally, let’s explore one of the <code class="calibre19">node-exporter</code> metrics. We can, for example, display the amount of available memory from each of the nodes.</p>

<p class="calibre3">Please type <code class="calibre19">sum by (instance) (node_memory_MemFree)</code> in the expression field and click the <em class="calibre21">Execute</em> button.</p>

<p class="calibre3">The result is a representation of free memory for each of the nodes of the cluster.</p>


<figure class="image">
  <img src="../images/00020.jpeg" alt="Figure 4-6: Graph with available memory" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-6: Graph with available memory</figcaption>
</figure>


<p class="calibre3">Now that we had a very brief overview of the ways we can query metrics, we should start using them.</p>

<h3 id="leanpub-auto-updating-service-constraints" class="calibre20">Updating Service Constraints</h3>

<p class="calibre3">The services we created so far are scheduled without any constraints. The exceptions are those that tie some of the services to one of the Swarm managers.</p>

<p class="calibre3">Without constraints, Swarm will distribute service replicas evenly. It will place them on a node that has fewest containers. Such a strategy can be disastrous. For example, we might end up with Prometheus, ElasticSearch, and MongoDB on the same node. Since all three of them require a fair amount of memory, their performance can deteriorate quickly. At the same time, the rest of the nodes might be running very undemanding services like <code class="calibre19">go-demo</code>. As a result, we can end up with a very uneven distribution of replicas from the resource perspective.</p>

<p class="calibre3">We cannot blame Swarm for a poor distribution of service replicas. We did not give it any information to work with. As a minimum, we should have defined how much memory it should reserve for each service as well as memory limits.</p>

<p class="calibre3">Memory reservation gives Swarm a hint how much it should reserve for a service. If, for example, we specify that a replica of a service should reserve 1GB of memory, Swarm will make sure to run it on a node that has that amount available. Bear in mind that it does not compare reservation with the actual memory usage but, instead, it compares it with the reservations made for other services and the total amount of memory allocated to each node.</p>

<aside class="tip">
    <p class="calibre3">When a service defines how much memory should be reserved, it is only a hint to Swarm how much memory we expect it to use. The service will not get that memory assigned to it. Doing something like that would require a VM instead of a container.</p>

</aside>

<p class="calibre3">Memory limit, on the other hand, should be set to the maximum amount we expect a service to use. If the actual usage surpasses it, the container will be shut down and, consequently, Swarm will reschedule it. Memory limit is, among other things, a useful protection against memory leaks and a way of preventing a single service abducting all the resources.</p>

<p class="calibre3">Let us revisit the services we are currently running and try to set their memory reservations and limits.</p>

<p class="calibre3">What should be the constraint values? How do we know how much memory should be reserved and what should be the limit? As it happens, there are quite a few different approaches we can take.</p>

<p class="calibre3">We could visit a fortune teller and consult a crystal ball, or we can make a lot of very inaccurate assumptions. Either of those is a bad way of defining constraints. You might be inclined to say that databases need more memory than backend services. We can assume that those written in Java require more resources than those written in Go. There is no limit to the number of guesses we could make. However, more often than not, they will be false and inaccurate. If those two would be the only options, I would strongly recommend visiting a fortune teller instead guessing. Since the result will be, more or less, the same, a fortune teller can, at least, provide a fun diversion from day to day monotony and lead to very popular photos uploaded to Instagram.</p>

<aside class="tip">
    <p class="calibre3">When faced only with bad options, choose the one that is most pleasing to execute.</p>

</aside>

<p class="calibre3">The correct approach is to let the services run for a while and consult metrics. Then let them run a while longer and revisit the metrics. Then wait some more and consult again. The point is that the constraints should be reviewed and, if needed, updated periodically. They should be redefined and adapted as a result of new data. It’s a task that should be repeated every once in a while. Fortunately, we can create alerts that will tell us when to revisit constraints. However, you’ll have to wait a while longer until we get there. For now, we are only concerned with the initial set of constraints.</p>

<p class="calibre3">While we should let the services run for at least a couple of hours before consulting metrics, my patience is reaching the limit. Instead, we’ll imagine that enough metrics were collected and consult Prometheus.</p>

<p class="calibre3">The first step is to get a list of the stacks we are currently running.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ls
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME      SERVICES
<code class="lineno">2 </code>exporter  3
<code class="lineno">3 </code>go-demo   2
<code class="lineno">4 </code>monitor   2
<code class="lineno">5 </code>proxy     2
</pre></div>

</figure>

<p class="calibre3">Let us consult the current memory usage of those services.</p>

<p class="calibre3">Please open Prometheus’ graph screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/graph"</code>
</pre></div>

</figure>

<p class="calibre3">Type <code class="calibre19">container_memory_usage_bytes{container_label_com_docker_stack_namespace="exporter"}</code> in the <em class="calibre21">Expression</em> field, click the <em class="calibre21">Execute</em> button, and switch to the <em class="calibre21">Graph</em> view.</p>

<p class="calibre3">If you hover over the lines in the graph, you’ll see that one of the labels is <code class="calibre19">container_label_com_docker_swarm_service_name</code>. It contains the name of a service allowing you to identify how much memory it is consuming.</p>

<p class="calibre3">While the exact numbers will differ from one case to another, <code class="calibre19">exporter_cadvisor</code> should be somewhere between 20MB and 30MB, while <code class="calibre19">exporter_node-exporter</code> and <code class="calibre19">exporter_ha-proxy</code> should have lower usage that is around 10MB.</p>

<p class="calibre3">With those numbers in mind, our <code class="calibre19">exporter</code> stack can be as follows (limited to relevant parts).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>  ha-proxy:
<code class="lineno"> 4 </code>    ...
<code class="lineno"> 5 </code>    deploy:
<code class="lineno"> 6 </code>      ...
<code class="lineno"> 7 </code>      resources:
<code class="lineno"> 8 </code>        reservations:
<code class="lineno"> 9 </code>          memory: 20M
<code class="lineno">10 </code>        limits:
<code class="lineno">11 </code>          memory: 50M
<code class="lineno">12 </code>    ...
<code class="lineno">13 </code>
<code class="lineno">14 </code>  cadvisor:
<code class="lineno">15 </code>    ...
<code class="lineno">16 </code>    deploy:
<code class="lineno">17 </code>      ...
<code class="lineno">18 </code>      resources:
<code class="lineno">19 </code>        reservations:
<code class="lineno">20 </code>          memory: 30M
<code class="lineno">21 </code>        limits:
<code class="lineno">22 </code>          memory: 50M
<code class="lineno">23 </code>
<code class="lineno">24 </code>  node-exporter:
<code class="lineno">25 </code>    ...
<code class="lineno">26 </code>    deploy:
<code class="lineno">27 </code>      ...
<code class="lineno">28 </code>      resources:
<code class="lineno">29 </code>        reservations:
<code class="lineno">30 </code>          memory: 20M
<code class="lineno">31 </code>        limits:
<code class="lineno">32 </code>          memory: 50M
<code class="lineno">33 </code>    ...
</pre></div>

</figure>

<p class="calibre3">We set memory reservations similar to the upper bounds of the current usage. That will help Swarm schedule the containers better, unless they are global and have to run everywhere. More importantly, it allows Swarm to calculate future schedules by excluding these reservations from the total available memory.</p>

<p class="calibre3">Memory limits, on the other hand, will provide limitations on how much memory containers created from those services can use. Without memory limits, a container might “go wild” and abduct all the memory on a node for itself. Good example are in-memory databases like Prometheus. If we would deploy it without any limitation, it could easily take over all the resources leaving the rest of the services running on the same node struggling.</p>

<p class="calibre3">Let’s deploy the updated version of the <code class="calibre19">exporter</code> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/exporters-mem.yml <code class="se">\</code>
<code class="lineno">3 </code>    exporter
</pre></div>

</figure>

<p class="calibre3">Since most of the stack are global services, we will not see much difference in the way Swarm schedules them. No matter the reservations, a replica will run on each node when the mode is global. Later on, we’ll see more benefits behind memory reservations. For now, the important thing to note is that Swarm has a better picture about the reserved memory on each node and will be able to do future scheduling with more precision.</p>

<p class="calibre3">We’ll continue with the rest of the stacks. The next in line is <code class="calibre19">go-demo</code>.</p>

<p class="calibre3">Please go back to Prometheus’ Graph screen, type <code class="calibre19">container_memory_usage_bytes{container_label_com_docker_stack_namespace="go-demo"}</code> in the <em class="calibre21">Expression</em> field, and click the <em class="calibre21">Execute</em> button.</p>

<p class="calibre3">The current usage of <code class="calibre19">go-demo_db</code> should be between 30MB and 40MB while <code class="calibre19">go-demo_main</code> is probably below 5MB. We’ll update the stack accordingly.</p>

<p class="calibre3">The new <code class="calibre19">go-demo</code> stack is as follows (limited to relevant parts).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    deploy:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      resources:
<code class="lineno"> 7 </code>        reservations:
<code class="lineno"> 8 </code>          memory: 5M
<code class="lineno"> 9 </code>        limits:
<code class="lineno">10 </code>          memory: 10M
<code class="lineno">11 </code>
<code class="lineno">12 </code>  db:
<code class="lineno">13 </code>    ...
<code class="lineno">14 </code>    deploy:
<code class="lineno">15 </code>      resources:
<code class="lineno">16 </code>        reservations:
<code class="lineno">17 </code>          memory: 40M
<code class="lineno">18 </code>        limits:
<code class="lineno">19 </code>          memory: 80M
<code class="lineno">20 </code>...
</pre></div>

</figure>

<p class="calibre3">Now we can deploy the updated version of the <code class="calibre19">go-demo</code> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo-mem.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">Two stacks are done, and two are still left to be updated. The <code class="calibre19">monitor</code> and <code class="calibre19">proxy</code> stacks should follow the same process. I’m sure that by now you can query Prometheus by yourself. You’ll notice that <code class="calibre19">monitor_monitor</code> service (Prometheus) is the one that uses the most memory (over 100MB). Since we can expect Prometheus memory usage to rise with time, we should be generous with its reservations and set it to 500MB. Similarly, a reasonable limit could be 800MB. The rest of the services are very moderate with their memory consumption.</p>

<p class="calibre3">Once you’re done exploring the rest of the stacks through Prometheus, the only thing left is to deploy of the updated versions.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DOMAIN</code><code class="o">=</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code> <code class="se">\</code>
<code class="lineno">2 </code>    docker stack deploy <code class="se">\</code>
<code class="lineno">3 </code>    -c stacks/docker-flow-monitor-mem.yml <code class="se">\</code>
<code class="lineno">4 </code>    monitor
<code class="lineno">5 </code>
<code class="lineno">6 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">7 </code>    -c stacks/docker-flow-proxy-mem.yml <code class="se">\</code>
<code class="lineno">8 </code>    proxy
</pre></div>

</figure>

<aside class="tip">
    <p class="calibre3">Memory limits and reservations need to be reviewed periodically. With time, the system will grow both in size as well as in the load it is experiencing. Service definitions need to be adapted to those and other changes.</p>

</aside>

<p class="calibre3">Now that our stacks are better-defined thanks to metrics, we can proceed and try to improve our queries through memory reservations and limits.</p>

<h3 id="leanpub-auto-using-memory-reservations-and-limits-in-prometheus" class="calibre20">Using Memory Reservations and Limits in Prometheus</h3>

<p class="calibre3">Metrics obtained through <em class="calibre21">cAdvisor</em> are not restricted to actual usage. We have, among others, metrics based on container specs. We can, for example, retrieve memory limits with the metric <code class="calibre19">container_spec_memory_limit_bytes</code>.</p>

<p class="calibre3">Please type <code class="calibre19">container_spec_memory_limit_bytes{container_label_com_docker_stack_namespace!=""}</code> in the <em class="calibre21">Expression</em> field and click the <em class="calibre21">Execute</em> button. The result should be straight lines that represent memory limits we defined in our stacks.</p>

<p class="calibre3">The usage of the <code class="calibre19">container_label_com_docker_stack_namespace</code> label is important. We used it to filter the metrics so that only those that come from the stacks are included. That way, we excluded root metrics from <code class="calibre19">cAdvisor</code> that provide summarized totals.</p>

<p class="calibre3">In Prometheus, memory limits are not very useful in themselves. However, if we combine them with the actual memory usage, we can get percentages that can provide indications of the health of our system.</p>

<p class="calibre3">Please type <code class="calibre19">container_memory_usage_bytes{container_label_com_docker_stack_namespace!=""} / container_spec_memory_limit_bytes{container_label_com_docker_stack_namespace!=""} * 100</code> in the <em class="calibre21">Expression</em> field and click the <em class="calibre21">Execute</em> button.</p>


<figure class="image1">
  <img src="../images/00021.jpeg" alt="Figure 4-7: Graph percentages based on memory limits and the actual usage" class="calibre17"/>
  <figcaption class="calibre18">Figure 4-7: Graph percentages based on memory limits and the actual usage</figcaption>
</figure>


<p class="calibre3">The result consists of percentages based on memory limits and the actual usage. The should all be below 60%. We will leverage this information later when we start working on alerts.</p>

<h3 id="leanpub-auto-what-now-3" class="calibre20">What Now?</h3>

<p class="calibre3">We did not go deep into metrics and queries. There are too many of them. Listing each metric would be the repetition of the <code class="calibre19">HELP</code> entries that already explain them (even though often not in much detail). More importantly, I believe that the best way to learn something is through a practical usage. We’ll use those metrics soon when we start creating alerts, and you will have plenty of opportunities to get a better understanding how they work. The same holds true for queries. They will be indispensable for creating alerts and will be explained in more details in the next chapter. Still, even though we’ll go through quite a few metrics and queries, the book will not provide a detailed documentation of every combination you can apply. <a href="https://prometheus.io/docs/querying/basics/">Querying Prometheus</a> is a much better place to learn how queries work. Instead, we’ll focus on practical hands-on experience.</p>

<p class="calibre3">Now it’s time for another break. Remove the VMs, grab a coffee, do something fun, and come back fresh. Alerts are coming next.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker-machine rm -f <code class="se">\</code>
<code class="lineno">2 </code>    swarm-1 swarm-2 swarm-3
</pre></div>

</figure>



</div>
</body></html>