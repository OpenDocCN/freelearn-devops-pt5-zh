<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Creating and Managing a Docker Swarm Cluster in DigitalOcean</h1>
            </header>

            <article>
                
<div class="packt_quote"><em>                               Plan to throw one (implementation) away; you will, anyhow.</em><em>                                                                                                                             </em><em>–Fred Brooks</em></div>
<p>We already saw a few ways to create and operate a Swarm cluster in AWS. Now we'll try to do the same in <em>DigitalOcean</em> (<a href="https://www.digitalocean.com/">https://www.digitalocean.com/</a>). We'll explore some of the tools and configurations that can be used with this hosting provider.</p>
<p>Unlike AWS that is known to everyone, DigitalOcean is relatively new and less well known. You might be wondering why I chose DigitalOcean before some other providers like Azure and GCE. The reason lies in the differences between AWS (and other similar providers) and DigitalOcean. The two differ in many aspects. Comparing them is like comparing David and Goliath. One is small while the other (AWS) is huge. DigitalOcean understands that it cannot compete with AWS on its own ground, so it decided to play a different game.</p>
<p>DigitalOcean launched in 2011 and is focused on very specific needs. Unlike AWS with its <em>everything-to-everyone</em> approach, DigitalOcean provides virtual machines. There are no bells and whistles. You do not get lost in their catalog of services since it is almost non-existent. If you need a place to host your cluster and you do not want to use services designed to lock you in, DigitalOcean might be the right choice.</p>
<p>DigitalOcean's main advantages are pricing, high performance, and simplicity. If that's what you're looking for, it is worth trying it out.</p>
<p>Let's go through these three advantages one by one.</p>
<p>DigitalOcean's pricing is probably the best among all cloud providers. No matter whether you are a small company in need of only a couple of servers, or a big entity that is looking for a place to instantiate hundreds or even thousands of servers, chances are that DigitalOcean will be cheaper than any other provider. That might leave you wondering about the quality. After all, cheaper things tend to sacrifice it. Is that the case with DigitalOcean?</p>
<p>DigitalOcean offers very high-performance machines. All disk drives are SSD, network speed is 1 Gbps, and it takes less than a minute to create and initialize droplets (their name for VMs). As a comparison, AWS EC2 instance startup time can vary between one and three minutes.</p>
<p>The last advantages DigitalOcean offers are their UI and API. Both are clean and easy to understand. Unlike AWS that can have a steep learning curve, you should have no trouble learning how to use them in a few hours.</p>
<p>Enough with the words of praise. Not everything can be great. What are the disadvantages?</p>
<p>DigitalOcean does not offer a plethora of services. It does a few things, and it does them well. It is a bare-bone <strong>Infrastructure as a service </strong>(<strong>IaaS</strong>) provider. It assumes that you will set up the services yourself. There is no load balancing, centralized logging, sophisticated analytics, hosted databases, and so on. If you need those things, you are expected to set them up yourself. Depending on your use case, that can be an advantage or a disadvantage.</p>
<p>A comparison between DigitalOcean and AWS is unfair since the scope of what each does is different. DigitalOcean is not trying to compete with AWS as a whole. If pressed to compare something, that would be DigitalOcean against AWS EC2. In such a case, DigitalOcean wins hands down.</p>
<p>I will assume that you already have a DigitalOcean account. If that's not the case, please register using: <a href="https://m.do.co/c/ee6d08525457">https://m.do.co/c/ee6d08525457</a> . You'll get 10 in credit. That should be more than enough to run the examples in this chapter. DigitalOcean is so cheap that you will probably finish this chapter with more than 9 remaining balance.</p>
<p>Even if you've already made a firm decision to use a different cloud computing provider or on-premise servers, I highly recommend going through this chapter. It will help you compare DigitalOcean with your provider of choice.</p>
<p>Let's give DigitalOcean a spin and judge through examples whether it is a good choice to host our Swarm cluster.</p>
<p>You might notice that some parts of this chapter are very similar, or even the same as those you read in the other cloud computing chapters like <a href="">Chapter 12</a>, <em>Creating and Managing a Docker Swarm Cluster in Amazon Web Services</em>. The reason for the partial repetition is the goal to make the cloud computing chapters useful not only to those who read everything, but also those who skipped other providers and jumped right here.</p>
<p>Before we move into practical exercises, we'll need to get the access keys and decide the region where we'll run the cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up the environment variables</h1>
            </header>

            <article>
                
<p>In the <a href="">Chapter 12</a>, <em>Creating And Managing A Docker Swarm Cluster in Amazon Web Services,</em> we installed AWS <strong>Command Line Interface</strong> (<strong>CLI</strong>) (<a href="https://aws.amazon.com/cli/" target="_blank">https://aws.amazon.com/cli/</a>) that helped us with some of the tasks. DigitalOcean has a similar interface called doctl. Should we install it? I don't think we need a CLI for DigitalOcean. Their API is clean and well defined, and we can accomplish everything a CLI would do with simple curl requests. DigitalOcean proves that a well designed API goes a long way and can be the only entry point into the system, saving us the trouble of dealing with middle-man applications like CLIs.</p>
<p>Before we start using the API, we should generate an access token that will serve as the authentication method.</p>
<p>Please open the <em>DigitalOcean tokens</em> screen (<a href="https://cloud.digitalocean.com/settings/api/tokens" target="_blank">https://cloud.digitalocean.com/settings/api/tokens</a>) and click the<span class="packt_screen">Generate New Token</span> button. You'll be presented with the <span class="packt_screen">New personal access token</span> popup, as shown in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="228" src="assets/new-personal-access-token.png" width="324"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12-1: DigitalOcean new personal access token screen</div>
<p>Type <kbd>devops21</kbd> as <span class="packt_screen">Token name</span> and click the <span class="packt_screen">Generate Token</span> button. You'll see the newly generated token. We'll put it into the environment variable <kbd>DIGITALOCEAN_ACCESS_TOKEN</kbd>.</p>
<p>All the commands from this chapter are available in the Gist <kbd>12-digital-ocean.sh</kbd> (<a href="https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e">https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e</a>).</p>
<p>Copy the token before running the command that follows:</p>
<pre>
<strong><span class="hljs-keyword">export</span> DIGITALOCEAN_ACCESS_TOKEN=[...]</strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual token.</p>
<p>Now we can decide which region our cluster will run in.</p>
<p>We can see the currently available regions by sending a request to <a href="https://api.digitalocean.com/v2/regions" target="_blank">https://api.digitalocean.com/v2/regions</a>:</p>
<pre>
<strong>curl -X GET <br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>" </span> <br/><span class="hljs-string">    "https://api.digitalocean.com/v2/regions" </span> <br/>    | jq <span class="hljs-string">'.'</span></strong>
</pre>
<p>We sent an HTTP <kbd>GET</kbd> request to the regions API. The request contains the access token. The response is piped to <kbd>jq</kbd>.</p>
<p>A part of the output is as follows:</p>
<pre>
<strong>{<br/>  "<span class="hljs-attribute">regions</span>": <span class="hljs-value">[<br/>    ...<br/>    {<br/>      "<span class="hljs-attribute">name</span>": <span class="hljs-value"><span class="hljs-string">"San Francisco 2"</span></span>,<br/>      "<span class="hljs-attribute">slug</span>": <span class="hljs-value"><span class="hljs-string">"sfo2"</span></span>,<br/>      "<span class="hljs-attribute">sizes</span>": <span class="hljs-value">[<br/><span class="hljs-string">"512mb"</span>,<br/><span class="hljs-string">"1gb"</span>,<br/><span class="hljs-string">"2gb"</span><br/>      ]</span>,<br/>      "<span class="hljs-attribute">features</span>": <span class="hljs-value">[<br/><span class="hljs-string">"private_networking"</span>,<br/><span class="hljs-string">"backups"</span>,<br/><span class="hljs-string">"ipv6"</span>,<br/><span class="hljs-string">"metadata"</span>,<br/><span class="hljs-string">"storage"</span><br/>      ]</span>,<br/>      "<span class="hljs-attribute">available</span>": <span class="hljs-value"><span class="hljs-literal">true</span><br/></span>},<br/>    ...<br/>  ]</span>,<br/>  "<span class="hljs-attribute">links</span>": <span class="hljs-value">{}</span>,<br/>  "<span class="hljs-attribute">meta</span>": <span class="hljs-value">{<br/>    "<span class="hljs-attribute">total</span>": <span class="hljs-value"><span class="hljs-number">12</span><br/></span>}<br/></span>}</strong>
</pre>
<p>As we can see from the bottom of the response, DigitalOcean currently supports twelve regions. Each contains the information about available droplet sizes and the supported features.</p>
<p>Throughout this chapter, I will be using <strong>San Francisco 2 </strong>(<strong>sfo2</strong>) region. Feel free to change it to the region closest to your location. If you choose to run the examples in a different region, please make sure that it contains the <kbd>private_networking</kbd> feature.</p>
<p>We'll put the region inside the environment variable <kbd>DIGITALOCEAN_REGION</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> DIGITALOCEAN_REGION=sfo2</strong>
</pre>
<p>Now we are all set with the prerequisites that will allow us to create the first Swarm cluster in DigitalOcean. Since we used Docker Machine throughout most of the book, it will be our first choice.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up a Swarm cluster with Docker Machine and DigitalOcean API</h1>
            </header>

            <article>
                
<p>We'll continue using the <kbd>vfarcic/cloud-provisioning</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning">https://github.com/vfarcic/cloud-provisioning</a>) repository. It contains configurations and scripts that'll help us out. You already have it cloned. To be on the safe side, we'll pull the latest version:</p>
<pre>
<strong><span class="hljs-built_in">cd</span> cloud-provisioning<br/><br/>git pull</strong>
</pre>
<p>Let's create the first droplet:</p>
<pre>
<strong>docker-machine create \<br/>    --driver digitalocean  \<br/>    --digitalocean-size <span class="hljs-number">1</span>gb  \<br/>    --digitalocean-private-networking \  <br/>    swarm-<span class="hljs-number">1</span></strong>
</pre>
<p>We specified that the <em>Docker Machine</em> should use the <kbd>digitalocean</kbd> driver to create an instance in the region we defined as the environment variable <kbd>DIGITALOCEAN_REGION</kbd>. The size of the droplet is 1 GB and it has private networking enabled.</p>
<p>Docker Machine launched a DigitalOcean droplet, provisioned it with Ubuntu, and installed and configured Docker Engine.</p>
<p>As you no doubt already noticed, everyone is trying to come up with a different name for the same thing. DigitalOcean is no exception. They came up with the term <em>droplet</em>. It is a different name for a virtual private server. Same thing, different name.</p>
<p>Now we can initialize the cluster. We should use private IPs for all communication between nodes. Unfortunately, <kbd>docker-machine ip</kbd> command returns only the public IP, so we'll have to resort to a different method to get the private IP.</p>
<p>We can send a <kbd>GET</kbd> request to the droplets API:</p>
<pre>
<strong>curl -X GET \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>" </span> \<br/><span class="hljs-string">"https://api.digitalocean.com/v2/droplets" </span> \<br/>    | jq <span class="hljs-string">'.'</span></strong>
</pre>
<p>A part of the output is as follows:</p>
<pre>
<strong>{<br/><span class="hljs-string">"droplets"</span>: [<br/>    {<br/><span class="hljs-string">          "id"</span>: <span class="hljs-number">33906152</span>,<br/><span class="hljs-string">          "name"</span>: <span class="hljs-string">"swarm-1"</span>,<br/><span class="hljs-keyword">          ...</span><br/><span class="hljs-string">          "networks"</span>: {<br/><span class="hljs-string">          "v4"</span>: [<br/>            {<br/><span class="hljs-string">              "ip_address"</span>: <span class="hljs-string">"138.68.11.80"</span>,<br/><span class="hljs-string">              "netmask"</span>: <span class="hljs-string">"255.255.240.0"</span>,<br/><span class="hljs-string">              "gateway"</span>: <span class="hljs-string">"138.68.0.1"</span>,<br/><span class="hljs-string">              "type"</span>: <span class="hljs-string">"public"</span><br/>           },<br/>          {<br/><span class="hljs-string">              "ip_address"</span>: <span class="hljs-string">"10.138.64.175"</span>,<br/><span class="hljs-string">              "netmask"</span>: <span class="hljs-string">"255.255.0.0"</span>,<br/><span class="hljs-string">              "gateway"</span>: <span class="hljs-string">"10.138.0.1"</span>,<br/><span class="hljs-string">              "type"</span>: <span class="hljs-string">"private"</span><br/>          }<br/>        ],<br/><span class="hljs-string">        "v6"</span>: []<br/>      },<br/><span class="hljs-keyword">      ...</span><br/>],<br/><span class="hljs-string">"links"</span>: {},<br/><span class="hljs-string">"meta"</span>: {<br/><span class="hljs-string">   "total"</span>: <span class="hljs-number">1</span><br/>  }<br/>}</strong>
</pre>
<p>The <kbd>droplets</kbd> API returned all information about all the droplets we own (at the moment only one). We are interested only in the private IP of the newly created instance called <kbd>swarm-1</kbd>. We can get it by filtering the results to include only the droplet named <kbd>swarm-1</kbd> and selecting the <kbd>v4</kbd> element with the type <kbd>private</kbd>.<br/>
We'll use <kbd>jq</kbd> (<a href="https://stedolan.github.io/jq/" target="_blank">https://stedolan.github.io/jq/</a>) to filter the output and get what we need. If you haven't already, please download and install the jq distribution suited for your OS.</p>
<p>The command that sends the request, filters the result, and stores the private IP as an environment variable is as follows:</p>
<pre>
<strong>MANAGER_IP=$(curl -X GET  \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>" </span> \<br/><span class="hljs-string">"https://api.digitalocean.com/v2/droplets" </span> \<br/>    | jq -r <span class="hljs-string">'.droplets[]<br/>    | select(.name=="swarm-1").networks.v4[]<br/>    | select(.type=="private").ip_address'</span>)</strong>
</pre>
<p>We sent a <kbd>GET</kbd> request to the <kbd>droplets</kbd> API, used the <kbd>jq select</kbd> statement to discard all the entries except the one with the name <kbd>swarm-1</kbd>. That was followed with another select statement that returned only the private address. The output was stored as the environment variable <kbd>MANAGER_IP</kbd>.</p>
<p>To be on the safe side, we can echo the value of the newly created variable:</p>
<pre>
<strong><span class="hljs-built_in">echo</span> <span class="hljs-variable">$MANAGER_IP</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>10.138.64.175</strong>
</pre>
<p>Now we can execute the <kbd>swarm init</kbd> command in the same way as we did in the previous chapters:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker swarm init \<br/>    --advertise-addr <span class="hljs-variable">$MANAGER_IP</span></strong>
</pre>
<p>Let's confirm that the cluster is indeed initialized:</p>
<pre>
<strong>docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME STATUS  AVAILABILITY  MANAGER STATUS<br/>swarm-<span class="hljs-number">1</span>  Ready   <span class="hljs-keyword">Active</span>        Leader</strong>
</pre>
<p>Now that we initialized the cluster, we can add more nodes. We'll start by creating two new instances and joining them as managers:</p>
<pre>
<strong>MANAGER_TOKEN=$(docker swarm join-token -q manager)<br/><br/><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>; <span class="hljs-keyword">do</span><br/>  docker-machine create \<br/>    --driver digitalocean \<br/>    --digitalocean-size <span class="hljs-number">1</span>gb \ <br/>    --digitalocean-private-networking \ <br/>    swarm-<span class="hljs-variable">$i</span><br/><br/>  IP=$(curl -X GET \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>" \</span> <br/><span class="hljs-string">"https://api.digitalocean.com/v2/droplets" \</span> <br/>    | jq -r <span class="hljs-string">".droplets[]<br/>    | select(.name==\"swarm-<span class="hljs-variable">$i\</span>").networks.v4[]<br/>    | select(.type==\"private\").ip_address"</span>)<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-variable">$i</span>)<br/><br/>  docker swarm join \<br/>    --token <span class="hljs-variable">$MANAGER_TOKEN \</span><br/>    --advertise-addr <span class="hljs-variable">$IP \</span><br/><span class="hljs-variable">$MANAGER_IP</span>:<span class="hljs-number">2377 </span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>There's no need to explain the commands we just executed since they are the combination of those we used before.</p>
<p>We'll add a few worker nodes as well:</p>
<pre>
<strong>WORKER_TOKEN=$(docker swarm join-token -q worker)</strong><br/><br/><strong>for i in 4 5; do</strong><br/><strong> docker-machine create \</strong><br/><strong>   --driver digitalocean \</strong><br/><strong>   --digitalocean-size 1gb \</strong><br/><strong>   --digitalocean-private-networking \ </strong><br/><strong> swarm-$i</strong><br/><br/><strong> IP=$(curl -X GET \</strong><br/><strong>   -H "Authorization: Bearer $DIGITALOCEAN_ACCESS_TOKEN" \</strong><br/><strong>   "https://api.digitalocean.com/v2/droplets" \</strong><br/><strong>   | jq -r ".droplets[]</strong><br/><strong>   | select(.name==\"swarm-$i\").networks.v4[]</strong><br/><strong>   | select(.type=="\private\").ip_address")</strong><br/><br/><strong> eval $(docker-machine env swarm-$i)</strong><br/><br/><strong> docker swarm join \</strong><br/><strong>   --token $WORKER_TOKEN \</strong><br/><strong>   --advertise-addr $IP \</strong><br/><strong>   $MANAGER_IP:2377</strong><br/><strong>done</strong>
</pre>
<p>Let's confirm that all five nodes are indeed forming the cluster:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME STATUS AVAILABILITY MANAGER STATUS<br/>swarm-<span class="hljs-number">5</span>  Ready  <span class="hljs-keyword">Active</span><br/>swarm-<span class="hljs-number">1</span>  Ready  <span class="hljs-keyword">Active</span>       Leader<br/>swarm-<span class="hljs-number">4</span>  Ready  <span class="hljs-keyword">Active</span><br/>swarm-<span class="hljs-number">2</span>  Ready  <span class="hljs-keyword">Active</span>       Reachable<br/>swarm-<span class="hljs-number">3</span>  Ready  <span class="hljs-keyword">Active</span>       Reachable</strong>
</pre>
<p>That's it. Our cluster is ready. The only thing left is to deploy a few services and confirm that the cluster is working as expected.</p>
<p>Since we already created the services quite a few times, we'll speed up the process with the <kbd>vfarcic/docker-flow-proxy/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml</a>) and <kbd>vfarcic/go-demo/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml</a>) Compose stacks. They'll create the <kbd>proxy</kbd>, <kbd>swarm-listener</kbd>, <kbd>go-demo-db</kbd>, and <kbd>go-demo</kbd> services:</p>
<pre>
<strong>docker-machine ssh swarm-<span class="hljs-number">1</span><br/><br/><span class="hljs-built_in">sudo</span> docker network create --driver overlay proxy<br/><br/>curl -o proxy-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/docker-flow-proxy/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c proxy-stack.yml proxy<br/><br/>curl -o go-demo-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/go-demo/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c go-demo-stack.yml go-demo<br/><br/><span class="hljs-keyword">exit</span><br/><br/>docker service ls</strong>
</pre>
<p>Non-Windows users do not need to enter the <kbd>swarm-1</kbd> machine and can accomplish the same result by deploying the stacks directly from their laptops.</p>
<p>It'll take a few moments until all the images are downloaded. After a while, the output of the <kbd>service ls command</kbd> should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME           REPLICAS IMAGE                              COMMAND<br/>go<span class="hljs-attribute">-demo</span>        <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.2</span><br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span>     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span><br/>proxy          <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span><br/>swarm<span class="hljs-attribute">-listener</span> <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-swarm</span><span class="hljs-attribute">-listener</span></strong>
</pre>
<p>Let's confirm that the <kbd>go-demo</kbd> service is accessible:</p>
<pre>
<strong>curl -i $(docker-machine ip swarm-<span class="hljs-number">1</span>)/demo/hello</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-status">HTTP/1.1 <span class="hljs-number">200</span> OK</span><br/><span class="hljs-attribute">Date</span>: <span class="hljs-string">Wed, 07 Dec 2016 05:05:58 GMT</span><br/><span class="hljs-attribute">Content-Length</span>: <span class="hljs-string">14</span><br/><span class="hljs-attribute">Content-Type</span>: <span class="hljs-string">text/plain; charset=utf-8</span><br/><br/><span class="erlang-repl"><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></span></strong>
</pre>
<p>We set up the whole Swarm cluster using Docker Machine and the DigitalOcean API. Is that everything we need? That depends on the requirements we might define for our cluster. We should probably add a few Floating IP addresses.</p>
<p>A DigitalOcean Floating IP is a publicly-accessible static IP address that can be assigned to one of your Droplets. A Floating IP can also be instantly remapped, via the DigitalOcean Control Panel or API, to one of your other Droplets in the same data center. This instant remapping capability grants you the ability to design and create <strong>High Availability </strong>(<strong>HA</strong>) server infrastructure setups that do not have a single point of failure, by adding redundancy to the entry point, or gateway, to your servers.</p>
<p>In other words, we should probably set at least two Floating IPs and map them to two of the droplets in the cluster. Those two (or more) IPs would be set as our DNS records. That way, when an instance fails, and we replace it with a new one, we can remap the Elastic IP without affecting our users.</p>
<p>There are quite a few other improvements we could do. However, that would put us in an awkward position. We would be using a tool that is not meant for setting up a complicated cluster.</p>
<p>The creation of VMs was quite slow. Docker Machine spent too much time provisioning it with Ubuntu and installing Docker Engine. We can reduce that time by creating snapshots with Docker Engine pre-installed. However, with such an action, the main reason for using Docker Machine would be gone. Its primary usefulness is simplicity. Once we start complicating the setup with other resources, we'll realize that the simplicity is being replaced with too many ad-hoc commands.</p>
<p>Running <kbd>docker-machine</kbd> combined with API requests works great when we are dealing with a small cluster, especially when we want to create something fast and potentially not very durable. The biggest problem is that everything we've done so far has been ad-hoc commands. Chances are that we would not be able to reproduce the same steps the second time. Our infrastructure is not documented so our team would not know what constitutes our cluster.</p>
<p>My recommendation is to use <kbd>docker-machine</kbd> in DigitalOcean as a quick and dirty way to create a cluster mostly for demo purposes. It can be useful for production as well, as long as the cluster is relatively small.</p>
<p>We should look at alternatives if we'd like to set up a more complex, bigger, and potentially more permanent solution.</p>
<p>Let us delete the cluster we created and explore the alternatives with a clean slate:</p>
<pre>
<strong><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>; <span class="hljs-keyword">do </span><br/>    docker-machine rm <span class="hljs-operator">-f</span> swarm-<span class="hljs-variable">$i</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>If you read the previous chapter, at this point you are probably expecting to see a sub-chapter named <em>Docker for DigitalOcean</em>. There is no such thing. At least not at the time I'm writing this chapter. Therefore, we'll jump right into <em>Packer</em> and <em>Terraform</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up a Swarm cluster with Packer and Terraform</h1>
            </header>

            <article>
                
<p>This time we'll use a set of tools completely unrelated to Docker. It'll be <em>Packer </em>(<a href="https://www.packer.io/" target="_blank">https://www.packer.io/</a>) and <em>Terraform</em> (<a href="https://www.terraform.io/" target="_blank">https://www.terraform.io/</a>). Both are coming from <em>HashiCorp</em> (<a href="https://www.hashicorp.com/" target="_blank">https://www.hashicorp.com/</a>). Packer <span>allows</span> us to create machine images. With Terraform <span>we</span> can create, change, and improve cluster infrastructure. Both tools support <span>almost</span> all <span>the</span> major providers.</p>
<p>They can be used with Amazon EC2, CloudStack, DigitalOcean, <strong>Google Compute Engine</strong> (<strong>GCE</strong>), Microsoft Azure, VMWare, VirtualBox, and quite a few others. The ability to be infrastructure agnostic allows us to avoid vendor lock-in. With a minimal change in configuration, we can easily transfer our cluster from one provider to another. Swarm is designed to work seamlessly no matter which hosting provider we use, as long as the infrastructure is properly defined. With Packer and Terraform we can define infrastructure in such a way that transitioning from one to another is as painless as possible.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using Packer to create DigitalOcean snapshots</h1>
            </header>

            <article>
                
<p>The <kbd>vfarcic/cloud-provisioning</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning">https://github.com/vfarcic/cloud-provisioning</a>) repository already has the Packer and Terraform configurations we'll use. They are located in the directory <kbd>terraform/do</kbd>:</p>
<pre>
<strong><span class="hljs-built_in">cd</span> terraform/<span class="hljs-keyword">do</span></strong>
</pre>
<p>The first step is to use Packer to create a snapshot. To do that, we'll need our DigitalOcean API token set as the environment variable <kbd>DIGITALOCEAN_API_TOKEN</kbd>. It is the same token we set as the environment variable <kbd>DIGITALOCEAN_ACCESS_TOKEN</kbd>. Unfortunately, Docker Machine and Packer have different naming standards:</p>
<pre>
<strong><span class="hljs-keyword">export</span> DIGITALOCEAN_API_TOKEN=[...]</strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual token.</p>
<p>We'll instantiate all Swarm nodes from the same snapshot. It'll be based on Ubuntu and have the latest Docker Engine installed.</p>
<p>The JSON definition of the image we are about to build is in <kbd>terraform/do/packer-ubuntu-docker.json</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json</a>):</p>
<pre>
<strong>cat packer-ubuntu-docker.json</strong>
</pre>
<p>The configuration consists of two sections: <kbd>builders</kbd> and <kbd>provisioners</kbd>:</p>
<pre>
<strong>{<br/><span class="hljs-string">   "builders"</span>: [{<br/><span class="hljs-keyword">     ...</span><br/>   }],<br/><span class="hljs-string">   "provisioners"</span>: [{<br/><span class="hljs-keyword">     ...</span><br/>   }]<br/>}</strong>
</pre>
<p>The <kbd>builders</kbd> section defines all the information Packer needs to build a snapshot. The <kbd>provisioners</kbd> section describes the commands that will be used to install and configure software for the machines created by builders. The only required section is builders.</p>
<p>Builders are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, and so on. Packer comes with many builders by default, and can also be extended to add new builders.</p>
<p>The builders section we'll use is as follows:</p>
<pre>
<strong><span class="hljs-string">"builders"</span>: [{<br/><span class="hljs-string">   "type"</span>: <span class="hljs-string">"digitalocean"</span>,<br/><span class="hljs-string">   "region"</span>: <span class="hljs-string">"sfo2"</span>,<br/><span class="hljs-string">   "image"</span>: <span class="hljs-string">"ubuntu-16-04-x64"</span>,<br/><span class="hljs-string">   "size"</span>: <span class="hljs-string">"1gb"</span>,<br/><span class="hljs-string">   "private_networking"</span>: <span class="hljs-literal">true</span>,<br/><span class="hljs-string">   "snapshot_name"</span>: <span class="hljs-string">"devops21-{{timestamp}}"</span><br/>}]</strong>
</pre>
<p>Each type of builder has specific arguments that can be used. We specified that the <kbd>type</kbd> is <kbd>digitalocean</kbd>. Please visit the <span class="packt_screen">DigitalOcean Builder</span> page (<a href="https://www.packer.io/docs/builders/digitalocean.html" target="_blank">https://www.packer.io/docs/builders/digitalocean.html</a>) for more info.</p>
<p>Please note, when using the <kbd>digitalocean</kbd> type, we have to provide the token. We could have specified it through the <kbd>api_token</kbd> field. However, there is an alternative. If the field is not specified, Packer will try to get the value from the environment variable <kbd>DIGITALOCEAN_API_TOKEN</kbd>. Since we already exported it, there's no need to repeat ourselves with the token inside Packer configuration. Moreover, the token should be secret.<br/>
Putting it inside the config would risk exposure.The region is critical since a snapshot can be created only within one region. If we wanted to share the same machine across multiple regions, each would need to be specified as a separate builder.</p>
<p>We set the image to <kbd>ubuntu-16-04-x64</kbd>. That will be the base image which we'll use to create our own. The size of the snapshot is not directly related to the size of the droplets we'll create, so there's no need to make it big. We set it to 1 GB.</p>
<p>By default, DigitalOcean enables only public networking, so we defined <kbd>private_networking</kbd> as <kbd>true</kbd>. Later on, we'll set up Swarm communication to be available only through the private network.</p>
<p>The <kbd>snapshot_name</kbd> field is the name we'll give to this snapshot. Since there is no option to overwrite an existing snapshot, the name must be unique so we added <kbd>timestamp</kbd> to the name.</p>
<p>Please visit the DigitalOcean Builder page (<a href="https://www.packer.io/docs/builders/digitalocean.html" target="_blank">https://www.packer.io/docs/builders/digitalocean.html</a>) for more information.</p>
<p>The second section is provisioners. It contains an array of all the provisioners that Packer should use to install and configure software within running machines before turning them into snapshots.</p>
<p>There are quite a few <kbd>provisioner</kbd> types we can use. If you read <em>The DevOps 2.0 Toolkit </em>(<a href="https://www.amazon.com/dp/B01BJ4V66M" target="_blank">https://www.amazon.com/dp/B01BJ4V66M</a>), you know that I advocated Ansible as the <kbd>provisioner</kbd> of choice. Should we use it here as well? In most cases, when building images meant to run Docker containers, I opt for a simple shell. The reasons for a change from Ansible to Shell lies in the different objectives <kbd>provisioners</kbd> should fulfill when running on live servers, as opposed to building images.</p>
<p>Unlike Shell, Ansible (and most other <kbd>provisioners</kbd>) are idempotent. They verify the actual state and execute one action or another depending on what should be done for the desired state to be fulfilled. That's a great approach since we can run Ansible as many times as we want and the result will always be the same. For example, if we specify that we want to have JDK 8, Ansible will SSH into a destination server, discover that the JDK is not present and install it. The next time we run it, it'll discover that the JDK is already there and do nothing. Such an approach allows us to run Ansible playbooks as often as we want and it'll always result in JDK being installed. If we tried to accomplish the same through a Shell script, we'd need to write lengthy <kbd>if/else</kbd> statements. If JDK is installed, do nothing, if it's not installed, install it, if it's installed, but the version is not correct, upgrade it, and so on.</p>
<p>So, why not use it with Packer? The answer is simple. We do not need idempotency since we'll run it only once while creating an image. We won't use it on running instances. Do you remember the <em>pets vs cattle</em> discussion? Our VMs will be instantiated from an image that already has everything we need. If the state of that VM changes, we'll terminate it and create a new one. If we need to do an upgrade or install additional software, we won't do it inside the running instance, but create a new image, destroy running instances, and instantiate new ones based on the updated image.</p>
<p>Is idempotency the only reason we would use Ansible? Definitely not! It is a very handy tool when we need to define complicated server setup. However, in our case the setup is simple. We need Docker Engine and not much more. Almost everything will be running inside containers. Writing a few Shell commands to install Docker is easier and faster than defining Ansible playbooks.</p>
<p>It would probably take the same number of commands to install Ansible as to install Docker.</p>
<p>Long story short, we'll use shell as our <kbd>provisioner</kbd> of choice for building AMIs.</p>
<p>The <kbd>provisioners</kbd> section we'll use is as follows:</p>
<pre>
<strong>"provisioners": [{<br/>  "type": "shell",<br/>  "inline": [<br/>    "sudo apt-get <span class="hljs-operator"><span class="hljs-keyword">update</span><span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">get</span> install -y apt-transport-https ca-certificates nfs-common<span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">key</span> adv --keyserver hkp://ha.pool.sks-keyservers.net:<span class="hljs-number">80\</span> <br/>--recv-keys <span class="hljs-number">58118E89</span>F3A912897C070ADBF76221572C52609D<span class="hljs-string">",<br/>    "</span>echo <span class="hljs-string">'deb https://apt.dockerproject.org/repo ubuntu-xenial main'\</span> <br/>| sudo tee /etc/apt/sources.list.d/docker.list<span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">get</span> <span class="hljs-keyword">update</span><span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">get</span> install -y docker-engine<span class="hljs-string">"<br/>  ]<br/>}]</span></span></strong>
</pre>
<p>The shell type is followed by a set of commands. They are the same as the commands we can find in the Install Docker <em>on Ubuntu</em> (<a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/" target="_blank">https://docs.docker.com/engine/installation/linux/ubuntulinux/</a>) page.</p>
<p>Now that we have a general idea how Packer configuration works, we can proceed and build an image:</p>
<pre>
<strong>packer build -machine-readable \<br/>    packer-ubuntu-docker.json \<br/>    | tee packer-ubuntu-docker.log</strong>
</pre>
<p>We run the <kbd>packer build</kbd> of the <kbd>packer-ubuntu-docker.json</kbd> with the <kbd>machine-readable</kbd> output sent to the <kbd>packer-ubuntu-docker.log</kbd> file. Machine readable output will allow us to parse it easily and retrieve the ID of the snapshot we just created.</p>
<p>The final lines of the output are as follows:</p>
<pre>
<strong>...<br/><span class="hljs-number">1481087549</span>,,ui,say,Build 'digitalocean' finished.<br/><span class="hljs-number">1481087549</span>,,ui,say,n==&gt; Builds finished. The artifacts of successful builds are:<br/><span class="hljs-number">1481087549</span>,digitalocean,artifact-count,<span class="hljs-number">1</span><br/><span class="hljs-number">1481087549</span>,digitalocean,artifact,<span class="hljs-number">0</span>,builder-id,pearkes.digitalocean<br/><span class="hljs-number">1481087549</span>,digitalocean,artifact,<span class="hljs-number">0</span>,id,sfo2:<span class="hljs-number">21373017</span><br/><span class="hljs-number">1481087549</span>,digitalocean,artifact,<span class="hljs-number">0</span>,string,<span class="hljs-literal">A</span> snapshot was created: \<br/>'devops21-<span class="hljs-number">1481087268</span>' (ID: <span class="hljs-number">21373017</span>) in region 'sfo2'<br/><span class="hljs-number">1481087549</span>,digitalocean,artifact,<span class="hljs-number">0</span>,files-count,<span class="hljs-number">0</span><br/><span class="hljs-number">1481087549</span>,digitalocean,artifact,<span class="hljs-number">0</span>,end<br/><span class="hljs-number">1481087549</span>,,ui,say,--&gt; digitalocean: <span class="hljs-literal">A</span> snapshot was created:\ <br/>'devops21-<span class="hljs-number">1481087268</span>' (ID: <span class="hljs-number">21373017</span>) in region 'sfo2'</strong>
</pre>
<p>Apart from the confirmation that the build was successful, the relevant part of the output is the line <kbd>id,sfo2:21373017</kbd>. It contains the snapshot ID we'll need to instantiate VMs based on the image. You might want to store the <kbd>packer-ubuntu-docker.log</kbd> in your code repository in case you need to get the ID from a different server.</p>
<p>The flow of the process we executed can be described through <em>figure 12-2:</em></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="223" src="assets/cloud-architecture-images-1.png" width="350"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12-2: The flow of the Packer process</div>
<p>Now we are ready to create a Swarm cluster with VMs based on the snapshot we built.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using Terraform to create a Swarm cluster in DigitalOcean</h1>
            </header>

            <article>
                
<p>Terraform is the third member of the <em>everyone-uses-a-different-environment-variable-for-the-token</em> club. It expects the token to be stored as environment variable <kbd>DIGITALOCEAN_TOKEN</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> DIGITALOCEAN_TOKEN=[...]</strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual token.</p>
<p>Terraform does not force us to have any particular file structure. We can define everything in a single file. However, that does not mean that we should. Terraform configs can get big, and separation of logical sections into separate files is often a good idea. In our case, we'll have three <kbd>tf</kbd> files. The <kbd>terraform/do/variables.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf</a>) holds all the variables. If we need to change any parameter, we'll know where to find it. The <kbd>terraform/do/common.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf</a>) file contains definitions of the elements that might be potentially reusable on other occasions. Finally, the <kbd>terraform/do/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf</a>) file has the Swarm-specific resources. We'll explore each of the Terraform configuration files separately.</p>
<p>The content of the <kbd>terraform/do/variables.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf</a>) file is as follows:</p>
<pre>
<strong><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_manager_token"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = ""</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_worker_token"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = ""</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_snapshot_id"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = "unknown"</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_manager_ip"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = ""</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_managers"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = 3</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_workers"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = 2</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_region"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = "sfo2"</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_instance_size"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = "1gb"</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_init"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">   default</span> = false</span><br/>}</strong>
</pre>
<p>The <kbd>swarm_manager_token</kbd> and <kbd>swarm_worker_token</kbd> will be required to join the nodes to the cluster. The <kbd>swarm_snapshot_id</kbd> will hold the ID of the snapshot we created with Packer. The <kbd>swarm_manager_ip</kbd> variable is the IP of one of the managers that we'll need to provide for the nodes to join the cluster. The <kbd>swarm_managers</kbd> and <kbd>swarm_workers</kbd> define how many nodes we want of each. The <kbd>swarm_region</kbd> defines the region our cluster will run in while the <kbd>swarm_instance_size</kbd> is set to 1 GB. Feel free to change it to a bigger size if you start using this Terraform config to create a real cluster. Finally, the <kbd>swarm_init</kbd> variable allows us to specify whether this is the first run and the node should initialize the cluster. We'll see its usage very soon.</p>
<p>The content of the <kbd>terraform/do/common.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf</a>) file is as follows:</p>
<pre>
<strong>resource <span class="hljs-string">"digitalocean_ssh_key"</span> <span class="hljs-string">"docker"</span> {<br/>  name = <span class="hljs-string">"devops21-do"</span><br/>  public_key = <span class="hljs-string">"<span class="hljs-subst">${file(<span class="hljs-string">"devops21-do.pub"</span>)}</span>"</span><br/>}<br/><br/>resource <span class="hljs-string">"digitalocean_floating_ip"</span> <span class="hljs-string">"docker_1"</span> {<br/>  droplet_id = <span class="hljs-string">"<span class="hljs-subst">${digitalocean_droplet.swarm-manager.<span class="hljs-number">0</span>.id}</span>"</span><br/>  region = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_region}</span>"</span><br/>}<br/><br/>resource <span class="hljs-string">"digitalocean_floating_ip"</span> <span class="hljs-string">"docker_2"</span> {<br/>  droplet_id = <span class="hljs-string">"<span class="hljs-subst">${digitalocean_droplet.swarm-manager.<span class="hljs-number">1</span>.id}</span>"</span><br/>  region = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_region}</span>"</span><br/>}<br/><br/>resource <span class="hljs-string">"digitalocean_floating_ip"</span> <span class="hljs-string">"docker_3"</span> {<br/>  droplet_id = <span class="hljs-string">"<span class="hljs-subst">${digitalocean_droplet.swarm-manager.<span class="hljs-number">2</span>.id}</span>"</span><br/>  region = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_region}</span>"</span><br/>}<br/>output <span class="hljs-string">"floating_ip_1"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-subst">${digitalocean_floating_ip.docker_1.ip_address}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"floating_ip_2"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-subst">${digitalocean_floating_ip.docker_2.ip_address}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"floating_ip_3"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-subst">${digitalocean_floating_ip.docker_3.ip_address}</span>"</span><br/>}</strong>
</pre>
<p>Each resource is defined with a type (For example, <kbd>digitalocean_ssh_key</kbd>) and a name (For example, <kbd>docker</kbd>). The type determines which resource should be created and must be one of those currently supported.</p>
<p>The first resource <kbd>digitalocean_ssh_key</kbd> allows us to manage SSH keys for droplet access. Keys created with this resource can be referenced in your droplet configuration via their ID or fingerprint. We set it as the value of the <kbd>devops21-do.pub</kbd> file that we'll create soon.</p>
<p>The second resource we're using is the <kbd>digitalocean_floating_ip</kbd>. It represents a publicly-accessible static IP address that can be mapped to one of our droplets. We defined three of those. They would be used in our DNS configuration. That way, when a request is made to your domain, DNS redirects it to one of the floating IPs. If one the droplets is down, DNS should use one of the other entries. That way, you'd have time to change the floating IP from the failed to a new droplet.</p>
<p>Please consult the <em>DIGITALOCEAN_SSH_KEY</em> (<a href="https://www.terraform.io/docs/providers/do/r/ssh_key.html">https://www.terraform.io/docs/providers/do/r/ssh_key.html</a>) and <em>DIGITALOCEAN_FLOATING_IP</em> (<a href="https://www.terraform.io/docs/providers/do/r/floating_ip.html">https://www.terraform.io/docs/providers/do/r/floating_ip.html</a>) pages for more info.</p>
<p>Besides the resources, we also defined a few outputs. They represent values that will be displayed when Terraform apply is executed and can be queried easily using the output command.</p>
<p>When building potentially complex infrastructure, Terraform stores hundreds or thousands of attribute values for all resources. But, as a user, we may only be interested in a few values of importance, such as manager IPs. Outputs are a way to tell Terraform what data is relevant.</p>
<p>In our case, the outputs are the addresses of the floating IPs.</p>
<p>Please consult the <em>Output Configuration</em> (<a href="https://www.terraform.io/docs/configuration/outputs.html" target="_blank">https://www.terraform.io/docs/configuration/outputs.html</a>) page for more info.</p>
<p>Now comes the real deal. The <kbd>terraform/do/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf</a>) file contains the definition of all the instances we'll create.<br/>
Since the content of this file is a bit bigger than the others, we'll examine each resource separately.</p>
<p>The first resource in line is the <kbd>digitalocean_droplet</kbd> type named <kbd>swarm-manager</kbd>. Its purpose is to create Swarm manager nodes:</p>
<pre>
<strong>resource <span class="hljs-string">"digitalocean_droplet"</span> <span class="hljs-string">"swarm-manager"</span> {<br/>  image = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_snapshot_id}</span>"</span><br/>  size = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_instance_size}</span>"</span><br/>  count = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_managers}</span>"</span><br/>  name = <span class="hljs-string">"<span class="hljs-subst">${<span class="hljs-keyword">format</span>(<span class="hljs-string">"swarm-manager-<span class="hljs-variable">%02d</span>"</span>, (count.<span class="hljs-keyword">index</span> + <span class="hljs-number">1</span>))}</span>"</span><br/>  region = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_region}</span>"</span><br/>  private_networking = true<br/>  ssh_keys = [<br/><span class="hljs-string">"<span class="hljs-subst">${digitalocean_ssh_key.docker.id}</span>"</span><br/>  ]<br/>  connection {<br/>    user = <span class="hljs-string">"root"</span><br/>    private_key = <span class="hljs-string">"<span class="hljs-subst">${file(<span class="hljs-string">"devops21-do"</span>)}</span>"</span><br/>    agent = false<br/>  }<br/>  provisioner <span class="hljs-string">"remote-exec"</span> {<br/>    inline = [<br/><span class="hljs-string">"if <span class="hljs-subst">${var.swarm_init}</span>; then docker swarm init \<br/>--advertise-addr <span class="hljs-subst">${self.ipv4_address_private}</span>; fi"</span>,<br/><span class="hljs-string">"if ! <span class="hljs-subst">${var.swarm_init}</span>; then docker swarm join \<br/>--token <span class="hljs-subst">${var.swarm_manager_token}</span> --advertise-addr <br/><span class="hljs-subst">${self.ipv4_address_private}</span> <span class="hljs-subst">${var.swarm_manager_ip}</span>:2377; fi"</span><br/>    ]<br/>  }<br/>}</strong>
</pre>
<p>The resource contains the image that references the snapshot we created with Packer. The actual value is a variable that we'll define at runtime. The size specifies the size of the instance we want to create. The default value is fetched from the variable <kbd>swarm_instance_size</kbd>. By default, it is set to 1 GB. Just as any other variable, it can be overwritten at runtime.</p>
<p>The count field defines how many managers we want to create. The first time we run terraform, the value should be 1 since we want to start with one manager that will initialize the cluster. Afterward, the value should be whatever is defined in variables. We'll see the use case of both combinations soon.</p>
<p>The name, the region, and the <kbd>private_networking</kbd> should be self-explanatory. The ssh-keys type is an array that, at this moment, contains only one element; the ID of the <kbd>digitalocean_ssh_key</kbd> resource we defined in the <kbd>common.tf</kbd> file.</p>
<p>The connection field defines the SSH connection details. The user will be root. Instead of a password, we'll use the <kbd>devops21-do</kbd> key.</p>
<p>Finally, the <kbd>provisioner</kbd> is defined. The idea is to do as much provisioning as possible during the creation of the images. That way, instances are created much faster since the only action is to create a VM out of an image. However, there is often a part of provisioning that cannot be done when creating an image. The <kbd>swarm init</kbd> command is one of those. We cannot initialize the first Swarm node until we get the IP of the server. In other words, the server needs to be running (and therefore has an IP) before the <kbd>swarm init</kbd> command is executed.</p>
<p>Since the first node has to initialize the cluster while any other should join, we're using <kbd>if</kbd> statements to distinguish one case from the other. If the variable <kbd>swarm_init</kbd> is <kbd>true</kbd>, the <kbd>docker swarm init</kbd> command will be executed. On the other hand, if the variable <kbd>swarm_init</kbd> is set to <kbd>false</kbd>, the command will be <kbd>docker swarm join</kbd>. In that case, we are using another variable <kbd>swarm_manager_ip</kbd> to tell the node which manager to use to join the cluster. Please note that the IP is obtained using the special syntax <kbd>self.ipv4_address_private</kbd>. We are referencing oneself and getting the <kbd>ipv4_address_private</kbd>. There are many other attributes we can get from a resource. Please consult the <em>DIGITALOCEAN_DROPLET</em> (<a href="https://www.terraform.io/docs/providers/do/r/droplet.html" target="_blank">https://www.terraform.io/docs/providers/do/r/droplet.html</a>) page for more info.</p>
<p>Let's take a look at the <kbd>digitalocean_droplet</kbd> resource named <kbd>swarm-worker</kbd>:</p>
<pre>
<strong>resource <span class="hljs-string">"digitalocean_droplet"</span> <span class="hljs-string">"swarm-worker"</span> {<br/>  image = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_snapshot_id}</span>"</span><br/>  size = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_instance_size}</span>"</span><br/>  count = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_workers}</span>"</span><br/>  name = <span class="hljs-string">"<span class="hljs-subst">${<span class="hljs-keyword">format</span>(<span class="hljs-string">"swarm-worker-<span class="hljs-variable">%02d</span>"</span>, (count.<span class="hljs-keyword">index</span> + <span class="hljs-number">1</span>))}</span>"</span><br/>  region = <span class="hljs-string">"<span class="hljs-subst">${var.swarm_region}</span>"</span><br/>  private_networking = true<br/>  ssh_keys = [<br/><span class="hljs-string">   "<span class="hljs-subst">${digitalocean_ssh_key.docker.id}</span>"</span><br/>  ]<br/>  connection {<br/>    user = <span class="hljs-string">"root"</span><br/>    private_key = <span class="hljs-string">"<span class="hljs-subst">${file(<span class="hljs-string">"devops21-do"</span>)}</span>"</span><br/>    agent = false<br/>  }<br/>  provisioner <span class="hljs-string">"remote-exec"</span> {<br/>    inline = [<br/><span class="hljs-string">      "docker swarm join --token <span class="hljs-subst">${var.swarm_worker_token}</span> \<br/>--advertise-addr <span class="hljs-subst">${self.ipv4_address_private} </span><span class="hljs-subst">${var.swarm_manager_ip}</span>:\<br/>2377"</span><br/>    ]<br/>  }<br/>}</strong>
</pre>
<p>The <kbd>swarm-worker</kbd> resource is almost identical to <kbd>swarm-manager</kbd>. The only difference is in the count field that uses the <kbd>swarm_workers</kbd> variable and the <kbd>provisioner</kbd>. Since a worker cannot initialize a cluster, there was no need for <kbd>if</kbd> statements, so the only command we want to execute is <kbd>docker swarm join</kbd>. Terraform uses a naming convention that allows us to specify values as environment variables by adding the <kbd>TF_VAR_ prefix</kbd>. For example, we can specify the value of the variable <kbd>swarm_snapshot_id</kbd> by setting the environment variable <kbd>TF_VAR_swarm_snapshot_id</kbd>. The alternative is to use the <kbd>-var</kbd> argument. I prefer environment variables since they allow me to specify them once instead of adding <kbd>-var</kbd> to every command. The last part of the <kbd>terraform/do/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf</a>) specification are outputs.</p>
<p>The outputs we defined are as follows:</p>
<pre>
<strong>output <span class="hljs-string">"swarm_manager_1_public_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${digitalocean_droplet.swarm-manager.0.ipv4_address}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_1_private_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${digitalocean_droplet.swarm-manager.0.ipv4_address_private}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_2_public_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${digitalocean_droplet.swarm-manager.1.ipv4_address}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_2_private_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${digitalocean_droplet.swarm-manager.1.ipv4_address_private}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_3_public_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${digitalocean_droplet.swarm-manager.2.ipv4_address}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_3_private_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${digitalocean_droplet.swarm-manager.2.ipv4_address_private}</span>"</span><br/>}</strong>
</pre>
<p>They are public and private IPs of the managers. Since there are only a few (if any) reasons to know worker IPs, we did not define them as outputs.</p>
<p>Since we'll use the snapshot we created with Packer, we need to retrieve the ID from the <kbd>packer-ubuntu-docker.log</kbd>. Let's take another look at the file:</p>
<pre>
<strong>cat packer-ubuntu-docker.log</strong>
</pre>
<p>The important line of the output is as follows:</p>
<pre>
<strong>1481087549,<span class="hljs-tag">digitalocean</span>,<span class="hljs-tag">artifact</span>,0,<span class="hljs-tag">id</span>,<span class="hljs-tag">sfo2</span><span class="hljs-pseudo">:21373017</span></strong>
</pre>
<p>The command that follows parses the output and extracts the ID:</p>
<pre>
<strong><span class="hljs-keyword">export</span> TF_VAR_swarm_snapshot_id=$( \<br/>    grep <span class="hljs-string">'artifact,0,id'</span> \<br/>    packer-ubuntu-docker.log \<br/>    | cut <span class="hljs-operator">-d</span>: <span class="hljs-operator">-f</span>2)</strong>
</pre>
<p>Let's double-check that the command worked as expected:</p>
<pre>
<strong><span class="hljs-built_in">echo</span> <span class="hljs-variable">$TF_VAR_swarm_snapshot_id</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>21373017</strong>
</pre>
<p>We got the ID of the snapshot. Before we start creating the resources, we need to create the SSH key <kbd>devops21-do</kbd> we referenced in the Terraform config.</p>
<p>We'll create the SSH key using <kbd>ssh-keygen</kbd>:</p>
<pre>
<strong>ssh-keygen -t rsa</strong>
</pre>
<p>When asked to <em>enter file in which to save the key</em>, please answer with <kbd>devops21-do</kbd>. The rest of the questions can be answered in any way you see fit. I'll leave them all empty. </p>
<p>The output should be similar to the one that follows:</p>
<pre>
<strong>Generating <span class="hljs-keyword">public</span>/<span class="hljs-keyword">private</span> rsa <span class="hljs-keyword">key</span> pair.<br/>Enter file <span class="hljs-keyword">in</span> which <span class="hljs-keyword">to</span> save the <span class="hljs-keyword">key</span> (/Users/vfarcic/.ssh/id_rsa): devops21-<span class="hljs-keyword">do</span><br/>Enter passphrase (empty <span class="hljs-keyword">for</span> no passphrase):<br/>Enter same passphrase again:<br/>Your identification has been saved <span class="hljs-keyword">in</span> devops21-<span class="hljs-keyword">do</span>.<br/>Your <span class="hljs-keyword">public</span> <span class="hljs-keyword">key</span> has been saved <span class="hljs-keyword">in</span> devops21-<span class="hljs-keyword">do</span>.pub.<br/>The <span class="hljs-keyword">key</span> fingerprint <span class="hljs-keyword">is</span>:<br/>SHA256:a9BqjLkcC9eMnuKH+TZPE6E9S0w+cDQD4HTWEY9CuVk \<br/>vfarcic@Viktors-MacBook-Pro-<span class="hljs-number">2.</span>local<br/>The <span class="hljs-keyword">key</span><span class="hljs-comment">'s randomart image is:</span><br/>+---[RSA <span class="hljs-number">2048</span>]----+<br/>|  o.=+*o         |<br/>| o +..E=         |<br/>|  . o+= .        |<br/>|    oX o         |<br/>|    . X S        |<br/>|     O B .       |<br/>|  .o* X o        |<br/>|  +=+B o         |<br/>| ..=Bo.          |<br/>+----[SHA256]-----+</strong>
</pre>
<p>Now that the <kbd>devops21-do</kbd> key is created, we can start using Terraform. Before we create our cluster and the infrastructure around it, we should ask Terraform to show us the execution plan.</p>
<div class="packt_infobox"><strong>A note to Terraform v0.8+users. </strong><br/>
Normally, we would not need to specify targets to see the whole execution plan. However, Terraform <em>v0.8</em> introduced a bug that sometimes prevents us from outputting a plan if a resource has a reference to another, not yet created, resource. In this case, the <kbd>digitalocean_floating_ip.docker_2</kbd> and <kbd>digitalocean_floating_ip.docker_3</kbd> are such resources. The targets from the command that follows are intended to act as a workaround until the problem is fixed:</div>
<pre>
<strong>terraform plan \<br/>    -target digitalocean_droplet.swarm-manager \ <br/>    -target digitalocean_droplet.swarm-worker \</strong>
</pre>
<p>The result is an extensive list of resources and their properties. Since the output is too big to be printed, I'll limit the output only to the resource types and names:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-manager.0<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-manager.1<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-manager.2<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-worker.0<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-worker.1<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_ssh_key.docker<br/><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">6</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>Since this is the first execution, all the resources would be created if we were to execute <kbd>terraform apply</kbd>. We would get five droplets; three managers and two workers. That would be accompanied by three floating IPs and one SSH key.</p>
<p>If you see the complete output, you'll notice that some of the property values are set to <kbd>&lt;computed&gt;</kbd>. That means that Terraform cannot know what will be the actual values until it creates the resources. A good example is IPs. They do not exist until the droplet is created.</p>
<p>We can also output the plan using the <kbd>graph</kbd> command:</p>
<pre>
<strong>terraform graph</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-title">digraph</span> {<br/>    compound = <span class="hljs-string">"true"</span><br/>    newrank = <span class="hljs-string">"true"</span><br/>    subgraph <span class="hljs-string">"root"</span> {<br/><span class="hljs-string">"[root] digitalocean_droplet.swarm-manager"</span> [label = \<br/><span class="hljs-string">"digitalocean_droplet.swarm-manager"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">"[root] digitalocean_droplet.swarm-worker"</span> [label = \<br/><span class="hljs-string">"digitalocean_droplet.swarm-worker"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_1"</span> [label = \<br/><span class="hljs-string">"digitalocean_floating_ip.docker_1"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_2"</span> [label = \<br/><span class="hljs-string">"digitalocean_floating_ip.docker_2"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_3"</span> [label = \<br/><span class="hljs-string">"digitalocean_floating_ip.docker_3"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">"[root] digitalocean_ssh_key.docker"</span> [label = \<br/><span class="hljs-string">"digitalocean_ssh_key.docker"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">"[root] provider.digitalocean"</span> [label = \<br/><span class="hljs-string">"provider.digitalocean"</span>, shape = <span class="hljs-string">"diamond"</span>]<br/><span class="hljs-string">"[root] digitalocean_droplet.swarm-manager"</span> \<br/>-&gt; <span class="hljs-string">"[root] digitalocean_ssh_key.docker"</span><br/><span class="hljs-string">"[root] digitalocean_droplet.swarm-manager"</span> \<br/>-&gt; <span class="hljs-string">"[root] provider.digitalocean"</span><br/><span class="hljs-string">"[root] digitalocean_droplet.swarm-worker"</span> \<br/>-&gt; <span class="hljs-string">"[root] digitalocean_ssh_key.docker"</span><br/><span class="hljs-string">"[root] digitalocean_droplet.swarm-worker"</span> \<br/>-&gt; <span class="hljs-string">"[root] provider.digitalocean"</span><br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_1"</span> <br/>-&gt; <span class="hljs-string">"[root] digitalocean_droplet.swarm-manager"</span><br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_1"</span> \<br/>-&gt; <span class="hljs-string">"[root] provider.digitalocean"</span><br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_2"</span> \<br/>-&gt; <span class="hljs-string">"[root] digitalocean_droplet.swarm-manager"</span><br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_2"</span> \<br/>-&gt; <span class="hljs-string">"[root] provider.digitalocean"</span><br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_3"</span> \<br/>-&gt; <span class="hljs-string">"[root] digitalocean_droplet.swarm-manager"</span><br/><span class="hljs-string">"[root] digitalocean_floating_ip.docker_3"</span> \<br/>-&gt; <span class="hljs-string">"[root] provider.digitalocean"</span><br/><span class="hljs-string">"[root] digitalocean_ssh_key.docker"</span> \<br/>-&gt; <span class="hljs-string">"[root] provider.digitalocean"</span><br/>    }<br/>}</strong>
</pre>
<p>That, in itself, is not very useful.</p>
<p>The <kbd>graph</kbd> command is used to generate a visual representation of either a configuration or an execution plan. The output is in the DOT format, which can be used by GraphViz to make graphs.</p>
<p>Please open <em>Graphviz</em> Download page (<a href="http://www.graphviz.org/Download.php" target="_blank">http://www.graphviz.org/Download.php</a>) and download and install the distribution compatible with your OS.</p>
<p>Now we can combine the <kbd>graph</kbd> command with <kbd>dot</kbd>:</p>
<pre>
<strong>terraform graph | dot -Tpng &gt; graph.png</strong>
</pre>
<p>The output should be the same as in the <em>Figure 11-10:</em></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="210" src="assets/terraform-graph-1.png" width="599"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12-3: The image generated by Graphviz from the output of the terraform graph command</div>
<p>Visualization of the plan allows us to see the dependencies between different resources. In our case, all resources will use the <kbd>digitalocean</kbd> provider. Both instance types will depend on the SSH key docker, and the floating IPs will be attached to manager droplets.</p>
<p>When dependencies are defined, we don't need to specify explicitly all the resources we need.</p>
<p>As an example, let's take a look at the plan Terraform will generate when we limit it only to one Swarm manager node so that we can initialize the cluster:</p>
<pre>
<strong>terraform plan \<br/>    -target digitalocean_droplet.swarm-manager \<br/>    -var swarm_init=<span class="hljs-literal">true</span> \<br/>    -var swarm_managers=<span class="hljs-number">1</span></strong>
</pre>
<p>The runtime variables <kbd>swarm_init</kbd> and <kbd>swarm_managers</kbd> will be used to tell Terraform that we want to initialize the cluster with one manager. The plan command takes those variables into account and outputs the execution plan.</p>
<p>The output, limited only to resource types and names, is as follows:</p>
<pre>
<strong>+ digitalocean_droplet.swarm-manager<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_ssh_key.docker<br/><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">2</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>Even though the specified that we want only the plan for the <kbd>swarm-manager</kbd> resource, Terraform noticed that it depends on the SSH key docker, and included it in the execution plan.</p>
<p>We'll start small and create only one manager instance that will initialize the cluster. As we saw from the plan, it depends on the SSH key, so Terraform will create it as well:</p>
<pre>
<strong>terraform apply \<br/>    -target digitalocean_droplet.swarm-manager \<br/>    -var swarm_init=<span class="hljs-literal">true</span> \<br/>    -var swarm_managers=<span class="hljs-number">1</span></strong>
</pre>
<p>The output is too big to be presented fully in the book. If you look at it from your terminal, you'll notice that the SSH key is created first since <kbd>swarm-manager</kbd> depends on it. Please note that we did not specify the dependency explicitly. However, since the resource has it specified in the <kbd>ssh_keys</kbd> field, Terraform understood that it is the dependency.</p>
<p>Once the <kbd>swarm-manager</kbd> instance is created, Terraform waited until SSH access is available. After it had managed to connect to the new instance, it executed provisioning commands that initialized the cluster.</p>
<p>The final lines of the output are as follows:</p>
<pre>
<strong>Apply complete! Resources: <span class="hljs-number">2</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">0</span> destroyed.<br/><br/><span class="hljs-keyword">...</span><br/><br/>Outputs:<br/><br/>swarm_manager_1_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.255</span><span class="hljs-number">.140</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.57</span><span class="hljs-number">.39</span></strong>
</pre>
<p>The outputs are defined at the bottom of the <kbd>terraform/do/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf</a>) file. Please note that not all outputs are listed but only those of the resources that were created.</p>
<p>We can use the public IP of the newly created droplet and SSH into it.</p>
<p>You might be inclined to copy the IP. There's no need for that. Terraform has a command that can be used to retrieve any information we defined as the output.</p>
<p>The command that retrieves the public IP of the first, and currently the only manager is as follows:</p>
<pre>
<strong>terraform output swarm_manager_1_public_ip</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>138.68.57.39</strong>
</pre>
<p>We can leverage the output command to construct SSH commands. As an example, the command that follows will SSH into the machine and retrieve the list of Swarm nodes:</p>
<pre>
<strong>ssh -i devops21-do \<br/>    root@$(terraform output \<br/>    swarm_manager_1_public_ip) \<br/>    docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME          STATUS  AVAILABILITY  MANAGER STATUS<br/>swarm-manager-<span class="hljs-number">01</span>  Ready   <span class="hljs-keyword">Active</span>        Leader</strong>
</pre>
<p>From now on, we won't be limited to a single manager node that initialized the cluster. We can create all the rest of the nodes. However, before we do that, we need to discover the manager and worker tokens. For security reasons, it is better that they are not stored anywhere, so we'll create environment variables:</p>
<pre>
<strong><span class="hljs-keyword">export</span> TF_VAR_swarm_manager_token=$(ssh \<br/>    -i devops21-do \<br/>    root@$(terraform output \<br/>    swarm_manager_1_public_ip) \<br/>    docker swarm join-token -q manager)<br/><br/><span class="hljs-keyword">export</span> TF_VAR_swarm_worker_token=$(ssh \<br/>    -i devops21-do \<br/>    root@$(terraform output \<br/>    swarm_manager_1_public_ip) \<br/>    docker swarm join-token -q worker)</strong>
</pre>
<p>We'll also need to set the environment variable <kbd>swarm_manager_ip</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> TF_VAR_swarm_manager_ip=$(terraform \<br/>    output swarm_manager_1_private_ip)</strong>
</pre>
<p>Even though we could use <kbd>digitalocean_droplet.swarm-manager.0.private_ip</kbd> inside the <kbd>terraform/do/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf</a>) it is a good idea to have it defined as an environment variable. That way, if the first manager fails, we can easily change it to <kbd>swarm_manager_2_private_ip</kbd> without modifying the <kbd>.tf</kbd> files.</p>
<p>Now, let us see the plan for the creation of the rest of Swarm nodes:</p>
<pre>
<strong>terraform plan \<br/>    -target digitalocean_droplet.swarm-manager \<br/>    -target digitalocean_droplet.swarm-worker</strong>
</pre>
<p>The relevant lines of the output are as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-manager.1<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-manager.2<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-worker.0<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-worker.1<br/><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">4</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>We can see that the plan is to create four new resources. Since we already have one manager running and specified that the desired number is three, two additional managers will be created together with two workers.</p>
<p>Let's apply the execution plan:</p>
<pre>
<strong>terraform apply \<br/>    -target digitalocean_droplet.swarm-manager \<br/>    -target digitalocean_droplet.swarm-worker</strong>
</pre>
<p>The last lines of the output are as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>Apply complete! Resources: <span class="hljs-number">4</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">0</span> destroyed.<br/><br/><span class="hljs-keyword">...</span><br/><br/>Outputs:<br/><br/>swarm_manager_1_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.255</span><span class="hljs-number">.140</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.57</span><span class="hljs-number">.39</span><br/>swarm_manager_2_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.224</span><span class="hljs-number">.161</span><br/>swarm_manager_2_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.17</span><span class="hljs-number">.88</span><br/>swarm_manager_3_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.224</span><span class="hljs-number">.202</span><br/>swarm_manager_3_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.29</span><span class="hljs-number">.23</span></strong>
</pre>
<p>All four resources were created, and we got the output of the manager public and private IPs.</p>
<p>Let's enter into one of the managers and confirm that the cluster indeed works:</p>
<pre>
<strong>ssh -i devops21-do \<br/>    root@$(terraform \<br/>    output swarm_manager_1_public_ip)<br/><br/>docker node ls</strong>
</pre>
<p>The output of the <kbd>node ls</kbd> command is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME         STATUS AVAILABILITY MANAGER STATUS<br/>swarm-manager-<span class="hljs-number">02</span> Ready  <span class="hljs-keyword">Active</span>       Reachable<br/>swarm-manager-<span class="hljs-number">01</span> Ready  <span class="hljs-keyword">Active</span>       Leader<br/>swarm-worker-<span class="hljs-number">02</span>  Ready  <span class="hljs-keyword">Active</span><br/>swarm-manager-<span class="hljs-number">03</span> Ready  <span class="hljs-keyword">Active</span>       Reachable<br/>swarm-worker-<span class="hljs-number">01</span>  Ready  <span class="hljs-keyword">Active</span></strong>
</pre>
<p>All the nodes are present, and the cluster seems to be working.</p>
<p>To be fully confident that everything works as expected, we'll deploy a few services. Those will be the same services we were creating throughout the book, so we'll save us some time and deploy the <kbd>vfarcic/docker-flow-proxy/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml</a>) and <kbd>vfarcic/go-demo/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml</a>) stacks:</p>
<pre>
<strong><span class="hljs-built_in">sudo</span> docker network create --driver overlay proxy<br/><br/>curl -o proxy-stack.yml \<br/>    https://raw.githubusercontent.com/\<br/>vfarcic/docker-flow-proxy/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c proxy-stack.yml proxy<br/><br/>curl -o go-demo-stack.yml \<br/>    https://raw.githubusercontent.com/\<br/>vfarcic/go-demo/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c go-demo-stack.yml go-demo</strong>
</pre>
<p>We downloaded the stacks from the repositories and executed the <kbd>stack deploy</kbd> commands.</p>
<p>All we have to do now is wait for a few moments, execute the <kbd>service ls</kbd> command, and confirm that all the replicas are running:</p>
<pre>
<strong>docker service ls</strong>
</pre>
<p>The output of the <kbd>service ls</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME           REPLICAS IMAGE                              COMMAND<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span>     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span><br/>proxy          <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span><br/>go<span class="hljs-attribute">-demo</span>        <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.2</span><br/>swarm<span class="hljs-attribute">-listener</span> <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-swarm</span><span class="hljs-attribute">-listener</span></strong>
</pre>
<p>Finally, let's send a request to the <kbd>go-demo</kbd> service through the <kbd>proxy</kbd>. If it returns the correct response, we'll know that everything works correctly:</p>
<pre>
<strong>curl -i localhost/demo/hello</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-status">HTTP/1.1 <span class="hljs-number">200</span> OK</span><br/><span class="hljs-attribute">Date</span>: <span class="hljs-string">Wed, 07 Dec 2016 06:21:01 GMT</span><br/><span class="hljs-attribute">Content-Length</span>: <span class="hljs-string">14</span><br/><span class="hljs-attribute">Content-Type</span>: <span class="hljs-string">text/plain; charset=utf-8</span><br/><br/><span class="erlang-repl"><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></span></strong>
</pre>
<p>It works!</p>
<p>Are we finished? We probably are. As a last check, let's validate that the <kbd>proxy</kbd> is accessible from outside the servers. We can confirm that by exiting the server and sending a request from our laptop:</p>
<pre>
<strong><span class="hljs-keyword">exit</span><br/><br/>curl -i $(terraform output \<br/>    swarm_manager_1_public_ip)/demo/hello</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-status">HTTP/1.1 <span class="hljs-number">200</span> OK</span><br/><span class="hljs-attribute">Date</span>: <span class="hljs-string">Wed, 07 Dec 2016 06:21:33 GMT</span><br/><span class="hljs-attribute">Content-Length</span>: <span class="hljs-string">14</span><br/><span class="hljs-attribute">Content-Type</span>: <span class="hljs-string">text/plain; charset=utf-8</span><br/><br/><span class="erlang-repl"><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></span></strong>
</pre>
<p>We are still missing floating IPs. While they are not necessary for this demo, we would create them if this were a production cluster, and use them to configure our DNSes.</p>
<p>This time, we can create the plan without specifying the targets:</p>
<pre>
<strong>terraform plan</strong>
</pre>
<p>The relevant parts of the output are as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>+ digitalocean_floating_ip.docker_1<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_floating_ip.docker_2<br/><span class="hljs-keyword">...</span><br/>+ digitalocean_floating_ip.docker_3<br/><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">3</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>As you can see, Terraform detected that all the resources except floating IPs are already created so it created the plan that would execute the creation of only three resources.</p>
<p>Let's apply the plan:</p>
<pre>
<strong>terraform apply</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>Apply complete! Resources: <span class="hljs-number">3</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">0</span> destroyed.<br/><br/><span class="hljs-keyword">...</span><br/><br/>Outputs:<br/><br/>floating_ip_1 = <span class="hljs-number">138.197</span><span class="hljs-number">.232</span><span class="hljs-number">.121</span><br/>floating_ip_2 = <span class="hljs-number">138.197</span><span class="hljs-number">.232</span><span class="hljs-number">.119</span><br/>floating_ip_3 = <span class="hljs-number">138.197</span><span class="hljs-number">.232</span><span class="hljs-number">.120</span><br/>swarm_manager_1_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.255</span><span class="hljs-number">.140</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.57</span><span class="hljs-number">.39</span><br/>swarm_manager_2_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.224</span><span class="hljs-number">.161</span><br/>swarm_manager_2_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.17</span><span class="hljs-number">.88</span><br/>swarm_manager_3_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.224</span><span class="hljs-number">.202</span><br/>swarm_manager_3_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.29</span><span class="hljs-number">.23</span></strong>
</pre>
<p>The floating IPs were created and we can see the output of their IPs.</p>
<p>The only thing left is to confirm that floating IPs are indeed created and configured correctly. We can do that by sending a request through one of them:</p>
<pre>
<strong>curl -i $(terraform output \<br/>    floating_ip_1)/demo/hello</strong>
</pre>
<p>As expected, the output is status <kbd>200 OK</kbd>:</p>
<pre>
<strong><span class="hljs-status">HTTP/1.1 <span class="hljs-number">200</span> OK</span><br/><span class="hljs-attribute">Date</span>: <span class="hljs-string">Wed, 07 Dec 2016 06:23:27 GMT</span><br/><span class="hljs-attribute">Content-Length</span>: <span class="hljs-string">14</span><br/><span class="hljs-attribute">Content-Type</span>: <span class="hljs-string">text/plain; charset=utf-8</span><br/><br/><span class="erlang-repl"><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></span></strong>
</pre>
<p>Let's see what happens if we simulate a failure of an instance.</p>
<p>We'll delete an instance using the DigitalOcean API. We could use Terraform to remove an instance. However, removing it with the API will be a closer simulation of an unexpected failure of a node.</p>
<p>To remove an instance, we need to find its ID. We can do that with the <kbd>terraform show</kbd> command.</p>
<p>Let's say that we want to remove the second worker. The command to find all its information is as follows:</p>
<pre>
<strong>terraform state show <span class="hljs-string">"digitalocean_droplet.swarm-worker[1]"</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-constant">id</span>                   = 33909722<br/><span class="hljs-constant">disk</span>                 = 30<br/><span class="hljs-constant">image</span>                = 21373017<br/><span class="hljs-constant">ipv4_address</span>         = 138.68.57.13<br/><span class="hljs-constant">ipv4_address_private</span> = 10.138.224.209<br/><span class="hljs-constant">locked</span>               = false<br/><span class="hljs-constant">name</span>                 = swarm-worker-02<br/><span class="hljs-constant">private_networking</span>   = true<br/><span class="hljs-constant">region</span>               = sfo2<br/><span class="hljs-constant">resize_disk</span>          = true<br/><span class="hljs-constant">size</span>                 = 1gb<br/><span class="hljs-constant">ssh_keys</span>.#           = 1<br/>ssh_keys.0           = 5080274<br/><span class="hljs-constant">status</span>               = active<br/><span class="hljs-constant">tags</span>.#               = 0<br/><span class="hljs-constant">vcpus</span>                = 1</strong>
</pre>
<p>Among other pieces of data, we got the ID. In my case, it is <kbd>33909722</kbd>.</p>
<p>Before running the command that follows, please change the ID to the one you got from the <kbd>terraform state show</kbd> command:</p>
<pre>
<strong>curl -i -X DELETE \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_TOKEN</span>"</span> \<br/><span class="hljs-string">"https://api.digitalocean.com/v2/droplets/33909722"</span></strong>
</pre>
<p>The relevant part of the output is as follows:</p>
<pre>
<strong>HTTP/<span class="hljs-number">1.1</span> <span class="hljs-number">204</span> No Content<br/><span class="hljs-keyword">...</span></strong>
</pre>
<p>DigitalOcean does not provide any response content to <kbd>DELETE</kbd> requests, so the status <kbd>204</kbd> indicates that the operation was successful.</p>
<p>It will take a couple of moments until the droplet is removed entirely.</p>
<p>Let's run the <kbd>terraform plan</kbd> command one more time:</p>
<pre>
<strong>terraform plan</strong>
</pre>
<p>The relevant parts of the output are as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>+ digitalocean_droplet.swarm-worker.1<br/><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">1</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>Terraform deduced that one resource <kbd>swarm-worker.1</kbd> needs to be added to reconcile the discrepancy between the state it has stored locally and the actual state of the cluster.</p>
<p>All we have to do to restore the cluster to the desirable state is to run <kbd>terraform apply</kbd>:</p>
<pre>
<strong>terraform apply</strong>
</pre>
<p>The relevant parts of the output are as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>Apply complete! Resources: <span class="hljs-number">1</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">0</span> destroyed.<br/><br/><span class="hljs-keyword">...</span><br/><br/>Outputs:<br/><br/>floating_ip_1 = <span class="hljs-number">138.197</span><span class="hljs-number">.232</span><span class="hljs-number">.121</span><br/>floating_ip_2 = <span class="hljs-number">138.197</span><span class="hljs-number">.232</span><span class="hljs-number">.119</span><br/>floating_ip_3 = <span class="hljs-number">138.197</span><span class="hljs-number">.232</span><span class="hljs-number">.120</span><br/>swarm_manager_1_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.255</span><span class="hljs-number">.140</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.57</span><span class="hljs-number">.39</span><br/>swarm_manager_2_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.224</span><span class="hljs-number">.161</span><br/>swarm_manager_2_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.17</span><span class="hljs-number">.88</span><br/>swarm_manager_3_private_ip = <span class="hljs-number">10.138</span><span class="hljs-number">.224</span><span class="hljs-number">.202</span><br/>swarm_manager_3_public_ip = <span class="hljs-number">138.68</span><span class="hljs-number">.29</span><span class="hljs-number">.23</span></strong>
</pre>
<p>We can see that one resource was added. The terminated worker has been recreated, and the cluster continues operating at its full capacity.</p>
<p>The state of the cluster is stored in the <kbd>terraform.tfstate</kbd> file. If you are not running it always from the same computer, you might want to store that file in your repository together with the rest of your configuration files. The alternative is to use Remote State (<a href="https://www.terraform.io/docs/state/remote/index.html" target="_blank">https://www.terraform.io/docs/state/remote/index.html</a>) and, for example, store it in Consul.</p>
<p>Changing the desired state of the cluster is easy as well. All we have to to is add more resources and rerun <kbd>terraform apply</kbd>.</p>
<p>We are finished with the brief introduction to Terraform for DigitalOcean.</p>
<p>The flow of the process we executed can be described through <em>Figure 12-4</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="281" src="assets/cloud-architecture-instances-1.png" width="489"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12-4: The flow of the Terraform process</div>
<p>Let's destroy what we did before we compare the different approaches we took to create and manage a Swarm cluster in DigitalOcean:</p>
<pre>
<strong>terraform destroy -force</strong>
</pre>
<p>The last line of the output is as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>Destroy complete! Resources: <span class="hljs-number">9</span> destroyed.</strong>
</pre>
<p>The cluster is gone as if it never existed, saving us from unnecessary expenses.</p>
<p>Let's see how to remove a snapshot.</p>
<p>Before we remove the snapshot we created, we need to find its <kbd>ID</kbd>.<br/>
The request that will return the list of all snapshots is as follows:</p>
<pre>
<strong>curl -X GET \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>"</span> \<br/><span class="hljs-string">"https://api.digitalocean.com/v2/snapshots?resource_type=droplet"</span> \<br/>    | jq <span class="hljs-string">'.'</span></strong>
</pre>
<p>The output of the response is as follows:</p>
<pre>
<strong>{<br/>  "<span class="hljs-attribute">snapshots</span>": <span class="hljs-value">[<br/>    {<br/>      "<span class="hljs-attribute">id</span>": <span class="hljs-value"><span class="hljs-string">"21373017"</span></span>,<br/>      "<span class="hljs-attribute">name</span>": <span class="hljs-value"><span class="hljs-string">"devops21-1481087268"</span></span>,<br/>      "<span class="hljs-attribute">regions</span>": <span class="hljs-value">[<br/><span class="hljs-string">"sfo2"</span><br/>      ]</span>,<br/>      "<span class="hljs-attribute">created_at</span>": <span class="hljs-value"><span class="hljs-string">"2016-12-07T05:11:05Z"</span></span>,<br/>      "<span class="hljs-attribute">resource_id</span>": <span class="hljs-value"><span class="hljs-string">"33907398"</span></span>,<br/>      "<span class="hljs-attribute">resource_type</span>": <span class="hljs-value"><span class="hljs-string">"droplet"</span></span>,<br/>      "<span class="hljs-attribute">min_disk_size</span>": <span class="hljs-value"><span class="hljs-number">30</span></span>,<br/>      "<span class="hljs-attribute">size_gigabytes</span>": <span class="hljs-value"><span class="hljs-number">1.32</span><br/></span>}<br/>  ]</span>,<br/>  "<span class="hljs-attribute">links</span>": <span class="hljs-value">{}</span>,<br/>  "<span class="hljs-attribute">meta</span>": <span class="hljs-value">{<br/>    "<span class="hljs-attribute">total</span>": <span class="hljs-value"><span class="hljs-number">1</span><br/></span>}<br/></span>}</strong>
</pre>
<p>We’ll use <kbd>jq</kbd> to get the snapshot ID:</p>
<pre>
<strong>SNAPSHOT_ID=$(curl -X GET \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>"</span> \<br/><span class="hljs-string">"https://api.digitalocean.com/v2/snapshots?resource_type=droplet"</span> \<br/>    | jq -r <span class="hljs-string">'.snapshots[].id'</span>)</strong>
</pre>
<p>We sent an <kbd>HTTP GET</kbd> request to retrieve all snapshots and used <kbd>jq</kbd> to retrieve only the ID. The result was stored in the environment variable <kbd>SNAPSHOT_ID</kbd>.</p>
<p>Now we can send a <kbd>DELETE</kbd> request that will remove the snapshot:</p>
<pre>
<strong>curl -X DELETE \<br/>    -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$DIGITALOCEAN_ACCESS_TOKEN</span>"</span> \<br/><span class="hljs-string">"https://api.digitalocean.com/v2/snapshots/<span class="hljs-variable">$SNAPSHOT_ID</span>"</span></strong>
</pre>
<p>The relevant output of the response is as follows:</p>
<pre>
<strong>HTTP/<span class="hljs-number">1.1</span> <span class="hljs-number">204</span> No Content<br/><span class="hljs-keyword">...</span></strong>
</pre>
<p>The snapshot has been removed. No resources are running on the DigitalOcean account, and you will not be charged anything more than what you spent from running the exercises in this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing the right tools to create and manage Swarm clusters in DigitalOcean</h1>
            </header>

            <article>
                
<p>We tried two different combinations to create a Swarm cluster in DigitalOcean. We used <em>Docker Machine</em> with the <em>DigitalOcean API</em> and <em>Packer</em> with <em>Terraform</em>. That is, by no means, the final list of the tools we can use. The time is limited, and I promised to myself that this book will be shorter than <em>War and Peace</em>, so I had to draw the line somewhere. Those two combinations are, in my opinion, the best candidates as your tools of choice. Even if you do choose something else, this chapter, hopefully, gave you an insight into the direction you might want to take.</p>
<p>Most likely you won't use both combinations so the million dollar question is which one should it be?</p>
<p>Only you can answer that question. Now you have the practical experience that should be combined with the knowledge of what you want to accomplish. Each use case is different, and no combination would be the best fit for everyone.</p>
<p>Nevertheless, I will provide a brief overview and some of the use-cases that might be a good fit for each combination.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">To Docker Machine or not to Docker Machine?</h1>
            </header>

            <article>
                
<p>Docker Machine is the weaker solution we explored. It is based on ad-hoc commands and provides little more than a way to create droplets and install Docker Engine. It uses <em>Ubuntu 15.10</em> as the base snapshot. Not only that it is old but is a temporary release. If we choose to use Ubuntu, the correct choice is 16.04 <strong>Long Term Support </strong>(<strong>LTS</strong>).</p>
<p>If Docker Machine would, at least, provide the minimum setup for Swarm Mode (as it did with the old Standalone Swarm), it could be a good choice for a small cluster.</p>
<p>As it is now, the only true benefit Docker Machine provides when working with a Swarm cluster in DigitalOcean is Docker Engine installation on a remote node and the ability to use the <kbd>docker-machine env</kbd> command to make our local Docker client seamlessly communicate with the remote cluster. Docker Engine installation is simple so that alone is not enough. On the other hand, <kbd>docker-machine env</kbd> command should not be used in a production environment. Both benefits are too weak.</p>
<p>Many of the current problems with Docker Machine can be fixed with some extra arguments (For example, <kbd>--digitalocean-image</kbd>) and in combination with other tools. However, that only diminishes the primary benefit behind Docker Machine. It was supposed to be simple and work out of the box. That was partly true before Docker 1.12. Now, at least in DigitalOcean, it is lagging behind.</p>
<p>Does that mean we should discard Docker Machine when working with DigitalOcean? Not always. It is still useful when we want to create an ad-hoc cluster for demo purposes or maybe experiment with some new features. Also, if you don't want to spend time learning other tools and just want something you're familiar with, Docker Machine might be the right choice. I doubt that's your case. The fact that you reached this far in this book tells me that you do want to explore better ways of managing a cluster.</p>
<p>The final recommendation is to keep Docker Machine as the tool of choice when you want to simulate a Swarm cluster locally as we did in the previous chapters. There are better choices for DigitalOcean.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">To Terraform or not to Terraform?</h1>
            </header>

            <article>
                
<p>Terraform, when combined with Packer, is an excellent choice. HashiCorp managed to make yet another tool that changes the way we configure and provision servers.</p>
<p>Configuration management tools have as their primary objective the task of making a server always be in the desired state. If a web server stops, it will be started again. If a configuration file is changed, it will be restored. No matter what happens to a server, its desired state will be restored. Except, when there is no fix to the issue. If a hard disk fails, there's nothing configuration management can do.</p>
<p>The problem with configuration management tools is that they were designed to work with physical, not virtual servers. Why would we fix a faulty virtual server when we can create a new one in a matter of seconds? Terraform understands how cloud computing works better than anyone and embraces the idea that our servers are not pets anymore. They are cattle. It'll make sure that all your resources are available.</p>
<p>When something is wrong on a server, it will not try to fix it. Instead, it will destroy it and create a new one based on the image we choose.</p>
<p>Does that mean that there is no place for Puppet, Chef, Ansible, and other similar tools? Are they obsolete when operating in the cloud? Some are more outdated than others. Puppet and Chef are designed to run an agent on each server, continuously monitoring its state and modifying it if things go astray. There is no place for such tools when we start treating our servers as cattle. Ansible is in a bit better position since it is more useful than others, as a tool designed to configure a server instead of monitor it. As such, it could be very helpful when creating images.</p>
<p>We can combine Ansible with Packer. Packer would create a new VM, Ansible would provision that VM with everything we need, and leave it to Packer to create an image out of it. If the server setup is complicated, that makes a lot of sense. We don't create a lot of system users since we do not log into a machine to deploy software. Swarm does that for us. We do not install web servers and runtime dependencies anymore. They are inside containers. Is there a true benefit from using configuration management tools to install a few things into VMs that will be converted into images? More often than not, the answer is no. The few things we need can be just as easily installed and configured with a few Shell commands. Configuration management of our cattle can, and often should, be done with bash.</p>
<p>I might have been too harsh. Ansible is still a great tool if you know when to use it and for what purpose. If you prefer it over bash to install and configure a server before it becomes an image, go for it. If you try to use it to control your nodes and create DigitalOcean resources, you're on a wrong path. Terraform does that much better. If you think that it is better to provision a running node instead of instantiating images that already have everything inside, you must have much more patience than I do.</p>
<p>The final recommendation is to use <em>Terraform</em> with <em>Packer</em> if you want to have control of all the pieces that constitute your cluster, or if you already have a set of rules that need to be followed. Be ready to spend some time tuning the configs until you reach the optimum setup. Unlike AWS that, for good or bad, forces us to deal with many types of resources, DigitalOcean is simple. You create droplets, add a few floating IPs, and that's it. You might want to install a firewall on the machines. If you do, the best way would be to do that during the creation of a snapshot with Packer. It is questionable whether a firewall is needed when using Swarm networking, but that's a discussion for some other time.</p>
<p>Since there is no such thing as <em>Docker for DigitalOcean</em>, Terraform is a clear winner. DigitalOcean is simple, and that simplicity is reflected through Terraform configuration.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The final verdict</h1>
            </header>

            <article>
                
<p>Terraform wins over Docker Machine in a landsilde. If there is such a thing as <em>Docker for DigitalOcean</em>, this discussion would be longer. As it is now, the choice is easy. If you choose DigitalOcean, manage your cluster with Packer and Terraform.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">To DigitalOcean or not to DigitalOcean</h1>
            </header>

            <article>
                
<p>Generally speaking, I like products and services that are focused on very few things and do them well. DigitalOcean is one of those. It is an <strong>Infrastructure as a Service</strong>(<strong>IaaS</strong>) provider and nothing else. The services it provides are few in number (For example, floating IP) and limited to those that are a real necessity. If you're looking for a provider that will offer you everything you can imagine, choose <strong>Amazon Web Services</strong>(<strong>AWS</strong>), Azure, GCE, or any other cloud computing provider that aims at delivering not only hosting but also numerous services on top. The fact that you reached this far in this book tells me that there are strong chances that you are interested in setting up infrastructure services yourself. If that's the case, DigitalOcean is worth a try. Doing everything often means not doing anything really well. DigitalOcean does a few things, and it does them well. What is does, it does better than most.</p>
<p>The real question is whether you need only an <strong>Infrastructure as a Service </strong>(<strong>IaaS</strong>) provider or you need <strong>Platform as a Service </strong>(<strong>PaaS</strong>) as well. In my opinion, containers make PaaS obsolete. It will be gradually replaced with containers managed by schedulers (For example, <em>Docker Swarm</em>) or <strong>Containers as a Service </strong>(<strong>CaaS</strong>). You might not agree with me. If you do, a huge part of AWS becomes obsolete leaving it with EC2, storage, VPCs, and only a handful of other services. In such a case, DigitalOcean is a mighty competitor and an excellent choice. The few things it does, it does better than AWS at a lower price. The performance is impressive. It's enough to measure the time it requires to create a droplet and compare it with the time AWS requires to create and initiate an EC2 instance. The difference is huge. The first time I created a droplet, I thought that there's something wrong. My brain could not comprehend that it could be done in less than a minute.</p>
<p>Did I mention simplicity? DigitalOcean is simple, and I love simplicity. Therefore, the logical conclusion is that I love DigitalOcean. The real mastery is to make complex things easy to use. That's where both Docker and DigitalOcean shine.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>