- en: Defining Cluster-Wide Alerts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common mistake is to focus on dashboards as the primary means of noticing
    when something is wrong. Dashboards have their place in the big scheme of things
    and are an indispensable part of any monitoring solution. However, they are not
    as critical as we think.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring systems are not meant to be a substitute for Netflix. They are not
    supposed to be watched. Instead, they should collect data and, if certain conditions
    are met, create alerts. Those alerts should try to communicate with the system
    and trigger a set of actions that will correct the problem automatically. Notifications
    should be sent to humans only if the system does not know how to fix the issue.
    In other words, we should strive to create a self-healing system that consults
    doctors (us, humans) only when it cannot fix itself.
  prefs: []
  type: TYPE_NORMAL
- en: Dashboards come in handy when we know that there is a problem. If the system
    is working as expected, looking at dashboards is a waste of time that could be
    better spent on improving the system.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a Slack notification that would say that “there is no available memory
    in the cluster and the system failed to create additional VMs.” Please notice
    the second part of that sentence. The system detected a problem and failed to
    correct it. Something went wrong, and it could not scale up. It failed to create
    new VMs. That was a good example of a type of notification that should be sent
    to a human operator. If the system managed to heal itself, there would be no need
    for that Slack notification.
  prefs: []
  type: TYPE_NORMAL
- en: We should consult dashboards only after we receive a message stating that the
    system failed to heal itself. Until that moment, everything is fine, and we can
    work on the next big improvement of the system. After receiving the message, we
    should visit a dashboard or two. We should try to get the high-level picture of
    the system. Sometimes, information from a dashboard is all we need. More often
    than not, we need more. We need to visit Prometheus and start querying it for
    additional information. Finally, once the culprit is found, we can create a fix,
    test it, employ it in production, improve the self-healing system so that the
    problem is fixed automatically the next time it happens, and write “post-mortem”
    report.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it all starts with a single alert, and that will be the focus
    of this chapter. For now, we will not distinguish alerts fired to a system that
    will auto-correct itself from those that are sending notifications to human operators.
    That will come later. For now, we’ll focus on creating alerts without defining
    events they should fire.
  prefs: []
  type: TYPE_NORMAL
- en: Creating The Cluster And Deploying Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll start by recreating the cluster and deploying the stacks that we used
    in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
