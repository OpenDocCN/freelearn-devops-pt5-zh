- en: OpenShift HA Architecture Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 高可用性架构概述
- en: In the previous chapter, we introduced you to CI/CD, Jenkins, OpenShift pipelines,
    and Jenkins integration with OpenShift. We also illustrated how to create a sample
    CI/CD pipeline in OpenShift, how to edit pipelines, and how to manage pipeline
    execution.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 CI/CD、Jenkins、OpenShift 管道以及 Jenkins 与 OpenShift 的集成。我们还演示了如何在 OpenShift
    中创建一个示例 CI/CD 管道，如何编辑管道以及如何管理管道执行。
- en: In this chapter, we will briefly touch on **high availability** (**HA**) in
    general, and will then focus on OpenShift HA. We will discuss how OpenShift provides
    redundancy in the case of a failure, and how you can prevent this from happening
    by properly designing your OpenShift cluster. At the end of this chapter, we will
    discuss how to back up and restore OpenShift cluster data in case something goes
    wrong.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将简要介绍**高可用性**（**HA**）的一般概念，然后专注于 OpenShift HA。我们将讨论 OpenShift 如何在发生故障时提供冗余，并讨论如何通过正确设计你的
    OpenShift 集群来避免故障发生。在本章结束时，我们将讨论如何在出现问题时备份和恢复 OpenShift 集群数据。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: What is high availability?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是高可用性？
- en: HA in OpenShift
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 中的高可用性（HA）
- en: OpenShift backup and restore
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 备份与恢复
- en: What is high availability?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是高可用性？
- en: HA is a very important topic when it comes to real customers and real money.
    We all work for different businesses, and the last thing that any business wants
    is to have an outage. This is a problem that can cause people to lose their jobs
    and companies to go bankrupt. It has happened, and it will continue to happen.
    But if you plan your HA design properly and implement it in the right way, you
    will have a better chance of keeping your job and maintaining your company's good
    reputation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及真实客户和真实金钱时，高可用性是一个非常重要的话题。我们每个人都为不同的企业工作，而任何企业最不希望发生的事情就是出现停机。这是一个可能导致员工失业和公司破产的问题。这样的事情已经发生过，并且会继续发生。但如果你正确地规划
    HA 设计并以正确的方式实施，你将有更好的机会保住工作，并维护公司良好的声誉。
- en: HA usually refers to a concept or strategy to keep a system up and running for
    a long time. That's where the terms *high* and *availability* come together. When
    people ask, *Does it support HA?*, they are usually asking whether the system
    is redundant, and whether it stays up and running if something goes wrong. In
    order to provide HA, each and every component of the system needs to be fault
    tolerant, and all of the lower- and upper-level components and protocols must
    be highly available. For example, if you have OpenShift designed and implemented
    in HA mode, but your network has a single point of failure, your application will
    stop working. So, it is critical to plan properly, and to make sure that your
    application stays up and running, no matter where a failure occurs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性通常指的是一种策略或概念，旨在使系统保持长时间运行。这就是*高*和*可用性*这两个术语结合的地方。当人们问，*它支持 HA 吗？*时，他们通常是在问系统是否具备冗余能力，并且在出现故障时是否能够持续运行。为了提供
    HA，系统的每一个组件都需要具备容错能力，所有下层和上层组件及协议都必须具有高可用性。例如，如果你设计并实施了 OpenShift HA 模式，但你的网络存在单点故障，那么你的应用程序将停止工作。因此，正确规划是至关重要的，确保无论故障发生在哪里，你的应用程序都能持续运行。
- en: HA in OpenShift
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 中的高可用性（HA）
- en: In the previous chapters, we ran our applications on a single node, or sometimes,
    two nodes. Some might say that if there is more than one OpenShift node in the
    cluster, it is considered a redundant configuration, but that is far from true.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们将应用程序运行在单个节点，或者有时是在两个节点上。有些人可能会说，如果集群中有超过一个 OpenShift 节点，那就算是冗余配置，但这远非事实。
- en: 'If we compare standard OpenShift architecture and OpenShift HA, you will see
    some differences between them:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较标准的 OpenShift 架构和 OpenShift HA，你会看到它们之间的一些差异：
- en: '![](img/00090.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00090.jpeg)'
- en: OpenShift classic architecture
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 经典架构
- en: 'Here we have nodes, masters, storage, and a routing layer consisting of infra
    nodes. OpenShift HA architecture is quite similar but has one distinct difference—in
    the routing layer we have load balances that make the overall solution always
    accessible. All other components are redundant by nature:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有节点、主控、存储和由 infra 节点组成的路由层。OpenShift HA 架构非常相似，但有一个明显的区别——在路由层，我们有负载均衡器，使得整体解决方案始终可访问。所有其他组件天生是冗余的：
- en: '![](img/00091.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.jpeg)'
- en: OpenShift HA architecture
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 高可用性架构
- en: Virtual IPs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟 IP
- en: 'We can see that in OpenShift HA, we have what''s called an enterprise load
    balancer, with two **Virtual IPs** (**VIPs**). We need one VIP for traffic to
    master nodes, and another VIP for traffic to actual OpenShift applications, running
    on OpenShift nodes within pods:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在 OpenShift 高可用性中，我们使用了所谓的企业级负载均衡器，并配置了两个**虚拟 IP（VIPs）**。我们需要一个 VIP 用于主节点的流量，另一个
    VIP 用于实际的 OpenShift 应用程序流量，这些应用程序运行在 OpenShift 节点内的 Pod 中：
- en: '![](img/00092.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.jpeg)'
- en: OpenShift with external load balancers
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 配备外部负载均衡器的 OpenShift
- en: Why can't we just use DNS load balancing? The reason is, if we use DNS load
    balancing and one of the masters or nodes goes down, some traffic will still keep
    flowing to the failed node. Load balancing allows for implementing health checks
    and stops routing the traffic to the failed endpoint. For example, if one of the
    infra nodes fails, the load balancer will detect the failure, remove that node
    from the server pool, and stop sending traffic to the node. When the node comes
    back up, the load balancer will detect that, and will start load balancing traffic
    to the node. So, having VIPs is essential for OpenShift to be highly available
    from the outside.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不能只使用 DNS 负载均衡呢？原因是，如果我们使用 DNS 负载均衡，且某个 master 或节点发生故障，仍然会有一部分流量流向故障节点。负载均衡可以实现健康检查，停止将流量路由到故障端点。例如，如果其中一个
    infra 节点发生故障，负载均衡器会检测到故障，移除该节点的服务器池，并停止向该节点发送流量。当该节点恢复时，负载均衡器会检测到这一点，并开始将流量分配到该节点。因此，拥有
    VIP（虚拟 IP）对 OpenShift 外部高可用性至关重要。
- en: '| **Node name** | **Physical IP address** | **Virtual IP address** | **DNS**
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **节点名称** | **物理 IP 地址** | **虚拟 IP 地址** | **DNS** |'
- en: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
- en: '| `Infra2` | `10.0.0.2/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `Infra2` | `10.0.0.2/24` | `10.0.0.11` | `*.apps.osp.com` |'
- en: '| `Infra3` | `10.0.0.3/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `Infra3` | `10.0.0.3/24` | `10.0.0.11` | `*.apps.osp.com` |'
- en: '| `Master1` | `10.0.1.4/24` | `10.0.0.14` | `console.osp.com` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `Master1` | `10.0.1.4/24` | `10.0.0.14` | `console.osp.com` |'
- en: '| `Master2` | `10.0.1.5/24` | `10.0.0.14` | `console.osp.com` |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `Master2` | `10.0.1.5/24` | `10.0.0.14` | `console.osp.com` |'
- en: '| `Master3` | `10.0.1.6/24` | `10.0.0.14` | `console.osp.com`  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `Master3` | `10.0.1.6/24` | `10.0.0.14` | `console.osp.com` |'
- en: Using an external load balancer is an ideal option when building OpenShift HA,
    because an external load balancer automatically detects a failure of any OpenShift
    infra or master node and distributes the load among the other nodes available.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用外部负载均衡器是构建 OpenShift 高可用性的理想选择，因为外部负载均衡器能够自动检测任何 OpenShift infra 或 master
    节点的故障，并将负载分配到其他可用的节点上。
- en: Let's suppose that we have three infra nodes, all serving the traffic at a speed
    of 50 Mbps. If the `Infra1` node fails, then the external load balancer automatically
    detects the failure and stops serving traffic to the `Infra1` node. So, there
    will be no downtime for both end users and applications, and the load balancer
    will automatically distribute the load between `Infra2` and `Infra3`, so both
    nodes will end up serving the traffic at a speed of 75 Mbps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有三个 infra 节点，每个节点的流量处理速度为 50 Mbps。如果 `Infra1` 节点发生故障，外部负载均衡器会自动检测到故障，并停止向
    `Infra1` 节点提供流量。因此，无论是最终用户还是应用程序都不会出现停机时间，负载均衡器会自动将负载分配到 `Infra2` 和 `Infra3`，这样两个节点的流量处理速度都会提升至
    75 Mbps。
- en: The downside of this scenario is that we have to use external load balancers,
    take care of their HA, implement additional health checks, and employ further
    configurations. And, if we are using commercial load balancer appliances from
    F5 or A10, they are going to be very expensive, as well. However, this is the
    most scalable solution that makes sure that OpenShift cluster is always accessible
    from the outside.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方案的缺点是，我们必须使用外部负载均衡器，确保它们的高可用性，实施额外的健康检查，并进行进一步的配置。此外，如果使用 F5 或 A10 等商业负载均衡设备，它们的成本也非常高。然而，这仍然是确保
    OpenShift 集群始终能够从外部访问的最具扩展性的解决方案。
- en: IP failover
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IP 故障转移
- en: 'Another way to ensure that your OpenShift applications are always available
    from the outside is to implement IP failover mechanisms. This method is useful
    when you do not have an external load balancer, but still want OpenShift to always
    be accessible from the outside. The OpenShift IP failover design primarily relies
    on two different technologies:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的 OpenShift 应用程序始终可以从外部访问的另一种方式是实现 IP 故障转移机制。当没有外部负载均衡器时，但你仍希望 OpenShift
    始终能够从外部访问时，这种方法非常有用。OpenShift 的 IP 故障转移设计主要依赖于两种不同的技术：
- en: '**Keepalived**: Provides high availability of VIPs across OpenShift infra nodes.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keepalived**: 提供 OpenShift infra 节点之间的 VIP 高可用性。'
- en: '**DNS**: Manages external traffic load balancing.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DNS**: 管理外部流量负载均衡。'
- en: '![](img/00093.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpeg)'
- en: OpenShift DNS and keepalived
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift DNS 和 keepalived
- en: 'In our example, keepalived separately manages several VIPs on master and infrastructure
    nodes. Two DNS mappings are used to load balance the traffic between the VIPs
    of the OpenShift infra and master nodes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，keepalived 分别管理主节点和基础设施节点上的多个 VIP。使用两个 DNS 映射来负载均衡 OpenShift infra 节点和主节点之间的流量：
- en: '| **Node name** | **Physical IP address** | **Virtual IP address** | **DNS**
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **节点名称** | **物理 IP 地址** | **虚拟 IP 地址** | **DNS** |'
- en: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
- en: '| `Infra2` | `10.0.0.2/24` | `10.0.0.12` | `*.apps.osp.com` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `Infra2` | `10.0.0.2/24` | `10.0.0.12` | `*.apps.osp.com` |'
- en: '| `Infra3` | `10.0.0.3/24` | `10.0.0.13` | `*.apps.osp.com` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `Infra3` | `10.0.0.3/24` | `10.0.0.13` | `*.apps.osp.com` |'
- en: '| `Master1` | `20.0.0.1/24` | `20.0.0.11` | `console.osp.com` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `Master1` | `20.0.0.1/24` | `20.0.0.11` | `console.osp.com` |'
- en: '| `Master2` | `20.0.0.2/24` | `20.0.0.12` | `console.osp.com` |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `Master2` | `20.0.0.2/24` | `20.0.0.12` | `console.osp.com` |'
- en: '| `Master3` | `20.0.0.3/24` | `20.0.0.13` | `console.osp.com` |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `Master3` | `20.0.0.3/24` | `20.0.0.13` | `console.osp.com` |'
- en: In the preceding example, we do not need an external load balancer, and, if
    one of the OpenShift nodes goes down, the Virtual IP will automatically be moved
    to another node. Depending on how we configure preemptive options, the Virtual
    IP may come back to an infra node if a failed info node recovers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们不需要外部负载均衡器，如果其中一个 OpenShift 节点出现故障，虚拟 IP 会自动转移到另一个节点。根据我们配置的抢占选项，虚拟
    IP 可能会在故障的 infra 节点恢复后重新回到该节点。
- en: There is a downside to this solution, which may or may not be critical for your
    particular case. Let's suppose that we have three infra nodes, all serving the
    traffic at a speed of 50 Mbps. If one of the nodes fails, then the VIP from `Infra1`
    will be moved to `Infra2`. There will be no interruption for end users or applications,
    but `Infra2` will now serve the traffic at a speed of 100 Mbps, while `Infra3`
    is still doing it at 50 Mbps.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案有一个缺点，这个缺点对于你的特定情况可能是关键，也可能不是。假设我们有三个 infra 节点，每个节点的流量处理速度为 50 Mbps。如果其中一个节点发生故障，那么来自
    `Infra1` 的 VIP 会被转移到 `Infra2`。对最终用户或应用程序来说，不会有中断，但 `Infra2` 将以 100 Mbps 的速度处理流量，而
    `Infra3` 仍然以 50 Mbps 的速度处理流量。
- en: This is OK when the workload is not too high, but if there is too much traffic,
    it may cause issues by overloading `Infra2`. So, you have to vet out all possible
    scenarios; by solving one particular failure scenario, you may create a new one.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作负载不太高时，这是可以的，但如果流量过大，可能会通过过载 `Infra2` 引发问题。因此，你必须排查所有可能的场景；通过解决一个特定的故障场景，你可能会创造一个新的问题。
- en: There are other methods for making your OpenShift cluster available externally.
    Discussing methods like DNS LB, GSLB, custom scripts, or even manual switchover,
    could easily extend this book by a thousand pages. We have focused on the methods
    that are proven to work and are supported by OpenShift.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法可以让你的 OpenShift 集群在外部可用。讨论像 DNS LB、GSLB、自定义脚本甚至手动切换等方法，可能会让这本书扩展到一千页。我们专注于那些经过验证并且
    OpenShift 支持的方法。
- en: OpenShift infrastructure nodes
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 基础设施节点
- en: 'OpenShift infrastructure nodes are labeled with infra, by default. They run
    on two main OpenShift components:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 基础设施节点默认标记为 infra。它们运行两个主要的 OpenShift 组件：
- en: OpenShift internal registry
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 内部注册表
- en: OpenShift router
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 路由器
- en: An Openshift infrastructure node is the easiest to install, operate, and troubleshoot,
    because the structure of an OpenShift infra node is simple, stable, and predictable.
    Infra nodes are usually installed in HA mode as a part of the initial OpenShift
    installation; they are rarely modified. The only time you might work with infra
    nodes directly is when you have a lot of traffic going through infra nodes and
    they can't handle it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 基础设施节点最容易安装、操作和排除故障，因为 OpenShift 基础设施节点的结构简单、稳定且可预测。基础设施节点通常以高可用模式安装，是
    OpenShift 初始安装的一部分；它们很少被修改。你唯一可能直接处理基础设施节点的情况是当大量流量通过基础设施节点时，而它们无法处理这些流量。
- en: 'But, by the time you run into a situation like that, you will have much bigger
    problems than just scaling the number of infra nodes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当你遇到这种情况时，你将面临比仅仅扩展 infra 节点数量更严重的问题：
- en: '![](img/00094.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.jpeg)'
- en: OpenShift infra nodes
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift infra 节点
- en: The best practice is to have a minimum of three infra nodes installed, with
    the pod anti-affinity feature enabled for both the registry and router. You need
    to have anti-affinity enabled, because you can run into a situation where you
    lose one infrastructure node and the new router pod start a on the node that is
    already running an OpenShift router. So if you have only two infra nodes, without
    the pod anti-affinity feature enabled, in the case of a failure, you will have
    two routers and two registries running on the same infra node, listening on the
    same ports. The pod anti-affinity rule prevents one pod from running with another
    pod on the same host, thus preventing two registries (or two routers) from running
    on the same OpenShift infra node.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是安装至少三个基础设施节点，并为注册表和路由器启用 Pod 反亲和性特性。你需要启用反亲和性功能，因为如果你丢失了一个基础设施节点，并且新的路由器
    Pod 启动在已经运行 OpenShift 路由器的节点上，你就会遇到问题。如果你只有两个基础设施节点，并且没有启用 Pod 反亲和性功能，在发生故障时，两个路由器和两个注册表可能会运行在同一个基础设施节点上，并监听相同的端口。Pod
    反亲和性规则防止一个 Pod 与另一个 Pod 在同一主机上运行，从而避免两个注册表（或两个路由器）在同一个 OpenShift 基础设施节点上运行。
- en: OpenShift masters
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 主节点
- en: OpenShift masters are control plane nodes and the central control points for
    OpenShift solutions. In earlier releases, you had to install an OpenShift master
    using Pacemaker (to provide failover), but in recent releases, this is taken care
    of by keepalived and external storage. If you ever have an OpenShift master fail
    completely, you can just delete the node and reinstall it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 主节点是控制平面节点，是 OpenShift 解决方案的核心控制点。在早期版本中，你需要使用 Pacemaker 安装 OpenShift
    主节点（以提供故障转移），但在最新版本中，这由 keepalived 和外部存储来处理。如果你的 OpenShift 主节点完全故障，你可以直接删除该节点并重新安装。
- en: In order to remove a node from your OpenShift cluster, you can use the `oc delete
    node` command, and then run `scaleup.yml` from the `openshift-ansible` Git project.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从 OpenShift 集群中移除一个节点，你可以使用 `oc delete node` 命令，然后从 `openshift-ansible` Git
    项目中运行 `scaleup.yml`。
- en: The `openshift-ansible` project is available at [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`openshift-ansible` 项目可在 [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible)
    上找到。'
- en: The `scaleup.yml` file is located in `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`,
    once you have downloaded the `openshift-ansible` project.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleup.yml` 文件位于 `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`，当你下载了
    `openshift-ansible` 项目后可以找到该文件。'
- en: You will be required to adjust your Ansible inventory file and add a new node
    under the `[new_masters]` section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要调整你的 Ansible 库存文件，并在 `[new_masters]` 部分添加一个新节点。
- en: If, at some point, you lose all OpenShift masters, it will not impact your end
    users, and customer-to-application traffic will keep flowing; however, you won't
    be able to make any new changes to the OpenShift cluster. At that point, there
    is not much that you can do, other than restore OpenShift masters from the last
    backup. We will discuss OpenShift backup and restore later in this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在某些时候，你失去了所有 OpenShift 主节点，它不会影响最终用户，客户与应用程序之间的流量将继续流动；但是，你将无法对 OpenShift
    集群进行任何新的更改。到那时，除了从最后的备份恢复 OpenShift 主节点外，你几乎无能为力。我们将在本章稍后讨论 OpenShift 的备份和恢复。
- en: OpenShift etcd
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift etcd
- en: The OpenShift etcd key-value store is the most critical and sensitive OpenShift
    component, because all OpenShift master persistent data is kept in the etcd cluster.
    The good news is that etcd itself works in active/active configuration, and is
    installed during the initial installation. You will need to properly design your
    etcd cluster, so that you do not run into a situation where you are required to
    reinstall your etcd in order to handle a greater load. There is a general recommendation
    to install and configure your etcd cluster on dedicated nodes, separate from OpenShift
    masters, in a quantity of three, five, or seven members.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift etcd 键值存储是 OpenShift 中最关键和最敏感的组件，因为所有 OpenShift 主节点的持久数据都保存在 etcd
    集群中。好消息是，etcd 本身支持主动/主动配置，并且在初始安装时就会安装。你需要正确设计你的 etcd 集群，以避免出现需要重新安装 etcd 来应对更大负载的情况。一般建议将
    etcd 集群安装并配置在专用节点上，与 OpenShift 主节点分开，并且节点数量为三、五或七个。
- en: OpenShift keeps all configuration data in the etcd key-value store, so it is
    very important to regularly back up your etcd—at some point you will be required
    to restore it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 将所有配置数据保存在 etcd 键值存储中，因此定期备份你的 etcd 非常重要——在某些时候，你将需要恢复它。
- en: OpenShift nodes
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 节点
- en: OpenShift nodes are the easiest to work with, when it comes to HA. Since OpenShift
    pods are stateless by nature, we do not need to directly take care of high availability
    of OpenShift nodes; we just need to make sure that the application pods are running
    on different OpenShift nodes, so that if an OpenShift node goes down, there is
    no downtime for the end user and a new application pod is brought up by the replication
    controller. If you ever have an OpenShift node fail completely, you can just delete
    that node and reinstall it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在高可用性方面，OpenShift 节点最容易处理。由于 OpenShift Pod 本质上是无状态的，因此我们不需要直接处理 OpenShift 节点的高可用性；我们只需确保应用程序
    Pod 在不同的 OpenShift 节点上运行，这样如果某个 OpenShift 节点发生故障，最终用户就不会有停机时间，且复制控制器会启动一个新的应用程序
    Pod。如果 OpenShift 节点完全故障，您只需删除该节点并重新安装它。
- en: In order to remove a node from your OpenShift cluster, you can use the `oc delete
    node` command, and then run `scaleup.yml` from the `openshift-ansible` Git project.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从 OpenShift 集群中移除节点，您可以使用 `oc delete node` 命令，然后从 `openshift-ansible` Git
    项目中运行 `scaleup.yml`。
- en: The `openshift-ansible` project is available at [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`openshift-ansible` 项目可在 [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible)
    上找到。'
- en: The `scaleup.yml` file is located in `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`,
    once you have downloaded the `openshift-ansible` project.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleup.yml` 文件位于 `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`，一旦您下载了
    `openshift-ansible` 项目。'
- en: You will be required to adjust your Ansible inventory file and add a new node,
    under the `[new_nodes]` section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要调整 Ansible 库存文件，并在 `[new_nodes]` 部分添加新节点。
- en: There is no need to back up any data on an OpenShift node since there is no
    stateful data located on a node. In most cases, you will want to delete an OpenShift
    node from an OpenShift cluster, reinstall it, and bring it back, new and fresh.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无需备份 OpenShift 节点上的任何数据，因为节点上没有状态数据。在大多数情况下，您将希望从 OpenShift 集群中删除 OpenShift
    节点，重新安装它，并让它重新投入使用，焕然一新。
- en: External storage for OpenShift persistent data
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 持久数据的外部存储
- en: The external storage configuration and design for OpenShift persistent data is
    out of the scope of this book, but some general advice is to make sure that you
    have your external storage available in a redundant and scalable fashion, meaning
    that if one or several components fail, it will not affect overall storage performance
    and will always be accessible by OpenShift.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 持久数据的外部存储配置和设计超出了本书的范围，但有一些常规建议是确保外部存储具备冗余和可扩展性，这意味着如果某些组件发生故障，整体存储性能不会受到影响，并且
    OpenShift 总是可以访问该存储。
- en: You will need to take care of regular external storage back up and restore procedures separately,
    and you will need to have a tested and verified procedure for if you lose persistent
    storage data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要单独处理定期的外部存储备份和恢复程序，并且需要有一个经过测试和验证的程序，以防丢失持久存储数据。
- en: OpenShift backup and restore
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 备份和恢复
- en: 'No matter what you do, there will be times when something goes wrong with your
    OpenShift cluster, and some (or all) data is lost. That''s why you need to know
    when and how to make OpenShift backups and how to bring OpenShift back to an operational
    state. The OpenShift installation procedure includes the following components
    that you will need to back up:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您做什么，OpenShift 集群总会有出现问题的时刻，可能会丢失部分（或全部）数据。这就是为什么您需要知道何时以及如何进行 OpenShift 备份，以及如何将
    OpenShift 恢复到正常运行状态。OpenShift 安装过程包括以下需要备份的组件：
- en: Etcd key-value store data
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd 键值存储数据
- en: Master configuration data
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点配置数据
- en: Ansible host installation playbooks
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansible 主机安装剧本
- en: Pod data
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 数据
- en: Registry data
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册表数据
- en: Project configuration data
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目配置数据
- en: Additionally installed software
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外安装的软件
- en: Depending on the failure situation, you may need to either reinstall the whole
    OpenShift cluster or reinstall some components separately. In most cases, you
    will be required to completely reinstall the OpenShift cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据故障情况，您可能需要重新安装整个 OpenShift 集群，或者单独重新安装某些组件。在大多数情况下，您需要完全重新安装 OpenShift 集群。
- en: Etcd key-value store backup
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Etcd 键值存储备份
- en: 'The etcd backup procedure can be performed on any etcd node, and consists of
    the following steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: etcd 备份过程可以在任何 etcd 节点上执行，步骤如下：
- en: 'Stop the etcd service: `systemctl stop etcd`'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止 etcd 服务：`systemctl stop etcd`
- en: 'Create an etcd backup: `etcdctl backup --data-dir /var/lib/etcd --backup-dir ~/etcd.back`'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 etcd 备份：`etcdctl backup --data-dir /var/lib/etcd --backup-dir ~/etcd.back`
- en: 'Copy the etcd `db` file: `cp /var/lib/etcd/member/snap/db ~/etcd/member/snap/db`'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制 etcd `db` 文件：`cp /var/lib/etcd/member/snap/db ~/etcd/member/snap/db`
- en: 'Start the etcd service: `systemtl start etcd`'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 etcd 服务：`systemctl start etcd`
- en: 'The etcd key-value store recovery procedure is performed on etcd nodes and consists
    of the following steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: etcd 键值存储恢复过程在 etcd 节点上执行，步骤如下：
- en: Create a single node cluster
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建单节点集群
- en: Restore data to `/var/lib/etcd/`, from backup, while etcd is not running
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 etcd 不运行时，从备份中恢复数据到`/var/lib/etcd/`目录
- en: Restore `/etc/etcd/etcd.conf`, from backup
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从备份中恢复`/etc/etcd/etcd.conf`文件
- en: Restart etcd
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启 etcd
- en: Add new nodes to the etcd cluster
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 etcd 集群添加新节点
- en: OpenShift masters
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 主节点
- en: 'The OpenShift master node backup procedure can be performed on all master nodes,
    and consists of the following steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 主节点备份过程可以在所有主节点上执行，步骤如下：
- en: '**Back up master certs and keys**: `cd /etc/origin/master; tar cf /tmp/certs-and-keys-$(hostname).tar
    *.key *.crt`'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**备份主证书和密钥**：`cd /etc/origin/master; tar cf /tmp/certs-and-keys-$(hostname).tar
    *.key *.crt`'
- en: '**Back up registry certificates**: `cd /etc/docker/certs.d/; tar cf /tmp/docker-registry-certs-$(hostname).tar
    *`'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**备份注册证书**：`cd /etc/docker/certs.d/; tar cf /tmp/docker-registry-certs-$(hostname).tar
    *`'
- en: 'The master node recovery procedure can be performed on all master nodes and consists
    of the following step:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点恢复过程可以在所有主节点上执行，步骤如下：
- en: Restore the previously saved data on every master node to `/etc/sysconfig/`
    , `/etc/origin/`, and `/etc/docker/` directories.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个主节点上将之前保存的数据恢复到 `/etc/sysconfig/`、`/etc/origin/` 和 `/etc/docker/` 目录。
- en: Restart OpenShift all services
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启 OpenShift 所有服务
- en: OpenShift nodes
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 节点
- en: There is no specific need to save any data on an OpenShift node, since there
    is no stateful data; you can easily reinstall all of the nodes one by one, or
    while reinstalling the OpenShift cluster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenShift 节点上没有特定的需求保存任何数据，因为没有有状态的数据；你可以轻松地一个一个地重新安装所有节点，或者在重新安装 OpenShift
    集群时一起重新安装。
- en: Persistent storage
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化存储
- en: In many cases, OpenShift pod persistent data can be saved and restored with
    the `oc rsync` command, but it is not the most reliable and efficient method.
    Persistent storage backup procedures are very different for every storage type,
    and must be considered separately.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，OpenShift pod 的持久化数据可以通过 `oc rsync` 命令进行保存和恢复，但这不是最可靠和高效的方法。不同存储类型的持久化存储备份程序各不相同，必须单独考虑。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we briefly touched on OpenShift HA and on HA in general. We
    discussed how OpenShift provides redundancy in the case of a failure and how you
    can prevent this from happening by properly designing your OpenShift cluster.
    We finished the chapter with the backup and restore methods and procedures in
    OpenShift.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了 OpenShift 高可用性和高可用性的一般概念。我们讨论了 OpenShift 如何在发生故障时提供冗余，并且如何通过正确设计 OpenShift
    集群来防止这种情况发生。我们在本章最后讨论了 OpenShift 中的备份和恢复方法。
- en: In the next chapter, we will discuss OpenShift DC in single and multiple data
    centers. OpenShift multi-DC is one of the most difficult topics when it comes
    to OpenShift design and implementation in a scalable and distributed environment.
    The next chapter will illustrate how to properly design OpenShift, in order to
    work in a distributed and redundant configuration across one or more data centers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论 OpenShift 在单数据中心和多数据中心中的 DC 配置。OpenShift 多数据中心是 OpenShift 设计和实现中最难的主题之一，尤其是在可扩展和分布式环境中。下一章将说明如何正确设计
    OpenShift，以便在一个或多个数据中心的分布式和冗余配置中正常工作。
- en: Questions
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Which of these HA methods provides external access to the OpenShift cluster
    using an external load balancer? choose one:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪种高可用性方法通过外部负载均衡器提供对 OpenShift 集群的外部访问？选择一个：
- en: Virtual IP
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虚拟 IP
- en: IP failover
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: IP 故障转移
- en: GSLB
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: GSLB
- en: DNS load balancing
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNS 负载均衡
- en: 'What are the two valid HA methods to provide access to OpenShift from the outside?
    choose two:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪两种有效的高可用性方法可以提供外部对 OpenShift 的访问？选择两个：
- en: Virtual IP
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虚拟 IP
- en: IP failover
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: IP 故障转移
- en: GSLB
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: GSLB
- en: DNS load balancing
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNS 负载均衡
- en: 'Etcd is a key-value store that is used to store the system''s configuration
    and state in OpenShift:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Etcd 是一个键值存储，用于存储 OpenShift 中系统的配置和状态：
- en: 'True'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: 'What command can be used to back up and restore application data in OpenShift? choose
    one:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么命令可以用于备份和恢复 OpenShift 中的应用数据？选择一个：
- en: '`oc rsync`'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`oc rsync`'
- en: '`` `oc backup` ``'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`` `oc backup` ``'
- en: '`oc save`'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`oc save`'
- en: '`oc load `'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`oc load`'
- en: 'There is no need to restore any data in the OpenShift master disaster recovery
    procedure:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 OpenShift 主节点灾难恢复过程中，无需恢复任何数据：
- en: 'True'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: Further reading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The following links will help you to dive deeper into some of this chapter''s
    topics:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接将帮助您深入了解本章的一些主题：
- en: '**OpenShift HA design**: [http://v1.uncontained.io/playbooks/installation/](http://v1.uncontained.io/playbooks/installation/)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 高可用性设计**：[http://v1.uncontained.io/playbooks/installation/](http://v1.uncontained.io/playbooks/installation/)'
- en: '**OpenShift High Availability**: [https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html](https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 高可用性**：[https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html](https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html)'
- en: '**Infrastructure nodes and pod anti-affinity**: [https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes](https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施节点与 Pod 反亲和性**：[https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes](https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes)'
- en: '**OpenShift backup and restore**: [https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html](https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 备份与恢复**：[https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html](https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html)'
- en: '**OpenShift scaling and performance guide**: [https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html](https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 扩展性与性能指南**：[https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html](https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html)'
- en: '**Etcd optimal cluster size**: [https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size](https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Etcd 最优集群大小**：[https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size](https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size)'
- en: '**Adding hosts to an OpenShift cluster**: [https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html](https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向 OpenShift 集群添加主机**：[https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html](https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html)'
