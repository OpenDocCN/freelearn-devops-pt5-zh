- en: OpenShift HA Architecture Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced you to CI/CD, Jenkins, OpenShift pipelines,
    and Jenkins integration with OpenShift. We also illustrated how to create a sample
    CI/CD pipeline in OpenShift, how to edit pipelines, and how to manage pipeline
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will briefly touch on **high availability** (**HA**) in
    general, and will then focus on OpenShift HA. We will discuss how OpenShift provides
    redundancy in the case of a failure, and how you can prevent this from happening
    by properly designing your OpenShift cluster. At the end of this chapter, we will
    discuss how to back up and restore OpenShift cluster data in case something goes
    wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is high availability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HA in OpenShift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift backup and restore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is high availability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HA is a very important topic when it comes to real customers and real money.
    We all work for different businesses, and the last thing that any business wants
    is to have an outage. This is a problem that can cause people to lose their jobs
    and companies to go bankrupt. It has happened, and it will continue to happen.
    But if you plan your HA design properly and implement it in the right way, you
    will have a better chance of keeping your job and maintaining your company's good
    reputation.
  prefs: []
  type: TYPE_NORMAL
- en: HA usually refers to a concept or strategy to keep a system up and running for
    a long time. That's where the terms *high* and *availability* come together. When
    people ask, *Does it support HA?*, they are usually asking whether the system
    is redundant, and whether it stays up and running if something goes wrong. In
    order to provide HA, each and every component of the system needs to be fault
    tolerant, and all of the lower- and upper-level components and protocols must
    be highly available. For example, if you have OpenShift designed and implemented
    in HA mode, but your network has a single point of failure, your application will
    stop working. So, it is critical to plan properly, and to make sure that your
    application stays up and running, no matter where a failure occurs.
  prefs: []
  type: TYPE_NORMAL
- en: HA in OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we ran our applications on a single node, or sometimes,
    two nodes. Some might say that if there is more than one OpenShift node in the
    cluster, it is considered a redundant configuration, but that is far from true.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare standard OpenShift architecture and OpenShift HA, you will see
    some differences between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: OpenShift classic architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have nodes, masters, storage, and a routing layer consisting of infra
    nodes. OpenShift HA architecture is quite similar but has one distinct difference—in
    the routing layer we have load balances that make the overall solution always
    accessible. All other components are redundant by nature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: OpenShift HA architecture
  prefs: []
  type: TYPE_NORMAL
- en: Virtual IPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can see that in OpenShift HA, we have what''s called an enterprise load
    balancer, with two **Virtual IPs** (**VIPs**). We need one VIP for traffic to
    master nodes, and another VIP for traffic to actual OpenShift applications, running
    on OpenShift nodes within pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: OpenShift with external load balancers
  prefs: []
  type: TYPE_NORMAL
- en: Why can't we just use DNS load balancing? The reason is, if we use DNS load
    balancing and one of the masters or nodes goes down, some traffic will still keep
    flowing to the failed node. Load balancing allows for implementing health checks
    and stops routing the traffic to the failed endpoint. For example, if one of the
    infra nodes fails, the load balancer will detect the failure, remove that node
    from the server pool, and stop sending traffic to the node. When the node comes
    back up, the load balancer will detect that, and will start load balancing traffic
    to the node. So, having VIPs is essential for OpenShift to be highly available
    from the outside.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node name** | **Physical IP address** | **Virtual IP address** | **DNS**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Infra2` | `10.0.0.2/24` | `10.0.0.11` | `*.apps.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Infra3` | `10.0.0.3/24` | `10.0.0.11` | `*.apps.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Master1` | `10.0.1.4/24` | `10.0.0.14` | `console.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Master2` | `10.0.1.5/24` | `10.0.0.14` | `console.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Master3` | `10.0.1.6/24` | `10.0.0.14` | `console.osp.com`  |'
  prefs: []
  type: TYPE_TB
- en: Using an external load balancer is an ideal option when building OpenShift HA,
    because an external load balancer automatically detects a failure of any OpenShift
    infra or master node and distributes the load among the other nodes available.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that we have three infra nodes, all serving the traffic at a speed
    of 50 Mbps. If the `Infra1` node fails, then the external load balancer automatically
    detects the failure and stops serving traffic to the `Infra1` node. So, there
    will be no downtime for both end users and applications, and the load balancer
    will automatically distribute the load between `Infra2` and `Infra3`, so both
    nodes will end up serving the traffic at a speed of 75 Mbps.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of this scenario is that we have to use external load balancers,
    take care of their HA, implement additional health checks, and employ further
    configurations. And, if we are using commercial load balancer appliances from
    F5 or A10, they are going to be very expensive, as well. However, this is the
    most scalable solution that makes sure that OpenShift cluster is always accessible
    from the outside.
  prefs: []
  type: TYPE_NORMAL
- en: IP failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to ensure that your OpenShift applications are always available
    from the outside is to implement IP failover mechanisms. This method is useful
    when you do not have an external load balancer, but still want OpenShift to always
    be accessible from the outside. The OpenShift IP failover design primarily relies
    on two different technologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keepalived**: Provides high availability of VIPs across OpenShift infra nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DNS**: Manages external traffic load balancing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: OpenShift DNS and keepalived
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, keepalived separately manages several VIPs on master and infrastructure
    nodes. Two DNS mappings are used to load balance the traffic between the VIPs
    of the OpenShift infra and master nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node name** | **Physical IP address** | **Virtual IP address** | **DNS**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Infra2` | `10.0.0.2/24` | `10.0.0.12` | `*.apps.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Infra3` | `10.0.0.3/24` | `10.0.0.13` | `*.apps.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Master1` | `20.0.0.1/24` | `20.0.0.11` | `console.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Master2` | `20.0.0.2/24` | `20.0.0.12` | `console.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: '| `Master3` | `20.0.0.3/24` | `20.0.0.13` | `console.osp.com` |'
  prefs: []
  type: TYPE_TB
- en: In the preceding example, we do not need an external load balancer, and, if
    one of the OpenShift nodes goes down, the Virtual IP will automatically be moved
    to another node. Depending on how we configure preemptive options, the Virtual
    IP may come back to an infra node if a failed info node recovers.
  prefs: []
  type: TYPE_NORMAL
- en: There is a downside to this solution, which may or may not be critical for your
    particular case. Let's suppose that we have three infra nodes, all serving the
    traffic at a speed of 50 Mbps. If one of the nodes fails, then the VIP from `Infra1`
    will be moved to `Infra2`. There will be no interruption for end users or applications,
    but `Infra2` will now serve the traffic at a speed of 100 Mbps, while `Infra3`
    is still doing it at 50 Mbps.
  prefs: []
  type: TYPE_NORMAL
- en: This is OK when the workload is not too high, but if there is too much traffic,
    it may cause issues by overloading `Infra2`. So, you have to vet out all possible
    scenarios; by solving one particular failure scenario, you may create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: There are other methods for making your OpenShift cluster available externally.
    Discussing methods like DNS LB, GSLB, custom scripts, or even manual switchover,
    could easily extend this book by a thousand pages. We have focused on the methods
    that are proven to work and are supported by OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift infrastructure nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenShift infrastructure nodes are labeled with infra, by default. They run
    on two main OpenShift components:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift internal registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Openshift infrastructure node is the easiest to install, operate, and troubleshoot,
    because the structure of an OpenShift infra node is simple, stable, and predictable.
    Infra nodes are usually installed in HA mode as a part of the initial OpenShift
    installation; they are rarely modified. The only time you might work with infra
    nodes directly is when you have a lot of traffic going through infra nodes and
    they can't handle it.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, by the time you run into a situation like that, you will have much bigger
    problems than just scaling the number of infra nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: OpenShift infra nodes
  prefs: []
  type: TYPE_NORMAL
- en: The best practice is to have a minimum of three infra nodes installed, with
    the pod anti-affinity feature enabled for both the registry and router. You need
    to have anti-affinity enabled, because you can run into a situation where you
    lose one infrastructure node and the new router pod start a on the node that is
    already running an OpenShift router. So if you have only two infra nodes, without
    the pod anti-affinity feature enabled, in the case of a failure, you will have
    two routers and two registries running on the same infra node, listening on the
    same ports. The pod anti-affinity rule prevents one pod from running with another
    pod on the same host, thus preventing two registries (or two routers) from running
    on the same OpenShift infra node.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift masters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift masters are control plane nodes and the central control points for
    OpenShift solutions. In earlier releases, you had to install an OpenShift master
    using Pacemaker (to provide failover), but in recent releases, this is taken care
    of by keepalived and external storage. If you ever have an OpenShift master fail
    completely, you can just delete the node and reinstall it.
  prefs: []
  type: TYPE_NORMAL
- en: In order to remove a node from your OpenShift cluster, you can use the `oc delete
    node` command, and then run `scaleup.yml` from the `openshift-ansible` Git project.
  prefs: []
  type: TYPE_NORMAL
- en: The `openshift-ansible` project is available at [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible).
  prefs: []
  type: TYPE_NORMAL
- en: The `scaleup.yml` file is located in `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`,
    once you have downloaded the `openshift-ansible` project.
  prefs: []
  type: TYPE_NORMAL
- en: You will be required to adjust your Ansible inventory file and add a new node
    under the `[new_masters]` section.
  prefs: []
  type: TYPE_NORMAL
- en: If, at some point, you lose all OpenShift masters, it will not impact your end
    users, and customer-to-application traffic will keep flowing; however, you won't
    be able to make any new changes to the OpenShift cluster. At that point, there
    is not much that you can do, other than restore OpenShift masters from the last
    backup. We will discuss OpenShift backup and restore later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift etcd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenShift etcd key-value store is the most critical and sensitive OpenShift
    component, because all OpenShift master persistent data is kept in the etcd cluster.
    The good news is that etcd itself works in active/active configuration, and is
    installed during the initial installation. You will need to properly design your
    etcd cluster, so that you do not run into a situation where you are required to
    reinstall your etcd in order to handle a greater load. There is a general recommendation
    to install and configure your etcd cluster on dedicated nodes, separate from OpenShift
    masters, in a quantity of three, five, or seven members.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift keeps all configuration data in the etcd key-value store, so it is
    very important to regularly back up your etcd—at some point you will be required
    to restore it.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift nodes are the easiest to work with, when it comes to HA. Since OpenShift
    pods are stateless by nature, we do not need to directly take care of high availability
    of OpenShift nodes; we just need to make sure that the application pods are running
    on different OpenShift nodes, so that if an OpenShift node goes down, there is
    no downtime for the end user and a new application pod is brought up by the replication
    controller. If you ever have an OpenShift node fail completely, you can just delete
    that node and reinstall it.
  prefs: []
  type: TYPE_NORMAL
- en: In order to remove a node from your OpenShift cluster, you can use the `oc delete
    node` command, and then run `scaleup.yml` from the `openshift-ansible` Git project.
  prefs: []
  type: TYPE_NORMAL
- en: The `openshift-ansible` project is available at [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible).
  prefs: []
  type: TYPE_NORMAL
- en: The `scaleup.yml` file is located in `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`,
    once you have downloaded the `openshift-ansible` project.
  prefs: []
  type: TYPE_NORMAL
- en: You will be required to adjust your Ansible inventory file and add a new node,
    under the `[new_nodes]` section.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need to back up any data on an OpenShift node since there is no
    stateful data located on a node. In most cases, you will want to delete an OpenShift
    node from an OpenShift cluster, reinstall it, and bring it back, new and fresh.
  prefs: []
  type: TYPE_NORMAL
- en: External storage for OpenShift persistent data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The external storage configuration and design for OpenShift persistent data is
    out of the scope of this book, but some general advice is to make sure that you
    have your external storage available in a redundant and scalable fashion, meaning
    that if one or several components fail, it will not affect overall storage performance
    and will always be accessible by OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to take care of regular external storage back up and restore procedures separately,
    and you will need to have a tested and verified procedure for if you lose persistent
    storage data.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift backup and restore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'No matter what you do, there will be times when something goes wrong with your
    OpenShift cluster, and some (or all) data is lost. That''s why you need to know
    when and how to make OpenShift backups and how to bring OpenShift back to an operational
    state. The OpenShift installation procedure includes the following components
    that you will need to back up:'
  prefs: []
  type: TYPE_NORMAL
- en: Etcd key-value store data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master configuration data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible host installation playbooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registry data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project configuration data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally installed software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the failure situation, you may need to either reinstall the whole
    OpenShift cluster or reinstall some components separately. In most cases, you
    will be required to completely reinstall the OpenShift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd key-value store backup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The etcd backup procedure can be performed on any etcd node, and consists of
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop the etcd service: `systemctl stop etcd`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create an etcd backup: `etcdctl backup --data-dir /var/lib/etcd --backup-dir ~/etcd.back`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy the etcd `db` file: `cp /var/lib/etcd/member/snap/db ~/etcd/member/snap/db`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start the etcd service: `systemtl start etcd`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The etcd key-value store recovery procedure is performed on etcd nodes and consists
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a single node cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore data to `/var/lib/etcd/`, from backup, while etcd is not running
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore `/etc/etcd/etcd.conf`, from backup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart etcd
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add new nodes to the etcd cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift masters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenShift master node backup procedure can be performed on all master nodes,
    and consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Back up master certs and keys**: `cd /etc/origin/master; tar cf /tmp/certs-and-keys-$(hostname).tar
    *.key *.crt`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Back up registry certificates**: `cd /etc/docker/certs.d/; tar cf /tmp/docker-registry-certs-$(hostname).tar
    *`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The master node recovery procedure can be performed on all master nodes and consists
    of the following step:'
  prefs: []
  type: TYPE_NORMAL
- en: Restore the previously saved data on every master node to `/etc/sysconfig/`
    , `/etc/origin/`, and `/etc/docker/` directories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart OpenShift all services
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no specific need to save any data on an OpenShift node, since there
    is no stateful data; you can easily reinstall all of the nodes one by one, or
    while reinstalling the OpenShift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, OpenShift pod persistent data can be saved and restored with
    the `oc rsync` command, but it is not the most reliable and efficient method.
    Persistent storage backup procedures are very different for every storage type,
    and must be considered separately.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly touched on OpenShift HA and on HA in general. We
    discussed how OpenShift provides redundancy in the case of a failure and how you
    can prevent this from happening by properly designing your OpenShift cluster.
    We finished the chapter with the backup and restore methods and procedures in
    OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss OpenShift DC in single and multiple data
    centers. OpenShift multi-DC is one of the most difficult topics when it comes
    to OpenShift design and implementation in a scalable and distributed environment.
    The next chapter will illustrate how to properly design OpenShift, in order to
    work in a distributed and redundant configuration across one or more data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Which of these HA methods provides external access to the OpenShift cluster
    using an external load balancer? choose one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Virtual IP
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: IP failover
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GSLB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DNS load balancing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What are the two valid HA methods to provide access to OpenShift from the outside?
    choose two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Virtual IP
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: IP failover
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GSLB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DNS load balancing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Etcd is a key-value store that is used to store the system''s configuration
    and state in OpenShift:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What command can be used to back up and restore application data in OpenShift? choose
    one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oc rsync`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`` `oc backup` ``'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oc save`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oc load `'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is no need to restore any data in the OpenShift master disaster recovery
    procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following links will help you to dive deeper into some of this chapter''s
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenShift HA design**: [http://v1.uncontained.io/playbooks/installation/](http://v1.uncontained.io/playbooks/installation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenShift High Availability**: [https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html](https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure nodes and pod anti-affinity**: [https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes](https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenShift backup and restore**: [https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html](https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenShift scaling and performance guide**: [https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html](https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Etcd optimal cluster size**: [https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size](https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding hosts to an OpenShift cluster**: [https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html](https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
