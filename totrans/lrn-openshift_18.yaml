- en: OpenShift HA Architecture Overview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced you to CI/CD, Jenkins, OpenShift pipelines,
    and Jenkins integration with OpenShift. We also illustrated how to create a sample
    CI/CD pipeline in OpenShift, how to edit pipelines, and how to manage pipeline
    execution.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will briefly touch on **high availability** (**HA**) in
    general, and will then focus on OpenShift HA. We will discuss how OpenShift provides
    redundancy in the case of a failure, and how you can prevent this from happening
    by properly designing your OpenShift cluster. At the end of this chapter, we will
    discuss how to back up and restore OpenShift cluster data in case something goes
    wrong.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: What is high availability?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HA in OpenShift
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift backup and restore
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is high availability?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HA is a very important topic when it comes to real customers and real money.
    We all work for different businesses, and the last thing that any business wants
    is to have an outage. This is a problem that can cause people to lose their jobs
    and companies to go bankrupt. It has happened, and it will continue to happen.
    But if you plan your HA design properly and implement it in the right way, you
    will have a better chance of keeping your job and maintaining your company's good
    reputation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: HA usually refers to a concept or strategy to keep a system up and running for
    a long time. That's where the terms *high* and *availability* come together. When
    people ask, *Does it support HA?*, they are usually asking whether the system
    is redundant, and whether it stays up and running if something goes wrong. In
    order to provide HA, each and every component of the system needs to be fault
    tolerant, and all of the lower- and upper-level components and protocols must
    be highly available. For example, if you have OpenShift designed and implemented
    in HA mode, but your network has a single point of failure, your application will
    stop working. So, it is critical to plan properly, and to make sure that your
    application stays up and running, no matter where a failure occurs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: HA in OpenShift
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we ran our applications on a single node, or sometimes,
    two nodes. Some might say that if there is more than one OpenShift node in the
    cluster, it is considered a redundant configuration, but that is far from true.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare standard OpenShift architecture and OpenShift HA, you will see
    some differences between them:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: OpenShift classic architecture
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have nodes, masters, storage, and a routing layer consisting of infra
    nodes. OpenShift HA architecture is quite similar but has one distinct difference—in
    the routing layer we have load balances that make the overall solution always
    accessible. All other components are redundant by nature:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: OpenShift HA architecture
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Virtual IPs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can see that in OpenShift HA, we have what''s called an enterprise load
    balancer, with two **Virtual IPs** (**VIPs**). We need one VIP for traffic to
    master nodes, and another VIP for traffic to actual OpenShift applications, running
    on OpenShift nodes within pods:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: OpenShift with external load balancers
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Why can't we just use DNS load balancing? The reason is, if we use DNS load
    balancing and one of the masters or nodes goes down, some traffic will still keep
    flowing to the failed node. Load balancing allows for implementing health checks
    and stops routing the traffic to the failed endpoint. For example, if one of the
    infra nodes fails, the load balancer will detect the failure, remove that node
    from the server pool, and stop sending traffic to the node. When the node comes
    back up, the load balancer will detect that, and will start load balancing traffic
    to the node. So, having VIPs is essential for OpenShift to be highly available
    from the outside.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node name** | **Physical IP address** | **Virtual IP address** | **DNS**
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| `Infra2` | `10.0.0.2/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| `Infra3` | `10.0.0.3/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| `Master1` | `10.0.1.4/24` | `10.0.0.14` | `console.osp.com` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| `Master2` | `10.0.1.5/24` | `10.0.0.14` | `console.osp.com` |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| `Master3` | `10.0.1.6/24` | `10.0.0.14` | `console.osp.com`  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: Using an external load balancer is an ideal option when building OpenShift HA,
    because an external load balancer automatically detects a failure of any OpenShift
    infra or master node and distributes the load among the other nodes available.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that we have three infra nodes, all serving the traffic at a speed
    of 50 Mbps. If the `Infra1` node fails, then the external load balancer automatically
    detects the failure and stops serving traffic to the `Infra1` node. So, there
    will be no downtime for both end users and applications, and the load balancer
    will automatically distribute the load between `Infra2` and `Infra3`, so both
    nodes will end up serving the traffic at a speed of 75 Mbps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The downside of this scenario is that we have to use external load balancers,
    take care of their HA, implement additional health checks, and employ further
    configurations. And, if we are using commercial load balancer appliances from
    F5 or A10, they are going to be very expensive, as well. However, this is the
    most scalable solution that makes sure that OpenShift cluster is always accessible
    from the outside.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: IP failover
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to ensure that your OpenShift applications are always available
    from the outside is to implement IP failover mechanisms. This method is useful
    when you do not have an external load balancer, but still want OpenShift to always
    be accessible from the outside. The OpenShift IP failover design primarily relies
    on two different technologies:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Keepalived**: Provides high availability of VIPs across OpenShift infra nodes.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DNS**: Manages external traffic load balancing.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: OpenShift DNS and keepalived
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, keepalived separately manages several VIPs on master and infrastructure
    nodes. Two DNS mappings are used to load balance the traffic between the VIPs
    of the OpenShift infra and master nodes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node name** | **Physical IP address** | **Virtual IP address** | **DNS**
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| `Infra1` | `10.0.0.1/24` | `10.0.0.11` | `*.apps.osp.com` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| `Infra2` | `10.0.0.2/24` | `10.0.0.12` | `*.apps.osp.com` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| `Infra3` | `10.0.0.3/24` | `10.0.0.13` | `*.apps.osp.com` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| `Master1` | `20.0.0.1/24` | `20.0.0.11` | `console.osp.com` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| `Master2` | `20.0.0.2/24` | `20.0.0.12` | `console.osp.com` |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| `Master3` | `20.0.0.3/24` | `20.0.0.13` | `console.osp.com` |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: In the preceding example, we do not need an external load balancer, and, if
    one of the OpenShift nodes goes down, the Virtual IP will automatically be moved
    to another node. Depending on how we configure preemptive options, the Virtual
    IP may come back to an infra node if a failed info node recovers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: There is a downside to this solution, which may or may not be critical for your
    particular case. Let's suppose that we have three infra nodes, all serving the
    traffic at a speed of 50 Mbps. If one of the nodes fails, then the VIP from `Infra1`
    will be moved to `Infra2`. There will be no interruption for end users or applications,
    but `Infra2` will now serve the traffic at a speed of 100 Mbps, while `Infra3`
    is still doing it at 50 Mbps.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: This is OK when the workload is not too high, but if there is too much traffic,
    it may cause issues by overloading `Infra2`. So, you have to vet out all possible
    scenarios; by solving one particular failure scenario, you may create a new one.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: There are other methods for making your OpenShift cluster available externally.
    Discussing methods like DNS LB, GSLB, custom scripts, or even manual switchover,
    could easily extend this book by a thousand pages. We have focused on the methods
    that are proven to work and are supported by OpenShift.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift infrastructure nodes
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenShift infrastructure nodes are labeled with infra, by default. They run
    on two main OpenShift components:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift internal registry
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift router
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Openshift infrastructure node is the easiest to install, operate, and troubleshoot,
    because the structure of an OpenShift infra node is simple, stable, and predictable.
    Infra nodes are usually installed in HA mode as a part of the initial OpenShift
    installation; they are rarely modified. The only time you might work with infra
    nodes directly is when you have a lot of traffic going through infra nodes and
    they can't handle it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'But, by the time you run into a situation like that, you will have much bigger
    problems than just scaling the number of infra nodes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: OpenShift infra nodes
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The best practice is to have a minimum of three infra nodes installed, with
    the pod anti-affinity feature enabled for both the registry and router. You need
    to have anti-affinity enabled, because you can run into a situation where you
    lose one infrastructure node and the new router pod start a on the node that is
    already running an OpenShift router. So if you have only two infra nodes, without
    the pod anti-affinity feature enabled, in the case of a failure, you will have
    two routers and two registries running on the same infra node, listening on the
    same ports. The pod anti-affinity rule prevents one pod from running with another
    pod on the same host, thus preventing two registries (or two routers) from running
    on the same OpenShift infra node.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift masters
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift masters are control plane nodes and the central control points for
    OpenShift solutions. In earlier releases, you had to install an OpenShift master
    using Pacemaker (to provide failover), but in recent releases, this is taken care
    of by keepalived and external storage. If you ever have an OpenShift master fail
    completely, you can just delete the node and reinstall it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: In order to remove a node from your OpenShift cluster, you can use the `oc delete
    node` command, and then run `scaleup.yml` from the `openshift-ansible` Git project.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The `openshift-ansible` project is available at [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The `scaleup.yml` file is located in `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`,
    once you have downloaded the `openshift-ansible` project.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: You will be required to adjust your Ansible inventory file and add a new node
    under the `[new_masters]` section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: If, at some point, you lose all OpenShift masters, it will not impact your end
    users, and customer-to-application traffic will keep flowing; however, you won't
    be able to make any new changes to the OpenShift cluster. At that point, there
    is not much that you can do, other than restore OpenShift masters from the last
    backup. We will discuss OpenShift backup and restore later in this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift etcd
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenShift etcd key-value store is the most critical and sensitive OpenShift
    component, because all OpenShift master persistent data is kept in the etcd cluster.
    The good news is that etcd itself works in active/active configuration, and is
    installed during the initial installation. You will need to properly design your
    etcd cluster, so that you do not run into a situation where you are required to
    reinstall your etcd in order to handle a greater load. There is a general recommendation
    to install and configure your etcd cluster on dedicated nodes, separate from OpenShift
    masters, in a quantity of three, five, or seven members.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift keeps all configuration data in the etcd key-value store, so it is
    very important to regularly back up your etcd—at some point you will be required
    to restore it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift nodes
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift nodes are the easiest to work with, when it comes to HA. Since OpenShift
    pods are stateless by nature, we do not need to directly take care of high availability
    of OpenShift nodes; we just need to make sure that the application pods are running
    on different OpenShift nodes, so that if an OpenShift node goes down, there is
    no downtime for the end user and a new application pod is brought up by the replication
    controller. If you ever have an OpenShift node fail completely, you can just delete
    that node and reinstall it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在高可用性方面，OpenShift 节点最容易处理。由于 OpenShift Pod 本质上是无状态的，因此我们不需要直接处理 OpenShift 节点的高可用性；我们只需确保应用程序
    Pod 在不同的 OpenShift 节点上运行，这样如果某个 OpenShift 节点发生故障，最终用户就不会有停机时间，且复制控制器会启动一个新的应用程序
    Pod。如果 OpenShift 节点完全故障，您只需删除该节点并重新安装它。
- en: In order to remove a node from your OpenShift cluster, you can use the `oc delete
    node` command, and then run `scaleup.yml` from the `openshift-ansible` Git project.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从 OpenShift 集群中移除节点，您可以使用 `oc delete node` 命令，然后从 `openshift-ansible` Git
    项目中运行 `scaleup.yml`。
- en: The `openshift-ansible` project is available at [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`openshift-ansible` 项目可在 [https://github.com/openshift/openshift-ansible](https://github.com/openshift/openshift-ansible)
    上找到。'
- en: The `scaleup.yml` file is located in `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`,
    once you have downloaded the `openshift-ansible` project.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleup.yml` 文件位于 `openshift-ansible/playbooks/byo/openshift-node/scaleup.yml`，一旦您下载了
    `openshift-ansible` 项目。'
- en: You will be required to adjust your Ansible inventory file and add a new node,
    under the `[new_nodes]` section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要调整 Ansible 库存文件，并在 `[new_nodes]` 部分添加新节点。
- en: There is no need to back up any data on an OpenShift node since there is no
    stateful data located on a node. In most cases, you will want to delete an OpenShift
    node from an OpenShift cluster, reinstall it, and bring it back, new and fresh.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无需备份 OpenShift 节点上的任何数据，因为节点上没有状态数据。在大多数情况下，您将希望从 OpenShift 集群中删除 OpenShift
    节点，重新安装它，并让它重新投入使用，焕然一新。
- en: External storage for OpenShift persistent data
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 持久数据的外部存储
- en: The external storage configuration and design for OpenShift persistent data is
    out of the scope of this book, but some general advice is to make sure that you
    have your external storage available in a redundant and scalable fashion, meaning
    that if one or several components fail, it will not affect overall storage performance
    and will always be accessible by OpenShift.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 持久数据的外部存储配置和设计超出了本书的范围，但有一些常规建议是确保外部存储具备冗余和可扩展性，这意味着如果某些组件发生故障，整体存储性能不会受到影响，并且
    OpenShift 总是可以访问该存储。
- en: You will need to take care of regular external storage back up and restore procedures separately,
    and you will need to have a tested and verified procedure for if you lose persistent
    storage data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要单独处理定期的外部存储备份和恢复程序，并且需要有一个经过测试和验证的程序，以防丢失持久存储数据。
- en: OpenShift backup and restore
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 备份和恢复
- en: 'No matter what you do, there will be times when something goes wrong with your
    OpenShift cluster, and some (or all) data is lost. That''s why you need to know
    when and how to make OpenShift backups and how to bring OpenShift back to an operational
    state. The OpenShift installation procedure includes the following components
    that you will need to back up:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您做什么，OpenShift 集群总会有出现问题的时刻，可能会丢失部分（或全部）数据。这就是为什么您需要知道何时以及如何进行 OpenShift 备份，以及如何将
    OpenShift 恢复到正常运行状态。OpenShift 安装过程包括以下需要备份的组件：
- en: Etcd key-value store data
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd 键值存储数据
- en: Master configuration data
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点配置数据
- en: Ansible host installation playbooks
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansible 主机安装剧本
- en: Pod data
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 数据
- en: Registry data
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册表数据
- en: Project configuration data
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目配置数据
- en: Additionally installed software
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外安装的软件
- en: Depending on the failure situation, you may need to either reinstall the whole
    OpenShift cluster or reinstall some components separately. In most cases, you
    will be required to completely reinstall the OpenShift cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据故障情况，您可能需要重新安装整个 OpenShift 集群，或者单独重新安装某些组件。在大多数情况下，您需要完全重新安装 OpenShift 集群。
- en: Etcd key-value store backup
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Etcd 键值存储备份
- en: 'The etcd backup procedure can be performed on any etcd node, and consists of
    the following steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: etcd 备份过程可以在任何 etcd 节点上执行，步骤如下：
- en: 'Stop the etcd service: `systemctl stop etcd`'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止 etcd 服务：`systemctl stop etcd`
- en: 'Create an etcd backup: `etcdctl backup --data-dir /var/lib/etcd --backup-dir ~/etcd.back`'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy the etcd `db` file: `cp /var/lib/etcd/member/snap/db ~/etcd/member/snap/db`'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start the etcd service: `systemtl start etcd`'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The etcd key-value store recovery procedure is performed on etcd nodes and consists
    of the following steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Create a single node cluster
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore data to `/var/lib/etcd/`, from backup, while etcd is not running
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore `/etc/etcd/etcd.conf`, from backup
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart etcd
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add new nodes to the etcd cluster
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift masters
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenShift master node backup procedure can be performed on all master nodes,
    and consists of the following steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Back up master certs and keys**: `cd /etc/origin/master; tar cf /tmp/certs-and-keys-$(hostname).tar
    *.key *.crt`'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Back up registry certificates**: `cd /etc/docker/certs.d/; tar cf /tmp/docker-registry-certs-$(hostname).tar
    *`'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The master node recovery procedure can be performed on all master nodes and consists
    of the following step:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Restore the previously saved data on every master node to `/etc/sysconfig/`
    , `/etc/origin/`, and `/etc/docker/` directories.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart OpenShift all services
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift nodes
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no specific need to save any data on an OpenShift node, since there
    is no stateful data; you can easily reinstall all of the nodes one by one, or
    while reinstalling the OpenShift cluster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Persistent storage
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, OpenShift pod persistent data can be saved and restored with
    the `oc rsync` command, but it is not the most reliable and efficient method.
    Persistent storage backup procedures are very different for every storage type,
    and must be considered separately.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly touched on OpenShift HA and on HA in general. We
    discussed how OpenShift provides redundancy in the case of a failure and how you
    can prevent this from happening by properly designing your OpenShift cluster.
    We finished the chapter with the backup and restore methods and procedures in
    OpenShift.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss OpenShift DC in single and multiple data
    centers. OpenShift multi-DC is one of the most difficult topics when it comes
    to OpenShift design and implementation in a scalable and distributed environment.
    The next chapter will illustrate how to properly design OpenShift, in order to
    work in a distributed and redundant configuration across one or more data centers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Which of these HA methods provides external access to the OpenShift cluster
    using an external load balancer? choose one:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Virtual IP
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: IP failover
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GSLB
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DNS load balancing
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What are the two valid HA methods to provide access to OpenShift from the outside?
    choose two:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Virtual IP
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: IP failover
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GSLB
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: DNS load balancing
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Etcd is a key-value store that is used to store the system''s configuration
    and state in OpenShift:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What command can be used to back up and restore application data in OpenShift? choose
    one:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oc rsync`'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`` `oc backup` ``'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oc save`'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oc load `'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is no need to restore any data in the OpenShift master disaster recovery
    procedure:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 OpenShift 主节点灾难恢复过程中，无需恢复任何数据：
- en: 'True'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: Further reading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The following links will help you to dive deeper into some of this chapter''s
    topics:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接将帮助您深入了解本章的一些主题：
- en: '**OpenShift HA design**: [http://v1.uncontained.io/playbooks/installation/](http://v1.uncontained.io/playbooks/installation/)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 高可用性设计**：[http://v1.uncontained.io/playbooks/installation/](http://v1.uncontained.io/playbooks/installation/)'
- en: '**OpenShift High Availability**: [https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html](https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 高可用性**：[https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html](https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html)'
- en: '**Infrastructure nodes and pod anti-affinity**: [https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes](https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施节点与 Pod 反亲和性**：[https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes](https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes)'
- en: '**OpenShift backup and restore**: [https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html](https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 备份与恢复**：[https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html](https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html)'
- en: '**OpenShift scaling and performance guide**: [https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html](https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift 扩展性与性能指南**：[https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html](https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html)'
- en: '**Etcd optimal cluster size**: [https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size](https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Etcd 最优集群大小**：[https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size](https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size)'
- en: '**Adding hosts to an OpenShift cluster**: [https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html](https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向 OpenShift 集群添加主机**：[https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html](https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html)'
