<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Looking at Schedulers</h1></div></div></div><p>In this chapter, we will look at a few different schedulers that are capable of launching containers on both your own infrastructures as well as public cloud-based infrastructures. To start with, we will look at two different schedulers, both of which we will use to launch clusters on <a id="id404" class="indexterm"/>Amazon Web Services. The two schedulers are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Kubernetes</strong>: <a class="ulink" href="http://kubernetes.io/">http://kubernetes.io/</a></li><li class="listitem" style="list-style-type: disc"><strong>Amazon</strong> <strong>ECS</strong>: <a class="ulink" href="https://aws.amazon.com/ecs/">https://aws.amazon.com/ecs/</a></li></ul></div><p>We will then <a id="id405" class="indexterm"/>take a look at a tool that offers its own scheduler as <a id="id406" class="indexterm"/>well as supports others:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Rancher</strong>: <a class="ulink" href="http://rancher.com/">http://rancher.com/</a></li></ul></div><p>Let's dive straight in by looking at Kubernetes.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec36"/>Getting started with Kubernetes</h1></div></div></div><p>Kubernetes is <a id="id407" class="indexterm"/>an open source tool, originally developed by Google. It is described as:</p><div><blockquote class="blockquote"><p><em>"A tool for automating deployment, operations, and scaling of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon a decade and a half of experience <a id="id408" class="indexterm"/>of running production workloads at Google, combined with best-of-breed ideas and practices from the community." <a class="ulink" href="http://www.kubernetes.io">http://www.kubernetes.io</a>
</em></p></blockquote></div><p>While it is not the exact tool that Google uses to deploy their containers internally, it has been built from the ground up to offer the same functionality. Google is also slowly transitioning to internally use Kubernetes themselves. It is designed around the following three principles:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Planet scale</strong>: Designed on<a id="id409" class="indexterm"/> the same principles that allow Google to run billions of containers a week, Kubernetes can scale without increasing your ops team</li><li class="listitem" style="list-style-type: disc"><strong>Never outgrow</strong>: Whether testing locally or running a global enterprise, Kubernetes' flexibility grows with you in order to deliver your applications consistently and easily no matter how complex your need is</li><li class="listitem" style="list-style-type: disc"><strong>Run anywhere</strong>: Kubernetes is open source, giving you the freedom to take advantage of on-premise, hybrid, or public cloud infrastructure, letting you effortlessly move workloads to where it matters to you</li></ul></div><p>Out of the box, it <a id="id410" class="indexterm"/>comes with quite a mature feature set:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Automatic bin packing</strong>: This is the core of the tool, a powerful scheduler that makes decisions on where to launch your containers based on the resources currently being consumed on your cluster nodes</li><li class="listitem" style="list-style-type: disc"><strong>Horizontal scaling</strong>: This allows you to scale up your application, either manually or based on CPU utilization</li><li class="listitem" style="list-style-type: disc"><strong>Self-healing</strong>: You can configure status checks; if your container fails a check, then it will be relaunched where the resource is available</li><li class="listitem" style="list-style-type: disc"><strong>Load balancing &amp; service discovery</strong>: Kubernetes allows you to attach your containers to services, these can expose your container either locally or externally</li><li class="listitem" style="list-style-type: disc"><strong>Storage orchestration</strong>: Kubernetes supports a number of backend storage modules out of the box, including Google Cloud Platform, AWS, and services such as NFS, iSCSI, Gluster, or Flocker to name a few</li><li class="listitem" style="list-style-type: disc"><strong>Secret and configuration management</strong>: This allows you to deploy and update secrets such as API keys to your containers, without exposing them or rebuilding your container images</li></ul></div><p>There are a lot more features that we could talk about; rather than covering these features, let's dive right in and install a Kubernetes cluster.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec64"/>Installing Kubernetes</h2></div></div></div><p>As hinted by<a id="id411" class="indexterm"/> the Kubernetes website, there are a lot of ways you can install Kubernetes. A lot of the documentation refers to Google's own public cloud; however, rather than introducing a third public cloud into the mix, we are going to be looking at deploying our Kubernetes cluster onto Amazon Web Services.</p><p>Before we start the Kubernetes installation, we need to ensure that you have the AWS Command Line Interface installed and configured.</p><div><div><h3 class="title"><a id="note20"/>Note</h3><p>The AWS <strong>Command Line Interface</strong> (<strong>CLI</strong>) is a unified tool to manage your AWS services. With <a id="id412" class="indexterm"/>just one tool to download and configure, you can control multiple AWS services from the <a id="id413" class="indexterm"/>command line and automate them through scripts:</p><p>
<a class="ulink" href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>
</p></div></div><p>As we have<a id="id414" class="indexterm"/> already used Homebrew several times during the previous chapters, we will use that to install the tools. To do this, simply run the following command:</p><div><pre class="programlisting">
<strong>brew install awscli</strong>
</pre></div><p>Once the tools have been installed, you will be able to configure the tools by running the following command:</p><div><pre class="programlisting">
<strong>aws configure</strong>
</pre></div><p>This will ask for the following four pieces of information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">AWS Access Key ID</li><li class="listitem" style="list-style-type: disc">AWS Secret Access Key</li><li class="listitem" style="list-style-type: disc">Default region name</li><li class="listitem" style="list-style-type: disc">Default output format</li></ul></div><p>You should have your AWS Access and Secret keys from the when we launched a Docker Machine in Amazon Web Services in <a class="link" href="ch02.html" title="Chapter 2. Introducing First-party Tools">Chapter 2</a>, <em>Introducing First-party Tools</em>. For the <code class="literal">Default region name</code>, I used <code class="literal">eu-west-1</code> (which is the closest region to me) and I left the <code class="literal">Default output format</code> as <code class="literal">None</code>:</p><div><img src="img/B05468_07_02.jpg" alt="Installing Kubernetes"/></div><p>Now that <a id="id415" class="indexterm"/>we have the AWS Command Line Tools installed and configured, we can install the Kubernetes Command Line Tools. This is a binary that will allow you to interact with your Kubernetes' cluster in the same way that the local Docker client connects to a remote Docker Engine. This can be installed using Homebrew, just run the following command:</p><div><pre class="programlisting">
<strong>brew install kubernetes-cli</strong>
</pre></div><div><img src="img/B05468_07_03.jpg" alt="Installing Kubernetes"/></div><p>We don't need to configure the tool once installed as this will be taken care of by the main Kubernetes deployment script that we will be running next.</p><p>Now that we have the tools needed to launch and interact with our AWS Kubernetes cluster, we can make a start deploying the cluster itself.</p><p>Before we <a id="id416" class="indexterm"/>kick off the installation, we need to let the installation script know a little bit of information about where we want our cluster to launch and also how big we would like it, this information is passed on to the installation script as environment variables.</p><p>First of all, I would like it launched in Europe:</p><div><pre class="programlisting">
<strong>export KUBE_AWS_ZONE=eu-west-1c</strong>
<strong>export AWS_S3_REGION=eu-west-1</strong>
</pre></div><p>Also, I would like two nodes:</p><div><pre class="programlisting">
<strong>export NUM_NODES=2</strong>
</pre></div><p>Finally, we need to instruct the installation script that we would like to launch the Kubernetes in Amazon Web Services:</p><div><pre class="programlisting">
<strong>export KUBERNETES_PROVIDER=aws</strong>
</pre></div><p>Now that we have told the installer where we would like our Kubernetes cluster to be launched, it's time to actually launch it. To do this, run the following command:</p><div><pre class="programlisting">
<strong>curl -sS https://get.k8s.io | bash</strong>
</pre></div><p>This will download the installer and the latest Kubernetes codebase, and then launch our cluster. The process itself can take anywhere between eight and fifteen minutes, depending on your network connection.</p><p>If you prefer not to run this installation yourself, you can view a recording of a Kubernetes cluster<a id="id417" class="indexterm"/> being deployed in Amazon Web Services at the following URL:</p><p>
<a class="ulink" href="https://asciinema.org">https://asciinema.org</a>
</p><div><img src="img/B05468_07_07.jpg" alt="Installing Kubernetes"/></div><p>Once the<a id="id418" class="indexterm"/> installation script has completed, you will be given information on where to access your Kubernetes cluster, you should also be able to run the following command to get a list of the nodes within your Kubernetes cluster:</p><div><pre class="programlisting">
<strong>kubectl get nodes</strong>
</pre></div><p>This should return something similar to the following screenshot:</p><div><img src="img/B05468_07_08.jpg" alt="Installing Kubernetes"/></div><p>Also, if you have the AWS Console open, you should see that a new VPC dedicated to Kubernetes has been created:</p><div><img src="img/B05468_07_04.jpg" alt="Installing Kubernetes"/></div><p>You will <a id="id419" class="indexterm"/>also see that three EC2 instances have been launched into the Kubernetes VPC:</p><div><img src="img/B05468_07_05.jpg" alt="Installing Kubernetes"/></div><p>The last<a id="id420" class="indexterm"/> thing to make a note of before we start to launch applications into our Kubernetes cluster is the username and password credentials for the cluster.</p><p>As you may have seen during the installation, these are stored in the Kubernetes CLI configuration, as they are right at the bottom of the file, you can get these by running the following command:</p><div><pre class="programlisting">
<strong>tail -3 ~/.kube/config</strong>
</pre></div><div><img src="img/B05468_07_06.jpg" alt="Installing Kubernetes"/></div><p>Now that our Kubernetes cluster has been launched, and we have access to it using the command-line tools, we can start launching an application.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec65"/>Launching our first Kubernetes application</h2></div></div></div><p>To start off with, we are going to be launching a really basic cluster of NGINX containers, each <a id="id421" class="indexterm"/>container within the cluster will be serving a simple graphic and also print its host name on the page. You can find the image <a id="id422" class="indexterm"/>for container on the Docker Hub at <a class="ulink" href="https://hub.docker.com/r/russmckendrick/cluster/">https://hub.docker.com/r/russmckendrick/cluster/</a>.</p><p>Like a lot of the tools we have looked at in the previous chapters, Kubernetes uses the YAML format for its definition file. The file we are going to launch into our cluster is the following file:</p><div><pre class="programlisting">apiVersion: v1
kind: ReplicationController
metadata:
  name: nginxcluster
spec:
  replicas: 5
  selector:
    app: nginxcluster
  template:
    metadata:
      name: nginxcluster
      labels:
        app: nginxcluster
    spec:
      containers:
      - name: nginxcluster
        image: russmckendrick/cluster
        ports:
        - containerPort: 80</pre></div><p>Let's call the file <code class="literal">nginxcluster.yaml</code>. To launch it, run the following command:</p><div><pre class="programlisting">
<strong>kubectl create -f nginxcluster.yaml</strong>
</pre></div><p>Once launched, you will be able to see the active pods by running the following command:</p><div><pre class="programlisting">
<strong>kubectl get pods</strong>
</pre></div><p>You may find that you need to run the <code class="literal">kubectl</code> <code class="literal">get pods</code> command a few times to ensure that everything is running as expected:</p><div><img src="img/B05468_07_09.jpg" alt="Launching our first Kubernetes application"/></div><p>Now that <a id="id423" class="indexterm"/>you have your pods up and running, we need to expose them so that we can access the cluster using a browser. To do this, we need to create a service. To view the current services, type the following:</p><div><pre class="programlisting">
<strong>kubectl get services</strong>
</pre></div><p>You should see just the main Kubernetes service. When we launched our pods, we defined a replication controller, this is the process that manages the number of pods. To view the replication controllers, run the following command:</p><div><pre class="programlisting">
<strong>kubectl get rc</strong>
</pre></div><p>You should see the nginxcluster controller with five pods in the desired and current column. Now that we have confirmed that our replication controller is active with the expected number of pods registered with it, let's create the service and expose the pods to the outside world by running the following command:</p><div><pre class="programlisting">
<strong>kubectl expose rc nginxcluster --port=80 --type=LoadBalancer</strong>
</pre></div><p>Now, if you run the <code class="literal">get services</code> command again, you should see our new service:</p><div><pre class="programlisting">
<strong>kubectl get services</strong>
</pre></div><p>Your terminal session should look something similar to the following screenshot:</p><div><img src="img/B05468_07_10.jpg" alt="Launching our first Kubernetes application"/></div><p>Great, you <a id="id424" class="indexterm"/>now have your pods exposed to the Internet. However, you may have noticed that the cluster IP address is an internal one, so how do you access your cluster?</p><p>As we are running our Kubernetes cluster in Amazon Web Services, when you exposed the service, Kubernetes made an API call to AWS and launched an Elastic Load Balancer. You can get the URL of the load balancer by running the following command:</p><div><pre class="programlisting">
<strong>kubectl describe service nginxcluster</strong>
</pre></div><div><img src="img/B05468_07_11.jpg" alt="Launching our first Kubernetes application"/></div><p>As you can see, in my case, my load balancer can be accessed at <code class="literal">http:// af92913bcf98a11e5841c0a7f321c3b2-1182773033.eu-west-1.elb.amazonaws.com/</code>.</p><p>Opening the load balancer URL in a browser shows our container page:</p><div><img src="img/B05468_07_13.jpg" alt="Launching our first Kubernetes application"/></div><p>Finally, if <a id="id425" class="indexterm"/>you open the AWS console, you should be able to see the elastic load balancer created by Kubernetes:</p><div><img src="img/B05468_07_12.jpg" alt="Launching our first Kubernetes application"/></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec66"/>An advanced example</h2></div></div></div><p>Let's try something more advanced than launching a few of the same instances and load balancing them.</p><p>For the following<a id="id426" class="indexterm"/> example, we are going to launch our <a id="id427" class="indexterm"/>WordPress stack. This time we are going to mount Elastic Block Storage volumes to store both our MySQL database and WordPress files on:</p><div><blockquote class="blockquote"><p><em>"Amazon Elastic Block Store (Amazon EBS) provides persistent block level storage <a id="id428" class="indexterm"/>volumes for use with Amazon EC2 instances in the AWS Cloud.  Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutes – all while paying a low price for only what you provision." - <a class="ulink" href="https://aws.amazon.com/ebs/">https://aws.amazon.com/ebs/</a>
</em></p></blockquote></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec20"/>Creating the volumes</h3></div></div></div><p>Before we<a id="id429" class="indexterm"/> launch our pods and services, we need to create the two EBS volumes that we will be attaching to our pods. As we already have the AWS Command Line Interface installed and configured, we will be using that to create the volume rather than logging into the console and creating it using the GUI.</p><p>To create the two volumes, simply run the following command twice, making sure that you update the availability zone to match where your Kubernetes cluster was configured to launch:</p><p>
<code class="literal">aws ec2 create-volume --availability-zone eu-west-1c --size 10 --volume-type gp2</code>
</p><p>Each time you run the command, you will get a blob of JSON returned, this will contain all of the metadata generated when the volume was created:</p><div><img src="img/B05468_07_14.jpg" alt="Creating the volumes"/></div><p>Make a <a id="id430" class="indexterm"/>note of VolumeId for each of the two volumes, you will need to know these when we create our MySQL and WordPress pods.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec21"/>Launching MySQL</h3></div></div></div><p>Now that <a id="id431" class="indexterm"/>we have the volumes created, we are now able to launch our MySQL Pod and Service. First of all, let's start with the Pod definition, make sure that you add one of the volumeIDs at the where promoted towards the bottom of the file:</p><div><pre class="programlisting">apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels: 
    name: mysql
spec: 
  containers: 
    - resources:
      image: russmckendrick/mariadb
      name: mysql
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: yourpassword
      ports: 
        - containerPort: 3306
          name: mysql
      volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
  volumes:
    - name: mysql-persistent-storage
      awsElasticBlockStore:
        volumeID:&lt;insert your volume id here&gt;
        fsType: ext4</pre></div><p>As you can <a id="id432" class="indexterm"/>see, this follows pretty closely to our first Kubernetes application, except this time, we are only creating a single Pod rather than one with a Replication Controller.</p><p>Also, as you can see, I have added my volumeID to the bottom of the file; you will need to add your own volumeID when you come to launch the Pod.</p><p>I call the file <code class="literal">mysql.yaml</code>, so to launch it, we need to run the following command:</p><div><pre class="programlisting">
<strong>kubectl create -f mysql.yaml</strong>
</pre></div><p>Kubernetes will validate the <code class="literal">mysql.yaml</code> file before it tries to launch the Pod; if you get any errors, please check whether the indentation is correct:</p><div><img src="img/B05468_07_15.jpg" alt="Launching MySQL"/></div><p>You should now have the Pod launched; however, you should probably check if it's there. Run the following command to view the status of your Pods:</p><div><pre class="programlisting">
<strong>kubectl get pods</strong>
</pre></div><p>If you see that the Pod has a status of <code class="literal">Pending</code>, like I did, you will probably be wondering <em>what's going on?</em> Luckily, you can easily find that out by getting more information on the Pod we are trying to launch by using the <code class="literal">describe</code> command:</p><div><pre class="programlisting">
<strong>kubectl describe pod mysql</strong>
</pre></div><p>This will print out everything you will ever want know about the Pod, as you can see from the following terminal output, we did not have enough capacity within our cluster to launch the Pod:</p><div><img src="img/B05468_07_16.jpg" alt="Launching MySQL"/></div><p>We can<a id="id433" class="indexterm"/> free up some resources by removing our previous Pods and Services by running the following command:</p><div><pre class="programlisting">
<strong>kubectl delete rc nginxcluster</strong>
<strong>kubectl delete service nginxcluster</strong>
</pre></div><p>Once you run the commands to remove <code class="literal">nginxcluster</code>, your mysql Pod should automatically launch after a few seconds:</p><div><img src="img/B05468_07_17.jpg" alt="Launching MySQL"/></div><p>Now that<a id="id434" class="indexterm"/> the Pod has been launched, we need to attach a service so that port <code class="literal">3306</code> is exposed, rather than doing this using the <code class="literal">kubectl</code> command like we did before, we will use a second file called <code class="literal">mysql-service.yaml</code>:</p><div><pre class="programlisting">apiVersion: v1
kind: Service
metadata: 
  labels: 
    name: mysql
  name: mysql
spec: 
  ports:
    - port: 3306
  selector: 
    name: mysql</pre></div><p>To launch the service, simply run the following command:</p><div><pre class="programlisting">
<strong>kubectl create -f mysql-service.yaml</strong>
</pre></div><p>So now that we have the MySQL Pod and Service launched, it's time to launch the actual WordPress container.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec22"/>Launching WordPress</h3></div></div></div><p>Like the <a id="id435" class="indexterm"/>MySQL Pod and Service, we will be launching our WordPress container using two files. The first file is for the Pod:</p><div><pre class="programlisting">apiVersion: v1
kind: Pod
metadata:
  name: wordpress
  labels: 
    name: wordpress
spec: 
  containers: 
    - image: wordpress
      name: wordpress
      env:
        - name: WORDPRESS_DB_PASSWORD
          value: yourpassword
      ports: 
        - containerPort: 80
          name: wordpress
      volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
  volumes:
    - name: wordpress-persistent-storage
      awsElasticBlockStore:
        volumeID: &lt;insert your volume id here&gt;
        fsType: ext4</pre></div><p>As an EBS <a id="id436" class="indexterm"/>volume cannot be attached to more than one device at a time, remember to use the second EBS volume you created here. Call the <code class="literal">wordpress.yaml</code> file and launch it using the following command:</p><div><pre class="programlisting">
<strong>kubectl create -f wordpress.yaml</strong>
</pre></div><p>Then wait for the Pod to launch:</p><div><img src="img/B05468_07_18.jpg" alt="Launching WordPress"/></div><p>As we have already removed <code class="literal">nginxcluster</code>, there should be enough resources to launch the Pod straightaway, meaning that you should not get any errors.</p><p>Although the Pod should be running, it's best to check whether the container launched without any problems. To do this, run the following command:</p><div><pre class="programlisting">
<strong>kubectl logs wordpress</strong>
</pre></div><p>This should print out the container logs, you will see something similar to the following screenshot:</p><div><img src="img/B05468_07_19.jpg" alt="Launching WordPress"/></div><p>Now that the <a id="id437" class="indexterm"/>Pod has launched and WordPress appears to have bootstrapped itself as expected, we should launch the service. Like <code class="literal">nginxcluster</code>, this will create an Elastic Load Balancer. The service definition file looks similar to the following code:</p><div><pre class="programlisting">apiVersion: v1
kind: Service
metadata: 
  labels: 
    name: wpfrontend
  name: wpfrontend
spec: 
  ports:
    - port: 80
  selector: 
    name: wordpress
  type: LoadBalancer</pre></div><p>To launch it, run the following command:</p><div><pre class="programlisting">
<strong>kubectl create -f wordpress-service.yaml</strong>
</pre></div><p>Once launched, check whether the service has been created and get the details of the Elastic Load Balancer by running the following command:</p><div><pre class="programlisting">
<strong>kubectl get services</strong>
<strong>kubectl describe service wpfrontend</strong>
</pre></div><p>When I ran the commands, I got the following output:</p><div><img src="img/B05468_07_20.jpg" alt="Launching WordPress"/></div><p>After a few<a id="id438" class="indexterm"/> minutes, you should be able to access the URL for Elastic Load Balancer, and as expected, you will be presented with a WordPress installation screen:</p><div><img src="img/B05468_07_21.jpg" alt="Launching WordPress"/></div><p>As we did in <a class="link" href="ch03.html" title="Chapter 3. Volume Plugins">Chapter 3</a>, <em>Volume Plugins</em> when we were looking at storage plugins, complete the installation, log in, and attach an image to the <code class="literal">Hello World</code> post.</p><p>Now that we have the WordPress site up and running, let's try removing the wordpress Pod and relaunching it, first of let's make a note of the Container ID:</p><div><pre class="programlisting">
<strong>kubectl describe pod wordpress | grep "Container ID"</strong>
</pre></div><p>Then delete<a id="id439" class="indexterm"/> the Pod and relaunch it:</p><div><pre class="programlisting">
<strong>kubectl delete pod wordpress</strong>
<strong>kubectl create -f wordpress.yaml</strong>
</pre></div><p>Check the Container ID again to make sure that we have a different one:</p><div><pre class="programlisting">
<strong>kubectl describe pod wordpress | grep "Container ID"</strong>
</pre></div><div><img src="img/B05468_07_22.jpg" alt="Launching WordPress"/></div><p>Going to your WordPress site, you should see everything exactly as you left it:</p><div><img src="img/B05468_07_23.jpg" alt="Launching WordPress"/></div><p>If we wanted to, we could perform the same action for the MySQL pod and our data would be <a id="id440" class="indexterm"/>exactly as we left it, as it is stored in the EBS volume.</p><p>Let's remove the Pod and Service for the WordPress application by running the following command:</p><div><pre class="programlisting">
<strong>kubectl delete pod wordpress</strong>
<strong>kubectl delete pod mysql</strong>
<strong>kubectl delete service wpfrontend</strong>
<strong>kubectl delete service mysql</strong>
</pre></div><p>This should leave us with a nice clean Kubernetes cluster for the next section of the chapter.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec23"/>Supporting tools</h3></div></div></div><p>You may be wondering to yourself why we bothered grabbing the username and password when we first deployed our Kubernetes cluster as we have not had to use it yet. Let's take a look at <a id="id441" class="indexterm"/>some of the supporting tools that are deployed as part of our Kubernetes cluster.</p><p>When you first deployed your Kubernetes cluster, there was a list of URLs printed on the screen, we will be using these for this section. Don't worry if you didn't make a note of them as you can get all the URLs for the supporting tools by running the following command:</p><div><pre class="programlisting">
<strong>kubectl cluster-info</strong>
</pre></div><p>This will print out a list of URLs for the various parts of your Kubernetes cluster:</p><div><img src="img/B05468_07_24.jpg" alt="Supporting tools"/></div><p>You will need the username and password to view some of these tools, again if you don't have these to hand, you can get them by running the following command:</p><div><pre class="programlisting">
<strong>tail -3 ~/.kube/config</strong>
</pre></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec01"/>Kubernetes Dashboard</h4></div></div></div><p>First of all, let's take<a id="id442" class="indexterm"/> a look at the <a id="id443" class="indexterm"/>Kubernetes Dashboard. You can get this by putting the URL for the Kubernetes-dashboard in your browser. When you enter it, depending on your browser, you will get warnings about the certificates, accept the warnings and you will be given a login prompt. Enter the username and password here. Once logged in, you will see the following screen:</p><div><img src="img/B05468_07_25.jpg" alt="Kubernetes Dashboard"/></div><p>Let's deploy the <a id="id444" class="indexterm"/>NGINX Cluster application <a id="id445" class="indexterm"/>using the UI. To do this, click on <strong>Deploy An App</strong> and enter the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>App name</strong> = <code class="literal">nginx-cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Container image</strong> = <code class="literal">russmckendrick/cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Number of pods</strong> = <code class="literal">5</code></li><li class="listitem" style="list-style-type: disc"><strong>Port</strong> = Leave blank</li><li class="listitem" style="list-style-type: disc"><strong>Port</strong> = <code class="literal">80</code></li><li class="listitem" style="list-style-type: disc"><strong>Target port</strong> = <code class="literal">80</code></li><li class="listitem" style="list-style-type: disc">Tick the box for <strong>Expose service externally</strong></li></ul></div><div><img src="img/B05468_07_26.jpg" alt="Kubernetes Dashboard"/></div><p>Once you <a id="id446" class="indexterm"/>click on <strong>Deploy</strong>, you will be taken<a id="id447" class="indexterm"/> back to the overview screen:</p><div><img src="img/B05468_07_27.jpg" alt="Kubernetes Dashboard"/></div><p>From here, you <a id="id448" class="indexterm"/>can click on <strong>nginx-cluster</strong> and be<a id="id449" class="indexterm"/> taken to an overview screen:</p><div><img src="img/B05468_07_28.jpg" alt="Kubernetes Dashboard"/></div><p>As you can see, this gives you all the details on both the Pod and Service, with details such as the CPU and memory utilization, as well as a link to the Elastic Load Balancer. Clicking the <a id="id450" class="indexterm"/>link should take you to the<a id="id451" class="indexterm"/> default cluster page of the image and the container's hostname.</p><p>Let's leave nginx-cluster up and running to look at the next tool.</p></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec02"/>Grafana</h4></div></div></div><p>The next URL<a id="id452" class="indexterm"/> that we are going to open is Grafana; going to<a id="id453" class="indexterm"/> the URL, you should see a quite dark and mostly empty page.</p><p>Grafana is the tool that is recording all the metrics that we saw being displayed in the Kubernetes dashboard. Let's take a look at the cluster stats. To do this, click on the <strong>Cluster</strong> dashboard:</p><div><img src="img/B05468_07_30.jpg" alt="Grafana"/></div><p>As you can see, this gives us a breakdown of all of the metrics that you would expect to see from a system-monitoring tool. Scrolling down, you can see:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CPU Usage</li><li class="listitem" style="list-style-type: disc">Memory Usage</li><li class="listitem" style="list-style-type: disc">Network Usage</li><li class="listitem" style="list-style-type: disc">Filesystem Usage</li></ul></div><p>Both collectively<a id="id454" class="indexterm"/> and per individual node. You can also view<a id="id455" class="indexterm"/> details on Pods by clicking on the <strong>Pods</strong> dashboard. As Grafana gets its data from the InfluxDB pod, which has been running since we first launched our Kubernetes cluster, you can view metrics for every Pod that you have launched, even if it is not currently running. The following is the Pod metrics for the <code class="literal">mysql</code> pod we launched when installing WordPress:</p><div><img src="img/B05468_07_31.jpg" alt="Grafana"/></div><p>I would recommend you to look around to view some of the other Pod metrics.</p></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec03"/>ELK</h4></div></div></div><p>The final tool we <a id="id456" class="indexterm"/>are going to look at is the ELK stack that has<a id="id457" class="indexterm"/> been running in the background since we first launch our Kubernetes cluster. An ELK stack is a collection of the following three different<a id="id458" class="indexterm"/> tools:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Elasticsearch</strong>: <a class="ulink" href="https://www.elastic.co/products/elasticsearch">https://www.elastic.co/products/elasticsearch</a></li><li class="listitem" style="list-style-type: disc"><strong>Logstash</strong>: <a class="ulink" href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a></li><li class="listitem" style="list-style-type: disc"><strong>Kibana</strong>: <a class="ulink" href="https://www.elastic.co/products/kibana">https://www.elastic.co/products/kibana</a></li></ul></div><p>Together<a id="id459" class="indexterm"/> they form a powerful central logging platform.</p><p>When we ran<a id="id460" class="indexterm"/> the following command earlier in this section of the chapter (please note you will not be able to run it again as we removed the WordPress pod):</p><div><pre class="programlisting">
<strong>kubectl logs wordpress</strong>
</pre></div><p>The logs displayed for our <code class="literal">wordpress</code> pod the log file entries were actually read from the Elasticsearch pod. Elasticsearch comes with its own dashboard called Kibana. Let's open the Kibana URL.</p><p>When you first open Kibana, you will be asked to configure an index pattern. To do this, just select Time-field name from the drop-down box and click on <strong>Create</strong> button:</p><div><img src="img/B05468_07_32.jpg" alt="ELK"/></div><p>Once the index pattern has been created, click on the <strong>Discover</strong> link in the top menu. You will then be taken to an overview of all of the log data that has been sent to Elasticsearch by the Logstash installations that are running on each of the nodes:</p><div><img src="img/B05468_07_33.jpg" alt="ELK"/></div><p>As you can <a id="id461" class="indexterm"/>see, there is a lot of data being logged; in<a id="id462" class="indexterm"/> fact, when I looked, there were 4,918 messages logged within 15 minutes alone. There is a lot of data in here, I would recommend clicking around and trying some searches to get an idea of what is being logged.</p><p>To give you an idea of what each log entry looks like, here is one for one of my nginx-cluser pods:</p><div><img src="img/B05468_07_34.jpg" alt="ELK"/></div></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec04"/>Remaining cluster tools</h4></div></div></div><p>The remaining<a id="id463" class="indexterm"/> cluster tools that we are yet to open in the browser are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Kubernetes</strong></li><li class="listitem" style="list-style-type: disc"><strong>Heapster</strong></li><li class="listitem" style="list-style-type: disc"><strong>KubeDNS</strong></li><li class="listitem" style="list-style-type: disc"><strong>InfluxDB</strong></li></ul></div><p>These all are API endpoints, so you will not see anything other than an API response, they are using by Kubernetes internally to both manage and schedule within the cluster.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec67"/>Destroying the cluster</h2></div></div></div><p>As the<a id="id464" class="indexterm"/> cluster is sat in your Amazon Web Services account on instances that are pay-as-you-go, we should look at removing the cluster; to do this, let's re-enter the original configuration that we entered when we first deployed the Kubernetes cluster by running the following command:</p><div><pre class="programlisting">
<strong>export KUBE_AWS_ZONE=eu-west-1c</strong>
<strong>export AWS_S3_REGION=eu-west-1</strong>
<strong>export NUM_NODES=2</strong>
<strong>export KUBERNETES_PROVIDER=aws</strong>
</pre></div><p>Then, from the same location you first deployed your Kubernetes cluster, run the following command:</p><div><pre class="programlisting">
<strong>./kubernetes/cluster/kube-down.sh</strong>
</pre></div><p>This will<a id="id465" class="indexterm"/> connect to the AWS API and start to tear down all of the instances, configuration, and any other resources that have been launched with Kubernetes.</p><p>The process will take several minutes, do not interrupt it or you maybe left with resources that incur costs running within your Amazon Web Services account:</p><div><img src="img/B05468_07_35.jpg" alt="Destroying the cluster"/></div><p>I would also recommend logging into your Amazon Web Services console and remove the unattached EBS volumes that we created for the WordPress installation and also any Kubernetes labelled S3 buckets as these will be incurring costs as well.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec68"/>Recap</h2></div></div></div><p>Kubernetes, like Docker, has matured a lot since its first public release. It has become easier to deploy and manage with each release without having a negative impact on the feature set.</p><p>As a solution<a id="id466" class="indexterm"/> that offers scheduling for your containers, it is second to none, and as it is not tied to any particular provider, you can easily deploy it to providers other than Amazon Web Services, such as Google's own Cloud Platform, where it is considered a first class citizen. It is also possible to deploy it on premise on your own bare metal of virtual servers, making sure that it keeps itself inline with the build once and deploy anywhere philosophy that Docker has.</p><p>Also, it adapts to work with the technologies available in every platform you deploy it onto; for example, if you need persistent storage, then as already mentioned, there are multiple options available to you.</p><p>Finally, just like Docker has been over the past 18 months, Kubernetes has quite a unifying platform, with multiple vendors such as Google, Microsoft, and Red Hat. They all support and use it as part of their products.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Amazon EC2 Container Service (ECS)</h1></div></div></div><p>The next tool <a id="id467" class="indexterm"/>that we are going to be looking at is the Elastic Container Service from Amazon. The description that Amazon gives is as follows:</p><div><blockquote class="blockquote"><p><em>"Amazon EC2 Container Service (ECS) is a highly scalable, high performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances. Amazon ECS eliminates the need for you to install, operate, and scale your own cluster management infrastructure. With simple API calls, you can launch and stop Docker-enabled applications, query the complete state of your cluster, and access many familiar features like security groups, Elastic Load Balancing, EBS volumes, and IAM roles. You can use Amazon ECS to schedule the placement of containers across your cluster based on your resource needs and availability requirements. You can also integrate your own scheduler or third-party schedulers to<a id="id468" class="indexterm"/> meet business or application specific requirements." - <a class="ulink" href="https://aws.amazon.com/ecs/">https://aws.amazon.com/ecs/</a>
</em></p></blockquote></div><p>It wasn't a surprise that Amazon would offer their own container-based service. After all, if you are following Amazon's best practices, then you will already be treating each of your EC2 instances in the same way you are treating your containers.</p><p>When I deploy applications into Amazon Web Services, I always try to ensure that I build and deploy production-ready images, along with ensuring that all the data written by the application is<a id="id469" class="indexterm"/> sent to a shared source as the instances could be terminated any time due to scaling events.</p><p>To help support this approach, Amazon offers a wide range of services such as:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Elastic Load Balancing</strong> (<strong>ELB</strong>): This is <a id="id470" class="indexterm"/>a highly available and scalable load balancer</li><li class="listitem" style="list-style-type: disc"><strong>Amazon Elastic Block Store</strong> (<strong>EBS</strong>): This provides persistent block-level storage volumes<a id="id471" class="indexterm"/> for your compute resources</li><li class="listitem" style="list-style-type: disc"><strong>Auto Scaling</strong>: This <a id="id472" class="indexterm"/>scales EC2 resources up and down, allowing you to manage both, peaks in traffic and failures within the application</li><li class="listitem" style="list-style-type: disc"><strong>Amazon Relational Database Service</strong> (<strong>RDS</strong>): This is a highly available database <a id="id473" class="indexterm"/>as a service supporting MySQL, Postgres, and Microsoft SQL</li></ul></div><p>All of these are designed to help you remove all single points of failure within your Amazon-hosted application.</p><p>Also, as all of Amazon's services are API-driven, it wasn't too much of a jump for them to extend support to Docker containers.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec69"/>Launching ECS in the console</h2></div></div></div><p>I am going<a id="id474" class="indexterm"/> to be using the the AWS Console to launch my ECS cluster. As my AWS account is quite old, a few of the steps may differ. To try and account for this, I will be launching my cluster in one of the newer AWS regions.</p><p>Once you have<a id="id475" class="indexterm"/> logged into the AWS Console at <a class="ulink" href="http://console.aws.amazon.com/">http://console.aws.amazon.com/</a>, make sure that you are in the region you would like to launch your ECS cluster in, and then click on the <strong>EC2 Container Service</strong> link from the <strong>Services</strong> drop-down menu.</p><p>As this is your first time launching an ECS cluster, you will be greeted with an overview video of the service.</p><p>Click on <strong>Get started</strong> to be taken to the Wizard that will help us launch our first cluster.</p><p>First of all, you will be prompted to create a task definition. This is the equivalent of creating a Docker Compose file. Here you will define the container image that you would like to run and the resources it is allowed to consume, such as RAM and CPU. You will also map the ports from the host to container here.</p><p>For now, we will use the defaults and look at launching our own containers once the cluster is up and running. Fill in the details as per the following screenshot and click on <strong>Next step</strong>:</p><div><img src="img/B05468_07_37.jpg" alt="Launching ECS in the console"/></div><p>Now that <a id="id476" class="indexterm"/>the task has been defined, we need to attach it to a service. This allows us to create a group of tasks, which initially will be three copies of the <code class="literal">console-sample-app-static</code> task, and register them with an Elastic Load Balancer. Fill in the details as per the following screenshot and click on <strong>Next step</strong> button:</p><div><img src="img/B05468_07_38.jpg" alt="Launching ECS in the console"/></div><p>Now that <a id="id477" class="indexterm"/>we have the service defined, we need a location to launch it. This is where EC2 instances come into play, and also where you still to be charged. While the Amazon EC2 Container Service is free of charge to set up, you will be charged for the resources used to deliver the compute side of the cluster. These will be your standard EC2 instance charges. Fill in the details as per the following screenshot and click on <strong>Review &amp; launch</strong>:</p><div><img src="img/B05468_07_39.jpg" alt="Launching ECS in the console"/></div><p>Before <a id="id478" class="indexterm"/>anything is launched, you will get the opportunity to double-check everything that is configured within your AWS account, this is your last chance to back out of launching the ECS cluster. If you are happy with everything, click on <strong>Launch instance &amp; run service</strong> button:</p><div><img src="img/B05468_07_40.jpg" alt="Launching ECS in the console"/></div><p>What you<a id="id479" class="indexterm"/> will see now is an overview of what is happening. Typically, it will take about 10 minutes to run through these tasks. In the background, it is doing the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating an IAM role that accesses the ECS service</li><li class="listitem" style="list-style-type: disc">Creating a VPC for your cluster to be launched in</li><li class="listitem" style="list-style-type: disc">Creating a Launch Configuration to run an Amazon ECS-optimized Amazon Linux AMI with the ECS IAM role</li><li class="listitem" style="list-style-type: disc">Attaching the newly created Launch Configuration to an Auto Scaling Group and configuring it with the number of instances you defined</li><li class="listitem" style="list-style-type: disc">Creating the ECS Cluster, Task, and Service within the Console</li><li class="listitem" style="list-style-type: disc">Waiting for the EC2 instances that have been launched by the Auto Scaling Group to launch and register themselves with the ECS service</li><li class="listitem" style="list-style-type: disc">Running the Service on your newly created ECS cluster</li><li class="listitem" style="list-style-type: disc">Creating an Elastic Load Balancer and registering your Service with it</li></ul></div><p>You can<a id="id480" class="indexterm"/> find more information on the Amazon ECS-Optimized Amazon Linux AMI on its AWS Marketplace page at <a class="ulink" href="https://aws.amazon.com/marketplace/pp/B00U6QTYI2/ref=srh_res_product_title?ie=UTF8&amp;sr=0-2&amp;qid=1460291696921">https://aws.amazon.com/marketplace/pp/B00U6QTYI2/ref=srh_res_product_title?ie=UTF8&amp;sr=0-2&amp;qid=1460291696921</a>. This image is a cut-down version of Amazon Linux that only runs on Docker.</p><p>Once everything is completed, you will be given the option to go to your newly created Service. You should see something similar to the following screenshot:</p><div><img src="img/B05468_07_41.jpg" alt="Launching ECS in the console"/></div><p>As you can see, we have three running tasks and a load balancer.</p><p>Now let's create our own task and service. From the preceding Service view, click on <strong>Update</strong> button and change the desired count from three to zero, this will stop the tasks and allow <a id="id481" class="indexterm"/>us to remove the Service. To do this, click on <strong>default</strong> button to go to the cluster view and then remove the Service.</p><p>Now that the <code class="literal">sample-webapp</code> Service has been removed, click on the <strong>Task Definitions</strong> button and then the <strong>Create new task definition</strong> button. On the page that opens, click on the <strong>Add container</strong> button and fill in the following details:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Container name</strong>: <code class="literal">cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Image</strong>: <code class="literal">russmckendrick/cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Maximum memory (MB)</strong>: <code class="literal">32</code></li><li class="listitem" style="list-style-type: disc"><strong>Port mappings</strong>: <code class="literal">80</code> (<strong>Host port</strong>) <code class="literal">80</code> (<strong>Container port</strong>) <code class="literal">tcp</code> (<strong>Protocol</strong>)</li></ul></div><p>Everything else can be left at the default values:</p><div><img src="img/B05468_07_42.jpg" alt="Launching ECS in the console"/></div><p>Once filled in, click on the <strong>Add</strong> button. This will take you back to the <strong>Create a Task Definition</strong> screen, fill in the Task Definition Name, let's call it <code class="literal">our-awesome-cluster</code> and then click <a id="id482" class="indexterm"/>on the <strong>Create</strong> button:</p><div><img src="img/B05468_07_43.jpg" alt="Launching ECS in the console"/></div><p>Now that we have our new Task defined, we need to create a Service to attach it to. Click on the <strong>Clusters</strong> tab, then click on the <strong>default</strong> cluster, you should see something similar to the following image:</p><div><img src="img/B05468_07_44.jpg" alt="Launching ECS in the console"/></div><p>Click <a id="id483" class="indexterm"/>on the <strong>Create</strong> button in the <strong>Services</strong> tab. From this screen, fill in the following information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Task Definition</strong>: <code class="literal">our-awesome-cluster:1</code></li><li class="listitem" style="list-style-type: disc"><strong>Cluster</strong>: <code class="literal">default</code></li><li class="listitem" style="list-style-type: disc"><strong>Service name</strong>: <code class="literal">Our-Awesome-Cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Number of tasks</strong>: <code class="literal">3</code></li><li class="listitem" style="list-style-type: disc"><strong>Minimum healthy percent</strong>: <code class="literal">50</code></li><li class="listitem" style="list-style-type: disc"><strong>Maximum percent</strong>: <code class="literal">200</code></li></ul></div><p>Also, in the <strong>Optional configurations</strong> section, click on <strong>Configure ELB</strong> button and use the Elastic Load Balancer that was originally configured for the <code class="literal">sample-webapp</code> service:</p><div><img src="img/B05468_07_45.jpg" alt="Launching ECS in the console"/></div><p>Once<a id="id484" class="indexterm"/> you have filled in the information, click on the <strong>Create Service</strong> button. If all goes well, you should see something similar to the following page:</p><div><img src="img/B05468_07_46.jpg" alt="Launching ECS in the console"/></div><p>Clicking <a id="id485" class="indexterm"/>on <strong>View Service</strong> will give you an overview similar to the one we first saw for the <code class="literal">Sample-Webapp</code> Service:</p><div><img src="img/B05468_07_47.jpg" alt="Launching ECS in the console"/></div><p>All that's <a id="id486" class="indexterm"/>left to do now is to click on <strong>Load Balancer Name</strong> to be taken to the ELB overview page; from here, you will be able to get the URL for the ELB, putting this into a browser should show you our clustered application:</p><div><img src="img/B05468_07_48.jpg" alt="Launching ECS in the console"/></div><p>Click<a id="id487" class="indexterm"/> refresh a few times and you should see the container's hostname change, indicating that we are being load balanced between different containers.</p><p>Rather than launching any more instances, let's terminate our cluster. To do this, go to the <strong>EC2</strong> service in the <strong>Services</strong> menu at the top of the AWS Console.</p><p>From here, scroll down to <strong>Auto Scaling Groups</strong> that can be found at the bottom of the left-hand side menu. From here, remove the auto scaling group and then the launch configuration. This will terminate the three EC2 instances that formed our ECS cluster.</p><p>Once the instances have been terminated, click on <strong>Load Balancer</strong> and terminate the Elastic Load Balancer.</p><p>Finally, go back to the <strong>EC2 Container Service</strong> and delete the default cluster by clicking on the <strong>x</strong>. This will remove the remainder of the resources that were created by us launching the ECS cluster.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec70"/>Recap</h2></div></div></div><p>As you can see, Amazon's EC2 Container Service can be run from the web-based AWS Console. There are command tools available, but I won't be covering them here. Why, you might ask?</p><p>Well, although the <a id="id488" class="indexterm"/>service offering Amazon has built is complete, it feels very much like a product that is in an early alpha stage. The versions of Docker that ship on the Amazon ECS-Optimized Amazon Linux AMI are quite old. The process of having to launch instances outside of the default stack feels very clunky. Its integration with some of the supporting services provided by Amazon is also a very manual process, making it feel incomplete. There is also the feeling that you don't have much control.</p><p>Personally, I think the service has a lot of potential; however, in the last 12 months, a lot of alternatives have launched and are being developed at a more rapid pace, meaning that Amazon's ECS service is left feeling old and quite outdated compared to the other services we are looking at.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Rancher</h1></div></div></div><p>Rancher is<a id="id489" class="indexterm"/> a relatively new player, at the time of writing this book, it has only just hit its 1.0 release. Rancher Labs (the developers) describe Rancher (the platform) as:</p><div><blockquote class="blockquote"><p><em>"An open source software platform that implements a purpose-built infrastructure for running containers in production. Docker containers, as an increasingly popular application workload, create new requirements in infrastructure services such as networking, storage, load balancer, security, service discovery, and resource management.</em></p><p><em>Rancher takes in raw computing resources from any public or private cloud in the form of Linux hosts. Each Linux host can be a virtual machine or a physical machine. Rancher does not expect more from each host than CPU, memory, local disk storage, and network connectivity. From Rancher's perspective, a VM instance from a cloud provider and a bare metal server hosted at a colo facility <a id="id490" class="indexterm"/>are indistinguishable." - <a class="ulink" href="http://docs.rancher.com/rancher/">http://docs.rancher.com/rancher/</a>
</em></p></blockquote></div><p>Rancher Labs also provide RancherOS—a tiny Linux distribution that runs the entire operating system as Docker containers. We will look at that in the next chapter.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec71"/>Installing Rancher</h2></div></div></div><p>Rancher needs<a id="id491" class="indexterm"/> a host to run on, so let's launch a server in DigitalOcean using Docker Machine:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    --driver digitalocean \</strong>
<strong>    --digitalocean-access-token sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0 \</strong>
<strong>    --digitalocean-region lon1 \</strong>
<strong>    --digitalocean-size 1gb \</strong>
<strong>    rancher</strong>
</pre></div><p>Rancher runs as a container, so rather than using SSH to connect to the newly launched Docker host, let's <a id="id492" class="indexterm"/>configure our local client to connect to the host and then we can launch Rancher:</p><div><pre class="programlisting">
<strong>eval $(docker-machine env rancher)</strong>
<strong>docker run -d --restart=always -p 8080:8080 rancher/server</strong>
</pre></div><p>That's it, Rancher will be up and running shortly. You can watch the logs to keep an eye on when Rancher is ready.</p><p>First of all, check what the Rancher container is called by running the following command:</p><div><pre class="programlisting">
<strong>docker ps</strong>
</pre></div><p>In my case, it was <code class="literal">jolly_hodgkin</code>, so now run the following command:</p><div><pre class="programlisting">
<strong>docker logs -f &lt;name of your container&gt;</strong>
</pre></div><div><img src="img/B05468_07_49.jpg" alt="Installing Rancher"/></div><p>You should see a lot of log file entries scroll pass, after a while, logs will stop being written. This is a sign that Rancher is ready and you can log in to the web interface. To do this, run the following command to open your browser:</p><div><pre class="programlisting">
<strong>open http://$(docker-machine ip rancher):8080/</strong>
</pre></div><p>Once open, you should see something similar to the following screenshot:</p><div><img src="img/B05468_07_50.jpg" alt="Installing Rancher"/></div><p>As you can<a id="id493" class="indexterm"/> see, we have logged in straight. As this is available on a public IP address, we have better lock the installation down. This is why the red warning icon is next to <strong>Admin</strong> in the top menu is there.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec72"/>Securing your Rancher installation</h2></div></div></div><p>As I don't have<a id="id494" class="indexterm"/> an Active Directory server configured, I am going to use GitHub to authenticate against my Rancher installation. Just like the installation itself, Rancher Labs have made this a really easy process. First of all, click on <strong>Admin</strong> in the top menu and then <strong>Access Control</strong> in the secondary menu, you will be taken to a screen that allows you to know everything you need in order to configure Rancher to use GitHub as its authentication backend.</p><p>For me, this screen looked similar to the following image:</p><div><img src="img/B05468_07_51.jpg" alt="Securing your Rancher installation"/></div><p>As I have a <a id="id495" class="indexterm"/>standard GitHub account rather than the Enterprise installation, all I had to do was click on the link, this took me to a page where I could register my Rancher installation.</p><p>This asked for several pieces of information, all of which are provided on the following screen:</p><div><img src="img/B05468_07_52.jpg" alt="Securing your Rancher installation"/></div><p>Once I filled <a id="id496" class="indexterm"/>in the information, I clicked on <strong>Register application</strong> button. Once the application had been registered, I was taken a page that gave me a Client ID and Client Secret:</p><div><img src="img/B05468_07_53.jpg" alt="Securing your Rancher installation"/></div><p>I entered <a id="id497" class="indexterm"/>these parameters into appropriate boxes on my Rancher page and then clicked on <strong>Authenticate with GitHub</strong>. This prompted a pop-up window from GitHub asking me to authorize the application. Clicking the <strong>Authorize application</strong> button refreshed the Rancher screen and logged me in, as you can see from the following screenshot, my application is now secure:</p><div><img src="img/B05468_07_54.jpg" alt="Securing your Rancher installation"/></div><p>Now that we<a id="id498" class="indexterm"/> have the authentication configured, you should probably log out and log back in just to double-check whether everything is working as expected before we move onto the next step. To do this, click on your avatar at the right-hand top of the page and click on <strong>Log Out</strong>.</p><p>You will be instantly taken to the following page:</p><div><img src="img/B05468_07_55.jpg" alt="Securing your Rancher installation"/></div><p>Click on <strong>Authenticate with GitHub</strong> to log back in.</p><p>So, why<a id="id499" class="indexterm"/> did we log out and then logged back in? Well, next up, we are going to be giving our Rancher installation our DigitalOcean API key so that it can launch hosts, if we hadn't secured our installation before adding this API key, it would mean that anyone could stumble upon our Rancher installation and start launching hosts as they see fit. This, as I am sure you could imagine, could get very expensive.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec73"/>Cattle cluster</h2></div></div></div><p>Rancher supports three different schedulers, we have already looked at two of them in both this and the previous chapters. From our Rancher installation, we will be able to launch a Docker Swarm Cluster, Kubernetes cluster, and also Rancher cluster.</p><p>For this part of the<a id="id500" class="indexterm"/> chapter, we are going to be looking at a Rancher cluster. The <a id="id501" class="indexterm"/>scheduler that will be used here is called Cattle. It is also the default scheduler, so we do not need to configure it, all we need to do is add some hosts.</p><p>As mentioned in the previous section, we are going to launch our hosts in DigitalOcean; to do this, click on <strong>Add Host</strong> in the <strong>Adding your first Host</strong> section of the front page.</p><p>You will be taken to a page with several hosting providers listed at the top, click on DigitalOcean and then enter the following details:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Quantity</strong>: I wanted to launch three hosts, so I dragged the slider to <code class="literal">3</code>.</li><li class="listitem" style="list-style-type: disc"><strong>Name</strong>: This is how the hosts will appear in my DigitalOcean control panel.</li><li class="listitem" style="list-style-type: disc"><strong>Description</strong>: A quick description to be attached to each host.</li><li class="listitem" style="list-style-type: disc"><strong>Access Token</strong>: This is my API token, you should have yours from <a class="link" href="ch02.html" title="Chapter 2. Introducing First-party Tools">Chapter 2</a>, <em>The First-party Tools</em>.</li><li class="listitem" style="list-style-type: disc"><strong>Image</strong>: At the moment, only Ubuntu 14.04x64 is supported.</li><li class="listitem" style="list-style-type: disc"><strong>Size</strong>: This is the size of the host you would like to launch. Don't forget, the bigger the host, the more money you will pay while the host is online.</li><li class="listitem" style="list-style-type: disc"><strong>Region</strong>: Which DigitalOcean data center would you like to launch the hosts in?</li></ul></div><p>I left the <a id="id502" class="indexterm"/>remainder<a id="id503" class="indexterm"/> of the options at their defaults:</p><div><img src="img/B05468_07_56.jpg" alt="Cattle cluster"/></div><p>Once I was happy with what I had entered, I clicked on <strong>Create</strong> button. Rancher then, using the DigitalOcean API, went ahead and launched my hosts.</p><p>To check the status of the hosts, you should click on <strong>Infrastructure</strong> in the top menu and then <strong>Hosts</strong> in the secondary menu.</p><p>Here, you should see the hosts you are deploying, along with their status, which is updating in<a id="id504" class="indexterm"/> real time. You should see messages saying the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The host has<a id="id505" class="indexterm"/> been launched</li><li class="listitem" style="list-style-type: disc">Docker is being installed and configured</li><li class="listitem" style="list-style-type: disc">The Rancher agent is being installed and configured</li></ul></div><p>Finally, all three of your hosts are shown as active:</p><div><img src="img/B05468_07_57.jpg" alt="Cattle cluster"/></div><p>There you have it, your first Cattle cluster. As you can see, so far it has been incredibly easy to install, secure, and configure our first cluster in Rancher. Next up, we need to deploy our containers.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec74"/>Deploying the Cluster application</h2></div></div></div><p>As per the<a id="id506" class="indexterm"/> previous two schedulers, let's look at deploying our basic cluster application. To do this, click on the <strong>Applications</strong> tab in the top menu, and then click on <strong>Add Service</strong>. There is an option to <strong>Add From Catalog</strong>, we will be looking at this option when we have launched our own application.</p><p>On the <strong>Add Service</strong> page, enter the following information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Scale</strong>: <code class="literal">Always run one instance of this container on every host</code></li><li class="listitem" style="list-style-type: disc"><strong>Name</strong>: <code class="literal">MyClusterApp</code></li><li class="listitem" style="list-style-type: disc"><strong>Description</strong>: <code class="literal">My really awesome clustered application</code></li><li class="listitem" style="list-style-type: disc"><strong>Select Image</strong>: <code class="literal">russmckendrick/cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Port map</strong>: Add a port map for port <code class="literal">80</code> just in the <strong>Private port</strong> box<div><img src="img/B05468_07_58.jpg" alt="Deploying the Cluster application"/></div></li></ul></div><p>For now, leave<a id="id507" class="indexterm"/> the rest of the forms at their default values and click on the <strong>Create</strong> button.</p><p>After a few minutes, you should see that your service is active, clicking on the service name will take you a screen that gives you the details on all of the containers running within the service:</p><div><img src="img/B05468_07_59.jpg" alt="Deploying the Cluster application"/></div><p>So, now<a id="id508" class="indexterm"/> that we have our containers running, we really need to be able to access them. To configure a load balancer, click on <strong>Stacks</strong> and then on the downward arrow on our default service:</p><div><img src="img/B05468_07_60.jpg" alt="Deploying the Cluster application"/></div><p>Selecting <strong>Add Load Balancer</strong> from the drop-down menu will take you to a screen that looks similar to the one where we added our cluster application.</p><p>Fill in the following details:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Scale</strong>: <code class="literal">Run 1 container</code></li><li class="listitem" style="list-style-type: disc"><strong>Name</strong>: <code class="literal">ClusterLoadBalancer</code></li><li class="listitem" style="list-style-type: disc"><strong>Description</strong>: <code class="literal">The Load Balancer for my clustered application</code></li><li class="listitem" style="list-style-type: disc"><strong>Listening</strong> <strong>Ports</strong>: <code class="literal">Source IP/Port 80 Default Target Post 80</code></li><li class="listitem" style="list-style-type: disc"><strong>Target</strong> <strong>Service</strong>: <code class="literal">MyClusterApp</code><div><img src="img/B05468_07_61.jpg" alt="Deploying the Cluster application"/></div></li></ul></div><p>Click<a id="id509" class="indexterm"/> on the <strong>Save</strong> button and wait for the service to launch. You will be taken back to the list of services that you have launched, clicking on the information sign next to name of the load balancer will open an information pane at the bottom of the screen. From here, click on the IP address listed in the Ports section:</p><div><img src="img/B05468_07_62.jpg" alt="Deploying the Cluster application"/></div><p>Your<a id="id510" class="indexterm"/> browser should open the now-familiar cluster application page.</p><p>Clicking on refresh a few times should change the host name of the container you are being connected to.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec75"/>What's going on in the background?</h2></div></div></div><p>One of <a id="id511" class="indexterm"/>Rancher's strengths is that there are a lot of tasks, configuration, and process running in the background, which are all hidden by an intuitive and easy-to-use web interface.</p><p>To get an idea of what's going on, let's have a look around the interface. To start off with, click on <strong>Infrastructure</strong> in the top menu, and then click on <strong>Hosts</strong>.</p><p>As you can see, the running containers are now listed; alongside the containers for our Default stack, there is a network agent container running on each host:</p><div><img src="img/B05468_07_64.jpg" alt="What's going on in the background?"/></div><p>These containers form a network between all three of our hosts using iptables, allowing cross-host connectivity for our containers.</p><div><div><h3 class="title"><a id="note21"/>Note</h3><p>iptables is <a id="id512" class="indexterm"/>a user-space application program that allows a system administrator to configure the tables provided by the Linux kernel firewall (implemented <a id="id513" class="indexterm"/>as different Netfilter modules) and the chains and rules it stores:</p><p>
<a class="ulink" href="https://en.wikipedia.org/wiki/Iptables">https://en.wikipedia.org/wiki/Iptables</a>
</p></div></div><p>To confirm this, click on <strong>Containers</strong> button in the secondary menu. You will see a list of the currently running containers, this list should include three containers running our cluster application.</p><p>Make a note of the IP address for <strong>Default_MyClusterApp_2</strong> (in my case, it's <code class="literal">10.42.220.91</code>) and then click on <strong>Default_MyClusterApp_1</strong>.</p><p>You will be taken to a page that gives you real-time information about the CPU, memory, network, and storage utilization of the container:</p><div><img src="img/B05468_07_65.jpg" alt="What's going on in the background?"/></div><p>As you can <a id="id514" class="indexterm"/>see, the container is currently active on my first Rancher host. Let's get a little more information about the container by connecting to it. At the top right-hand side of the page, where it says <strong>Running</strong>, there is an icon with three dots, click on that, and then select <strong>Execute Shell</strong> from the drop-down menu.</p><p>This will open a terminal within your browser to the running container. Try entering some commands such as the following:</p><div><pre class="programlisting">
<strong>ps aux</strong>
<strong>hostname</strong>
<strong>cat /etc/*release</strong>
</pre></div><p>Also, while we have the shell open, let's ping our second container that is hosted on another one of our hosts (make sure that you replace the IP address with the one made a note of):</p><div><pre class="programlisting">
<strong>ping -c 2 10.42.220.91</strong>
</pre></div><p>As you can see, although it is on a different host within our cluster, we are able to ping it without any problems:</p><div><img src="img/B05468_07_66.jpg" alt="What's going on in the background?"/></div><p>Another<a id="id515" class="indexterm"/> feature that is useful is Health Check. Let's configure Health Check for our service and then simulate an error.</p><p>Click on <strong>Applications</strong> in the top menu, then on the <strong>+</strong> next to our Default stack, this will bring up a list of services that make up the stack. Click on the <strong>MyClusterApp</strong> service to be taken to the overview page.</p><p>From here, as we did to access the container shell, click on the icon with the three dots in the top right-hand side, next to where it says <strong>Active</strong>. From the drop-down menu, select <strong>Upgrade</strong>, this will take us to a stripped-down version of the page we filled in to create the initial service.</p><p>At the bottom of this page there are several tabs, click on <strong>Health Check</strong> and fill out the following information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Health Check</strong>: <code class="literal">HTTP Responds 2xx/3xx</code></li><li class="listitem" style="list-style-type: disc"><strong>HTTP Request</strong>: <code class="literal">/index.html</code></li><li class="listitem" style="list-style-type: disc"><strong>Port</strong>: <code class="literal">80</code></li><li class="listitem" style="list-style-type: disc"><strong>When Unhealthy</strong>: <code class="literal">Re-create</code></li></ul></div><div><img src="img/B05468_07_67.jpg" alt="What's going on in the background?"/></div><p>Leave the rest of the settings as they are and then click on the <strong>Upgrade</strong> button. You will be taken back to the list of services that are in the Default stack, and next to the <strong>MyClusterApp</strong> service, it will say <strong>Upgrading</strong>.</p><p>During<a id="id516" class="indexterm"/> the upgrade process, Rancher has relaunched our containers with the new configuration. It did this one at a time, meaning that there would have been no downtime as far as people browsing our application would have been concerned.</p><p>You may also notice that it says there are six containers, and also that the stack is degraded; to resolve this, click on the <strong>MyClusterApp</strong> service in order to be taken to the list of containers.</p><p>As you can see, three of them have a state of Stopped. To remove them, click on the <strong>Finish Upgrade</strong> button, next to where it says <strong>Degraded</strong>, this will remove the stopped containers and return us to a stopped state.</p><p>So now that we have a health checking, make sure that each of our containers is serving a web page, let's stop NGINX from running and see what happens.</p><p>To do this, click on any of our three containers and then open a console by selecting <strong>Execute Shell</strong> from the drop-down menu.</p><p>As our container is running supervised to manage the processes within the container, all we need to do is run the following command to stop NGINX:</p><div><pre class="programlisting">
<strong>supervisorctl stop nginx</strong>
</pre></div><p>Then we need to kill the NGINX processes; to do this, find out the process IDs by running the following code:</p><div><pre class="programlisting">
<strong>ps aux</strong>
</pre></div><p>In my case, the PIDs were 12 and 13, so to kill them, I will run the following command:</p><div><pre class="programlisting">
<strong>kill 12 13</strong>
</pre></div><p>This will<a id="id517" class="indexterm"/> stop NGINX, but keep the container up and running. After a few seconds, you will notice that the stats in the background disappear:</p><div><img src="img/B05468_07_68.jpg" alt="What's going on in the background?"/></div><p>Then your console will close, leaving you with something that looks similar to the following screenshot:</p><div><img src="img/B05468_07_69.jpg" alt="What's going on in the background?"/></div><p>Going back to the list of containers for the MyClusterApp service, you will notice that there is a new <strong>Default_MyClusterApp_2</strong> container running under a different IP address:</p><div><img src="img/B05468_07_70.jpg" alt="What's going on in the background?"/></div><p>Rancher<a id="id518" class="indexterm"/> has done exactly as we instructed it to, if port 80 on any of our containers stops responding for more than six seconds, it has to fail three checks that are made every 2,000 ms, then remove the container, and replace it with a new one.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec76"/>The catalog</h2></div></div></div><p>I am pretty <a id="id519" class="indexterm"/>sure that you would have clicked on the <strong>Catalog</strong> item in the top <a id="id520" class="indexterm"/>menu, this lists all the pre-built stacks that you can launch within Rancher. Let's look at launching WordPress using the catalog item. To do this, click on <strong>Catalog</strong> and scroll down to the bottom where you will see an entry for WordPress.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec24"/>WordPress</h3></div></div></div><p>Click on <strong>View Details</strong> to be taken to a screen where you are able to add a WordPress stack. All it<a id="id521" class="indexterm"/> asks is for you to provide a <strong>Name</strong> and <strong>Description</strong> for the stack, fill these in, and click on <strong>Launch</strong>.</p><p>This will launch two containers, one running MariaDB and the other running the WordPress container. These containers use the same images from the Docker Hub that we have been launching throughout the book.</p><p>If you click on <strong>Stacks</strong> in the secondary menu and then expand the two stacks. Once the WordPress stack is active, you will be able to click on the information icon next to where it says <strong>wordpress</strong>. Like before, this will give the IP address where you can access your WordPress installation:</p><div><img src="img/B05468_07_71.jpg" alt="WordPress"/></div><p>Clicking <a id="id522" class="indexterm"/>on it will open a new browser window and you will see a very familiar WordPress installation screen.</p><p>Again, Rancher did something interesting here. Remember that we have three hosts in total. One of these hosts is running a container that is acting as a load balancer for our <strong>ClusterApp</strong>, this is<a id="id523" class="indexterm"/> bound to port 80 on one of these hosts.</p><p>By default, the WordPress catalog stack launches the WordPress container and maps port 80 from the host to port 80 on the container. With no prompting from us, Rancher realized that one of our hosts already has a service bound to port 80, so it didn't even attempt to launch the WordPress container here, instead it chose the next available host without a service mapped to port 80 and launched our WordPress container there.</p><p>This is another example of Rancher doing tasks in the background to make the best use of the resources you have launched.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec25"/>Storage</h3></div></div></div><p>So far so good <a id="id524" class="indexterm"/>with Rancher, let's take a look at how we can add some shared storage to our installation. One of the things that DigitalOcean doesn't provide is block storage, because of which we will need to use a clustered filesystem, as we do<a id="id525" class="indexterm"/> not want to introduce a single point of failure within our application.</p><div><div><h3 class="title"><a id="note22"/>Note</h3><p>Gluster FS is <a id="id526" class="indexterm"/>a scalable network filesystem. Using common off-the-shelf hardware, you can create large distributed storage solutions for media streaming, data analysis, and other data and bandwidth-intensive<a id="id527" class="indexterm"/> tasks:</p><p>
<a class="ulink" href="https://www.gluster.org">https://www.gluster.org</a>
</p></div></div><p>As you may have noticed when browsing the catalog, there are several storage items in there that we are going to be looking at GlusterFS to provide our distributed storage:</p><div><img src="img/B05468_07_72.jpg" alt="Storage"/></div><p>Once we have our Gluster cluster up and running, we will then use Convoy to expose it to our containers. Before we do this, we need to start GlusterFS. To do this, click on <strong>View Details</strong> on the <strong>Gluster FS</strong> catalog item.</p><p>You will be taken to a form that details exactly what is going to be configured and how. For our purpose, we can leave all the settings as they are and click on the <strong>Launch</strong> button at the bottom of the page.</p><p>It will take a few minutes to launch. When it has completed, you will see that a total of 12 containers have been created. Of these, six of them will be running and the other six will be marked as started. This is not anything to worry about, as they are acting as the volumes for the running containers:</p><div><img src="img/B05468_07_73.jpg" alt="Storage"/></div><p>Now that we<a id="id528" class="indexterm"/> have our Gluster FS cluster up and running, we need to launch Convoy and let it know about the Gluster FS cluster. Go back to the catalog page and click on <strong>View Details</strong> next to the <strong>Convoy Gluster FS</strong> entry.</p><p>As we kept of the default options and names selected when we launched the Gluster FS cluster, we can leave everything at the defaults here, all we have to do is select our Gluster FS cluster from the Gluster FS service drop-down menu.</p><p>Once you have made the selection and clicked on <strong>Launch</strong>, it won't take long to download and launch the <code class="literal">convoy-gluster</code> containers. Once completed, you should have four containers running. As you may have noticed, a new icon for <strong>System</strong> has appeared next to <strong>Stacks</strong> on the secondary menu, this is where you will find your <code class="literal">Convoy Gluster</code> stack:</p><div><img src="img/B05468_07_74.jpg" alt="Storage"/></div><p>So, we now<a id="id529" class="indexterm"/> have our distributed storage ready. Before we put it to use, let's look at one more catalog item.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec26"/>Clustered database</h3></div></div></div><p>We don't <a id="id530" class="indexterm"/>really want to store our database on a shared or distrusted filesystem, one of the other items in the catalog launches a MariaDB Galera Cluster.</p><div><div><h3 class="title"><a id="note23"/>Note</h3><p>Galera Cluster<a id="id531" class="indexterm"/> for MySQL is a true Multimaster Cluster based on synchronous replication. Galera Cluster is an easy-to-use, high-availability solution that provides high-system uptime, no data loss, and scalability<a id="id532" class="indexterm"/> for future growth:</p><p>
<a class="ulink" href="http://galeracluster.com/products/">http://galeracluster.com/products/</a>
</p></div></div><p>The cluster will sit behind a load balancer, meaning that your database requests will always be directed to an active master database server. As earlier, click on <strong>View Details</strong> on the <strong>Galera Cluster</strong> item and then fill in the database credentials you wish the cluster to be configured with. These credentials are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">MySQL Root Password</li><li class="listitem" style="list-style-type: disc">MySQL Database Name</li><li class="listitem" style="list-style-type: disc">MySQL DB User</li><li class="listitem" style="list-style-type: disc">MySQL DB Password</li></ul></div><p>Once filled in, click on the <strong>Launch</strong> button. The cluster will take a few minutes to launch. Once launched, it will contain 13 containers, these make up the cluster and load balancer.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec27"/>Looking at WordPress again</h3></div></div></div><p>Now that we<a id="id533" class="indexterm"/> have our clustered filesystem configured, and also our clustered database, let's look at launching WordPress again.</p><p>To do this, click on <strong>Applications</strong> from the top menu, and then make sure that you are on the <strong>Stacks</strong> page, click on <strong>New Stack</strong>.</p><p>From here, give it the name <code class="literal">WordPress</code> and then click on <strong>Create</strong>, and now click on <strong>Add Service</strong>. You will need to fill in the following information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Scale</strong>: <code class="literal">Run 1 container (we will scale up later)</code></li><li class="listitem" style="list-style-type: disc"><strong>Name</strong>: <code class="literal">WordPress</code></li><li class="listitem" style="list-style-type: disc"><strong>Description</strong>: <code class="literal">My WordPress cluster</code></li><li class="listitem" style="list-style-type: disc"><strong>Select Image</strong>: <code class="literal">wordpress</code></li><li class="listitem" style="list-style-type: disc"><strong>Port Map</strong>: <code class="literal">Leave the public port blank and add 80 in the private port</code></li><li class="listitem" style="list-style-type: disc"><strong>Service Links</strong>: <strong>Destination Service</strong> should your <code class="literal">galera-lb</code> and the <code class="literal">
</code><strong>As Name</strong> <code class="literal">galera-lb</code></li></ul></div><p>We then need to <a id="id534" class="indexterm"/>enter the following details on the tabbed options along the bottom:</p><p>Command:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Enviroment Vars: Add the following variables:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Variable</strong> = <code class="literal">WORDPRESS_DB_HOST</code></li><li class="listitem" style="list-style-type: disc"><strong>Value</strong> = <code class="literal">galera-lb</code></li><li class="listitem" style="list-style-type: disc"><strong>Variable</strong> = <code class="literal">WORDPRESS_DB_NAME</code></li><li class="listitem" style="list-style-type: disc"><strong>Value</strong> = The name of the DB you created when setting up Galera</li><li class="listitem" style="list-style-type: disc"><strong>Variable</strong> = <code class="literal">WORDPRESS_DB_USER</code></li><li class="listitem" style="list-style-type: disc"><strong>Value</strong> = The user you created when setting up Galera</li><li class="listitem" style="list-style-type: disc"><strong>Variable</strong> = <code class="literal">WORDPRESS_DB_PASSWORD</code></li><li class="listitem" style="list-style-type: disc"><strong>Value</strong> = The password of the user you created when setting up Galera</li></ul></div></li></ul></div><p>Volumes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Add a volume as <code class="literal">wpcontent:/var/www/html/wp-content/</code></li><li class="listitem" style="list-style-type: disc">Volume Driver: convoy-gluster</li></ul></div><p>Then click on the <strong>Launch</strong> button. It will take a minute to download and start the container, once it has started, you should see the status change to Active. Once you have a healthy service, click on the drop-down menu next to Add Service and add a Load Balancer:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Name</strong>: <code class="literal">WordPressLB</code></li><li class="listitem" style="list-style-type: disc"><strong>Description</strong>: <code class="literal">My WordPress Load Balancer</code></li><li class="listitem" style="list-style-type: disc"><strong>Source IP/Port</strong>: <code class="literal">80</code></li><li class="listitem" style="list-style-type: disc"><strong>Default Target Port</strong>: <code class="literal">80</code></li><li class="listitem" style="list-style-type: disc"><strong>Target Service</strong>: <code class="literal">WordPress</code></li></ul></div><p>Once you have added the Load Balancer, click on the information icon next to the Load Balancer service to get the IP address, open this in your browser and then perform the WordPress installation, and add the featured image as we have done in other chapters.</p><p>Now we have a WordPress container up and running with a highly available database backend, which we can move between hosts maintaining the same IP address and content thanks to the load balancer and Gluster FS storage.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec28"/>DNS</h3></div></div></div><p>The last catalog item I thought I would cover is one of the DNS managers. What these items do is<a id="id535" class="indexterm"/> automatically connect with your DNS provider's API and create DNS records for each of the stacks and services you launch. As I use Route53 to manage my DNS records, I clicked on <strong>View Details</strong> on the <strong>Route53 DNS Stack</strong> on the catalog screen.</p><p>In the <em>Configuration Options</em> section, I entered the following information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>AWS access key</strong>: My<a id="id536" class="indexterm"/> access key, the user must have permission to access Route53</li><li class="listitem" style="list-style-type: disc"><strong>AWS secret key</strong>: The <a id="id537" class="indexterm"/>secret key that accompanies the preceding access key</li><li class="listitem" style="list-style-type: disc"><strong>AWS region</strong>: The<a id="id538" class="indexterm"/> region I want to use</li><li class="listitem" style="list-style-type: disc"><strong>Hosted zone</strong>: The<a id="id539" class="indexterm"/> zone I wanted to use was <code class="literal">mckendrick.io</code>, so I entered that here</li><li class="listitem" style="list-style-type: disc"><strong>TTL</strong>: I left this <a id="id540" class="indexterm"/>as the default <code class="literal">299 seconds</code>, if you want a quicker update to your DNS, you should set this to <code class="literal">60 seconds</code></li></ul></div><p>Then I clicked on the <strong>Launch</strong> button. After a few minutes, I checked the hosted zone in the Route53 control panel and the service had connected automatically and created the following records for stacks and services I already had running.</p><p>The DNS entries are formatted in the following way:</p><div><pre class="programlisting">&lt;service&gt;.&lt;stack&gt;.&lt;environment&gt;.&lt;hosted zone&gt;</pre></div><p>So in my case, I had entries for the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">clusterloadbalancer.default.default.mckendrick.io</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">myclusterapp.default.default.mckendrick.io</code></li></ul></div><p>As <code class="literal">myclusterapp</code> contained three containers, three IP addresses were added to the entry so that round robin DNS would direct traffic to each container:</p><div><img src="img/B05468_07_75.jpg" alt="DNS"/></div><p>Another <a id="id541" class="indexterm"/>good thing about the DNS catalog items is that they are automatically updated, meaning that if we were to move a container to a different host, the DNS for the container would automatically be updated to reflect the new IP address.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec77"/>Docker &amp; Rancher Compose</h2></div></div></div><p>Another <a id="id542" class="indexterm"/>thing that you may have noticed is that when you<a id="id543" class="indexterm"/> go to add a stack, Rancher gives you two boxes where you can enter the content of a Docker and Rancher Compose file.</p><p>So far, we have been creating services manually using the web interface, for each of the stacks we have built up with way you have the option of viewing it as a configuration files.</p><p>In the following screenshot, we are looking at the Docker and Rancher compose files for our Clustered Application stack. To get this view, click on the icon to the left of where it says <strong>Active</strong>:</p><div><img src="img/B05468_07_76.jpg" alt="Docker &amp; Rancher Compose"/></div><p>This feature <a id="id544" class="indexterm"/>allows you to ship your stacks to other Rancher users. The<a id="id545" class="indexterm"/> contents of the preceding files are given in the following so that you can try it on your own Rachner installation.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec29"/>Docker Compose</h3></div></div></div><p>This is a standard version one Docker Compose file, there are Rancher settings passed as labels:</p><div><pre class="programlisting">ClusterLoadBalancer:
  ports:
  - 80:80
  tty: true
  image: rancher/load-balancer-service
  links:
  - MyClusterApp:MyClusterApp
  stdin_open: true
MyClusterApp:
  ports:
  - 60036:80/tcp
  log_driver: ''
  labels:
    io.rancher.scheduler.global: 'true'
    io.rancher.container.pull_image: always
  tty: true
  log_opt: {}
  image: russmckendrick/cluster
  stdin_open: true</pre></div></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec30"/>Rancher Compose</h3></div></div></div><p>The Rancher Compose file wraps the containers defined in the Docker Compose file in Rancher <a id="id546" class="indexterm"/>services, as you can see where we are defining the health checks for both the Load Balancer and Cluster containers:</p><div><pre class="programlisting">ClusterLoadBalancer:
  scale: 1
  load_balancer_config:
    haproxy_config: {}
  health_check:
    port: 42
    interval: 2000
    unhealthy_threshold: 3
    healthy_threshold: 2
    response_timeout: 2000
MyClusterApp:
  health_check:
    port: 80
    interval: 2000
    initializing_timeout: 60000
    unhealthy_threshold: 3
    strategy: recreate
    request_line: GET "/index.html" "HTTP/1.0"
    healthy_threshold: 2
    response_timeout: 2000</pre></div><p>Rancher Compose is also the name of the command-line tool that can locally install to interact with your Rancher installation. As the command line duplicates the functionality, we have already covered, I won't be going into any detail about it here; however, if you would like<a id="id547" class="indexterm"/> give it a go, complete details about it can be found in the official Rancher documentation at <a class="ulink" href="http://docs.rancher.com/rancher/rancher-compose/">http://docs.rancher.com/rancher/rancher-compose/</a>.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec78"/>Back to where we started</h2></div></div></div><p>The last task <a id="id548" class="indexterm"/>we are going to do using Rancher is to launch a Kubernetes cluster in DigitalOcean. As mentioned at the start of the chapter, Rancher not only manages its own Cattle clusters, but also Kubernetes and Swarm ones.</p><p>To create a <a id="id549" class="indexterm"/>Kubernetes cluster, click on the drop-down menu where it says <strong>Environment</strong>, underneath your avatar and click on <strong>Add Environment</strong>:</p><div><img src="img/B05468_07_77.jpg" alt="Back to where we started"/></div><p>On the page, you will be asked which container-orchestration tool would you like to use for the environment, what it should be called, and finally who should be able to access it.</p><p>Select Kubernetes, fill in the remaining information, and click on the <strong>Create</strong> button. Once you have your second environment, you will be able to check between them on the <strong>Environment</strong> drop-down menu.</p><p>Similar to when we first launched Rancher, we will need to add some hosts that will make up our Kubernetes cluster. To do this, click on <strong>Add Host</strong> and then enter the details as done earlier, apart from this, time call them Kubernetes rather than Rancher.</p><p>You will then be taken to a screen that looks like the following screenshot:</p><div><img src="img/B05468_07_78.jpg" alt="Back to where we started"/></div><p>It will take <a id="id550" class="indexterm"/>about 10 minutes to complete the installation. Once it has completed, you will be taken to a familiar-looking Rancher screen; however, you will now have <strong>Services</strong>, <strong>RCS</strong>, <strong>Pods</strong>, and <strong>kubectl</strong> listed in the secondary menu.</p><p>Clicking on <strong>kubectl</strong> will take you to a page that allows you to run kubectl commands in your browser and also you will get an option to download a kubectl config file so that you can interact with Kubernetes from your local machine as well:</p><div><img src="img/B05468_07_79.jpg" alt="Back to where we started"/></div><p>Another <a id="id551" class="indexterm"/>thing you will notice is that a different catalog has been loaded, this is because Docker and Rancher Compose files won't work with Kubernetes:</p><div><img src="img/B05468_07_80.jpg" alt="Back to where we started"/></div><p>Feel free<a id="id552" class="indexterm"/> to launch services like we did in the first part of this chapter or use the catalog items to create a service.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec79"/>Removing the hosts</h2></div></div></div><p>At this<a id="id553" class="indexterm"/> point, you will have around seven instances launched in DigitalOcean. As we are coming to the end of this chapter, you should terminate all these machines so that you do not get charged for resources you are not using.</p><p>I would recommend doing this using the DigitalOcean control panel rather than through Rancher, that way you can be 100% sure that the Droplets have been successfully powered down and removed, meaning that you do not get billed for them.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec80"/>Summing up Rancher</h2></div></div></div><p>As you have <a id="id554" class="indexterm"/>seen, Rancher is not only an incredibly powerful piece of open source software, it is also extremely user-friendly and well-polished.</p><p>We have only touched on some of the features of Rancher here, for example, you can split your hosts between providers to create your own regions, there is a full API that allows you to interact with Rancher from your own applications and also there is a full command-line interface.</p><p>For a 1.0 release, it is incredibly feature-rich and stable. I don't think I saw it having any problems during <a id="id555" class="indexterm"/>my time using it.</p><p>If you want a tool that allows you launch your own clusters and then give end users, such as developers, access to an intuitive interface, then Rancher is going to be a match made in heaven.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Summary</h1></div></div></div><p>The three tools that we have looked are not the only schedulers available, there are also tools such as the following to name a few:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Nomad</strong>: <a class="ulink" href="https://www.nomadproject.io/">https://www.nomadproject.io/</a></li><li class="listitem" style="list-style-type: disc"><strong>Fleet</strong>: <a class="ulink" href="https://coreos.com/using-coreos/clustering/">https://coreos.com/using-coreos/clustering/</a></li><li class="listitem" style="list-style-type: disc"><strong>Marathon</strong>: <a class="ulink" href="https://mesosphere.github.io/marathon/">https://mesosphere.github.io/marathon/</a></li></ul></div><p>All these schedulers have their own requirements, complexities, and use cases.</p><p>If you had asked me a year ago which of the three schedulers that we have looked in this chapter would I recommend, I would have said Amazons EC2 Container Service. Kubernetes would have been second and I probably wouldn't have mentioned Rancher.</p><p>In the past 12 months, Kubernetes has vastly reduced its complexity when it comes to installing the service has removed its biggest barrier to people adopting it, and as we have demonstrated, Rancher reduces this complexity even further.</p><p>Unfortunately, this has left EC2 Container Service feeling like it is a lot more complex to both configure and operate when compared to the other tools, especially as both Kubernetes and Rancher support launching hosts in Amazon Web Services and can take advantage of the myriad of supporting services offer by Amazon's public cloud.</p><p>In our next and final chapter, we are going to be reviewing all the tools that we have looked at throughout the previous chapters, we will come up with some use cases as well, and talk about the security considerations that we will need to take when using them.</p></div></body></html>