<html><head></head><body>
		<div><h1 id="_idParaDest-78"><em class="italic"><a id="_idTextAnchor080"/>Chapter 5</em>: Alternatives for Deploying and Running Containers in Production</h1>
			<p>As container technology and cloud computing mature, the number of ways in which you can deploy your Docker containers has exploded. Some of the options are as simple as running Docker on a single host, and others feature advanced features such as autoscaling, multi-cloud support, and more. You could even run your Docker containers on-premises on bare-metal servers or adopt a hybrid cloud solution.</p>
			<p>After reading this chapter, you will understand that the many choices available offer different trade-offs. You will learn how to build the smallest viable production environment. You will be able to choose between different cloud providers and their managed container runtimes, as well as articulate the benefits of running Docker either on-premises or in a hybrid cloud. Most importantly, you will be able to make an informed decision about choosing a production path for deploying Docker containers given competing objectives.</p>
			<p>Understanding the spectrum of choices will help guide you toward making better decisions.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Running Docker in production – many paths, choose wisely</li>
				<li>What is the minimum realistic production environment?</li>
				<li>Managed cloud services</li>
				<li>Running your own Kubernetes cluster – from bare-metal servers to OpenStack</li>
				<li>Deciding on the right Docker production setup</li>
			</ul>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor081"/>Technical requirements</h1>
			<p>To complete the exercises in this chapter, you'll need Git and Docker on your local workstation. For Mac and Windows users, please install Docker Desktop (<a href="https://www.docker.com/products/docker-desktop">https://www.docker.com/products/docker-desktop</a>) as this is how most people using Docker use it on their local workstations. You need to learn more about the options before you choose a production deployment tool.</p>
			<p>Depending on what avenues you explore, you may also want to establish accounts with Amazon Web Services, Google Cloud, Microsoft Azure, or Digital Ocean. Most of these services have fairly generous free tiers that may allow you to experiment without spending much money, especially if you only use the services for a short duration. When considering what sort of environment might be suitable for your application, it helps to have multiple options. If you do create resources in the cloud, don't forget to terminate resources that you are done with or are not planning to keep, or you could receive a nasty surprise when you see the bill. Most cloud providers have a billing alert system. Please consider setting up an alarm that will notify you if your spending exceeds your budget.</p>
			<p>If you want to explore hosting a more complex on-premises setup, or use a bare-metal hosting service such as Packet (<a href="https://www.packet.com/">https://www.packet.com/</a>), you may need one or more server computers that meet the specifications for running Docker or OpenStack on bare-metal computer hardware.</p>
			<p>The GitHub repository for this chapter is <a href="https://github.com/Packt-Publishing/Docker-for-Developers">https://github.com/Packt-Publishing/Docker-for-Developers</a> – please see the <code>chapter5</code> folder inside.</p>
			<p>Check out the following video to see the Code in Action:</p>
			<p><a href="https://bit.ly/2DYMria">https://bit.ly/2DYMria</a></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor082"/>Example application – ShipIt Clicker</h1>
			<p>The linked GitHub repository for this chapter has code for a prototype for an online game – called ShipIt Clicker. In <a id="_idIndexMarker229"/>this game, a fedora-clad squirrel urges you to deploy containers to production; the faster you click, the <a id="_idIndexMarker230"/>faster you accumulate <code>docker-compose</code> to run multiple containers. The game features communications between a web browser game client, a Node.js server using Express and a Swagger-driven API, and a Redis NoSQL database used to track scores and other game information.</p>
			<p>You can experiment with ShipIt Clicker to get familiar with more elaborate applications than previous chapters explored. Feel free to adapt and improve both the configuration files and the code in <a id="_idIndexMarker231"/>conjunction with a variety of tools and services in order to learn more about deploying to production. In subsequent chapters, we will learn how to deploy this application to production in several different ways, each offering progressively more capabilities, but different trade-offs in terms of cost, complexity, and availability. Before we do that, let's learn more about these alternatives.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor084"/>Running Docker in production – many paths, choose wisely</h1>
			<p>If you thought running Docker on your local workstation offered many choices, buckle up as the variety available <a id="_idIndexMarker232"/>to developers and system administrators in deploying an application built using Docker in a robust way makes the local development environment look simple by comparison. Some of the largest information technology companies in the world use Docker (or equivalent container technologies) to run at a massive scale, and container orchestration makes that possible. The promise of having a self-healing cluster that can continue to run applications in the face of network partitions and hardware failure has lured many into the Docker arena. Many people see their enthusiasm wane when the complexity of running a fault-tolerant cluster becomes evident.</p>
			<p>However, you don't have to do it all yourself. Multiple cloud providers offer services that make running applications with Docker more manageable. The solution larger organizations are gravitating toward is Kubernetes, a project sponsored by Google as a public and community-supported alternative to proprietary container orchestration tools. Kubernetes takes the lessons that Google learned from building and operating Borg, their internal container orchestration tool, and makes them available to the public.</p>
			<p>Or maybe you just need to run a simple dynamic website on as small a setup as possible – you don't have to learn <a id="_idIndexMarker233"/>cloud orchestration to do that if you have access to an internet-connected server that itself can run Docker.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor085"/>What is the minimum realistic production environment?</h1>
			<p>Docker can run on a wide variety of hardware and software, but the level of support you will receive from either Docker <a id="_idIndexMarker234"/>itself or from a third party, such as an operating system distribution that bundles Docker, may vary significantly. Docker can run on a wide variety of operating systems: Linux, Apple macOS, Microsoft Windows, and even IBM S/390x.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor086"/>Bare minimum – run Docker and Docker Compose on one host</h2>
			<p>Given the wide distribution of Docker on different environments, the minimum production <a id="_idIndexMarker235"/>environment for a Docker-hosted application is a single host, whether it is physical or virtual, running an operating system that supports Docker and Docker Compose. Many popular mainstream operating systems and distributions have some version of Docker built in, including <a id="_idIndexMarker236"/>the current <strong class="bold">Long-Term Support</strong> (<strong class="bold">LTS</strong>) versions of Ubuntu (16.04, 18.04, and 20.04) and CentOS (7 and 8). Other more specialized operating systems, such as CoreOS and Container Linux, focus exclusively on running containers and may be good choices, albeit with a learning curve for people used to more mainstream systems.</p>
			<p>You could even run Docker on Windows or macOS for a production system. You might be more comfortable running Docker on a platform that has support, depending on your risk tolerance and needs. Trade-offs abound!</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor087"/>Docker support</h2>
			<p>The community <a id="_idIndexMarker237"/>edition of Docker receives support from the parent company for a very limited time – the developer-focused <a id="_idIndexMarker238"/>Docker Inc. company (<a href="https://www.docker.com">https://www.docker.com</a>) produces quarterly <a id="_idIndexMarker239"/>releases of the <strong class="bold">Community Edition</strong> (<strong class="bold">CE</strong>) Docker toolchain with a 4-month rolling support window. As of November 13, 2019, the <strong class="bold">Enterprise Edition</strong> (<strong class="bold">EE</strong>) of Docker is a Mirantis <a id="_idIndexMarker240"/>product; see <a href="https://www.mirantis.com/company/press-center/company-news/mirantis-acquires-docker-enterprise/">https://www.mirantis.com/company/press-center/company-news/mirantis-acquires-docker-enterprise/</a> for more details. The EE version of Docker features longer support horizons; support for a variety of Linux, Windows, and macOS operating systems; and an expanded set of supported orchestration systems; see <a href="https://docs.docker.com/ee/">https://docs.docker.com/ee/</a> for <a id="_idIndexMarker241"/>more information on Docker EE. Mirantis announced that it would end support for the  Docker Swarm container orchestrator, a <a id="_idIndexMarker242"/>part of Docker EE, in November 2021, but retracted the retirement announcement in February 2020. See <a href="https://github.com/PacktPublishing/Docker-for-Developers">https://devclass.com/2020/02/25/mirantis-to-keep-docker-swarm-buzzing-around-pledges-new-features/</a> for more details.</p>
			<p>Kubernetes appears to be the winner of the Docker container orchestration wars, given this news, although Mirantis is still supporting Docker Swarm.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor088"/>Problems with single-host deployment</h2>
			<p>Running Docker on a single host has major drawbacks, however. If that host suffers a major hardware or software <a id="_idIndexMarker243"/>failure or has impaired internet connectivity, your application will suffer decreased availability. Computers are fundamentally unreliable and even systems that have enterprise availability features, such as redundant disks, power supplies, and cooling features, can suffer failures due to environmental factors. If you do go down this route, it would be prudent to add some sort of external monitoring and ensure you have a reliable backup and restore routine to mitigate these risks. In order to avoid these risks, we need to consider more sophisticated approaches, such as relying on more container orchestration systems that a third party runs.</p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor089"/>Managed cloud services </h1>
			<p>In order to overcome the limitations of deploying applications on a single host, the easiest option to <a id="_idIndexMarker244"/>choose is to consider running your application using a managed cloud service that provides a container orchestration solution. Some of the most popular solutions include the following:</p>
			<ul>
				<li><strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>)</li>
				<li>Amazon Web Services <strong class="bold">Elastic Beanstalk</strong> (<strong class="bold">EB</strong>)</li>
				<li>Amazon Web Services <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>)</li>
				<li>Amazon Web Services <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>)</li>
				<li>Microsoft <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>)</li>
				<li>DigitalOcean Docker Swarm</li>
			</ul>
			<p>Most of these services support running a set of Docker containers <a id="_idIndexMarker245"/>through Kubernetes (<a href="https://kubernetes.io/">https://kubernetes.io/</a>), a project initiated by Google. For many years, Google has run a container orchestration system <a id="_idIndexMarker246"/>called Borg (<a href="https://ai.google/research/pubs/pub43438">https://ai.google/research/pubs/pub43438</a>), and Google used that as inspiration to create a container orchestration system suitable for external use, which got named Kubernetes.</p>
			<p>Some managed cloud services support Docker Swarm, while others (including AWS Elastic Beanstalk and AWS ECS) have their own custom orchestration systems.</p>
			<p>All of the container orchestration systems allow software developers and system administrators to run a fleet of servers that execute multiple containers simultaneously, with policy-based <a id="_idIndexMarker247"/>mechanisms for distributing multiple container instances among the cluster. The container orchestrators are responsible for starting, monitoring, and moving container workloads from host to host as health checks and scaling constraints dictate. Since Google popularized running these container orchestration systems, many vendors have devised managed service offerings, including Google, Microsoft, Amazon Web Services, Digital Ocean, and others, as we will discuss in the following subsections.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor090"/>Google Kubernetes Engine</h2>
			<p>Google offers a system called <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) (<a href="https://cloud.google.com/kubernetes-engine/">https://cloud.google.com/kubernetes-engine/</a>), which <a id="_idIndexMarker248"/>offers a supported Kubernetes cluster running <a id="_idIndexMarker249"/>within the Google Cloud. If you use this service, you don't have to operate and upgrade the Kubernetes cluster master nodes yourself; you won't see the master nodes in the cloud console at all, as Google operates them directly. Furthermore, Google does not charge customers for running those Kubernetes master nodes. This option is appealing to developers because it has a way to run low-cost Kubernetes clusters. Having the support directly from Google to run Kubernetes workloads gives some customers additional confidence with this system.</p>
			<p>However, Google Cloud is <a id="_idIndexMarker250"/>not the first or even the second biggest cloud provider, and the rest of the services available from Google Cloud are not as varied as the services that Azure, AWS, or other <a id="_idIndexMarker251"/>cloud providers such as AliBaba offer.</p>
			<p>If you are invested in Google Cloud, or you want a low-cost environment to experiment with Kubernetes or take it to production and you are not tied to cloud services from other providers, evaluate GKE for running Docker and Kubernetes loads.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor091"/>AWS Elastic Beanstalk</h2>
			<p>Amazon Web Services offers a way to run Docker applications through its platform-as-a-service <a id="_idIndexMarker252"/>offering, Elastic Beanstalk (<a href="https://aws.amazon.com/elasticbeanstalk/">https://aws.amazon.com/elasticbeanstalk/</a>). You can run either <a id="_idIndexMarker253"/>single Docker containers or a setup that supports multiple Docker containers. Under the covers, Elastic Beanstalk uses ECS if you select multiple containers. With Elastic Beanstalk, developers use a command-line interface tool that simplifies deployment to multiple environments, in conjunction with some concise configuration files that hide some of the complexity of running an autoscaling cluster.</p>
			<p>It is easier to set up Elastic Beanstalk than it is to set up either ECS or EKS, and developers needing an easy on-ramp to get to production with low overhead and minimal setup might consider using Elastic Beanstalk.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor092"/>AWS ECS and Fargate</h2>
			<p>AWS also offers a container orchestration system <a id="_idIndexMarker254"/>called ECS (<a href="https://aws.amazon.com/ecs/">https://aws.amazon.com/ecs/</a>). ECS has <a id="_idIndexMarker255"/>two basic modalities: one where containers run on a fleet of EC2 instances managed directly by the account owner, and one where AWS manages the <a id="_idIndexMarker256"/>nodes that containers <a id="_idIndexMarker257"/>run on, called Fargate (<a href="https://aws.amazon.com/fargate/">https://aws.amazon.com/fargate/</a>).</p>
			<p>Using ECS with either EC2 or Fargate can make sense if you are invested in AWS. While this path allows you to deploy containers without having to deal with Kubernetes or Docker Swarm, however, it is a proprietary system that only AWS supports, so you would have to do extra work to move your systems away from it compared to using Kubernetes or Docker Swarm as an orchestrator. It has its own learning curve and requires that you commit to running your Docker workloads on AWS because these interfaces are AWS-specific.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor093"/>AWS EKS</h2>
			<p>Amazon Web Services (AWS) offers EKS, a managed Kubernetes service that offloads the maintenance and configuration of the Kubernetes master servers to AWS. EKS is the AWS equivalent <a id="_idIndexMarker258"/>of Google's GKE. It offers robust integration with the rest of the AWS services, and even though it is not as economical as the GKE <a id="_idIndexMarker259"/>service with respect to running the Kubernetes masters, the baseline costs are modest compared with the cost of running a busy application. AWS has generally had support available for Kubernetes through EKS since 2018 and has fixed enough of the initial rough spots that surfaced after its launch (such as a lack of support for some common autoscaling strategies) to make EKS a formidable Kubernetes distribution. In December 2019, AWS announced support for running Kubernetes containers managed by EKS through Fargate, melding the support AWS has for EKS with the managed container runtime and elastic and transparent provisioning that AWS provides.</p>
			<p>AWS has the largest and most comprehensive set of services available from a cloud provider as of early 2020. If you have an investment in AWS, and you want a well-trod path that many people have traveled, consider using AWS EKS as your Kubernetes master environment.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor094"/>Microsoft Azure Kubernetes Service</h2>
			<p>Microsoft Azure provides a robust container deployment service in <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>). This option <a id="_idIndexMarker260"/>may be particularly appealing if you or your company have a large investment in Microsoft platform <a id="_idIndexMarker261"/>tooling, including Windows, Visual Studio Code, or Active Directory. Microsoft claims to have robust support for all these concerns. The developer tooling from Microsoft also tends to have a gentler learning curve than the tools from some other organizations. However, if you rely really heavily on elements of the Microsoft stack, it may be more difficult to migrate to other solutions.</p>
			<p>If you are working for a Microsoft shop, or you want an easy on-ramp to Kubernetes that is tightly integrated into Visual Studio Code, consider AKS.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor095"/>Digital Ocean Docker Swarm</h2>
			<p>Digital Ocean provides support for running a fleet of containers using Docker Swarm, a relatively simple <a id="_idIndexMarker262"/>container orchestration system. This technology has a reputation for being easier to deploy than deploying containers <a id="_idIndexMarker263"/>on Kubernetes or even AWS ECS. The Docker tooling has support for deploying to Docker Swarm out of the box.</p>
			<p>However, after the Mirantis acquisition, Docker Swarm's support status was deprecated and then revived after customers demanded continuing support. Given the wavering commitment from the main vendor supporting it, you should carefully consider whether you should field new applications using Docker Swarm.</p>
			<p>Now that we have seen what the alternatives entail for running Docker applications in production, let's examine the set of alternatives for running applications using Docker and Kubernetes.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor096"/>Running your own Kubernetes cluster – from bare metal to OpenStack</h1>
			<p>If you must run your application on-premises, in a data center, or if you have the need to run across multiple cloud <a id="_idIndexMarker264"/>computing providers, you may need to run your own Kubernetes cluster. Once you learn more about the benefits and drawbacks of running Docker and Kubernetes either on-premises or in a hybrid cloud, you should be able to know when it is an appropriate solution. While these scenarios are more complex than using one of the managed services, they can provide different benefits, listed as follows:</p>
			<ul>
				<li>Upgrading cluster software (or not) on your own schedule, with full control of what versions you run today and tomorrow. Cloud vendors may lag in what versions are supported, or deprecate versions in ways that can impose operational risk. </li>
				<li>Using one of the many mature Kubernetes provisioning solutions, such as Kops, that  facilitate setting up k8s clusters on AWS EC2.</li>
				<li>Operating a hybrid cloud solution across a mixture of data center and cloud computing environments. While some cloud provider solutions, such as Google Cloud Anthos or Azure Arc, can support hybrid environments, many do not.</li>
				<li>Running high-performance Kubernetes clusters on bare metal, without the overhead of a hypervisor.</li>
				<li>Running on platforms not supported by major cloud vendors, such as running Docker and Kubernetes on a cluster of Raspberry Pi computers.</li>
				<li>Having complete control <a id="_idIndexMarker265"/>over the supporting infrastructure of your cluster integrating with a platform that uses Kubernetes as a starting point, such as the OpenShift platform.</li>
				<li>Running on a private cloud solution, such as OpenStack or VMware Tanzu (formerly known as VMware Enterprise PKS).</li>
				<li>Running Docker containers as part of a comprehensive computing platform that has other major features and capabilities beyond vanilla Kubernetes, such as Red Hat OpenShift or Rancher.</li>
			</ul>
			<p>In practice, running any of these solutions is more complex than relying on either a single-host deployment of Docker or a vendor-managed software-as-a-service Kubernetes clustering solution.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor097"/>Deciding on the right Docker production setup </h1>
			<p>Because of the <a id="_idIndexMarker266"/>bewildering number of choices, picking the right path to deploy your application in production is daunting. You may need to weigh many factors, including <a id="_idIndexMarker267"/>the following:</p>
			<ul>
				<li><strong class="bold">Setup</strong>: How hard is it to go from local development to production?</li>
				<li><strong class="bold">Features</strong>: Deployment, testing, monitoring, alerting, and cost reporting.</li>
				<li><strong class="bold">Cost</strong>: Initial and ongoing monthly charges.</li>
				<li><strong class="bold">Support</strong>: Is support easily available either from vendors or from the community?</li>
				<li><strong class="bold">Elasticity</strong>: Can it scale out as the load increases, with automatic or manual controls?</li>
				<li><strong class="bold">Availability</strong>: Can the setup survive <a id="_idIndexMarker268"/>the loss of services, hosts, and networks?</li>
				<li><strong class="bold">Stickiness</strong>: How hard will it be to change the deployment strategy?</li>
			</ul>
			<p>Running Docker on a single host is inexpensive and easy to set up but has poor scaling and availability characteristics. All the major cloud orchestration services that support Kubernetes are well-balanced in terms of features and scaling and availability characteristics, but they are <a id="_idIndexMarker269"/>more complex to set up and operate. The non-Kubernetes options are stickier than the Kubernetes options. Running your own clusters either in the cloud, on bare-metal servers, or in a hybrid cloud gives you enormous flexibility at the cost of increased complexity and support burden.</p>
			<p>Learning the relative strengths and weaknesses of these systems will help you judge the right set of technologies to use to deploy your applications. The following matrix shows my snap judgements on a scale of 1 to 5, where 5 is the best, of how well the different technology options compare.</p>
			<div><div><img src="img/B11641_Table_5.1.jpg" alt=""/>
				</div>
			</div>
			<div><div><img src="img/B11641_Table_5.1a.jpg" alt=""/>
				</div>
			</div>
			<p>You can use this matrix to help rank alternative solutions. By comparing two or more of the choices, you can get a better idea of what sort of solution would be appropriate. In order to evaluate this matrix, you could build an evaluation table where you compare alternatives. If you rank the <a id="_idIndexMarker270"/>priorities with a number, where 5 is the highest priority and 1 is the lowest priority, you can multiply the priority by the scores in Table 1 in order to get a scaled score.</p>
			<p>The following example matrix has priorities that emphasize ease of setup, minimization of cost, and minimization of stickiness, while disregarding robustness in the form of high availability or elasticity under load. That set of priorities matches up with the priorities many real-world applications have when they first launch – the struggle developers face is often to get things up and running quickly, and it is OK to compromise on the other factors. The scaled scores in the Alternative columns represent the result of multiplying the priority versus the Production Alternatives Rank table for each alternative.</p>
			<div><div><img src="img/B11641_Table_5.2.jpg" alt=""/>
				</div>
			</div>
			<p>In this case, alternative 1, Docker on a single host, has the highest-ranked scaled score, 78 versus 74. The factors that are important, setup, cost, and stickiness, combine with the weights to push it above the other alternative. Given this score, you should consider using that deployment <a id="_idIndexMarker271"/>alternative. Consider though that if the availability or elasticity priority was even one notch higher, the other alternative, Google Cloud GKE, would have been the higher-ranking service.</p>
			<p>You may find that your needs are served by a hybrid solution also, where more than one of the solutions is appropriate and necessary to solve your problems. For example, you might find that your everyday demands tilt toward an on-premises cluster, but peak demand might require scaling out into the cloud.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor098"/>Exercise – join the ShipIt Clicker team</h2>
			<p>Let's pretend that you have <a id="_idIndexMarker272"/>just joined the ShipIt Clicker development team. Other people on the team have created the basic design for the game (see the game design document in <a href="https://github.com/PacktPublishing/Docker-for-Developers/blob/master/chapter5/ShipIt_Clicker-spec.md">https://github.com/PacktPublishing/Docker-for-Developers/blob/master/chapter5/ShipIt_Clicker-spec.md</a>) and written a prototype that has only the bare minimum required functionality to build, test, and package the application with Docker.</p>
			<p>The rest of the team might be experts in design, or frontend or backend development, but they are not sure how they should proceed regarding deploying to production. At this point, you have more experience using Docker than any of the other developers on the team. The Dockerfile and <code>docker-compose.yml</code> files they have produced are functioning.</p>
			<p>Get the ShipIt Clicker—the version made for this chapter—running on your local workstation to better understand how it is put together.</p>
			<p>Run <code>docker-compose up</code> in order to start the containers on your local machine. This will allow you to evaluate the <a id="_idIndexMarker273"/>deployment alternatives and experiment with changes that will prepare the application for production use. You will see output similar to the following; we will explain in detail what each group of lines in the output means:</p>
			<pre>$ docker-compose up
Building shipit-clicker-web
Step 1/11 : FROM ubuntu:bionic
---&gt; 775349758637
Step 2/11 : RUN apt-get -qq update &amp;&amp;     apt-get -qq install -y nodejs npm &gt; /dev/null
---&gt; Using cache
---&gt; f8a9a6eddb8e</pre>
			<p>The preceding output shows Docker using the <code>ubuntu:bionic</code> image, and then the installation of the operating system packages. </p>
			<p>Steps 3-5 of the Dockerfile prepare the container image for the application installation by creating essential directories and copying the package configuration file for node modules into place:</p>
			<pre>Step 3/11 : RUN mkdir -p /app/public /app/server
---&gt; Using cache
---&gt; f7e56a628e8b
Step 4/11 : COPY src/package.json* /app
---&gt; eede94466dc7
Step 5/11 : WORKDIR /app
---&gt; Running in adcadb6616c2
Removing intermediate container adcadb6616c2
---&gt; 6256f613803e</pre>
			<p>Next, the Dockerfile installs the node modules:</p>
			<pre>Step 6/11 : RUN npm install &gt; /dev/null
---&gt; Running in 02ae124cf711
npm WARN deprecated superagent@3.8.3: Please note that v5.0.1+ of superagent removes User-Agent header by default, therefore you may need to add it yourself (e.g. GitHub blocks requests without a User-Agent header).  This notice will go away with v5.0.2+ once it is released.
npm WARN optional Skipping failed optional dependency /chokidar/fsevents:
npm WARN notsup Not compatible with your operating system or architecture: fsevents@1.2.11
npm WARN shipit-clicker@1.0.5 No repository field.
npm WARN shipit-clicker@1.0.5 No license field.
Removing intermediate container 02ae124cf711
---&gt; 64ea4b348ed1</pre>
			<p>After this, the Dockerfile copies <a id="_idIndexMarker274"/>more configuration files into the container image, as well as copying the sources for the application itself into place within the container under <code>/app</code>:</p>
			<pre>Step 7/11 : COPY src/.babelrc      src/.env      src/.nodemonrc.json      /app/
---&gt; 88e88c1bc35d
Step 8/11 : COPY src/public/ /app/public/
---&gt; c9872fccc1c9
Step 9/11 : COPY src/server/ /app/server/
---&gt; f6e76811659a</pre>
			<p>Finally, the Dockerfile tells Docker what port to expose and how to run the application:</p>
			<pre>Step 10/11 : EXPOSE 3000
---&gt; Running in 75fbd217ef27
Removing intermediate container 75fbd217ef27
---&gt; 03faaa0e8030
Step 11/11 : ENTRYPOINT DEBUG='shipit-clicker:*' npm run dev
---&gt; Running in 0a44ab13b0d3
Removing intermediate container 0a44ab13b0d3
---&gt; ab6e4da773e7
Successfully built ab6e4da773e7</pre>
			<p>At this point, the Docker <a id="_idIndexMarker275"/>container is built, and Docker applies the <code>latest</code> tag:</p>
			<pre>Successfully tagged chapter5_shipit-clicker-web:latest
WARNING: Image for service shipit-clicker-web was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.</pre>
			<p>The power of using <code>docker-compose up</code> is on display next, as the one command we ran at the beginning not only builds the Docker container for our application, but it also starts all the containers together. When it starts the containers, it starts both the application container, and the Redis container. The Redis container emits some detailed output as part of its startup. The output of our <code>docker-compose up</code> command continues with container startup messages:</p>
			<pre>Starting chapter5_redis_1 ... done
Creating chapter5_shipit-clicker-web_1 ... done
Attaching to chapter5_redis_1, chapter5_shipit-clicker-web_1
redis_1               | 1:C 04 Feb 2020 06:15:08.774 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis_1               | 1:C 04 Feb 2020 06:15:08.774 # Redis version=5.0.7, bits=64, commit=00000000, modified=0, pid=1, just started
redis_1               | 1:C 04 Feb 2020 06:15:08.774 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis_1               | 1:M 04 Feb 2020 06:15:08.776 * Running mode=standalone, port=6379.
redis_1               | 1:M 04 Feb 2020 06:15:08.776 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
redis_1               | 1:M 04 Feb 2020 06:15:08.776 # Server initialized</pre>
			<p>Note that Redis is not entirely happy being run as part of a Docker container that uses a Linux kernel that is not tuned <a id="_idIndexMarker276"/>explicitly for it. This is an example where using Docker might not yield optimal results, but results that are good enough anyway:</p>
			<pre>redis_1               | 1:M 04 Feb 2020 06:15:08.776 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
redis_1               | 1:M 04 Feb 2020 06:15:08.776 * DB loaded from disk: 0.000 seconds
redis_1               | 1:M 04 Feb 2020 06:15:08.776 * Ready to accept connections</pre>
			<p>You can see that Redis is <a id="_idIndexMarker277"/>now ready to go. Next, <code>docker-compose</code> starts up the ShipIt Clicker container, using the command given in the preceding <code>ENTRYPOINT DEBUG</code> output (<code>'shipit-clicker:*' npm run dev</code>):</p>
			<pre>shipit-clicker-web_1  |
shipit-clicker-web_1  | &gt; shipit-clicker@1.0.5 dev /app
shipit-clicker-web_1  | &gt; nodemon server --exec babel-node --config .nodemonrc.json | pino-pretty
shipit-clicker-web_1  |
shipit-clicker-web_1  | [nodemon] 1.19.4
shipit-clicker-web_1  | [nodemon] to restart at any time, enter `rs`
shipit-clicker-web_1  | [nodemon] watching dir(s): *.*
shipit-clicker-web_1  | [nodemon] watching extensions: js,json,mjs,yaml,yml
shipit-clicker-web_1  | [nodemon] starting `babel-node server`
shipit-clicker-web_1  | [1580796912837] INFO  (shipit-clicker/47 on 52e6d59c6121): Redis connection established
shipit-clicker-web_1  |     redis_url: "redis://redis:6379"
shipit-clicker-web_1  | [1580796913083] INFO  (shipit-clicker/47 on 52e6d59c6121): up and running in development @: 52e6d59c6121 on port: 3000}</pre>
			<p>Once you have done this, you can play the game by going to <code>http://localhost:3005/</code> in a web browser. In the following figure, we see the output of the main menu of the game, with a link to the API documentation at <code>http://localhost:3005/api-explorer/</code>:</p>
			<div><div><img src="img/B11641_05_001.jpg" alt="Figure 5.1 – ShipIt Clicker game main menu"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – ShipIt Clicker game main menu</p>
			<p>Once you have the <a id="_idIndexMarker278"/>application running and have explored it, you can learn how to deploy it in different ways.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor099"/>Exercise – choosing from reasonable deployment alternatives</h2>
			<p>The setup in this chapter <a id="_idIndexMarker279"/>works to get the game running on a local development environment. However, the setup has some issues that might cause problems for a production deployment.</p>
			<p>The initial audiences for the game in this prototype stage are as follows:</p>
			<ul>
				<li>Your fellow game developers and the management team of the company</li>
				<li>A globally distributed team of enthusiasts who signed up for an Alpha program </li>
				<li>A professional cadre of testers twelve time zones away from where you live </li>
			</ul>
			<p>Management wants to get the prototype available for the alpha tester volunteers and the professional testers as soon as possible, but wants to know what the options and costs will be to support a <a id="_idIndexMarker280"/>more robust deployment environment that can scale if the game goes viral or the investors approve an ad campaign to boost subscribers.</p>
			<p>Your tasks, given what you know about Docker and the alternatives for deploying to production, are as follows:</p>
			<ul>
				<li>Advise management on what the first production deployment should be, after constructing a <em class="italic">Production Decision Alternatives</em> table.</li>
				<li>Advise management on what one or more reasonable alternatives to the first deployment would be, which would increase elasticity and availability.</li>
				<li>Build a spreadsheet model of the one-time and recurring costs incurred over the first year for each option, after consulting current price lists from vendors.</li>
			</ul>
			<h3>Solution</h3>
			<p>Compare your decision matrix to the preceding example in the <em class="italic">Deciding on the right Docker production setup</em> section <a id="_idIndexMarker281"/>and see whether your result differs. Show the spreadsheet model of costs and your decision matrix to a colleague and ask them what they might choose and whether they agree with your decision.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor100"/>Exercise – Dockerfile and docker-compose.yml evaluation </h2>
			<p>Management <a id="_idIndexMarker282"/>wants you to stretch a little and help smooth the way for a production deployment. They want you to identify areas for improvement:</p>
			<ul>
				<li>Are the choices made in the Dockerfile and <code>docker-compose.yml</code> files reasonable for this application?</li>
				<li>What choices could be made to better prepare the application for a production deployment?</li>
				<li>What effect does the <a id="_idIndexMarker283"/>choice of a commodity operating system distribution have when choosing a container base to use in <code>FROM</code>?</li>
			</ul>
			<h3>Solution</h3>
			<p>Look at the versions of the Dockerfile and <code>docker-compose.yml</code> files in <a href="https://github.com/PacktPublishing/Docker-for-Developers/tree/master/chapter6">https://github.com/PacktPublishing/Docker-for-Developers/tree/master/chapter6</a> and see how your recommendations line up. We will explore this in more detail in <a href="B11641_06_Final_NM_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 6</em></a>, <em class="italic">Deploying Applications with Docker Compose</em>.</p>
			<p>Now that we have learned <a id="_idIndexMarker284"/>more about the alternatives for deploying Docker containers into production, and done some practical exercises, let's review what we have learned.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor101"/>Summary</h1>
			<p>In this chapter, we learned about the alternatives for deploying your Docker-based application to production. We learned that the many choices involve trade-offs, and how to build the smallest viable production environment. We learned how to choose between different cloud providers and their managed container runtimes, and how to articulate the benefits of running Docker either on-premises or in a hybrid cloud. We also learned how to decide on a production path for deploying Docker containers given competing objectives.</p>
			<p>Given these lessons, you can apply what you have learned to create a real production deployment. Having enough context about the technology alternatives is very important – because different strategies offer different advantages and disadvantages. Your company might need a super-robust autoscaling deployment in the future but might only need something that works today.</p>
			<p>In the next chapter, we will show how you can create a robust single-host Docker production deployment while maintaining the ability to develop locally.</p>
		</div>
	</body></html>