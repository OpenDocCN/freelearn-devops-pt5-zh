<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Putting It All Together</h1></div></div></div><p>In this, our final chapter, we will look at how we put everything we have learned in the previous chapters together along with how it could fit with your development and deployment workflows.</p><p>Also, we will talk about how to best describe Docker to others, typically you will find that people will assume that Docker containers are just like virtual machines. We will also look at what the benefits are along with some use cases.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Workflows</h1></div></div></div><p>The first five chapters of this book work through a typical workflow<a id="id293" class="indexterm"/> for working with Docker containers through development all the way through to production:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Local development &amp; packaging (<a class="link" href="ch01.html" title="Chapter 1. Installing Docker Locally">Chapter 1</a>, <em>Installing Docker Locally</em> and <a class="link" href="ch02.html" title="Chapter 2. Launching Applications Using Docker">Chapter 2</a>, <em>Launching Applications Using Docker</em>)</li><li class="listitem" style="list-style-type: disc">Staging and remote testing (<a class="link" href="ch03.html" title="Chapter 3. Docker in the Cloud">Chapter 3</a>, <em>Docker in the Cloud</em>)</li><li class="listitem" style="list-style-type: disc">Production (<a class="link" href="ch04.html" title="Chapter 4. Docker Swarm">Chapter 4</a>, <em>Docker Swarm</em> and <a class="link" href="ch05.html" title="Chapter 5. Docker Plugins">Chapter 5</a>, <em>Docker Plugins</em>)</li><li class="listitem" style="list-style-type: disc">On-going support (<a class="link" href="ch06.html" title="Chapter 6. Troubleshooting and Monitoring">Chapter 6</a>, <em>Troubleshooting and Monitoring</em>)</li></ul></div><p>In our first few chapters we learned how to install and interact with Docker locally, typically when developing an application or software stack a developer or system administrator will test locally first.</p><p>Once the application / stack has been fully developed you can share it using the Docker Hub as both a public or private image, or if your image contains things you do now want to distribute via a third party you can host your own Docker Registry.</p><p>Once you have your packaged image, you may need other people to test it. As your image is available in a registry your colleagues or friends can pull your image and run it as you intended locally on their own machine without the worry of having to install and configure either your application or software stack.</p><p>If you need people to test remotely then you can spin up a Docker host on a public cloud provider and quickly deploy your application or software stack there.</p><p>Once everyone is happy you can deploy your application / software stack a servicein a multi-host cluster running Docker Swarm, this means that your service will be running in both a highly available and easy maintain environment. Deploying as a service will also allow you to easily roll out updates for your application or software stack using Swarms in-built service update features.</p><p>If you need share <a id="id294" class="indexterm"/>or persist storage between your containers or hosts then you can install one of the many volume plugins, likewise if you need to something more advanced than the multi-host networking provided by Swarm, no problem, replace it with a network plugin, remember "batteries included, but replaceable".</p><p>Finally, if you need to debug your images or running container you can use the commands and tools discussed in <a class="link" href="ch06.html" title="Chapter 6. Troubleshooting and Monitoring">Chapter 6</a>, <em>Troubleshooting and Monitoring</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec42"/>Describing containers</h1></div></div></div><p>Compartmentalization<a id="id295" class="indexterm"/> that comprises both virtualization and containerization is the new normal for IT agility. Virtualization has been the enigmatic foundation for the enormous success of cloud computing. Now with the containerization idea becoming ubiquitous and usable, there is a renewed focus on using containers for faster application building, deployment, and delivery. Containers are distinctively fitted with a few game-changing capabilities and hence there is a rush in embracing and evolving the containerization technologies and tools.</p><p>Essentially a container is lightweight, virtualized, portable, and the software-defined environment in which software can run in isolation of other software running on the same physical host. The software that runs inside a container is typically a single-purpose application. Containers bring forth the much-coveted modularity, portability, and simplicity for IT environments. Developers love containers because they speed up the software engineering whereas operation team loves because they can just focus on runtime tasks such as logging, monitoring, lifecycle management and resource utilization rather than deployment and dependency management.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec43"/>Describing Docker</h1></div></div></div><p>Linux containers are hugely complicated <a id="id296" class="indexterm"/>and not user-friendly. Having realized the fact that several complexities are coming in the way of massively producing and fluently using containers, an open-source project got initiated with the goal of deriving a sophisticated and modular platform comprising an enabling engine for simplifying and streamlining various containers' lifecycle phases. That is, the Docker platform is built to automate the crafting, packaging, shipping, deployment and delivery of any software application embedded inside a lightweight, extensible, and self-sufficient container.</p><p>Docker is <a id="id297" class="indexterm"/>being positioned as the most flexible and futuristic containerization technology in realizing highly competent and enterprise-class distributed applications. This is to make deft and decisive impacts as the brewing trend in the IT industry is that instead of large monolithic applications distributed on a single physical or virtual server, companies are building smaller, self-defined and sustainable, easily manageable and discrete ones. In short, services are becoming microservices these days to give the fillip to the containerization movement.</p><p>The Docker platform enables artistically assembling applications from disparate and distributed components and eliminates any kind of deficiencies and deviations that could come when shipping code. Docker through a host of scripts and tools simplifies the isolation of software applications and makes them self-sustainable by running them in transient containers. Docker brings the required separation for each of the applications from one another as well as from the underlying host. We have been hugely accustomed to virtual machines that are formed through an additional layer of indirection in order to bring the necessary isolation.</p><p>This additional layer and overhead consumes a lot of precious resources and hence it is an unwanted cause for the slowdown of the system. On the other hand, Docker containers share all the resources (compute, storage and networking) to the optimal level and hence can run much faster. Docker images, being derived in a standard form, can be widely shared and stocked easily for producing bigger and better application containers. In short, the Docker platform lays a stimulating and scintillating foundation for optimal consumption, management, and maneuverability of various IT infrastructures</p><p>The Docker platform is an open-source containerization solution that smartly and swiftly automates the bundling of any software applications and services into containers and accelerates the deployment of containerized applications in any IT environments (local or remote systems, virtualized or bare metal machines, generalized or embedded devices, etc.). The container lifecycle management tasks are fully taken care of by the Docker platform. The whole process starts with the formation of a standardized and optimized image for the identified software and its dependencies. Now the Docker platform takes the readied image to form the containerized software. There are image repositories made available publicly as well as in private locations. Developers and operations teams can leverage them to speed up software deployment in an automated manner.</p><p>The Docker ecosystem<a id="id298" class="indexterm"/> is rapidly growing with a number of third-party product and tool developers in order to make Docker an enterprise-scale containerization platform. It helps to skip the setup and maintenance of development environments and language-specific tooling. Instead, it focuses on creating and adding new features, fixing issues and shipping software. Build once and run everywhere is the endemic mantra of the Docker-enabled containerization. Concisely speaking, the Docker platform <a id="id299" class="indexterm"/>brings in the following competencies.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Agility</strong>: Developers <a id="id300" class="indexterm"/>have freedom to define environments and the ability to create applications. IT Operation team can deploy applications faster allowing the business to outpace competition.</li><li class="listitem" style="list-style-type: disc"><strong>Controllability</strong>: Developers <a id="id301" class="indexterm"/>own all the code from infrastructure to application.</li><li class="listitem" style="list-style-type: disc"><strong>Manageability</strong>: IT <a id="id302" class="indexterm"/>operation team members have the manageability to standardize, secure, and scale the operating environment while reducing overall costs to the organization.</li></ul></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec31"/>Distinguishing Docker containers</h2></div></div></div><p>Precisely speaking, Docker<a id="id303" class="indexterm"/> containers wrap a piece of software in a complete filesystem that contains everything needed to run: source code, runtime, system tools, and system libraries (anything that can be installed on a server). This guarantees that the software will always run the same, regardless of its operating environment:</p><p>Containers running on a single machine share the same operating system kernel. They start instantly and use less RAM. Container images are constructed from layered filesystems and share common files, making disk usage and image downloads much more efficient.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Docker containers are based on open standards. This standardization enables containers to run on all major Linux distributions and other operating systems such as Windows and macOS.</li></ul></div><p>There are several benefits being associated with <a id="id304" class="indexterm"/>Docker containers as enlisted below.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Efficiency</strong>: Containers <a id="id305" class="indexterm"/>running on a single machine all leverage a common kernel so they are lightweight, start instantly and make more efficient use of RAM.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Resource sharing</strong> among workloads allows greater efficiency compared to the use of dedicated and single-purpose equipment. This sharing enhances the utilization rate of resources</li><li class="listitem" style="list-style-type: disc"><strong>Resource partitioning</strong> ensures that resources are appropriately segmented to meet up the system requirements of each workload. Another objective for this partitioning is to prevent any kind of untoward interactions among workloads.</li><li class="listitem" style="list-style-type: disc"><strong>Resource as a Service</strong> (<strong>RaaS</strong>): Various resources can be individually and collectively <a id="id306" class="indexterm"/>chosen, provisioned and given to applications directly or to users to run applications.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><strong>Native Performance</strong>: Containers <a id="id307" class="indexterm"/>guarantee higher performance <a id="id308" class="indexterm"/>due to its lightweight nature and less wastage</li><li class="listitem" style="list-style-type: disc"><strong>Portability</strong>: Applications, dependencies, and configurations are all bundled together in a<a id="id309" class="indexterm"/> complete filesystem, ensuring applications work seamlessly in any environment (virtual machines, bare metal servers, local or remote, generalized or specialized machines, etc.). The main advantage of this portability is it is possible to change the runtime dependencies (even programming language) between deployments. Couple this with Volume plugins and your containers are truly portable.</li><li class="listitem" style="list-style-type: disc"><strong>Real-time Scalability</strong>: Any number of<a id="id310" class="indexterm"/> fresh containers can be provisioned in a few seconds in order to meet up the user and data loads. On the reverse side, additionally provisioned containers can be knocked down when the demand goes down. This <a id="id311" class="indexterm"/>ensures higher throughput and <a id="id312" class="indexterm"/>capacity on <a id="id313" class="indexterm"/>demand. Tools like:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Docker Swarm</li><li class="listitem" style="list-style-type: disc">Kubernetes (<a class="ulink" href="https://kubernetes.io/">https://kubernetes.io/</a>)</li><li class="listitem" style="list-style-type: disc">Apache Mesos(<a class="ulink" href="http://mesos.apache.org/">http://mesos.apache.org/</a>)</li><li class="listitem" style="list-style-type: disc">DC/OS (<a class="ulink" href="https://dcos.io/">https://dcos.io/</a>)</li></ul></div><p>To name but a few of the clustering solutions which further simplify elastic scaling</p></li><li class="listitem" style="list-style-type: disc"><strong>High Availability</strong>: By running with <a id="id314" class="indexterm"/>multiple containers, redundancy can be built into the application. If one container fails, then the surviving peers – which are providing the same capability – continue to provide service. With orchestration, failed containers can be automatically recreated (rescheduled) either on the same or a different host, restoring full capacity and redundancy.</li><li class="listitem" style="list-style-type: disc"><strong>Maneuverability</strong>: Applications<a id="id315" class="indexterm"/> running in Docker containers can be easily modified, updated or extended without impacting other containers in the host.</li><li class="listitem" style="list-style-type: disc"><strong>Flexibility</strong>: Developers <a id="id316" class="indexterm"/>are free to use whichever programming languages and development tools they prefer.</li><li class="listitem" style="list-style-type: disc"><strong>Clusterability</strong>: Containers <a id="id317" class="indexterm"/>can be clustered for specific purposes on demand and there are integrated management platforms for cluster-enablement and management.</li><li class="listitem" style="list-style-type: disc"><strong>Composability</strong>: Software<a id="id318" class="indexterm"/> services hosted in containers can be discovered, matched for, and linked to form business-critical, process-aware and composite services.</li><li class="listitem" style="list-style-type: disc"><strong>Security</strong>: Containers <a id="id319" class="indexterm"/>isolate applications from one another<a id="id320" class="indexterm"/> and the underlying infrastructure by providing an additional layer of protection for the application</li><li class="listitem" style="list-style-type: disc"><strong>Predictability</strong>: With <a id="id321" class="indexterm"/>immutable images, the image always exhibits the same behavior everywhere because the code is contained in the image. That means a lot in terms of deployment and in the management of the application lifecycle.</li><li class="listitem" style="list-style-type: disc"><strong>Repeatability</strong>: With <a id="id322" class="indexterm"/>Docker, one can build an image, test that image and then use that same image in production. </li><li class="listitem" style="list-style-type: disc"><strong>Replicability</strong>: With <a id="id323" class="indexterm"/>containers, it is easy to instantiate identical copies of full application stack and configuration. These can then be used by new hires, partners, support teams, and others to safely experiment in isolation.</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec44"/>Virtual Machines versus containers</h1></div></div></div><p>Containers quite<a id="id324" class="indexterm"/> drastically vary from the highly visible and viable <strong>virtual machines</strong> (<strong>VMs</strong>). Virtual machines represent hardware virtualization whereas <a id="id325" class="indexterm"/>containers facilitate operating system-level virtualization. Some literature points out that virtual machines are system or OS containers whereas containers typically stand for application containers.</p><p>On the functional side, containers are like VMs, but there are dissimilar in many other ways.  Like virtual machines, containers too share the various system resources such as processing, memory, storage, etc. The key difference is that all containers in a host machine share the same OS kernel of the host operating system.</p><p>Though there is heavy sharing, containers intrinsically maintain a high isolation by keeping applications, runtimes, and other associated services separated from each other using the recently incorporated kernel features such as namespaces and cgroups.</p><p>On the resource provisioning front, application containers can be realized in a few seconds, whereas virtual machines often take a few minutes. Containers also allow direct access to device drivers through the kernel and this makes I/O operations faster.</p><p>Workload migration to<a id="id326" class="indexterm"/> nearby or faraway cloud environments can <a id="id327" class="indexterm"/>be accelerated with the containerization capability. The tools and APIs provided by the Docker container technology are very powerful and more developer-friendly than those available with VMs.These APIs allow the management of containers to be integrated into a variety of automated systems for accelerated software engineering.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec45"/>The Docker use cases</h1></div></div></div><p>Containerization is emerging <a id="id328" class="indexterm"/>as the way forward for the software industry as it brings forth a newer and richer way of building and bundling any kind of software, shipping and running them everywhere. That is the fast-evolving aspect of containerization promises and provides software portability, which has been a constant nuisance for IT developers and administrators for long decades now. The Docker idea is flourishing here because of a number of enabling factors and facets. This section is specially prepared for telling the key use cases of the Docker idea.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec32"/>Integrating containers into workflows</h2></div></div></div><p>Workflows are a <a id="id329" class="indexterm"/>widely accepted and used abstraction for unambiguously representing the right details of any complicated and large-scale business and scientific applications and executing them on distributed compute systems such as clusters, clouds, and grids. However, workflow management systems have been largely evasive on conveying the relevant information of the underlying environment on which the tasks inscribed in the workflow are to run. That is, the workflow tasks can run perfectly on the environment for which they were designed. The real challenge is to run the tasks across multiple IT environments without tweaking and twisting the source codes of the ordained tasks. Increasingly the IT environments are heterogeneous with the leverage of disparate <strong>operating systems</strong> (<strong>OSes</strong>), middleware, programming languages and frameworks, databases, etc. Typically workflow systems focus on data interchange between tasks and environment-specific. The same workflow, which is working fine in one environment, starts to crumble when it is being migrated and deployed on different IT environments. All kinds of known and unknown dependencies and incompatibilities spring up to denigrate the workflows delaying the whole job of IT setup, application installation and configuration, deployment, and delivery. Containers are the best bet for resolving this imbroglio once for all.</p><p>
<em>Chao Zheng and Douglas Thain (Integrating Containers into Workflows: A Case Study Using Makeflow, Work Queue, and Docker)</em>has done a good job of analyzing several methods in order to experimentally prove the unique contributions of containers in empowering workflow / process management systems. They have explored the performance of a large bioinformatics workload on a Docker-enabled cluster and observed the best configuration<a id="id330" class="indexterm"/> to be locally managed containers that are shared between multiple tasks.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec33"/>Docker for High-Performance Computing (HPC) and Technical Computing (TC) applications </h2></div></div></div><p>(Douglas M. Jacobsen and Richard Shane Canon) – Currently containers are being overwhelmingly used for the web, enterprise, mobile and cloud applications. However, there are questions<a id="id331" class="indexterm"/> being asked and doubts being raised on whether containers can be a <a id="id332" class="indexterm"/>viable runtime for hosting technical and scientific computing applications. Especially there are many high-performance computing applications yearning for perfect a deployment and execution environment. The authors of this research paper have realized that Docker containers can be a perfect answer for HPC workloads.  </p><p>In many cases, users desire to have the ability to easily execute their scientific applications and workflows in the same environment used for development or adopted by their community. Some researchers have tried out the cloud option but the challenges there are many. The users need to solve how they handle workload management, file systems, and basic provisioning. Containers promise to offer the flexibility of cloud-type systems coupled with the performance of bare-metal systems. Furthermore, containers have the potential to be more easily integrated into traditional HPC environments which mean that users can obtain the benefits of flexibility without the added burden of managing other layers of the system (i.e. batch systems, file systems, etc.).</p><p>
<em>Minh Thanh Chung</em> and the team have analyzed the performance of virtual machines and containers for high-performance applications and benchmarked the results that clearly show containers are the next-generation runtime for HPC applications. In short, Docker offers many attractive benefits in an HPC environment. To test these, IBM Platform LSF and Docker have been integrated outside the core of Platform LSF and the integration leverages the rich Platform LSF plugin framework.</p><p>We all know that the aspect of compartmentalization is for resource partitioning and provisioning. That is, physical machines are subdivided into multiple logical machines (virtual machines and containers). Now on the reverse side, such kinds of logical systems carved out of multiple physical machines can be linked together to buildavirtual supercomputer to solve certain complicated problems.  <em>Hsi-En Yu</em> and <em>Weicheng Huang</em> have described how they built a virtual HPC cluster in the research paper <em>"Building a Virtual HPC Cluster with Auto Scaling by the Docker".</em> They have integrated the auto-scaling feature of service discovery with the lightweight virtualization paradigm (Docker) and embarked on the realization of a virtual cluster on top of physical cluster hardware. </p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec05"/>Containers for telecom applications</h3></div></div></div><p>
<em>Csaba Rotter and the team</em> has explored and <a id="id333" class="indexterm"/>published a survey article on the title <em>"Using Linux Containers in Telecom Applications"</em>. Telecom applications exhibit strong performance and high availability requirements, therefore running them in containers requires additional investigations. A telecom application is a single or multiple node application responsible for a well-defined task. Telecom applications use standardized interfaces to connect to other network elements and implements standardized functions. On top of the standardized functions, a telecom application can have vendor-specific functionality. There is a set of <strong>quality of service</strong> (<strong>QoS</strong>) <a id="id334" class="indexterm"/>and <strong>quality of experience</strong> (<strong>QoE</strong>)<a id="id335" class="indexterm"/> attributes such as high availability, capacity, performance / throughput, etc. The paper has clearly laid out the reasons for the unique contributions of containers in having next-generation telecom applications.</p><p>
<em>Efficient Prototyping of Fault Tolerant Map-Reduce Applications with Docker-Hadoop (Javier Rey and the team)</em> – Distributed computing<a id="id336" class="indexterm"/> is the way forward for compute and data-intensive workloads. There are two major trends. Data becomes big and there are realizations that big data leads to big insights through the leverage of pioneering algorithms, script and parallel<a id="id337" class="indexterm"/> languages such as Scala, integrated platforms, new-generation databases, and dynamic IT infrastructures. MapReduce is a parallel programming paradigm currently used to perform computations on massive amounts of data. Docker-Hadoop1, a virtualization testbed conceived to allow the rapid deployment of a Hadoop cluster. With Docker-Hadoop, it is possible to control the nodes characteristics and run scalability and performance tests that otherwise would require a large computing environment. Docker-Hadoop facilitates simulation and reproduction of different failure scenarios for the validation of an application. </p><p>
<em>Interactive Social Media Applications - AlinCalinciuc</em> and the team has come out with a research publication titled as <em>OpenStack and Docker: building a high-performance IaaS platform for interactive social media applications</em>. It is a well-known truth that interactive social media applications face the challenge of efficiently provisioning new resources in order to meet the demands of the growing number of application users. The authors have given the necessary description on how Docker can run as a hypervisor, and how the authors could manage to enable for the fast provisioning of computing resources inside of an OpenStack IaaS using the nova-docker plug-in that they had developed.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec46"/>Summary</h1></div></div></div><p>At this point of time, Docker is nothing short of an epidemic and every enterprising business across the globe is literally obsessed with the containerization mania for their extreme automation, transformation, and disruption.</p><p>With the blossoming of hybrid IT, the role of Docker-enabled containerization is steadily growing to smartly empower IT-enabled businesses. In this chapter, we have discussed the prime capabilities and contributions of the Docker paradigm.</p><p>It is not often that you can summarize an entire book with a single meme, but I think that at the very least your journey into the world of containers will resolve this all too common problem:</p><div><img src="img/B06455_07_01.jpg" alt="Summary"/><div><p>Picture taken by Dave Roth</p></div></div><p>The days of developing code on version of a language with a configuration which is only local to a single developer which looks nothing like your production platform should now be over as you can easily develop, package and ship consistent containers which can run anywhere.</p></div></body></html>