<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Onto Production</h1></div></div></div><p class="calibre8">Docker came out of dotCloud's PaaS, where it fulfils the needs of IT to develop and deploy web applications in a fast and scalable manner. This is needed to keep up with the ever-accelerating pace of using the Web. Keeping everything running in our Docker container in production is no simple feat.</p><p class="calibre8">In this chapter, we will wrap up what you learned about optimizing Docker and illustrate how it relates to operating our web applications in production. It consists of the following topics:</p><div><ul class="itemizedlist"><li class="listitem">Performing web operations</li><li class="listitem">Supporting our application with Docker</li><li class="listitem">Deploying applications</li><li class="listitem">Scaling applications</li><li class="listitem">Further reading on web operations in general</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec44" class="calibre1"/>Performing web operations</h1></div></div></div><p class="calibre8">Keeping a web <a id="id302" class="calibre1"/>application running 24/7 on the Internet poses challenges in both software development and systems administration. Docker positions itself as the glue that allows both disciplines to come together by creating Docker images that can be built and deployed in a consistent manner.</p><p class="calibre8">However, Docker <a id="id303" class="calibre1"/>is not a silver bullet to the Web. It is still important to know the fundamental concepts in software development and systems administration as web applications become more complex. The complexity naturally arises because these days, with Internet technologies in particular, the multitude of web applications is becoming more ubiquitous in people's lives.</p><p class="calibre8">Dealing with the complexity of keeping web applications up and running involves mastering the ins and outs of web operations, and like any road to mastery, Theo Schlossnagle boils it down to four basic pursuits: knowledge, tools, experience, and discipline. <em class="calibre9">Knowledge</em> refers to absorbing information about web operations available on the Internet, in conferences, and technology meetings like a sponge. Understanding them and knowing how to filter out the signal from the noise will aid us in designing our application's architecture when they burn in production. With Docker and Linux containers increasing in popularity, it is important to be aware of the different technologies that support it and dive into its basics. In <a class="calibre1" title="Chapter 7. Troubleshooting Containers" href="part0046_split_000.html#1BRPS2-afc4585f6623427885a0b0c8e5b2e22e">Chapter 7</a>, <em class="calibre9">Troubleshooting Containers</em>, we showed that regular Linux debugging tools are still useful in debugging running Docker containers. By knowing how containers interact with our Docker host's operating system, we were able to debug the problems occurring in Docker.</p><p class="calibre8">The second aspect is mastering our <em class="calibre9">tools</em>. This book basically revolved around mastering the use of Docker by <a id="id304" class="calibre1"/>looking at how it works and how to optimize its usage. In <a class="calibre1" title="Chapter 2. Optimizing Docker Images" href="part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e">Chapter 2</a>, <em class="calibre9">Optimizing Docker Images</em>, we learned how to optimize Docker <a id="id305" class="calibre1"/>images based on how Docker builds the images and runs the container using its copy-on-write filesystem underneath. This was guided by our knowledge of web operations on why optimized Docker images are important both from a scalability and deployability standpoint. Knowing how to use Docker effectively does not happen overnight. Its mastery can only be gained by a continuous practice of using Docker in production. Sure, we might be paged at 2 am for our first Docker deployment in production, but as time goes by, the experience we gain from continuous usage will make Docker an extension of our limbs and senses, as Schlossnagle puts it.</p><p class="calibre8">By applying the knowledge and continuously using our tools, we gain <em class="calibre9">experience</em> that we can draw upon in the future. This aids us in making good judgments based on bad decisions that we made in the past. It is the place where we can see the theory of container technology and the practice of running Docker in production collide. Scholassnagle mentioned the challenges of acquiring experience in web operations and how to survive the bad judgments and draw experiences from them. He suggests having limited environments in which a bad decision's impact is minimal. Docker is the best place to draw these types of experiences. Having a standard format of ready-to-deploy Docker images, junior web operations engineers can have their own environments that they can experiment with and learn from their mistakes in. Also, since Docker environments look very similar when they move forward to production, these engineers will already have their experience to draw upon.</p><p class="calibre8">The last part in the pursuit of mastering web operations is <em class="calibre9">discipline</em>. However, as it is a very young discipline, such processes are not well defined. Even with Docker, it took a few years for people to realize the best ways to use container technologies. Before this, the convenience of including the whole kitchen sink in Docker images was very common. However, as we can see in <a class="calibre1" title="Chapter 2. Optimizing Docker Images" href="part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e">Chapter 2</a>, <em class="calibre9">Optimizing Docker Images</em>, reducing the footprint of Docker images helps aid in managing the complexity of the applications that we have to debug. This makes the experience of debugging in <a class="calibre1" title="Chapter 7. Troubleshooting Containers" href="part0046_split_000.html#1BRPS2-afc4585f6623427885a0b0c8e5b2e22e">Chapter 7</a>, <em class="calibre9">Troubleshooting Containers</em>, much simpler because we have fewer components and factors to think about. These disciplines of using Docker do not come overnight just by reading Docker blogs (well, some do). It involves continuous exposure to the knowledge of the Docker community and the practice of using Docker in various settings for production use.</p><p class="calibre8">In the remaining <a id="id306" class="calibre1"/>sections, we will show how the theory and <a id="id307" class="calibre1"/>practice of using Docker's container technology can aid in the operation of our web applications.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec45" class="calibre1"/>Supporting web applications with Docker</h1></div></div></div><p class="calibre8">The following <a id="id308" class="calibre1"/>diagram shows the typical architecture of a web application. We have the load balancer tier that receives traffic from the Internet and then the traffic, which is typically composed of user requests, is relayed to a farm of web application servers in a load-balanced fashion. Depending on the nature of the request, some states will be grabbed by the web application from the persistent storage tier, similar to database servers:</p><div><img src="img/00035.jpeg" alt="Supporting web applications with Docker" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As we can see in the <a id="id309" class="calibre1"/>preceding diagram, each tier is run inside a Docker container on top of a Docker host. With this layout for each component, we can take advantage of Docker's uniform way of deploying load balancers, applications, and databases, as we did in <a class="calibre1" title="Chapter 2. Optimizing Docker Images" href="part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e">Chapter 2</a>, <em class="calibre9">Optimizing Docker Images</em>, and <a class="calibre1" title="Chapter 6. Load Balancing" href="part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e">Chapter 6</a>, <em class="calibre9">Load Balancing</em>. However, in addition to the Docker daemons in each Docker host, we need supporting infrastructure to manage and observe the whole stack of our web architecture in a scalable fashion. On the right-hand side, we can see that each of our Docker hosts sends diagnostic information—for example, application and system events such as log messages and metrics—to our centralized logging and monitoring system. We deployed such a system in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>, where we rolled out Graphite and an ELK stack. In addition, there might be another system that listens for specific signals in the logs and metrics and sends alerts to the engineers responsible for the operation of our Docker-based web application stack. These events can relate to critical events, such as the availability and performance of our application, that we need to take action on to ensure that our application is fulfilling the needs of our business as expected. An internally managed system, such as Nagios, or a third-party one, such as PagerDuty, is used for our Docker deployments to call and wake us up at 2 am for deeper monitoring and troubleshooting sessions as in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>, and <a class="calibre1" title="Chapter 7. Troubleshooting Containers" href="part0046_split_000.html#1BRPS2-afc4585f6623427885a0b0c8e5b2e22e">Chapter 7</a>, <em class="calibre9">Troubleshooting Containers</em>.</p><p class="calibre8">The left-hand <a id="id310" class="calibre1"/>side of the diagram contains the configuration management system. This is the place where each of the Docker hosts downloads all the settings it needs to function properly. In <a class="calibre1" title="Chapter 3. Automating Docker Deployments with Chef" href="part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 3</a>, <em class="calibre9">Automating Docker Deployments with Chef</em>, we used a Chef server to store the configuration of our Docker host. It contained information such as a Docker host's role in our architecture's stack. The Chef server stores information on which Docker containers to run in each tier and how to run them using the Chef recipes we wrote. Finally, the configuration management system also tells our Docker hosts where the Graphite and Logstash monitoring and logging endpoints are.</p><p class="calibre8">All in all, it takes various components to support our web application in production aside from Docker. Docker allows us to easily set up this infrastructure because of the speed and flexibility of deploying containers. Nonetheless, we shouldn't skip doing our homework about having these supporting infrastructures in place. In the next section, we will see the supporting infrastructure of deploying web applications in Docker using the skills you learned in the previous chapters.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec46" class="calibre1"/>Deploying applications</h1></div></div></div><p class="calibre8">An important <a id="id311" class="calibre1"/>component when tuning the performance of Docker <a id="id312" class="calibre1"/>containers is the feedback telling us that we were able to improve our web application correctly. The deployment of Graphite and the ELK stack in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>, gave us visibility on the effects of what we changed in our Docker-based web application. As much as it is important to gather feedback, it is more important to gather feedback in a timely manner. Therefore, the deployment of our Docker containers needs to be in a fast and scalable manner. Being able to configure a Docker host automatically, as we did in <a class="calibre1" title="Chapter 3. Automating Docker Deployments with Chef" href="part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 3</a>, <em class="calibre9">Automating Docker Deployments with Chef</em>, is an important component for a fast and automated deployment system. The rest of the components are described in the following diagram:</p><div><img src="img/00036.jpeg" alt="Deploying applications" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Whenever we submit changes to our application's code or the <code class="literal">Dockerfile</code> describing how it is run and built, we need supporting infrastructure to propagate this change all the way to our Docker <a id="id313" class="calibre1"/>hosts. In the preceding diagram, we can see that the <a id="id314" class="calibre1"/>changes we submit to our version control system, such as Git, generate a trigger to build the new version of our code. This is usually done through Git's postreceive hooks in the form of shell scripts. The triggers will be received by a build server, such as Jenkins. The steps to propagate the change will be similar to the blue-green deployment process we made in <a class="calibre1" title="Chapter 6. Load Balancing" href="part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e">Chapter 6</a>, <em class="calibre9">Load Balancing</em>. After receiving the trigger to build the new changes we submitted, Jenkins will take a look at the new version of our code and run <code class="literal">docker build</code> to create the Docker image. After the build, Jenkins will push the new Docker image to a Docker registry, such as Docker Hub, as we set up in <a class="calibre1" title="Chapter 2. Optimizing Docker Images" href="part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e">Chapter 2</a>, <em class="calibre9">Optimizing Docker Images</em>. In addition, it will update the target Docker hosts indirectly by updating the entry in the Chef server configuration management system we laid out in <a class="calibre1" title="Chapter 3. Automating Docker Deployments with Chef" href="part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 3</a>, <em class="calibre9">Automating Docker Deployments with Chef</em>. With the artifacts of changes available in the Chef server and Docker registry, our Docker host will now notice the new configuration and download, deploy, and run the new version of our web application inside a Docker container.</p><p class="calibre8">In the next section, we will discuss how a similar process is used to scale out our Docker application.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec47" class="calibre1"/>Scaling applications</h1></div></div></div><p class="calibre8">When we receive <a id="id315" class="calibre1"/>alerts from our monitoring system, as in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>, that the pool of Docker containers running our <a id="id316" class="calibre1"/>web application is not loaded, it is time to scale out. We accomplished this using load balancers in <a class="calibre1" title="Chapter 6. Load Balancing" href="part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e">Chapter 6</a>, <em class="calibre9">Load Balancing</em>. The following diagram shows the high-level architecture of the commands we ran in <a class="calibre1" title="Chapter 6. Load Balancing" href="part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e">Chapter 6</a>, <em class="calibre9">Load Balancing</em>:</p><div><img src="img/00037.jpeg" alt="Scaling applications" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Once we decide to scale out and add an additional Docker host, we can automate the process with a scale-out orchestrator component. This can be a series of simple shell scripts that we will <a id="id317" class="calibre1"/>install inside a build server, such as Jenkins. The <a id="id318" class="calibre1"/>orchestrator will basically ask the cloud provider API to create a new Docker host. This request will then provision the Docker host and run the initial bootstrap script to download the configuration from our configuration management system in <a class="calibre1" title="Chapter 3. Automating Docker Deployments with Chef" href="part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 3, </a>
<em class="calibre9">Automating Docker Deployments with Chef</em>. This will automatically set up the Docker host to download our application's Docker image from the Docker registry. After this whole provisioning process is finished, our scale-out orchestrator will then update the load balancer in our Chef server with the new list of application servers to forward traffic to. So, the next time the <code class="literal">chef-client</code> inside our load balancer Docker host polls the Chef Server, it will add the new Docker host and start forwarding traffic to it.</p><p class="calibre8">As we can note, learning <a id="id319" class="calibre1"/>the way to automate setting up our Docker host in <a class="calibre1" title="Chapter 3. Automating Docker Deployments with Chef" href="part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 3</a>, <em class="calibre9">Automating Docker Deployments with Chef</em>, is crucial to realizing the scalable load <a id="id320" class="calibre1"/>balancing architecture setup we did in <a class="calibre1" title="Chapter 6. Load Balancing" href="part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e">Chapter 6</a>, <em class="calibre9">Load Balancing</em>.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec48" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">The supporting architecture to help our web applications use Docker is nothing but a scratch on the surface. The fundamental concepts in this chapter are described in greater detail in the following books:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre9">Web Operations: Keeping the Data On Time</em>, which is edited by J. Allspaw and J. Robbins. 2010 O'Reilly Media.</li><li class="listitem"><em class="calibre9">Continuous Delivery</em>, by J. Humble and D. Farley. 2010 Addison-Wesley.</li><li class="listitem"><em class="calibre9">Jenkins: The Definitive Guide</em>, J. F. Smart. 2011 O'Reilly Media.</li><li class="listitem"><em class="calibre9">The Art of Capacity Planning: Scaling Web Resources</em>, J. Allspaw. 2008 O'Reilly Media.</li><li class="listitem"><em class="calibre9">Pro Git</em>, S. Chacon and B. Straub. 2014 Apress.</li></ul></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec49" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">You learned a lot about how Docker works throughout this book. In addition to the basics of Docker, we looked back at some fundamental concepts of web operations and how it helps us realize the full potential of Docker. You gained knowledge of key Docker and operating systems concepts to get a deeper understanding of what is happening behind the scenes. You now have an idea of how our application goes from our code down to the actual call in the operating system of our Docker host. You learned a lot about the tools to deploy and troubleshoot our Docker containers in production in a scalable and manageable fashion.</p><p class="calibre8">However, this should not stop you from continuing to develop and practice using Docker to run our web applications in production. We should not be afraid to make mistakes and gain further experience on the best ways to run Docker in production. As the Docker community evolves, so do these practices through the collective experience of the community. So, we should continue and be disciplined in learning the fundamentals we started to master little by little. Don't hesitate to run Docker in production!</p></div></body></html>