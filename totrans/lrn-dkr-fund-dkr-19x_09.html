<html><head></head><body>
        

                            
                    <h1 class="header-title">Using Docker to Supercharge Automation</h1>
                
            
            
                
<p class="mce-root">In the last chapter, we introduced techniques commonly used to allow a developer to evolve, modify, debug, and test their code while running in a container. We also learned how to instrument applications so that they generate logging information that can help us to do root cause analysis of failures or misbehaviors of applications or application services that are running in production.</p>
<p>In this chapter, we will show how you can use tools to perform administrative tasks without having to install those tools on the host computer. We will also illustrate the use of containers that host and run test scripts or code used to test and validate application services running in containers. Finally, we will guide the reader through the task of building a simple Docker-based CI/CD pipeline.</p>
<p>This is a quick overview of all of the subjects we are going to touch on in this chapter:</p>
<ul>
<li>Executing simple admin tasks in a container</li>
<li>Using test containers</li>
<li>Using Docker to power a CI/CD pipeline</li>
</ul>
<p>After finishing this chapter, you will be able to do the following:</p>
<ul>
<li>Run a tool not available on the host in a container</li>
<li>Use a container to run test scripts or code against an application service</li>
<li>Build a simple CI/CD pipeline using Docker<br/></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>In this section, if you want to follow along with the code, you need Docker for Desktop on your macOS or Windows machine and a code editor, preferably Visual Studio Code. The sample will also work on a Linux machine with Docker and VS Code installed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Executing simple admin tasks in a container</h1>
                
            
            
                
<p>Let's assume you need to strip all leading whitespaces from a file and you found the following handy Perl script to do exactly that:</p>
<pre><strong>$ cat sample.txt | perl -lpe 's/^\s*//'</strong></pre>
<p>As it turns out, you don't have Perl installed on your working machine. What can you do? Install Perl on the machine? Well, that would certainly be an option, and it's exactly what most developers or system admins do. But wait a second, you already have Docker installed on your machine. Can't we use Docker to circumvent the need to install Perl? Yes, we can. This is how we're going to do it:</p>
<ol>
<li>Create a folder, <kbd>ch07/simple-task</kbd>, and navigate to it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir -p ~/fod/ch07/simple-task &amp;&amp; cd ~/fod/ch07/simple-task</strong></pre>
<ol start="2">
<li>Open VS Code from within this folder:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ code .</strong></pre>
<ol start="3">
<li>In this folder, create a <kbd>sample.txt</kbd> file with the following content:</li>
</ol>
<pre style="padding-left: 60px">1234567890<br/>  This is some text<br/>   another line of text<br/> more text<br/>     final line</pre>
<p style="padding-left: 60px">Please note the whitespaces at the beginning of each line. Save the file.</p>
<ol start="4">
<li>Now, we can run a container with Perl installed in it. Thankfully, there is an official Perl image on Docker Hub. We are going to use the slim version of the image:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run --rm -it \</strong><br/><strong>    -v $(pwd):/usr/src/app \</strong><br/><strong>    -w /usr/src/app \</strong><br/><strong>    perl:slim sh -c "cat sample.txt | perl -lpe 's/^\s*//'"</strong></pre>
<p style="padding-left: 60px">The preceding command runs a Perl container (<kbd>perl:slim</kbd>) interactively, maps the content of the current folder into the <kbd>/usr/src/app</kbd> folder of the container, and sets the working folder inside the container to <kbd>/usr/src/app</kbd>. The command that is run inside the container is <kbd>sh -c "cat sample.txt | perl -lpe 's/^\s*//'"</kbd>, basically spawning a Bourne shell and executing our desired Perl command.</p>
<p style="padding-left: 60px">The output generated by the preceding command should look like this:</p>
<pre style="padding-left: 60px">1234567890<br/>This is some text<br/>another line of text<br/>more text<br/>final line</pre>
<ol start="5">
<li>Without needing to install Perl on our machine, we were able to achieve our goal.</li>
</ol>
<p style="padding-left: 60px">If that doesn't convince you yet because if you're on macOS, you already have Perl installed, then consider you're looking into running a Perl script named <kbd>your-old-perl-script.pl</kbd> that is old and not compatible with the newest release of Perl that you happen to have installed on your system. Do you try to install multiple versions of Perl on your machine and potentially break something? No, you just run a container with the (old) version of Perl that is compatible with your script, as in this example:</p>
<pre style="padding-left: 60px"><strong>$ docker container run -it --rm \</strong><br/><strong>    </strong><strong>-v $(pwd):/usr/src/app \</strong><br/><strong>    -w /usr/src/app \</strong><br/><strong>    perl:&lt;old-version&gt; perl your-old-perl-script.pl</strong></pre>
<p style="padding-left: 60px">Here, <kbd>&lt;old-version&gt;</kbd> corresponds to the tag of the version of Perl that you need to run your script. The nice thing is that, after the script has run, the container is removed from your system without leaving any traces because we used the <kbd>--rm</kbd> flag in the <kbd>docker container run</kbd> command.</p>
<p>A lot of people use quick and dirty Python scripts or mini apps to automate tasks that are not easily coded with, say, Bash. Now if the Python script has been written in Python 3.7 and you only happen to have Python 2.7 installed, or no version at all on your machine, then the easiest solution is to execute the script inside a container. Let's assume a simple example where the Python script counts lines, words, and letters in a given file and outputs the result to the console:</p>
<ol>
<li>Still in the <kbd>ch07/simple-task</kbd> folder add a <kbd>stats.py</kbd> file and add the following content:</li>
</ol>
<pre style="padding-left: 60px">import sys<br/> <br/>fname = sys.argv[1]<br/>lines = 0<br/>words = 0<br/>letters = 0<br/> <br/>for line in open(fname):<br/>    lines += 1<br/>    letters += len(line)<br/> <br/>    pos = 'out'<br/>    for letter in line:<br/>        if letter != ' ' and pos == 'out':<br/>            words += 1<br/>            pos = 'in'<br/>        elif letter == ' ':<br/>            pos = 'out'<br/> <br/>print("Lines:", lines)<br/>print("Words:", words)<br/>print("Letters:", letters)</pre>
<ol start="2">
<li>After saving the file, you can run it with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run --rm -it \</strong><br/><strong>    -v $(pwd):/usr/src/app \</strong><br/><strong>    -w /usr/src/app \</strong><br/><strong>    python:3.7.4-alpine python stats.py sample.txt</strong></pre>
<p style="padding-left: 60px">Note that, in this example, we are reusing the <kbd>sample.txt</kbd> file from before. The output in my case is as follows:</p>
<pre style="padding-left: 60px">Lines: 5<br/>Words: 13<br/>Letters: 81</pre>
<p style="padding-left: 60px">The beauty of this approach is that this Python script will now run on any computer with any OS installed, as long as the machine is a Docker host and, hence, can run containers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using test containers</h1>
                
            
            
                
<p>For each serious software project out there, it is highly recommended to have plenty of tests in place. There are various test categories such as unit tests, integration tests, stress and load tests, and end-to-end tests. I have tried to visualize the different categories in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-876 image-border" src="img/f5d017fc-0214-43d4-9c0a-a701c9731c84.png" style="width:26.83em;height:27.67em;"/></p>
<p>Categories of application tests</p>
<p>Unit tests assert the correctness and quality of an individual, isolated piece of the overall application or application service. Integration tests make sure that pieces that are closely related work together as expected. Stress and load tests often take the application or service as a whole and assert a correct behavior under various edge cases such as high load through multiple concurrent requests handled by the service, or by flooding the service with a huge amount of data. Finally, end-to-end tests simulate a real user working with the application or application service. The typical tasks that a user would do are automated.</p>
<p>The code or component under test is often called a <strong>System Under Test</strong> (<strong>SUT</strong>).</p>
<p>Unit tests are in their nature tightly coupled to the actual code or SUT. It is, hence, necessary that those tests run in the same context as the code under test. Hence, the test code lives in the same container as the SUT. All external dependencies of the SUT are either mocked or stubbed.</p>
<p>Integration tests, stress and load tests, and end-to-end tests, on the other hand, act on public interfaces of the system under test and it is, hence, most common to run that test code in a separate container:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-878 image-border" src="img/3e2a02c0-87e7-412b-b912-4060b337280f.png" style="width:35.17em;height:19.25em;"/></p>
<p>Integration tests using containers</p>
<p>In the preceding diagram, we can see the <strong>Test Code</strong> running in its own <strong>Test Container</strong>. The <strong>Test Code</strong> accesses the public interface of the <strong>API</strong> component that also runs in a dedicated container. The <strong>API</strong> component has external dependencies such as <strong>Other</strong> <strong>Service</strong> and <strong>Database</strong> that each run in their dedicated container. In this case, the whole ensemble of <strong>API</strong>, <strong>Other</strong> <strong>Service</strong>, and <strong>Database </strong>is our system under test, or SUT.</p>
<p>What exactly would stress and load tests look like? Imagine a situation where we have a Kafka Streams application we want to put under test. The following diagram gives an idea of what exactly we could test, from a high level:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-879 image-border" src="img/7b928ee2-e7df-4a26-9c6c-1504683653ee.png" style="width:40.58em;height:17.33em;"/></p>
<p>Stress and load test a Kafka Streams application</p>
<p>In a nutshell, a <strong>Kafka Streams application</strong> consumes data from one or more topics stored in Apache Kafka(R). The application filters, transforms, or aggregates the data. The resulting data is written back to one or several topics in Kafka. Typically, when working with Kafka, we deal with real-time data streaming into Kafka. Tests could now simulate the following:</p>
<ul>
<li>Large topics with a huge amount of records</li>
<li>Data flowing into Kafka with a very high frequency</li>
<li>Data being grouped by the application under test, where there is a lot of distinct keys, each one with low cardinality</li>
<li>Data aggregated by time windows where the size of the window is small, for example, each only a few seconds long</li>
</ul>
<p>End-to-end tests automate the users that interact with an application by the use of tools such as the Selenium Web Driver, which provides a developer means to automate actions on a given web page such as filling out fields in a form or clicking buttons.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Integration tests for a Node.js application</h1>
                
            
            
                
<p class="mce-root">Let's now have a look at a sample integration test implemented in Node.js. Here is the setup that we are going to look into:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-880 image-border" src="img/b2695574-24aa-48e3-9760-f0ab1e9e8247.png" style="width:27.17em;height:11.75em;"/></p>
<p>Integration tests for an Express JS Application</p>
<p>Following are the steps to create such an integration test:</p>
<ol>
<li>Let's first prepare our project folder structure. We create the project root and navigate to it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir ~/fod/ch07/integration-test-node &amp;&amp; \<br/>    cd ~/fod/ch07/integration-test-node</strong></pre>
<ol start="2">
<li>Within this folder, we create three subfolders, <kbd>tests</kbd>, <kbd>api</kbd>, and <kbd>database</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir tests api database</strong></pre>
<ol start="3">
<li>Now, we open VS Code from the project root:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ code .</strong></pre>
<ol start="4">
<li>To the <kbd>database</kbd> folder, add an <kbd>init-script.sql</kbd> file with the following content:</li>
</ol>
<pre style="padding-left: 60px"><strong>CREATE TABLE hobbies(</strong><br/><strong>   hobby_id serial PRIMARY KEY,</strong><br/><strong>   hobby VARCHAR (255) UNIQUE NOT NULL</strong><br/><strong>);</strong><br/><br/><strong>insert into hobbies(hobby) values('swimming');</strong><br/><strong>insert into hobbies(hobby) values('diving');</strong><br/><strong>insert into hobbies(hobby) values('jogging');</strong><br/><strong>insert into hobbies(hobby) values('dancing');</strong><br/><strong>insert into hobbies(hobby) values('cooking');</strong></pre>
<p style="padding-left: 60px">The preceding script will create a <kbd>hobbies</kbd> table in our Postgres database that we are going to use and fill it with some seed data. Save the file.</p>
<ol start="5">
<li>Now we can start the database. Of course, we are going to use the official Docker image for Postgres to run the database in a container. But first, we will create a Docker volume where the database will store its files. We will call the volume <kbd>pg-data</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker volume create pg-data</strong></pre>
<ol start="6">
<li>Now, it's time to run the database container. From within the project root folder (<kbd>integration-test-node</kbd>), run the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run -d \</strong><br/><strong>    --name postgres \</strong><br/><strong>    -p 5432:5432 \</strong><br/><strong>    -v $(pwd)/database:/docker-entrypoint-initdb.d \</strong><br/><strong>    -v pg-data:/var/lib/postgresql/data \</strong><br/><strong>    -e POSTGRES_USER=dbuser \</strong><br/><strong>    -e POSTGRES_DB=sample-db \</strong><br/><strong>    postgres:11.5-alpine</strong></pre>
<p style="padding-left: 60px">Note that the folder from which you run the preceding command matters, due to the volume mounting we are using for the database initialization script, <kbd>init-script.sql</kbd>. Also note that we are using environment variables to define the name and user of the database in Postgres, and we are mapping port <kbd>5432</kbd> of Postgres to the equivalent port on our host machine.</p>
<ol start="7">
<li>After you have started the database container, double-check that it runs as expected by retrieving its logs:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container logs postgres</strong></pre>
<p style="padding-left: 60px">You should see something similar to this:</p>
<pre style="padding-left: 60px">...<br/>server started<br/>CREATE DATABASE<br/><br/>/usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init-db.sql<br/>CREATE TABLE<br/>INSERT 0 1<br/>INSERT 0 1<br/>INSERT 0 1<br/>INSERT 0 1<br/>INSERT 0 1<br/><br/>...<br/><br/>PostgreSQL init process complete; ready for start up.<br/><br/>2019-09-07 17:22:30.056 UTC [1] LOG: listening on IPv4 address "0.0.0.0", port 5432<br/>...</pre>
<p style="padding-left: 60px">Note, we have shortened the output for better readability. The important parts of the preceding output are the first few lines, where we can see that the database has picked up our initialization script, created the <kbd>hobbies</kbd> table and seeded it with five records. Also important is the last line, telling us that the database is ready to work. The container logs are always your first stop when troubleshooting problems!</p>
<p>With that, our first piece of the SUT is ready. Let's move on to the next one, which is our API implemented in Express JS:</p>
<ol start="1">
<li class="mce-root">In the Terminal window, navigate to the <kbd>api</kbd> folder:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd ~/fod/ch07/integration-test-node/api</strong></pre>
<ol start="2">
<li>Then, run <kbd>npm init</kbd> to initialize the API project. Just accept all defaults:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ npm init</strong></pre>
<p style="padding-left: 60px">The resulting <kbd>package.json</kbd> file should look like this:</p>
<pre style="padding-left: 60px">{<br/>  "name": "api",<br/>  "version": "1.0.0",<br/>  "description": "",<br/>  "main": "index.js",<br/>  "scripts": {<br/>    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"<br/>  },<br/>  "author": "",<br/>  "license": "ISC"<br/>}</pre>
<ol start="3">
<li>Modify the <kbd>scripts</kbd> node of the preceding file so that it contains a start command:</li>
</ol>
<div><img src="img/6c851736-d5d7-4656-b565-8666cd016cf8.png" style="width:42.67em;height:7.25em;"/> </div>
<p>Adding a start script to the package.json file</p>
<ol start="4">
<li>We then have to install Express JS and can do so with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ npm install express --save</strong></pre>
<p style="padding-left: 60px">This will install the library and all of its dependencies and add a dependencies node to our <kbd>package.json</kbd> file that looks similar to this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6d01c027-f5be-4de4-be22-57af5ff422e0.png" style="width:33.42em;height:4.08em;"/></p>
<p>Adding Express JS as a dependency to the API</p>
<ol start="5">
<li>In the <kbd>api</kbd> folder, create a <kbd>server.js</kbd> file and add the following code snippet:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/55518a75-754f-4ef7-98bf-e93eb73332a7.png" style="width:30.92em;height:12.67em;"/></p>
<p>Simple Express JS API</p>
<p style="padding-left: 60px">This is a simple Express JS API with only the <kbd>/</kbd> endpoint implemented. It serves as a starting point for our exploration into integration testing. Note that the API will be listening at port <kbd>3000</kbd>, on all endpoints inside the container (<kbd>0.0.0.0</kbd>).</p>
<ol start="6">
<li>Now we can start the API with <kbd>npm start</kbd> and then test the home endpoint, for example, with <kbd>curl</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ curl localhost:3000</strong><br/>Sample API</pre>
<p style="padding-left: 60px">After all of these steps, we're ready to scaffold the test environment.</p>
<ol start="7">
<li>We will be using <kbd>jasmine</kbd> to write our tests. Navigate to the <kbd>tests</kbd> folder and run <kbd>npm init</kbd> to initialize the test project:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd ~/fod/ch07/integration-test-node/tests &amp;&amp; \<br/>    npm init</strong></pre>
<p style="padding-left: 60px">Accept all of the defaults.</p>
<ol start="8">
<li>Next, add <kbd>jasmine</kbd> to the project:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ npm install --save-dev jasmine</strong></pre>
<ol start="9">
<li>Then initialize <kbd>jasmine</kbd> for this project:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ node node_modules/jasmine/bin/jasmine init</strong></pre>
<ol start="10">
<li>We also need to change our <kbd>package.json</kbd> file so that the scripts block looks like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/f06ce569-69d4-47ab-9346-700ec2fa8eaf.png" style="width:35.42em;height:4.33em;"/></p>
<p>Adding a test script for our integration tests</p>
<ol start="11">
<li>We cannot run the tests any time by executing <kbd>npm test</kbd> from within the <kbd>tests</kbd> folder. The first time we run it, we will get an error since we have not yet added any tests:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/fa888ef5-069b-4297-a38e-2af3a671686c.png" style="width:33.33em;height:13.67em;"/></p>
<p>The first run fails since no tests were found</p>
<ol start="12">
<li>Now in the <kbd>spec/support</kbd> subfolder of the project, let's create a <kbd>jasmine.json</kbd> file. This will contain the configuration settings for the <kbd>jasmine</kbd> test framework. Add the following code snippet to this file and save:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  "spec_dir": "spec",<br/>  "spec_files": [<br/>    "**/*[sS]pec.js"<br/>  ],<br/>  "stopSpecOnExpectationFailure": false,<br/>  "random": false<br/>}</pre>
<ol start="13">
<li>Since we are going to author integration tests we will want to access the SUT via its public interface, which, in our case, is a RESTful API. Hence, we need a client library that allows us to do so. My choice is the Requests library. Let's add it to our project:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>npm install request --save-dev</strong></pre>
<ol start="14">
<li>Add an <kbd>api-spec.js</kbd> file to the <kbd>spec</kbd> subfolder of the project. It will contain our test functions. Let's start with the first one:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/6f2f86bd-53a7-4447-ac3d-171f7955d2de.png" style="width:39.83em;height:29.25em;"/></p>
<p>Sample test suite for the API</p>
<p style="padding-left: 60px">We are using the <kbd>request</kbd> library to make RESTful calls to our API (line <kbd>1</kbd>). Then, on line <kbd>3</kbd>, we're defining the base URL on which the API is listening. Note, the code that we use allows us to override the default of <kbd>http://localhost:3000</kbd> with whatever we define in an environment variable called <kbd>BASE_URL</kbd>. Line <kbd>5</kbd> defines our test suite, which, on line <kbd>6</kbd>, has a test for <kbd>GET /</kbd>. We then assert two outcomes, namely that the status code of a <kbd>GET</kbd> call to <kbd>/</kbd> is <kbd>200</kbd> (OK) and that the text returned in the body of the response is equal to <kbd>Sample API</kbd>.</p>
<ol start="15">
<li>If we run the test now, we get the following outcome:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/11d9290c-3d06-4c15-84ac-ae664f87ad9b.png" style="width:36.42em;height:11.42em;"/></p>
<p>Successfully running Jasmine-based integration tests</p>
<p style="padding-left: 60px">We have two specifications—another word for tests—running; all of them are successful since we have zero failures reported.</p>
<ol start="16">
<li>Before we continue, please stop the API and remove the Postgres container with <kbd>docker container rm -f postgres</kbd>.</li>
</ol>
<p>So far so good, but now let's bring containers to the table. That's what we are most excited about, isn't it? We're excited to run everything, including test code in containers. If you recall, we are going to deal with three containers, the database, the API, and the container with the test code. For the database, we are just using the standard Postgres Docker image, but, for the API and tests, we will create our own images:</p>
<ol>
<li>Let's start with the API. To the <kbd>api</kbd> folder, add a <kbd>Dockerfile</kbd> file with this content:</li>
</ol>
<pre style="padding-left: 60px"><strong>FROM node:alpine</strong><br/><strong>WORKDIR /usr/src/app</strong><br/><strong>COPY package.json ./</strong><br/><strong>RUN npm install</strong><br/><strong>COPY . .</strong><br/><strong>EXPOSE 3000</strong><br/><strong>CMD npm start</strong></pre>
<p style="padding-left: 60px">This is just a very standard way of creating a container image for a Node.js based application. There's nothing special here. </p>
<ol start="2">
<li>To the <kbd>tests</kbd> folder, also add a Dockerfile with this content:</li>
</ol>
<pre style="padding-left: 60px"><strong>FROM node:alpine</strong><br/><strong>WORKDIR /usr/src/app</strong><br/><strong>COPY package.json ./</strong><br/><strong>RUN npm install</strong><br/><strong>COPY . .</strong><br/><strong>CMD npm test</strong></pre>
<ol start="3">
<li>Now, we're ready to run all three containers, in the right sequence. To simplify this task, let's create a shell script that does exactly that. Add a <kbd>test.sh</kbd> file to the <kbd>integration-test-node</kbd> folder, our project root folder. Add the following content to this file and save:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker image build -t api-node api</strong><br/><strong>docker image build -t tests-node tests</strong><br/><br/><strong>docker network create test-net</strong><br/><br/><strong>docker container run --rm -d \</strong><br/><strong>    --name postgres \</strong><br/><strong>    --net test-net \</strong><br/><strong>    -v $(pwd)/database:/docker-entrypoint-initdb.d \</strong><br/><strong>    -v pg-data:/var/lib/postgresql/data \</strong><br/><strong>    -e POSTGRES_USER=dbuser \</strong><br/><strong>    -e POSTGRES_DB=sample-db \</strong><br/><strong>    postgres:11.5-alpine</strong><br/><br/><strong>docker container run --rm -d \</strong><br/><strong>    --name api \</strong><br/><strong>    --net test-net \</strong><br/><strong>    </strong><strong>api-node</strong><br/><br/><strong>echo "Sleeping for 5 sec..."</strong><br/><strong>sleep 5</strong><br/><br/><strong>docker container run --rm -it \</strong><br/><strong>    --name tests \</strong><br/><strong>    --net test-net \</strong><br/><strong>    -e BASE_URL="http://api:3000" \</strong><br/><strong>    tests-node</strong></pre>
<p style="padding-left: 60px">On the first two lines of the script, we make sure that the two container images for API and tests are built with the newest code. Then, we create a Docker network called <kbd>test-net</kbd> on which we will run all three containers. Don't worry about the details of this as we will explain networks in detail in <a href="f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml" target="_blank">Chapter 10</a>, <em>Single Host Networking</em>. For the moment, suffice to say that if all containers run on the same network, then the applications running inside those containers can see each other as if they were running natively on the host, and they can call each other by name.</p>
<p style="padding-left: 60px">The next command starts the database container, followed by the command that starts the API. Then, we pause for a few seconds to give the database and the API time to completely start up and initialize, before we start the third and final container, the tests container.</p>
<ol start="4">
<li>Make this file an executable with the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ chmod +x ./test.sh<br/></strong></pre>
<ol start="5">
<li>Now you can run it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./test.sh</strong></pre>
<p style="padding-left: 60px">If everything works as expected, you should see something along these lines (shortened for readability):</p>
<pre style="padding-left: 60px"><strong>.</strong>..<br/>Successfully built 44e0900aaae2<br/>Successfully tagged tests-node:latest<br/>b4f233c3578898ae851dc6facaa310b014ec86f4507afd0a5afb10027f10c79d<br/>728eb5a573d2c3c1f3a44154e172ed9565606af8e7653afb560ee7e99275ecf6<br/>0474ea5e0afbcc4d9cd966de17e991a6e9a3cec85c53a934545c9352abf87bc6<br/>Sleeping for 10 sec...<br/><br/>&gt; tests@1.0.0 test /usr/src/app<br/>&gt; jasmine<br/><br/>Started<br/>..<br/><br/><br/>2 specs, 0 failures<br/>Finished in 0.072 seconds</pre>
<ol start="6">
<li>We can also create a script that cleans up after testing. For this, add a file called <kbd>cleanup.sh</kbd> and make it an executable the same way as you've done with the <kbd>test.sh</kbd> script. Add the following code snippet to this file:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker container rm -f postgres api</strong><br/><strong>docker network rm test-net</strong><br/><strong>docker volume rm pg-data</strong></pre>
<p style="padding-left: 60px">Line one removes the <kbd>postgres</kbd> and <kbd>api</kbd> containers. Line 2 removes the network we used for the third container, and finally, line 3 removes the volume used by Postgres. After each test run, execute this file with <kbd>./cleanup.sh</kbd>.</p>
<p style="padding-left: 60px">Now you can start adding more code to your API component and more integration tests. Each time you want to test new or modified code, just run the <kbd>test.sh</kbd> script.</p>
<p>Challenge: How can you optimize this process further, so that fewer manual steps are required? <br/>
Use what we have learned in <a href="b6647803-2c5c-4b9d-9a4a-a836ac356329.xhtml" target="_blank">Chapter 6</a>, <em>Debugging Code Running in Containers</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Testcontainers project</h1>
                
            
            
                
<p>If you're a Java developer, then there is a nice project called Testcontainers (<a href="https://testcontainers.org" target="_blank">https://testcontainers.org</a>). In their own words, the project can be summarized as follows:</p>
<p>"Testcontainers is a Java library that supports JUnit tests, providing lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in  Docker container."</p>
<p>To experiment with Testcontainer follow along:</p>
<ol>
<li>First create a <kbd>testcontainer-node</kbd> folder and navigate to it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir ~/fod/ch07/testcontainer-node &amp;&amp; cd ~/fod/ch07/testcontainer-node</strong></pre>
<ol start="2">
<li>Next open VS Code from within that folder with <kbd>code .</kbd>. Create three subfolders, <kbd>database</kbd>, <kbd>api</kbd>, and <kbd>tests</kbd>, within the same folder. To the <kbd>api</kbd> folder, add a <kbd>package.json</kbd> file with the following content:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/7eb83bfd-88b9-4891-9349-2098351469b2.png" style="width:25.50em;height:11.67em;"/></p>
<p>Content of package.json for the API</p>
<ol start="3">
<li>Add a <kbd>server.js</kbd> file to the <kbd>api</kbd> folder with this content:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/e1dd1426-e8cf-48fb-8cc7-f79f5b4f6c95.png" style="width:37.67em;height:33.00em;"/></p>
<p>The sample API using the pg library to access Postgres</p>
<p style="padding-left: 60px">Here, we create an Express JS application listening at port <kbd>3000</kbd>. The application uses the <kbd>pg</kbd> library, which is a client library for Postgres, to access our database. On lines <kbd>8</kbd> through <kbd>15</kbd>, we are defining a connection pool object that will allow us to connect to Postgres and retrieve or write data. On lines <kbd>21</kbd> through <kbd>24</kbd>, we're defining a <kbd>GET</kbd> method on the <kbd>/hobbies</kbd> endpoint, which returns the list of hobbies that are retrieved from the database via the SQL query, <kbd>SELECT hobby FROM hobbies</kbd>.</p>
<p class="mce-root"/>
<ol start="4">
<li>Now add a Dockerfile to the same folder with this content:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/b63beff8-619f-435f-801c-7fe06c7a1333.png" style="width:31.25em;height:7.92em;"/></p>
<p>Dockerfile for the API</p>
<p style="padding-left: 60px">This is exactly the same definition as we used in the previous example. With this, the API is ready to be used. Let's now continue with the tests that will use the <kbd>testcontainer</kbd> library to simplify container-based testing.</p>
<ol start="5">
<li>In your Terminal, navigate to the <kbd>tests</kbd> folder that we created earlier and use <kbd>npm init</kbd> to initialize it as a Node.js project. Accept all of the defaults. Next, use <kbd>npm</kbd> to install the <kbd>request</kbd> library and the <kbd>testcontainers</kbd> library:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ npm install request --save-dev</strong><br/><strong>$ npm install testcontainers --save-dev</strong></pre>
<p style="padding-left: 60px">The result of this is a <kbd>package.json</kbd> file that should look similar to this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d2880fcf-f231-45d2-878b-3b4b2289f79f.png" style="width:30.17em;height:19.33em;"/></p>
<p>The package.json file for the tests project</p>
<ol start="6">
<li>Now, still in the <kbd>tests</kbd> folder, create a <kbd>tests.js</kbd> file and add the following code snippet:</li>
</ol>
<pre style="padding-left: 60px"><strong>const request = require("request");</strong><br/><strong>const path = require('path');</strong><br/><strong>const dns = require('dns');</strong><br/><strong>const os = require('os');</strong><br/><strong>const { GenericContainer } = require("testcontainers");</strong><br/><br/><strong>(async () =&gt; {</strong><br/><strong> // TODO</strong><br/><strong>})();</strong></pre>
<p style="padding-left: 60px">Note how we're requesting a new object such as the <kbd>request</kbd> object, which will help us to access the RESTful interface of our sample API component. We are also requesting the <kbd>GenericContainer</kbd> object from the <kbd>testcontainers</kbd> library that will allow us to build and run any container.</p>
<p style="padding-left: 60px">We then define an async self-invoking function, which will be the wrapper for our setup and test code. It has to be an async function since, inside it, we will be awaiting other async functions, such as the various methods used from the <kbd>testcontainers</kbd> library.</p>
<ol start="7">
<li>As a very first step, we want to use the <kbd>testcontainers</kbd> library to create a Postgres container with the necessary seed data loaded. Let's add this code snippet after <kbd>//TODO</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>const localPath = path.resolve(__dirname, "../database");</strong><br/><strong>const dbContainer = await new GenericContainer("postgres")</strong><br/><strong>    .withName("postgres")</strong><br/><strong>    .withExposedPorts(5432)</strong><br/><strong>    .withEnv("POSTGRES_USER", "dbuser")</strong><br/><strong>    .withEnv("POSTGRES_DB", "sample-db")</strong><br/><strong>    .withBindMount(localPath, "/docker-entrypoint-initdb.d")</strong><br/><strong>    .withTmpFs({ "/temp_pgdata": "rw,noexec,nosuid,size=65536k" })</strong><br/><strong>    .start();</strong></pre>
<p style="padding-left: 60px">The preceding snippet has some similarities with a Docker <kbd>run</kbd> command. That is no accident since we are instructing the <kbd>testcontainers</kbd> library to do exactly that and run an instance of PostgreSQL for us.</p>
<ol start="8">
<li>Next, we need to find out to which host port the exposed port <kbd>5432</kbd> is mapped. We can do that with the following logic:</li>
</ol>
<pre style="padding-left: 60px"><strong>const dbPort = dbContainer.getMappedPort(5432);</strong><strong><br/></strong></pre>
<p style="padding-left: 60px">We will need this information since the API component will have to access Postgres via this port.</p>
<ol start="9">
<li>We also need to know which IP address the host is reachable from within a container—note, localhost won't work from within a container since that would map to the loopback adapter of the container's own network stack. We can get this host IP address like this:</li>
</ol>
<pre style="padding-left: 60px"><strong>const myIP4 = await lookupPromise();</strong></pre>
<p style="padding-left: 60px">The <kbd>lookupPromise</kbd> function is a wrapper function to make the normal async <kbd>dns.lookup</kbd> function return a promise so that we can <kbd>await</kbd> it. Here is its definition:</p>
<pre style="padding-left: 60px"><strong>async function lookupPromise(){</strong><br/><strong>    return new Promise((resolve, reject) =&gt; {</strong><br/><strong>        dns.lookup(os.hostname(), (err, address, family) =&gt; {</strong><br/><strong>            if(err) throw reject(err);</strong><br/><strong>            resolve(address);</strong><br/><strong>        });</strong><br/><strong>   });</strong><br/><strong>};</strong></pre>
<ol start="10">
<li>Now, with this information, we are ready to instruct the <kbd>testcontainer</kbd> library to first build the container image for the API and then run a container from this image. Let's start with the build:</li>
</ol>
<pre style="padding-left: 60px"><strong>const buildContext = path.resolve(__dirname, "../api");</strong><br/><strong>const apiContainer = await GenericContainer</strong><br/><strong>    .fromDockerfile(buildContext)</strong><br/><strong>    .build();</strong></pre>
<p style="padding-left: 60px">Note how this command uses the Dockerfile that we defined in the <kbd>api</kbd> subfolder.</p>
<ol start="11">
<li>Once we have the <kbd>apiContainer</kbd> variable referencing the new image, we can use this to run a container from it:</li>
</ol>
<pre style="padding-left: 60px"><strong>const startedApiContainer = await apiContainer</strong><br/><strong>    .withName("api")</strong><br/><strong>    .withExposedPorts(3000)</strong><br/><strong>    .withEnv("DB_HOST", myIP4)</strong><br/><strong>    .withEnv("DB_PORT", dbPort)</strong><br/><strong>    .start();</strong></pre>
<ol start="12">
<li>Once again, we need to find out to which host port the exposed port <kbd>3000</kbd> of the API component has been mapped. The <kbd>testcontainer</kbd> library makes this a breeze:</li>
</ol>
<pre style="padding-left: 60px"><strong>const apiPort = startedApiContainer.getMappedPort(3000);</strong></pre>
<ol start="13">
<li>With this last line, we have finished the test setup code and can now finally start implementing some tests. We start by defining the base URL for the API component that we want to access. Then, we use the <kbd>request</kbd> library to make an HTTP GET request to the <kbd>/hobbies</kbd> endpoint:</li>
</ol>
<pre style="padding-left: 60px"><strong>const base_url = `http://localhost:${apiPort}`</strong><br/><strong>request.get(base_url + "/hobbies", (error, response, body) =&gt; {</strong><br/><strong>    //Test code here...</strong><br/><strong>})</strong></pre>
<ol start="14">
<li>Let's now implement some assertions right after the <kbd>//Test code here...</kbd> comment:</li>
</ol>
<pre style="padding-left: 60px"><strong>console.log("&gt; expecting status code 200");</strong><br/><strong>if(response.statusCode != 200){</strong><br/><strong>    logError(`Unexpected status code ${response.statusCode}`);</strong><br/><strong>}</strong></pre>
<p style="padding-left: 60px">First, we log our expectation to the console as a feedback when running tests. Then, we assert that the returned status code is <kbd>200</kbd>, and, if not, we log an error. The <kbd>logError</kbd> helper function just writes the given message in red to the console, and prefixes it with <kbd>***ERR</kbd>. Here is the definition of this function:</p>
<pre style="padding-left: 60px"><strong>function logError(message){</strong><br/><strong>    console.log('\x1b[31m%s\x1b[0m', `***ERR: ${message}`);</strong><br/><strong>}</strong></pre>
<ol start="15">
<li>Let's add two more assertions:</li>
</ol>
<pre style="padding-left: 60px"><strong>const hobbies = JSON.parse(body);</strong><br/><strong>console.log("&gt; expecting length of hobbies == 5");</strong><br/><strong>if(hobbies.length != 5){</strong><br/><strong>    logError(`${hobbies.length} != 5`);</strong><br/><strong>}</strong><br/><strong>console.log("&gt; expecting first hobby == swimming");</strong><br/><strong>if(hobbies[0].hobby != "swimming"){</strong><br/><strong>    logError(`${hobbies[0].hobby} != swimming`);</strong><br/><strong>}</strong></pre>
<p style="padding-left: 60px">I leave it up to you, dear reader, to find out what these assertions do exactly.</p>
<ol start="16">
<li>At the end of the assertions, we have to clean up so that we're ready for a next run:</li>
</ol>
<pre style="padding-left: 60px"><strong>await startedApiContainer.stop()</strong><br/><strong>await dbContainer.stop();</strong></pre>
<p style="padding-left: 60px">What we're doing is just stopping the API and the database container. This will automatically remove them from memory too.</p>
<ol start="17">
<li>Now we can run this test suite using the following command from within the <kbd>tests</kbd> subfolder:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ node tests.js<br/></strong></pre>
<p style="padding-left: 60px">The output in my case looks like this (note, I have sprinkled a few <kbd>console.log</kbd> statements in the code to more easily follow along what exactly is happening at a give time):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ba68b227-ae11-4625-9b29-b37915043c4f.png" style="width:45.75em;height:13.17em;"/></p>
<p>Running the testcontainer-based integration tests</p>
<p>The full code is given in the sample code repository that you cloned from GitHub. If you have problems running your tests, please compare your implementation to the given sample solution.</p>
<p>Now that we have a good understanding of how to use containers to run our integration tests, we'll move on to another very popular use case for container based automation, namely, building a Continuous Integration and Continuous Deployment or Delivery (CI/CD) pipeline.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Docker to power a CI/CD pipeline</h1>
                
            
            
                
<p>The goal of this section is to build a CI/CD pipeline that looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-882 image-border" src="img/6e8eff3d-22ee-4c18-9b2b-30f7745a3e91.png" style="width:45.50em;height:23.17em;"/></p>
<p>A simple CI/CD pipeline using Jenkins</p>
<p>We are going to use Jenkins (<a href="https://jenkins.io" target="_blank">https://jenkins.io</a>) as our automation server. Other automation servers such as TeamCity (<a href="https://www.jetbrains.com/teamcity" target="_blank">https://www.jetbrains.com/teamcity</a>) work equally well. When using Jenkins, the central document is the <kbd>Jenkinsfile</kbd>, which will contain the definition of the pipeline with its multiple stages.</p>
<p>A simple <kbd>Jenkinsfile</kbd> with the <kbd>Build</kbd>, <kbd>Test</kbd>, <kbd>Deploy to Staging</kbd>, and <kbd>Deploy to Production</kbd> stages could look like this:</p>
<pre>pipeline {<br/>    agent any<br/>    options {<br/>        skipStagesAfterUnstable()<br/>    }<br/>    stages {<br/>        stage('Build') {<br/>            steps {<br/>                echo 'Building'<br/>            }<br/>        }<br/>        stage('Test') {<br/>            steps {<br/>                echo 'Testing'<br/>            }<br/>        }<br/>        stage('Deploy to Staging') {<br/>            steps {<br/>                echo 'Deploying to Staging'<br/>            }<br/>        }<br/>        stage('Deploy to Production') {<br/>            steps {<br/>                echo 'Deploying to Production'<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>Of course, the preceding pipeline just outputs a message during each stage and does nothing else. It is useful though as a starting point from which to build up our pipeline:</p>
<ol>
<li>Create a project folder named <kbd>jenkins-pipeline</kbd> and navigate to it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir ~/fod/ch07/jenkins-pipeline &amp;&amp; cd ~/fod/ch07/jenkins-pipeline</strong></pre>
<ol start="2">
<li>Now, let's run Jenkins in a Docker container. Use the following command to do so:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker run </strong><strong>--rm -d \</strong><br/><strong>   --name jenkins \</strong><br/><strong>   -u root \</strong><br/><strong>   </strong><strong>-p 8080:8080 \</strong><br/><strong>   </strong><strong>-v jenkins-data:/var/jenkins_home \</strong><br/><strong>   -v /var/run/docker.sock:/var/run/docker.sock \</strong><br/><strong>   -v "$HOME":/home \</strong><br/><strong>   jenkinsci/blueocean</strong></pre>
<p style="padding-left: 60px">Note that we are running as the <kbd>root</kbd> user inside the container and that we are mounting the Docker socket into the container (<kbd>-v /var/run/docker.sock:/var/run/docker.sock</kbd>) so that Jenkins can access Docker from within the container. Data produced and used by Jenkins will be stored in the Docker volume, <kbd>jenkins-data</kbd>.</p>
<ol start="3">
<li>We can find the initial admin password generated automatically by Jenkins with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container exec jenkins cat </strong>/var/jenkins_home/secrets/initialAdminPassword</pre>
<p style="padding-left: 60px">In my case, this outputs <kbd>7f449293de5443a2bbcb0918c8558689</kbd>. Save this password as you will be using it in the next step. </p>
<ol start="4">
<li>In your browser, navigate to <kbd>http://localhost:8080</kbd> to access the graphical UI of Jenkins.</li>
<li>Unlock Jenkins with the admin password that you retrieved with the previous command.</li>
<li>Next, choose Install suggested plugins to have Jenkins automatically install the most useful plugins. Plugins include the GitHub integration, an email extension, Maven and Gradle integration, and so on.</li>
<li>As soon as the plugins are installed, create your first admin account. When asked to restart Jenkins, do so.</li>
</ol>
<ol start="8">
<li>Once you have configured your Jenkins server, start by creating a new project; you may need to click <strong>New Item</strong> in the main menu:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/f64e3315-eaf5-43c9-8ffc-d8fd63e0cd4e.png" style="width:28.33em;height:18.17em;"/></p>
<p>Add a new project in Jenkins</p>
<ol start="9">
<li>Give the project the name <kbd>sample-pipeline</kbd>, select the <kbd>Pipeline</kbd> type, and click OK.</li>
<li>In the configuration view, select the Pipeline tab and add the pipeline definition from the preceding into the Script textbox:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/b96d4516-0374-453f-b7a2-b419e6815a24.png"/></p>
<p>Defining the pipeline in our Jenkins project called sample-pipeline</p>
<ol start="11">
<li>Click Save and then, in the main menu of Jenkins, select Build Now. After a short moment, you should see this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/009185ff-8c65-4336-82f3-1fb4673adc22.png" style="width:37.17em;height:21.08em;"/></p>
<p>Running our sample pipeline in Jenkins</p>
<ol start="12">
<li>Now that we have prepared Jenkins, we can start to integrate our sample application. Let's start with the build step. First, we initialize the <kbd>jenkins-pipeline</kbd> project folder as a Git project:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd ~/fod/ch07/jenkins-pipeline &amp;&amp; git init</strong></pre>
<ol start="13">
<li>Add a <kbd>package.json</kbd> file to this folder with this content:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  "name": "jenkins-pipeline",<br/>  "version": "1.0.0",<br/>  "main": "server.js",<br/>  "scripts": {<br/>    "start": "node server.js",<br/>    "test": "jasmine"<br/>  },<br/>  "dependencies": {<br/>    "express": "^4.17.1"<br/>  },<br/>  "devDependencies": {<br/>    "jasmine": "^3.4.0"<br/>  }<br/>}</pre>
<p style="padding-left: 60px">There is nothing exceptional in this file other the usual list of external dependencies, <kbd>express</kbd> and <kbd>jasmine</kbd>, in this case. Also, note the two scripts <kbd>start</kbd> and <kbd>test</kbd> that we define for use with <kbd>npm</kbd>.</p>
<ol start="14">
<li>Add a <kbd>hobbies.js</kbd> file to the project, which implements the logic to retrieve hobbies as a JavaScript module called <kbd>hobbies</kbd>:</li>
</ol>
<pre style="padding-left: 60px">const hobbies = ["jogging","cooking","diving","swimming","reading"];<br/><br/>exports.getHobbies = () =&gt; {<br/>    return hobbies;<br/>}<br/><br/>exports.getHobby = id =&gt; {<br/>    if(id&lt;1 || id &gt; hobbies.length)<br/>        return null;<br/>    return hobbies[id-1];<br/>}</pre>
<p style="padding-left: 60px">This code evidently is simulating a database by serving pre-canned data stored in the <kbd>hobbies</kbd> array. We do this for simplicity.</p>
<ol start="15">
<li>Next add a <kbd>server.js</kbd> file to the folder that defines a RESTful API with the three endpoints, <kbd>GET /</kbd>,  <kbd>GET /hobbies</kbd>, and <kbd>GET /hobbies/:id</kbd>. The code uses the logic defined in the <kbd>hobbies</kbd> module to retrieve data:</li>
</ol>
<pre style="padding-left: 60px">const hobbies = require('./hobbies');<br/>const express = require('express');<br/>const app = express();<br/><br/>app.listen(3000, '0.0.0.0', () =&gt; {<br/>    console.log('Application listening at 0.0.0.0:3000');<br/>})<br/><br/>app.get('/', (req, res) =&gt; {<br/>    res.send('Sample API');<br/>})<br/><br/>app.get('/hobbies', async (req, res) =&gt; {<br/>    res.send(hobbies.getHobbies());<br/>})<br/><br/>app.get('/hobbies/:id', async (req, res) =&gt; {<br/>    const id = req.params.id;<br/>    const hobby = hobbies.getHobby(id);<br/>    if(!hobby){<br/>        res.status(404).send("Hobby not found");<br/>        return;<br/>    }<br/>    res.send();<br/>})</pre>
<ol start="16">
<li>Now we need to define some unit tests. Create a <kbd>spec</kbd> subfolder in the project and add the <kbd>hobbies-spec.js</kbd> file to it with the following code that tests the <kbd>hobbies</kbd> module:</li>
</ol>
<pre style="padding-left: 60px">const hobbies = require('../hobbies');<br/>describe("API unit test suite", () =&gt; {<br/>    describe("getHobbies", () =&gt; {<br/>        const list = hobbies.getHobbies();<br/>        it("returns 5 hobbies", () =&gt; {<br/>            expect(list.length).toEqual(5);<br/>        });<br/>        it("returns 'jogging' as first hobby", () =&gt; {<br/>            expect(list[0]).toBe("jogging");<br/>        });<br/>    })<br/>})</pre>
<ol start="17">
<li>The last step is to add a <kbd>support/jasmine.json</kbd> file to configure our test framework, Jasmine. Add the following code snippet:</li>
</ol>
<pre style="padding-left: 60px">{<br/>    "spec_dir": "spec",<br/>    "spec_files": [<br/>      "**/*[sS]pec.js"<br/>    ],<br/>    "stopSpecOnExpectationFailure": false,<br/>    "random": false<br/>}</pre>
<p>This is all the code that we need for the moment.</p>
<p>We can now start to build the CI/CD pipeline:</p>
<ol start="1">
<li>Commit the code just created locally with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git add -A &amp;&amp; git commit -m "First commit"</strong></pre>
<ol start="2">
<li>To avoid all of the node modules being saved to GitHub, add a <kbd>.gitignore</kbd> file to the project <kbd>root</kbd> folder with the following content:</li>
</ol>
<pre style="padding-left: 60px"><strong>node_modules</strong></pre>
<ol start="3">
<li>Now, we need to define a repository on GitHub. Log in to your account on GitHub at <a href="https://github.com" target="_blank">https://github.com</a>.</li>
<li>Create a new repository there and call it <kbd>jenkins-pipeline</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/5bd043cb-4a05-4e38-b286-30d4c48a7b40.png" style="width:31.42em;height:28.50em;"/></p>
<p>Create a new GitHub repository for the Jenkins pipeline sample application</p>
<p>Note that my GitHub account is <kbd>gnschenker</kbd>. In your case, it will be your own account.</p>
<ol start="5">
<li>After you have clicked the green button, <strong>Create repository</strong>, go back to you project and execute the following two commands from within the project <kbd>root</kbd> folder:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git remote add origin https://github.com/gnschenker/jenkins-pipeline.git</strong><br/><strong>$ git push -u origin master</strong></pre>
<p style="padding-left: 60px">Make sure you replace <kbd>gnschenker</kbd> in the first line with your own GitHub account name. After this step, your code will be available on GitHub for further use. One of the users will be Jenkins, which will pull the code from this repository as we will show shortly.</p>
<ol start="6">
<li>The next thing is to go back to Jenkins (<kbd>localhost:8080</kbd>) and modify the configuration of the project. Log in to Jenkins if needed and select your project, <kbd>sample-pipeline</kbd>.</li>
<li>Then, select Configure in the main menu. Select the Pipeline tab and modify the settings so that they look similar to this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/c975f0de-984a-4fc0-b092-e4e848fd34dd.png" style="width:42.50em;height:39.75em;"/></p>
<p>Configuring Jenkins to pull source from GitHub</p>
<p style="padding-left: 60px">With this, we configure Jenkins to pull code from GitHub and use a <kbd>Jenkinsfile</kbd> to define the pipeline. <kbd>Jenkinsfile</kbd> is expected to be found in the <kbd>root</kbd> of the project. Note that for the repository URL path, we need to give the relative path to the <kbd>/home</kbd> directory where our project is located. Remember that, when running the Jenkins container, we mapped our own home folder on the host to the <kbd>/home</kbd> folder inside the Jenkins container with this: <kbd>-v "$HOME":/home</kbd>.</p>
<ol start="8">
<li>Hit the green Save button to accept the changes.</li>
<li>We have defined that <kbd>Jenkinsfile</kbd> needs to be in the project <kbd>root</kbd> folder. This is the foundation of <strong>Pipeline-as-Code</strong>, since the pipeline definition file will be committed to the GitHub repository along with the rest of the code. Hence, add a file called <kbd>Jenkinsfile</kbd> to the <kbd>jenkins-pipeline</kbd> folder and add this code to it:</li>
</ol>
<pre style="padding-left: 60px">pipeline {<br/>    environment {<br/>        registry = "gnschenker/jenkins-docker-test"<br/>        DOCKER_PWD = credentials('docker-login-pwd')<br/>    }<br/>    agent {<br/>        docker {<br/>            image 'gnschenker/node-docker'<br/>            args '-p 3000:3000'<br/>            args '-w /app'<br/>            args '-v /var/run/docker.sock:/var/run/docker.sock'<br/>        }<br/>    }<br/>    options {<br/>        skipStagesAfterUnstable()<br/>    }<br/>    stages {<br/>        stage("Build"){<br/>            steps {<br/>                sh 'npm install'<br/>            }<br/>        }<br/>        stage("Test"){<br/>            steps {<br/>                sh 'npm test'<br/>            }<br/>        }<br/>        stage("Build &amp; Push Docker image") {<br/>            steps {<br/>                sh 'docker image build -t $registry:$BUILD_NUMBER .'<br/>                sh 'docker login -u gnschenker -p $DOCKER_PWD'<br/>                sh 'docker image push $registry:$BUILD_NUMBER'<br/>                sh "docker image rm $registry:$BUILD_NUMBER"<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>OK, let's dive into this file one part at a time. At the top, we're defining two environment variables that will be available throughout every stage of the pipeline. We will be using those variables in the <kbd>Build &amp; Push Docker image</kbd> stage:</p>
<pre>environment {<br/>    registry = "gnschenker/jenkins-docker-test"<br/>    DOCKER_PWD = credentials('docker-login-pwd')<br/>}</pre>
<p>The first variable, <kbd>registry</kbd>, just contains the full name of the container image we will eventually produce and push to Docker Hub. Replace <kbd>gnschenker</kbd> with your own GitHub username. The second variable, <kbd>DOCKER_PWD</kbd>, is a bit more interesting. It will contain the password to log in to my Docker Hub account. Of course, I don't want to have the value hardcoded here in code, hence, I use the credentials function of Jenkins that gives me access to a secret stored under the name <kbd>docker-login-pwd</kbd> in Jenkins. </p>
<p>Next, we define the agent we want to use to run the Jenkins pipeline on. In our case, it is based on a Docker image. We are using the <kbd>gnschenker/node-docker</kbd> image for this purpose. This is an image based on <kbd>node:12.10-alpine</kbd>, which has Docker and <kbd>curl</kbd> installed, as we will need these two tools in some of the stages:</p>
<pre>agent {<br/>    docker {<br/>        image 'gnschenker/node-docker'<br/>        args '-v /var/run/docker.sock:/var/run/docker.sock'<br/>    }<br/>}</pre>
<p>With the <kbd>args</kbd> parameter, we are also mapping the Docker socket into the container so that we can use Docker from within the agent. </p>
<p>Ignore the options part for the moment. We then are defining three stages:</p>
<pre>stages {<br/>    stage("Build"){<br/>        steps {<br/>            sh 'npm install'<br/>        }<br/>    }<br/>    stage("Test"){<br/>        steps {<br/>            sh 'npm test'<br/>        }<br/>    }<br/>    stage("Build &amp; Push Docker image") {<br/>        steps {<br/>            sh 'docker image build -t $registry:$BUILD_NUMBER .'<br/>            sh 'docker login -u gnschenker -p $DOCKER_PWD'<br/>            sh 'docker image push $registry:$BUILD_NUMBER'<br/>            sh "docker image rm $registry:$BUILD_NUMBER"<br/>        }<br/>    }<br/>}</pre>
<p>The first stage, <kbd>Build</kbd>, just runs <kbd>npm install</kbd> to make sure all external dependencies of our app can be installed. If this were, for example, a Java application, we would probably also compile and package the application in this step.</p>
<p>In the second stage, <kbd>Test</kbd>, we run <kbd>npm test</kbd>, which runs our unit tests that we have defined for the sample API.</p>
<p>The third stage, <kbd>Build &amp; Push Docker image</kbd>, is a bit more interesting. Now that we have successfully built and tested our application, we can create a Docker image for it and push it to a registry. We are using Docker Hub as our registry, but any private or public registry would work. In this stage, we define four steps:</p>
<ol>
<li>We use Docker to build the image. We use the <kbd>$registry</kbd> environment variable we have defined in the first part of the Jenkinsfile. The <kbd>$BUILD_NUMBER</kbd> variable is defined by Jenkins itself.</li>
<li>Before we can push something to the registry, we need to log in. Here, I am using the <kbd>$DOCKER_PWD</kbd> variable that I defined earlier on.</li>
<li>Once we're successfully logged in to the registry, we can push the image.</li>
<li>Since the image is now in the registry, we can delete it from the local cache to avoid wasting space.</li>
</ol>
<p>Remember that all of the stages run inside our <kbd>gnschenker/node-docker</kbd> builder container. Hence, we're running Docker inside Docker. But, since we have mapped the Docker socket into the builder, the Docker commands act on the host.</p>
<p>Let's add two more stages to the pipeline. The first one looks like this:</p>
<pre>stage('Deploy and smoke test') {<br/>    steps{<br/>        sh './jenkins/scripts/deploy.sh'<br/>    }<br/>}</pre>
<p>Add it just after the <kbd>Build &amp; Push Docker image</kbd> stage. This stage just executes a <kbd>deploy.sh</kbd> script located in the <kbd>jenkins/scripts</kbd> subfolder. We do not yet have such a file in our project.</p>
<p>Hence, add this file to your project with the following content:</p>
<pre>#!/usr/bin/env sh<br/><br/>echo "Removing api container if it exists..."<br/>docker container rm -f api || true<br/>echo "Removing network test-net if it exists..."<br/>docker network rm test-net || true<br/><br/>echo "Deploying app ($registry:$BUILD_NUMBER)..."<br/>docker network create test-net<br/><br/>docker container run -d \<br/>    --name api \<br/>    --net test-net \<br/>    $registry:$BUILD_NUMBER<br/><br/># Logic to wait for the api component to be ready on port 3000<br/><br/>read -d '' wait_for &lt;&lt; EOF<br/>echo "Waiting for API to listen on port 3000..."<br/>while ! nc -z api 3000; do <br/>  sleep 0.1 # wait for 1/10 of the second before check again<br/>  printf "."<br/>done<br/>echo "API ready on port 3000!"<br/>EOF<br/><br/>docker container run --rm \<br/>    --net test-net \<br/>    node:12.10-alpine sh -c "$wait_for"<br/><br/>echo "Smoke tests..."<br/>docker container run --name tester \<br/>    --rm \<br/>    --net test-net \<br/>    gnschenker/node-docker sh -c "curl api:3000"</pre>
<p>OK, so this code does the following. First, it tries to remove any artifacts that might have been left over from an earlier, failed run of the pipeline. Then, it creates a Docker network called <kbd>test-net</kbd>. Next, it runs a container from the image we built in the previous step. This container is our Express JS API and is called <kbd>api</kbd> accordingly.</p>
<p>This container and the application within it may take a moment to be ready. Hence, we define some logic that uses the <kbd>netcat</kbd> or <kbd>nc</kbd> tool to probe port <kbd>3000</kbd>. Once the application is listening at port <kbd>3000</kbd>, we continue with the smoke test. In our case, the smoke test is just making sure it can access the <kbd>/</kbd> endpoint of our API. We are using <kbd>curl</kbd> for this task. In a more realistic setup, you would run some more sophisticated tests here.</p>
<p>As a last stage, we are adding a <kbd>Cleanup</kbd> step:</p>
<ol>
<li>Add the following snippet as a last stage to your <kbd>Jenkinsfile</kbd>:</li>
</ol>
<pre style="padding-left: 60px">stage('Cleanup') {<br/>    steps{<br/>        sh './jenkins/scripts/cleanup.sh'<br/>    }<br/>}</pre>
<p style="padding-left: 60px">Once again, this <kbd>Cleanup</kbd> stage uses a script located in the <kbd>jenkins/script</kbd> subfolder.</p>
<ol start="2">
<li>Please add such a file to your project with the following content:</li>
</ol>
<pre style="padding-left: 60px">#!/usr/bin/env sh<br/><br/>docker rm -f api<br/>docker network rm test-net</pre>
<p style="padding-left: 60px">This script removes the <kbd>api</kbd> container and the Docker network, <kbd>test-net</kbd>, that we used to run our containers on.</p>
<ol start="3">
<li>Now, we are ready to roll. Use <kbd>git</kbd> to commit your changes and push them to your repository:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git -a . &amp;&amp; git commit -m "Defined code based Pipeline"<br/>$ git push origin master</strong></pre>
<p style="padding-left: 60px">Once the code is pushed to GitHub, go back to Jenkins.</p>
<ol start="4">
<li>Select your <kbd>sample-pipeline</kbd> project and click Build now in the main menu. Jenkins will start to build the pipeline. If everything goes well, you should see something like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/09e00d85-9229-40a2-bad3-1fbf4e92e1bc.png"/></p>
<p>Running our full code-based pipeline in Jenkins</p>
<p class="mce-root">Our pipeline is executed successfully and now has six steps. The checkout from GitHub has been automatically added as a first enabling step. To access the logs generated during the pipeline execution, you can click the little ball icon on the left side of the run under Build History. In the preceding screenshot, it is the bluish icon on the left of <strong>#26</strong>. This is especially helpful if the pipeline step fails to quickly find the root cause of the failure.</p>
<p>To summarize, we have built a simple CI/CD pipeline where everything, including the automation server, Jenkins, is running in containers. We have only scratched the surface of what is possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned how to use Docker containers to optimize various kinds of automation tasks, from running a simple one-off task to building up a containerized CI/CD pipeline.</p>
<p>In the next chapter, we will introduce advanced tips, tricks, and concepts useful when containerizing complex distributed applications or when using Docker to automate sophisticated tasks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>Name a few pros and cons for running a one-off task in a container instead of directly on the host machine.</li>
<li>List two or three advantages of running tests in containers.</li>
<li>Sketch a high-level diagram of a containerized CI/CD pipeline, starting from the user producing code till the code being deployed into production. </li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<ul>
<li>Write Maintainable Integration Tests with Docker at <a href="https://www.docker.com/blog/maintainable-integration-tests-with-docker/" target="_blank">https://www.docker.com/blog/maintainable-integration-tests-with-docker/</a><em> </em></li>
<li>A Docker Workflow for .NET Developer - Part 2 (Integration Tests) at <a href="https://gabrielschenker.com/index.php/2019/10/09/a-docker-workflow-for-net-developers-part-2/" target="_blank">https://gabrielschenker.com/index.php/2019/10/09/a-docker-workflow-for-net-developers-part-2/</a></li>
<li>Jenkins on Docker Hub at <a href="https://hub.docker.com/_/jenkins/" target="_blank">https://hub.docker.com/_/jenkins/</a></li>
<li>Jenkins Tutorial Overview at <a href="https://jenkins.io/doc/tutorials/" target="_blank">https://jenkins.io/doc/tutorials/</a></li>
</ul>


            

            
        
    </body></html>