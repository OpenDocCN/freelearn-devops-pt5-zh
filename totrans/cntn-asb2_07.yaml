- en: Deploying Your First Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book so far, we have looked at the various ways we can run builds
    and containers using the Ansible Container workflow. We learned about running
    containers in a local Docker daemon, pushing built containers to a remote Docker
    image repository, managing container images, and even running containers at scale
    using container orchestration tools such as Kubernetes and OpenShift. We have
    almost come full circle, demonstrating the rich capabilities of Ansible Container
    and how it can be leveraged as a fully functional tool for building, running,
    and testing container images throughout an application's life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is one aspect of the Ansible Container workflow we have not
    yet looked at in depth. In previous chapters, we alluded to the `deploy` command,
    and how `deploy` can be leveraged to run containers in production environments,
    or on remote systems. Now that we have covered a lot of the basics of how Docker,
    Kubernetes, and OpenShift work, it is time we turned our attention to the final
    Ansible Container workflow component: `ansible-container deploy`. It is my goal
    that, by reading through this chapter and following along with the examples, it
    will become evident to the reader that Ansible Container is more than a tool used
    to build and run container images locally. It is a robust tool for complex containerized
    application deployments across a variety of popular container platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of ansible-container deploy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying containers to Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying containers to OpenShift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of ansible-container deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ansible-container deploy` command is the component of the Ansible Container
    workflow that is responsible for, you guessed it, deploying containers to remote
    container service engines. At the time of writing, these engines include Docker,
    Kubernetes, and OpenShift. By leveraging configuration in the `container.yml`
    file, Ansible Container has the ability to authenticate to these services and
    leverage API calls to start containers according to the configuration specified
    by the user. Deployment with Ansible Container is a two-step process. First, Ansible
    Container pushes the built container images to a remote image registry, similar
    to Docker Hub or `Quay.io`. This enables the remote container runtime service
    to have access to the containers during the deployment process. Second, Ansible
    Container generates deployment playbooks that can be executed locally and performs
    the deployment using the `ansible-container run` command. Working through the
    deploy process can be a little confusing at first. The following flowchart demonstrates
    the deployment process after first building and running a project locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa204a2c-74c0-4408-9f86-a0518a29f1af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: ansible-container deploy workflow'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to start out by looking at examples using our simple NGINX container
    project. Later, we are going to look at deploying examples to Kubernetes and OpenShift
    using the MariaDB project we built in [Chapter 4](747fd1c6-46e4-424a-be59-5bbf20deb5ed.xhtml),
    *What's in a Role?*
  prefs: []
  type: TYPE_NORMAL
- en: ansible-container deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start looking at `ansible-container deploy`, let''s first rebuild
    the NGINX project we created earlier. In your Ubuntu Vagrant lab VM, navigate
    to the `/vagrant/AnsibleContainer/nginx_demo` directory; or, if you built this
    example yourself in another directory, navigate to it and run the `ansible-container
    build` command. This will make sure that the lab VM has a fresh build of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can validate that the project has successfully been built and the container
    images are cached by running the `docker images` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the container cached locally, we can use the `ansible-container
    deploy` command to simulate project deployment. Without providing any arguments
    about what engine we will deploy our container to, `ansible-container deploy`
    will generate playbooks that can be used to deploy our project onto a local or
    remote host that is running Docker. It will also push our project to the registries
    configured in the `container.yml` file located in the `root` directory of our
    project. Due to the fact that `deploy` leverages much of the same functionality
    as `ansible-container push`, we will provide `deploy` with the same flags we would
    provide the `push` command concerning our container image registry. In this case,
    we will tell it to push to our Docker Hub registry, as we will provide the username
    for our account and any tags we want to use to differentiate this version of the
    container from previous versions. For the purposes of demonstration, we will use
    the `deploy` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The deploy process, in a similar fashion to the push process, will prompt you
    for the password for your Docker Hub account. Upon successful authentication,
    it will push your container image layers to the container image registry. So far,
    this might look exactly identical to the push process. However, you may notice
    that, in the `root` directory of your project, a new directory called `ansible-deployment`
    now exists. Within this directory, you will find a single Ansible playbook that
    is named identically to that of your project, `nginx_demo`. Here is a sample of
    what this playbook looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You may have to ensure the `image` line reflects the image path in this format, `docker.io/username/containername:tag`,
    as some versions of Ansible Container supply the wrong path as input in the playbook.
    If this is the case, simply modify the playbook in a text editor.
  prefs: []
  type: TYPE_NORMAL
- en: The deploy playbook works by making calls to the `docker_service` module running
    on the target hosts. By default, the container uses `localhost` as the target
    host for deployment. However, you can easily provide a standard Ansible inventory
    file to have this project run on remote hosts. The deploy playbook supports full
    Docker life cycle application management, such as starting the container, restarting,
    and ultimately destroying the project by providing a series of playbook tags to
    conditionally execute the desired functionality. You may notice that the playbook
    inherits many of the settings we configured in the `container.yml` file. Ansible
    Container uses these settings so that the playbooks can be executed independently
    of the project itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have already looked at using `ansible-container run` to run our containers
    locally throughout this book,  let''s try executing the playbook directly to start
    the container. This mimics the same process used if you want to manually run a
    deployment outside of the Ansible Container workflow. This can be accomplished
    by using the `ansible-playbook` command with the `start` tag to deploy a project
    on our localhost. You may notice that this process is exactly the same process
    as running the `ansible-container run` command. It is important to note that any
    of the core Ansible Container functionality (run, restart, stop, and destroy)
    can be executed independently of Ansible Container by running playbooks directly
    and supplying the appropriate tag according to the functionality you are trying
    to achieve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the playbook execution has completed, `PLAY RECAP` will show that one
    task has executed a change on your localhost. You can execute the `docker ps -a`
    command to confirm the project has successfully been deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In a similar way, we can run this playbook again, passing in the `--restart`
    tag to restart the container on the Docker host. After executing the playbook
    for a second time, you should see that a single task has once more changed, indicating
    the container has been restarted. This mimics the functionality that the `ansible-container
    restart` command provides. The `status` column in `docker ps -a` will show that
    the container has only been up for a handful of seconds after executing the restart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `stop` tag can be passed into the `ansible-playbook` command to temporarily
    stop the running container. Similar to `restart`, the `docker ps -a` output will
    show that the container is in an `exit` status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now check the status using `docker ps -a` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the project can be removed entirely from our Docker host by passing
    in the `destroy` tag. Running this `playbook` tag will execute a few more steps
    in the playbook, but will ultimately remove all traces of your project from the
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, when any of the core Ansible Container commands are executed,
    they are essentially wrappers around the same playbook that gets generated as
    a part of your project. The purpose of this portion of the chapter was to demonstrate
    to the reader the overall flow of deploying projects using Ansible Container locally,
    and to build upon these skills deeper in the lesson. Where deployment gets really
    interesting is when using Kubernetes and OpenShift as the target deployment engines.
    Using the Ansible Container workflow commands with the corresponding container
    platform engine, we can manage containerized deployments directly using the Ansible
    Container workflow commands instead of executing the playbooks directly.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying containers to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the many aspects that makes the Ansible Container workflow so flexible
    and appealing to organizations and individuals looking to adopt Ansible as the
    native support for remote deployments using Kubernetes and OpenShift. In this
    section, we will look at using the `ansible-container deploy` command to deploy
    our containers to our Google Cloud Kubernetes cluster we created in [Chapter 5](ccc07e61-25e7-4984-953b-586b28b12aab.xhtml),
    C*ontainers at Scale with Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in the previous section, running `ansible-container deploy`
    by itself will, by default, push your container to any image registries you have
    configured in your `container.yml` file and generate a new directory in the `root`
    of your project called `ansible-deployment`. Inside of this directory, a single
    YAML playbook file named after the project will be present. This playbook is used
    to deploy your container to any Docker host, quite similar to the `ansible-container
    run` command. For the purposes of this example, we are going to run `ansible-container
    deploy` using the Kubernetes engine so we can leverage Ansible Container as a
    deployment tool for Kubernetes, creating service definitions and deployment abstractions
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: In order to enable the Kubernetes to deploy functionality in Ansible Container,
    we will add a couple of new parameters to the project `container.yml` file. Specifically,
    we need to point our project to our Kubernetes authentication configuration file,
    and define the namespace our container operates in within Kubernetes. For this
    example, I will use our previously used MariaDB project, but with a few modifications
    to the `container.yml` file to support Kubernetes.  For reference, this project
    can be found in the official Git repository for this book, in the  `Kubernetes/mariadb_demo_k8s` directory.
    Feel free to follow along, or modify the existing MariaDB project to support Kubernetes.
    In the settings section, we will add the `k8s_namespace` and `k8s_auth`.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes `namespace` will contain the name of the namespace in our cluster
    we want to deploy our project into, and the `auth` section will provide the path
    to our Kubernetes authentication configuration file. The default location of the
    Kubernetes authentication configuration is `/home/user/.kube/config`. If you are
    following along using the Vagrant lab, Google Cloud SDK places this configuration
    file at `/home/ubuntu/.kube/config`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can begin, though, we need to set up a default access token in order
    for Ansible Container to access the Google Cloud API. We can do this by executing
    the `gcloud auth application-default login` command. Executing this command will
    provide you with a hyperlink you can use to allow permissions to the Google Cloud
    API using your Google login credentials, similar to what we did in [Chapter 5](ccc07e61-25e7-4984-953b-586b28b12aab.xhtml),
    *Containers at Scale with Kubernetes*. The Google Cloud API will give you a token
    you can enter at the command line that will generate an application default credentials
    file, located at `/home/ubuntu/.config/gcloud/application_default_credentials.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These credentials will be used by any library that requests access to any Google
    Cloud resources, including Ansible Container.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Container Engineer-specific steps are only required if you are using
    a Google Cloud Kubernetes cluster. You can skip these step if you are using a
    local Kubernetes environment such as Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the proper permissions set in the Google Cloud API, we can
    modify the `container.yml` file of our MariaDB project to support the Kubernetes
    deployment engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice the following changes we have made to support the Kubernetes
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`project_name`: For this example, we have added a field in the `settings` block
    called `project_name`. Throughout this book, we have allowed our projects to take
    the default name of the directory that it is built in. Kubernetes is limited as
    to the characters it can use to define services and pods, so we want to ensure
    we do not use illegal characters in our project name by overriding them in the
    `container.yml` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k8s_namespace`: This defines the Kubernetes namespace we will deploy our pods
    into. Leaving this stanza blank will cause Ansible Container to use the default
    namespace that we used in our NGINX deployment earlier in the chapter. In this
    example, we will use a different namespace called `database`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k8s_auth`: This is where we specify the location of our Kubernetes authentication
    configuration file. Within this file, Ansible Container is able to extract the
    IP address of our API server, the access credentials to create resources in the
    cluster, as well as the SSL certificates required to connect to Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once these changes have been placed in your `container.yml` file, let''s build
    the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the project has finished building, we can use the `ansible-container deploy`
    command, specifying the `--engine` flag to use `k8s`, and providing the details
    for the Docker image registry we want to push to as configured in our `container.yml`
    file. For the sake of separation, let''s also tag the image version with `kubernetes`
    so we can keep this version separate in our repository. Ansible Container will
    then push our image to the Docker Hub repository and generate the deployment playbooks
    specific to Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: K8s is shorthand for Kubernetes since Kubernetes comprises the letter K with
    8 letters after it. This is commonly pronounced in the community as **Kay-Eights**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this process has completed, you will notice that, similar to the last
    example, a new directory has appeared in your project called `ansible-deployment`.
    Inside of this directory, you will find a single playbook and a `roles` directory
    that is responsible for performing the actual deployment of our service to Kubernetes.
    As with our previous localhost example, this playbook is likewise divided up based
    on tags that control starting, stopping, and restarting the service in our cluster.
    Since we have not yet deployed our service, we can start the deployment using
    the `ansible-container run` command with the `--engine k8s` flag to indicate a
    Kubernetes deployment. Assuming we have configured everything correctly in the
    `container.yml` file, you should see a successful playbook run, indicating the
    container has been deployed to the Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `kubectl get pods` command from earlier, we can validate that our
    Kubernetes pod has been deployed and is successfully running. Since we deployed
    this particular pod in its own namespace, we need to use the `--namespace` flag
    to see pods that are running in other namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using the `ansible-container run` command with the Kubernetes engine is a powerful
    tool for creating cloud-native services that run on a Kubernetes cluster. Using
    Ansible Container, you have the flexibility to choose how you want to deploy applications,
    by executing the playbooks directly or automatically using the Ansible Container
    workflow. If you wish to delete the current deployment from your Kubernetes cluster,
    you can simply run the `ansible-container --engine k8s destroy` command to completely
    remove the pods and deployment artifacts from the cluster. It is important to
    note that the other Ansible Container workflow commands (start, stop, and restart)
    are perfectly applicable suffixes to use with the Kubernetes deployment engine.
    For the sake of reducing redundancy, let's take a look at how `ansible-container
    deploy` and the workflow commands work with the OpenShift deployment engine. Functionally,
    the Ansible Container workflow commands are identical for Kubernetes and OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying containers to OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ansible-container deploy` command also supports deployments directly to
    OpenShift using the native OpenShift APIs. Since OpenShift is built on top of
    Kubernetes, you will discover that deploying containers to OpenShift is quite
    a similar process to Kubernetes deployments, since OpenShift authentication works
    very similarly to Kubernetes on the backend. Before beginning, these examples
    are going to use the Vagrant lab VM running at the same time as the Minishift
    VM we created in [Chapter 6](d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml), *Managing
    Containers with OpenShift*. This can get quite CPU and RAM intensive. If you're
    attempting to run these examples with 8 GB of RAM or higher, you should get good
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are constrained on resources, these examples can run reasonably
    well using the OpenShift free tier cloud account, although you may run into issues
    with the limited quotas provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before beginning, we need to first ensure that the Vagrant lab environment,
    as well as our Minishift VM, are running and reachable from the VirtualBox network.
    Since the hypervisors used to create our Vagrant lab environment and our Minishift
    cluster are both using VirtualBox, we should by default have network connectivity
    between the two VMs. We can validate this by attempting to ping the Minishift
    VM from our Vagrant lab VM. First, we need to start the Minishift VM using reasonable
    specifications for your local workstation. In this example, I am going to start
    the Minishift VM allocating 8 GB of RAM and 50 GB virtual hard disk storage for
    it. If you are running both VMs simultaneously, you may only be able to allocate
    the minimum 2 GB of RAM for Minishift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the startup process, you should receive an IP from which the
    OpenShift Web UI is accessible. We need to ensure this IP address is reachable
    from our Vagrant lab node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If this IP address is not pingable, you may have to configure your VirtualBox
    networking so that network connectivity is available. A great resource to learn
    more about configuring and debugging VirtualBox networks is the official VirtualBox
    documentation: [https://www.virtualbox.org/manual/ch06.html](https://www.virtualbox.org/manual/ch06.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once networking has been established and verified between the Minishift VM
    and the Vagrant lab VM, next we will need to install the OC on our Vagrant lab
    VM to allow us to authenticate to OpenShift. This will be the exact same process
    we completed in [Chapter 6](d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml), *Managing
    Containers with OpenShift*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the OC binary packages using `wget`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the TAR archive using the `tar -xf` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the binary to a `$PATH` location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have existing Kubernetes credentials in `/home/ubuntu/.kube/config`,
    you will need to back them up to another location so the OC does not overwrite
    them (or simply delete the config file if you have no further use for it any longer:
    `rm -rf ~/.kube/config`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to authenticate to the local OpenShift cluster using the `oc
    login` command in order to generate our Kubernetes credential file that Ansible
    Container will leverage. The `oc login` command takes the URL endpoint of the
    OpenShift cluster as a parameter. By default, the OC will write a Kubernetes configuration
    file to `/home/ubuntu/.kube/config`. This file will serve as our means of authenticating
    to OpenShift Kubernetes to perform automated deployments. Remember to log in as
    the `developer` user, which uses any user-provided password to log in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have successfully authenticated, you should notice there is now a
    new Kubernetes configuration file written to the path `/home/ubuntu/.kube/config`.
    This is the configuration file that Ansible Container will use for access to OpenShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test authentication to the local OpenShift instance by using the `oc
    get all` command. If authentication has been successful, you should see a list
    of pods, deployments, services, and routes currently running in your local OpenShift
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenShift, by default, leverages the same authentication mechanism that Kubernetes
    uses in our `container.yml` file. The only thing we need to provide is the path
    to our Kubernetes configuration file, as well as the Kubernetes namespace the
    project will be deployed into. Since we have previously configured this in our
    MariaDB project in the last section, let''s reuse this same configuration to deploy
    our project to OpenShift. As a review, let''s look at the content of our MariaDB
    project in the Vagrant Lab VM (`/vagrant/Kubernetes/mariadb_demo_k8s`), and look
    at the contents of the `container.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference here is that the `k8s_namespace` parameter will define
    which OpenShift project you want to deploy your container into. In OpenShift terminology,
    `project` and `namespace` are essentially identical. For now, let''s leave the
    configuration as is and look at how to deploy our project using the OpenShift
    engine. Deploying projects using OpenShift is very similar to how we deployed
    using Kubernetes, with the exception that we will prefix our Ansible Container
    commands with the `--engine openshift` flag so that our project will know to talk
    to the OpenShift API directly.  The same syntax rules apply here as well. We will
    give our `deploy` command the name of the repository defined in the `container.yml`
    file to push our container image to, and give it a unique tag to reference later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our container image has been pushed, we can validate the deployment playbooks
    have been generated in the `ansible-deployment` directory (`Kubernetes/mariadb_demo_k8s/ansible-deployment/mariadb-k8s.yml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the Docker and Kubernetes deployment engines, these playbooks can
    be executed independently using the `ansible-playbook` command, or by using the
    `ansible-container run` command. Let''s run our project and deploy it into OpenShift
    using the `ansible-container run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful completion of the playbook run, we can log in to the OpenShift
    web user interface to look at the deployment we just executed. In a web browser,
    navigate to the URL provided in the output of the `minishift start` command (in
    my case, it is `192.168.99.100`), accept the self-signed certificate, and log
    in as the `developer` user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2e97279-fcd3-47a3-9978-d3688f7efa3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Logging into the OpenShift console'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon logging in, you should see a new project has been created, called database.
    In this project you can see everything that the Ansible Container deployment has
    generated by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38260fa4-260a-460b-a698-c2336770d800.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The database project has been created under My Projects in the OpenShift
    console'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the project, `database` will bring you to a dashboard showing the
    relevant details for the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0209240b-cc70-4315-84d7-fd71d2297ed8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: MariaDB database deployment'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, by default the Ansible Container playbooks used to deploy OpenShift
    run with a very useable set of default configuration options. Right away, we can
    see that Ansible Container has created a new project for our project, called `database`.
    Within this project, a default deployment exists that has our MariaDB pod created
    and running. It has even taken the steps for us to create a default service with
    a pre-configured set of labels, and created a route to access the service using
    the `nip.io` DNS service. Essentially, our new service is deployed and ready to
    go right out-of-the-box. In order to use the OpenShift deployment engine, we didn't
    actually have to change any of the `container.yml` configuration; we used exactly
    the same configuration we used to deploy to Kubernetes, with the exception of
    using a different Kubernetes config file, and specifying the OpenShift engine
    in our `run` command. As I'm sure you can see, having the ability to deploy to
    OpenShift or Kubernetes transparently is immensely powerful. This allows Ansible
    Container to function seamlessly no matter what target architecture your service
    is configured to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also validate the deployment by using the OC command-line interface
    client. From the Vagrant lab VM, you can use the `oc project` command to switch
    to the `database` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have switched to a new project context, we can use the `oc get all`
    command to show everything configured to run in this project, including the pods,
    services, and route configuration generated by Ansible Container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Along with `ansible-container run`, we can also use the standard Ansible Container
    workflow commands to manage our deployment, such as `stop`, `restart`, and `destroy`.
    As we discussed earlier, these workflow commands function identically with the
    Kubernetes engine. Let''s first start the `ansible-container stop` command. `stop`
    will gracefully stop all running pods in the deployment, while keeping the other
    resources deployed and active. Let''s try stopping the deployment and re-running
    the `get all` command to learn what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once `stop` has completed successfully, re-run the `oc get all` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, we can see that OpenShift has created a new revision
    for the configuration change we deployed (`REVISION 2`), which describes the deployment
    as having zero running pod replicas, indicative of the deployment existing in
    the stopped state (`Current 0`, `Desired 0`, `Ready 0`). However, the route and
    service artifacts still exist and are running in the cluster. One of the major
    benefits of OpenShift is the nature of OpenShift to readily track the changes
    made to the project under various revision definitions. This makes it very easy
    to roll back to a previous deployment should a change fail or need to be rolled
    back. Complementary to the `stop` command is the `restart` command, which ensures
    the current revision is in a running state, after first stopping the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike `stop`, `restart` does not create a new revision, since our current
    revision is already scaled down to zero replicas, but instead will scale up the
    current revision to ensure that the desired number of pods is running in the project.
    Let''s execute the `ansible-container restart` command for the OpenShift engine
    and see how this affects our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the `oc get all` command once more, we will see that our current
    revision (#2) is now running with the desired number of pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the `ansible-container destroy` command to completely remove
    all traces of our service from the OpenShift (or Kubernetes) cluster. Keep in
    mind that this will also remove the project as well as any other containers that
    are also running within the project that may have been deployed manually or by
    other means outside of Ansible Container. This is why it is important to separate
    application deployments by OpenShift project and Kubernetes namespace, especially
    when running commands such as `ansible-container destroy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the task execution, it appears that a single task that was run
    deleted the entire OpenShift project. This is reflected if we execute the `oc
    get all` command one final time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: These errors indicate that our user can no longer list anything that exists
    inside of the `database` project due to the fact that it no longer exists. All
    traces of the project, deployments, services, pods, and routes, have been deleted
    from the cluster. This is also apparent from the web interface because refreshing
    the web page will indicate that the projects no longer exist.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ansible Container Deployment Guide**: [https://docs.ansible.com/ansible-container/reference/deploy.html](https://docs.ansible.com/ansible-container/reference/deploy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VirtualBox Networking Guide**: [https://www.virtualbox.org/manual/ch06.html](https://www.virtualbox.org/manual/ch06.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of this chapter, we looked at the final Ansible Container workflow
    command: `ansible-container deploy`. `deploy` is one of the most versatile commands
    available in the Ansible Container arsenal since it allows us to run and manage
    containers in production-grade Kubernetes and OpenShift environments. `deploy`
    opens a new path in our journey to enable the flexibility and agility that containers
    give our infrastructure. We can now truly use a single tool to not only build
    and debug containerized applications locally, but also to deploy and manage these
    same applications in production. Having the ability to use the same expressive
    Ansible Playbook language to truly build reliable and scalable applications means
    that deployments can be built around DevOps and automation best practices from
    day one, instead of the painstaking task of re-engineering deployments so they
    are automated after the fact.
  prefs: []
  type: TYPE_NORMAL
- en: Just because we have finished learning about the major Ansible Container workflow
    components does not mean that our journey has ended. So far in this book, we have
    looked at using Ansible Container to deploy single-function microservices that
    require no dependencies on other services. Ansible Container being as powerful
    as it is also has the innate ability to build and deploy multiple containerized
    applications by expressing links and dependencies on other services. In the next
    chapter, we will look at how to build and deploy multi-container applications.
  prefs: []
  type: TYPE_NORMAL
