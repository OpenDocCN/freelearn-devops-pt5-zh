- en: Choosing A Solution For Metrics Storage And Query
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every cluster needs to collect metrics. They are the basis of any alerting system
    we might want to employ. Without the information about the current and the past
    state of a cluster, we would not be able to react to problems when they occur
    nor would we be able to prevent them from happening in the first place. Actually,
    that is not entirely accurate. We could do all those things, but not in a way
    that is efficient and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: A good analogy is blindness. Being blind does not mean that we cannot feel our
    way through an environment. Similarly, we are not helpless without a way to collect
    and query metrics. We can SSH into each of the nodes and check the system manually.
    We can start by fiddling with `top`, `mem`, `df`, and other commands. We can check
    the status of the containers with the `docker stats` command. We can go from one
    container to another and check their logs. We can do all those things, but such
    an approach does not scale. We cannot increase the number of operators with the
    same rhythm as the number of servers. We cannot convert ourselves into human machines.
    Even if we could, we would be terrible at it. That’s why we have tools to help
    us. And, if they do not fulfill our needs, we can build our own solutions on top
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools we can choose. It would be impossible to compare them all,
    so we’ll limit the scope to only a handful.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll focus on open source projects only. Some of the tools we’ll discuss have
    a paid enterprise offering in the form of additional features. We’ll exclude them
    from the comparison. The reason behind the exclusion lies in my belief that we
    should always start with open source software, get comfortable with it, and only
    once it proves its worth, evaluate whether it is worthwhile switching to the enterprise
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we’ll introduce one more limitation. We will explore only the solutions
    that we can host ourselves. That excludes hosted services like, for example, [Scout](https://scoutapp.com/)
    or [DataDog](https://www.datadoghq.com/). The reason behind such a decision is
    two-fold. Many organizations are not willing to “give” their data to a third-party
    hosted service. Even if there is no such restriction, a hosted service would need
    to be able to send alerts back to our system and that would be a huge security
    breach. If neither of those matters to you, they are not flexible enough. None
    of the services I know will give us enough flexibility to build a *self-adapting*
    and *self-healing* system. Besides, the purpose of this book is to give you free
    solutions, hence the insistence on open source solutions that you can host yourself.
  prefs: []
  type: TYPE_NORMAL
- en: That does not mean that paid software is not worth the price nor that we should
    not use, and pay for, hosted service. Quite the contrary. However, I felt it would
    be better to start with things we can build ourselves and explore the limits.
    From there on, you will have a better understanding what you need and whether
    paying money for that is worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Dimensional vs. Dimensional Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we explore the tools we’ll choose from, we should discuss different approaches
    to storing and collecting metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We can divide the tools by dimensions. Some can store data with dimensions while
    others cannot. Representatives of those that are dimensionless would be Graphite
    and Nagios. Truth be told, there is a semblance of dimensions in Graphite, but
    they are so limited in their nature that we’ll treat it as dimensionless. Some
    of the solutions that do support dimensions are, for example, InfluxDB and Prometheus.
    The former supports them in the form of key/value pairs while the latter uses
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Non-dimensional (or dimensionless) metric storage belongs to the *old world*
    when servers were relatively static, and the number of targets that were monitored
    was relatively small. That can be seen from the time those tools were created.
    Both Nagios and Graphite are older tools than InfluxDB and Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Why are dimensions relevant? Query language needs them to be effective. Without
    dimensions, the language is bound to be limited in its capabilities. That does
    not mean that we always need dimensions. For a simple monitoring, they might be
    an overhead. However, running a scalable cluster where services are continuously
    deployed, scaled, updated, and moved around is far from simple. We need metrics
    that can represent all the dimensions of our cluster and the services running
    on top of it. A dynamic system requires dynamic analytics, and that is accomplished
    with metrics that include dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a dimensionless metric would be `container_memory_usage`. Compare
    that with `container_memory_usage{service_name="my-service", task_name="my-service.2.###",
    memory_limit="20000000", ...}"`. The latter example provides much more freedom.
    We can calculate average memory usage as we’d do with dimensionless but we can
    also deduce what the memory limit is, what is the name of the service, which replica
    (task) it is, and so on, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Are dimensions (or lack of them) the only thing that distinguishes tools for
    storing and analyzing metrics? Among others, the way those metrics end up in a
    database makes a difference that might be significant. Some of the tools expect
    data to be pushed while others will pull (or scrape) them.
  prefs: []
  type: TYPE_NORMAL
- en: If we stick with the tools we mentioned previously, representatives of a push
    method would be Graphite and InfluxDB, while Nagios and Prometheus would belong
    to the pull group.
  prefs: []
  type: TYPE_NORMAL
- en: Those that fall into the push category are expecting data to come to them. They
    are passive (at least when metrics gathering is concerned). Each of the services
    that collect data is supposed to push them into one central location. Popular
    examples would be *collectD* and *statsD*. Pull system, on the other hand, is
    active. It will scrape data from all specified targets. Data collectors do not
    know about the existence of the database. Their only purpose is to gather data
    and expose them through a protocol acceptable to the system that will pull them.
  prefs: []
  type: TYPE_NORMAL
- en: A discussion about pros and cons of each system is raging for quite some time.
    There are many arguments in favor of one over the other system, and we could spend
    a lot of time going through all of them. Instead, we’ll discuss discovery, the
    argument that is, in my opinion, the most relevant.
  prefs: []
  type: TYPE_NORMAL
- en: With the push system, discovery is easy. All that data collectors need to know
    is the address of the metrics storage and push data. As long as that address keeps
    being operational, the configuration is very straight forward. With the pull system,
    the system needs to know the location of all the data collectors (or exporters).
    When there are only a few, that is easy to configure. If that number jumps to
    tens, hundreds, or even thousands of targets, the configuration can become very
    tedious. That situation clearly favors the push model. But, technology changed.
    We have reliable systems that provide service discovery. Docker Swarm, for example,
    has it baked in as part of Docker Engine. Finding targets is easy and, assuming
    that we trust service discovery, we always have up to date information about all
    the data collectors.
  prefs: []
  type: TYPE_NORMAL
- en: With a proper service discovery in place, pull vs. push debate becomes, more
    or less, irrelevant. That brings us to an argument that makes pull more appealing.
    It is much easier to discover a failed instance or a missing service when pulling
    data. When a system expects data collectors to push data, it is oblivious whether
    something is missing. We can summarize the problem with “I don’t know what I don’t
    know.” Pull systems, on the other hand, know what to expect. They know what their
    targets are and it is very easy to deduce that when a scraping target does not
    respond, the likely cause is that it stopped working.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2-1: Monitoring tools placement based on dimensions and data collection
    methods](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-1: Monitoring tools placement based on dimensions and data collection
    methods'
  prefs: []
  type: TYPE_NORMAL
- en: Neither of the arguments for push or pull are definitive, and we should not
    make a choice only based on that criteria. Instead, we’ll explore the tools we
    discussed a bit more.
  prefs: []
  type: TYPE_NORMAL
- en: The first one on the list is Graphite.
  prefs: []
  type: TYPE_NORMAL
- en: Graphite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graphite is a passive metrics storage tool. The reason we call it passive lies
    in its inability to collect metrics. They need to be collected and pushed in a
    separate process.
  prefs: []
  type: TYPE_NORMAL
- en: It is a time series database with its own query language and capabilities to
    produce graphs. Querying API is powerful. Or, to be more precise, was considered
    powerful when it appeared. Today, when compared with some other tools, its query
    language is limiting, mainly due to its dimensionless format for storing metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Graphite stores numeric data in time series format. Its metric names consist
    of dot-separated elements.
  prefs: []
  type: TYPE_NORMAL
- en: Data is stored on a local disk.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like Graphite, InfluxDB is a time series database. Unlike Graphite, Influx
    DB data model is based on key/value pairs in the form of labels.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB (open source version, to be more precise) relies on local storage for
    storing data, and its scraping, rule processing, and alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Nagios and Sensu
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nagios is a monitoring system that originated in the 90s as NetSaint. It is
    primarily about alerting based on the exit codes of scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other solutions, the amount and types of data it stores is limited to
    check state making it suitable only for a very basic monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Sensu can be considered a more modern version of Nagios. The primary difference
    is that Sensu clients register themselves, and can determine the checks to run
    either from a central or local configuration. There is also a client socket permitting
    arbitrary check results to be pushed into Sensu.
  prefs: []
  type: TYPE_NORMAL
- en: Sensu uses (almost) the same data model as Nagios and shares its limitation
    of the format it uses to store metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prometheus is a full monitoring and trending system that includes built-in and
    active scraping, storing, querying, graphing, and alerting based on time series
    data. It has knowledge about what the world should look like (which endpoints
    should exist, what time series patterns mean trouble, etc.), and actively tries
    to find faults.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus has a rich data model and probably the most powerful query language
    among time series databases. It encodes dimensions explicitly as key-value pairs
    (labels) attached to a metric name. That allows easy filtering, grouping, and
    matching by these labels via in the query language.
  prefs: []
  type: TYPE_NORMAL
- en: Which Tool Should We Choose?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the tools we listed are (or were) good in their merit. They are different
    in many aspects while similar in others.
  prefs: []
  type: TYPE_NORMAL
- en: Nagios and Sensu served us well in the past. They were designed in a different
    era and based on principles that are today considered obsolete. They work well
    with static clusters and monolithic applications and services running on predefined
    locations. The metrics they store (or lack of them) are not suitable for more
    complex decision making. We would have a hard time using them as means to accomplish
    our goals of operating a scheduler like Docker Swarm running in an auto-scalable
    cluster. Among the solutions we explored, they are the first ones we should discard.
    One is out; three are left to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Dot-separated metrics format used by Graphite is limiting. Excluding elements
    of a metric with asterisks (`*`) is often inadequate for proper filtering, grouping,
    and other operations. Its query language, when compared with InfluxDB and Prometheus,
    is the main reason we’ll discard it.
  prefs: []
  type: TYPE_NORMAL
- en: We’re left with InfluxDB and Prometheus as finalists and are facing only minor
    differences.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB and Prometheus are similar in many ways, so the choice is not going
    to be an easy one. Truth be told, we cannot make a wrong decision. Whichever we
    choose of the two, the choice will be based on slight differences.
  prefs: []
  type: TYPE_NORMAL
- en: If we would not limit ourselves to open source solutions as the only candidates,
    InfluxDB enterprise version could be the winner due to its scalability. However,
    we will discard it in favor of Prometheus. It provides a more complete solution.
    More importantly, Prometheus is slowly becoming the de-facto standard, at least
    when working with schedulers. It is a preferred solution in Kubernetes. Docker
    (and therefore Swarm) is soon going to expose its metrics in Prometheus format.
    That, in itself, is the tipping point that should make us lean slightly more towards
    Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: The decision is made. We’ll use Prometheus to store metrics, to query them,
    and to trigger alerts.
  prefs: []
  type: TYPE_NORMAL
- en: What Now?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we decided which tool will be the basis for storing metrics, we should
    proceed with the setup. Since we will be using Docker Swarm services, deploying
    Prometheus in its most basic form will be a breeze.
  prefs: []
  type: TYPE_NORMAL
