<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift Installation</h1>
                
            
            <article>
                
<p class="calibre2">In the previous chapter, we discussed how to set up your OpenShift lab environment quickly and easily using the most popular and simple methods with <kbd class="calibre12">oc cluster up</kbd>, MiniShift, and Vagrant on CentOS7, macOS, and Windows operating systems. </p>
<p class="calibre2">In this chapter, you will learn about the hardware and software requirements of OpenShift Origin, and you will obtain a basic understanding of OpenShift deployment scenarios and installation methods. You will also be able to perform an advanced installation of OpenShift using Ansible and learn about various options to customize your setup with the Ansible inventory.</p>
<p class="calibre2">In this chapter, we will discuss the following topics:</p>
<p class="calibre2"/>
<ul class="calibre9">
<li class="calibre10">Prerequisites</li>
<li class="calibre10">Overview of OpenShift installation methods</li>
<li class="calibre10">Environment preparation </li>
<li class="calibre10">Advanced installation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Technical requirements</h1>
                
            
            <article>
                
<p class="calibre2">The learning environment for this chapter will be represented by a single virtual machine deployed through Vagrant on VirtualBox. You will need the following minimal configuration in order to support the environment:</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">CPU</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">RAM, GiB</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">OS</strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">2 cores with HT enabled</p>
</td>
<td class="calibre25">
<p class="calibre2">6</p>
</td>
<td class="calibre25">
<p class="calibre2">Fedora 26/CentOS 7/RHEL 7</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">The VM will be about 3 GB in size, so make sure that you have enough free space on your <kbd class="calibre12">/home</kbd> partition, or change the location of the directory where VirtualBox stores the VM's files in the <span class="calibre11">File</span> | <span class="calibre11">Preferences</span> | <span class="calibre11">General</span> | <span class="calibre11">Default Machine</span> folder.</p>
<p class="calibre2">The <kbd class="calibre12">Vagrantfile</kbd> that can be used for deploying our VM may look similar to the following:</p>
<pre class="calibre18"><strong class="calibre1">$ cat Vagrantfile</strong> <br class="title-page-name"/>Vagrant.configure("2") do |config|<br class="title-page-name"/>  config.vm.box = "centos/7"<br class="title-page-name"/>  config.vm.hostname = "openshift.example.com"<br class="title-page-name"/>  config.vm.provider "virtualbox" do |v|<br class="title-page-name"/>    v.memory = 4096<br class="title-page-name"/>    v.cpus = 4<br class="title-page-name"/>  end<br class="title-page-name"/>  config.vm.network "private_network", ip: "172.24.0.11"<br class="title-page-name"/>end</pre>
<div class="packt_infobox">As all OpenShift services in this chapter will be deployed on a single VM, we will provide it with 4 vCPUs and 4 GiB RAM so that all processes have an adequate amount of resources to start with.<br class="title-page-name"/>
In order to be able to pull <kbd class="calibre26">centos/7</kbd> box, you will need Vagrant version 2.x.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Prerequisites</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we will focus on the Origin flavor of OpenShift, which is publicly accessible without any subscription. Like other OpenShift variants, Origin supports all features of commercial OpenShift versions that are provided by RedHat. OpenShift is a complex platform that consists of many components that interact and work together, with each having their own requirements, so before we start, we will have to satisfy them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Hardware requirements</h1>
                
            
            <article>
                
<p class="calibre2">Official OpenShift Origin documentation provides sufficient hardware requirements for different host types. They are summarized in the following table:</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25"/>
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">Masters</strong></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">Nodes</strong></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">External etcd</strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">vCPU</strong></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">2</p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">1</p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">2</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">RAM, GiB</strong></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">16</p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">8</p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">8</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">Disk Storage/Partitioning, GB</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">/var: 40</kbd></p>
<p class="calibre2"><kbd class="calibre12">/usr/local/bin: 1</kbd></p>
<p class="calibre2"><kbd class="calibre12">/tmp: 1</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">/var: 15</kbd></p>
<p class="calibre2"><kbd class="calibre12">/usr/local/bin: 1</kbd></p>
<p class="calibre2"><kbd class="calibre12">/tmp: 1</kbd></p>
<p class="calibre2"><kbd class="calibre12">Docker storage backend: 15</kbd></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter"><kbd class="calibre12">20, SSD is recommended</kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">Network, GB/s</strong></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">1</p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">1</p>
</td>
<td class="calibre25">
<p class="cdpaligncenter">1</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_infobox">The preceding information is not carved in stone and is only provided as a guideline. Hardware requirements for your particular installation are heavily influenced by <span>such </span>factors, as expected number of applications and workload. In fact, they may even be less strict.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Overview of OpenShift installation methods</h1>
                
            
            <article>
                
<p class="calibre2">OpenShift Origin can be installed in several ways, depending on OS and requirements regarding availability, reliability, and scalability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">RPM installation</h1>
                
            
            <article>
                
<p class="calibre2">This is the default method. It installs all required services as systemd units from RPM packages, and they are available on RHEL/CentOS/Fedora. We will focus on this method using the CentOS 7 Linux distribution in the upcoming sections so that you may get a better understanding of the mechanisms involved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Containerized installation</h1>
                
            
            <article>
                
<p class="calibre2">This is the only available method for Atomic Host OS. It installs OpenShift services in Docker containers, which provides an additional level of control. This method is preferable in an enterprise environment, as Atomic Host makes any dependency issues between packages a thing of the past and makes the host preparation process much easier, as Atomic Host provides a container-focused environment out of the box.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deployment scenarios</h1>
                
            
            <article>
                
<p class="calibre2">OpenShift Origin supports various models of deployment, which are listed as follows:</p>
<ul class="calibre9">
<li class="calibre20"><strong class="calibre1">All in one</strong>: Single master, single etcd, and single node installed on the same system.</li>
<li class="calibre20"><strong class="calibre1">Single master, single etcd, and multiple nodes</strong>: This is the scenario we are going to focus on, as it is relatively simple to set up while providing relevant experience. Master and etcd will be installed on the same host.</li>
<li class="calibre20"><strong class="calibre1">Single master, multiple external etcd, and multiple nodes</strong>: This scenario provides HA of etcd nodes by clustering. Multiple etcd nodes form quorum, which is why an odd number of nodes is advised.</li>
<li class="calibre20"><strong class="calibre1">Multiple masters, multiple external etcd, and multiple nodes</strong>: This scenario provides native HA of API as well, which will be explored later in this book. Masters by themselves are stateless, which means that they don't require any synchronization mechanisms.</li>
</ul>
<p class="calibre2">The following diagram illustrates an all-in-one installation:  </p>
<div class="cdpaligncenter2"><img class="alignnone37" src="../images/00044.jpeg"/></div>
<div class="cdpaligncenter1">All-in-one</div>
<p class="calibre2">The following diagram shows a multi-node OpenShift installation with single master and etcd instances:</p>
<div class="cdpaligncenter2"><img class="alignnone38" src="../images/00045.jpeg"/></div>
<div class="cdpaligncenter1">Single master, single etcd, and multiple nodes</div>
<p class="calibre2">The following diagram shows the installation scenario by using a multi-node cluster of etcd:</p>
<div class="cdpaligncenter2"><img class="alignnone39" src="../images/00046.jpeg"/></div>
<div class="cdpaligncenter1">Single master, multiple etcd, and multiple nodes</div>
<p class="calibre2">In a production environment, redundancy and data durability are very important. The following diagram gives an example of a production OpenShift installation. The installation contains three master and three etcd services. All ingress traffic is load balanced:</p>
<div class="cdpaligncenter2"><img class="alignnone40" src="../images/00047.jpeg"/></div>
<div class="cdpaligncenter1">Multiple masters, multiple etcd, and multiple nodes</div>
<div class="packt_infobox">Although it's not uncommon to deploy etcd on the same hosts as masters, they are independent entities and while the former require quorum in an HA setup, masters do not and can be separately deployed in practically any numbers.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Environment preparation</h1>
                
            
            <article>
                
<p class="calibre2">For the RPM installation method, certain configurations need to be made before the installation begins. All commands in the following subsections are to be performed as the root.</p>
<p class="calibre2">First, start the vagrant VM and login as root:</p>
<pre class="calibre18"><strong class="calibre1">$ vagrant up</strong><br class="title-page-name"/>Bringing machine 'default' up with 'virtualbox' provider...<br class="title-page-name"/>...<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>...<br class="title-page-name"/><strong class="calibre1">$ vagrant ssh</strong><br class="title-page-name"/><strong class="calibre1">$ sudo -i</strong><br class="title-page-name"/>#</pre>
<p class="calibre2">Before you do anything else, make sure that all hosts are up-to-date so that all features required by OpenShift are supported by the kernel and user-space libraries:</p>
<pre class="calibre18"><strong class="calibre1"># yum -y update</strong><br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/>Complete!<br class="title-page-name"/><strong class="calibre1"># reboot</strong></pre>
<p class="calibre2">You will also need the following packages:</p>
<pre class="calibre18"><strong class="calibre1"># yum -y install git docker epel-release</strong><br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/>Complete!<br class="title-page-name"/><strong class="calibre1"># yum -y install ansible</strong><br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/>Complete!</pre>
<div class="packt_infobox">We also installed the <kbd class="calibre26">epel-release</kbd> package that enables the EPEL repository needed for installing the latest version of Ansible, required by OpenShift 3.9.<br class="title-page-name"/>
Generally speaking, you can install Ansible on any system, even on your laptop, and install OpenShift from there. But for the sake of simplicity we consolidate everything, including Ansible control functions, on a single VM.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Docker</h1>
                
            
            <article>
                
<p class="calibre2">As with Kubernetes, Docker is relied on by OpenShift for providing a container runtime environment. Therefore, we will need to activate Docker on our OpenShift VM:</p>
<pre class="calibre18"><strong class="calibre1"># systemctl start docker</strong><br class="title-page-name"/><strong class="calibre1"># systemctl enable docker</strong><br class="title-page-name"/>Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SELinux</h1>
                
            
            <article>
                
<p class="calibre2">SELinux must be enabled on all OpenShift nodes, including masters, before running the installer. This is required for isolating container processes with special MLS labels.</p>
<p class="calibre2">Open the <kbd class="calibre12">/etc/selinux/config</kbd> file on the VM and make sure that the <kbd class="calibre12">SELINUX</kbd> and <kbd class="calibre12">SELINUXTYPE</kbd> parameters are set as follows:</p>
<pre class="calibre18"><strong class="calibre1">$ cat /etc/selinux/config</strong><br class="title-page-name"/># This file controls the state of SELinux on the system.<br class="title-page-name"/># SELINUX= can take one of these three values:<br class="title-page-name"/># enforcing - SELinux security policy is enforced.<br class="title-page-name"/># permissive - SELinux prints warnings instead of enforcing.<br class="title-page-name"/># disabled - No SELinux policy is loaded.<br class="title-page-name"/>SELINUX=<strong class="calibre1">enforcing</strong><br class="title-page-name"/># SELINUXTYPE= can take one of three two values:<br class="title-page-name"/># targeted - Targeted processes are protected,<br class="title-page-name"/># minimum - Modification of targeted policy. Only selected processes are protected.<br class="title-page-name"/># mls - Multi Level Security protection.<br class="title-page-name"/>SELINUXTYPE=targeted</pre>
<p class="calibre2">SELinux should already be configured by default. You will need to reboot the VM to activate the changes if you made any.</p>
<p class="calibre2">Check to see if <kbd class="calibre12">SELINUX</kbd> is enabled:</p>
<pre class="calibre18"># <strong class="calibre1">getenforce</strong><br class="title-page-name"/>Enforcing</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Ansible installation</h1>
                
            
            <article>
                
<p class="calibre2">Ansible is a configuration management and orchestration tool that's written in Python. OpenShift Origin installer is actually a collection of Ansible playbooks in YAML format that define what services must be installed and how they are to be configured.</p>
<p class="calibre2">Ansible playbooks are run from a single point, which is called a control machine in Ansible terminology. It can be any machine and in our demonstration of advanced installation, we will use the master as a control machine.</p>
<p class="calibre2">These packages are provided by the EPEL repository, which is already enabled out-of-the-box in our lab. If they weren't, you would have to install the <kbd class="calibre12">epel-release</kbd> package.</p>
<p class="calibre2">Next, clone the installation playbooks themselves from the Git repository:</p>
<pre class="calibre18"><strong class="calibre1"># git clone https://github.com/openshift/openshift-ansible</strong><br class="title-page-name"/>Cloning into 'openshift-ansible'...<br class="title-page-name"/>remote: Counting objects: 95983, done.<br class="title-page-name"/>remote: Compressing objects: 100% (13/13), done.<br class="title-page-name"/>remote: Total 95983 (delta 1), reused 6 (delta 0), pack-reused 95969<br class="title-page-name"/>Receiving objects: 100% (95983/95983), 24.45 MiB | 972.00 KiB/s, done.<br class="title-page-name"/>Resolving deltas: 100% (59281/59281), done.<br class="title-page-name"/><strong class="calibre1"># cd openshift-ansible</strong><br class="title-page-name"/><strong class="calibre1"># git branch -r</strong><br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/>origin/release-3.9<br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/><strong class="calibre1"># git checkout release-3.9</strong><br class="title-page-name"/>Branch release-3.9 set up to track remote branch release-3.9 from origin.<br class="title-page-name"/>Switched to a new branch 'release-3.9'</pre>
<p class="calibre2">We switched specifically to the <strong class="calibre4">3.9</strong> release because the master branch actually tracks the development version.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SSH access</h1>
                
            
            <article>
                
<p class="calibre2">Ansible connects to other systems over SSH protocol, so it requires an SSH key pair. In this section, we are going to ensure SSH access from the root account of our VM to the root account of the same VM. This can be accomplished with the following commands:</p>
<pre class="calibre18"><strong class="calibre1"># ssh-keygen</strong> <br class="title-page-name"/>Generating public/private rsa key pair.<br class="title-page-name"/>Enter file in which to save the key (/root/.ssh/id_rsa): <br class="title-page-name"/>Created directory '/root/.ssh'.<br class="title-page-name"/>Enter passphrase (empty for no passphrase): <br class="title-page-name"/>Enter same passphrase again: <br class="title-page-name"/>Your identification has been saved in /root/.ssh/id_rsa.<br class="title-page-name"/>Your public key has been saved in /root/.ssh/id_rsa.pub.<br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/><strong class="calibre1"># cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys</strong></pre>
<p class="calibre2">You can check whether access is indeed enabled by running the following commands:</p>
<pre class="calibre18"><strong class="calibre1"># ssh 172.24.0.11</strong><br class="title-page-name"/>The authenticity of host '172.24.0.11 (172.24.0.11)' can't be established.<br class="title-page-name"/>ECDSA key fingerprint is SHA256:JX1N6Zt7136jH2cXzd0cwByvFTahuOj3NHYvcIjpG2A.<br class="title-page-name"/>ECDSA key fingerprint is MD5:9b:04:4a:89:5d:65:7a:b0:4b:02:62:fa:25:91:d3:05.<br class="title-page-name"/>Are you sure you want to continue connecting (yes/no)? <strong class="calibre1">yes</strong><br class="title-page-name"/>Warning: Permanently added '172.24.0.11' (ECDSA) to the list of known hosts.<br class="title-page-name"/><strong class="calibre1">[root@openshift ~]# logout</strong><br class="title-page-name"/>Connection to 172.24.0.11 closed.</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Advanced installation</h1>
                
            
            <article>
                
<p class="calibre2">Being able to start a small cluster in a matter of minutes with minimal effort is great, but what if you need more control over various features so that you get a fully functional cluster right after installation? The answer is advanced installation. As opposed to quick installation, it involves the following:</p>
<ol class="calibre13">
<li value="1" class="calibre10">Creating an Ansible inventory file, containing all hosts spread over groups in accordance with the chosen deployment model, and variables customizing the installation for your environment</li>
<li value="2" class="calibre10">Running the Ansible playbook to install, configure, and start all OpenShift components, including internal registry, registry console, and router</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift Ansible inventory</h1>
                
            
            <article>
                
<p class="calibre2">A typical Ansible inventory is a text file that contains groups of hosts and their variables. In the case of OpenShift, there are specific sections that may be present depending on the deployment topology and other requirements. They are summarized in the following table:</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<div class="title-page-name"><strong class="calibre1">Section</strong></div>
</td>
<td class="calibre25">
<div class="title-page-name"><strong class="calibre1">Description</strong></div>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">Required</strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">masters</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Hosts for installation of master services, notably API. Can be a single node for standalone deployment, or an odd number of hosts for an HA setup.</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">new_masters</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Additional hosts for installation of master components when scaling an existing cluster up.</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">nodes</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Hosts for installation of node components, notably kubelet. Usually, there is more than one node, because best practice suggests designating at least one of the nodes as an infrastructure node to host such system services as registry and router.</p>
<p class="calibre2">This section must include all masters as well.</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">new_nodes</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Additional hosts for the installation of node components when scaling an existing cluster up.</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">etcd</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">This section is specified only in the case of HA configuration with multiple external etcds. It may contain the same hosts as masters, but since masters and etcd have different system requirements, it is better to deploy them on separate machines. An odd number is suggested for a quorum.</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">nfs</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">The host for configuring NFS as a persistent storage backend for Ansible broker, internal registry, Hawkular metrics, and Elasticsearch logging.</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">lb</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">In the case of multiple masters, you will have to provide a point of contact for external clients, which will distribute traffic between master hosts. By placing a host in this section, you will instruct Ansible to install HAProxy on that host and configure it for load balancing and TLS/SSL passthrough.</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">glusterfs</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Hosts for configuring GlusterFS as a persistent storage backend.</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">glusterfs_registry</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Hosts for configuring GlusterFS as a persistent storage backend for the internal registry.</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">OSEv3:vars</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">This section contains global variables for configuring various aspects of OpenShift, such as authentication, registry placement, and so on.</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">OSEv3:children</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">This section lists all of the groups that are specified in the rest of the file.</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">Ansible looks for specific variables to determine how various subsystems must be configured. The ones that we are going to use are listed in the following table. The full list of variables is available in the official documentation at <a href="https://docs.openshift.org/latest/install_config/install/advanced_install.html" class="calibre8">https://docs.openshift.org/latest/install_config/install/advanced_install.html</a>:</p>
<div class="title-page-name">
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<div class="cdpaligncenter2"><strong class="calibre1">Variable</strong></div>
</td>
<td class="calibre25">
<div class="cdpaligncenter2"><strong class="calibre1">Description</strong></div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">openshift_node_labels</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Labels assigned to a particular node or to all nodes, depending on whether it is set for a node or globally in <kbd class="calibre12">OSEv3:vars</kbd>. You should have at least one node labeled as <kbd class="calibre12">{'region': 'infra'}</kbd>.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">openshift_schedulable</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Controls whether a node can be used to run pods. Masters are configured as unschedulable by default, but if you have no nodes labeled as infra, you must explicitly set it to true for at least one master, otherwise, pods for the registry and router will fail to start.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">ansible_ssh_user</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">The user account used by Ansible to connect to hosts via SSH.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">openshift_master_identity_providers</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Authentication backends. By default, OpenShift uses <kbd class="calibre12">AllowAllPasswordIdentityProvider</kbd>, effectively accepting all credentials, which is insecure and unacceptable in enterprise environments.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">deployment_type</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">OpenShift distribution to install. Acceptable values are <kbd class="calibre12">enterprise</kbd> for the Red Hat OpenShift Container Platform and <kbd class="calibre12">origin</kbd> for OpenShift Origin.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">openshit_master_default_subdomain</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">The subdomain for exposed services. By default, it is <kbd class="calibre12">&lt;namespace&gt;.svc.cluster.local</kbd>.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">openshift_disable_check</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">Prior to actual installation, Ansible runs a number of checks to make sure that the environment satisfies certain requirements, such as available memory, disk space, and so on. This is meant as a safeguard against poor resource planning, but can be excessive in proof-of-concepts installations, like ours. Since we do not need 8 GB of RAM, 40 GB of disk space for the master, and 15 GB for nodes in /var for a testing environment, these checks can be safely skipped.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">openshift_clock_enabled</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2">OpenShift hosts rely on timestamps for the correct propagation of updates through <kbd class="calibre12">etcd</kbd>, failover, and quorum, which is achieved by time synchronization through NTP. This setting controls whether the <kbd class="calibre12">chronyd</kbd> daemon must be activated.</p>
</td>
</tr>
</tbody>
</table>
</div>
<p class="calibre2"> </p>
<p class="calibre2">Summarizing the information given in the preceding tables, the inventory file's structure should resemble the following:</p>
<pre class="calibre18"><strong class="calibre1"># cat /etc/ansible/hosts</strong><br class="title-page-name"/>...<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>...<br class="title-page-name"/>[masters]<br class="title-page-name"/>172.24.0.11<br class="title-page-name"/><br class="title-page-name"/>[nodes]<br class="title-page-name"/>172.24.0.11 openshift_node_labels="{'region': 'infra', 'zone': 'default'}" openshift_schedulable=true<br class="title-page-name"/><br class="title-page-name"/>[etcd]<br class="title-page-name"/>172.24.0.11<br class="title-page-name"/><br class="title-page-name"/>[OSEv3:vars]<br class="title-page-name"/>openshift_deployment_type=origin<br class="title-page-name"/>openshift_disable_check=memory_availability,disk_availability<br class="title-page-name"/>openshift_ip=172.24.0.11<br class="title-page-name"/>ansible_service_broker_install=false<br class="title-page-name"/>openshift_master_cluster_hostname=172.24.0.11<br class="title-page-name"/>openshift_master_cluster_public_hostname=172.24.0.11<br class="title-page-name"/>openshift_hostname=172.24.0.11<br class="title-page-name"/>openshift_public_hostname=172.24.0.11<br class="title-page-name"/><br class="title-page-name"/>[OSEv3:children]<br class="title-page-name"/>masters<br class="title-page-name"/>nodes<br class="title-page-name"/>etcd</pre>
<div class="packt_infobox">As we have only one node, we have to mark it as schedulable and belonging to <kbd class="calibre26">infra</kbd> region, so that registry, registry console, and router can be deployed. In production, you would usually dedicated masters to management and synchronization tasks, while leaving all the work of running containers to nodes, <span>as well as have multiple nodes split up between different regions for availability.</span>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift Ansible playbooks</h1>
                
            
            <article>
                
<p class="calibre2">The OpenShift Ansible repository contains various playbooks for different tasks. Some of them are presented in the following table:</p>
<div class="title-page-name">
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">Playbook</strong></p>
</td>
<td class="calibre25">
<p class="cdpaligncenter"><strong class="calibre4">Description</strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpalignleft2"><kbd class="calibre12">playbooks/prerequisites.yml</kbd></p>
</td>
<td class="calibre25">
<p class="cdpalignleft2">Sets up prerequisites for the cluster deployment, such as subscribing hosts to Red Hat (in case of Red Hat Enterprise Linux), enabling repositories, and setting up firewall.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpalignleft2"><kbd class="calibre12">playbooks/deploy_cluster.yml</kbd></p>
</td>
<td class="calibre25">
<p class="cdpalignleft2">Performs a complete installation of OpenShift Origin by installing all required packages, configuring, and starting services. This is the playbook we will use in this chapter.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpalignleft2"><kbd class="calibre12">playbooks/openshift-master/scaleup.yml</kbd></p>
</td>
<td class="calibre25">
<p class="cdpalignleft2">Looks for the host group <kbd class="calibre12">new_masters</kbd> in the inventory and configures these hosts as new members of the cluster. After scaleup is complete, you must move these hosts to the group <kbd class="calibre12">masters</kbd> to prevent Ansible from treating them as new ones during the next run.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpalignleft2"><kbd class="calibre12">playbooks/openshift-node/scaleup.yml</kbd></p>
</td>
<td class="calibre25">
<p class="cdpalignleft2">Looks for the host group <kbd class="calibre12">new_nodes</kbd> in the inventory and configures these hosts as new members of the cluster. After scaleup is complete, you must move these hosts to the group <kbd class="calibre12">nodes</kbd> to prevent Ansible from treating them as new ones during the next run.</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="cdpalignleft2"><kbd class="calibre12">playbooks/openshift-etcd/scaleup.yml</kbd></p>
</td>
<td class="calibre25">
<p class="cdpalignleft2">Looks for the host group <kbd class="calibre12">new_etcd</kbd> in the inventory and configures these hosts as new members of the cluster. After scaleup is complete, you must move these hosts to the <kbd class="calibre12">etcd</kbd> group to prevent Ansible from treating them as new ones during the next run.</p>
</td>
</tr>
</tbody>
</table>
</div>
<p class="calibre2"> </p>
<p class="calibre2">Familiarity with Ansible is beneficial, but not required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Installation</h1>
                
            
            <article>
                
<p class="calibre2">At this point, we should have everything we need to begin the installation, so without further delay, let's get started by running the prerequisites playbook:</p>
<pre class="calibre18"><strong class="calibre1"># ansible-playbook playbooks/prerequisites.yml</strong><br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/>PLAY RECAP **************************************************************************************<br class="title-page-name"/>172.24.0.11 : ok=65 changed=17 unreachable=0 failed=0 <br class="title-page-name"/>localhost   : ok=12 changed=0  unreachable=0 failed=0 <br class="title-page-name"/><br class="title-page-name"/>INSTALLER STATUS ********************************************************************************<br class="title-page-name"/>Initialization : Complete (0:00:13)<br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…</pre>
<p class="calibre2">This playbook usually runs for a couple of minutes.</p>
<p class="calibre2">Finally, let's fire up the actual deployment:</p>
<pre class="calibre18"><strong class="calibre1"># ansible-playbook playbooks/deploy_cluster.yml</strong><br class="title-page-name"/>…<br class="title-page-name"/>&lt;output omitted&gt;<br class="title-page-name"/>…<br class="title-page-name"/>PLAY RECAP **************************************************************************************<br class="title-page-name"/>172.24.0.11 : ok=555 changed=234 unreachable=0 failed=0 <br class="title-page-name"/>localhost   : ok=13  changed=0   unreachable=0 failed=0 <br class="title-page-name"/><br class="title-page-name"/>INSTALLER STATUS ********************************************************************************<br class="title-page-name"/>Initialization            : Complete (0:00:18)<br class="title-page-name"/>Health Check              : Complete (0:01:12)<br class="title-page-name"/>etcd Install              : Complete (0:00:50)<br class="title-page-name"/>NFS Install               : Complete (0:00:17)<br class="title-page-name"/>Master Install            : Complete (0:04:27)<br class="title-page-name"/>Master Additional Install : Complete (0:00:29)<br class="title-page-name"/>Node Install              : Complete (0:02:37)<br class="title-page-name"/>Hosted Install            : Complete (0:02:21)<br class="title-page-name"/>Web Console Install       : Complete (0:01:13)<br class="title-page-name"/>Service Catalog Install   : Complete (0:01:23)</pre>
<p class="calibre2">It is quite normal for this process to take 15-20 minutes, so you might as well use this time to skim through this book for basic operations to get you up and running faster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Validation</h1>
                
            
            <article>
                
<p class="calibre2">Once the installation has completed without any failures, you may perform basic sanity checks by querying the master API for installed nodes:</p>
<pre class="calibre18"><strong class="calibre1"># oc get node</strong><br class="title-page-name"/>NAME          STATUS   ROLES          AGE      VERSION<br class="title-page-name"/>172.24.0.11   Ready  compute,master   32m      v1.9.1+a0ce1bc657</pre>
<p class="calibre2"><span class="calibre11">As one would expect from the inventory, our single node acts both as master and node.</span></p>
<p class="calibre2">Another check you can run is to see if infrastructure components, like registry, registry console, and router were successfully deployed:</p>
<pre class="calibre18"><strong class="calibre1"># oc get po -n default</strong><br class="title-page-name"/>NAME                     READY STATUS  RESTARTS AGE<br class="title-page-name"/>docker-registry-1-8g89z  1/1   Running 0        42m<br class="title-page-name"/>registry-console-1-2srg8 1/1   Running 0        42m<br class="title-page-name"/>router-1-c6h95           1/1   Running 0        42m</pre>
<div class="packt_infobox">Infrastructure components of OpenShift reside in the <kbd class="calibre26">default</kbd> project/namespace.</div>
<p class="calibre2">You may also login to the web console at <kbd class="calibre12">https://172.24.0.11:8443</kbd>, where you will be prompted to accept a self-signed certificate, which you should do. You can use any credentials, as OpenShift accepts anyone by default, and you will see the following page:</p>
<div class="cdpaligncenter2"><img src="../images/00048.jpeg" class="calibre33"/></div>
<div class="cdpaligncenter1">Figure 1. OpenShift Service Catalog</div>
<p class="calibre2"><span class="calibre11">The first thing users see when they log in to OpenShift is the Service Catalog, which presents them with various languages and runtimes they can use for deploying their applications. Technically, these are templates which you will get an in-depth understanding of in <a target="_blank" href="part0195.html#5PUTM0-78aafb146b304cdeb9b3261a70edabde" class="calibre8">Chapter 9</a>, Advanced OpenShift Concepts and in <a target="_blank" href="part0265.html#7SN520-78aafb146b304cdeb9b3261a70edabde" class="calibre8">Chapter 13</a>, <em class="calibre17">Deploying Multi-Tier Applications Using Templates</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, you learned about various deployment scenarios of OpenShift Origin, as well as installation methods. You wrote an Ansible inventory file and used it to deploy a ready-to-use OpenShift Origin platform with an internal registry, registry console, and a router installed out-of-the-box.</p>
<p class="calibre2">In the following chapter, we are going to cover OpenShift core concepts such as creating new applications with OpenShift pods, services, routes, projects, and users. This will give you foundational skills, which will be enough for you to be able to run and manage your application container infrastructure in OpenShift. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Questions</h1>
                
            
            <article>
                
<ol class="calibre13">
<li value="1" class="calibre10">What sections of the Ansible inventory file are mandatory for OpenShift installation? choose two:
<ol class="calibre14">
<li value="1" class="calibre10">masters</li>
<li value="2" class="calibre10">nfs</li>
<li value="3" class="calibre10">new_masters</li>
<li value="4" class="calibre10">etcd</li>
</ol>
</li>
<li value="2" class="calibre10">What label must be assigned to at least one node for successful deployment of a router and internal registry?
<ol class="calibre14">
<li value="1" class="calibre10">infrastructure</li>
<li value="2" class="calibre10">dedicated</li>
<li value="3" class="calibre10">infra</li>
<li value="4" class="calibre10">special</li>
</ol>
</li>
</ol>
<ol start="3" class="calibre13">
<li value="3" class="calibre10">What Ansible playbook is being used to deploy the OpenShift cluster?
<ol class="calibre14">
<li value="1" class="calibre10">playbooks/deploy_cluster.yml</li>
<li value="2" class="calibre10">playbooks/byo/config.yml</li>
<li value="3" class="calibre10">playbooks/prerequisites.yml</li>
<li value="4" class="calibre10">playbooks/common/openshift-cluster/config.yml</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Further reading</h1>
                
            
            <article>
                
<p class="calibre2">Please see the following for further reading to accompany this chapter:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Hardware requirements</strong>: <a href="https://docs.openshift.org/latest/install_config/install/prerequisites.html" class="calibre8">https://docs.openshift.org/latest/install_config/install/prerequisites.html</a></li>
<li class="calibre10"><strong class="calibre1">Deployment scenarios</strong>: <a href="https://docs.openshift.org/latest/install_config/install/planning.html#installation-methods" class="calibre8">https://docs.openshift.org/latest/install_config/install/planning.html#installation-methods</a></li>
<li class="calibre10"><strong class="calibre1">Advanced installation and variables reference</strong>: <a href="https://docs.openshift.org/latest/install_config/install/advanced_install.html" class="calibre8">https://docs.openshift.org/latest/install_config/install/advanced_install.html</a></li>
<li class="calibre10"><strong class="calibre1">OpenShit Installation and Configuration</strong>: <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.9/html-single/installation_and_configuration/#install-config-install-planning" class="calibre8">https://access.redhat.com/documentation/en-us/openshift_container_platform/3.9/html-single/installation_and_configuration/#install-config-install-planning</a></li>
</ul>


            </article>

            
        </section>
    </body></html>