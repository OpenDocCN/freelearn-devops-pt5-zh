- en: Chapter 7. Container Schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are at the business end of the book. There is a lot of buzz at the moment
    around this topic. This is where containers are going to go in future, and schedulers
    solve a lot of problems, for instance, spreading the load of our application across
    multiple hosts for us pending on load and starting our containers on another instance
    if the original host fails. In this chapter, we will look at three different schedulers.
    First, we will look at Docker Swarm. This is a Docker open source scheduler. We
    will build five servers and look at how to create a replicated master. We will
    then run a few containers and look at how Swarm will schedule them across nodes.
    The next scheduler we will look at is Docker **UCP** (**Universal Control Plane**).
    This is a Docker enterprise solution that is integrated with Docker Compose. We
    will build a three-node cluster and deploy our Consul module. As UCP has a graphical
    interface, we will look at how UCP is scheduled from there. The final scheduler
    we will look at is Kubernetes. This is Google's offering and is also open source.
    For Kubernetes, we will build a single node using containers and use Puppet to
    define the more complex types. As you can see, we are going to look at each one
    differently, as they all have their individual strengths and weaknesses. Depending
    on your use case, you might decide on one or all of them to solve a problem that
    you may face.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our first scheduler, we are going to look at Docker Swarm. This is a really
    solid product and in my opinion is a bit underrated compared to Kubernetes. It
    has really come on in leaps and bounds in the last few releases. It now supports
    replicated masters, rescheduling containers on failed hosts. So, let's look at
    the architecture of what we are building. Then, we will get into the coding.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Swarm architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we are going to build five servers, where two will be replicated
    masters and the other three nodes will be in the swarm cluster. As Docker Swarm
    needs a key/value store backend, we will use Consul. In this instance, we are
    not going to use our Consul modules; instead, we are going to use [https://forge.puppetlabs.com/KyleAnderson/consul](https://forge.puppetlabs.com/KyleAnderson/consul).
    The reason for this is that in all three examples, we are going to use different
    design choices. So, when you are trying to build a solution, you are exposed to
    more than one way to skin the cat. In this example, we are going to install Swarm
    and Consul onto the OS using Puppet and then run containers on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we are going create a new Vagrant repo. So, we will Git clone
    [https://github.com/scotty-c/vagrant-template.git](https://github.com/scotty-c/vagrant-template.git)
    into the directory of our choice. The first thing that we will edit is the Puppetfile.
    This can be found in the root of our Vagrant repo. We will add the following changes
    to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next file we will edit is `servers.yaml`. Again, this is located in the
    root of our Vagrant repo. We are going to add five servers to it. So, I will break
    down this file into five parts, one for each server.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the code for server 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the code for server 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the code for server 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the code for server 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the code for server 5 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All this should look fairly familiar to you. The one call out is that we have
    used servers one through three as our cluster nodes. Four and five will be our
    masters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we are going to do is add some values to Hiera. This is the
    first time we have used Hiera, so let''s look at our `hiera.yaml` file to see
    our configuration in the root of our Vagrant repo, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have a basic hierarchy. We just have one file to look at,
    which is our `global.yaml` file. We can see that the file lives in our `hieradata`
    folder as it is declared as our `data` directory. So, if we open the `global.yaml`
    file, we are going to add the following values to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first value will tell Swarm what version we want to use. The last value
    sets the version of Consul that we will be using, which is `0.6.3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we need to do is write our module that will deploy both our
    Consul and Swarm clusters. We have created quite a few modules so far in the book,
    so we won''t cover it again. In this instance, we will create a module called
    `<AUTHOR>-config`. We will then move the module into our `modules` folder at the
    root of our Vagrant repo. Now, let''s look at what we are going to add to our
    `init.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, this sets up our module. We will need to create a `.pp` file
    for each of our entries in our `init.pp` file, for example, `consul_config.pp`,
    `swarm.pp`, and so on. We have also declared one variable called `consul_ip`,
    whose value will actually come from Hiera, as that is where we set the value earlier.
    We will look at our files in alphabetical order. So, we will start with `config::compose.pp`,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this class, we are setting up a registrator. We are going to use Docker
    Compose for this, as it is a familiar configuration and something that we have
    already covered. You will note that there is an `if` statement in the code. This
    is a logic to run the container only on a cluster member. We don''t want to run
    container applications on our masters. The next file we will look at is `consul_config.pp`,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this class, we will configure Consul, something that we have already covered
    in this book. I always like to look at multiple ways to do the same job, as you
    never know the solution that you might need to produce tomorrow and you always
    want the right tool for the right job. So, in this instance, we will not configure
    Consul in a container but natively on the host OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the code is broken into three blocks. The first block bootstraps
    our Consul cluster. You will note that the configurations are familiar, as they
    are the same as those we used to set up our container in an earlier chapter. The
    second block of code sets the members of the cluster after the cluster is bootstrapped.
    We have not seen this third block before. This sets up a service for Consul to
    monitor. This is just a taste of what we can manage, but we will look at this
    in anger in the next chapter. Let''s look at our next file, that is, `dns.pp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will note that this file is exactly the same as we used in our `consul`
    module. So, we can move on to the next file, which is `run_containers.pp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the class that will run our containers on our cluster. At the top,
    we are declaring that we want master number two to initiate the call to the cluster.
    We are going to deploy three container applications. The first is `jenkins`, and
    as you can see, we will expose Jenkins web port `8080` to the host that the container
    runs on. The next container is `nginx`, and we are going to forward both `80`
    and `443` to the host, but also connect `nginx` to our private network across
    our `swarm-private` cluster. To get our logs from `nginx`, we will tell our container
    to use syslog as the logging driver. The last application that we will run is
    `redis`. This will be the backend of `nginx`. You will note that we are not forwarding
    any ports, as we are hiding Redis on our internal network. Now that we have our
    containers sorted, we have one file left, `swarm.pp`. This will configure our
    Swarm cluster and internal Swarm network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first resource declaration will install the Swarm binaries. The next resource
    will configure our Docker network on each host. The next `if` state will define
    if the Swarm node is a master or part of the cluster via `hostname`. As you can
    see, we are declaring a few defaults that are the first values that tell Swarm
    what backend to use Consul with. The second value tells Swarm the IP address of
    the Consul backend which is `172.17.8.101`. The third value tells Swarm about
    the port it can access Swarm on, `8500`. The fourth value tells Swarm which interface
    to advertise the cluster on, which in our case is `enp0s8`. The last value sets
    the root of the key value store Swarm will use. Now, let's create our `templates`
    folder in the root of our module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create three files there. The first file will be `consul.conf.erb`
    and the contents will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next file will be `named.conf.erb` and its contents will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last file will be `registrator.yml.erb`, and the file''s content will be
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next thing that we need to add is our `config` class to our `default.pp`
    manifest file in the `manifest` folder in the root of our Vagrant repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, our module is complete and we are ready to run our cluster. So, let''s
    open our terminal and change the directory to the root of our Vagrant repo and
    issue the `vagrant up` command. Now, we are building five servers, so be patient.
    Once our last server is built, our terminal output should look like that shown
    in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Terminal output
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now look at our Consul cluster''s web UI at `127.0.0.1:9501` (remember
    that we changed our ports on our `servers.yaml` file) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s see what host our Jenkins service is running on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, my service has come up on cluster node 101\. You could get
    a different host for your cluster. So, we need to check what port `8080` forwards
    to our `servers.yaml` file guest. In my example, it''s `8081`. So, if I go to
    my web browser and open a new tab and browse to `127.0.0.1:8081`, we will get
    the Jenkins page, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can do the same with nginx, and I will leave that up to you as a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Docker UCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this topic, we will be looking at Docker's new offering, UCP. This product
    is not open sourced, so there is a licensing cost. You can get a trial license
    for 30 days ([https://www.docker.com/products/docker-universal-control-plane](https://www.docker.com/products/docker-universal-control-plane)),
    and that is what we will be using. Docker UCP takes out the complexity of managing
    all the moving parts of a scheduler. This could be a massive plus pending your
    use case. Docker UCP also brings with it a web UI for administration. So, if container
    schedulers seem daunting, this could be a perfect solution for you.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker UCP architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, in this example, we are going to build three nodes. The first will be our
    UCP controller and the other two nodes will be UCP HA replicas that give the design
    some fault tolerance. As UCP is a wrapped product, I am not going to go into all
    the moving parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram to get a visualization of the main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Docker UCP architecture](img/B05201_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to do something different in this module, just to show you that
    our modules are portable to any OS. We will build this module with the Ubuntu
    14.04 server. Again, for this environment, we are going to create a new Vagrant
    repo. So, let''s Git clone our Vagrant repo ([https://github.com/scotty-c/vagrant-template.git](https://github.com/scotty-c/vagrant-template.git)).
    As in the last topic, we will look at the plumbing first, before we write our
    module. The first thing we are going to do is create a file called `config.json`.
    This will have your Docker Hub auth in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next will be `docker_subscription.lic`. This will contain your trial license.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at our `servers.yaml` file in the root of our Vagrant repo,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The main call out here is that now we are using the `puppetlabs/ubuntu-14.04-64-puppet-enterprise`
    vagrant box. We have changed `yum` to `apt-get`. Then, we are copying both our
    `config.json` and `docker_subscription.lic` files to their correct place on our
    vagrant box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the changes we need to make to our Puppetfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will see that we need a few new modules from the Forge. The Docker module
    is familiar, as is stdlib. We will need Puppetlab's `apt` module to control our
    repos that Ubuntu will use to pull Docker. The last module is the Puppetlabs module
    for UCP itself. To find out more information about this module, you can read all
    about it at [https://forge.puppetlabs.com/puppetlabs/docker_ucp](https://forge.puppetlabs.com/puppetlabs/docker_ucp).
    We will write a module that wraps this class and configures it for our environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at our Hiera file in `hieradata/global.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have added two values. The first is `ucpconfig::ucp_url:`,
    which we will set to our first vagrant box. The next is the value for `ucpconfig::ucp_fingerprint:`,
    which we will leave blank for the moment. But remember it, as we will come back
    to this later in the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will create a module called `<AUTHOR>-ucpconfig`. We have done this
    a few times now, so once you have created a module, create a folder in the root
    of our Vagrant repo called `modules` and move `ucpconfig` into that folder.
  prefs: []
  type: TYPE_NORMAL
- en: We will then create three manifests files in the module's `manifest` directory.
    The first will be `master.pp`, the second will be `node.pp`, and the last file
    will be `params.pp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add our code to the `params.pp` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have four values: `Ucp_url`, which comes from Hiera; `ucp_username`,
    whose default value is set to `admin`; we then have `ucp_password` whose default
    value is set to `orca`. The last value is `ucp_fingerprint`, which again comes
    from Hiera. Now, in a production environment, I would set both the username and
    password in Hiera and overwrite the defaults, which we have set in `params.pp`.
    In this case, as it is a test lab, we will just use the defaults.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next file we will look at is our `init.pp` file, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that at the top of the class, we are mapping our `params.pp` file.
    The next declaration installs the `docker` class and sets the `socket_bind` parameter
    for the daemon. Now, the next bit of logic defines whether the node is a master
    or a node depending on the hostname. As you can see, we are only setting `ucp-01`
    as our master.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at `master.pp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this class, we have the logic to install the UCP controller or master. At
    the top of the class, we are mapping our parameters to our `init.pp` file. The
    next block of code calls the `docker_ucp` class. As you can see, we are setting
    the value of controller to `true`, the host address to our second interface, the
    alternate name of the cluster to our first interface, and the version to `1.0.1`
    (which is the latest at the time of writing this book). We will then set the ports
    for both the controller and Swarm. Then, we will tell UCP about the Docker socket
    location and also the location of the license file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at our last file, `node.pp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, most of the settings might look familiar. The call out for
    a node is that we need to point it to the controller URL (which we set in Hiera).
    We will get to know about the admin username and password and the cluster fingerprint
    just a little bit later. So, that completes our module. We now need to add our
    class to our nodes which we will do by adding the `default.pp` manifest file in
    the `manifests/default.pp` location from the root of our Vagrant repo, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's go to our terminal and change the directory to the root of our Vagrant
    repo. This time, we are going to do something different. We are going to issue
    the `vagrant up ucp-01` command. This will bring up only the first node. We do
    this as we need to get the fingerprint that is generated as UCP comes up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our terminal output should look like that shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Terminal output
  prefs: []
  type: TYPE_NORMAL
- en: 'You will note that the fingerprint has been displayed on your terminal output.
    For my example, the fingerprint is `INFO[0031] UCP Server SSL: SHA1 Fingerprint=C2:7C:BB:C8:CF:26:59:0F:DB:BB:11:BC:02:18:C4:A4:18:C4:05:4E`.
    So, we will add this to our Hiera file, which is `global.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have our first node up, we should be able to log in to the web
    UI. We do this in our browser. We will go to `https:127.0.0.1:8443` and get the
    login page as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then add the username and password that we set in our `params.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, after we log in, you can see that we have a health cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Health cluster after logging in
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's return to our terminal and issue the `vagrant up ucp-02 && vagrant
    up ucp-03` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that is complete, if we look at our web UI, we can see that we have three
    nodes in our cluster, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this book, we are not going to go into how to administer the cluster through
    the web UI. I would definitely recommend that you explore this product, as it
    has some really cool features. All the documentation is available at [https://docs.docker.com/ucp/overview/](https://docs.docker.com/ucp/overview/).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is massive buzz around Kubernetes at the moment. This is Google's offering
    to the container world. Kubernetes has some very heavy backers such as Google,
    CoreOS, and Netflix. Out of all the schedulers that we have looked at, Kubernetes
    is the most complex and is highly driven by APIs. If you are new to Kubernetes,
    I would suggest that you read further about the product ([http://kubernetes.io/](http://kubernetes.io/)).
    We are going to first look at the architecture of Kubernetes, as there are a few
    moving parts. Then, we will write our module, and we are going to build Kubernetes
    completely with containers.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be building Kubernetes on a single node. The reason for this is that
    it will cut out some of the complexity on how to use flannel over the Docker bridge.
    This module will give you a good understanding of how Kubernetes works using more
    advanced Puppet techniques. If you want to take your knowledge further after you
    have mastered this chapter, I would recommend that you check out the module on
    the forge at [https://forge.puppetlabs.com/garethr/kubernetes](https://forge.puppetlabs.com/garethr/kubernetes).
    This module really takes Puppet and Kubernetes to the next level.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what we are going to code looks like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture](img/B05201_07_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have a container that runs **etcd** (to read more about etcd,
    go to [https://coreos.com/etcd/docs/latest/](https://coreos.com/etcd/docs/latest/)).
    etcd is similar to Consul, which we are familiar with. In the next few containers,
    we are going to use hyperkube ([https://godoc.org/k8s.io/kubernetes/pkg/hyperkube](https://godoc.org/k8s.io/kubernetes/pkg/hyperkube)).
    This will load balance the required Kubernetes components across multiple containers
    for us. It seems pretty easy, right? Let's get into the code so that we get a
    better perspective on all the moving parts.
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to create a new Vagrant repo again. We won't cover how to do that
    as we have covered it twice in this chapter. If you are unsure, just take a look
    at the initial part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created our Vagrant repo, let''s open our `servers.yaml` file,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is nothing special there that we have not covered in
    this book. There''s just a single node that we mentioned earlier, `kubernetes`.
    The next file we will look at is our Puppetfile. We will, of course, need our
    Docker module, `stdlib`, and lastly, `wget`. We need `wget` to get `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is all the plumbing that we need to set up for our repo. Let's create a
    new module called `<AUTHOR>-kubernetes_docker`. Once it is created, we will move
    our module to the `modules` directory in the root of our Vagrant repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to create two new folders in our module. The first will be the
    `templates` folder, and the other folder will be the `lib` directory. We will
    get to the `lib` directory toward the end of our coding. The first file we will
    create and edit is `docker-compose.yml.erb`. The reason for this is that it is
    the foundation of our module. We will add the following code to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's break this file down into three chunks, as there is a lot going on there.
    The first block of code is going to set up our etcd cluster. You can see from
    the screenshot name that we are using Google's official images, and we are using
    etcd version 2.2.1\. We are giving the container access to the host network. Then,
    in the command resource, we pass some arguments to etcd as it starts.
  prefs: []
  type: TYPE_NORMAL
- en: The next container we create is hyperkube. Again, it is an official Google image.
    Now, we are giving this container access to a lot of host volumes, the host network,
    and the host process, making the container privileged. This is because the first
    container will bootstrap Kubernetes and it will spawn more containers running
    the various Kubernertes components. Now, in the command resource, we are again
    passing some arguments for hyperkube. The two major ones we need to worry about
    are the API server address and config manifests. You will note that we have a
    mapped folder from `/kubeconfig:/etc/kubernetes/manifests:ro`. We are going to
    modify our manifest file to make our Kubernetes environment available to the outside
    world. We will get to that next. But, we will finish looking at the code in this
    file first.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last container and the third block of code is going to set up our service
    proxy. We are going to give this container access to the host network and process.
    In the command resource, we are going to specify that this container is a proxy.
    The next thing to take notice of is that we specify where the proxy can find the
    API. Now, let''s create the next file, `master.json.erb`. This is the file that
    hyperkube will use to schedule all the Kubernetes components, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have defined three more containers. This is the first time
    we will define a Kubernetes pod ([http://kubernetes.io/docs/user-guide/pods/](http://kubernetes.io/docs/user-guide/pods/)).
    A pod is a group of containers that creates an application. It is similar to what
    we have done with Docker Compose. As you can see, we have changed all the IP addresses
    to the `<%= @master_ip %>` parameter. We will create four new files: `apps.pp`,
    `config.pp`, `install.pp`, and `params.pp`.'
  prefs: []
  type: TYPE_NORMAL
- en: We will now move on to our files in the `modules` manifest directory. Now, strap
    yourselves in, as this is where the magic happens. Well, that's not true. The
    magic happens here and in our `lib` directory. We will need to write some custom
    types and providers for Puppet to be able to control Kubernertes as `kubectl`
    is the user interface (for types, visit [https://docs.puppetlabs.com/guides/custom_types.html](https://docs.puppetlabs.com/guides/custom_types.html),
    and for providers, visit [https://docs.puppetlabs.com/guides/provider_development.html](https://docs.puppetlabs.com/guides/provider_development.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with our `init,pp` file, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is not much in this file. We are going to use our `init.pp`
    file to control the order in which classes are executed. We are also declaring
    `param <%= @master_ip %>`. We will now move on to our `install.pp` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this file, we install Docker as we did before. We will place our two templates
    that we created earlier. Then, we will run Docker Compose to bring up our cluster.
    Now, we will move on to `config.pp`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first thing that we declare is that we want `wget` to be our `kubectl`
    client, and we place it at `/usr/bin/` ([http://kubernetes.io/docs/user-guide/kubectl/kubectl/](http://kubernetes.io/docs/user-guide/kubectl/kubectl/)).
    You really need to understand what this interface does; otherwise, you might get
    a bit lost from here. So, I suggest that you have a fairly good idea of what kubectl
    is and what it is capable of. Next, we will make it executable and available for
    all our users. Now, this last piece of code does not make sense, as we have not
    called the `kubectl_config` class yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We now need to jump to our `lib` directory. This first thing we will do is
    create all our folders that we need. The first folder that we will create is `puppet`
    under the `lib` directory. We will look at our custom types first. We will create
    a folder called `type` under `puppet`. The following screenshot will give you
    a visualization of the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Under the `type` folder, we will create a file called `kubectl_config.rb`.
    In that file, we will add the new parameters of `type` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let me explain what is happening here. In the first line, we are going to declare
    our new type, `kubectl_config`. We are then going to set the default value of
    the new type when it is declared as `present`. We are going to declare three values
    to our type, `name`, `cluster`, and `kube_context`. These are all settings that
    we will add to our `config` file that `kubectl` will use when we interface with
    it. Now, we will create a folder under the `lib` directory called `provider`.
    Then, under that, we will create a folder with the same name as our custom type,
    `kubectl_config`. Inside that folder, we will create a file called `ruby.rb`.
    In this file, we will put the Ruby code that provides logic to our type as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A provider needs to have three methods for Puppet to be able to run the code.
    They are `exsists?`, `create`, and `destroy`. These are pretty easy to understand.
    The `exists?` method checks whether the type has already been executed by Puppet,
    `create` runs the type, and `destroy` is invoked when the type is set to `absent`.
  prefs: []
  type: TYPE_NORMAL
- en: We are now going to work through the file, from top to bottom. We need to first
    load a few Ruby libraries for some of our methods. We will then tie this provider
    to our type. The next thing that we need to declare is the `kubectl` executable.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will write our first method, `interface`. This will get the IP address
    from the hostname of the box. We will then create three more methods. We will
    also create an array and add all our configuration to them. You will note that
    we are mapping our parameters from our type in the arrays.
  prefs: []
  type: TYPE_NORMAL
- en: In our `exists?` method, we will check for our `kubectl` config file.
  prefs: []
  type: TYPE_NORMAL
- en: In our `create` method, we are calling our `kubectl` executable and then passing
    our arrays as arguments. We will then link our `config` file to roots' home directory
    (this is fine for our lab. In a production environment, I would use a named user
    account).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will remove the `config` file if the type is set to `absent`. We
    will now go back to our `manifests` directory and look at our last file, which
    is `apps.pp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this file, we are going to run a container application on our Kubernetes
    cluster. Again, we will write another custom type and provider. Before we get
    to that, we should look at the code in this class. As you can see, our type is
    called `kubernetes_run`. We can see that our service is named `nginx`, the Docker
    image we will pull is `nginx`, and we will then expose port `80`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to our `lib` directory. We will then create a file in our `type`
    folder called `kubernetes_run.rb`. In this file, we will set up our custom type
    as we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we are mapping the same parameters that we had in our `apps.pp`
    file. We will then create a folder under the `provider` folder with the same name
    as our `kubernetes_run` type. Again, in the newly created directory, we will create
    a file called `ruby.rb`. It will have the code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this file, we are going to add two commands this time: the first is `kubectl`
    and the second is `docker`. We will create two methods, again with arrays that
    map the values from our type.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at our `exists?` method. We are going to pass an array as an
    argument to `kubectl` to check whether the service exists. We are then going to
    catch the error if `kubectl` throws an error with the request and returns `false`.
    This is used if there are no services deployed on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In our `create` method, we will first pass an array to `kubectl` to get the
    nodes in the cluster. We are using this as an arbitrary command to make sure that
    the cluster is up. Under that, we will capture the error and retry the command
    until it is successful. Once it is successful, we will deploy our container with
    the `ensure` resource.
  prefs: []
  type: TYPE_NORMAL
- en: In the `destroy` method, we will use `docker` to remove our container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have all our coding done. We just need to add our class to our node
    by editing our `default.pp` file in the `manifests` folder in the root of our
    Vagrant repo as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s open our terminal and change the directory to the root of our Vagrant
    repo and issue the `vagrant up` command. Once Puppet has completed its run, our
    terminal should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now log in to our vagrant box by issuing the `vagrant ssh` command
    and then `sudo -i` and change to root. Once we are root, we will look at our service
    on our cluster. We will do this by issuing the `kubectl get svc` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have two services running our cluster: `Kubernetes` and
    `nginx`. If we go to our web browser and go to the address we gave to the second
    network interface, `http://172.17.9.101`, we will get the following nginx default
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Coding](img/B05201_07_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, our cluster is running successfully with our `nginx` service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered my favorite three container schedulers; each one of them has their
    own pros and cons. Now, you have the knowledge and the required code to give all
    three a really good test run. I would suggest that you do so so that you can make
    the right choice when choosing the design in your environment.
  prefs: []
  type: TYPE_NORMAL
