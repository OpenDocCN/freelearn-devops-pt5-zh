- en: Chapter 6. Load Balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how we tune our Docker applications, we will reach our application's
    performance limits. Using the benchmarking techniques we discussed in the previous
    chapter, we should be able to identify the capacity of our application. In the
    near future, our Docker application's users will exceed this limit. We cannot
    turn these users away just because our Docker application cannot handle their
    requests anymore. We need to scale out our application so that it can serve our
    growing number of users.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will talk about how to scale out our Docker applications
    to increase our capacity. We will use load balancers, which are a key component
    in the architecture of various web scale applications. Load balancers distribute
    our application''s users to multiple Docker applications deployed in our farm
    of Docker hosts. The following steps covered in this chapter will help us accomplish
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a Docker host farm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing load with Nginx
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling out our Docker applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing zero downtime releases with load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a Docker host farm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key component in load balancing our Docker application is to have a farm of
    servers to send our application's requests to. In the case of our infrastructure,
    this involves preparing a farm of Docker hosts to deploy our application to. The
    scalable way to do this is to have a common base configuration that is managed
    by configuration management software, such as Chef, as we previously covered in
    [Chapter 3,](part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e "Chapter 3. Automating
    Docker Deployments with Chef") *Automating Docker Deployments with Chef*.
  prefs: []
  type: TYPE_NORMAL
- en: After preparing the farm of Docker hosts, it is time to prepare the application
    that we will run. In this chapter, we will scale a simple NodeJS application.
    The rest of this section will describe how this application works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The web application is a small NodeJS application written in a file called
    `app.js`. For the purpose of visualizing how our application load balances, we
    will also log some information about our application and the Docker host it is
    running in. The `app.js` file will contain the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To deploy the preceding application code, we will package it in a Docker image
    called `hubuser/app:1.0.0` with the following `Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that our Docker image is built and available at Docker Hub. This
    way, we can easily deploy it. Run this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As the final step in our preparation, we will deploy our Docker application
    to three Docker hosts: `greenhost00`, `greenhost01`, and `greenhost02`. Log in
    to each of the hosts and type the following command to start the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Better yet, we can write a Chef cookbook that will deploy the Docker application
    that we just wrote.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing load with Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a pool of Docker applications to forward traffic to, we can
    prepare our load balancer. In this section, we will briefly cover Nginx, a popular
    web server that has high concurrency and performance. It is commonly used as a
    reverse proxy to forward requests to more dynamic web applications, such as the
    NodeJS one we wrote earlier. By configuring Nginx to have multiple reverse proxy
    destinations, such as our pool of Docker applications, it will balance the load
    of requests coming to it across the pool.
  prefs: []
  type: TYPE_NORMAL
- en: In our load balancer deployment, we will deploy our Nginx Docker container in
    a Docker host called `dockerhost`. After deployment, the Nginx container will
    start forwarding to the pool of Docker hosts called `greenhost*`, which we provisioned
    in the earlier section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a simple configuration of Nginx that will forward traffic
    to the pool of Docker applications that we deployed earlier. Save this file in
    `/root/nginx.conf` inside the `dockerhost` Docker host, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding Nginx configuration file is basically composed of directives.
    Each directive has a corresponding effect on Nginx's configuration. To define
    our pool of applications, we will use the `upstream` directive to define a group
    of servers. Next, we will place the list of servers in our pool using the `server`
    directive. A server in the pool is normally defined in the `<hostname-or-ip>:<port>`
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the references referring to the described directives mentioned
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '`upstream`—[http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`server`—[http://nginx.org/en/docs/http/ngx_http_upstream_module.html#server](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#server)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxy_pass`—[http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introductory material discussing the basics of directives can be found at [http://nginx.org/en/docs/beginners_guide.html#conf_structure](http://nginx.org/en/docs/beginners_guide.html#conf_structure).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have prepared our `nginx.conf` file, we can deploy our Nginx container
    together with this configuration. To perform this deployment, let''s run the following
    command in our `dockerhost` Docker host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our web application is now accessible via `http://dockerhost`. Each request
    will then be routed to one of the `hubuser/webapp:1.0.0` containers we deployed
    to our pool of Docker hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm our deployment, we can look at our Kibana visualization to show
    the distribution of traffic across our three hosts. To show the distribution of
    traffic, we must first generate load for our application. We can use our JMeter
    testing infrastructure described in [Chapter 5](part0035_split_000.html#11C3M2-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 5. Benchmarking"), *Benchmarking*, to achieve this. For quick testing,
    we can also generate load using a long-running command similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Recall that in the application we prepared earlier, we printed out `$HOSTNAME`
    as a part of the HTTP response. In the preceding case, the responses show the
    Docker container's hostname. Note that Docker containers get the short hash of
    their container IDs as their hostname by default. As we can note from the initial
    output of our test workload, we are getting responses from three containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the response better in a Kibana visualization if we set up
    our logging infrastructure as we did in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*. In the following screenshot, we can count the number of responses
    per minute according to the Docker host that the log entry came from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Balancing load with Nginx](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can note in the preceding figure that our workload gets distributed evenly
    by Nginx to our three Docker hosts: **greenhost00**, **greenhost01**, and **greenhost02**.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To properly visualize our deployment in Kibana we have to annotate our Docker
    containers and filter these log entries in Logstash so that they get properly
    annotated to Elasticsearch. We can do this via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will make sure that we use the `syslog-tag` option when deploying
    our Docker container. This makes our application easier to filter out later in
    Logstash. Run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, Logstash will receive our Docker container''s log entries with the
    `docker/webapp` tag. We can then use a Logstash `filter` as follows to get this
    information in Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Scaling out our Docker applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, suppose that the workload in the previous section starts to overload each
    of our three Docker hosts. Without a load balancer such as our preceding Nginx
    setup, our application's performance will start to degrade. This may mean a lower
    quality of service to our application's users or being paged in the middle of
    the night to perform heroic systems operations. However, with a load balancer
    managing the connections to our applications, it is very simple to add more capacity
    to scale out the performance of our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'As our application is already designed to be load balanced, our scale-out process
    is very simple. The next few steps form a typical workflow on how to add capacity
    to a load-balanced application:'
  prefs: []
  type: TYPE_NORMAL
- en: First, provision new Docker hosts with the same base configuration as the first
    three in our Docker host pool. In this section, we will create two new Docker
    hosts, named `greenhost03` and `greenhost04`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step in our scale-out process is to then deploy our applications in
    these new Docker hosts. Type the same command before for deployment as the following
    one to each of the new Docker hosts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, new application servers in our pool are ready to accept connections.
    It is now time to add them as destinations to our Nginx-based load balancer. To
    add them to our pool of upstream servers, first update the `/root/nginx.conf`
    file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will notify our running Nginx Docker container to reload its configuration.
    In Nginx, this is done by sending a `HUP` Unix signal to its master process. To
    send the signal to a master process inside the Docker container, type the following
    Docker command. Send the reload signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: More information on how to control Nginx with various Unix signals is documented
    at [http://nginx.org/en/docs/control.html](http://nginx.org/en/docs/control.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we are done scaling out our Docker application, let''s look back at
    our Kibana visualization to observe the effect. The following screenshot shows
    the distribution of traffic across the five Docker hosts we currently have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling out our Docker applications](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can note in the preceding screenshot that after we reloaded Nginx, it started
    to distribute load across our new Docker containers. Before this, each Docker
    container received only a third of the traffic from Nginx. Now, each Docker application
    in the pool only receives a fifth of the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with zero downtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another advantage of having our Docker application load balanced is that we
    can use the same load balancing techniques to update our application. Normally,
    operations engineers have to schedule downtime or a maintenance window in order
    to update an application deployed in production. However, as our application's
    traffic goes to a load balancer before it reaches our application, we can use
    this intermediate step to our advantage. In this section, we will employ a technique
    called blue-green deployments to update our running application with zero downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our current pool of `hubuser/app:1.0.0` Docker containers is called our *green*
    Docker host pool because it actively receives requests from our Nginx load balancer.
    We will update the application being served by our Nginx load balancer to pool
    of `hubuser/app:2.0.0` Docker containers. The following are the steps to perform
    the update:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s update our application by changing the version string in our
    `app.js` file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After updating the content, we will prepare a new version of our Docker image
    called `hubuser/app:2.0.0` and publish it to Docker Hub via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will provision a set of Docker hosts called `bluehost01`, `bluehost02`,
    and `bluehost03`, either through our cloud provider or by buying actual hardware.
    This will become our *blue* Docker host pool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that our Docker hosts are prepared, we will deploy our new Docker application
    on each of the new hosts. Type the following commands on each Docker host to perform
    the deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our *blue* Docker host pool is now prepared. It is called blue because although
    it is now live and running, it has yet to receive user traffic. At this point,
    we can do whatever is needed, such as performing preflight checks and tests before
    siphoning our users to the new version of our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we are confident that our blue Docker host pool is fully functional and
    working, it will be time to send traffic to it. As in the scaling-out process
    of our Docker host pool, we will simply add our blue Docker hosts to the list
    of servers inside our `/root/nginx.conf` configuration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To complete the activation, reload our Nginx load balancer by sending it the
    `HUP` signal through the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: At this point, Nginx sends traffic to both the old version (`hubuser/app:1.0.0`)
    and the new version (`hubuser/app:2.0.0`) of our Docker application. With this,
    we can completely verify that our new application is indeed working as expected
    because it now serves live traffic from our application's users. In the cases
    when it does not work properly, we can safely roll back by removing the `bluehost*`
    Docker hosts in the pool and resending the `HUP` signal to our Nginx container.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, suppose we are already satisfied with our new application. We can
    then safely remove the old Docker application from our load balancer''s configuration.
    In our `/root/nginx.conf` file, we can perform this by removing all the `greenhost*`
    lines, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can complete our zero-downtime deployment with another `HUP` signal
    to Nginx. At this point, our blue Docker host pool serves all the production traffic
    of our application. This, therefore, becomes our new green Docker host pool. Optionally,
    we can deprovision our old green Docker host pool to save on resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole blue-green deployment process we did earlier can be summarized in
    the following Kibana visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying with zero downtime](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding graph, our application still serves traffic even
    though we updated our application. Note also that before this, all of the traffic
    was distributed to our five **1.0.0** applications. After activating the blue
    Docker host pool, three-eighths of the traffic started going to version **2.0.0**
    of our application. In the end, we deactivated all the endpoints in our old green
    Docker host pool, and all of the application's traffic is now served by version
    **2.0.0** of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about blue-green deployments and other types of zero-downtime
    release techniques can be found in a book called *Continuous Delivery* by Jez
    Humble and Dave Farley. The book's website can be found at [http://continuousdelivery.com](http://continuousdelivery.com).
  prefs: []
  type: TYPE_NORMAL
- en: Other load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other tools that can be used to load balance applications. Some are
    similar to Nginx, where configuration is defined through external configuration
    files. Then, we can send a signal to the running process to reload the updated
    configuration. Some have their pool configurations stored in an outside store,
    such as Redis, etcd, and even regular databases, so that the list is dynamically
    loaded by the load balancer itself. Even Nginx has some of these functionalities
    with its commercial offering. There are also other open source projects that extend
    Nginx with third-party modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a short list of load balancers that we can deploy as some
    form of Docker containers in our infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: Redx ([https://github.com/rstudio/redx](https://github.com/rstudio/redx))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy ([http://www.haproxy.org](http://www.haproxy.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache HTTP Server ([http://httpd.apache.org](http://httpd.apache.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vulcand ([http://vulcand.github.io/](http://vulcand.github.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CloudFoundry's GoRouter ([https://github.com/cloudfoundry/gorouter](https://github.com/cloudfoundry/gorouter))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dotCloud's Hipache ([https://github.com/hipache/hipache](https://github.com/hipache/hipache))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also hardware-based load balancers that we can procure ourselves and
    configure via their own proprietary formats or APIs. If we use cloud providers,
    some of their own load balancer offerings would have their own cloud APIs that
    we can use as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the benefits of using load balancers and how to
    use them. We deployed and configured Nginx as a load balancer in a Docker container
    so that we can scale out our Docker application. We also used the load balancer
    to perform zero-downtime releases to update our application to a new version.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue to improve our Docker optimization skills
    by debugging inside the Docker containers we deploy.
  prefs: []
  type: TYPE_NORMAL
