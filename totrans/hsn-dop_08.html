<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">DevOps Continuous Deployment</h1>
                </header>
            
            <article>
                
<p>DevOps continuous deployment enables changes to be ported quickly from development to production. Infrastructure and automation play key roles for enabling continuous deployment. In this chapter, we will learn about configuration automation and the implementation of infrastructure automation (Infrastructure as Code) with tools such as Chef and Ansible. We will also discuss the continuous monitoring process with the tools, Splunk and Nagios:</p>
<ul>
<li>Continuous deployment</li>
<li>Chef
<ul>
<li>Components</li>
<li>Terminology</li>
<li>Architecture</li>
</ul>
</li>
<li>Ansible
<ul>
<li>Components</li>
<li>Terminology</li>
<li>Architecture</li>
</ul>
</li>
<li>Continuous Monitoring</li>
<li>Splunk</li>
<li>Nagios</li>
</ul>
<p>As we have discussed in the previous chapters, the following figure shows the process of continuous integration, continuous deployment, and continuous delivery alignment.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/4cec5b6a-f83c-48ae-ba6e-7575c0cd1e58.png"/></div>
<p><span><strong>Continuous Integration</strong> (<strong>CI</strong>) is the process of making the development, unit test and build process on a continuous mode as opposed to staggered (step-by-step) methodology. In the CI process, every developer merges their code changes to a central version control system, each commit triggers automated build. So the latest versions are always available in the code repository and also built executable is from latest code.</span></p>
<p><strong>Continuous Delivery</strong> (<strong>CD</strong>) is a next step to the continuous integration process for software engineering to produce software in short cycles of testing, and releasing software faster and more frequently. The automated testing process ensures that the software can be reliably released at any time.</p>
<p>Continuous deployment is the process to minimize lead time (the elapsed time) between the development of new code and its availability in production for usage. To accomplish this, continuous deployment relies on infrastructure that automates various steps, after each successful code integration meeting the release criteria, leading to deployment, the live application is updated with new code.</p>
<p>Traditionally, a new machine is built by administrators and system engineers from documentation and customized scripts, and so on. Managing Infrastructure through manual procedures such as custom scripts, golden image configurations, and so on, are both time consuming and error-prone. Organizations looking for faster and matured deployments and concepts adopt infrastructure configuration automation, which means managing Infrastructure like a software code for reproducible results, hence it's also termed as <strong>Infrastructure as Code</strong>.</p>
<p>Just like the SDLC process, infrastructure can be managed with similar tools and processes such as version control, continuous integration, code review, and automated testing extended to make configuration changes for infrastructure robust and automated. The infrastructure code and configuration changes are consistently tested, shared, and promoted across all the environments from development to QA test systems and to production more easily, rapidly, safely, and reliably with the detailed audit trail of changes. With infrastructure code as a service, the configuration of the new machines to the desired state can be written as a code to set up multiple machines at the same time. This scalable model is more effective by leveraging the elasticity of the cloud. Adopting DevOps to Infrastructure as Code str, and so on goes beyond simple infrastructure automation to extend multiple benefits as below:</p>
<ul>
<li>Ensure error-free automation scripts are repeatable</li>
<li>To be redeployed on multiple servers</li>
<li>Ability to roll back in case of issues</li>
<li>Infrastructure code testing standards such as unit testing, functional testing, and integration testing can be effectively enforced</li>
<li>Since the documented state of the machine is maintained as code and made up-to-date, written documentation is avoided</li>
<li>Enable collaboration between dev and ops around infrastructure configuration and provisioning, infrastructure code as change management</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="275" src="assets/dbef7987-5f46-4a03-a8de-7aad250be31a.png" width="490"/></div>
<p>We will discuss continuous deployment from the perspective of popular tool features and functionality listed in the preceding figure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chef</h1>
                </header>
            
            <article>
                
<p>Chef is one of the prominent configuration management and infrastructure automation platforms; it provides a full suite of enterprise capabilities such as workflow, visibility, and compliance. It enables continuous deployments for both infrastructure and applications from development to production. Infrastructure configuration automation as code is written, tested, deployed, and managed by Chef across networks such as the cloud, on-premises, or hybrid environments with comprehensive 24 x 7 support services. Examples are client systems, security patches can be updated from master server by writing configurations as a set of instructions and executed on multiple nodes simultaneously.</p>
<p>The Chef platform as shown in the following figure, supports multiple environments such as Amazon Web Services, Azure, VMware, OpenStack, Google Cloud, and so on. Platforms such as Windows, Linux, VMware, and so on, are available. All the popular continuous integration tools such as Bitbucket, Jenkins, GitHub, CircleCI, and so on, are supported for workflow integration. The runtime environment is available on Kubernetes, Docker, and Swarm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chef landscape components</h1>
                </header>
            
            <article>
                
<p>The Chef landscape comprising various elements of Chef, including the nodes, the server, and the workstation along with their relationships is shown here. We will discuss each of the components, the terminology, and the role it plays in the ecosystem to enable a Chef client to execute the job assigned. Chef terminology resembles food preparation. Cookbooks are the formula to make food dishes and recipes are ingredients.</p>
<p>The components of Chef are:</p>
<ul>
<li>Chef server</li>
<li>Chef client</li>
<li>Chef workstation</li>
<li>Chef repo</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chef server</h1>
                </header>
            
            <article>
                
<p>The Chef server is the hub for maintaining the configuration data across the network, storing cookbooks, applying policies to nodes, and each registered node detailed metadata managed by the Chef client. Chef server provides configuration details, such as recipes, templates, and file distributions through the Chef client installed on the respective nodes. Accordingly, the Chef clients implement the configuration on their nodes relieving the Chef server of any processing tasks. This model is scalable to consistently apply the configuration throughout the organization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Features of Chef server</h1>
                </header>
            
            <article>
                
<p>Web-enabled user interface with management console and search.</p>
<ul>
<li>Management console on the chef server is a web-based interface for managing multiple functions such as:
<ul>
<li>Nodes in the network</li>
<li>Cookbooks and recipes</li>
<li>Roles assigned</li>
<li>Data bags--JSON datastores, might include encrypted data</li>
<li>Environment details</li>
<li>Search facility for indexed data</li>
<li>Administrative user accounts and data for chef server access</li>
</ul>
</li>
<li>Search functionality facilitates querying any types of data indexed on Chef server such as nodes, roles, platforms, environments, data bags, and so on. The Apache Solr search engine is the base search engine and extends all the features such as pattern search with exact, wildcard, range, and fuzzy. A full indexed search can be run with different options within the recipe, command line, management console search feature, and so on.</li>
<li>Data bags are located in a secure sub-area on the Chef server; they store sensitive data such as passwords, user account data, and other confidential types of data. They can only be accessed by nodes with the authentic SSL certificates validated by the Chef server. A data bag is accessed by the Chef server with its global variables stored in JSON format. It can be searched and accessed by recipe and loaded.</li>
<li>A policy defines how roles, environments, and cookbook versions are to be implemented in the business and operational requirements, processes, and production workflows:
<ul>
<li>A role is a way to assign the tasks based on specific functions, patterns, and processes performed in an organization such as power or business user, and so on. Each node, web, or database server consists of unique attributes and a run list is assigned per role. When a node is to perform a task, it compares its attributes list with those required to execute the function. The Chef client ensures the attributes and run lists are up-to-date with those on the server.</li>
<li>Environments reflect organizations real-life requirements such as development, staging, or production systems, each are maintained with a cookbook version.</li>
<li>Cookbooks maintain organization-specific configuration policies. Different cookbook versions are maintained such as source control with associated environments, metadata, run lists for different needs; they are uploaded on to a Chef server and applied by a Chef client while configuring the nodes. A cookbook defines a scenario and everything that is required to support that scenario is contained such as:
<ul>
<li>Recipes that specify which resources to use and the order as well</li>
<li>Attribute values</li>
<li>File distributions</li>
<li>Templates</li>
<li>Chef extensions such as custom resources and libraries</li>
</ul>
</li>
<li>A run-list contains all the required information for Chef to configure a node to a desired state. It is an ordered list of roles and recipes specified in the exact order to be run to reach its intended state. It's tailored for each node and stored on the Chef server as part of the node object. It is maintained using knife commands or using the Chef management console on the workstation and uploaded to the Chef server.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chef client on nodes</h1>
                </header>
            
            <article>
                
<p>Chef clients can be installed on different node types--physical, virtual, cloud, network device, and so on, that are registered with Chef server.</p>
<ul>
<li>Types of nodes:
<ul>
<li>A physical node is an active device (system or virtual machine) attached to a network on which a Chef client is installed to communicate with a Chef server.</li>
<li>A cloud-based node is hosted in external cloud environments such as AWS, Microsoft Azure OpenStack, Google Compute Engine, or Rackspace. Knife with plugins provides support for external cloud-based services and creates instances to deploy, configure, and maintain those instances.</li>
<li>A virtual node is a system that runs like a software implementation without direct physical machine access.</li>
<li>A network node such as a switch can be configured with Chef and automated for physical and logical Ethernet link properties and VLANs. Examples of network devices are Juniper Networks, Arista, Cisco, and F5.</li>
<li>Containers are virtual systems running individual configurations sharing the same operating system. Containers are effective at managing distributed and scalable applications and services.</li>
</ul>
</li>
<li>Chef client:
<ul>
<li>The Chef client does the actual configuration. It contacts the Chef server periodically to retrieve the latest cookbooks to update the current state of the node, if required, in accordance with the cookbook instructions. This iterative process is enforced by business policy to ensure the network is in accordance with the envisioned target state.</li>
<li>The Chef client is the local agent that is installed and runs on every node registered with the Chef server to ensure the node is at the expected state. Chef client does most of the computational effort. It's typically a virtual machine, container instance, or physical server.</li>
<li>Authentication between Chef client with the Chef server happens through RSA public key/pairs for every transaction request. The data stored on the Chef server is shared after authentication of registered nodes. Any unauthorized data access is avoided.</li>
<li>After installation of the chef client, the nodes become compute resources on infrastructure that is managed by Chef for performing the tasks such as:
<ul>
<li>Registering the node with the Chef server</li>
<li>Authentication services</li>
<li>Creating the node object</li>
<li>Synchronizing cookbooks with Chef server</li>
<li>The required cookbooks, with recipes, attributes, and all other dependencies, are compiled and loaded</li>
<li>Configuring the node as per the requirements</li>
<li>Exception handling and notifications</li>
</ul>
</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ohai</h1>
                </header>
            
            <article>
                
<p>Ohai is a tool run by Chef client to collect system configuration and metrics data with many built-in plugins to determine the system state for use within cookbooks. The metrics collected by Ohai are:</p>
<ul>
<li>Operating System</li>
<li>Kernel</li>
<li>Host names</li>
<li>Fully-qualified domain names</li>
<li>Virtualization</li>
<li>Cloud service provider metadata</li>
<li>Network</li>
<li>Memory</li>
<li>Disk</li>
<li>CPU</li>
</ul>
<p>Attributes that are collected by Ohai are automatically used by the Chef client to ensure that these attributes remain consistent with the definitions on the server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workstations</h1>
                </header>
            
            <article>
                
<p>Workstations facilitate users to author, test, and maintain cookbooks and interact with the Chef server and nodes. The Chef development toolkit is also installed and configured on a workstation. The Chef development kit is a package comprising prescribed sets of tools, and includes Chef, the command-line tools, Test Kitchen, ChefSpec, Berkshelf, and a few others. Users use workstations for:</p>
<ul>
<li>Developing the cookbooks and test recipes</li>
<li>Testing the Chef code in different environments</li>
<li>Version source control synchronized with Chef repo</li>
<li>Defining and configuring roles and environments and organizational policy</li>
<li>Enforcing data bags are used for storing the critical data</li>
<li>Performing a bootstrap operation on nodes</li>
</ul>
<p>Cookbooks are repositories for files, templates, recipes, attributes, libraries, custom resources, tests, and metadata. Chef client configures each node in the organization through cookbooks and recipes, the fundamental unit of configuration is the cookbook and provides structure to your recipes. Infrastructure state is defined as a file, a template, or a package in policy distribution as per the required scenario.</p>
<p>The programming language for Chef cookbooks is Ruby as a full-fledged programming language with syntax definitions. Recipes are simple patterns for specific configuration items such as packages, files, services, templates, and users with blocks that define properties and values that map to them. Recipes are the fundamental configuration element in a cookbook. A Chef recipe is a file that groups related resources, such as everything needed to configure a web server, database server, or a load balancer. Recipes are stored in cookbooks and can have dependencies on other recipes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chef repo</h1>
                </header>
            
            <article>
                
<p>The Chef repo, as the name suggests, is the repository artifact to author, test, and maintain the cookbooks. The Chef repo is managed like source code, synchronizing with a version control system (such as GitHub, Bitbucket, and so on). The Chef repo directory structure can contain a Chef repo for every cookbook or all of their cookbooks in a single Chef repo.</p>
<p>The <kbd>knife</kbd> is a command interface to communicate with the Chef server from the workstation to upload the cookbooks. To specify configuration details, the <kbd>knife.rb</kbd> file is used, <kbd>knife</kbd> helps to manage:</p>
<ul>
<li>Nodes bootstrapping</li>
<li>Recipes and cookbooks</li>
<li>Environments, roles, and data bags</li>
<li>Various cloud environment resources</li>
<li>Chef client installation to nodes</li>
<li>Chef server indexed data search features</li>
</ul>
<p>The package of tools and utilities to work with Chef is called <strong>Chef Development Kit</strong> (<span><strong>Chef DK</strong>)</span>. It includes command-line tools interacting with Chef such as <kbd>knife</kbd> Chef server and Chef clients and with local Chef code repository (<kbd>chef-repo</kbd>). The components of Chef DK are as follows:</p>
<ul>
<li>Chef client</li>
<li>Chef and <kbd>knife</kbd> command-line tools</li>
<li>Test Kitchen, Cookstyle, and Foodcritic as testing tools</li>
<li>Compliance and security requirements with InSpec as executable code</li>
<li>Cookbooks are authored to upload to Chef server</li>
<li>To encryption and decryption of data bag items is with Chef-Vault using the public keys for registered nodes</li>
<li>Cookbooks dependency manager</li>
<li>Workflow tool Chef</li>
<li>Unit testing framework Chef Specto tests resources locally</li>
<li>For style-checking to write clean cookbooks Rubocop-based tool Cookstyle</li>
<li>Continuous delivery workflow on Chef Automate server also command-line tools to set up and execute</li>
<li>For static analysis of recipe code Foodcritic is a lint tool</li>
<li>It is to test cookbooks across platforms, an integration testing framework tool is Test Kitchen</li>
<li>For rapid cookbook testing and container development <kbd>kitchen-dokken</kbd> is <kbd>test-kitchen</kbd> plugin with a driver, transport, and provisioner for using Docker and Chef</li>
<li>Kitchen driver for Vagrant is <kbd><kbd>kitchen-vagrant</kbd></kbd></li>
<li>People to work together in the same <kbd>chef-repo</kbd> and Chef server knife workflow plugin is <kbd>knife-spork</kbd></li>
<li>The preferred language for Chef is Ruby</li>
</ul>
<p>A recipe is the collection of resources, defined using patterns such as resource names, attribute-value pairs, and actions. It is the fundamental configuration element designed to read and act in a predictable manner and authored in Ruby programming language.</p>
<p>A few properties are as follows:</p>
<ul>
<li>Include all that is required to configure the system</li>
<li>To be stored in a cookbook</li>
<li>For the Chef client to be used, it must be added to a run list</li>
<li>It is executed in the same sequence as listed in a run list</li>
<li>Chef client will run the recipe only when instructed</li>
<li>Could be included in another recipe</li>
<li>Might read the contents of a data bag (encrypted data bag)</li>
<li>Might input the results of a search query</li>
<li>Might have dependency on other recipes</li>
<li>Facilitate the creation of arbitrary groupings by tagging a node</li>
<li>If the recipe is constant, then there won't be any change by repeated execution</li>
</ul>
<p>Recipe DSL is a Ruby DSL that is used to declare resources primarily from within a recipe. It also helps to ensure recipes interact with nodes (and node properties) in the expected manner. Most of the Recipe DSL methods find a specific parameter to advise Chef client on actions to take according to the node parameter.</p>
<p>A resource is a configuration policy statement that:</p>
<ul>
<li>Describes the configuration item desired state</li>
<li>Declares the steps on the item required for the desired state</li>
<li>Resource type is specified such as package, template, or service</li>
<li>Lists additional resource properties</li>
<li>Are grouped into recipes, that describe working configurations</li>
</ul>
<p>Chef has built-in resources to cover common actions across common platforms and can be built to handle any customized situation.</p>
<p>With different versions of cookbooks, multiple environments of production, staging, development/testing are managed.</p>
<p>Cookbook template resources are used to add to recipes for dynamic generation of static text files.</p>
<p>To manage configuration files, <strong>Embedded Ruby</strong> (<strong>ERB</strong>) templates are used.</p>
<p>The cookbooks/templates directory contains ERB template files with Ruby expressions and statements.</p>
<p>The cookbooks are written consistently as per standards and tested for same.</p>
<p>With unit and integration testing, the cookbooks recipes are validated, testing code quality is also called <strong>syntax testing</strong>.</p>
<p>Test Kitchen, ChefSpec, and Foodcritic, and so on, are tools for testing Chef recipes.</p>
<p>The attribute files are executed in the same order as defined in the cookbook.</p>
<p>Chef is built on top of Ruby, it is a thin <strong>domain-specific language</strong> (<span><strong>DSL</strong>) </span>with built-in taxonomy for customizations need of organization.</p>
<p>To manage environments, cookbooks, data bags, and to configure role-based access for users and groups, attributes, run lists, roles, and so on, the Chef server user interface is the Chef management console.</p>
<p> </p>
<p>Chef Supermarket is the community location to share and manage. Cookbooks may be used by any Chef user or organization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extended features of Chef</h1>
                </header>
            
            <article>
                
<p>It is a powerful automation platform that transforms infrastructure into code that operates on the cloud, on-premises, or in a hybrid environment. Infrastructure is configured, deployed, and managed across your network irrespective of the organization size with Chef Automate. Integral parts of Chef Automate are Chef, Habitat, and InSpec.</p>
<p>Three open source power-packed engines are shown in the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a15d9069-6e23-475c-b965-21dc9e6ce48c.png"/></div>
<p>Chef is the core engine for infrastructure automation. Habitat is an application automation tool emulating concepts of containers and microservices. InSpec ensures compliance and security requirements by specifying executable code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Habitat</h1>
                </header>
            
            <article>
                
<p>Habitat comes with a prescribed packaging format for application automation; the Habitat supervisor and application dependencies are packaged and deployed as one unit. The Habitat package format defines on how to be structured, these are isolated, immutably executed for any kind of runtime environments such as a container, bare metal, or PaaS. The Habitat supervisor manages the package's peer relationships, upgrade strategy, and security policies, which are auditable as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">InSpec</h1>
                </header>
            
            <article>
                
<p>InSpec is an open source to test for adherence to security policies. It's a framework for specifying compliance, security, and policy requirements to automatically testing any node in the infrastructure. Compliance can be expressed as code and integrated into a deployment pipeline:</p>
<ul>
<li>InSpec using the Compliance DSL enables you to write auditing rules quickly and easily</li>
<li>InSpec examines infrastructure nodes to run the tests locally or remotely</li>
<li>Security, compliance, or policy issues noncompliance is logged</li>
</ul>
<p>The InSpec audit resource framework and Chef Compliance are fully compatible.</p>
<p>It runs on multiple platforms with remote commands such as SSH or using Docker API, apart from ensuring compliance using APIs, it can access the database, inspect, and can restrict usage of services or protocols and the configuration of virtual machines. An example is to Restrict Telnetd or the FTP service on the client or server machines.</p>
<p>The continuous deployment full-stack pipeline is Chef Automate. It includes automated testing for compliance and security. The workflow provides visibility for both applications and infrastructure, as well as changes propagating throughout the pipeline from development production.</p>
<p>Chef High Level Architecture components are Chef DK, Chef Server, and clients:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/6bab3b21-48b8-450c-ac19-67c271e35774.png"/></div>
<p>The Chef server plays multiple roles and acts as a hub for configuration data. It stores cookbooks, applies the policies to the systems in accordance with the infrastructure, and metadata defined for each system.</p>
<p>Cookbook development workflow is prescribed by the Chef Development kit as below:</p>
<ul>
<li>Skeleton cookbook creation: A cookbook with the standard files already part of the Chef Development kit, the Berkshelf is the package manager that helps manage cookbooks and related dependencies.</li>
<li>Virtual machine environment creation using Test Kitchen: Environment that develops the cookbook with the location details for performing automated testing and debugging of that cookbook during development.</li>
<li>Prepare and debug the recipes for the cookbook: An iterative process to develop and test cookbooks, fix bugs, and test till they meet their purpose. Cookbooks are authored with any text editor such as Sublime Text, vim, TextMate, EditPad, and so on.</li>
<li>Conduct acceptance tests: These tests are done against a full Chef server using a near production environment as opposed to development environment.</li>
<li>The cookbooks that pass all the acceptance tests in the desired manner are deployed to the production environment.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chef Automate workflow</h1>
                </header>
            
            <article>
                
<p>Chef Automate pipeline is for continuous delivery of full-stack approaches for infrastructure and applications. It facilitates safe deployment with any application, changes at high velocity, and relates infrastructure changes.</p>
<p>The Chef Automate pipeline quality gates are automated to move changes from a developer's workstation from deployment to production. A proposed change is approved by a team and afterwards, acceptance tests are approved and released to the respective artefact for delivery into production.</p>
<p>This diagram shows the workflow from development, test, and deployment of Chef code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-924 image-border" height="310" src="assets/542c68a8-e78b-4246-a18f-82a67f557f87.png" width="619"/></div>
<p>The artefact moves through the pipeline after the acceptance stage, moves to the union stage of quality assurance, rehearsal (pre-production), and delivered (production).</p>
<p>The Chef Automate graphical user interface provides views into operational and workflow events. Its data warehouse collects inputs from Chef, Habitat, Automate workflow, and compliance. Dashboards track each change status through the pipeline and query languages available to customize dashboards.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><a href="https://github.com/chef/chef-web-docs/blob/master/images/visibility1.png"><br/>
<img src="assets/168eaf6a-35b1-4467-b831-72cdef762d60.png"/></a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compliance</h1>
                </header>
            
            <article>
                
<p>Compliance issues, security risks, and outdated software can be identified by creating customizable reports with compliance rules in InSpec. There are built-in profiles with predefined rule sets for security frameworks such as <strong>Centre for Internet Security</strong> (<strong>CIS</strong>) benchmarks, and so on. Compliance reposting can be standalone or integrated. Also, the Chef Automate server provides high availability with fault tolerance, real-time data about your infrastructure, and consistent search results.</p>
<p>The Chef Compliance server facilitates centralized management of the infrastructure's compliance, performing the following tasks:</p>
<ul>
<li>Create and manage profiles of rules</li>
<li>Test nodes as per the organization's security management life cycle regularly</li>
<li>The scans are fully executed remotely; no footprint is installed on the node</li>
<li>Compliance reports ensure infrastructure meets security requirements</li>
<li>Auditing statistics for nodes compliance are available</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="445" src="assets/7e235ecf-8702-4fee-8eaa-576b4c48536b.png" width="565"/></div>
<p>Chef compliance reports detailing multiple parameters such as node wise for patch and compliance are shown in the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/c5f443fa-23f8-4649-a1fe-40e8a913e728.png"/></div>
<p>Chef Compliance Report views from Automate.</p>
<p>Chef Automate provides the ability to analyze compliance reports to pivot the data for either nodes, platform of the node, environment, or profiles with the ability to drill down on the information.</p>
<p>Chef Automate compliance control status report provides a comprehensive dashboard on major, minor, critical, patch levels, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ansible</h1>
                </header>
            
            <article>
                
<p>Ansible is a popular and powerful automation framework for continuous delivery with features and benefits are listed in the following topics:</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prominent features</h1>
                </header>
            
            <article>
                
<p>Ansible provides following features:</p>
<ul>
<li><strong>Modernize</strong>
<ul>
<li>Automate existing deployment process</li>
<li>Manage legacy systems and process, updated like DevOps</li>
</ul>
</li>
<li><strong>Migrate</strong>
<ul>
<li>Define applications once and redeploy anywhere</li>
</ul>
</li>
<li><strong>DevOps</strong>
<ul>
<li>Model everything, deploy continuously</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits of Ansible</h1>
                </header>
            
            <article>
                
<p>Using Ansible provides multiple advantages as listed following:</p>
<ul>
<li><strong>Simple to use</strong>
<ul>
<li>Special coding skills not required</li>
<li>Tasks are executed sequentially</li>
<li>Get productive quickly</li>
<li>Automation is easily understood</li>
</ul>
</li>
<li><strong>Powerful with functionality</strong>
<ul>
<li>App deployment</li>
<li>Configuration management</li>
<li>Orchestration of workflow</li>
<li>Orchestration of app life cycle</li>
</ul>
</li>
<li><strong>Agentless</strong>
<ul>
<li>Agentless architecture</li>
<li>Uses OpenSSH and WinRM</li>
<li>No agents to exploit or update</li>
<li>More efficient and secure</li>
</ul>
</li>
</ul>
<p>Ansible is a multi-dimensional IT automation engine that simplifies automation of cloud provisioning, intra-service orchestration, configuration management, application deployment, and many other IT functionalities.</p>
<p>Ansible models your IT infrastructure by prescribing to interrelate systems for multi-tier deployments against managing the systems individually.</p>
<p>As discussed under features, there are neither client-side agents nor additional custom security infrastructure for Ansible. It makes deployment very simple by describing automation jobs in a plain English language called YAML and in the form of Ansible playbooks.</p>
<p>Ansible architecture is as shown in the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b56855b6-bede-4875-9a4c-7728f5b1df17.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ansible terminology, key concepts, workflow, and usage</h1>
                </header>
            
            <article>
                
<p>Ansible Tower is a web-based solution for enterprise automation frameworks designed to be the hub for controlling, securing, and managing your Ansible environment with a user interface and RESTful APIs. It provides the following rich features:</p>
<ul>
<li>Access control is role-based to keep the environment secure, and efficient in managing – allows sharing of SSH credentials but not transfer</li>
<li>With push-button deployment access even non-privileged users can safely deploy entire applications by providing access on the fly</li>
<li>Ensuring complete auditability and compliance as all Ansible automations are centrally logged</li>
<li>Inventory with a wide variety of cloud sources, can be graphically managed or synced</li>
<li>It's based on a robust REST API, integrates well with LDAP, and logs all jobs</li>
<li>Easy integration with the continuous integration tool Jenkins, command-line tools options are available</li>
<li>Supports auto scaling topologies though provisioning callback</li>
<li>Ansible Tower is installed using Ansible playbooks</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/605db491-99ad-4f78-9b38-1438862f5749.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CMDB</h1>
                </header>
            
            <article>
                
<p>Ansible <strong>configuration management database</strong> (<strong>CMDB</strong>) maintains the entire configuration information of the enterprise in the database and supports cloud creation options in multiple formats for different vendors.</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="268" src="assets/9211e19d-1b53-4b95-bdc3-2435de60e80b.png" width="490"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playbooks</h1>
                </header>
            
            <article>
                
<p>Playbooks are the configuration programs written in YAML to automate the systems. Ansible can finely orchestrate multiple instances of infrastructure topology with very detailed control over many machines at the same time. Ansible's approach to orchestration is finely-tuned automation code managed through simple YAML on syntax or features.</p>
<p>Ansible playbooks describe a policy to be orchestrated for enforcement on remote systems for configuration and deployment to enforce general IT process adherence steps.</p>
<p>A simple analogy is, an inventory of hosts is raw material, instruction manuals are playbooks, and Ansible modules are the tools in the workshop.</p>
<p>To manage configurations of deployments to remote machines, playbooks can be used at a basic level. They can sequence multi-tier rollouts involving rolling updates on a more advanced level, to interact with monitoring servers and load balancers along the way and delegate actions to other hosts.</p>
<p>Playbooks are developed in a basic text language conveniently designed to be human-readable. Organizing playbooks and the files can be done in multiple ways.</p>
<p>A simple playbook example<strong>:</strong></p>
<pre>- hosts: webservers 
serial: 6 # update 6 machines at a time 
roles: 
- common 
- webapp 
- hosts: content_servers 
roles: 
- common 
- content </pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="268" src="assets/f97f57d2-1b78-4efa-a7f7-3735c4645e97.png" width="498"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modules</h1>
                </header>
            
            <article>
                
<p>Ansible modules can control system resources, such as services, packages, or files to handle and execute system commands. These resource modules are pushed by Ansible on nodes to configure them to the desired state of the system. These Ansible modules are executed over SSH (Secured Shell) on the target nodes and removed after accomplishing the task. The module library is shipped by default with a number of modules to be executed through playbooks or directly on remote hosts. The modules can reside on any machine, there is no concept of servers, daemons, or databases to maintain them. The modules and libraries are customizable, typically created with any terminal program, a text editor, and to keep track of changes to the content, the version control system is used effectively.</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ab4aae27-96d4-4b4d-8a1c-88664d81b55c.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inventory</h1>
                </header>
            
            <article>
                
<p>Ansible inventory is a list of resources:</p>
<ul>
<li>Hosts and groups</li>
<li>Host variables</li>
<li>Group variables</li>
<li>Groups of groups and group variables</li>
<li>Default groups</li>
<li>Splitting out group and host and specific data</li>
<li>List of inventory behavioral Parameters</li>
<li>Non-SSH connection types</li>
</ul>
<p>Ansible, through the inventory list, works on multiple systems in the infrastructure simultaneously. The dynamic inventory mechanism allows multiple inventory files to be flexible and customizable at the same time through inventory plugins. The inventory list can be in a default location or specify inventory file location of your choice from dynamic or cloud sources, EC2, Rackspace, OpenStack, or different formats.</p>
<p>Here's what a plain text inventory file looks like:</p>
<pre>[webservers] 
www1.example.com 
www2.example.com  
[dbservers] 
db0.example.com 
db1.example.com </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/184531f5-f278-4b0a-82bd-dc9def97e184.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plugins</h1>
                </header>
            
            <article>
                
<p>Ansible's core functionality is augmented by a number of handy plugins and can be customized in JSON (Ruby, Python, Bash, and so on). Plugins can connect to any data source, extend the connection types for transport other than with SSH, call back for logs, and even add new server-side behaviors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ansible Tower</h1>
                </header>
            
            <article>
                
<p>Offers multiple features such as:</p>
<ul>
<li>LDAP, AD, SAML, and other directories can be connected</li>
<li>Access control engines that are role based</li>
<li>Credentials without exposure storage</li>
<li>Simple for first time users</li>
<li>Smart Search enabled information lookup</li>
<li>Configure automation at runtime</li>
<li>REST API based integration with processes and tools</li>
<li>Tower clusters to extend capacity</li>
</ul>
<p>Ansible Tower can invoke multi-playbook workflows to link any number of playbooks, with different inventories, run as different users, run as batch, or with different credentials.</p>
<p>Ansible Tower workflows facilitate many complex operations, build workflows to provision the machines, apply base configurations of systems, and deploy the applications by different teams maintaining different playbooks. A workflow can be built for CI/CD to build an application, deploy to a test environment, run tests, and based on test results, automatically promotes the application. Ansible Tower's intuitive workflow editor easily models complex processes with different playbooks set up to run as alternatives in case of success or failure of a prior workflow playbook.</p>
<p>A typical workflow may be as follows, it can be effectively used on multiple systems quickly without taking their infrastructure offline. To achieve continuous deployment, automated QA is vital to mature to this level:</p>
<ul>
<li>Script automation to deploy local development VMs</li>
<li>CI system such as Jenkins to deploy to a staging environment on every code change</li>
<li>The deploy job executes the test scripts on build for pass/fail for every deploy</li>
<li>Upon success of the deploy job, the same playbook is run against production inventory</li>
</ul>
<p>The Ansible Tower workflow brings the following features and functionality:</p>
<ul>
<li>Jobs schedule</li>
<li>Built-in notifications to inform the teams</li>
<li>Stabilized API to connect to existing tooling and processes</li>
<li>New workflows to model entire processes</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="403" src="assets/cb0550aa-ac5f-4b7a-9bce-433b5fd347b0.png" width="739"/></div>
<p>The Ansible Tower dashboard (refer to the image) offers functionality as listed:</p>
<ul>
<li>Dashboard and real-time automation updates</li>
<li>Graphical inventory management</li>
<li>Integrated RBAC with credential management</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ansible Vault</h1>
                </header>
            
            <article>
                
<p>Ansible Vault is a feature to keep sensitive data in encrypted form, for example passwords or keys as opposed to saving them as plain text in roles or playbooks. These vault files can be placed in source control or distributed to multiple locations. The data files such as Ansible tasks, handlers, arbitrary files, even binary files can be encrypted with Vault as well. These are decrypted at the destination on target host.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ansible Galaxy</h1>
                </header>
            
            <article>
                
<p>Ansible Galaxy is an open source website designed for community information and contributing to collaborate on building IT automation solutions to bring together administrators and developers. There are preconfigured roles to downloaded and jump start automation projects with Galaxy search index. These are also available with a GitHub account.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing strategies with Ansible</h1>
                </header>
            
            <article>
                
<p>Though testing is a very organizational and site-specific concept, Ansible Integrated Testing with Ansible playbooks is designed as an ordered and fail-fast system. It facilitates embed testing directly in Ansible playbooks through a push-based mechanism.</p>
<p>Ansible playbooks are models of desired-state of the system that will ensure the things declared, such as services to be started and packages installed, are in accordance with declarative statements. Ansible is an order-based system on unhandled errors. A host will fail immediately and prevent further configuration of that host and shows them as a summary at the end of the Ansible run. Ansible is a multi-tier orchestration system to incorporate tests into the playbook run, either as tasks or roles.</p>
<p>Testing the application for integrating tests of infrastructure before deployment in the workflow will be effective to check the code quality and performance before it moves to production systems. Being push-based, the checks and balances in the workflow and even upgrading is very easy to maintain on the localhost or test servers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring</h1>
                </header>
            
            <article>
                
<p>Enterprise monitoring is a primary activity and it categorizes monitoring development milestones, application logs, server health, operations, infrastructure, vulnerabilities, deployments, and user activity. These are accomplished with:</p>
<ul>
<li>Collecting and key messages</li>
<li>Mature monitoring tools</li>
<li>Avoid perceptions and making decisions based on uncertainty</li>
<li>Participative monitoring and evaluation</li>
<li>Selecting and using right indicators</li>
<li>Interpreting indicator results in business context</li>
<li>Real-time data collection</li>
<li>Managing data and information</li>
</ul>
<p>Development Milestones: Monitoring of development milestones is an indicator of how well your DevOps adoption strategy is working by gaining insights of the actual process and team effectively. Some of the metrics are sprint scope changes, bugs count of field and fixed, and the ratio of promised-to-delivered features. These metrics are the drivers on team effectiveness and adherence to the schedule, this monitoring is built-in as an Agile plugin for issue tracking.</p>
<p>Code Vulnerabilities: Monitoring vulnerabilities in application code, lists the weaknesses induced in the top-level code by insecure coding practices. These can be addressed by conducting regular code reviews or changing third-party dependencies , and so on.</p>
<p>Deployments: Deployment monitoring is configuring your build servers to have some monitoring built into the process to notify the team. Notification-capable continuous integration servers communicate with chat servers and promptly alert teams of failed builds and deployments.</p>
<p>Application log output: Application log output to be planned for centralized logging if services are distributed to gain full benefit, errors and exceptions provides value in real-time. The ability to trace notifications from error-producing code in a searchable format generates benefit, before production move.</p>
<p>Server Health: Monitoring of uptime and performance of available resources downed or over-utilized servers fall in this category. Intrusion detection and health monitoring systems being on the same notification pipeline will provide additional value.</p>
<p>Activity Monitoring: User activity monitoring is both feature development and the scaling of infrastructure. Along with monitoring development milestones volume of data is monitored.</p>
<p>The centralized storage of consolidated logging data for application logs, user activity monitoring, project history enhances the value to detect and analyze in a global context correlating different log sources about the state of the application and the project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splunk</h1>
                </header>
            
            <article>
                
<p>Splunk is a popular application monitoring tool to gain real-time visibility into DevOps-driven application delivery for Continuous Delivery or Continuous Integration to move from concept to production quickly. Splunk enterprise helps improve the business impact of application delivery by enhancing velocity and quality.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/5790a9bb-77d6-451d-ae6d-af701ffde362.png"/></div>
<p>Splunk improves code quality with the following benefits:</p>
<ul>
<li>Resolve code issues before customers see them</li>
<li>Detect and fix issues related to production faster</li>
<li>Objective metrics are available to ensure code is operational and meets quality SLAs</li>
</ul>
<p>Splunk is a platform to capture and record all the activity and behavior of your customers, machine data, users, transactions, applications, servers, networks, and mobile devices.</p>
<p>The Splunk platform enhances its business impact by integrated real-time insights from application development to testing to production monitoring. It provides cohesive views across all stages of the delivery life cycle as opposed to discrete release components.</p>
<p>Real-time visibility into business-relevant data for business and DevOps leaders on both development and operations, such as application performance, usage, revenue systems, cart fulfillment, and registration data provides insights to better plan inventory and report and improve the customer experience.</p>
<p>Development life cycle integration and visibility across diverse, multiple supported phases and applications is supported:</p>
<p>Operations lifecycle integration and visibility across diverse, multiple supported phases and application is supported. Applications are delivered faster using analytics:</p>
<ul>
<li>End-to-end visibility across every DevOps delivery tool chain component</li>
<li>Correlated insights that iterate faster across the application delivery lifecycle</li>
<li>Measuring and benchmarking release contributions and improving DevOps team efficiency</li>
</ul>
<p>Splunk helps organizations by enabling a feedback loop to business leaders, evaluating the real impact of code changes on their customers. Continuous interaction helps to build more intelligence about machine behavior and deep asset models.</p>
<p>The benefits reflect the business impact of application delivery:</p>
<ul>
<li>Gain new business insights by correlating business metrics with code changes</li>
<li>Enhance user experience through delivery of better-performing code</li>
<li>Delivering more secure and compliant code improves reputation</li>
</ul>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nagios monitoring tool for infrastructure</h1>
                </header>
            
            <article>
                
<p>There are multiple variants of the Nagios open source tool for monitoring mission-critical infrastructure components specific to each segment on any operating system:</p>
<ul>
<li>Network monitoring software</li>
<li>Network traffic monitoring</li>
<li>Server (Linux, Windows) monitoring</li>
<li>Application monitoring tools</li>
<li>Web application monitoring</li>
<li>Monitoring core engine and a basic web interface</li>
<li>Nagios core plugins package with add-ons</li>
<li>Nagios log server security threats with audit system</li>
</ul>
<p>Nagios facilitates monitoring of the network for problems such as overloaded data links, network connections, monitoring routers, switches, problems caused by overloaded of crashed servers, and so on.</p>
<p>Nagios can deliver the metric results in a variety of visual representations and reports to monitor availability, uptime, and response time of every node on the network with both agent-based and agent-less monitoring.</p>
<p>Effective application monitoring with Nagios enables organizations to quickly detect applications, services, or process problems, and take corrective action to prevent downtime for your application users.</p>
<p>Nagios tools for monitoring of applications and application state extends to Windows applications, Linux applications, Unix applications, and web applications. It has an active community collaboration network.</p>
<p>The router monitoring capabilities offer benefits such as immediate notification on unresponsive machines, early warning by detecting network outages and protocol failures, increased servers, services, and application availability.</p>
<p>Windows monitoring with Nagios enables increased servers, services, and application availability, quick detection of network outages, failed services, processes, batch jobs and protocol failures, and so on. The extensive metrics are gathered for system metrics, event logs, applications (IIS, Exchange, , and so on), services (Active Directory, DHCP, service states, process states, performance counters, and so on).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nagios – enterprise server and network monitoring software</h1>
                </header>
            
            <article>
                
<p>Built-in advanced features are:</p>
<ul>
<li>Integrated overview of sources, checks, network flow data, and so on, provided with comprehensive dashboard</li>
<li>Alert of suspicious network activity by security and reliability network analyzer.</li>
<li>Insights and drill down options on network traffic, bandwidth, overall network health, and so on, with advanced visualizations</li>
<li>Monitor network usage of specific applications, custom application monitoring, custom queries, views, and reports are available</li>
<li>Historical network flow data with subsets of network flow information through specialized views</li>
<li>Abnormal activity alerts with automated alert system example bandwidth usage exceeds specified thresholds</li>
<li>Integrated metrics of network analyzer server loads with hard disk space availability</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrated dashboards for network analysis, monitoring, and bandwidth</h1>
                </header>
            
            <article>
                
<p>The Nagios dashboard with multiple monitoring options such as source groups, Server CPU, disk usage, and so on, can be extended and customized with many more choices based on business requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In the next chapter, we will discus on advanced topics of visualization, containers offered by multiple vendors, orchestration options, Internet of Things, microservices, and so on.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>