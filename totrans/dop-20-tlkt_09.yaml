- en: Chapter 9. Proxy Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We reached the point where we need something that will tie together the containers
    we're deploying. We need to simplify the access to the services and unify all
    the servers and ports our containers are (or will be) deployed on. Multiple solutions
    are trying to solve this problem, with **Enterprise Service Bus** (**ESB**) products
    being most commonly used. That is not to say that their only goal is redirection
    towards destination services. It indeed isn't, and that is one of the reasons
    we rejected ESB as (part of) the solution for our architecture. The significant
    difference in the approach is that ESBs tend to do a lot (much more than we need)
    while we are trying to compose our system by using very specific small components
    or services that do (almost) exactly what we need. Not more, not less. ESBs are
    an antithesis of microservices and, in a way, are betraying the initial ideas
    behind service-oriented architecture. With us being committed to microservices
    and looking for more concrete solutions, the alternative is a proxy service. It
    stands to reason that we should dedicate a bit more time discussing what proxy
    services are and which products might be able to help us in our architecture and
    processes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: A *proxy service* is a service that acts as an intermediary between clients
    performing requests and services that serve those requests. A client sends a request
    to the proxy service that, in turn, redirects that request to the destination
    service thus simplifying and controlling complexity laying behind the architecture
    where the services reside.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'There are at least three different types of proxy services:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A *gateway* or *tunneling service* is the kind of a proxy service that redirect
    requests to the destination services and responses back to the clients that made
    those requests.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *forward proxy* is used for retrieving data from different (mostly internet)
    sources.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *reverse proxy* is usually used to control and protect access to a server
    or services on a private network. Besides its primary function, a reverse proxy
    often also performs tasks such as load-balancing, decryption, caching and authentication.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reverse proxy is probably the best solution for the problem at hand, so we'll
    spend a bit more time trying to understand it better.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Reverse Proxy Service
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main purpose of the proxy service is to hide the rest of the services as
    well as to redirect requests to their final destination. The same holds true for
    responses. Once a service responds to a request, that response goes back to the
    proxy service and from there is redirected to the client that initially requested
    it. For all purposes, from the point of view of the destination service, the request
    came from the proxy. In other words, neither the client that generates the request
    knows what is behind the proxy nor the service responding to the request knows
    that it originated from beyond the proxy. In other words, both clients and services
    know only about the existence of the proxy service.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: We'll concentrate on usages of a proxy service in the context of an architecture
    based on (micro) services. However, most of the concepts are the same if a proxy
    service would be used on whole servers (except that it would be called proxy server).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the main purposes of a proxy services (beyond orchestration of requests
    and responses) are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: While almost any applications server can provide **encryption** (most commonly
    **Secure Sockets Layer** (**SSL**)), it is often easier to let the middle man
    be in charge of it.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing** is the process when, in this case, proxy service balances
    loads between multiple instances of the same service. In most cases, those instances
    would be scaled over multiple servers. With that combination (load balancing and
    scaling), especially when architecture is based on microservices, we can quickly
    accomplish performance improvements and avoid timeouts and downtimes.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression** is another candidate for a feature that is easily accomplished
    when centralized in a single service. Main products that act as proxy services
    are very efficient in compression and allow relatively easy setup. The primary
    reason for a compression of the traffic is a speedup of the load time. The smaller
    the size, the faster the load.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caching** is another one of the features that are easy to implement within
    a proxy service that (in some cases) benefits from being centralized. By caching
    responses, we can offload part of the work our services need to do. The gist of
    caching is that we set up the rules (for example, cache requests related to the
    products listing) and cache timeouts. From there on, the proxy service will send
    a request to the destination service only the first time and store the responses
    internally. From there on, as long as the request is the same, it will be served
    directly by the proxy without even sending the request to the service. That is,
    until the timeout is reached and the process is repeated. The are much more complicated
    combinations we can employ, but the most common usage is the one we described.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most proxy services serve as a *single point of entry* to the public APIs exposed
    through services. That in itself increases **security**. In most cases only ports
    `80` (`HTTP`) and `443` (`HTTPS`) would be available to the public usage. All
    other ports required by services should be open only to the internal use.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数代理服务作为*单一入口点*服务公开的公共 API。仅此一点就增加了**安全性**。在大多数情况下，只有端口`80`（`HTTP`）和`443`（`HTTPS`）会对外公开，所有其他服务所需的端口应仅对内部使用开放。
- en: Different types of authentication (for example OAuth) can be implemented through
    the proxy service. When the request does not have the user identification, the
    proxy service can be set to return with an appropriate response code to the caller.
    On the other hand, when identification is present, a proxy can choose to continue
    going to the destination and leave the verification of that identification to
    the target service or perform it itself. Of course, many variations can be used
    to implement the authentication. The crucial thing to note is that if a proxy
    is used, it will most likely be involved in this process one way or another.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的身份验证（例如 OAuth）可以通过代理服务实现。当请求没有用户身份时，代理服务可以设置为返回适当的响应码给调用者。另一方面，当身份信息存在时，代理可以选择继续访问目标，并将身份验证交由目标服务处理，或者由代理自行处理。当然，许多不同的变体可以用来实现身份验证。关键要注意的是，如果使用了代理，它很可能会在这个过程中以某种方式参与其中。
- en: This list is by no means extensive nor final but contains some of the most commonly
    used cases. Many other combinations are possible involving both legal and illegal
    purposes. As an example, a proxy is an indispensable tool for any hacker that
    wants to stay anonymous.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这份清单绝不是详尽无遗的，也不是最终的，但包含了一些最常见的使用案例。许多其他组合也可能存在，包括合法和非法的用途。例如，代理是任何想要保持匿名的黑客不可或缺的工具。
- en: Throughout this books, we'll focus mostly on its primary function; we'll use
    proxy services to act as proxies. They will be in charge of the orchestration
    of all traffic between microservices we'll be deploying. We'll start with simple
    usages used in deployments and slowly progress towards more complicated orchestration,
    namely *blue-green deployment*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将主要关注代理服务的基本功能；我们将使用代理服务作为代理。它们将负责所有微服务之间流量的调度，这些微服务将被部署。我们从部署中使用的简单用法开始，逐步推进到更复杂的调度方式，即*蓝绿部署*。
- en: To some, it might sound that a proxy service deviates from microservices approach
    since it can do (as is often the case) multiple things. However, when looking
    from the functional point of view, it has a single purpose. It provides a bridge
    between the outside world and all the services we host internally. At the same
    time, it tends to have a very low resource usage and can be handled with only
    a few configuration files.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些人来说，代理服务可能偏离了微服务的思路，因为它（通常情况下）可以做多件事。然而，从功能角度看，它只有一个单一的目的。它提供了外部世界与我们内部托管的所有服务之间的桥梁。同时，它往往资源占用非常低，只需要几个配置文件即可处理。
- en: Equipped with the basic understanding about proxy services, the time has come
    to take a look at some of the products we can use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了代理服务的基本概念后，现在是时候了解我们可以使用的一些产品了。
- en: From now on, we'll refer to *reverse proxy* as, simply, *proxy*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将*反向代理*简化为*代理*。
- en: How can Proxy Service help our project?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理服务如何帮助我们的项目？
- en: By now we managed to have a controlled way to deploy our services. Due to the
    nature of deployments we are trying to accomplish, those services should be deployed
    on ports and, potentially, servers that are unknown to us in advance. Flexibility
    is the key to scalable architecture, fault tolerance, and many other concepts
    we'll explore further on. However, that flexibility comes at a cost. We might
    not know in advance where will the services be deployed nor which ports they are
    exposing. Even if this information would be available before the deployment, we
    should not force users of our services to specify different ports and IPs when
    sending requests. The solution is to centralize all communications both from third
    parties as well as from internal services at a single point. The singular place
    that will be in charge of redirecting requests is a proxy service. We'll explore
    some of the tools that are at our disposal and compare their strengths and weaknesses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: As before, we'll start by creating virtual machines that we'll use to experiment
    with different proxy services. We'll recreate the `cd` node and use it to provision
    the `proxy` server with different proxy services.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first tool we'll explore is `nginx`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: nginx
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The nginx (engine x) is an HTTP and reverse proxy server, a mail proxy server,
    and a generic TCP proxy server. Igor Sysoev originally wrote it. In the beginning,
    it powered many Russian sites. Since then, it become a server of choice for some
    of the busiest sites in the world (NetFlix, Wordpress, and FastMail are only a
    few of the examples). According to Netcraft, nginx served or proxied around 23%
    of busiest sites in September 2015\. That makes it second to Apache. While numbers
    provided by Netcraft might be questionable, it is clear that nginx is highly popular
    and probably is closer to the third place after Apache and IIS. Since everything
    we did by now is based on Linux, Microsoft IIS should be discarded. That leaves
    us with Apache as a valid candidate to be our proxy service or choice. Stands
    to reason that the two should be compared.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Apache has been available for many years and built a massive user base. Its
    huge popularity is partly thanks to Tomcat that runs on top of Apache and is one
    of the most popular application servers today. Tomcat is only one out of many
    examples of Apache's flexibility. Through its modules, it can be extended to process
    almost any programming language.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Being most popular does not necessarily makes something the best choice. Apache
    can slow down to a crawl under a heavy load due to its design deficiencies. It
    spawns new processes that, in turn, consume quite a lot of memory. On top of that,
    it creates new threads for all requests making them compete with each others for
    access to CPU and memory. Finally, if it reaches the configurable limit of processes,
    it just refuses new connections. Apache was not designed to serve as a proxy service.
    That function is very much an after-thought.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: nginx was created to address some of the problems Apache has, in particular,
    the C10K problem. At the time, C10K was a challenge for web servers to begin handling
    ten thousand concurrent connections. nginx was released in 2004 and met the goal
    of the challenge. Unlike Apache, its architecture is based on asynchronous, non-blocking,
    event-driven architecture. Not only that it beats Apache in the number of concurrent
    requests it can handle, but its resource usage was much lower. It was born after
    Apache and designed from ground up as a solution for concurrency problems. We
    got a server capable of handling more requests and a lower cost.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: nginx' downside is that it is designed to serve static content. If you need
    a server to serve content generated by Java, PHP, and other dynamic languages,
    Apache is a better option. In our case, this downside is of almost no importance
    since we are looking for a proxy service with the capability to do load balancing
    and few more features. We will not be serving any content (static or dynamic)
    directly by the proxy, but redirect requests to specialized services.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: All in all, while Apache might be a good choice in a different setting, nginx
    is a clear winner for the task we're trying to accomplish. It will perform much
    better than Apache if its only task is to act as a proxy and load balancing. It's
    memory consumption will be minuscule and it will be capable of handling a vast
    amount of concurrent requests. At least, that is the conclusion before we get
    to other contestants for the proxy supremacy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up nginx
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we set up the nginx proxy service, let''s take a quick look at the Ansible
    files that we''re about to run. The nginx.yml playbook is similar to those we
    used before. We''ll be running the roles we already run before with the addition
    of nginx:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`oles/nginx/tasks/main.yml` role also doesn''t contain anything extraordinary:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are creating few directories, making sure that the nginx container is running,
    passing few files and, if any of them changed, reloading nginx. Finally, we are
    putting the nginx IP to Consul in case we need it for later. The only important
    thing to notice is the nginx configuration file `roles` `/nginx/files/services.conf`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For the moment, you can ignore log formatting and jump to the `server` specification.
    We specified that nginx should `listen` to the standard HTTP port `80` and accept
    requests sent to any server (`server_name _`). Next are the `include` statements.
    Instead of specifying all the configuration in one place, with includes we'll
    be able to add configuration for each service separately. That, in turn, will
    allow us to focus on one service at a time and make sure that the one we deploy
    is configured correctly. Later on, we'll explore in more depth which types of
    configurations go into each of those includes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the nginx playbook and start *playing* with it. We''ll enter the
    `cd` node and execute the playbook that will provision the `proxy` node:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Living without a Proxy
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we see nginx in action, it might be worthwhile to refresh our memory
    of the difficulties we are facing without a proxy service. We''ll start by running
    the `books-ms` application:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the last command is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Even though we run the application with `docker-compose` and confirmed that
    it is running on the `proxy` node by executing `docker-compose ps`, we observed
    through `curl` that the service is not accessible on the standard HTTP port 80
    (there was a `404 Not Found` message served through nginx). This result was to
    be expected since our service is running on a random port. Even if we did specify
    the port (we already discussed why that is a bad idea), we could not expect users
    to memorize a different port for each separately deployed service. Besides, we
    already have service discovery with Consul in place:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the last command is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can also obtain the port by inspecting the container:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We inspected the container, applied formatting to retrieve only the port of
    the service and stored that information in the `PORT` variable. Later on, we used
    that variable to make a proper request to the service. As expected, this time,
    the result was correct. Since there is no data, the service returned an empty
    JSON array (this time without the 404 error).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Be it as it may, while this operation was successful, it is even less acceptable
    one for our users. They cannot be given access to our servers only so that they
    can query Consul or inspect containers to obtain the information they need. Without
    a proxy, services are unreachable. They are running, but no one can use them:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Living without a Proxy](img/B05848_09_01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1 – Services without proxy
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Now that we felt the pain our users would feel without a proxy, let us configure
    nginx correctly. We'll start with manual configuration, and from there on, progress
    towards automated one.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Manually Configuring nginx
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Do you remember the first `includes` statement in the nginx configuration?
    Let''s use it. We already have the `PORT` variable, and all that we have to do
    is make sure that all requests coming to nginx on port `80` and starting with
    the address `/api/v1/books` are redirected to the correct port. We can accomplish
    that by running the following commands:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We created the `books-ms.conf` file that will proxy all requests for `/api/v1/books`
    to the correct IP and port. The `location` statement will match all requests starting
    with `/api/v1/books` and proxy them to the same address running on the specified
    IP and port. While IP was not necessary, it is a good practice to use it since,
    in most cases, the proxy service will run on a separate server. Further on, we
    used **secure copy** (**scp**) to transfer the file to the `/data/nginx/includes/`
    directory in the `proxy` node. Once the configuration was copied, all we had to
    do was reload nginx using `kill -s HUP` command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see whether the change we just did works correctly:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We successfully made a `PUT` request that inserted a book to the database and
    queried the service that returned that same book. Finally, we can make requests
    without worrying about the ports.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Are our problems solved? Only partly. We still need to figure out the way to
    make these updates to the nginx configuration automatic. After all, if we''ll
    be deploying our microservices often, we cannot rely on human operators to continuously
    monitor deployments and perform configuration updates:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Manually Configuring nginx](img/B05848_09_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2 – Services with manual proxy
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Automatically Configuring nginx
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We already discussed service discovery tools and the nginx playbook we run earlier
    made sure that Consul, Registrator, and Consul Template are properly configured
    on the *proxy* node. That means that Registrator detected the service container
    we ran and stored that information to the Consul registry. All that is left is
    to make a template, feed it to Consul Template that will output the configuration
    file and reload nginx.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make the situation a bit more complicated and scale our service by running
    two instances. Scaling with Docker Compose is relatively easy:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the latter command is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can observe that there are two instances of our service, both using different
    random ports. Concerning nginx, this means several things, most important being
    that we cannot proxy in the same way as before. It would be pointless to run two
    instances of the service and redirect all requests only to one of them. We need
    to combine proxy with *load* *balancing*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t go into all possible load balancing techniques. Instead, we''ll use
    the simplest one called *round robin* that is used by nginx by default. Round
    robin means that the proxy will distribute requests equally among all services.
    As before, things closely related to a project should be stored in the repository
    together with the code and nginx configuration files and templates should not
    be an exception.take a look at the `nginx-includes.conf` configuration file:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This time, instead of specifying IP and port, we're using `books_ms`. Obviously,
    that domain does not exist. It is a way for us to tell nginx to proxy all requests
    from the location to an upstream. Additionally, we also added `proxy_next_upstream`
    instruction. If an error, timeout, invalid header or an error 500 is received
    as a service response, nginx will pass to the next upstream connection.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'That is the moment when we can start using the second include statement from
    the main configuration file. However, since we do not know the IPs and ports the
    service will use, the upstream is the Consul Template file `nginx-upstreams.ctmpl`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: What this means is that the upstream request `books-ms` we set as the proxy
    upstream will be load balanced between all instances of the service and that data
    will be obtained from Consul. We'll see the result once we run Consul Template.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first. Let''s download the two files we just discussed:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that the proxy configuration and the upstream template are on the `cd`
    server, we should run Consul Template:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Consul Template took the downloaded template as the input and created the `books-ms.conf`
    upstream configuration. The second command output the result that should look
    similar to the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since we are running two instances of the same service, Consul template retrieved
    their IPs and ports and put them in the format we specified in the `books-ms.ctmpl`
    template.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we could have passed the third argument to Consul Template,
    and it would run any command we specify. We'll use it later on throughout the
    book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that all the configuration files are created, we should copy them to the
    `proxy` node and reload nginx:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All that''s left is to double check that proxy works and is balancing requests
    among those two instances:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After making four requests we output nginx logs that should look like following
    (timestamps are removed for brevity).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'While ports might be different in your case, it is obvious that the first request
    was sent to the port `32768`, the next one to the `32769`, then to the `32768`
    again, and, finally, to the `32769`. It is a success, with nginx not only acting
    as a proxy but also load balancing requests among all instances of the service
    we deployed:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatically Configuring nginx](img/B05848_09_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3 – Services with automatic proxy with Consul Template
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'We still haven''t tested the error handling we set up with the `proxy_next_upstream`
    instruction. Let''s remove one of the service instances and confirm that nginx
    handles failures correctly:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We stopped one service instance and made several requests. Without the *proxy_next_upstream*
    instruction, nginx would fail on every second request since one of the two services
    set as upstreams are not working anymore. However, all four requests worked correctly.
    We can observe what nginx did by taking a look at its logs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output should be similar to the following (timestamps are removed for brevity):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first request went to the port `32768` served by the instance that is still
    running. As expected, nginx sent the second request to the port `32768`. Since
    the response was `111` (Connection refused), it decided to temporarily disable
    this upstream and try with the next one in line. From there on, all the rest of
    requests were proxied to the port `32768`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: With only a few lines in configuration files, we managed to set up the proxy
    and combine it with load balancing and failover strategy. Later on, when we get
    to the chapter that will explore *self-healing systems*, we'll go even further
    and make sure not only that proxy works only with running services, but also how
    to restore the whole system to a healthy state.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'When nginx is combined with service discovery tools, we have an excellent solution.
    However, we should not use the first tool that comes along, so we''ll evaluate
    a few more options. Let us stop the nginx container and see how *HAProxy* behaves:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: HAProxy
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like nginx, HAProxy is a free, very fast and reliable solution offering
    high availability, load balancing, and proxying. It is particularly suited for
    very high traffic websites and powers quite many of the world's most visited ones.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: We'll speak about the differences later on when we compare all proxy solutions
    we're exploring. For now, suffice to say that HAProxy is an excellent solution
    and probably the best alternative to nginx.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with practical exercises and try to accomplish with HAProxy the
    same behavior as the one with have with nginx. Before we provision the *proxy*
    node with HAProxy, let us take a quick look at the tasks in the Ansible role haproxy:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The *haproxy* role is very similar to the one we used for nginx. We created
    some directories and copied some files (we''ll see them later on). The major thing
    to note is that, unlike most other containers not built by us, we''re not using
    the official *haproxy* container. The main reason is that the official image has
    no way to reload HAProxy configuration. We''d need to restart the container every
    time we update HAProxy configuration, and that would produce some downtime. Since
    one of the goals is to accomplish zero-downtime, restarting the container is not
    an option. Therefore, we had to look at alternatives, and the user *million12*
    has just what we need. The `million12/haproxy` container comes with *inotify*
    (*inode notify*). It is a Linux kernel subsystem that acts by extending filesystems
    to notice changes, and report them to applications. In our case, inotify will
    reload HAProxy whenever we change its configuration:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us proceed and provision HAProxy on the proxy node:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Manually Configuring HAProxy
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll start by checking whether HAProxy is running:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `docker ps` command showed that the *haproxy* container has the status
    `Exited`, and the logs produced the output similar to the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: HAProxy complained that there is no `haproxy.cfg` configuration file and stopped
    the process. Actually, the fault is in the playbook we run. The only file we created
    is *haproxy.cfg.orig* (more about it later) and that there is no `haproxy.cfg`.
    Unlike nginx, HAPRoxy cannot be run without having, at least, one proxy set. We'll
    set up the first proxy soon but, at the moment, we have none. Since creating the
    configuration without any proxy is a waste of time (HAProxy fails anyway) and
    we cannot provide one when provisioning the node for the first time since at that
    point there would be no services running, we just skipped the creation of the
    *haproxy.cfg*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed with the configuration of the first proxy, let us comment
    another difference that might complicate the process. Unlike nginx, HAProxy does
    not allow includes. The complete configuration needs to be in a single file. That
    will pose certain problems since the idea is to add or modify only configurations
    of the service we are deploying and ignore the rest of the system. We can, however,
    simulate includes by creating parts of the configuration as separate files and
    concatenate them every time we deploy a new container. For this reason, we copied
    the `haproxy.cfg.orig` file as part of the provisioning process. Feel free to
    take a look at it. We won't go into details since it contains mostly the default
    settings and HAProxy has a decent documentation that you can consult. The important
    thing to note is that the `haproxy.cfg.orig` file contains settings without a
    single proxy being set.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create the HAProxy configuration related to the service we have running
    in the similar way as we did before:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We started by inspecting the `vagrant_app_1` container in order to assign the
    current port to the `PORT` variable and use it to create the `books-ms.service.cfg`
    file.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy uses similar logic as nginx even though things are named differently.
    The *frontend* defines how requests should be forwarded to *backends*. In a way,
    the *frontend* is analogous to the nginx *location* instruction and the *backend*
    to the `upstream`. What we did can be translated to the following. Define a frontend
    called `books-ms-fe`, bind it to the port `80` and, whenever the request part
    starts with `/api/v1/books`, use the backend called `books-ms-be`. The backend
    `books-ms-be` has (at the moment) only one server defined with the IP `10.100.193.200`
    and the port assigned by Docker. The `check` argument has (more or less) the same
    meaning as in nginx and is used to skip proxying to services that are not healthy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the general settings in the file `haproxy.cfg.orig` and those
    specific to services we''re deploying (named with the `.service.cfg` extension),
    we can concatenate them into a single `haproxy.cfg` configuration file and copy
    it to the `proxy` node:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Since the container is not running, we''ll need to start it (again), and then
    we can check whether the proxy is working correctly by querying the service:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first request returned the `Connection refused` error. We used it to confirm
    that no proxy is running. Then we started the `haproxy` container and saw through
    the container logs that the configuration file we created is valid and indeed
    used by the proxy service. Finally, we sent the request again, and, this time,
    it returned a valid response.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good. We can proceed and automate the process using Consult Template.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Automatically Configuring HAProxy
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll try to do the same or very similar steps as what we did before with nginx.
    That way you can compare the two tools more easily.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by scaling the service:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next we should download the `haproxy.ctmpl` template from the code repository.
    Before we do that, let us take a quick look at its contents:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The way we created the template follows the same pattern as the one we used
    with nginx. The only difference is that HAProxy needs each server to be uniquely
    identified so we added the service `Node` and `Port` that will serve as the server
    ID.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download the template and run it through Consul Template:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We downloaded the template using `wget` and run the `consul-template` command.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us concatenate all the files into haproxy.cfg, copy it to the `proxy` node
    and take a look at `haproxy` logs:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'All that''s left is to double check whether the proxy balancing works with
    two instances:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Unfortunately, HAProxy cannot output logs to stdout (preferred way to log Docker
    containers) so we cannot confirm that balancing works. We could output logs to
    syslog, but that is outside of the scope of this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'We still haven''t tested the error handling we set up with the `backend` instruction.
    Let''s remove one of the service instances and confirm that HAProxy handles failures
    correctly:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We stopped one service instance and made several requests, and all of them worked
    properly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Without the possibility to include files into HAProxy configuration, our job
    was slightly more complicated. Not being able to log to stdout can be solved with
    syslog but will go astray from one of the containers best practices. There is
    a reason for this HAProxy behavior. Logging to stdout slows it down (noticeable
    only with an enormous number of requests). However, it would be better if that
    is left as our choice and maybe the default behavior, instead of not being supported
    at all. Finally, not being able to use the official HAProxy container might be
    considered a minor inconvenience. None of those problems are of great importance.
    We solved the lack of includes, could log into syslog and ended up using the container
    from `million12/haproxy` (we could also create our own that would extend from
    the official one).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Proxy Tools Compared
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache, nginx and HAProxy are by no means the only solutions we could use. There
    are many projects available and making a choice is harder than ever.f the open
    source projects worth trying out is `lighttpd` (`pron. lighty`). Just like nginx
    and HAProxy, it was designed for security, speed, compliance, flexibility and
    high performance. It features a small memory footprint and efficient management
    of the CPU-load.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: If JavaScript is your language of preference, [node-http-proxy] could be a worthy
    candidate. Unlike other products we explored, node-http-proxy uses JavaScript
    code to define proxies and load balancing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The VulcanD is a project to keep an eye on. It is programmable proxy and load
    balancer backed by etcd. A similar process that we did with Consul Template and
    nginx/HAProxy is incorporated inside VulcanD. It can be combined with Sidekick
    to provide functionality similar to `check` arguments in nginx and HAProxy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: There are many similar projects available, and it is certain that new and existing
    ones are into making. We can expect more `unconventional` projects to appear that
    will combine proxy, load balancing, and service discovery in many different ways.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: However, my choice, for now, stays with nginx or HAProxy. None of the other
    products we spoke about has anything to add and, in turn, each of them, at least,
    one deficiency.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Apache is process based, making its performance when faced with a massive traffic
    less than desirable. At the same time, its resource usage skyrockets easily. If
    you need a server that will serve dynamic content, Apache is a great option, but
    should not be used as a proxy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Lighttpd was promising when it appeared but faced many obstacles (memory leaks,
    CPU usage, and so on) that made part of its users switch to alternatives. The
    community maintaining it is much smaller than the one working on nginx and HAProxy.
    While it had its moment and many had high expectations from it, today it is not
    the recommended solution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: What can be said about `node-http-proxy`? Even though it does not outperform
    nginx and HAProxy, it is very close. The major obstacle would be its programmable
    configuration that is not well suited for continuously changing proxies. If your
    language of choice is JavaScript and proxies should be relatively static, node-http-proxy
    is a valid option. However, it still doesn't provide any benefit over nginx and
    HAProxy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: VulcanD, in conjunction with Sidekick, is a project to keep an eye on, but it
    is not yet production ready (at least, not at the time this text was written).
    It is very unlikely that it will manage to outperform main players. The potential
    problem with VulcanD is that it is bundled with etcd. If that's what you're already
    using, great. On the other hand, if your choice fell to some other type of Registry
    (for example Consul or Zookeeper), there is nothing VulcanD can offer. I prefer
    keeping proxy and service discovery separated and put the glue between them myself.
    Real value VulcanD provides is in a new way of thinking that combines proxy service
    with service discovery, and it will probably be considered as one of the pioneers
    that opened the door for new types of proxy services.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: That leaves us with nginx and HAProxy. If you spend some more time investigating
    opinions, you'll see that both camps have an enormous number of users defending
    one over the other. There are areas where nginx outperforms HAProxy and others
    where it underperforms. There are some features that HAProxy doesn't have and
    other missing in nginx. But, the truth is that both are battle-tested, both are
    an excellen solution, both have a huge number of users, and both are successfully
    used in companies that have colossal traffic. If what you're looking for is a
    proxy service with load balancing, you cannot go wrong with either of them.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: I am slightly more inclined towards nginx due to its better (official) Docker
    container (for example, it allows configuration reloads with a HUP signal), option
    to log to stdout and the ability to include configuration files. Excluding Docker
    container, HAProxy made the conscious decision not to support those features due
    to possible performance issues they can create. However, I prefer having the ability
    to choose when it's appropriate to use them and when it isn't. All those are truly
    preferences of no great importance and, in many cases, the choice is made depending
    on a particular use case one is trying to accomplish. However, there is one critical
    nginx feature that HAProxy does not support. HAProxy can drop traffic during reloads.
    If microservices architecture, continuous deployment, and blue-green processes
    are adopted, configuration reloads are very common. We can have several or even
    hundreds of reloads each day. No matter the reload frequency, with HAProxy there
    is a possibility of downtime.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: We have to make a choice, and it falls to nginx. It will be out proxy of choice
    throughout the rest of the book.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'With that being said, let us destroy the VMs we used in this chapter and finish
    the implementation of the deployment pipeline. With service discovery and the
    proxy, we have everything we need:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
