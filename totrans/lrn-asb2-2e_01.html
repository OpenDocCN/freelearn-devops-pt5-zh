<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Getting Started with Ansible</h1></div></div></div><p>ICT is often described as a fast-growing industry. I think the best quality of the ICT industry is not related to its ability to grow at a super high speed, but to its ability to revolutionize itself and the rest of the world at an astonishing speed.</p><p>Every 10 to 15 years there are major shifts in how this industry works and every shift solves problems that were very hard to manage up to that point, creating new challenges. Also, at every major shift, many best practices of the previous iteration are classified as anti-patterns and new best practices are created. Although it might appear that those changes are impossible to predict, this is not always true. Obviously, it is not possible to know exactly what changes will occur and when they will take place, but looking at companies with a large number of servers and many lines of code usually reveals what the next steps will be.</p><p>The current shift has already happened in big companies like Amazon Web Services, Facebook, and Google. It is the implementation of IT automation systems to create and manage servers.</p><p>In this chapter we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">IT automation</li><li class="listitem" style="list-style-type: disc">What is Ansible?</li><li class="listitem" style="list-style-type: disc">The secure shell</li><li class="listitem" style="list-style-type: disc">Installing Ansible</li><li class="listitem" style="list-style-type: disc">Creating a test environment with QEMU and KVM</li><li class="listitem" style="list-style-type: disc">Version control system</li><li class="listitem" style="list-style-type: disc">Using Ansible with Git</li></ul></div><div><div><div><div><h1 class="title"><a id="ch01lvl1sec7"/>IT automation</h1></div></div></div><p>IT automation is in its larger sense—the processes and software that help with the management of the IT infrastructure (servers, networking, and storage). In the current shift, we are assisting to a huge implementation of such processes and software.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec6"/>The history of IT automation</h2></div></div></div><p>At the beginning of IT history, there were very few servers and a lot of people were needed to make them work properly, usually more than one person for each machine. Over the years, servers became more reliable and easier to manage so it was possible to have multiple servers managed by a single system administrator. In that period, the administrators manually installed the software, upgraded the software manually, and changed the configuration files manually. This was obviously a very labor-intensive and error-prone process, so many administrators started to implement scripts and other means to make their life easier. Those scripts were (usually) pretty complex and they did not scale very well.</p><p>In the early years of this century, data centers started to grow a lot due to companies' needs. Virtualization helped in keeping prices low and the fact that many of these services were web services, meant that many servers were very similar to each other. At this point, new tools were needed to substitute the scripts that were used before, the configuration management tools.</p><p>
<strong>CFEngine</strong> was one of the first tools to demonstrate configuration management capabilities way back in the 1990s; more recently, there has been Puppet, Chef, and Salt, besides Ansible.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec7"/>Advantages of IT automation</h2></div></div></div><p>People often wonder if IT automation really brings enough advantages considering that implementing it has some direct and indirect costs. The main advantages of IT automation are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ability to provision machines quickly</li><li class="listitem" style="list-style-type: disc">Ability to recreate a machine from scratch in minutes</li><li class="listitem" style="list-style-type: disc">Ability to track any change performed on the infrastructure</li></ul></div><p>For these reasons, it's possible to reduce the cost of managing the IT infrastructure by reducing the repetitive operations often performed by system administrators.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec8"/>Disadvantages of IT automation</h2></div></div></div><p>As with any other technology, IT automation does come with some disadvantages. From my point of view these are the biggest disadvantages:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Automating all of the small tasks that were once used to train new system administrators</li><li class="listitem" style="list-style-type: disc">If an error is performed, it will be propagated everywhere</li></ul></div><p>The consequence of the first is that new ways to train junior system administrators will need to be implemented.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec1"/>Limiting the possible damages of an error propagation</h3></div></div></div><p>The second one is trickier. There are a lot of ways to limit this kind of damage, but none of those will prevent it completely. The following mitigation options are available:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Always have backups</strong>: Backups will not prevent you from nuking your machine; they will only make the restore process possible.</li><li class="listitem" style="list-style-type: disc"><strong>Always test your infrastructure code (playbooks/roles) in a non-production environment</strong>: Companies have developed different pipelines to deploy code and those usually include environments such as dev, test, staging, and production. Use the same pipeline to test your infrastructure code. If a buggy application reaches the production environment it could be a problem. If a buggy playbook reaches the production environment, it could be catastrophic.</li><li class="listitem" style="list-style-type: disc"><strong>Always peer-review your infrastructure code</strong>: Some companies have already introduced peer-reviews for the application code, but very few have introduced it for the infrastructure code. As I was saying in the previous point, I think infrastructure code is way more critical than application code, so you should always peer-review your infrastructure code, whether you do it for your application code or not.</li><li class="listitem" style="list-style-type: disc"><strong>Enable SELinux</strong>: SELinux is a security kernel module that is available on all Linux distributions (it is installed by default on Fedora, Red Hat Enterprise Linux, CentOS, Scientific Linux, and Unbreakable Linux). It allows you to limit users and process powers in a very granular way. I suggest using SELinux instead of other similar modules (such as AppArmor) because it is able to handle more situations and permissions. SELinux will prevent a huge amount of damage because, if correctly configured, it will prevent many dangerous commands from being executed.</li><li class="listitem" style="list-style-type: disc"><strong>Run the playbooks from a limited account</strong>: Even though user and privilege escalation schemes have been in UNIX code for more than 40 years, it seems as if not many companies use them. Using a limited user for all your playbooks, and escalating privileges only for commands that need higher privileges will help prevent you nuking a machine while trying to clean an application temporary folder.</li><li class="listitem" style="list-style-type: disc"><strong>Use horizontal privilege escalation</strong>: The <code class="literal">sudo</code> is a well-known command but is often used in its more dangerous form. The <code class="literal">sudo</code> command supports the '<code class="literal">-u</code>' parameter that will allow you to specify a user that you want to impersonate. If you have to change a file that is owned by another user, please do not escalate to <code class="literal">root</code> to do so, just escalate to that user. In Ansible, you can use the <code class="literal">become_user</code> parameter to achieve this.</li><li class="listitem" style="list-style-type: disc"><strong>When possible, don't run a playbook on all your machines at the same time</strong>: Staged deployments can help you detect a problem before it's too late. There are many problems that are not detectable in a dev, test, staging, and qa environment. The majority of them are related to load that is hard to emulate properly in those non-production environments. A new configuration you have just added to your Apache HTTPd or MySQL servers could be perfectly OK from a syntax point of view, but disastrous for your specific application under your production load. A staged deployment will allow you to test your new configuration on your actual load without risking downtime if something was wrong.</li><li class="listitem" style="list-style-type: disc"><strong>Avoid guessing commands and modifiers</strong>: A lot of system administrators will try to remember the right parameter and try to guess if they don't remember it exactly. I've done it too, a lot of times, but this is very risky. Checking the man page or the online documentation will usually take you less than two minutes and often, by reading the manual, you'll find interesting notes you did not know. Guessing modifiers is dangerous because you could be fooled by a non-standard modifier (that is, <code class="literal">-v</code> is not the verbose mode for <code class="literal">grep</code> and <code class="literal">-h</code> is not the <code class="literal">help</code> command for the MySQL CLI).</li><li class="listitem" style="list-style-type: disc"><strong>Avoid error-prone commands</strong>: Not all commands have been created equally. Some commands are (way) more dangerous than others. If you can assume a <code class="literal">cat</code> command safe, you have to assume that a <code class="literal">dd</code> command is dangerous, since <code class="literal">dd</code> perform copies and conversion of files and volumes. I've seen people using <code class="literal">dd</code> in scripts to transform DOS files to UNIX (instead of <code class="literal">dos2unix</code>) and many other, very dangerous, examples. Please, avoid such commands, because they could result in a huge disaster if something goes wrong.</li><li class="listitem" style="list-style-type: disc"><strong>Avoid unnecessary modifiers</strong>: If you need to delete a simple file, use <code class="literal">rm ${file}</code> not <code class="literal">rm -rf ${file}</code>. The latter is often performed by users that have learned that; "to be sure, always use <code class="literal">rm -rf</code>", because at some time in their past, they have had to delete a folder. This will prevent you from deleting an entire folder if the <code class="literal">${file}</code> variable is set wrongly.</li><li class="listitem" style="list-style-type: disc"><strong>Always check what could happen if a variable is not set</strong>: If you want to delete the contents of a folder and you use the <code class="literal">rm -rf ${folder}/*</code> command, you are looking for trouble. If the <code class="literal">${folder}</code> variable is not set for some reason, the shell will read a <code class="literal">rm -rf /*</code> command, which is deadly (considering the fact that the <code class="literal">rm -rf /</code> command will not work on the majority of current OSes because it requires a <code class="literal">--no-preserve-root</code> option, while <code class="literal">rm -rf /*</code> will work as expected). I'm using this specific command as an example because I have seen such situations: the variable was pulled from a database which, due to some maintenance work, was down and an empty string was assigned to that variable. What happened next is probably easy to guess. In case you cannot prevent using variables in dangerous places, at least check them to see if they are not empty before using them. This will not save you from every problem but may catch some of the most common ones.</li><li class="listitem" style="list-style-type: disc"><strong>Double check your redirections</strong>: Redirections (along with pipes) are the most powerful elements of Linux shells. They could also be very dangerous: a <code class="literal">cat /dev/rand &gt; /dev/sda</code> command can destroy a disk even if a <code class="literal">cat</code> command is usually overlooked because it's not usually dangerous. Always double-check all commands that include a redirection.</li><li class="listitem" style="list-style-type: disc"><strong>Use specific modules wherever possible</strong>: In this list I've used shell commands because many people will try to use Ansible as if it's just a way to distribute them: it's not. Ansible provides a lot of modules and we'll see them in this book. They will help you create more readable, portable, and safe playbooks.</li></ul></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec9"/>Types of IT automation</h2></div></div></div><p>There are a lot of ways to classify IT automation systems, but by far the most important is related to how the configurations are propagated. Based on this, we can distinguish between agent-based systems and agent-less systems.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec2"/>Agent-based systems</h3></div></div></div><p>Agent-based systems have two different components: a <strong>server</strong> and a client called <strong>agent</strong>.</p><p>There is only one server and it contains all of the configuration for your whole environment, while the agents are as many as the machines in the environment.</p><div><div><h3 class="title"><a id="note3"/>Note</h3><p>In some cases, more than one server could be present to ensure high availability, but treat it as if it's a single server, since they will all be configured in the same way.</p></div></div><p>Periodically, client will contact the server to see if a new configuration for its machine is present. If a new configuration is present, the client will download it and apply it.</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec3"/>Agent-less systems</h3></div></div></div><p>In agent-less systems, no specific agent is present. Agent-less systems do not always respect the server/client paradigm, since it's possible to have multiple servers and even the same number of servers and clients . Communications are initialized by the server that will contact the client(s) using standard protocols (usually via SSH and PowerShell).</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec4"/>Agent-based versus Agent-less systems</h3></div></div></div><p>Aside from the differences outlined above, there are other contrasting factors which arise because of those differences.</p><p>From a security standpoint, an agent-based system can be less secure. Since all machines have to be able to initiate a connection to the server machine, this machine could be attacked more easily than in an agent-less case where the machine is usually behind a firewall that will not accept any incoming connections.</p><p>From a performance point of view, agent-based systems run the risk of having the server saturated and therefore the roll-out could be slower. It also needs to be considered that, in a pure agent-based system, it is not possible to force-push an update immediately to a set of machines. It will have to wait until those machines check-in. For this reason, multiple agent-based systems have implemented out-of-bands wait to implement such feature. Tools such as Chef and Puppet are agent-based but can also run without a centralized server to scale a large number of machines, commonly called <strong>Serverless Chef</strong> and <strong>Masterless Puppet</strong>, respectively.</p><p>An agent-less system is easier to integrate in an infrastructure that is already present, since it will be seen by the clients as a normal SSH connection and therefore no additional configuration is needed.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec8"/>What is Ansible?</h1></div></div></div><p>Ansible is an agent-less IT automation tool developed in 2012 by <em>Michael DeHaan</em>, a former Red Hat associate. The Ansible design goals are for it to be: minimal, consistent, secure, highly reliable, and easy to learn. The Ansible company has recently been bought out by Red Hat and now operates as part of Red Hat, Inc.</p><p>Ansible primarily runs in push mode using SSH, but you can also run Ansible using <code class="literal">ansible-pull</code>, where you can install Ansible on each agent, download the playbooks locally, and run them on individual machines. If there is a large number of machines (large is a relative term; in our view, greater than 500 and requiring parallel updates), and you plan to deploy updates to the machines in parallel, this might be the right way to go about it.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec9"/>Secure Shell (SSH)</h1></div></div></div><p>
<strong>Secure Shell</strong> (also known as <strong>SSH</strong>) is a network service that allows you to login and access a shell remotely in a fully encrypted connection. The SSH daemon is today, the standard for UNIX system administration, after having replaced the unencrypted telnet. The most frequently used implementation of the SSH protocol is OpenSSH.</p><p>In the last few months, Microsoft has shown an implementation (at the time of writing) of OpenSSH for Windows.</p><p>Since Ansible performs SSH connections and commands in the same way any other SSH client would do, no specific configuration has been applied to the OpenSSH server.</p><p>To speed up default SSH connections, you can always enable <code class="literal">ControlPersist</code> and the pipeline mode, which makes Ansible faster and secure.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Why Ansible?</h1></div></div></div><p>We will try and compare Ansible with Puppet and Chef during the course of this book since many people have good experience with those tools. We will also point out specifically how Ansible would solve a problem compared to Chef or Puppet.</p><p>Ansible, as well as Puppet and Chef, are declarative in nature and are expected to move a machine to the desired state specified in the configuration. For example, in each of these tools, in order to start a service at a point in time and start it automatically on restart, you would need to write a declarative block or module; every time the tool runs on the machine, it will aspire to obtain the state defined in your <strong>playbook</strong> (Ansible), <strong>cookbook</strong> (Chef), or <strong>manifest</strong> (Puppet).</p><p>The difference in the toolset is minimal at a simple level but as more situations arise and the complexity increases, you will start finding differences between the different toolsets. In Puppet, you need to take care of the order, and the Puppet server will create the sequence of instructions to execute every time you run it on a different box. To exploit the power of Chef, you will need a good Ruby team. Your team needs to be good at the Ruby language to customize both Puppet and Chef, and there will be a bigger learning curve with both of the tools.</p><p>With Ansible, the case is different. It uses the simplicity of Chef when it comes to the order of execution, the top-to-bottom approach, and allows you to define the end state in YAML format, which makes the code extremely readable and easy for everyone, from development teams to operations teams, to pick up and make changes. In many cases, even without Ansible, operations teams are given playbook manuals to execute instructions from, whenever they face issues. Ansible mimics that behavior. Do not be surprised if you end up having your project manager change the code in Ansible and check it into Git because of its simplicity!</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Installing Ansible</h1></div></div></div><p>Installing Ansible is rather quick and simple. You can use the source code directly, by cloning it from the GitHub project (<a class="ulink" href="https://github.com/ansible/ansible">https://github.com/ansible/ansible</a>), install it using your system's package manager, or use Python's package management tool (<strong>pip</strong>). You can use Ansible on any Windows, Mac, or UNIX-like system. Ansible doesn't require any databases and doesn't need any daemons running. This makes it easier to maintain Ansible versions and upgrade without any breaks.</p><p>We'd like to call the machine where we will install Ansible our Ansible workstation. Some people also refer to it as the command center.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Installing Ansible using the system's package manager</h2></div></div></div><p>It is possible to install Ansible using the system's package manager and in my opinion this is the preferred option if your system's package manager ships at least Ansible 2.0. We will look into installing Ansible via <strong>Yum</strong>, <strong>Apt</strong>, <strong>Homebrew</strong>, and <strong>pip</strong>.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec5"/>Installing via Yum</h3></div></div></div><p>If you are running a Fedora system you can install Ansible directly, since from Fedora 22, Ansible 2.0+ is available in the official repositories. You can install it as follows:</p><pre class="programlisting">
<strong>$ sudo dnf install ansible</strong>
</pre><p>For RHEL and RHEL-based (CentOS, Scientific Linux, Unbreakable Linux) systems, versions 6 and 7 have Ansible 2.0+ available in the EPEL repository, so you should ensure that you have the EPEL repository enabled before installing Ansible as follows:</p><pre class="programlisting">
<strong>$ sudo yum install ansible</strong>
</pre><div><div><h3 class="title"><a id="note4"/>Note</h3><p>On Cent 6 or RHEL 6, you have to run the command <code class="literal">rpm -Uvh</code>. Refer to <a class="ulink" href="http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm">http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</a> for instructions on how to install EPEL.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec6"/>Installing via Apt</h3></div></div></div><p>Ansible is available for Ubuntu and Debian. To install Ansible on those operating systems, use the following command:</p><pre class="programlisting">
<strong>$ sudo apt-get install ansible</strong>
</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec7"/>Installing via Homebrew</h3></div></div></div><p>You can install Ansible on Mac OS X using Homebrew, as follows:</p><pre class="programlisting">
<strong>$ brew update</strong>
<strong>$ brew install ansible</strong>
</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec8"/>Installing via pip</h3></div></div></div><p>You can install Ansible via pip. If you don't have pip installed on your system, install it. You can use pip to install Ansible on Windows too, using the following command line:</p><pre class="programlisting">
<strong>$ sudo easy_install pip</strong>
</pre><p>You can now install Ansible using <code class="literal">pip</code>, as follows:</p><pre class="programlisting">
<strong>$ sudo pip install ansible</strong>
</pre><p>Once you're done installing Ansible, run <code class="literal">ansible --version</code> to verify that it has been installed:</p><pre class="programlisting">
<strong>$ ansible --version</strong>
</pre><p>You will get the following output from the preceding command line:</p><pre class="programlisting">
<strong>ansible 2.0.2</strong>
</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec9"/>Installing Ansible from source</h3></div></div></div><p>In case the previous methods do not fit your use case, you can install Ansible directly from the source. Installing from source does not require any root permissions. Let's clone a repository and activate <code class="literal">virtualenv</code>, which is an isolated environment in Python where you can install packages without interfering with the system's Python packages. The command and the resulting output for the repository is as follows:</p><pre class="programlisting">
<strong>$ git clone git://github.com/ansible/ansible.git</strong>
<strong>Cloning into 'ansible'...</strong>
<strong>remote: Counting objects: 116403, done.</strong>
<strong>remote: Compressing objects: 100% (18/18), done.</strong>
<strong>remote: Total 116403 (delta 3), reused 0 (delta 0), pack-reused 116384</strong>
<strong>Receiving objects: 100% (116403/116403), 40.80 MiB | 844.00 KiB/s, done.</strong>
<strong>Resolving deltas: 100% (69450/69450), done.</strong>
<strong>Checking connectivity... done.</strong>
<strong>$ cd ansible/</strong>
<strong>$ source ./hacking/env-setup</strong>
<strong>Setting up Ansible to run out of checkout...</strong>
<strong>PATH=/home/vagrant/ansible/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/vagrant/bin</strong>
<strong>PYTHONPATH=/home/vagrant/ansible/lib:</strong>
<strong>MANPATH=/home/vagrant/ansible/docs/man:</strong>
<strong>Remember, you may wish to specify your host file with -i</strong>
<strong>Done!</strong>
</pre><p>Ansible needs a couple of Python packages, which you can install using <code class="literal">pip</code>. If you don't have pip installed on your system, install it using the following command. If you don't have <code class="literal">easy_install</code> installed, you can install it using Python's <code class="literal">setuptools</code> package on Red Hat systems, or by using Brew on the Mac:</p><pre class="programlisting">
<strong>$ sudo easy_install pip</strong>
<strong>&lt;A long output follows&gt;</strong>
</pre><p>Once you have installed <code class="literal">pip</code>, install the <code class="literal">paramiko</code>, <code class="literal">PyYAML</code>, <code class="literal">jinja2</code>, and <code class="literal">httplib2</code> packages using the following command lines:</p><pre class="programlisting">
<strong>$ sudo pip install paramiko PyYAML jinja2 httplib2</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade): paramiko in /usr/lib/python2.6/site-packages</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade): PyYAML in /usr/lib64/python2.6/site-packages</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade): jinja2 in /usr/lib/python2.6/site-packages</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade): httplib2 in /usr/lib/python2.6/site-packages</strong>
<strong>Downloading/unpacking markupsafe (from jinja2)</strong>
<strong>  Downloading MarkupSafe-0.23.tar.gz</strong>
<strong>  Running setup.py (path:/tmp/pip_build_root/markupsafe/setup.py) egg_info for package markupsafe</strong>
<strong>Installing collected packages: markupsafe</strong>
<strong>  Running setup.py install for markupsafe</strong>
<strong>    building 'markupsafe._speedups' extension</strong>
<strong>    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python2.6 -c markupsafe/_speedups.c -o build/temp.linux-x86_64-2.6/markupsafe/_speedups.o</strong>
<strong>    gcc -pthread -shared build/temp.linux-x86_64-2.6/markupsafe/_speedups.o -L/usr/lib64 -lpython2.6 -o build/lib.linux-x86_64-2.6/markupsafe/_speedups.so</strong>
<strong>Successfully installed markupsafe</strong>
<strong>Cleaning up...</strong>
</pre><div><div><h3 class="title"><a id="note5"/>Note</h3><p>By default, Ansible will be running against the development branch. You might want to check out the latest stable branch. Check what the latest stable version is using the following command line:</p><p>
<strong>$ git branch -a</strong>
</p></div></div><p>Copy the latest version you want to use. Version 2.0.2 was the latest version available at the time of writing. Check the latest version using the following command lines:</p><pre class="programlisting">
<strong>[node ansible]$ git checkout v2.0.2</strong>
<strong>Note: checking out 'v2.0.2'.</strong>
<strong>[node ansible]$ ansible --version</strong>
<strong>ansible 2.0.2 (v2.0.2 268e72318f) last updated 2014/09/28 21:27:25 (GMT +000)</strong>
</pre><p>You now have a working setup of Ansible ready. One of the benefits of running Ansible from source is that you can enjoy the new features immediately, without waiting for your package manager to make them available for you.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Creating a test environment with QEMU and KVM</h1></div></div></div><p>To be able to learn Ansible, we will need to make quite a few playbooks and run them.</p><div><div><h3 class="title"><a id="tip6"/>Tip</h3><p>Doing it directly on your computer will be very risky. For this reason, I would suggest using virtual machines.</p></div></div><p>It's possible to create a test environment with cloud providers in a few seconds, but often it is more useful to have those machines locally. To do so, we will use <strong>Kernel-based Virtual Machine</strong> (<strong>KVM</strong>) with <strong>Quick Emulator</strong> (<strong>QEMU</strong>).</p><p>The first thing will be installing <code class="literal">qemu-kvm</code> and <code class="literal">virt-install</code>. On Fedora it will be enough to run:</p><pre class="programlisting">
<strong>$ sudo dnf install -y @virtualization</strong>
</pre><p>On Red Hat/CentOS/Scientific Linux/Unbreakable Linux it will be enough to run:</p><pre class="programlisting">
<strong>$ sudo yum install -y qemu-kvm virt-install virt-manager</strong>
</pre><p>If you use Ubuntu, you can install it using:</p><pre class="programlisting">
<strong>$ sudo apt install virt-manager</strong>
</pre><p>On Debian, you'll need to execute:</p><pre class="programlisting">
<strong>$ sudo apt install qemu-kvm libvirt-bin</strong>
</pre><p>For our examples, I'll be using CentOS 7. This is for multiple reasons; the main ones are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CentOS is free and 100% compatible with Red Hat, Scientific Linux, and Unbreakable Linux</li><li class="listitem" style="list-style-type: disc">Many companies use Red Hat/CentOS/Scientific Linux/Unbreakable Linux for their servers</li><li class="listitem" style="list-style-type: disc">Those distributions are the only ones with SELinux support built in, and as we have seen earlier, SELinux can help you make your environment much more secure</li></ul></div><p>At the time of writing this book, the most recent CentOS cloud image is <a class="ulink" href="http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1603.qcow2">http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1603.qcow2</a>, So let's download this image with the help of the following command:</p><pre class="programlisting">
<strong>$ wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1603.qcow2</strong>
</pre><p>Since we will probably need to create many machines, it's better if we create a copy of it so the original one will not be modified:</p><pre class="programlisting">
<strong>$ cp CentOS-7-x86_64-GenericCloud-1603.qcow2 centos_1.qcow2</strong>
</pre><p>Since the <code class="literal">qcow2</code> images will run <code class="literal">cloud-init</code> to set up the networking, users, and so on, we will need to provide a couple of files. Let's start by creating a metadata file for networking:</p><pre class="programlisting">instance-id: centos_1 &#13;
local-hostname: centos_1.local &#13;
network-interfaces: | &#13;
  iface eth0 inet static &#13;
  address (An IP in your virtual bridge class) &#13;
  network (The first IP of the virtual bridge class) &#13;
  netmask (Your virtual bridge class netmask) &#13;
  broadcast (Your virtual bridge class broadcast) &#13;
  gateway (Your virtual bridge class gateway) &#13;
</pre><p>To find your virtual bridge data, you have to look for a device that has the name <code class="literal">virbrX</code> or something similar, in my case it is <code class="literal">virtbr0</code>, so I can find all of its information using the following command:</p><pre class="programlisting">
<strong>$ ip addr show virbr0</strong>
</pre><p>The previous command will give this as an output:</p><pre class="programlisting">
<strong>5: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000</strong>
<strong>    link/ether 52:54:00:38:1a:e6 brd ff:ff:ff:ff:ff:ff</strong>
<strong>    inet 192.168.124.1/24 brd 192.168.124.255 scope global virbr0</strong>
<strong>       valid_lft forever preferred_lft forever</strong>
</pre><p>So, for me the meta-data file looks like the following:</p><pre class="programlisting">instance-id: centos_1 &#13;
local-hostname: centos_1.local &#13;
network-interfaces: | &#13;
  iface eth0 inet static &#13;
  address 192.168.124.10 &#13;
  network 192.168.124.1 &#13;
  netmask 255.255.255.0 &#13;
  broadcast 192.168.124.255 &#13;
  gateway 192.168.124.1 &#13;
</pre><p>This file will set up the <code class="literal">eth0</code> interface of the virtual machine at boot time. We also need another file (user-data) to set up the <code class="literal">users</code> properly:</p><pre class="programlisting">users: &#13;
- name: (yourname) &#13;
  shell: /bin/bash &#13;
  sudo: ['ALL=(ALL) NOPASSWD:ALL'] &#13;
  ssh-authorized-keys: &#13;
  - (insert ssh public key here) &#13;
</pre><p>For me, the file looks like the following:</p><pre class="programlisting">users: &#13;
- name: fale &#13;
  shell: /bin/bash &#13;
  sudo: ['ALL=(ALL) NOPASSWD:ALL'] &#13;
  ssh-authorized-keys: &#13;
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDRoZzfNif+wXFqzsmvHg4jJt8+ZO/dQxm5k7pXYAwdWVbiFrZYGhMQl5FPfzC7rkDaC31fod3Y85QkQVgNKCVYUy5QR5LfxUjSQDv+y2Nfao4be/BKla0ffc7JVSzFFAELGGDLn1lMN0e0D9syqQbKgSRdOdvweq/0Et3KNIF9e7XgEdSuAHls17NDtMkWUfyi5yvEtdtMcp9gO4OlG6Vh0iCXOdx+f0QA2hh1JnvePvzJ4a8CeckN5JwL7Q027nlsHPBYq9K1jvv+diUs48FflPJI4fgMq3Zo7zyCpf8qE7Dlx+u7OvR5kxNdrpnOsDgHeAGNkrzfcmxU7kbU29NX4VFgWd0sdlzu1nOWFEH7Cnd547tx5VFxBzJwEAUCh7QSiU2Ne/hCnjFkZuDZ5pN4pNw+yu+Feoz79gV/utoLHuCodYyAvSQlQ7VSfC+djLD/9wHC2yGksvc9ICnSUv3JyQEEEG4K26z6szF9+a3vU0qIq7YYa8QHgWIHtzSxztYRIWJOzTZlwyuNmhbRNYDaMC5BMzvQ8JREv0obMLmrlvolJPWT4gn1N9sDNNXIC6RDRE5yGsIEf0CliYW1X/8XG40U+g9LG+lrYOGWD4OymZ2P/VDIzZbVT6NG/rdSSGnf4D1AwlOGR7eNTv30AK9o0LVjqGaJWKWYUF9zY6I3+Q== &#13;
</pre><p>To provide those files at boot time, we will need to create an ISO file containing them:</p><pre class="programlisting">
<strong>$ genisoimage -output centos_1.iso -volid cidata -joliet -rock user-data meta-data</strong>
</pre><p>After the ISO file is ready, we can instruct <code class="literal">virt-install</code> to actually create the virtual machine:</p><pre class="programlisting">virt-install --name CentOS_1 \ &#13;
--ram 2048 \ &#13;
--disk centos_1.qcow2 \ &#13;
--vcpus 2 \ &#13;
--os-variant fedora21 \ &#13;
--connect qemu:///system \ &#13;
--network bridge:br0,model=virtio \ &#13;
--cdrom centos_1.iso \ &#13;
--boot hd &#13;
virt-install --name CentOS_1 \ --ram 2048 \ --disk centos_1.qcow2 \ --vcpus 2 \ --os-variant fedora21 \ --connect qemu:///system \ --network bridge:br0,model=virtio \ --cdrom centos_1.iso \ --boot hd &#13;
</pre><p>Since our network configuration is in the ISO file, we will need it at every boot. Sadly, by default this does not happen, so we will need to do a few more steps. Firstly, run <code class="literal">virsh</code>:</p><pre class="programlisting">
<strong>$ virsh</strong>
</pre><p>At this point, a <code class="literal">virsh</code> shell should appear with an output like the following:</p><pre class="programlisting">
<strong>Welcome to virsh, the virtualization interactive terminal.</strong>
<strong>Type:  'help' for help with commands</strong>
<strong>       'quit' to quit</strong>
<strong>virsh #</strong>
</pre><p>This means that we switched from bash (or your shell, if you are not using bash) to the virtualization shell. Issue the following command:</p><pre class="programlisting">
<strong>virsh # edit CentOS_1</strong>
</pre><p>By doing this we will be able to tweak the configuration of the <code class="literal">CentOS_1</code> machine. In the disk section, you'll need to find the <code class="literal">cdrom</code> device that should look like this:</p><pre class="programlisting">    &lt;disk type='block' device='cdrom'&gt; &#13;
      &lt;driver name='qemu' type='raw'/&gt; &#13;
      &lt;target dev='hda' bus='ide'/&gt; &#13;
      &lt;readonly/&gt; &#13;
      &lt;address type='drive' controller='0' bus='0' target='0'&#13;
      unit='0'/&gt; &#13;
    &lt;/disk&gt; &#13;
</pre><p>You'll need to change it to the following as highlighted in bold:</p><pre class="programlisting">    &lt;disk type='file' device='cdrom'&gt; &#13;
      &lt;driver name='qemu' type='raw'/&gt; &#13;
        &lt;source file='(Put here your ISO path)/centos_1.iso'/&gt; &#13;
      &lt;target dev='hda' bus='ide'/&gt; &#13;
      &lt;readonly/&gt; &#13;
      &lt;address type='drive' controller='0' bus='0' target='0'&#13;
      unit='0'/&gt; &#13;
    &lt;/disk&gt; &#13;
</pre><p>At this point, our virtual machine will always start with the ISO file mounted as a <code class="literal">cdrom</code> and therefore <code class="literal">cloud-init</code> will be able to correctly initiate the networking.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Version control system</h1></div></div></div><p>In this chapter, we have already encountered the expression <em>infrastructure code</em> to describe the Ansible code that will create and maintain your infrastructure. We use the expression infrastructure code to distinguish it from the application code, which is the code that composes your applications, websites, and so on. This distinction is needed for clarity, but in the end, both types are a bunch of text files that the software will be able to read and interpret.</p><p>For this reason, a version control system will help you a lot. Its main advantages are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ability to have multiple people working simultaneously on the same project.</li><li class="listitem" style="list-style-type: disc">Ability to perform code reviews in a simple way.</li><li class="listitem" style="list-style-type: disc">Ability to have multiple branches for multiple environments (that is, dev, test, qa, staging, and production).</li><li class="listitem" style="list-style-type: disc">Ability to track a change so we know when it was introduced, and who introduced it. This makes it easier to understand why that piece of code is there, years (or months) later.</li></ul></div><p>Those advantages are provided to you by the majority of version control systems out there.</p><p>Version control systems can be divided into three major groups based on the three different models that they can implement:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Local data model</li><li class="listitem" style="list-style-type: disc">Client-server model</li><li class="listitem" style="list-style-type: disc">Distributed model</li></ul></div><p>The first category, the local data model, is the oldest (circa 1972) approach and is used for very specific use cases. This model requires all users to share the same filesystem. Famous examples of it are the <strong>Revision Control System</strong> (<strong>RCS</strong>) and <strong>Source Code Control System</strong> (<strong>SCCS</strong>).</p><p>The second category, the client-server model, arrived later (circa 1990) and tried to solve the limitations of the local data model, creating a server that respected the local data model and a set of clients that dealt with the server instead of with the repository itself. This additional layer allowed multiple developers to use local files and synchronize them with a centralized server. Famous examples of this approach are Apache <strong>Subversion</strong> (<strong>SVN</strong>), and <strong>Concurrent Versions System</strong> (<strong>CVS</strong>).</p><p>The third category, the distributed model, arrived at the beginning of the twenty-first century and tried to solve the limitations of the client-server model. In fact, in the client-server mode, you could work on the code offline, but you needed to be <em>online</em> to commit the changes. The distributed model allows you to handle everything on your local repository (like the local data model), and to merge different repositories on different machines in an easy way. In this new model, it's possible to perform all actions as in the client-server model, with the added benefits of being able to work completely offline as well as the ability to merge changes between peers without passing by the centralized server. Examples of this model are BitKeeper (proprietary software), Git, GNU Bazaar, and Mercurial.</p><p>There are some additional advantages that will be provided by only the distributed model, such as:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Possibility of making commits, browsing history, and performing any other action even if the server is not available</li><li class="listitem" style="list-style-type: disc">Easier management of multiple branches for different environments</li></ul></div><p>When it comes to infrastructure code, we have to consider that, frequently, the infrastructure that retains and manages your infrastructure code is kept in the infrastructure code itself. This is a recursive situation that can create problems. In fact, until you have your code server in place you cannot deploy your Ansible, and until you have your Ansible in place, you cannot deploy your code server. A distributed version control system will prevent this problem.</p><p>As for the simplicity of managing multiple branches, even if this is not a hard rule, often distributed version control systems have much better merge handling than the other kinds of version control systems.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Using Ansible with Git</h1></div></div></div><p>For the reasons that we have just seen and because of its huge popularity, I suggest always using Git for your Ansible repositories.</p><p>There are a few suggestions that I always provide to the people I talk to, so Ansible gets the best out of Git:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Create environment branches</strong>: Creating environment branches such as dev, prod, test, and stg, will allow you to easily keep track of the different environments and their respective update statuses. I often suggest keeping the master branch for the development environment, since I find many people are used to pushing new changes directly to the master. If you use a master for a production environment, people can inadvertently push changes in the production environment while they wanted to push them in a development environment.</li><li class="listitem" style="list-style-type: disc"><strong>Always keep environment branches stable</strong>: One of the big advantages of having environment branches is the possibility of destroying and recreating any environment from scratch at any given moment. This is only possible if your environment branches are in a stable (not broken) state.</li><li class="listitem" style="list-style-type: disc"><strong>Use feature branches</strong>: Using different branches for specific long-development features (such as a refactor or some other big changes) will allow you to keep your day-to-day operations while your new feature is in the Git repository (so you'll not lose track of who did what and when they did it).</li><li class="listitem" style="list-style-type: disc"><strong>Push often</strong>: I always suggest that people <em>push commits</em> as often as possible. This will make Git work as both a version control system and a backup system. I have seen laptops broken, lost, or stolen with days or weeks of unpushed work on them far too often. Don't waste your time, push often. Also, by pushing often, you'll detect merge conflicts sooner, and conflicts are always easier to handle when they are detected early, instead of waiting for multiple changes.</li><li class="listitem" style="list-style-type: disc"><strong>Always deploy after you have made a change</strong>: I have seen times when a developer has created a change in the infrastructure code, tested in the dev and test environments, pushed to the production branch, and then went to have lunch before deploying the changes in production. His lunch did not end well. One of his colleagues deployed the code to production inadvertently (he was trying to deploy a small change he had made in the meantime) and was not prepared to handle the other developer's deployment. The production infrastructure broke and they lost a lot of time figuring out how it was possible that such a small change (the one the person who made the deployment was aware of) created such a big mess.</li><li class="listitem" style="list-style-type: disc"><strong>Choose multiple small changes rather than a few huge changes</strong>: Making small changes, whenever possible, will make debugging easier. Debugging an infrastructure is not very easy. There is no compiler that will allow you to see <em>obvious problems</em> (even though Ansible performs a syntax check of your code, no other test is performed), and the tools for finding something that is broken are not always as good as you would imagine. The infrastructure as a code paradigm is new and tools are not yet as good as the ones for the application code.</li><li class="listitem" style="list-style-type: disc"><strong>Avoid binary files as much as possible</strong>: I always suggest keeping your binaries outside your Git repository, whether it is an application code repository or an infrastructure code repository. In the application code example, I think it is important to keep your repository light (Git as well as the majority of the version control systems, do not perform very well with binary blobs), while for the infrastructure code example, it is vital because you'll be tempted to put a huge number of binary blobs in it, since very often it is easier to put a binary blob in the repository than to find a cleaner (and better) solution.</li></ul></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Summary</h1></div></div></div><p>In this chapter, we have seen what IT automation is, it's advantages, disadvantages, what kind of tools you can find, and how Ansible fits into this big picture. We have also seen how to install Ansible and how to create a KVM-based virtual machine. In the end, we analyzed the version control systems and spoke about the advantages Git brings to Ansible if used properly.</p><p>In the next chapter, we will start looking at the infrastructure code that we mentioned in this chapter without explaining exactly what it is and how to write it. Also in the next chapter, we'll see how to automate simple operations that you probably perform every single day, such as managing users, managing files, and file content.</p></div></body></html>