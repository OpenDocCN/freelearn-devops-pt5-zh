<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift HA Architecture Overview</h1>
                
            
            <article>
                
<p class="calibre2">In the previous chapter,<span class="calibre11"> we introduced you to CI/CD, Jenkins, OpenShift pipelines, and Jenkins integration with OpenShift. We also illustrated how to create a sample CI/CD pipeline in OpenShift, how to edit pipelines, and how to manage pipeline execution.</span><span class="calibre11"> </span></p>
<p class="calibre2">In this chapter, we will briefly touch on <strong class="calibre4">high availability</strong> (<strong class="calibre4">HA</strong>) in general, and will then focus on OpenShift HA. We will discuss how OpenShift provides redundancy in the case of a failure, and how you can prevent this from happening by properly designing your OpenShift cluster. At the end of this chapter, we will discuss how to back up and restore OpenShift cluster data in case something goes wrong.  </p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<p class="calibre2"/>
<ul class="calibre9">
<li class="calibre10"><span>What is high availability?</span></li>
<li class="calibre10"><span>HA in OpenShift</span></li>
<li class="calibre10">OpenShift backup and restore</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">What is high availability?</h1>
                
            
            <article>
                
<p class="calibre2">HA is a very important topic when it comes to real customers and real money. We all work for different businesses, and the last thing that any business wants is to have an outage. This is a problem that can cause people to lose their jobs and companies to go bankrupt. It has happened, and it will continue to happen. But if you plan your HA design properly and implement it in the right way, you will have a better chance of keeping your job and maintaining your company's good reputation.</p>
<p class="calibre2">HA usually refers to a concept or strategy to keep a system up and running for a long time. That's where the terms <em class="calibre17">high</em> and <em class="calibre17">availability</em> come together. When people ask, <em class="calibre17">Does it support HA?</em>, they are usually asking whether the system is <span class="calibre11">redundant, and whether it stays up and running if something goes wrong</span><span class="calibre11">. In order to provide HA, each and every component of the system needs to be fault tolerant, and all of the lower- and upper-level components and protocols must be highly available. For example, if you have OpenShift designed and implemented in HA mode, but your network has a single point of failure, your application will stop working. </span><span class="calibre11">So, it is critical to plan properly, and to make sure that your application stays up and running, no matter where a failure occurs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">HA in OpenShift</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre11">In the previous chapters, we ran our applications on a single node, or sometimes, two nodes. Some might say that if there is more than one OpenShift node in the cluster, it is considered a redundant configuration, but that is far from true.</span></p>
<p class="calibre2"><span class="calibre11">If we compare standard OpenShift architecture and OpenShift HA, you will see some differences between them:</span></p>
<p class="cdpaligncenter"><img class="alignnone31" src="../images/00090.jpeg"/></p>
<div class="cdpaligncenter1">OpenShift classic architecture</div>
<p class="calibre2">Here we have nodes, masters, storage, and a routing layer consisting of infra nodes. OpenShift HA architecture is quite similar but has one distinct difference—in the routing layer we have load balances that make the overall solution always accessible. All other components are redundant by nature:</p>
<p class="cdpaligncenter"><img class="alignnone67" src="../images/00091.jpeg"/></p>
<div class="cdpaligncenter1">OpenShift HA architecture</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Virtual IPs</h1>
                
            
            <article>
                
<p class="calibre2">We can see that in OpenShift HA, we have what's called an enterprise load balancer, with two <strong class="calibre4">Virtual IPs</strong> (<strong class="calibre4">VIPs</strong>). We need one VIP for traffic to master nodes, and another VIP for traffic to actual OpenShift applications, running on OpenShift nodes within pods:</p>
<p class="cdpaligncenter"><img class="alignnone68" src="../images/00092.jpeg"/></p>
<div class="cdpaligncenter1"><span>OpenShift with external load balancers</span></div>
<p class="calibre2">Why can't we just use DNS load balancing? The reason is, if we use DNS load balancing and one of the masters or nodes goes down, some traffic will still keep flowing to the failed node. Load balancing allows for implementing health checks and stops routing the traffic to the failed endpoint. For example, if one of the infra nodes fails, the load balancer will detect the failure, remove that node from the server pool, and stop sending traffic to the node. When the node comes back up, the load balancer will detect that, and will start load balancing traffic to the node. So, having VIPs is essential for OpenShift to be highly available from the outside.</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><strong class="calibre4">Node name</strong></p>
</td>
<td class="calibre47">
<p class="calibre2"><strong class="calibre4">Physical IP address</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Virtual IP address</strong></p>
</td>
<td class="calibre49">
<p class="calibre2"><strong class="calibre4">DNS</strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><kbd class="calibre12">Infra1</kbd></p>
</td>
<td class="calibre47">
<p class="calibre2"><kbd class="calibre12">10.0.0.1/24</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">10.0.0.11</kbd></p>
</td>
<td class="calibre49">
<p class="calibre2"><kbd class="calibre12">*.apps.osp.com</kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><kbd class="calibre12">Infra2</kbd></p>
</td>
<td class="calibre47">
<p class="calibre2"><kbd class="calibre12">10.0.0.2/24</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">10.0.0.11</kbd></p>
</td>
<td class="calibre49">
<p class="calibre2"><kbd class="calibre12"><span>*.apps.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><kbd class="calibre12">Infra3</kbd></p>
</td>
<td class="calibre47">
<p class="calibre2"><kbd class="calibre12">10.0.0.3/24</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">10.0.0.11</kbd></p>
</td>
<td class="calibre49">
<p class="calibre2"><kbd class="calibre12"><span>*.apps.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><kbd class="calibre12">Master1</kbd></p>
</td>
<td class="calibre47">
<p class="calibre2"><kbd class="calibre12">10.0.1.4/24</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">10.0.0.14</kbd></p>
</td>
<td class="calibre49">
<p class="calibre2"><kbd class="calibre12"><span>console.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><kbd class="calibre12">Master2</kbd></p>
</td>
<td class="calibre47">
<p class="calibre2"><kbd class="calibre12">10.0.1.5/24</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">10.0.0.14</kbd></p>
</td>
<td class="calibre49">
<p class="calibre2"><kbd class="calibre12"><span>console.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre46">
<p class="calibre2"><kbd class="calibre12">Master3</kbd></p>
</td>
<td class="calibre47">
<p class="calibre2"><kbd class="calibre12">10.0.1.6/24</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">10.0.0.14</kbd></p>
</td>
<td class="calibre49">
<p class="calibre2"><kbd class="calibre12"><span>console.osp.com<br class="title-page-name"/></span></kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">Using an external load balancer is an ideal option when building OpenShift HA, because an external load balancer automatically detects a failure of any OpenShift infra or master node and distributes the load among the other nodes available.</p>
<p class="calibre2">Let's suppose that we have three infra nodes, all serving the traffic at a speed of 50 Mbps. If the <kbd class="calibre12">Infra1</kbd> node fails, then the external load balancer automatically detects the failure and stops serving traffic to the <kbd class="calibre12">Infra1</kbd> node. So, there will be no downtime for both end users and applications, and the load balancer will automatically distribute the load between <kbd class="calibre12">Infra2</kbd> and <kbd class="calibre12">Infra3</kbd>, so both nodes will end up serving the traffic at a speed of 75 Mbps. </p>
<p class="calibre2">The downside of this scenario is that we have to use external load balancers, take care of their HA, implement additional health checks, and employ further configurations. And, if we are using commercial load balancer appliances from F5 or A10, they are going to be very expensive, as well. However, this is the most scalable solution that makes sure that OpenShift cluster is always accessible from the outside. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">IP failover</h1>
                
            
            <article>
                
<p class="calibre2">Another way to ensure that your OpenShift applications are always available from the outside is to implement IP failover mechanisms. This method is useful when you do not have an external load balancer, but still want OpenShift to always be accessible from the outside. The OpenShift IP failover design primarily relies on two different technologies:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Keepalived</strong>: Provides high availability of VIPs across OpenShift infra nodes.</li>
<li class="calibre10"><strong class="calibre1">DNS</strong>: Manages external traffic load balancing.</li>
</ul>
<p class="cdpaligncenter"><img class="alignnone69" src="../images/00093.jpeg"/></p>
<div class="cdpaligncenter1">OpenShift DNS and keepalived</div>
<p class="calibre2">In our example, keepalived separately manages several VIPs on master and infrastructure nodes. Two DNS mappings are used to load balance the traffic between the VIPs of the OpenShift infra and master nodes:</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">Node name</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">Physical IP address</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">Virtual IP address</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">DNS</strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">Infra1</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">10.0.0.1/24</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">10.0.0.11</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">*.apps.osp.com</kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">Infra2</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">10.0.0.2/24</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">10.0.0.12</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12"><span>*.apps.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">Infra3</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">10.0.0.3/24</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">10.0.0.13</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12"><span>*.apps.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">Master1</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">20.0.0.1/24</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">20.0.0.11</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12"><span>console.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">Master2</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">20.0.0.2/24</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">20.0.0.12</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12"><span>console.osp.com</span></kbd></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">Master3</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">20.0.0.3/24</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12">20.0.0.13</kbd></p>
</td>
<td class="calibre25">
<p class="calibre2"><kbd class="calibre12"><span>console.osp.com</span></kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">In the preceding example, we do not need an external load balancer, and, if one of the OpenShift nodes goes down, the Virtual IP will automatically be moved to another node. Depending on how we configure preemptive options, the Virtual IP may come back to an infra node if a failed info node recovers.</p>
<p class="calibre2"/>
<p class="calibre2">There is a downside to this solution, which may or may not be critical for your particular case. Let's suppose that we have three infra nodes, all serving the traffic at a speed of 50 Mbps. If one of the nodes fails, then the VIP from <kbd class="calibre12">Infra1</kbd> will be moved to <kbd class="calibre12">Infra2</kbd>. There will be no interruption for end users or applications, but <kbd class="calibre12">Infra2</kbd> will now serve the traffic at a speed of 100 Mbps, while <kbd class="calibre12">Infra3</kbd> is still doing it at 50 Mbps. </p>
<p class="calibre2">This is OK when the workload is not too high, but if there is too much traffic, it may cause issues by overloading <kbd class="calibre12">Infra2</kbd>. So, you have to vet out all possible scenarios; by solving one particular failure scenario, you may create a new one.</p>
<div class="packt_infobox">There are other methods for making your OpenShift cluster available externally. Discussing methods like DNS LB, GSLB, custom scripts, or even manual switchover, could easily extend this book by a thousand pages. We have focused on the methods that are proven to work and are supported by OpenShift.   </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift infrastructure nodes  </h1>
                
            
            <article>
                
<p class="calibre2">OpenShift infrastructure nodes are labeled with infra, by default. They run on two main OpenShift components:</p>
<ul class="calibre9">
<li class="calibre10">OpenShift internal registry</li>
<li class="calibre10">OpenShift router</li>
</ul>
<p class="calibre2">An Openshift infrastructure node is the easiest to install, operate, and troubleshoot, because the structure of an OpenShift infra node is simple, stable, and predictable. Infra nodes are usually installed in HA mode as a part of the initial OpenShift installation; they are rarely modified. The only time you might work with infra nodes directly is when you have a lot of traffic going through infra nodes and they can't handle it.</p>
<p class="calibre2">But, by the time you run into a situation like that, you will have much bigger problems than just scaling the number of infra nodes:</p>
<p class="cdpaligncenter"><img class="alignnone70" src="../images/00094.jpeg"/></p>
<div class="cdpaligncenter1">OpenShift infra nodes</div>
<p class="calibre2">The best practice is to have a minimum of three infra nodes <span class="calibre11">installed, </span>with the pod anti-affinity feature enabled for both the registry and router. You need to have anti-affinity enabled, because you can run into a situation where you lose one infrastructure node and the new router pod start a on the node that is already running an OpenShift router. So if you have only two infra nodes, without the pod anti-affinity feature enabled, in the case of a failure, you will have two routers and two registries running on the same infra node, listening on the same ports. The pod anti-affinity rule prevents one pod from running with another pod on the same host, thus preventing two registries (or two routers) from running on the same OpenShift infra node. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift masters </h1>
                
            
            <article>
                
<p class="calibre2">OpenShift masters are control plane nodes and the central control points for OpenShift solutions. In earlier releases, you had to install an OpenShift master using Pacemaker (to provide failover), but in recent releases, this is taken care of by keepalived and external storage. If you ever have an OpenShift master fail completely, you can just delete the node and reinstall it.</p>
<p class="calibre2">In order to remove a node from your OpenShift cluster, you can use the <kbd class="calibre12">oc delete node</kbd> command, and then run <kbd class="calibre12">scaleup.yml</kbd> from the <kbd class="calibre12">openshift-ansible</kbd> Git project.</p>
<div class="packt_infobox">The <kbd class="calibre26">openshift-ansible</kbd><span> </span>project is available at <a href="https://github.com/openshift/openshift-ansible" class="calibre6">https://github.com/openshift/openshift-ansible</a>.<br class="title-page-name"/>
<br class="title-page-name"/>
The <kbd class="calibre26">scaleup.yml</kbd><span> </span>file is located in <kbd class="calibre26">openshift-ansible/playbooks/byo/openshift-node/scaleup.yml</kbd>, once you have downloaded the <kbd class="calibre26">openshift-ansible</kbd> project.<br class="title-page-name"/>
<br class="title-page-name"/>
You will be required to adjust your Ansible inventory file and add a new node under the <kbd class="calibre26">[new_masters]</kbd> section.</div>
<p class="calibre2">If, at some point, you lose all OpenShift masters, it will not impact your end users, and customer-to-application traffic will keep flowing; however, you won't be able to make any new changes to the OpenShift cluster. At that point, there is not much that you can do, other than restore OpenShift masters from the last backup. We will discuss OpenShift backup and restore later in this chapter.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift etcd</h1>
                
            
            <article>
                
<p class="calibre2">The OpenShift etcd key-value store is the most critical and sensitive OpenShift component, because all OpenShift master persistent data is kept in the etcd cluster. The good news is that etcd itself works in active/active configuration, and is installed during the initial installation. You will need to properly design your etcd cluster, so that you do not run into a situation where you are required to reinstall your etcd in order to handle a greater load. There is a general recommendation to install and configure your etcd cluster on dedicated nodes, separate from OpenShift masters, in a quantity of three, five, or seven members.</p>
<p class="calibre2">OpenShift keeps all configuration data in the etcd key-value store, so it is very important to regularly back up your etcd—at some point you will be required to restore it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift nodes  </h1>
                
            
            <article>
                
<p class="calibre2">OpenShift nodes are the easiest to work with, when it comes to HA. Since OpenShift pods are stateless by nature, we do not need to directly take care of high availability of OpenShift nodes; we just need to make sure that the application pods are running on different OpenShift nodes, so that if an OpenShift node goes down, there is no downtime for the end user and a new application pod is brought up by the replication controller. If you ever have an OpenShift node fail completely, you can just delete that node and reinstall it.</p>
<p class="calibre2">In order to remove a node from your OpenShift cluster, you can use the <kbd class="calibre12">oc delete node</kbd> command, and then run <kbd class="calibre12">scaleup.yml</kbd> from the <kbd class="calibre12">openshift-ansible</kbd> Git project.</p>
<div class="packt_infobox">The <kbd class="calibre26">openshift-ansible</kbd> project is available at <a href="https://github.com/openshift/openshift-ansible" class="calibre6">https://github.com/openshift/openshift-ansible</a>.<br class="title-page-name"/>
<br class="title-page-name"/>
The <kbd class="calibre26">scaleup.yml</kbd> file is located in <kbd class="calibre26">openshift-ansible/playbooks/byo/openshift-node/scaleup.yml</kbd>, once you have downloaded the <kbd class="calibre26">openshift-ansible</kbd> project.<br class="title-page-name"/>
<br class="title-page-name"/>
You will be required to adjust your Ansible inventory file and add a new node, under the <kbd class="calibre26">[new_nodes]</kbd> section.</div>
<p class="calibre2">There is no need to back up any data on an OpenShift node since there is no stateful data located on a node. In most cases, you will want to delete an OpenShift node from an OpenShift cluster, reinstall it, and bring it back, new and fresh.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">External storage for OpenShift persistent data</h1>
                
            
            <article>
                
<p class="calibre2">The external storage configuration <span class="calibre11">and design </span>for <span class="calibre11">OpenShift persistent data</span> is out of the scope of this book, but some general advice is to make sure that you have your external storage available in a redundant and scalable fashion, <span class="calibre11">meaning that if one or several components fail, it will not affect overall storage performance and will always be accessible by OpenShift. </span></p>
<p class="calibre2">You will need to take care of regular external storage back up and restore procedures <span class="calibre11">separately, and you will need to have a tested and verified procedure for if you lose persistent storage data.</span></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift backup and restore</h1>
                
            
            <article>
                
<p class="calibre2">No matter what you do, there will be times when something goes wrong with your OpenShift cluster, and some (or all) data is lost. That's why you need to know when and how to make OpenShift backups and how to bring OpenShift back to an operational state. The OpenShift installation procedure includes the following components that you will need to back up:</p>
<ul class="calibre9">
<li class="calibre10">Etcd key-value store data </li>
<li class="calibre10">Master configuration data </li>
<li class="calibre10">Ansible host installation playbooks</li>
<li class="calibre10">Pod data</li>
<li class="calibre10">Registry data </li>
<li class="calibre10">Project configuration data</li>
<li class="calibre10">Additionally installed software</li>
</ul>
<p class="calibre2">Depending on the failure situation, you may need to either reinstall the whole OpenShift cluster or reinstall some components separately. In most cases, you will be required to completely reinstall the OpenShift cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Etcd key-value store backup</h1>
                
            
            <article>
                
<p class="calibre2">The etcd backup procedure can be <span class="calibre11">performed on any etcd node, and </span>consists of the following steps:</p>
<ol class="calibre13">
<li value="1" class="calibre10">Stop the etcd service: <kbd class="calibre12">systemctl stop etcd</kbd></li>
<li value="2" class="calibre10">Create an etcd backup: <kbd class="calibre12">etcdctl backup --data-dir /var/lib/etcd --backup-dir ~/etcd.back</kbd></li>
<li value="3" class="calibre10">Copy the etcd <kbd class="calibre12">db</kbd> file: <kbd class="calibre12">cp <span>/var/lib/etcd</span>/member/snap/db <span>~/etcd</span>/member/snap/db</kbd></li>
<li value="4" class="calibre10"><span>Start the etcd service: <kbd class="calibre12">systemtl start etcd</kbd></span></li>
</ol>
<p class="calibre2">The etcd key-value store recovery procedure is <span class="calibre11">performed on etcd nodes and </span>consists of the following steps:</p>
<ol class="calibre13">
<li value="1" class="calibre10">Create a single node cluster</li>
<li value="2" class="calibre10">Restore data to <kbd class="calibre12">/var/lib/etcd/</kbd>, from <span>backup, while etcd is not running<br class="title-page-name"/></span></li>
<li value="3" class="calibre10">Restore <kbd class="calibre12">/etc/etcd/etcd.conf</kbd>, from backup</li>
</ol>
<ol start="4" class="calibre13">
<li value="4" class="calibre10">Restart etcd</li>
<li value="5" class="calibre10">Add new nodes to the etcd cluster</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift masters </h1>
                
            
            <article>
                
<p class="calibre2">The OpenShift master node backup procedure can be <span class="calibre11">performed on all master nodes, and </span>consists of the following steps:</p>
<ol class="calibre13">
<li value="1" class="calibre10"><strong class="calibre1">Back up master certs and keys</strong>: <kbd class="calibre12">cd /etc/origin/master; tar cf /tmp/certs-and-keys-$(hostname).tar *.key *.crt</kbd></li>
<li value="2" class="calibre10"><strong class="calibre1">Back up registry certificates</strong>: <kbd class="calibre12">cd /etc/docker/certs.d/; tar cf /tmp/docker-registry-certs-$(hostname).tar *</kbd></li>
</ol>
<p class="calibre2"><span class="calibre11">The master node</span> recovery procedure can be <span class="calibre11">performed on all master nodes and </span>consists of the following step:</p>
<ol class="calibre13">
<li value="1" class="calibre10">Restore the previously saved data on every master node to <kbd class="calibre12">/etc/sysconfig/</kbd> , <kbd class="calibre12">/etc/origin/</kbd>, and <kbd class="calibre12">/etc/docker/</kbd> directories.</li>
<li value="2" class="calibre10">Restart OpenShift all services</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift nodes </h1>
                
            
            <article>
                
<p class="calibre2">There is no specific need to save any data on an OpenShift node, since there is no stateful data; you can easily reinstall all of the nodes one by one, or while reinstalling the OpenShift cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Persistent storage  </h1>
                
            
            <article>
                
<p class="calibre2">In many cases, OpenShift pod persistent data can be saved and restored with the <kbd class="calibre12">oc rsync</kbd> command, but it is not the most reliable and efficient method. Persistent storage backup procedures are very different for every storage type, and must be considered separately.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we briefly touched on OpenShift HA and on <span class="calibre11">HA in general</span>. We discussed how OpenShift provides redundancy in the case of a failure and how you can prevent this from happening by properly designing your OpenShift cluster. We finished the chapter with the backup and restore methods and procedures in OpenShift.  </p>
<p class="calibre2">In the next chapter<span class="calibre11">, </span>we will discuss OpenShift<span class="calibre11"> </span>DC in single and multiple data centers. OpenShift multi-DC is one of the most difficult topics when it comes to OpenShift design and implementation in a scalable and distributed environment. The next chapter will illustrate how to properly design OpenShift, in order to work in a<span class="calibre11"> </span>distributed<span class="calibre11"> and redundant </span>configuration across one or more data centers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Questions</h1>
                
            
            <article>
                
<ol class="calibre13">
<li value="1" class="calibre10">Which of these HA methods provides external access to the OpenShift cluster using an external load balancer? choose one:
<ol class="calibre14">
<li value="1" class="calibre10">Virtual IP</li>
<li value="2" class="calibre10">IP failover</li>
<li value="3" class="calibre10">GSLB</li>
<li value="4" class="calibre10">DNS load balancing</li>
</ol>
</li>
<li value="2" class="calibre10">What are the two valid HA methods to provide access to OpenShift from the outside? choose two:
<ol class="calibre14">
<li value="1" class="calibre10">Virtual IP</li>
<li value="2" class="calibre10">IP failover</li>
<li value="3" class="calibre10">GSLB</li>
<li value="4" class="calibre10">DNS load balancing</li>
</ol>
</li>
</ol>
<ol start="3" class="calibre13">
<li value="3" class="calibre10">Etcd is a key-value store that is used to store the system's configuration and state in OpenShift:
<ol class="calibre14">
<li value="1" class="calibre10">True</li>
<li value="2" class="calibre10">False</li>
</ol>
</li>
<li value="4" class="calibre10"><span>What command can be used to back up and restore application data in OpenShift?</span> choose one:
<ol class="calibre14">
<li value="1" class="calibre10"><kbd class="calibre12">oc rsync</kbd></li>
<li value="2" class="calibre10"><kbd class="calibre12"><kbd class="calibre26">oc backup</kbd></kbd></li>
<li value="3" class="calibre10"><kbd class="calibre12">oc save</kbd></li>
<li value="4" class="calibre10"><kbd class="calibre12">oc load </kbd></li>
</ol>
</li>
<li value="5" class="calibre10">There is no need to restore any data in the OpenShift master disaster recovery procedure:
<ol class="calibre14">
<li value="1" class="calibre10">True</li>
<li value="2" class="calibre10">False</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Further reading</h1>
                
            
            <article>
                
<p class="calibre2">The following links will help you to dive deeper into some of this chapter's topics:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">OpenShift HA design</strong>: <a href="http://v1.uncontained.io/playbooks/installation/" class="calibre8">http://v1.uncontained.io/playbooks/installation/</a></li>
<li class="calibre10"><strong class="calibre1">OpenShift High Availability</strong>: <a href="https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html" class="calibre8">https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html</a></li>
<li class="calibre10"><strong class="calibre1">Infrastructure nodes and pod anti-affinity</strong>: <a href="https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes" class="calibre8">https://docs.openshift.com/container-platform/3.7/admin_guide/manage_nodes.html#infrastructure-nodes</a></li>
<li class="calibre10"><strong class="calibre1">OpenShift backup and restore</strong>: <a href="https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html" class="calibre8">https://docs.openshift.com/container-platform/3.4/admin_guide/backup_restore.html</a></li>
<li class="calibre10"><strong class="calibre1">OpenShift scaling and performance guide</strong>: <a href="https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html" class="calibre8">https://docs.openshift.com/container-platform/3.7/scaling_performance/index.html</a></li>
<li class="calibre10"><strong class="calibre1">Etcd optimal cluster size</strong>: <a href="https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size" class="calibre8">https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size</a></li>
<li class="calibre10"><strong class="calibre1">Adding hosts to an OpenShift cluster</strong>: <a href="https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html" class="calibre8">https://docs.openshift.com/container-platform/latest/install_config/adding_hosts_to_existing_cluster.html</a></li>
</ul>


            </article>

            
        </section>
    </body></html>