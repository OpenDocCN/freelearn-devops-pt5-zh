<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Scaling Pods With ReplicaSets</h1>
                </header>
            
            <article>
                
<div class="packt_tip">Most applications should be scalable and all must be fault tolerant. Pods do not provide those features, ReplicaSets do.</div>
<p>We learned that Pods are the smallest unit in Kubernetes. We also learned that Pods are not fault tolerant. If a Pod is destroyed, Kubernetes will do nothing to remedy the problem. That is, if Pods are created without Controllers.</p>
<p>The first Controller we'll explore is called <em>ReplicaSet</em>. Its primary, and pretty much only function, is to ensure that a specified number of replicas of a Pod matches the actual state (almost) all the time. That means that ReplicaSets make Pods scalable.</p>
<p>We can think of ReplicaSets as a self-healing mechanism. As long as elementary conditions are met (for example, enough memory and CPU), Pods associated with a ReplicaSet are guaranteed to run. They provide fault-tolerance and high availability.</p>
<p>It is worth mentioning ReplicaSet is the next-generation ReplicationController. The only significant difference is that ReplicaSet has extended support for selectors. Everything else is the same. ReplicationController is considered deprecated, so we'll focus only on ReplicaSet.</p>
<div class="packt_tip">ReplicaSet's primary function is to ensure that the specified number of replicas of a service are (almost) always running.</div>
<p>Let's explore ReplicaSet through examples and see how it works and what it does.</p>
<p>The first step is to create a Kubernetes cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Cluster</h1>
                </header>
            
            <article>
                
<p>We'll continue using Minikube to simulate a cluster locally.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>04-rs.sh</kbd> (<a href="https://gist.github.com/vfarcic/f6588da3d1c8a82100a81709295d4a93" target="_blank"><span class="URLPACKT">https://gist.github.com/vfarcic/f6588da3d1c8a82100a81709295d4a93</span></a>) Gist.</div>
<pre><strong>minikube start --vm-driver=virtualbox</strong>
    
<strong>kubectl config current-context</strong>  </pre>
<p>We created a single-node cluster and configured <kbd>kubectl</kbd> to use it.</p>
<p>Before we explore the first ReplicaSet example, we'll enter into the local copy of the <kbd>vfarcic/k8s-spec</kbd> repository and pull the latest version. Who knows, maybe I added some new stuff since the last time you checked it out.</p>
<pre><strong>cd k8s-specs</strong>
    
<strong>git pull</strong>  </pre>
<p>Now that the cluster is running and the repository with the specs is up-to-date, we can create our first ReplicaSet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating ReplicaSets</h1>
                </header>
            
            <article>
                
<p>Let's take a look at a ReplicaSet based on the Pod we created in the previous chapter:</p>
<pre><strong>cat rs/go-demo-2.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: apps/v1beta2</strong>
<strong>kind: ReplicaSet</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2</strong>
<strong>spec:</strong>
<strong>  replicas: 2</strong>
<strong>  selector:</strong>
<strong>    matchLabels:</strong>
<strong>      type: backend</strong>
<strong>      service: go-demo-2</strong>
<strong>  template:</strong>
<strong>    metadata:</strong>
<strong>      labels:</strong>
<strong>        type: backend</strong>
<strong>        service: go-demo-2</strong>
<strong>        db: mongo</strong>
<strong>        language: go</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: db</strong>
<strong>        image: mongo:3.3</strong>
<strong>      - name: api</strong>
<strong>        image: vfarcic/go-demo-2</strong>
<strong>        env:</strong>
<strong>        - name: DB</strong>
<strong>          value: localhost</strong>
<strong>        livenessProbe:</strong>
<strong>          httpGet:</strong>
<strong>            path: /demo/hello</strong>
<strong>            port: 8080</strong>  </pre>
<p>The <kbd>apiVersion</kbd>, <kbd>kind</kbd>, and <kbd>metadata</kbd> fields are mandatory with all Kubernetes objects. ReplicaSet is no exception.</p>
<p>We specified that the <kbd>apiVersion</kbd> is <kbd>apps/v1beta2</kbd>. At the time of this writing, ReplicaSet is still in beta. Soon it will be considered stable, and you'll be able to replace the value with <kbd>apps/v1</kbd>. The <kbd>kind</kbd> is <kbd>ReplicaSet</kbd> and <kbd>metadata</kbd> has the <kbd>name</kbd> key set to <kbd>go-demo-2</kbd>. We could have extended ReplicaSet <kbd>metadata</kbd> with labels. However, we skipped that part since they would serve only for informational purposes. They do not affect the behavior of the ReplicaSet.</p>
<p>You should be familiar with the three fields since we already explored them when we worked with Pods. In addition to them, the <kbd>spec</kbd> section is mandatory as well.</p>
<p>The first field we defined in the <kbd>spec</kbd> section is <kbd>replicas</kbd>. It sets the desired number of replicas of the Pod. In this case, the ReplicaSet should ensure that two Pods should run concurrently. If we did not specify the value of the <kbd>replicas</kbd>, it would default to <kbd>1</kbd>.</p>
<p>The next <kbd>spec</kbd> section is the <kbd>selector</kbd>. We use it to select which pods should be included in the ReplicaSet. It does not distinguish between the Pods created by a ReplicaSet or some other process. In other words, ReplicaSets and Pods are decoupled. If Pods that match the <kbd>selector</kbd> exist, ReplicaSet will do nothing. If they don't, it will create as many Pods to match the value of the <kbd>replicas</kbd> field. Not only that ReplicaSet creates the Pods that are missing, but it also monitors the cluster and ensures that the desired number of <kbd>replicas</kbd> is (almost) always running. In case there are already more running Pods with the matching <kbd>selector</kbd>, some will be terminated to match the number set in <kbd>replicas</kbd>.</p>
<p>We used <kbd>spec.selector.matchLabels</kbd> to specify a few labels. They must match the labels defined in the <kbd>spec.template</kbd>. In our case, ReplicaSet will look for Pods with <kbd>type</kbd> set to <kbd>backend</kbd> and <kbd>service</kbd> set to <kbd>go-demo-2</kbd>. If Pods with those labels do not already exist, it'll create them using the <kbd>spec.template</kbd> section.</p>
<p>The last section of the <kbd>spec</kbd> field is the <kbd>template</kbd>. It is the only required field in the <kbd>spec</kbd>, and it has the same schema as a Pod specification. At a minimum, the labels of the <kbd>spec.template.metadata.labels</kbd> section must match those specified in the <kbd>spec.selector.matchLabels</kbd>. We can set additional labels that will serve informational purposes only. ReplicaSet will make sure that the number of replicas matches the number of Pods with the same labels. In our case, we set <kbd>type</kbd> and <kbd>service</kbd> to the same values and added two additional ones (<kbd>db</kbd> and <kbd>language</kbd>).</p>
<p>It might sound confusing that the <kbd>spec.template.spec.containers</kbd> field is mandatory. ReplicaSet will look for Pods with the matching labels created by other means. If we already created a Pod with labels <kbd>type: backend</kbd> and <kbd>service: go-demo-2</kbd>, this ReplicaSet would find them and would not create a Pod defined in <kbd>spec.template</kbd>. The main purpose of that field is to ensure that the desired number of <kbd>replicas</kbd> is running. If they are created by other means, ReplicaSet will do nothing. Otherwise, it'll create them using the information in <kbd>spec.template</kbd>.</p>
<p>Finally, the <kbd>spec.template.spec</kbd> section contains the same <kbd>containers</kbd> definition we used in the previous chapter. It defines a Pod with two containers (<kbd>db</kbd> and <kbd>api</kbd>).</p>
<p>In the previous chapter, I claimed that those two containers should not belong to the same Pod. The same is true for the containers in Pods managed by the ReplicaSet. However, we did not yet have the opportunity to explore ways to allow containers running in different Pods to communicate with each other. So, for now, we'll continue using the same flawed Pods definition.</p>
<p>Let's create the ReplicaSet and experience its advantages first hand.</p>
<pre><strong>kubectl create -f rs/go-demo-2.yml</strong>  </pre>
<p>We got the response that the <kbd>replicaset "go-demo-2"</kbd> was <kbd>created</kbd>. We can confirm that by listing all the ReplicaSets in the cluster.</p>
<pre><strong>kubectl get rs</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME      DESIRED CURRENT READY AGE</strong>
<strong>go-demo-2 2       2       0     14s</strong>  </pre>
<p>We can see that the desired number of replicas is <kbd>2</kbd> and that it matches the current value. The value of the <kbd>ready</kbd> field is still <kbd>0</kbd> but, after the images are pulled, and the containers are running, it'll change to <kbd>2</kbd>.</p>
<p>Instead of retrieving all the replicas in the cluster, we can retrieve those specified in the <kbd>rs/go-demo-2.yml</kbd> file.</p>
<pre><strong>kubectl get -f rs/go-demo-2.yml</strong>  </pre>
<p>The output should be the same since, in both cases, there is only one ReplicaSet running inside the cluster.</p>
<p>All the other <kbd>kubectl get</kbd> arguments we explored in the previous chapter also apply to ReplicaSets or, to be more precise, to all Kubernetes objects. The same is true for <kbd>kubectl describe</kbd> command:</p>
<pre><strong>kubectl describe -f rs/go-demo-2.yml</strong>  </pre>
<p>The last lines of the output are as follows:</p>
<pre><strong>...</strong>
<strong>Events:</strong>
<strong>  Type   Reason           Age  From                  Message</strong>
<strong>  ----   ------           ---- ----                  -------</strong>
<strong>  Normal SuccessfulCreate 3m   replicaset-controller Created pod: <br/> go-demo-2-v59t5</strong>
<strong>  Normal SuccessfulCreate 3m   replicaset-controller Created pod: <br/> go-demo-2-5fd54</strong>  </pre>
<p>Judging by the events, we can see that ReplicaSet created two Pods while trying to match the desired state with the actual state.</p>
<p>Finally, if you are not yet convinced that the ReplicaSet created the missing Pods, we can list all those running in the cluster and confirm it:</p>
<pre><strong>kubectl get pods --show-labels</strong>  </pre>
<p>To be on the safe side, we used the <kbd>--show-labels</kbd> argument so that we can verify that the Pods in the cluster match those created by the ReplicaSet.</p>
<p>The output is as follows:</p>
<pre><strong>NAME            READY STATUS  RESTARTS AGE LABELS</strong>
<strong>go-demo-2-5fd54 2/2   Running 0        6m  db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-v59t5 2/2   Running 0        6m  db=mongo,language=go,service=go-demo-2,type=backend</strong>    </pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5168d886-97b9-4c75-a86d-279f7bc93cfd.png" style="width:70.92em;height:19.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4-1: A ReplicaSet with two replicas of a Pod</div>
<p>The sequence of events that transpired with the <kbd>kubectl create -f rs/go-demo-2.yml</kbd> command is as follows:</p>
<ol>
<li>Kubernetes client (<kbd>kubectl</kbd>) sent a request to the API server requesting the creation of a ReplicaSet defined in the <kbd>rs/go-demo-2.yml</kbd> file.</li>
<li>The controller is watching the API server for new events, and it detected that there is a new ReplicaSet object.</li>
<li>The controller creates two new pod definitions because we have configured replica value as <kbd>2</kbd> in <kbd>rs/go-demo-2.yml</kbd> file.</li>
<li>Since the scheduler is watching the API server for new events, it detected that there are two unassigned Pods.</li>
<li>The scheduler decided to which node to assign the Pod and sent that information to the API server.</li>
</ol>
<ol start="6">
<li>Kubelet is also watching the API server. It detected that the two Pods were assigned to the node it is running on.</li>
<li>Kubelet sent requests to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines two containers based on the <kbd>mongo</kbd> and <kbd>api</kbd> image. So in total four containers are created.</li>
<li>Finally, Kubelet sent a request to the API server notifying it that the Pods were created successfully.</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e77ce395-1239-44a8-8b0a-d150458b7f8c.png" style="width:75.33em;height:41.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-2: The sequence of events followed by request to create a ReplicaSet</div>
<p>The sequence we described is useful when we want to understand everything that happened in the cluster from the moment we requested the creation of a new ReplicaSet. However, it might be too confusing so we'll try to explain the same process through a diagram that more closely represents the cluster.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0141d08f-50be-4132-a43b-544231eb196a.png" style="width:31.67em;height:34.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-3: The events followed by request to create a ReplicaSet</div>
<div class="packt_infobox">Typically, we'd have a multi-node cluster, and the Pods would be distributed across it. For now, while we're using Minikube, there's only one server that acts as both the master and the node. Later on, when we start working on multi-node clusters, the distribution of Pods will become evident. The same can be said for the architecture. We'll explain different Kubernetes components in more detail later on.</div>
<p>Let's see which types of operations we can perform on ReplicaSets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating ReplicaSets</h1>
                </header>
            
            <article>
                
<p>What would happen if we delete the ReplicaSet? As you might have guessed, both the ReplicaSet and everything it created (the Pods) would disappear with a single <kbd>kubectl delete -f rs/go-demo-2.yml</kbd> command. However, since ReplicaSets and Pods are loosely coupled objects with matching labels, we can remove one without deleting the other. We can, for example, remove the ReplicaSet we created while leaving the two Pods intact.</p>
<pre><strong>kubectl delete -f rs/go-demo-2.yml \</strong>
<strong>    --cascade=false</strong>  </pre>
<p>We used the <kbd>--cascade=false</kbd> argument to prevent Kubernetes from removing all the downstream objects. As a result, we got the confirmation that <kbd>replicaset "go-demo-2"</kbd> was <kbd>deleted</kbd>. Let's confirm that it is indeed removed from the system.</p>
<pre><strong>kubectl get rs</strong>  </pre>
<p>As expected, the output states that <kbd>no resources</kbd> were <kbd>found</kbd>.</p>
<p>If <kbd>--cascade=false</kbd> indeed prevents Kubernetes from removing the downstream objects, the Pods should continue running in the cluster. Let's confirm the assumption.</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME            READY STATUS  RESTARTS AGE</strong>
<strong>go-demo-2-md5xp 2/2   Running 0        9m</strong>
<strong>go-demo-2-vnmf7 2/2   Running 0        9m</strong>  </pre>
<p>The two Pods created by the ReplicaSet are indeed still running in the cluster even though we removed the ReplicaSet.</p>
<p>The Pods that are currently running in the cluster do not have any relation with the ReplicaSet we created earlier. We deleted the ReplicaSet, and the Pods are still there. Knowing that the ReplicaSet uses labels to decide whether the desired number of Pods is already running in the cluster, should lead us to the conclusion that if we create the same ReplicaSet again, it should reuse the two Pods that are running in the cluster. Let's confirm that.</p>
<p>In addition to the <kbd>kubectl create</kbd> command we executed previously, we'll also add the <kbd>--save-config</kbd> argument. It'll save the configuration of the ReplicaSet thus allowing us to perform a few additional operations later on. We'll get to them shortly. For now, the important thing is that we are about to create the same ReplicaSet we had before.</p>
<pre><strong>kubectl create -f rs/go-demo-2.yml \</strong>
<strong>    --save-config</strong>  </pre>
<p>The output states that the <kbd>replicaset "go-demo-2" was created</kbd>. Let's see what happened with the Pods.</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME            READY STATUS  RESTARTS AGE</strong>
<strong>go-demo-2-md5xp 2/2   Running 0        10m</strong>
<strong>go-demo-2-vnmf7 2/2   Running 0        10m</strong>  </pre>
<p>If you compare the names of the Pods, you'll see that they are the same as before we created the ReplicaSet. It looked for matching labels, deduced that there are two Pods that match them, and decided that there's no need to create new ones. The matching Pods fulfil the desired number of replicas.</p>
<p>Since we saved the configuration, we can <kbd>apply</kbd> an updated definition of the ReplicaSet. For example, we can use <kbd>rs/go-demo-2-scaled.yml</kbd> file that differs only in the number of replicas set to <kbd>4</kbd>. We could have created the ReplicaSet with <kbd>apply</kbd> in the first place, but we didn't. The <kbd>apply</kbd> command automatically saves the configuration so that we can edit it later on. The <kbd>create</kbd> command does not do such thing by default so we had to save it with <kbd>--save-config</kbd>.</p>
<pre><strong>kubectl apply -f rs/go-demo-2-scaled.yml</strong>  </pre>
<p>This time, the output is slightly different. Instead of saying that the ReplicaSet was created, we can see that it was <kbd>configured</kbd>:</p>
<p>Let's take a look at the Pods.</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME            READY STATUS  RESTARTS AGE</strong>
<strong>go-demo-2-ckmtv 2/2   Running 0        50s</strong>
<strong>go-demo-2-lt4qm 2/2   Running 0        50s</strong>
<strong>go-demo-2-md5xp 2/2   Running 0        11m</strong>
<strong>go-demo-2-vnmf7 2/2   Running 0        11m</strong>  </pre>
<p>As expected, now there are four Pods in the cluster. If you pay closer attention to the names of the Pods, you'll notice that two of them are the same as before.</p>
<p>When we applied the new configuration with <kbd>replicas</kbd> set to <kbd>4</kbd> instead of <kbd>2</kbd>, Kubernetes updated the ReplicaSet which, in turn, evaluated the current state of the Pods with matching labels. It found two with the same labels and decided to create two more so that the new desired state can match the actual state.</p>
<p>Let's see what happens when a Pod is destroyed.</p>
<pre><strong>POD_NAME=$(kubectl get pods -o name \</strong>
<strong>    | tail -1)</strong>
    
<strong>kubectl delete $POD_NAME</strong>  </pre>
<p>We retrieved all the Pods and used <kbd>-o name</kbd> to retrieve only their names. The result was piped to <kbd>tail -1</kbd> so that only one of the names is output. The result is stored in the environment variable <kbd>POD_NAME</kbd>. The latter command used that variable to remove the Pod as a simulation of a failure.</p>
<p>Let's take another look at the Pods in the cluster:</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME              READY     STATUS        RESTARTS   AGE</strong>
<strong>go-demo-2-ckmtv   2/2       Running       0          10m</strong>
<strong>go-demo-2-lt4qm   2/2       Running       0          10m</strong>
<strong>go-demo-2-md5xp   2/2       Running       0          13m</strong>
<strong>go-demo-2-t8sfs   2/2       Running       0          30s</strong>
<strong>go-demo-2-vnmf7   0/2       Terminating   0          13m</strong>  </pre>
<p>We can see that the Pod we deleted is <kbd>terminating</kbd>. However, since we have a ReplicaSet with <kbd>replicas</kbd> set to <kbd>4</kbd>, as soon as it discovered that the number of Pods dropped to <kbd>3</kbd>, it created a new one. We just witnessed self-healing in action. As long as there are enough available resources in the cluster, ReplicaSets will make sure that the specified number of Pod replicas are (almost) always up-and-running.</p>
<p>Let's see what happens if we remove one of the Pod labels ReplicaSet uses in its selector.</p>
<pre><strong>POD_NAME=$(kubectl get pods -o name \</strong>
<strong>    | tail -1)</strong>
    
<strong>kubectl label $POD_NAME service-</strong>
  
<strong>kubectl describe $POD_NAME</strong>  </pre>
<p>We used the same command to retrieve the name of one of the Pods and executed the command that removed the label <kbd>service</kbd>. Please note <kbd>-</kbd> at the end of the name of the label. It is the syntax that indicates that a label should be removed:</p>
<p>Finally, we described the Pod:</p>
<p>The output of the last command, limited to the labels section, is as follows:</p>
<pre><strong>...</strong>
<strong>Labels: db=mongo</strong>
<strong>        language=go</strong>
<strong>        type=backend</strong>
<strong>...</strong>  </pre>
<p>As you can see, the label <kbd>service</kbd> is gone.</p>
<p>Now, let's list the Pods in the cluster and check whether there is any change:</p>
<pre><strong>kubectl get pods --show-labels</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME            READY STATUS  RESTARTS AGE LABELS</strong>
<strong>go-demo-2-ckmtv 2/2   Running 0        24m db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-lt4qm 2/2   Running 0        24m db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-md5xp 2/2   Running 0        28m db=mongo,language=go,type=backend</strong>
<strong>go-demo-2-nrnbh 2/2   Running 0        4m  db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-t8sfs 2/2   Running 0        15m db=mongo,language=go,service=go-demo-2,type=backend</strong> </pre>
<p>The total number of Pods increased to five. The moment we removed the <kbd>service</kbd> label from one of the Pods, the ReplicaSet discovered that the number of Pods matching the <kbd>selector</kbd> labels is three and created a new Pod. Right now, we have four Pods controlled by the ReplicaSet and one running freely due to non-matching labels.</p>
<p>What would happen if we add the label we removed?</p>
<pre><strong>kubectl label $POD_NAME service=go-demo-2</strong>
    
<strong>kubectl get pods --show-labels</strong>  </pre>
<p>We added the <kbd>service=go-demo-2</kbd> label and listed all the Pods.</p>
<p>The output of the latter command is as follows:</p>
<pre><strong>NAME            READY STATUS      RESTARTS AGE LABELS</strong>
<strong>go-demo-2-ckmtv 2/2   Running     0        28m db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-lt4qm 2/2   Running     0        28m db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-md5xp 2/2   Running     0        31m db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-nrnbh 0/2   Terminating 0        7m  db=mongo,language=go,service=go-demo-2,type=backend</strong>
<strong>go-demo-2-t8sfs 2/2   Running     0        18m db=mongo,language=go,service=go-demo-2,type=backend</strong></pre>
<p>The moment we added the label, the ReplicaSet discovered that there are five Pods with matching selector labels. Since the specification states that there should be four replicas of the Pod, it removed one of the Pods so that the desired state matches the actual state.</p>
<p>The previous few examples showed, one more time, that ReplicaSets and Pods are loosely coupled through matching labels and that ReplicaSets are using those labels to maintain the parity between the actual and the desired state. So far, self-healing worked as expected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What now?</h1>
                </header>
            
            <article>
                
<p>The good news is that ReplicaSets are relatively straightforward. They provide a guarantee that the specified number of replicas of a Pod will be running in the system as long as there are available resources. That's the primary and, arguably, the only purpose.</p>
<p>The bad news is that ReplicaSets are rarely used independently. You will almost never create a ReplicaSet directly just as you're not going to create Pods. Instead, we tend to create ReplicaSets through Deployments. In other words, we use ReplicaSets to create and control Pods, and Deployments to create ReplicaSets (and a few other things). We'll get to Deployment soon. For now, please delete your local Minikube cluster. The next chapter will start from scratch.</p>
<pre><strong>minikube delete</strong>  </pre>
<div class="packt_infobox">If you'd like to know more about ReplicaSets, please explore ReplicaSet v1 apps (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#replicaset-v1-apps" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#replicaset-v1-apps</span></a>) API documentation.</div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/70f75cdc-8333-4071-8b93-d73fe282608a.png" style="width:74.67em;height:21.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-4: The components explored so far</div>


            </article>

            
        </section>
    </body></html>