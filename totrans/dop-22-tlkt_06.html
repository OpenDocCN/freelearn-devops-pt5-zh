<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-choosing-a-solution-for-metrics-storage-and-query" class="calibre15">Choosing A Solution For Metrics Storage And Query</h2>

<p class="calibre3">Every cluster needs to collect metrics. They are the basis of any alerting system we might want to employ. Without the information about the current and the past state of a cluster, we would not be able to react to problems when they occur nor would we be able to prevent them from happening in the first place. Actually, that is not entirely accurate. We could do all those things, but not in a way that is efficient and scalable.</p>

<p class="calibre3">A good analogy is blindness. Being blind does not mean that we cannot feel our way through an environment. Similarly, we are not helpless without a way to collect and query metrics. We can SSH into each of the nodes and check the system manually. We can start by fiddling with <code class="calibre19">top</code>, <code class="calibre19">mem</code>, <code class="calibre19">df</code>, and other commands. We can check the status of the containers with the <code class="calibre19">docker stats</code> command. We can go from one container to another and check their logs. We can do all those things, but such an approach does not scale. We cannot increase the number of operators with the same rhythm as the number of servers. We cannot convert ourselves into human machines. Even if we could, we would be terrible at it. That’s why we have tools to help us. And, if they do not fulfill our needs, we can build our own solutions on top of them.</p>

<p class="calibre3">There are many tools we can choose. It would be impossible to compare them all, so we’ll limit the scope to only a handful.</p>

<p class="calibre3">We’ll focus on open source projects only. Some of the tools we’ll discuss have a paid enterprise offering in the form of additional features. We’ll exclude them from the comparison. The reason behind the exclusion lies in my belief that we should always start with open source software, get comfortable with it, and only once it proves its worth, evaluate whether it is worthwhile switching to the enterprise version.</p>

<p class="calibre3">Moreover, we’ll introduce one more limitation. We will explore only the solutions that we can host ourselves. That excludes hosted services like, for example, <a href="https://scoutapp.com/">Scout</a> or <a href="https://www.datadoghq.com/">DataDog</a>. The reason behind such a decision is two-fold. Many organizations are not willing to “give” their data to a third-party hosted service. Even if there is no such restriction, a hosted service would need to be able to send alerts back to our system and that would be a huge security breach. If neither of those matters to you, they are not flexible enough. None of the services I know will give us enough flexibility to build a <em class="calibre21">self-adapting</em> and <em class="calibre21">self-healing</em> system. Besides, the purpose of this book is to give you free solutions, hence the insistence on open source solutions that you can host yourself.</p>

<p class="calibre3">That does not mean that paid software is not worth the price nor that we should not use, and pay for, hosted service. Quite the contrary. However, I felt it would be better to start with things we can build ourselves and explore the limits. From there on, you will have a better understanding what you need and whether paying money for that is worthwhile.</p>

<aside class="tip">
    <h3 id="leanpub-auto-a-note-to-the-devops-21-toolkit-readers" class="calibre22">A note to <em class="calibre23">The DevOps 2.1 Toolkit</em> readers</h3>

  <p class="calibre3">You might be able to guess which tool will be chosen. Never the less, this chapter provides a more detailed explanation behind the choice. I think that the overview that follows is important since it provides a short description of the types of the solutions for storing and querying metrics as well as some of the pros and cons of some of the tools on the market.</p>

</aside>

<h3 id="leanpub-auto-non-dimensional-vs-dimensional-metrics" class="calibre20">Non-Dimensional vs. Dimensional Metrics</h3>

<p class="calibre3">Before we explore the tools we’ll choose from, we should discuss different approaches to storing and collecting metrics.</p>

<p class="calibre3">We can divide the tools by dimensions. Some can store data with dimensions while others cannot. Representatives of those that are dimensionless would be Graphite and Nagios. Truth be told, there is a semblance of dimensions in Graphite, but they are so limited in their nature that we’ll treat it as dimensionless. Some of the solutions that do support dimensions are, for example, InfluxDB and Prometheus. The former supports them in the form of key/value pairs while the latter uses labels.</p>

<p class="calibre3">Non-dimensional (or dimensionless) metric storage belongs to the <em class="calibre21">old world</em> when servers were relatively static, and the number of targets that were monitored was relatively small. That can be seen from the time those tools were created. Both Nagios and Graphite are older tools than InfluxDB and Prometheus.</p>

<p class="calibre3">Why are dimensions relevant? Query language needs them to be effective. Without dimensions, the language is bound to be limited in its capabilities. That does not mean that we always need dimensions. For a simple monitoring, they might be an overhead. However, running a scalable cluster where services are continuously deployed, scaled, updated, and moved around is far from simple. We need metrics that can represent all the dimensions of our cluster and the services running on top of it. A dynamic system requires dynamic analytics, and that is accomplished with metrics that include dimensions.</p>

<p class="calibre3">An example of a dimensionless metric would be <code class="calibre19">container_memory_usage</code>. Compare that with <code class="calibre19">container_memory_usage{service_name="my-service", task_name="my-service.2.###", memory_limit="20000000", ...}"</code>. The latter example provides much more freedom. We can calculate average memory usage as we’d do with dimensionless but we can also deduce what the memory limit is, what is the name of the service, which replica (task) it is, and so on, and so forth.</p>

<aside class="tip">
    <p class="calibre3">Dimensions, when added to metrics, can be combined into innumerable combinations, and paint an accurate picture of the system and answer complex questions.</p>

</aside>

<p class="calibre3">Are dimensions (or lack of them) the only thing that distinguishes tools for storing and analyzing metrics? Among others, the way those metrics end up in a database makes a difference that might be significant. Some of the tools expect data to be pushed while others will pull (or scrape) them.</p>

<p class="calibre3">If we stick with the tools we mentioned previously, representatives of a push method would be Graphite and InfluxDB, while Nagios and Prometheus would belong to the pull group.</p>

<p class="calibre3">Those that fall into the push category are expecting data to come to them. They are passive (at least when metrics gathering is concerned). Each of the services that collect data is supposed to push them into one central location. Popular examples would be <em class="calibre21">collectD</em> and <em class="calibre21">statsD</em>. Pull system, on the other hand, is active. It will scrape data from all specified targets. Data collectors do not know about the existence of the database. Their only purpose is to gather data and expose them through a protocol acceptable to the system that will pull them.</p>

<p class="calibre3">A discussion about pros and cons of each system is raging for quite some time. There are many arguments in favor of one over the other system, and we could spend a lot of time going through all of them. Instead, we’ll discuss discovery, the argument that is, in my opinion, the most relevant.</p>

<p class="calibre3">With the push system, discovery is easy. All that data collectors need to know is the address of the metrics storage and push data. As long as that address keeps being operational, the configuration is very straight forward. With the pull system, the system needs to know the location of all the data collectors (or exporters). When there are only a few, that is easy to configure. If that number jumps to tens, hundreds, or even thousands of targets, the configuration can become very tedious. That situation clearly favors the push model. But, technology changed. We have reliable systems that provide service discovery. Docker Swarm, for example, has it baked in as part of Docker Engine. Finding targets is easy and, assuming that we trust service discovery, we always have up to date information about all the data collectors.</p>

<p class="calibre3">With a proper service discovery in place, pull vs. push debate becomes, more or less, irrelevant. That brings us to an argument that makes pull more appealing. It is much easier to discover a failed instance or a missing service when pulling data. When a system expects data collectors to push data, it is oblivious whether something is missing. We can summarize the problem with “I don’t know what I don’t know.” Pull systems, on the other hand, know what to expect. They know what their targets are and it is very easy to deduce that when a scraping target does not respond, the likely cause is that it stopped working.</p>


<figure class="image">
  <img src="../images/00007.jpeg" alt="Figure 2-1: Monitoring tools placement based on dimensions and data collection methods" class="calibre17"/>
  <figcaption class="calibre18">Figure 2-1: Monitoring tools placement based on dimensions and data collection methods</figcaption>
</figure>


<p class="calibre3">Neither of the arguments for push or pull are definitive, and we should not make a choice only based on that criteria. Instead, we’ll explore the tools we discussed a bit more.</p>

<p class="calibre3">The first one on the list is Graphite.</p>

<h3 id="leanpub-auto-graphite" class="calibre20">Graphite</h3>

<p class="calibre3">Graphite is a passive metrics storage tool. The reason we call it passive lies in its inability to collect metrics. They need to be collected and pushed in a separate process.</p>

<p class="calibre3">It is a time series database with its own query language and capabilities to produce graphs. Querying API is powerful. Or, to be more precise, was considered powerful when it appeared. Today, when compared with some other tools, its query language is limiting, mainly due to its dimensionless format for storing metrics.</p>

<p class="calibre3">Graphite stores numeric data in time series format. Its metric names consist of dot-separated elements.</p>

<p class="calibre3">Data is stored on a local disk.</p>

<h3 id="leanpub-auto-influxdb" class="calibre20">InfluxDB</h3>

<p class="calibre3">Just like Graphite, InfluxDB is a time series database. Unlike Graphite, Influx DB data model is based on key/value pairs in the form of labels.</p>

<p class="calibre3">InfluxDB (open source version, to be more precise) relies on local storage for storing data, and its scraping, rule processing, and alerting.</p>

<h3 id="leanpub-auto-nagios-and-sensu" class="calibre20">Nagios and Sensu</h3>

<p class="calibre3">Nagios is a monitoring system that originated in the 90s as NetSaint. It is primarily about alerting based on the exit codes of scripts.</p>

<p class="calibre3">Unlike other solutions, the amount and types of data it stores is limited to check state making it suitable only for a very basic monitoring.</p>

<p class="calibre3">Sensu can be considered a more modern version of Nagios. The primary difference is that Sensu clients register themselves, and can determine the checks to run either from a central or local configuration. There is also a client socket permitting arbitrary check results to be pushed into Sensu.</p>

<p class="calibre3">Sensu uses (almost) the same data model as Nagios and shares its limitation of the format it uses to store metrics.</p>

<h3 id="leanpub-auto-prometheus" class="calibre20">Prometheus</h3>

<p class="calibre3">Prometheus is a full monitoring and trending system that includes built-in and active scraping, storing, querying, graphing, and alerting based on time series data. It has knowledge about what the world should look like (which endpoints should exist, what time series patterns mean trouble, etc.), and actively tries to find faults.</p>

<p class="calibre3">Prometheus has a rich data model and probably the most powerful query language among time series databases. It encodes dimensions explicitly as key-value pairs (labels) attached to a metric name. That allows easy filtering, grouping, and matching by these labels via in the query language.</p>

<h3 id="leanpub-auto-which-tool-should-we-choose" class="calibre20">Which Tool Should We Choose?</h3>

<p class="calibre3">All the tools we listed are (or were) good in their merit. They are different in many aspects while similar in others.</p>

<p class="calibre3">Nagios and Sensu served us well in the past. They were designed in a different era and based on principles that are today considered obsolete. They work well with static clusters and monolithic applications and services running on predefined locations. The metrics they store (or lack of them) are not suitable for more complex decision making. We would have a hard time using them as means to accomplish our goals of operating a scheduler like Docker Swarm running in an auto-scalable cluster. Among the solutions we explored, they are the first ones we should discard. One is out; three are left to choose from.</p>

<p class="calibre3">Dot-separated metrics format used by Graphite is limiting. Excluding elements of a metric with asterisks (<code class="calibre19">*</code>) is often inadequate for proper filtering, grouping, and other operations. Its query language, when compared with InfluxDB and Prometheus, is the main reason we’ll discard it.</p>

<p class="calibre3">We’re left with InfluxDB and Prometheus as finalists and are facing only minor differences.</p>

<p class="calibre3">InfluxDB and Prometheus are similar in many ways, so the choice is not going to be an easy one. Truth be told, we cannot make a wrong decision. Whichever we choose of the two, the choice will be based on slight differences.</p>

<p class="calibre3">If we would not limit ourselves to open source solutions as the only candidates, InfluxDB enterprise version could be the winner due to its scalability. However, we will discard it in favor of Prometheus. It provides a more complete solution. More importantly, Prometheus is slowly becoming the de-facto standard, at least when working with schedulers. It is a preferred solution in Kubernetes. Docker (and therefore Swarm) is soon going to expose its metrics in Prometheus format. That, in itself, is the tipping point that should make us lean slightly more towards Prometheus.</p>

<p class="calibre3">The decision is made. We’ll use Prometheus to store metrics, to query them, and to trigger alerts.</p>

<h3 id="leanpub-auto-what-now-1" class="calibre20">What Now?</h3>

<p class="calibre3">Now that we decided which tool will be the basis for storing metrics, we should proceed with the setup. Since we will be using Docker Swarm services, deploying Prometheus in its most basic form will be a breeze.</p>



</div>
</body></html>