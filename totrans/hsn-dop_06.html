<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Big Data Applications</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn to build big data applications, analyze a traditional end-to end data workflow life cycle, and on similar lines build a big data application step by step. We will cover the big data process--discovery, ingestion, visualization, and governance. The emphasis will be on the Spark platform and data science prediction models. DevOps applications to various phases of big data will be explored in the subsequent chapters.</p>
<ul>
<li>Traditional data platforms</li>
<li>Big data platform core principles</li>
<li>Big data life cycle:
<ul>
<li>Data discovery</li>
<li>Data quality</li>
<li>Data ingestion</li>
<li>Data analytics</li>
<li>Spark platform</li>
<li>Data visualization</li>
<li>Data governance</li>
</ul>
</li>
<li>Building enterprise applications</li>
<li>Data science--prediction models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Traditional enterprise architecture</h1>
                </header>
            
            <article>
                
<p>Traditionally, an <strong>enterprise data warehouse </strong>(<strong>EDW</strong>) system is considered as a core component of the business intelligence environment. Data warehouse systems are central repositories built by integrating data from multiple disparate source systems, used for data analysis and reporting the needs of the enterprise.</p>
<p>Let's review the end-to end data life cycle components of the traditional system:</p>
<ul>
<li>The <strong>data discovery</strong> phase is where the source systems are explored and analyzed for relevant data and data structures. If the analyzed data is valid, correct, and usable, it is ingested into the data warehouse system. For example, if we need customer ID information, we should be connecting and extracting data from the correct columns and tables.</li>
<li><strong>Data quality </strong>ensures that the ingested data is acceptable and usable. A simple example is name formats of the first name and last name convention, which should be adhered to and, as appropriate, corrected for a few records.</li>
<li><strong>Data transformation</strong> is the phase where data manipulation rules as per business logic are applied in tune with business needs. For example, every employee's yearly salary is computed from multiple systems and saved in the system.</li>
<li><strong>Extract</strong>, <strong>Transform</strong>, and <strong>Load</strong> (<strong>ETL</strong>) is the common term for consolidated reference for all the preceding phases together (data discovery, data quality, and data transformation) as a cycle.</li>
<li><strong>Data staging </strong>is the landing area on your systems for data collected from source systems.</li>
<li><strong>Data lineage </strong>traces the origin and credibility of the data ingested into the system to ensure only authentic, trusted, and authorized data is inducted into the system.</li>
<li><strong>Metadata </strong>is data about data. For example, a sales receipt is the origin of the record with transaction details, from where the requisite data from our computations is extracted. Details such as store ID, sales amount, item ID, date of transaction, and so on, are extracted from a sales receipt.</li>
<li><strong>Data warehouse</strong> is the storage layer, into which the transformed data is loaded as consolidated copy. It is time-variant, consistent, and read-intensive.</li>
<li><strong>Data marts</strong> are data serving repositories specializing in some category, such as customer data, product data, employee data, and so on.</li>
<li><strong>Data analytics </strong>is the common term for an analysis to address all the business needs. Its building queries address business demands, such as how many customers were added last month, or what products are selling above targets this week.</li>
<li><strong>Semantic layer</strong>--its business interface builds queries on the database with business intelligence tools, hiding complex data tables from business users.</li>
<li><strong>Reporting</strong>--reports are for business use, such as all products sold last month in a state.</li>
<li>The <strong>dashboard </strong>provides a consolidated quick view of important key performance indicators. An analogy is a car dashboard with speed, battery, petrol reserve, and so on.</li>
<li><strong>Data visualization</strong>--finding key performance business trends solely based on Excel reports can be a daunting task. Presenting them in a visual form is quite appealing, such as representing them as charts, histograms, and pie diagrams.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principles to build big data enterprise applications</h1>
                </header>
            
            <article>
                
<p>Big data platforms and applications manage, integrate, analyze, and secure analytics on many data types both within the enterprise as well as in external data. They integrate multiple data sources in real time, taking into account volume, velocity, and variety. The platform can be built as a repository of an enterprise knowledge base with the organization's collective data assets.</p>
<p>Some of the salient features for building these platforms are discussed and as we see DevOps is very appropriate and instruments to enhance value at every stage, like versioning systems for building algorithms, data models, scalable reproducible platforms with virtual machines as seen in previous chapter:</p>
<ul>
<li><strong>Flexible data modeling</strong>: Big data systems integrate many different forms of data from multiple data sources. Rather than a pre-defined schema of rigid rows and columns, the schema is to be defined on the fly and data modeled to reflect how information is to be assimilated. To reflect real-world entities, it is also flexibly specified as a graph of objects with relationships.</li>
<li><strong>Knowledge management</strong>: Version controlled knowledge base with accumulated insights of an organization can be leveraged as an enterprise asset.</li>
<li><strong>Privacy and security controls</strong>: The platform is designed for data lineage, multi-level security, and audit compliance. Every object integrated into the platform is traced to its original data source, where access restrictions are in place including authorization and authentication.</li>
<li><strong>Algorithms for data processing</strong>: These are massive datasets to be compiled and analyzed with built-in machine learning algorithms to augment the human user's ability to make sense of large-scale data by identifying patterns in the data.</li>
<li><strong>Scalable platforms</strong>: These platforms handle petabyte-scale data through a combination of a scalable architecture with federated data storage to hold large types of unstructured data, such as documents, emails, audio, video, images, and so on. These platforms are designed as open platforms extendable at every layer of the stack. The provision for efficient data discovery, data lineage, and elastic search tools should be considered.</li>
<li><strong>Collaboration</strong>: This platform enables multiple users, within and across organizations, to seamlessly, securely collaborate to analyze the same data concurrently, from low-level data integration, importing pipeline customizations, to building custom user interfaces. Data that has been integrated can be accessed as objects via APIs or can be exported for other frameworks and tools</li>
<li><strong>Building models on models</strong>: Simple models can serve as building blocks for more complex models, building out sophisticated analyses to be streamlined as a modular process. Models can be built using various in-built rich reusable libraries of statistical and mathematical operators.</li>
<li><strong>Data visualization</strong>: This is an interactive user interface to provide a seamless holistic view of all the integrated data of interest in the form of rich visualizations, such as tables, scatter plots, and charts. These visualizations in real-time are up to date with the source data so that users always see the most accurate and current information at any given time.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Big data systems life cycle</h1>
                </header>
            
            <article>
                
<p>Big data systems are built in accordance with the data life cycle model, which can be broadly categorized in the following stages:</p>
<ul>
<li>Data discovery</li>
<li>Data quality</li>
<li>Ingesting data into the system</li>
<li>Persisting the data in storage</li>
<li>Analytics on the data</li>
<li>Data governance</li>
<li>Visualizing the results</li>
</ul>
<p>We will study them in detail next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data discovery into the system</h1>
                </header>
            
            <article>
                
<p>Data discovery, like in the traditional process, ingests raw data from multiple source systems; however, the data will be divergent in volume, variety, and velocity when it comes to transforming it into business insights. Leveraging the power of big data, the data discovery process enables data wrangling and data enrichment facilitates combining datasets to recreate new perspectives and interactive visual analytics. An interactive data catalog facilitates guided search capabilities and enables us to thoroughly analyze and understand the data quality. A matured and robust data discovery process ensures possible data correlations; it lets users define attribute-based rules, relationships between data sources, harmonize data sources, and create enriched data based on a data mart.</p>
<p>To expand the boundaries of traditional business intelligence systems, harnessing the potential of big data is the key for business success. It helps to unlock the insights from new sources of information effectively. Organizations have the potential to access a wealth of knowledge to be tapped appropriately, with the proliferation of new sources of digital information.</p>
<p>Data discovery with big data tools and technology facilitates deep exploration for visibility into business performance with a big variety of data across the organization and even beyond. The business can explore new dimensions, transforming the way that business analytic systems are built and used more efficiently. Data discovery with any combination of data sources allows rapid, intuitive exploration and analysis of information; it enables deeper insight into the business, and the opportunity for greater efficiency. It builds new relations redefining roles between business and IT, adding new roles, new leadership, and revised means of data governance as well.</p>
<p>Many organizations have updated business intelligence decision systems to improve business performance based on existing data and systems to understand and monitor. These days, the vast majority of data growth is from systems beyond the reach of traditional BI environments, such as websites, social media, content management systems, emails, documents, sensor data, external databases, and so on. Hence the need to adopt to new age data discovery tools, also to consider that this diverse and changing data is growing exponentially. Data types to be dealt by the discover phase are varied, ranging from structured database tables to semi-structured forms containing a mix of numbers and free-form text to wholly unstructured documents.</p>
<p>The biggest challenge along with the volume of data is the variety rather and its uncertain value. An internet-savvy business culture and the impact of consumer interaction with enterprise business software with mobile and web applications have created an urgent need to quickly explore relevant information to discover new insight for business prospects and decisions.</p>
<p><strong>Datafication</strong> is the process of quantifying data from all types of sources, in all types of formats. Datafication allows information to be collected, tabulated, and analyzed, so that the potential uses of the information are limited only by the ingenuity of the skilled business user. The data's true value is like an iceberg floating in the ocean. Only a tiny part of it is visible at first sight, while much of it is hidden beneath the surface. Innovative companies that understand this can extract that hidden value and reap potentially huge benefits.</p>
<p>Enterprise-class data discovery systems enable rapid, intuitive exploration and analysis of data from a wide combination of structured and unstructured sources. They enable organizations to channel their existing investments to extend business analytics capabilities to new combinations of a greater variety of sources, including social media, websites, content systems, e-mail, and database text, providing a new level of visibility into data and business processes, saving time and cost and leading to better business decisions. Some of the advantages of this approach are:</p>
<ul>
<li>You can gain deeper insight into the business by enabling users with increased insight and visibility to find the data they want to analyze.</li>
<li>Near real-time data and content can be delivered by access to fresher information, helping people make decisions based on the most current information.</li>
<li>Increased reuse of assets. You are able to reuse information assets and eliminate the costs of re-creating these assets.</li>
<li>Ease of use for BI professionals to develop and deliver analytic consumer-style applications for business professionals, leading to higher adoption rates, lower training costs, and faster time to value.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data discovery stages</h1>
                </header>
            
            <article>
                
<p>The data discovery process involves stages such as prototyping, visualization, bridging, replication, and transformation. These concepts applied in tandem in the context of data discovery provide value out of big data:</p>
<ul>
<li><strong>Prototyping</strong>: Generally, in any business context, projects are always under pressure and constrained to deliver under-limited resources of time and budgets. For a complex project, prototyping is an effective way of making progress when challenges are pressing, demanding, seem difficult, and are under time constraints. Prototyping is about acting before you've got the answers, about taking chances in the absence of a proven formula or a known way of solving a problem. Prototyping lets us test a hypothesis and explore alternative approaches, building in small blocks to get the big picture. DevOps expedites the process of prototyping to make it a continuous cycle of development</li>
</ul>
<p style="padding-left: 60px">With data discovery, people are empowered to wander around in the data, try new combinations of sources, filter and refine the analysis, find previously hidden patterns, or jump to another experimental thread if the first one yields no insight. One of the key challenges is synchronizing data schema changes at the source systems with staging and development systems in real time; the DevOps methodology and process can be adopted to address this.</p>
<p style="padding-left: 60px">Experimentation through data discovery comprises three main tasks--asking new questions, seeing new patterns, and adding new data. These steps comprise a continuous process which is iterative and which flows in any direction, depending upon what the user sees or theorizes at any given moment. DevOps integrated with the data discovery phase makes it an automated process from source systems identification to ingestion of data to staging systems.</p>
<ul>
<li><strong>Visualization</strong>: Data visualization helps data analysts gain immediate feedback on their hypotheses, as graphics are more convenient instruments for realizing patterns in quantitative information. Viewing the results of empirical business analysis in real time enables data analysts to determine what refined or further search could lead to a deeper understanding of a performance gap or market opportunity. Effective data discovery is augmented by quick visualization of analysis and results to aid both experienced data analysts and non-technical business users. Data visualization techniques such as drag and drop dashboards, quickly produced charts, easy to use wizards, and consumer-style navigation, accelerate understanding of patterns of data, and its behavior attributes quickly.</li>
<li><strong>Bridging</strong>: Business and IT can have a more effective working relationship through the data discovery process. In a traditional setup, business and IT perform separate activities as a provider and user in the data discovery phase. This enables them to work together as a team to combine efforts to jointly explore and learn. IT analysts help their business partners, showing them how to add data sources, refine searches, and explore new questions, stepping through the capabilities of the software. They can work with the business directly on how to build discovery applications in real time, and how to become self-sufficient in analysis of the possibilities. Conventional extracting requirements, writing complex specs, and going through a lengthy development and deployment process are tedious in terms of time and effort. IT gains huge efficiencies bypassing much of the SDLC process and by empowering business users via self-service data.</li>
<li><strong>Systematizing</strong>: As data exploration becomes more versed with deep insight skills, in some cases a certain path of investigation could be repeatable a few times. It could also yield valuable insights beyond a limited one-time scenario to more effectively grasp a part of the business question to continually repeat and track the outcome. Once the business queries and responses are well established by the metrics and dimensions that are used to describe key business processes, they can be considered to roll the analysis and metrics into the organization's enterprise BI system. Business intelligence platforms excel in allowing users to perform standardized queries for well-known questions on standard datasets.</li>
</ul>
<p style="padding-left: 60px">Data discovery and business intelligence systems complement each other, with data discovery excelling in unknown questions and BI focusing on systematized analysis, as depicted in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="275" src="assets/e727252e-fd20-4acf-a1fb-3f486db0a40d.png" width="433"/></div>
<p style="padding-left: 60px">DevOps will standardize the process of data models, dashboards, and visualization reports can be maintained into the repository and automated testing and deployment from development systems to QA and production as continuous integration and deployment models.</p>
<ul>
<li><strong>Transformation</strong>: The data discovery process can help explore an unlimited number of new avenues to address business problems and find new opportunities previously hidden in the data. Uncovering these new dimensions through a process of experimentation uncovers valuable new insights, paving the way for transformation. However, for standard known areas, they will continue using existing BI systems, and use data discovery to explore ways to give insights for new questions and problems. Thus data discovery is proving its value in deploying an easy exploration of diverse data to uncover insights that drive dramatic increases in revenue and productivity while improving the alignment and relationship of business and IT. It enables a transformation in the world of business analytics.</li>
</ul>
<p>There are several tools in big data discovery, such as Apache PIG, Oracle Big Data Discovery, Zoomdata, Exasol, Revolution, GridGain, and so on.</p>
<p>Traditional BI tools such as Tableau, QlikView, Microstrategy, Informatica, and so on, are also offering extensive data discovery features for big data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data quality</h1>
                </header>
            
            <article>
                
<p>One of the biggest challenges is ensuring data quality and accuracy, which is easier said than done. DevOps will augment the data quality process to ensure the data quality cycle from scripts to automation of the entire validation process. Let's look at some of the challenges:</p>
<ul>
<li><strong>Data variety</strong>: The data coming from diverse data sources such as mobile devices, web technologies, sensor data, social media, and so on brings with it multiple complex data types and data structures, increasing the difficulty of data integration. These data sources produce data types such as unstructured data in the form of documents, video, audio, and so on, and semi-structured data like software package, modules, spreadsheets, financial reports, and structured data. The valuable insights gained depend on the data collected, stored, and verified. They come from so many divergent sources and rely on the effectiveness of the integration process. When working with data-intensive and sensitive industries such as life sciences, this process has to be foolproof.</li>
<li><strong>Complexity of data</strong>: The data becomes complex, accounting for multiple attributes such as raw data from a variety of different sources directly from consumers, salespeople, operations, and other sources within the organization. The timeliness of the data is crucial; you need to gather the required data in real time or deal with the data needs in real time, otherwise the data can become stale, outdated, and invalid. Processing analysis based on the data will produce useless or misleading information and conclusions, misleading the decision-making systems.</li>
<li><strong>Ensuring data security</strong>: There are so many technologies that contribute to multiple channels of communications across applications of mobile, web, and ERP, which adds to the complexity of maintaining data security. New age technologies such as cloud, big data, and mobile are expanding their reach very quickly and becoming popular. However, data security needs and challenges are more complex than before.</li>
</ul>
<p>General guidelines to ensure data quality are the following:</p>
<ul>
<li><strong>Data availability</strong>: The data intended for consumption to the big data systems should be appropriately available for the intended use. It should be timely data with either real-time streaming or batch mode. The data should be accessible with API interfaces available and also the data should be authorized for use.</li>
<li><strong>Data appropriateness</strong>: The data should be credible; otherwise, the purpose is defeated. The data lineage process traces the origin of the data. The data definitions should be appropriate and acceptable on freshness and regular updates for the data. The data documentation and metadata should be audited for correctness within an acceptable range of values.</li>
<li><strong>Data accuracy</strong>: The data should be reliable; the data value should represent the true state of the source information and the data should not be ambiguous and should be a single version of a fact. Data should be consistent and verifiable during the intended time as per the time stamp. The data should be complete and auditable.</li>
<li><strong>Data integrity</strong>: The data format should be clear and meet the set criteria of consistency with the structure and content. Its integrity should be intact. The data should be relevant and match the required purpose, and be complete in all aspects and fit to use for its intended purpose.</li>
<li><strong>Presentation quality</strong>: The data content and format for the data should be clear and understandable. The data description, classification, and coding content should be easy to understand and meet the stated objectives and specifications.</li>
</ul>
<p>There are many big data open tools providing multiple features for data quality, such as Talend, AB Initio, Data Manager, Datamartist, and iManage Data.</p>
<p>In the data ingestion process, raw data is added to the system from multiple data sources. The process can be complex, depending on the format and quality of the data from the source systems and the state of the data to be processed from the desired target state for consumption. There are multiple ways and means of ingesting data into big data systems, depending on the type of big data ingested. To prepare raw data for the system's use, some level of analysis, sorting, and labeling usually takes place during the ingestion process. Extending conventionally from the legacy data warehousing processes consists of extracting, transforming, and loading, referred to as the ETL process. Some of the same concepts apply to data entering the big data system as well.</p>
<p>The process of data ingestion involves massaging the input data for proper formatting, categorizing, and labeling. The data structure should adhere to predefined standards by eliminating unwanted data. The data thus captured from multiple source systems in large volumes is used for further processing. It is stored in a data lake in raw format</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch processing</h1>
                </header>
            
            <article>
                
<p>Batch jobs, as the name indicates, are dataset jobs which are non-time sensitive and processed in batches in large datasets. The processing of batch jobs can happen in multiple modes, as described next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RDBMS to NoSQL</h1>
                </header>
            
            <article>
                
<p>Most of the legacy data stored in RDBMS can be imported into NoSQL databases on HDFS using Sqoop, DevOps can aid for baseline of the Sqoop scripts, automating the process of importing and exporting the data, scalability of the storage and computing systems, test automation of the data integrity, and automated deployment.. The data migration process is described here:</p>
<ul>
<li>Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. It supports incremental loads of a single table or a free form SQL query as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in Hive or HBase.</li>
<li>Oozie can be used to schedule and create flows for importing/exporting data.</li>
<li>Full as well as incremental imports can be configured in Sqoop/Oozie</li>
<li>We can directly import and create Hive tables, but if we build a layered architecture it is suggested to import to the staging.</li>
<li>For example--<kbd>sqoop import -connect jdbc:mysql://:/ -username -password --table --target-dir</kbd>.</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="294" src="assets/a9a8a246-8299-4065-84ef-8292381e98a2.png" width="244"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flume</h1>
                </header>
            
            <article>
                
<p>For other batch sources, we can deploy Flume. It will watch for new source files and push the data to HDFS.</p>
<ul>
<li>Flume is effective and reliable for moving log data in large volumes to a distributed system, collecting and aggregating it in accordance with business demand</li>
<li>The architecture is simple and flexible, matching the incoming streaming data flow needs</li>
<li>Flume is robust, fault-tolerant, customizable, with advanced features such as failover and a recovery mechanism, and so on</li>
<li>A simple extensible data model supports online analytic applications</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ed49622-6fcc-4849-ae46-48f9851634ce.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stream processing</h1>
                </header>
            
            <article>
                
<p>Stream processing, as the name indicates, is the computing of real-time data which is time-sensitive, usually with high-velocity metrics. Analytics are usually performed on the streaming data while it is being ingested for quality checks, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time</h1>
                </header>
            
            <article>
                
<p>If data is available in streams such as application logs, program output (like web scrapper), sensors, geo-location, or social media, this can be collected using Kafka on a real-time basis.</p>
<ul>
<li>Kafka is an open-source message broker project that aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. It is, in essence, <em>a massively scalable pub/sub message queue architected as a distributed transaction log, making it highly valuable for enterprise infrastructures to process streaming data</em>.</li>
<li>Kafka can be easily scaled to support more data sources and growing data volumes.</li>
<li>Kafka also supports direct connectivity to Spark.</li>
<li>There is one topic per incoming data source and one topic per consumer group.</li>
<li>The number of partitions per topic will depend on the data size.</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="296" src="assets/b08b30a6-0cd8-4657-89e0-09effbfb6389.png" width="264"/></div>
<p>Apart from Sqoop and Kafka, Some other specialized data ingestion tools for importing and aggregating both server and application logs are Apache Flume and Apache Chukwa. <strong>Gobblin</strong> also offers a data ingestion framework to aggregate and normalize date.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lambda architecture</h1>
                </header>
            
            <article>
                
<p>The lambda architecture is effective when both batch and streaming data are ingested to systems at the same time, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ddfe9eef-df2d-4862-b1c2-479da1a168fe.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The data storage layer</h1>
                </header>
            
            <article>
                
<p>Persisting big data has multiple storage options, such as Data Lakes and data warehouse; cloud technologies for data storage greatly augment the needs of big data storage systems, scalable to terabytes and petabytes through simple storage devices and virtual machines. DevOps is an effective solution for managing scalable data storage with infrastructure as code discussed in detail in this book:</p>
<ul>
<li><strong>Data Lake</strong>: Data Lake is synonymous with a water lake where water is stored and consumed by many people. A Data Lake is the repository for the collection of raw data. The data collected could be unstructured and frequently changing, so, initially the data is pooled into the large repository to be consumed by users as per their scheduled needs.</li>
<li><strong>Data warehouse</strong>: A data warehouse is an ordered repository for large volumes of data to be used for analysis and reporting. The data in a data warehouse is typically being cleaned, is well-ordered, and is integrated with other sources. It is generally more prominent with conventional systems.</li>
</ul>
<p>The ingestion process ensures the incoming data is processed as per business needs to be persisted reliably in the storage disk. The ingestion process can be complex, depending on the volume and variety of the data from the source systems. The availability of the distributed storage system is accomplished by Apache Hadoop's HDFS filesystem. With HDFS, large quantities of raw data are written to multiple nodes simultaneously with redundancy.</p>
<p>Ceph and GlusterFS are other filesystems offering all these capabilities.</p>
<p>Distributed databases such as NoSQL are well placed to import data for more structured access. They have features such as fault tolerance, and they have the capability to ingest heterogeneous data formats. Based on the organization's business needs, appropriate databases can be selected from a variety of available choices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data storage - best practices for better organization and effectiveness</h1>
                </header>
            
            <article>
                
<div class="CDPAlignCenter CDPAlign"><img height="275" src="assets/fdae2dc1-b9d9-4e7d-bcea-62398e3a1785.png" width="453"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Landing</h1>
                </header>
            
            <article>
                
<p>A landing zone is where in the initial data lands from different source systems in the as it is state to the storage system.</p>
<ul>
<li>The landing zone is where incoming data is stored</li>
<li>All input validation should be done here</li>
<li>The folder structure can be <kbd>&lt;source&gt;/&lt;type of data&gt;/&lt;yyyymmddhhisss&gt;</kbd></li>
<li>The archive mechanism should be applied as well (day/week/month)</li>
<li>Access should be restricted to only processing users and not end users</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Raw</h1>
                </header>
            
            <article>
                
<p>Once the data lands in the landing zone, it undergoes sanity checks as to its appropriateness, format, and quality; upon satisfactory compliance, it is stored as raw data.</p>
<ul>
<li>This is where the raw data is stored in its original format</li>
<li>Validated input from the landing layer is stored here</li>
<li>The directory structure is managed by the ingestion framework</li>
<li>Only selected super-users and system users will have access to this data</li>
<li>Snappy/LZO compression should be applied</li>
<li>A data classifier should be applied (hot, cold) and set to the archive policy</li>
<li>The folder structure can be <kbd>&lt;base dir&gt;/&lt;system type&gt;/&lt;dataset Source Name&gt;/&lt;Source Type&gt;</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Work</h1>
                </header>
            
            <article>
                
<p>The work area stores identified clean data to be used for business purposes.</p>
<ul>
<li>This is the temporary working area and cleanup up should take place after related jobs unless the jobs require data for debugging</li>
<li>The checkpoint location for Spark streaming will be located here as well</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gold</h1>
                </header>
            
            <article>
                
<p>Gold data is the valuable data that is the transformed, partitioned, and classified as master data.</p>
<ul>
<li>This is the location that stores transformed data</li>
<li>Partitioning is important in this layer and should be done based on the most frequently accessed columns</li>
<li>Classification should be done for very hot, hot, cold, and very cold data</li>
<li>Very cold data should be archived to blob storage (or any other cheap storage)</li>
<li>The folder structure could be <kbd>&lt;base dir&gt;/&lt;system type&gt;/&lt;dataset Source Name&gt;/&lt;Source Type&gt;/&lt;Job ID&gt;</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quarantine</h1>
                </header>
            
            <article>
                
<p>This is the place where unwanted and stale data, which is of no active immediate use, is saved.</p>
<ul>
<li>All rejected files from the various steps will be stored here, such as ingestion, transformation, as well as validation exception records</li>
<li>Classification should be done for very hot, hot, cold, and very cold data
<ul>
<li>Very cold data should be archived to blob storage (or any other cheap storage)</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Business</h1>
                </header>
            
            <article>
                
<p>Business data is the master data for a client's particular details, such as address, product preference, and so on.</p>
<ul>
<li>This layer will have application/client-specific data</li>
<li>All data must be store in parquet format since all data at this stage will be structured</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Outgoing</h1>
                </header>
            
            <article>
                
<p>Outgoing data is what is to be shared with external entities, such as suppliers, vendors, or third-party APIs.</p>
<ul>
<li>This is the location from where data can be shared with the external application</li>
<li>Files need to be archived once they are copied</li>
<li>Clients can have temporary or permanent access</li>
</ul>
<p>A good easy-to-use backup and disaster recovery solution provides integrated data sync between Hadoop clusters. It enables data protection by replicating data stored in HDFS platforms and across data centers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing and analyzing data</h1>
                </header>
            
            <article>
                
<p>The data made available from the preceding processes can be analyzed to unearth the actual information value. The computational layer performs diverse functions where data is often processed iteratively with a combination of tools. The different types of insight needed for business needs are extracted by tailored practices that vary from organization to organization.</p>
<p>Batch processing is one method of computing over a large dataset where data is ingested in batch mode either daily or hourly. Apache Hadoop's MapReduce is the most prominent and powerful batch processing engine and it is known as a distributed Map Reduce algorithm; it adopts the following strategy and is most useful when dealing with very large datasets that require quite a bit of computation:</p>
<ul>
<li><strong>Splitting</strong>: In this process, the work is divided into smaller pieces</li>
<li><strong>Mapping</strong>: This is the process of scheduling each piece on an individual machine</li>
<li><strong>Shuffling</strong>: Reshuffling data based on the intermediate results</li>
<li><strong>Reducing</strong>: Processing each group of output data</li>
<li><strong>Assembling</strong>: The final result is assembled together</li>
</ul>
<p>The MapReduce framework forms the compute node while the HDFS filesystem forms the data node. Typically, in the Hadoop ecosystem architecture, both the data node and compute node perform similar roles. The delegation tasks of the MapReduce component are performed by two daemons, the <strong>Job Tracker</strong> and <strong>Task Tracker</strong>, Their activities are shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3a3405a6-ad22-4820-b3f5-fba4b8975a6b.png"/></div>
<p>Data processing in batch, real-time, and stream processing is discussed next. DevOps is integral to big data systems for high volume data processing from source system discovery to the scalability of infrastructure to support storage needs.</p>
<ul>
<li><strong>Batch processing</strong>: It is very efficient in processing high volume data.The data is ingested to the system, processed, and then results are produced in batches. The computational power of the system is designed based on the size of the data being processed. The systems are configured to run automatically without manual intervention. The system can scale very quickly to accommodate the entire dataset for computational analyses of the huge volume of data files. Based on the volume of data processed and the computational power of the system defined, the output timelines can vary significantly.</li>
<li><strong>Real-time/stream processing</strong>: Though batch processing is a good choice for certain types of data and computation, other workloads require a more low-latency turnaround. There are real-time systems which are required to respond in real-time as they process information and make the analytics or visualizations readily available to the business while assimilating the new information continuously. These systems are called stream processing, which operates on a continuous stream of data composed of individual items. These systems, real-time or stream processing systems, utilize the real-time processing capability of in-memory engines; computational analytics are performed in the cluster's memory to avoid having to write back to disk as in traditional disk-based persistent systems. Real-time processing is best suited for analyzing smaller chunks of data that are changing or being added to the system rapidly.</li>
</ul>
<p>There are many platforms and tools to achieve real-time processing, such as Apache Storm, Apache Flink, and Apache Spark. Each of them is designed as different ways of achieving real-time or near real-time processing. Apart from these listed computational frameworks, there are many other means of analyzing data or performing computations within a big data ecosystem. These tools frequently plug into the aforementioned frameworks and provide additional interfaces for interacting with the underlying layers. We have already discussed in the previous chapter their applicability to different scenarios and the best application for any individual problem, but we will recap a few of the tools here again:</p>
<ul>
<li>Apache Hive provides a data warehouse interface for Hadoop</li>
<li>Apache Pig provides a high-level querying interface</li>
<li>Apache Drill, Apache Impala, Apache Spark SQL, Presto, and so on, provide SQL-like interactions with data</li>
<li>R and Python are popular choices for simple analytics programming</li>
<li>Apache SystemML, Apache Mahout, and Apache Spark's MLlib, provide building prediction models for machine learning libraries</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark analytic platform</h1>
                </header>
            
            <article>
                
<p>Apache Spark is a next-generation in-memory open-source platform, combining batch, streaming, and interactive analytics under one umbrella. Spark facilitates ease of use, providing the ability to quickly write applications with built-in operators and APIs along with faster performance and implementation.</p>
<p>Spark provides a faster and more general data processing platform and runs programs up to 100x faster in memory, or 10x faster on disk, than Hadoop, with lightning-fast cluster computing. The Spark framework is built on top of Hadoop clusters to process data from structured system such as Hive and stream data from Flume and Kafka. It has many advanced features and supports a variety of languages, including Java, Python, and Scala. It has extensive features for analytics, out-of-the-box algorithms, machine learning, interactive queries, and complex function analytics.</p>
<p>Spark's popularity is due to many advantages, including:</p>
<ul>
<li>Spark is a subset of the Hadoop ecosystem. It integrates well with the reliable and secure storage platform, HDFS of the ecosystem. It is compatible with other data sources, such as Amazon S3, Hive, HBase, and Cassandra. It can run on clusters managed by Hadoop YARN or Apache Mesos, and can also run as a standalone.</li>
<li>Spark is a fast technology to enable large-scale data processing. This framework provides Java-, Scala-, and Python-based high-level APIs with a rich set of data stores for stream processing and machine learning. Though primary APIs are for Scala, Java, and Python, languages such as R are also supported.</li>
<li>Spark ensures parallel processing for data, very well integrated with Hadoop/HDFS for data storage, and to support variety of filesystems and databases.</li>
<li>Spark's machine learning capabilities are proved to be excellent solution for stream processing. With Spark REPL writing code is easier and quicker with inbuilt high-level (80) operators . Spark's <strong>Read Evaluate Print Loop</strong><span> (<strong>REPL</strong>) </span>is interactive (out-of-the box) shell is a modified version of the interactive scala REPL. With REPL, no need to compile and execute the code. User expressions are evaluated and REPL will display the results of the expression. The <em>Read</em> takes expression as an input and parses and stores in memory as an internal data structure. <em>Eval</em> traverses the data structure, evaluates the called functions. <em>Print</em> displays the results with print ability. <em>Loop</em> iterates going back to read state to terminate the loop on exit. REPL expedites the turnaround time also supports ad hoc data query analysis.</li>
<li>Spark is more nimble and well suited for big data analytics. Continuous micro-batch processing with integrated advanced analytics is based on its own streaming API, which is developer-friendly.</li>
<li>Spark is more efficient compared to MapReduce. It is 100 times faster than MapReduce for the same process.</li>
<li>The popular batch job processing engine for Hadoop, MapReduce poses a significant challenge due to its high-latency to the batch-mode response and it is difficult to maintain due to inherent inefficiencies associated with its architecture design and code. Spark's main component is the Spark Core Engine. It is complemented by a set of powerful, higher-level libraries that can be seamlessly used in the same application:
<ul>
<li>Spark SQL is a powerful query language with inbuilt DataFrames</li>
<li>Spark Streaming engine is for data streaming</li>
<li>Spark MLlib for machine learning along with machine learning pipelines models</li>
<li>GraphX with GraphFrames stores relationship between entities as a graphical representation.</li>
</ul>
</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="169" src="assets/a6de56f0-97e8-40bf-adde-a96b9aa8c862.png" width="256"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark Core Engine</h1>
                </header>
            
            <article>
                
<p>The base engine for Spark Core to perform large-scale parallel and distributed data processing is the Spark Core Engine. It performs the following functions:</p>
<ul>
<li>Fault-tolerant and recovery-based memory management</li>
<li>Cluster job scheduling, distributing, and monitoring</li>
<li>Storage device systems interfacing</li>
</ul>
<p>Spark is built and based on an immutable, fault-tolerant, distributed collection of objects called <strong>Resilient Distributed Dataset</strong> (<span><strong>RDD</strong>)</span> that can be operated on in parallel. Objects are created through loading an external dataset or distributing from the internal driver program. The operations performed on objects through RDD are transformations.</p>
<p>Transformation operations include map, filter, join, and union, and are performed on RDD and yield a new result.</p>
<p>Action operations include reduce, count, first and they return a value after running a computation on RDD.</p>
<p>The Spark Engine is designed efficiently. The transformations are actually computed when an action is called and the result is returned to the driver program. Transformations are lazy and do not compute their results right away; however, they remember the task to be performed and the dataset (for example, a file) to perform the operation. The transformed RDD can be recomputed for the next transformation. The advantage is avoiding unnecessary changes and computations in the process. All this is achieved by the in-memory engine where the data is persisted and cached for the RDD objects. Spark will keep the elements around on the cluster-cached memory for much faster access the next time you query it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark SQL</h1>
                </header>
            
            <article>
                
<p>The Spark component, Spark SQL, supports querying data either through SQL or through the Hive query language. Spark SQL is integrated with the Spark stack providing support for various data sources. It allows you to weave SQL queries with code transformations, making it a very powerful tool.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark Streaming</h1>
                </header>
            
            <article>
                
<p>Spark Streaming is a powerful functionality built on in-memory technology. The Spark Streaming API is compatible with the Spark Core Engine. It facilitates both batch and streaming data to process real-time streaming data from systems such as web server log files, Twitter-based social media data, and so on. Spark interfaces with other tools, such as Kafka, for various messaging queues.</p>
<p>Spark Streaming receives the input data from upstream systems such as Apache Flume, Kafka, and so on, and divides the data into batches to process them with the Spark Engine and generate a final stream of results in batches to store them on HDFS/S3, and so on.</p>
<div class="CDPAlignCenter CDPAlign"><img height="285" src="assets/f28d00a5-e3c5-4e7b-b5b2-a3b0c1d8b977.png" width="460"/></div>
<p>MLlib is a set of library functions that provides various algorithms of machine learning, such as classification, regression, clustering, and collaborative filtering. Apache Mahout (a machine learning library for Hadoop) is integrated into Spark MLlib. A few algorithms such as linear regression or k-means clustering also work with streaming data designed to scale out on a cluster.</p>
<p>GraphX provides ETL functionality, exploratory analysis, and iterative graph computations. It provides a library for manipulating graphs and performing graph-parallel operations on common graph algorithms such as <strong>page rank</strong>.</p>
<p>Spark is ideal to simplify challenging and compute-intensive task for real-time data processing of high volumes of streaming or archived data, both structured and unstructured, seamlessly integrating relevant complex capabilities, such as machine learning and graph algorithms.</p>
<p>Some challenges include operational complexity and the high skills required to develop and manage applications. Spark performs well with Hadoop to take advantage of Hadoop's HDFS. Performance tuning of both systems is imperative; otherwise, Spark's nuances can lead to out-of-memory error issues and memory lag, if jobs are not tuned well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualization with big data systems</h1>
                </header>
            
            <article>
                
<p>We are well versed with the saying: <em>garbage in, garbage out</em>. Identifying and recognizing trends, variations, and changes in data over time is often more important and data visualization is an inevitable step. Due to the complexity of information being processed in big data systems, visualizing data is one of the most important and useful ways to spot trends and create meaningful insights from a large number of data points.</p>
<p>We will discuss some real-time processing tools here:</p>
<ul>
<li><strong>Prometheus</strong>: To visualize application and server metrics in real-time processing, data streams as a time-series database and visualizes that information. The health of the systems is gauged by the frequent data changes and large variations in the metrics typically indicate significant KPIs.</li>
<li><strong>Elastic Stack</strong>: It is popular for visualizing big data systems to visually interface with the results of calculations or raw metrics. It is also known as the ELK stack, composed of Logstash for data collection, Elasticsearch for indexing data, and Kibana for visualization.</li>
<li><strong>SILK</strong>: It is a similar stack achieved by using Apache Solr for indexing and a Kibana fork called <strong>Banana for visualization</strong>.</li>
<li><strong>Jupyter Notebook</strong> and <strong>Apache Zeppelin</strong> offer visualization interfaces for interactive exploration and visualization of data in a format conducive to sharing, presenting, or collaborating. This technology is typically used for interactive data science work and is termed a data <em>notebook</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data governance</h1>
                </header>
            
            <article>
                
<p>Data governance is the process of classifying enterprise data and providing the right access and privileges for appropriate roles and personnel. It refers to the overall process of managing the availability, usability, integrity, and security of the data assets in an organization. A matured data governance model is a defined set of strategies, and includes a governing group and a well-orchestrated plan to execute those procedures.</p>
<p>Adopting an open source software development approach for platform/product development will ensure many advantages, such as the following:</p>
<ul>
<li>Seamless collaboration across different diverse technology teams spread across the organization</li>
<li>Leverage and re-use of existing software and knowledge assets across the organization</li>
<li>It ensures region, client, and country-specific needs are addressed</li>
</ul>
<p>This approach for software development helps a governance mechanism in place to ensuring avoid duplication of work and enable transparency following a common standard framework.</p>
<p>Data governance should define the minimum necessary rules instead of being an overhead. It should also balance across:</p>
<ul>
<li>Rules versus public (free for all)</li>
<li>People versus process</li>
<li>Empowerment versus directing</li>
</ul>
<p>Open source governance operates on three pillars:</p>
<ul>
<li>Transparency</li>
<li>Set governance parameters</li>
<li>Faster delivery of every team and every member</li>
</ul>
<p>Open source communities that practice transparency, encourage active participation, and recognize the contributions of all constituents are more likely to thrive, iterate, and strengthen their prospects. The guiding principles of governance in the open source community development model can be better demonstrated by describing the seven pillars illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/530f5272-4684-40bf-8606-8a2ca5c1d53f.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">People and collaboration in accordance with DevOps core concept</h1>
                </header>
            
            <article>
                
<p>Technology is driven by people; therefore, the success of technology depends on a few attributes--it should be quite easily adoptable for people, it should be flexible, easy to learn, and convenient to collaborate with. We will discuss them here:</p>
<ul>
<li>Establishing ownership for the process example code, submission, and review process</li>
<li>A common dashboard to check the status of any component, changes made, review status, testing, impacted components, and so on</li>
<li>Adopt forums for discussions for resolution of queries than e-mail, Yammer Groups, and so on</li>
<li>Regular (weekly, bi-weekly) all-hands deployment to discuss changes in components and roadmaps</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Environment management</h1>
                </header>
            
            <article>
                
<p>Management of environments in information technology is a complex and critical function.</p>
<ul>
<li>Parallel environments for development to be maintained, for example, current programs and other production fixes</li>
<li>Define access restriction for components to interact with the database</li>
<li>Define allocation of resources (disk space, threads/mappers, and so on) for each component/program</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Documentation</h1>
                </header>
            
            <article>
                
<p>Documentation of the ongoing work is important for the shell life of the project, features upgrades, and also to support and maintain as an operations manual.</p>
<ul>
<li>Comprehensive documentation of the core components, business/assembly line usage, governance, and so on. It is important for collaboration across teams, new members, and so on.</li>
<li>The artifacts documents to help each team member could be revised regularly to add more granular details:
<ul>
<li>Master architecture reference document--created in TOGAF suggested format to speak in a common language</li>
<li>Developer guides</li>
<li>Governance guide</li>
<li>Deployment guide</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture board</h1>
                </header>
            
            <article>
                
<p>The architecture board has the responsibility for the long-term enterprise and for ensuring the architecture meets the business goals such as service-oriented architecture, usage of components meeting the security guidelines, open source tools usage percentage as a roadmap, and so on.</p>
<ul>
<li>The <strong>Architecture Review Board</strong> (<strong>ARB</strong>) is the authority for defining and participating in Tollgate (or milestone) planning</li>
<li>Define the Tollgate, the milestone for every program's high-level design</li>
<li>Budget the re-factoring effort in every program and approve this in Tollgate meetings</li>
<li>Centralize decision making for design approvals</li>
<li>Review checklists used for Tollgate and reviews</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Development and build best practices</h1>
                </header>
            
            <article>
                
<p>Development best practices are the adoption of coding standards, adequate documentation, peer reviews, quality of the code, and so on, to ensure high quality of the code and performance. Build is a complex task with many interfaces. Adherence to proper guidelines will make it robust and well functioning as per the organization's needs.</p>
<ul>
<li>Automated builds with automated testing processes</li>
<li>Peer review of source code based on sample</li>
<li>Common IDE, code review tools (PMD, check style, and so on), build tools (Hudson and Maven)</li>
<li>Standard build promotion procedures and schedules</li>
<li>Publish environment stability on a weekly basis through continuous integration and testing</li>
<li>Define a test bed and regression suite to execute impacted modules</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Version control</h1>
                </header>
            
            <article>
                
<p>Version control will ensure code changes are well tracked for traceability and accountability.</p>
<p>All of the organization teams using an enterprise standard tool for version control is the ideal. Governance in versioning has the following attributes:</p>
<ul>
<li>Automated email for every check-in to a controlled group of supervisors mailing list</li>
<li>Define contributors for every program component and restrict access to the components</li>
<li>Periodic audit of check-ins and approval from a component owner</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Release management</h1>
                </header>
            
            <article>
                
<p>A mature release management process is a strong asset for the organization to deliver timely dependable products to its customers with quality.</p>
<ul>
<li>Centralized release management</li>
<li>Common priority defined across programs and features</li>
<li>Employ microservices/incremental deployment--architecture for independent deployments.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building enterprise applications with Spark</h1>
                </header>
            
            <article>
                
<p>For enterprise applications to be successful, it is very important that you carefully define the data access, processing, and governance framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Client-services presentation tier</h1>
                </header>
            
            <article>
                
<p>This graphical user interface will be backed by a set of APIs to help on-board new users. Some features that can be supported are as follows:</p>
<ul>
<li>Manage client data sources, file formats, delivery frequency, validation rules, join conditions (if multiple datasets are present), and so on.</li>
<li>Validate and transform datasets</li>
<li>Manage access to datasets</li>
<li>Additional data delivery requirements from Eureka</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data catalog services</h1>
                </header>
            
            <article>
                
<p>This graphical user interface will be backed by a set of APIs to provide data-related services. Some of the features that can be supported are:</p>
<ul>
<li>Search for any dataset/data in the data lake in a fashion similar to Google Search</li>
<li>Browse (preview with pagination) search results</li>
<li>Display the lineage and data profile of the selected data set</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workflow catalog</h1>
                </header>
            
            <article>
                
<p>This graphical user interface will let you define workflows for an application and schedule runs. The main functionalities include:</p>
<ul>
<li>Create workflows for an application and schedule them</li>
<li>Display the execution status of workflows, the time taken, as well as the datasets involved along with the lineage</li>
<li>Ability to restart the process in case of a failure or from any given point</li>
<li>Configure status updates, notifications, and alerts</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Usage and tracking</h1>
                </header>
            
            <article>
                
<p>This graphical user interface will be in use for Sentry and Navigator to track the usage of datasets across the cluster. The main functionalities include</p>
<ul>
<li><strong>Valid usage tracking of datasets</strong>: How many times a dataset was accessed and by who</li>
<li><strong>Invalid usage tracking</strong>: Who tried to access a dataset they did not have access to</li>
<li>Process tracking in terms of which user is running what process and what resources are being consumed</li>
<li>Need to define dashboards based on requirements from the operations team</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Security catalog</h1>
                </header>
            
            <article>
                
<p>This graphical user interface will be backed by REST APIs to configure access control for user groups. The features included are as follows:</p>
<ul>
<li>Manage users and groups</li>
<li>Manage user access to various datasets across the cluster and applications that can run in the data lake</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Processing framework</h1>
                </header>
            
            <article>
                
<p>This is where all transformations and processing on the data will take place. Processing can be batch as well as real-time with support for various frameworks such as Spark and MapReduce, along with querying engines such as Hive and Impala.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ingestion services</h1>
                </header>
            
            <article>
                
<p>Data can be ingested from various sources, ranging from batch to real-time streams using tools such as Sqoop, Flume, Kafka, and SFTP.</p>
<ul>
<li>Ingestion will be metadata-driven</li>
<li>In order to ingest new data sources, new code is not required as long as it makes use of the supported ingestion methods, such as Kafka, Flume, Sqoop, and so on</li>
<li>We should be able to register datasets and start cataloging them into the data catalog as soon as they are ingested</li>
</ul>
<p>The ingested data can be used in multiple forms: stored, persisted into a device, published to external vendors. It can be accessed by other third-party programs through APIs.</p>
<ul>
<li><strong>Storage layers</strong>: In order to maintain data integrity and isolation, we can spread our data in HDFS across multiple layers so that each layer defines a certain stage between ingesting raw data and generating insights.</li>
<li><strong>Publishing</strong>: This service will be used to publish data to external users and subscribers, as well as applications, as an outward push from the data lake.</li>
<li><strong>Bulk API</strong>: This service will be used to download data from the system using an asynchronous API. Users will make requests for data retrieval and they will be notified when the data set is ready. Delivery of the data can be provided in multiple ways:
<ol>
<li>Download link.</li>
<li>Push to SFTP location.</li>
<li>HDFS location for internal users (same cluster or another cluster).</li>
<li>API contracts need to defined.</li>
</ol>
</li>
<li><strong>Data Access API</strong>: This API is similar to the Bulk API, but will only support small datasets such as a credit score to be synchronized frequently.</li>
<li><strong>Notebooks</strong>: These can include interfaces such as Apache Zeppelin or the Hue data science workbench, which will give a GUI interface for users to access datasets in the cluster and query them using Impala, Spark, R, and so on.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data science</h1>
                </header>
            
            <article>
                
<p>Data science as a field has many dimensions and applications. As we all are familiar with science by formulating reusable and established formulas, we understand the features, behavior patterns, and meaningful sights. In a similar way from relevant data too through engineering and statistical methods, we understand the behavior patterns and meaningful sights. Thus, it's also viewed as data plus science, the science of data or data science.</p>
<p>Data science has been in use for decades across industries. Many algorithms have been developed and are in use across the industries, including:</p>
<ul>
<li>K-means clustering</li>
<li>Association rule mining</li>
<li>Linear regression</li>
<li>Logistic regression</li>
<li>Naïve Bayesian classifiers</li>
<li>Decision trees</li>
<li>Time series analysis</li>
<li>Text analytics</li>
<li>Big data processing</li>
<li>Visual work flows</li>
<li>Apriori</li>
<li>Neural networks</li>
</ul>
<p>Combinations of the preceding algorithms are used to solve popular business problems such as the following, and new business opportunities are surfacing continuously:</p>
<table>
<tbody>
<tr>
<td>
<p><span>Cross-selling</span></p>
</td>
<td>
<p class="mce-root"><span>Find relationship among customer characteristics</span></p>
<p>Match campaigns to potential customers</p>
</td>
</tr>
<tr>
<td>
<p>Product yield analysis</p>
</td>
<td>
<p>Classify product defects</p>
</td>
</tr>
<tr>
<td>
<p>Direct marketing</p>
</td>
<td>
<p class="mce-root">Classify customers</p>
<p>Match campaigns to potential customers</p>
</td>
</tr>
<tr>
<td>
<p>Churn analysis</p>
</td>
<td>
<p class="mce-root">Predict the tendency of churning</p>
<p>Win back customers</p>
</td>
</tr>
<tr>
<td>
<p>Cross-selling</p>
</td>
<td>
<p>Find relationship of products in transactions</p>
</td>
</tr>
<tr>
<td>
<p>Segmentation analysis</p>
</td>
<td>
<p class="mce-root">Classify customers</p>
<p>Match campaigns to potential customers</p>
</td>
</tr>
<tr>
<td>
<p>Inventory analysis</p>
</td>
<td>
<p class="mce-root">Find relationship of products in transactions</p>
<p>Make replenishment decision</p>
</td>
</tr>
<tr>
<td>
<p>Product-mix-analysis</p>
</td>
<td>
<p class="mce-root">Classify customers</p>
<p>Match campaigns to potential customers</p>
<p>Estimate the revenue of product mix</p>
</td>
</tr>
<tr>
<td>
<p>Fraud detection</p>
</td>
<td>
<p class="mce-root">Classify customers</p>
<p class="mce-root">Detect unusual activities</p>
</td>
</tr>
<tr>
<td>
<p>Credit-rating</p>
</td>
<td>
<p class="mce-root">Classify customers</p>
<p>Credit rating customers</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For example, in data classification itself we can implement the following three models to get a refined and accurate model:</p>
<ul>
<li><strong>Random forests</strong>: Random forests or random decision forests are an ensemble learning method for classification, regression, and other tasks. They operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.</li>
</ul>
<div class="packt_infobox">Please check the following link: <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier"><span class="URLPACKT">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</span></a><span class="URLPACKT">.</span></div>
<ul>
<li><strong>Naive Bayes</strong>: Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters to be linear in the number of variables (features/predictors) in a learning problem.</li>
<li><strong>Support vector machine</strong>: Support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one of two categories. An SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier.</li>
</ul>
<p>The following are the steps towards building a prediction model to solve a business problem:</p>
<ul>
<li>Defining the tangible business goal is the most important criteria. The business value and purpose of the data science problem agreement by different stakeholders is the most important step.</li>
<li>Sponsorship and buy-in from key stakeholders is crucial to the success of the project.</li>
<li>Collaboration among data engineers and data scientists is critical; otherwise, working in silos will not lead to project success.</li>
<li>A data lake is a repository that gathers useful data from different source systems of appropriate, valid, meaningful, useful, and historical data required for the business problem. For building a data lake, capacity planning for the initial data and growth considerations for the future should be taken into account.</li>
<li>Cleanse data as per quality norms to ensure only valid and appropriate data is used and no dirty or stale data enters the system.</li>
<li>Feature engineering is core engineering of the data science project to extract meaningful insights (features) from the raw data.</li>
<li>Feature selection is to eliminate irrelevant, redundant, or highly correlated features.</li>
<li>Test the prediction models by following the proper validation methods, such as K-Fold Cross Validation or 70:30 models.</li>
<li>Establish the model results. As with any project, the repetition of results accuracy validates the model's effectiveness.</li>
<li>Optimize the model by continuous improvement with new data iteratively, and by fine-tuning.</li>
</ul>
<p>Many popular statistical modeling tools are on the market, such as:</p>
<ul>
<li>SPSS modeler</li>
<li>KNIME</li>
<li>Microsoft Revolution Analytics</li>
<li>RapidMiner</li>
<li>SAP Predictive Analytics</li>
<li>SAS Enterprise Miner</li>
<li>Oracle Advanced Analytics (Oracle Data Miner, Oracle R Advanced Analytics)</li>
</ul>
<p>A few popular open source tools offer all the functionality on a par with commercial established statistical packages:</p>
<ul>
<li>R</li>
<li>Python</li>
<li>Scala</li>
<li>MatLab</li>
<li>Julia</li>
</ul>
<p>The recent surge in low-cost technology availability such as Hadoop Eco systems, cloud computing, big data and open source tools has led to large-scale adoption by every industry from small enterprises to large giants.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Approach to data science</h1>
                </header>
            
            <article>
                
<p>The approach to data science solutions involves the following staged approach:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/064bb679-aa95-48ab-b565-0e068db1a8f9.png"/></div>
<p>Knowledge mining (discovery) in datasets is an interactive and iterative process involving several steps for identifying valid, useful, and understandable patterns in data.</p>
<ul>
<li><strong>Data processing</strong>: Data cleansing pre-transforms the raw data into an easy and convenient format for usage; a few related tasks are:
<ul>
<li>Sampling, that is, selecting representative subsets from a large population of data</li>
<li>Remove noise</li>
<li>Missing data handling for incomplete rows</li>
<li>Normalization of data</li>
<li>Feature extraction: data useful in a particular context is extracted</li>
</ul>
</li>
<li><strong>Data transformation</strong>: Ensure usability of data by missing data treatment methods like:
<ul>
<li>Use only rows with relevant data</li>
<li>Substitution of values:
<ul>
<li>Mean value of particular attribute is used</li>
<li>Regression substitution with historical value from similar case</li>
<li>Matching imputation, similar attribute correlation case is used</li>
<li>Maximum likelihood, EM, and so on</li>
</ul>
</li>
</ul>
</li>
<li><strong>Data mining</strong>: It is automating the searching patterns in the data by methods like:
<ul>
<li>Association rules</li>
<li>Sequence and path analysis</li>
<li>Clustering analysis:
<ul>
<li>Partitioning a data set into clusters or subsets based on some common attributes</li>
</ul>
</li>
</ul>
<ul>
<li>Classification methods:
<ul>
<li>Division of samples into classes</li>
<li>Trained set is used previous labelled data</li>
</ul>
</li>
<li>Regression models
<ul>
<li>Prediction of new values based on past data by inference</li>
<li>Compute new values for dependent variable values based on few other measured attributes</li>
</ul>
</li>
<li>Visualization patterns</li>
<li>Classification is similar to clustering but requires classes to be defined ahead of time:
<ul>
<li>Classification with classifier based on input label is returned</li>
<li>Probabilistic classification is where classifier returns probable values to assign them to a class</li>
<li>Specific criteria like data more than 90% to avoid costly mistakes</li>
<li>Assign objects to class based on probability limits (greater than 40%, and so on)</li>
</ul>
</li>
<li>Regression and forecasting
<ul>
<li>Data table statistical correlation:
<ul>
<li>Data distribution in functional forms with prior assumptions</li>
<li>Machine learning-based algorithms</li>
</ul>
</li>
<li>Curve fitting:
<ul>
<li>A well defined and known function underlying the data is explored</li>
<li>Theory- and expertise based</li>
</ul>
</li>
</ul>
</li>
<li>Machine learning:
<ul>
<li>Supervised:
<ul>
<li>Both input and desired results are part of training data</li>
<li>Model training process inputs are based on correct known target and results</li>
<li>Proper training, validation, and test set construction are crucial</li>
<li>Fast and accurate results</li>
<li>Ability to generalize, new data should produce correct results without prior knowledge of target</li>
<li>Generalization means the ability to produce valid outputs for inputs not available during training</li>
</ul>
</li>
<li>Unsupervised:
<ul>
<li>Correct results are not provided to the model during training</li>
<li>Input data clustered to classes based on their statistical properties alone</li>
<li>Significance of cluster and label</li>
<li>Even small number of objects which are representative of desired classes and labels can be applied</li>
</ul>
</li>
</ul>
</li>
<li>Curve fitting
<ul>
<li>Is by using proper subsets and early stopping</li>
<li>Based on data learning and not just underlying function</li>
<li>Data used in training should perform well with new data</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="176" src="assets/4c3da999-84f6-4ed8-8796-31c7f7e62f69.png" width="581"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li>Data sets:
<ul>
<li><strong>Training set</strong>: Used for learning where target value is known.</li>
<li><strong>Validation set</strong>: Used to tune classifier architecture to estimate error.</li>
<li><strong>Test set</strong>: Performance of classifier is assessed only; it's never used in the training process. Test set error should provide unbiased generalization error estimate.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="204" src="assets/312fc9c9-cc15-44a2-92a9-c97b076e211f.png" width="236"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Data selection</strong>: Garbage In , Garbage Out, underlying model representation should be training, validation, and test data.</li>
<li><strong>Unbalanced datasets</strong>:
<ul>
<li>Network minimizes overall error so the proportion of types of data in the set is critical</li>
<li>Loss matrix inclusion</li>
<li>An even representation of different cases is the best approach to interpret networks decision</li>
</ul>
</li>
</ul>
</li>
<li>Learning process:
<ul>
<li>Back propagation:
<ul>
<li>Output values compared with target value to compute the predefined error function</li>
<li>Error is fed back into the network</li>
<li>Using these inputs the algorithm adjusts the weights of each connection to reduce the value of error function.</li>
<li>The network will converge after repeating the process for longer training cycles for sufficient numbers</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="162" src="assets/9cf39fb0-b1cc-4dec-9ef3-fedfac318dd2.png" width="351"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li>Results:
<ul>
<li>Confusion matrix: The prediction results on X axis are compared to target values on Y. Rows represent the true classes and columns predicted classes.</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="211" src="assets/2695048c-bfd3-44fe-8db7-1698019e41c6.png" width="463"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li>Completeness and contamination
<ul>
<li>Performances are rated as the following criteria for the classifiers, for example between two classes:
<ul>
<li><strong>Completeness</strong>: The percentage of objects of class A correctly classified</li>
<li><strong>Contamination</strong>: The percentage of objects of class A incorrectly classified as objects belonging to class B</li>
<li><strong>Classification rate</strong>: The overall percentage of objects correctly classified</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised models</h1>
                </header>
            
            <article>
                
<ul>
<li>Neural networks</li>
<li>Multi layer perceptron</li>
<li>Decision trees</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network</h1>
                </header>
            
            <article>
                
<p>It is a structured flow consisting of the input layer of neurons and the output layer of neurons with one or more hidden layers in the middle.</p>
<p>Neurons are well connected with adjacent layers though being in different topologies, they are connection by a choice of activation function, the <strong>weights</strong> are assigned as function values associated with the connections of various types and architectures.</p>
<div class="CDPAlignCenter CDPAlign"><img height="378" src="assets/eefb53d0-7d3f-46a3-ac3a-09156bcc0cd0.png" width="285"/></div>
<p><strong>Artificial neural networks</strong> this is inspired by the biological nervous system process. An artificial neural network is an information-processing mechanism.</p>
<p>Highly interconnected large number of simple processing neuron elements working together to address specific problems.</p>
<p>A simple artificial neuron:</p>
<div class="CDPAlignCenter CDPAlign"><img height="301" src="assets/c9c38d79-06f3-41bd-934f-098d854172b8.png" width="309"/></div>
<p>A node or unit is a basic computational element that receives input from other units or external source.</p>
<p>To model synaptic learning, each input is considered with associated weight <em>w</em>.</p>
<p>The weighted sum of its inputs is computed as a function by the unit:</p>
<div class="CDPAlignCenter CDPAlign"><img height="280" src="assets/3e295094-0714-43c9-9891-a83e78188e96.png" width="462"/></div>
<p>The different types of neural network are:</p>
<ul>
<li><strong>Feedforward</strong>: <strong>Adaptive Linear Neuron</strong> (<span><strong>ADALINE</strong>)</span>, RBF, single layer perception</li>
<li><strong>Self-organised</strong>: SOM (Kohonen Maps)</li>
<li><strong>Recurrent</strong>: Simple recurrent network, Hopfield network</li>
<li><strong>Stochastic</strong>: Boltzmann machines, RBM</li>
<li><strong>Modular</strong>: <strong>Associative Neural Networks</strong> (<span><strong>ASNN</strong>)</span>, committee of machines</li>
<li><strong>Others</strong>: NeuroFuzzy, Cascades, PPS, GTM, Spiking (SNN), Instantaneously Trained</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="351" src="assets/0daf9047-0a3b-41d3-8981-1e5e6f53247c.png" width="304"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi layer perceptron</h1>
                </header>
            
            <article>
                
<p>This is a most popular supervised model consisting of multiple layers of computational units inter connected in a feed-forward way usually. Each neuron in one layer is connected to subsequent layer neurons through direct connections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree</h1>
                </header>
            
            <article>
                
<p>This is a classification method with a set of simple rules; they are non-parametric without the need for any assumptions on distribution of the variables in each class.</p>
<p>As example decision tree is depicted in following:</p>
<div class="CDPAlignCenter CDPAlign"><img height="343" src="assets/395ad59d-c432-4a07-a086-1788017f9062.png" width="382"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised models</h1>
                </header>
            
            <article>
                
<ul>
<li>Clusters</li>
<li>Distances</li>
<li>Normalization</li>
<li>K-means</li>
<li>Self-organizing maps</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clusters</h1>
                </header>
            
            <article>
                
<p>Clusters are hierarchical, it finds successive clusters using previously assigned clusters with bottom up (agglomerative) or top-down (divisive) and partitional type cluster depicted below as right and left side respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distances</h1>
                </header>
            
            <article>
                
<p>To determine the similarity between two clusters and the shape of clusters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalization</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>VAR</strong>: For a transformed set of data points for each attribute, the mean is  reduced to zero; this is by subtracting the mean of each attribute from the  values of the attributes and dividing the result by the standard deviation of the attribute.</li>
<li><strong>RANGE (Min‐Max  Normalization)</strong>: It subtracts the minimum value of an attribute from each value of the attribute and then divides the difference by  the range of the attribute. The advantage is preserving all relationship in the data precisely, without adding any bias.</li>
<li><strong>SOFTMAX</strong>: It is a way of reducing the influence  of extreme values or outliers in the data without removing them from the dataset. It is useful when you have outlier data that you wish to include in the dataset while still preserving the significance of data within a standard deviation of the mean.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means</h1>
                </header>
            
            <article>
                
<p><span>K-means is popular model as it's fast and simple; however it does not yield the same result with every run.</span></p>
<ul>
<li>Partitions data into K clusters based on their features</li>
<li>Each cluster is represented by its centroid, the center of the cluster points</li>
<li>Each point is assigned to the nearest cluster</li>
<li>The goal is to minimize intra-cluster variance or the sum of squares of distances between data and the corresponding cluster centroid</li>
<li>Computes the mean point--centroid</li>
</ul>
<p>A new partition is built by associating each point with the nearest centroid. Computes the mean point or centroid of each set.</p>
<p>The recent surge of low-cost technology availability such as Hadoop eco systems, cloud computing, big data, and open source tools has led to large-scale adoption by every industry from small to large giants. Data science penetration is also witnessed across every industry with eco system being available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have covered the key concepts of building big data applications, covering the tools for data discovery, quality, and ingestion. We discussed the Spark Stream in-memory engine and its versatility; data science models for various industry solutions were also discussed.</p>


            </article>

            
        </section>
    </body></html>