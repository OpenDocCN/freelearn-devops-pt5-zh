- en: Chapter 8. Logging, Monitoring, and Recovery Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 日志记录、监控和恢复技术
- en: 'In this chapter, we are going to look into one of our schedulers and look at
    wrapping some more operational tasks around it. So far, in this book, we have
    covered more glamorous subjects; however, monitoring, logging, and automated recovery
    are just as important. We want to take this knowledge and make it work in the
    real world. From there, we can start to see the benefits to both the development
    and operations teams. We are going to use Docker Swarm as our scheduler in this
    chapter. For logging, we will use the ELK stack, and for monitoring, we will use
    Consul. Since Docker Swarm version 1.1.3, there are some cool features that will
    help us use recovery, so we will look at them. We will cover the following topics
    in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究我们的一个调度器，并在其上进行一些额外的操作任务包装。到目前为止，在本书中，我们已经涵盖了许多更为引人注目的主题；然而，监控、日志记录和自动恢复同样重要。我们希望将这些知识应用到实际工作中。从那里开始，我们可以开始看到开发和运维团队的好处。本章我们将使用Docker
    Swarm作为调度器。对于日志记录，我们将使用ELK堆栈，而对于监控，我们将使用Consul。自Docker Swarm版本1.1.3以来，出现了一些很酷的功能，帮助我们使用恢复，因此我们将重点讨论这些功能。本章将涵盖以下主题：
- en: Logging
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录
- en: Monitoring
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控
- en: Recovery techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恢复技术
- en: Logging
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: The importance of logging is second to none in a solution. If we need to debug
    any issues with any code/infrastructure, the logs are the first place to go. In
    the container world, this is no different. In one of the previous chapters, we
    built the ELK stack. We are going to use that again to process all the logs from
    our containers. In this solution, we will use a fair chunk of the knowledge that
    we got so far. We will use a scheduler, a Docker network, and lastly, service
    discovery with Consul. So, let's look at the solution, and like we did in the
    past chapters, we will get to coding.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录在解决方案中的重要性无可比拟。如果我们需要调试任何代码/基础设施的问题，日志是最先需要查看的地方。在容器世界中，这一点也不例外。在之前的章节中，我们构建了ELK堆栈。我们将再次使用它来处理所有来自容器的日志。在这个解决方案中，我们将利用到目前为止学到的大量知识。我们将使用调度器、Docker网络，并最终使用Consul进行服务发现。所以，让我们来看一下这个解决方案，就像在之前的章节中一样，我们将开始编码。
- en: The solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'As I mentioned in the introduction of this chapter, we will be using Docker
    Swarm for this solution. The reason for this choice is that I want to highlight
    some of the features of Swarm as it has come on leaps and bounds in the last few
    releases. For the logging portion of this chapter, we are going to deploy our
    three containers and let Swarm schedule them. We will use a combination of the
    Docker networking DNS and our service discovery with Consul to tie everything
    together. In Swarm, we will use the same servers as we did in the last chapter:
    three member nodes and two replicated masters. Each node will be a member of our
    Consul cluster. We will again use Puppet to install Consul on the host system
    natively.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本章介绍部分提到的，我们将使用Docker Swarm来实现这个解决方案。选择Swarm的原因是我想强调Swarm的一些新特性，因为它在最近几个版本中有了显著的进展。在本章的日志记录部分，我们将部署三个容器并让Swarm来调度它们。我们将结合使用Docker网络DNS和Consul的服务发现来将所有内容连接起来。在Swarm中，我们将使用与上一章相同的服务器：三个成员节点和两个复制的主节点。每个节点都将是我们Consul集群的成员。我们将再次使用Puppet在主机系统上本地安装Consul。
- en: The code
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'In this chapter, we will build on the code that we used in the last chapter
    for Docker Swarm. So, we will go through the plumbing of the Vagrant repo fairly
    swiftly, only calling out the differences from the last chapter. We are going
    to create a new Vagrant repo again for this chapter. You should be a master at
    this by now. Once the new repo is set up, open the `servers.yaml` file. We will
    add the following code to it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将基于上一章中使用的Docker Swarm代码进行扩展。所以，我们将快速浏览Vagrant仓库的基本架构，只指出与上一章的不同之处。我们将再次为本章创建一个新的Vagrant仓库。到现在为止，你应该已经非常熟练了。一旦新仓库设置好，打开`servers.yaml`文件。我们将向其中添加以下代码：
- en: '![The code](img/B05201_08_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_01.jpg)'
- en: Code for the severs.yaml file
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`servers.yaml`文件的代码'
- en: 'As you can see, it''s not that different from the last chapter. There is one
    call out. We have added a new line to each server `- { shell: ''echo -e "PEERDNS=no\nDNS1=127.0.0.1\nDNS2=8.8.8.8">>/etc/sysconfig/network-scripts/ifcfg-enp0s8
    && systemctl restart network''}`. We will add this as the server is multihomed.
    We will want to make sure that we are resolving DNS correctly on each interface.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '正如你所看到的，与上一章节相比，并没有太大的不同。有一个值得注意的地方。我们为每个服务器添加了一个新的行 `- { shell: ''echo -e
    "PEERDNS=no\nDNS1=127.0.0.1\nDNS2=8.8.8.8">>/etc/sysconfig/network-scripts/ifcfg-enp0s8
    && systemctl restart network''}`。我们会在服务器多重主机环境下确保正确解析DNS。'
- en: 'The next thing we will look at is the puppetfile, which is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看一下puppetfile文件，如下所示：
- en: '![The code](img/B05201_08_03.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_03.jpg)'
- en: 'As you can see, there are no new changes compared with the last chapter. So,
    let''s move to our Hiera file located at `heiradata/global.yml` in the root of
    our module:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从代码中看到的，与上一章节相比，并没有什么新的变化。所以，让我们转向我们模块根目录中的`heiradata/global.yml`中的Hiera文件：
- en: '![The code](img/B05201_08_04.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_04.jpg)'
- en: Again, we are setting the Swarm version to `v1.1.3`, and the backend is set
    to `consul`. We set the IP address of the first node in the Consul cluster, and
    we set the Consul port to `8500`. We will set the `swarm` interface that we will
    advertise from, and last but not least, we will set our Consul version to `0.6.3`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从代码中看到的，我们正在将Swarm版本设置为`v1.1.3`，后端设置为`consul`。我们设置了Consul集群中第一个节点的IP地址，并将Consul端口设置为`8500`。我们将设置我们从中广播的`swarm`接口，并最后但同样重要的是，我们将设置我们的Consul版本为`0.6.3`。
- en: Now, we will create our module. We will again call the `config` module. Once
    you have created your `<AUTHOR>-config` module, move it to the `modules` folder
    in the root of your Vagrant repo.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建我们的模块。我们再次调用`config`模块。一旦你创建了你的`<AUTHOR>-config`模块，请将它移动到你的Vagrant存储库根目录中的`modules`文件夹中。
- en: 'Now that we have our module, let''s add our code to it. We will need to create
    the following files in the `manifests` directory: `compose.pp`, `consul_config`,
    `dns.pp`, `run_containers.pp`, and `swarm.pp`. We have no `params.pp` as we are
    using Hiera in this example.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的模块，让我们在其中添加我们的代码。我们需要在`manifests`目录下创建以下文件：`compose.pp`、`consul_config`、`dns.pp`、`run_containers.pp`和`swarm.pp`。由于在此示例中使用Hiera，我们不需要`params.pp`。
- en: 'So, let''s go through the files in an alphabetical order. In our `compose.pp`
    file, we will add the following code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们按照字母顺序浏览文件。在我们的`compose.pp`文件中，我们将添加以下代码：
- en: '![The code](img/B05201_08_05.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_05.jpg)'
- en: 'As you can see from the code, we are adding our `docker-compose.yml` file to
    any node that is not a swarm master. We will come back to the `docker compose.yml`
    file when we look at the `templates` directory. The next file is `consul_config.pp`,
    which is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从代码中看到的，我们正在将我们的`docker-compose.yml`文件添加到任何不是Swarm主节点的节点上。当我们查看`templates`目录时，我们将回到`docker-compose.yml`文件。接下来的文件是`consul_config.pp`，如下所示：
- en: '![The code](img/B05201_08_06.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_06.jpg)'
- en: 'In this file, we are declaring our Consul cluster and defining the bootstrap
    server. This should look familiar as it is the same code that we used in the last
    chapter. The next file is `dns.pp`, which is given as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们声明了我们的Consul集群，并定义了引导服务器。这应该看起来很熟悉，因为这与我们在上一章节中使用的代码相同。接下来是`dns.pp`文件，如下所示：
- en: '![The code](img/B05201_08_07.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_07.jpg)'
- en: 'Again, this code should look familiar to you, as we have used it before in
    the last chapter. Just to recap, this is setting and configuring our bind package
    to use Consul as the DNS server. The next file we will look at is `init.pp`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该看起来很熟悉，因为我们在上一章节中已经使用过它。简要回顾一下，这里是设置和配置我们的绑定包，使用Consul作为DNS服务器。接下来我们将查看`init.pp`文件：
- en: '![The code](img/B05201_08_08.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_08.jpg)'
- en: 'In the `init.pp` file, we are just ordering our classes within our module.
    We will now move on to `run_containers.pp`. This is where we will schedule our
    ELK containers across the swarm cluster:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在`init.pp`文件中，我们只是在我们的模块内部排序我们的类。现在我们将转向`run_containers.pp`。这里我们将在Swarm集群中安排我们的ELK容器：
- en: '![The code](img/B05201_08_09.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_08_09.jpg)'
- en: Let's have a look at this in detail, as there is a lot of new code here. The
    first declaration that we will use is to schedule the containers from the second
    Swarm master.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下这一点，因为这里有很多新的代码。我们将使用的第一个声明是从第二个Swarm主节点调度容器。
- en: The next block of code will configure our `logstash` container. We will need
    to first have these containers here in this example, as we are using them as the
    syslog server. If at the time of spawning the containers they can't connect to
    logstash on TCP port `5000`, the build of the container will fail. So, let's move
    on to the configuration of `logstash`. We will use the container that I put in,
    as it is the official container with the `logstash.conf` file that we already
    added. We will then add `logstash` to our internal `swarm-private` Docker network
    and expose all the ports for `logstash` on all networks. So, we can pipe logs
    from anywhere to it. After this, we will set the location of `elasticsearch` and
    then we will give the command to start.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块将配置我们的`logstash`容器。我们需要首先在这个示例中有这些容器，因为我们将它们作为syslog服务器使用。如果在创建容器时它们无法连接到`logstash`的TCP端口`5000`，容器构建将失败。因此，让我们继续配置`logstash`。我们将使用我提供的容器，因为它是官方容器，并且我们已经添加了`logstash.conf`文件。接下来，我们将`logstash`添加到我们的内部`swarm-private`
    Docker网络，并在所有网络上暴露`logstash`的所有端口。这样，我们就可以从任何地方将日志传输到它。之后，我们将设置`elasticsearch`的位置，然后给出启动命令。
- en: Logstash
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Logstash
- en: In the second block of code, we will install and configure `elasticsearch`.
    We will use the official `elasticsearch` container (version 2.1.0). We will only
    add `elasticsearch` to our private Docker network, `swarm-private`. We will make
    the data persistent by declaring a volume mapping. We will set the command with
    arguments to start `elasticsearch` with the command value. Next, we will set the
    log drive to syslog and the syslog server to `tcp://logstash-5000.service.consul:5000`.
    Note that we are using our Consul service discovery address as we are exposing
    `logstash` on the external network. Lastly, we set the dependency on `logstash`.
    As I mentioned earlier, the syslog server needs to be available at the time this
    container spawns, so we need `logstash` to be there prior to either this container
    or `kibana`. Talking of Kibana, let's move on to our last block of code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个代码块中，我们将安装并配置`elasticsearch`。我们将使用官方的`elasticsearch`容器（版本2.1.0）。我们只会将`elasticsearch`添加到我们的私有Docker网络`swarm-private`中。我们将通过声明卷映射来保持数据持久性。我们将设置命令和参数，以便通过命令值启动`elasticsearch`。接下来，我们将设置日志驱动为syslog，并将syslog服务器设置为`tcp://logstash-5000.service.consul:5000`。请注意，我们正在使用我们的Consul服务发现地址，因为我们在外部网络上暴露了`logstash`。最后，我们设置`logstash`的依赖关系。正如我之前提到的，syslog服务器需要在该容器启动时可用，因此我们需要`logstash`在此容器或`kibana`之前就已经存在。说到Kibana，让我们继续到最后一个代码块。
- en: In our `kibana` container, we will add the following configuration. First, we
    will use the official `kibana` image (Version 4.3.0). We will add `kibana` to
    our `swarm-private` network so it can access our `elasticsearch` container. We
    will map and expose ports `5601` to `80` on the host network. In the last few
    lines, we will set the syslog configuration in the same way as we did with `elasticsearch`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`kibana`容器中，我们将添加以下配置。首先，我们将使用官方的`kibana`镜像（版本4.3.0）。我们将`kibana`添加到我们的`swarm-private`网络，以便它能够访问我们的`elasticsearch`容器。我们将把端口`5601`映射到主机网络上的`80`端口。在最后几行中，我们将以与`elasticsearch`相同的方式设置syslog配置。
- en: 'Now, it''s time for our last file, `swarm.pp`, which is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候处理我们的最后一个文件`swarm.pp`，内容如下：
- en: '![Logstash](img/B05201_08_10.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_10.jpg)'
- en: In this code, we are configuring our Swarm cluster and Docker network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们正在配置我们的Swarm集群和Docker网络。
- en: We will now move to our `templates` folder in the root of our module. We need
    to create three files. The two files `Consul.conf.erb` and `named.conf.erb` are
    for our bind config. The last file is our `registrator.yml.erb` Docker compose
    file. We will add the code to the following files.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入模块根目录下的`templates`文件夹。我们需要创建三个文件。两个文件`Consul.conf.erb`和`named.conf.erb`是用于我们的绑定配置。最后一个文件是我们的`registrator.yml.erb`
    Docker Compose文件。我们将把代码添加到以下文件中。
- en: 'Let''s first see the code for `consul.conf.erb`, which is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看`consul.conf.erb`的代码，内容如下：
- en: '![Logstash](img/B05201_08_16.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_16.jpg)'
- en: 'Now, let''s see the code for `named.conf.erb`, which is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看`named.conf.erb`的代码，内容如下：
- en: '![Logstash](img/B05201_08_17.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_17.jpg)'
- en: 'Finally, let''s see the code for `registrator.yml.erb`, which is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看`registrator.yml.erb`的代码，内容如下：
- en: '![Logstash](img/B05201_08_18.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_18.jpg)'
- en: All the code in these files should look fairly familiar, as we have used it
    in previous chapters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件中的所有代码应该都非常熟悉，因为我们在之前的章节中已经使用过它。
- en: Now, we have just one more configuration before we can run our cluster. So,
    let's go to our `default.pp` manifest file in the `manifests` folder located in
    the root of our Vagrant repo.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在运行我们的集群之前，我们只需要做最后一项配置。让我们前往Vagrant仓库根目录中的`manifests`文件夹中的`default.pp`清单文件。
- en: 'Now, we will add the relevant node definitions to our manifest file:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将向清单文件中添加相关的节点定义：
- en: '![Logstash](img/B05201_08_11.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_11.jpg)'
- en: We are ready to go to our terminal, where we will change the directory to the
    root of our Vagrant repo. As so many times that we did before, we will issue the
    `vagrant up` command. If you have the boxes still configured from the last chapter,
    issue the `vagrant destroy -f && vagrant up` command.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备进入终端，切换到我们的Vagrant仓库根目录。像以前一样，我们将运行`vagrant up`命令。如果你还保留了上一章配置的盒子，可以运行`vagrant
    destroy -f && vagrant up`命令。
- en: 'Once Vagrant is run and Puppet has built our five nodes, we can now open a
    browser and enter the `http://127.0.0.1:9501/`. We should get the following page
    after this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Vagrant运行并且Puppet构建了我们的五个节点，我们就可以打开浏览器并访问`http://127.0.0.1:9501/`。此时我们应该看到以下页面：
- en: '![Logstash](img/B05201_08_12.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_12.jpg)'
- en: As you can see, all our services are shown with green color that displays the
    state of health. We will now need to find what node our `kibana` container is
    running on. We will do that by clicking on the **kibana** service.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有我们的服务都以绿色显示，表示健康状态。接下来，我们需要找到我们的`kibana`容器运行在哪个节点上。我们将通过点击**kibana**服务来完成这一步。
- en: '![Logstash](img/B05201_08_13.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_13.jpg)'
- en: 'In my example, **kibana** has come up on **swarm-101**. If this is not the
    same for you, don''t worry as the Swarm cluster could have scheduled the container
    on any of the three nodes. Now, I will open a browser tab and enter the `127.0.0.1:8001/`,
    as shown in the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的示例中，**kibana**已经在**swarm-101**上启动。如果你的情况不同，不用担心，因为Swarm集群可能已经将容器调度到了三个节点中的任何一个。现在，我将打开一个浏览器标签，输入`127.0.0.1:8001/`，如下面的截图所示：
- en: '![Logstash](img/B05201_08_14.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_14.jpg)'
- en: If your host is different, consult the `servers.yaml` file to get the right
    port.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的主机不同，请参考`servers.yaml`文件以获取正确的端口。
- en: 'We will then create our index and click on the **Discovery** tab, and as you
    can see in this screenshot, we have our logs coming in:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建索引并点击**Discovery**标签，如截图所示，我们的日志已经进入：
- en: '![Logstash](img/B05201_08_15.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Logstash](img/B05201_08_15.jpg)'
- en: Logs after creating index
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 创建索引后的日志
- en: Monitoring
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: In the world of containers, there are a few levels of monitoring that you can
    deploy. For example, you have your traditional ops monitoring. So your Nagios
    and Zabbix of the world or even perhaps a cloud solution like Datadog. All these
    solutions have good hooks into Docker and can be deployed with Puppet. We are
    going to assume in this book that the ops team has this covered and your traditional
    monitoring is in place. We are going to look at the next level of monitoring.
    We will concentrate on the container connectivity and Swarm cluster health. We
    will do this all in Consul and deploy our code with Puppet.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器的世界里，你可以部署几种监控级别。例如，你有传统的运维监控。比如Nagios、Zabbix，甚至可能是像Datadog这样的云解决方案。这些解决方案都能很好地与Docker集成，并且可以通过Puppet进行部署。在本书中，我们假设运维团队已经完成了这一部分工作，传统监控已经就绪。我们将关注下一个监控级别。我们将集中在容器的连接性和Swarm集群的健康状况上。我们将在Consul中完成这些，并通过Puppet部署我们的代码。
- en: The reason we are looking at this level of monitoring is because we can make
    decisions about what Consul is reporting. Do we need to scale containers? Is a
    Swarm node sick? Should we take it out of the cluster? Now to cover these topics,
    we will need to write a separate book. I won't be covering these solutions. What
    we will look at is the first step to get there. Now that the seed has been planted,
    you will want to explore your options further. The challenge is to change the
    way we think about monitoring and how it needs reactive interaction from a human,
    so we can trust our code to make choices for us and to make our solutions fully
    automated.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注这个级别的监控是因为我们可以根据Consul报告做出决策。我们是否需要扩展容器？一个Swarm节点是否出现故障？我们是否应该将它从集群中移除？要解决这些问题，我们需要写一本单独的书。我不会覆盖这些解决方案。我们将关注的是达到这些目标的第一步。现在，种子已经播下，你将希望进一步探索你的选项。挑战在于改变我们对监控的思维方式，以及它如何需要人类的反应性互动，这样我们就可以信任我们的代码为我们做出选择，并使我们的解决方案完全自动化。
- en: Monitoring with Consul
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Consul进行监控
- en: 'One really good thing about using Consul is that Hashicorp did an awesome job
    at documenting their applications. Consul is no exception. If you would like to
    read more about the options you have with Consul monitoring, refer to the documentation
    at [https://www.consul.io/docs/agent/services.html](https://www.consul.io/docs/agent/services.html)
    and [https://www.consul.io/docs/agent/checks.html](https://www.consul.io/docs/agent/checks.html).
    We are going to set up both checks and a service. In the last chapter, we wrote
    a service with Consul to monitor our Docker service on every node:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Consul 的一个很好的优点是，Hashicorp 在文档编写方面做得非常出色，Consul 也不例外。如果你想了解更多关于 Consul 监控选项的信息，请参考
    [https://www.consul.io/docs/agent/services.html](https://www.consul.io/docs/agent/services.html)
    和 [https://www.consul.io/docs/agent/checks.html](https://www.consul.io/docs/agent/checks.html)
    的文档。我们将设置检查和服务。在上一章中，我们编写了一个服务来使用 Consul 监控每个节点上的 Docker 服务：
- en: '![Monitoring with Consul](img/B05201_08_19.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Consul 进行监控](img/B05201_08_19.jpg)'
- en: 'On the Consul web UI, we get the following reading of the Docker service on
    the node:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Consul Web UI 上，我们看到以下 Docker 服务的节点读取：
- en: '![Monitoring with Consul](img/B05201_08_20.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Consul 进行监控](img/B05201_08_20.jpg)'
- en: We are going to roll out all our new checks on both the Swarm masters. The reason
    for this is that both the nodes are external from the nodes cluster. The abstraction
    gives us the benefit of not having to worry about the nodes that the containers
    are running on. You also have the monitoring polling from multiple locations.
    For example, in AWS, your Swarm masters could be split across multiple AZs. So,
    if you lose an AZ, your monitoring will still be available.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把所有新的检查应用到两个 Swarm 主节点。这样做的原因是，这两个节点位于集群之外。这样的抽象化让我们无需担心容器运行在哪些节点上。你还可以从多个位置进行监控轮询。例如，在
    AWS 中，你的 Swarm 主节点可能分布在多个可用区（AZ）。因此，即使丢失一个可用区，你的监控仍然可用。
- en: As we are going to use the logging solution from the example that we covered
    in the previous section, we will check and make sure that both Logstash and Kibana
    are available; Logstash on port 5000 and Kibana on port 80.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将使用前一节中介绍的日志解决方案，所以我们需要检查并确保 Logstash 和 Kibana 都可用；Logstash 在端口 5000，Kibana
    在端口 80。
- en: 'We are going to add two new service checks to our config module in the `consul_config.pp`
    file, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向 `consul_config.pp` 文件中的配置模块添加两个新的服务检查，如下所示：
- en: '![Monitoring with Consul](img/B05201_08_21.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Consul 进行监控](img/B05201_08_21.jpg)'
- en: As you can see, we have set a TCP check for both `kibana` and `logstash`, and
    we will use the service discovery address to test the connections. We will open
    our terminal and change the directory to the root of our Vagrant repo.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们为 `kibana` 和 `logstash` 都设置了 TCP 检查，并且我们将使用服务发现地址来测试连接。接下来，我们将在终端中打开并切换到
    Vagrant 仓库的根目录。
- en: 'Now, we are going to assume that your five boxes are running. We will issue
    the command to Vagrant to provision only the two master nodes. This command is
    `vagrant provision swarm-master-01 && vagrant provision swarm-master-02`. We will
    then open our web browser and enter `127.0.0.1:9501`. We can then click on **swarm-master-01**
    or **swarm-master-02**, the choice is up to you. After this, we will get the following
    result:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们假设你的五个节点正在运行。我们将向 Vagrant 发出命令，仅为两个主节点进行配置。这个命令是 `vagrant provision swarm-master-01
    && vagrant provision swarm-master-02`。然后，我们将打开浏览器并输入 `127.0.0.1:9501`。接着你可以点击**swarm-master-01**或**swarm-master-02**，选择权在你。完成后，你应该会看到如下结果：
- en: '![Monitoring with Consul](img/B05201_08_22.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Consul 进行监控](img/B05201_08_22.jpg)'
- en: 'As you can see, our monitoring was successful. We will move back to our code
    and add a check for our swarm master to determine its health. We will do that
    with the following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的监控是成功的。接下来我们将回到代码中，为我们的 swarm master 添加一个检查，以确定其健康状况。我们将通过以下代码来实现：
- en: '![Monitoring with Consul](img/B05201_08_23.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Consul 进行监控](img/B05201_08_23.jpg)'
- en: 'We will then issue the Vagrant provision command, `vagrant provision swarm-master-01
    && vagrant provision swarm-master-02`. Again, we will open our web browser and
    click on **swarm-master-01** or **swarm-master-02**. You should get the following
    result after this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将执行 Vagrant 提供命令，`vagrant provision swarm-master-01 && vagrant provision
    swarm-master-02`。接着，我们将打开浏览器并点击**swarm-master-01**或**swarm-master-02**。完成后，你应该会看到如下结果：
- en: '![Monitoring with Consul](img/B05201_08_24.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Consul 进行监控](img/B05201_08_24.jpg)'
- en: As you can see from the information in the check, we can easily see which Swarm
    master is primary, the strategy for scheduling. This will come in really handy
    when you have any issues.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从健康检查中可以看到的信息，我们可以轻松地识别出哪个 Swarm 主节点是主节点，以及调度策略。这在你遇到问题时会非常有用。
- en: So as you can see, Consul is a really handy tool, and if you want to take away
    the things we covered in this chapter, you can really do some cool stuff.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Consul 是一个非常实用的工具，如果你想了解我们在本章中讨论的内容，你真的可以做一些很酷的事情。
- en: Recovery techniques
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复技术
- en: It is important in every solution to have some recovery techniques. In the container
    world, this is no different. There are many ways to skin this cat, such as load
    balancing with HA proxy or even using a container-based application that was built
    for this purpose such as interlock ([https://github.com/ehazlett/interlock](https://github.com/ehazlett/interlock)).
    If you have not checked out interlock, it's awesome!!! There are so many combinations
    of solutions we could use depending on the underlying application. So here, we
    are going to look at the built-in HA in Docker Swarm. From there, you could use
    something like an interlock to make sure that there is no downtime in accessing
    your containers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个解决方案中，拥有一些恢复技术是非常重要的。在容器世界中，情况也不例外。有很多种方式可以实现这一点，比如使用 HA 代理进行负载均衡，甚至使用专为此目的设计的容器化应用程序，例如
    interlock（[https://github.com/ehazlett/interlock](https://github.com/ehazlett/interlock)）。如果你还没有查看过
    interlock，真的是太棒了！！！根据底层应用程序的不同，我们可以使用许多不同的解决方案组合。所以在这里，我们将讨论 Docker Swarm 中的内建高可用性。从这里，你可以使用类似
    interlock 的工具，确保你的容器访问没有停机时间。
- en: Built-in HA
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内建高可用性
- en: 'Docker Swarm has two kind of nodes: master nodes and member nodes. Each one
    of these has different built-in protection for failure. The first node type we
    will look at is master nodes.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 有两种类型的节点：主节点和成员节点。每种节点都有不同的内建故障保护。我们将首先看一下主节点。
- en: 'In the last topic, we set up a health check to get the information regarding
    our Swarm cluster. There, we saw that we had a master or primary Swarm master
    and a replica. Swarm replicates all its cluster information over TCP port 4000\.
    So, to simulate failure, we are going to turn off the master. My master is `swarm-master-01`,
    but yours could be different. We will use the health check that we already created
    to test out the failure and watch how Swarm handles itself. We will issue the
    `vagrant halt swarm-master-01` command. We will open up our browser again to our
    Consul web UI, `127.0.0.1:9501`. As we can see in the following screenshot, `swarm-master-02`
    is now a master:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一主题中，我们设置了健康检查，以获取有关我们的 Swarm 集群的信息。我们看到我们有一个主节点或主要的 Swarm 主节点和一个副本。Swarm
    会通过 TCP 端口 4000 复制所有的集群信息。因此，为了模拟故障，我们将关闭主节点。我的主节点是 `swarm-master-01`，但你的可能不同。我们将使用已经创建的健康检查来测试故障并观察
    Swarm 如何自我处理。我们将执行 `vagrant halt swarm-master-01` 命令。然后，我们会再次打开浏览器，访问我们的 Consul
    Web UI，`127.0.0.1:9501`。正如我们在以下截图中看到的，`swarm-master-02` 现在是主节点：
- en: '![Built-in HA](img/B05201_08_26.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![内建高可用性](img/B05201_08_26.jpg)'
- en: Now, we will move on to container resecluding with our Swarm node HA. As of
    version 1.1.3, Swarm is shipped with a feature where a container will respawn
    on a healthy node if the original node fails. There are some rules to this such
    as when you have filtering rules or linked containers. To know more on this topic,
    you can read the Docker docs about Swarm located at [https://github.com/docker/swarm/tree/master/experimental](https://github.com/docker/swarm/tree/master/experimental).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论在我们的 Swarm 节点高可用性（HA）下的容器重启。从版本 1.1.3 开始，Swarm 提供了一项功能，当原始节点发生故障时，容器会在健康的节点上重新启动。对此有一些规则，例如在你使用过滤规则或链接容器时。想要了解更多相关信息，可以阅读位于
    [https://github.com/docker/swarm/tree/master/experimental](https://github.com/docker/swarm/tree/master/experimental)
    的 Docker Swarm 文档。
- en: 'To test this out, I will halt the node that hosts Kibana. We will need to add
    some code to our `kibana` container so that it will restart on failure. This is
    added to the `env` resource, as shown in the following screenshot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，我将暂停托管 Kibana 的节点。我们需要在 `kibana` 容器中添加一些代码，以便在故障时能够重启。如下截图所示，这些设置被添加到了
    `env` 资源中：
- en: '![Built-in HA](img/B05201_08_27.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![内建高可用性](img/B05201_08_27.jpg)'
- en: We will first need to kill our old container to add the restart policy. We can
    do that by setting the **ensure** resource to **absent** and running the Vagrant
    provision `swarm-master-02`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要终止旧的容器，以便添加重启策略。我们可以通过将 **ensure** 资源设置为 **absent** 来做到这一点，然后运行 Vagrant
    配置 `swarm-master-02`。
- en: Once the Vagrant run is complete, we will change it back to **present** and
    run `vagrant provision swarm-master-02`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Vagrant运行完成，我们将其恢复为**当前**状态，并运行`vagrant provision swarm-master-02`。
- en: 'For me, my `kibana` container is on **swarm-102** (this could be different
    for you). Once that node fails, `kibana` will restart on a healthy node. So, let''s
    issue `vagrant halt swarm-102`. If we go to our Consul URL, `127.0.0.1:9501`,
    we should see some failures on our nodes and checks, as shown in the following
    screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，我的`kibana`容器在**swarm-102**上（这可能对你来说会有所不同）。一旦该节点失败，`kibana`会在健康节点上重启。所以，让我们执行`vagrant
    halt swarm-102`。如果我们访问Consul的URL `127.0.0.1:9501`，我们应该会看到一些节点和检查的失败，如下图所示：
- en: '![Built-in HA](img/B05201_08_28.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![内置高可用性](img/B05201_08_28.jpg)'
- en: 'If you wait a minute or so, you will see that `kibana` alerts came back and
    the container spawned on another server. For me, `kibana` came back on **swarm-101**,
    as you can see in the following screenshot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你等个一分钟左右，你会看到`kibana`警报恢复，并且容器在另一台服务器上启动。对我来说，`kibana`恢复在**swarm-101**上，正如你在下面的截图中看到的那样：
- en: '![Built-in HA](img/B05201_08_31.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![内置高可用性](img/B05201_08_31.jpg)'
- en: 'We can then go to our browser and look at `kibana`. For me, it will be at `127.0.0.1:8001`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在浏览器中查看`kibana`。对我来说，它会在`127.0.0.1:8001`上：
- en: '![Built-in HA](img/B05201_08_32.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![内置高可用性](img/B05201_08_32.jpg)'
- en: Kibana after connecting to Elasticsearch
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到Elasticsearch后的Kibana
- en: As you can see, all our logs are there; our service discovery worked perfectly
    as once the container changed nodes, our health checks turned green.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有的日志都在那；我们的服务发现工作得非常完美，因为容器一旦更换节点，我们的健康检查就变成了绿色。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at how to operationalize our container environment
    using Puppet. We covered a logging solution using ELK. We took Consul to the next
    level with more in-depth health checks and creating services to monitor our cluster.
    We then tested the built-in HA functionality that ships with Swarm. We have covered
    a lot of ground now since our humble first module in [Chapter 2](ch02.html "Chapter 2. Working
    with Docker Hub"), *Working with Docker Hub*. You are fully equipped to take the
    knowledge that you have got here and apply it in the real world.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了如何使用Puppet将容器环境实现自动化。我们介绍了使用ELK的日志解决方案。我们将Consul提升到了新的水平，进行了更深入的健康检查，并创建了监控集群的服务。接着我们测试了Swarm自带的内置高可用性功能。从我们在[第二章](ch02.html
    "第2章：使用Docker Hub")《与Docker Hub合作》中的初步尝试开始，我们已经走了很长一段路。你已经完全装备好，可以将这里获得的知识应用到实际工作中了。
