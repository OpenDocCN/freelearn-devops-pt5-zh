- en: Chapter 8. Logging, Monitoring, and Recovery Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to look into one of our schedulers and look at
    wrapping some more operational tasks around it. So far, in this book, we have
    covered more glamorous subjects; however, monitoring, logging, and automated recovery
    are just as important. We want to take this knowledge and make it work in the
    real world. From there, we can start to see the benefits to both the development
    and operations teams. We are going to use Docker Swarm as our scheduler in this
    chapter. For logging, we will use the ELK stack, and for monitoring, we will use
    Consul. Since Docker Swarm version 1.1.3, there are some cool features that will
    help us use recovery, so we will look at them. We will cover the following topics
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The importance of logging is second to none in a solution. If we need to debug
    any issues with any code/infrastructure, the logs are the first place to go. In
    the container world, this is no different. In one of the previous chapters, we
    built the ELK stack. We are going to use that again to process all the logs from
    our containers. In this solution, we will use a fair chunk of the knowledge that
    we got so far. We will use a scheduler, a Docker network, and lastly, service
    discovery with Consul. So, let's look at the solution, and like we did in the
    past chapters, we will get to coding.
  prefs: []
  type: TYPE_NORMAL
- en: The solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As I mentioned in the introduction of this chapter, we will be using Docker
    Swarm for this solution. The reason for this choice is that I want to highlight
    some of the features of Swarm as it has come on leaps and bounds in the last few
    releases. For the logging portion of this chapter, we are going to deploy our
    three containers and let Swarm schedule them. We will use a combination of the
    Docker networking DNS and our service discovery with Consul to tie everything
    together. In Swarm, we will use the same servers as we did in the last chapter:
    three member nodes and two replicated masters. Each node will be a member of our
    Consul cluster. We will again use Puppet to install Consul on the host system
    natively.'
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we will build on the code that we used in the last chapter
    for Docker Swarm. So, we will go through the plumbing of the Vagrant repo fairly
    swiftly, only calling out the differences from the last chapter. We are going
    to create a new Vagrant repo again for this chapter. You should be a master at
    this by now. Once the new repo is set up, open the `servers.yaml` file. We will
    add the following code to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Code for the severs.yaml file
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it''s not that different from the last chapter. There is one
    call out. We have added a new line to each server `- { shell: ''echo -e "PEERDNS=no\nDNS1=127.0.0.1\nDNS2=8.8.8.8">>/etc/sysconfig/network-scripts/ifcfg-enp0s8
    && systemctl restart network''}`. We will add this as the server is multihomed.
    We will want to make sure that we are resolving DNS correctly on each interface.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we will look at is the puppetfile, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there are no new changes compared with the last chapter. So,
    let''s move to our Hiera file located at `heiradata/global.yml` in the root of
    our module:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, we are setting the Swarm version to `v1.1.3`, and the backend is set
    to `consul`. We set the IP address of the first node in the Consul cluster, and
    we set the Consul port to `8500`. We will set the `swarm` interface that we will
    advertise from, and last but not least, we will set our Consul version to `0.6.3`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will create our module. We will again call the `config` module. Once
    you have created your `<AUTHOR>-config` module, move it to the `modules` folder
    in the root of your Vagrant repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our module, let''s add our code to it. We will need to create
    the following files in the `manifests` directory: `compose.pp`, `consul_config`,
    `dns.pp`, `run_containers.pp`, and `swarm.pp`. We have no `params.pp` as we are
    using Hiera in this example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go through the files in an alphabetical order. In our `compose.pp`
    file, we will add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the code, we are adding our `docker-compose.yml` file to
    any node that is not a swarm master. We will come back to the `docker compose.yml`
    file when we look at the `templates` directory. The next file is `consul_config.pp`,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this file, we are declaring our Consul cluster and defining the bootstrap
    server. This should look familiar as it is the same code that we used in the last
    chapter. The next file is `dns.pp`, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, this code should look familiar to you, as we have used it before in
    the last chapter. Just to recap, this is setting and configuring our bind package
    to use Consul as the DNS server. The next file we will look at is `init.pp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the `init.pp` file, we are just ordering our classes within our module.
    We will now move on to `run_containers.pp`. This is where we will schedule our
    ELK containers across the swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The code](img/B05201_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's have a look at this in detail, as there is a lot of new code here. The
    first declaration that we will use is to schedule the containers from the second
    Swarm master.
  prefs: []
  type: TYPE_NORMAL
- en: The next block of code will configure our `logstash` container. We will need
    to first have these containers here in this example, as we are using them as the
    syslog server. If at the time of spawning the containers they can't connect to
    logstash on TCP port `5000`, the build of the container will fail. So, let's move
    on to the configuration of `logstash`. We will use the container that I put in,
    as it is the official container with the `logstash.conf` file that we already
    added. We will then add `logstash` to our internal `swarm-private` Docker network
    and expose all the ports for `logstash` on all networks. So, we can pipe logs
    from anywhere to it. After this, we will set the location of `elasticsearch` and
    then we will give the command to start.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second block of code, we will install and configure `elasticsearch`.
    We will use the official `elasticsearch` container (version 2.1.0). We will only
    add `elasticsearch` to our private Docker network, `swarm-private`. We will make
    the data persistent by declaring a volume mapping. We will set the command with
    arguments to start `elasticsearch` with the command value. Next, we will set the
    log drive to syslog and the syslog server to `tcp://logstash-5000.service.consul:5000`.
    Note that we are using our Consul service discovery address as we are exposing
    `logstash` on the external network. Lastly, we set the dependency on `logstash`.
    As I mentioned earlier, the syslog server needs to be available at the time this
    container spawns, so we need `logstash` to be there prior to either this container
    or `kibana`. Talking of Kibana, let's move on to our last block of code.
  prefs: []
  type: TYPE_NORMAL
- en: In our `kibana` container, we will add the following configuration. First, we
    will use the official `kibana` image (Version 4.3.0). We will add `kibana` to
    our `swarm-private` network so it can access our `elasticsearch` container. We
    will map and expose ports `5601` to `80` on the host network. In the last few
    lines, we will set the syslog configuration in the same way as we did with `elasticsearch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time for our last file, `swarm.pp`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this code, we are configuring our Swarm cluster and Docker network.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to our `templates` folder in the root of our module. We need
    to create three files. The two files `Consul.conf.erb` and `named.conf.erb` are
    for our bind config. The last file is our `registrator.yml.erb` Docker compose
    file. We will add the code to the following files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first see the code for `consul.conf.erb`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s see the code for `named.conf.erb`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s see the code for `registrator.yml.erb`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All the code in these files should look fairly familiar, as we have used it
    in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have just one more configuration before we can run our cluster. So,
    let's go to our `default.pp` manifest file in the `manifests` folder located in
    the root of our Vagrant repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will add the relevant node definitions to our manifest file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are ready to go to our terminal, where we will change the directory to the
    root of our Vagrant repo. As so many times that we did before, we will issue the
    `vagrant up` command. If you have the boxes still configured from the last chapter,
    issue the `vagrant destroy -f && vagrant up` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Vagrant is run and Puppet has built our five nodes, we can now open a
    browser and enter the `http://127.0.0.1:9501/`. We should get the following page
    after this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, all our services are shown with green color that displays the
    state of health. We will now need to find what node our `kibana` container is
    running on. We will do that by clicking on the **kibana** service.
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In my example, **kibana** has come up on **swarm-101**. If this is not the
    same for you, don''t worry as the Swarm cluster could have scheduled the container
    on any of the three nodes. Now, I will open a browser tab and enter the `127.0.0.1:8001/`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If your host is different, consult the `servers.yaml` file to get the right
    port.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then create our index and click on the **Discovery** tab, and as you
    can see in this screenshot, we have our logs coming in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logstash](img/B05201_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logs after creating index
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of containers, there are a few levels of monitoring that you can
    deploy. For example, you have your traditional ops monitoring. So your Nagios
    and Zabbix of the world or even perhaps a cloud solution like Datadog. All these
    solutions have good hooks into Docker and can be deployed with Puppet. We are
    going to assume in this book that the ops team has this covered and your traditional
    monitoring is in place. We are going to look at the next level of monitoring.
    We will concentrate on the container connectivity and Swarm cluster health. We
    will do this all in Consul and deploy our code with Puppet.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we are looking at this level of monitoring is because we can make
    decisions about what Consul is reporting. Do we need to scale containers? Is a
    Swarm node sick? Should we take it out of the cluster? Now to cover these topics,
    we will need to write a separate book. I won't be covering these solutions. What
    we will look at is the first step to get there. Now that the seed has been planted,
    you will want to explore your options further. The challenge is to change the
    way we think about monitoring and how it needs reactive interaction from a human,
    so we can trust our code to make choices for us and to make our solutions fully
    automated.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with Consul
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One really good thing about using Consul is that Hashicorp did an awesome job
    at documenting their applications. Consul is no exception. If you would like to
    read more about the options you have with Consul monitoring, refer to the documentation
    at [https://www.consul.io/docs/agent/services.html](https://www.consul.io/docs/agent/services.html)
    and [https://www.consul.io/docs/agent/checks.html](https://www.consul.io/docs/agent/checks.html).
    We are going to set up both checks and a service. In the last chapter, we wrote
    a service with Consul to monitor our Docker service on every node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Consul](img/B05201_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the Consul web UI, we get the following reading of the Docker service on
    the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Consul](img/B05201_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are going to roll out all our new checks on both the Swarm masters. The reason
    for this is that both the nodes are external from the nodes cluster. The abstraction
    gives us the benefit of not having to worry about the nodes that the containers
    are running on. You also have the monitoring polling from multiple locations.
    For example, in AWS, your Swarm masters could be split across multiple AZs. So,
    if you lose an AZ, your monitoring will still be available.
  prefs: []
  type: TYPE_NORMAL
- en: As we are going to use the logging solution from the example that we covered
    in the previous section, we will check and make sure that both Logstash and Kibana
    are available; Logstash on port 5000 and Kibana on port 80.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to add two new service checks to our config module in the `consul_config.pp`
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Consul](img/B05201_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have set a TCP check for both `kibana` and `logstash`, and
    we will use the service discovery address to test the connections. We will open
    our terminal and change the directory to the root of our Vagrant repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to assume that your five boxes are running. We will issue
    the command to Vagrant to provision only the two master nodes. This command is
    `vagrant provision swarm-master-01 && vagrant provision swarm-master-02`. We will
    then open our web browser and enter `127.0.0.1:9501`. We can then click on **swarm-master-01**
    or **swarm-master-02**, the choice is up to you. After this, we will get the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Consul](img/B05201_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, our monitoring was successful. We will move back to our code
    and add a check for our swarm master to determine its health. We will do that
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Consul](img/B05201_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then issue the Vagrant provision command, `vagrant provision swarm-master-01
    && vagrant provision swarm-master-02`. Again, we will open our web browser and
    click on **swarm-master-01** or **swarm-master-02**. You should get the following
    result after this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Consul](img/B05201_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the information in the check, we can easily see which Swarm
    master is primary, the strategy for scheduling. This will come in really handy
    when you have any issues.
  prefs: []
  type: TYPE_NORMAL
- en: So as you can see, Consul is a really handy tool, and if you want to take away
    the things we covered in this chapter, you can really do some cool stuff.
  prefs: []
  type: TYPE_NORMAL
- en: Recovery techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important in every solution to have some recovery techniques. In the container
    world, this is no different. There are many ways to skin this cat, such as load
    balancing with HA proxy or even using a container-based application that was built
    for this purpose such as interlock ([https://github.com/ehazlett/interlock](https://github.com/ehazlett/interlock)).
    If you have not checked out interlock, it's awesome!!! There are so many combinations
    of solutions we could use depending on the underlying application. So here, we
    are going to look at the built-in HA in Docker Swarm. From there, you could use
    something like an interlock to make sure that there is no downtime in accessing
    your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in HA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Docker Swarm has two kind of nodes: master nodes and member nodes. Each one
    of these has different built-in protection for failure. The first node type we
    will look at is master nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last topic, we set up a health check to get the information regarding
    our Swarm cluster. There, we saw that we had a master or primary Swarm master
    and a replica. Swarm replicates all its cluster information over TCP port 4000\.
    So, to simulate failure, we are going to turn off the master. My master is `swarm-master-01`,
    but yours could be different. We will use the health check that we already created
    to test out the failure and watch how Swarm handles itself. We will issue the
    `vagrant halt swarm-master-01` command. We will open up our browser again to our
    Consul web UI, `127.0.0.1:9501`. As we can see in the following screenshot, `swarm-master-02`
    is now a master:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Built-in HA](img/B05201_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will move on to container resecluding with our Swarm node HA. As of
    version 1.1.3, Swarm is shipped with a feature where a container will respawn
    on a healthy node if the original node fails. There are some rules to this such
    as when you have filtering rules or linked containers. To know more on this topic,
    you can read the Docker docs about Swarm located at [https://github.com/docker/swarm/tree/master/experimental](https://github.com/docker/swarm/tree/master/experimental).
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this out, I will halt the node that hosts Kibana. We will need to add
    some code to our `kibana` container so that it will restart on failure. This is
    added to the `env` resource, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Built-in HA](img/B05201_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will first need to kill our old container to add the restart policy. We can
    do that by setting the **ensure** resource to **absent** and running the Vagrant
    provision `swarm-master-02`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Vagrant run is complete, we will change it back to **present** and
    run `vagrant provision swarm-master-02`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For me, my `kibana` container is on **swarm-102** (this could be different
    for you). Once that node fails, `kibana` will restart on a healthy node. So, let''s
    issue `vagrant halt swarm-102`. If we go to our Consul URL, `127.0.0.1:9501`,
    we should see some failures on our nodes and checks, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Built-in HA](img/B05201_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you wait a minute or so, you will see that `kibana` alerts came back and
    the container spawned on another server. For me, `kibana` came back on **swarm-101**,
    as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Built-in HA](img/B05201_08_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then go to our browser and look at `kibana`. For me, it will be at `127.0.0.1:8001`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Built-in HA](img/B05201_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kibana after connecting to Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all our logs are there; our service discovery worked perfectly
    as once the container changed nodes, our health checks turned green.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to operationalize our container environment
    using Puppet. We covered a logging solution using ELK. We took Consul to the next
    level with more in-depth health checks and creating services to monitor our cluster.
    We then tested the built-in HA functionality that ships with Swarm. We have covered
    a lot of ground now since our humble first module in [Chapter 2](ch02.html "Chapter 2. Working
    with Docker Hub"), *Working with Docker Hub*. You are fully equipped to take the
    knowledge that you have got here and apply it in the real world.
  prefs: []
  type: TYPE_NORMAL
