<html><head></head><body>
        

                            
                    Container Persistency and Networking
                
            
            
                
<p class="mce-root">  Containers are processes that run on a host. This seems very simple, but how will this work on a pool of nodes? If we are looking for high availability, being able to run our containers on any host from a pool will ensure execution everywhere. But this approach requires some special logic in our applications. Our applications must be completely portable and avoid friction and dependencies on any host. Applications with many dependencies are always less portable. We need to find a way to manage status data for containers. We will review different persistence strategies in this chapter.</p>
<p>On the other hand, the aforementioned pool of hosts must be able to communicate with all containers. In this chapter, we will learn about basic standalone host networking and introduce advanced cluster-orchestrated networking concepts.</p>
<p>In this chapter, we will cover the differences between stateless and stateful applications, how volumes work and how can we use them, and how the Docker daemon provides networking on standalone environments. We'll also consider interactions between containers and how to publish services provided by processes running within containers.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Understanding stateless and stateful containers</li>
<li>Learning about different persistence strategies</li>
<li>Networking in containers</li>
<li>Learning about container interactions</li>
<li>Publishing applications</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-d5d2f868-857d-4d0c-8c05-627842b2c6c3">Technical requirements</h1>
<p>In this chapter, we will learn about Docker volumes and networking concepts. We'll provide some labs at the end of this chapter that will help you understand and learn about the concepts shown. These labs can be run on your laptop or PC using the provided Vagrant standalone environment or any already deployed Docker host of your own. You can find additional information in this book's GitHub repository: <a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a></p>
<p>Check out the following video to see the Code in Action:</p>
<p>"<a href="https://bit.ly/34DJ3V4" target="_blank">https://bit.ly/34DJ3V4</a>"</p>
<h1 id="uuid-3a013b64-3c73-4583-80ae-da66b9437399" class="mce-root">Understanding stateless and stateful containers</h1>
<p>Portability is key in modern applications because they should run in every environment (on-premises or the cloud). Containers are prepared for these situations. We will also seek the high availability of applications in production, and containers will help us here too.</p>
<p>Not all applications are ready for containers by default. Processes' states and their data are difficult to manage inside containers.</p>
<p>In <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>, we learned that containers are not ephemeral. They live in our hosts. Containers are created, executed, and stopped or killed, but they will remain in our host until they are deleted. We can restart a previously stopped container. But this is only true in standalone environments because all information resides under the host data path-defined directory (<kbd>/var/lib/docker</kbd> and <kbd>C:\ProgramData\docker</kbd> by default on Linux and Windows, respectively). If we move our workloads (that is, our application components running as containers) to another host, we will not have their data and state there. What happens if we need to upgrade their image versions? In that case, we could run a new container and everything will be recreated again. We can launch a new container, but we need to maintain all application data.</p>
<p>Previously, we introduced volumes as a method used to bypass the internal filesystem of containers and their life cycles. Everything inside a volume is, in fact, outside of the container's filesystem. This will help us with application performance using direct access to a host's devices' but it will also keep data. Volumes will persist even when containers are removed (unless we use <kbd>--volumes</kbd> or <kbd>-v</kbd> on removal). Therefore, volumes will help us maintain application data locally, but how about execution on other Docker hosts? We can share images, but a container's associated data will not be there unless we can also share volumes between them.</p>
<p>Under these circumstances, stateless processes – those that do not require any kind of persistent data to work – are easier to manage. These processes are always candidates to run within containers.</p>
<p>And what about stateful processes – those using persistent data between executions? We have to take care in this case. We should provide external volumes or databases to store the process's state and its required data. These concepts are very important when we design microservice-based application architectures.</p>
<p>Let's deep dive into how volumes work.</p>
<h2 id="uuid-6dd5ba03-fa6d-4848-8528-4ca2a5d9bab2">Learning how volumes work</h2>
<p>Previously, we learned how to define volumes in images to simply bypass a container's filesystem. Here is a simple Dockerfile definition showing a defined volume (this is an excerpt from the PostgreSQL database official image):</p>
<pre>FROM alpine:3.10<br/>RUN set -ex; \<br/> postgresHome="$(getent passwd postgres)"; \<br/> postgresHome="$(echo "$postgresHome" | cut -d: -f6)"; \<br/> [ "$postgresHome" = '/var/lib/postgresql' ]; \<br/> mkdir -p "$postgresHome"; \<br/> chown -R postgres:postgres "$postgresHome"<br/>...<br/>...<br/>RUN mkdir -p /var/run/postgresql &amp;&amp; chown -R postgres:postgres /var/run/postgresql &amp;&amp; chmod 2777 /var/run/postgresql<br/>ENV PGDATA /var/lib/postgresql/data<br/>RUN mkdir -p "$PGDATA" &amp;&amp; chown -R postgres:postgres "$PGDATA" &amp;&amp; chmod 777 "$PGDATA"<br/><br/><strong>VOLUME /var/lib/postgresql/data</strong><br/><br/>COPY docker-entrypoint.sh /usr/local/bin/<br/>ENTRYPOINT ["docker-entrypoint.sh"]<br/>EXPOSE 5432<br/>CMD ["postgres"]</pre>
<p>We have omitted many lines because we just want to review the <kbd>VOLUME</kbd> definition. In this case, all data stored under the <kbd>/var/lib/postgresql/data</kbd> directory will be outside of the container's filesystem. This is an <strong>unnamed volume</strong> definition and it will be identified in our system by a random ID when we run a container using this image. It was defined for bypassing copy-on-write filesystems. Every time we create or run a new container, a new random identifier volume will be created. These volumes should be removed manually or by using the <kbd>--volume</kbd> or <kbd>-v</kbd> options when we remove their associated containers.</p>
<p>Now, it is time to define the different volumes types we can have on Docker:</p>
<ul>
<li><strong>Unnamed volumes</strong>: These are the volumes that are defined on images and therefore created using random identifiers. It is hard to track them on local filesystems because they are unnamed. As volumes can grow very fast, depending on your application, it is very important to check for volume definitions before running any image on your local system. Remember that unnamed volumes will grow under your Docker data root path, wherever it is.</li>
<li><strong>Named volumes</strong>: These are the volumes we create manually. As we learned in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml"><em>Chapter 1</em></a>, <em>Modern Infrastructures and Applications with Docker</em>, volumes are Docker objects and we have some actions to control them. In this chapter, we will learn about their associated actions and how to use them. These volumes will be located under the data root path also, but we can use different plugins or drivers to create them. Drivers will allow local or remote volumes, via NFS for example. In these cases, what we will have under the data root path is a link to the real mounted remote filesystem. Consequently, these volumes will not consume local storage if they are remote.</li>
<li><strong>Localhost directories or files</strong>: In this case, we will use host directories and files inside containers. We usually refer to these volumes as <strong>bind mounts</strong>. We must take care of file and directory permissions because we can also use any special file inside containers (including devices). Adding permissions that are too open will give users access to your host's devices. They will require appropriate process capabilities and permissions. It is important to understand that Docker does not care about how block devices, directories, and filesystems are mounted on the Docker host. They will be used always as if they were locally available. Bind mounts will not be listed as volumes.</li>
<li><strong>tmpfs volumes</strong>: This kind of volume is temporal. They will only persist in the host memory. When the container stops, the volume will be removed. Files inside them will not persist.</li>
</ul>
<p>All kinds of volumes can be mounted in read-only mode inside containers. This is very important and useful when volume data shouldn't be modified by running processes, for example, when serving static web content. We can have containers that should be able to modify data and others that will only read and serve this modified data using read-only mode.</p>
<p>Named volumes or bind mounts will retain data. Unnamed volumes will be created with new containers. Keep this in mind. If we need to provide some data to an unnamed volume, it should be done when the container starts. We can also define a procedure in the image definition. This concept is very important as the position of the <kbd>VOLUME</kbd> definition in Dockerfiles matters. As we learned in <a href="3952ec16-ca49-4bc2-b7e6-d6f17fec3fab.xhtml">Chapter 2</a>, <em>Building Docker Images</em>, image creation is based on a sequence of container executions. If we add a volume for a specific path, all subsequent executions will not retain data in that directory. The building process will create a new unnamed volume on each new container and content will not be used between executions.</p>
<h2 id="uuid-306acc00-1519-4d0f-b17c-2100654c5497">Learning about volume object actions</h2>
<p>Volumes can be created, used, and removed. We will also be able to inspect all their properties. The following table shows the actions that are allowed for volume objects:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Objects</strong></p>
</td>
<td>
<p><strong>Actions</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p> </p>
<p><kbd>create</kbd></p>
</td>
<td>
<p>We are able to create named volumes. We can add labels for filtering the listing output, as we learned in previous chapters. We can specify the driver to be used for creating a new volume. By default, volumes will use a local driver. This driver will create directories under the <kbd>volumes</kbd> directory. Each new volume will have its own directory containing the required meta-information and a <strong><kbd>_data</kbd></strong><em><strong> </strong></em>subdirectory. This directory contains all files added to the volume. As we mentioned previously, some drivers will provide host external storage resources. Linked directories will provide connection information instead of their data.</p>
<p>We will use <kbd>--driver</kbd> to specify a driver other than <kbd>local</kbd>. The <kbd>--opt</kbd> or <kbd>-o</kbd> arguments allow us to add required options for the specified driver. Each driver will have its own special options.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p> </p>
<p><kbd>inspect</kbd></p>
</td>
<td>
<p>All objects can be inspected. In this case, the <kbd>inspect</kbd> action will provide information about the object's location, the driver used, and the labels provided.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p> </p>
<p><kbd>ls</kbd></p>
</td>
<td>
<p>We can list all volumes using the <kbd>ls</kbd> action. Almost all filtering and formatting options learned throughout this book can be applied. Formatting will also depend on a given volume's properties.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p> </p>
<p><kbd>prune</kbd></p>
</td>
<td>
<p>The <kbd>prune</kbd> option will help us with volume housekeeping. It will remove all created volumes not used by any container. It will not delete any bind mount because they are not really treated as volumes.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p> </p>
<p><kbd>rm</kbd></p>
</td>
<td>
<p>We can remove volumes using the <kbd>rm</kbd> action. It is important to note that volumes attached to existing containers cannot be removed. Containers should be removed before volumes. Alternatively, you can use the <kbd>--volumes</kbd> option on container removal.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, let's introduce how containers use volumes.</p>
<h2 id="uuid-f3de6a22-8ca0-493d-b8b6-53f33b2e00a8">Using volumes in containers</h2>
<p>First, we will start with unnamed volumes. These are volumes defined in a container's images. As we mentioned previously, always review images before execution. If we run an application that stores a huge amount of data on a predefined unnamed volume, our Docker host can run out of disk space. It is very important to review what image will run and what resources will be required. If we take a quick view of the <kbd>postgres:alpine</kbd> image (the PostgreSQL database image based on Alpine Linux), for example, we will find a volume definition (we first pull the <kbd>postgres:alpine</kbd> image from Docker Hub):</p>
<pre><strong>$ docker image pull --quiet postgres:alpine</strong><br/><strong>docker.io/library/postgres:alpine</strong><br/><br/><strong>$ docker image inspect postgres:alpine --format "{{ .Config.Volumes }} "</strong><br/><strong>map[/var/lib/postgresql/data:{}]</strong> </pre>
<p>As we can see, <kbd>postgres:alpine</kbd> will define an unnamed volume to bypass the copy-on-write container filesystem to allow a process to write or modify any content under the <kbd>/var/lib/postgresql/data</kbd> directory.</p>
<p>Let's create a container named <kbd>mydb</kbd> using the <kbd>postgres:alpine</kbd> image:</p>
<pre><strong>$ docker container run -d --name mydb postgres:alpine</strong><br/><strong>e1eb5e5df725541d6a3b31ee86746ab009251c5292b1af95b22b166c9d0922de</strong></pre>
<p>Now, we can inspect the <kbd>mydb</kbd> container, looking for its mount points (identifiers will be different in your system):</p>
<pre><strong>$ docker container inspect mydb --format "{{ .Mounts }} "</strong><br/><strong>[{volume c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f /var/lib/docker/volumes/c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f/_data /var/lib/postgresql/data local true }]</strong> </pre>
<p>Using the obtained volume identifier, we can review its properties:</p>
<pre><strong>$ docker volume inspect c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong><br/><strong>[</strong><br/><strong> {</strong><br/><strong> "CreatedAt": "2019-11-03T19:20:59+01:00",</strong><br/><strong> "Driver": "local",</strong><br/><strong> "Labels": null,</strong><br/><strong> "Mountpoint": "/var/lib/docker/volumes/c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f/_data",</strong><br/><strong> "Name": "c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f",</strong><br/><strong> "Options": null,</strong><br/><strong> "Scope": "local"</strong><br/><strong> }</strong><br/><strong>]</strong></pre>
<p>The output shows where this volume is mounted on our host (<kbd>/var/lib/docker/volumes/c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f/_data</kbd>) and what container is using it that it's mounted on (<kbd>/var/lib/postgresql/data</kbd>).</p>
<p>If we take a look at the <kbd>/var/lib/docker/volumes/c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f/_data</kbd> directory, we can list all PostgreSQL database data files (notice in the following log that the directory is owned by root, so root access will be required):</p>
<pre><strong>$ sudo ls -lart /var/lib/docker/volumes/c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f/_data</strong><br/><strong>total 64</strong><br/><strong>drwxr-xr-x 3 root root 19 nov 3 19:20 ..</strong><br/><strong>-rw------- 1 70 70 3 nov 3 19:20 PG_VERSION</strong><br/><strong>drwx------ 2 70 70 6 nov 3 19:20 pg_twophase</strong><br/><strong>...</strong><br/><strong>...</strong><br/><strong>-rw------- 1 70 70 94 nov 3 19:20 postmaster.pid</strong><br/><strong>drwx------ 2 70 70 25 nov 3 19:42 pg_stat_tmp</strong></pre>
<p>Notice that files and directories are owned by <kbd>userid</kbd> (<kbd>70</kbd>) and <kbd>groupid</kbd> (<kbd>70</kbd>). This is because the container's main process is not running under the root user and, as a result, all files created by the PostgreSQL process will be owned by an internal <kbd>postgres:postgres</kbd> user, whose ID may be different or even may not exist on our host. This is the ID used within the container.</p>
<p>Let's stop the <kbd>mydb</kbd> container and check our volume. You will see that the volume is still in our system:</p>
<pre><strong>$ docker container stop mydb</strong><br/><strong> mydb</strong><br/><br/><strong>$ docker volume ls --filter name=c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong><br/><strong> DRIVER VOLUME NAME</strong><br/><strong> local c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong></pre>
<p>Again, we can start our <kbd>mydb</kbd> container and it will reuse its volume data. If we had added data to this database, we would still be able to access it, because the volume persists our data.</p>
<p class="mce-root">Now, let's remove the <kbd>mydb</kbd> container:</p>
<pre><strong>$ docker container rm mydb</strong><br/><strong>mydb</strong></pre>
<p>We can verify that the volume is still under <kbd>/var/lib/docker/volumes</kbd>:</p>
<pre><strong>$ docker volume ls --filter name=c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong><br/><strong>DRIVER VOLUME NAME</strong><br/><strong>local c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong></pre>
<p>Volumes survive containers unless we use <kbd>--volume</kbd> to remove them with its associated container. We can also reuse volume content with other containers. But unnamed containers are not easy to manage because they are identified only by a digest. We will remove this volume:</p>
<pre><strong>$ docker volume rm c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong><br/><strong>c888a831d6819aea6c6b4474f53b7d6c60e085efaa30d17db60334522281d76f</strong></pre>
<p>Now, let's create a volume named <kbd>mydata</kbd>:</p>
<pre><strong>$ docker volume create mydata</strong><br/><strong>mydbdata</strong></pre>
<p>In this case, we can create a new container using this volume and its content will be available for our new process.</p>
<p>It is important to understand that the <kbd>VOLUME</kbd> definition in an image is not required to use volumes on containers. But they will help us understand what directories should be managed out of the container filesystem. Good container images will define the directories where persistent data should be stored.</p>
<p>Docker containers can mount volumes using two different options in terms of container creation or execution:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 14.0087%">
<p><kbd>--volume</kbd> or <kbd>-v</kbd></p>
</td>
<td style="width: 85.2176%">
<p>We will use this option with three arguments, separated by <kbd>:</kbd>. We will use the last argument to declare what type of access will be provided (read-only or read-write). The second argument will indicate the container's directory or file where the volume will be mounted within the container. The first argument will be different, depending on what type of resource we are using. If we are using bind mounts, we will use them as a file or directory in the host. If we are using named volumes, this argument will declare which volume will be mounted inside the container.</p>
</td>
</tr>
</tbody>
</table>
<p>There are other options for the third argument when using the <kbd>--volume</kbd> option. In addition to read or write access, we can specify <kbd>z</kbd> or <kbd>Z</kbd> when we use SELinux. If the volume is going to be shared between multiple containers, we will use these options to declare the volume content as private and unshareable.</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><kbd>--mount</kbd></p>
</td>
<td>
<p>This notation allows more arguments than <kbd>--volume</kbd>. We will use the key/value format to declare multiple options. The available keys are as follows:</p>
<strong>- type</strong>: Values available are <kbd>bind</kbd>, <kbd>volume</kbd>, or <kbd>tmpfs</kbd>.
<p><strong>- source (or src)</strong>: This will describe the volume or host path.</p>
<p><strong>- destination (or dst or target)</strong>: This describes the path where the volume content will be mounted.</p>
<p><strong>- readonly</strong>: This identifies the access type for the volume content.</p>
</td>
</tr>
</tbody>
</table>
<p>There is only one difference between using the <kbd>--volume</kbd> and <kbd>--mount</kbd> options. Using <kbd>--volume</kbd> will create the endpoint if we specify a path that does not exist in the Docker host when using bind mounts, while <kbd>--mount</kbd> will raise an error in this instance and it will not be created.</p>
<p>Now, we'll start an <kbd>alpine</kbd> container using the defined volume mounted in <kbd>/data</kbd>. We named it <kbd>c1</kbd> here. We will just touch a file under its <kbd>/data</kbd> directory:</p>
<pre><strong>$ docker container run --name c1 -v mydata:/data -ti alpine</strong><br/><strong>/ # touch /data/persistent-file-test</strong><br/><strong>/ # exit</strong></pre>
<p>After exiting the container, we can list the files under the <kbd>mydata</kbd> volume filesystem:</p>
<pre><strong>$ sudo ls -lart /var/lib/docker/volumes/mydata/_data</strong><br/><strong>total 0</strong><br/><strong>drwxr-xr-x 3 root root 19 nov 3 20:34 ..</strong><br/><strong>-rw-r--r-- 1 root root 0 nov 3 20:44 persistent-file-test</strong><br/><strong>drwxr-xr-x 2 root root 34 nov 3 20:44 .</strong></pre>
<p>Now, we can create a new container and reuse our previously created named volume, <kbd>mydata</kbd>. In this example, we will mount it under <kbd>/tmp</kbd>:</p>
<pre><strong>$ docker container run --name c2 -v mydata:/tmp -ti alpine ls -lart /tmp</strong><br/><strong>total 0</strong><br/><strong>-rw-r--r-- 1 root root 0 Nov 3 19:44 persistent-file-test</strong><br/><strong>drwxr-xr-x 2 root root 34 Nov 3 19:44 .</strong><br/><strong>drwxr-xr-x 1 root root 6 Nov 3 19:48 ..</strong></pre>
<p>Now, both containers, <kbd>c1</kbd> and <kbd>c2</kbd>, have mounted the <kbd>mydata</kbd> volume. Consequently, we can't remove the <kbd>mydata</kbd> volume unless both are removed from the local system (even if we use <kbd>--force</kbd> for removal):</p>
<pre><strong>$ docker volume rm mydata</strong><br/><strong>Error response from daemon: remove mydata: volume is in use - [a40f15ab8977eba1c321d577214dc4aca0f58c6aef0eefd50d6989331a8dc723, 472b37cc19571960163cdbcd902e83020706a46f06fbb6c7f9f1679c2beeed0e]</strong></pre>
<p>We will only be able to remove the <kbd>mydata</kbd> volume when both containers have been removed:</p>
<pre><strong>$ docker container rm c1 c2</strong><br/><strong>c1</strong><br/><strong>c2</strong><br/><br/><strong>$ docker volume rm mydata</strong><br/><strong>mydata</strong></pre>
<p>Now, let's learn about some strategies and use cases for storing persistent data in containerized environments.</p>
<h1 id="uuid-d79cefca-5b28-4e89-a28b-058590e061ba" class="mce-root">Learning about different persistence strategies</h1>
<p>As we've already learned, there are different approaches to persistence in containers. Choosing the right solution will depend on the use case or requirements of the environment and our applications.</p>
<h2 id="uuid-c2df236c-1e6c-4fae-b8c3-ff9d13e38deb">Local persistence</h2>
<p>We will use local directories or files whenever we are deploying applications on isolated and standalone Docker daemons. In this approach, you should take care of filesystem permissions and secure module configurations. This strategy is quite interesting for developers as they can run multi-container applications on their laptops using local source code files inside containers. Therefore, all changes made on their local files will be synced within the containers (in fact, they will not quite be synced; rather, they are the same files that are mounted inside the container filesystem as a bind mount volume). We will review some examples of this in the <em>Chapter labs</em> section. This solution will not provide high availability.</p>
<h2 id="uuid-0b3af1d0-14de-418d-87e5-af3e8cdbab2e">Distributed or remote volumes</h2>
<p>These are the preferred solutions for orchestrated environments. We should provide a pool of distributed or remote storage endpoints to allow applications to run everywhere within the cluster. Depending on your applications, volume speed could be key for deciding which driver to use. We will also have different choices regarding cloud providers. But for common use cases with static content, <strong>Network File System </strong>(<strong>NFS</strong>) will be fine. While it would not be enough for databases or high I/O application requirements, locking filesystem files is needed when we scale instances using shared resources. The Docker daemon will not manage these situations as they are out of Docker's scope. Volume I/O and file locking will really depend on the application logic and its architecture. Neither distributed nor remote volume solutions will provide high availability. In fact, Docker doesn't really know anything about storage. It just cares about volumes, no matter how storage was implemented on your host.</p>
<p>Volume drivers provide extensions to extend Docker's out-of-the-box features. The Docker plugin system changed in version 1.12 of Docker. Therefore, we refer to old plugins as <em>legacy plugins</em>, which are not managed using <kbd>docker plugin</kbd> actions. We can find a list of legacy volume plugins at <a href="https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins">https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins</a>. New plugins are always managed using <kbd>docker plugin</kbd> command-line actions. These plugins may require special capabilities because they should be able to execute privileged actions at the host system level. We will review a quick lab at the end of this chapter, where we'll use the <kbd>sshfs</kbd> plugin.</p>
<p>These described use cases are closer to data management. But what about the application state? This is usually managed using volumes, but it really depends on your application architecture. One recommendation for new application development projects is to track the application state out of containers or even volumes. This makes it easier to manage instance replication when we need to scale up or down some components. But remember, it should be managed at the application level. Docker will just manage how your containerized application components run; it will not manage their application states or dependencies.</p>
<p class="mce-root">Now that we know how to manage container data and their states using persistent volumes, let's get into networking features.</p>
<h1 id="uuid-0f72d08e-0027-445a-9e42-b67c93c93b93" class="mce-root">Networking in containers</h1>
<p>We have already learned that containers are processes that run isolated on top of host operating systems. This isolation is provided using different namespaces for users, processes trees, inter-process communications, and a set of complete network resources for each containerized process. Therefore, each container will have its own network interfaces. To be able to communicate with the world, by default, the Docker daemon will create a bridged interface called <kbd>docker0</kbd>. The Docker network plane has not changed too much in the latest releases. It can be extended using external tools and plugins and is based on bridged and virtual network interfaces that connect hosts and container resources.</p>
<p>By default, a fresh Docker installation will show three network objects:</p>
<pre><strong>$ docker network ls</strong><br/><strong> NETWORK ID NAME DRIVER SCOPE</strong><br/><strong> 033e4c3f3608 bridge bridge local</strong><br/><strong> 82faac964567 host host local</strong><br/><strong> 2fb14f721dc3 none null local</strong></pre>
<p>As we have already learned, all objects are identified by their unique ID. The Docker network listing shows the network <kbd>NAME</kbd> (we can set our own network name), <kbd>DRIVER</kbd> (the network type), and <kbd>SCOPE</kbd> columns (indicating where this network will be available). There are different types of networks, according to which network driver containers will be used to attach to that network.</p>
<p>Besides all common object actions such as <kbd>create</kbd>, <kbd>list</kbd> (using <kbd>ls</kbd>), <kbd>inspect</kbd>, and <kbd>remove</kbd> (using <kbd>rm</kbd> or <kbd>prune</kbd>), networks also have <kbd>connect</kbd> and <kbd>disconnect</kbd> actions in order to attach or detach containers to/from them.</p>
<p>Let's review some of the creation options before deep diving on each network type:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Option</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--attachable</kbd></p>
</td>
<td>
<p>This option enables manual container attachment. It is not required for locally scoped networks.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--aux-address</kbd></p>
</td>
<td>
<p>Using <kbd>--aux-address</kbd>, we can add a host and its addresses to this network. For example, we can use <kbd>--aux-address="mygateway=192.168.1.10"</kbd> to set a specific host-to-IP mapping on the declared network. It is usually used on <kbd>macvlan</kbd> networks.</p>
</td>
</tr>
<tr>
<td>
<p class="mce-root"><kbd>--config-from</kbd></p>
<p>and</p>
<p><kbd>--config-only</kbd></p>
</td>
<td>
<p>We can create (or reuse previously created) network configurations. This is very useful for building configurations using automation tools, for example, on different hosts and being able to use them when needed.</p>
</td>
</tr>
<tr>
<td>
<p class="mce-root"><kbd>--driver</kbd> or <kbd>-d</kbd></p>
<p>and</p>
<p><kbd>--opt</kbd></p>
</td>
<td>
<p>This option allows us to specify which driver to use. By default, we can only use <kbd>macvlan</kbd>, <kbd>none</kbd>, <kbd>host</kbd>, and <kbd>bridge</kbd>. But we can extend Docker's networking capabilities using other external plugins. We will use <kbd>--opt</kbd> to customize the applied driver.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--gateway</kbd></p>
</td>
<td>
<p>We can overwrite the default gateway (the lower IP address of the defined subnet, by default) and specify another IP address for this purpose.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--ingress</kbd></p>
</td>
<td>
<p>This option will be used in cases where we want to create a special Swarm vxLan network for internal service management.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--internal</kbd></p>
</td>
<td>
<p>This option is only available on overlay networks. We will only use it to define internal networks because, by default, all overlay networks will be attached to the <kbd>docker_gwbridge</kbd> bridge network (created automatically when operating on a Swarm) to provide external connectivity.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--ip-range</kbd></p>
</td>
<td>
<p>Once we have configured a subnet, we can specify a range of IP addresses to be used for containers.</p>
</td>
</tr>
<tr>
<td>
<p class="mce-root"><kbd>--ipam-driver</kbd></p>
<p>and</p>
<p><kbd>--ipam-opt</kbd></p>
</td>
<td>
<p>With these options, we can use an external IP address management driver.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--ipv6</kbd></p>
</td>
<td>
<p>We will use this option to enable IPv6 on this network.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--label</kbd></p>
</td>
<td>
<p>With this, we can add metadata information to networks for better filtering.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--scope</kbd></p>
</td>
<td>
<p>With this option, we declare the scope where the network will be created for local or Swarm usage.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--subnet</kbd></p>
</td>
<td>
<p>This specifies a subnet in CIDR format that represents a network segment.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Once created, network objects will exist until they are removed. But removal is only possible when no containers are attached to them. It is important to understand that dead containers will still have endpoints configured for existing networks and must, as a result, be deleted before network removal. On the other hand, the <kbd>prune</kbd> action will remove all unused networks.</p>
<p>Docker manipulates the <kbd>iptables</kbd> rules for you every time a network is created or some connection or container process publication must be implemented. You can avoid this feature, but we strongly recommend allowing the Docker daemon to manage these rules for you. It is not easy to track unexpected behaviors and there will be many rules to manage.</p>
<p>Now that we have the basic <kbd>create</kbd> command options under our belts, let's look at the different standard networks we can create.</p>
<h2 id="uuid-5d1476a7-ff99-45c6-9dac-5daafd2e2e99">Using the default bridge network</h2>
<p>Bridge is the default network type for all containers. Any other network types must be declared on container creation or execution using the <kbd>--network</kbd> optional parameter.</p>
<p>In operating system terms, we use bridged interfaces to allow forwarded traffic from other virtual interfaces. All those virtual interfaces will use a physical interface, associated with the bridge, to talk to other network devices or connected hosts. In the world of containers, all container interfaces are virtual and they will be attached to these bridge interfaces at the host level. Therefore, all containers attached to the same bridge interface will see each other.</p>
<p>Let's look at a quick example of using a bridge network:</p>
<ol>
<li>We just run two containers, <kbd>c1</kbd> and <kbd>c2</kbd>, attached to the default network (notice that we have not defined any network at all):</li>
</ol>
<pre style="padding-left: 60px"><strong>zero@sirius:~$ docker container run -ti -d --name c1 alpine ping 8.8.8.8</strong><br/><strong>c44fbefb96b9321ef1a0e866fa6aaeb26408fc2ef484bbc9ecf904546f60ada7</strong><br/><br/><strong>zero@sirius:~$ docker container run -ti -d --name c2 alpine ping 8.8.8.8</strong><br/><strong>cee980d7f9e587357375e21dafcb406688ac1004d8d7984ec39e4f97533492ef</strong></pre>
<ol start="2">
<li>We find their IP addresses:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container inspect c1 --format "{{ .NetworkSettings.Networks.bridge.IPAddress }}"</strong><br/><strong>172.17.0.2</strong><br/><br/><strong>$ docker container inspect c2 --format "{{ .NetworkSettings.Networks.bridge.IPAddress }}"</strong><br/><strong>172.17.0.3</strong></pre>
<ol start="3">
<li>Consequently, we can ping each of them:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker exec c1 ping -c 2 172.17.0.3 </strong><br/><strong>PING 172.17.0.3 (172.17.0.3): 56 data bytes</strong><br/><strong>64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.113 ms</strong><br/><strong>64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.210 ms</strong><br/><strong>--- 172.17.0.3 ping statistics ---</strong><br/><strong>2 packets transmitted, 2 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.113/0.161/0.210 ms</strong></pre>
<ol start="4">
<li>Let's quickly review some of the <kbd>c1</kbd> container properties:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container inspect c1 --format "{{json .NetworkSettings.Networks }}"</strong><br/><strong>{"bridge":{"IPAMConfig":null,"Links":null,"Aliases":null,"NetworkID":"033e4c3f360841b0826f3b850fe9f5544d145bea644ee1955717e67d02df92ce","EndpointID":"390d2cf0b933ddd3b11fdebdbf6293c97f2a8568315c80794fad6f5b8eef3207","Gateway":"172.17.0.1","IPAddress":"172.17.0.2","IPPrefixLen":16,"IPv6Gateway":"","GlobalIPv6Address":"","GlobalIPv6PrefixLen":0,"MacAddress":"02:42:ac:11:00:02","DriverOpts":null}}</strong></pre>
<ol start="5">
<li>Each container will have its own IP address and <kbd>EndpointID</kbd>. Let's inspect the bridge network's configuration (created by Docker by default):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker network inspect bridge</strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "Name": "bridge",</strong><br/><strong>        "Id": "033e4c3f360841b0826f3b850fe9f5544d145bea644ee1955717e67d02df92ce",</strong><br/><strong>       ...</strong><br/><strong>        "IPAM": {</strong><br/><strong>          ...            </strong><br/><strong>            "Config": [</strong><br/><strong>                {</strong><br/><strong>                    "Subnet": "172.17.0.0/16",</strong><br/><strong>                    "Gateway": "172.17.0.1"</strong><br/><strong>                }</strong><br/><strong>            ]</strong><br/><strong>        },</strong><br/><strong>        ...</strong><br/><strong>        "Containers": {</strong><br/><strong>            "c44fbefb96b9321ef1a0e866fa6aaeb26408fc2ef484bbc9ecf904546f60ada7": {</strong><br/><strong>                "Name": "c1",</strong><br/><strong>                "EndpointID": "390d2cf0b933ddd3b11fdebdbf6293c97f2a8568315c80794fad6f5b8eef3207",</strong><br/><strong>                "MacAddress": "02:42:ac:11:00:02",</strong><br/><strong>                "IPv4Address": "172.17.0.2/16",</strong><br/><strong>                "IPv6Address": ""</strong><br/><strong>            },</strong><br/><strong>            "cee980d7f9e587357375e21dafcb406688ac1004d8d7984ec39e4f97533492ef": {</strong><br/><strong>                "Name": "c2",</strong><br/><strong>                "EndpointID": "cb49b93bc0bdd3eb887ad3b6fcd43155eb4ca7688c788719a27acc9e2f2e2a9d",</strong><br/><strong>                "MacAddress": "02:42:ac:11:00:03",</strong><br/><strong>                "IPv4Address": "172.17.0.3/16",</strong><br/><strong>                "IPv6Address": ""</strong><br/><strong>            }</strong><br/><strong>        },</strong><br/><strong>        "Options": {</strong><br/><strong>            "com.docker.network.bridge.default_bridge": "true",</strong><br/><strong>            "com.docker.network.bridge.enable_icc": "true",</strong><br/><strong>            "com.docker.network.bridge.enable_ip_masquerade": "true",</strong><br/><strong>            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",</strong><br/><strong>            "com.docker.network.bridge.name": "docker0",</strong><br/><strong>            "com.docker.network.driver.mtu": "1500"</strong><br/><strong>        },</strong><br/><strong>        "Labels": {}</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>Let's talk about some of the most important sections in this output:</p>
<ul>
<li>This network is not using IPv6. It's called <kbd>bridge</kbd>, was created using the <kbd>bridge</kbd> driver, and will only be available locally on this host.</li>
<li>It was created using the <kbd>172.17.0.0/16</kbd> subnet and consequently, all containers on this network will get an IP address on this segment range.</li>
<li>The bridge interface has the IP address <kbd>172.17.0.1</kbd> and will be the default gateway for all containers.</li>
<li>We have two running containers on this network. They are both listed under the <kbd>Containers</kbd> section with their virtual MAC addresses, IP addresses, and associated endpoints.</li>
<li>There are a number of options that can be used during network creation that are of interest:
<ul>
<li><kbd>com.docker.network.bridge.default_bridge: true</kbd>: This means that this is the default bridge when no network is defined.</li>
<li><kbd>com.docker.network.bridge.enable_icc: true</kbd>: This parameter indicates that containers connected to this network can talk to each other. We can disable this feature on custom bridges, allowing just North-South traffic.</li>
<li><kbd>com.docker.network.bridge.name: docker0</kbd>: This is the name of the associated host interface.</li>
</ul>
</li>
</ul>
<p>When we refer to <em>North-South traffic</em>, we mean the type of communication that goes out of the Docker host to the containers and vice versa. On the other hand, <em>East-West traffic</em> is the traffic between different containers. These are references to well-known network terms that are applied to describe network traffic. </p>
<h2 id="uuid-31948c7a-aa68-4456-8e7d-d7769b7a284e">Understanding null networks</h2>
<p>Null or none networks are used when we need to deploy a container that should run without any network interface. Although it might sound useless, there are many situations where we may need to launch a task for executing a mathematical operation, compression, or many other examples that don't require networking capabilities. In these cases, we just need to use volumes and we really do not need any network operation. Using a null network ensures that the task will only have access to its required resources. If it does not require network access, do not provide it. By default, the container will use a <kbd>bridge</kbd> network unless we specify <kbd>none</kbd>:</p>
<pre><strong>$ docker run -ti --network none alpine</strong><br/><strong>/ # ip add</strong><br/><strong>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000</strong><br/><strong> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</strong><br/><strong> inet 127.0.0.1/8 scope host lo</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>/ #</strong></pre>
<p>Now that we understand that containers can have a null interface to avoid networking, we can look at the host's network namespace.</p>
<h2 id="uuid-78aa09bb-d16c-4a94-b196-57031ecf3b51">Understanding the host network</h2>
<p>Host networking is only available on Linux hosts. This is important because it is an important difference in Windows containers.</p>
<p>Using host networking, the container shares the <kbd>host</kbd> networking namespace. Therefore, the container will get all host IP addresses, and every port that's used at the container level will be set on the host. Consequently, no more than one container using a specific given port will be allowed to run at a time. But, on the other hand, network performance is better because container services are directly attached to host ports. There isn't any NAT or firewall rule adaptation:</p>
<pre><strong>$ docker run -ti --network host alpine</strong><br/><strong>/ # ip add</strong><br/><strong>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000</strong><br/><strong> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</strong><br/><strong> inet 127.0.0.1/8 scope host lo</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong> inet6 ::1/128 scope host </strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>2: enp0s25: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN qlen 1000</strong><br/><strong> link/ether 68:f7:28:c1:bc:13 brd ff:ff:ff:ff:ff:ff</strong><br/><strong>3: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</strong><br/><strong> link/ether 34:02:86:e3:f6:25 brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 192.168.200.165/24 brd 192.168.200.255 scope global dynamic wlp3s0</strong><br/><strong> valid_lft 51sec preferred_lft 51sec</strong><br/><strong> inet6 fe80::ee87:e44f:9189:f720/64 scope link </strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>...</strong><br/><strong>...</strong><br/><strong>10: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </strong><br/><strong> link/ether 02:42:11:73:cc:2b brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong> inet6 fe80::42:11ff:fe73:cc2b/64 scope link </strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>...</strong><br/><strong>...</strong><br/><strong>18: docker_gwbridge: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </strong><br/><strong> link/ether 02:42:4b:21:09:6d brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 172.18.0.1/16 brd 172.18.255.255 scope global docker_gwbridge</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong> inet6 fe80::42:4bff:fe21:96d/64 scope link </strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>20: veth82a8134@if19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue master docker_gwbridge state UP </strong><br/><strong> link/ether a6:5d:02:ed:79:0a brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet6 fe80::a45d:2ff:feed:790a/64 scope link </strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>22: veth4b1102e@if21: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue master docker0 state UP </strong><br/><strong> link/ether fa:08:70:aa:b1:4b brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet6 fe80::f808:70ff:feaa:b14b/64 scope link </strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>27: wwp0s20u4: &lt;BROADCAST,MULTICAST,NOARP&gt; mtu 1428 qdisc noop state DOWN qlen 1000</strong><br/><strong> link/ether 06:1b:05:d6:e9:12 brd ff:ff:ff:ff:ff:ff</strong><br/><strong>/ #</strong> </pre>
<p>Here, you can see that all host interfaces are listed because the container is using its network namespace.</p>
<p>This networking mode is risky because we are allowing any kind of communication on the containers. This should be used with care in <strong>privileged mode</strong>. It is very common in monitoring tools or when we run applications that require high levels of network interface performance.</p>
<p>We can define our own network interfaces. We'll create custom bridge networks in the next section.</p>
<h2 id="uuid-61838d33-9f14-467a-88d1-6391c7271770">Creating custom bridge networks</h2>
<p>As we discussed in the default bridge network example, this networking type will be associated with host <kbd>bridge</kbd> interfaces. By default, it is attached to <kbd>docker0</kbd>, but every time we create a new bridge network, a new <kbd>bridge</kbd> interface will be created for us and all attached containers will have a virtual interface linked to this one.</p>
<p>There are a few very important differences between a default bridge network and custom created ones:</p>
<ul>
<li><strong>Custom bridge isolation</strong>: Each new custom bridge network created will have its own associated bridge with its own subnet and host <kbd>iptables</kbd>. This feature provides a higher level of isolation as only attached containers can talk to each other. All other containers running on the same host will not <em>see</em> these containers running on custom bridge networks.</li>
<li><strong>Internal DNS</strong>: The Docker daemon provides a custom DNS for each custom bridge network. This means that all containers running on the same network will know each other by name. This is a very important feature because your service discovery will not need any external source of knowledge. But remember that this is valid only for internal usage within the network.</li>
</ul>
<p>We can provide this kind of DNS resolution on default bridge networks using the legacy <kbd>--link</kbd> functionality. This was the way of interconnecting containers on old Docker releases. Nowadays, using custom bridge networks is considered as providing better isolation.</p>
<ul>
<li><strong>On-the-fly container attachment</strong>: In default bridge networks, we must provide connectivity in terms of container creation or execution. Imagine that we used a null or none network for a container and we want to attach it to a default bridge network later – this is not possible. Once a container is created, it can't be attached to a default bridge network later. It must be recreated from the beginning with that network attachment. On the other hand, custom bridge networks are attachable, which means that we can consider a situation where our container was created without a specific attachment and can add it later. We can also run a container with multiple interfaces on different custom networks, with its name resolution.</li>
</ul>
<p>Let's review a quick example. We will provide more detailed examples in the <em>Chapter labs</em> section of this chapter:</p>
<pre><strong>$ docker network create --driver bridge --internal --subnet 192.168.30.0/24 --label internal-only internal-only</strong><br/><strong>c275cdd25b422b35d3f2b4fbbb153e7cd09c8721133667cfbeb9c297af89364a</strong></pre>
<p>We review the created network properties (notice the defined subnet) and internal settings:</p>
<pre><strong>$ docker network inspect internal-only </strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "Name": "internal-only",</strong><br/><strong>        "Id": "c275cdd25b422b35d3f2b4fbbb153e7cd09c8721133667cfbeb9c297af89364a",</strong><br/><strong>        "Created": "2019-11-10T11:03:20.490907017+01:00",</strong><br/><strong>        "Scope": "local",</strong><br/><strong>        "Driver": "bridge",</strong><br/><strong>        "EnableIPv6": false,</strong><br/><strong>        "IPAM": {</strong><br/><strong>            "Driver": "default",</strong><br/><strong>            "Options": {},</strong><br/><strong>            "Config": [</strong><br/><strong>                {</strong><br/><strong>                    "Subnet": "192.168.30.0/24"</strong><br/><strong>                }</strong><br/><strong>            ]</strong><br/><strong>        },</strong><br/><strong>        "Internal": true,</strong><br/><strong>        "Attachable": false,</strong><br/><strong>        "Ingress": false,</strong><br/><strong>        "ConfigFrom": {</strong><br/><strong>            "Network": ""</strong><br/><strong>        },</strong><br/><strong>        "ConfigOnly": false,</strong><br/><strong>        "Containers": {},</strong><br/><strong>        "Options": {},</strong><br/><strong>        "Labels": {</strong><br/><strong>            "internal-only": ""</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>Now, we create a container and test internet access:</p>
<pre><strong>$ docker container run --network internal-only -ti --name intc1 alpine sh</strong><br/><strong>/ # ping 8.8.8.8 -c 2</strong><br/><strong>PING 8.8.8.8 (8.8.8.8): 56 data bytes</strong><br/><strong>--- 8.8.8.8 ping statistics ---</strong><br/><strong>2 packets transmitted, 0 packets received, 100% packet loss</strong><br/><strong>/ #</strong> </pre>
<p>Remember to use the <em>Ctrl</em> + <em>P</em> + <em>Q</em> shortcut to leave the <kbd>intc1</kbd> container running in the background.</p>
<p>You may have noticed that we do not have any egress connectivity. Let's review the internal connectivity with another container:</p>
<pre><strong>$ docker container run --network internal-only -ti --name intc2 alpine sh</strong><br/><strong>/ # ping intc1 -c2</strong><br/><strong>PING intc1 (192.168.30.2): 56 data bytes</strong><br/><strong>64 bytes from 192.168.30.2: seq=0 ttl=64 time=0.185 ms</strong><br/><strong>64 bytes from 192.168.30.2: seq=1 ttl=64 time=0.157 ms</strong><br/><strong>--- intc1 ping statistics ---</strong><br/><strong>2 packets transmitted, 2 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.157/0.171/0.185 ms</strong><br/><strong>/ #</strong> </pre>
<p>As shown in the preceding output, we have internal communication and DNS resolution, but we are unable to talk to any other external IP address.</p>
<p>If we take a look at <kbd>iptables</kbd>, we can see that the creation of the internal network added some very interesting rules to our local firewall. Executing <kbd>iptables -L</kbd> and avoiding all non-Docker related rules, we can observe these rules:</p>
<pre><strong>Chain DOCKER (4 references)</strong><br/><strong>target prot opt source destination</strong><br/><strong>Chain DOCKER-ISOLATION-STAGE-1 (1 references)</strong><br/><strong>target prot opt source destination </strong><br/><strong>DOCKER-ISOLATION-STAGE-2 all -- anywhere anywhere </strong><br/><strong>DOCKER-ISOLATION-STAGE-2 all -- anywhere anywhere </strong><br/><strong>DROP all -- !192.168.30.0/24 anywhere </strong><br/><strong>DROP all -- anywhere !192.168.30.0/24 </strong><br/><strong>DOCKER-ISOLATION-STAGE-2 all -- anywhere anywhere </strong><br/><strong>DOCKER-ISOLATION-STAGE-2 all -- anywhere anywhere </strong><br/><strong>RETURN all -- anywhere anywhere</strong><br/><br/><strong>Chain DOCKER-ISOLATION-STAGE-2 (4 references)</strong><br/><strong>target prot opt source destination </strong><br/><strong>DROP all -- anywhere anywhere </strong><br/><strong>DROP all -- anywhere anywhere </strong><br/><strong>DROP all -- anywhere anywhere </strong><br/><strong>DROP all -- anywhere anywhere </strong><br/><strong>RETURN all -- anywhere anywhere</strong><br/><br/><strong>Chain DOCKER-USER (1 references)</strong><br/><strong>target prot opt source destination </strong><br/><strong>RETURN all -- anywhere anywhere </strong></pre>
<p>These are the rules that manage the internal network isolation we created previously.</p>
<p>We will examine some multi-interface examples toward the end of this chapter in the <em>Chapter labs</em> section.</p>
<h2 id="uuid-ed5320cf-7642-4c9c-891c-eb24403c64e8">The MacVLAN network – macvlan</h2>
<p>The MacVLAN driver assigns a virtual MAC address to each container interface. Consequently, a container will be able to manage its own IP address on the real network. To manage this type of network interface, we need to declare a host physical interface. As containers will get their own MACs, we can use VLANs on these interfaces to provide containers with access only to the defined VLAN. But note that in these cases, we will need to assign all required VLANs to the <kbd>macvlan</kbd> assigned host interface.</p>
<p>The <kbd>macvlan</kbd> driver will only work on Linux hosts (with a kernel version above 3.9; 4.0 is recommended). This kind of interface is usually blocked on cloud providers.</p>
<p>As a result, we have described two different modes for <kbd>macvlan</kbd>:</p>
<ul>
<li><strong>Bridge mode</strong>: In this case (the default one), traffic will go through the defined host physical interface.</li>
<li><strong>802.1q trunk bridge mode</strong>: Traffic will go through an 802.1q VLAN interface, created by the Docker daemon on network creation.</li>
</ul>
<p>In these networks, we usually use <kbd>--aux-address</kbd> to add existing nodes or network devices to this newly created Docker network.</p>
<p>We have been reviewing different interfaces that are provided by Docker out of the box. Now, let's continue our journey and understand how these communications happen at the host level.</p>
<h1 id="uuid-c57476f2-2d75-4b72-a077-9b1bb1e1751f">Learning about container interactions</h1>
<p>There are two different types of communication in container environments:</p>
<ul>
<li>Communication with the external world</li>
<li>Inter-container communications</li>
</ul>
<p>We'll take a look at both of these in this section.</p>
<h2 id="uuid-9b336158-0f6d-4135-a10d-54aaa43ff6f2">Communication with the external world</h2>
<p>There are two features at the host level that are required to allow containers to talk to the external world:</p>
<ul>
<li>IP forwarding is required to allow packets from container IP addresses to go outside the containerized environment. This is done at the kernel level and the Docker daemon will manage the required parameters (the <kbd>ip_forward</kbd> kernel parameter will be set to <kbd>1</kbd>) to allow this strategy. We can change this default behavior setting with <kbd>--ip-forward=false</kbd> in the daemon configuration. This forwarding is required for all kinds of communications between containers in general.</li>
<li><kbd>iptables</kbd> will manage the required rules to strictly allow only required communications once forwarding is enabled. We can manually set <kbd>iptables</kbd> rules, instead of allowing the Docker daemon to take care of these settings, using the <kbd>--iptables=false</kbd> option in the daemon configuration. It is recommended to allow the Docker daemon to manage these rules unless you are sure of what changes to implement. Docker will only manage <kbd>DOCKER</kbd> and <kbd>DOCKER-ISOLATION</kbd> filter chains and we are able to manage custom rules in the <kbd>DOCKER-USER</kbd> chain.</li>
</ul>
<p>By default, Docker forwards all packets and permits all external source IP addresses. If we need to allow only required IP addresses, we can add custom rules to <kbd>DROP</kbd> all non-permitted communications.</p>
<h2 id="uuid-02aca682-c4b8-4af4-ae5a-165821e9d01a">Inter-container communications</h2>
<p>We can also manage inter-container communications with IP forwarding and <kbd>iptables</kbd>. As we've already learned, we can use <kbd>--internal</kbd> on network creation to only allow internal communications. Any other communication out of this defined subnet will be dropped.</p>
<p>On the other hand, we can disallow any inter-container communication by applying <kbd>--icc=false</kbd>. This option manages the internal interaction within containers linked to the same bridge. If we set this parameter to <kbd>false</kbd>, no inter-container communication will be allowed, even if they are running on the same subnet. This is the most secure network configuration because we can still allow specific communications using the <kbd>--link</kbd> option. Container links will create special <kbd>iptables</kbd> rules to allow these specific communications.</p>
<h2 id="uuid-43bdc4b1-45ae-45c6-a545-be93b4511581">DNS on custom bridge networks</h2>
<p>We've already learned that custom bridge networks own an internal DNS. This means that any container interaction can be managed using container names. This internal DNS will always run on <kbd>127.0.0.11</kbd>. We can modify some of its features, such as adding new hosts, for example.</p>
<p>Let's review some of the common features that can easily be manipulated to improve application discovery and interactions:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Features</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>--network-alias=ALIAS</kbd></p>
</td>
<td>
<p>This option allows us to add another internal DNS name to a container.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--link=CONTAINER_NAME:ALIAS</kbd></p>
</td>
<td>
<p>We have been talking about the link option for legacy environments. It is also a way to allow specific communications when no container interaction is allowed by default. This option will also add an entry to the internal DNS to allow the resolution of <kbd>CONTAINER_NAME</kbd> as a defined <kbd>ALIAS</kbd>. This use case is different to <kbd>--network-alias</kbd> because it is used on different containers.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>--dns</kbd>,</p>
<p><kbd>--dns-search</kbd>,</p>
<p>and <kbd>--dns-option</kbd></p>
</td>
<td>
<p>These options will manage forwarded DNS resolution in cases where an internal DNS cannot resolve a defined name. We can add a forwarder DNS, with its specific options to allow or disallow external searches for some containers. This will help us use different name resolutions to access internal or external applications.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now that we have learned about the different interfaces that are available and how communications work at the host system level, let's go ahead and learn how applications will be accessed from the client side. We have just introduced <kbd>iptables</kbd> as a mechanism to gain that access automatically when deploying containers on different networks. In the next section, we will deep dive into publishing application methods for standalone Docker hosts.</p>
<h1 id="uuid-5a6bb926-efdc-4771-a76b-44d898eb8e3c" class="mce-root">Publishing applications</h1>
<p>By default, all container processes are isolated from outside access. This means that although we had defined a port for the process service (using <kbd>EXPOSE</kbd> on images), it will not be accessible unless we declare it publicly available. This is a great security measure. No external communication will be allowed until it is specifically declared. Only containers attached to the same bridged network or host, using its host internal IP (attached to the bridge), will be able to use the process service.</p>
<p>Let's review a quick example using the <kbd>nginx:alpine</kbd> base image. We know that <kbd>nginx:alpine</kbd> exposes port <kbd>80</kbd>:</p>
<pre><strong>$ docker container run -d --name webserver nginx:alpine</strong><br/><strong>4a37b49721b4fe6ffc57aee07c3fb42e5c08d4bcc0932e07eb7ce75fe696442d</strong><br/><br/><strong>$ docker container inspect webserver --format "{{json .NetworkSettings.Networks.bridge.IPAddress }}"</strong><br/><strong>"172.17.0.4"</strong><br/><br/><strong>$ curl http://172.17.0.4</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>&lt;title&gt;Welcome to nginx!&lt;/title&gt;</strong><br/><strong>&lt;style&gt;</strong><br/><strong>    body {</strong><br/><strong>        width: 35em;</strong><br/><strong>        margin: 0 auto;</strong><br/><strong>        font-family: Tahoma, Verdana, Arial, sans-serif;</strong><br/><strong>    }</strong><br/><strong>&lt;/style&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</strong><br/><strong>&lt;p&gt;If you see this page, the nginx web server is successfully installed and</strong><br/><strong>working. Further configuration is required.&lt;/p&gt;</strong><br/><br/><strong>&lt;p&gt;For online documentation and support please refer to</strong><br/><strong>&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</strong><br/><strong>Commercial support is available at</strong><br/><strong>&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</strong><br/><br/><strong>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</strong><br/><strong>&lt;/body&gt;</strong></pre>
<p class="mce-root">Our host IP on the default bridge network is <kbd>172.17.0.1</kbd> in this case, and we can reach container port <kbd>80</kbd>, but no other host will be able to reach this port. It is exposed internally by a <kbd>webserver</kbd> container.</p>
<p>To publish a port exposed internally, we need to declare it during container creation or execution using the <kbd>--publish</kbd> or <kbd>-p</kbd> parameters.</p>
<p>We will use <kbd>--publish [HOST_IP:][HOST_PORT:]CONTAINER_PORT[/PROTOCOL]</kbd> for this. This means that the only required argument is the container port. By default, the TCP protocol and a random port between <kbd>32768</kbd> and <kbd>65000</kbd> will be used, and the port will be publicly published on all host IP addresses (<kbd>0.0.0.0</kbd>). We can also use <kbd>-P</kbd> to publish all ports exposed in a given container's image definition.</p>
<p>If we need to declare a UDP application publication, we need to specify this protocol.</p>
<p>Host mode networking does not require any publication of ports because any exposed container process will be accessible from outside.</p>
<p>We can declare a range of ports in the form <kbd>--publish StartPort-EndPort[/PROTOCOL]</kbd> to publish more than one port.</p>
<p>For security reasons, it is important to use a specific IP address on multi-homed hosts in order to only allow access to specified IP addresses:</p>
<pre><strong>$ docker container run -d --name public-webserver --publish 80 nginx:alpine</strong><br/><strong>562bfebccd728fdc3dff649fe6ac578d52e77c409e84eed8040db3cfc5589e40</strong><br/><br/><strong>$ docker container ls --filter name=public-webserver</strong><br/><strong>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</strong><br/><strong>562bfebccd72 nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 0.0.0.0:32768-&gt;80/tcp public-webserver</strong><br/><br/><strong>$ curl http://0.0.0.0:32768</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>&lt;title&gt;Welcome to nginx!&lt;/title&gt;</strong><br/><strong>&lt;style&gt;</strong><br/><strong>    body {</strong><br/><strong>        width: 35em;</strong><br/><strong>        margin: 0 auto;</strong><br/><strong>        font-family: Tahoma, Verdana, Arial, sans-serif;</strong><br/><strong>    }</strong><br/><strong>&lt;/style&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</strong><br/><strong>&lt;p&gt;If you see this page, the nginx web server is successfully installed and</strong><br/><strong>working. Further configuration is required.&lt;/p&gt;</strong><br/><br/><strong>&lt;p&gt;For online documentation and support please refer to</strong><br/><strong>&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</strong><br/><strong>Commercial support is available at</strong><br/><strong>&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</strong><br/><br/><strong>&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<p>We will see more examples of this in the next section.</p>
<h1 id="uuid-d00d5941-1c49-4e8c-937a-01243af9ec58">Chapter labs</h1>
<p>This chapter was dedicated to learning how to manage stateful environments and the magic behind container networking. Now, let's complete some labs to review what we've learned. For these labs, we will use a CentOS Linux host with a Docker engine installed.</p>
<p>Deploy <kbd>environments/standalone-environment</kbd> from this book's GitHub repository (<a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a>) if you have not done so yet. You can use your own CentOS 7 server. Use <kbd>vagrant up</kbd> from the <kbd>environments/standalone-environment</kbd> folder to start your virtual environment.</p>
<p>If you are using a standalone environment, wait until it is running. We can check the statuses of our nodes using <kbd>vagrant status</kbd>. Connect to your lab node using <kbd>vagrant ssh standalone</kbd>. <kbd>standalone</kbd> is the name of your node. You will be using the <kbd>vagrant</kbd> user with root privileges using <kbd>sudo</kbd>. You should have the following output:</p>
<pre><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/standalone$ vagrant up</strong><br/><strong>Bringing machine 'standalone' up with 'virtualbox' provider...</strong><br/><strong>...</strong><br/><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/standalone$ vagrant status</strong><br/><strong>Current machine states:</strong><br/><strong>standalone running (virtualbox)</strong><br/><strong>...</strong><br/><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/standalone$</strong></pre>
<p class="mce-root">We can now connect to the standalone node using <kbd>vagrant ssh standalone</kbd>. This process may vary if you deployed a standalone virtual node previously and you just started it using <kbd>vagrant up</kbd>:</p>
<pre><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/standalone$ vagrant ssh standalone</strong><br/><strong>[vagrant@standalone ~]$</strong> </pre>
<p>If you are reusing your standalone environment, this means Docker Engine is installed. If you started a new instance, please execute the <kbd>/vagrant/install_requirements.sh</kbd> script so that you have all the required tools (Docker Engine and <kbd>docker-compose</kbd>):</p>
<pre><strong>[vagrant@standalone ~]$ /vagrant/install_requirements.sh</strong> </pre>
<p>Now, you are ready to start the labs.</p>
<h2 id="uuid-cfbe6c52-3fe0-4ded-b520-fb04adb9af8e">Using volumes to code on your laptop</h2>
<p>In this lab, we will run a container with our application code inside. As the application is created using an interpreted language, any change or code modification will be refreshed (we added debugging to reload the application on each change using <kbd>debug=True</kbd>):</p>
<ol>
<li>We've created a simple Python Flask application for you. The following is the content of the <kbd>app.py</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>from flask import Flask, render_template</strong><br/><br/><strong>app = Flask(__name__)</strong><br/><br/><strong>@app.route('/')</strong><br/><br/><strong>def just_run():</strong><br/><strong>    return render_template('index.html')</strong><br/><br/><strong>if __name__ == '__main__':</strong><br/><strong>    app.run(debug=True,host='0.0.0.0')</strong></pre>
<ol start="2">
<li>We only require the <kbd>Flask</kbd> Python module, so we will only have one line in our <kbd>requirements.txt</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>Flask</strong></pre>
<ol start="3">
<li>We will use a simple template HTML file under <kbd>templates/index.html</kbd> with this content:</li>
</ol>
<pre style="padding-left: 60px"><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html lang="en"&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>    &lt;meta charset="UTF-8"&gt;</strong><br/><strong>    &lt;title&gt;Simple Flask Application&lt;/title&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>    &lt;h1&gt;Simple Flask Application&lt;/h1&gt;</strong><br/><strong>    &lt;h1&gt;Version 1&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<ol start="4">
<li>We will run this application inside a container. We will create a Dockerfile and build an image called <kbd>simpleapp</kbd>, with a tag of <kbd>v1.0</kbd>. This is the content of the Dockerfile:</li>
</ol>
<pre style="padding-left: 60px"><strong>FROM python:alpine</strong><br/><strong>WORKDIR /app</strong><br/><strong>COPY ./requirements.txt requirements.txt</strong><br/><strong>RUN pip install -r requirements.txt</strong><br/><strong>COPY app.py .</strong><br/><strong>COPY templates templates </strong><br/><strong>EXPOSE 5000</strong><br/><strong>CMD ["python", "app.py"]</strong></pre>
<ol start="5">
<li>Let's build our application image (<kbd>simpleapp:v1.0</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker image build -q -t simpleapp:v1.0 .</strong><br/><strong>sha256:1cf398d39b51eb7644f98671493767267be108b60c3142b3ca9e0991b4d3e45b</strong></pre>
<ol start="6">
<li>We can run this simple application by executing a detached container exposing port <kbd>5000</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run -d --name v1.0 simpleapp:v1.0 </strong><br/><strong>1e775843a42927c25ee350af052f3d8e34c0d26f2510fb2d85697094937f574f</strong></pre>
<ol start="7">
<li>Now, we can review the container's IP address. We are running this container in a host, which means we can access the process port and defined IP address:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>[vagrant@standalone ~]$ docker container ls --filter name=v1.0</strong><br/><strong>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</strong><br/><strong>1e775843a429 simpleapp:v1.0 "python app.py" 35 seconds ago Up 33 seconds 5000/tcp v1.0</strong><br/><strong> "python app.py" 35 seconds ago Up 33 seconds 5000/tcp v1.0</strong><br/><br/><strong>[vagrant@standalone ~]$ docker container inspect v1.0 \<br/> --format "{{.NetworkSettings.Networks.bridge.IPAddress }}"</strong><br/><strong><br/>172.17.0.6</strong></pre>
<ol start="8">
<li>We can access our application as expected using the container's defined IP and port:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ curl http://172.17.0.6:5000</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html lang="en"&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>    &lt;meta charset="UTF-8"&gt;</strong><br/><strong>    &lt;title&gt;Simple Flask Application&lt;/title&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>    &lt;h1&gt;Simple Flask Application&lt;/h1&gt;</strong><br/><strong>    &lt;h1&gt;Version 1&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<ol start="9">
<li>It is simple to change <kbd>index.html</kbd> if we get into the container. The problem is that when we run a new container, changes will not be stored and <kbd>index.html</kbd> will be lost. Every time, we will get <kbd>index.html</kbd> defined in the base image. As a result, if we want changes to persist, we need to use volumes. Let's use a bind mount to change the <kbd>index.html</kbd> file while the container is running:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run -d \<br/>--name v1.0-bindmount -v $(pwd)/templates:/app/templates simpleapp:v1.0 </strong><br/><strong><br/>fbf3c35c2f11121ed4a0eedc2f47b42a5ecdc6c6ff4939eb4658ed19999f87d4</strong><br/><br/><strong>[vagrant@standalone ~]$ docker container inspect v1.0-bindmount --format "{{.NetworkSettings.Networks.bridge.IPAddress }}"</strong><br/><strong>172.17.0.6</strong><br/><br/><strong>[vagrant@standalone ~]$ curl http://172.17.0.6:5000</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html lang="en"&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>    &lt;meta charset="UTF-8"&gt;</strong><br/><strong>    &lt;title&gt;Simple Flask Application&lt;/title&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>    &lt;h1&gt;Simple Flask Application&lt;/h1&gt;</strong><br/><strong>    &lt;h1&gt;Version 1&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<ol start="10">
<li>We can now change <kbd>templates/index.html</kbd> because we have used <kbd>-v $(pwd)/templates:/app/templates</kbd>, assuming the current directory. Using the vi editor, we can modify the content of the <kbd>templates/index.html</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html lang="en"&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>    &lt;meta charset="UTF-8"&gt;</strong><br/><strong>    &lt;title&gt;Simple Flask Application&lt;/title&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>    &lt;h1&gt;Simple Flask Application&lt;/h1&gt;</strong><br/><strong>    &lt;h1&gt;Version 2&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong><br/><strong>~ </strong><br/><strong>~</strong> </pre>
<ol start="11">
<li>We change the line containing the <kbd>Version</kbd> key and we access it again using <kbd>curl</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ curl http://172.17.0.6:5000</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html lang="en"&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>    &lt;meta charset="UTF-8"&gt;</strong><br/><strong>    &lt;title&gt;Simple Flask Application&lt;/title&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>    &lt;h1&gt;Simple Flask Application&lt;/h1&gt;</strong><br/><strong>    &lt;h1&gt;Version 2&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<p>The changes are reflected because we did them on our host filesystem and it is mounted inside our container. We can also change our application code by mounting <kbd>app.py</kbd>. Depending on what programming language we are using, we can change the application code on the fly. If changes must be persistent, we need to follow a versioning strategy. We will build a new image with the required changes.</p>
<h2 id="uuid-2c5e0637-8622-4fa7-9039-892b6dd831ce">Mounting SSHFS</h2>
<p>In this lab, we will install and use the <kbd>sshfs</kbd> volume plugin:</p>
<ol>
<li>First, we need to install the <kbd>sshfs</kbd> plugin:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker plugin install vieux/sshfs</strong><br/><strong>Plugin "vieux/sshfs" is requesting the following privileges:</strong><br/><strong> - network: [host]</strong><br/><strong> - mount: [/var/lib/docker/plugins/]</strong><br/><strong> - mount: []</strong><br/><strong> - device: [/dev/fuse]</strong><br/><strong> - capabilities: [CAP_SYS_ADMIN]</strong><br/><strong>Do you grant the above permissions? [y/N] y</strong><br/><strong>latest: Pulling from vieux/sshfs</strong><br/><strong>52d435ada6a4: Download complete </strong><br/><strong>Digest: sha256:1d3c3e42c12138da5ef7873b97f7f32cf99fb6edde75fa4f0bcf9ed277855811</strong><br/><strong>Status: Downloaded newer image for vieux/sshfs:latest</strong><br/><strong>Installed plugin vieux/sshfs</strong></pre>
<ol start="2">
<li>Let's review our host IP address and start the <kbd>sshd</kbd> or <kbd>ssh</kbd> daemons (depending on your system and whether it is already running):</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ sudo systemctl status ssh</strong><br/><strong>● ssh.service - OpenBSD Secure Shell server</strong><br/><strong>   Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled)</strong><br/><strong>   Active: active (running) since Mon 2019-11-11 23:59:38 CET; 6s ago</strong><br/><strong> Main PID: 13711 (sshd)</strong><br/><strong>    Tasks: 1 (limit: 4915)</strong><br/><strong>   CGroup: /system.slice/ssh.service</strong><br/><strong>           └─13711 /usr/sbin/sshd -D</strong><br/><br/><strong>nov 11 23:59:38 sirius systemd[1]: Starting OpenBSD Secure Shell server...</strong><br/><strong>nov 11 23:59:38 sirius sshd[13711]: Server listening on 0.0.0.0 port 22.</strong><br/><strong>nov 11 23:59:38 sirius sshd[13711]: Server listening on :: port 22.</strong><br/><strong>nov 11 23:59:38 sirius systemd[1]: Started OpenBSD Secure Shell server.</strong></pre>
<ol start="3">
<li>Let's review the installed plugin:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker plugin ls</strong><br/><strong>ID NAME DESCRIPTION ENABLED</strong><br/><strong>eb37e5a2e676 vieux/sshfs:latest sshFS plugin for Docker true</strong></pre>
<p style="padding-left: 60px">Since plugins are objects, we can inspect installed plugins. We can review important aspects such as version, debug mode, or the type of mount points that will be managed with this plugin:</p>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker plugin inspect eb37e5a2e676</strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "Config": {</strong><br/><strong>..</strong><br/><strong>            "Description": "sshFS plugin for Docker",</strong><br/><strong>            "DockerVersion": "18.05.0-ce-rc1",</strong><br/><strong>            "Documentation": "https://docs.docker.com/engine/extend/plugins/",</strong><br/><strong>            "Entrypoint": [</strong><br/><strong>                "/docker-volume-sshfs"</strong><br/><strong>            ],</strong><br/><strong>...</strong><br/><strong>...</strong><br/><strong>                    "Source": "/var/lib/docker/plugins/",</strong><br/><strong>                    "Type": "bind"</strong><br/><strong>                },</strong><br/><strong>...</strong><br/><strong>...</strong><br/><strong>        "Enabled": true,</strong><br/><strong>        "Id": "eb37e5a2e676138b6560bd91715477155f669cd3c0e39ea054fd2220b70838f1",</strong><br/><strong>        "Name": "vieux/sshfs:latest",</strong><br/><strong>        "PluginReference": "docker.io/vieux/sshfs:latest",</strong><br/><strong>        "Settings": {</strong><br/><strong>            "Args": [],</strong><br/><strong>            "Devices": [</strong><br/><strong>...</strong><br/><strong>...</strong><br/><strong>]</strong></pre>
<ol start="4">
<li>Now, we will create a new volume named <kbd>sshvolume</kbd> (we assume that you have a valid SSH username and password here). Notice that we're using <kbd>127.0.0.1</kbd> and the <kbd>/tmp</kbd> directory or filesystem for demo purposes:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker volume create -d vieux/sshfs \</strong><br/><strong>-o sshcmd=ssh_user@127.0.0.1:/tmp \</strong><br/><strong>-o password=ssh_userpasswd \</strong><br/><strong>sshvolume</strong></pre>
<ol start="5">
<li>Now, we can easily run an <kbd>alpine</kbd> container by mounting previously created <kbd>sshvolume</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run --rm -it -v sshvolume:/data alpine sh</strong><br/><strong>/ # ls -lart /data</strong><br/><strong>total 92</strong><br/><strong>drwx------ 1 root root 17 Nov 9 08:27 systemd-private-809bb564862047608c79c2cc81f67f24-systemd-timesyncd.service-gQ5tZx</strong><br/><strong>drwx------ 1 root root 17 Nov 9 08:27 systemd-private-809bb564862047608c79c2cc81f67f24-systemd-resolved.service-QhsXg9</strong><br/><strong>drwxrwxrwt 1 root root 6 Nov 9 08:27 .font-unix</strong><br/><strong>drwxrwxrwt 1 root root 6 Nov 9 08:27 .XIM-unix</strong><br/><strong>drwxr-xr-x 1 root root 30 Nov 11 23:13 ..</strong><br/><strong>drwxrwxrwt 1 root root 4096 Nov 11 23:13 .</strong><br/><strong>/ #</strong> </pre>
<p class="mce-root">Let's continue with some network labs.</p>
<h2 id="uuid-95a9bfd3-46de-4d7b-827b-8319bfcc3c04">Multi-homed containers</h2>
<p>We will now look at a quick lab on attaching containers to multiple networks. Let's get started:</p>
<ol>
<li>First, we'll create two different zones, <kbd>zone-a</kbd> and <kbd>zone-b</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker network create zone-a</strong><br/><strong>bb7cb5d22c03bffdd1ef52a7469636fe2e635b031b7528a687a85ff9c7ee4141</strong><br/><br/><strong>[vagrant@standalone ~]$ docker network create zone-b</strong><br/><strong>818ba644512a2ebb44c5fd4da43c2b1165f630d4d0429073c465f0fe4baff2c7</strong></pre>
<ol start="2">
<li>Now, we can start a container named <kbd>cont1</kbd> on <kbd>zone-a</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run -d --name cont1 --network zone-a alpine sleep 3000 </strong><br/><strong>ef3dfd6a354b5310a9c97fa9247739ac320da1b4f51f6a2b8da2ca465b12f95e</strong></pre>
<ol start="3">
<li>Next, we connect the <kbd>cont1</kbd> container to <kbd>zone-b</kbd> and review its IP addresses:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker network connect zone-b cont1</strong><br/><br/><strong>$ docker exec cont1 ip add</strong><br/><strong>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000</strong><br/><strong> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</strong><br/><strong> inet 127.0.0.1/8 scope host lo</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>92: eth0@if93: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </strong><br/><strong> link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 172.19.0.2/16 brd 172.19.255.255 scope global eth0</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>94: eth1@if95: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </strong><br/><strong> link/ether 02:42:ac:14:00:02 brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 172.20.0.2/16 brd 172.20.255.255 scope global eth1</strong><br/><strong> valid_lft forever preferred_lft forever</strong></pre>
<ol start="4">
<li>Now, we can run two containers with just one interface. One of them will run attached to <kbd>zone-a</kbd>, while the other one will just be attached to <kbd>zone-b</kbd>:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>[vagrant@standalone ~]$ docker container run -d --name cont2 --network zone-b --cap-add NET_ADMIN alpine sleep 3000 </strong><br/><strong>048e362ea27b06f5077306a71cf8adc95ea9844907aec84ec09c0b991d912a33</strong><br/><br/><strong>[vagrant@standalone ~]$ docker container run -d --name cont3 --network zone-a --cap-add NET_ADMIN alpine sleep 3000 </strong><br/><strong>20c7699c54786700c65a0bbe002c750672ffb3986f41d106728b3d598065ecb5</strong></pre>
<ol start="5">
<li>Let's review the IP addresses and routes on both containers:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>[vagrant@standalone ~]$ docker exec cont2 ip route</strong><br/><strong>default via 172.20.0.1 dev eth0 </strong><br/><strong>172.20.0.0/16 dev eth0 scope link src 172.20.0.3 </strong><br/><br/><strong>[vagrant@standalone ~]$ docker exec cont3 ip route</strong><br/><strong>default via 172.19.0.1 dev eth0 </strong><br/><strong>172.19.0.0/16 dev eth0 scope link src 172.19.0.3 </strong></pre>
<ol start="6">
<li>If we want the <kbd>cont3</kbd> container to contact the <kbd>cont2</kbd> container, we should add a route through the <kbd>cont1</kbd> container, which contains both networks. In the <kbd>cont2</kbd> container, enter the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker exec cont2 route add -net 172.19.0.0 netmask 255.255.255.0 gw 172.20.0.2</strong><br/><br/><strong>[vagrant@standalone ~]$ docker exec cont2 ip route </strong><br/><strong>default via 172.20.0.1 dev eth0 </strong><br/><strong>172.19.0.0/24 via 172.20.0.2 dev eth0 </strong><br/><strong>172.20.0.0/16 dev eth0 scope link  src 172.20.0.3</strong></pre>
<p style="padding-left: 60px">In the <kbd>cont3</kbd> container, enter the following:</p>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker exec cont3 route add -net 172.20.0.0 netmask 255.255.255.0 gw 172.19.0.2</strong><br/><br/><strong>[vagrant@standalone ~]$ docker exec cont3 ip route </strong><br/><strong>default via 172.19.0.1 dev eth0 </strong><br/><strong>172.19.0.0/16 dev eth0 scope link  src 172.19.0.3 </strong><br/><strong>172.20.0.0/24 via 172.19.0.2 dev eth0</strong> </pre>
<ol start="7">
<li class="mce-root">Remember that we don't have name resolution between different networks. Therefore, we cannot reach <kbd>cont2</kbd> using its name:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker exec cont3 ping -c 3 cont2</strong><br/><strong>ping: bad address 'cont2'</strong><br/><br/><strong>[vagrant@standalone ~]$ docker exec cont3 ping -c 3 cont1</strong><br/><strong>PING cont1 (172.19.0.2): 56 data bytes</strong><br/><strong>64 bytes from 172.19.0.2: seq=0 ttl=64 time=0.063 ms</strong><br/><strong>64 bytes from 172.19.0.2: seq=1 ttl=64 time=0.226 ms</strong><br/><strong>64 bytes from 172.19.0.2: seq=2 ttl=64 time=0.239 ms</strong><br/><br/><strong>--- cont1 ping statistics ---</strong><br/><strong>3 packets transmitted, 3 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.063/0.176/0.239 ms</strong></pre>
<p style="padding-left: 60px">As we expected, name resolution within the <kbd>zone-a</kbd> network works fine. Any other container on another network will not be able to resolve containers by their names.</p>
<ol start="8">
<li>We should be able to ping from <kbd>cont3</kbd> to <kbd>cont2</kbd> using its IP address:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker exec cont3 ping -c 3 172.20.0.3</strong><br/><strong>PING 172.20.0.3 (172.20.0.3): 56 data bytes</strong><br/><strong>64 bytes from 172.20.0.3: seq=0 ttl=63 time=0.151 ms</strong><br/><strong>64 bytes from 172.20.0.3: seq=1 ttl=63 time=0.229 ms</strong><br/><strong>64 bytes from 172.20.0.3: seq=2 ttl=63 time=0.201 ms</strong><br/><br/><strong>--- 172.20.0.3 ping statistics ---</strong><br/><strong>3 packets transmitted, 3 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.151/0.193/0.229 ms</strong></pre>
<p class="mce-root">So, although we do not have name resolution, we can reach containers on other networks using a container gateway that has interfaces on all networks. For this to work, we added a route to each network container to route all other network traffic to the gateway container. We could have added aliases to reach other network containers by name. Try it yourself – it's easy!</p>
<h2 id="uuid-451d7765-c813-4d42-8526-0c5ca4751e9c">Publishing applications</h2>
<p>In this lab, we are going to deploy a simple three-layer application. In fact, it's a two-layer application with the addition of a load balancer for our lab purposes:</p>
<ol>
<li>First, we'll create a bridge network named <kbd>simplenet</kbd>, where we will attach all application components:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker network create simplenet</strong><br/><strong>b5ff93985be84095e70711dd3c403274c5ab9e8c53994a09e4fa8adda97f37f7</strong></pre>
<ol start="2">
<li>We will deploy a PostgreSQL database with <kbd>changeme</kbd> as the password for the root user. We created a simple database named <kbd>demo</kbd> with a <kbd>demo</kbd> user and a password of <kbd>d3m0</kbd> for this lab:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run -d \</strong><br/><strong>--name simpledb \</strong><br/><strong>--network simplenet \</strong><br/><strong>--env "POSTGRES_PASSWORD=changeme" \</strong><br/><strong>codegazers/simplestlab:simpledb</strong></pre>
<p style="padding-left: 60px">Notice that we have not published any port for the database.</p>
<p>Never use environment variables for secure content. There are other mechanisms to manage this kind of data. Use the secrets functionality of Docker Swarm or Kubernetes to provide security for these keys.</p>
<ol start="3">
<li>Now, we need to launch the backend application component, named <kbd>simpleapp</kbd>. Notice that in this case, we used many environment variables to configure the application side. We set the database host, database name, and the required credentials, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run -d \</strong><br/><strong> --name simpleapp \</strong><br/><strong>--network simplenet \</strong><br/><strong>--env dbhost=simpledb \</strong><br/><strong>--env dbname=demo \</strong><br/><strong>--env dbuser=demo \</strong><br/><strong>--env dbpasswd=d3m0 \</strong><br/><strong>codegazers/simplestlab:simpleapp</strong><br/><strong>556d6301740c1f3de20c9ff2f30095cf4a49b099190ac03189cff3db5b6e02ce</strong></pre>
<p style="padding-left: 60px">We have not published the application. Therefore, it is only accessible locally.</p>
<ol start="4">
<li>Let's review the application component IP addresses deployed. We will inspect the containers attached to <kbd>simplenet</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker network inspect simplenet --format "{{range .Containers}} {{.IPv4Address }} {{.Name}} {{end}}"</strong><br/><strong> 172.22.0.4/16 simpleapp 172.22.0.3/16 simpledb</strong></pre>
<ol start="5">
<li>If we take a look at the exposed (not published) ports on each image definition, we will observe the following in the database component:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker inspect codegazers/simplestlab:simpledb \<br/>--format "{{json .Config.ExposedPorts }}"<br/></strong><br/><strong>{"5432/tcp":{}}</strong></pre>
<p style="padding-left: 60px">In the application backend, we will observe the following:</p>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker inspect codegazers/simplestlab:simpleapp \<br/>--format "{{json .Config.ExposedPorts }}" </strong><br/><strong><br/>{"3000/tcp":{}} </strong></pre>
<ol start="6">
<li>Now, we have all the required information to test the connections to both components. We can even use the <kbd>curl</kbd> command to test whether the server is a database server. Let's try the database with an IP address of <kbd>172.22.0.3</kbd> on port <kbd>5432</kbd>. We will use <kbd>curl -I</kbd> because we don't really care about the response content. We just want to be able to connect to the exposed port:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ curl -I 172.22.0.3:5432</strong><br/><strong>curl: (52) Empty reply from server</strong></pre>
<p style="padding-left: 60px">In this case, <kbd>Empty reply from server</kbd> is <kbd>OK</kbd> (it does not use the HTTP protocol). The database is listening on that IP-port combination. The same will happen on the application backend on IP address <kbd>172.22.0.4</kbd> and port <kbd>3000</kbd>:</p>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ curl -I 172.22.0.4:3000</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Content-Type: text/html; charset=UTF-8</strong><br/><strong>Date: Sat, 16 Nov 2019 11:38:22 GMT</strong><br/><strong>Connection: keep-alive</strong></pre>
<p style="padding-left: 60px">In this situation, we will be able to open <kbd>http://172.22.0.4:3000</kbd> in the browser. The application will be visible, but it can only be consumed locally. It hasn't been published yet.</p>
<ol start="7">
<li>Let's deploy the load balancer component. This component will publish a port on our host. Notice that we added two environment variables to allow the load balancer to connect to the backend application (we configured the load balancer on the fly with these variables because this image is modified for this behavior):</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ docker container run -d \</strong><br/><strong>--name simplelb \</strong><br/><strong>--env APPLICATION_ALIAS=simpleapp \</strong><br/><strong>--env APPLICATION_PORT=3000 \</strong><br/><strong>--network simplenet \</strong><br/><strong>--publish 8080:80 \</strong><br/><strong>codegazers/simplestlab:simplelb</strong><br/><strong>35882fb4648098f7c1a1d29a0a12f4668f46213492e269b6b8262efd3191582b</strong></pre>
<ol start="8">
<li>Let's take a look at our local <kbd>iptables</kbd>. The Docker daemon has added a NAT rule to guide traffic from port <kbd>8080</kbd> to port <kbd>80</kbd> on the load balancer component:</li>
</ol>
<pre style="padding-left: 60px"><strong>[vagrant@standalone ~]$ sudo iptables -L DOCKER -t nat --line-numbers --numeric</strong><br/><strong>Chain DOCKER (2 references)</strong><br/><strong>num target prot opt source destination </strong><br/><strong>1 RETURN all -- 0.0.0.0/0 0.0.0.0/0 </strong><br/><strong>2 RETURN all -- 0.0.0.0/0 0.0.0.0/0 </strong><br/><strong>3 RETURN all -- 0.0.0.0/0 0.0.0.0/0 </strong><br/><strong>4 RETURN all -- 0.0.0.0/0 0.0.0.0/0 </strong><br/><strong>5 RETURN all -- 0.0.0.0/0 0.0.0.0/0 </strong><br/><strong>6 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.22.0.2:80</strong></pre>
<p>Notice that the load balancer will be available on all host IP addresses because we have not set any specific IP in the publish option. </p>
<ol start="9">
<li>Now, open <kbd>http://localhost:8080</kbd> in your web browser. You will be able to consume the deployed application. You will see the following GUI in your browser:</li>
</ol>
<div><img src="img/9f8bdd0c-3c63-488a-9b32-bad2d56a8754.png" style=""/></div>
<p>This GUI is, in fact, the application backend's front page. As we mentioned previously, it is not a real three-layer application. We added a load balancer as a frontend just to be able to publish it and add some rules there.</p>
<p>To ensure that the application only listens on the required interfaces, we can specify them to avoid unsecured ones. Always use a specific IP address with the <kbd>--publish</kbd> option (for example, <kbd>--listen MY_PUBLIC_IP_ONLY:8080:80</kbd>) to publish your application on a defined IP address.</p>
<p>In this lab, we published a simple application and ensured that only specific components are visible externally. Remember that it is possible to use container gateways and internal-only networks. These features will improve application security.</p>
<h1 id="uuid-5d707ed2-68a6-49a9-9a7c-27f17984fc30" class="mce-root">Summary</h1>
<p>Throughout this chapter, we have reviewed how to manage data associated with containers. We took a look at different strategies to manage the data of processes and their statuses. We used host filesystems and unnamed and named volumes, and we learned how to extend the available Docker daemon volume management functionality by using plugins. We noticed that the Docker daemon will not take care of any application lock or even determine how storage resources are defined at the host level.</p>
<p>There are two different options for mounting volumes or bind mounts on containers using <kbd>--volume</kbd> or <kbd>--mount</kbd>. We also reviewed all the parameters required and the differences between them.</p>
<p>We talked about how to manage data and process states in high-availability environments. We haven't introduced any orchestration concepts yet, but it is important to understand that high availability or multiple instances of a process will require special application logic. Docker will not manage that logic and this is something you must be aware of.</p>
<p>We also introduced some basic networking concepts. We explained the different types of networks we can use out of the box on the Docker daemon and the special features of each one. We then reviewed the interactions between containers and how they can talk to external networks. Lastly, we finished this chapter by learning how to publish application processes running inside containers.</p>
<p>The next chapter will introduce you to how to run applications on multiple containers. We will learn how an application's components run and interact.</p>
<h1 id="uuid-67896c67-5ddc-47f9-b9b6-5e69d89c9771" class="mce-root">Questions</h1>
<p>In this chapter, we reviewed container persistency and networking in non-cluster environments. Let's verify our understanding of these topics with some questions:</p>
<ol>
<li>Which of the following statements is not true?</li>
</ol>
<p style="padding-left: 90px">a) Containers are not ephemeral – once created, they will stay in the host unless they are removed.<br/>
b) We can run more than one container at a time using the same image.<br/>
c) Containers created from the same image share their filesystems.<br/>
d) All of these statements are false.</p>
<ol start="2">
<li>Which methods are allowed when creating a volume?</li>
</ol>
<p style="padding-left: 90px">a) We can manually create a volume using the <kbd>docker volume create</kbd> command for volume objects.<br/>
b) We can declare a <kbd>VOLUME</kbd> sentence in a Dockerfile to use a volume on containers created from a built image.<br/>
c) We can use Docker host filesystems inside containers as if they were Docker volumes.<br/>
d) Volume creation is only allowed in terms of container creation or execution.</p>
<ol start="3">
<li>When we remove a container, all associated volumes will be removed. Is this true?</li>
</ol>
<p style="padding-left: 90px">a) This is false. You need to use the <kbd>--force</kbd> or <kbd>-f</kbd> option on container removal.<br/>
b) This is false. You need to use the <kbd>--volumes</kbd> or <kbd>-v</kbd> options on container removal.<br/>
c) This is false. You need to use the <kbd>--volumes</kbd> or <kbd>-v</kbd> options on container removal, and only unnamed volumes are removed.<br/>
d) This is false. Volumes can only be removed manually using <kbd>docker volume rm</kbd> or <kbd>docker volume purge</kbd>.</p>
<ol start="4">
<li>Which of the following statements is not true regarding container networking?</li>
</ol>
<p style="padding-left: 90px">a) By default, all exposed container ports are accessible from the Docker host`<br/>
b) <kbd>docker network prune</kbd> will remove all unused networks`<br/>
c) By default, all bridge networks are attachable on the fly`<br/>
d) Docker provides an internal DNS for each custom bridge network`</p>
<ol start="5">
<li>Which of the following statements is true regarding a container publishing an Nginx web server with port <kbd>80</kbd> exposed?</li>
</ol>
<p style="padding-left: 90px">a) If we use the host driver, we need to run this container with <kbd>NET_ADMIN</kbd> capabilities.<br/>
b) If we use the <kbd>--publish-all</kbd> or <kbd>-P</kbd> options, a random port between <kbd>32768</kbd> and <kbd>65535</kbd> will be associated at the host level with each container port exposed. You need to add a NAT rule in <kbd>iptables</kbd> to allow requests to reach the container's internal port <kbd>80</kbd>.<br/>
c) Using <kbd>--publish 192.168.2.100:1080:80</kbd>, we will ensure that only requests to the host IP address <kbd>192.168.2.100</kbd> on port <kbd>1080</kbd> will be redirected to the internal web server container port. (We are assuming that IP address <kbd>192.168.2.100</kbd> is a host interface.)<br/>
d) If we use <kbd>--publish 80</kbd> or <kbd>-p 80</kbd>, a random port between <kbd>32768</kbd> and <kbd>65535</kbd> will be associated at the host level with port <kbd>80</kbd>, and a NAT rule will be added to <kbd>iptables</kbd>.</p>
<h1 id="uuid-c39f8475-8e59-441b-a07e-7aa9d5f1e33f">Further reading</h1>
<p>The following links will help you learn more about volumes and networking concepts:</p>
<ul>
<li>Using storage volumes: <a href="https://docs.docker.com/storage/volumes/">https://docs.docker.com/storage/volumes/</a></li>
<li>Volume plugins: <a href="https://docs.docker.com/engine/extend/legacy_plugins/">https://docs.docker.com/engine/extend/legacy_plugins/</a><a href="https://docs.docker.com/engine/extend/legacy_plugins/"/></li>
<li>Networking overview: <a href="https://docs.docker.com/network/">https://docs.docker.com/network/</a></li>
</ul>


            

            
        
    </body></html>