<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Big Data Hadoop Ecosystems</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have discussed the key concepts of <strong><span>big data technologies</span></strong> in the preceding chapters. In this chapter, we will cover the building of big data clusters, frameworks, key components, and the architecture of popular vendors. <span>We will discuss big data DevOps concepts in successive chapters.</span></p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Big data Hadoop ecosystems</li>
<li>Big data clusters
<ul>
<li>Types and application</li>
<li>High availability</li>
<li>Load balancing</li>
</ul>
</li>
<li>Big data nodes
<ul>
<li>Master, worker, edge nodes</li>
<li>Their roles</li>
</ul>
</li>
<li>Hadoop frameworks
<ul>
<li>Cloudera CDH Hadoop distribution</li>
<li><strong>Hortonworks Data Platform</strong> (<strong>HDP</strong>)</li>
<li>MapR Hadoop distribution</li>
<li>Pivotal big data suite</li>
<li>IBM open platform</li>
<li>Cloud-based Hadoop distribution</li>
<li>Amazon Elastic MapReduce</li>
<li>Microsoft Azure's HDInsight</li>
</ul>
</li>
<li>Capacity planning
<ul>
<li>Factors</li>
<li>Guidelines</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Big data Hadoop ecosystems</h1>
                </header>
            
            <article>
                
<p>Apache Hadoop is an open source software platform built from commodity hardware and used to scale clusters up to terabytes or petabytes for big data, spanning across thousands of servers. It is highly popular and efficient for distributed data storage and distributed processing of very large datasets. Hadoop offers a full scale of services such as data persistence, data processing, data access, data governance, data security, and operations. A few of the benefits associated with Hadoop clusters are listed as follows:</p>
<ul>
<li><strong>Data scalability</strong>:<strong> </strong>Big data volumes can grow exponentially to accommodate these large data volumes, Hadoop enables distributed processing of data; each node in the data cluster participates in storing, managing, processing, and analyzing data. The addition of nodes enables quick scaling of clusters to store data at a scale of petabytes.</li>
<li><strong>Data reliability</strong>: Hadoop cluster configurations provide data redundancy. For example, in case of accidental failure of one or more nodes, Hadoop cluster management software will automatically replicate the data and processing to the rest of the active nodes. Thus business continuity is assured with the application and data functional, even during the non-availability of a few nodes.</li>
<li><strong>Data</strong> <strong>flexibility</strong>: In traditional relational database management systems, schema tables are created before storing structured data into the system commonly referred to as <em>schema on write</em>. Based on the processing application data requirements, diverse data formats can be loaded into Hadoop systems such as structured, semi-structured, or unstructured data. Hence, the schema is dynamically created during the data load, and referred to as <em>schema on read</em>.</li>
<li><strong>Economical</strong>:<strong> </strong>Hadoop is open source and built on low-cost commodity hardware, and hence, is more economical than proprietary licensed software.</li>
</ul>
<p>Organizations adopt the Hadoop system for its versatility and its ability to persist on large data volumes, managing, visualizing and analyzing vast amounts of data quickly, reliably, efficiently, and with a low cost for a variety of data formats, with data governance, workflow, security, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inbuilt tools and capabilities in Hadoop ecosystem</h1>
                </header>
            
            <article>
                
<p>Hadoop ecosystem offers many inbuilt tools, features, and capabilities, listed as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-546 image-border" height="266" src="assets/160dc7ec-fd30-448f-a257-4579441c6e09.png" width="524"/></div>
<ul>
<li><strong>Data storage</strong>: The <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) provides scalable, fault-tolerant, cost-efficient storage. Hadoop can handle exponential data growth by distributing storage across many nodes; the combined storage capacity can grow with demand while remaining economical per unit of storage. There are other storage managers, such as HBase, Solr, and so on.</li>
<li><strong>Data lake</strong>: One of the key strengths of Hadoop is its ability to build a data lake economically. It will be a valuable asset for an organization to store all their relevant data needs, gathered and consolidated from various data sources. For example, in the manufacturing industry the machine maintenance data, inventory data, sales data, machine sensor data on performance, social media data on customer feedback, vendors and suppliers data, weather reports, and so on can be captured regularly as per the requirements of the data lake.</li>
<li><strong>Data processing</strong>:
<ul>
<li><strong>Hadoop ecosystem</strong>: <span>Hadoop ecosystem</span> offers data processing in batch, stream, and hybrid systems.</li>
<li><strong>MapReduce</strong>: <span>MapReduce</span> is an initial processing framework for batch jobs in Hadoop. MapReduce's processing technique follows the map, shuffle and reduce algorithm using key-value pairs. Batch jobs are like monthly telephone invoices for customers.</li>
<li><strong>Stream processing</strong>: Like stock price information and airline reservation data, Apache storm is well suited for processing data in streams.</li>
<li><strong>Hybrid processing systems</strong><span>: These processing frameworks can handle both batch and stream workloads, such as Apache Spark, Flink, and so on. One use case is the <strong>Internet of Things</strong> (<strong>IoT</strong>), truck sensor data capture, aggregation in the cloud and analytics to derive patterns, and so on.</span></li>
</ul>
</li>
<li><strong>Data access</strong>:
<ul>
<li><strong>Hadoop</strong>: <span>Hadoop </span>offers multiple ways of accessing and processing the data.</li>
<li><strong>Apache Solr</strong><span>:</span> <span>Apache Solr</span><span> provides indexing and search capabilities for the data stored in HDFS.</span></li>
<li><strong>Hive</strong><span>:</span> <span>Hive</span><span> provides data warehouse functionality for Hadoop with a simple SQL-like language called</span> <strong>HiveQL</strong> <span>that provides indexes, making querying faster. A standard SQL programming interface can be used and provides better integration with a few analytics packages such as Tableau,</span> QlikView<span>, and so on.</span></li>
<li><strong>HBase</strong><span>: A NoSQL columnar database that provides capabilities such as the columnar data storage model and storage for sparse data to Hadoop systems.</span></li>
<li><strong>Flume</strong><span>: Flume collects data from the source systems like a web server log data from Flume</span> <strong>agents</strong><span> which it then aggregates and moves into Hadoop.</span></li>
<li><strong>Mahout</strong><span>: Mahout is a machine learning library with a</span> collection <span>of key algorithms for clustering,</span> classification<span>, and collaborative filtering.  These algorithms can be implemented from any processing framework or engines such as</span> MapReduce, and<span> are more efficient for</span> in-memory <span><span>data mining frameworks such as Spark.</span></span></li>
<li><strong>Sqoop</strong><span>: Sqoop is a valuable tool for transitioning data from other database systems (mainly relational databases) into Hadoop.</span></li>
<li><strong>Pig</strong><span>: Pig Latin is a Hadoop-based language that is adept at very deep, very long data pipelines (a limitation of SQL). It is relatively simple and easier to use than SQL.</span></li>
</ul>
</li>
<li><strong>Resource management</strong>:<strong> </strong>YARN is a great enabler of dynamic resource utilization, an integral part of the Hadoop framework. It manages the increasing workloads of multi-tenant users, running various Hadoop applications without performance impact.</li>
<li><strong>Unified administration</strong>: Ambari is a RESTful API that provides a user-friendly web interface for Hadoop administration. It’s a tool for Apache Hadoop clusters' provisioning, managing, and monitoring. Provisioning tasks for a Hadoop cluster include installing Hadoop services across multiple hosts and configuring Hadoop services for the cluster. Ambari manages Hadoop cluster services such as starting, stopping, and reconfiguring Hadoop services across the entire cluster through the central management console. Ambari monitors a Hadoop cluster with a dashboard for monitoring the health and status of the Hadoop cluster, and integrates with the Ambari Metrics System for metrics collection and the Ambari alert framework. This alerting system will notify if a node goes down, or disk utilization is higher than a threshold, and so on.</li>
<li><strong>Workflow management</strong><span>:</span>
<ul>
<li><strong>Oozie</strong>: A workflow processing system that manages and schedules a series of jobs. Jobs can be written in multiple languages such as MapReduce, Pig, and Hive, and linked logically to one another. Oozie allows scheduling dependent jobs as an output of one query to be completed to feed data into the next job as input.  </li>
<li><strong>ZooKeeper</strong><span>:</span> <span>ZooKeeper</span><span> is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications.</span></li>
</ul>
</li>
</ul>
<ul>
<li><strong>Comprehensive data security and governance:</strong>
<ul>
<li>Security in Hadoop has three modes of implementation--authentication, authorization, and encryption.</li>
<li>Authentication ensures only genuine users have access to the Hadoop cluster. Currently, the tools used are MIT Kerberos, AD, OpenLDAP, and so on.</li>
<li>Authorization grants users data privileges such as read-only, write, modify, delete, and so on. The currently used tool is Apache Sentry.</li>
<li>Data encryption ensures data protection from unauthorized access to data, both at rest and in transit. The encryption tool for data at rest is Navigator Encrypt and the tools for data in transit can be implemented by enabling TLS/SSL.</li>
<li>Access administration of Hadoop systems can be a challenging task in distributed environments that host the individual components of Hadoop on different clusters for optimal performance. For example, in a large production environment, there will be different cluster groups responsible for workflow, for data storage, data analytics, and so on; so managing the respective group access privileges could be a daunting task.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Big data clusters</h1>
                </header>
            
            <article>
                
<p>A Hadoop cluster is a system comprising two or more computers or systems (called nodes). It represents a single unified system for the users. The nodes work together to execute applications or perform other tasks like a virtual machine. There are variants of Hadoop clusters that cater for different data needs. The key features in the construction of these platforms are reliability, load balancing, and performance.</p>
<p>The single node or pseudo-distributed cluster has the essential daemons such as NameNode, DataNode, JobTracker, and TaskTracker, all run on the same machine. A single node cluster is a simple configuration system used to test Hadoop applications by simulating a full cluster-like environment with a replication factor of 1.</p>
<p>A small Hadoop cluster comprises a single master and multiple worker nodes. The master node is comprised of a Job Tracker, Task Tracker, NameNode, and DataNode. A slave or worker node performs the roles of both a DataNode and TaskTracker if required; data-only worker nodes and compute-only worker nodes can be configured. Such nodes are used for full stack development of Hadoop application and projects with a replication factor of 3, such as a multi-node cluster for effective backup.</p>
<p>Multi-node or fully distributed clusters follow the master-slave architecture pattern of Hadoop cluster. The NameNode and TaskTracker daemon runs on the master machine and the DataNode and TaskTracker daemon runs on one or more slave machines. It is deployed for full stack production deployment of the Hadoop application and for projects with a replication factor of 3.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop cluster attributes</h1>
                </header>
            
            <article>
                
<p>In this section, we will discuss the key attributes of Hadoop clusters, such as load balancing for high availability and distributed processing, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High availability</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>High availability</strong> (<strong>HA</strong>) clusters provide services and resources in an uninterrupted manner through the redundancy built to the system. High availability is to be accomplished both within the cluster and between clusters too.</p>
<p class="mce-root">High availability within a cluster is accomplished by a master node that monitors the worker nodes for any failure and ensures that the load is distributed to other active, working nodes.</p>
<p class="mce-root">High availability between cluster examples across regions is accomplished by having each cluster system monitor the others, and in the event of failover, replicating servers and services through redundant hardware and software reconfiguration. Fault tolerance for hardware is achieved with raid systems, and for network systems, in the event of link breakdown alternative link paths are provided for continuity of services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load balancing</h1>
                </header>
            
            <article>
                
<p>Load balancing among servers is a key valuable functionality in the increasing and explosive use of network and internet-based applications. It is an important feature that distributes incoming traffic requests from clients evenly across all the active node machines of the cluster that are allocated to the application. In case of a node failure, the requests are redistributed among the rest of the active nodes available and responsible for processing the orders. The web cluster, to be scalable, must ensure that each server is fully utilized, providing increased network capacity and improving performance. Web application servers based on load balancing are called <strong>web farms</strong>, and redirect requests independently as they arrive, based on a scheduler and an algorithm. A few popular algorithms for load balancing are least connections, round robin, and weighted fair; each have unique applicability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High availability and load balancing</h1>
                </header>
            
            <article>
                
<p>High availability and load balancing combines the features of both types of clusters, increasing the availability and scalability of services and resources. Consistent HA and load balancing is the backbone of the entire web hosting and e-commerce project; it needs to ensure support of the scalability of traffic volume on networks, without eventually becoming a bottleneck or single point of failure. Apart from simply redirecting client traffic to other servers, the web systems need to have verification of servers, redundancy, and balancing characteristics such as full-time communication checks. This type of cluster configuration is widely used in airline and train reservation systems, e-commerce, banking, email servers, and 24/7 applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributed processing and parallel processing</h1>
                </header>
            
            <article>
                
<p>Distributed processing involves dividing a large computational task into smaller tasks, for processing to run in parallel on individual smaller clusters of nodes. It represents a massively parallel supercomputer. This model of cluster is effective for application in large computational tasks, and improves the availability and performance. These cluster systems are used for scientific research-based computing, weather forecasts, and so on; tasks that require high-processing power.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Usage of Hadoop big data cluster</h1>
                </header>
            
            <article>
                
<p>Big data clusters are built for varied purposes, such as storage, analytics, testing, development, and so on. It is imperative to have the right size of the cluster for the right kind of workload, so capacity planning for a cluster is an important and critical task.</p>
<p> Big data clusters catering to different purposes are as follows:</p>
<ul>
<li><strong>Development cluster</strong>: There are many requirements for building a development platform, such as the technological validation of porting an application to develop functionality on a big data platform, so as to develop advanced analytics use cases with machine learning.</li>
<li><strong>Test cluster</strong>: A test platform is built to test the features and functionality developed in the development cluster.</li>
<li><strong>Data lake cluster</strong>: A build used to provide extensive storage capacity, data from different source systems, including third-party data sources, are gathered into the data lake. There will be preprocessing activities to filter and perform aggregations on the incoming data before it is loaded into the data lake. A data lake serves multipurpose data needs for different departments of an organization.</li>
<li><strong>Analytical cluster</strong>: A platform for performing advance analytics using appropriate algorithms, and publishing the generated results.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop big data cluster nodes</h1>
                </header>
            
            <article>
                
<p>We will discuss the different types of nodes along with their role and usage in Hadoop Ecosystem:</p>
<ul>
<li><strong>NameNode</strong>: The NameNode is an important part of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks across where  the cluster data files are stored. The data for these files is not stored at all. Client applications communicate with NameNode whenever there is a need to locate a file, or when they want to modify a file. The modifications are stored by NameNode as a log that is appended to a native file system file edits. When a NameNode starts up, it reads the HDFS state from an image file, fsimage, and then applies the edits to the log file.</li>
<li><strong>Secondary NameNode</strong>: Secondary NameNode's whole purpose is to have a checkpoint in HDFS. The Secondary NameNode is just a helper node for NameNode; it merges the fsimage and the edits log files periodically and keeps edits log size within a limit.</li>
<li><strong>DataNode</strong>: A DataNode stores data in HDFS. A functional file system has more than one DataNode, with data replicated across them. Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.</li>
<li><strong>Edge/Hop Node</strong>: Edge nodes are the interface between the Hadoop cluster and the outside network. Most commonly, edge nodes are used to run client applications and cluster administration tools. They're also often used as staging areas for data being transferred into the Hadoop cluster.</li>
<li><strong>Cluster management</strong>: Cluster management software applications provide end-to-end functionality and features for managing cluster landscapes. It facilitates the improvement of performance, enhances the quality of service, increases compliance and reduces administrative costs.</li>
<li><strong>Security</strong> for the HDFS file level and node level is depicted as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-547 image-border" height="339" src="assets/f49a9d39-e82a-4b2c-b762-5b600c511c7b.png" width="411"/></div>
<p>Typically, the edge node is connected to the outside world through an external switch, that enables third-party systems to access through Kafka or STB-based, and SSH access for inbound users.</p>
<p>The preceding entire mentioned cluster servers interact with each other by a dedicated network switch, isolating traffic from the outside world.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of nodes and their roles</h1>
                </header>
            
            <article>
                
<div class="CDPAlignCenter CDPAlign">
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-548 image-border" height="314" src="assets/b30dd897-1b90-4288-9038-aca34bdddafd.png" width="479"/></div>
</div>
<ul>
<li><strong>Nodes</strong>:<span> </span>A Hadoop cluster can have a different configuration of servers based on the role they fulfill. This can be broadly divided into three types, with a different hardware configuration for each type:
<ul>
<li><strong>Master</strong><span> <strong>n</strong></span><strong>ode</strong><span> (also referred to as <strong>n</strong></span><strong>ame node</strong><span>): In an enterprise deployment, this runs crucial management services. These nodes only store metadata so do not need a lot of storage, but since these files are critical and important, master node services include as follows: </span>
<ul>
<li>Enterprise manager</li>
<li>Resource manager</li>
<li>Standby resource manager</li>
<li>NameNode</li>
<li>Standby NameNode</li>
<li>Journal nodes</li>
<li>HBase master</li>
<li>Hive server</li>
<li>Sqoop server</li>
<li>ZooKeeper</li>
<li>Oozie server</li>
<li>Spark/Job history server</li>
<li>Cloudera search</li>
<li>Cloudera Navigator</li>
<li>Hive metastore</li>
<li>Kafka master</li>
<li>Flume master</li>
</ul>
</li>
<li><strong>Worker node</strong>:<span> </span>Worker/slave nodes in a Cloudera enterprise deployment are the ones that run worker services. Since tasks are performed by these nodes along with the storing of actual data, they are designed to be fault tolerant. Worker nodes can have the following roles: 
<ul>
<li>Data node</li>
<li>Node manager</li>
<li>HBase region server</li>
<li>Impala daemons</li>
<li>Solr servers</li>
<li>Kafka broker</li>
<li>Flume agent</li>
</ul>
</li>
<li><strong>Gateway/edge node:</strong><span> These are</span> where Hadoop client services run, and include: 
<ul>
<li>Third-party tools</li>
<li>Hadoop command-line clients</li>
<li>Beeline</li>
<li>Impala shell</li>
<li>Flume agents</li>
<li>Hue server</li>
<li>Spark and other gateway services</li>
<li>HA proxy</li>
</ul>
</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Commercial Hadoop distributions</h1>
                </header>
            
            <article>
                
<p>As we have seen in the previous section, Hadoop is open stack community distribution with an integration of multiple components or interface layers. Many commercial vendors have built on the basic Hadoop platform and customized it to offer in the market, both as a hardware product platform and a service. We will discuss a few of the popular options as follows:</p>
<ul>
<li>Cloudera CDH Hadoop distribution</li>
<li><strong>Hortonworks Data Platform</strong> (<strong>HDP</strong>)</li>
<li>MapR Hadoop distribution</li>
<li>Amazon Elastic MapReduce</li>
<li>IBM open platform</li>
<li>Microsoft Azure's HDInsight--cloud-based Hadoop distribution</li>
<li>Pivotal big data suite</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop Cloudera enterprise distribution</h1>
                </header>
            
            <article>
                
<p>The standard framework of Hadoop open source, with different layers and components, is presented as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-549 image-border" height="292" src="assets/ea1a7098-e85e-4626-ac75-887eee989597.png" width="559"/></div>
<div>
<p>The Cloudera proprietary distribution framework is built on the Hadoop open source code, customizing the services as shown in the following topics. We will discuss the various components that make it a leading enterprise product.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data integration services</h1>
                </header>
            
            <article>
                
<p>Data from external source systems ingests into Hadoop systems can be through multiple modes based on the business need.</p>
<ul>
<li><strong>Batch transfer</strong>:<strong> </strong>
<ul>
<li><strong>Apache Sqoop</strong>: Sqoop is a command-line interface application for transferring data between relational databases and Hadoop. It supports the saved jobs that can be run multiple times helping us to import updates made to a database since the last import. It also supports the incremental loads of a single table or a free-form SQL query.</li>
</ul>
</li>
<li><strong>Real-time data transfer</strong><span>:</span>
<ul>
<li><strong>Apache Kafka</strong>: Kafka is an open source message broker project developed by the Apache Software Foundation, written in Scala. The project aims to provide a complete, high-throughput, low-latency platform for handling real-time data feeds. It’s a highly scalable pub/sub message queue architected as a distributed transaction log, making it very important for enterprise infrastructures to process streaming data.</li>
<li><strong>Apache Flume</strong>: Flume adopts a simple, flexible and distributed architecture for streaming data. It effectively ingests large amounts of log data reliably and aggregates it. It has a simple, extensible data model, flexible for building and supporting online analytical applications. Flume is a pretty robust, fault tolerant, reliable service with built-in failover and recovery features.</li>
<li><strong>Apache Chukwa</strong><span>: Chukwa is a framework for data collection and analysis on</span> distributed f<span><span>ile systems such as Hadoop, simplifying log analysis, processing and monitoring. Chukwa agents run on respective machines to collect the logs generated from various applications. It offers a high degree of flexibility to ingest the huge log data generated by servers. Collectors receive the data from the agent and write them to HDFS, which serves as storage, and the MapReduce framework will process, analyze, and parse the jobs and archive the huge log data.</span></span></li>
<li><strong>Apache Avro</strong>: Avro is a language-neutral <span>remote procedure call and data serialization framework developed within Apache's Hadoop project. Since Hadoop writable classes lack language portability, Avro uses JSON for defining data types and</span> protocols <span>and serializes data in a compact binary format. Avro is quite helpful for dealing with data formats that can be processed by multiple languages such as Java, C, C++, C#, Python, and Ruby.</span></li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop data storage</h1>
                </header>
            
            <article>
                
<p>Data storage in Hadoop is a key function and we will discuss various modes of accomplishing this:</p>
<ul>
<li><strong>Apache HDFS (Filesystem)</strong>: The HDFS is a distributed, scalable, and portable file system written in Java for the Hadoop framework. HDFS stores large files--typically in the range of gigabytes to petabytes--across multiple machines and data nodes. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high.</li>
<li><strong>Apache HBase (NoSQL)</strong>: HBase is an open source, non-relational, distributed data store running atop of HDFS providing a fault-tolerant means of storing large amount of sparse data. HBase is a column-oriented key-value data store and has been admired greatly due to its lineage with Hadoop and HDFS. It is suitable for faster read and write operations on extensive datasets with high throughput and low input/output latency.</li>
<li><strong>Apache Kudu (Relational)</strong>: Apache Kudu is an open source storage engine intended for structured data that supports low-latency random access, together with efficient analytical access patterns. It bridges the gap between HDFS and the HBase NoSQL database. Kudu tables look like those in SQL relational databases, to act as a storage system for structured data. Like RDBMS principles, primary keys are made up of one or more columns that enforce uniqueness and act as an index for efficient updates and deletes, as a storage system for tables of structured data.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data access services</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Apache Spark</strong>: Spark is an open source framework for machine learning and stream processing based on in-memory technology for data processing. It provides programmers with a data structure called the resilient distributed dataset (RDD), an application programming interface. The RDD is read-only, distributing multiple sets of data items over a cluster of machines with fault tolerant features.</li>
<li><strong>Apache Hive</strong>: The Hive data warehouse software facilitates reading, writing, and managing large datasets that reside in distributed storage using SQL. The structure can be projected onto data already in storage. A command-line tool and JDBC driver are provided to connect users to Hive.</li>
<li><strong>Impala</strong>: Impala is Cloudera's SQL query engine for data stored in an Apache Hadoop cluster and running open source massively parallel processing (MPP). Impala enables users to run low latency SQL queries on data stored in HDFS and Apache HBase, without requiring additional data movement or transformation.</li>
<li><strong>Solr</strong>: Apache Solr is a search platform for websites, popular for enterprise search because it can be used to index and search documents and email attachments. It's built upon a Java library called <strong>Lucene</strong> (<a href="http://whatis.techtarget.com/definition/Apache-Lucene">http://whatis.techtarget.com/definition/Apache-Lucene</a>), written in Java, and provides both a RESTful XML interface and a JSON API that are used to build search applications. Solr can search and index multiple websites, returning content related recommendations based on the taxonomy (<a href="http://searchcontentmanagement.techtarget.com/definition/taxonomy">http://searchcontentmanagement.techtarget.com/definition/taxonomy</a>) of the search query (<a href="http://searchsqlserver.techtarget.com/definition/query">http://searchsqlserver.techtarget.com/definition/query</a>).</li>
<li><strong>Apache Pig</strong>: Pig provides a high-level language known as Pig Latin, a SQL-like language with many built-in operators for performing data operations such as joins, filters, ordering, and so on, and is used to perform all the data manipulation operations in Hadoop. The component of Apache Pig is Pig Engine that ingests the Pig Latin scripts as input and converts the scripts into MapReduce jobs. As a tool, it's very efficient, reducing development and coding time. As a platform, it adopts multiple query paths, representing them as data flows to analyze large sets of data. </li>
<li><strong>Kite</strong> is a high-level data layer for Hadoop, that provides an API and a set of tools to create logical abstractions on top of storage systems (such as HDFS) and operates in terms of records, datasets, and dataset repositories. It can access Mavin through plug-in and aid-in packaging, deploying and running distributed applications. It speeds up the development of stream processing ETL applications in Hadoop that extract, transform, and load data into target repositories such as Apache Solr, enterprise data warehouses, HDFS, HBase, and OLAP applications.</li>
<li><strong>MapReduce</strong> is a processing technique and framework based on Java for distributed computing. As the sequence of the name MapReduce implies, the reduce task is always performed after the map job. A map job usually splits the input dataset into independent chunks where individual elements are broken down into tuples (key/value pairs). The reduce framework sorts the outputs of the map jobs, which are then input to the reduce tasks. The tasks are processed in a completely parallel manner to scale data processing over multiple computing nodes. The input jobs and the output jobs are stored in a file system. The framework takes care of scheduling tasks, monitoring them and re-executing failed tasks. The MapReduce model framework can easily scale the application to run over tens of thousands of machines in a cluster through a configuration change.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Database</h1>
                </header>
            
            <article>
                
<p>Apache Cassandra architecture is a distributed NoSQL database management system known for its ability to scale, perform, and offer continuous uptime. Apache Cassandra is based on a ring design wherein all nodes play the equivalent role without any master concept. Compared to other architectures such as master-slave, legacy, or sharded design, Cassandra is quite easy to set up and maintain, and is designed for handling a high volume of structured data across commodity servers.</p>
<p>Apache Cassandra's high availability and scalable architecture enable it to handle large amounts of data, and thousands of concurrent users and operations spread across multiple data centers to ensure high-performance by distributing user traffic. Cassandra has built-in features such as data modeling, high availability clusters, monitoring tools, query language, and so on.</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-550 image-border" height="176" src="assets/d4b779ed-b3ba-41a6-b928-78829e7ce525.png" width="329"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unified (common) services</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Resource management</strong>: Apache Hadoop <strong>Yet Another Resource Negotiator</strong> (<strong>YARN</strong>) is a cluster management technology. YARN is one of the key features of the second-generation Hadoop 2 version of the Apache Software Foundation's open source distributed processing framework. It also enables versatility; the resource manager can support additional paradigms and not just map/reduce.</li>
<li><strong>Apache Oozie</strong>: Apache Oozie is a system to schedule a workflow to manage Hadoop jobs. The directed acyclic graph is the mode of representing workflows in Oozie, which are a collection of control flow and activity nodes. The beginning and the end of a workflow and the mechanism to control the workflow execution path are defined by control flow nodes. The execution of a computation processing task through workflow triggers happens on action nodes.</li>
<li><strong>Apache Sentry</strong>: Hadoop's strong security at the file system level lacks the granular support to adequately secure row-level access to data by users and BI applications. Sentry allows access control at the server, database, and table, and grants different privilege levels including select, insert, and so on. It provides for authenticated users privileges on data, the ability to control and enforce access to data, and so on. It enables fine-grained access control to data and metadata in Hadoop. The column level security can be implemented by creating a view of a subset of allowed columns by restricting the base table and granted privileges. Sentry administration is simple and convenient through role-based authorization. It is a policy engine that can easily grant multiple groups access to the same data at different privilege levels such as resource, roles, users, and groups.</li>
</ul>
<p>The following diagram and table provide a comprehensive view of the security model adopted by Hadoop systems. There are security requirements at multiple levels, such as clusters, user level, and application level in an enterprise-wide implementation.</p>
<p>Cloudera offers four layers of security as follows; perimeter, access, visibility, and data. Cloudera Enterprise Security can be classified into four broad categories; authentication, authorization, data protection and auditing:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-551 image-border" height="322" src="assets/3090755d-043a-4806-9485-c26549aae793.png" width="470"/></div>
<p>Security features offered by popular tools are listed in the following table:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-552 image-border" height="195" src="assets/7252d1d0-f435-4573-a00c-fe5de170c2bd.png" width="652"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloudera proprietary services and operations/cluster management</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Cloudera Navigator</strong>: Cloudera Navigator is part of Cloudera Enterprise, and is a fully integrated data management and security system for the Hadoop platform. Cloudera Navigator is the data governance solution for Hadoop, presenting crucial capabilities such as data discovery, continuous optimization, audit, lineage, metadata management, and policy enforcement. Cloudera Navigator supports continuous data architecture optimization and meeting regulatory compliance requirements.</li>
<li><strong>Cloudera Manager</strong>: Cloudera Manager is an end-to-end application for managing CDH clusters. Cloudera Manager sets the standard for enterprise deployment by delivering granular visibility into and control over every part of the CDH cluster—empowering operators to improve performance, enhance the quality of service, increase compliance and reduce administrative costs.</li>
<li><strong>Cloudera Director</strong>: Cloudera Director works with Cloudera Manager and the cloud service provider to provide centralized and programmatic administration of clusters in the cloud, including deployment, configuration, and maintenance of CDH clusters. With Cloudera Director, you can monitor and manage multiple Cloudera Manager and CDH deployments, across different cloud environments.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Hadoop Hortonworks framework</h1>
                </header>
            
            <article>
                
<p>The following image is the framework of Hortonworks. Many components are the same as the Hadoop stack, as seen previously; we will discuss the components unique to this distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-553 image-border" height="1064" src="assets/34af9bb3-26af-44c4-8b42-401d066fcfc1.png" width="2288"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data governance and schedule pipeline</h1>
                </header>
            
            <article>
                
<p>Apache Falcon is a data management tool for managing dependencies between the system infrastructure, data, and processing logic. Data administrators can define operational and data governance policies for Hadoop workflows for overseeing data pipelines in Hadoop (<a href="http://searchdatamanagement.techtarget.com/definition/Hadoop-2">http://searchdatamanagement.techtarget.com/definition/Hadoop-2</a>) clusters. Using Falcon, we can manage thousands of compute nodes, with a large number of jobs typically running on a cluster at any given time, ensuring a consistent and dependable performance on complex processing jobs.</p>
<p>Falcon relies on Oozie job scheduling software to generate the processing workflows, to set procedures for replication, and for the retention and archiving of incoming data. The data governance engine schedules and monitors data management policies such as enhanced monitoring, and so on. The other features are tracing jobs activities for failures, dependencies, audits, and lineage, and also tagging the data to comply with data retention and discovery requirements:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-554 image-border" height="252" src="assets/7609f8d4-f321-4a37-9a77-799b8ca34371.png" width="460"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster management</h1>
                </header>
            
            <article>
                
<p>Apache Knox is a bastion security server; shielding direct access to Hadoop cluster nodes helps setups be more secure for enterprise-ready installations. Knox can easily scale horizontally by supporting stateless protocols. Knox provides authentication functionality to be managed by users and groups using LDAP or active directory. Identity Federation is SSO and HTTP header based.</p>
<p>Authorization is supported through an <strong>access control list</strong> (<strong>ACL</strong>) on service levels. The Knox Policy enforcement ranges from authentication, federation, authorization, audit, dispatch, host mapping and content rewrite rules. The policy is enforced through a list of providers, defined within the topology and the cluster definition, for purposes of routing and translation between user-facing URLs and cluster internals.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data access</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Apache Tez</strong>: This is an application framework that allows for a complex directed-acyclic-graph of tasks for processing data. It is a resource-management framework, based on Apache YARN functionality, that is extensible for building high-performance batch and interactive data processing applications, leading to significant improvements in response times while maintaining MapReduce's ability to scale to petabytes of data. Tez caters for cases that require the near-real-time performance of query processing and machine learning, and so on, with a powerful framework based on expressing computations as a data flow graph.</li>
<li><strong>Apache HCatalog</strong>: This is a storage management layer for Hadoop, that facilitates the easy reading and writing of data from the Hadoop cluster grid with different data processing tools such as Hive, Pig, MapReduce, and so on. For different kinds of data formats stored on HDFS, such as RCFile, Parquet, ORC files, or Sequence files, it uses Hive <strong>Serializer-Deserializer</strong> (<strong>SerDe</strong>) to enable a relational view. Apache HCatalog provides features such as table abstraction and data visibility to tools for cleaning and archiving.</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-555 image-border" height="238" src="assets/f11c4c6c-a956-47cb-9d52-2651bfaca05c.png" width="442"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data workflow</h1>
                </header>
            
            <article>
                
<p>The WebHDFS protocol provides external applications over the internet, HTTP, or web access for managing files and data stored in the HDFS cluster, on a par with the high-performance native protocol or native Java API default with Hadoop Cluster. WebHDFS is based on an industry-standard RESTful mechanism that provides security on par with native Hadoop protocols. Using WebHDFS common tools such as <kbd>curl/wget</kbd>, users can access the HDFS for operations such as reading files, writing to files, making directories, changing permissions, renaming, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Hadoop MapR framework</h1>
                </header>
            
            <article>
                
<p>MapR is a commercial distribution of Apache Hadoop with HDFS replaced with MapR-FS<strong>.</strong> The following is the MapR framework with common Hadoop open source components; we will now review the components unique to this distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-556 image-border" height="1012" src="assets/af731398-08f5-49d7-ae2a-a3c98b7178c4.png" width="2328"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning</h1>
                </header>
            
            <article>
                
<p>MLLIB is a collection of machine-learning algorithms and utilities for prediction models and data sciences. There are some broad groups, such as classification, clustering, collaborative filtering, and so on. A few of the ML algorithms used for each category are listed following:</p>
<ul>
<li><strong>Classification</strong>: Logistic regression, naive Bayes</li>
<li><strong>Regression</strong>: Generalized linear regression, survival regression, decision trees, random forests, and gradient-boosted trees</li>
<li><strong>Clustering</strong>: K-means, Gaussian mixtures (GMMs), frequent itemsets, association rules, and sequential pattern mining</li>
</ul>
<p><strong>GraphX</strong> is a new component in Spark for graphs and graph-parallel computation, used to implement new types of algorithms that require the modeling of relationships between objects. In many real-world applications, such as social networks, networking, and astrophysics, graph processing is very effective and efficient at representing a model relationship between entities visually.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SQL stream</h1>
                </header>
            
            <article>
                
<p>Apache Drill is a distributed SQL engine that enables data exploration and analytics on non-relational data stores such as Hadoop, MapR, CDH, NoSQL (MongoDB, HBase), cloud storage (Amazon S3, Google Cloud Storage, Azure Blog Storage, Swift), and so on. It uses a shredded, in-memory, columnar execution engine for distributed query optimization and execution, for complex data and schema-free data. By using a query engine that compiles and re-compiles queries at runtime, high performance is achieved for any structure of data. Using standard SQL and BI tools, users can query the data without having to create and manage schemas. It supports schema-free JSON document models, similar to MongoDB and Elasticsearch, and industry-standard APIs--ANSI SQL, ODBC, JDBC, and RESTful APIs.</p>
<p><strong>Apache Shark</strong> is a data warehouse-based system used with Apache Spark; its distributed query engine enhances high-end analytical results and the performance of Hive Queries multifold. Shark supports most of the Hive's features, such as query language, metastore, serialization formats, and user-defined functions. Apache Shark is built on top of Apache Spark, which is a parallel data execution engine; hence, Shark can respond to complex queries in sub-second latency.  It provides the maximum performance gains offered by column-wise memory storage systems, as data is stored and processed within the cluster's memory or in a database with in-memory materialized views.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storage, retrieval, and access control</h1>
                </header>
            
            <article>
                
<p><strong>Accumulo</strong> provides fine-grained data access control and cell-level access control, with complex policies governing access to sensitive data. It is a low-latency, large table data storage and retrieval system, with a key/value store based design. Accumulo provides extremely fast access to data in massive HDFS tables, while also controlling access to its millions of rows and columns down to the individual cell. It enables the intermingling of different data sets with access control policies for fine-grained access to data sets by encoding the policy rules for each individual data cell, and controls fine-grained access.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data integration and access</h1>
                </header>
            
            <article>
                
<p>Hue is an open source web interface for analyzing data within any HDFS cluster through Apache Oozie. It comes with many built-in features such as Hue's Editor to build workflows and then schedule them to run regularly and automatically. It has a dashboard for data querying, monitoring progress and logs, and performing actions such as pausing or stopping jobs. The applications supported are Apache Hive, Apache Impala (incubating), MySQL, Oracle, PostgreSQL, SparkSQL, Apache Solr SQL, Apache Phoenix, Apache Solr, and Apache Spark.</p>
<p><strong>HttpFS</strong>--Apache Hadoop HttpFS is a service that provides HTTP access to HDFS through REST APIs, supporting all HDFS file system operations (both read and write). It supports data transfer between HDFS clusters running different versions of Hadoop (overcoming RPC versioning issues) or a cluster behind a firewall.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Provisioning and coordination</h1>
                </header>
            
            <article>
                
<p>The data provision tools enable cloud hosting and coordination services for Hadoop systems, two popular choices are discussed following:</p>
<ul>
<li><strong>Juju</strong>:<strong> </strong>The container option available with Hadoop distribution is the Juju framework. This allows users to deploy software built locally on a range of services, including MAAS, EC2, Azure, LXD containers. Juju can model, configure and manage services and deploy to all major public and private clouds with only a few commands. Hundreds of preconfigured services are available in the Juju store.</li>
<li><strong>Apache Whirr--big data on clouds</strong>: Apache Whirr can be used to define, provision and configure big data solutions on cloud platforms such as Amazon EC2, Rackspace servers, and CloudStack. Whirr automatically starts instances (set of libraries) in the cloud and bootstraps Hadoop through on them. It initiates cloud-neutral big data services to define and provision Hadoop clusters in the cloud, and adds packages such as Hive, HBase, and Yarn for MapReduce jobs.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pivotal Hadoop platform HD Enterprise</h1>
                </header>
            
            <article>
                
<div>
<p>The following is the Pivotal HD Enterprise framework. The open source Hadoop components in this framework were already discussed earlier:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-557 image-border" height="1234" src="assets/55ad2cfb-01c9-4fa2-94fe-79f9a3131f90.png" width="2206"/></div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Hadoop ecosystem on IBM big data</h1>
                </header>
            
            <article>
                
<p><span>The following is a Hadoop ecosystem based on IBM big data. We are already familiar with most of the open source Hadoop big data components listed in this framework:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-558 image-border" height="412" src="assets/5c0369df-41b9-4d1e-86a0-ad2009d2a128.png" width="501"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Hadoop ecosystem on AWS</h1>
                </header>
            
            <article>
                
<p>Amazon <strong>Elastic MapReduce</strong> (<strong>EMR</strong>) is a service that allows users to launch and scale Hadoop clusters inside of Amazon's web infrastructure. EMR instances use Amazon's prebuilt and customized EC2 instances, which greatly simplifies the setup and management of the cluster of Hadoop and MapReduce components. EMR can analyze large datasets on AWS cloud Hadoop clusters quite effectively.</p>
<p>An AWS EMR framework depicting multiple service layers is shown in the following diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-559 image-border" height="384" src="assets/37f71ce0-7ed8-4c92-b30a-fb3a40f49ea5.png" width="439"/></div>
<div>
<p>An AWS EMR framework that offers integrations with a wide choice of Hadoop open source components is presented as follows:</p>
</div>
<ul>
<li><strong>R</strong>: It is a GNU package, open source programming language and software environment for statistical computing and graphics, and is widely used among statisticians and data miners for developing statistical software and data analysis. R as a language is both flexible and powerful.</li>
<li><strong>Presto</strong>:<strong> </strong>Apache Presto is a distributed parallel cross-platform query execution engine on the Hadoop platform. Presto supports using standard ANSI SQL to query multiple sources such as HDFS, MySQL, Cassandra, Hive, relational databases, and other data stores. Presto runs multiple analytic queries, optimized for low latency and interactive query analysis, and scales without downtime<span>. </span>Presto supports most of today's best industrial applications such as Facebook, Teradata and Airbnb, and so on.</li>
<li><strong>Gradle</strong>:<strong> </strong>The landscape of modern software development is continuously evolving, and so are the needs for build automation. Projects involve large and diverse software stacks with multiple programming languages and a wide spectrum of testing strategies. Adopting agile practices leads to the early <span>integration of code, as well as frequent and easy delivery to both test and production environments, as supported by builds. </span>Gradle is an open source build automation<span> system that builds upon the concepts of </span>Apache Ant,<span> and </span>Apache Maven<span>.  Gradle uses a </span>directed acyclic graph <span>to schedule the order of the tasks and introduces a </span>Groovy<span>-based </span>domain-specific language<span> for declaring the project configuration. </span>Gradle was designed for multi-project builds for its ability to manage dependencies. Gradle can define and organize large project builds, as well as modeling dependencies between projects. It supports incremental builds by intelligently determining the build tree dependencies and need for re-execution<span><span>.</span></span></li>
<li><strong>Cascading</strong>:<strong> </strong>Cascading is an application development platform for building big data applications on Apache Hadoop, providing an abstraction <span>layer for Apache Hadoop and Apache Flink. </span>Cascading is used to create and execute complex data processing workflows on a Hadoop cluster, hiding the underlying complexity of MapReduce jobs. Cascading provides an optimal level of abstraction with the necessary options through a computation engine, systems integration framework, data processing<span>, and scheduling capabilities. </span>Cascading offers Hadoop development teams portability for simple or complex data applications without incurring the costs of rewriting them.</li>
<li><strong>Apache Phoenix</strong>:<strong> </strong>Apache Phoenix is an open source, massively parallel relational database engine that uses Apache HBase as a base <span>to support OLTP for Hadoop. It provides random,</span> real-time <span>access to</span> large <span>datasets with a familiar SQL interface to Hadoop systems such as Spark, Hive, Pig, Flume, and MapReduce.  Apache Phoenix abstracts away the underlying data store. Aggregation queries are executed on the nodes where data is stored, reducing the need to send massive data over the network.</span></li>
<li><strong>Apache Mahout</strong>: Apache Mahout is a suite of scalable machine learning algorithms focused primarily in the areas of collaborative filtering, clustering, and classifications.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microsoft Hadoop platform is HDInsight hosted on Microsoft Azure</h1>
                </header>
            
            <article>
                
<p>The following is the architecture of an HDInsight Ecosystem hosted on Microsoft Azure. While some of the native open source layers are embedded as is, some others are tailored and customized as per Microsoft proprietary offerings:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-560 image-border" height="364" src="assets/8fc5e870-d38c-462e-a4cb-29a61e6ee0a8.png" width="636"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capacity planning for systems</h1>
                </header>
            
            <article>
                
<p>Sizing a Hadoop cluster is an important task as there are many factors influencing the performance. Capacity planning and the sizing of a Hadoop cluster are imperative for optimizing the distributed cluster environment with its related software. The number of machines, specifications of the machines, and effective process per node planning will allow you to optimize the performance effectively.</p>
<p>Within the Hadoop ecosystems, different layers (components/services) interact with each other, leading to performance overheads associated within a complex cluster stack between any of the layers; hence the need for requisite performance tests at each interface and appropriate tuning, as depicted in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-561 image-border" height="249" src="assets/46744cf0-b7cc-460f-ac1b-92abbc66feba.png" width="625"/></div>
<p>There are many factors that influence the capacity planning, sizing, and performance of a complex Hadoop-distributed cluster. The following are a few factors for consideration:</p>
<ul>
<li><strong>Amount of data</strong>:
<ul>
<li>The volume of data and growth</li>
<li>The data retention policy of how many years to hold data before discarding</li>
<li>Also the data storage mechanism (data container, type of compression used if any)</li>
</ul>
</li>
<li><strong>Type of workload</strong>:<strong> </strong>If workloads are CPU/IO /memory intensive, we will have to consider hardware accordingly. If processing might grow rapidly, we have to consider adding new data nodes.</li>
<li><strong>Frequency</strong> <strong>of workload</strong>: If the data load is batch or real-time streaming data, would it be a few times a day, nightly, weekly or monthly loads?</li>
<li><strong>Type of security</strong>:<strong> </strong>Authentication, authorization<span>, and encryption</span></li>
<li><strong>Type of services required</strong>:
<ul>
<li>What is the business SLA for the cluster? Is there is a requirement for real-time support?</li>
<li>What types of services are running other than core Hadoop services?</li>
<li>How many third-party tools will <span>be</span> installed/used<span>?</span></li>
</ul>
</li>
<li><strong>Choice of an operating system</strong>:<strong> </strong>Selecting an operating system depends on multiple factors, such as your team's administration competency, the cost of procurement and maintenance, stability, performance, reliability, support availability, and so on.
<ul>
<li><strong>CentOS</strong>: Linux CentOS is functionally compatible with RHEL, and a popular choice towards work nodes in Hadoop clusters.</li>
<li><strong>RedHat Enterprise Linux (RHEL)</strong>: Linux RHEL is widely used for servers in Hadoop clusters.</li>
<li><strong>Ubuntu</strong>: A very popular distribution, based on Debian –  both desktop and server versions available.</li>
<li><strong>SLSE</strong>: A Linux Enterprise Server developed by SUSE. It is designed for servers, mainframes, and workstations but can be installed on desktop computers for testing as well.</li>
<li><strong>Solaris, OpenSolaris</strong>: Not popular in production clusters.</li>
<li><strong>Fedora Core:</strong> Linux distributions for servers and workstations.</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Network considerations</strong>: Hadoop is very bandwidth-intensive, since most of the time all the nodes are communicating with one other simultaneously. Consider the usage of dedicated switches, a network Ethernet bandwidth of 10 GB/sec, and racks that are interconnected through switches.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Guideline for estimating and capacity planning</h1>
                </header>
            
            <article>
                
<p>Proper cluster sizing includes selecting the right hardware for a master and a worker, as well as edge nodes, while keeping the cost low.  </p>
<p>Here are a few factors to consider, along with some general guidelines for planning capacity sizing:</p>
<ul>
<li><strong>Data size</strong>:
<ul>
<li>For data sizing, the recommended replication factor is 3</li>
<li>If the total data to be stored is <em>Y</em> TB, it will become <em>3Y</em> TB after replication</li>
<li>If any compression techniques are used to store data then a compression factor can be considered</li>
<li>For disk efficiency, only 60-70% usage is recommended for the total disk availability</li>
<li>So, the total disk capacity including disk compression factor = <em>3 x Y x 7</em>  </li>
</ul>
</li>
</ul>
<ul>
<li><strong>Data growth rate</strong>:
<ul>
<li>Considering data growth factor, say 1 TB per year</li>
<li>Consider a 3 x replication factor</li>
<li>Disk efficiency factor of 70%</li>
<li>We need to calculate <em>1 TB * 3 / .6  = 4-5 TB</em></li>
<li>Storage capacity per 1 TB of data growth</li>
<li>It is equivalent to adding a new node, considering the data processing needs and growth of data volume</li>
<li>The number of data nodes planner:
<ul>
<li>Let's consider that we need to store 200 TB of data in HDFS</li>
<li>With 3 x replication, it will be 600 TB</li>
<li>Considering the replication factor, it will be <em>600 * 1.3   = 780</em> TB storage (nearly)</li>
<li>Assuming per node there are 12 disks with 2 TB capacity = 24 TB per node</li>
<li>The number of nodes required would be <em>780/24</em> = <em>33</em> nodes</li>
<li>Considering data growth requirements, we need to plan for cluster expansion in terms of additional nodes required per month, week, year, and so on</li>
</ul>
</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster-level sizing estimates</h1>
                </header>
            
            <article>
                
<p>Cluster sizing and capacity planning is important for various nodes as per their roles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">For master node</h1>
                </header>
            
            <article>
                
<p>Capacity planning for master mode is critical and needs consideration of system resources and services hosted on it as detailed following:</p>
<ul>
<li><strong>Memory</strong>:<strong> </strong>Capacity sizing of the master name node memory is a very important task. The rule of thumb is to have 1 GB of heap size to store 1 million blocks. For example:
<ul>
<li>Consider a 5-node cluster </li>
<li>Raw storage on each node is 20 TB</li>
<li>Consider that the HDFS block size is 128MB</li>
<li>Total number of blocks = <em>5 * (20*1024*1024)</em>  = <em>33</em> <em>million</em> (approximately)</li>
<li>Total HDFS block size is 33 million</li>
<li>Based on a replication factor of 3, the required heap size = <em>33/3</em> = <em>11</em></li>
<li>The total number of actual blocks would be approximately 11 GB</li>
<li>There are other factors that we need to consider as well: 
<ul>
<li>Name node would be the stable entity, so you should plan for the future growth prospects as well, while planning the sizing configuration</li>
<li>Services such as resource manager, HBase master, zookeeper, and so on would also run on this node</li>
<li>Rule of thumb is to keep a minimum of 2 GB for services such as HBase and 4 GB for the resource manager</li>
<li>Based on normal usage, the memory configuration for the master node should be 128 GB to 512 GB</li>
</ul>
</li>
</ul>
</li>
<li><strong>CPU</strong>: Cores of 2 GHz or above of processor per node are reasonable, depending on the number of services running on the node.</li>
<li><strong>Disk</strong>: The local disks on the master node should be configured as RAID10 with hot spares to give good performance and fast recovery on the loss of a disk. There should be a separate partition for <kbd>/var</kbd> with a minimum size of 1 TB, or based on the capacity required for log storage. It is strongly recommended to configure High Availability in production.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Worker node</h1>
                </header>
            
            <article>
                
<p>Capacity planning for worker node needs due consideration for system resources (memory, CPU, disk) as per the services it runs:</p>
<ul>
<li><strong>Memory</strong>: Memory on the worker nodes is based on the type of workload and daemons they will be running. You should consider 256 GB to 512 GB of memory for each of the worker nodes.</li>
<li><strong>CPU</strong>: CPU capacity is based on the type of workload and numbers of parallel MapReduce tasks that are planned. By enabling hyperthreading, the total number of MapReduce tasks should be 1.5 times of the number of the cores. Consider a minimum of 24 cores with processors greater than 2 GHz.</li>
<li><strong>Disk</strong>: You should consider having a large number of small SATA disks (2 to 4 TB), instead of a small number of large disks. All disks should be configured as JBODs with noatime, nodiratime along with <kbd>/var</kbd> as a separate partition from the root OS partition. It is recommended you add additional nodes to enhance storage, as it will increase processing power too.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gateway node</h1>
                </header>
            
            <article>
                
<p>Gateway nodes do not run any specific Hadoop services; their configuration should be based on what jobs they will run.</p>
<ul>
<li><strong>Network</strong>:
<ul>
<li>Network is a core component and should be considered very carefully as processing in Hadoop is based on data proximity.</li>
<li>If workloads consist of map-only jobs that are only transforming the data, there isn't a lot of data movement over the wire. If workloads have a lot of reduce actions such as aggregation, joins, and so on, then there is a lot of data movement between the nodes, in which case we should consider a minimum network capacity of 10 GB.</li>
<li>Running other services such as HBase, Spark should also be taken into account while considering network configuration.</li>
<li>Consider an average of 15 to 20 nodes per rack.</li>
<li>Dual Ethernet cards bonded together are recommended to support failover.</li>
<li>It's good practice to set up data locality and replication configured across the rack (rack awareness), so that even if one rack goes down, data is still available.</li>
<li> Core switches connected to top of the rack switches should be of high bandwidth (10 GB/sec or  more). Consider redundancy for top of rack as well as core switches.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have covered big data Hadoop components, popular frameworks, and their unique components for various services. In the next chapter, we will cover the terminology and technologies of the cloud such as public, private, hybrid models, service offerings for infrastructure, platform, identity, software, and network as service. We will also present a few popular market vendors.</p>


            </article>

            
        </section>
    </body></html>