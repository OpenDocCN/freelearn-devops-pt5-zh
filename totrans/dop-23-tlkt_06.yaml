- en: Deploying Releases with Zero-Downtime
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机发布部署
- en: If we are to survive in the face of competition, we have to release features
    to production as soon as they are developed and tested. The need for frequent
    releases fortifies the need for zero-downtime deployments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要在竞争中生存下去，我们必须在功能开发和测试完成后尽快将其发布到生产环境。频繁发布的需求进一步强调了零停机部署的重要性。
- en: We learned how to deploy our applications packaged as Pods, how to scale them
    through ReplicaSets, and how to enable communication through Services. However,
    all that is useless if we cannot update those applications with new releases.
    That is where Kubernetes Deployments come in handy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何部署打包为 Pod 的应用程序，如何通过 ReplicaSets 扩展它们，以及如何通过 Services 启用通信。然而，如果我们无法用新版本更新这些应用程序，那一切都是无用的。Kubernetes
    部署在这一点上非常有用。
- en: The desired state of our applications is changing all the time. The most common
    reasons for new states are new releases. The process is relatively simple. We
    make a change and commit it to a code repository. We build it, and we test it.
    Once we're confident that it works as expected, we deploy it to a cluster. It
    does not matter whether that deployment is to a development, test, staging, or
    production environment. We need to deploy a new release to a cluster, even when
    that is a single-node Kubernetes running on a laptop. No matter how many environments
    we have, the process should always be the same or, at least, as similar as possible.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序的期望状态始终在变化。导致新状态的最常见原因是新的发布版本。这个过程相对简单。我们进行更改并将其提交到代码仓库，然后进行构建和测试。一旦我们确认它按预期工作，我们就将其部署到集群中。无论该部署是针对开发、测试、预生产还是生产环境，我们都需要将新版本部署到集群中，即使这个集群只是运行在笔记本电脑上的单节点
    Kubernetes。不论我们有多少个环境，过程应该始终保持相同，或者至少尽可能相似。
- en: The deployment must produce no downtime. It does not matter whether it is performed
    on a testing or a production cluster. Interrupting consumers is disruptive, and
    that leads to loss of money and confidence in a product. Gone are the days when
    users did not care if an application sometimes did not work. There are so many
    competitors out there that a single bad experience might lead users to another
    solution. With today's scale, 0.1% of failed requests is considered disastrous.
    While we might never be able to reach 100% availability, we should certainly not
    cause downtime ourselves and must minimise other factors that could cause downtime.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 部署必须确保无停机。无论是在测试集群还是生产集群上执行，都不应中断消费者的服务，这样会导致金钱和对产品的信任损失。过去的时代，用户可能不在乎应用偶尔无法工作，但现在竞争如此激烈，单次糟糕的体验可能会导致用户选择其他解决方案。在今天的规模下，0.1%的请求失败率都被视为灾难性的。虽然我们可能永远无法达到100%的可用性，但我们至少不应该自己导致停机，并且必须尽量减少其他可能导致停机的因素。
- en: Failures caused by circumstances outside of our control are things which, by
    definition, we can do nothing about. However, failures caused by obsolete practices
    or negligence are failures which should not happen. Kubernetes Deployments provide
    us with the tools we need to avoid such failures by allowing us to update our
    applications without downtime.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由我们无法控制的外部环境引起的失败，按定义来说是我们无法采取任何措施的事情。然而，由于过时的做法或疏忽导致的失败则是应该避免的失败。Kubernetes
    部署为我们提供了必要的工具，使我们能够在不产生停机的情况下更新应用程序，从而避免这些失败。
- en: Let's explore how Kubernetes Deployments work and the benefits we gain by adopting
    them.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下 Kubernetes 部署的工作原理，以及我们通过采用它们获得的好处。
- en: Creating a Cluster
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个集群
- en: Creating a cluster at the beginning of each chapter allows us to jump into any
    part of the book without worrying whether there is a requirement to meet from
    previous chapters. It also allows us to pause between chapters without stressing
    our laptops by running a VM that is not in use. The downside is that this is the
    boring part of every chapter. Therefore, the talk stops here. Let's get it over
    with.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一章开始时创建集群可以让我们直接进入书中的任何部分，而不需要担心是否需要完成前面的章节要求。它还使我们可以在章节之间暂停，而无需让笔记本电脑因运行一个未使用的虚拟机而承受压力。缺点是，这部分是每章最枯燥的部分。因此，今天就讲到这里，让我们尽快完成这项工作。
- en: All the commands from this chapter are available in the `06-deploy.sh` ([https://gist.github.com/vfarcic/677a0d688f65ceb01e31e33db59a4400](https://gist.github.com/vfarcic/677a0d688f65ceb01e31e33db59a4400))
    Gist.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有命令都可以在 `06-deploy.sh` 文件中找到，地址为 [https://gist.github.com/vfarcic/677a0d688f65ceb01e31e33db59a4400](https://gist.github.com/vfarcic/677a0d688f65ceb01e31e33db59a4400)。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code was updated, the cluster is up-and-running, and we can start exploring
    Deployments.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已更新，集群正常运行，我们可以开始探索Deployments。
- en: Deploying new releases
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署新版本
- en: Just as we are not supposed to create Pods directly but using other controllers
    like ReplicaSet, we are not supposed to create ReplicaSets either. Kubernetes
    Deployments will create them for us. If you're wondering why, you'll have to wait
    a little while longer to find out. First, we'll create a few Deployments and,
    once we are familiar the process and the outcomes, it'll become obvious why they
    are better at managing ReplicaSets than we are.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们不应该直接创建Pods，而是使用ReplicaSet等其他控制器一样，我们也不应该直接创建ReplicaSets。Kubernetes Deployments会为我们创建它们。如果你在想为什么，稍等片刻，我们将揭示答案。首先，我们将创建几个Deployments，一旦熟悉了这个过程和结果，就会很明显为什么它们在管理ReplicaSets方面比我们更有效。
- en: Let's take a look at a Deployment specification for the database ReplicaSet
    we've been using thus far.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下我们迄今为止使用的数据库ReplicaSet的Deployment规格。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you compare this Deployment with the ReplicaSet we created in the previous
    chapter, you'll probably have a hard time finding a difference. Apart from the
    `kind` field, they are the same.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个Deployment与我们在前一章中创建的ReplicaSet进行比较，你可能会很难找到差异。除了`kind`字段之外，它们是相同的。
- en: Since, in this case, both the Deployment and the ReplicaSet are the same, you
    might be wondering what the advantage of using one over the other is.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这种情况下，Deployment和ReplicaSet是相同的，你可能会想知道使用其中一个而不是另一个的优势是什么。
- en: We will regularly add `--record` to the `kubectl create` commands. This allows
    us to track each change to our resources such as a Deployments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定期在`kubectl create`命令中添加`--record`选项。这样，我们就能跟踪每次对资源（如Deployments）的更改。
- en: Let's create the Deployment and explore what it offers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建Deployment并探索它所提供的功能。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the latter command is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出如下：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Deployment was created. However, `get` does not provide us much info, so
    let's `describe` it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment已创建。然而，`get`命令没有提供太多信息，所以让我们用`describe`命令查看。
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output, limited to the last few lines, is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果（仅限最后几行）如下：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From the `Events` section, we can observe that the Deployment created a ReplicaSet.
    Or, to be more precise, that it scaled it. That is interesting. It shows that
    Deployments control ReplicaSets. The Deployment created the ReplicaSet which,
    in turn, created Pods. Let''s confirm that by retrieving the list of all the objects:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Events`部分，我们可以看到Deployment创建了一个ReplicaSet。更准确地说，它对其进行了扩展。这很有趣，它显示了Deployments控制ReplicaSets。Deployment创建了ReplicaSet，而ReplicaSet又创建了Pods。让我们通过检索所有对象的列表来确认这一点：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: All three objects were created, and you might be wondering why we created the
    Deployment at all. You might think that we'd have the same result if we created
    a ReplicaSet directly. You'd be right. So far, from the functional point of view,
    there is no difference between a ReplicaSet created directly or using a Deployment.
    The real advantage of Deployments becomes evident if we try to change some of
    its aspects. For example, we might choose to upgrade MongoDB to version 3.4.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 三个对象都已创建，你可能会想，为什么要创建Deployment呢？你可能认为如果直接创建ReplicaSet，结果应该是一样的。你是对的。到目前为止，从功能角度来看，直接创建ReplicaSet或通过Deployment创建没有什么区别。Deployments的真正优势在于我们尝试改变它的一些方面时才会显现出来。例如，我们可能会选择将MongoDB升级到3.4版本。
- en: '![](img/7ca574b5-1c0c-406a-ba52-3f5559c0f47d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ca574b5-1c0c-406a-ba52-3f5559c0f47d.png)'
- en: 'Figure 6-1: Deployment and its cascading effect that creates a ReplicaSet and,
    though it, Pods'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-1：Deployment及其级联效应，创建ReplicaSet，并通过ReplicaSet创建Pods
- en: Before we move onto Deployment updates, we'll go through our usual ritual of
    seeing the process through a sequence diagram. We won't repeat the explanation
    of the events that happened after the ReplicaSet object was created as those steps
    were already explained in the previous chapters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论Deployment更新之前，我们将通过一个序列图来回顾我们惯常的流程。我们不会重复解释ReplicaSet对象创建后发生的事件，因为这些步骤已经在前面的章节中解释过。
- en: Kubernetes client (`kubectl`) sent a request to the API server requesting the
    creation of a Deployment defined in the `deploy/go-demo-2-db.yml` file
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes客户端（`kubectl`）向API服务器发送请求，请求创建`deploy/go-demo-2-db.yml`文件中定义的Deployment。
- en: The deployment controller is watching the API server for new events, and it
    detected that there is a new Deployment object
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署控制器正在监视API服务器的新事件，并检测到有新的Deployment对象。
- en: The deployment controller creates a new ReplicaSet object
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署控制器创建了一个新的 ReplicaSet 对象
- en: '![](img/0e76e8fa-76b4-4608-956e-7c5ccd76ea69.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e76e8fa-76b4-4608-956e-7c5ccd76ea69.png)'
- en: 'Figure 6-2: The sequence of events followed by request to create a deployment'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-2：请求创建部署时的事件顺序
- en: Updating Deployments
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新部署
- en: Let's see what happens when we `set` a new image to the `db` Pod.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们给`db` Pod设置新镜像时会发生什么。
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It'll take a while until the new image is pulled, so you might as well fetch
    yourself a coffee. Once you're back, we can `describe` the Deployment by checking
    the events it created.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 新镜像被拉取需要一些时间，所以你可以去喝杯咖啡。一旦你回来，我们可以通过检查它创建的事件来`describe`该部署。
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The last few lines of the output are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that it created a new ReplicaSet and that it scaled the old ReplicaSet
    to `0`. If, in your case, the last line did not appear, you'll need to wait until
    the new version of the `mongo` image is pulled.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它创建了一个新的 ReplicaSet，并将旧的 ReplicaSet 缩放到了`0`。如果在你的情况中，最后一行没有出现，你需要等到新的
    `mongo` 镜像被拉取完毕。
- en: Instead of operating directly on the level of Pods, the Deployment created a
    new ReplicaSet which, in turn, produced Pods based on the new image. Once they
    became fully operational, it scaled the old ReplicaSet to `0`. Since we are running
    a ReplicaSet with only one replica, it might not be clear why it used that strategy.
    When we create a Deployment for the API, things will become more evident.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 部署并不是直接在 Pods 层面操作，而是创建了一个新的 ReplicaSet，后者基于新镜像生成 Pods。等它们完全可用后，旧的 ReplicaSet
    会被缩放到`0`。由于我们只运行了一个副本的 ReplicaSet，可能不太清楚为什么采用这种策略。当我们为 API 创建一个部署时，情况会变得更加明确。
- en: 'To be on the safe side, we might want to retrieve all the objects from the
    cluster:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们可能想从集群中获取所有对象：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, both ReplicaSets are there. However, one is inactive (scaled
    to `0`).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两个 ReplicaSets 都存在。然而，一个处于非活动状态（已缩放为`0`）。
- en: You'll notice that contained within the name of the Pod is a hash which matches
    the hash in the name of the new ReplicaSet, namely `f8d4b86ff`. Even though it
    might look like it is a random value, it is not. If you destroy the Deployment
    and create it again, you'll notice that the hash in the Pod name and ReplicaSet
    name remain consistent. This value is generated by hashing the PodTemplate of
    the ReplicaSet. As long as the PodTemplate is the same, the hash value will be
    the same as well. That way a Deployment can know whether anything related to the
    Pods has changed and, if it does, will create a new ReplicaSet.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，Pod 名称中包含了一个与新 ReplicaSet 名称中的哈希值相匹配的哈希值，即 `f8d4b86ff`。尽管看起来像是一个随机值，但实际上并不是。如果你销毁部署并重新创建它，你会发现
    Pod 名称和 ReplicaSet 名称中的哈希值保持一致。这个值是通过对 ReplicaSet 的 PodTemplate 进行哈希生成的。只要 PodTemplate
    不变，哈希值也会保持一致。这样，部署就可以知道是否有与 Pods 相关的更改，如果有更改，就会创建一个新的 ReplicaSet。
- en: The `kubectl set image` command is not the only way to update a Deployment.
    We could also have used `kubectl edit` as well. The command would be as follows.
    **Please do NOT execute it.** If you do (against my advice), you'll need to type
    `:q` followed by the *Enter* key to exit.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl set image`命令并不是更新部署的唯一方法。我们也可以使用`kubectl edit`。命令如下。**请不要执行它。**如果你执行了（违背我的建议），你需要输入`:q`并按*Enter*键才能退出。'
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: I don't think the above `edit` command is a good way to update the definition.
    It is unpractical and undocumented. The `kubectl set image` is more useful if
    we'd like to integrate Deployment updates with one of the CI/CD tools. Since we'll
    have a chapter dedicated to continuous deployment, we'll continue using `kubectl
    set image` from now on.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为上述的`edit`命令并不是更新定义的好方法。它既不实用，也没有文档说明。如果我们希望将部署更新与 CI/CD 工具之一集成，`kubectl set
    image`会更有用。由于接下来会有专门讲解持续部署的章节，我们从现在开始将继续使用`kubectl set image`。
- en: Another alternative would be to update the YAML file and execute the `kubectl
    apply` command. While that is a good idea for applications that do not update
    frequently, it does not fit well with those that change weekly, daily, or even
    hourly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种替代方法是更新 YAML 文件并执行`kubectl apply`命令。虽然这对不常更新的应用程序来说是个好主意，但对于每周、每天甚至每小时都会发生变化的应用程序来说并不适用。
- en: MongoDB is one of those that might get updated with a new release only a couple
    of times a year so having an always up-to-date YAML file in your source code repository
    is an excellent practice. We used `kubectl set image` just as a way to introduce
    you to what's coming next when we explore frequent deployments without downtime.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB是那些每年可能只更新几次新版本的应用之一，因此在源代码仓库中始终保持最新的YAML文件是一种很好的实践。我们使用了`kubectl set
    image`，只是为了让你了解接下来我们将要探讨的内容：如何在没有停机的情况下进行频繁的部署。
- en: A simple update of Pod images is far from what Deployment offers. To see its
    real power, we should deploy the API. Since it can be scaled to multiple Pods,
    it'll provide us with a much better playground.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯更新Pod镜像远远无法展示Deployment的真正优势。为了看到其真正的力量，我们应该部署API。由于它可以扩展到多个Pod，它将为我们提供一个更好的操作环境。
- en: 'Before we move on, let''s finish with the database by adding a Service and,
    therefore, enabling internal cluster communication to it:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们通过添加一个Service来完成数据库的配置，从而启用集群内部的通信：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Zero-Downtime Deployments
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署
- en: Updating a single-replica MongoDB cannot demonstrate true power behind Deployments.
    We need a scalable service. It's not that MongoDB cannot be scaled (it can), but
    it is not as straight-forward as an application that was designed to be scalable.
    We'll jump to the second application in the stack and create a Deployment of the
    ReplicaSet that will create Pods based on the `vfarcic/go-demo-2` image. But,
    before we do that, we'll spend a few moments discussing the need for zero-downtime
    deployments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 更新单个副本的MongoDB无法展示Deployment背后的真正能力。我们需要一个可扩展的服务。并不是说MongoDB不能扩展（它是可以的），但它不像一个从一开始就设计为可扩展的应用程序那样简单。我们将跳到堆栈中的第二个应用程序，并创建一个ReplicaSet的Deployment，基于`vfarcic/go-demo-2`镜像创建Pod。但在此之前，我们将花一些时间讨论零停机部署的需求。
- en: On the one hand, our applications are supposed to have very high availability.
    Depending on the context and the goals, we usually discuss how many nines are
    coming after 99%. At the very least, an application must have availability of
    at least 99.9%. More likely, it should be something closer to 99.99 or even 99.999
    percent availability. Hundred percent availability is often not possible or too
    expensive to accomplish. We cannot avoid all failures, but we can reduce them
    to acceptable limits.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们的应用程序应该具备非常高的可用性。根据上下文和目标的不同，我们通常会讨论99%之后有多少个“9”。至少，一个应用程序的可用性必须达到99.9%。更常见的是，它应该接近99.99%甚至99.999%的可用性。百分之百的可用性通常是不可能的，或者实现起来成本过高。我们无法避免所有故障，但可以将其减少到可接受的范围内。
- en: No matter what the availability of SLA is, applications (at least when developed
    by us) must be scalable. Only when there are multiple replicas, can we hope for
    any decent availability. Scaled applications can not only spread the load across
    various instances but ensure that a failure of one replica will not produce downtime.
    Healthy instances are handling the load until the scheduler recreates failed ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 无论SLA的可用性是多少，应用程序（至少是我们开发的应用程序）必须是可扩展的。只有当有多个副本时，我们才能期望有合理的可用性。可扩展的应用程序不仅可以将负载分摊到多个实例上，还能确保一个副本的故障不会导致停机。健康的实例会继续处理负载，直到调度器重新创建失败的副本。
- en: High availability is accomplished through fault tolerance and scalability. If
    either is missing, any failure might have disastrous effects.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性是通过容错和可扩展性来实现的。如果其中任何一项缺失，任何故障可能都会产生灾难性的后果。
- en: The reason we're discussing failures and scalability lies in the nature of immutable
    deployments. If a Pod is unchangeable, the only way to update it with a new release
    is to destroy the old ones and put the Pods based on the new image in their place.
    Destruction of Pods is not much different from failures. In both cases, they cease
    to work. On the other hand, fault tolerance (re-scheduling) is a replacement of
    failed Pods.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论故障和可扩展性的原因在于不可变部署的特性。如果一个Pod是不可变的，那么更新它的新版本的唯一方法是销毁旧的Pod，并用基于新镜像的Pod替换它们。Pod的销毁与故障没有太大区别。在两种情况下，Pod都停止工作。另一方面，容错（重新调度）是对失败Pod的替代。
- en: The only essential difference is that new releases result in Pods being replaced
    with new ones based on the new image. As long as the process is controlled, new
    releases should not result in any downtime when multiple replicas of an application
    are running and when they are adequately designed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的实质性区别在于，新版本发布会导致Pod被新的基于新镜像的Pod替换。只要过程得到控制，当应用程序有多个副本运行并且设计得当时，新版本发布应该不会导致停机。
- en: We should not worry about the frequency of new releases. The process should
    be the same no matter whether we make releases once a month, once a week, once
    a day, or every few minutes. If the release process produces any downtime, we
    might be compelled to deploy new versions infrequently. As a matter of fact, throughout
    the history of software development, we were taught that releases should be limited
    in number. A couple a year was the norm. Part of the reasons behind such infrequent
    releases was due to the downtime they produce. If we can reach zero-downtime deployments,
    the frequency can change, and we can aim for continuous deployment. We won't go
    into benefits behind continuous deployment just yet. It's not relevant at this
    point. Instead, we'll focus on zero-downtime deployments. Given a choice, no one
    would choose the little-bit-of-downtime strategy, so I'll assume that everyone
    wants to be able to release without interruptions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该担心新版本发布的频率。无论是每月发布一次、每周发布一次、每天发布一次还是每几分钟发布一次，过程应该是相同的。如果发布过程中出现任何停机时间，我们可能会被迫减少新版本发布的频率。事实上，在软件开发历史上，我们曾经被告知发布应该是有限的。每年发布几次是常态。之所以发布不频繁，部分原因是它们会造成停机时间。如果我们能够实现零停机部署，那么发布频率就可以发生变化，我们可以目标为持续部署。我们现在不打算深入探讨持续部署的好处，这在此时并不相关。相反，我们将专注于零停机部署。如果有选择的话，没人会选择那种“稍微有点停机”的策略，所以我假设每个人都希望能够在没有中断的情况下发布。
- en: Zero-downtime deployment is a prerequisite for higher frequency releases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 零停机部署是频繁发布的前提条件。
- en: 'Let''s take a look at the Deployment definition of the API:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看API的Deployment定义：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We'll skip explaining `apiVersion`, `kind`, and `metadata`, since they always
    follow the same pattern.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过对`apiVersion`、`kind`和`metadata`的解释，因为它们总是遵循相同的模式。
- en: The `spec` section has a few of the fields we haven't seen before, and a few
    of those we are familiar with. The `replicas` and the `selector` are the same
    as what we used in the ReplicaSet from the previous chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec`部分包含了一些我们之前没有见过的字段，也有一些我们熟悉的字段。`replicas`和`selector`与我们在上一章的ReplicaSet中使用的相同。'
- en: '`minReadySeconds` defines the minimum number of seconds before Kubernetes starts
    considering the Pods healthy. We put the value of this field to `1` second. The
    default value is `0`, meaning that the Pods will be considered available as soon
    as they are ready and, when specified, `livenessProbe` returns OK. If in doubt,
    omit this field and leave it to the default value of `0`. We defined it mostly
    for demonstration purposes.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`minReadySeconds`定义了Kubernetes在将Pod视为健康之前的最小秒数。我们将该字段的值设置为`1`秒。默认值为`0`，这意味着一旦Pod准备好，并且在指定时`livenessProbe`返回OK，它们就会被视为可用。如果有疑问，可以省略此字段，保留默认值`0`。我们主要是为了演示目的才定义了这个值。'
- en: The next field is `revisionHistoryLimit`. It defines the number of old ReplicaSets
    we can rollback. Like most of the fields, it is set to the sensible default value
    of `10`. We changed it to `5` and, as a result, we will be able to rollback to
    any of the previous five ReplicaSets.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个字段是`revisionHistoryLimit`。它定义了我们可以回滚的旧ReplicaSet数量。像大多数字段一样，它的默认值是合理的`10`。我们将其更改为`5`，因此，我们将能够回滚到之前的五个ReplicaSet中的任何一个。
- en: The `strategy` can be either the `RollingUpdate` or the `Recreate` type. The
    latter will kill all the existing Pods before an update. `Recreate` resembles
    the processes we used in the past when the typical strategy for deploying a new
    release was first to stop the existing one and then put a new one in its place.
    This approach inevitably leads to downtime. The only case when this strategy is
    useful is when applications are not designed for two releases to coexist. Unfortunately,
    that is still more common than it should be. If you're in doubt whether your application
    is like that, ask yourself the following question. Would there be an adverse effect
    if two different versions of my application are running in parallel? If that's
    the case, a `Recreate` strategy might be a good choice and *you must be aware
    that you cannot accomplish zero-downtime deployments*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`strategy`可以是`RollingUpdate`或`Recreate`类型。后者将在更新之前杀死所有现有的Pod。`Recreate`类似于我们过去使用的过程，当时部署新版本的典型策略是先停止现有版本，然后用新版本替换它。这种方法不可避免地会导致停机。只有在应用程序没有设计为支持两个版本并行存在时，这个策略才有用。不幸的是，这种情况比应有的还要普遍。如果你怀疑你的应用程序是否属于这种情况，可以问自己以下问题：如果我的应用程序的两个不同版本并行运行，是否会有不良影响？如果是这样，`Recreate`策略可能是一个不错的选择，并且*你必须意识到，无法实现零停机部署*。'
- en: The `recreate` strategy is much better suited for our single-replica database.
    We should have set up the native database replication (not the same as Kubernetes
    ReplicaSet object), but, as explained earlier, that is out of the scope of this
    chapter (and probably this book).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`recreate`策略更适合我们的单副本数据库。我们应该已经设置了本地数据库复制（与Kubernetes的ReplicaSet对象不同），但正如前面所述，这超出了本章的范围（也许甚至超出了本书的范围）。'
- en: If we're running the database as a single replica, we must have mounted a network
    drive volume. That would allow us to avoid data loss when updating it or in case
    of a failure. Since most databases (MongoDB included) cannot have multiple instances
    writing to the same data files, killing the old release before creating a new
    one is a good strategy when replication is absent. We'll apply it later.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据库作为单副本运行，必须挂载一个网络驱动器卷。这样可以避免在更新过程中或发生故障时数据丢失。由于大多数数据库（包括MongoDB）不能让多个实例写入相同的数据文件，因此在没有复制的情况下，先停止旧版本再创建新版本是一种很好的策略。我们稍后会应用它。
- en: The `RollingUpdate` strategy is the default type, for a good reason. It allows
    us to deploy new releases without downtime. It creates a new ReplicaSet with zero
    replicas and, depending on other parameters, increases the replicas of the new
    one, and decreases those from the old one. The process is finished when the replicas
    of the new ReplicaSet entirely replace those from the old one.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`RollingUpdate`策略是默认类型，这也是有原因的。它允许我们在不间断的情况下部署新版本。它会创建一个副本数为零的新ReplicaSet，并根据其他参数，逐步增加新ReplicaSet的副本数，同时减少旧ReplicaSet的副本数。该过程在新ReplicaSet的副本完全替代旧版本副本时结束。'
- en: When `RollingUpdate` is the strategy of choice, it can be fine-tuned with the
    `maxSurge` and `maxUnavailable` fields. The former defines the maximum number
    of Pods that can exceed the desired number (set using `replicas`). It can be set
    to an absolute number (for example, `2`) or a percentage (for example, `35%`).
    The total number of Pods will never exceed the desired number (set using `replicas`)
    and the `maxSurge` combined. The default value is `25%`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当`RollingUpdate`作为首选策略时，可以通过`maxSurge`和`maxUnavailable`字段进行微调。前者定义了可以超过所需数量（通过`replicas`设置）的Pod的最大数量。它可以设置为一个绝对值（例如，`2`）或一个百分比（例如，`35%`）。Pod的总数永远不会超过所需数量（通过`replicas`设置）和`maxSurge`的总和。默认值为`25%`。
- en: '`maxUnavailable` defines the maximum number of Pods that are not operational.
    If, for example, the number of replicas is set to 15 and this field is set to
    4, the minimum number of Pods that would run at any given moment would be 11\.
    Just as the `maxSurge` field, this one also defaults to `25%`. If this field is
    not specified, there will always be at least 75% of the desired Pods.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxUnavailable`定义了不可操作的Pod的最大数量。例如，如果副本数设置为15，而此字段设置为4，则在任何时刻运行的最少Pod数量将为11。就像`maxSurge`字段一样，这个字段的默认值也是`25%`。如果没有指定此字段，将始终至少有75%的Pod是期望中的Pod。'
- en: In most cases, the default values of the Deployment specific fields are a good
    option. We changed the default settings only as a way to demonstrate better all
    the options we can use. We'll remove them from most of the Deployment definitions
    that follow.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，Deployment特定字段的默认值是一个不错的选择。我们仅通过更改默认设置来展示我们可以使用的更多选项。在随后的大多数Deployment定义中，我们将移除这些设置。
- en: The `template` is the same `PodTemplate` we used before. Best practice is to
    be explicit with image tags like we did when we set `mongo:3.3`. However, that
    might not always be the best strategy with the images we're building. Given we
    employ right practices, we can rely on `latest` tags being stable. Even if we
    discover they're not, we can remedy that quickly by creating a new `latest` tag.
    However, we cannot expect the same from third-party images. They must always be
    tagged to a specific version.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`template`是我们之前使用的相同的`PodTemplate`。最佳实践是像我们为`mongo:3.3`设置镜像标签时那样，明确指定镜像标签。然而，这对于我们构建的镜像可能并不是最佳策略。只要我们遵循正确的实践，就可以依赖`latest`标签保持稳定。即使发现它们不稳定，我们也可以通过创建一个新的`latest`标签来迅速修复。然而，我们不能指望第三方镜像也如此，它们必须始终使用特定版本的标签。'
- en: Never deploy third-party images based on `latest` tags. By being explicit with
    the release, we have more control over what is running in production, as well
    as what should be the next upgrade.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要基于`latest`标签部署第三方镜像。通过明确发布版本，我们可以更好地控制生产环境中正在运行的内容，并明确下一次升级应该是什么。
- en: We won't always use `latest` for our services, but only for the initial Deployments.
    Assuming that we are doing our best to maintain the `latest` tag stable and production-ready,
    it is handy when setting up the cluster for the first time. After that, each new
    release will be with a specific tag. Our automated continuous deployment pipeline
    will do that for us in one of the next chapters.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会一直使用`latest`标签来标识我们的服务，而只是用于初始部署。假设我们尽最大努力保持`latest`标签稳定且适合生产环境，它在首次设置集群时非常方便。之后，每个新的发布版本将使用一个特定的标签。我们的自动化持续部署流水线将在后续章节中为我们完成此任务。
- en: If you are confident in your ability to maintain `latest` stable, it is handy
    using it for the first Deployment of an application.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有信心保持`latest`标签的稳定性，那么在应用程序的首次部署时使用它是非常方便的。
- en: Before we explore rolling updates, we should create the Deployment and, with
    it, the first release of our application.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索滚动更新之前，我们应该创建部署，并通过它发布我们应用程序的第一个版本。
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We created the Deployment and retrieved the object from the Kubernetes API server.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了Deployment，并从Kubernetes API服务器获取了该对象。
- en: 'The output of the latter command is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出如下：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Please make sure that the number of available Pods is `3`. Wait for a few moments,
    if that's not the case. Once all the Pods are up-and-running, we'll have a Deployment
    that created a new ReplicaSet which, in turn, created three Pods based on the
    latest release of the `vfarcic/go-demo-2` image.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保可用的Pods数量为`3`。如果不是，稍等片刻。一旦所有Pods都启动并运行，我们将拥有一个创建了新ReplicaSet的Deployment，而ReplicaSet又根据`vfarcic/go-demo-2`镜像的最新版本创建了三个Pods。
- en: Let's see what happens when we set a new image.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看设置新镜像时会发生什么。
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are a few ways we can observe what is happening during the update. One
    of those is through the `kubectl rollout status` command.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式观察更新过程中发生了什么。其中一种方法是通过`kubectl rollout status`命令。
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: From the last entry, we can see that the rollout of the new deployment was successful.
    Depending on the time that passed between setting the new image and displaying
    the rollout status, you might have seen other entries marking the progress. However,
    I think that the events from the `kubectl describe` command are painting a better
    picture of the process that was executed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一条记录中，我们可以看到新部署的发布成功了。根据设置新镜像和显示发布状态之间经过的时间，您可能会看到其他记录标记了进展情况。不过，我认为`kubectl
    describe`命令的事件提供了一个更清晰的执行过程图景。
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The last lines of the output are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can see that the number of desired replicas is `3`. The same number was updated
    and all are available.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，期望的副本数是`3`。该数量已更新并且都处于可用状态。
- en: At the bottom of the output are events associated with the Deployment. The process
    started by increasing the number of replicas of the new ReplicaSet (`go-demo-2-api-68c75f4f5`)
    to `1`. Next, it decreased the number of replicas of the old ReplicaSet (`go-demo-2-api-68df567fb5`)
    to `2`. The same process of increasing replicas of the new, and decreasing replicas
    of the old ReplicaSet continued until the new one got the desired number (`3`),
    and the old one dropped to zero.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输出底部是与部署相关的事件。该过程始于增加新 ReplicaSet (`go-demo-2-api-68c75f4f5`) 的副本数至 `1`。接下来，它减少了旧
    ReplicaSet (`go-demo-2-api-68df567fb5`) 的副本数至 `2`。同样的过程，增加新的副本并减少旧的 ReplicaSet
    的副本数一直持续，直到新的 ReplicaSet 达到所需的数量 (`3`)，而旧的则减少到零。
- en: There was no downtime throughout the process. Users would receive a response
    from the application no matter whether they sent it before, during, or after the
    update. The only important thing is that, during the update, a response might
    have come from the old or the new release. During the update process, both releases
    were running in parallel.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程中没有停机时间。用户无论在更新前、期间还是之后发送请求，都会收到应用程序的响应。唯一重要的是，在更新期间，响应可能来自旧版或新版。更新过程中，两个版本都在并行运行。
- en: 'Let''s take a look at the rollout history:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下回滚历史记录：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can see that, so far, there were two revisions of the software. The change
    cause shows which command created each of those revisions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以看到软件有两个修订版。更改原因显示了每个修订版由哪个命令创建。
- en: How about ReplicaSets?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSets 怎么样？
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output, limited to `go-demo-2-api`, is as follows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果仅限于 `go-demo-2-api`，如下所示。
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can see that the Deployment did not modify the ReplicaSet, but that it created
    a new one and, at the end of the process, the old one was scaled to zero replicas.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到部署没有修改 ReplicaSet，而是创建了一个新的 ReplicaSet，在过程结束时，旧的 ReplicaSet 被缩减为零副本。
- en: The diagram in the *Figure 6-2* shows the flow of the events that occurred since
    we executed the `kubectl set image` command. It closely depicts the events we
    already saw from the `kubectl describe` command.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-2 中的图示了自执行 `kubectl set image` 命令以来发生的事件流。它紧密地描述了我们从 `kubectl describe`
    命令中已经看到的事件。
- en: '![](img/43ddf2cc-9300-4176-905c-fe6f3a01ac61.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43ddf2cc-9300-4176-905c-fe6f3a01ac61.png)'
- en: 'Figure 6-3: Deployment controller rolling update workflow'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-3：部署控制器滚动更新工作流程
- en: We made great progress. However, the unexpected can happen at any time, and
    we must be prepared to deal with it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取得了很大进展。然而，意外随时可能发生，我们必须做好应对准备。
- en: Rolling back or rolling forward?
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚还是向前滚动？
- en: At this point, we are, more or less, capable of deploying new releases to production
    as soon as they are ready. However, there will be problems. Something unexpected
    will happen. A bug will sneak in and put our production cluster at risk. What
    should we do in such a case? The answer to that question largely depends on the
    size of the changes and the frequency of deployments.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们或多或少能够在准备好的时候立即向生产环境部署新的发布版。然而，总会出现问题。意外情况会发生。错误会悄悄溜进来，使我们的生产集群面临风险。在这种情况下，我们该怎么办？这个问题的答案很大程度上取决于变更的规模和部署的频率。
- en: If we are using continuous deployment process, we are deploying new releases
    to production fairly often. Instead of waiting until features accumulate, we are
    deploying small chunks. In such cases, fixing a problem might be just as fast
    as rolling back. After all, how much time would it take you to fix a problem caused
    by only a few hours of work (maybe a day) and that was discovered minutes after
    you committed? Probably not much. The problem was introduced by a very recent
    change that is still in engineer's head. Fixing it should not take long, and we
    should be able to deploy a new release soon.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在使用持续部署流程，我们会经常向生产环境部署新的发布版。与其等待功能累积，我们更倾向于部署小块。在这种情况下，修复问题可能与回滚一样快。毕竟，解决仅仅几小时工作（也许一天）导致的问题需要多少时间？可能不多。问题是由最近的一个变更引入的，该变更仍在工程师的头脑中。修复它不应该需要很长时间，我们应该能够很快部署新版本。
- en: You might not have frequent releases, or the amount of changes included is more
    than a couple of hundreds of lines of code. In such a case, rolling forward might
    not be as fast as it should be. Still, rolling back might not even be possible.
    We might not be able to revert the deployment if database schema changed, and
    it is not compatible with the previous versions of the back-end that uses it.
    The moment the first transaction enters, we might lose the option to roll-back.
    At least, not without losing the data generated since the new release.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能没有频繁发布，或者所包含的更改量超过了几百行代码。在这种情况下，前进的速度可能不如预期。即便如此，回滚可能也无法实现。如果数据库模式发生变化，且与使用它的后端旧版本不兼容，我们可能无法回滚部署。一旦第一个事务开始处理，我们可能就无法再回滚，至少没有丢失自新发布以来生成的数据。
- en: Rolling back a release that introduced database changes is often not possible.
    Even when it is, rolling forward is usually a better option when practicing continuous
    deployment with high-frequency releases limited to a small scope of changes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚引入了数据库更改的发布通常是不可行的。即便可行，在实践持续部署时，尤其是高频发布且更改范围较小的情况下，前进通常是更好的选择。
- en: I did my best to discourage you from rolling back. Still, in some cases that
    is a better option. In others, that might be the only option. Luckily, rolling
    back is reasonably straightforward with Kubernetes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经尽力劝阻你不要回滚，但在某些情况下回滚是更好的选择。在其他情况下，它可能是唯一的选择。幸运的是，使用Kubernetes回滚相对来说是非常直接的。
- en: 'We''ll imagine that we just discovered that the latest release of the `vfarcic/go-demo-2`
    image is faulty and that we should roll back to the previous release. The command
    that will do just that is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设刚刚发现最新发布的`vfarcic/go-demo-2`镜像存在问题，需要回滚到先前的版本。执行这一操作的命令如下：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output of the latter command, limited to the last lines, is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 后者命令的输出，仅限最后几行，结果如下：
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see from the events section that the Deployment initiated rollback and,
    from there on, the process we experienced before was reversed. It started increasing
    the replicas of the older ReplicaSet, and decreasing those from the latest one.
    Once the process is finished, the older ReplicaSet became active with all the
    replicas, and the newer one was scaled down to zero.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从事件部分可以看到，Deployment启动了回滚操作，从那里开始，我们之前经历的过程被逆转了。它开始增加旧ReplicaSet的副本数，并减少最新ReplicaSet的副本数。一旦该过程完成，旧的ReplicaSet变为活动状态并拥有所有副本，而新的ReplicaSet则被缩放为零。
- en: The end result might be easier to see from the `NewReplicaSet` entry located
    just above `Events`. Before we undid the rollout, the value was `go-demo-2-api-68c75f4f5`,
    and now it's `go-demo-2-api-68df567fb5`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果可能更容易通过位于`Events`上方的`NewReplicaSet`条目看到。在我们撤销部署之前，值是`go-demo-2-api-68c75f4f5`，现在是`go-demo-2-api-68df567fb5`。
- en: Knowing only the current state of the latest Deployment is often insufficient,
    and we might need a list of the past rollouts. We can get it with the `kubectl
    rollout history` command.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 仅了解最新Deployment的当前状态通常是不够的，我们可能需要获取过去的发布列表。可以通过`kubectl rollout history`命令来获取。
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you look at the third revision, you'll notice that the change cause is the
    same command we used to create the Deployment the first time. Before we executed
    `kubectl rollout undo`, we had two revisions; `1` and `2`. The `undo` command
    checked the second-to-last revision (`1`). Since new deployments do no destroy
    ReplicaSets but scale them to `0`, all it had to do to undo the last change was
    to scale it back to the desired number of replicas and, at the same time, scale
    the current one to zero.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看第三次修订，你会注意到变更原因是我们第一次创建Deployment时使用的相同命令。在执行`kubectl rollout undo`之前，我们有两个修订版本；`1`和`2`。`undo`命令检查了倒数第二个修订版本（`1`）。由于新的部署不会删除ReplicaSets，而是将其缩放到`0`，因此撤销最后一次更改所需做的只是将其缩放回所需的副本数，并同时将当前的副本数缩放为零。
- en: Let's fast track a bit and deploy a few new releases. That will provide us with
    a broader playground to explore a few additional things we can do with Deployments.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加快进度，部署几个新版本。这样可以为我们提供更广阔的探索空间，以尝试一些我们可以在Deployments中做的额外操作。
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We updated the image to `vfarcic/go-demo-2:3.0` and retrieved the rollout status.
    The last line of the latter command is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将镜像更新为`vfarcic/go-demo-2:3.0`并检索了发布状态。后者命令的最后一行如下：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The deployment was successfully updated and, as a result, it created a new ReplicaSet
    and scaled it up to the desired number of replicas. The previously active ReplicaSet
    was scaled to `0`. As a result, we're running tag `3.0` of the `vfarcic/go-demo-2`
    image.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll repeat the process with the tag `4.0`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output of the last line of the `rollout status` confirmed that the rollout
    was successful.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we deployed a few releases, we can check the current `rollout history`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can clearly see the commands that produced the changes and, through them,
    how our application progressed all the way until the current release based on
    the image `vfarcic/go-demo-2:4.0`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: You saw that we can rollback to the previous release through the `kubectl rollout
    undo` command. In most cases, that should be the correct action when faced with
    problems and without the ability to roll forward by creating a new release with
    the fix. However, sometimes even that is not enough, and we have to go back in
    time further than the previous release.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we discovered not only that the current release is faulty but
    also that a few before it have bugs as well. Following the same narrative, we''ll
    imagine that the last correct release was based on the image `vfarcic/go-demo-2:2.0`.
    We can remedy that by executing the command that follows (**please do NOT run
    it**):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'While that command would certainly fix the problem, there is an easier way
    to accomplish the same result. We can `undo` the `rollout` by moving to the last
    revision that worked correctly. Assuming that we want to revert to the image `vfarcic/go-demo-2:2.0`,
    reviewing the change causes listed in the history tells us we should roll back
    to revision `2`. That can be accomplished through the `--to-revision` argument.
    The command is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We undid the rollout by moving to revision `2`. We also retrieved the `history`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the latter command is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Through the new revision `6`, we can see that the currently active Deployment
    is based on the image `vfarcic/go-demo-2:2.0`. We successfully moved back to the
    specific point in time. The problem is solved and, if this was the "real" application
    running in a production cluster, our users would continue interacting with the
    version of our software that actually works.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back failed Deployments
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discovering a critical bug is probably the most common reason for a rollback.
    Still, there are others. For example, we might be in a situation when Pods cannot
    be created. An easy to reproduce case would be an attempt to deploy an image with
    a tag that does not exist.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: After seeing such a message, you might be under the impression that everything
    is OK. However, that output only indicates that the definition of the image used
    in the Deployment was successfully updated. That does not mean that the Pods behind
    the ReplicaSet are indeed running. For one, I can assure you that the `vfarcic/go-demo-2:does-not-exist`
    image does not exist.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure that at least `60` seconds have passed since you executed the
    `kubectl set image` command. If you're wondering why we are waiting, the answer
    lies in the `progressDeadlineSeconds` field set in the `go-demo-2-api` Deployment
    definition. That's how much the Deployment has to wait before it deduces that
    it cannot progress due to a failure to run a Pod.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the ReplicaSets.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: By now, under different circumstances, all the Pods from the new ReplicaSet
    (`go-demo-2-api-dc7877dcd`) should be set to `3`, and the Pods of the previous
    one (`go-demo-2-api-68c75f4f5`) should have been scaled down to `0`. However,
    the Deployment noticed that there is a problem and stopped the update process.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'We should be able to get more detailed information with the `kubectl rollout
    status` command:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The Deployment realized that it shouldn't proceed. The new Pods are not running,
    and the limit was reached. There's no point to continue trying.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'If you expected that the Deployment would roll back after it failed, you''re
    wrong. It will not do such a thing. At least, not without additional add-ons.
    That does not mean that I would expect you to sit in front of your terminal, wait
    for timeouts, and check the `rollout status` before deciding whether to keep the
    new update or to roll back. I expect you to deploy new releases as part of your
    automated CDP pipeline. Fortunately, the `status` command returns `1` if the deployment
    failed and we can use that information to decide what to do next. For those of
    you not living and breathing Linux, any exit code different than `0` is considered
    an error. Let''s confirm that by checking the exit code of the last command:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The output is indeed `1`, thus confirming that the rollout failed.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore automated CDP pipeline soon. For now, just remember that we can
    find out whether Deployment updates were successful or not.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Now that we discovered that our last rollout failed, we should undo it. You
    already know how to do that, but I'll remind you just in case you're of a forgetful
    nature.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output of the last command confirmed that `deployment "go-demo-2-api"` was
    `successfully rolled out`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to rollback no matter whether the problem is a
    critical bug or inability to run the new release, we can take a short pause from
    learning new stuff and merge all the definitions we explored thus far into a single
    YAML file. But, before we do that, we'll remove the objects we created.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Merging everything into the same YAML definition
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider this section a short intermezzo. We'll merge the definitions we used
    in this chapter into a single YAML file. You already had a similar example before,
    so there's no need for lengthy explanations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: If you start searching for differences with the previous definitions, you will
    find a few. The `minReadySeconds`, `progressDeadlineSeconds`, `revisionHistoryLimit`,
    and `strategy` fields are removed from the `go-demo-2-api` Deployment. We used
    them mostly as a way to demonstrate their usage. But, since Kubernetes has sensible
    defaults, we omitted them from this definition. You'll also notice that there
    are two Services even though we created only one in this chapter. We did not need
    the `go-demo-2-api` Service in our examples since we didn't need to access the
    API. But, for the sake of completeness, it is included in this definition. Finally,
    the strategy for deploying the database is set to `recreate`. As explained earlier,
    it is more suited for a single-replica database, even though we did not mount
    a volume that would preserve the data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Let's create the objects defined in `deploy/go-demo-2.yml`. Remember, with `--save-config`
    we're making sure we can edit the configuration later. The alternative would be
    to use `kubectl apply` instead.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output of the latter command is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: All four objects (two Deployments and two Services) were created, and we can
    move on and explore ways to update multiple objects with a single command.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Updating multiple objects
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though most of the time we send requests to specific objects, almost everything
    is happening using selector labels. When we updated the Deployments, they looked
    for matching selectors to choose which ReplicaSets to create and scale. They,
    in turn, created or terminated Pods also using the matching selectors. Almost
    everything in Kubernetes is operated using label selectors. It's just that sometimes
    that is obscured from us.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We do not have to update an object only by specifying its name or the YAML file
    where its definition resides. We can also use labels to decide which object should
    be updated. That opens some interesting possibilities since the selectors might
    match multiple objects.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we are running several Deployments with Mongo databases and that
    the time has come to update them all to a newer release. Before we explore how
    we could do that, we'll create another Deployment so that we have at least two
    with the database Pods.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us first take a look at the definition:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: When compared with the `go-demo-2-db` Deployment, the only difference is in
    the `service` label. Both have the `type` set to `db`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the deployment:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now that we have two deployments with the `mongo:3.3` Pods, we can try to update
    them both at the same time.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to find a label (or a set of labels) that uniquely identifies all
    the Deployments we want to update.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the list of Deployments with their labels:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We want to update `mongo` Pods created using `different-app-db` and `go-demo-2-db`
    Deployments. Both are uniquely identified with the labels `type=db` and `vendor=MongoLabs`.
    Let''s test that:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can see that filtering with those two labels worked. We retrieved only the
    Deployments we want to update, so let''s proceed and roll out the new release:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, before we move into the next subject, we should validate that the
    image indeed changed to `mongo:3.4`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As we can see, the update was indeed successful, at least with that Deployment.
    Feel free to describe the Deployment defined in `deploy/different-app-db.yml`.
    You should see that its image was also updated to the newer version.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Deployments
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are quite a few different ways we can scale Deployments. Everything we
    do in this section is not unique to Deployments and can be applied to any Controller,
    like ReplicaSet, and those we did not yet explore.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: If we decide that the number of replicas changes with relatively low frequency
    or that Deployments are performed manually, the best way to scale is to write
    a new YAML file or, even better, modify the existing one. Assuming that we store
    YAML files in a code repository, by updating existing files we have a documented
    and reproducible definition of the objects running inside a cluster.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: We already performed scaling when we applied the definition from the `go-demo-2-scaled.yml`.
    We'll do something similar, but with Deployments.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at `deploy/go-demo-2-scaled.yml`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We won't display the contents of the whole file since it is almost identical
    to `deploy/go-demo-2.yml`. The only difference is the number of replicas of the
    `go-demo-2-api` Deployment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: At the moment, we're running three replicas. Once we apply the new definition,
    it should increase to five.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Please note that, even though the file is different, the names of the resources
    are the same so `kubectl apply` did not create new objects. Instead, it updated
    those that changed. In particular, it changed the number of replicas of the `go-demo-2-api`
    Deployment.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Let's confirm that there are indeed five replicas of the Pods controlled through
    the Deployment.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output, limited to the `deploy/go-demo-2-api`, is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The result should come as no surprise. After all, we executed the same process
    before, when we explored ReplicaSets.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: While scaling Deployments using YAML files (or other Controllers) is an excellent
    way to keep documentation accurate, it rarely fits the dynamic nature of the clusters.
    We should aim for a system that will scale (and de-scale) services automatically.
    When scaling is frequent and, hopefully, automated, we cannot expect to update
    YAML definitions and push them to Git. That would be too inefficient and would
    probably cause quite a few unwanted executions of delivery pipelines if they are
    triggered through repository Webhooks. After all, do we really want to push updated
    YAML files multiple times a day?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The number of `replicas` should not be part of the design. Instead, they are
    a fluctuating number that changes continuously (or at least often), depending
    on the traffic, memory and CPU utilization, and so on.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Depending on release frequency, the same can be said for `image`. If we are
    practicing continuous delivery or deployment, we might be releasing once a week,
    once a day, or even more often. In such cases, new images would be deployed often,
    and there is no strong argument for the need to change YAML files every time we
    make a new release. That is especially true if we are deploying through an automated
    process (as we should).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore automation later on. For now, we'll limit ourselves to a command
    similar to `kubectl set image`. We used it to change the `image` used by Pods
    with each release. Similarly, we'll use `kubectl scale` to change the number of
    replicas. Consider this an introduction to automation that is coming later on.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We scaled the number of replicas associated with the Deployment `go-demo-2-api`.
    Please note that, this time, we did not use `-f` to reference a file. Since we
    have two Deployments specified in the same YAML, that would result in scaling
    of both. Since we wanted to limit it to a particular Deployment, we used its name
    instead.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Let's confirm that scaling indeed worked as expected.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output, limited to Deployments, is as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: As I mentioned earlier, we'll dedicate quite a lot of time to automation, and
    you won't have to scale your applications manually. However, I thought that it
    is useful to know that the `kubectl scale` command exists. For now, you know how
    to scale Deployments (and other Controllers).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything we learned led us to Deployments. Pods must not be created directly,
    but through ReplicaSets which, similarly, must not be created directly, but through
    Deployments. They are the objects that allow us not only to create the ReplicaSets
    and Pods, but that can also be updated without producing any downtime (when applications
    are designed accordingly). We combined Deployments with Services so that Pods
    can communicate with each other, or can be accessed from outside a cluster. All
    in all, we have everything we need to release our services to production. That
    is not to say that we understand all the crucial aspects of Kubernetes. We're
    not even close to that point. But, we do have almost everything we need for running
    some types of applications in production. What we're missing is networking.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Before we enter the next stage of our knowledge seeking mission, we'll destroy
    the cluster we're running and give our laptops a break.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: If you'd like to know more about Deployments, please explore Deployment v1 apps
    ([https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#deployment-v1-apps](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#deployment-v1-apps))
    API documentation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daf85690-4328-4ab1-9a11-5a87a9e5d3a0.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4: The components explored so far'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Before we move onto the next chapter, we'll explore the differences between
    Kubernetes Deployments and Docker Swarm stacks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Deployments compared to Docker Swarm stacks
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have already used Docker Swarm, the logic behind Kubernetes Deployments
    should be familiar. Both serve the same purpose and can be used to deploy new
    applications or update those that are already running inside a cluster. In both
    cases, we can easily deploy new releases without any downtime (when application
    architecture permits that).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the previous comparison between Kubernetes Pods, ReplicaSets, and Services
    with Docker Swarm Stacks, Kubernetes Deployments do provide a few potentially
    important functional differences. But, before we dive into the functional comparison,
    we'll take a moment to explore differences in how we define objects.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'An example Kubernetes Deployment and Service definition is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'An equivalent Docker Swarm stack definition is as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Both definitions provide, more or less, the same functionality.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: It is evident that a Kubernetes Deployment requires a much longer definition
    with more a complex syntax. It is worth noting that Swarm's equivalent to `readinessProbe`
    and `livenessProbe` is not present in the stack because it is defined as a `HEALTHCHECK`
    inside the Dockerfile. Still, even if we remove them, a Kubernetes Deployment
    remains longer and more complicated.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: When comparing only the differences in the ways to define objects, Docker Swarm
    is a clear **winner**. Let's see what we can conclude from the functional point
    of view.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Creating the objects is reasonably straight-forward. Both `kubectl create` and
    `docker stack deploy` will deploy new releases without any downtime. New containers
    or, in case of Kubernetes, Pods will be created and, in parallel, the old ones
    will be terminated. So far, both solutions are, more or less, the same.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: One of the main differences is what happens in case of a failure. A Kubernetes
    Deployment will not perform any corrective action in case of a failure. It will
    stop the update, leaving a combination of new and old Pods running in parallel.
    Docker Swarm, on the other hand, can be configured to rollback automatically.
    That might seem like another win for Docker Swarm. However, Kubernetes has something
    Swarm doesn't. We can use `kubectl rollout status` command to find out whether
    the update was a success or failure and, in case of the latter, we can `undo`
    the `rollout`. Even though we need a few commands to accomplish the same result,
    that might fare better when updates are automated. Knowing whether an update succeeded
    or failed allows us to not only execute a subsequent rollback action but also
    notify someone that there is a problem.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Both approaches have their pros and cons. Docker Swarm's automated rollback
    is better suited in some cases, and Kubernetes update status works better in others.
    The methods are different, and there is no clear winner, so I'll proclaim it a
    tie.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Deployments can record history. We can use the `kubectl rollout history`
    command to inspect past rollout. When updates are working as expected, `history`
    is not very useful. But, when things go wrong, it might provide additional insight.
    That can be combined with the ability to rollback to a specific revision, not
    necessarily the previous one. However, most of the time, we rollback to the previous
    version. The ability to go back further in time is not very useful. Even when
    such a need arises, both products can do that. The difference is that Kubernetes
    Deployments allow us to go to a specific revision (for example, we're on the revision
    five, rollback to the revision two). With Docker Swarm, we'd have to issue a new
    update (for example, update the image to the tag 2.0). Since containers are immutable,
    the result is the same, so the difference is only in the syntax behind a rollback.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: The ability to rollback to a specific version or a tag exists in both products.
    We can argue which syntax is more straightforward or more useful. The differences
    are minor, and I'll proclaim that there is no winner for that functionality. It's
    another tie.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Since almost everything in Kubernetes is based on label selectors, it has a
    feature that Docker Swarm doesn't. We can update multiple Deployments at the same
    time. We can, for example, issue an update (`kubetl set image`) that uses filters
    to find all Mongo databases and upgrade them to a newer release. It is a feature
    that would require a few lines of bash scripting with Docker Swarm. However, while
    the ability to update all Deployments that match specific labels might sound like
    a useful feature, it often isn't. More often than not, such actions can produce
    undesirable effects. If, for example, we have five back-end applications that
    use Mongo database (one for each), we'd probably want to upgrade them in a more
    controlled fashion. Teams behind those services would probably want to test each
    of those upgrades and give their blessings. We probably wouldn't wait until all
    are finished, but upgrade a single database when the team in charge of it feels
    confident.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: There are the cases when updating multiple objects is useful so I must give
    this one to Kubernetes. It a minor win, but it still counts.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: There are a few other things that are easier to accomplish with Kubernetes.
    For example, due to the way Kubernetes Services work, creating a blue-green deployment
    process, instead of using rolling updates, is much easier. However, such a process
    falls into advanced usage so I'll leave it out of this comparison. It'll (probably)
    come later.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: It's difficult to say which solution provides better results. Docker Swarm continues
    to shine from the user-friendliness perspective. On the other hand, Kubernetes
    Deployments offer a few additional features.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: It is much simpler and easier to write a Docker Swarm stack file than a Kubernetes
    Deployment definition. Kubernetes Deployments offer a few additional functional
    features that Swarm does not have. However, those features are, for most use cases,
    of minor importance. Those that indeed matter are, more or less, the same.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Don't make a decision based on the differences between Kubernetes Deployments
    and Docker Swarm stacks. Definition syntax is where Swarm has a clear win, while
    on the functional front Kubernetes has a tiny edge over Swarm. If you'd make a
    decision only based on deployments, Swarm might be a slightly better candidate.
    Or not. It all depends on what matters more in your case. Do you care about YAML
    syntax? Are those additional Kubernetes Deployment features something you will
    ever use?
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: In any case, Kubernetes has much more to offer, and any conclusion based on
    such a limited comparison scope is bound to be incomplete. We only scratched the
    surface. Stay tuned for more.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
