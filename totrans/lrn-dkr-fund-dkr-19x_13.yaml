- en: Single-Host Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the most important architectural patterns
    and best practices that are used when dealing with a distributed application architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the Docker container networking model and
    its single-host implementation in the form of the bridge network. This chapter
    also introduces the concept of software-defined networks and how they are used
    to secure containerized applications. Furthermore, we will demonstrate how container
    ports can be opened to the public and thus make containerized components accessible
    to the outside world. Finally, we will introduce Traefik, a reverse proxy, which
    can be used to enable sophisticated HTTP application-level routing between containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting the container network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network firewalling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The host and null network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running in an existing network namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing container ports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP-level routing using a reverse proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After completing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create, inspect, and delete a custom bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a container attached to a custom bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolate containers from each other by running them on different bridge networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish a container port to a host port of your choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add Traefik as a reverse proxy to enable application-level routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, the only thing you will need is a Docker host that is able
    to run Linux containers. You can use your laptop with either Docker for macOS
    or Windows or have Docker Toolbox installed.
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting the container network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have been mostly working with single containers. But in reality,
    a containerized business application consists of several containers that need
    to collaborate to achieve a goal. Therefore, we need a way for individual containers
    to communicate with each other. This is achieved by establishing pathways that
    we can use to send data packets back and forth between containers. These pathways are
    called **networks**. Docker has defined a very simple networking model, the so-called **container
    network model** (**CNM**), to specify the requirements that any software that
    implements a container network has to fulfill. The following is a graphical representation
    of the CNM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01bba337-0560-4f72-a31e-7e51d7cb98ac.png)'
  prefs: []
  type: TYPE_IMG
- en: The Docker CNM
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNM has three elements – sandbox, endpoint, and network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sandbox:** The sandbox perfectly isolates a container from the outside world.
    No inbound network connection is allowed into the sandboxed container. But, it
    is very unlikely that a container will be of any value in a system if absolutely
    no communication with it is possible. To work around this, we have element number
    two, which is the endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoint:** An endpoint is a controlled gateway from the outside world into
    the network''s sandbox that shields the container. The endpoint connects the network
    sandbox (but not the container) to the third element of the model, which is the
    network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network:** The network is the pathway that transports the data packets of
    an instance of communication from endpoint to endpoint or, ultimately, from container
    to container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that a network sandbox can have zero to many endpoints,
    or, said differently, each container living in a network sandbox can either be attached
    to no network at all or it can be attached to multiple different networks at the
    same time. In the preceding diagram, the middle of the three **Network Sandboxes**
    is attached to both **Network** **1** and **Network** **2** through an **endpoint**.
  prefs: []
  type: TYPE_NORMAL
- en: This networking model is very generic and does not specify where the individual
    containers that communicate with each other over a network run. All containers
    could, for example, run on one and the same host (local) or they could be distributed
    across a cluster of hosts (global).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the CNM is just a model describing how networking works among containers.
    To be able to use networking with our containers, we need real implementations
    of the CNM. For both local and global scope, we have multiple implementations
    of the CNM. In the following table, we''ve given a short overview of the existing
    implementations and their main characteristics. The list is in no particular order:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Network** | **Company** | **Scope** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Bridge | Docker | Local | Simple network based on Linux bridges to allow
    networking on a single host |'
  prefs: []
  type: TYPE_TB
- en: '| Macvlan | Docker | Local | Configures multiple layer 2 (that is, MAC) addresses
    on a single physical host interface |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay | Docker | Global | Multinode-capable container network based on **Virtual
    Extensible LAN** (**VXLan**) |'
  prefs: []
  type: TYPE_TB
- en: '| Weave Net | Weaveworks | Global | Simple, resilient, multi-host Docker networking
    |'
  prefs: []
  type: TYPE_TB
- en: '| Contiv Network Plugin | Cisco | Global | Open source container networking
    |'
  prefs: []
  type: TYPE_TB
- en: All network types not directly provided by Docker can be added to a Docker host
    as a plugin.
  prefs: []
  type: TYPE_NORMAL
- en: Network firewalling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker has always had the mantra of security first. This philosophy had a direct
    influence on how networking in a single and multi-host Docker environment was
    designed and implemented. Software-defined networks are easy and cheap to create,
    yet they perfectly firewall containers that are attached to this network from
    other non-attached containers, and from the outside world. All containers that
    belong to the same network can freely communicate with each other, while others
    have no means to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we have two networks called **front** and **back**.
    Attached to the front network, we have containers **c1** and **c2**, and attached
    to the back network, we have containers **c3** and **c4**. **c1** and **c2** can
    freely communicate with each other, as can **c3** and **c4**. But **c1** and **c2** have
    no way to communicate with either **c3** or **c4**, and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc455aaa-c719-42f7-bbe6-9183e1cc50e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker networks
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what about the situation where we have an application consisting of three
    services: **webAPI**, **productCatalog**, and **database**? We want **webAPI **to
    be able to communicate with **productCatalog**, but not with the **database**, and
    we want **productCatalog **to be able to communicate with the **database **service.
    We can solve this situation by placing **webAPI** and the database on different
    networks and attaching **productCatalog** to both of these networks, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86503975-ece8-4312-b380-9454f4494e0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Container attached to multiple networks
  prefs: []
  type: TYPE_NORMAL
- en: Since creating SDNs is cheap, and each network provides added security by isolating
    resources from unauthorized access, it is highly recommended that you design and
    run applications so that they use multiple networks and only run services on the
    same network that absolutely need to communicate with each other. In the preceding
    example, there is absolutely no need for the **webAPI** component to ever communicate
    directly with the **database** service, so we have put them on different networks.
    If the worst-case scenario happens and a hacker compromises the **webAPI**, they
    cannot access the **database** from there without also hacking the **productCatalog**
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the bridge network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker bridge network is the first implementation of the container network
    model that we're going to look at in detail. This network implementation is based
    on the Linux bridge. When the Docker daemon runs for the first time, it creates
    a Linux bridge and calls it `docker0`. This is the default behavior and can be
    changed by changing the configuration. Docker then creates a network with this
    Linux bridge and calls the network `bridge`. All the containers that we create
    on a Docker host and that we do not explicitly bind to another network leads to
    Docker automatically attaching to this bridge network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that we indeed have a network called `bridge` of the `bridge` type defined
    on our host, we can list all the networks on the host with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This should provide an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/593ac4e8-8745-4f2b-8e50-ed4c631a97ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing all the Docker networks available by default
  prefs: []
  type: TYPE_NORMAL
- en: In your case, the IDs will be different, but the rest of the output should look
    the same. We do indeed have a first network called `bridge` using the `bridge` driver.
    The scope being `local` just means that this type of network is restricted to
    a single host and cannot span multiple hosts. In [Chapter 13](a6f04592-db31-452a-aad1-ca56d9999767.xhtml),
    *Introduction to Docker Swarm*, we will also discuss other types of networks that
    have a global scope, meaning they can span whole clusters of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look a little bit deeper into what this bridge network is all about.
    For this, we are going to use the Docker `inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When executed, this outputs a big chunk of detailed information about the network
    in question. This information should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/443fa38e-8fb7-4a8b-a781-08bd8b40bdc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Output generated when inspecting the Docker bridge network
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw the `ID`, `Name`, `Driver`, and `Scope` values when we listed all the
    networks, so that is nothing new. But let''s have a look at the **IP address management** (**IPAM**)
    block. IPAM is a piece of software that is used to track IP addresses that are
    used on a computer. The important part of the `IPAM` block is the `Config` node
    with its values for `Subnet` and `Gateway`. The subnet for the bridge network
    is defined by default as `172.17.0.0/16`. This means that all containers attached
    to this network will get an IP address assigned by Docker that is taken from the
    given range, which  is `172.17.0.2` to `172.17.255.255`. The `172.17.0.1` address is
    reserved for the router of this network whose role in this type of network is
    taken by the Linux bridge. We can expect that the very first container that will
    be attached to this network by Docker will get the `172.17.0.2 `address. All subsequent
    containers will get a higher number; the following diagram illustrates this fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/305f89fe-d167-4b3b-bb00-8f6bd3b8e800.png)'
  prefs: []
  type: TYPE_IMG
- en: The bridge network
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see the network namespace of the host, which
    includes the host's **eth0** endpoint, which is typically a NIC if the Docker
    host runs on bare metal or a virtual NIC if the Docker host is a VM. All traffic
    to the host comes through **eth0**. The **Linux** **bridge** is responsible for
    routing the network traffic between the host's network and the subnet of the bridge
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, only egress traffic is allowed, and all ingress is blocked. What
    this means is that while containerized applications can reach the internet, they
    cannot be reached by any outside traffic. Each container attached to the network
    gets its own **virtual ethernet** (**veth**) connection with the bridge. This
    is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4b34b80-5e37-4257-8a0e-c79110390eda.png)'
  prefs: []
  type: TYPE_IMG
- en: Details of the bridge network
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows us the world from the perspective of the **Host**.
    We will explore what this situation looks like from within a container later on
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not limited to just the bridge network, as Docker allows us to define
    our own custom bridge networks. This is not just a feature that is nice to have, but
    it is a recommended best practice to not run all containers on the same network.
    Instead, we should use additional bridge networks to further isolate containers
    that have no need to communicate with each other. To create a custom bridge network
    called `sample-net`, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do this, we can then inspect what subnet Docker has created for this
    new custom network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Evidently, Docker has just assigned the next free block of IP addresses to
    our new custom bridge network. If, for some reason, we want to specify our own
    subnet range when creating a network, we can do so by using the `--subnet` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To avoid conflicts due to duplicate IP addresses, make sure you avoid creating
    networks with overlapping subnets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed what a bridge network is and how we can create a
    custom bridge network, we want to understand how we can attach containers to these networks.
    First, let''s interactively run an Alpine container without specifying the network
    to be attached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In another Terminal window, let''s inspect the `c1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the vast output, let''s concentrate for a moment on the part that provides
    network-related information. This can be found under the `NetworkSettings` node.
    I have it listed in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa6b5fcb-a394-4fa6-85bf-fbdced83fdbe.png)'
  prefs: []
  type: TYPE_IMG
- en: The NetworkSettings section of the container metadata
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see that the container is indeed attached to
    the `bridge` network since `NetworkID` is equal to `026e65...`, which we can see
    from the preceding code is the ID of the `bridge` network. We can also see that
    the container got the IP address of `172.17.0.4` assigned as expected and that
    the gateway is at `172.17.0.1`. Please note that the container also had a `MacAddress` associated
    with it. This is important as the Linux bridge uses the `MacAddress` for routing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have approached this from the outside of the container''s network
    namespace. Now, let''s see what the situation looks like when we''re not only
    inside the container but inside the containers'' network namespace. Inside the `c1` container, let''s
    use the `ip` tool to inspect what''s going on. Run the `ip addr` command and observe
    the output that is generated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27033243-df1d-4f06-bc3f-e75f79595799.png)'
  prefs: []
  type: TYPE_IMG
- en: Container namespace, as seen by the IP tool
  prefs: []
  type: TYPE_NORMAL
- en: The interesting part of the preceding output is number `19`, that is, the `eth0` endpoint.
    The `veth0` endpoint that the Linux bridge created outside of the container namespace
    is mapped to `eth0` inside the container. Docker always maps the first endpoint
    of a container network namespace to `eth0`, as seen from inside the namespace.
    If the network namespace is attached to an additional network, then that endpoint
    will be mapped to `eth1`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since at this point we''re not really interested in any endpoint other than `eth0`,
    we could have used a more specific variant of the command, which would have given
    us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the output, we can also see what MAC address (`02:42:ac:11:00:02`) and what
    IP (`172.17.0.2`) have been associated with this container network namespace by
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get some information about how requests are routed by using the `ip
    route` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This output tells us that all the traffic to the gateway at `172.17.0.1` is
    routed through the `eth0` device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run another container called `c2` on the same network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `c2` container will also be attached to the `bridge` network since we have
    not specified any other network. Its IP address will be the next free one from
    the subnet, which is `172.17.0.3`, as we can readily test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have two containers attached to the `bridge` network. We can try to
    inspect this network once again to find a list of all containers attached to it
    in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This information can be found under the `Containers` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86ee4f8a-562b-48c9-a5f5-f592fd090036.png)'
  prefs: []
  type: TYPE_IMG
- en: The Containers section of the output of the Docker network inspect bridge
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we have shortened the output to the relevant part for readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create two additional containers, `c3` and `c4`, and attach them
    to `test-net`. For this, we''ll use the `--network` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s inspect `network test-net` and confirm that containers `c3` and `c4` are
    indeed attached to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output for the `Containers` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ae3275a-6e10-49e8-8c51-59287cb8c746.png)'
  prefs: []
  type: TYPE_IMG
- en: Containers section of the docker network inspect test-net command
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question we''re going to ask ourselves is whether the `c3` and `c4` containers can
    freely communicate with each other. To demonstrate that this is indeed the case,
    we can `exec` into the `c3` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once inside the container, we can try to ping container `c4` by name and by
    IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of the ping using the IP address of `c4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The answer in both cases confirms to us that the communication between containers
    attached to the same network is working as expected. The fact that we can even use the
    name of the container we want to connect to shows us that the name resolution
    provided by the Docker DNS service works inside this network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to make sure that the `bridge` and the `test-net` networks are
    firewalled from each other. To demonstrate this, we can try to ping the `c2` container from
    the `c3` container, either by its name or by its IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of the ping using the IP address of the `c2` container instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command remained hanging and I had to terminate the command with *Ctrl*+*C*.
    From the output of pinging `c2`, we can also see that the name resolution does
    not work across networks. This is the expected behavior. Networks provide an extra
    layer of isolation, and thus security, to containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we learned that a container can be attached to multiple networks.
    Let''s attach the `c5` container to the `sample-net` and `test-net` networks at
    the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can test that `c5` is reachable from the `c2` container, similar to
    when we tested the same for the `c4` and `c2` containers. The result will show
    that the connection indeed works.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to remove an existing network, we can use the `docker network rm` command,
    but note that we cannot accidentally delete a network that has containers attached
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we continue, let''s clean up and remove all the containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can remove the two custom networks that we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we could remove all the networks that no container is attached
    to with the `prune` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: I used the `--force` (or `-f`) argument here to prevent Docker from reconfirming
    that I really want to remove all unused networks.
  prefs: []
  type: TYPE_NORMAL
- en: The host and null network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to look at two predefined and somewhat unique
    types of networks, the `host` and the `null` networks. Let's start with the former.
  prefs: []
  type: TYPE_NORMAL
- en: The host network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are occasions where we want to run a container in the network namespace
    of the host. This can be necessary when we need to run some software in a container
    that is used to analyze or debug the host networks' traffic. But keep in mind
    that these are very specific scenarios. When running business software in containers,
    there is no good reason to ever run the respective containers attached to the
    host's network. For security reasons, it is strongly recommended that you do not
    run any such container attached to the `host` network on a production or production-like
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, *how can we run a container inside the network namespace of the
    host?* Simply by attaching the container to the `host` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use the `ip` tool to analyze the network namespace from within the container,
    we will see that we get exactly the same picture as we would if we were running
    the `ip` tool directly on the host. For example, if I inspect the `eth0` device on
    my host, I get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, I can see that `192.168.65.3` is the IP address that the host has been
    assigned and that the MAC address shown here also corresponds to that of the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also inspect the routes to get the following (shortened):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Before I let you go on to the next section of this chapter, I want to once more
    point out that the use of the `host` network is dangerous and needs to be avoided
    if possible.
  prefs: []
  type: TYPE_NORMAL
- en: The null network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, we need to run a few application services or jobs that do not need
    any network connection at all to execute the task at hand. It is strongly advised
    that you run those applications in a container that is attached to the `none` network.
    This container will be completely isolated, and is thus safe from any outside
    access. Let''s run such a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once inside the container, we can verify that there is no `eth0` network endpoint
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also no routing information available, as we can demonstrate by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This returns nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Running in an existing network namespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, Docker creates a new network namespace for each container we run.
    The network namespace of the container corresponds to the sandbox of the container
    network model we described earlier on. As we attach the container to a network,
    we define an endpoint that connects the container network namespace with the actual
    network. This way, we have one container per network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker provides an additional way for us to define the network namespace that
    a container runs in. When creating a new container, we can specify that it should
    be attached to (or maybe we should say included) in the network namespace of an
    existing container. With this technique, we can run multiple containers in a single
    network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fab345c0-6d16-4d82-ab56-ed17edd9cb1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple containers running in a single network namespace
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see that in the leftmost **Network** **Namespace**,
    we have two containers. The two containers, since they share the same namespace,
    can communicate on localhost with each other. The network namespace (and not the
    individual containers) is then attached to **Network 1**.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when we want to debug the network of an existing container without
    running additional processes inside that container. We can just attach a special
    utility container to the network namespace of the container to inspect. This feature
    is also used by Kubernetes when it creates a pod. We will learn more about Kubernetes
    and pods in [Chapter 15](b8e4dc09-b2ce-4f89-9682-d8f0c6e126f6.xhtml), *Introduction
    to Kubernetes* of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s demonstrate how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new bridge network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run a container attached to this network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we run another container and attach it to the network of our `web` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Specifically, note how we define the network: `--network container:web`. This
    tells Docker that our new container shall use the same network namespace as the
    container called `web`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the new container is in the same network namespace as the web container
    running nginx, we''re now able to access nginx on localhost! We can prove this
    by using the `wget` tool, which is part of the Alpine container, to connect to
    nginx. We should see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have shortened the output for readability. Please also note that
    there is an important difference between running two containers attached to the
    same network and two containers running in the same network namespace. In both
    cases, the containers can freely communicate with each other, but in the latter
    case, the communication happens over localhost.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clean up the container and network, we can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we are going to learn how to expose container ports on
    the container host.
  prefs: []
  type: TYPE_NORMAL
- en: Managing container ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how we can isolate firewall containers from each other by placing
    them on different networks, and that we can have a container attached to more
    than one network, we have one problem that remains unsolved. *How can we expose
    an application service to the outside world?* Imagine a container running a web
    server hosting our webAPI from before. We want customers from the internet to
    be able to access this API. We have designed it to be a publicly accessible API.
    To achieve this, we have to, figuratively speaking, open a gate in our firewall
    through which we can funnel external traffic to our API. For security reasons,
    we don't just want to open the doors wide; we want to have a single controlled
    gate that traffic flows through.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create such a gate by mapping a container port to an available port
    on the host. We''re also calling this opening a gate to the container port to
    publish a port. Remember that the container has its own virtual network stack,
    as does the host. Therefore, container ports and host ports exist completely independently
    and by default have nothing in common at all. But we can now wire a container
    port with a free host port and funnel external traffic through this link, as illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed84f752-d199-4870-90eb-1ee9216cc749.png)'
  prefs: []
  type: TYPE_IMG
- en: Mapping container ports to host ports
  prefs: []
  type: TYPE_NORMAL
- en: 'But now, it is time to demonstrate how we can actually map a container port
    to a host port. This is done when creating a container. We have different ways
    of doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can let Docker decide which host port our container port shall be
    mapped to. Docker will then select one of the free host ports in the range of
    32xxx. This automatic mapping is done by using the `-P` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command runs an nginx server in a container. nginx is listening
    at port `80` inside the container. With the `-P` parameter, we''re telling Docker
    to map all the exposed container ports to a free port in the 32xxx range. We can
    find out which host port Docker is using by using the `docker container port` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The nginx container only exposes port `80`, and we can see that it has been
    mapped to the host port `32768`. If we open a new browser window and navigate
    to `localhost:32768`, we should see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b50f760-dad5-486e-bc18-2cec42bbaae3.png)'
  prefs: []
  type: TYPE_IMG
- en: The welcome page of nginx
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way to find out which host port Docker is using for our container
    is to inspect it. The host port is part of the `NetworkSettings` node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the third way of getting this information is to list the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Please note that in the preceding output, the `/tcp` part tells us that the
    port has been opened for communication with the TCP protocol, but not for the
    UDP protocol. TCP is the default, and if we want to specify that we want to open
    the port for UDP, then we have to specify this explicitly. `0.0.0.0` in the mapping
    tells us that traffic from any host IP address can now reach container port `80` of
    the `web` container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we want to map a container port to a very specific host port. We
    can do this by using the `-p` parameter (or `--publish`). Let''s look at how this
    is done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The value of the `-p` parameter is in the form of `<host port>:<container port>`.
    Therefore, in the preceding case, we map container port `80` to host port `8080`.
    Once the `web2` container runs, we can test it in the browser by navigating to
    `localhost:8080`, and we should be greeted by the same nginx welcome page that
    we saw in the previous example that dealt with automatic port mapping.
  prefs: []
  type: TYPE_NORMAL
- en: When using the UDP protocol for communication over a certain port, the `publish`
    parameter will look like `-p 3000:4321/udp`. Note that if we want to allow communication
    with both TCP and UDP protocols over the same port, then we have to map each protocol
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP-level routing using a reverse proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you have been tasked with containerizing a monolithic application. The
    application has organically evolved over the years into an unmaintainable monster.
    Changing even a minor feature in the source code may break other features due
    to the tight coupling existing in the code base. Releases are rare due to their
    complexity and require the whole team to be on deck. The application has to be
    taken down during the release window, which costs the company a lot of money due
    to lost opportunities, not to mention their loss of reputation.
  prefs: []
  type: TYPE_NORMAL
- en: Management has decided to put an end to that vicious cycle and improve the situation
    by containerizing the monolith. This alone will lead to a massively decreased
    time between releases as witnessed by the industry. In a later step, the company
    wants to break out every piece of functionality from the monolith and implement
    them as microservices. This process will continue until the monolith has been
    completely starved.
  prefs: []
  type: TYPE_NORMAL
- en: But it is this second point that leads to some head-scratching in the team involved.
    How will we break down the monolith into loosely coupled microservices without
    affecting all the many clients of the monolith out there? The public API of the
    monolith, though very complex, has a well-structured design. Public URIs had been
    carefully crafted and should not be changed at all costs. For example, there is
    a product catalog function implemented in the app that can be accessed via `https://acme.com/catalog?category=bicycles` so
    that we can access a list of bicycles offered by the company.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there is a URL called `https://acme.com/checkout` that we
    can use to initiate the checkout of a customers' shopping cart, and so on. I hope
    it is clear where we are going with this.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing the monolith
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with the monolith. I have prepared a simple code base that has
    been implemented in Python 2.7 and uses Flask to implement the public REST API.
    The sample app is not really a full-blown application but just complex enough
    to allow for some redesign. The sample code can be found in the `ch10/e-shop` folder.
    Inside this folder is a subfolder called `monolith` containing the Python application.
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new Terminal window, navigate to that folder, install the required dependencies,
    and run the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The application will be starting and listening on `localhost` on port `5000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0898d2ee-bb7a-47ed-a3b3-d4b30633783d.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the Python monolith
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `curl` to test the app. Use the following command to retrieve a
    list of all the bicycles the company offers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You should see a JSON formatted list of three types of bicycles. OK – so far,
    so good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s change the `hosts` file, add an entry for `acme.com`, and map it
    to `127.0.0.1`, the loop-back address. This way, we can simulate a real client
    accessing the app with the URL `http://acme.cnoteom/catalog?category=bicycles`
    instead of using `localhost`. You need to use sudo to edit the hosts file on a
    macOS or on Linux. You should add a line to the `hosts` file that looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Save your changes and assert that it works by pinging `acme.com`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/68e14aa6-b00c-441b-a1c9-bcc6d6804765.png)'
  prefs: []
  type: TYPE_IMG
- en: Mapping `acme.com` to the loop-back address via the `hosts` file
  prefs: []
  type: TYPE_NORMAL
- en: On Windows, you can edit the file by, for example, running Notepad as an administrator,
    opening the `c:\Windows\System32\Drivers\etc\hosts` file, and modifying it.
  prefs: []
  type: TYPE_NORMAL
- en: After all this, it is time to containerize the application. The only change
    we need to make in the application is ensuring that we have the application web
    server listening on `0.0.0.0` instead of `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this easily by modifying the application and adding the following
    start logic at the end of `main.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can start the application with `python main.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, add a `Dockerfile` to the `monolith` folder with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In your Terminal window, from within the monolith folder, execute the following
    command to build a Docker image for the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'After the image has been built, try to run the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the output from the app now running inside a container is indistinguishable
    from what we got when running the application directly on the host. We can now
    test if the application still works as before by using the two `curl` commands
    to access the catalog and the checkout logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb78770c-6b08-4775-aa10-bddd915b27ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing the monolith while running in a container
  prefs: []
  type: TYPE_NORMAL
- en: Evidently, the monolith still works exactly the same way as before, even when
    using the correct URL, that is, `http://acme.com`. Great! Now, let's break out
    part of the monolith's functionality into a Node.js microservice, which will be
    deployed separately.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the first microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The team, after some brainstorming, has decided that the product `catalog` is
    a good candidate for the first piece of functionality that is cohesive yet self-contained
    enough to be extracted from the monolith. They decide to implement the product
    catalog as a microservice implemented in Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code they came up with and the `Dockerfile` in the `catalog` subfolder
    of the project folder, that is, `e-shop`. It is a simple Express.js application
    that replicates the functionality that was previously available in the monolith.
    Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your Terminal window, from within the `catalog` folder, build the Docker
    image for this new microservice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run a container from the new image you just built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'From a different Terminal window, try to access the microservice and validate
    that it returns the same data as the monolith:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Please notice the differences in the URL compared to when accessing the same
    functionality in the monolith. Here, we are accessing the microservice on port
    `3000` (instead of `5000`). But we said that we didn't want to have to change
    the clients that access our e-shop application. What can we do? Luckily, there
    are solutions to problems like this. We need to reroute incoming requests. We'll
    show you how to do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Traefik to reroute traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we realized that we will have to reroute incoming traffic
    with a target URL starting with `http://acme.com:5000/catalog` to an alternative
    URL such as `product-catalog:3000/catalog`. We will be using Traefik to do exactly
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Traefik is a cloud-native edge router and it is open source, which is great
    for our specific case. It even has a nice web UI that you can use to manage and
    monitor your routes. Traefik can be combined with Docker in a very straightforward
    way, as we will see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: To integrate well with Docker, Traefik relies on metadata found on each container
    or service. This metadata can be applied in the form of labels that contain the
    routing information.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at how to run the catalog service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Docker `run` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly look at the four labels we define:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`traefik.enable=true`: This tells Traefik that this particular container should
    be included in the routing (the default is `false`).'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traefik.port=3000`: The router should forward the call to port `3000` (which
    is the port that the Express.js app is listening on).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traefik.priority=10`: Give this route high priority. We will see why in a
    second.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traefik.http.routers.catalog.rule="Host(\"acme.com\") && PathPrefix(\"/catalog\")"`:
    The route must include the hostname, `acme.com`, and the path must start with
    `/catalog` in order to be rerouted to this service. As an example, `acme.com/catalog?type=bicycles`
    would qualify for this rule.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note the special form of the fourth label. Its general form is `traefik.http.routers.<service
    name>.rule`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how we can run the `eshop` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, we forward any matching calls to port `5000`, which corresponds to the
    port where the `eshop` application is listening. Pay attention to the priority,
    which is set to `1` (low). This, in combination with the high priority of the
    `catalog` service, allows us to have all URLs starting with `/catalog` being filtered
    out and redirected to the `catalog` service, while all other URLs will go to the
    `eshop` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can finally run Traefik as the edge router that will serve as a reverse
    proxy in front of our application. This is how we start it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Note how we mount the Docker socket into the container so that Traefik can interact
    with the Docker engine. We will be able to send web traffic to port `80` of Traefik,
    from where it will be rerouted according to our rules in the routing definitions
    found in the metadata of the participating container. Furthermore, we can access
    the web UI of Traefik via port `8080`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything is running, that is, the monolith, the first microservice
    called `catalog`, and Traefik, we can test if all works as expected. Use `curl`
    once again to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned earlier, we are now sending all traffic to port `80`, which
    is what Traefik is listening on. This proxy will then reroute the traffic to the
    correct destination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding, stop all containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: That's it for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about how containers running on a single host
    can communicate with each other. First, we looked at the CNM, which defines the
    requirements of a container network, and then we investigated several implementations
    of the CNM, such as the bridge network. We then looked at how the bridge network
    functions in detail and also what kind of information Docker provides us with
    about the networks and the containers attached to those networks. We also learned
    about adopting two different perspectives, from both outside and inside the container. Last
    but not least we introduced Traefik as a means to provide application level routing
    to our applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to introduce Docker Compose. We will learn
    about creating an application that consists of multiple services, each running
    in a container, and how Docker Compose allows us to easily build, run, and scale
    such an application using a declarative approach.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess the skills that you have gained from this chapter, please try to
    answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Name the three core elements of the **container network model** (**CNM**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you create a custom bridge network called, for example, `frontend`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you run two `nginx:alpine` containers attached to the `frontend` network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the `frontend` network, get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The IPs of all the attached containers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The subnet associated with the network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the purpose of the `host` network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name one or two scenarios where the use of the `host` network is appropriate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of the `none` network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In what scenarios should the `none` network be used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would we use a reverse proxy such as Traefik together with our containerized
    application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some articles that describe the topics that were presented in this
    chapter in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking overview: [http://dockr.ly/2sXGzQn](http://dockr.ly/2sXGzQn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container networking: [http://dockr.ly/2HJfQKn](http://dockr.ly/2HJfQKn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a bridge?: [https://bit.ly/2HyC3Od](https://bit.ly/2HyC3Od)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using bridge networks: [http://dockr.ly/2BNxjRr](http://dockr.ly/2BNxjRr)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Macvlan networks: [http://dockr.ly/2ETjy2x](http://dockr.ly/2ETjy2x)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking using the host network: [http://dockr.ly/2F4aI59](http://dockr.ly/2F4aI59)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
