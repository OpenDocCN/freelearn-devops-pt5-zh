<html><head></head><body>
        

                            
                    Orchestration Using Docker Swarm
                
            
            
                
<p class="mce-root">In the previous chapter, we learned about orchestration features. In this chapter, we will build on this by learning about Docker Swarm. It comes with Docker Engine (Docker installation packages) out of the box, so we don't need to install any other software. It is simpler to master the basics of Docker Swarm compared to the other orchestrators available, and it is powerful enough for production deployments.</p>
<p>In summary, in this chapter, we will learn how to deploy Docker Swarm in production. We will also review the new objects introduced by Docker Swarm and the steps required to deploy a complete application based on containers with orchestration. Networking is key for node-distributed applications, so we will examine how Docker Swarm provides solutions for internal networking, service discovery, and publishing deployed applications. At the end of the chapter, we will review how Docker Swarm can help us upgrade our application's components without service interruption.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Deploying Docker Swarm</li>
<li>Creating a Docker Swarm cluster</li>
<li>Scheduling workloads in the cluster – tasks and services</li>
<li>Deploying applications using Stacks and other Docker Swarm resources</li>
<li>Networking in Docker Swarm</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-d4aaf0bd-2357-4bd2-8b39-1f8d268db915">Technical requirements</h1>
<p>In this chapter, we will learn about Docker Swarm's orchestrator features. We'll provide some labs at the end of this chapter that you can use to test your understanding and demonstrate the concepts you've learned. These labs can be run on your laptop or PC using the provided Vagrant "Docker Swarm" environment, or any already deployed Docker Swarm cluster of your own. Check out this book's GitHub code repository for the code we're going to be using in this chapter, along with additional information, at <a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a>.</p>
<p>Check out the following video to see the Code in Action:</p>
<p>"<a href="https://bit.ly/31wfqmu" target="_blank">https://bit.ly/31wfqmu</a>"</p>
<h1 id="uuid-16190251-7cca-4789-8c12-d84832ee36ab">Deploying Docker Swarm</h1>
<p class="mce-root">Docker Swarm is the built-in orchestrator that comes with Docker Engine out of the box. It was introduced in Docker Engine release 1.12 (the release numbers changed after 1.13 to four-digit numbers) as <em>swarm mode</em>. There was a previous swarm approach currently known as Legacy Swarm, which was closer in architecture to Kubernetes. It required an external key-value store database, among other components. Swarm mode is different from this because it includes everything needed for the orchestrator to work out of the box.</p>
<p>The Swarm architecture is quite simple as it provides secure communications between components by default. Before deploying a Docker Swarm cluster, let's review its main features:</p>
<ul>
<li><strong>Container orchestration for multiple nodes is included on each Docker Engine</strong>: This means that we can deploy a cluster without any other software. Docker Engine provides all the required components to deploy and manage the cluster out of the box.</li>
<li><strong>Node roles can be changed at runtime</strong>: Orchestration is based on different node roles. While the control plane is managed by managers or master nodes, computation or application deployment will be done on slave, worker, or minion nodes. Each orchestrator uses different names for these different roles, but they are essentially the same. Swarm allows us to change nodes from one role to another when one of them is unhealthy or when we need to do some maintenance tasks.</li>
<li><strong>Workloads will be declared as services, defining a number of instances to be healthy</strong>: The Docker orchestrator will keep the required number of replicas alive. If some of them die, the orchestrator will run new tasks to keep the required number alive. The orchestrator will manage this reconciliation process. If a node dies, the orchestrator will move all containers to a new, healthy node.</li>
<li><strong>As workloads are based on the number of instances required, the orchestrator will allow us to change this number any time we require</strong>: As a result, we can scale up or down the number of instances of a service (application component) to respond to a high demand for requests, for example.</li>
<li><strong>We will deploy applications based on multiple service components, with all their requirements and connectivity between them</strong>: As components may run on any cluster node, Docker Swarm will provide an internal overlay network to interconnect all application components.</li>
<li><strong>Swarm will provide service discovery and internal load balancing</strong>: In the <em>Service discovery and load balancing </em>section, we will learn how Docker Swarm can provide internal application DNS resolution so that all the components will easily be able to discover each other, along with load balancing between service replicas using a virtual IP.</li>
<li><strong>Orchestration will allow us to update application components automatically</strong>: In fact, all we need to decide is how these updates will be managed; orchestration will do the rest. This way, we can update application components without impacting users.</li>
<li><strong>We can ensure that our cluster runs securely by default</strong>: Docker Swarm will deploy <strong>Transport Layer Security</strong> (<strong>TLS</strong>) to interconnect control plane components. It will manage certificates for all of our nodes, creating an internal CA and verifying all node certificates itself.</li>
</ul>
<p>It is important to know that only the control plane is secure by default. Users' access to features such as application publishing will require additional configuration.</p>
<p>As we learned in the previous chapter, orchestrators require databases to store and manage workloads and any other cluster resource information. Docker Swarm has a built-in key-value store under the <kbd>/var/lib/docker/swarm</kbd> path (this is on Linux; it can be found in its equivalent directory on Windows, under <kbd>C:\ProgramData\docker</kbd>).</p>
<p>It is important to understand that the <kbd>/var/lib/docker/swarm</kbd> directory is essential, should we need to restore an unhealthy cluster. Take care of this directory and keep a backup of it.</p>
<p>We can lock users' access to the <kbd>/var/lib/docker/swarm</kbd> path using a key. This improves security. If it is unlocked, someone with enough system privileges can obtain Docker Swarm certificates.</p>
<h2 id="uuid-03c8cb66-3f6b-446e-91f9-4b39a4270d42">Docker Swarm overall architecture</h2>
<p>As we mentioned previously, Docker Swarm deploys its own secure control plane. There are two kinds of node roles:</p>
<ul>
<li><strong>Managers</strong>: These manage the overall Swarm cluster environment. They share an internal key-value database. More specifically, one of the managers has a different role. This is the manager's leader. There is only one leader per cluster and it makes all the necessary updates to the database. All other manager nodes will follow and sync their databases with the leader's one. Managers maintain cluster health, serve the Swarm HTTP API, and schedule workloads on available compute nodes.</li>
<li><strong>Workers</strong>: Workloads will run on worker nodes. It is important to know that managers have worker roles too. This means that workloads can also run on managers if we do not specify any special scheduling location. Workers will never participate in scheduling decisions; they will just run assigned workloads.</li>
</ul>
<p>We manage workload locations either by using location constraints on each workload or by disabling container execution on some nodes.</p>
<p>On nodes with multiple interfaces, we will be able to choose which interface we will use for the control plane. Manager nodes will implement the Raft consensus algorithm to manage the Swarm cluster state. This algorithm requires multiple servers coming to an agreement on data and status. Once they reach a decision on a value, that decision is written to disk. This will ensure information distribution with consistency across multiple managers.</p>
<p>As we mentioned previously, there is a leader node that modifies and store changes on its database; all other nodes will sync their databases with it. To maintain this consistency, Swarm implements Raft. This algorithm will manage all changes in the database, as well as the election of a new leader when it is unhealthy. When the leader needs to make a change (such as to the application's component status, and its data), it will query all the other nodes for their opinions. If they all agree with the change, the leader will commit it, and the change will be synced to all the nodes. If the leader fails (as in, the node goes down, the server process dies, and so on), a new election is triggered. In this case, all the remaining manager nodes will vote for a new leader.</p>
<p>This process requires reaching a consensus, with the majority of nodes agreeing on the result of the election. If there is no majority, a new election process will be triggered until a new leader is elected. After that, the cluster will be healthy again. Keep these concepts in mind because they are key in Docker Swarm and other orchestrators.</p>
<p>The following diagram represents the basic architecture of a Swarm orchestrator:</p>
<div><img src="img/e8491519-b7cd-4e8d-b03b-49a057f12859.png" style=""/></div>
<p>Let's review each plane in detail.</p>
<h3 id="uuid-fef9afa2-83b3-4151-85b2-3e850b7dd2c7">Management plane</h3>
<p>The management plane is the layer where all management tasks run. All cluster management traffic and workload maintenance will take place on this plane. The management plane provides high availability based on an odd number of manager nodes.</p>
<p>All communication in this plane is mutually encrypted using TLS (mutual TLS) by default. This is where the Raft protocol operates.</p>
<h3 id="uuid-f7b84c53-dfdb-40be-995a-74e5a8f8e7e9">Control plane</h3>
<p>This plane manages the state of the cluster. The gossip protocol will periodically inform all nodes about the cluster state, reducing the amount of information required by nodes to simply having an overview of the health of the cluster. This protocol manages host-to-host communications and is called the <em>control plane</em> because each host communicates only with its closest companions, and information flows through this to reach all the nodes within the control plane.</p>
<h3 id="uuid-d6749b38-cf02-4e8c-99ff-3e71c3135215">Data plane</h3>
<p>The data plane manages all the service's internal communications. It is based on VXLAN tunneling, encapsulating layer-2 packets within layer-3 headers. It will use UDP transport but VXLAN guarantees no dropped packets. We will be able to isolate the data plane from the control and management planes using the appropriate flags upon Docker Swarm creation (or joining).</p>
<p>When we initialize a new Docker Swarm cluster, it generates a self-signed <strong>Certificate Authority</strong> (<strong>CA</strong>) and issues self-signed certificates to every node. This ensures mutual TLS communication. The following is a summary of the steps taken to ensure secure communications when a new node joins the cluster:</p>
<ol>
<li class="mce-root">When a node joins, it sends the manager its join token, along with a certificate request.</li>
<li class="mce-root">Then, if the token is valid, the manager accepts the node's request and sends back a self-signed node certificate.</li>
<li class="mce-root">The manager then registers the new node in the cluster and it will appear as part of the Docker Swarm cluster.</li>
<li class="mce-root">Once the node is included in the cluster, it is ready (by default) to accept any new workload scheduled by the manager.</li>
</ol>
<p>In the next section, we will learn how to easily deploy a Docker Swarm cluster using common Docker command-line actions.</p>
<h2 id="uuid-45ba7d16-23fc-4920-b85e-fa95976faf50">Deploying a Docker Swarm cluster using the command line</h2>
<p>We can use the Docker <kbd>swarm</kbd> object to initialize a new cluster, join or leave a previously created one, and manage all Docker Swarm properties. Let's take a look at the <kbd>docker swarm</kbd> actions:</p>
<ul>
<li><kbd>init</kbd>: We will use <kbd>docker swarm init</kbd> to initialize a new cluster or recreate an existing one (we will describe this situation in more detail in the <em>High availability with Swarm</em> section). We will set many cluster options during cluster creation, but there are a few that can be changed later. The most important options are <kbd>--data-path-addr</kbd> and <kbd>--data-path-port</kbd> because they are used to set which node interface will be dedicated to the control plane on multi-homed nodes.</li>
</ul>
<p style="padding-left: 60px">These are the most commonly used arguments for creating the cluster:</p>
<ul>
<li style="padding-left: 30px"><kbd>--advertise-addr</kbd>: This option allows us to set which interface will be used to announce the cluster. All other nodes will use this interface's IP address to join the cluster.</li>
<li style="padding-left: 30px"><kbd>--data-path-addr</kbd>/<kbd>--data-path-port</kbd>: These options configure the interface and port used for the control plane. All traffic in this interface will be encrypted using TLS, and certificates will be managed internally by Swarm. We can use the IP address/port or host interface notation. The default port is <kbd>4789</kbd>.</li>
<li style="padding-left: 30px"><kbd>--external-ca</kbd>/<kbd>--cert-expiry</kbd>: Although Swarm will manage TLS for us, we can deploy our own CA for all certificates using this parameter. We can also specify how often certificates will be rotated. By default, they will be automatically recreated every 90 days (2,160 hours).</li>
<li style="padding-left: 30px"><kbd>--listen-addr</kbd>: This option allows us to specify which host interface will be used to serve the cluster API. We can use IP address/port or host interface notation, with the default of <kbd>0.0.0.0:2377</kbd>.</li>
<li style="padding-left: 30px"><kbd>--autolock</kbd>: As we mentioned previously, we can lock access to internal Docker Swarm data. This is important because <kbd>/var/lib/docker/swarm</kbd> contains the CA and other certificates. If you are not sure about node access, it is better to lock this directory from users. Take care with this option because any system or Docker daemon restart will require the unlock key in order to enable this node in the cluster again.</li>
<li style="padding-left: 30px"><kbd>--dispatcher-heartbeat</kbd>: This option will manage how often nodes will report their health. It defaults to 5 seconds, but you can change it if your cluster has high latency.</li>
<li style="padding-left: 30px"><kbd>--max-snapshots</kbd>/<kbd>--snapshot-interval</kbd>: Swarm will take database snapshots for manager synchronization. We can set the number of snapshots to keep. By default, none will be kept (just one for synchronization), but these can be useful for debugging or disaster recovery. We can also set the interval between snapshots. Take care when changing this option because having many snapshots will trigger a lot of sync operations to other nodes and can incur high-performance costs. But on the other hand, syncing less frequently can guide the cluster to non-synced states. This parameter defaults to 10,000 ms.</li>
</ul>
<ul>
<li><kbd>join</kbd>: After cluster initialization, all the other nodes will join the previously created cluster, regardless of whether they are managers or workers. Joining Docker nodes to a cluster requires a cluster-specific token, with different tokens for the manager and worker nodes. We will always require a token and the leader IP address to join the cluster. Remember that the leader can change from time to time. We will also be able to set the control plane's IP and port, the IP address to be announced to the other nodes, and the listen IP address for the API. We will execute this command on the joining node using the following format: <kbd>docker swarm join --token &lt;MANAGER_OR_WORKER_TOKEN&gt; &lt;LEADER_IP:PORT&gt;</kbd>.</li>
<li><kbd>leave</kbd>: Once a node is part of the cluster, it can leave it whenever we need it to. It is important to understand what it means to <em>leave</em> the cluster. The <kbd>leave</kbd> command will be executed on the node leaving the cluster. Manager nodes are not able to leave the cluster because this would force the cluster into an unhealthy state. We can use <kbd>--force</kbd> to make a node leave the cluster, even if it is a manager node, but this comes with risks that you need to understand before proceeding. Leaving the cluster will not remove the node from the internal Docker Swarm database. Rather, we need to inform the managers of this change by issuing <kbd>docker node rm &lt;NODE_NAME_OR_ID&gt;</kbd>.</li>
<li><kbd>update</kbd>: With this action, we can change some of the Docker Swarm cluster's described properties, such as the external CA, certificate expiration settings, and snapshot behavior.</li>
<li><kbd>ca</kbd>: As we mentioned previously, all internal control plane communication is based on TLS certificates. The <kbd>ca</kbd> option allows us to customize the CA and other certificate behavior. We can rotate them or choose our own CA.</li>
<li><kbd>join-token</kbd>: With this action, we can review the current tokens for managers and workers. In fact, we can execute <kbd>join-token</kbd>, followed by the required role, to retrieve their values. We do not need to keep them safe since we can retrieve them as needed. These tokens are only used when joining the cluster. We can change them whenever we want, using <kbd>docker swarm join-token --rotate</kbd> to create a new one. This will not affect already joined nodes. We usually execute <kbd>docker swarm join-token worker</kbd> to retrieve the command line and token required to join the node to the cluster. We can use <kbd>--quiet</kbd> to retrieve only the token, which is useful for automating the joining process.</li>
</ul>
<ul>
<li>
<p class="mce-root"><kbd>unlock</kbd>/<kbd>unlock-key</kbd>: We mentioned previously that it is unsafe to allow users to access the <kbd>/var/lib/docker</kbd> directory. Access is only allowed to root by default, but it is even more secure to lock Docker Swarm information. For example, all cluster certificates will be stored under the <kbd>/var/lib/docker/swarm/certificates</kbd> directory. Locking Swarm information is a good practice, but be aware of losing your unlock key. Every time the cluster node starts (such as a Docker Engine or node restart, for example), the unlock key will be required. This leaves the cluster in a non-automatic, high-availability environment in some situations. The <kbd>unlock</kbd> option unlocks the Docker Swarm cluster information, while <kbd>unlock-key</kbd> allows us to manage the defined key used for this behavior.</p>
</li>
</ul>
<p>Docker Swarm will also create new objects:</p>
<ul>
<li><kbd>swarm</kbd>: This is the cluster itself, along with its own properties, as described earlier.</li>
<li><kbd>node</kbd>: These are the nodes that are part of the cluster. We will add labels to them and manage their roles as part of the cluster.</li>
<li><kbd>service</kbd>: We deploy services on our Docker Swarm cluster. We won't deploy a standalone container. We will learn more about services in the <em>Scheduling workloads in the cluster – tasks and services</em> section.</li>
<li><kbd>secret</kbd> and <kbd>config</kbd>: Both objects allow us to share service configurations in the cluster. Remember, it is not easy to manage information on different hosts, even if the application is completely stateless.</li>
<li><kbd>stack</kbd>: We will use stacks to deploy applications. We will use a Docker Compose-like file format containing all application components and their interactions.</li>
</ul>
<p>All these objects will have common actions associated with them, including listing, deploying/creating, removing, and inspecting their properties. Services and stacks will have containers associated with them, so we will be able to list their processes' distributions cluster-wide.</p>
<p>We can run a single node cluster on our laptop. It is not a problem running a single node cluster for testing or developing services or stacks.</p>
<p>In the next section, we will learn how to deploy a Docker Swarm environment with high availability.</p>
<h2 id="uuid-30688bf5-289b-40df-a46b-f895ab937322">Deploying Docker Swarm with high availability</h2>
<p>So far, we have learned about the different roles in Docker Swarm clusters. However, in order to provide high availability, we will need to deploy more than one manager and worker.</p>
<p>The Raft consensus algorithm requires an odd number of healthy nodes to work because a majority of the nodes must agree on all the changes and resources states. This means that we will need at least <em>N/2+1</em> healthy nodes to agree before committing a change or resource state. In other words, we will not grant Docker Swarm availability if fewer than <em>N/2+1</em> manager nodes are healthy. Let's review the options in the following table to get a better understanding:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign"><strong>Number of managers</strong></td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign"><strong>Required for consensus (<em>N/2+1)</em></strong></td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign"><strong>Allowed failures</strong></td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign"><strong>Provides high availability?</strong></td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">1</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">1</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">0</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">No.</td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">2</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">2</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">0</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">No.</td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">3</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">2</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">1</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">Yes.</td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">4</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">3</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">1</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">Yes, but this is not better than the three-manager option and can lead to election problems if the leader fails.</td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">5</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">3</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">2</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">Yes.</td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">6</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">4</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">2</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">Yes, but this is not better than the five-manager option and can lead to election problems if the leader fails.</td>
</tr>
<tr>
<td style="width: 12%" class="CDPAlignCenter CDPAlign">7</td>
<td style="width: 17%" class="CDPAlignCenter CDPAlign">4</td>
<td style="width: 12.31%" class="CDPAlignCenter CDPAlign">3</td>
<td style="width: 51.69%" class="CDPAlignCenter CDPAlign">Yes.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>When a manager fails in the 3-manager configuration, two nodes can agree and changes will be updated without problems. But if one of those fails, only one will be left and changes can't be committed. There is no consensus and no cluster operations can be deployed. This means that any service deployed in the cluster will stay running. Users will not be affected unless one service loses some replicas and Docker Swarm should have started new ones to achieve the required number. No automatic actions will be allowed because these have to update the database data, and this is not permitted. We will not be able to add or remove any nodes in that situation, so the cluster will be inconsistent.</p>
<p>Consequently, Swarm requires an odd number of managers to provide high availability. Although there is no limit regarding the number of manager nodes, more than seven is not recommended. Increasing the number of managers will reduce write performance because the leader will require more acknowledged responses from more nodes to update cluster changes. This will result in more round-trip network traffic.</p>
<p>It is key to understand these behaviors. Even if we have deployed a three-node cluster, we can still lose quorum if a sufficient number of nodes become unhealthy. It is important to attend to node failures as soon as possible.</p>
<p>We will usually deploy three-node clusters because they allow for the failure of 1 node. It is enough for production, but in some critical environments, we will deploy five-node clusters to allow for two node failures.</p>
<p>In cases where a Swarm cluster needs to be distributed between different locations, the recommended number of managers is seven. It will allow distribution across multiple data centers. We will deploy three nodes in the first data center, two in the second data center, and a final two in the third data center (3+2+2). This distribution will allow us to handle a full data center failure with services being redistributed if worker nodes have sufficient resources.</p>
<p>What happens when a manager node fails? The leader will start to store committed changes in order to sync the unhealthy manager node when it is ready again. This will increase Docker Swarm's directory size. If you did not set your node disk space sufficiently to allow for these situations, your nodes will probably consume your entire filesystem if the failure doesn't recover soon. And then, you will get a second unhealthy node and your cluster will be inconsistent. This situation we've described is not a horror movie – it happens all too often on new installations where administrators think that the cluster will be alright with some unhealthy nodes for weeks at a time.</p>
<p>We mentioned one important option in the <kbd>docker swarm</kbd> command-line table when we talked about Docker Swarm cluster initialization. We will use <kbd>docker swarm init --force-new-cluster</kbd> in situations where the cluster is unhealthy, but at least one manager is working. If the cluster isn't quorate and no operations can be performed with cluster resources (that is, nodes can't be added/removed and services won't be repaired if they fail), we can force a new cluster. This is an extreme situation.</p>
<p>Take care of your environment before recreating the cluster. Forcing a new cluster will set the node where the command was executed as the leader. All other nodes in the cluster (including those managers that were insufficient for a quorum) will be set as workers. It is like a <strong>cluster quorum reset</strong>. Services and other resources will retain their states and configurations (as they were committed or retrieved from nodes). Therefore, we will end up with a one-manager node cluster with all the other nodes as workers. Services and other stuff should not be affected. In these situations, it is a good practice to review the manager node's logs because some containers can be left unmanaged if some cluster changes were not committed.</p>
<p>Although managers can act as workers, it is safer in production to run workloads on worker-role nodes only. A manager's processes may impact the application and vice versa.</p>
<p>We will always deploy more than one worker in production environments. This will ensure the health of our services if one of the workers goes offline unexpectedly or if we need to perform any maintenance tasks, such as updating Docker Engine. We should usually deploy worker nodes according to our application's resource requirements. Adding workers will increase the total cluster workload capacity.</p>
<p>In the next section, we will learn how to deploy a Docker Swarm cluster.</p>
<h1 id="uuid-8addadff-88ba-448d-b853-781b4a2ef3e1">Creating a Docker Swarm cluster </h1>
<p>Now that we have reviewed the Docker Swarm architecture and the command-line actions required to initialize the cluster, we can create a cluster. By the end of this chapter, we will have a fully functional cluster with high availability. Let's start by reviewing the Docker Swarm cluster creation process:</p>
<ol>
<li>First, we initialize a Swarm cluster on a manager node. This node automatically becomes the cluster leader because no other manager is available. If we have a node with multiple interfaces, we will choose which interface will be associated with the control plane and which ones will be announced for other nodes and the Swarm API. The output will vary from the following in your environment. Let's execute <kbd>docker swarm init</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker swarm init</strong><br/><strong>Swarm initialized: current node (ev4ocuzk61lj0375z80mkba5f) is now a manager.</strong><br/><strong>To add a worker to this swarm, run the following command:</strong><br/><strong>docker swarm join --token SWMTKN-1-4dtk2ieh3rwjd0se5rzwyf2hbk7zlyxh27pbh4plg2sn0qtitx-50zsub5f0s4kchwjcfcbyuzn5  192.168.200.18:2377</strong><br/><strong>To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.</strong></pre>
<ol start="2">
<li>Once the cluster has been created, we can review the cluster nodes and their properties by using <kbd>docker node ls</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker node ls</strong><br/><strong>ID                            HOSTNAME    STATUS AVAILABILITY     MANAGER STATUS ENGINE VERSION</strong><br/><strong>ev4ocuzk61lj0375z80mkba5f    * sirius     Ready     Active         Leader             19.03.2</strong></pre>
<p style="padding-left: 60px">The first column shows the node object identifier. As we mentioned previously, new objects have been created with Docker Swarm. The second column shows its name from the internal host resolution service (this may contain a <strong>Fully Qualified Domain Name</strong> (<strong>FQDN</strong>)). Notice the asterisk near the hostname. This means that we are working on this node right now. All the commands are executed on that node, regardless of whether it is a leader.</p>
<p>On Docker Swarm, cluster commands related to cluster-wide objects are only available on manager nodes. We won't need to execute commands on the leader node, but we won't be able to execute any cluster commands on a worker node. We can't list nodes or deploy a service.</p>
<p style="padding-left: 60px">The last column shows each node's Docker Engine version. Let's take a look at the <kbd>STATUS</kbd>, <kbd>AVAILABILITY</kbd>, and <kbd>MANAGER STATUS</kbd> columns:</p>
<ul>
<li><kbd>STATUS</kbd>, as its name suggests, shows the status of the node within the cluster. If it is not healthy, it will be shown here.</li>
<li><kbd>MANAGER STATUS</kbd> shows the current role of the node (in this case, the node is the leader). We have three different states:
<ul>
<li><kbd>Leader</kbd>, when the node is the cluster leader.</li>
<li><kbd>Manager</kbd>, which means that the node is one of the cluster managers.</li>
<li>An empty value will mean that the node has a worker role, and is therefore not part of the control plane.</li>
</ul>
</li>
<li><kbd>AVAILABILITY</kbd> represents a node's availability to receive workloads. Here, we can see that managers can receive workloads too. We can set this node property. In fact, there are three different states:
<ul>
<li><kbd>active</kbd>, which means that the node will be able to receive any workload.</li>
<li><kbd>passive</kbd>, which means that the node will not run any other additional workload. Those already running will maintain their state, but no additional workloads will be allowed.</li>
<li><kbd>drain</kbd> is the state that we get when we disable any workload on this node. When this happens, all running workloads on the node will be moved to any other healthy and available node.</li>
</ul>
</li>
</ul>
<p>We can enforce the behavior of any node when joining the cluster, or even when we create the cluster, using the <kbd>--availability</kbd> flag with <kbd>docker swarm init</kbd> or <kbd>docker swarm join</kbd>. We will set the node availability for new workloads (<kbd>active</kbd> | <kbd>pause</kbd> | <kbd>drain</kbd>). By default, all the nodes will be active and ready to receive workloads.</p>
<ol start="3">
<li>We will join another node as a worker to demonstrate this, using the previously shown cluster initialization output with <kbd>docker swarm join</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker swarm join --token SWMTKN-1-4dtk2ieh3rwjd0se5rzwyf2hbk7zlyxh27pbh4plg2sn0qtitx-50zsub5f0s4kchwjcfcbyuzn5 192.168.200.18:2377</strong></pre>
<ol start="4">
<li>Now, we can review the cluster node status (remember, this command will only be available on manager nodes) once more by executing <kbd>docker node ls</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker node ls</strong><br/><strong>ID                         HOSTNAME     STATUS   AVAILABILITY MANAGER STATUS  ENGINE VERSION</strong><br/><strong>glc1ovbcqubmfw6vgzh5ocjgs   antares     Ready     Active                          19.03.5</strong><br/><strong>ev4ocuzk61lj0375z80mkba5f * sirius      Ready     Active          Leader          19.03.2</strong></pre>
<p style="padding-left: 60px">In this example, we are executing commands on the <kbd>sirius</kbd> node (marked with <kbd>*</kbd>), which is a leader and hence a manager. Notice that <kbd>antares</kbd> is a worker node because it has an empty value in the <kbd>MANAGER STATUS</kbd> column.</p>
<p style="padding-left: 60px">We can review node information by executing the <kbd>docker node inspect</kbd> action (the following output is truncated):</p>
<pre style="padding-left: 60px"><strong>$ docker node inspect antares </strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "ID": "glc1ovbcqubmfw6vgzh5ocjgs",</strong><br/><strong>...<br/></strong><strong>        "Spec": {</strong><br/><strong>            "Labels": {},</strong><br/><strong>            "Role": "worker",</strong><br/><strong>            "Availability": "active"</strong><br/><strong>        },</strong><br/><strong>        "Description": {</strong><br/><strong>            "Hostname": "antares",</strong><br/><strong>            "Platform": {</strong><br/><strong>                "Architecture": "x86_64",</strong><br/><strong>                "OS": "linux"</strong><br/><strong>            },</strong><br/><strong>            "Resources": {</strong><br/><strong>                "NanoCPUs": 16000000000,</strong><br/><strong>                "MemoryBytes": 33736785920</strong><br/><strong>            },</strong><br/><strong>            "Engine": {</strong><br/><strong>                "EngineVersion": "19.03.5",</strong><br/><strong>   </strong><strong>            ...</strong><br/><strong>               ...</strong><br/><strong>   </strong><strong>         },</strong><br/><strong>            "TLSInfo": {</strong><br/><strong>                "TrustRoot": "-----BEGIN CERTIFICATE-----\nMIIBaTCCARCgAwIBAgIUUB8yKqt3uUh2wmF/z450dyg9EDAwCgYIKoZIzj0EAwIw\nEzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMTkxMjI5MTA1NTAwWhcNMzkxMjI0MTA1\nNTAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH\nA0IABACDe6KWpqXiEMyWB9Qn6y2O2+wH8HLoikR+48xqnjeU0SkW/+rPQkW9PilB\ntIYGwaviLPXpuL4EpVBWxHtMDQCjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB\nAf8EBTADAQH/MB0GA1UdDgQWBBTbL48HmUp/lYB1Zqu3GL7q5oMrwTAKBggqhkjO\nPQQDAgNHADBEAiAh1TVNulaIHf2vh6zM9v6raer5WgTcGu8xQYBcDViPnwIgU4sl\ntK70bgSfEzLx6WpOv4yjr+c0tlJt/6Gj3waQl10=\n-----END CERTIFICATE-----\n",</strong><br/><strong>                "CertIssuerSubject": "MBMxETAPBgNVBAMTCHN3YXJtLWNh",</strong><br/><strong>                "CertIssuerPublicKey": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAIN7opampeIQzJYH1CfrLY7b7AfwcuiKRH7jzGqeN5TRKRb/6s9CRb0+KUG0hgbBq+Is9em4vgSlUFbEe0wNAA=="</strong><br/><strong>            }</strong><br/><strong>        },</strong><br/><strong>        "Status": {</strong><br/><strong>            "State": "ready",</strong><br/><strong>            "Addr": "192.168.200.15"</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p style="padding-left: 60px">When we inspect a node, information regarding its status, node IP address, and TLS information will be shown in JSON format.</p>
<p style="padding-left: 60px">We can use labels on nodes to help Docker Swarm choose the best location for specific workloads. It uses node architectures to deploy workloads in the right place, but if we want a workload to run on a specific node, we can add a unique label and add a constraint to deploy the workload. We will learn more about service locations and labels in the <em>Chapter labs</em> section.</p>
<p style="padding-left: 60px">Under the <kbd>Spec</kbd> key, we can review the node role in the <kbd>docker node inspect</kbd> output. We can change the node role whenever necessary. This is a big improvement over other orchestrators, where roles are static. Keep in mind that role changes will affect your Docker Swarm architecture because it will change the number of managers and worker nodes. Keep high availability in mind, its requirement of an odd number of managers, and the consequences of this in case of node failures.</p>
<ol start="5">
<li>A role is just a node property, which means we can change it just like any other object property. Remember that changes can only be deployed from manager nodes. We change a node's role by executing <kbd>docker node update</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker node update --role manager antares</strong><br/><strong>antares</strong></pre>
<p style="padding-left: 60px" class="mce-root">Once again, let's list all the nodes in the cluster by executing <kbd>docker node ls</kbd>, this time with a filter to retrieve only managers:</p>
<pre style="padding-left: 60px"><strong>$ docker node ls --filter role=manager</strong><br/><strong>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION</strong><br/><strong>glc1ovbcqubmfw6vgzh5ocjgs antares Ready Active Reachable 19.03.5</strong><br/><strong>ev4ocuzk61lj0375z80mkba5f * sirius Ready Active Leader 19.03.2</strong></pre>
<p style="padding-left: 60px">We can now use <kbd>docker node inspect</kbd> to retrieve the <kbd>ManagerStatus</kbd> key:</p>
<pre style="padding-left: 60px"><strong>$ docker node inspect antares --format "{{.ManagerStatus}}"</strong><br/><strong>{false reachable 192.168.200.15:2377}</strong></pre>
<p>Nodes can be removed from the cluster by using <kbd>docker node rm</kbd>, just as we did with other Docker objects. We will only remove worker nodes. The usual sequence for removing a manager node from a Docker Swarm cluster will require a previous step to change its role to a worker. Once a node role has changed to a worker, we can remove the node. If we need to remove a failed manager, we can force node removal using <kbd>--force</kbd>. However, this is not recommended as you can leave the cluster in an inconsistent state. The manager's database should be updated before you remove any node, which is why the removal sequence we've described here is so important.</p>
<p>Remember to make sure that you have an odd number of manager nodes if you demote or remove any manager. If you have problems with the leader when you do not have an odd number of managers, you can reach an inconsistent state when other managers have to elect a new leader.</p>
<p>As we mentioned previously, labels are node properties. We can add and remove them at runtime. This is a big difference compared to the labels learned about in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>. Those labels were set at the Docker daemon level and are static. We needed to add them to the <kbd>daemon.json</kbd> file, so we were required to restart the node's Docker Engine to make them effective. In this case, however, node labels are managed by Docker Swarm and can be changed with the common node object's <kbd>update</kbd> action (<kbd>docker node update</kbd>).</p>
<p>The Docker command line provides some shortcuts, as we have observed in previous chapters. In this case, we can change node roles by demoting a manager to a worker role, or by promoting a worker to a manager role. We use <kbd>docker node &lt;promote|demote&gt; &lt;NODENAME_OR_ID&gt;</kbd> to change between node roles.</p>
<p>We can also change a node's workload availability. This allows a node to receive (or not) cluster-deployed workloads. As with any other node property, we will use <kbd>docker node update --availability &lt;available|drain|pause&gt; &lt;NODENAME_OR_ID&gt;</kbd> to drain or pause a node when it was active. Both drain and pause will prevent us from scheduling any new workload on the node, while drain on its own will remove any currently running one from the affected node.</p>
<p>Remember that when we drain a node, the scheduler will reassign any tasks running on the affected node to another available worker. Keep in mind that the other nodes should have enough resources before draining the given node.</p>
<p>In the next section, we will review how to back up and recover a faulty Docker Swarm cluster.</p>
<h2 id="uuid-9d5f9ef9-4b0a-4f44-8d22-abafd2478ec7">Recovering a faulty Docker Swarm cluster</h2>
<p>We will review a few steps to back up and restore Docker Swarm clusters. Losing your cluster quorum is not a big problem. As we have learned, we can recover the cluster by forcing the initialization of a new one, even with just one healthy manager. However, losing your cluster data will completely destroy your environment if you don't have any manager nodes that are operational and working correctly. In these situations, we can recover the cluster by restoring a copy containing healthy data that was taken when the cluster was running correctly. Let's learn how to take backups of our clusters now.</p>
<h3 id="uuid-f4926c1d-4404-42ef-b67b-e980f870e8a0">Backing up your Swarm</h3>
<p>As we learned in this chapter, <kbd>/var/lib/docker/swarm</kbd> (and its Microsoft Windows equivalent directory) contains the key-value store data, the certificates, and the encrypted Raft logs. Without them, we can't recover a faulty cluster, so let's back up this directory on any manager.</p>
<p>Having a consistent backup requires static files. If files are opened or some process is writing them, they will not be consistent. Therefore, we need to stop Docker Engine on the given node. Do not launch the backup procedure on the leader node.</p>
<p>Keep in mind that while the backup operation is running, if the Docker daemon is stopped, the number of managers will be affected. The leader will continue managing changes and generating new sync points to recover synchronization with the lost manager. Your cluster will be vulnerable to losing quorum if other managers fail. If you plan to do daily backups, consider using five managers.</p>
<h3 id="uuid-34662fdf-f22f-443b-93a9-558cd026e2ef">Recovering your Swarm</h3>
<p>In case we need to recover a completely failed cluster (where no managers can't achieve quorum and we can't force a new cluster), we will stop Docker Engine on one manager. Remove all <kbd>/var/lib/docker/swarm</kbd> directory content (or its Microsoft Windows equivalent) and restore the backed-up content to this directory. Then, start Docker Engine again and reinitialize the cluster with <kbd>docker swarm init --force-new-cluster</kbd>.</p>
<p>When the single-manager cluster is healthy, start to add the other old Swarm cluster managers. Before adding those managers, ensure that they've left the old Swarm cluster.</p>
<p>If we set up Swarm auto-lock, we will need the key that was stored with the restored backup. Even if you changed it after the backup was issued, you will still need the old one.</p>
<p>In the next section, we will learn how workloads are deployed on the cluster and how Docker Swarm tracks the health of application components to ensure that services are not impacted when something goes wrong.</p>
<h1 id="uuid-036da95d-5820-4726-ab1e-d92b043a52d0">Scheduling workloads in the cluster – tasks and services</h1>
<p>We don't run containers on a Swarm cluster; rather, we deploy services. These are atomic workloads that can be deployed in a Docker Swarm cluster. Services are defined by tasks, and each task is represented by a container in the Docker Swarm model. Swarm is based on SwarmKit and its logic is inherited. SwarmKit was created as a response to clustering any kind of task (such as virtual machines, for example), but Docker Swarm works with containers.</p>
<p class="mce-root">The Docker Swarm orchestrator uses a declarative model. This means that we define the desired state for our services and Docker Swarm will take care of the rest. If the defined number of replicas or tasks for a service is wrong – for example, if one of them died – Docker Swarm will take action to recover the correct state of the service. In this example, it will deploy a new replica to keep all the required nodes healthy.</p>
<p>The following diagram represents services and tasks in relation to containers. The <kbd>colors</kbd> service has five replicas (<kbd>colors.1</kbd> to <kbd>colors.5</kbd>). Each replica runs on one container from the same image, <kbd>codegazers/colors:1.13</kbd>, and these containers run distributed cluster-wide across <kbd>node1</kbd>, <kbd>node2</kbd>, and <kbd>node3</kbd>:</p>
<div><img src="img/a777ffe3-5e76-482f-9bb3-415cbbed3f62.jpg" style=""/></div>
<p>Service creation requires the following information:</p>
<ul>
<li>Which image will run the associated containers?</li>
<li>How many containers does this service require to be healthy?</li>
<li>Should the service be available to users on any port and protocol?</li>
<li>How should service updates be managed?</li>
<li>Is there any preferred location for this service to run?</li>
</ul>
<p>Service creation will require all this information to be entered on the command line. Since services are Docker objects, we can use common actions such as listing, creating, removing, updating, and inspecting their properties. Docker Swarm will manage all our tasks' integration with services. We will never deploy tasks or containers. We will just create and manage services. Let's take a look at Docker command-line actions and options related to services:</p>
<ul>
<li><kbd>create</kbd>: This is common to other objects, but services have many non-standard properties. We will not list and review all service arguments because most of them are inherited from containers. Here, we'll review the most important ones related to service behavior:
<ul>
<li><kbd>--config</kbd>: We can create a service configuration only, not a real service. This will create all service environments and requirements but without running any task.</li>
<li><kbd>--container-label</kbd>/<kbd>--label</kbd>: We added this option here because it is important to understand that services and containers are different objects and that we can add labels to both. By default, Docker Swarm will create many labels on each service container to relate them to each other. We can easily use those labels to filter information regarding our services' containers on any host.</li>
<li><kbd>--constraint</kbd>/<kbd>--placement-pref</kbd>: As we mentioned previously, we can specify which nodes should run a given service's tasks. We use a list of key-value pairs as constraints to do this. All defined keys must be fulfilled to schedule the service's tasks on a given node. If no node satisfies the defined constraints, the tasks will not be run because Docker Swarm's scheduler will not find any node with those requirements. On the other hand, <kbd>placement-pref</kbd> will provide a placement preference. This will not limit which nodes will run the tasks, but we can use this to spread our services' tasks across different nodes using a defined key. For example, we might distribute a given service's tasks across different physical locations (such as data centers).</li>
<li><kbd>--mode</kbd>: There are two different service modes (in fact, there are three, as we will find out later in the <em>Networking in Docker Swarm </em>section, but at this point, just keep the following two in mind). By default, all services will use replication mode. This means that we will set a number of replicas to be healthy (by default, this is one replica). We also have global services. In this case, we will create as many replicas as nodes in the cluster, but we will just run one replica per node. This mode is very interesting for monitoring applications, for example, because all the nodes will receive their monitoring process. One important thing about these services is that every node that gets into the cluster will receive its own replica. Docker Swarm will deploy it on the new node for us automatically.</li>
<li><kbd>--with-registry-auth</kbd>: This is a very important option because it allows us to distribute credentials among cluster nodes so that we can use private images. It is also important to understand that Docker Swarm requires external or internal registries to work. We will not work with local images on cluster nodes anymore. Local images will lead to inconsistent deployments because image names can match, while content could be completely different across nodes.</li>
<li><kbd>--endpoint-mode</kbd>: This option sets how services announce or manage their tasks. We can use <kbd>vip</kbd> and <kbd>dnsrr</kbd> for this. Services will default to <kbd>vip</kbd>, so each service will receive a virtual IP associated with its name, and an internal load balancer will route traffic to each replicated process (container/task) associated with it. On the other hand, <kbd>dnsrr</kbd> will use internal name resolution to associate each replica IP address whenever we ask for a service name. This way, internal name resolution will give us a different IP address when a given service has been deployed with more than one task.</li>
<li><kbd>--network</kbd>: We can attach new services to an existing network. As we did with containers, we can also use a host network namespace. The difference here is that we can't execute privileged services, so our services will have to expose ports numbered higher than <kbd>1024</kbd>.</li>
<li><kbd>--publish</kbd>: We will use this option to publish ports externally. Docker Swarm will expose ports using Docker Swarm's router mesh on every node. If external requests arrive on a host that does not execute any service tasks, Docker Swarm will internally reroute requests to an appropriate node.</li>
<li><kbd>--replicas</kbd>/<kbd>--replicas-max-per-node</kbd>: Services are defined by how many replicas or tasks are deployed to maintain their healthy state. By default, all services deploy one single replica. As we will see later, we can change the number of replicas at any time we need. Not all application components (processes) will work well if we scale up or down their replicas. Imagine, for example, a SQL database. It is a completely stateful component because the database process will write data. If we add a new database replica accessing the same storage, the database will become corrupted. If each database replica has its own storage, they will manage different data. As a result, not all services can be scaled up or down.</li>
<li><kbd>--reserve-cpu</kbd>/<kbd>--reserve-memory</kbd>: We can reserve the amount of resources required for a service to work. If no node presents enough resources, it will not be scheduled.</li>
<li><kbd>--update-delay</kbd>/<kbd>--update-failure-action</kbd>/<kbd>--update-max-failure-ratio</kbd>/<kbd>--update-monitor</kbd>/<kbd>--update-order</kbd>/<kbd>--update-parallelism</kbd>: <kbd>update</kbd> options manage how changes are executed for a service. We will set how many services' tasks will be updated at once, how many seconds we will wait between instances' updates, and how the update process will be done. The <kbd>--update-order</kbd> option sets how this update process will be executed. By default, the running container will be stopped and a new one will be created after the old one is completely finished. With this setting, the service will be impacted. We can set a different order by starting a new container first. Then, once everything is fine, the old one will be stopped. This way, the service will not be impacted, but your application process must be able to allow for this situation. For example, it will not work on a standard SQL database, as we mentioned previously. We will also set what to do when some of the updates fail, either by executing an automatic rollback or by pausing the rest of the service updates until manual action is taken.</li>
<li><kbd>--rollback-delay</kbd>/<kbd>--rollback-failure-action</kbd>/<kbd>--rollback-max-failure-ratio</kbd>/<kbd>--rollback-monitor</kbd>/<kbd>--rollback-order</kbd>/<kbd>--rollback-parallelism</kbd>: If the update process goes wrong, we can set an automatic rollback. These settings modify how rollbacks will be done. We have the same options we reviewed for the <kbd>update</kbd> process, but this time, the arguments will refer to the <kbd>rollback</kbd> process.</li>
</ul>
</li>
</ul>
<ul>
<li><kbd>ps</kbd>: With this, we can review all our service's tasks and their distributions in the cluster. We can also use filters and output format. We will see a couple of examples of this in the <em>Chapter labs</em> section.</li>
<li><kbd>logs</kbd>: This is a very useful action because Docker Swarm will retrieve the logs of all our tasks for us. We can then review them from the manager command line instead of going to wherever the tasks were running to read the container's logs.</li>
<li><kbd>update</kbd>: Service properties can be updated. For example, we can change image release versions, publish new ports, and change the number of replicas, among other things.</li>
<li><kbd>rollback</kbd>: With this, we can return to the service's previous properties. It is important to understand that images from previous executions should be kept in our hosts to allow for the application's rollbacks.<br/></li>
<li><kbd>inspect</kbd>/<kbd>ls</kbd>/<kbd>rm</kbd>: These are the common actions we encounter with all other kinds of objects. We've already learned how to use them.</li>
</ul>
<p>It is important to note that privileged containers are not allowed on services. Therefore, if we want to use the host network namespace, container processes should expose and use non-privileged ports (higher than <kbd>1024</kbd>).</p>
<p>Docker service constraints can be set with custom labels, but there are some internal ones that are created by default that are very useful: </p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 23%" class="CDPAlignCenter CDPAlign"><strong>Label</strong></td>
<td style="width: 31.803%" class="CDPAlignCenter CDPAlign"><strong>Attribute</strong></td>
</tr>
<tr>
<td style="width: 23%" class="CDPAlignCenter CDPAlign"><kbd>node.id</kbd></td>
<td style="width: 31.803%" class="CDPAlignCenter CDPAlign">Node ID</td>
</tr>
<tr>
<td style="width: 23%" class="CDPAlignCenter CDPAlign"><kbd>node.hostname</kbd></td>
<td style="width: 31.803%" class="CDPAlignCenter CDPAlign">Node hostname; for example, <kbd>node.hostname==antares</kbd></td>
</tr>
<tr>
<td style="width: 23%" class="CDPAlignCenter CDPAlign"><kbd>node.role</kbd></td>
<td style="width: 31.803%" class="CDPAlignCenter CDPAlign">Node Swarm role; for example, <kbd>node.role!=manager</kbd></td>
</tr>
<tr>
<td style="width: 23%" class="CDPAlignCenter CDPAlign"><kbd>node.labels</kbd></td>
<td style="width: 31.803%" class="CDPAlignCenter CDPAlign">Swarm node-assigned labels; for example, <kbd>node.labels.environment==production</kbd></td>
</tr>
<tr>
<td style="width: 23%" class="CDPAlignCenter CDPAlign"><kbd>engine.labels</kbd></td>
<td style="width: 31.803%" class="CDPAlignCenter CDPAlign">Docker Engine-defined labels; for example, <kbd>engine.labels.operatingsystem==ubuntu 18.04</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can use variables to define service properties. In the following example, we're using internal Docker Swarm variables in the container's hostname:</p>
<pre><strong>$ docker service create --name "top" --hostname="{{.Service.Name}}-{{.Task.ID}}" busybox top</strong></pre>
<p>To summarize, before continuing with other Swarm resources: services are a group of tasks, each executing one container. All these containers run together to maintain the service's state. Docker Swarm will monitor the service's state and if one container dies, it will run a new container to maintain the number of instances. It is important to note that a container's IDs and names will change. However, while new tasks can be created, the task's name will not be changed.</p>
<p>Let's have a look at a quick example before moving on to the next topic. We will create a simple NGINX web server service using <kbd>docker service create</kbd>:</p>
<pre><strong>$ docker service create --name webserver --publish 80 nginx:alpine</strong><br/><strong>lkcig20f3wpfcbfpe68s72fas</strong><br/><strong>overall progress: 1 out of 1 tasks </strong><br/><strong>1/1: running [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<p>We can review where the created task is running by using <kbd>docker service ps</kbd>:</p>
<pre><strong>$ docker service ps webserver</strong><br/><strong>ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS</strong> <br/><strong>lb1akyp4dbvc webserver.1 nginx:alpine sirius Running Running about a minute ago</strong> </pre>
<p>Then, we move to the node where the task is running. Once there, we kill the associated container using <kbd>docker container kill</kbd>:</p>
<pre><strong>$ docker container ls</strong><br/><strong>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</strong><br/><strong>6aeaee25ff9b nginx:alpine "nginx -g 'daemon of…" 6 minutes ago Up 6 minutes 80/tcp webserver.1.lb1akyp4dbvcqcfznezlhr4zk</strong><br/><br/><strong>$ docker container kill 6aeaee25ff9b</strong>              <br/><strong>6aeaee25ff9b</strong></pre>
<p>After a few seconds, a new task will be created automatically with a new container. The task name hasn't changed, but it is a new task, as we can tell from its ID:</p>
<pre><strong>$ docker service ps webserver</strong><br/><strong>ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS</strong><br/><strong>lnabvvg6k2ne webserver.1 nginx:alpine sirius Running Running less than a second ago </strong><br/><strong>lb1akyp4dbvc \_ webserver.1 nginx:alpine sirius Shutdown Failed 7 seconds ago "task: non-zero exit (137)"</strong></pre>
<p>Finally, we can review some of the labels that were created by Swarm to fully identify containers using their services. We use <kbd>docker container inspect</kbd> for this:</p>
<pre><strong>$ docker container ls</strong><br/><strong>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</strong><br/><strong>1d9dc2407f74 nginx:alpine "nginx -g 'daemon of…" 13 minutes ago Up 13 minutes 80/tcp webserver.1.lnabvvg6k2ne6boqv3hvqvth8</strong><br/><br/><strong>$ docker container inspect 1d9dc2407f74 --format "{{.Config.Labels}}"</strong><br/><strong>map[com.docker.swarm.node.id:ev4ocuzk61lj0375z80mkba5f com.docker.swarm.service.id:lkcig20f3wpfcbfpe68s72fas com.docker.swarm.service.name:webserver com.docker.swarm.task: com.docker.swarm.task.id:lnabvvg6k2ne6boqv3hvqvth8 com.docker.swarm.task.name:webserver.1.lnabvvg6k2ne6boqv3hvqvth8 maintainer:NGINX Docker Maintainers &lt;docker-maint@nginx.com&gt;]</strong></pre>
<p>There are some service options that can be set using strings to help us identify their configuration and other associated resources. This is very important when we need to isolate resources attached to a specific service's tasks or use some special information to access other services, such as the container's hostname. We can use labels to add meta-information to containers, but there are also Docker Swarm-defined variables that we can use within strings. These variables use Go's template syntax (as we also learned when formatting the listing command's output) and can be used with <kbd>docker service create</kbd> and the <kbd>--hostname</kbd>, <kbd>--mount</kbd>, and <kbd>--env</kbd> arguments.</p>
<p>Therefore, we can set an associated service container's hostname to be unique between tasks using these variables; for example, <kbd>--hostname="{{.Service.Name}}-{{.Task.ID}}"</kbd>. We can even use the node's name to identify this task with the node in which it is running using <kbd>--hostname="{{.Node.Hostname}}"</kbd>. This can be very useful with global services.</p>
<p>The following is a quick list of valid service template substitutions:</p>
<ul>
<li><strong>Service</strong>: <kbd>.Service.ID</kbd>, <kbd>.Service.Name</kbd>, and <kbd>.Service.Labels</kbd></li>
<li><strong>Node</strong>: <kbd>.Node.ID</kbd> and <kbd>.Node.Hostname</kbd></li>
<li><strong>Task</strong>: <kbd>.Task.ID</kbd>, <kbd>.Task.Name</kbd>, and <kbd>.Task.Slot</kbd></li>
</ul>
<p>In the next section, we will introduce some new Docker Swarm objects that will help us deploy our applications on clusters.</p>
<h1 id="uuid-1a0db2ef-1d7a-4277-856f-62b4a0a6ac7c">Deploying applications using Stacks and other Docker Swarm resources</h1>
<p class="mce-root">In this section, we will learn about other Docker Swarm objects that will help us to fully deploy applications within the cluster.</p>
<p>We've already learned how to configure applications using environment variables. This is not recommended for production because anyone with system Docker access can read their values. To avoid this situation, we will use external data sources. We also learned how to integrate host resources inside containers. We can set configurations and passwords in files shared between hosts and containers. This will work on standalone environments but not for distributed workloads, where containers can run on different hosts. We will need to sync those files on all cluster nodes.</p>
<p>To avoid syncing files on multiple nodes, Docker Swarm provides two different objects for managing them. We can have private files or secrets and configurations. Both objects store their values in the Swarm key-value store. Stored values will be available for every cluster node that requires them. These objects are similar, but secrets are used for passwords, certificates, and so on, while config objects are used for application configuration files. Now, let's examine them in depth.</p>
<h2 id="uuid-5bab2267-6957-46de-b49a-78f9716a2f68">Secrets</h2>
<p>A secret is a blob of data that contains passwords, certificates, and any other information that should not be shared over the network. They will be stored in an encrypted fashion so that they're safe from snoopers. Docker Swarm will manage and store secrets for us. Because this kind of data is stored in the key-value store, only managers will have access to any secrets we create. When a container needs to use that stored secret, the host responsible for running that container (a service task container) will have access too. The container will receive a temporal filesystem (in-memory <kbd>tmpfs</kbd> on Linux hosts) containing that secret. When the container dies, the secret will not be accessible on the host. Secrets will only be available to running containers when they are required.</p>
<p>Since secrets are Docker Swarm objects, we can use all of the usual actions (<kbd>list</kbd>, <kbd>create</kbd>, <kbd>remove</kbd>, <kbd>inspect</kbd>, and so on). Do not expect to read secret data with the <kbd>inspect</kbd> action. Once created, it is not possible to read or change a secret's content. We create secrets with files or by using standard input for data. We can add labels for easy listing in big cluster environments.</p>
<p>Once a secret has been created, we can use it within our services. We have both short and long notations. By default, using the short format, a file with secret data will be created under <kbd>/run/secrets/&lt;SECRET_NAME&gt;</kbd>. This file will be mounted in a <kbd>tmpfs</kbd> filesystem on Linux. Windows is different because it does not support on-memory filesystems. We can use the long format to specify the filename to be used for the secret file under <kbd>/run/secrets</kbd>, along with its ownership and file permissions. This will help us avoid root usage inside the container in order to access the file. Let's create a secret with <kbd>docker secret create</kbd> and then use it on a service:</p>
<pre><strong>$ echo this_is_a_super_secret_password|docker secret create app-key -</strong><br/><strong>o9sh44stjm3kxau4c5651ujvr</strong><br/><br/><strong>$ docker service create --name database \</strong><br/><strong> --secret source=ssh-key,target=ssh \</strong><br/><strong> --secret source=app-key,target=app,uid=1000,gid=1001,mode=0400 \</strong><br/><strong> redis:3.0</strong></pre>
<p>As we mentioned previously, it is not possible to retrieve secret data. We can inspect previously created secrets using the common <kbd>docker secret inspect</kbd> action:</p>
<pre><strong>$ docker secret inspect app-key</strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "ID": "o9sh44stjm3kxau4c5651ujvr",</strong><br/><strong>        "Version": {</strong><br/><strong>            "Index": 12</strong><br/><strong>        },</strong><br/><strong>        "CreatedAt": "2019-12-30T20:42:59.050992148Z",</strong><br/><strong>        "UpdatedAt": "2019-12-30T20:42:59.050992148Z",</strong><br/><strong>        "Spec": {</strong><br/><strong>            "Name": "app-key",</strong><br/><strong>            "Labels": {}</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>In the next section, we will learn about configuration objects.</p>
<h2 id="uuid-da403dc7-f5df-49a5-a9c5-fe75b29adb6e">Config</h2>
<p>Config objects are similar to secrets, but they aren't encrypted on the Docker Swarm Raft log and will not be mounted on a <kbd>tmpfs</kbd> filesystem in containers. Configs can be added or removed while service tasks are running. In fact, we can even update service configurations. We will use these objects to store configurations for applications. They can contain strings or binaries (up to 500 KB, which is more than enough for configurations).</p>
<p>When we create a config object, Docker Swarm will store it in the Raft log, which is encrypted, and it will be replicated to other managers by mutual TLS. Therefore, all the managers will have the new config object value.</p>
<p>Using config files on services requires there to be a mount path inside the containers. By default, the mounted configuration file will be world-readable and owned by the user running the container, but we can adjust both properties should we need to.</p>
<p>Let's look at a quick example. We will create a configuration file using <kbd>docker config create</kbd> and then use it inside a service:</p>
<pre><strong>$ echo "This is a sample configuration" | docker config create sample-config -</strong><br/><strong>d0nqny24g5y1tiogwggxmesox</strong><br/><br/><strong>$ docker service create \</strong><br/><strong> --name sample-service \</strong><br/><strong> --config source=sample-config,target=/etc/sample.cfg,mode=0440 \</strong><br/><strong> nginx:alpine</strong></pre>
<p>In this case, we can review the config content and see that it is readable. Using <kbd>docker config inspect</kbd>, we get the following output:</p>
<pre><strong>$ docker config inspect sample-config --pretty</strong><br/><strong>ID: d0nqny24g5y1tiogwggxmesox</strong><br/><strong>Name: sample-config</strong><br/><strong>Created at: 2019-12-10 21:07:51.350109588 +0000 utc</strong><br/><strong>Updated at: 2019-12-10 21:07:51.350109588 +0000 utc</strong><br/><strong>Data:</strong><br/><strong>This is a sample configuration</strong></pre>
<p>Let's move on to stacks.</p>
<h2 id="uuid-3f53e568-1d41-45e2-9a9f-2b66b1edaa11">Stacks</h2>
<p>Stacks help us deploy complete applications. They are <strong>Infrastructure-as-Code</strong> (<strong>IaC</strong>) files with all their component definitions, their interactions, and the external resources required to deploy an application. We will use <kbd>docker-compose</kbd> file definitions (<kbd>docker-compose.yaml</kbd>). Not all <kbd>docker-compose</kbd> file primitive keys will be available. For example, <kbd>depends_on</kbd><strong><em> </em></strong>will not be available for stacks because they don't have dependency declarations. This is something you have to manage in your own application logic.</p>
<p>As we learned with the <kbd>docker-compose</kbd> command in <a href="1c86479c-e4f5-4508-9eca-d29bb3dbaf4b.xhtml">Chapter 5</a>, <em>Deploying Multi-Container Applications</em>, every application that's deployed will run by default in its own network. When using stacks on Docker Swarm, application components are deployed cluster-wide. Overlay networks will be used because each component should reach others, regardless of where they are running. Stacks will also be deployed on their own networks by default.</p>
<p>Stacks deploy applications based on services. Therefore, we will keep our service definitions in the <kbd>docker-compose</kbd> file. To be able to identify these services from other stacks, we will set the stacks' names.</p>
<p>It is important to understand that <kbd>docker-compose</kbd> will deploy multi-container applications on one Docker Engine, while <kbd>docker stack</kbd> will deploy multi-service applications on a Swarm cluster. Note that, nonetheless, both use the same kind of IaC file.</p>
<p>Let's have a quick look at the <kbd>docker stack</kbd> command line:</p>
<ul>
<li><kbd>deploy</kbd>: Deploying Stacks requires a <kbd>docker-compose</kbd> file version of 3.0 and above. We will use the <kbd>deploy</kbd> action to create and run all application components at once. It is also possible to use a Docker Application Bundle file, which is something that will not be covered in this book, but it is good to know that we have multiple options with Docker Stacks for deploying applications on Docker Swarm. As we mentioned previously, we will need to name our stack's deployment to fully identify all its components within the cluster. All of the stack's resources will receive the stack's name as a prefix unless they were externally created from the stack's file definition. In this latter case, they will retain their original names.
<p>These are the main options for <kbd>docker stack deploy</kbd>:</p>
<ul>
<li><kbd>--compose-file</kbd>/<kbd>-c</kbd>: We use <kbd>docker-compose.yaml</kbd> as the stack definition file unless we specify a custom filename with this option.</li>
<li><kbd>--orchestrator</kbd>: This option was recently added and allows us to choose which orchestrator will deploy and manage the stack. We will be able to choose between Docker Swarm and Kubernetes when both are available in our environment.</li>
<li><kbd>--with-registry-auth</kbd>: As we learned with services, sharing authentication is vital when using private registries. Without this option, we can't ensure all the nodes are using the same image or that they have access to the registry because this will depend on locally stored authentication.</li>
</ul>
</li>
<li><kbd>services</kbd>: The <kbd>services</kbd> option shows us a list of the deployed stack's services. As with all other listing actions, we can format and filter its output.</li>
<li><kbd>ps</kbd>: This action lists all the services and where tasks were deployed. It is easy to filter and format its output, as we will see in the <em>Chapter labs</em> section of this chapter.</li>
<li><kbd>ls</kbd>/<kbd>rm</kbd>: These are common object actions for listing and removing them.</li>
</ul>
<p>There is not much more to say about stacks. IaC requires that every deployment is reproducible. Even for a simple standalone service, make sure to use a stack file to deploy it. The <em>Chapter l</em><em>abs</em> section will cover these actions and options with some more examples. In the next section, we will learn how Swarm can change application networking cluster-wide.</p>
<h1 id="uuid-09b013ea-ab67-4ad8-a9f0-0eb94b471331">Networking in Docker Swarm </h1>
<p>When we talk about Docker Swarm, we need to introduce a new concept regarding networks: <em>overlay</em> networks. As we mentioned at the beginning of this chapter, a new network driver will be available because Docker Swarm will distribute all application components across multiple nodes. They have to be reachable no matter where they run. The overlay network will work over VXLAN tunnels using the <strong>User Datagram Protocol</strong> (<strong>UDP</strong>). We will be able to encrypt this communication, but some overhead should normally be expected.</p>
<p>The overlay network driver will create a distributed network across cluster nodes and automatically provides routing of packets to interconnect distributed containers.</p>
<p>When Swarm is first initialized, two networks are created:</p>
<ul>
<li><kbd>docker_gwbridge</kbd>: This bridge network will connect all Docker daemons that are part of the cluster.</li>
<li><kbd>ingress</kbd>: This is an overlay network that will manage Docker Swarm services' control and data traffic. All the services will be connected to this network so that they can reach each other if we do not specify any custom overlay network.</li>
</ul>
<p>Docker Swarm will only manage overlay networks. We can create new overlay networks for our applications that will be isolated from each other. The same happens when working locally with custom bridged networks. We will be able to connect services to different networks at once, as we did with bridged environments. We will also be able to connect containers to overlay networks, although this is not something that's commonly done. Remember that we will not run standalone containers in Docker Swarm.</p>
<p>If firewalls are enabled in your environment, you'll need to allow the following traffic:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 18%" class="CDPAlignCenter CDPAlign">
<p><strong>Port or Range of Ports</strong></p>
</td>
<td style="width: 10.2888%" class="CDPAlignCenter CDPAlign">
<p><strong>Protocol</strong></p>
</td>
<td style="width: 68.7112%" class="CDPAlignCenter CDPAlign">
<p><strong>Purpose</strong></p>
</td>
</tr>
<tr>
<td style="width: 18%" class="CDPAlignCenter CDPAlign">
<p><strong>2377</strong></p>
</td>
<td style="width: 10.2888%" class="CDPAlignCenter CDPAlign">
<p>TCP</p>
</td>
<td style="width: 68.7112%" class="CDPAlignCenter CDPAlign">
<p>Cluster management traffic</p>
</td>
</tr>
<tr>
<td style="width: 18%" class="CDPAlignCenter CDPAlign">
<p><strong>7946</strong></p>
</td>
<td style="width: 10.2888%" class="CDPAlignCenter CDPAlign">
<p>TCP/UDP</p>
</td>
<td style="width: 68.7112%" class="CDPAlignCenter CDPAlign">
<p>Swarm node intercommunication</p>
</td>
</tr>
<tr>
<td style="width: 18%" class="CDPAlignCenter CDPAlign">
<p><strong>4789</strong></p>
</td>
<td style="width: 10.2888%" class="CDPAlignCenter CDPAlign">
<p>UDP</p>
</td>
<td style="width: 68.7112%" class="CDPAlignCenter CDPAlign">
<p>Overlay networking</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Docker Swarm management traffic is always encrypted by default, as we learned in previous sections. We can also encrypt overlay networking. When we use encryption arguments on overlay network creation, Docker Swarm creates <strong>Internet Protocol Security</strong> (<strong>IPSEC</strong>) encryption on overlay VXLANs. It adds security, though a performance overhead is to be expected. It is up to you to manage the balance between security and performance in your applications. As encryption is done upon network creation, it can't be changed once the network has been created.</p>
<p>Creating overlay networks is easy – we just specify the overlay driver with <kbd>docker network create</kbd>:</p>
<pre><strong>$ docker network create -d overlay testnet </strong><br/><strong>1ff11sixrjj7cqppgoxhrdu3z</strong></pre>
<p>By default, it is created unencrypted and non-attachable. This means that containers will not be able to connect to this network. Only services will be allowed. Let's verify this by trying to attach a simple container to the created network using <kbd>docker container run</kbd>:</p>
<pre><strong>$ docker container run -ti --network testnet alpine </strong><br/><strong>Unable to find image 'alpine:latest' locally</strong><br/><strong>latest: Pulling from library/alpine</strong><br/><strong>Digest: sha256:2171658620155679240babee0a7714f6509fae66898db422ad803b951257db78</strong><br/><strong>Status: Downloaded newer image for alpine:latest</strong><br/><strong>docker: Error response from daemon: Could not attach to network testnet: rpc error: code = PermissionDenied desc = network testnet not manually attachable.</strong></pre>
<p>To avoid this, we need to declare the network as attachable from the very beginning. This second example also adds an encryption option using <kbd>docker network create --attachable --opt encrypted</kbd>:</p>
<pre><strong>$ docker network create -d overlay testnet2 --attachable --opt encrypted</strong><br/><strong>9blpskhcvahonytkifn31w91d</strong><br/><br/><strong>$ docker container run -ti --network testnet2 alpine </strong><br/><strong>/ #</strong> </pre>
<p>We connected to the newer sample encrypted network without any problem because it was created with the <kbd>attachable</kbd> property.</p>
<p>All services that are connected to the same overlay network will see each other by their names, and all their exposed ports will be available internally, regardless of whether they are published.</p>
<p>By default, all Swarm overlay networks will have 24-bit masks, which means we will be able to allocate 255 IP addresses. Each service that's deployed may consume multiple IP addresses, as well as one for each node peering on a given overlay network. You may run into IP exhaustion in some situations. To avoid this, consider creating bigger networks if many services need to use them.</p>
<p>In the next section, we will take a closer look at service discovery and how Docker routes traffic to all service replicas.</p>
<h2 id="uuid-a33b6d8c-992f-4d9e-afbe-6b1796ae81ce">Service discovery and load balancing</h2>
<p>Docker Swarm has internal <strong>Internet Protocol Address Management</strong> (<strong>IPAM</strong>) and <strong>Domain Name System</strong> (<strong>DNS</strong>) components to automatically assign a virtual IP address and a DNS entry for each service that's created. Internal load balancing will distribute requests among a service's tasks based on the service's DNS name. As we mentioned earlier, all the services on the same network will know each other and will be reachable on their exposed ports.</p>
<p>Docker Swarm managers (in fact, the leader) will use the created ingress overlay network to publish the services we declared as accessible from outside the cluster. If no port was declared during service creation, Docker Swarm will automatically assign one for each exposed port that's declared in the <kbd>30000</kbd>-<kbd>32767</kbd> range. We have to manually declare any port above <kbd>1024</kbd> because we can't create privileged services.</p>
<p>All the nodes will participate in this ingress router mesh. Therefore, the nodes will accept connections on the published port, regardless of whether they run one of the requested tasks. The router mesh will route all incoming requests to published ports on all the nodes to running tasks (containers). Therefore, published ports will be allocated on all Swarm nodes and hence only one service will be able to use declared ports. In other words, if we publish a service on port <kbd>8080</kbd>, we will not be able to reuse that port for another service. This will limit the maximum number of services that can run on the cluster to the number of free ports available on the Linux or Windows systems used. We learned that Docker Engine will not be able to publish more than one container on the same port using NAT. In this case, all the nodes will fix ports to published services.</p>
<p>The router mesh listens on the published ports on all the node's available IP addresses. We will use cluster-external load balancers to route traffic to the cluster's hosts. We usually use a couple of them for publishing, with the load balancer forwarding all requests to them.</p>
<p>We can use <kbd>docker service update</kbd> to modify or remove already declared ports or add new ones.</p>
<p>The following schema shows how a router mesh works on a three-node cluster publishing a service with two replicas. The colors service runs two tasks. Therefore, one container runs on NODE1 and NODE2, respectively (these are Docker Swarm-scheduled tasks on the nodes in the following diagram). Internally, these containers expose their application on port <kbd>3000</kbd>. The service that defined that container's port as <kbd>3000</kbd> will be published on the host's port; that is, <kbd>8080</kbd>. This port will be published on all the nodes, even if they do not run any service tasks. Internal load balancing will route requests to the appropriate containers using the ingress overlay network. Finally, users will access the published service through an external load balancer. This is not part of the Docker Swarm environment, but it helps us to provide high-availability forwarding requests to a set of available nodes:</p>
<div><img src="img/7af011c5-ed04-4aa2-b68d-5c7e7d199f86.png" style=""/></div>
<p>We will have short and long formats for publishing services. Long formats always provide more options. In the following example, we're publishing an NGINX process on cluster port <kbd>8080</kbd> and forwarding its traffic to the container's port, <kbd>80</kbd>, using <kbd>docker service create --publish</kbd>:</p>
<pre><strong>$ docker service create --name webserver \</strong><br/><strong> --publish published=8080,target=80,protocol=tcp \</strong><br/><strong>nginx:alpine</strong></pre>
<p>On any node, we will be able to access the NGINX service on port <kbd>8080</kbd>. We can test this using the <kbd>curl</kbd> command:</p>
<pre><strong>$ curl -I 0.0.0.0:8080</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Tue, 31 Dec 2019 17:51:26 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong></pre>
<p>We can retrieve the current service tasks' IP addresses by querying the DNS for <kbd>tasks.&lt;SERVICE_NAME&gt;</kbd>.</p>
<p>By default, all the services use the router mesh. However, we can avoid this default behavior, as we will see in the following section.</p>
<h2 id="uuid-2c9f92b2-0d07-4d41-a913-05d79b950805">Bypassing the router mesh</h2>
<p>Using host mode or a <strong>Round-Robin</strong> <strong>DNS</strong> (<strong>RRDNS</strong>) endpoint, we can bypass the router mesh. This will allow us to access instances on given nodes on defined ports or apply our own load balancer. In some situations, we need to include special load balancing features such as weights or persistence of users' sessions. The default Docker Swarm's router mesh behavior will route requests to all available services' backend instances. It is important to identify your application's requirements to determine whether you should deploy its components using Docker Swarm's default load balancing.</p>
<p>Docker's internal load balancer will just do L3 routing. It will not provide any weight-based routing or special features.</p>
<h3 id="uuid-e78ca778-e000-4fe8-9f19-37254fd44427">Using host mode</h3>
<p>Using host mode, only nodes with running instances will receive traffic. We can label nodes so that they only schedule some tasks on them and route traffic to them from load balancers. In this case, we can't run more replicas for this service than the defined and labeled number of nodes.</p>
<p>In the following example, we will run one NGINX process on each node in the cluster since we defined a global service. We will use <kbd>docker service create --mode global --publish  mode=host</kbd>:</p>
<pre><strong>$ docker service create --name webserver \</strong><br/><strong> --publish published=8080,target=80,protocol=tcp,mode=host \</strong><br/><strong> --mode global \</strong><br/><strong>nginx:alpine</strong></pre>
<p>The service's defined port will be available on all the nodes in the cluster.</p>
<h3 id="uuid-557abf84-bea3-450a-a13c-dcce2bca8398">Using Round-Robin DNS mode</h3>
<p>We can also use RRDNS mode to avoid the service's virtual IP address. In this situation, Docker Swarm will not assign a virtual IP for the service, so it will create a service DNS entry with all its replicas' IP addresses. This is useful when we want to use our own load balancer inside the Docker Swarm cluster to deploy this load balancer as another service. It is not easy to maintain the IP addresses of replicas inside the load balancer service. We will probably use DNS resolution inside the load balancer's configuration, querying the DNS to retrieve all instances' IP addresses.</p>
<p>The next section will help us understand the concepts we've learned in this chapter with some labs.</p>
<h1 id="uuid-3dbe6a73-4c21-4714-bd0a-59bf6dd482fd">Chapter labs</h1>
<p>Now, we will complete this chapter's lab to help us improve our understanding of the concepts we've learned. Deploy <kbd>environments/swarm-environment</kbd> from this book's GitHub repository (<a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a>) if you have not done so yet. You can use your own Linux server. Use <kbd>vagrant up</kbd> from the <kbd>environments/swarm</kbd><em> </em>folder to start your virtual environment.</p>
<p>Wait until all your nodes are running. We can check the nodes' status using <kbd>vagrant status</kbd>. Connect to your lab node using <kbd>vagrant ssh swarm-node1</kbd>. Vagrant has deployed four nodes for you. You will be using the <kbd>vagrant</kbd> user with root privileges using <kbd>sudo</kbd>. You should get the following output:</p>
<pre><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant up<br/>--------------------------------------------------------------------------------------------<br/> Docker SWARM MODE Vagrant Environment<br/> Engine Version: current<br/> Experimental Features Enabled<br/>--------------------------------------------------------------------------------------------<br/>Bringing machine 'swarm-node1' up with 'virtualbox' provider...<br/>Bringing machine 'swarm-node2' up with 'virtualbox' provider...<br/>Bringing machine 'swarm-node3' up with 'virtualbox' provider...<br/>Bringing machine 'swarm-node4' up with 'virtualbox' provider...<br/></strong><br/><strong>...</strong><br/><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$</strong></pre>
<p>Nodes will have three interfaces (IP addresses and virtual hardware resources can be modified by changing the <kbd>config.yml</kbd> file):</p>
<ul>
<li><kbd>eth0 [10.0.2.15]</kbd>: Internal, required for Vagrant.</li>
<li><kbd>eth1 [10.10.10.X/24]</kbd>: Prepared for Docker Swarm internal communication. The first node will get the IP address <kbd>10.10.10.11</kbd>, and so on.</li>
<li><kbd>eth2 [192.168.56.X/24]</kbd>: A host-only interface for communication between your host and the virtual nodes. The first node will get the IP address <kbd>192.168.56.11</kbd>, and so on.</li>
</ul>
<p>We will use the <kbd>eth1</kbd> interface for Docker Swarm and we will be able to connect to published applications using the <kbd>192.168.56.X/24</kbd> IP address range. All nodes have Docker Engine Community Edition installed and the <kbd>vagrant</kbd> user is allowed to execute <kbd>docker</kbd>.</p>
<p>Now, we can connect to the first deployed virtual node using <kbd>vagrant ssh swarm-node1</kbd>. This process may vary if you've already deployed a Docker Swarm virtual environment before and just started it using <kbd>vagrant up</kbd>:</p>
<pre><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1</strong><br/><strong>vagrant@swarm-node1:~$</strong></pre>
<p>Now, you are ready to start the labs. Let's start by creating a Docker Swarm cluster.</p>
<h2 id="uuid-c52c3a5e-bb89-4d2f-8762-cec94e804bb1">Creating a Docker Swarm cluster</h2>
<p>Once Vagrant (or your own environment) has been deployed, we will have four nodes (named <kbd>node&lt;index&gt;</kbd>, from <kbd>1</kbd> to <kbd>4</kbd>) with Ubuntu Xenial and Docker Engine installed.</p>
<p>First, review your lab node's IP addresses (<kbd>10.10.10.11</kbd> to <kbd>10.10.10.14</kbd> if you used Vagrant since the first interface will be Vagrant's internal host-to-node interface). Once you are familiar with the environment's IP addresses, we can initiate a cluster on <kbd>node1</kbd>, for example.</p>
<p>If you are using Linux as a VirtualBox host, you can execute <kbd>alias vssh='vagrant ssh'</kbd> on your Terminal to use <kbd>vssh</kbd> instead of <kbd>vagrant ssh</kbd> to connect to nodes as it will be more familiar with non-Vagrant-based real environments.</p>
<p>Now that we have our environment ready for the labs, along with four nodes and Docker Engine already installed, let's get started:</p>
<ol>
<li>Connect to <kbd>node1</kbd> and initialize a new cluster using <kbd>docker swarm init</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1</strong><br/><strong>--------------------------------------------------------------------------------------------</strong><br/><strong> Docker SWARM MODE Vagrant Environment</strong><br/><strong> Engine Version: current</strong><br/><strong> Experimental Features Enabled</strong><br/><strong>--------------------------------------------------------------------------------------------</strong><br/><strong>...</strong><br/><strong>...</strong><br/><br/><strong>vagrant@swarm-node1:~$ docker swarm init</strong><br/><strong>Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 10.10.10.11 on eth1) - specify one with --advertise-addr</strong></pre>
<p style="padding-left: 60px">This is normal if you are using Vagrant as nodes will have at least two interfaces. The first interface is internal to Vagrant, while the other is the one fixed for the labs. In this case, we will need to specify which interface to use for the cluster with <kbd>--advertise-addr</kbd>. We will execute <kbd>docker swarm init --advertise-addr</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node1:~$ docker swarm init --advertise-addr 10.10.10.11</strong><br/><strong>Swarm initialized: current node (b1t5o5x8mqbz77e9v4ihd7cec) is now a manager.</strong><br/><br/><strong>To add a worker to this swarm, run the following command:</strong><br/><br/><strong>    docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-didcmb39w7apwokrah6xx4cus 10.10.10.11:2377</strong><br/><br/><strong>To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.</strong></pre>
<p style="padding-left: 60px">Now, Swarm is initialized correctly.</p>
<ol start="2">
<li>Add a second node that's connecting to <kbd>node2</kbd> and executing the command described in the initialization output. We will join the cluster using the <kbd>docker swarm join</kbd> command with the obtained token:</li>
</ol>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node2</strong><br/><br/><strong>vagrant@swarm-node2:~$ docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-didcmb39w7apwokrah6xx4cus 10.10.10.11:2377</strong><br/><strong>This node joined a swarm as a worker.</strong></pre>
<p style="padding-left: 60px">With this, a node is added as a worker.</p>
<ol start="3">
<li>On <kbd>node1</kbd>, verify that the new node was added by using <kbd>docker node ls</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1</strong><br/><br/><strong>vagrant@swarm-node1:~$ docker node ls</strong><br/><strong>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION</strong><br/><strong>b1t5o5x8mqbz77e9v4ihd7cec * swarm-node1 Ready Active Leader 19.03.5</strong><br/><strong>rj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active 19.03.5</strong></pre>
<p>Notice that <kbd>swarm-node1</kbd> is the leader because this is the node that initialized the cluster. We couldn't have executed <kbd>docker node ls</kbd> on <kbd>swarm-node2</kbd> because it was not a manager node.</p>
<ol start="4">
<li>We will execute the same joining process on <kbd>swarm-node3</kbd>, using <kbd>docker swarm join</kbd> again:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node1:~$ docker node ls</strong><br/><strong>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION</strong><br/><strong>b1t5o5x8mqbz77e9v4ihd7cec * swarm-node1 Ready Active Leader 19.03.5</strong><br/><strong>rj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active 19.03.5</strong><br/><strong>ui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5</strong></pre>
<ol start="5">
<li>Now, we will review the token for managers, so the next node will be added as a manager. We will use <kbd>docker swarm join-token manager</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node1:~$ docker swarm join-token manager</strong><br/><strong>To add a manager to this swarm, run the following command:</strong><br/><br/><strong>    docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-aidvtmglkdyvvqurnivcsmyzm 10.10.10.11:2377</strong></pre>
<p style="padding-left: 60px">Now, we connect to <kbd>swarm-node4</kbd> and execute the shown joining command (<kbd>docker swarm join</kbd>) with the new token:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-aidvtmglkdyvvqurnivcsmyzm 10.10.10.11:2377</strong><br/><strong>This node joined a swarm as a manager</strong></pre>
<ol start="6">
<li>The cluster now has four nodes: two managers and two workers. This will not provide high availability should the leader fail. Let's promote <kbd>swarm-node2</kbd> to the manager role too, for example, by executing <kbd>docker node update --role manager</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node update --role manager swarm-node2</strong><br/><strong>swarm-node2</strong></pre>
<p>We can change node roles using the <kbd>promote</kbd> and <kbd>demote</kbd> commands as well, but it is more convenient to know what they really mean for node property updates. Also, notice that we can change node roles whenever we want, but we should maintain the number of healthy managers.</p>
<p style="padding-left: 60px">We can review the node's status again. Managers are shown as <kbd>Reachable</kbd> or <kbd>Leader</kbd>, indicating that this node is the cluster leader. Using <kbd>docker node ls</kbd>, we get the following output:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node ls</strong><br/><strong>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION</strong><br/><strong>b1t5o5x8mqbz77e9v4ihd7cec swarm-node1 Ready Active Leader 19.03.5</strong><br/><strong>rj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active Reachable 19.03.5</strong><br/><strong>ui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5</strong><br/><strong>jw9uvjcsyg05u1slm4wu0hz6l * swarm-node4 Ready Active Reachable 19.03.5</strong></pre>
<p>Notice that we executed these commands on <kbd>node4</kbd>. We can do this because it is a manager (not a leader, but a manager). We can use any manager to manage the cluster, but only the leader will perform updates on the internal database.</p>
<ol start="7">
<li>We will just leave one manager for the rest of the labs, but first, we will kill the <kbd>node1</kbd> Docker Engine daemon to see what happens in the cluster. We will stop the Docker daemon using <kbd>systemctl stop docker</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1</strong><br/><br/><strong>vagrant@swarm-node1:~$ sudo systemctl stop docker</strong></pre>
<p style="padding-left: 60px">Connect to the other manager (<kbd>node2</kbd>, for example; that is, the recently promoted node). Now, let's review the node's status with <kbd>docker node ls</kbd>:</p>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node2</strong><br/><br/><strong>vagrant@swarm-node2$ docker node ls</strong><br/><strong>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION</strong><br/><strong>b1t5o5x8mqbz77e9v4ihd7cec swarm-node1 Down Active Unreachable 19.03.5</strong><br/><strong>rj3rgb9egnb256cms0zt8pqew * swarm-node2 Ready Active Reachable 19.03.5</strong><br/><strong>ui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5</strong><br/><strong>jw9uvjcsyg05u1slm4wu0hz6l swarm-node4 Ready Active Leader 19.03.5</strong></pre>
<p style="padding-left: 60px">A new leader was elected from among the other running managers. Now, we can start the <kbd>node1</kbd> Docker Engine daemon again using <kbd>systemctl start docker</kbd>:</p>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1</strong><br/><br/><strong>vagrant@swarm-node1$ sudo systemctl start docker</strong><br/><br/><strong>vagrant@swarm-node1$ docker node ls</strong><br/><strong>ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION</strong><br/><strong>b1t5o5x8mqbz77e9v4ihd7cec * swarm-node1 Ready Active Reachable 19.03.5</strong><br/><strong>rj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active Reachable 19.03.5</strong><br/><strong>ui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5</strong><br/><strong>jw9uvjcsyg05u1slm4wu0hz6l swarm-node4 Ready Active Leader 19.03.5</strong></pre>
<p style="padding-left: 60px">The node remains as a manager but is no longer the leader of the cluster because a new one was elected when it failed.</p>
<ol start="8">
<li>Let's demote all non-leader nodes to workers for the rest of the labs using <kbd>docker node update --role worker</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node1$ docker node update --role worker swarm-node2</strong><br/><strong>swarm-node2</strong><br/><br/><strong>vagrant@swarm-node1:~$ docker node update --role worker swarm-node1</strong><br/><strong>swarm-node1</strong><br/><br/><strong>vagrant@swarm-node1:~$ docker node ls</strong><br/><strong>Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.</strong></pre>
<p style="padding-left: 60px">Notice the error when listing again. <kbd>node1</kbd> is not a manager now, so we can't manage the cluster from this node anymore. All management commands will now run from <kbd>node4</kbd> for the rest of the labs. <kbd>node4</kbd> is the only manager, which makes it the cluster leader, as we can observe using <kbd>docker node ls</kbd> once more:</p>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node4</strong><br/><br/><strong>vagrant@swarm-node4:~$ docker node ls</strong><br/><strong>ID                            HOSTNAME      STATUS    AVAILABILITY    MANAGER STATUS    ENGINE VERSION</strong><br/><strong>b1t5o5x8mqbz77e9v4ihd7cec     swarm-node1    Ready    Active                            19.03.5</strong><br/><strong>rj3rgb9egnb256cms0zt8pqew     swarm-node2    Ready    Active                            19.03.5</strong><br/><strong>ui67xyztnw8kn6fjjezjdtwxd     swarm-node3    Ready    Active                            19.03.5</strong><br/><strong>jw9uvjcsyg05u1slm4wu0hz6l *   swarm-node4    Ready    Active        Leader              19.03.5</strong></pre>
<p>In the next lab, we will deploy a simple web server service.</p>
<h2 id="uuid-f6bfc83b-94f1-4aa8-b4fd-518322a14199">Deploying a simple replicated service</h2>
<p>From <kbd>swarm-node4</kbd>, we will create a replicated service (by default) and test how we can distribute more replicas on different nodes. Let's get started:</p>
<ol>
<li>Deploy the <kbd>webserver</kbd> service using a simple <kbd>nginx:alpine</kbd> image by executing <kbd>docker service create</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service create --name webserver nginx:alpine</strong><br/><strong>kh906v3xg1ni98xk466kk48p4</strong><br/><strong>overall progress: 1 out of 1 tasks </strong><br/><strong>1/1: running [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<p style="padding-left: 60px">Notice that we had to wait a few seconds until all the instances were correctly running. The amount of time this takes may vary if the image has some configured health check.</p>
<p>We can overwrite the image-defined health checks on service creation or by updating the configuration using <kbd>--health-cmd</kbd> and other related arguments. In fact, we can change almost everything on a used image, just as we did with containers.</p>
<ol start="2">
<li>Once it is deployed, we can review where the replica was started by using <kbd>docker service ps</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service ps webserver</strong><br/><strong>ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</strong><br/><strong>wb4knzpud1z5        webserver.1         nginx:alpine        swarm-node3               Running             Running 14 seconds ago                                        </strong> </pre>
<p style="padding-left: 60px">In this case, <kbd>nginx</kbd> was deployed on <kbd>swarm-node3</kbd>. This may vary in your environment.</p>
<ol start="3">
<li>We can scale the number of replicas to <kbd>3</kbd> and review how they were distributed. We will use <kbd>docker service update --replicas</kbd> for this:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker service update --replicas 3 webserver</strong><br/><strong>webserver</strong><br/><strong>overall progress: 3 out of 3 tasks </strong><br/><strong>1/3: running [==================================================&gt;] </strong><br/><strong>2/3: running [==================================================&gt;] </strong><br/><strong>3/3: running [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<p style="padding-left: 60px">If we review the replicas' distribution, we can discover where the containers are running using <kbd>docker service ps webserver</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service ps webserver</strong><br/><strong>ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS</strong><br/><strong>wb4knzpud1z5 webserver.1 nginx:alpine swarm-node3 Running Running 2 minutes ago </strong><br/><strong>ie9br2pblxu6 webserver.2 nginx:alpine swarm-node4 Running Running 50 seconds ago </strong><br/><strong>9d021pmvnnrq webserver.3 nginx:alpine swarm-node1 Running Running 50 seconds ago</strong> </pre>
<p style="padding-left: 60px">Notice that, in this case, <kbd>swarm-node2</kbd> did not receive a replica, but we can force replicas to run there.</p>
<ol start="4">
<li>To force specific locations, we can add labels to specific nodes and add constraints to nodes. We'll add a label using <kbd>docker node update --label-add</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node update --label-add tier=front swarm-node2</strong><br/><strong>swarm-node2</strong></pre>
<p style="padding-left: 60px">Now, we can modify the current service so that it runs on specific nodes labeled as <kbd>tier==front</kbd>. We will use <kbd>docker service update --constraint-add node.labels.tier</kbd> and then review its distributed tasks again with <kbd>docker service ps</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service update --constraint-add node.labels.tier==front webserver</strong><br/><strong>webserver</strong><br/><strong>overall progress: 3 out of 3 tasks </strong><br/><strong>1/3: running   [==================================================&gt;] </strong><br/><strong>2/3: running   [==================================================&gt;] </strong><br/><strong>3/3: running   [==================================================&gt;] </strong><br/><strong>verify: Service converged </strong><br/><br/><strong>vagrant@swarm-node4:~$ docker service ps webserver</strong><br/><strong>ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS</strong><br/><strong>wjgkgkn0ullj        webserver.1         nginx:alpine        swarm-node2               Running             Running 24 seconds ago                        </strong><br/><strong>wb4knzpud1z5         \_ webserver.1     nginx:alpine        swarm-node3               Shutdown            Shutdown 25 seconds ago                       </strong><br/><strong>bz2b4dw1emvw        webserver.2         nginx:alpine        swarm-node2               Running             Running 26 seconds ago                        </strong><br/><strong>ie9br2pblxu6         \_ webserver.2     nginx:alpine        swarm-node4               Shutdown            Shutdown 27 seconds ago                       </strong><br/><strong>gwzvykixd5oy        webserver.3         nginx:alpine        swarm-node2               Running             Running 28 seconds ago                        </strong><br/><strong>9d021pmvnnrq         \_ webserver.3     nginx:alpine        swarm-node1               Shutdown            Shutdown 29 seconds ago </strong> </pre>
<p style="padding-left: 60px">Now, all the replicas are running on <kbd>swarm-node2</kbd>.</p>
<ol start="5">
<li>Now, we will perform some maintenance tasks on <kbd>node2</kbd>. In this situation, we will remove the <kbd>service</kbd> constraint before draining <kbd>swarm-node2</kbd>. If we do not do that, no other node will receive workloads because they are restricted to <kbd>tier=front</kbd> node labels. We removed the service's constraints using <kbd>docker service update --constraint-rm node.labels.tier</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service update --constraint-rm node.labels.tier==front webserver</strong><br/><strong>webserver</strong><br/><strong>overall progress: 3 out of 3 tasks </strong><br/><strong>1/3: running   [==================================================&gt;] </strong><br/><strong>2/3: running   [==================================================&gt;] </strong><br/><strong>3/3: running   [==================================================&gt;] </strong><br/><strong>verify: Service converged </strong><br/><br/><strong>vagrant@swarm-node4:~$ docker service ps webserver</strong><br/><strong>ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS</strong><br/><strong>wjgkgkn0ullj        webserver.1         nginx:alpine        swarm-node2               Running             Running 4 minutes ago                        </strong><br/><strong>wb4knzpud1z5         \_ webserver.1     nginx:alpine        swarm-node3               Shutdown            Shutdown 4 minutes ago                       </strong><br/><strong>bz2b4dw1emvw        webserver.2         nginx:alpine        swarm-node2               Running             Running 4 minutes ago                        </strong><br/><strong>ie9br2pblxu6         \_ webserver.2     nginx:alpine        swarm-node4               Shutdown            Shutdown 4 minutes ago                       </strong><br/><strong>gwzvykixd5oy        webserver.3         nginx:alpine        swarm-node2               Running             Running 4 minutes ago                        </strong><br/><strong>9d021pmvnnrq         \_ webserver.3     nginx:alpine        swarm-node1               Shutdown            Shutdown 4 minutes ago                      </strong> </pre>
<p style="padding-left: 60px">The tasks did not move to other nodes because the tasks were already satisfying the service constraints (no constraint in the new situation).</p>
<p>Docker Swarm will never move tasks if it is not really necessary because it will always try to avoid any service disruption. We can force an update regarding service task redistribution by using <kbd>docker service update --force &lt;SERVICE_NAME&gt;</kbd>.</p>
<ol start="6">
<li>In this step, we will pause <kbd>swarm-node3</kbd> and drain <kbd>swarm-node2</kbd>. We will use <kbd>docker node update --availability pause</kbd> and <kbd>docker node update --availability drain</kbd> to do so, respectively:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node update --availability pause swarm-node3</strong><br/><strong>swarm-node3</strong><br/><br/><strong>vagrant@swarm-node4:~$ docker node update --availability drain swarm-node2</strong><br/><strong>swarm-node2</strong></pre>
<p style="padding-left: 60px">Now, let's review our service replica distribution again using <kbd>docker service ps</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running</strong><br/><strong>ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS</strong><br/><strong>6z55nch0q8ai        webserver.1         nginx:alpine        swarm-node4               Running             Running 3 minutes ago                       </strong><br/><strong>8il59udc4iey        webserver.2         nginx:alpine        swarm-node4               Running             Running 3 minutes ago                       </strong><br/><strong>1y4q96hb3hik        webserver.3         nginx:alpine        swarm-node1               Running             Running 3 minutes ago     </strong> </pre>
<p style="padding-left: 60px">Notice that only <kbd>swarm-node1</kbd> and <kbd>swarm-node4</kbd> get some tasks because <kbd>swarm-node3</kbd> is paused and we removed all tasks on <kbd>swarm-node2</kbd>.</p>
<p>We can use <kbd>docker node ps &lt;NODE&gt;</kbd> to get all the tasks from all the services running on the specified node.</p>
<ol start="7">
<li>We will remove the <kbd>webserver</kbd> service and enable nodes <kbd>node2</kbd> and <kbd>node3</kbd> again. We will execute <kbd>docker service rm</kbd> to remove the service:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service rm webserver</strong><br/><strong>webserver</strong><br/><br/><strong>vagrant@swarm-node4:~$ docker node update --availability active swarm-node2</strong><br/><strong>swarm-node2</strong><br/><br/><strong>vagrant@swarm-node4:~$ docker node update --availability active swarm-node3</strong><br/><strong>swarm-node3</strong></pre>
<p>In the next lab, we will create a global service.</p>
<h2 id="uuid-bcd715ee-2439-4aa5-9fdf-bfd9b3431dd4">Deploying a global service</h2>
<p>In this lab, we will deploy a global service. It will run one task on each cluster node. Let's learn how to use <kbd>global</kbd> mode:</p>
<ol>
<li>In this chapter, we learned that global services will deploy one replica on each node. Let's create one and review its distribution. We will use <kbd>docker service create  --mode global</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service create --name webserver --mode global nginx:alpine</strong><br/><strong>4xww1in0ozy3g8q6yb6rlbidr</strong><br/><strong>overall progress: 4 out of 4 tasks </strong><br/><strong>ui67xyztnw8k: running   [==================================================&gt;] </strong><br/><strong>b1t5o5x8mqbz: running   [==================================================&gt;] </strong><br/><strong>rj3rgb9egnb2: running   [==================================================&gt;] </strong><br/><strong>jw9uvjcsyg05: running   [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<p style="padding-left: 60px">All the nodes receive their own replicas, as we can see with <kbd>docker service ps</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running</strong><br/><strong>ID                  NAME                                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS</strong><br/><strong>0jb3tolmta6u        webserver.ui67xyztnw8kn6fjjezjdtwxd   nginx:alpine        swarm-node3               Running             Running about a minute ago                       </strong><br/><strong>im69ybzgd879        webserver.rj3rgb9egnb256cms0zt8pqew   nginx:alpine        swarm-node2               Running             Running about a minute ago                       </strong><br/><strong>knh5ntkx7b3r        webserver.jw9uvjcsyg05u1slm4wu0hz6l   nginx:alpine        swarm-node4               Running             Running about a minute ago                       </strong><br/><strong>26kzify7m7xd        webserver.b1t5o5x8mqbz77e9v4ihd7cec   nginx:alpine        swarm-node1               Running             Running about a minute ago           </strong> </pre>
<ol start="2">
<li>We will now drain <kbd>swarm-node1</kbd>, for example, and review the new task distribution. We will drain the node using <kbd>docker node update --availability drain</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node update --availability drain swarm-node1</strong><br/><strong>swarm-node1</strong><br/><br/><strong>vagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running</strong><br/><strong>ID                  NAME                                  IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS</strong><br/><strong>0jb3tolmta6u        webserver.ui67xyztnw8kn6fjjezjdtwxd   nginx:alpine        swarm-node3               Running             Running 3 minutes ago                       </strong><br/><strong>im69ybzgd879        webserver.rj3rgb9egnb256cms0zt8pqew   nginx:alpine        swarm-node2               Running             Running 3 minutes ago                       </strong><br/><strong>knh5ntkx7b3r        webserver.jw9uvjcsyg05u1slm4wu0hz6l   nginx:alpine        swarm-node4               Running             Running 3 minutes ago                      </strong> </pre>
<p style="padding-left: 60px">None of the nodes received the <kbd>swarm-node1</kbd> task because global services will only run one replica of a defined service.</p>
<ol start="3">
<li>If we enable <kbd>swarm-node1</kbd> once more using <kbd>docker node update --availability active</kbd>, its replica will start to run again:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node update --availability active swarm-node1</strong><br/><strong>node1</strong><br/><strong>vagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running</strong><br/><strong>ID                  NAME                                  IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS</strong><br/><strong>sun8lxwu6p3k        webserver.b1t5o5x8mqbz77e9v4ihd7cec   nginx:alpine        swarm-node1               Running             Running 1 second ago                        </strong><br/><strong>0jb3tolmta6u        webserver.ui67xyztnw8kn6fjjezjdtwxd   nginx:alpine        swarm-node3               Running             Running 5 minutes ago                       </strong><br/><strong>im69ybzgd879        webserver.rj3rgb9egnb256cms0zt8pqew   nginx:alpine        swarm-node2               Running             Running 5 minutes ago                       </strong><br/><strong>knh5ntkx7b3r        webserver.jw9uvjcsyg05u1slm4wu0hz6l   nginx:alpine        swarm-node4               Running             Running 5 minutes ago                      </strong> </pre>
<p>Swarm will run one task of any global service on each node. When a new node joins the cluster, it will also receive one replica of each global service defined in the cluster.</p>
<ol start="4">
<li>We will remove the <kbd>webserver</kbd> service again to clear the cluster for the following labs by using <kbd>docker service rm webserver</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service rm webserver</strong><br/><strong>webserver</strong></pre>
<p>We will now take a quick look at service updates to learn how to update a service's base image.</p>
<h2 id="uuid-c606e006-ace5-4fd0-a634-74e38e36fd08">Updating a service's base image</h2>
<p>Let's learn how to refresh a new image version of a deployed and running service while <em>avoiding</em> user access interruption:</p>
<ol>
<li class="mce-root">First, we create a 6-replica <kbd>webserver</kbd> service using <kbd>docker service create --replicas 6</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service create --name webserver \</strong><br/><strong>--replicas 6 --update-delay 10s --update-order start-first \</strong><br/><strong>nginx:alpine </strong><br/><strong>vpllw7cxlma7mwojdyswbkmbk</strong><br/><strong>overall progress: 6 out of 6 tasks </strong><br/><strong>1/6: running   [==================================================&gt;] </strong><br/><strong>2/6: running   [==================================================&gt;] </strong><br/><strong>3/6: running   [==================================================&gt;] </strong><br/><strong>4/6: running   [==================================================&gt;] </strong><br/><strong>5/6: running   [==================================================&gt;] </strong><br/><strong>6/6: running   [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<ol start="2">
<li>Next, we update to a specific <kbd>nginx:alpine</kbd> version with <kbd>perl</kbd> support, for example. We use <kbd>docker service update --image</kbd> to change only its base image:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service update --image nginx:alpine-perl webserver</strong><br/><strong>webserver</strong><br/><strong>overall progress: 6 out of 6 tasks </strong><br/><strong>1/6: running [==================================================&gt;] </strong><br/><strong>2/6: running [==================================================&gt;] </strong><br/><strong>3/6: running [==================================================&gt;] </strong><br/><strong>4/6: running [==================================================&gt;] </strong><br/><strong>5/6: running [==================================================&gt;] </strong><br/><strong>6/6: running [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<p style="padding-left: 60px">The update took more than 60 seconds because Swarm updated tasks one by one at 10-second intervals. It will first start the new container with the newly defined image. Once it is healthy, it will stop the old version of the container. This must be done on each task and therefore takes more time, but this way, we can ensure that there is always a <kbd>webserver</kbd> task running. In this example, we have not published any <kbd>webserver</kbd> ports, so no user interaction is expected. It is just a simple lab – but real-life environments will be the same, and internal Docker Swarm load balancing will always guide the user's requests to alive instances while an update is running.</p>
<p style="padding-left: 60px">The new version is running now, as we can observe by using <kbd>docker service ps</kbd> again:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running</strong><br/><strong>ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS</strong><br/><strong>n9s6lrk8zp32        webserver.1         nginx:alpine-perl   swarm-node4               Running             Running 4 minutes ago                       </strong><br/><strong>68istkhse4ei        webserver.2         nginx:alpine-perl   swarm-node1               Running             Running 5 minutes ago                       </strong><br/><strong>j6pqig7njhdw        webserver.3         nginx:alpine-perl   swarm-node1               Running             Running 6 minutes ago                       </strong><br/><strong>k4vlmeb56kys        webserver.4         nginx:alpine-perl   swarm-node2               Running             Running 5 minutes ago                       </strong><br/><strong>k50fxl1gms44        webserver.5         nginx:alpine-perl   swarm-node3               Running             Running 5 minutes ago                       </strong><br/><strong>apur3w3nq95m        webserver.6         nginx:alpine-perl   swarm-node3               Running             Running 5 minutes ago      </strong> </pre>
<ol start="3">
<li>We will remove the <kbd>webserver</kbd> service again to clear the cluster for the following labs using <kbd>docker service rm</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service rm webserver</strong><br/><strong>webserver</strong></pre>
<p>In the next lab, we will deploy applications using stacks instead of creating services manually, which might lead to us making configuration errors, for example. Using stacks will provide environment reproducibility because we will always run the same IaC definitions.</p>
<h2 id="uuid-31942c0f-9187-45d4-ba15-66229827876f">Deploying using Docker Stacks</h2>
<p>In this lab, we will deploy a PostgreSQL database using secrets, configurations, and volumes on an IaC file. This file will contain all the application's requirements and will be used to deploy the application as a Docker Stack. Let's get started:</p>
<ol>
<li>First, we will create a secret for the required PostgreSQL admin user password. We will execute <kbd>docker service create</kbd> with the standard input as the secret content:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ echo SuperSecretPassword|docker secret create postgres_password -</strong><br/><strong>u21mmo1zoqqguh01u8guys9gt</strong></pre>
<p style="padding-left: 60px">We will use it as an external secret inside the <kbd>docker-compose</kbd> file.</p>
<ol start="2">
<li>We are going to create a simple initialization script to create a new database when PostgreSQL starts. We will create a simple file in the current directory named <kbd>create-docker-database.sh</kbd> with the following content and appropriate <kbd>755</kbd> permissions:</li>
</ol>
<pre style="padding-left: 60px">#!/bin/bash<br/>set -e<br/><br/>psql -v ON_ERROR_STOP=0 --username "$POSTGRES_USER" --dbname "$POSTGRES_DB" &lt;&lt;-EOSQL<br/>    CREATE USER docker;<br/>    CREATE DATABASE docker;<br/>    GRANT ALL PRIVILEGES ON DATABASE docker TO docker;<br/>EOSQL</pre>
<p style="padding-left: 60px">Then, we create a config file with the file's content. We will use this file to create a database named <kbd>docker</kbd> on starting up PostgreSQL. This is something we can use because it is provided by the official Docker Hub PostgreSQL image. We will use <kbd>docker config create</kbd> with the <kbd>create-docker-database.sh</kbd> file:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker config create create-docker-database ./create-docker-database.sh</strong><br/><strong>uj6zvrdq0682anzr0kobbyhk2</strong></pre>
<ol start="3">
<li>We will add labels to some of the nodes to ensure the database is always running there since we will create an external volume only on that node. For this example, we will use <kbd>node2</kbd>. We will create a volume using <kbd>docker volume create</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node2</strong><br/><br/><strong>vagrant@swarm-node2:~$ docker volume create PGDATA</strong><br/><strong>PGDATA</strong></pre>
<p style="padding-left: 60px">This volume will only exist on <kbd>swarm-node2</kbd>, so we will create a constraint based on a node label to run the service task only on <kbd>swarm-node2</kbd>. We will use <kbd>docker node update --label-add tier=database</kbd> for this:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker node update --label-add tier=database swarm-node2</strong><br/><strong>swarm-node2<br/></strong></pre>
<p>This is a simple sample. In your production environment, you will never use local volumes. We will need to define and use some plugin that allows us to share the same volume on different hosts, such as NFS and RexRay.</p>
<ol start="4">
<li>Now, we will create the following Docker Compose file, named <kbd>postgres-stack.yaml</kbd>:</li>
</ol>
<pre style="padding-left: 60px">version: '3.7'<br/>services:<br/>  database:<br/>    image: postgres:alpine<br/>    deploy:<br/>      placement:<br/>        constraints:<br/>          - node.role == worker<br/>          - node.labels.tier == database<br/>    environment:<br/>      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password<br/>    secrets:<br/>      - source: postgres_password<br/>        target: "/run/secrets/postgres_password"<br/>    configs:<br/>      - source: create-docker-database<br/>        target: "/docker-entrypoint-initdb.d/create-db.sh"<br/>        mode: 0755<br/>        uid: "0"<br/>    volumes:<br/>      - type: volume<br/>        source: PGDATA<br/>        target: /var/lib/postgresql/data<br/>    ports:<br/>      - target: 5432<br/>        published: 15432<br/>        protocol: tcp<br/>    networks:<br/>      net:<br/>        aliases:<br/>         - postgres<br/>         - mydatabase<br/>configs:<br/>  create-docker-database:<br/>    external: true<br/>secrets:<br/>  postgres_password:<br/>    external: true<br/>volumes:<br/>  PGDATA:<br/>    external: true<br/>networks:<br/>  net:<br/>    driver: overlay<br/>    attachable: true</pre>
<p style="padding-left: 60px">Take note of the following things in this file; we added a lot of learned information here:</p>
<ul>
<li>We defined the <kbd>postgres:alpine</kbd> image for the <kbd>database</kbd> service.</li>
<li>The <kbd>database</kbd> service will only be scheduled on worker nodes with a <kbd>tier</kbd> label key and a value of <kbd>database</kbd>. In this case, it will run tasks only on <kbd>node2</kbd>.</li>
<li>The <kbd>postgres</kbd> image can use Docker Swarm secret files as environment variables, and in this case, it will use <kbd>postgres_password</kbd> mounted on <kbd>/run/secrets/postgres_password</kbd>. The secret is declared externally because it was previously created outside of this file.</li>
<li>We also added a config file to create an initial database called <kbd>docker</kbd>. The config file is external as well because we added it outside the <kbd>postgres-stack.yaml</kbd> file.</li>
<li>We also added an external volume named <kbd>PGDATA</kbd>. We will use this volume for the database but it will only exist on <kbd>node2</kbd>. It is defined as external because we manually create the <kbd>PGDATA</kbd> volume locally on <kbd>node2</kbd>.</li>
<li>We published the PostgreSQL application's port <kbd>5432</kbd> on the host's port; that is, <kbd>15432</kbd>. We changed the published port to recognize that they are not the same because <kbd>5432</kbd> will be an internal port on the defined network named <kbd>net</kbd>.</li>
<li>Finally, we defined the <kbd>net</kbd> network as <kbd>attachable</kbd> to be able to test our database with a simple container running a <kbd>postgres</kbd> client. We added two aliases to the <kbd>database</kbd> service inside this network: <kbd>postgres</kbd> and <kbd>mydatabase</kbd>.</li>
</ul>
<p>Notice that all the objects that were created for the stack will use the stack's name as a prefix. This will not happen on externally defined objects. They will be used, but we create them manually, outside of the stack's life cycle.</p>
<ol start="5">
<li>We deploy the <kbd>postgres</kbd> stack using <kbd>docker stack deploy</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker stack deploy -c postgres-stack.yaml postgres</strong><br/><strong>Creating network postgres_net</strong><br/><strong>Creating service postgres_database</strong></pre>
<p style="padding-left: 60px">We can easily review the stack's status using <kbd>docker stack ps</kbd>.</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker stack ps postgres</strong><br/><strong>ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS</strong><br/><strong>53in2mik27r0 postgres_database.1 postgres:alpine swarm-node2 Running Running 19 seconds ago</strong> </pre>
<p style="padding-left: 60px">It is running on <kbd>swarm-node2</kbd>, as we expected.</p>
<ol start="6">
<li>We published port <kbd>5432</kbd> on port <kbd>15432</kbd>. We can connect to this port from any node IP address in the cluster because Swarm uses a routing mesh. We use the <kbd>curl</kbd> command to review the port's availability:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ curl 0.0.0.0:15432</strong><br/><strong>curl: (52) Empty reply from server</strong><br/><br/><strong>vagrant@swarm-node2:~$ curl 0.0.0.0:15432</strong><br/><strong>curl: (52) Empty reply from server</strong><br/><br/><strong>vagrant@swarm-node3:~$ curl 0.0.0.0:15432</strong><br/><strong>curl: (52) Empty reply from server</strong></pre>
<p style="padding-left: 60px">We get this response to <kbd>curl</kbd> because we are not using the right software client (but the ports are listening). Let's run a simple <kbd>alpine</kbd> container with the <kbd>postgres</kbd> client.</p>
<ol start="7">
<li>Now, we can run a simple <kbd>alpine</kbd> container attached to the stack's deployed network. In this example, it is <kbd>postgres_net</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker network ls --filter name=postgres_net</strong><br/><strong>NETWORK ID NAME DRIVER SCOPE</strong><br/><strong>mh53ek97pi3a postgres_net overlay swarm</strong></pre>
<p style="padding-left: 60px" class="mce-root">Here, we ran a simple <kbd>alpine</kbd> container and installed the <kbd>postgresql-client</kbd> package using <kbd>docker container run</kbd> with an appropriate network:</p>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker container run -ti --network postgres_net alpine</strong><br/><strong>Unable to find image 'alpine:latest' locally</strong><br/><strong>latest: Pulling from library/alpine</strong><br/><strong>e6b0cf9c0882: Pull complete </strong><br/><strong>Digest: sha256:2171658620155679240babee0a7714f6509fae66898db422ad803b951257db78</strong><br/><strong>Status: Downloaded newer image for alpine:latest</strong><br/><strong>/ # apk add --update --no-cache postgresql-client --quiet</strong></pre>
<p style="padding-left: 60px">Remember that we added the <kbd>mydatabase</kbd> and <kbd>postgres</kbd> aliases to the <kbd>database</kbd> service. Therefore, any of them will be valid for testing database connectivity since Swarm added these entries to the internal DNS. We can test this by running a simple <kbd>ping</kbd> command inside the container:</p>
<pre style="padding-left: 60px"><strong>/ # ping -c 1 mydatabase</strong><br/><strong>PING mydatabase (10.0.3.2): 56 data bytes</strong><br/><strong>64 bytes from 10.0.3.2: seq=0 ttl=64 time=0.237 ms</strong><br/><strong>--- mydatabase ping statistics ---</strong><br/><strong>1 packets transmitted, 1 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.237/0.237/0.237 ms</strong><br/><br/><strong>/ # ping -c 1 postgres</strong><br/><strong>PING postgres (10.0.3.2): 56 data bytes</strong><br/><strong>64 bytes from 10.0.3.2: seq=0 ttl=64 time=0.177 ms</strong><br/><strong>--- postgres ping statistics ---</strong><br/><strong>1 packets transmitted, 1 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.177/0.177/0.177 ms</strong><br/><br/><strong>/ # ping -c 1 database</strong><br/><strong>PING database (10.0.3.2): 56 data bytes</strong><br/><strong>64 bytes from 10.0.3.2: seq=0 ttl=64 time=0.159 ms</strong><br/><strong>--- database ping statistics ---</strong><br/><strong>1 packets transmitted, 1 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.159/0.159/0.159 ms</strong></pre>
<p style="padding-left: 60px">We will use the installed client to test our deployed PostgreSQL. Remember to use the previously defined password that we created as a secret, <kbd>SuperSecretPassword</kbd>. We will test our database's connectivity using the <kbd>psql</kbd> command:</p>
<pre style="padding-left: 60px"><strong>/ # psql -h mydatabase -U postgres</strong><br/><strong>Password for user postgres: </strong><br/><strong>psql (12.1)</strong><br/><strong>Type "help" for help.</strong><br/><br/><strong>postgres=# \l</strong><br/><strong>                                 List of databases</strong><br/><strong>   Name | Owner | Encoding | Collate | Ctype | Access privileges </strong><br/><strong>-----------+----------+----------+------------+------------+-----------------------</strong><br/><strong> docker | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =Tc/postgres +</strong><br/><strong>           | | | | | postgres=CTc/postgres+</strong><br/><strong>           | | | | | docker=CTc/postgres</strong><br/><strong> postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | </strong><br/><strong> template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres +</strong><br/><strong>           | | | | | postgres=CTc/postgres</strong><br/><strong> template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres +</strong><br/><strong>           | | | | | postgres=CTc/postgres</strong><br/><strong>(4 rows)</strong><br/><br/><strong>postgres=#</strong> </pre>
<p style="padding-left: 60px">We listed the deployed databases using <kbd>\l</kbd> and the <kbd>docker</kbd> database, which was created with our <kbd>create-db.sh</kbd> script. Notice that we used the default PostgreSQL database port <kbd>5432</kbd> (we omitted any port customization on client request) instead of <kbd>15432</kbd>. This is because the <kbd>docker</kbd> container was connecting to the database internally. Both the <kbd>postgres_database.1</kbd> task and the externally run container are using the same network, <kbd>postgres_net</kbd>.</p>
<p>Notice that we can use all learned options with the created stack service, <kbd>postgres_database</kbd>. Anyway, we can modify the Docker Compose file and redeploy the same stack again with some changes. Swarm will review the required updates and take the necessary actions on all components.</p>
<p style="padding-left: 60px">Let's exit the running container by executing the <kbd>exit</kbd> command, and then remove the <kbd>postgres</kbd> stack and <kbd>node2</kbd> volume to clean up for the following labs using <kbd>docker stack rm</kbd>:</p>
<pre style="padding-left: 60px"><strong>postgres=# exit</strong><br/><strong>/ # exit</strong><br/><br/><strong>vagrant@swarm-node4:~$ docker stack rm postgres</strong><br/><strong>Removing service postgres_database</strong><br/><strong>Removing network postgres_net</strong></pre>
<p>In the next lab, we will launch a simple replicated service and review internal ingress load balancing.</p>
<h2 id="uuid-53e57021-be92-40d1-a134-b1e856a82c3f">Swarm ingress internal load balancing</h2>
<p>In this lab, we will use the <kbd>codegazers/colors:1.13</kbd> image. This is a simple application that will show different random front page colors or texts. Let's get started:</p>
<ol>
<li>Let's create a service named <kbd>colors</kbd> based on the <kbd>codegazers/colors:1.13</kbd> image. Since we won't be setting any specific color using environment variables, random ones will be chosen for us. Use <kbd>docker service create --constraint node.role==worker</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service create --name colors \</strong><br/><strong> --publish 8000:3000 \</strong><br/><strong>--constraint node.role==worker \</strong><br/><strong>codegazers/colors:1.13 </strong><br/><br/><strong>mkyz0d94ovb144xmvo0q4py41</strong><br/><strong>overall progress: 1 out of 1 tasks </strong><br/><strong>1/1: running [==================================================&gt;] </strong><br/><strong>verify: Service converged</strong> </pre>
<p style="padding-left: 60px">We chose not to run a replica on the manager node because we will use <kbd>curl</kbd> from <kbd>node4</kbd> in this lab.</p>
<ol start="2">
<li>Let's test local connectivity from the <kbd>swarm-node4</kbd> manager with <kbd>curl</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ curl 0.0.0.0:8000/text</strong><br/><strong>APP_VERSION: 1.0</strong><br/><strong>COLOR: orange</strong><br/><strong>CONTAINER_NAME: d3a886d5fe34</strong><br/><strong>CONTAINER_IP: 10.0.0.11 172.18.0.3</strong></pre>
<pre style="padding-left: 60px"><strong>CLIENT_IP: ::ffff:10.0.0.5</strong><br/><strong>CONTAINER_ARCH: linux</strong></pre>
<p style="padding-left: 60px">We deployed one replica and it is running the <kbd>orange</kbd> color. Take note of the container's IP address and its name.</p>
<ol start="3">
<li>Let's run five more replicas by executing <kbd>docker service update --replicas 6</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service update --replicas 6 colors --quiet</strong><br/><strong>colors</strong></pre>
<ol start="4">
<li>If we test service port <kbd>8080</kbd> with <kbd>curl</kbd> once more, we will get different colors. This is because the containers were launched without color settings:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ curl 0.0.0.0:8000/text</strong><br/><strong>APP_VERSION: 1.0</strong><br/><strong>COLOR: red</strong><br/><strong>CONTAINER_NAME: 64fb2a3009b2</strong><br/><strong>CONTAINER_IP: 10.0.0.12 172.18.0.4</strong><br/><strong>CLIENT_IP: ::ffff:10.0.0.5</strong><br/><strong>CONTAINER_ARCH: linux</strong><br/><br/><strong>vagrant@swarm-node4:~$ curl 0.0.0.0:8000/text</strong><br/><strong>APP_VERSION: 1.0</strong><br/><strong>COLOR: cyan</strong><br/><strong>CONTAINER_NAME: 73b07ee0c287</strong><br/><strong>CONTAINER_IP: 10.0.0.14 172.18.0.3</strong><br/><strong>CLIENT_IP: ::ffff:10.0.0.5</strong><br/><strong>CONTAINER_ARCH: linux</strong></pre>
<p style="padding-left: 60px">We get different colors on different containers. The router mesh is guiding our requests to the <kbd>colors</kbd> tasks' containers using the ingress overlay network.</p>
<p>We can access all the <kbd>colors</kbd> service task logs using <kbd>docker service logs colors</kbd>.</p>
<ol start="5">
<li>Let's remove the <kbd>colors</kbd> service for the next and final lab using <kbd>docker service rm</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagranr@swarm-node4:~$ docker service rm colors</strong><br/><strong>colors</strong></pre>
<p>In the next lab, we will review service endpoint modes and consider how DNS resolves <kbd>vip</kbd> and <kbd>dnsrr</kbd> situations.</p>
<h2 id="uuid-94b81dda-6435-4176-a254-56da5322dad2">Service discovery</h2>
<p>In this lab, we will create a test overlay attachable network and review DNS entries for the <kbd>vip</kbd> and <kbd>dnsrr</kbd> endpoint modes. Let's get started:</p>
<ol>
<li>First, we need to create an attachable overlay <kbd>test</kbd> network using <kbd>docker network create --attachable -d overlay</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker network create --attachable -d overlay test</strong><br/><strong>32v9pibk7cqfseknretmyxfsw</strong></pre>
<ol start="2">
<li>Now, let's create two different <kbd>colors</kbd> services. Each one will use different endpoint modes. For the <kbd>vip</kbd> mode, we will use <kbd>docker service create</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker service create --replicas 2 \</strong><br/><strong>--name colors-vip --network test --quiet codegazers/colors:1.13</strong><br/><strong>4m2vvbnqo9wgf8awnf53zr5b2</strong></pre>
<p style="padding-left: 60px">Let's create the second one for <kbd>dnsrr</kbd> using <kbd>docker service create --endpoint-mode dnsrr</kbd>, as follows:</p>
<pre style="padding-left: 60px" class="mce-root"><strong>vagrant@swarm-node4:~$ docker service create --replicas 2 \</strong><br/><strong>--name colors-dnsrr --network test --quiet --endpoint-mode dnsrr codegazers/colors:1.13</strong><br/><strong>wqpv929pe5ehniviclzkdvcl0</strong></pre>
<ol start="3">
<li class="mce-root">Now, let's run a simple <kbd>alpine</kbd> container on the <kbd>test</kbd> network using <kbd>docker container run</kbd> and test the internal name resolution functionality. We will need to install the <kbd>bind-tools</kbd> package to be able to use the <kbd>host</kbd> and <kbd>nslookup</kbd> tools:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker run -ti --rm --network test alpine </strong><br/><strong>/ # apk add --update --no-cache bind-tools --quiet</strong><br/><strong>/ # host colors-vip</strong><br/><strong>colors-vip has address 10.0.4.2</strong><br/><strong>/ # host colors-dnsrr</strong><br/><strong>colors-dnsrr has address 10.0.4.7</strong><br/><strong>colors-dnsrr has address 10.0.4.8</strong><br/><strong>/ #exit</strong></pre>
<p style="padding-left: 60px">As expected, when using the <kbd>vip</kbd> endpoint mode, the service receives a virtual IP address. All requests will be redirected to that address and ingress will route to the appropriate container using internal load balancing.</p>
<p style="padding-left: 60px">On the other hand, using the <kbd>dnsrr</kbd> endpoint will not provide a virtual IP address. The internal DNS will add an entry for each container IP.</p>
<ol start="4">
<li>We can also take a look at the containers attached to the <kbd>test</kbd> network. These containers will get one internal IP address and one that will be routed on the overlay network. We can launch the <kbd>ip add show</kbd> command attached to one of the running <kbd>colors-dnsrr</kbd> tasks' containers using <kbd>docker container exec</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@swarm-node4:~$ docker exec -ti colors-dnsrr.1.vtmpdf0w82daq6fdyk0wwzqc7 ip add show</strong><br/><strong>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</strong><br/><strong> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</strong><br/><strong> inet 127.0.0.1/8 scope host lo</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>111: eth0@if112: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue state UP </strong><br/><strong> link/ether 02:42:0a:00:04:07 brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 10.0.4.7/24 brd 10.0.4.255 scope global eth0</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><strong>113: eth1@if114: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </strong><br/><strong> link/ether 02:42:ac:12:00:04 brd ff:ff:ff:ff:ff:ff</strong><br/><strong> inet 172.18.0.4/16 brd 172.18.255.255 scope global eth1</strong><br/><strong> valid_lft forever preferred_lft forever</strong><br/><br/></pre>
<p>All Vagrant environments can easily be removed by executing <kbd>vagrant destroy -f</kbd> to remove all previously created nodes for this lab. This command should be executed on your <kbd>environments/swarm</kbd> local directory.</p>
<p>Remove all the services that you created for this last lab with <kbd>docker service rm colors-dnsrr colors-vip</kbd>.</p>
<h1 id="uuid-d53d5099-0198-46a9-ab35-caa3883055ba" class="mce-root">Summary</h1>
<p class="mce-root">In this chapter, we reviewed how to deploy and work with the Docker Swarm orchestrator. This is the default orchestrator in Docker as it comes out of the box with Docker Engine.</p>
<p class="mce-root">We learned about Docker Swarm's features and how to deploy applications using stacks (IaC files) and services instead of containers. Orchestration will manage the application's components to keep them running, helping us to even upgrade them without impacting users. Docker Swarm also introduced new objects such as secrets and config, which help us distribute workloads within cluster nodes. Volumes and networks should be managed cluster-wide. We also learned about overlay networking and how Docker Swarm's router mesh has simplified application publishing.</p>
<p>In the next chapter, we will learn about the Kubernetes orchestrator. Currently, Kubernetes is a small part of the Docker Certified Associate exam, but this will probably be increased in the following releases. It is also useful for you to know and understand the concepts of Kubernetes alongside Docker Swarm. Docker Enterprise provides both and we can make them work together.</p>
<h1 id="uuid-3bed935f-44a3-4bd2-81bb-fe6b0c8949ab" class="mce-root">Questions</h1>
<ol>
<li>Choose all of the false statements from the following options:</li>
</ol>
<p style="padding-left: 90px">a) Docker Swarm is the only orchestrator that can work with Docker.<br/>
b) Docker Swarm comes included out of the box with Docker Engine.<br/>
c) Docker Swarm will allow us to deploy applications on a pool of nodes working together, known as a cluster.<br/>
d) All of the preceding statements are false.</p>
<ol start="2">
<li>Which of the following statements are false regarding what Swarm provides by default?</li>
</ol>
<p style="padding-left: 90px">a) Service discovery<br/>
b) Internal load balancing<br/>
c) Overlay networking among distributed containers on cluster nodes<br/>
d) All of the preceding statements are false</p>
<ol start="3">
<li>Which of the following statements are true in relation to managers?</li>
</ol>
<p style="padding-left: 90px">a) We can't create replicated services with tasks running on managers.<br/>
b) There is just one leader node on each cluster that manages all Swarm cluster changes and object statuses.<br/>
c) If the leader node dies, all the changes will be frozen until the leader node is healthy again.<br/>
d) All of the preceding statements are true.</p>
<ol start="4">
<li>Which of the following statements are false in relation to workers?</li>
</ol>
<p style="padding-left: 90px">a) Worker nodes just run workloads.<br/>
b) If we drain a worker node, all the workloads running on that node will be moved to other available nodes.<br/>
c) Swarm roles can be changed for any node in the cluster whenever this is required.<br/>
d) All of the preceding statements are true.</p>
<ol start="5">
<li>Which of the following statements are false about Swarm Stacks?</li>
</ol>
<p style="padding-left: 90px">a) By default, all Stacks will be deployed on their own networks.<br/>
b) Stacks will use Docker Compose files to define all application components.<br/>
c) Everything that's used for a Stack should be defined inside the <kbd>docker-compose</kbd> file. We can't add external objects.<br/>
d) All of the preceding statements are true.<strong><br/></strong></p>
<h1 id="uuid-797cabcf-86d5-46c8-a8ba-e4532d3c2d4b">Further reading</h1>
<p>Refer to the following links for more information regarding the topics that were covered in this chapter:</p>
<ul>
<li>Docker Swarm overview: <a href="https://docs.docker.com/engine/swarm/">https://docs.docker.com/engine/swarm/</a></li>
<li>Deploying applications on Docker Swarm: <a href="https://docs.docker.com/get-started/swarm-deploy/">https://docs.docker.com/get-started/swarm-deploy/</a></li>
<li>Orchestration with Docker Swarm: <a href="https://hub.packtpub.com/orchestration-docker-swarm/">https://hub.packtpub.com/orchestration-docker-swarm/</a></li>
<li>Native Docker clustering with Swarm: <a href="https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm">https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm</a></li>
</ul>


            

            
        
    </body></html>