["```\n$ docker swarm init\nSwarm initialized: current node (ev4ocuzk61lj0375z80mkba5f) is now a manager.\nTo add a worker to this swarm, run the following command:\ndocker swarm join --token SWMTKN-1-4dtk2ieh3rwjd0se5rzwyf2hbk7zlyxh27pbh4plg2sn0qtitx-50zsub5f0s4kchwjcfcbyuzn5  192.168.200.18:2377\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n```", "```\n$ docker node ls\nID                            HOSTNAME    STATUS AVAILABILITY     MANAGER STATUS ENGINE VERSION\nev4ocuzk61lj0375z80mkba5f    * sirius     Ready     Active         Leader             19.03.2\n```", "```\n$ docker swarm join --token SWMTKN-1-4dtk2ieh3rwjd0se5rzwyf2hbk7zlyxh27pbh4plg2sn0qtitx-50zsub5f0s4kchwjcfcbyuzn5 192.168.200.18:2377\n```", "```\n$ docker node ls\nID                         HOSTNAME     STATUS   AVAILABILITY MANAGER STATUS  ENGINE VERSION\nglc1ovbcqubmfw6vgzh5ocjgs   antares     Ready     Active                          19.03.5\nev4ocuzk61lj0375z80mkba5f * sirius      Ready     Active          Leader          19.03.2\n```", "```\n$ docker node inspect antares \n[\n {\n \"ID\": \"glc1ovbcqubmfw6vgzh5ocjgs\",\n...\n \"Spec\": {\n \"Labels\": {},\n \"Role\": \"worker\",\n \"Availability\": \"active\"\n },\n \"Description\": {\n \"Hostname\": \"antares\",\n \"Platform\": {\n \"Architecture\": \"x86_64\",\n \"OS\": \"linux\"\n },\n \"Resources\": {\n \"NanoCPUs\": 16000000000,\n \"MemoryBytes\": 33736785920\n },\n \"Engine\": {\n \"EngineVersion\": \"19.03.5\",\n            ...\n ...\n         },\n \"TLSInfo\": {\n \"TrustRoot\": \"-----BEGIN CERTIFICATE-----\\nMIIBaTCCARCgAwIBAgIUUB8yKqt3uUh2wmF/z450dyg9EDAwCgYIKoZIzj0EAwIw\\nEzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMTkxMjI5MTA1NTAwWhcNMzkxMjI0MTA1\\nNTAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH\\nA0IABACDe6KWpqXiEMyWB9Qn6y2O2+wH8HLoikR+48xqnjeU0SkW/+rPQkW9PilB\\ntIYGwaviLPXpuL4EpVBWxHtMDQCjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB\\nAf8EBTADAQH/MB0GA1UdDgQWBBTbL48HmUp/lYB1Zqu3GL7q5oMrwTAKBggqhkjO\\nPQQDAgNHADBEAiAh1TVNulaIHf2vh6zM9v6raer5WgTcGu8xQYBcDViPnwIgU4sl\\ntK70bgSfEzLx6WpOv4yjr+c0tlJt/6Gj3waQl10=\\n-----END CERTIFICATE-----\\n\",\n \"CertIssuerSubject\": \"MBMxETAPBgNVBAMTCHN3YXJtLWNh\",\n \"CertIssuerPublicKey\": \"MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAIN7opampeIQzJYH1CfrLY7b7AfwcuiKRH7jzGqeN5TRKRb/6s9CRb0+KUG0hgbBq+Is9em4vgSlUFbEe0wNAA==\"\n }\n },\n \"Status\": {\n \"State\": \"ready\",\n \"Addr\": \"192.168.200.15\"\n }\n }\n]\n```", "```\n$ docker node update --role manager antares\nantares\n```", "```\n$ docker node ls --filter role=manager\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nglc1ovbcqubmfw6vgzh5ocjgs antares Ready Active Reachable 19.03.5\nev4ocuzk61lj0375z80mkba5f * sirius Ready Active Leader 19.03.2\n```", "```\n$ docker node inspect antares --format \"{{.ManagerStatus}}\"\n{false reachable 192.168.200.15:2377}\n```", "```\n$ docker service create --name \"top\" --hostname=\"{{.Service.Name}}-{{.Task.ID}}\" busybox top\n```", "```\n$ docker service create --name webserver --publish 80 nginx:alpine\nlkcig20f3wpfcbfpe68s72fas\noverall progress: 1 out of 1 tasks \n1/1: running [==================================================>] \nverify: Service converged \n```", "```\n$ docker service ps webserver\nID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS \nlb1akyp4dbvc webserver.1 nginx:alpine sirius Running Running about a minute ago \n```", "```\n$ docker container ls\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n6aeaee25ff9b nginx:alpine \"nginx -g 'daemon of…\" 6 minutes ago Up 6 minutes 80/tcp webserver.1.lb1akyp4dbvcqcfznezlhr4zk\n\n$ docker container kill 6aeaee25ff9b              \n6aeaee25ff9b\n```", "```\n$ docker service ps webserver\nID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS\nlnabvvg6k2ne webserver.1 nginx:alpine sirius Running Running less than a second ago \nlb1akyp4dbvc \\_ webserver.1 nginx:alpine sirius Shutdown Failed 7 seconds ago \"task: non-zero exit (137)\"\n```", "```\n$ docker container ls\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n1d9dc2407f74 nginx:alpine \"nginx -g 'daemon of…\" 13 minutes ago Up 13 minutes 80/tcp webserver.1.lnabvvg6k2ne6boqv3hvqvth8\n\n$ docker container inspect 1d9dc2407f74 --format \"{{.Config.Labels}}\"\nmap[com.docker.swarm.node.id:ev4ocuzk61lj0375z80mkba5f com.docker.swarm.service.id:lkcig20f3wpfcbfpe68s72fas com.docker.swarm.service.name:webserver com.docker.swarm.task: com.docker.swarm.task.id:lnabvvg6k2ne6boqv3hvqvth8 com.docker.swarm.task.name:webserver.1.lnabvvg6k2ne6boqv3hvqvth8 maintainer:NGINX Docker Maintainers <docker-maint@nginx.com>]\n```", "```\n$ echo this_is_a_super_secret_password|docker secret create app-key -\no9sh44stjm3kxau4c5651ujvr\n\n$ docker service create --name database \\\n --secret source=ssh-key,target=ssh \\\n --secret source=app-key,target=app,uid=1000,gid=1001,mode=0400 \\\n redis:3.0\n```", "```\n$ docker secret inspect app-key\n[\n {\n \"ID\": \"o9sh44stjm3kxau4c5651ujvr\",\n \"Version\": {\n \"Index\": 12\n },\n \"CreatedAt\": \"2019-12-30T20:42:59.050992148Z\",\n \"UpdatedAt\": \"2019-12-30T20:42:59.050992148Z\",\n \"Spec\": {\n \"Name\": \"app-key\",\n \"Labels\": {}\n }\n }\n]\n```", "```\n$ echo \"This is a sample configuration\" | docker config create sample-config -\nd0nqny24g5y1tiogwggxmesox\n\n$ docker service create \\\n --name sample-service \\\n --config source=sample-config,target=/etc/sample.cfg,mode=0440 \\\n nginx:alpine\n```", "```\n$ docker config inspect sample-config --pretty\nID: d0nqny24g5y1tiogwggxmesox\nName: sample-config\nCreated at: 2019-12-10 21:07:51.350109588 +0000 utc\nUpdated at: 2019-12-10 21:07:51.350109588 +0000 utc\nData:\nThis is a sample configuration\n```", "```\n$ docker network create -d overlay testnet \n1ff11sixrjj7cqppgoxhrdu3z\n```", "```\n$ docker container run -ti --network testnet alpine \nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\nDigest: sha256:2171658620155679240babee0a7714f6509fae66898db422ad803b951257db78\nStatus: Downloaded newer image for alpine:latest\ndocker: Error response from daemon: Could not attach to network testnet: rpc error: code = PermissionDenied desc = network testnet not manually attachable.\n```", "```\n$ docker network create -d overlay testnet2 --attachable --opt encrypted\n9blpskhcvahonytkifn31w91d\n\n$ docker container run -ti --network testnet2 alpine \n/ # \n```", "```\n$ docker service create --name webserver \\\n --publish published=8080,target=80,protocol=tcp \\\nnginx:alpine\n```", "```\n$ curl -I 0.0.0.0:8080\nHTTP/1.1 200 OK\nServer: nginx/1.17.6\nDate: Tue, 31 Dec 2019 17:51:26 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Tue, 19 Nov 2019 15:14:41 GMT\nConnection: keep-alive\nETag: \"5dd406e1-264\"\nAccept-Ranges: bytes\n```", "```\n$ docker service create --name webserver \\\n --publish published=8080,target=80,protocol=tcp,mode=host \\\n --mode global \\\nnginx:alpine\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant up\n--------------------------------------------------------------------------------------------\n Docker SWARM MODE Vagrant Environment\n Engine Version: current\n Experimental Features Enabled\n--------------------------------------------------------------------------------------------\nBringing machine 'swarm-node1' up with 'virtualbox' provider...\nBringing machine 'swarm-node2' up with 'virtualbox' provider...\nBringing machine 'swarm-node3' up with 'virtualbox' provider...\nBringing machine 'swarm-node4' up with 'virtualbox' provider... \n...\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1\nvagrant@swarm-node1:~$\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1\n--------------------------------------------------------------------------------------------\n Docker SWARM MODE Vagrant Environment\n Engine Version: current\n Experimental Features Enabled\n--------------------------------------------------------------------------------------------\n...\n...\n\nvagrant@swarm-node1:~$ docker swarm init\nError response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 10.10.10.11 on eth1) - specify one with --advertise-addr\n```", "```\nvagrant@swarm-node1:~$ docker swarm init --advertise-addr 10.10.10.11\nSwarm initialized: current node (b1t5o5x8mqbz77e9v4ihd7cec) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-didcmb39w7apwokrah6xx4cus 10.10.10.11:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node2\n\nvagrant@swarm-node2:~$ docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-didcmb39w7apwokrah6xx4cus 10.10.10.11:2377\nThis node joined a swarm as a worker.\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1\n\nvagrant@swarm-node1:~$ docker node ls\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nb1t5o5x8mqbz77e9v4ihd7cec * swarm-node1 Ready Active Leader 19.03.5\nrj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active 19.03.5\n```", "```\nvagrant@swarm-node1:~$ docker node ls\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nb1t5o5x8mqbz77e9v4ihd7cec * swarm-node1 Ready Active Leader 19.03.5\nrj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active 19.03.5\nui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5\n```", "```\nvagrant@swarm-node1:~$ docker swarm join-token manager\nTo add a manager to this swarm, run the following command:\n\n docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-aidvtmglkdyvvqurnivcsmyzm 10.10.10.11:2377\n```", "```\nvagrant@swarm-node4:~$ docker swarm join --token SWMTKN-1-3xfi4qggreh81lbr98d63x7299gtz1fanwfjkselg9ok5wroje-aidvtmglkdyvvqurnivcsmyzm 10.10.10.11:2377\nThis node joined a swarm as a manager\n```", "```\nvagrant@swarm-node4:~$ docker node update --role manager swarm-node2\nswarm-node2\n```", "```\nvagrant@swarm-node4:~$ docker node ls\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nb1t5o5x8mqbz77e9v4ihd7cec swarm-node1 Ready Active Leader 19.03.5\nrj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active Reachable 19.03.5\nui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5\njw9uvjcsyg05u1slm4wu0hz6l * swarm-node4 Ready Active Reachable 19.03.5\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1\n\nvagrant@swarm-node1:~$ sudo systemctl stop docker\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node2\n\nvagrant@swarm-node2$ docker node ls\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nb1t5o5x8mqbz77e9v4ihd7cec swarm-node1 Down Active Unreachable 19.03.5\nrj3rgb9egnb256cms0zt8pqew * swarm-node2 Ready Active Reachable 19.03.5\nui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5\njw9uvjcsyg05u1slm4wu0hz6l swarm-node4 Ready Active Leader 19.03.5\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node1\n\nvagrant@swarm-node1$ sudo systemctl start docker\n\nvagrant@swarm-node1$ docker node ls\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nb1t5o5x8mqbz77e9v4ihd7cec * swarm-node1 Ready Active Reachable 19.03.5\nrj3rgb9egnb256cms0zt8pqew swarm-node2 Ready Active Reachable 19.03.5\nui67xyztnw8kn6fjjezjdtwxd swarm-node3 Ready Active 19.03.5\njw9uvjcsyg05u1slm4wu0hz6l swarm-node4 Ready Active Leader 19.03.5\n```", "```\nvagrant@swarm-node1$ docker node update --role worker swarm-node2\nswarm-node2\n\nvagrant@swarm-node1:~$ docker node update --role worker swarm-node1\nswarm-node1\n\nvagrant@swarm-node1:~$ docker node ls\nError response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node4\n\nvagrant@swarm-node4:~$ docker node ls\nID                            HOSTNAME      STATUS    AVAILABILITY    MANAGER STATUS    ENGINE VERSION\nb1t5o5x8mqbz77e9v4ihd7cec     swarm-node1    Ready    Active                            19.03.5\nrj3rgb9egnb256cms0zt8pqew     swarm-node2    Ready    Active                            19.03.5\nui67xyztnw8kn6fjjezjdtwxd     swarm-node3    Ready    Active                            19.03.5\njw9uvjcsyg05u1slm4wu0hz6l *   swarm-node4    Ready    Active        Leader              19.03.5\n```", "```\nvagrant@swarm-node4:~$ docker service create --name webserver nginx:alpine\nkh906v3xg1ni98xk466kk48p4\noverall progress: 1 out of 1 tasks \n1/1: running [==================================================>] \nverify: Service converged \n```", "```\nvagrant@swarm-node4:~$ docker service ps webserver\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nwb4knzpud1z5        webserver.1         nginx:alpine        swarm-node3               Running             Running 14 seconds ago                                         \n```", "```\n$ docker service update --replicas 3 webserver\nwebserver\noverall progress: 3 out of 3 tasks \n1/3: running [==================================================>] \n2/3: running [==================================================>] \n3/3: running [==================================================>] \nverify: Service converged \n```", "```\nvagrant@swarm-node4:~$ docker service ps webserver\nID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS\nwb4knzpud1z5 webserver.1 nginx:alpine swarm-node3 Running Running 2 minutes ago \nie9br2pblxu6 webserver.2 nginx:alpine swarm-node4 Running Running 50 seconds ago \n9d021pmvnnrq webserver.3 nginx:alpine swarm-node1 Running Running 50 seconds ago \n```", "```\nvagrant@swarm-node4:~$ docker node update --label-add tier=front swarm-node2\nswarm-node2\n```", "```\nvagrant@swarm-node4:~$ docker service update --constraint-add node.labels.tier==front webserver\nwebserver\noverall progress: 3 out of 3 tasks \n1/3: running   [==================================================>] \n2/3: running   [==================================================>] \n3/3: running   [==================================================>] \nverify: Service converged \n\nvagrant@swarm-node4:~$ docker service ps webserver\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS\nwjgkgkn0ullj        webserver.1         nginx:alpine        swarm-node2               Running             Running 24 seconds ago \nwb4knzpud1z5         \\_ webserver.1     nginx:alpine        swarm-node3               Shutdown            Shutdown 25 seconds ago \nbz2b4dw1emvw        webserver.2         nginx:alpine        swarm-node2               Running             Running 26 seconds ago \nie9br2pblxu6         \\_ webserver.2     nginx:alpine        swarm-node4               Shutdown            Shutdown 27 seconds ago \ngwzvykixd5oy        webserver.3         nginx:alpine        swarm-node2               Running             Running 28 seconds ago \n9d021pmvnnrq         \\_ webserver.3     nginx:alpine        swarm-node1               Shutdown            Shutdown 29 seconds ago  \n```", "```\nvagrant@swarm-node4:~$ docker service update --constraint-rm node.labels.tier==front webserver\nwebserver\noverall progress: 3 out of 3 tasks \n1/3: running   [==================================================>] \n2/3: running   [==================================================>] \n3/3: running   [==================================================>] \nverify: Service converged \n\nvagrant@swarm-node4:~$ docker service ps webserver\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\nwjgkgkn0ullj        webserver.1         nginx:alpine        swarm-node2               Running             Running 4 minutes ago \nwb4knzpud1z5         \\_ webserver.1     nginx:alpine        swarm-node3               Shutdown            Shutdown 4 minutes ago \nbz2b4dw1emvw        webserver.2         nginx:alpine        swarm-node2               Running             Running 4 minutes ago \nie9br2pblxu6         \\_ webserver.2     nginx:alpine        swarm-node4               Shutdown            Shutdown 4 minutes ago \ngwzvykixd5oy        webserver.3         nginx:alpine        swarm-node2               Running             Running 4 minutes ago \n9d021pmvnnrq         \\_ webserver.3     nginx:alpine        swarm-node1               Shutdown            Shutdown 4 minutes ago                       \n```", "```\nvagrant@swarm-node4:~$ docker node update --availability pause swarm-node3\nswarm-node3\n\nvagrant@swarm-node4:~$ docker node update --availability drain swarm-node2\nswarm-node2\n```", "```\nvagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\n6z55nch0q8ai        webserver.1         nginx:alpine        swarm-node4               Running             Running 3 minutes ago \n8il59udc4iey        webserver.2         nginx:alpine        swarm-node4               Running             Running 3 minutes ago \n1y4q96hb3hik        webserver.3         nginx:alpine        swarm-node1               Running             Running 3 minutes ago      \n```", "```\nvagrant@swarm-node4:~$ docker service rm webserver\nwebserver\n\nvagrant@swarm-node4:~$ docker node update --availability active swarm-node2\nswarm-node2\n\nvagrant@swarm-node4:~$ docker node update --availability active swarm-node3\nswarm-node3\n```", "```\nvagrant@swarm-node4:~$ docker service create --name webserver --mode global nginx:alpine\n4xww1in0ozy3g8q6yb6rlbidr\noverall progress: 4 out of 4 tasks \nui67xyztnw8k: running   [==================================================>] \nb1t5o5x8mqbz: running   [==================================================>] \nrj3rgb9egnb2: running   [==================================================>] \njw9uvjcsyg05: running   [==================================================>] \nverify: Service converged \n```", "```\nvagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running\nID                  NAME                                  IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS\n0jb3tolmta6u        webserver.ui67xyztnw8kn6fjjezjdtwxd   nginx:alpine        swarm-node3               Running             Running about a minute ago \nim69ybzgd879        webserver.rj3rgb9egnb256cms0zt8pqew   nginx:alpine        swarm-node2               Running             Running about a minute ago \nknh5ntkx7b3r        webserver.jw9uvjcsyg05u1slm4wu0hz6l   nginx:alpine        swarm-node4               Running             Running about a minute ago \n26kzify7m7xd        webserver.b1t5o5x8mqbz77e9v4ihd7cec   nginx:alpine        swarm-node1               Running             Running about a minute ago            \n```", "```\nvagrant@swarm-node4:~$ docker node update --availability drain swarm-node1\nswarm-node1\n\nvagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running\nID                  NAME                                  IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\n0jb3tolmta6u        webserver.ui67xyztnw8kn6fjjezjdtwxd   nginx:alpine        swarm-node3               Running             Running 3 minutes ago \nim69ybzgd879        webserver.rj3rgb9egnb256cms0zt8pqew   nginx:alpine        swarm-node2               Running             Running 3 minutes ago \nknh5ntkx7b3r        webserver.jw9uvjcsyg05u1slm4wu0hz6l   nginx:alpine        swarm-node4               Running             Running 3 minutes ago                       \n```", "```\nvagrant@swarm-node4:~$ docker node update --availability active swarm-node1\nnode1\nvagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running\nID                  NAME                                  IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nsun8lxwu6p3k        webserver.b1t5o5x8mqbz77e9v4ihd7cec   nginx:alpine        swarm-node1               Running             Running 1 second ago \n0jb3tolmta6u        webserver.ui67xyztnw8kn6fjjezjdtwxd   nginx:alpine        swarm-node3               Running             Running 5 minutes ago \nim69ybzgd879        webserver.rj3rgb9egnb256cms0zt8pqew   nginx:alpine        swarm-node2               Running             Running 5 minutes ago \nknh5ntkx7b3r        webserver.jw9uvjcsyg05u1slm4wu0hz6l   nginx:alpine        swarm-node4               Running             Running 5 minutes ago                       \n```", "```\nvagrant@swarm-node4:~$ docker service rm webserver\nwebserver\n```", "```\nvagrant@swarm-node4:~$ docker service create --name webserver \\\n--replicas 6 --update-delay 10s --update-order start-first \\\nnginx:alpine \nvpllw7cxlma7mwojdyswbkmbk\noverall progress: 6 out of 6 tasks \n1/6: running   [==================================================>] \n2/6: running   [==================================================>] \n3/6: running   [==================================================>] \n4/6: running   [==================================================>] \n5/6: running   [==================================================>] \n6/6: running   [==================================================>] \nverify: Service converged \n```", "```\nvagrant@swarm-node4:~$ docker service update --image nginx:alpine-perl webserver\nwebserver\noverall progress: 6 out of 6 tasks \n1/6: running [==================================================>] \n2/6: running [==================================================>] \n3/6: running [==================================================>] \n4/6: running [==================================================>] \n5/6: running [==================================================>] \n6/6: running [==================================================>] \nverify: Service converged \n```", "```\nvagrant@swarm-node4:~$ docker service ps webserver --filter desired-state=running\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS\nn9s6lrk8zp32        webserver.1         nginx:alpine-perl   swarm-node4               Running             Running 4 minutes ago \n68istkhse4ei        webserver.2         nginx:alpine-perl   swarm-node1               Running             Running 5 minutes ago \nj6pqig7njhdw        webserver.3         nginx:alpine-perl   swarm-node1               Running             Running 6 minutes ago \nk4vlmeb56kys        webserver.4         nginx:alpine-perl   swarm-node2               Running             Running 5 minutes ago \nk50fxl1gms44        webserver.5         nginx:alpine-perl   swarm-node3               Running             Running 5 minutes ago \napur3w3nq95m        webserver.6         nginx:alpine-perl   swarm-node3               Running             Running 5 minutes ago       \n```", "```\nvagrant@swarm-node4:~$ docker service rm webserver\nwebserver\n```", "```\nvagrant@swarm-node4:~$ echo SuperSecretPassword|docker secret create postgres_password -\nu21mmo1zoqqguh01u8guys9gt\n```", "```\n#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=0 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-EOSQL\n    CREATE USER docker;\n    CREATE DATABASE docker;\n    GRANT ALL PRIVILEGES ON DATABASE docker TO docker;\nEOSQL\n```", "```\nvagrant@swarm-node4:~$ docker config create create-docker-database ./create-docker-database.sh\nuj6zvrdq0682anzr0kobbyhk2\n```", "```\nDocker-Certified-Associate-DCA-Exam-Guide/environments/swarm$ vagrant ssh swarm-node2\n\nvagrant@swarm-node2:~$ docker volume create PGDATA\nPGDATA\n```", "```\nvagrant@swarm-node4:~$ docker node update --label-add tier=database swarm-node2\nswarm-node2 \n```", "```\nversion: '3.7'\nservices:\n  database:\n    image: postgres:alpine\n    deploy:\n      placement:\n        constraints:\n          - node.role == worker\n          - node.labels.tier == database\n    environment:\n      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password\n    secrets:\n      - source: postgres_password\n        target: \"/run/secrets/postgres_password\"\n    configs:\n      - source: create-docker-database\n        target: \"/docker-entrypoint-initdb.d/create-db.sh\"\n        mode: 0755\n        uid: \"0\"\n    volumes:\n      - type: volume\n        source: PGDATA\n        target: /var/lib/postgresql/data\n    ports:\n      - target: 5432\n        published: 15432\n        protocol: tcp\n    networks:\n      net:\n        aliases:\n         - postgres\n         - mydatabase\nconfigs:\n  create-docker-database:\n    external: true\nsecrets:\n  postgres_password:\n    external: true\nvolumes:\n  PGDATA:\n    external: true\nnetworks:\n  net:\n    driver: overlay\n    attachable: true\n```", "```\nvagrant@swarm-node4:~$ docker stack deploy -c postgres-stack.yaml postgres\nCreating network postgres_net\nCreating service postgres_database\n```", "```\nvagrant@swarm-node4:~$ docker stack ps postgres\nID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS\n53in2mik27r0 postgres_database.1 postgres:alpine swarm-node2 Running Running 19 seconds ago \n```", "```\nvagrant@swarm-node4:~$ curl 0.0.0.0:15432\ncurl: (52) Empty reply from server\n\nvagrant@swarm-node2:~$ curl 0.0.0.0:15432\ncurl: (52) Empty reply from server\n\nvagrant@swarm-node3:~$ curl 0.0.0.0:15432\ncurl: (52) Empty reply from server\n```", "```\nvagrant@swarm-node4:~$ docker network ls --filter name=postgres_net\nNETWORK ID NAME DRIVER SCOPE\nmh53ek97pi3a postgres_net overlay swarm\n```", "```\nvagrant@swarm-node4:~$ docker container run -ti --network postgres_net alpine\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\ne6b0cf9c0882: Pull complete \nDigest: sha256:2171658620155679240babee0a7714f6509fae66898db422ad803b951257db78\nStatus: Downloaded newer image for alpine:latest\n/ # apk add --update --no-cache postgresql-client --quiet\n```", "```\n/ # ping -c 1 mydatabase\nPING mydatabase (10.0.3.2): 56 data bytes\n64 bytes from 10.0.3.2: seq=0 ttl=64 time=0.237 ms\n--- mydatabase ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.237/0.237/0.237 ms\n\n/ # ping -c 1 postgres\nPING postgres (10.0.3.2): 56 data bytes\n64 bytes from 10.0.3.2: seq=0 ttl=64 time=0.177 ms\n--- postgres ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.177/0.177/0.177 ms\n\n/ # ping -c 1 database\nPING database (10.0.3.2): 56 data bytes\n64 bytes from 10.0.3.2: seq=0 ttl=64 time=0.159 ms\n--- database ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.159/0.159/0.159 ms\n```", "```\n/ # psql -h mydatabase -U postgres\nPassword for user postgres: \npsql (12.1)\nType \"help\" for help.\n\npostgres=# \\l\n List of databases\n Name | Owner | Encoding | Collate | Ctype | Access privileges \n-----------+----------+----------+------------+------------+-----------------------\n docker | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =Tc/postgres +\n | | | | | postgres=CTc/postgres+\n | | | | | docker=CTc/postgres\n postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | \n template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres +\n | | | | | postgres=CTc/postgres\n template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres +\n | | | | | postgres=CTc/postgres\n(4 rows)\n\npostgres=# \n```", "```\npostgres=# exit\n/ # exit\n\nvagrant@swarm-node4:~$ docker stack rm postgres\nRemoving service postgres_database\nRemoving network postgres_net\n```", "```\nvagrant@swarm-node4:~$ docker service create --name colors \\\n --publish 8000:3000 \\\n--constraint node.role==worker \\\ncodegazers/colors:1.13 \n\nmkyz0d94ovb144xmvo0q4py41\noverall progress: 1 out of 1 tasks \n1/1: running [==================================================>] \nverify: Service converged \n```", "```\nvagrant@swarm-node4:~$ curl 0.0.0.0:8000/text\nAPP_VERSION: 1.0\nCOLOR: orange\nCONTAINER_NAME: d3a886d5fe34\nCONTAINER_IP: 10.0.0.11 172.18.0.3\n```", "```\nCLIENT_IP: ::ffff:10.0.0.5\nCONTAINER_ARCH: linux\n```", "```\nvagrant@swarm-node4:~$ docker service update --replicas 6 colors --quiet\ncolors\n```", "```\nvagrant@swarm-node4:~$ curl 0.0.0.0:8000/text\nAPP_VERSION: 1.0\nCOLOR: red\nCONTAINER_NAME: 64fb2a3009b2\nCONTAINER_IP: 10.0.0.12 172.18.0.4\nCLIENT_IP: ::ffff:10.0.0.5\nCONTAINER_ARCH: linux\n\nvagrant@swarm-node4:~$ curl 0.0.0.0:8000/text\nAPP_VERSION: 1.0\nCOLOR: cyan\nCONTAINER_NAME: 73b07ee0c287\nCONTAINER_IP: 10.0.0.14 172.18.0.3\nCLIENT_IP: ::ffff:10.0.0.5\nCONTAINER_ARCH: linux\n```", "```\nvagranr@swarm-node4:~$ docker service rm colors\ncolors\n```", "```\nvagrant@swarm-node4:~$ docker network create --attachable -d overlay test\n32v9pibk7cqfseknretmyxfsw\n```", "```\nvagrant@swarm-node4:~$ docker service create --replicas 2 \\\n--name colors-vip --network test --quiet codegazers/colors:1.13\n4m2vvbnqo9wgf8awnf53zr5b2\n```", "```\nvagrant@swarm-node4:~$ docker service create --replicas 2 \\\n--name colors-dnsrr --network test --quiet --endpoint-mode dnsrr codegazers/colors:1.13\nwqpv929pe5ehniviclzkdvcl0\n```", "```\nvagrant@swarm-node4:~$ docker run -ti --rm --network test alpine \n/ # apk add --update --no-cache bind-tools --quiet\n/ # host colors-vip\ncolors-vip has address 10.0.4.2\n/ # host colors-dnsrr\ncolors-dnsrr has address 10.0.4.7\ncolors-dnsrr has address 10.0.4.8\n/ #exit\n```", "```\nvagrant@swarm-node4:~$ docker exec -ti colors-dnsrr.1.vtmpdf0w82daq6fdyk0wwzqc7 ip add show\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n inet 127.0.0.1/8 scope host lo\n valid_lft forever preferred_lft forever\n111: eth0@if112: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue state UP \n link/ether 02:42:0a:00:04:07 brd ff:ff:ff:ff:ff:ff\n inet 10.0.4.7/24 brd 10.0.4.255 scope global eth0\n valid_lft forever preferred_lft forever\n113: eth1@if114: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP \n link/ether 02:42:ac:12:00:04 brd ff:ff:ff:ff:ff:ff\n inet 172.18.0.4/16 brd 172.18.255.255 scope global eth1\n valid_lft forever preferred_lft forever\n\n```"]