<html><head></head><body>
        

                            
                    <h1 class="header-title">Running a Containerized App in the Cloud</h1>
                
            
            
                
<p>In the previous chapter, we learned how to deploy, monitor, and troubleshoot an application in production.</p>
<p>In this chapter, we will give an overview of some of the most popular ways of running containerized applications in the cloud. We will explore self-hosting and hosted solutions and discuss their pros and cons. Fully managed offerings from vendors such as Microsoft Azure and Google Cloud Engine will be briefly discussed.</p>
<p>Here are the topics we will be discussing in this chapter:</p>
<ul>
<li>Deploying and using Docker <strong>Enterprise Edition</strong> (<strong>EE</strong>) on <strong>Amazon Web Services</strong> (<strong>AWS</strong>)</li>
<li>Exploring Microsoft's <strong>Azure Kubernetes Service</strong> (<strong>AKS</strong>)</li>
<li>Understanding <strong>Google Kubernetes Engine</strong> (<strong>GKE</strong>)</li>
</ul>
<p>After reading this chapter, you will be able to do the following:</p>
<ul>
<li>Create a Kubernetes cluster in AWS using Docker EE</li>
<li>Deploy and run a simple distributed application in a Docker EE cluster in AWS</li>
<li>Deploy and run a simple distributed application on Microsoft's AKS</li>
<li>Deploy and run a simple distributed application on GKE</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>We are going to use AWS, Microsoft Azure, and Google Cloud in this chapter. Therefore, it is necessary to have an account for each platform. If you do not have an existing account, you can ask for a trial account for all of these cloud providers.</p>
<p>We'll also use the files in the <kbd>~/fod-solution/ch18</kbd> folder of our <kbd>labs</kbd> repository from GitHub at <a href="https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch18" target="_blank">https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch18</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying and using Docker EE on AWS</h1>
                
            
            
                
<p>In this section, we're going to install Docker <strong>Universal Control Plane</strong> (<strong>UCP</strong>) version 3.0. UCP is part of Docker's enterprise offering and supports two orchestration engines, Docker Swarm and Kubernetes. UCP can be installed in the cloud or on-premises. Even hybrid clouds are possible with UCP.</p>
<p>To try this, you need a valid license for Docker EE or you can claim a free test license on Docker Store.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Provisioning the infrastructure</h1>
                
            
            
                
<p>In this first section, we are going to set up the infrastructure needed to install Docker UCP. This is relatively straightforward if you are somewhat familiar with AWS. Let's do this by following these steps:</p>
<ol>
<li>Create an <strong>A</strong><strong>uto Scaling group</strong> (<strong>ASG</strong>) in AWS using the Ubuntu 16.04 server AMI. Configure the ASG to contain three instances of size <kbd>t2.xlarge</kbd>. Here is the result of this:</li>
</ol>
<div><img src="img/3aef6cd3-8664-4019-8eff-5ef9712222f7.png" style="width:71.33em;height:37.75em;"/></div>
<p>ASG on AWS ready for Docker EE</p>
<p style="padding-left: 60px">Once the ASG has been created, and before we continue, we need to open the <strong>security group</strong> (<strong>SG</strong>) a bit (which our ASG is part of) so that we can access it through SSH from our laptop and also so that the <strong>virtual machines</strong> (<strong>VMs</strong>) can communicate with each other.</p>
<ol start="2">
<li>Navigate to your SG and add two new inbound rules, which are shown here:</li>
</ol>
<div><img src="img/00df1c32-a142-4c54-9890-8f69a6fa1a88.png" style="width:71.50em;height:36.00em;"/></div>
<p>AWS SG settings</p>
<p>In the preceding screenshot, the first rule allows any traffic from my personal laptop (with the IP address <kbd>70.113.114.234</kbd>) to access any resource in the SG. The second rule allows any traffic inside the SG itself. These settings are not meant to be used in a production-like environment as they are way too permissive. However, for this demo environment, they work well.</p>
<p>Next, we will show you how to install Docker on the VMs we just prepared.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Docker</h1>
                
            
            
                
<p>After having provisioned the cluster nodes, we need to install Docker on each of them. This can be easily achieved by following these steps:</p>
<ol>
<li>SSH into all three instances and install Docker. Using the downloaded key, SSH into the first machine:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ssh -i pets.pem ubuntu@&lt;IP address&gt;</strong></pre>
<p style="padding-left: 60px">Here, <kbd>&lt;IP address&gt;</kbd> is the public IP address of the VM we want to SSH into. </p>
<ol start="2">
<li>Now we can install Docker. For detailed instructions, refer to <a href="https://dockr.ly/2HiWfBc" target="_blank">https://dockr.ly/2HiWfBc</a>. We have a script in the <kbd>~/fod/ch18/aws</kbd> folder called <kbd>install-docker.sh</kbd> that we can use.</li>
<li>First, we need to clone the <kbd>labs</kbd> GitHub repository to the VM:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git clone https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition.git ~/fod</strong><br/><strong>$ cd ~/fod/ch18/aws</strong></pre>
<ol start="4">
<li>Then, we run the script to install Docker:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./install-docker.sh</strong></pre>
<ol start="5">
<li>Once the script is finished, we can verify that Docker is indeed installed using <kbd>sudo docker version</kbd>. Repeat the preceding code for the two other VMs.</li>
</ol>
<div><kbd>sudo</kbd> is only necessary until the next SSH session is opened to this VM since we have added the <kbd>ubuntu</kbd> user to the <kbd>docker</kbd> group. So, we need to exit the current SSH session and connect again. This time, <kbd>sudo</kbd> should not be needed in conjunction with <kbd>docker</kbd>.</div>
<p>Next, we will show how to install Docker UCP on the infrastructure we just prepared.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Docker UCP</h1>
                
            
            
                
<p>We need to set a few environment variables, as follows:</p>
<pre><strong>$ export UCP_IP=&lt;IP address&gt;</strong><br/><strong>$ export UCP_FQDN=&lt;FQDN&gt;</strong><br/><strong>$ export UCP_VERSION=3.0.0-beta2</strong></pre>
<p>Here, <kbd>&lt;IP address&gt;</kbd> and <kbd>&lt;FQDN&gt;</kbd> are the public IP address and the public DNS name of the AWS EC2 instance we're installing in UCP.</p>
<p>After that, we can use the following command to download all the images that UCP needs:</p>
<pre><strong>$ docker run --rm docker/ucp:${UCP_VERSION} images --list \</strong><br/><strong>    | xargs -L 1 docker pull</strong></pre>
<p>Finally, we can install UCP:</p>
<div><img src="img/cd7dc52c-302c-4bee-8746-89a1bde6a8da.png" style="width:49.25em;height:26.42em;"/></div>
<p>Installing UCP 3.0.0-beta2 on a VM in AWS</p>
<p>Now, we can open a browser window and navigate to <kbd>https://&lt;IP address&gt;</kbd>. Log in with your username, <kbd>admin</kbd>, and password, <kbd>adminadmin</kbd>. When asked for the license, upload your license key or follow the link to procure a trial license.</p>
<p>Once logged in, on the left-hand side under the Shared Resources section, select Nodes and then click on the Add Node button:</p>
<div><img src="img/fbd3044c-9c09-4899-ab0b-8dd5ba14112b.png" style="width:61.33em;height:28.75em;"/></div>
<p>Adding a new node to UCP</p>
<p>In the subsequent Add Node dialog box, make sure that the node type is Linux and the Worker node role is selected. Then, copy the <kbd>docker swarm join</kbd> command at the bottom of the dialog box. SSH into the other two VMs you created and run this command to have the respective node join the Docker swarm as a worker node:</p>
<div><img src="img/a45a4a6d-6e7d-4c02-be40-c3175c9511b8.png" style="width:49.58em;height:21.42em;"/></div>
<p>Joining a node as a worker to the UCP cluster</p>
<p>Back in the web UI of UCP, you should see that we now have three nodes ready, as shown here:</p>
<div><img src="img/ee9b3da4-5b1e-4a53-be1a-72c52291bd58.png" style="width:53.08em;height:9.42em;"/></div>
<p>List of nodes in the UCP cluster</p>
<p>By default, worker nodes are configured so that they can only run the Docker Swarm workload. This can be changed in the node details, though. In this, three settings are possible: Swarm only, Kubernetes only, or mixed workload. Let's start with Docker Swarm as the orchestration engine and deploy our pets application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using remote admin for the UCP cluster</h1>
                
            
            
                
<p>To be able to manage our UCP cluster remotely from our laptop, we need to create and download a so-called <strong>client bundle</strong> from UCP. Proceed with the following steps:</p>
<ol>
<li>In the UCP web UI, on the left-hand side under admin, select the My Profile option.</li>
<li>In the subsequent dialog, select the New Client Bundle option and then Generate Client Bundle:</li>
</ol>
<div><img src="img/e6dd0c9c-f925-4ae3-a78f-5113da2f806d.png" style="width:37.00em;height:15.92em;"/></div>
<p>Generating and downloading a UCP client bundle</p>
<ol start="3">
<li>Locate the downloaded bundle on your disk and unzip it.</li>
<li>In a new Terminal window, navigate to that folder and source the <kbd>env.sh</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ source env.sh</strong></pre>
<p style="padding-left: 60px">You should get an output similar to this:</p>
<pre style="padding-left: 60px"><strong>Cluster "ucp_34.232.53.86:6443_admin" set.</strong><br/><strong>User "ucp_34.232.53.86:6443_admin" set.</strong><br/><strong>Context "ucp_34.232.53.86:6443_admin" created.</strong></pre>
<p style="padding-left: 60px">Now, we can verify that we can indeed remotely access the UCP cluster by, for example, listing all the nodes of the cluster:</p>
<div><img src="img/8d630be3-12cd-4a63-9a0b-d9b2a8aab377.png" style="width:60.92em;height:7.00em;"/></div>
<p>Listing all the nodes of our remote UCP cluster</p>
<p>In the next section, we will look at how to deploy the pets application as a stack using Docker Swarm as the orchestration engine.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying to Docker Swarm</h1>
                
            
            
                
<p>It is now time to deploy our distributed application to our cluster orchestrated by Docker Swarm. Follow these steps to do so:</p>
<ol>
<li>In the Terminal, navigate to the <kbd>~/fod/ch18/ucp</kbd> folder and create the <kbd>pets</kbd> stack using the <kbd>stack.yml</kbd> file:</li>
</ol>
<div><img src="img/b83cbe73-2b3b-4b9b-9812-f81f645566b5.png" style="width:20.17em;height:5.67em;"/></div>
<p>Deploying the pets stack into the UCP cluster</p>
<ol start="2">
<li>In the UCP web UI, we can verify that the stack has been created:</li>
</ol>
<div><img src="img/a520aaa1-33d0-4ec8-9365-8f17118a6a54.png" style="width:56.92em;height:16.42em;"/></div>
<p>The pets stack listing in the UCP web UI</p>
<ol start="3">
<li>To test the application, we can navigate to Services under the main menu, Swarm. The list of services running in the cluster will be displayed as follows:</li>
</ol>
<div><img src="img/738af281-8a2d-4e01-a299-c9580ae9e18c.png" style="width:61.75em;height:29.92em;"/></div>
<p>Details of the 'web' services of the pets stack</p>
<p style="padding-left: 60px">In the preceding screenshot, we see our two services, <kbd>web</kbd> and <kbd>db</kbd>, of the <kbd>pets</kbd> stack. If we click on the <kbd>web</kbd> service, its details are displayed on the right-hand side. There we find an entry, Published Endpoints.</p>
<ol start="4">
<li>Click on the link and our <kbd>pets</kbd> application should be displayed in the browser.</li>
</ol>
<p style="padding-left: 60px">When done, remove the stack from the console with the following:</p>
<pre style="padding-left: 60px"><strong>$ docker stack rm pets</strong></pre>
<p>Alternatively, you can try to remove that stack from within the UCP web UI.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying to Kubernetes</h1>
                
            
            
                
<p>From the same Terminal that you used to remotely access the UCP cluster to deploy the pets application as a stack using Docker Swarm as the orchestration engine, we can now try to deploy the pets application to the UCP cluster using Kubernetes as the orchestration engine.</p>
<p>Make sure you're still in the <kbd>~/fod/ch18/ucp</kbd> folder. Use <kbd>kubectl</kbd> to deploy the pets application. First, we need to test that we can get all the nodes of the cluster with the Kubernetes CLI:</p>
<div><img src="img/cf58df9f-5abd-4502-8992-0b22ae829ff9.png" style="width:41.08em;height:7.33em;"/></div>
<p>Getting all the nodes of the UCP cluster using the Kubernetes CLI</p>
<p>Apparently, my environment is configured correctly and <kbd>kubectl</kbd> can indeed list all the nodes in the UCP cluster. That means I can now deploy the pets application using the definitions in the <kbd>pets.yaml</kbd> file:</p>
<div><img src="img/ea01cf48-9dc5-4983-8d4d-b0febc33cb13.png" style="width:16.67em;height:7.25em;"/></div>
<p>Creating the pets application in the UCP cluster using the Kubernetes CLI</p>
<p>We can list the objects created by using <kbd>kubectl get all</kbd>. In a browser, we can then navigate to <kbd>http://&lt;IP address&gt;:&lt;port&gt;</kbd> to access the pets application, where <kbd>&lt;IP address&gt;</kbd> is the public IP address of one of the UCP cluster nodes and <kbd>&lt;port&gt;</kbd> is the port published by the <kbd>web</kbd> Kubernetes service.</p>
<p>We have created a cluster of three VMs in an AWS ASG and have installed Docker and UCP 3.0 on them. We then deployed our famous pets application into the UCP cluster, once using Docker Swarm as the orchestration engine and once Kubernetes. </p>
<p>Docker UCP is a platform-agnostic container platform that offers a secure enterprise-grade software supply chain in any cloud and on-premises, on bare metal, or in virtualized environments. It even offers freedom of choice when it comes to orchestration engines. The user can choose between Docker Swarm and Kubernetes. It is also possible to run applications in both orchestrators in the same cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring Microsoft's Azure Kubernetes Service (AKS)</h1>
                
            
            
                
<p>To experiment with Microsoft's container-related offerings in Azure, we need an account on Azure. You can create a trial account or use an existing account. You can get a free trial account here: <a href="https://azure.microsoft.com/en-us/free/" target="_blank">https://azure.microsoft.com/en-us/free/</a>.</p>
<p>Microsoft offers different container-related services on Azure. The easiest one to use is probably Azure Container Instances, which promises the fastest and simplest way to run a container in Azure, without having to provision any virtual machines and without having to adopt a higher-level service. This service is only really useful if you want to run a single container in a hosted environment. The setup is quite easy. In the Azure portal (<a href="http://portal.azure.com" target="_blank">portal.azure.com</a>), you first create a new resource group and then create an Azure container instance. You only need to fill out a short form with properties such as the name of the container, the image to use, and the port to open. The container can be made available on a public or private IP address and will be automatically restarted if it crashes. There is a decent management console available, for example, to monitor resource consumption such as CPU and memory.</p>
<p>The second choice is <strong>Azure Container Service</strong> (<strong>ACS</strong>), which provides a way to simplify the creation, configuration, and management of a cluster of VMs that are preconfigured to run containerized applications. ACS uses Docker images and provides a choice between three orchestrators: Kubernetes, Docker Swarm, and DC/OS (powered by Apache Mesos). Microsoft claims that their service can be scaled to tens of thousands of containers. ACS is free and you are only charged for computing resources.</p>
<p>In this section, we will concentrate on the most popular offering, based on Kubernetes. It is called AKS and can be found here: <a href="https://azure.microsoft.com/en-us/services/kubernetes-service/" target="_blank">https://azure.microsoft.com/en-us/services/kubernetes-service/</a>. AKS makes it easy for you to deploy applications into the cloud and run them on Kubernetes. All the difficult and tedious management tasks are handled by Microsoft and you can concentrate fully on your applications. What that means is that you will never have to deal with tasks such as installing and managing Kubernetes, upgrading Kubernetes, or upgrading the operating system of the underlying Kubernetes nodes. All this is handled by the experts at Microsoft Azure. Furthermore, you will never have to deal with <kbd>etc</kbd> or Kubernetes master nodes. This is all hidden from you, and the only things you will interact with are the Kubernetes worker nodes that run your applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the Azure CLI</h1>
                
            
            
                
<p>That said, let's start. We assume that you have created a free trial account or that you are using an existing account on Azure. There are various ways to interact with your Azure account. We will use the Azure CLI running on our local computer. We can either download and install the Azure CLI natively on our computer or run it from within a container running on our local Docker for Desktop. Since this book is all about containers, let's select the latter approach.</p>
<p>The latest version of the Azure CLI can be found on Docker Hub. Let's pull it:</p>
<pre><strong>$</strong> <strong>docker image pull mcr.microsoft.com/azure-cli:latest</strong></pre>
<p>We will be running a container from this CLI and executing all subsequent commands from within the shell running inside this container. Now, there is a little problem we need to overcome. This container will not have a Docker client installed. But we will also run some Docker commands, so we have to create a custom image derived from the preceding image, which contains a Docker client. The <kbd>Dockerfile</kbd> that's needed to do so can be found in the <kbd>~/fod/ch18</kbd> folder and has this content:</p>
<pre>FROM mcr.microsoft.com/azure-cli:latest<br/>RUN apk update &amp;&amp; apk add docker</pre>
<p>On line 2, we are just using the Alpine package manager, <kbd>apk</kbd>, to install Docker. We can then use Docker Compose to build and run this custom image. The corresponding <kbd>docker-compose.yml</kbd> file looks like this:</p>
<pre>version: "2.4"<br/>services:<br/>    az:<br/>        image: fundamentalsofdocker/azure-cli<br/>        build: .<br/>        command: tail -F anything<br/>        working_dir: /app<br/>        volumes:<br/>            - /var/run/docker.sock:/var/run/docker.sock<br/>            - .:/app</pre>
<p>Please note the command that is used to keep the container running, as well as the mounting of the Docker socket and the current folder in the <kbd>volumes</kbd> section. </p>
<p>If you are running Docker for Desktop on Windows, then you need to define the <kbd>COMPOSE_CONVERT_WINDOWS_PATHS</kbd> environment variable to be able to mount the Docker socket. Use<br/>
<kbd>export COMPOSE_CONVERT_WINDOWS_PATHS=1</kbd> from a Bash shell or <kbd>$Env:COMPOSE_CONVERT_WINDOWS_PATHS=1</kbd> when running PowerShell. Please refer to the following link for more details: <a href="https://github.com/docker/compose/issues/4240" target="_blank">https://github.com/docker/compose/issues/4240</a>.<a href="https://github.com/docker/compose/issues/4240" target="_blank"/></p>
<p>Now, let's build and run this container:</p>
<pre><strong>$ docker-compose up --build -d</strong></pre>
<p>Then, let's execute into the <kbd>az</kbd> container and run a Bash shell in it with the following:</p>
<pre><strong>$ docker-compose exec az /bin/bash</strong><br/><br/>bash-5.0#</pre>
<p>We will find ourselves running in a Bash shell inside the container. Let's first check the version of the CLI:</p>
<pre><strong>bash-5.0# az --version</strong></pre>
<p>This should result in an output similar to this (shortened):</p>
<pre><strong>azure-cli 2.0.78</strong><br/><strong>...</strong><br/><strong>Your CLI is up-to-date.</strong></pre>
<p>OK, we're running on version <kbd>2.0.78</kbd>. Next, we need to log in to our account. Execute this command:</p>
<pre><strong>bash-5.0# az login</strong></pre>
<p>You will be presented with the following message:</p>
<pre><strong>To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code &lt;code&gt; to authenticate.</strong></pre>
<p>Follow the instructions and log in through the browser. Once you have successfully authenticated your Azure account, you can go back to your Terminal and you should be logged in, as indicated by the output you'll get:</p>
<pre>[<br/>  {<br/>    "cloudName": "AzureCloud",<br/>    "id": "&lt;id&gt;",<br/>    "isDefault": true,<br/>    "name": "&lt;account name&gt;",<br/>    "state": "Enabled",<br/>    "tenantId": "&lt;tenant-it&gt;",<br/>    "user": {<br/>      "name": "xxx@hotmail.com",<br/>      "type": "user"<br/>    }<br/>  }<br/>]</pre>
<p>Now, we are ready to first move our container images to Azure.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a container registry on Azure</h1>
                
            
            
                
<p>First, we create a new resource group named <kbd>animal-rg</kbd>. In Azure, resource groups are used to logically group a collection of associated resources. To have an optimal cloud experience and keep latency low, it is important that you select a data center located in a region near you. You can use the following command to list all regions:</p>
<pre><strong>bash-5.0# az account list-locations<br/></strong><br/>[<br/>  {<br/>    "displayName": "East Asia",<br/>    "id": "/subscriptions/186760ad-9152-4499-b317-c9bff441fb9d/locations/eastasia",<br/>    "latitude": "22.267",<br/>    "longitude": "114.188",<br/>    "name": "eastasia",<br/>    "subscriptionId": null<br/>  },<br/>  ...<br/>]<strong><br/></strong></pre>
<p>This will give you a rather long list of all possible regions you can select from. Use the <kbd>name</kbd>, for example, <kbd>eastasia</kbd>, to identify the region of your choice. In my case, I will be selecting <kbd>westeurope</kbd>. Please note that not all locations listed are valid for resource groups.</p>
<p>The command to create a resource group is simple; we just need a name for the group and the location:</p>
<pre><strong>bash-5.0# az group create --name animals-rg --location westeurope</strong><br/><br/>{<br/>  "id": "/subscriptions/186760ad-9152-4499-b317-c9bff441fb9d/resourceGroups/animals-rg",<br/>  "location": "westeurope",<br/>  "managedBy": null,<br/>  "name": "animals-rg",<br/>  "properties": {    <br/>    "provisioningState": "Succeeded"<br/>  },<br/>  "tags": null,<br/>  "type": "Microsoft.Resources/resourceGroups"<br/>}</pre>
<p>Make sure that your output shows <kbd>"provisioningState": "Succeeded"</kbd>.</p>
<p>When running a containerized application in production, we want to make sure that we can freely download the corresponding container images from a container registry. So far, we have always downloaded our images from Docker Hub. But this is often not possible. For security reasons, the servers of a production system often have no direct access to the internet and thus are not able to reach out to Docker Hub. Let's follow this best practice and assume the same for our Kubernetes cluster that we are going to create in an instant.</p>
<p>So, what can we do? Well, the solution is to use a container image registry that is close to our cluster and that is in the same security context. In Azure, we can create an <strong>Azure container registry</strong> (<strong>ACR</strong>) and host our images there. Let's first create such a registry:</p>
<pre><strong>bash-5.0# az acr create --resource-group animals-rg --name &lt;acr-name&gt; --sku Basic</strong></pre>
<p>Note that <kbd>&lt;acr-name&gt;</kbd> needs to be unique. In my case, I have chosen the name <kbd>fodanimalsacr</kbd>. The (shortened) output looks like this:</p>
<pre><strong>{</strong><br/><strong>  "adminUserEnabled": false,</strong><br/><strong>  "creationDate": "2019-12-22T10:31:14.848776+00:00",</strong><br/><strong>  "id": "/subscriptions/186760ad...",</strong><br/><strong>  "location": "westeurope",</strong><br/><strong>  "loginServer": "fodanimalsacr.azurecr.io",</strong><br/><strong>  "name": "fodanimalsacr",</strong><br/><strong>  ...</strong><br/><strong>  "provisioningState": "Succeeded",</strong></pre>
<p>After successfully creating the container registry, we need to log in to that registry using the following:</p>
<pre><strong>bash-5.0# az acr login --name &lt;acr-name&gt;<br/></strong><br/>Login Succeeded<br/>WARNING! Your password will be stored unencrypted in /root/.docker/config.json.<br/>Configure a credential helper to remove this warning. See<br/>https://docs.docker.com/engine/reference/commandline/login/#credentials-store<strong><br/></strong></pre>
<p>Once we are successfully logged in to the container registry on Azure, we need to tag our containers correctly so that we can then push them to ACR. Tagging and pushing images to ACR will be described next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Pushing our images to ACR</h1>
                
            
            
                
<p>Once we have successfully logged in to ACR, we can tag our images such that they can be pushed to the registry. For this, we need to get the URL of our ACR instance. We can do so with this command:</p>
<pre><strong>$ az acr list --resource-group animals-rg \</strong><br/><strong>    --query "[].{acrLoginServer:loginServer}" \</strong><br/><strong>    --output table</strong><br/><br/>AcrLoginServer<br/>------------------------<br/>fodanimalsacr.azurecr.io</pre>
<p>We now use the preceding URL to tag our images:</p>
<pre><strong>bash-5.0# docker image tag fundamentalsofdocker/ch11-db:2.0 </strong>fodanimalsacr.azurecr.io/ch11-db:2.0<br/><strong>bash-5.0# docker image tag fundamentalsofdocker/ch11-web:2.0 </strong>fodanimalsacr.azurecr.io/ch11-web:2.0</pre>
<p>Then, we can push them to our ACR:</p>
<pre><strong>bash-5.0# docker image push fodanimalsacr.azurecr.io/ch11-db:2.0</strong><br/><strong>bash-5.0#</strong> <strong>docker image push fodanimalsacr.azurecr.io/ch11-web:2.0</strong></pre>
<p>To double-check that our images are indeed in our ACR, we can use this command:</p>
<pre><strong>bash-5.0# az acr repository list --name &lt;acr-name&gt; --output </strong><strong>table<br/></strong><br/>Result<br/>--------<br/>ch11-db<br/>ch11-web<br/></pre>
<p>Indeed, the two images we just pushed are listed. With that, we are ready to create our Kubernetes cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a Kubernetes cluster</h1>
                
            
            
                
<p>Once again, we will be using our custom Azure CLI to create the Kubernetes cluster. We will have to make sure that the cluster can access our ACR instance, which we just created and is where our container images reside. So, the command to create a cluster named <kbd>animals-cluster</kbd> with two worker nodes looks like this:</p>
<pre><strong>bash-5.0#</strong> <strong>az aks create \</strong><br/><strong>    --resource-group animals-rg \</strong><br/><strong>    --name animals-cluster \</strong><br/><strong>    --node-count 2 \</strong><br/><strong>    --generate-ssh-keys \</strong><br/><strong>    --attach-acr &lt;acr-name&gt;</strong></pre>
<p>This command takes a while, but after a few minutes, we should receive some JSON-formatted output with all the details about the newly created cluster.</p>
<p>To access the cluster, we need <kbd>kubectl</kbd>. We can easily get it installed in our Azure CLI container using this command:</p>
<pre><strong>bash-5.0# az aks install-cli</strong></pre>
<p>Having installed <kbd>kubectl</kbd>, we need the necessary credentials to use the tool to operate on our new Kubernetes cluster in Azure. We can get the necessary credentials with this:</p>
<pre><strong>bash-5.0# az aks get-credentials --resource-group animals-rg --name animals-cluster<br/></strong><br/>Merged "animals-cluster" as current context in /root/.kube/config<strong><br/></strong></pre>
<p>After the success of the preceding command, we can list all the nodes in our cluster:</p>
<pre><strong>bash-5.0# kubectl get nodes<br/><br/></strong>NAME                                STATUS   ROLES   AGE     VERSION<br/>aks-nodepool1-12528297-vmss000000   Ready    agent   4m38s   v1.14.8<br/>aks-nodepool1-12528297-vmss000001   Ready    agent   4m32s   v1.14.8<strong><br/></strong></pre>
<p>As expected, we have two worker nodes up and running. The version of Kubernetes that is running on those nodes is <kbd>1.14.8</kbd>.</p>
<p>We are now ready to deploy our application to this cluster. In the next section, we are going to learn how we can do this.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying our application to the Kubernetes cluster</h1>
                
            
            
                
<p>To deploy the application, we can use the <kbd>kubectl apply</kbd> command:</p>
<pre><strong>bash-5.0# kubectl apply -f animals.yaml </strong></pre>
<p>The output of the preceding command should look similar to this:</p>
<pre>deployment.apps/web created<br/>service/web created<br/>deployment.apps/db created<br/>service/db created</pre>
<p>Now, we want to test the application. Remember that we had created a service of type <kbd>LoadBalancer</kbd> for the web component. This service exposes the application to the internet. This process can take a moment, as AKS, among other tasks, needs to assign a public IP address to this service. We can observe this with the following command:</p>
<pre><strong>bash-5.0# kubectl get service web --watch</strong></pre>
<p>Please note the <kbd>--watch</kbd> parameter in the preceding command. It allows us to monitor the progress of the command over time. Initially, we should see output like this:</p>
<pre>NAME TYPE        CLUSTER-IP  EXTERNAL-IP  PORT(S)         AGE<br/>web LoadBalancer 10.0.124.0  &lt;pending&gt;    3000:32618/TCP  5s</pre>
<p>The public IP address is marked as pending. After a few minutes, that should change to this:</p>
<pre>NAME TYPE        CLUSTER-IP  EXTERNAL-IP    PORT(S)         AGE<br/>web LoadBalancer 10.0.124.0  51.105.229.192 3000:32618/TCP  63s</pre>
<p>Our application is now ready at the IP address <kbd>51.105.229.192</kbd> and port number <kbd>3000</kbd>. Note that the load balancer maps the internal port <kbd>32618</kbd> to the external port <kbd>3000</kbd>; this was not evident to me the first time.</p>
<p>Let's check it out. In a new browser tab, navigate to <kbd>http://51.105.229.192:3000/pet</kbd> and you should see our familiar application:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f45ab4b8-610f-4909-b6ce-4fce569200c8.png" style="width:43.25em;height:30.25em;"/></p>
<p>Our sample application running on AKS</p>
<p>With that, we have successfully deployed our distributed application to Kubernetes hosted in Azure. We did not have to worry about installing or managing Kubernetes; we could concentrate on the application itself.</p>
<p>Now that we are done experimenting with the application, we should not forget to delete all resources on Azure to avoid incurring unnecessary costs. We can delete all resources created by deleting the resource group as follows:</p>
<pre><strong>bash-5.0#</strong> <strong>az group delete --name animal-rg --yes --no-wait</strong> </pre>
<p>Azure has a few compelling offerings regarding the container workload, and the lock-in is not as evident as it is on AWS due to the fact that Azure does mainly offer open source orchestration engines, such as Kubernetes, Docker Swarm, DC/OS, and Rancher. Technically, we remain mobile if we initially run our containerized applications in Azure and later decide to move to another cloud provider. The cost should be limited.</p>
<p>It is worth noting that, when you delete your resource group, the Azure Active Directory service principal used by the AKS cluster is not removed. Refer to the online help for details on how to delete the service principal.</p>
<p>Next on the list is Google with their Kubernetes Engine.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding GKE</h1>
                
            
            
                
<p>Google is the inventor of Kubernetes and, to this date, the driving force behind it. You would therefore expect that Google has a compelling offering around hosted Kubernetes. Let's have a peek into it now. To continue, you need to either have an existing account with Google Cloud or create a test account here: <a href="https://console.cloud.google.com/freetrial" target="_blank">https://console.cloud.google.com/freetrial</a>. Proceed with the following steps:</p>
<ol>
<li>In the main menu, select Kubernetes Engine. The first time you do that, it will take a few moments until the Kubernetes engine is initialized.</li>
<li>Next, create a new project and name it <kbd>massai-mara</kbd>; this may take a moment.</li>
<li>Once this is ready, we can create a cluster by clicking on Create Cluster in the popup.</li>
<li>Select the <strong>Your first cluster</strong> template on the left-hand side of the form.</li>
<li>Name the cluster <kbd>animals-cluster</kbd>, select the region or zone that's closest to you, leave all other settings in the Create a Kubernetes Cluster form with their default values, and click on Create at the bottom of the form.</li>
</ol>
<p>It will again take a few moments to provision the cluster for us. Once the cluster has been created, we can open Cloud Shell<strong> </strong>by clicking on the shell icon in the upper-right corner of the view. This should look similar to the following screenshot:</p>
<div><img src="img/f1bd8565-7c02-4260-bb8e-0dc482fc8051.png" style="width:67.67em;height:36.25em;"/></div>
<p>The first Kubernetes cluster ready and Cloud Shell open in GKE</p>
<p>We can now clone our <kbd>labs</kbd> GitHub repository to this environment with the following command:</p>
<pre><strong>$ git clone https://github.com/PacktPublishing/Learn-Docker---  Fundamentals-of-Docker-19.x-Second-Edition.git ~/fod</strong><br/><strong>$ cd ~/fod/ch18/gce</strong></pre>
<p>We should now find an <kbd>animals.yaml</kbd> file in the current folder, which we can use to deploy the animals application into our Kubernetes cluster. Have a look at the file:</p>
<pre><strong>$ less animals.yaml</strong></pre>
<p>It has pretty much the same content as the same file we used in the previous chapter. The two differences are these:</p>
<ul>
<li>We use a service of type <kbd>LoadBalancer</kbd> (instead of <kbd>NodePort</kbd>) to publicly expose the <kbd>web</kbd> component.</li>
<li>We do not use volumes for the PostgreSQL database since configuring StatefulSets correctly on GKE is a bit more involved than in Minikube. The consequence of this is that our animals application will not persist the state if the <kbd>db</kbd> pod crashes. How to use persistent volumes on GKE lies outside the scope of this book.</li>
</ul>
<p>Also, note that we are not using Google Container Registry to host the container images but are instead directly pulling them from Docker Hub. It is very easy, and similar to what we have learned in the section about AKS, to create such a container registry in Google Cloud.</p>
<p>Before we can continue, we need to set up <kbd>gcloud</kbd> and <kbd>kubectl</kbd> credentials:</p>
<pre><strong>$ gcloud container clusters get-credentials animals-cluster </strong><strong>--zone europe-west1-b<br/></strong><br/>Fetching cluster endpoint and auth data.<br/>kubeconfig entry generated for animals-cluster.<strong><br/></strong></pre>
<p>Having done that, it's time to deploy the application:</p>
<pre><strong>$ kubectl create -f animals.yaml<br/></strong><br/>deployment.apps/web created<br/>service/web created<br/>deployment.apps/db created<br/>service/db created<strong><br/></strong></pre>
<p>Once the objects have been created, we can observe the <kbd>LoadBalancer</kbd> service <kbd>web</kbd> until it is assigned a public IP address:</p>
<pre><strong>$ kubectl get svc/web --watch<br/><br/></strong>NAME   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)          AGE<br/>web    LoadBalancer   10.0.5.222   &lt;pending&gt;       3000:32139/TCP   32s<br/>web    LoadBalancer   10.0.5.222   146.148.23.70   3000:32139/TCP   39s<strong><br/></strong></pre>
<p>The second line in the output is showing the situation while the creation of the load balancer is still pending, and the third one gives the final state. Press <em>Ctrl</em> + <em>C</em> to quit the <kbd>watch</kbd> command. Apparently, we got the public IP address <kbd>146.148.23.70</kbd> assigned and the port is <kbd>3000</kbd>.</p>
<p>We can then use this IP address and navigate to <kbd>http://&lt;IP address&gt;:3000/pet</kbd>, and we should be greeted by the familiar animal image.</p>
<p>Once you are done playing with the application, delete the cluster and the project in the Google Cloud console to avoid any unnecessary costs.</p>
<p>We have created a hosted Kubernetes cluster in GKE. We have then used Cloud Shell, provided through the GKE portal, to first clone our <kbd>labs</kbd> GitHub repository and then the <kbd>kubectl</kbd> tool to deploy the animals application into the Kubernetes cluster. </p>
<p>When looking into a hosted Kubernetes solution, GKE is a compelling offering. It makes it very easy to start, and since Google is the main driving force behind Kubernetes, we can rest assured that we will always be able to leverage the full functionality of Kubernetes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this final chapter of the book, you first got a quick introduction to how to install and use Docker's UCP, which is part of Docker's enterprise offering on AWS. Then, you learned how to create a hosted Kubernetes cluster in AKS and run the animals application on it, followed by the same for Google's own hosted Kubernetes offering, GKE.</p>
<p>I am honored that you selected this book, and I want to thank you for accompanying me on this journey, where we explored Docker containers and container orchestration engines. I hope that this book has served as a valuable resource on your learning journey. I wish you all the best and much success when using containers in your current and future projects.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<p>To assess your knowledge, please answer the following questions:</p>
<ol>
<li>Give a high-level description of the tasks needed to provision and run Docker UPC on AWS.</li>
<li>List a few reasons why you would select a hosted Kubernetes offering, such as Microsoft's AKS or Google's GKE, to run your applications on Kubernetes.</li>
<li>Name two reasons when using a hosted Kubernetes solution, such as AKS or GKE, to consider hosting your container images in the container registry of the respective cloud provider.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>The following articles give you some more information related to the topics we discussed in this chapter:</p>
<ul>
<li>Install individual Docker EE components on Linux servers at <a href="https://dockr.ly/2vH5dpN" target="_blank">https://dockr.ly/2vH5dpN</a><a href="https://dockr.ly/2vH5dpN" target="_blank"/></li>
<li>Azure Container Service (AKS) at <a href="https://bit.ly/2JglX9d" target="_blank">https://bit.ly/2JglX9d</a><a href="https://bit.ly/2JglX9d" target="_blank"/></li>
<li>Google Kubernetes Engine at <a href="https://bit.ly/2I8MjJx" target="_blank">https://bit.ly/2I8MjJx</a><a href="https://bit.ly/2I8MjJx" target="_blank"/></li>
</ul>


            

            
        
    </body></html>