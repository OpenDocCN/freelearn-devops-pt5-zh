- en: Managing OpenShift Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理 OpenShift 网络
- en: In the previous chapter, we introduced you to the realm of security in OpenShift.
    OpenShift is an enterprise-ready application management platform that supports
    multiple security features, making it able to integrate into any corporate security
    landscape.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 OpenShift 中的安全领域。OpenShift 是一个企业级的应用管理平台，支持多种安全功能，能够融入任何企业的安全环境。
- en: 'Like any cloud platform, OpenShift heavily relies on a networking stack on
    two different layers:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 和任何云平台一样，OpenShift 在两个不同的层次上严重依赖网络堆栈：
- en: The underlying network topology, which is directly determined either by physical
    network equipment or virtual network devices in the case of OpenShift itself deployed
    in the virtual environment. This level provides connectivity to OpenShift masters
    and nodes, and is beyond the control of OpenShift itself.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础网络拓扑，直接由物理网络设备或在虚拟环境中部署的 OpenShift 自身的虚拟网络设备确定。此层提供 OpenShift 主节点和节点之间的连接，超出了
    OpenShift 本身的控制范围。
- en: The virtual network topology, which is determined by the OpenShift SDN plugin
    being used. This level is concerned with managing connectivity between applications
    and providing external access to them.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由所使用的 OpenShift SDN 插件确定的虚拟网络拓扑。此层关注于管理应用程序之间的连接性并提供外部访问。
- en: 'In this chapter, we are going to work with networking on the upper level—OpenShift
    SDN—and we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理更高层次的网络配置——OpenShift SDN，并将涵盖以下主题：
- en: Network topology in OpenShift
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 中的网络拓扑
- en: SDN plugins
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDN 插件
- en: Egress routers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出口路由器
- en: Static IPs for external project traffic
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部项目流量的静态 IP
- en: Egress network policies
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出口网络策略
- en: DNS
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, we will be using the following configuration of VMs managed
    via Vagrant using the default VirtualBox provider:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用通过 Vagrant 管理的以下 VM 配置，使用默认的 VirtualBox 提供者：
- en: '| **Name** | **Role** |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **角色** |'
- en: '| openshift-master | Master |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| openshift-master | 主节点 |'
- en: '| openshift-node-1 | Node |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| openshift-node-1 | 节点 |'
- en: '| openshift-node-2 | Node |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| openshift-node-2 | 节点 |'
- en: Make sure you have enough RAM on your desktop or laptop you use. The configuration
    above was tested with 8GB RAM, but it was barely enough, so we recommend running
    it on a system with 16GB at least.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你使用的桌面或笔记本电脑有足够的 RAM。上面的配置在 8GB RAM 上进行了测试，但几乎不够，所以我们建议至少在 16GB 的系统上运行。
- en: 'This configuration corresponds to the following Vagrantfile:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置对应以下 Vagrantfile：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to be able to reach the cluster inside the VM from your host system,
    make sure file `/etc/hosts` on your laptop looks like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够从你的主机系统访问 VM 中的集群，确保你的笔记本电脑上的 `/etc/hosts` 文件如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run `vagrant up` and wait till it finishes all the work. It may take up to
    30 mins depending on your internet connectivity and compute resources:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `vagrant up` 并等待直到它完成所有工作。根据你的网络连接和计算资源，这可能需要最多 30 分钟：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once it''s done, open SSH session into the master VM and become root:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，打开 SSH 会话进入主 VM，并切换为 root 用户：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can use the following inventory for deploying OpenShift:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下清单来部署 OpenShift：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Even though variables in the `nodes` group appear to be on separate lines, they
    are actually on previous ones with hosts they are associated with. If you just
    copy this file as it is from the one provided with other materials on this book,
    it will work.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 `nodes` 组中的变量看起来在单独的行上，它们实际上是与其关联的主机上的前一行。如果你直接复制这个文件，并且它与本书其他材料提供的文件一致，它将正常工作。
- en: 'Now, it''s time to install OpenShift:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候安装 OpenShift 了：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Network topology in OpenShift
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 中的网络拓扑
- en: In order to provide a common medium for containers to communicate with each
    other, OpenShift makes use of an overlay network that's implemented via VXLAN.
    The **Virtual eXtensible Local Area Network** (**VXLAN**) protocol provides a
    mechanism for transferring Layer 2 (Ethernet) frames across Layer 3 (IP) networks.
    Depending on the SDN plugin being used, the scope of communication may be limited
    to pods within the same project or maybe completely unrestricted. No matter which
    plugin is used, the network topology is still the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供容器之间通信的公共介质，OpenShift 使用了通过 VXLAN 实现的覆盖网络。**虚拟扩展局域网**（**VXLAN**）协议提供了一种在第
    3 层（IP 网络）上传输第 2 层（以太网）帧的机制。根据使用的 SDN 插件，通信的范围可能仅限于同一项目内的 Pods，或者可能完全不受限制。无论使用哪个插件，网络拓扑仍然是相同的。
- en: 'When a new node is registered in `etcd`, the master allocates a private `/23`
    subnet from the cluster network. By default, subnets are allocated from `10.128.0.0/14`,
    but can be configured in the `networkConfig` stanza of the master configuration
    file. The following is an excerpt from the file containing the relevant parameters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当新节点在 `etcd` 中注册时，主节点从集群网络中分配一个私有的`/23`子网。默认情况下，子网从 `10.128.0.0/14` 中分配，但可以在主节点配置文件的
    `networkConfig` 部分进行配置。以下是文件中包含相关参数的摘录：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `hostSubnetLength` setting determines how many IP addresses are allocated
    to every node to be distributed between pods running on a given node. In our default
    configuration, the size of each subnet is 2⁹=512 addresses, which makes `510`
    IPs available for pods. Summing up, each pod's IP address will have mask /23 (14+9).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostSubnetLength` 设置决定了每个节点分配多少个 IP 地址，以便在该节点上运行的 Pod 之间分配。在我们默认的配置中，每个子网的大小是
    2⁹ = 512 个地址，这使得 `510` 个 IP 可供 Pod 使用。总结一下，每个 Pod 的 IP 地址将有 /23 的子网掩码（14+9）。'
- en: Please note that the `clusterNetworks[].cidr` setting can only be changed to
    a larger subnet that includes the previous setting. For example, it can be set
    to `10.128.0.0/12`, as it contains `/14`, but not to `10.128.0.0/16`, as they
    don't overlap completely.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`clusterNetworks[].cidr` 设置只能更改为一个包含先前设置的较大子网。例如，它可以设置为 `10.128.0.0/12`，因为它包含
    `/14`，但不能设置为 `10.128.0.0/16`，因为它们不完全重叠。
- en: Also, `hostSubnetLength` cannot be changed once the cluster is created.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`hostSubnetLength` 一旦集群创建后不能更改。
- en: 'The overlay network in OpenShift is built from the following components:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 中的覆盖网络由以下组件构成：
- en: '`br0`: An OVS bridge that all pods running on a particular node are plugged
    into via a `veth` pair. Each node has a single `br0` device which serves as a
    virtual switch.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`br0`：一个 OVS 桥接器，所有在特定节点上运行的 Pod 都通过 `veth` 对连接到它。每个节点都有一个 `br0` 设备，充当虚拟交换机。'
- en: '`tun0`: This is an internal port of `br0`  numbered 2, which is assigned each
    node subnet''s default gateway address and is used for external access. OpenShift
    also creates routing and netfilter rules to direct traffic to the external network
    via NAT.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tun0`：这是 `br0` 的一个内部端口，编号为 2，分配给每个节点子网的默认网关地址，并用于外部访问。OpenShift 还创建了路由和 netfilter
    规则，通过 NAT 将流量引导到外部网络。'
- en: '`vxlan_sys_4789`: An OVS port `1` of `br0` which provides connectivity between
    pods running on different nodes. It''s referred to as `vxlan` in the OVS rules.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vxlan_sys_4789`：`br0` 的 OVS 端口 `1`，提供跨不同节点运行的 Pod 之间的连接。在 OVS 规则中，它被称为 `vxlan`。'
- en: Tracing connectivity
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪连接性
- en: In this subsection, we will create a `demo` project hosting a single `httpd`
    pod in order to see first-hand how the overlay network is constructed in OpenShift.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将创建一个名为 `demo` 的项目，托管一个 `httpd` Pod，以便亲身体验 OpenShift 中覆盖网络的构建过程。
- en: 'First, let''s create the project:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建项目：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, create a pod running Apache web server in the project:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在该项目中创建一个运行 Apache Web 服务器的 Pod：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will need the IP address allocated to the pod, as well as the address of
    the node it was scheduled to:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要分配给 Pod 的 IP 地址，以及它被调度到的节点的地址：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another step is to get the name of the pod''s network interface, which is actually
    one end of the `veth` pair that''s used to connect the pod to the `br0` bridge:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个步骤是获取 Pod 网络接口的名称，它实际上是用来连接 Pod 和 `br0` 桥接器的 `veth` 对的一端：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s move on to another project, called `default`. This project is used
    to host special pods for the router and internal Docker registry. Both pods are
    deployed on the node labeled `infra`, which is `openshift-node-1` in our case.
    Let''s confirm this and find out the IP address of the registry pod:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转到另一个名为 `default` 的项目。此项目用于托管路由器和内部 Docker 注册表的特殊 Pod。这两个 Pod 部署在标记为 `infra`
    的节点上，在我们这里是 `openshift-node-1`。让我们确认这一点并找出注册表 Pod 的 IP 地址：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The reason we picked the registry pod is that the router pod runs in privileged
    mode to have direct access to the node's networking stack; as such, it wouldn't
    represent a typical configuration.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择注册表 Pod 的原因是路由器 Pod 以特权模式运行，以便能够直接访问节点的网络栈；因此，它不能代表典型的配置。
- en: 'Now, launch the following command to get the name of the registry pod''s NIC:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行以下命令获取注册表 Pod 的网络接口卡（NIC）的名称：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The following steps will be performed on the first node—`openshift-node-1`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将在第一个节点`openshift-node-1`上执行。
- en: 'First, let''s see what network devices were created on that node after OpenShift
    was installed:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看 OpenShift 安装后在该节点上创建了哪些网络设备：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`br0` is the OVS bridge that was mentioned at the beginning of the *Network
    topology in OpenShift* section. In order to see its active ports, use the `ovs-vsctl`
    command, which is provided by the `openvswitch` package:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`br0` 是在 *OpenShift 网络拓扑* 部分开始时提到的 OVS 桥接器。为了查看其活动端口，请使用 `ovs-vsctl` 命令，该命令由
    `openvswitch` 包提供：'
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, discover the same information about the second node, `openshift-node-2`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，了解关于第二个节点 `openshift-node-2` 的相同信息：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In order to sum up the preceding code, the following diagram provides a visual
    representation of what the resulting overlay network looks like in our cluster:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结前面的代码，以下图示直观展示了我们集群中结果覆盖网络的样子：
- en: '![](img/00064.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: Figure 1 - Overlay network topology
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 - 覆盖网络拓扑
- en: 'Finally, let''s clean up before the next section:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在下一节之前进行清理：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: SDN plugins
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SDN 插件
- en: In the previous section, we learned what components the overlay network in OpenShift
    comprises. Now, it's time to see how it can be configured to suit the requirements
    of a particular environment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们了解了 OpenShift 中覆盖网络的组成部分。现在，是时候查看如何配置它以满足特定环境的需求了。
- en: 'OpenShift makes its internal SDN plugins available out-of-the-box, as well
    as plugins for integration with third-party SDN frameworks. The following are
    three built-in plugins that are available in OpenShift:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 默认提供内部 SDN 插件，以及与第三方 SDN 框架集成的插件。以下是 OpenShift 中可用的三个内置插件：
- en: '`` `ovs-subnet` ``'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `ovs-subnet` ``'
- en: '`ovs-multitenant`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-multitenant`'
- en: '`ovs-networkpolicy`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-networkpolicy`'
- en: The decision regarding which plugin to use is based on what level of security
    and control you aim to achieve. In the following subsections, we will discuss
    the main features and use cases for each of those plugins.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用哪个插件的决策取决于你希望达到的安全性和控制级别。在以下小节中，我们将讨论每个插件的主要功能和使用场景。
- en: 'With SDNs taking over networking, third-party vendors have also started to
    develop their own solutions for programmable networks. Red Hat works closely with
    such providers to ensure smooth integration of their products into OpenShift.
    The following solutions have been tested and verified by Red Hat as production-ready:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 SDN 接管网络，第三方供应商也开始开发他们自己的可编程网络解决方案。Red Hat 与这些供应商紧密合作，确保其产品能够顺利集成到 OpenShift
    中。以下解决方案已被 Red Hat 测试并验证为生产就绪：
- en: Nokia Nuage
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nokia Nuage
- en: Cisco Contiv
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cisco Contiv
- en: Juniper Contrail
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juniper Contrail
- en: Tigera Calico
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tigera Calico
- en: VMWare NSX-T
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMWare NSX-T
- en: Getting each of those to work with OpenShift is beyond the scope of this book,
    but you will find detailed instructions by following the links provided at the
    end of this chapter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使这些与 OpenShift 配合工作超出了本书的范围，但你可以通过访问本章结尾提供的链接找到详细的说明。
- en: ovs-subnet plugin
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ovs-subnet 插件
- en: 'This is the default plugin that''s enabled after OpenShift has just been installed.
    It provides connectivity for pods across the entire cluster with no limitations
    whatsoever, meaning that traffic can flow freely between all pods. This may be
    undesirable in large multi-tenant environments that place high importance on security.
    The SDN plugin being used is determined by the `networkConfig.networkPluginName`
    setting in the master configuration file:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 OpenShift 安装后默认启用的插件。它为整个集群中的 Pods 提供了无任何限制的连接，这意味着流量可以在所有 Pods 之间自由流动。在高度重视安全性的多租户环境中，这可能是不可取的。正在使用的
    SDN 插件由主配置文件中的 `networkConfig.networkPluginName` 设置决定：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The SDN plugin can also be specified explicitly upon installation via the `os_sdn_network_plugin_name`
    Ansible variable. By default, it's `redhat/openshift-ovs-subnet`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 安装时，也可以通过 `os_sdn_network_plugin_name` Ansible 变量显式指定 SDN 插件。默认情况下，它是 `redhat/openshift-ovs-subnet`。
- en: In order to see for yourself what exactly the `ovs-subnet` plugin does (or,
    rather, does not do), create two projects with one pod each and try to reach one
    of them from the other one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了亲自了解 `ovs-subnet` 插件的作用（或者说不作用），可以创建两个项目，每个项目一个 Pod，并尝试从一个 Pod 访问另一个 Pod。
- en: 'First, create a `demo-1` project:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个 `demo-1` 项目：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, launch a pod by running the httpd web server using the same YAML definition,
    like we did in the *Tracing connectivity* subsection:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用相同的 YAML 定义启动一个 Pod，运行 httpd Web 服务器，就像我们在 *跟踪连接性* 小节中做的那样：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s find out the IP address assigned to our pod:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们找出分配给 Pod 的 IP 地址：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Move on to creating the second project:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续创建第二个项目：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And create the same pod in that project:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 并在该项目中创建相同的 Pod：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s see whether we can `ping` the first pod from the one we have just
    created:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看一下能否通过 `ping` 测试从刚创建的 pod 访问第一个 pod：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Just to be sure, let''s reverse our experiment and try to reach the pod in
    the `demo-2` project from the one deployed in `demo-1`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保准确性，让我们逆向实验，尝试从 `demo-1` 部署的 pod 访问 `demo-2` 项目的 pod：
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, communication between pods is completely uninhibited, which
    may be undesirable. In the two following subsections, we will demonstrate how
    to enforce project isolation using other OpenShift plugins.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，pod 之间的通信完全没有限制，这可能并不是我们所希望的。在接下来的两个子节中，我们将演示如何使用其他 OpenShift 插件强制执行项目隔离。
- en: ovs-multitenant plugin
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ovs-multitenant 插件
- en: While it's usually not that big of a deal in PoC and sandboxes, security becomes
    a matter of utmost importance in large enterprises with diverse teams and project
    portfolios, even more so when the development of certain applications is outsourced
    to third-party companies. The `ovs-multitenant` plugin is a perfect choice if
    just having projects separated is enough. Unlike the `ovs-subnet` plugin, which
    passes all traffic across all pods, this one assigns the same VNID to all pods
    for each project, keeping them unique across projects, and sets up flow rules
    on the `br0` bridge to make sure that traffic is only allowed between pods with
    the same VNID.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 PoC 和沙箱环境中通常不是大问题，但在拥有多元团队和项目组合的大型企业中，安全问题变得至关重要，尤其是当某些应用的开发外包给第三方公司时。如果仅仅将项目分开就足够，`ovs-multitenant`
    插件是一个完美的选择。与将所有流量通过所有 pod 的 `ovs-subnet` 插件不同，`ovs-multitenant` 插件为每个项目的所有 pod
    分配相同的 VNID，确保项目间的唯一性，并在 `br0` 桥接器上设置流量规则，确保只有拥有相同 VNID 的 pod 之间才能互相通信。
- en: There is, however, an exception to that rule—traffic is allowed to flow between
    the `default` project and each of the other ones. This is because that project
    is privileged and is assigned VNID, so that all pods in the cluster have access
    to the router and internal registry. Both of these are integral components of
    OpenShift.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这条规则有一个例外——流量可以在 `default` 项目与其他项目之间流动。因为 `default` 项目是特权项目并分配了 VNID，所以集群中的所有
    pod 都可以访问路由器和内部注册表。它们是 OpenShift 的核心组件。
- en: In order to switch to the new plugin, we will have to perform a series of steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了切换到新插件，我们需要执行一系列步骤。
- en: 'First, change `networkPluginName` in the master''s configuration file:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，修改主节点配置文件中的 `networkPluginName`：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, on all nodes:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在所有节点上：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, restart the master API and controllers:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，重启主 API 和控制器：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Stop the node processes on all nodes:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止所有节点上的节点进程：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Restart the OpenVSwitch service on all nodes:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启所有节点上的 OpenVSwitch 服务：
- en: '[PRE29]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, start the node processes again:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，再次启动节点进程：
- en: '[PRE30]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that when you restart a node process, pods on that node will get a new
    IP addresses.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你重启节点进程时，该节点上的 pod 会获得新的 IP 地址。
- en: 'Now, let''s see if projects `demo-1` and `demo-2` are able to reach each other.
    First, let''s get the new IP address of the `httpd` pod from the `demo-1` project:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下 `demo-1` 和 `demo-2` 项目是否能够互相访问。首先，我们从 `demo-1` 项目获取 `httpd` pod 的新
    IP 地址：
- en: '[PRE31]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, do the same for the `demo-2` project:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对 `demo-2` 项目做同样的操作：
- en: '[PRE32]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s try and `ping` the pod in the `demo-1` project from `demo-2`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试 `ping` 从 `demo-2` 访问 `demo-1` 项目的 pod：
- en: '[PRE33]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And vice versa:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来也可以：
- en: '[PRE34]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have confirmed that the projects are indeed isolated. But what if in a real-world
    scenario there is an exception and you need some projects to be able to communicate
    with each other? An example would be a standard 3-tier application with a database,
    backend, and frontend residing in different projects for more granular control
    over resource allocation. For these kinds of use cases, the OpenShift CLI provides
    a command to `join` projects together, effectively enabling communication between
    them:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确认项目之间确实是隔离的。但如果在实际场景中有例外，且需要某些项目之间能够通信该怎么办呢？例如，一个标准的三层应用，数据库、后台和前端部署在不同的项目中，以便对资源分配进行更细粒度的控制。对于这些用例，OpenShift
    CLI 提供了一个命令来将项目 `join` 在一起，从而有效地启用它们之间的通信：
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This command provides no output and can be used as a quick way to make exceptions
    in your security policy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令没有输出，可以作为快速在安全策略中做出例外的方式。
- en: It's worth noting that the same result can be achieved by swapping projects: `oc
    adm pod-network join-projects --to=demo-2 demo-1`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，通过交换项目也可以实现相同的结果：`oc adm pod-network join-projects --to=demo-2 demo-1`。
- en: 'Now, let''s see if it worked. Try to `ping` our pod from the `demo-1` project
    first and then from `demo-2`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看是否成功。先从`demo-1`项目尝试`ping`我们的pod，然后从`demo-2`尝试：
- en: '[PRE36]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have tested the plugin''s functionality with ordinary projects,
    let''s go ahead and confirm the `default` project''s privileged status. Switch
    to `default` and find out the IP address of the registry pod:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经用普通项目测试过插件的功能，接下来确认`default`项目的特权状态。切换到`default`，查找注册表pod的IP地址：
- en: '[PRE37]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, switch back to `demo-1` and try to reach the registry pod from there:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，切换回`demo-1`并尝试从那里访问注册表pod：
- en: '[PRE38]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Again, do the same for the `demo-2` project:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 再次对`demo-2`项目执行相同操作：
- en: '[PRE39]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Joining projects together is not an irreversible operation—you can isolate
    a certain project from the rest of the environment just as easily:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 将项目连接在一起并不是不可逆的操作——你可以轻松地将某个项目从其他环境中隔离出来：
- en: '[PRE40]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding command effectively blocks all traffic to and from all pods in
    the `demo-1` project. Let''s confirm that by trying to reach the pod in that project
    from `demo-2`, which is where we are right now:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令有效地阻止了所有流量进出`demo-1`项目中的所有pod。让我们通过尝试从`demo-2`（我们当前所在的项目）访问该项目中的pod来确认这一点：
- en: '[PRE41]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Just like we did previously, let''s do the same from `demo-1`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前做的那样，继续从`demo-1`执行相同操作：
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As you can see, the project was successfully isolated using just a single command.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该项目已经成功隔离，只用了一个命令。
- en: Besides joining and isolating projects, OpenShift also provides another feature
    for managing pod networking—making a project global. This allows traffic to the
    project from all pods across all projects and vice versa—the same as with the `default`
    project. A potential use case for such a configuration is project hosting a messaging
    bus that's used by all other applications in the cluster.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将项目连接和隔离，OpenShift还提供了另一种管理pod网络的功能——将项目设为全局。这允许来自所有项目的所有pod与该项目之间的流量互通——与`default`项目相同。这样的配置的一个潜在用例是项目承载一个消息总线，供集群中的所有其他应用程序使用。
- en: 'Let''s make the `demo-2` project global:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把`demo-2`项目设为全局：
- en: '[PRE43]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s see if it worked:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否成功：
- en: '[PRE44]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Unlike before, where the `demo-1` project was isolated, traffic is now allowed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前将`demo-1`项目隔离不同，现在允许流量通过。
- en: Now, let's move to the last SDN plugin, which is provided by OpenShift.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续处理最后一个由OpenShift提供的SDN插件。
- en: ovs-networkpolicy plugin
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`ovs-networkpolicy`插件'
- en: While providing a simple to use and mostly adequate mechanism for managing access
    between projects, the `ovs-multitenant` plugin lacks the ability to control access
    at a more granular level. This is where the `ovs-networkpolicy` plugin steps in—it
    lets you create custom `NetworkPolicy` objects that, for example, can apply restrictions
    to ingress or egress traffic.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提供了一个简单易用且大致足够的机制来管理项目间的访问，但`ovs-multitenant`插件缺乏更细粒度的访问控制能力。此时，`ovs-networkpolicy`插件发挥作用——它允许你创建自定义的`NetworkPolicy`对象，例如，可以对流入或流出流量进行限制。
- en: 'In order to migrate from the `ovs-multitenant` plugin to this one, we have
    to isolate ordinary projects from each other and allow traffic to and from global
    projects. Global projects are distinguished by having `0` as their NETID, as seen
    in the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从`ovs-multitenant`插件迁移到这个插件，我们需要将普通项目彼此隔离，并允许与全局项目之间的流量。全局项目的NETID为`0`，如下输出所示：
- en: '[PRE45]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In our case, the only global projects are `default` and `demo-2`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，唯一的全局项目是`default`和`demo-2`。
- en: To spare you the manual effort, a helper script has already been written to
    create all of the necessary `NetworkPolicy` objects to allow traffic between pods
    in the same project and between each project and global ones. This script must
    be run prior to carrying out the usual steps for migrating from one OpenShift
    plugin to another.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省你的手动操作，已经编写了一个辅助脚本，用于创建所有必要的`NetworkPolicy`对象，以允许同一项目内的pod之间以及每个项目与全局项目之间的流量。此脚本必须在执行从一个OpenShift插件迁移到另一个插件的常规步骤之前运行。
- en: 'First, we have to download the script and make it executable:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要下载脚本并使其可执行：
- en: '[PRE46]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, run it and observe what steps are being taken to ensure the presence
    of correct network policies across projects:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行它并观察采取了哪些步骤，以确保跨项目存在正确的网络策略：
- en: '[PRE47]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Notice the special treatment that global projects get: they were assigned the `pod.network.openshift.io/legacy-netid=0` label,
    which is used as a selector by NetworkPolicy objects to enable access from such
    projects. To see this for yourself, export the `allow-from-global-namespaces`
    network policy''s definition:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意全局项目的特殊处理：它们被赋予了`pod.network.openshift.io/legacy-netid=0`标签，NetworkPolicy对象使用该标签作为选择器，允许来自这些项目的访问。要亲自查看这一点，可以导出`allow-from-global-namespaces`网络策略的定义：
- en: '[PRE48]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Once this is done, the rest of the process is the same as in the previous subsection
    with the `networkPluginName` set to `redhat/openshift-ovs-networkpolicy`. Refer
    to the previous section for detailed instructions on how to enable an OpenShift
    plugin.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，接下来的过程与前一小节相同，只需将`networkPluginName`设置为`redhat/openshift-ovs-networkpolicy`。有关如何启用OpenShift插件的详细说明，请参阅前一部分。
- en: 'Now that this is out of the way, let''s remind ourselves what project we are
    in:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 既然这些问题已经解决，接下来我们来提醒一下自己当前所在的项目：
- en: '[PRE49]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, find out the new IP address of our Apache pod for future reference:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，查找我们Apache pod的新IP地址以供将来参考：
- en: '[PRE50]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Do the same for the `demo-2` project:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对`demo-2`项目做同样的操作：
- en: '[PRE51]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, try pinging the pod in `demo-1` from `demo-2`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尝试从`demo-2`中ping`demo-1`中的pod：
- en: '[PRE52]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And vice versa, from `demo-1` to `demo-2`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，从`demo-1`到`demo-2`：
- en: '[PRE53]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Astute readers may recall that `demo-2` is actually a global project, meaning
    that both ingress and egress traffic is enabled between it and any other project,
    thanks to the `allow-from-global-namespaces` network policy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能会记得，`demo-2`实际上是一个全局项目，这意味着由于`allow-from-global-namespaces`网络策略，它与任何其他项目之间的入站和出站流量都是启用的。
- en: 'Let''s create another project called `demo-3` to host the same `httpd` pod
    and get the IP address of the pod:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个名为`demo-3`的项目，来托管相同的`httpd` pod，并获取该pod的IP地址：
- en: '[PRE54]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Try to reach the pod in the `demo-1` project:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试访问`demo-1`项目中的pod：
- en: '[PRE55]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This time, packets didn''t come through because `demo-3` is just a regular
    project and as such it''s subject to network policy restrictions. Let''s change
    that by creating a network policy in the `demo-1` project that will allow traffic
    from `demo-3`, but before that, we will have to label the `demo-3` project so
    that the policy can refer to it using a selector:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这次数据包没有通过，因为`demo-3`只是一个普通项目，因此它受网络策略限制。我们通过在`demo-1`项目中创建一个网络策略，允许来自`demo-3`的流量，但在此之前，我们需要为`demo-3`项目添加标签，以便策略能够通过选择器引用它：
- en: '[PRE56]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Notice that `ingress` is on the same level of indentation as `podSelector`—this
    is not a typo, but an omitted pod selector, because in our example we match namespaces
    instead of pods.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`ingress`与`podSelector`的缩进在同一层级——这不是笔误，而是省略了pod选择器，因为在我们的示例中，我们是根据命名空间来匹配，而不是pod。
- en: 'Let''s try accessing `demo-1` again:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再尝试访问`demo-1`：
- en: '[PRE57]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: As you can see, the network policy is now in effect.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网络策略现在已经生效。
- en: 'OpenShift can also be configured to create a default network policy for every
    project when it''s being instantiated. The OpenShift CLI provides a command for
    bootstrapping a project template:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift还可以配置为在实例化项目时为每个项目创建一个默认的网络策略。OpenShift CLI提供了一个用于引导项目模板的命令：
- en: '[PRE58]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Modify the template so that it contains a network policy that blocks all ingress
    traffic—this is the easiest way to see if it''s working or not:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 修改模板，使其包含一个网络策略，阻止所有的入站流量——这是检查它是否有效的最简单方法：
- en: '[PRE59]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Create the template from its YAML definition:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从其YAML定义创建模板：
- en: '[PRE60]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The template was created in the `demo-3` project because it's not technically
    important, but it's recommended to store it in one of the pre-existing projects,
    such as `default` or `openshift-infra`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该模板是在`demo-3`项目中创建的，因为它在技术上并不重要，但建议将其存储在现有的项目中，如`default`或`openshift-infra`。
- en: 'To configure OpenShift to pick up the new template, make the following edit
    to the master''s configuration file:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让OpenShift能够使用新的模板，修改主节点的配置文件如下：
- en: '[PRE61]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Lastly, restart the master API service to activate the changes:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重启主API服务以激活更改：
- en: '[PRE62]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s create a `new-project` and see if the network policy was created:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个`new-project`，看看是否创建了网络策略：
- en: '[PRE63]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now that the project has been successfully instantiated with the security policy
    we configured, let''s see if the policy itself works. Like we did previously,
    we will create a pod by running Apache web server and getting its IP address:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在项目已经成功实例化，并应用了我们配置的安全策略，接下来让我们检查一下该策略是否有效。就像我们之前做的那样，我们将创建一个运行Apache Web服务器的pod，并获取它的IP地址：
- en: '[PRE64]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we will switch the project to `demo-3` and see if we can reach our pod
    from there:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将切换项目到`demo-3`，看看是否能从那里访问我们的 Pod：
- en: '[PRE65]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As expected, all incoming traffic is blocked.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，所有进入的流量都被阻止了。
- en: On this note, we conclude the section on OpenShift SDN plugins.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们结束了关于 OpenShift SDN 插件的部分。
- en: Egress routers
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 出口路由器
- en: 'As you have learned previously, routers in OpenShift direct ingress traffic
    from external clients to services that, in turn, forward it to pods. OpenShift
    also offers a reverse type of router intended for forwarding egress traffic from
    pods to a certain destination in the external network. But unlike ingress routers
    implemented via HAProxy, egress ones are built on Squid. Egress routers are potentially
    useful for cases such as:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您之前所学，OpenShift 中的路由器将来自外部客户端的入口流量引导到服务，而服务又将其转发到 Pod。OpenShift 还提供了一种反向类型的路由器，用于将
    Pod 中的出口流量转发到外部网络的特定目标。但与通过 HAProxy 实现的入口路由器不同，出口路由器是基于 Squid 构建的。出口路由器在以下情况下可能非常有用：
- en: Masking different external resources being used by several applications with
    a single global resource. For example, applications may be developed in such a
    way that they are built pulling dependencies from different mirrors, and collaboration
    between their development teams is rather loose. So, instead of getting them to
    use the same mirror, an operations team can just set up an egress router to intercept
    all traffic directed to those mirrors and redirect it to the same site.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单一全局资源来屏蔽多个应用程序所使用的不同外部资源。例如，应用程序可能以这样一种方式开发，即它们从不同的镜像拉取依赖，而它们的开发团队之间的协作相对松散。因此，操作团队可以设置一个出口路由器来拦截所有指向这些镜像的流量，并将其重定向到同一个站点，而不是让它们使用相同的镜像。
- en: To redirect all suspicious requests for specific sites to the audit system for
    further analysis.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有针对特定站点的可疑请求重定向到审计系统进行进一步分析。
- en: 'OpenShift supports the following types of egress router:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 支持以下类型的出口路由器：
- en: '*redirect* for redirecting traffic to a certain destination IP'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*redirect* 用于将流量重定向到特定的目标 IP。'
- en: '*http-proxy* for proxying HTTP, HTTPS, and DNS traffic'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*http-proxy* 用于代理 HTTP、HTTPS 和 DNS 流量。'
- en: Due to limitations regarding `macvlan` interfaces in VirtualBox, an egress router
    cannot be set up in our virtual lab, nor in AWS. The best platform to use it on
    is bare-metal.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在 VirtualBox 中`macvlan`接口的限制，我们无法在虚拟实验室或 AWS 中设置出口路由器。最适合使用的环境是裸机。
- en: Static IPs for external project traffic
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部项目流量的静态 IP
- en: The OpenShift scheduler takes all decisions regarding the placement of pods
    on nodes, taking into account factors such as the even distribution of pods, node
    affinity, and available resources. The whole point of default scheduling in OpenShift
    is to use of available resources as efficiently as possible, but it doesn't take
    into account the project pods that are created in them. The reason for this is
    that developers shouldn't be concerned with the placement of an applications'
    pods across the cluster, and that's why they have absolutely no control over where
    their pods end up. The problem starts to manifest itself in large organizations
    with multiple applications subject to different policies regarding security and
    compliance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 调度器负责决定 Pod 在节点上的部署位置，考虑因素包括 Pod 的均匀分布、节点亲和性和可用资源。OpenShift 默认的调度策略的核心目标是尽可能高效地利用可用资源，但它不会考虑其中创建的项目
    Pods。这是因为开发人员不应该关心应用程序的 Pods 在集群中的位置，这也是他们完全无法控制 Pods 最终位置的原因。问题在于，当组织规模较大，涉及多个应用程序，并且这些应用程序受到不同的安全性和合规性政策的约束时，这个问题就开始显现出来。
- en: For example, an application handling bank account details must be subject to
    thorough audit, while its development version must have no access to production
    databases. Since the concept of projects is unknown to the scheduler, pods with
    different applications may end up on the same node, generating traffic with the
    same source IP address (the node's IP address), making it impossible to distinguish
    them from each other on the corporate firewall and to apply the appropriate policies.
    Technically, one can create a custom scheduling policy which will `pin` pods with
    specific labels to a specific node or set of nodes, which will provide a consistent
    pool of source addresses to be permitted through the firewall. However, over time,
    it will seriously skew the pods' distribution across the cluster, leading to inefficient
    use of resources and mix operations and the development teams' areas of control,
    which defeats the purpose of scheduling.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，处理银行账户信息的应用程序必须经过严格的审计，而其开发版本不能访问生产数据库。由于调度器无法识别项目的概念，不同应用程序的pod可能会部署在同一节点上，从而产生相同源IP地址（节点的IP地址）的流量，使得在公司防火墙上无法区分它们，也无法应用适当的策略。从技术上讲，可以创建一个自定义调度策略，将具有特定标签的pod固定到特定节点或节点集，这将提供一个一致的源地址池，允许通过防火墙。然而，随着时间的推移，这将严重扭曲pod在集群中的分布，导致资源使用效率低下，并混淆操作和开发团队的控制范围，最终破坏调度的目的。
- en: OpenShift provides a solution for exactly this kind of problem—you can assign
    an externally routable IP address to a particular project and whitelist it on
    the corporate firewall, at the same time leaving scheduling completely transparent
    to developers.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift为这种问题提供了解决方案——您可以为特定项目分配一个可外部路由的IP地址，并在公司防火墙上将其列入白名单，同时使调度对开发人员保持完全透明。
- en: As with egress routers, the virtual environment of VirtualBox places limitations
    on the possibility of demonstrating this feature.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与出口路由器一样，VirtualBox的虚拟环境对演示此功能的可能性有限制。
- en: Egress network policies
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 出口网络策略
- en: While the idea behind network policies is to control access between pods across
    projects, egress network policies allow you to restrict access from all pods in
    a project to certain *external* resources. A typical use case for this feature
    would be denying pods access to source code from hosting providers and content
    mirrors to prevent any updates of applications and/or system libraries in those
    pods. It's important to understand that, unlike egress routers, egress network
    policies don't perform any redirection of traffic, working on just an *Allow versus
    Deny* basis instead.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略的理念是控制跨项目的pod间访问，而出口网络策略允许您限制来自项目中所有pod对某些*外部*资源的访问。此功能的典型用例是禁止pod访问托管提供商的源代码和内容镜像，以防止在这些pod中更新应用程序和/或系统库。需要理解的是，与出口路由器不同，出口网络策略不进行任何流量重定向，而是仅基于*允许与拒绝*来工作。
- en: 'Let''s see what level of access pods our `demo-1` project has:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下`demo-1`项目的pod访问级别：
- en: '[PRE66]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Currently, there are no egress network policies being enforced in the project,
    so access to external resources is completely unrestricted.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，项目中没有强制执行出口网络策略，因此访问外部资源完全不受限制。
- en: 'Now, create a custom egress network policy from the YAML definition, which
    is going to block all traffic to GitHub and permit traffic to all other external
    resources:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从YAML定义中创建一个自定义出口网络策略，阻止所有流量到GitHub并允许流量访问所有其他外部资源：
- en: '[PRE67]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let''s try accessing the same resources as in the beginning of this section:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试访问与本节开始时相同的资源：
- en: '[PRE68]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As you can see, GitHub is now inaccessible, which is exactly what we expected.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，GitHub现在无法访问，这正是我们预期的结果。
- en: 'In this example, we implemented a *deny all but* type of security policy, but
    we can also implement a reverse type, granting access to single resources, blocking
    everything else. Continuing our example with GitHub and Google, `edit` the policy''s
    specification to resemble the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们实现了*仅拒绝其他*类型的安全策略，但我们也可以实现一个反向类型，允许访问单个资源，阻止所有其他访问。继续我们以GitHub和Google为例，`编辑`策略的规范如下：
- en: '[PRE69]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The preceding configuration directs the policy to block traffic to all external
    resources, except for GitHub and `dnsmasq` on the node.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置指示策略阻止所有流量到外部资源，除了GitHub和节点上的`dnsmasq`。
- en: 'Let''s test this out:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下：
- en: '[PRE70]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Again, the policy works as expected.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，策略如预期般工作。
- en: Note that egress rules are evaluated in the order in which they are specified
    and the first matching rule wins, meaning that, if we had placed the Deny rule
    first, traffic to GitHub would have been blocked as well, even though it's explicitly
    permitted in one of the subsequent rules.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，出站规则是按照指定的顺序进行评估的，匹配的第一个规则会生效。这意味着，如果我们先放置了拒绝规则，GitHub 的流量也会被阻止，即使在后续规则中明确允许了该流量。
- en: DNS
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNS
- en: One of the mechanisms for linking pods together, which has been discussed earlier
    in this book, relies on environment variables—the same as you would achieve by
    using plain Docker. When you deploy a multi-container application on OpenShift,
    pods that provide certain environment variables for pods that consume them must
    be started first, so that the variables are configured correctly by OpenShift.
    For example, if you deploy a 3-tier application consisting of a database, backend,
    and frontend, you will have to deploy the database first so that the backend pod
    picks up environment variables with the correct address and port for the database.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 之前在本书中讨论的将 Pod 连接在一起的一种机制依赖于环境变量——这和使用普通 Docker 实现的方式相同。当你在 OpenShift 上部署一个多容器应用时，提供某些环境变量给消费它们的
    Pod 的那些 Pod 必须先启动，以便 OpenShift 能正确配置这些变量。例如，如果你部署一个包含数据库、后端和前端的三层应用，你必须首先部署数据库，以便后端
    Pod 能够获取到带有正确地址和端口的数据库环境变量。
- en: Pods can access each other's services directly via their IPs, but in a highly
    dynamic environment, where services may often be re-created, there is a need for
    a more stable solution. Aside from using environment variables, OpenShift provides
    its internal DNS, implemented via SkyDNS and dnsmasq for service discovery. This
    approach doesn't limit your deployment to a certain order and spares you the need
    to implement additional logic in your deployment strategy. Using OpenShift DNS,
    all applications can discover each other across the entire cluster via consistent
    names, which makes it possible for developers to rely on them when migrating to
    OpenShift. The only thing they need to do is agree with Operations on the names
    of the services.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 可以通过它们的 IP 直接访问彼此的服务，但在一个高度动态的环境中，服务可能经常被重建，因此需要一种更稳定的解决方案。除了使用环境变量，OpenShift
    还提供了其内部 DNS，使用 SkyDNS 和 dnsmasq 实现服务发现。这个方法不会限制部署顺序，且不需要在部署策略中实现额外的逻辑。通过 OpenShift
    DNS，所有应用可以通过一致的名称在整个集群中发现彼此，这使得开发人员在迁移到 OpenShift 时可以依赖这些名称。唯一需要做的就是与运维团队达成一致，确定服务的名称。
- en: 'DNS in OpenShift gives pods the ability to discover the following resources
    in OpenShift:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 中的 DNS 使 Pod 能够发现以下资源：
- en: '| **Name** | **Domain** |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **域名** |'
- en: '| Services | `<service>.<project>.svc.cluster.local` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | `<service>.<project>.svc.cluster.local` |'
- en: '| Endpoints | `<service>.<project>.endpoints.cluster.local` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 端点 | `<service>.<project>.endpoints.cluster.local` |'
- en: In the following exercise, we will see how two applications that are deployed
    in different projects can reach each other.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将看到部署在不同项目中的两个应用如何互相连接。
- en: 'First, let''s create a project called `demo-1`:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个名为 `demo-1` 的项目：
- en: '[PRE71]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next, create a pod running Apache web server. We will be using the same YAML
    configuration as before:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个运行 Apache Web 服务器的 Pod。我们将使用之前相同的 YAML 配置：
- en: '[PRE72]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'In order to simulate the way *real* applications interact with each other,
    we will have to create a service to serve as a sole ingress point for our pod:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟 *真实* 应用之间的交互，我们需要创建一个服务，作为 Pod 唯一的入口点：
- en: '[PRE73]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now that the first project is ready, let''s create another one:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在第一个项目已准备好，我们来创建另一个项目：
- en: '[PRE74]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Like we did previously, create a pod from the same YAML definition as before:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，使用之前相同的 YAML 定义创建一个 Pod：
- en: '[PRE75]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'And create a service by exposing the pod:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过暴露 Pod 创建一个服务：
- en: '[PRE76]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now, let''s open a bash session into the newly created pod and try to reach
    the pod from the `demo-1` project:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开一个 bash 会话，进入新创建的 Pod，并尝试从 `demo-1` 项目访问该 Pod：
- en: '[PRE77]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'For the sake of completeness, let''s switch the project to `demo-1` and try
    the same from the first pod:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们将切换项目到 `demo-1` 并从第一个 Pod 尝试相同的操作：
- en: '[PRE78]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'It''s even possible to get all endpoints of a particular service, although
    it''s recommended to use services as points of contact:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，也可以获取某个特定服务的所有端点，尽管建议使用服务作为接入点：
- en: '[PRE79]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: OpenShift injects cluster-level subdomains into the local resolver's configuration
    at `/etc/resolv.conf`, so if you take a look in that file, you will find the line
    `search <project>.svc.cluster.local svc.cluster.local cluster.local`. Therefore,
    FQDNs don't have to be specified in order to reach resources across a project's
    boundaries and in the same project. For example, you can use `httpd.demo-1` to
    call a service named `httpd` in the `demo-1` project, or just `httpd` if it's
    in the same project.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 将集群级别的子域注入到本地解析器的配置中，路径为 `/etc/resolv.conf`，所以如果你查看该文件，你会找到一行 `search
    <project>.svc.cluster.local svc.cluster.local cluster.local`。因此，FQDN（完全限定域名）不必在项目边界和同一项目内的资源间传递。例如，你可以使用
    `httpd.demo-1` 来调用 `demo-1` 项目中的名为 `httpd` 的服务，或者如果在同一项目中，则只需使用 `httpd`。
- en: As you can see, both pods can reach each other via their services, which makes
    it possible not to rely on environment variables. So, in order to migrate their
    applications to OpenShift, developers will have to configure environment variables
    of their applications to point to the DNS names of dependent services.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两个 pod 可以通过它们的服务互相访问，这使得不再依赖环境变量成为可能。因此，为了将他们的应用迁移到 OpenShift，开发人员需要配置应用的环境变量，以指向依赖服务的
    DNS 名称。
- en: At the beginning of this chapter, we provided diagrams detailing the DNS architecture
    and DNS request flow in OpenShift. Now, let's see what it looks like on a live
    cluster.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们提供了详细的 OpenShift 中 DNS 架构和 DNS 请求流的图示。现在，让我们看看在实际集群中的表现。
- en: 'Run the following command on the master to see what processes are listening
    on TCP and UDP ports ending with `53` (DNS):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在 master 节点上运行以下命令，查看监听 `53`（DNS）端口的 TCP 和 UDP 进程：
- en: '[PRE80]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The process launched from the `openshift` binary is no other than the OpenShift
    Master API, as SkyDNS is embedded into it and uses etcd as a source of authority
    to keep track of new services and to delete records for deleted ones:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `openshift` 二进制文件启动的进程就是 OpenShift Master API，因为 SkyDNS 被嵌入其中，并且使用 etcd 作为权威源来跟踪新服务，并删除已删除服务的记录：
- en: '[PRE81]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, let''s take a look at listening ports on our first node—the setup is completely
    the same for all nodes in a cluster:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看第一个节点上的监听端口——该设置在集群中的所有节点上完全相同：
- en: '[PRE82]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: From the preceding output, we can see that SkyDNS is still present on nodes,
    but there's also `dnsmasq`. The latter actually forwards DNS requests into the `cluster.local`
    and `in-addr.arpa` zones, while redirecting all others to an upstream DNS server—in
    our case, its DNS is provided by VirtualBox itself.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到 SkyDNS 仍然存在于节点上，但也有 `dnsmasq`。后者实际上将 DNS 请求转发到 `cluster.local`
    和 `in-addr.arpa` 区域，同时将其他所有请求重定向到上游 DNS 服务器——在我们的案例中，其 DNS 由 VirtualBox 本身提供。
- en: 'Let''s take a look at OpenShift processes running on nodes:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看在节点上运行的 OpenShift 进程：
- en: '[PRE83]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Notice that this is the same OpenShift process as listed in the output of the `ss`
    command. As with the Master API, SkyDNS is embedded into the Node process as well
    to serve DNS requests for services of applications that are deployed on OpenShift.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是与 `ss` 命令输出中列出的相同的 OpenShift 进程。与 Master API 一样，SkyDNS 也被嵌入到节点进程中，以为部署在
    OpenShift 上的应用服务提供 DNS 请求服务。
- en: 'The information we''ve learned can be represented by the following diagram:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到的信息可以通过以下图示表示：
- en: '![](img/00065.jpeg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: Figure 2 - The DNS architecture in OpenShift
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 - OpenShift 中的 DNS 架构
- en: Lastly, let's figure out the actual path DNS queries take before reaching their
    destinations. For that, we will take a look into various resolver and `dnsmasq`
    configuration files.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们弄清楚 DNS 查询在到达目的地之前实际经过的路径。为此，我们将查看各种解析器和 `dnsmasq` 配置文件。
- en: 'Our first stop is the configuration of the local DNS resolver for the `httpd`
    pod in the `demo-2` project:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一站是 `demo-2` 项目中 `httpd` pod 的本地 DNS 解析器配置：
- en: '[PRE84]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: According to the preceding configuration, DNS queries for domains specified
    in the `search` directive are to be resolved by the DNS server available at `10.0.2.15`,
    which is the IP of one of the network interfaces dnsmasq is listening to on the
    node.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的配置，`search` 指令中指定的域名的 DNS 查询将由 `10.0.2.15` 上的 DNS 服务器解析，这个 IP 地址是 dnsmasq
    在节点上监听的网络接口之一。
- en: 'Now, let''s take a look into the file specifying the DNS forwarding policy
    for internal zones:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看指定内部区域 DNS 转发策略的文件：
- en: '[PRE85]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The preceding configuration directs dnsmasq to forward all DNS queries for domains
    `in-addr.arpa` and `cluster.local` to whatever DNS server is listening on localhost,
    which is SkyDNS.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置指示 dnsmasq 将所有针对 `in-addr.arpa` 和 `cluster.local` 域的 DNS 查询转发到监听在本地主机上的
    DNS 服务器，即 SkyDNS。
- en: 'Next, open the following file:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开以下文件：
- en: '[PRE86]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: As opposed to the previous configuration, this directive configures `dnsmasq`
    to forward all other DNS queries to the upstream DNS, which is the DNS provided
    by VirtualBox in our case.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的配置不同，这个指令配置 `dnsmasq` 将所有其他 DNS 查询转发到上游 DNS，实际上是我们使用的 VirtualBox 提供的 DNS。
- en: 'What we have just discovered can be represented by the following diagram:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚发现的内容可以通过以下图示表示：
- en: '![](img/00066.jpeg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: Figure 3 - DNS query flow in OpenShift
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 - OpenShift 中的 DNS 查询流
- en: This concludes our exploration of OpenShift DNS.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 以上就是我们对 OpenShift DNS 的探索总结。
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you learned about the importance of SDN and the role it plays
    in OpenShift, the composed network topology diagram for an existing OpenShift
    cluster, gained knowledge on various OpenShift and third-party plugins, and saw
    for yourself what features they provide. You also learned about use cases of both
    egress routers and static IPs for external project traffic, and also created your
    own egress network policy to restrict access to an external resource.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，你了解了 SDN 的重要性以及它在 OpenShift 中的作用，了解了现有 OpenShift 集群的网络拓扑图，掌握了各种 OpenShift
    插件和第三方插件的功能，并亲自体验了它们提供的特性。你还了解了出口路由器和静态 IP 在外部项目流量中的使用场景，并创建了自己的出口网络策略以限制访问外部资源。
- en: In the next chapter, we will be working on the deployment of a simple application.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将部署一个简单的应用程序。
- en: Questions
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which interface of OVS bridge is used to pass traffic from pods running on a
    particular node to and from pods running on other nodes?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OVS 桥接器的哪个接口用于在特定节点上运行的 pod 与在其他节点上运行的 pod 之间传递流量？
- en: tun0
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: tun0
- en: br0
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: br0
- en: veth...
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: veth...
- en: vxlan_sys_4789
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: vxlan_sys_4789
- en: Suppose we have a multi-tenant environment with many applications developed
    by independent teams and outsource contractors that must be able to collaborate
    in rare cases, but for the most part must be totally isolated from each other.
    What is the simplest course of action to achieve that?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个多租户环境，里面有许多由独立团队和外包承包商开发的应用，这些应用需要在少数情况下能够协作，但大部分时间必须完全隔离。要实现这一点，最简单的方式是什么？
- en: Use the ovs-networkpolicy plugin and write custom network policies to enable
    omni-directional traffic between the projects used by those parties
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ovs-networkpolicy 插件，并编写自定义网络策略，允许相关方使用的项目之间进行全向流量传输
- en: Use the ovs-subnet plugin
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ovs-subnet 插件
- en: Use the ovs-multitenant plugin and join and isolate projects as needed
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ovs-multitenant 插件，根据需要加入或隔离项目
- en: Use the plugin for a third-party solution, such as VMWare NSX-T or Tigera Calico
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第三方解决方案的插件，例如 VMWare NSX-T 或 Tigera Calico
- en: What feature is best suited for whitelisting traffic coming from all pods in
    a specific project?
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个特性最适合对白名单中的流量进行过滤，针对来自特定项目中所有 pod 的流量？
- en: Egress router in proxy mode
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理模式下的出口路由器
- en: Egress router in redirect mode
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定向模式下的出口路由器
- en: Static IP for external traffic from the project
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 项目外部流量的静态 IP
- en: Custom scheduling policy
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义调度策略
- en: Custom iptables rules in the  `OPENSHIFT-ADMIN-OUTPUT-RULES` chain of the `filter`
    table
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OPENSHIFT-ADMIN-OUTPUT-RULES` 链中的自定义 iptables 规则（属于 `filter` 表）'
- en: Which of the following is the correct specification of the egress network policy
    that allows access to `rubygems.org` and `launchpad.net` only, assuming that this
    is the only egress network policy in the project?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪项是正确的出口网络策略，允许仅访问 `rubygems.org` 和 `launchpad.net`，假设这是项目中的唯一出口网络策略？
- en: '`- type: Deny`'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`- type: Deny`'
- en: '`  to:`'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`     cidrSelector: 0.0.0.0/0`'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    cidrSelector: 0.0.0.0/0`'
- en: '`- type: Allow`'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsName: rubygems.org`'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsName: rubygems.org`'
- en: '`- type: Allow`'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsName: launchpad.net`'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsName: launchpad.net`'
- en: '`- type: Allow`'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsName: rubygems.org`'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsName: rubygems.org`'
- en: '`- type: Allow`'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsName: launchpad.net`'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsName: launchpad.net`'
- en: '`- type: Allow`'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsName: rubygems.org`'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsName: rubygems.org`'
- en: '`- type: Allow`'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsName: launchpad.net`'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsName: launchpad.net`'
- en: '`- type: Deny`'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`- type: Deny`'
- en: '`  to:`'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    cidrSelector: 0.0.0.0/0`'
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    cidrSelector: 0.0.0.0/0`'
- en: '`- type: Allow`'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`- type: Allow`'
- en: '`  to:`'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    dnsNames:`'
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    dnsNames:`'
- en: '`    - launchpad.net`'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`    - launchpad.net`'
- en: '`   - rubygems.org`'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`   - rubygems.org`'
- en: '`- type: Deny`'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`- type: Deny`'
- en: '`  to:`'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`  to:`'
- en: '`    cidrSelector: 0.0.0.0/0`'
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`    cidrSelector: 0.0.0.0/0`'
- en: What is the correct DNS name for the service named `web` in the `dev` project?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务`web`在`dev`项目中的正确DNS名称是什么？
- en: '`web.dev.cluster.local`'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`web.dev.cluster.local`'
- en: '`web.cluster.local`'
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`web.cluster.local`'
- en: '`web.dev.svc.cluster.local`'
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`web.dev.svc.cluster.local`'
- en: '`web.dev.endpoints.cluster.local`'
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`web.dev.endpoints.cluster.local`'
- en: Further reading
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please look at the following links for further reading relating to this chapter:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下链接，进一步了解本章节相关内容：
- en: '**OpenShift DNS overview**:[ https://docs.openshift.org/latest/architecture/networking/networking.html](https://docs.openshift.org/latest/architecture/networking/networking.html)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift DNS概述**：[https://docs.openshift.org/latest/architecture/networking/networking.html](https://docs.openshift.org/latest/architecture/networking/networking.html)'
- en: '**High-level overview of SDN plugins and network topology in OpenShift**:[ https://docs.openshift.org/latest/architecture/networking/sdn.html](https://docs.openshift.org/latest/architecture/networking/sdn.html)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift中SDN插件和网络拓扑的高级概述**：[https://docs.openshift.org/latest/architecture/networking/sdn.html](https://docs.openshift.org/latest/architecture/networking/sdn.html)'
- en: '**A few examples of third-party SDN plugins**:[ https://docs.openshift.org/latest/architecture/networking/network_plugins.html](https://docs.openshift.org/latest/architecture/networking/network_plugins.html)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**几个第三方SDN插件的示例**：[https://docs.openshift.org/latest/architecture/networking/network_plugins.html](https://docs.openshift.org/latest/architecture/networking/network_plugins.html)'
- en: '**Optimizing network performance for OpenShift**: [https://docs.openshift.org/latest/scaling_performance/network_optimization.html](https://docs.openshift.org/latest/scaling_performance/network_optimization.html)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化OpenShift网络性能**：[https://docs.openshift.org/latest/scaling_performance/network_optimization.html](https://docs.openshift.org/latest/scaling_performance/network_optimization.html)'
- en: '**Configuring the SDN in OpenShift and migrating between SDN plugins**[:](https://docs.openshift.org/latest/install_config/configuring_sdn.html)[ https://docs.openshift.org/latest/install_config/configuring_sdn.html](https://docs.openshift.org/latest/install_config/configuring_sdn.html)[ ](https://docs.openshift.org/latest/install_config/configuring_sdn.html)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在OpenShift中配置SDN并在SDN插件之间迁移**：[https://docs.openshift.org/latest/install_config/configuring_sdn.html](https://docs.openshift.org/latest/install_config/configuring_sdn.html)'
- en: '**NSX-T Container Plug-in for OpenShift, Installation and Administration Guide**:[ https://docs.vmware.com/en/VMware-NSX-T/2.1/nsxt_21_ncp_openshift.pdf](https://docs.vmware.com/en/VMware-NSX-T/2.1/nsxt_21_ncp_openshift.pdf)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenShift的NSX-T容器插件安装和管理指南**：[https://docs.vmware.com/en/VMware-NSX-T/2.1/nsxt_21_ncp_openshift.pdf](https://docs.vmware.com/en/VMware-NSX-T/2.1/nsxt_21_ncp_openshift.pdf)'
- en: '**Installing Red Hat OpenShift Container Platform with Contrail Networking**:[ https://www.jnpr.net/documentation/en_US/contrail4.0/topics/task/installation/install-redhat-openshift.html](https://www.jnpr.net/documentation/en_US/contrail4.0/topics/task/installation/install-redhat-openshift.html)'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Contrail Networking安装Red Hat OpenShift容器平台**：[https://www.jnpr.net/documentation/en_US/contrail4.0/topics/task/installation/install-redhat-openshift.html](https://www.jnpr.net/documentation/en_US/contrail4.0/topics/task/installation/install-redhat-openshift.html)'
- en: '**Using Contiv with OpenShift**: [http://contiv.github.io/documents/openshift/index.html](http://contiv.github.io/documents/openshift/index.html)'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在OpenShift中使用Contiv**：[http://contiv.github.io/documents/openshift/index.html](http://contiv.github.io/documents/openshift/index.html)'
- en: '**Installing Calico on OpenShift**: [https://docs.projectcalico.org/v2.4/getting-started/openshift/installation](https://docs.projectcalico.org/v2.4/getting-started/openshift/installation)'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在OpenShift上安装Calico**：[https://docs.projectcalico.org/v2.4/getting-started/openshift/installation](https://docs.projectcalico.org/v2.4/getting-started/openshift/installation)'
- en: '**Configuring Nuage SDN**: [https://docs.openshift.com/container-platform/3.9/install_config/configuring_nuagesdn.html](https://docs.openshift.com/container-platform/3.9/install_config/configuring_nuagesdn.html)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置Nuage SDN**：[https://docs.openshift.com/container-platform/3.9/install_config/configuring_nuagesdn.html](https://docs.openshift.com/container-platform/3.9/install_config/configuring_nuagesdn.html)'
- en: '**Managing networking in OpenShift via CLI**:[ https://docs.openshift.org/latest/admin_guide/managing_networking.html](https://docs.openshift.org/latest/admin_guide/managing_networking.html)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过CLI管理OpenShift中的网络**：[https://docs.openshift.org/latest/admin_guide/managing_networking.html](https://docs.openshift.org/latest/admin_guide/managing_networking.html)'
- en: '**Red Hat OpenShift Container Platform DNS deep dive**:[ https://www.redhat.com/en/blog/red-hat-openshift-container-platform-dns-deep-dive-dns-changes-red-hat-openshift-container-platform-36](https://www.redhat.com/en/blog/red-hat-openshift-container-platform-dns-deep-dive-dns-changes-red-hat-openshift-container-platform-36)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Red Hat OpenShift 容器平台 DNS 深入探讨**：[https://www.redhat.com/en/blog/red-hat-openshift-container-platform-dns-deep-dive-dns-changes-red-hat-openshift-container-platform-36](https://www.redhat.com/en/blog/red-hat-openshift-container-platform-dns-deep-dive-dns-changes-red-hat-openshift-container-platform-36)'
