- en: Managing OpenShift Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced you to the realm of security in OpenShift.
    OpenShift is an enterprise-ready application management platform that supports
    multiple security features, making it able to integrate into any corporate security
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like any cloud platform, OpenShift heavily relies on a networking stack on
    two different layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying network topology, which is directly determined either by physical
    network equipment or virtual network devices in the case of OpenShift itself deployed
    in the virtual environment. This level provides connectivity to OpenShift masters
    and nodes, and is beyond the control of OpenShift itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The virtual network topology, which is determined by the OpenShift SDN plugin
    being used. This level is concerned with managing connectivity between applications
    and providing external access to them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to work with networking on the upper level—OpenShift
    SDN—and we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Network topology in OpenShift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SDN plugins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egress routers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static IPs for external project traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egress network policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, we will be using the following configuration of VMs managed
    via Vagrant using the default VirtualBox provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Role** |'
  prefs: []
  type: TYPE_TB
- en: '| openshift-master | Master |'
  prefs: []
  type: TYPE_TB
- en: '| openshift-node-1 | Node |'
  prefs: []
  type: TYPE_TB
- en: '| openshift-node-2 | Node |'
  prefs: []
  type: TYPE_TB
- en: Make sure you have enough RAM on your desktop or laptop you use. The configuration
    above was tested with 8GB RAM, but it was barely enough, so we recommend running
    it on a system with 16GB at least.
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration corresponds to the following Vagrantfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to be able to reach the cluster inside the VM from your host system,
    make sure file `/etc/hosts` on your laptop looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `vagrant up` and wait till it finishes all the work. It may take up to
    30 mins depending on your internet connectivity and compute resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it''s done, open SSH session into the master VM and become root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the following inventory for deploying OpenShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Even though variables in the `nodes` group appear to be on separate lines, they
    are actually on previous ones with hosts they are associated with. If you just
    copy this file as it is from the one provided with other materials on this book,
    it will work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time to install OpenShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Network topology in OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to provide a common medium for containers to communicate with each
    other, OpenShift makes use of an overlay network that's implemented via VXLAN.
    The **Virtual eXtensible Local Area Network** (**VXLAN**) protocol provides a
    mechanism for transferring Layer 2 (Ethernet) frames across Layer 3 (IP) networks.
    Depending on the SDN plugin being used, the scope of communication may be limited
    to pods within the same project or maybe completely unrestricted. No matter which
    plugin is used, the network topology is still the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new node is registered in `etcd`, the master allocates a private `/23`
    subnet from the cluster network. By default, subnets are allocated from `10.128.0.0/14`,
    but can be configured in the `networkConfig` stanza of the master configuration
    file. The following is an excerpt from the file containing the relevant parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `hostSubnetLength` setting determines how many IP addresses are allocated
    to every node to be distributed between pods running on a given node. In our default
    configuration, the size of each subnet is 2⁹=512 addresses, which makes `510`
    IPs available for pods. Summing up, each pod's IP address will have mask /23 (14+9).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the `clusterNetworks[].cidr` setting can only be changed to
    a larger subnet that includes the previous setting. For example, it can be set
    to `10.128.0.0/12`, as it contains `/14`, but not to `10.128.0.0/16`, as they
    don't overlap completely.
  prefs: []
  type: TYPE_NORMAL
- en: Also, `hostSubnetLength` cannot be changed once the cluster is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overlay network in OpenShift is built from the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`br0`: An OVS bridge that all pods running on a particular node are plugged
    into via a `veth` pair. Each node has a single `br0` device which serves as a
    virtual switch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tun0`: This is an internal port of `br0`  numbered 2, which is assigned each
    node subnet''s default gateway address and is used for external access. OpenShift
    also creates routing and netfilter rules to direct traffic to the external network
    via NAT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vxlan_sys_4789`: An OVS port `1` of `br0` which provides connectivity between
    pods running on different nodes. It''s referred to as `vxlan` in the OVS rules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracing connectivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will create a `demo` project hosting a single `httpd`
    pod in order to see first-hand how the overlay network is constructed in OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create the project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a pod running Apache web server in the project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need the IP address allocated to the pod, as well as the address of
    the node it was scheduled to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Another step is to get the name of the pod''s network interface, which is actually
    one end of the `veth` pair that''s used to connect the pod to the `br0` bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s move on to another project, called `default`. This project is used
    to host special pods for the router and internal Docker registry. Both pods are
    deployed on the node labeled `infra`, which is `openshift-node-1` in our case.
    Let''s confirm this and find out the IP address of the registry pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The reason we picked the registry pod is that the router pod runs in privileged
    mode to have direct access to the node's networking stack; as such, it wouldn't
    represent a typical configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, launch the following command to get the name of the registry pod''s NIC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The following steps will be performed on the first node—`openshift-node-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s see what network devices were created on that node after OpenShift
    was installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`br0` is the OVS bridge that was mentioned at the beginning of the *Network
    topology in OpenShift* section. In order to see its active ports, use the `ovs-vsctl`
    command, which is provided by the `openvswitch` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, discover the same information about the second node, `openshift-node-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to sum up the preceding code, the following diagram provides a visual
    representation of what the resulting overlay network looks like in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 - Overlay network topology
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s clean up before the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: SDN plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned what components the overlay network in OpenShift
    comprises. Now, it's time to see how it can be configured to suit the requirements
    of a particular environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenShift makes its internal SDN plugins available out-of-the-box, as well
    as plugins for integration with third-party SDN frameworks. The following are
    three built-in plugins that are available in OpenShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '`` `ovs-subnet` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ovs-multitenant`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ovs-networkpolicy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision regarding which plugin to use is based on what level of security
    and control you aim to achieve. In the following subsections, we will discuss
    the main features and use cases for each of those plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 'With SDNs taking over networking, third-party vendors have also started to
    develop their own solutions for programmable networks. Red Hat works closely with
    such providers to ensure smooth integration of their products into OpenShift.
    The following solutions have been tested and verified by Red Hat as production-ready:'
  prefs: []
  type: TYPE_NORMAL
- en: Nokia Nuage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cisco Contiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Juniper Contrail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tigera Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMWare NSX-T
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting each of those to work with OpenShift is beyond the scope of this book,
    but you will find detailed instructions by following the links provided at the
    end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ovs-subnet plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the default plugin that''s enabled after OpenShift has just been installed.
    It provides connectivity for pods across the entire cluster with no limitations
    whatsoever, meaning that traffic can flow freely between all pods. This may be
    undesirable in large multi-tenant environments that place high importance on security.
    The SDN plugin being used is determined by the `networkConfig.networkPluginName`
    setting in the master configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The SDN plugin can also be specified explicitly upon installation via the `os_sdn_network_plugin_name`
    Ansible variable. By default, it's `redhat/openshift-ovs-subnet`.
  prefs: []
  type: TYPE_NORMAL
- en: In order to see for yourself what exactly the `ovs-subnet` plugin does (or,
    rather, does not do), create two projects with one pod each and try to reach one
    of them from the other one.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a `demo-1` project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, launch a pod by running the httpd web server using the same YAML definition,
    like we did in the *Tracing connectivity* subsection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s find out the IP address assigned to our pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Move on to creating the second project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And create the same pod in that project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see whether we can `ping` the first pod from the one we have just
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to be sure, let''s reverse our experiment and try to reach the pod in
    the `demo-2` project from the one deployed in `demo-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, communication between pods is completely uninhibited, which
    may be undesirable. In the two following subsections, we will demonstrate how
    to enforce project isolation using other OpenShift plugins.
  prefs: []
  type: TYPE_NORMAL
- en: ovs-multitenant plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it's usually not that big of a deal in PoC and sandboxes, security becomes
    a matter of utmost importance in large enterprises with diverse teams and project
    portfolios, even more so when the development of certain applications is outsourced
    to third-party companies. The `ovs-multitenant` plugin is a perfect choice if
    just having projects separated is enough. Unlike the `ovs-subnet` plugin, which
    passes all traffic across all pods, this one assigns the same VNID to all pods
    for each project, keeping them unique across projects, and sets up flow rules
    on the `br0` bridge to make sure that traffic is only allowed between pods with
    the same VNID.
  prefs: []
  type: TYPE_NORMAL
- en: There is, however, an exception to that rule—traffic is allowed to flow between
    the `default` project and each of the other ones. This is because that project
    is privileged and is assigned VNID, so that all pods in the cluster have access
    to the router and internal registry. Both of these are integral components of
    OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: In order to switch to the new plugin, we will have to perform a series of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, change `networkPluginName` in the master''s configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, on all nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, restart the master API and controllers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Stop the node processes on all nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the OpenVSwitch service on all nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, start the node processes again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that when you restart a node process, pods on that node will get a new
    IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see if projects `demo-1` and `demo-2` are able to reach each other.
    First, let''s get the new IP address of the `httpd` pod from the `demo-1` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, do the same for the `demo-2` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try and `ping` the pod in the `demo-1` project from `demo-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have confirmed that the projects are indeed isolated. But what if in a real-world
    scenario there is an exception and you need some projects to be able to communicate
    with each other? An example would be a standard 3-tier application with a database,
    backend, and frontend residing in different projects for more granular control
    over resource allocation. For these kinds of use cases, the OpenShift CLI provides
    a command to `join` projects together, effectively enabling communication between
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This command provides no output and can be used as a quick way to make exceptions
    in your security policy.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that the same result can be achieved by swapping projects: `oc
    adm pod-network join-projects --to=demo-2 demo-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see if it worked. Try to `ping` our pod from the `demo-1` project
    first and then from `demo-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have tested the plugin''s functionality with ordinary projects,
    let''s go ahead and confirm the `default` project''s privileged status. Switch
    to `default` and find out the IP address of the registry pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, switch back to `demo-1` and try to reach the registry pod from there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, do the same for the `demo-2` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Joining projects together is not an irreversible operation—you can isolate
    a certain project from the rest of the environment just as easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command effectively blocks all traffic to and from all pods in
    the `demo-1` project. Let''s confirm that by trying to reach the pod in that project
    from `demo-2`, which is where we are right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like we did previously, let''s do the same from `demo-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the project was successfully isolated using just a single command.
  prefs: []
  type: TYPE_NORMAL
- en: Besides joining and isolating projects, OpenShift also provides another feature
    for managing pod networking—making a project global. This allows traffic to the
    project from all pods across all projects and vice versa—the same as with the `default`
    project. A potential use case for such a configuration is project hosting a messaging
    bus that's used by all other applications in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make the `demo-2` project global:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see if it worked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Unlike before, where the `demo-1` project was isolated, traffic is now allowed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move to the last SDN plugin, which is provided by OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: ovs-networkpolicy plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While providing a simple to use and mostly adequate mechanism for managing access
    between projects, the `ovs-multitenant` plugin lacks the ability to control access
    at a more granular level. This is where the `ovs-networkpolicy` plugin steps in—it
    lets you create custom `NetworkPolicy` objects that, for example, can apply restrictions
    to ingress or egress traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to migrate from the `ovs-multitenant` plugin to this one, we have
    to isolate ordinary projects from each other and allow traffic to and from global
    projects. Global projects are distinguished by having `0` as their NETID, as seen
    in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the only global projects are `default` and `demo-2`.
  prefs: []
  type: TYPE_NORMAL
- en: To spare you the manual effort, a helper script has already been written to
    create all of the necessary `NetworkPolicy` objects to allow traffic between pods
    in the same project and between each project and global ones. This script must
    be run prior to carrying out the usual steps for migrating from one OpenShift
    plugin to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to download the script and make it executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run it and observe what steps are being taken to ensure the presence
    of correct network policies across projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the special treatment that global projects get: they were assigned the `pod.network.openshift.io/legacy-netid=0` label,
    which is used as a selector by NetworkPolicy objects to enable access from such
    projects. To see this for yourself, export the `allow-from-global-namespaces`
    network policy''s definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, the rest of the process is the same as in the previous subsection
    with the `networkPluginName` set to `redhat/openshift-ovs-networkpolicy`. Refer
    to the previous section for detailed instructions on how to enable an OpenShift
    plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this is out of the way, let''s remind ourselves what project we are
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, find out the new IP address of our Apache pod for future reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the same for the `demo-2` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, try pinging the pod in `demo-1` from `demo-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'And vice versa, from `demo-1` to `demo-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Astute readers may recall that `demo-2` is actually a global project, meaning
    that both ingress and egress traffic is enabled between it and any other project,
    thanks to the `allow-from-global-namespaces` network policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create another project called `demo-3` to host the same `httpd` pod
    and get the IP address of the pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Try to reach the pod in the `demo-1` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, packets didn''t come through because `demo-3` is just a regular
    project and as such it''s subject to network policy restrictions. Let''s change
    that by creating a network policy in the `demo-1` project that will allow traffic
    from `demo-3`, but before that, we will have to label the `demo-3` project so
    that the policy can refer to it using a selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `ingress` is on the same level of indentation as `podSelector`—this
    is not a typo, but an omitted pod selector, because in our example we match namespaces
    instead of pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try accessing `demo-1` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the network policy is now in effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenShift can also be configured to create a default network policy for every
    project when it''s being instantiated. The OpenShift CLI provides a command for
    bootstrapping a project template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify the template so that it contains a network policy that blocks all ingress
    traffic—this is the easiest way to see if it''s working or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the template from its YAML definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The template was created in the `demo-3` project because it's not technically
    important, but it's recommended to store it in one of the pre-existing projects,
    such as `default` or `openshift-infra`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure OpenShift to pick up the new template, make the following edit
    to the master''s configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, restart the master API service to activate the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a `new-project` and see if the network policy was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the project has been successfully instantiated with the security policy
    we configured, let''s see if the policy itself works. Like we did previously,
    we will create a pod by running Apache web server and getting its IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will switch the project to `demo-3` and see if we can reach our pod
    from there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: As expected, all incoming traffic is blocked.
  prefs: []
  type: TYPE_NORMAL
- en: On this note, we conclude the section on OpenShift SDN plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Egress routers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you have learned previously, routers in OpenShift direct ingress traffic
    from external clients to services that, in turn, forward it to pods. OpenShift
    also offers a reverse type of router intended for forwarding egress traffic from
    pods to a certain destination in the external network. But unlike ingress routers
    implemented via HAProxy, egress ones are built on Squid. Egress routers are potentially
    useful for cases such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Masking different external resources being used by several applications with
    a single global resource. For example, applications may be developed in such a
    way that they are built pulling dependencies from different mirrors, and collaboration
    between their development teams is rather loose. So, instead of getting them to
    use the same mirror, an operations team can just set up an egress router to intercept
    all traffic directed to those mirrors and redirect it to the same site.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To redirect all suspicious requests for specific sites to the audit system for
    further analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenShift supports the following types of egress router:'
  prefs: []
  type: TYPE_NORMAL
- en: '*redirect* for redirecting traffic to a certain destination IP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*http-proxy* for proxying HTTP, HTTPS, and DNS traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to limitations regarding `macvlan` interfaces in VirtualBox, an egress router
    cannot be set up in our virtual lab, nor in AWS. The best platform to use it on
    is bare-metal.
  prefs: []
  type: TYPE_NORMAL
- en: Static IPs for external project traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenShift scheduler takes all decisions regarding the placement of pods
    on nodes, taking into account factors such as the even distribution of pods, node
    affinity, and available resources. The whole point of default scheduling in OpenShift
    is to use of available resources as efficiently as possible, but it doesn't take
    into account the project pods that are created in them. The reason for this is
    that developers shouldn't be concerned with the placement of an applications'
    pods across the cluster, and that's why they have absolutely no control over where
    their pods end up. The problem starts to manifest itself in large organizations
    with multiple applications subject to different policies regarding security and
    compliance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an application handling bank account details must be subject to
    thorough audit, while its development version must have no access to production
    databases. Since the concept of projects is unknown to the scheduler, pods with
    different applications may end up on the same node, generating traffic with the
    same source IP address (the node's IP address), making it impossible to distinguish
    them from each other on the corporate firewall and to apply the appropriate policies.
    Technically, one can create a custom scheduling policy which will `pin` pods with
    specific labels to a specific node or set of nodes, which will provide a consistent
    pool of source addresses to be permitted through the firewall. However, over time,
    it will seriously skew the pods' distribution across the cluster, leading to inefficient
    use of resources and mix operations and the development teams' areas of control,
    which defeats the purpose of scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift provides a solution for exactly this kind of problem—you can assign
    an externally routable IP address to a particular project and whitelist it on
    the corporate firewall, at the same time leaving scheduling completely transparent
    to developers.
  prefs: []
  type: TYPE_NORMAL
- en: As with egress routers, the virtual environment of VirtualBox places limitations
    on the possibility of demonstrating this feature.
  prefs: []
  type: TYPE_NORMAL
- en: Egress network policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the idea behind network policies is to control access between pods across
    projects, egress network policies allow you to restrict access from all pods in
    a project to certain *external* resources. A typical use case for this feature
    would be denying pods access to source code from hosting providers and content
    mirrors to prevent any updates of applications and/or system libraries in those
    pods. It's important to understand that, unlike egress routers, egress network
    policies don't perform any redirection of traffic, working on just an *Allow versus
    Deny* basis instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what level of access pods our `demo-1` project has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Currently, there are no egress network policies being enforced in the project,
    so access to external resources is completely unrestricted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a custom egress network policy from the YAML definition, which
    is going to block all traffic to GitHub and permit traffic to all other external
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try accessing the same resources as in the beginning of this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, GitHub is now inaccessible, which is exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we implemented a *deny all but* type of security policy, but
    we can also implement a reverse type, granting access to single resources, blocking
    everything else. Continuing our example with GitHub and Google, `edit` the policy''s
    specification to resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration directs the policy to block traffic to all external
    resources, except for GitHub and `dnsmasq` on the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Again, the policy works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Note that egress rules are evaluated in the order in which they are specified
    and the first matching rule wins, meaning that, if we had placed the Deny rule
    first, traffic to GitHub would have been blocked as well, even though it's explicitly
    permitted in one of the subsequent rules.
  prefs: []
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the mechanisms for linking pods together, which has been discussed earlier
    in this book, relies on environment variables—the same as you would achieve by
    using plain Docker. When you deploy a multi-container application on OpenShift,
    pods that provide certain environment variables for pods that consume them must
    be started first, so that the variables are configured correctly by OpenShift.
    For example, if you deploy a 3-tier application consisting of a database, backend,
    and frontend, you will have to deploy the database first so that the backend pod
    picks up environment variables with the correct address and port for the database.
  prefs: []
  type: TYPE_NORMAL
- en: Pods can access each other's services directly via their IPs, but in a highly
    dynamic environment, where services may often be re-created, there is a need for
    a more stable solution. Aside from using environment variables, OpenShift provides
    its internal DNS, implemented via SkyDNS and dnsmasq for service discovery. This
    approach doesn't limit your deployment to a certain order and spares you the need
    to implement additional logic in your deployment strategy. Using OpenShift DNS,
    all applications can discover each other across the entire cluster via consistent
    names, which makes it possible for developers to rely on them when migrating to
    OpenShift. The only thing they need to do is agree with Operations on the names
    of the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'DNS in OpenShift gives pods the ability to discover the following resources
    in OpenShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Domain** |'
  prefs: []
  type: TYPE_TB
- en: '| Services | `<service>.<project>.svc.cluster.local` |'
  prefs: []
  type: TYPE_TB
- en: '| Endpoints | `<service>.<project>.endpoints.cluster.local` |'
  prefs: []
  type: TYPE_TB
- en: In the following exercise, we will see how two applications that are deployed
    in different projects can reach each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a project called `demo-1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a pod running Apache web server. We will be using the same YAML
    configuration as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to simulate the way *real* applications interact with each other,
    we will have to create a service to serve as a sole ingress point for our pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the first project is ready, let''s create another one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Like we did previously, create a pod from the same YAML definition as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'And create a service by exposing the pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s open a bash session into the newly created pod and try to reach
    the pod from the `demo-1` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of completeness, let''s switch the project to `demo-1` and try
    the same from the first pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s even possible to get all endpoints of a particular service, although
    it''s recommended to use services as points of contact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: OpenShift injects cluster-level subdomains into the local resolver's configuration
    at `/etc/resolv.conf`, so if you take a look in that file, you will find the line
    `search <project>.svc.cluster.local svc.cluster.local cluster.local`. Therefore,
    FQDNs don't have to be specified in order to reach resources across a project's
    boundaries and in the same project. For example, you can use `httpd.demo-1` to
    call a service named `httpd` in the `demo-1` project, or just `httpd` if it's
    in the same project.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, both pods can reach each other via their services, which makes
    it possible not to rely on environment variables. So, in order to migrate their
    applications to OpenShift, developers will have to configure environment variables
    of their applications to point to the DNS names of dependent services.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we provided diagrams detailing the DNS architecture
    and DNS request flow in OpenShift. Now, let's see what it looks like on a live
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command on the master to see what processes are listening
    on TCP and UDP ports ending with `53` (DNS):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The process launched from the `openshift` binary is no other than the OpenShift
    Master API, as SkyDNS is embedded into it and uses etcd as a source of authority
    to keep track of new services and to delete records for deleted ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at listening ports on our first node—the setup is completely
    the same for all nodes in a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, we can see that SkyDNS is still present on nodes,
    but there's also `dnsmasq`. The latter actually forwards DNS requests into the `cluster.local`
    and `in-addr.arpa` zones, while redirecting all others to an upstream DNS server—in
    our case, its DNS is provided by VirtualBox itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at OpenShift processes running on nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Notice that this is the same OpenShift process as listed in the output of the `ss`
    command. As with the Master API, SkyDNS is embedded into the Node process as well
    to serve DNS requests for services of applications that are deployed on OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: 'The information we''ve learned can be represented by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 - The DNS architecture in OpenShift
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, let's figure out the actual path DNS queries take before reaching their
    destinations. For that, we will take a look into various resolver and `dnsmasq`
    configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first stop is the configuration of the local DNS resolver for the `httpd`
    pod in the `demo-2` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: According to the preceding configuration, DNS queries for domains specified
    in the `search` directive are to be resolved by the DNS server available at `10.0.2.15`,
    which is the IP of one of the network interfaces dnsmasq is listening to on the
    node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look into the file specifying the DNS forwarding policy
    for internal zones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration directs dnsmasq to forward all DNS queries for domains
    `in-addr.arpa` and `cluster.local` to whatever DNS server is listening on localhost,
    which is SkyDNS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open the following file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: As opposed to the previous configuration, this directive configures `dnsmasq`
    to forward all other DNS queries to the upstream DNS, which is the DNS provided
    by VirtualBox in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have just discovered can be represented by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 - DNS query flow in OpenShift
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our exploration of OpenShift DNS.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the importance of SDN and the role it plays
    in OpenShift, the composed network topology diagram for an existing OpenShift
    cluster, gained knowledge on various OpenShift and third-party plugins, and saw
    for yourself what features they provide. You also learned about use cases of both
    egress routers and static IPs for external project traffic, and also created your
    own egress network policy to restrict access to an external resource.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be working on the deployment of a simple application.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which interface of OVS bridge is used to pass traffic from pods running on a
    particular node to and from pods running on other nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tun0
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: br0
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: veth...
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: vxlan_sys_4789
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we have a multi-tenant environment with many applications developed
    by independent teams and outsource contractors that must be able to collaborate
    in rare cases, but for the most part must be totally isolated from each other.
    What is the simplest course of action to achieve that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the ovs-networkpolicy plugin and write custom network policies to enable
    omni-directional traffic between the projects used by those parties
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the ovs-subnet plugin
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the ovs-multitenant plugin and join and isolate projects as needed
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the plugin for a third-party solution, such as VMWare NSX-T or Tigera Calico
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What feature is best suited for whitelisting traffic coming from all pods in
    a specific project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Egress router in proxy mode
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Egress router in redirect mode
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Static IP for external traffic from the project
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom scheduling policy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom iptables rules in the  `OPENSHIFT-ADMIN-OUTPUT-RULES` chain of the `filter`
    table
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following is the correct specification of the egress network policy
    that allows access to `rubygems.org` and `launchpad.net` only, assuming that this
    is the only egress network policy in the project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`- type: Deny`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`     cidrSelector: 0.0.0.0/0`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsName: rubygems.org`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsName: launchpad.net`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsName: rubygems.org`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsName: launchpad.net`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsName: rubygems.org`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsName: launchpad.net`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Deny`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    cidrSelector: 0.0.0.0/0`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Allow`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    dnsNames:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    - launchpad.net`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`   - rubygems.org`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`- type: Deny`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`  to:`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`    cidrSelector: 0.0.0.0/0`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the correct DNS name for the service named `web` in the `dev` project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`web.dev.cluster.local`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`web.cluster.local`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`web.dev.svc.cluster.local`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`web.dev.endpoints.cluster.local`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please look at the following links for further reading relating to this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenShift DNS overview**:[ https://docs.openshift.org/latest/architecture/networking/networking.html](https://docs.openshift.org/latest/architecture/networking/networking.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-level overview of SDN plugins and network topology in OpenShift**:[ https://docs.openshift.org/latest/architecture/networking/sdn.html](https://docs.openshift.org/latest/architecture/networking/sdn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A few examples of third-party SDN plugins**:[ https://docs.openshift.org/latest/architecture/networking/network_plugins.html](https://docs.openshift.org/latest/architecture/networking/network_plugins.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing network performance for OpenShift**: [https://docs.openshift.org/latest/scaling_performance/network_optimization.html](https://docs.openshift.org/latest/scaling_performance/network_optimization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuring the SDN in OpenShift and migrating between SDN plugins**[:](https://docs.openshift.org/latest/install_config/configuring_sdn.html)[ https://docs.openshift.org/latest/install_config/configuring_sdn.html](https://docs.openshift.org/latest/install_config/configuring_sdn.html)[ ](https://docs.openshift.org/latest/install_config/configuring_sdn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX-T Container Plug-in for OpenShift, Installation and Administration Guide**:[ https://docs.vmware.com/en/VMware-NSX-T/2.1/nsxt_21_ncp_openshift.pdf](https://docs.vmware.com/en/VMware-NSX-T/2.1/nsxt_21_ncp_openshift.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Installing Red Hat OpenShift Container Platform with Contrail Networking**:[ https://www.jnpr.net/documentation/en_US/contrail4.0/topics/task/installation/install-redhat-openshift.html](https://www.jnpr.net/documentation/en_US/contrail4.0/topics/task/installation/install-redhat-openshift.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using Contiv with OpenShift**: [http://contiv.github.io/documents/openshift/index.html](http://contiv.github.io/documents/openshift/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Installing Calico on OpenShift**: [https://docs.projectcalico.org/v2.4/getting-started/openshift/installation](https://docs.projectcalico.org/v2.4/getting-started/openshift/installation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuring Nuage SDN**: [https://docs.openshift.com/container-platform/3.9/install_config/configuring_nuagesdn.html](https://docs.openshift.com/container-platform/3.9/install_config/configuring_nuagesdn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managing networking in OpenShift via CLI**:[ https://docs.openshift.org/latest/admin_guide/managing_networking.html](https://docs.openshift.org/latest/admin_guide/managing_networking.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Red Hat OpenShift Container Platform DNS deep dive**:[ https://www.redhat.com/en/blog/red-hat-openshift-container-platform-dns-deep-dive-dns-changes-red-hat-openshift-container-platform-36](https://www.redhat.com/en/blog/red-hat-openshift-container-platform-dns-deep-dive-dns-changes-red-hat-openshift-container-platform-36)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
