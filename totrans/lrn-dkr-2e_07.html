<html><head></head><body>
  
    
      <h1>Securing Docker Containers</h1>
    

    
      <p>So far, we have talked a lot about the fast-evolving Docker technology in this book. It is not a nice and neat finish if the Docker-specific security issues and solution approaches are not articulated in detail to you. Hence, this chapter is specially crafted and incorporated into this book in order to explain all about the security challenges of Docker-inspired containerization. We also wanted to throw more light on how the lingering security concerns are being addressed through a host of pioneering technologies, high-quality algorithms, enabling tools, and best practices. In this chapter, we will deal with the following crucial topics in detail:</p>

      <ul>
        <li>Are the Docker containers secure?</li>

        <li>The security features of containers</li>

        <li>The emerging security-enabling approaches</li>

        <li>The best practices for ensuring container security</li>
      </ul>
    
  

  
    
      <h2 id="sigil_toc_id_144">The security scenario in the containerization domain</h2>
    

    
      <p>Ensuring unbreakable and impenetrable security for any IT systems and business services has been one of the prime needs and the predominant challenges in the IT field for decades now. Brilliant minds can identify and exploit all kinds of security holes and flaws (some of them are being carelessly and unknowingly introduced at the system conceptualization and concretization stages). This loophole ultimately brings innumerable breaches and chaos during IT service delivery. Sometimes, systems are even becoming unavailable for consumers and clients.</p>

      <p>Security experts and engineers, on the other hand, try out every kind of trick and technique at their disposal in order to stop hackers in their evil journey. However, it has not been an outright victory so far. Here and there, there are some noteworthy intrusions from unknown sources resulting in highly disconcerting IT slowdowns and sometimes breakdowns. Organizations and governments across the globe are, therefore, investing heavily their talents, time, and treasures in security research endeavors in order to completely decimate all the security and safety-related incidents and accidents. There are many security-specific product vendors and managed security service providers aiming to minimize the irreparable and indescribable consequences of security threats and vulnerabilities on IT systems. Precisely speaking, for any existing and emerging technology, security has been the most crucial and critical aspect. The point to be noted here is that enterprise and cloud IT teams can't be carefree and complacent in fulfilling the security needs.</p>

      <p>Docker-enabled containerization represents the next logical step on the memorable and indomitable journey from physical, underutilized, closed, monolithic, and single-tenanted IT resources to supple, open, affordable, automated, shared, service-oriented, optimally utilized, and virtual IT resources. Precisely speaking, we are tending toward software-defined and containerized cloud environments in order to reap a bunch of widely articulated business, technical, and user benefits. As accentuated several times in this book, Docker containers typically comprise a filesystem, network stack, process space, and everything else needed to run an application anywhere. This means that each Docker container includes the designated application and all its dependencies to be shipped, hosted, and executed in an independent manner. This widely celebrated abstraction, however, is prone to fresh and advanced security attacks, vulnerabilities, and holes. Systems can be made inaccessible, datasets can be breached, services can be stopped, and so on.</p>

      <p>Precisely speaking, the raging Docker technology promises to drastically transform the way worldwide enterprises develop, deploy, and manage critical software applications. However, containers are no panacea. The same challenges we face while deploying and delivering an application on hybrid IT environments get replicated in containers. This chapter pinpoints the proven approaches for mitigating the containerization-induced and inspired security issues. As cloud environments are extensively containerized, the unbreakable and impenetrable containers ultimately vouch for safe, secure, and smart cloud centers. The long-term goal is to have many robust, resilient, and rewarding containers in publicly discoverable locations. There are undoubtedly pioneering tools and platforms to compose better and bigger containers out of those customizable, configurable, and compact containers through commingling and collaboration.</p>
    
  

  
    
      <h3 id="sigil_toc_id_145">The security ramifications of Docker containers</h3>
    

    
      <p>The surging popularity of Docker technology is primarily due to the fact that Docker Inc., in collaboration with other interested parties, has introduced an open and industry-strength image format for efficiently packaging, distribution, and running of software applications. However, stuffing many applications into a system opens up definite worries and vulnerabilities:</p>

      <ul>
        <li><strong>Exploiting host kernels</strong>: Containers share the same host kernel and this sharing may turn out to be a single point of failure for the containerization paradigm. A flaw in the host kernel could allow a process within a container to break out to bring down the host machine. Thus the domain of Docker security is about exploring various options toward limiting and controlling the attack surface on the kernel. Security administrators and architects have to meticulously leverage the security features of the host operating system to secure the kernel.</li>

        <li><strong>Denial-of-service (DoS) attacks</strong>: All containers ought to share kernel resources. If one container can monopolize access to certain resources including memory and processing, other containers on the host are bound to starve for computing, storage, and networking resources. Ultimately, the enigma of DoS creeps in and legitimate users would struggle for accessing the services.</li>

        <li><strong>Container breakouts</strong>: An attacker who gains access to a container should not be able to gain access to other containers or the host. By default, users are not namespaced and hence any process that breaks out of the container will have the same privileges on the host as it has in the container. That is, if a process has the root privilege, then it has the root privilege on the host machine also. This means that a user can gain the elevated and even root privileges through a bug in an application code. Then the result is simply irreparable damages. That is, we need to adhere to the least privilege: each process and container should run with the minimum set of access rights and resources.</li>

        <li><strong>Poisoned images</strong>: Docker images also can be compromised and tampered resulting in bad containers and hosts. We wrote about the methods for thoroughly cleansing and curating Docker images while depositing in image repositories. Similarly, strong access control mechanisms are in place for mitigating the poisoning of images.</li>
      </ul>

      <p>Thus, Docker images, containers, clusters, hosts, and clouds are bound to be impeded with a litany of viruses, malware, and other crucial threats. Thus, the domain of Docker security has become the most challenging area for both researchers and practitioners lately and we can expect a number of game-changing and security-enhancing algorithms, approaches, and articulations in the days ahead.</p>
    
  

  
    
      <h3 id="sigil_toc_id_146">The security facets - virtual machines versus Docker containers</h3>
    

    
      <p>Docker security is being given prime importance, considering the fact that the adoption and adaptation of Docker containers are consistently on the rise. Undoubtedly, there are a lot of works for ensuring utmost security for Docker containers and the latest releases of the Docker platform have a number of security-enabling features embedded.</p>

      <p>In this section, we are going to describe where the Docker containers stand as far as the security imbroglio is concerned. As containers are being closely examined in synchronization with <strong>Virtual Machines</strong> (<strong>VMs</strong>), we will start with a few security-related points ofÂ VMs and containers. Let's start with understanding how VMs differ from containers. Typically, VMs are heavyweight and hence bloating, whereas containers are lightweight and hence, slim and sleek. The following table captures the renowned qualities of VMs and containers:</p>

      <table class="table">
        <tbody>
          <tr>
            <td>
              <p><strong>Virtual Machines</strong></p>
            </td>

            <td>
              <p><strong>Containers</strong></p>
            </td>
          </tr>

          <tr>
            <td>
              <p>A few VMs run together on a single physical machine (low density).</p>
            </td>

            <td>
              <p>Tens of containers can run on a single physical machine or VM (high density).</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>This ensures complete isolation of VMs for security.</p>
            </td>

            <td>
              <p>This enables the isolation at the process level and provides additional isolation using features, such as namespaces and cgroups.</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Each VM has its own OS and the physical resources managed by an underlying hypervisor.</p>
            </td>

            <td>
              <p>Containers share the same kernel with their Docker host.</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>For networking, VMs can be linked to virtual or physical switches. Hypervisors have a buffer for I/O performance improvement, NIC bonding, and so on.</p>
            </td>

            <td>
              <p>Containers leverage standard IPC mechanisms, such as signals, pipes, sockets, and so on, for networking. Each container gets its own network stack.</p>
            </td>
          </tr>
        </tbody>
      </table>

      <p>The following diagram illustrates how hypervisor-based virtualization enables the realization of VMs out of a physical machine:</p>

      <div><img class="image-border" height="406" src="img/image_11_001.jpg" width="542"/>
      </div>

      <p>The following diagram vividly conveys how containerization is distinguishably deviating from hypervisor-based virtualization:</p>

      <div><img class="image-border" height="414" src="img/image_11_002.jpg" width="552"/>
      </div>

      <p class="packt_figure">The debate on the security side of VMs and containers is heating up. There are arguments and counter arguments in favor of one or the other. In the case of the virtualization paradigm, the hypervisors are the centralized and core controllers for the VMs. Any kind of access of freshly provisioned VMs needs to go through this hypervisor solution, which stands as a solid wall for any kind of unauthenticated, unauthorized, and unethical purposes. Therefore, the attack surface of a VM is smaller when compared to containers. The hypervisor has to be hacked or broken into in order to impact other VMs. This means that an attacker has to route an attack through both the VM kernel and the hypervisor before being able to touch the host kernel.</p>

      <p>In contrast to the virtualization paradigm, the containers are placed directly on top of the kernel of the host system. This lean and mean architecture gives a very much higher efficiency because it completely eliminates the emulation layer of a hypervisor and also offers a much higher container density. However, unlike the VM paradigm, the container paradigm does not have many layers, so one can easily gain access to the host and other containers if any of the containers is compromised. Therefore, the attack surface of a container is larger when compared to VMs.</p>

      <p>However, the designers of the Docker platform have given a due consideration to this security risk and designed the system to thwart most of the security risks. In the ensuing sections, we will discuss the security that is innately designed in the system, the solutions being prescribed to substantially enhance the container security, and the best practices and guidelines.</p>
    
  

  
    
      <h3 id="sigil_toc_id_147">The prominent security-fulfilment features of containers</h3>
    

    
      <p>Linux containers, especially Docker containers, have a few interesting security-fulfilling features innately.</p>

      <p class="packt_figure">As discussed, Docker uses a host of security barricades to stop breaking out. That is, if one security mechanism gets broken, other mechanisms quickly come in the way of containers being hacked. There are a few mainline zones that are to be examined when evaluating the security implications of Docker containers. As emphasized previously, Docker brings a variety of isolation capabilities to containerized applications to sharply increase their security. Most of them are made available out of the box. The policy addition, annulment, and amendment capabilities at granular level take care of the security requirements of containerization. The Docker platform allows you to do the following:</p>

      <ul>
        <li>Isolate applications from each other</li>

        <li>Isolate applications from the host</li>

        <li>Improve the security of your application by restricting its capabilities</li>

        <li>Encourage adoption of the principle of least privilege</li>
      </ul>

      <p>This open-source platform is inherently able to provide these isolations for all kinds of applications on different runtime environments such as VMs, bare metal servers, and traditional IT.</p>
    
  

  
    
      <h2 id="sigil_toc_id_148">Immutable infrastructure</h2>
    

    
      <p>When you deploy an update to your application, you should create new instances (servers and/or containers) and destroy the old ones, instead of trying to upgrade them in place. Once your application is running, <em>you don't touch it!</em> The benefits come in the form of repeatability, reduced management overhead, easier rollbacks, and so on. An <strong>immutable image</strong> is an image that contains everything it needs to run the application, so it comprises the source code. One of the principles of Docker containers is that an image is immutable. That is, once built, it is unchangeable, and if you want to make changes, you'll get a new image as a result.</p>

      <p>Docker containers are self-sufficient and hence we just have to run the container without any hassle about anything else, such as mounting volumes. This means that we can share our application with our users or partners in a more easy and transparent way. The direct consequence is that we can easily scale our system in an automated manner with tools, such as Kubernetes, that allows us to run a set of containers on a set of machines, that is, a <strong>cluster</strong>.</p>

      <p>Finally, the immutable containers are bound to collapse if someone tries to play with them and hence any kind of manipulation toward malfunctioning is nullified at the initial stage itself.</p>
    
  

  
    
      <h3 id="sigil_toc_id_149">Resource isolation</h3>
    

    
      <p>As we all know, containers are being positioned for the era of the <strong>Microservices Architecture</strong> (<strong>MSA</strong>). That is, in a single system, there can be multiple generic as well as purpose-specific services that dynamically collaborate with one another for realizing easy-to-sustain distributed applications. With the multiplicity and heterogeneity of services in a physical system on the climb, it is natural that the security complexity is bound to shoot up. Therefore, resources need to be clearly demarcated and isolated in order to escape from any kind of perilous security breaches. The widely accepted security approach is to leverage the kernel features including namespaces. The following is the explanation of namespaces and cgroups:</p>

      <ul>
        <li><strong>Namespaces</strong>: A Linux namespace wraps a set of system resources and presents them to processes within the namespace, making it look as if they are dedicated to the processes. In short, the namespace is a resource management tool that helps in isolating system resources for processes. Kernel namespaces provide the first and foremost form of isolation. Processes running in a container don't affect processes running in another container or in the host system. The network namespace ensures that each container gets its own network stack, thus restricting the access to the interfaces of other containers.</li>

        <li><strong>Cgroups</strong>: This is a Linux kernel concept that governs the isolation and usage of system resources, such as CPU and memory, for a group of processes. For example, if you have an application that is taking up a lot of CPU cycles and memory, such as a scientific computing application, you can put the application in a cgroup to limit its CPU and memory usage. It ensures that each container gets its fair share of memory, CPU, and disk I/O, and more importantly, that a single container cannot bring the system down by exhausting one of those resources.</li>
      </ul>
    
  

  
    
      <h4 id="sigil_toc_id_150">Resource accounting and control</h4>
    

    
      <p>Containers consume different physical resources in order to deliver their unique capabilities. However, the resource consumption has to be disciplined, orderly and hence, critically regulated. When there is a deviation, there is a greater possibility of invalidating the containers from performing their assigned tasks in time. For example, theÂ DoS results if the resource usage is not systematically synchronized.</p>

      <p>The Linux containers leverage cgroups to implement resource accounting and auditing to run applications in a frictionless manner. As we all know, there are multiple resources that contribute to run the containers successfully. They provide many useful metrics and ensure that each container gets its fair share of memory, CPU, and disk I/O. Further, they guarantee that a single container cannot bring the system down by exhausting any one of these resources. This feature helps you fend off some DoS attacks. This feature helps in running containers as multi-tenant citizens in cloud environments to ensure their uptime and performance. Any kind of exploitation by other containers are identified proactively and nipped in the bud so that any kind of misadventure gets avoided.</p>
    
  

  
    
      <h3 id="sigil_toc_id_151">The root privilege - impacts and best practices</h3>
    

    
      <p>The Docker Engine efficiently protects the containers from any malicious activities by leveraging the recently mentioned resource isolation and control techniques. Nonetheless, Docker exposes a few potential security threats because the Docker daemon runs with the root privilege. Here, in this section, we list out a few security risks and the best practices to mitigate them.</p>

      <p>Another important principle to adhere to is the least privilege. Each process within a container has to run with the minimal access rights and resources in order to deliver its function. The advantage here is that if a container gets compromised, the other resources and data can escape from further attacks.</p>
    
  

  
    
      <h3 id="sigil_toc_id_152">The trusted user control</h3>
    

    
      <p>Since the Docker daemon runs with the root privilege, it has the capability to mount any directory from the Docker host to the container, without limiting any access rights. That is, you can start a container, where the <code>/host</code> directory will be the <code>/</code> directory on your host, and the container will be able to alter your host filesystem without any restriction. This is just an example among a myriad of malicious uses. Considering these activities, the latter versions of Docker restricts the access to the Docker daemon through a UNIX socket. Docker can be configured to access the daemon through the REST API over HTTP if you explicitly decide to do so. However, you should ensure that it will be reachable only from a trusted network or VPN or protected with stunnel and client SSL certificates. You can also secure them with HTTPS and certificates.</p>
    
  

  
    
      <h4 id="sigil_toc_id_153">Non-root containers</h4>
    

    
      <p>As mentioned previously, the Docker containers by default run with the root privilege and so does the application that runs inside the container. This is another major concern from the security perspective because hackers can gain root access to the Docker host by hacking the application running inside the container. Docker provides a simple yet powerful solution to change the container's privilege to a non-root user and thus thwart malicious root access to the Docker host. This change to the non-root user can be accomplished using the <code>-u</code> or <code>--user</code> option of the <code>docker run</code> subcommand or the <code>USER</code> instruction in the <code>Dockerfile</code>.</p>

      <p>In this section, we will demonstrate by showing you the default root privilege of the Docker container and then continue to modify the root privilege to a non-root user using the <code>USER</code> instruction in the <code>Dockerfile</code>.</p>

      <p>First, demonstrate the default root privilege of the Docker container by running a simple <code>id</code> command in a <code>docker run</code> subcommand, as shown here:</p>

      <pre><strong>$ sudodocker run --rm ubuntu:16.04 id</strong><br/><strong>uid=0(root) gid=0(root) groups=0(root)</strong>  
</pre>

      <p>Now, let us perform the following steps:</p>

      <ol>
        <li>Craft a <code>Dockerfile</code> that creates a non-root privilege user and modify the default root user to the newly-created non-root privilege user, as shown here:</li>
      </ol>

      <pre>      ##########################################<br/>      # Dockerfile to change from root to <br/>      # non-root privilege<br/>      ###########################################<br/>      # Base image is Ubuntu<br/>      FROM ubuntu:16.04<br/>      # Add a new user "peter" with user id 7373<br/>      RUN useradd -u 7373 peter<br/>      # Change to non-root privilege<br/>      USER peter 
</pre>

      <ol start="2">
        <li>Proceed to build the Docker image using the <code>docker build</code> subcommand, as depicted here:</li>
      </ol>

      <pre>      <strong>$ sudo docker build -t nonrootimage .</strong>
</pre>

      <ol start="3">
        <li>Finally, let's verify the current user of our container using the <code>id</code> command in a <code>docker run</code> subcommand:</li>
      </ol>

      <pre><strong> $ sudo docker run --rm nonrootimage id</strong><br/><strong> uid=7373(peter) gid=7373(peter) groups=7373(peter)</strong> 
</pre>

      <p>Evidently, the container's user, group, and the groups are now changed to a non-root user.</p>

      <p>Modifying the default root privilege to a non-root privilege is a very effective way of containing malevolent penetration into the Docker host kernel.</p>

      <p>So far, we discussed the unique security-related kernel characteristics and capabilities. Most of the security holes can be closed down by understanding and applying those kernel capabilities. Security experts and exponents, having considered the faster and widespread adoption of the raging containerization idea in production environments, have brought forth a few more additional security solutions, described as follows in detail. These security methods need to be given utmost importance by developers as well as system administrators while developing, deploying, and delivering enterprise-class containers in order to nullify any kind of inside or outside security attacks.</p>
    
  

  
    
      <h3 id="sigil_toc_id_154">SELinux for container security</h3>
    

    
      <p><strong>Security-Enhanced Linux</strong> (<strong>SELinux</strong>) is a brave attempt to clean up the security holes in Linux containers and is an implementation of a <strong>Mandatory Access Control</strong> (<strong>MAC</strong>) mechanism, <strong>Multi-Level Security</strong> (<strong>MLS</strong>), and <strong>Multi-Category Security</strong> (<strong>MCS</strong>) in the Linux kernel. There is a new collaborative initiative, referred to as the sVirt project, which is being built on SELinux, and this is getting integrated with Libvirt to provide an adaptable MAC framework for VMs as well as containers. This new architecture provides a sheltered separation and safety net for containers, as it primarily prevents root processes, within the container, from interfacing and interfering with other processes running outside this container. Docker containers are automatically assigned to an SELinux context specified in the SELinux policy.</p>

      <p>SELinux always checks for all the allowed operations after the standard <strong>Discretionary Access Control</strong> (<strong>DAC</strong>) is completely checked. SELinux can establish and enforce rules on files and processes in a Linux system and on their actions based on defined policies. As per the SELinux specifications, files, including directories and devices, are referred to as objects. Similarly, processes, such as a user running a command, are being termed as subjects. Most operating systems use a DAC system that controls how subjects interact with objects and one another. Using DAC on operating systems, users can control the permissions of their own objects. For example, on a Linux OS, users can make their home directories readable, giving users and subjects a handle to steal potentially sensitive information. However, DAC alone is not a fool-proof security method and DAC access decisions are solely based on user identity and ownership. Generally, DAC simply ignores other security enabling parameters, such as the role of the user, the function, trustworthiness of the program, and the sensitivity and integrity of the data.</p>

      <p>As each user typically has the complete discretion over their files, ensuring a system-wide security policy is difficult. Further, every program run by a user simply inherits all the permissions granted to the user, and the user is free to change the access to their files. All this leads to a minimal protection against malicious software. Many system services and privileged programs run with coarse-grained privileges so that any flaw in any one of these programs can be easily exploited and extended to gain the catastrophic access to the system.</p>

      <p>As mentioned at the beginning, SELinux adds MAC to the Linux kernel. This means that the owners of an object have no control or discretion over the access to an object. The kernel enforces MAC, which is a general-purpose MAC mechanism, and it needs the ability to enforce administratively set security policies to all the processes and files in the system. These files and processes will be used to base decisions on labels containing a variety of security-centric information.</p>

      <p>MAC has the inherent capability to sufficiently protect the system. Further on, MAC ensures application security against any willful hacking and tampering. MAC also provides a strong separation of applications so that any attacked and compromised application runs separately.</p>

      <p>Next in line is MCS. It is mainly used to protect containers from other containers. That is, any affected container does not have the capability to bring down other containers in the same Docker host. MCS is based on the MLS capability and uniquely takes advantage of the last component of the SELinux label, <em>the MLS field</em>. In general, when containers are launched, the Docker daemon picks a random MCS label. The Docker daemon labels all of the content in the container with that MCS label. When the daemon launches the container process, it tells the kernel to label the processes with the same MCS label. The kernel only allows the container processes to read/write their own content as long as their MCS label matches the filesystem content's MCS label. The kernel blocks the container processes from reading/writing content that is labeled with a different MCS label. This way, a hacked container process is prevented from attacking different containers. The Docker daemon is responsible for guaranteeing that no containers use the same MCS label. The cascading of errors among containers is prohibited through the adroit usage of MCS.</p>

      <p>SELinux is not installed by default in Ubuntu 16.04, unlike, Red Hat Fedora or CentOS distribution, so install SELinux by running the <code>apt-get</code> command, as shown here:</p>

      <pre><strong>$ sudo apt-get install selinux</strong>
</pre>

      <p>Then continue to enable the SELinux mode by running the following <code>sed</code> scripts:</p>

      <pre><strong>$ sudo sed -i 's/SELINUX=.*/SELINUX=enforcing/' /etc/selinux/config</strong><br/><strong>$ sudo sed -i 's/SELINUXTYPE=.*/SELINUXTYPE=default/' \</strong><br/><strong>/etc/selinux/config</strong>  
</pre>

      <p><strong>Application Armor</strong> (<strong>AppArmor</strong>) is an effective and easy-to-use Linux application security system. AppArmor proactively protects the OS and applications from any kind of external or internal threats and prevents even unknown application flaws from being misused by any hackers. AppArmor is being made available for guaranteeing Docker containers and applications present inside the containers. Policies are turning out to be a powerful mechanism for ensuring container security. Policy formulation and the automated enforcement of policies go a long way in guaranteeing the safety of containers. AppArmor comes by default with Ubuntu 16.04, so this is always recommended to be used.</p>

      <p>On Docker versions 1.13.0 and later, the Docker binary generates this profile in TMPFS and then loads it into the kernel. On Docker versions earlier than 1.13.0, this profile is generated in <code>/etc/apparmor.d/docker</code> instead.</p>

      <p>The <code>docker-default</code> profile is the default one for running containers. It is moderately protective while providing wide application compatibility. When you run a container, it uses the <code>docker-default</code> policy unless you override it with the <code>security-opt</code> option. For example, the following explicitly specifies the default policy:</p>

      <pre><strong>$ docker run --rm -it </strong><strong>--security-opt \<br/> apparmor=docker-default hello-world</strong>
</pre>

      <p><strong>Secure computing mode</strong> (<strong>seccomp</strong>) is supported by the Docker Engine, a security feature made available in the Linux kernel. This allows the administrator to restrict the actions available within a container down to the granularity of a single system call. This capability greatly restricts the access that an application container has to the host system to perform actions. Enterprises can configure seccomp profiles accordingly and apply them to the Docker environment.</p>

      <p>The default seccomp profile provides a sane default for running containers with seccomp and disables around 44 system calls out of over 300. It is moderately protective while providing wide application compatibility.</p>

      <p>The vast majority of applications will be able to operate without any issue with the default profile. In fact, the default profile has been able to proactively protect Dockerized applications from several previously unknown bugs.</p>

      <p>This is enabled by default on Ubuntu 16.04:</p>

      <pre><strong>$ cat /boot/config-`uname -r` | grep CONFIG_SECCOMP= </strong><strong>CONFIG_SECCOMP=y</strong>
</pre>

      <p><strong>SCONE: Secure Linux Containers with Intel SGX</strong>, is described by Sergei Arnautov and his team as a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design objectives of SCONE are fixed as follows:</p>

      <ul>
        <li>Firstly, it attains small <strong>Trusted Computing Base</strong> (<strong>TCB</strong>)</li>

        <li>Secondly, it has to have a low-performance overhead</li>
      </ul>

      <p>SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data to significantly reduce the performance impact of thread synchronization and system calls within SGX enclaves. SCONE supports user-level threading and asynchronous system calls. As per their research paper, the evaluation of SCONE is greatly appreciated by Docker fans.</p>
    
  

  
    
      <h4 id="sigil_toc_id_155">Loading the Docker images and the security implications</h4>
    

    
      <p>Docker typically pulls images from the network, which are usually curated and verified at the source. However, for the purpose of backup and restore, the Docker images can be saved using the <code>docker save</code> subcommand and loaded back using the <code>docker load</code> subcommand. This mechanism can also be used to load third-party images through unconventional means. Unfortunately, in such a practice, the Docker Engine cannot verify the source and, hence, the images can carry malicious code. So, as the first shield of safety, Docker extracts the image in a <em>chrooted</em> subprocess for privilege separation. Even though Docker ensures the privilege separation, it is not recommended to load arbitrary images.</p>

      <p><strong>Using container scanning to secure Docker deployments</strong>: <strong>Docker Content Trust</strong> (<strong>DCT</strong>) gives publishers an easy and expedited way to guarantee the authenticity of containers that are getting published in web-scale repositories such as Docker Hub. However, organizations need to take pragmatic measures to access, assess, and act accordingly for ensuring the security of their containerized applications throughout their complete life cycle. Precisely speaking, DCT is a means by which you can securely sign your Docker images that you have created to ensure that they are from who they say they are from.</p>

      <p><strong>Managing container security with Black Duck Hub</strong>: Black Duck Hub is a vital tool for managing the security of application containers throughout the full application life cycle. Black Duck Hub allows organizations to identify and track vulnerable open-source applications and components within their environment. Assessments draw on Black Duck's KnowledgeBase, which contains information on 1.1 million open-source projects and detailed data on more than 100,000 known open-source vulnerabilities across more than 350 billion lines of code. Through a partnership with Red Hat, Black Duck's ability to identify and inventory open source and proprietary code production environments is now being applied to containerized environments. Red Hat has launched <strong>Deep Container Inspection</strong> (<strong>DCI</strong>), an enterprise-focused offering that wraps container certification, policy and trust into an overall architecture for deploying and managing application containers. As part of DCI, Red Hat is partnering with Black Duck to give organizations a means of validating the contents of a container before, during, and after deployment.</p>

      <p>Integration of Black Duck Hub's vulnerability scanning and mapping capabilities enables OpenShift customers to consume, develop, and run containerized applications with increased confidence and security, knowing that these applications contain code that has been independently validated and certified. The integration also provides a means to track the impact of newly disclosed vulnerabilities or changes related to container aging that may impact on security and risk. Black Duck Hub's application vulnerability scanning and mapping capability give Docker customers the ability to identify vulnerabilities both before and after deployment and spot issues that arise as containerized applications age or become exposed to new security vulnerabilities and attacks.</p>
    
  

  
    
      <h3 id="sigil_toc_id_156">Image signing and verification using TUF</h3>
    

    
      <p>The Docker community expects to have a strong cryptographic guarantee regarding the code and versions of the Dockerized software. DCT is the new security-related feature associated with the 1.8 version of the Docker platform. DCT intrinsically integrates <strong>The Update Framework</strong> (<strong>TUF</strong>) into Docker using Notary, an open source tool that provides trust over any content.</p>

      <p>TUF helps developers to secure new or existing software update systems, which are often found to be vulnerable to many known attacks. TUF addresses this widespread problem by providing a comprehensive and flexible security framework that developers can integrate with any software update system. A software update system is an application running on a client system that obtains and installs software. This can include updates to software that is already installed or even completely new software.</p>

      <p><strong>Protection against image forgery</strong>: Once trust is established, DCT provides the ability to withstand a malicious actor with a privileged network position also known as a <strong>Man-in-the-Middle</strong> (<strong>MitM</strong>) attack.</p>

      <p><strong>Protection against replay attacks</strong>: In the typical replay attacks, previously valid payloads are replayed to trick another system. In the case of software update systems, old versions of signed software can be presented as the most recent ones. If a user is fooled into installing an older version of a particular software, the malicious actor can make use of the known security vulnerabilities to compromise the user<strong>'</strong>s host. DCT uses the timestamp key when publishing the image, providing protection against replay attacks. This ensures that what the user receives is the most recent one.</p>

      <p><strong>Protection against key compromise</strong>: If a key is compromised, you can utilize that offline key to perform a key rotation. That key rotation can only be done by the one with the offline key. In this scenario, you will need to create a new key and sign it with your offline key.</p>

      <p>Other security-enhancing projects include the following:</p>

      <ul>
        <li><strong>Clair</strong>: This is an open-source project for the static analysis of vulnerabilities in application Docker containers (<a href="https://github.com/coreos/clair">https://github.com/coreos/clair</a>). It audits the Docker image locally and also checks vulnerability in container registry integration. Finally, during the first run, Clair will bootstrap its database with vulnerability data from its data sources.</li>

        <li><strong>Notary</strong>: The Docker Notary project is a framework that allows anyone to securely publish and access content (for example, Docker images) over a potentially insecure network. Notary allows a user to digitally sign and verify content.</li>

        <li><strong>Project Nautilus</strong>: Nautilus is Docker's image scanning capability, which can examine images in Docker Hub to help vulnerabilities that may exist in Docker containers. Today, Nautilus only works with Docker Hub. It does not support private or on-premises registries.</li>

        <li><strong>AuthZ Plugins</strong>: The native Docker access control is all or nothingâyou either have access to all Docker resources or none. The AuthZ framework is Twistlock's contribution to the Docker code base. AuthZ allows anyone to write an authorization plugin for Docker to provide fine-grained access control to Docker resources.</li>

        <li><strong>Docker Trusted Registry</strong> (<strong>DTR</strong>): This is Docker's enterprise version of Docker Hub. You can run DTR on-premises or in your virtual private cloud to support security or compliance requirements. Docker Hub is open source, whereas DTR is a subscription-based product sold by Docker. Communications with the registries use TLS, to ensure both confidentiality and content integrity. By default, the use of certificates trusted by the public PKI infrastructure is mandatory, but Docker allows the addition of a company internal CA root certificate to the trust store.</li>
      </ul>
    
  

  
    
      <h3 id="sigil_toc_id_157">The emerging security approaches</h3>
    

    
      <p>As we all know, the Docker platform makes it easy for developers to update and control the data and software in containers. Similarly, Docker enables efficiently ensuring all the components that make an application are current and consistent at all times. Docker also innately delivers logical segregation of applications running on the same physical host. This celebrated isolation perfectly promotes fine-grained and efficient enforcement of security policies. However, as in the traditional environment, data at rest is susceptible to various attacks ceaselessly from cyber and internal attackers. There are other negative opportunities and possibilities for Docker environments to be subjected to heavy bombardment. Consequently, there is an insistence for proper safeguards to be in place. The faster and easier proliferation of containers and data can significantly expand the number and types of threats targeting containerized clouds.</p>

      <p>About Vormetric transparent encryption<br/>

        Organizations can establish strong controls around their sensitive data in Docker implementations in an efficient manner. This solution enables data-at-rest encryption, privileged user access control, and the collection of security intelligence logs for structured databases and unstructured files. With these capabilities, organizations can establish persistent, strong controls around their stored Docker images and protect all data generated by Docker containers when the data is being written to the Docker host storage on an NFS mount or a local folder.
      </p>
    
  

  
    
      <h2 id="sigil_toc_id_158">The best practices for container security</h2>
    

    
      <p>There are robust and resilient security solutions to boost the confidence of providers as well as users toward embracing the containerization journey with clarity and alacrity. In this section, we provide a number of tips, best practices, and key guidelines collected from different sources in order to enable security administrators and consultants to tightly secure Docker containers. At the bottom line, if containers are running in a multi-tenant system and you are not using the proven security practices, then there are definite dangers lurking around the security front.</p>

      <p>The first and foremost advice is, don't run random and untested Docker images on your system. Strategize and leverage trusted repositories of Docker images and containers to subscribe and use applications and data containers for application development, packaging, shipping, deployment, and delivery. It is clear from past experiences that any untrusted containers that are downloaded from the public domain may result in malevolent and messy situations. Linux distributions, such as <strong>Red Hat Enterprise Linux</strong> (<strong>RHEL</strong>), have the following mechanisms in place in order to assist administrators to ensure the utmost security.</p>

      <p>The best practices widely recommended by Docker experts (Daniel Walsh Consulting Engineer, Red Hat) are as follows:</p>

      <ul>
        <li>Only run container images from trusted parties</li>

        <li>Container applications should drop privileges or run without privileges whenever possible</li>

        <li>Make sure the kernel is always updated with the latest security fixes; the security kernel is critical</li>

        <li>Make sure you have support teams watching for security flaws in the kernel</li>

        <li>Use a good quality supported host system for running the containers, with regular security updates</li>

        <li>Do not disable security features of the host operating system</li>

        <li>Examine your container images for security flaws and make sure the provider fixes them in a timely manner</li>
      </ul>

      <p>As mentioned previously, the biggest problem is that everything in Linux is not namespaced. Currently, Docker uses five namespaces to alter the process's view of any system: process, network, mount, hostname, and shared memory. While these give the users some level of security, it is by no means a comprehensive one such as KVM. In a KVM environment, processes in a VM do not talk to the host kernel directly. They do not have any access to kernel filesystems. Device nodes can talk to the VMs kernel, not the hosts. Therefore, in order to have a privilege escalation out of a VM, the process has to subvert the VM's kernel, find an enabling vulnerability in the hypervisor, break through SELinux controls (sVirt), and attack the host's kernel. In the container landscape, the approach is to protect the host from the processes within the container and to protect containers from other containers. It is all about combining or clustering together multiple security controls to defend containers and their contents.</p>

      <p>Basically, we want to put in as many security barriers as possible to prevent any sort of break out. If a privileged process can break out of one containment mechanism, the idea is to block them with the next barrier in the hierarchy. With Docker, it is possible to take advantage of as many security mechanisms of Linux as possible. The following are the possible security measures that can be taken:</p>

      <ul>
        <li><strong>Filesystem protection</strong>: Filesystems need to be read-only in order to escape from any kind of unauthorized writing. That is, privileged container processes cannot write to them and do not affect the host system too. Generally, most of the applications need not write anything to their filesystems. There are several Linux distributions with read-only filesystems. It is, therefore, possible to block the ability of the privileged container processes from remounting filesystems as read and write. It is all about blocking the ability to mount any filesystems within the container.</li>

        <li><strong>Copy-on-write filesystems</strong>: Docker has been using theÂ <strong>Advanced Multi-Layered Unification Filesystem</strong> (<strong>AUFS</strong>) as a filesystem for containers. AUFS is a layered filesystem that can transparently overlay one or more existing filesystems. When a process needs to modify a file, AUFS first creates a copy of that file and is capable of merging multiple layers into a single representation of a filesystem. This process is called copy-on-write, and this prevents one container from seeing the changes of another container even if they write to the same filesystem image. One container cannot change the image content to affect the processes in another container.</li>

        <li><strong>The choice of capabilities</strong>: Typically, there are two ways to perform permission checks: privileged processes and unprivileged processes. Privileged processes bypass all sorts of kernel permission checks, while unprivileged processes are subject to the full permission checking based on the process's credentials. The recent Linux kernel divides the privileges traditionally associated with the superuser into distinct units known as <strong>capabilities</strong>, which can be independently enabled and disabled. Capabilities are a per-thread attribute. Removing capabilities can bring forth several positive changes in Docker containers. Invariably, capabilities decide the Docker functionality, accessibility, usability, security, and so on. Therefore, it needs a deeper thinking while embarking on the journey of adding as well as removing capabilities.</li>

        <li><strong>Keeping systems and data secure</strong>: Some security issues need to be addressed before enterprises and service providers use containers in production environments. Containerization will eventually make it easier to secure applications for the following three reasons:</li>

        <li style="list-style: none; display: inline"><ul>
          <li>A smaller payload reduces the surface area for security flaws</li>

          <li>Instead of incrementally patching the operating system, you can update it</li>

          <li>By allowing a clear separation of concerns, containers help IT and application teams collaborate purposefully</li>
        </ul></li>
      </ul>

      <p style="padding-left: 60px">The IT department is responsible for security flaws associated with the infrastructure. The application team fixes flaws inside the container and is also responsible for runtime dependencies. Easing the tension between IT and applications development teams helps smooth the transition to a hybrid cloud model. The responsibilities of each team are clearly demarcated in order to secure both containers and their runtime infrastructures. With such a clear segregation, proactively identifying any visible and invisible endangering security ordeals and promptly eliminating time, policy engineering and enforcement, precise and perfect configuration, leveraging appropriate security-unearthing and mitigation tools, and so on, are being systematically accomplished.</p>

      <ul>
        <li><strong>Leveraging Linux kernel capabilities</strong>: An average server (bare metal or VM) needs to run a bunch of processes as root. These typically include <code>ssh</code>, <code>cron</code>, <code>syslogd</code>, hardware management tools (for example, load modules), and network configuration tools (for example, handling DHCP, WPA, or VPNs). A container is very different because almost all of these tasks are being handled by the infrastructures on which the containers are to be hosted and run. There are several best practices, key guidelines, technical know-how, and so on in various blogs authored by security experts. You can find some of the most interesting and inspiring security-related details at <a href="https://docs.docker.com/">https://docs.docker.com/</a>.</li>
      </ul>
    
  

  
    
      <h3 id="sigil_toc_id_159">Secure deployment guidelines for Docker containers</h3>
    

    
      <p>Docker containers are increasingly hosted in production environments to be publicly discovered and used by many. Especially, with the faster adoption of cloud technologies, the IT environments of worldwide organizations and institutions are getting methodically optimized and transformed to deftly and decisively host a wider variety of VMs and containers. There are new improvements and enablements, such as Flocker and Clocker, in order to speed up the process of taking containers to cloud environments (private, public, hybrid, and community). There are recommendations that have to be followed while deploying containers. As we all know, containers remarkably reduce the overhead by allowing developers and system administrators to seamlessly deploy containers for applications and services required for business operations. However, because Docker leverages the same kernel as the host system to reduce the need for resources, containers can be exposed to significant security risks if not adequately configured. There are a few carefully annotated guidelines to be strictly followed by both developers and system administrators while deploying containers. For example, <a href="https://github.com/GDSSecurity/Docker-Secure-Deployment-Guidelines">https://github.com/GDSSecurity/Docker-Secure-Deployment-Guidelines</a> elaborates in a tabular form with all the right details.</p>

      <p>An indisputable truth is that the software flaws in distributed and complex applications open the way for intelligent attackers and hackers to break into systems that host critical, confidential, and customer data. Therefore, security solutions are being insisted and ingrained across all the layers in the IT stack, and hence, there arise many types of security vulnerabilities at different levels and layers. For example, the perimeter security that solves only part of the problem because the changing requirements are mandated for allowing network access to employees, customers, and partners. Similarly, there are firewalls, intrusion detection and prevention systems, <strong>Application Delivery Controllers</strong> (<strong>ADCs</strong>), access controls, multifactor authentication and authorization, patching, and so on. Then, for securing data while in transit, persistence, and being used by applications, there are encryption, steganography, and hybrid security models. All these are reactive and realistic mechanisms, but the increasing tendency is all about virtual businesses insisting on proactive and preemptive security methods. As IT is tending and trending toward the much anticipated virtual IT, the security issues and implications are being given extra importance by security experts.</p>
    
  

  
    
      <h3 id="sigil_toc_id_160">The future of Docker security</h3>
    

    
      <p>There will be many noteworthy improvisations, transformations, and disruptions in the containerization space in the near future. Through a host of innovations and integrations, the Docker platform is being positioned as the leading one for strengthening the containerization journey. The following are the prime accomplishments through the smart leverage of the Docker technology:</p>

      <ul>
        <li><strong>Strengthening the distributed paradigm</strong>: While computing is going to be increasingly distributed and federated, the MSA plays a very decisive and deeper role in IT. Docker containers are emerging as the most efficient ones for hosting and delivering a growing array of microservices. With container orchestration technologies and tools gaining greater recognition, microservices (specific as well as generic) get identified, matched, orchestrated, and choreographed to form business-aware composite services.</li>

        <li><strong>Empowering the cloud paradigm</strong>: The cloud idea is strongly gripping the IT world to bring in the much-insisted IT infrastructure rationalization, simplification, standardization, automation, and optimization. The abstraction and virtualization concepts, the key ones for the unprecedented success of the cloud paradigm, are penetrating into every kind of IT module. Originally, it started with server virtualization and now it is all about storage and networking virtualization. With all the technological advancements around us, there is a widespread keenness to realize software-defined infrastructures (software-defined compute, storage, and networking). The Docker Engine, the core and critical portion of the Docker platform, is duly solidified in order to bring in the necessary eligibility for containers to run on software-defined environments without any hitch or hurdle.</li>

        <li><strong>Enabling IT elasticity, portability, agility, and adaptability</strong>: Containers are emerging as the flexible and futuristic IT building blocks for bringing in more resiliency, versatility, elegance, and suppleness. Faster provisioning of IT resources for ensuring higher availability and real-time scalability, the easy elimination of all kinds of frictions between the development and operation teams, the guarantee of native performance of IT, the realization of organized and optimized IT for enhanced IT productivity, and so on, are some of the exemplary things being visualized for Docker containers toward the smarter IT.</li>
      </ul>

      <p>Containers will be a strategic addition to VMs and bare metal servers in order to bring in deeper IT automation, acceleration, and augmentation, thereby the much-hyped and hoped for business agility, autonomy, and affordability will be achieved.</p>
    
  

  
    
      <h2 id="sigil_toc_id_161">Summary</h2>
    

    
      <p>Security is definitely a challenge and an important aspect not to be sidestepped. If a container gets compromised, then bringing down the container host is not a difficult task. Thus, ensuring security for containers and then hosts is indispensable for the flourishing of the containerization concept, especially when the centralization and federation of IT systems are on the climb. In this chapter, we specifically focused on the sickening and devastating security issues on Docker containers and explained the ways and means of having foolproof security solutions for containers that host dynamic, enterprise-class, and mission-critical applications. In the days to unfurl, there will be fresh security approaches and solutions in order to guarantee impenetrable and unbreakable security for Docker containers and hosts, as the security of containers and their contents is of the utmost importance for service providers as well as consumers.</p>
    
  
</body></html>