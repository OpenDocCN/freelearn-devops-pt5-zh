<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Big Data and Data Sciences</h1>
                </header>
            
            <article>
                
<p>We will discuss the key technology concepts such as In-memory computing and NoSQL databases that are the building blocks of the big data subject, along with concepts of data visualization and data sciences. These concepts are enriching for novice readers and will be appreciated while building big data applications in future sections. Readers familiar with these concepts can skip this section if they wish to. In this chapter we will cover the following topics:</p>
<ul>
<li>Big data attributes</li>
<li>In-memory concepts</li>
<li>NoSQL databases</li>
<li>Data visualization</li>
<li>Data science</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Big data </h1>
                </header>
            
            <article>
                
<p>Big data has many interpretations and definitions across industries, academics, organizations, and individuals. It's a very broad and evolving field, and many organizations are adopting big data in some <span>shape</span><span> </span><span>or</span><span> </span><span>form to supplement their existing analysis and business tools. Big data systems are primarily used to derive meaningful value and hidden patterns from data. They are also implemented to supplement different types of traditional workloads for economies of scale that lower costs. The three key sources of big data are people, organizations, and sensors.</span></p>
<p>Big data systems are characterized by a few attributes such as volume, velocity, a variety of data, and value; additional characteristics are veracity, validity, volatility, and visualization:</p>
<ul>
<li><strong>Value</strong>:<strong> </strong>The ultimate objective of big data is to generate some business value and purpose for the company by doing all the analysis with a big data project.</li>
<li><strong>Volume of data</strong>:<strong> </strong>Big data system volumes can scale as per business needs to gigabytes, terabytes, petabytes, exabytes, zettabytes, and so on. Each business has unique volume needs; for example, an <strong>Enterprise Resource Planning</strong> (<span><strong>ERP</strong>) </span>system could run into gigabytes of data, while <strong>Internet of Things</strong> (<strong>IoT</strong>) and machine sensor data could run into petabytes.</li>
<li><strong>Velocity of data</strong>:<strong> </strong>The speed at which the data is accessed could be batch jobs, periodic, near-real-time, real-time data from web server logs, streaming data of live videos and multimedia, IoT sensor information, weather forecasts, and so on. We can correlate the quantity of SMS messages, Facebook status updates, or credit card swipes being sent every minute of every day by an organization.</li>
<li><strong>Variety of data</strong>:<strong> </strong>Variety of data is one of the key ingredients of big data. The data can be of many forms, such as a structured data format similar to a sales invoice statement date, sales amount, store ID, store address, and so on, which can easily fit into traditional RDBMS systems; semi-structured data, such as web or server logs, machine sensor data, and mobile device data; unstructured data such as social media data, including Twitter feeds and Facebook data, photos, audio, video, <span>MRI images, </span><span>and so on. </span></li>
</ul>
<p style="padding-left: 60px">Structured data has a form and rules for a metadata model, and dates follow a specific pattern. However, unstructured and semi-structured data have no predefined metadata model rules. One of the goals of big data is to gather business meaning from unstructured data with technology.</p>
<ul>
<li><strong>Veracity of data</strong>:<strong> </strong>This is the trustworthiness of data; it should be devoid of bias and abnormalities. It's ensuring the quality and accuracy of data gathered from different source systems, and doing preprocessing quality checks to keep data clean and ensure no dirty data accumulates.</li>
<li><strong>Validity of data</strong>:<strong> </strong>Data should be correct and valid for its intended use, ensuring its appropriateness. Even in traditional data analytics, data validity is crucial for the program's success.</li>
<li><strong>Volatility of data</strong>: This is the shelf life of the data, its validity for the time period of intended use. Stale data will not be able to generate intended results for any project or program.</li>
<li><strong>Visualization</strong>:<strong> </strong>Visualization through pictures appeals to the human eye more than raw data in metric or Excel format. Being able to visualize the data trends and patterns from the input data systems or streams till the end analysis is an asset to big data programs.</li>
</ul>
<p style="padding-left: 60px">In the traditional data warehouse, the data is structured, like RDBMS data, and the schema is modeled for the data to be loaded into the database.</p>
<p style="padding-left: 60px">Data handling in big data systems from ingestion to persistence, computation, analytics, and so on is quite different from traditional data warehouse systems as data volumes, velocities, and formats are quite divergent from source system ingestion to persistence. These systems require high availability and scalability for staging to persistence and analytics.</p>
<p style="padding-left: 60px">Scalability is achieved through cluster-resource pooling of memory, computation, and disk space. New machines can be added to the cluster to supplement the resource needs as per varying workload demands, or increased data volumes with business expansion. High availability is quite important for critical systems performing real-time analytics, production systems, or staging and edge systems holding real-time data. High availability clusters mean ensuring fault-tolerant systems even in the event of hardware or software failures, ensuring uninterrupted access to data and systems.</p>
<p style="padding-left: 60px">We will be discussing building Hadoop-based clusters as support in <span><a href="bb8a06a5-6188-45c9-9a08-747f299a09bb.xhtml" target="_blank">Chapter 4</a>, </span><em>Big Data Hadoop Ecosystems</em>, including the various industry tools available.</p>
<p style="padding-left: 60px">Another prominent big data technology is In-memory computing, which encompasses both software and hardware technology advancements to handle the huge data loads of volumes, velocity, and variety in big data systems. We will discuss these in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">In-memory technology</h1>
                </header>
            
            <article>
                
<p>In traditional application development, the disk was the main persistence for data storage. The challenge in this method was that, for business logic and application computation, data was transferred from storage disk to main memory, causing huge I/O overhead. Again, after the computations based on the business logic, the data from aggregation, computational, or analytic results was transferred from CPU and main memory to store, or <span>the data was </span><span>persisted back to storage disk, causing I/O overhead multiple times.</span></p>
<p><span>As the following simple illustration shows, disk speed is growing slower compared to other hardware components, while the need for higher performance and speed is increasing day by day:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-522 image-border" height="292" src="assets/29f275d4-cc85-4ff0-b81c-6ac988a68832.png" width="627"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">In-memory database (IMDB)</h1>
                </header>
            
            <article>
                
<p>With In-memory technology, in contrast to traditional disk-based data persistence methods, the complete application requires the data is loaded into the main memory of the system. That makes the applications perform 10 to 20 times faster.</p>
<ul>
<li>Data resides permanently in the main memory</li>
<li>The main memory is the primary <em>persistence</em> of data storage</li>
<li>The disk serves as persistent storage for logging and recovery from the disk</li>
<li>Main memory access is crucial to performance</li>
<li>Algorithms/data structures relying on the cache are more efficient in response</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><strong> <img class="alignnone size-full wp-image-523 image-border" height="414" src="assets/68ab94c7-ef84-4967-bd60-f03cdce53975.png" width="500"/></strong></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hardware technology advances adopted for In-memory systems</h1>
                </header>
            
            <article>
                
<p>Many hardware advancements have been integrated into modern In-memory computing architecture, such as multi-core architecture and massive parallel scaling.</p>
<p>The In-memory database is optimized to use the capabilities of multi-core processors to enable incredibly fast queries. Processor speed is no longer dependent on clock speed but rather on the degree of system parallelism. Modern server boards have many CPUs with several cores each.</p>
<p>Parallelism can be achieved at different levels, such as from the application level to query execution on the database level. Multi-threaded application processing is handled by mapping each of the queries to a single core, and hence multiple queries are distributed to multiple cores in parallel. Query processing also involves data processing (that is, the database needs to be queried in parallel). In-memory systems distribute the workload across multiple cores of a single system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software  technology advances adopted for In-memory systems</h1>
                </header>
            
            <article>
                
<p>Software technology advancements have contributed to the development of <strong>In-memory Database</strong>  (<strong>IMDB</strong>) engines; they are listed as follows:</p>
<ul>
<li>Data compression</li>
<li>No aggregate tables</li>
<li>Insert-only tables</li>
<li>Column, row, and hybrid storage</li>
<li>Partitioning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data compression</h1>
                </header>
            
            <article>
                
<p>In-memory technology enables data compression techniques to achieve data compression of up to 20 times. There are multiple algorithms such as bitmaps, run length encoding, dictionary encoding, prefix/suffix encoding, cluster encoding, relative encoding, delta encoding, and indirect encoding. The In-memory engine will apply the most appropriate algorithm or combination of algorithms to achieve the optimum compression ratio and performance.</p>
<p class="mce-root">The objectives of data compression are to reduce the volume of data transfer back and forth from the system as quickly as possible. To accomplish this, use the appropriate algorithms and techniques to minimize the overhead associated with the compression and uncompression of data. Improving overall query performance is also an objective of effective data compression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">No aggregate tables</h1>
                </header>
            
            <article>
                
<p>An In-memory database eliminates the need to maintain expensive pre-aggregate tables, since the data resides in the system memory computations and aggregations are on the fly, since there is no transfer of data back and forth. This eliminates the overhead of maintaining materialized aggregate views, which we can generate in real time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Insert-only tables</h1>
                </header>
            
            <article>
                
<p>In a typical database, deleted records are <span>performance and </span><span>overhead </span><span>intensive</span>. In-memory database instead of deleting a record its marked as obsolete record like in version control system, it not used for computation. So, the overhead associated with deleting a record and re-indexing the rest of records which are performance intensive are eliminated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Column, row, and hybrid storage</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Online Transaction Processing</strong><span> (</span><strong>OLTP</strong><span>) applications are row-oriented storage, wherein table data  is stored as a sequence of records.</span> Row-based tables and storage are more efficient in OLTP applications for the following reasons:</p>
<ul>
<li>Only a single record is processed by the application at any given time</li>
<li>A single record can be subjected to many selects and/or updates at any point in time </li>
<li>A complete record (or row) is typically accessed by the application</li>
<li>Since the column values are many distinct values, compression rates are low </li>
<li>In OLTP applications, aggregations or fast searching is required</li>
<li>The configuration tables have a small number of rows</li>
</ul>
<p>In <strong>Online Analytical Processing</strong> <span>(<strong>OLAP</strong>) </span>systems such as data warehouses, column storage is used, where aggregate functions play an important role; the entries of a column are stored in contiguous memory locations so the aggregations are quick and efficient in the column store. Column storage or tables are beneficial in the following situations:</p>
<ul>
<li>Calculations are typically executed on a single, or a few, columns only</li>
<li>Based on values of a few columns, the table search is performed</li>
<li>There are a large number of columns in the table </li>
<li>Columnar intensive operations, such as aggregate, scan, and so on, are required to be performed on the table rows</li>
<li>Since the majority of the columns contain only a few distinct values (compared to the number of rows in the table), high compression rates can be achieved  </li>
</ul>
<p>IMDB technology offers higher efficiency due to the usage of hybrid storage. It uses algorithms for selecting the appropriate combination of both row and columnar storage to gain maximum performance efficiency. It also provides users a choice to customize the storage options, such as select or alter columnar or table-wise storage for any specific table. In a few circumstances, using column stores in OLTP applications maximizes efficiency; it requires a balanced and well-understood approach to insertion and indexing column data storage for transaction systems. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partitioning</h1>
                </header>
            
            <article>
                
<p>Data partitioning is done for enhanced performance, easy management, convenient backup and recovery, and so on. There are several types of data partition techniques.</p>
<p>Partitioning based on some common affinity involves, for example,  grouping of data based on equal time zone segments such as months, weeks, years, and partitioning by the size of tables, dimensions, functions, and so on.</p>
<p>Examples are tables preceding 1M records grouped together, tables fetching payroll data grouped together, and so on. </p>
<p>Various techniques are used in data partitioning strategies, such as group partitioning, horizontal partitioning, vertical partitioning, mixed partitioning, and so on.</p>
<p>In-memory database systems are listed, as follows:</p>
<ul>
<li>Apache Spark</li>
<li>Pivot GemFire</li>
<li>eXtremeDB</li>
<li>SAP HANA</li>
<li>IBM SolidDB</li>
<li>MSSQL server</li>
<li>Oracle TimesTen</li>
<li>MemSQL</li>
<li>VoltDB</li>
</ul>
<p>In-memory technologies enabled to do business intelligence and visualization tools <span>are listed, as follows</span><span>:</span></p>
<ul>
<li>Microstrategy</li>
<li>Tableau</li>
<li>QlikView</li>
<li>PowerBI</li>
<li>TIBCO Spotfire</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NoSQL databases</h1>
                </header>
            
            <article>
                
<p>Traditional RDBMS are popular for their structured data, pre-configured schema, and rigid data consistency for transactional enterprise applications, and are characterized by the following attributes:</p>
<ul>
<li>Supporting centralized applications, such as ERP systems, which consolidate enterprise data</li>
<li>Application availability could range from moderate to high availability</li>
<li>The data velocity supported applications is moderate</li>
<li>Typically, data input is limited to a few source systems</li>
<li>The data they handle is primarily structured in nature</li>
<li>The databases support complex and nested transactions</li>
<li>The primary expectation is to scale up to support the read operations for multiple concurrent users</li>
<li>Support moderate data volumes with cache and purge features</li>
</ul>
<p>Most modern day applications are based on NoSQL databases to create flexible data schema, schema on read, or no schema to design web and cloud-based systems effectively. The key requirements are the ability to process very large volumes of data and quickly distribute that data across computing clusters to enable fast changes to applications that are continually updated. Traditional RDBMS systems are unable to cater to large-scale database clustering in cloud and web applications.</p>
<p>NoSQL database systems are designed for the following scenarios:</p>
<ul>
<li>Support decentralized applications which are spread across multiple locations such as web applications, mobile applications, IoT, and so on</li>
<li>The applications are continuously available and can't afford downtime</li>
<li>They support high-velocity data, which could be from devices, sensors, and so on</li>
<li>The data ingested is not confined to a single location and could range to multiple locations</li>
<li>Data forms are structured, semi-structured, and unstructured</li>
<li>The transaction types are mostly simple; however, they maintain high data volumes and retain data for a very long time</li>
<li>Scalability for systems with intensive write and read operations and data volumes</li>
<li>Concurrency support for number of users</li>
</ul>
<p>There are four types of NoSQL databases, with specific purposes:</p>
<ul>
<li><strong>Key-value database</strong>: It is also called a <strong>key-value store</strong>. It stores data without the schema. The name derives from the fact that data is stored as an indexed or unique key and associated value. They are highly scalable and provide session management and caching in web applications, high performance, and schema-less design. This type of database is popular, and examples include Cassandra, DyanmoDB, <strong>Azure Table Storage</strong> (<strong>ATS</strong>), Riak, BerkeleyDB, Aerospike, and MemchacheDB.</li>
<li><strong>Document databases:</strong>  These databases store semi-structured data such as document descriptions and information. Each document is assigned a unique key, which is used to retrieve the document data. An advantage is data records can be created and updated for storing, retrieving, and managing document based on unique key. For web-based applications, the data exchange is through JavaScript and <strong>JavaScript Object Notation</strong> (<strong>JSON</strong>), which is popular for content management and mobile application data handling. Popular examples of document databases are Couchbase server, CouchDB, DocumentDB, MarkLogic, and MongoDB.</li>
<li><strong>Wide-column stores</strong>: These are designed to organize data tables as columns instead of as rows. Wide-column stores can be found both in SQL and NoSQL databases, and offer very high performance and a highly scalable architecture. Wide-column stores are faster than traditional relational databases and can query large data volumes very fast, hence they are used for intensive data processing systems such as recommendation engines, catalogs, fraud detection, and so on. A few popular examples of wide-column stores are Google Bigtable, Cassandra, and HBase.</li>
<li><strong>Graph database</strong>: Graph data stores represent data as relationship models and organize data as nodes. They are designed to represent data relationships as interconnected elements, such as a graph with a pictorial representation of the number of relations between them. An example is connections between nodes; the graph data model can evolve over time so is used with a flexible schema. Graph databases are effectively used in systems that must map relationships, such as friends connected on a social network, reservation systems, or customer relationship management. Examples of graph databases include AllegroGraph, IBM Graph, Neo4j, and Titan.</li>
</ul>
<p style="padding-left: 60px">The selection of NoSQL DBs are based on business requirements like building big data and web applications demanding high performance, scalability, flexibility, functionality, and complexity. </p>
<ul>
<li><strong>Architecture</strong>: Basic requirements of web and cloud-based systems are constant uptime, multi-geography data replication, predictable performance, and so on. The architecture should be designed to support diverse workload patterns, technology to support ingesting data of high volume, variety, and velocity. Ability to perform transactions in real time, run real-time analytics on data lake or multiple systems. There are a few based on the master/slave model, such as MongoDB, and a few are masterless, where all nodes in database cluster perform the same role, as in Cassandra.</li>
<li><strong>Data model scalability</strong>:  The data models are based on types, like a wide-row tabular store, document-oriented, key-value, or graph, and so on to scale very rapidly and elastically, in order to be applicable to all situations and times, scaling across multiple data centers and even to the cloud if needed.</li>
<li><strong>Data distribution model</strong><span><span>: Based on their inherent architectural differences, NoSQL databases perform differently for the reading, writing, and distribution of data. For example, Cassandra is popular to support writes and reads on every node in a cluster, and can replicate and synchronize data between many data centers across cloud providers.</span></span></li>
<li><strong>Development model and support</strong>: Based on their development APIs, NoSQL databases might have unique SQL-like languages (for example, Cassandra's CQL). Vendor or community-related support for technology will be an invaluable resource for the individuals and teams managing the environment.</li>
<li><strong>Performance and continuous availability</strong>: In an online world, big data must perform at extremely high velocities under varying workloads, so databases must scale and perform to support applications and environments, as nanosecond delays can cost you sales. Revenue generating systems like flight reservation systems (and data) need to be available 24*7 , as businesses can't afford  any downtime.</li>
<li><strong>Manageability and cost</strong>: There is a need to balance the cost of NoSQL platform development and operational complexity, to be viable to businesses from a cost and usage perspective. Deploying a well-structured NoSQL program provides all of the benefits already listed, while also lowering operational costs.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits of NoSQL</h1>
                </header>
            
            <article>
                
<p>As we have seen, for cloud applications and decentralized systems NoSQL databases are the de facto databases to use, primarily as NoSQL databases offer many robust features and benefits compared to other database management systems, such as the following:</p>
<ul>
<li><strong>Continuous availability</strong>: A database should be available 24/7 or 99.999% of the time, even during standing infrastructure outages.</li>
<li><strong>Economical/minimal cost of operations</strong>: Investment and expenses related to maintenance; scalability of the NoSql systems should be affordable to businesses and compatible to support existing applications.</li>
<li><strong>Scalable architecture</strong>: Web applications support multiple geographies, so architectural features of the database should be resilient and scalable. Data manipulation features and capabilities are to be supported for multiple concurrent  enterprise systems.</li>
<li><strong>High responsiveness</strong>: Cloud and web-based applications are low latency applications that must respond as per business demands quickly. These applications should perform under various conditions like varying and mixed workloads and multiple data models, and integrations with third-party tools and applications.</li>
<li><strong>Elastically scalable</strong>: The database and supporting applications should be designed for current and future data needs linearly and predictably, and be operationally mature.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data visualization</h1>
                </header>
            
            <article>
                
<p>A picture is worth a thousand words. D<span>ata visualization</span><span> is the representation of data in the form of graphs, charts, pictures, or any other visual means</span><span>. It helps users to quickly understand and analyze the complex data patterns, variations, and deviations associated with data. As we can all agree, skimming through multiple records of numerical data would be very tiring. The ability to graphically visualize the same data would be a very effective, efficient, and time-saving way to identify the patterns we need. There are many tools that help with data visualizations; the simplest forms are bar charts, pie diagrams, and so on.</span></p>
<p>In big data visualizations, data patterns play an important role. A few benefits of data visualization are as follows:</p>
<ul>
<li>A quick, easy way to convey concepts in a universal manner</li>
<li>Identify areas that need attention or improvement</li>
<li>Clarify which factors influence customer behavior</li>
<li>Help you understand which products to place where</li>
<li>Predict sales volumes by segment and time period</li>
</ul>
<p>Data representation and visualization methods are listed, as follows:</p>
<ul>
<li> Graph plots: 
<ul>
<li>Area graph</li>
<li>Bar chart</li>
<li>Bubble chart</li>
<li>Density plot</li>
<li>Error bars</li>
<li>Histogram</li>
<li>Line graph</li>
<li>Multi-set bar chart</li>
<li>Parallel coordinates plot</li>
<li>Point and figure chart</li>
<li>Population pyramid</li>
<li>Radar bar chart</li>
<li>Radial column chart</li>
<li>Scatterplot</li>
<li>Span chart</li>
<li>Spiral plot</li>
<li>Stacked area graph</li>
<li>Streamgraph</li>
</ul>
</li>
<li>Diagrams:
<ul>
<li>Flowchart</li>
<li>Illustration diagram</li>
<li>Timeline</li>
<li>Tree diagram</li>
<li>Network diagram</li>
<li>Venn diagram</li>
<li>Pie chart</li>
<li>Pictogram chart</li>
<li>Bubble map</li>
<li>Treemap</li>
</ul>
</li>
<li>Tables:
<ul>
<li>Calendar</li>
<li>Gantt chart</li>
<li>Heatmap</li>
<li>Stem and leaf plot</li>
<li>Tally chart</li>
<li>Timetable</li>
</ul>
</li>
<li>Maps:
<ul>
<li>Choropleth map</li>
<li>Connection map</li>
<li>Dot map</li>
<li>Flow map</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data visualization application</h1>
                </header>
            
            <article>
                
<p>Data visualization is quite valuable to preview the raw data in form of pictures and graphs as presentable formats. Few tools to aid the same are listed following:</p>
<ul>
<li><strong>Time series</strong><em>:</em> It's used to plot the performance trend of a single variable, such as the sales of a particular car model over a time period of 5 years, with a line chart (<a href="https://en.wikipedia.org/wiki/Line_chart">https://en.wikipedia.org/wiki/Line_chart</a>) to demonstrate the trend.</li>
<li><strong>Nominal comparison</strong>: Comparing general trends, say in sales volume by car model with a bar chart.</li>
<li><strong>Ranking</strong>: Used to compare car sales between different locations over the period of 5 years for a particular model. The performance over a period of time for different segments is represented by a bar chart (<a href="https://en.wikipedia.org/wiki/Bar_chart">https://en.wikipedia.org/wiki/Bar_chart</a>) to show the comparison across the zones on sales.</li>
<li><strong>Part-to-whole</strong><em>:</em> Used to measure a ratio to the whole (that is, a percentage out of 100%). The percentage of students securing <em>A</em> grade in a class is represented by a pie chart (<a href="https://en.wikipedia.org/wiki/Pie_chart">https://en.wikipedia.org/wiki/Pie_chart</a>) or bar chart to show the comparison ratio.</li>
<li><strong>Frequency distribution</strong>: Used to evaluate the trend of a particular variable for a given interval, such as the number of years in which the property price change is between intervals such as 0-10%, 11-20%, and so on. A histogram (<a href="https://en.wikipedia.org/wiki/Histogram">https://en.wikipedia.org/wiki/Histogram</a>) or bar chart may be used. A box plot (<a href="https://en.wikipedia.org/wiki/Box_plot">https://en.wikipedia.org/wiki/Box_plot</a>) helps visualize key statistics such as median, quartiles, outliers, and so on for the distribution.</li>
<li><strong>Correlation</strong>: Used to compare dependency of trend movement between two variables (<em>X</em> and <em>Y</em>) to determine if they tend to move together or in opposite directions, for example, plotting unemployment (<em>X</em>) and GDP growth (<em>Y</em>) for a time period in months with a scatter plot (<a href="https://en.wikipedia.org/wiki/Scatter_plot">https://en.wikipedia.org/wiki/Scatter_plot</a>).</li>
<li><strong>Deviation</strong>: Used for comparison of the actual versus the reference amount, such as the comparison of actual versus budget expenses for several portfolios of a business for a given time period, represented by a bar chart.</li>
<li><strong>Geographic or geospatial:</strong> Used to compare the spread of a variable across a map or geographical layout, such as the store locations by state. A cartogram (<a href="https://en.wikipedia.org/wiki/Cartogram">https://en.wikipedia.org/wiki/Cartogram</a>) is a typical graphic used to plot a number of cars on the various floors of a parking lot.</li>
</ul>
<p>A few of the <span>commercial</span><span> </span><span>data visualization tools are as follows:</span></p>
<ul>
<li>Tableau</li>
<li>QlikView</li>
<li>Microstrategy</li>
<li>Microsoft PowerBI</li>
<li>TIBCO Spotfire</li>
<li>Looker</li>
<li>DOMO</li>
<li>ZOHO</li>
<li>Information Builders</li>
<li>Chart</li>
</ul>
<p>Some data visualization open source tools are as follows:</p>
<ul>
<li>Google Charts</li>
<li>Js</li>
<li>Ploty</li>
<li>Chart Js</li>
<li>Charted</li>
<li> Leaflet</li>
<li> FusionCharts</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data science</h1>
                </header>
            
            <article>
                
<p>A few similar terms associated with data science include:</p>
<ul>
<li><strong>Data science</strong>: An interdisciplinary field<span> also known as data-driven science.</span> There are few stages such as data discovery, the study of information originating from varied data sources in different forms, structured or unstructured, <span>ingesting  the data, </span>and applying scientific methods and processes to gain insights and <span>apply knowledge </span>for the creation of value-added business and IT strategies.</li>
<li><strong>Data mining:</strong> Data mining is a broad term for the practice of trying to find patterns in large sets of data. It is the process of trying to categorize a mass of data into a more understandable and cohesive set of information. Mining large amounts of structured and unstructured data to identify patterns can help an organization rein in costs, increase efficiency, recognize new market opportunities, and increase their competitive advantage.</li>
<li><strong>Machine learning:</strong> Machine learning is the study and practice of designing systems that can learn, adjust, and improve based on the data fed to them. This typically involves the implementation of predictive and statistical algorithms that can continually zero in on <em>correct</em> behavior and insights as more data flows through the system.</li>
</ul>
<p>The interdependency and interfacing of data science with other interfacing sections of big data and data discovery are depicted, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-524 image-border" height="269" src="assets/f3e61679-7e70-4cf9-9099-fb80a735073d.png" width="520"/></div>
<p>Big data discovery is the combination of big data, data science, and data discovery.</p>
<p>Gartner analysts have defined a new evolving role of <strong>citizen data scientists</strong> who, using these tools, will marry the skills of traditional business analysts with some of the expertise of expert statisticians.</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-525 image-border" height="176" src="assets/67523347-be78-456e-87dc-84cba85634b9.png" width="387"/></div>
<p class="mce-root">The role of a modern data scientist requires a mixture of broad, multidisciplinary skills ranging from an intersection of mathematics, statistics, computer science, communication, and business understanding. A data scientist's most basic, universal skill is the ability to integrate systems and derive meaningful and reproducible patterns from the underlying data. More enduring will be the need for data scientists to communicate in a language that all their stakeholders understand, and to demonstrate the special skills involved in storytelling with data, whether verbally or visually, or ideally both.</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-131 image-border" height="392" src="assets/28af1fb1-530f-43b1-926f-fea205afaf71.png" width="429"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned about the concepts of big data, In-memory technology, NoSQL databases, data visualization, and data science.</p>
<p>In the next chapter, we will discuss the concepts of DevOps frameworks and best practices.</p>


            </article>

            
        </section>
    </body></html>