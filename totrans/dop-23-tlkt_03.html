<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating Pods</h1>
                </header>
            
            <article>
                
<div class="packt_tip">Pods are equivalent to bricks we use to build houses. Both are uneventful and not much by themselves. Yet, they are fundamental building blocks without which we could not construct the solution we are set to build.</div>
<p>If you used Docker or Docker Swarm, you're probably used to thinking that a container is the smallest unit and that more complex patterns are built on top of it. With Kubernetes, the smallest unit is a Pod. A Pod is a way to represent a running process in a cluster. From Kubernetes' perspective, there's nothing smaller than a Pod.</p>
<p>A Pod encapsulates one or more containers. It provides a unique network IP, it attaches storage resources, and it decides how containers should run. Everything in a Pod is tightly coupled.</p>
<p>We should clarify that containers in a Pod are not necessarily made by Docker. Other container runtimes are supported as well. Still, at the time of this writing, Docker is the most commonly used container runtime, and all our examples will use it.</p>
<div class="packt_infobox">From this chapter onward, we will break the publishing tradition of having a long explanation of concepts before diving into practical examples. Instead, we'll try to learn theory through practice. One step at a time.</div>
<p>We'll move straight into hands-on exercises. Since we cannot create Pods without a Kubernetes cluster, our first order of business is to create one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Cluster</h1>
                </header>
            
            <article>
                
<p>We'll create a local Kubernetes cluster using Minikube.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/d860631d0dd3158c32740e9260c7add0"><kbd>03-pods.sh</kbd></a> (<a href="https://gist.github.com/vfarcic/d860631d0dd3158c32740e9260c7add0" target="_blank"><span class="URLPACKT">https://gist.github.com/vfarcic/d860631d0dd3158c32740e9260c7add0</span></a>) Gist.</div>
<pre><strong>minikube start --vm-driver=virtualbox</strong>
    
<strong>kubectl get nodes</strong>  </pre>
<p>The output of the latter command is as follows:</p>
<pre><strong>NAME     STATUS ROLES  AGE VERSION</strong>
<strong>minikube Ready  &lt;none&gt; 47s v1.8.0</strong>  </pre>
<p>To simplify the process and save you from writing all the configuration files, we'll clone the GitHub repository <a href="https://github.com/vfarcic/k8s-specs"><kbd>vfarcic/k8s-specs</kbd></a> (<a href="https://github.com/vfarcic/k8s-specs" target="_blank"><span class="URLPACKT">https://github.com/vfarcic/k8s-specs</span></a>). It contains everything we'll need for this chapter, as well as for most of the others in this book.</p>
<pre><strong>git clone https://github.com/vfarcic/k8s-specs.git</strong>
    
<strong>cd k8s-specs</strong>  </pre>
<p>We cloned the repository and entered into the directory that was created.</p>
<p>Now we can run our first Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quick and dirty way to run Pods</h1>
                </header>
            
            <article>
                
<p>Just as we can execute <kbd>docker run</kbd> to create containers, <kbd>kubectl</kbd> allows us to create Pods with a single command. For example, if we'd like to create a Pod with a Mongo database, the command is as follows.</p>
<pre><strong>kubectl run db --image mongo</strong>  </pre>
<p>You'll notice that the output says that <kbd>deployment "db" was created</kbd>. Kubernetes runs more than a single Pod. It created a Deployment and a few other things. We won't go into all the details just yet. What matters, for now, is that we created a Pod. We can confirm that by listing all the Pods in the cluster:</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                READY STATUS            RESTARTS AGE</strong>
<strong>db-59d5f5b96b-kch6p 0/1   ContainerCreating 0        1m</strong>  </pre>
<p>We can see the name of the Pod, its readiness, the status, the number of times it restarted, and for how long it has existed (its age). If you were fast enough, or your network is slow, none of the pods might be ready. We expect to have one Pod, but there's zero running at the moment. Since the <kbd>mongo</kbd> image is relatively big, it might take a while until it is pulled from Docker Hub. After a while, we can retrieve the Pods one more time to confirm that the Pod with the Mongo database is running.</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                READY STATUS  RESTARTS AGE</strong>
<strong>db-59d5f5b96b-kch6p 1/1   Running 0        6m</strong>  </pre>
<p>We can see that, this time, the Pod is ready and we can start using the Mongo database.</p>
<p>We can confirm that a container based on the <kbd>mongo</kbd> image is indeed running inside the cluster.</p>
<pre><strong>eval $(minikube docker-env)</strong>
    
<strong>docker container ls -f ancestor=mongo</strong>  </pre>
<p>We evaluated <kbd>minikube</kbd> variables so that our local Docker client is using Docker server running inside the VM. Further on, we listed all the containers based on the <kbd>mongo</kbd> image. The output is as follows (IDs are removed for brevity):</p>
<pre><strong>IMAGE COMMAND                CREATED       STATUS       PORTS NAMES</strong>
<strong>mongo "docker-entrypoint.s..." 5 minutes ago Up 5 minutes       k8s<br/> _db_db-...</strong>  </pre>
<p>As you can see, the container defined in the Pod is running.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cf2002b0-d8e2-4f24-acde-fb22a9d8366e.png" style="width:32.42em;height:11.08em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 3-1: A Pod with a single container</div>
<p>That was not the best way to run Pods so we'll delete the deployment which, in turn, will delete everything it envelops, including the Pod.</p>
<pre><strong>kubectl delete deployment db</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>deployment "db" deleted</strong>  </pre>
<p>Why did I say that was not the best way to run Pods? We used the imperative way to tell Kubernetes what to do. Even though there are cases when that might be useful, most of the time we want to leverage the declarative approach. We want to have a way to define what we need in a file and pass that information to Kubernetes. That way, we can have a documented and repeatable process, that can (and should) be version controlled as well. Moreover, the <kbd>kubectl run</kbd> was reasonably simple. In real life, we need to declare much more than the name of the deployment and the image. Commands like <kbd>kubectl</kbd> can quickly become too long and, in many cases, very complicated. Instead, we'll write specifications in YAML format. Soon, we'll see how we can accomplish a similar result using declarative syntax.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining Pods through declarative syntax</h1>
                </header>
            
            <article>
                
<p>Even though a Pod can contain any number of containers, the most common use case is to use the single-container-in-a-Pod model. In such a case, a Pod is a wrapper around one container. From Kubernetes' perspective, a Pod is the smallest unit. We cannot tell Kubernetes to run a container. Instead, we ask it to create a Pod that wraps around a container.</p>
<p>Let's take a look at a simple Pod definition:</p>
<pre><strong>cat pod/db.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>kind: Pod</strong>
<strong>metadata:</strong>
<strong>  name: db</strong>
<strong>  labels:</strong>
<strong>    type: db</strong>
<strong>    vendor: Mongo Labs</strong>
<strong>spec:</strong>
<strong>  containers:</strong>
<strong>  - name: db</strong>
<strong>    image: mongo:3.3</strong>
<strong>    command: ["mongod"]</strong>
<strong>    args: ["--rest", "--httpinterface"]</strong>  </pre>
<p>We're using <kbd>v1</kbd> of Kubernetes Pods API. Both <kbd>apiVersion</kbd> and <kbd>kind</kbd> are mandatory. That way, Kubernetes knows what we want to do (create a Pod) and which API version to use.</p>
<p>The next section is <kbd>metadata</kbd>. It provides information that does not influence how the Pod behaves. We used <kbd>metadata</kbd> to define the name of the Pod (<kbd>db</kbd>) and a few labels. Later on, when we move into Controllers, labels will have a practical purpose. For now, they are purely informational.</p>
<p>The last section is the <kbd>spec</kbd> in which we defined a single container. As you might have guessed, we can have multiple containers defined as a Pod. Otherwise, the section would be written in singular (<kbd>container</kbd> without <kbd>s</kbd>). We'll explore multi-container Pods later.</p>
<p>In our case, the container is defined with the name (<kbd>db</kbd>), the image (<kbd>mongo</kbd>), the command that should be executed when the container starts (<kbd>mongod</kbd>), and, finally, the set of arguments. The arguments are defined as an array with, in this case, two elements (<kbd>--rest</kbd> and <kbd>--httpinterface</kbd>).</p>
<p>We won't go into details of everything you can use to define a Pod. Throughout the book, you'll see quite a few other commonly (and not so commonly) used things we should define in Pods. Later on, when you decide to learn all the possible arguments you can apply, explore the official, and ever-changing, <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core"><kbd>Pod v1 core</kbd></a>                                                                                                                                                    (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core</span></a>) documentation.</p>
<p>Let's create the Pod defined in the <kbd>db.yml</kbd> file.</p>
<pre><strong>kubectl create -f pod/db.yml</strong>  </pre>
<p>You'll notice that we did not need to specify <kbd>pod</kbd> in the command. The command will create the kind of resource defined in the <kbd>pod/db.yml</kbd> file. Later on, you'll see that a single YAML file can contain definitions of multiple resources.</p>
<p>Let's take a look at the Pods in the cluster:</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME READY STATUS  RESTARTS AGE</strong>
<strong>db   1/1   Running 0        11s</strong>  </pre>
<p>Our Pod named <kbd>db</kbd> is up and running.</p>
<p>In some cases, you might want to retrieve a bit more information by specifying <kbd>wide</kbd> output.</p>
<pre><strong>kubectl get pods -o wide</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME READY STATUS  RESTARTS AGE IP         NODE</strong>
<strong>db   1/1   Running 0        1m  172.17.0.4 minikube</strong>  </pre>
<p>As you can see, we got two additional columns; the IP and the node.</p>
<p>If you'd like to parse the output, using <kbd>json</kbd> format is probably the best option.</p>
<pre><strong>kubectl get pods -o json</strong>  </pre>
<p>The output is too big to be presented in the book, especially since we won't go through all the information provided through the <kbd>json</kbd> output format.</p>
<p>When we want more information than provided with the default output, but still in a format that is human-friendly, <kbd>yaml</kbd> output is probably the best choice.</p>
<pre><strong>kubectl get pods -o yaml</strong>  </pre>
<p>Just as with the <kbd>json</kbd> output, we won't go into details of everything we got from Kubernetes. With time, you'll become familiar with all the information related to Pods. For now, we want to focus on the most important aspects.</p>
<p>Let's introduce a new <kbd>kubectl</kbd> sub-command.</p>
<pre><strong>kubectl describe pod db</strong>  </pre>
<p>The <kbd>describe</kbd> sub-command returned details of the specified resource. In this case, the resource is the Pod named <kbd>db</kbd>.</p>
<p>The output is too big for us to go into every detail. Besides, most of it should be self-explanatory if you're familiar with containers. Instead, we'll briefly comment on the last section called <kbd>events</kbd>.</p>
<pre><strong>...</strong>
<strong>Events:</strong>
<strong>  Type    Reason                 Age   From               Message</strong>
<strong>  ----    ------                 ----  ----               -------</strong>
<strong>  Normal  Scheduled              2m    default-scheduler  Successfully assigned db to minikube</strong>
<strong>  Normal  SuccessfulMountVolume  2m    kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-x27md"</strong>
<strong>  Normal  Pulling                2m    kubelet, minikube  pulling image "mongo:3.3"</strong>
<strong>  Normal  Pulled                 2m    kubelet, minikube  Successfully pulled image "mongo:3.3"</strong>
<strong>  Normal  Created                2m    kubelet, minikube  Created container</strong>
<strong>  Normal  Started                2m    kubelet, minikube  Started container</strong></pre>
<p>We can see that the Pod was created and went through several stages as shown in the following sequence diagram. Even though the process was simple from a user's perspective, quite a few things happened in the background.</p>
<p>This might be a right moment to pause with our exercises, discuss some of the details of Kubernetes components, and try to get an understanding of how Pod scheduling works.</p>
<p>Three major components were involved in the process.</p>
<p>The <em>API server</em> is the central component of a Kubernetes cluster and it runs on the master node. Since we are using Minikube, both master and worker nodes are baked into the same virtual machine. However, a more serious Kubernetes cluster should have the two separated on different hosts.</p>
<p>All other components interact with API server and keep watch for changes. Most of the coordination in Kubernetes consists of a component writing to the API Server resource that another component is watching. The second component will then react to changes almost immediately.</p>
<p>The <em>scheduler</em> is also running on the master node. Its job is to watch for unassigned pods and assign them to a node which has available resources (CPU and memory) matching Pod requirements. Since we are running a single-node cluster, specifying resources would not provide much insight into their usage so we'll leave them for later.</p>
<p><em>Kubelet</em> runs on each node. Its primary function is to make sure that assigned pods are running on the node. It watches for any new Pod assignments for the node. If a Pod is assigned to the node Kubelet is running on, it will pull the Pod definition and use it to create containers through Docker or any other supported container engine.</p>
<p>The sequence of events that transpired with the <kbd>kubectl create -f pod/db.yml</kbd> command is as follows:</p>
<ol>
<li>Kubernetes client (<kbd>kubectl</kbd>) sent a request to the API server requesting creation of a Pod defined in the <kbd>pod/db.yml</kbd> file.</li>
<li>Since the scheduler is watching the API server for new events, it detected that there is an unassigned Pod.</li>
<li>The scheduler decided which node to assign the Pod to and sent that information to the API server.</li>
<li>Kubelet is also watching the API server. It detected that the Pod was assigned to the node it is running on.</li>
<li>Kubelet sent a request to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines a single container based on the <kbd>mongo</kbd> image.</li>
<li>Finally, Kubelet sent a request to the API server notifying it that the Pod was created successfully.</li>
</ol>
<p>The process might not make much sense right now since we are running a single-node cluster. If we had more VMs, scheduling might have happened somewhere else, and the complexity of the process would be easier to grasp. We'll get there in due time.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/90442108-970e-42fb-95a8-5337d9c16567.png" style="width:39.92em;height:26.33em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 3-2: Pod scheduling sequence</div>
<p>In many cases, it is more useful to describe resources by referencing the file that defines them. That way there is no confusion nor need to remember the names of resources. We could have executed the command that follows:</p>
<pre><strong>kubectl describe -f pod/db.yml</strong>  </pre>
<p>The output should be the same since, in both cases, <kbd>kubectl</kbd> sent a request to Kubernetes API requesting information about the Pod named <kbd>db</kbd>.</p>
<p>Just as with Docker, we can execute a new process inside a running container inside a Pod.</p>
<pre><strong>kubectl exec db ps aux</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>USER PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND</strong>
<strong>root   1  0.5  2.9 967452 59692 ?   Ssl  21:47 0:03 mongod --rest --httpinterface</strong>
<strong>root  31  0.0  0.0  17504  1980 ?   Rs   21:58 0:00 ps aux</strong>
  </pre>
<p>We told Kubernetes that we'd like to execute a process inside the first container of the Pod <kbd>db</kbd>. Since our Pod defines only one container, this container and the first container are one and the same. The <kbd>--container</kbd> (or <kbd>-c</kbd>) argument can be set to specify which container should be used. That is particularly useful when running multiple containers in a Pod.</p>
<p>Apart from using Pods as the reference, <kbd>kubectl exec</kbd> is almost the same as the <kbd>docker container exec</kbd> command. The significant difference is that <kbd>kubectl</kbd> allows us to execute a process in a container running in any node inside a cluster, while <kbd>docker container exec</kbd> is limited to containers running on a specific node.</p>
<p>Instead of executing a new short-lived process inside a running container, we can enter into it. For example, we can make the execution interactive with <kbd>-i (stdin)</kbd> and <kbd>-t</kbd> (terminal) arguments and run <kbd>shell</kbd> inside a container.</p>
<pre><strong>kubectl exec -it db sh</strong>  </pre>
<p>We're inside the <kbd>sh</kbd> process inside the container. Since the container hosts a Mongo database, we can, for example, execute <kbd>db.stats()</kbd> to confirm that the database is indeed running.</p>
<pre><strong>echo 'db.stats()' | mongo localhost:27017/test</strong>  </pre>
<p>We used <kbd>mongo</kbd> client to execute <kbd>db.stats()</kbd> for the database <kbd>test</kbd> running on <kbd>localhost:27017</kbd>. Since we're not trying to learn Mongo (at least not in this book), the only purpose of this exercise was to prove that the database is up-and-running. Let's get out of the container.</p>
<pre><strong>exit</strong>  </pre>
<p>Logs should be shipped from containers to a central location. However, since we did not yet explore that subject, it would be useful to be able to see logs of a container in a Pod.</p>
<p>The command that outputs logs of the only container in the <kbd>db</kbd> Pod is as follows:</p>
<pre><strong>kubectl logs db</strong>  </pre>
<p>The output is too big and not that important in its entirety. One of the last line is as follows:</p>
<pre><strong>...</strong>
<strong>2017-11-10T22:06:20.039+0000 I NETWORK  [thread1] waiting for connections on port 27017</strong>
<strong>...</strong>  </pre>
<p>With the <kbd>-f</kbd> (or <kbd>--follow</kbd>) we can follow the logs in real-time. Just as with the <kbd>exec</kbd> sub-command, if a Pod defines multiple containers, we can specify which one to use with the <kbd>-c</kbd> argument.</p>
<p>What happens when a container inside a Pod dies? Let's simulate a failure and observe what happens.</p>
<pre><strong>kubectl exec -it db pkill mongod</strong>
    
<strong>kubectl get pods</strong>  </pre>
<p>We killed the main process of the container and listed all the Pods. The output is as follows:</p>
<pre><strong>NAME READY STATUS  RESTARTS AGE</strong>
<strong>db   1/1   Running 1        13m</strong>  </pre>
<p>The container is running (<kbd>1/1</kbd>). Kubernetes guarantees that the containers inside a Pod are (almost) always running. Please note that the <kbd>RESTARTS</kbd> field now has the value of <kbd>1</kbd>. Every time a container fails, Kubernetes will restart it:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/6cba91dd-bc5f-45b8-aec9-e1928cf54952.png" style="width:37.08em;height:12.42em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 3-3: Pod with a failed container</div>
<p>Finally, we can delete a Pod if we don't need it anymore.</p>
<pre><strong>kubectl delete -f pod/db.yml</strong>
    
<strong>kubectl get pods</strong>  </pre>
<p>We removed the Pods defined in <kbd>db.yml</kbd> and retrieved the list of all the Pods in the cluster. The output of the latter command is as follows:</p>
<pre><strong>NAME READY STATUS      RESTARTS AGE</strong>
<strong>db   0/1   Terminating 1        3h</strong>  </pre>
<p>The number of ready containers dropped to <kbd>0</kbd>, and the status of the <kbd>db</kbd> Pod is <kbd>terminating</kbd>.</p>
<p>When we sent the instruction to delete a Pod, Kubernetes tried to terminate it gracefully. The first thing it did was to send the <kbd>TERM</kbd> signal to all the main processes inside the containers that form the Pod. From there on, Kubernetes gives each container a period of thirty seconds so that the processes in those containers can shut down gracefully. Once the grace period expires, the <kbd>KILL</kbd> signal is sent to terminate all the main processes forcefully and, with them, all the containers. The default grace period can be changed through the <kbd>gracePeriodSeconds</kbd> value in YAML definition or <kbd>--grace-period</kbd> argument of the <kbd>kubectl delete</kbd> command.</p>
<p>If we repeat the <kbd>get pods</kbd> command thirty seconds after we issued the <kbd>delete</kbd> instruction, the Pod should be removed from the system:</p>
<pre><strong>kubectl get pods</strong>  </pre>
<p>This time, the output is different.</p>
<pre><strong>No resources found.</strong>  </pre>
<p>The only Pod we had in the system is no more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running multiple containers in a single Pod</h1>
                </header>
            
            <article>
                
<p>Pods are designed to run multiple cooperative processes that should act as a cohesive unit. Those processes are wrapped in containers. All the containers that form a Pod are running on the same machine. A Pod cannot be split across multiple nodes.</p>
<p>All the processes (containers) inside a Pod share the same set of resources, and they can communicate with each other through <kbd>localhost</kbd>. One of those shared resources is storage. A volume defined in a Pod can be accessed by all the containers thus allowing them all to share the same data. We'll explore storage in more depth later on. For now, let's take a look at the <kbd>pod/go-demo-2.yml</kbd> specification:</p>
<pre><strong>cat pod/go-demo-2.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>kind: Pod</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2</strong>
<strong>  labels:</strong>
<strong>    type: stack</strong>
<strong>spec:</strong>
<strong>  containers:</strong>
<strong>  - name: db</strong>
<strong>    image: mongo:3.3</strong>
<strong>  - name: api</strong>
<strong>    image: vfarcic/go-demo-2</strong>
 <strong>   env:</strong>
<strong>    - name: DB</strong>
<strong>      value: localhost</strong>  </pre>
<p>The YAML file defines a Pod with two containers named <kbd>db</kbd> and <kbd>api</kbd>. The service inside the <kbd>vfarcic/go-demo-2</kbd> image uses environment variable <kbd>DB</kbd> to know where the database is. The value is <kbd>localhost</kbd> since all the containers in the same Pod are reachable through it.</p>
<p>Let's create the Pod:</p>
<pre><strong>kubectl create -f pod/go-demo-2.yml</strong>
    
<strong>kubectl get -f pod/go-demo-2.yml</strong>  </pre>
<p>We created a new Pod defined in the <kbd>go-demo-2.yml</kbd> file and retrieved its information from Kubernetes. The output of the latter command is as follows:</p>
<pre><strong>NAME      READY STATUS  RESTARTS AGE</strong>
<strong>go-demo-2 2/2   Running 0        2m</strong>  </pre>
<p>We can see from the <kbd>READY</kbd> column that, this time, the Pod has two containers (<kbd>2/2</kbd>).</p>
<p>This might be an excellent opportunity to introduce formatting to retrieve specific information.</p>
<p>Let's say that we want to retrieve the names of the containers in a Pod. The first thing we'd have to do is get familiar with Kubernetes API. We can do that by going to <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core"><kbd>Pod v1 core</kbd></a> (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core</span></a>) documentation. While reading the documentation will become mandatory sooner or later, we'll use a simpler route and inspect the output from Kubernetes.</p>
<pre><strong>kubectl get -f pod/go-demo-2.yml -o json</strong>  </pre>
<p>The output is too big to be presented in a book, so we'll focus on the task at hand. We need to retrieve the names of the containers in the Pod. Therefore, the part of the output we're looking for is as follows:</p>
<pre><strong>{</strong>
<strong>    ...</strong>
<strong>    "spec": {</strong>
<strong>        "containers": [</strong>
<strong>            {</strong>
<strong>                ...</strong>
<strong>                "name": "db",</strong>
<strong>                ...</strong>
<strong>            },</strong>
<strong>            {</strong>
<strong>                ...</strong>
<strong>                "name": "api",</strong>
<strong>                ...</strong>
<strong>            }</strong>
<strong>        ],</strong>
<strong>        ...</strong>
<strong>    },</strong>
<strong>    ...</strong>
<strong>}</strong>  </pre>
<p>The <kbd>get</kbd> command that would filter the output and retrieve only the names of the containers is as follows:</p>
<pre><strong>kubectl get -f pod/go-demo-2.yml \</strong>
<strong>    -o jsonpath="{.spec.containers[*].name}"</strong></pre>
<p>The output is as follows:</p>
<pre><strong>db api</strong>  </pre>
<p>We used <kbd>jsonpath</kbd> as the output format and specified that we want to retrieve names of all the <kbd>containers</kbd> from the <kbd>spec</kbd>. The ability to filter and format information might not look that important right now but, once we move into more complex scenarios, it will prove to be invaluable. That will become especially evident when we try to automate the processes and requests sent to Kubernetes API.</p>
<p>How would we execute a command inside the Pod? Unlike the previous examples that did a similar task, this time we have two containers in the Pod, so we need to be more specific.</p>
<pre><strong>kubectl exec -it -c db go-demo-2 ps aux</strong>  </pre>
<p>The output should display the processes inside the <kbd>db</kbd> container. Namely, the <kbd>mongod</kbd> process.</p>
<p>How about logs from a container? As you might have guessed, we cannot execute something like <kbd>kubectl logs go-demo-2</kbd> since the Pod hosts multiple containers. Instead, we need to be specific and name the container from which we want to see the logs:</p>
<pre><strong>kubectl logs go-demo-2 -c db</strong>  </pre>
<p>How about scaling? How would we, for example, scale the service so that there are two containers of the API and one container for the database?</p>
<p>One option could be to define two containers in the Pod. Let's take a look at a Pod definition that might accomplish what we need.</p>
<pre><strong>cat pod/go-demo-2-scaled.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>kind: Pod</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2</strong>
<strong>  labels:</strong>
<strong>    type: stack</strong>
<strong>spec:</strong>
<strong>  containers:</strong>
<strong>  - name: db</strong>
<strong>    image: mongo:3.3</strong>
<strong>  - name: api-1</strong>
<strong>    image: vfarcic/go-demo-2</strong>
<strong>    env:</strong>
<strong>    - name: DB</strong>
<strong>      value: localhost</strong>
<strong>  - name: api-2</strong>
<strong>    image: vfarcic/go-demo-2</strong>
<strong>    env:</strong>
<strong>    - name: DB</strong>
<strong>      value: localhost</strong>  </pre>
<p>We defined two containers for the API and named them <kbd>api-1</kbd> and <kbd>api-2</kbd>. The only thing left is to create the Pod. But, we're not going to do that.</p>
<p>We should not think of Pods as resources that should do anything beyond a definition of the smallest unit in our cluster. A Pod is a collection of containers that share the same resources. Not much more. Everything else should be accomplished with higher-level constructs. We'll explore how to scale Pods without changing their definition in one of the next chapters.</p>
<p>Let's go back to our original multi-container Pod that defined <kbd>api</kbd> and <kbd>db</kbd> containers. That was a terrible design choice since it tightly couples one with the other. As a result, when we explore how to scale Pods (not containers), both would need to match. If, for example, we scale the Pod to three, we'd have three APIs and three DBs. Instead, we should have defined two Pods, one for each container (<kbd>db</kbd> and <kbd>api</kbd>). That would give us enough flexibility to treat each independently from the other.</p>
<p>There are quite a few other reasons not to put multiple containers in the same Pod. For now, just be patient. Most of the scenarios where you might think that multi-container Pod is a good solution will probably be solved through other resources.</p>
<div class="packt_infobox">A Pod is a collection of containers. However, that does not mean that multi-container Pods are common. They are rare. Most Pods you'll create will be single container units.</div>
<p>Does that mean that multi-container Pods are useless? They're not. There are scenarios when having multiple containers in a Pod is a good idea. However, they are very specific and, in most cases, are based on one container that acts as the main service and the rest serving as side-cars. A frequent use case are multi-container Pods used for <strong>continuous integration</strong> (<strong>CI</strong>), <strong>delivery</strong> (<strong>CD</strong>), or <strong>deployment</strong> (<strong>CDP</strong>) processes. We'll explore them later. For now, we'll focus on single-container Pods.</p>
<p>Let's remove the Pod before we move onto container health.</p>
<pre><strong>kubectl delete -f pod/go-demo-2.yml</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring health</h1>
                </header>
            
            <article>
                
<p>The <kbd>vfarcic/go-demo-2</kbd> Docker image is designed to fail on the first sign of trouble. In cases like that, there is no need for any health checks. When things go wrong, the main process stops, the container hosting it stops as well, and Kubernetes restarts the failed container. However, not all services are designed to fail fast. Even those that are might still benefit from additional health checks. For example, a back-end API can be up and running but, due to a memory leak, serve requests much slower than expected. Such a situation might benefit from a health check that would verify whether the service responds within, for example, two seconds. We can exploit Kubernetes liveness and readiness probes for that.</p>
<p><kbd>livenessProbe</kbd> can be used to confirm whether a container should be running. If the probe fails, Kubernetes will kill the container and apply restart policy which defaults to <kbd>Always</kbd>. <kbd>readinessProbe</kbd>, on the other hand, should be used as an indication that the service is ready to serve requests. When combined with <kbd>Services</kbd> construct, only containers with the <kbd>readinessProbe</kbd> state set to <kbd>Success</kbd> will receive requests. We'll leave <kbd>readinessProbe</kbd> for later since it is directly tied to <kbd>Services</kbd>. Instead, we'll explore <kbd>livenessProbe</kbd>. Both are defined in the same way so the experience with one of them can be easily applied to the other.</p>
<p>Let's take a look at an updated definition of the Pod we used thus far:</p>
<pre><strong>cat pod/go-demo-2-health.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: v1</strong>
<strong>kind: Pod</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2</strong>
<strong>  labels:</strong>
<strong>    type: stack</strong>
<strong>spec:</strong>
<strong>  containers:</strong>
<strong>  - name: db</strong>
<strong>    image: mongo:3.3</strong>
<strong>  - name: api</strong>
<strong>    image: vfarcic/go-demo-2</strong>
<strong>    env:</strong>
<strong>    - name: DB</strong>
<strong>      value: localhost</strong>
<strong>    livenessProbe:</strong>
<strong>      httpGet:</strong>
<strong>        path: /this/path/does/not/exist</strong>
<strong>        port: 8080</strong>
<strong>      initialDelaySeconds: 5</strong>
<strong>      timeoutSeconds: 2 # Defaults to 1</strong>
<strong>      periodSeconds: 5 # Defaults to 10</strong>
<strong>      failureThreshold: 1 # Defaults to 3</strong>  </pre>
<p>Don't get confused by seeing two containers in this Pod. I stand by my words. Those two should be defined in separate Pods. However, since that would require knowledge we are yet to obtain, and <kbd>vfarcic/go-demo-2</kbd> doesn't work without a database, we'll have to stick with the example that specifies two containers. It won't take long until we break it into pieces.</p>
<p>The additional definition is inside the <kbd>livenessProbe</kbd>.</p>
<p>We defined that the action should be <kbd>httpGet</kbd> followed with the <kbd>path</kbd> and the <kbd>port</kbd> of the service. Since <kbd>/this/path/does/not/exist</kbd> is true to itself, the probe will fail, thus showing us what happens when a container is unhealthy. The <kbd>host</kbd> is not specified since it defaults to the Pod IP.</p>
<p>Further down, we declared that the first execution of the probe should be delayed by five seconds (<kbd>initialDelaySeconds</kbd>), that requests should timeout after two seconds (<kbd>timeoutSeconds</kbd>), that the process should be repeated every five seconds (<kbd>periodSeconds</kbd>), and (<kbd>failureThreshold</kbd>) define how many attempts it must try before giving up.</p>
<p>Let's take a look at the probe in action.</p>
<pre><strong>kubectl create \</strong>
<strong>    -f pod/go-demo-2-health.yml</strong>  </pre>
<p>We created the Pod with the probe. Now we must wait until the probe fails a few times. A minute is more than enough. Once we're done waiting, we can describe the Pod:</p>
<pre><strong>kubectl describe \</strong>
<strong>    -f pod/go-demo-2-health.yml</strong>  </pre>
<p>The bottom of the output contains events. They are as follows:</p>
<pre><strong>...</strong>
<strong>Events:</strong>
<strong>  Type     Reason                 Age              From           Message</strong>
<strong>  ----     ------                 ----             ----           -------</strong>
<strong>  Normal   Scheduled              6m               default-scheduler  Successfully assigned go-demo-2 to minikube</strong>
<strong>  Normal   SuccessfulMountVolume  6m               kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-7jc7q"</strong>
<strong>  Normal   Pulling                6m               kubelet, minikube  pulling image "mongo"</strong>
<strong>  Normal   Pulled                 6m               kubelet, minikube  Successfully pulled image "mongo"</strong>
<strong>  Normal   Created                6m               kubelet, minikube  Created container</strong>
<strong>  Normal   Started                6m               kubelet, minikube  Started container</strong>
<strong>  Normal   Created                5m (x3 over 6m)  kubelet, minikube  Created container</strong>
<strong>  Normal   Started                5m (x3 over 6m)  kubelet, minikube  Started container</strong>
<strong>  Warning  Unhealthy              5m (x3 over 6m)  kubelet, minikube  Liveness probe failed: HTTP probe failed with statuscode: 404</strong>
<strong>  Normal   Pulling                5m (x4 over 6m)  kubelet, minikube  pulling image "vfarcic/go-demo-2"<br/>  Normal   Killing                5m (x3 over 6m)  kubelet, minikube  Killing container with id docker://api:Container failed live ness probe.. Container will be killed and recreated.</strong>
<strong>  Normal   Pulled                 5m (x4 over 6m)  kubelet, minikube  Successfully pulled image "vfarcic/go-demo-2"</strong>  </pre>
<p>We can see that, once the container started, the probe was executed, and that it failed. As a result, the container was killed only to be created again. In the preceding output, we can see that the process was repeated three times (<kbd>3x over ...</kbd>).</p>
<p>Please visit <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#probe-v1-core">Probe v1 core</a> (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#probe-v1-core" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#probe-v1-core</span></a>) if you'd like to learn all the available options.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pods are (almost) useless (by themselves)</h1>
                </header>
            
            <article>
                
<p>Pods are fundamental building blocks in Kubernetes. In most cases, you will not create Pods directly. Instead, you'll use higher level constructs like Controllers.</p>
<p>Pods are disposable. They are not long lasting services. Even though Kubernetes is doing its best to ensure that the containers in a Pod are (almost) always up-and-running, the same cannot be said for Pods. If a Pod fails, gets destroyed, or gets evicted from a Node, it will not be rescheduled. At least, not without a Controller. Similarly, if a whole node is destroyed, all the Pods on it will cease to exist. Pods do not heal by themselves. Excluding some special cases, Pods are not meant to be created directly.</p>
<div class="packt_infobox">Do not create Pods by themselves. Let one of the controllers create Pods for you.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What now?</h1>
                </header>
            
            <article>
                
<p>We'll remove the cluster and start the next chapter fresh.</p>
<pre><strong>minikube delete</strong>  </pre>
<p>Please take some time to get more familiar with Pods. They are the most basic and, arguably, the essential building block in Kubernetes. Since, by now, you have a solid understanding what the Pods are, a good next step might be to go through PodSpec v1 core (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core</span></a>) documentation.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/69055cc6-8934-4c02-b717-ab650ec5dffc.png" style="width:62.00em;height:18.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 3-4: The components explored so far</div>


            </article>

            
        </section>
    </body></html>