<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Introducing First-party Tools</h1></div></div></div><p>Docker provides several tools that extend the functionality outside of the core Docker engine. In this chapter, you will walk-through installing, configuring, and running the following tools:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Docker Toolbox</li><li class="listitem" style="list-style-type: disc">Docker Machine</li><li class="listitem" style="list-style-type: disc">Docker Swarm</li><li class="listitem" style="list-style-type: disc">Docker Compose</li></ul></div><p>These tools, while not as functional as some of the more advanced ones that we will be working with in the upcoming chapters, will serve as a good introduction to both adding additional functionality to core Docker engine as well as concepts for deploying and orchestrating your containers, which we will be doing more of towards the end of the book.</p><div><div><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Docker Toolbox</h1></div></div></div><p>Before <a id="id24" class="indexterm"/>we start to look at how to use the three other tools, we should look at installing them on our local machine. In the previous chapter, we downloaded a script supplied by Docker and piped it through bash to quickly configure the official Docker YUM or APT repository (depending on the operating system you are running) on an already provisioned server, the command we executed was as follows:</p><div><pre class="programlisting">
<strong>curl -sSL https://get.docker.com/ | sh</strong>
</pre></div><p>This is useful if you already have a Linux-based server up and running on one of the many cloud services or locally on virtual machine; however, what if you want to install Docker on a non-Linux operating system such as Mac OSX or Windows?</p><div><div><h3 class="title"><a id="tip02"/>Tip</h3><p>Always check the source. It is best practice to check the source of the bash script that you are going to be downloading and installing; in our case, you can check this by going to <a class="ulink" href="https://get.docker.com/">https://get.docker.com/</a> in your browser.</p></div></div><p>Before we look at the tools that Docker provides to do just that, we should answer the question why?</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec11"/>Why install Docker locally?</h2></div></div></div><p>So, why <a id="id25" class="indexterm"/>would we want to install Docker Toolbox, Compose, Machine, and Swarm on a non-Linux machine? Well, to start with, you need to remember that Docker, at its core, is an API to Linux Kernel-based technologies, such as run (<a class="ulink" href="https://github.com/opencontainers/runc">https://github.com/opencontainers/runc</a>) and LXC (<a class="ulink" href="https://linuxcontainers.org">https://linuxcontainers.org</a>), so while you will not be able to launch containers on your Mac OS X or Windows machine, you will be able to interact with a Docker installation on a Linux machine.</p><p>Being able to interact with Docker from your local machine means that you launch and interact with containers across multiple hosts that can be hosted externally on a public cloud/hosting service or locally on a virtual machine.</p><p>Luckily, Docker has you covered for installing Docker and the three other services that we are going to be looking at in this chapter on your local machine.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Installing Docker Toolbox</h2></div></div></div><p>Docker <a id="id26" class="indexterm"/>provides a global installer for all of their tools called Docker Toolbox, it makes installing the following software as painless as possible:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Docker Client</li><li class="listitem" style="list-style-type: disc">Docker Machine</li><li class="listitem" style="list-style-type: disc">Docker Compose</li><li class="listitem" style="list-style-type: disc">Docker Kitematic</li><li class="listitem" style="list-style-type: disc">VM VirtualBox</li></ul></div><p>To get started, you will need to be running a machine that either has Mac OS X 10.8+ or has Windows 7+ installed. In my case, I am running Mac OS X 10.11 (El Capitan); there is very little difference between the Mac OS X and Windows installers:</p><div><ol class="orderedlist arabic"><li class="listitem">First of all, to get started, you will need to download the installer from the Docker website. You can find links to download an executable for your chosen operating system at <a class="ulink" href="https://www.docker.com/docker-toolbox/">https://www.docker.com/docker-toolbox/</a>.<div><img src="img/B05468_02_01.jpg" alt="Installing Docker Toolbox"/></div></li><li class="listitem">Once you <a id="id27" class="indexterm"/>have downloaded the installer, you can launch it by double-clicking on it. You will then be presented by a series of screens and install options.<p>The first screen is a welcome page that confirms the version of the toolbox you are running. If you downloaded from the page in the preceding screenshot, then you will always have the latest version:</p><div><img src="img/B05468_02_02.jpg" alt="Installing Docker Toolbox"/></div></li><li class="listitem">To move <a id="id28" class="indexterm"/>to the next step of the installation, click on <strong>Continue</strong>.</li><li class="listitem">The next screen goes into more detail about the packages that will be installed, as well as the location at which they will be installed. There is also a box, which, if left ticked, will gather data about the machine you are installing Docker Toolbox on, anonymize it, and then submit it back to Docker.<p>This information is useful in giving Docker an idea about the types of machine their software is being installed on, and also it will report back any errors that you may encounter when running the installer:</p><div><img src="img/B05468_02_03.jpg" alt="Installing Docker Toolbox"/></div><p>I always recommend keeping this box ticked, as it all goes toward Docker making a better product and improving the experience of future versions of the installer.</p></li><li class="listitem">To progress <a id="id29" class="indexterm"/>to the next step of the installation, click on <strong>Continue</strong>.</li><li class="listitem">The next screen will give you the option of which disk you would like to install the various tools on. In most cases, you should stick with the defaults, unless you are running applications across multiple drives:<div><img src="img/B05468_02_04.jpg" alt="Installing Docker Toolbox"/></div></li><li class="listitem">To move on to the next step of the installation, again click on the <strong>Continue</strong> button.</li><li class="listitem">For majority of the people, a standard installation will be enough; however, if its not to install one of the tools, you can click the <strong>Customize</strong> button. The only two tools you have to install are the Docker Client and Docker Machine.<p>As I want to install all of the tools, I have chosen to go with the standard installation:</p><div><img src="img/B05468_02_05.jpg" alt="Installing Docker Toolbox"/></div></li><li class="listitem">Once you <a id="id30" class="indexterm"/>have chosen either a standard or custom installation, you can perform the installation by clicking the <strong>Install</strong> button.</li><li class="listitem">The installation itself takes a few minutes, during which you will get feedback on the task the installer is running:<div><img src="img/B05468_02_06.jpg" alt="Installing Docker Toolbox"/></div></li><li class="listitem">Once the installation is complete, click on the <strong>Continue</strong> button.<p>As running the installer also acts as an upgrader for any components you have installed, it will run a check to see if any of the files managed by the services (such as the virtual machine images used by the various tools) need to be updated.</p><p>Depending on the size of any updates and how much data you have, this process can take several minutes.</p><p>This process only applies to updates, so if you have performed a fresh installation like I have done, this section will be skipped.</p></li><li class="listitem">Now that <a id="id31" class="indexterm"/>the tools have been installed, you will be given the options of launching either the Docker Quickstart Terminal or Kitematic. For the purpose of this book, we will be skipping past this screen by clicking on the <strong>Continue</strong> button:<div><img src="img/B05468_02_07.jpg" alt="Installing Docker Toolbox"/></div></li><li class="listitem">If everything has gone as planned, you will see a message confirming that the installation has been completed and you can click on the <strong>Close</strong> button to quit the installer:<div><img src="img/B05468_02_08.jpg" alt="Installing Docker Toolbox"/></div></li></ol></div><p>Now, you have all of the tools installed on your local machine to continue with the rest of the chapter and the book.</p><p>Before we <a id="id32" class="indexterm"/>start to look at the individual tools, we need to configure the Docker agent. To do this, run the <strong>Docker Quickstart Terminal</strong> application. If you have multiple terminal emulators installed, it will pop up a prompt asking you which one you would like to use; I prefer to use the one that ships with Mac OS X, so I chose Terminal.</p><p>Once you have made your selection, a new terminal window will open and the application will configure your local installation of Docker for you:</p><div><img src="img/B05468_02_09.jpg" alt="Installing Docker Toolbox"/></div><p>In my case, I got<a id="id33" class="indexterm"/> the preceding terminal output when launching the <strong>Docker Quickstart Terminal</strong> application.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Docker Machine</h1></div></div></div><p>So, when<a id="id34" class="indexterm"/> you ran the <strong>Docker Quickstart Terminal</strong> application, it created a bunch of certificates, SSH keys, and configured your user's environment to run Docker. It also launched a virtual machine running Docker.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Developing locally</h2></div></div></div><p>The <strong>Docker Quickstart Terminal</strong> application did this using Docker machine, you can check <a id="id35" class="indexterm"/>the status of the machine launched by the application by running the following command:</p><div><pre class="programlisting">
<strong>docker-machine active</strong>
</pre></div><p>This will list the names of any active machines, the default machine launched when you first install Docker is called <code class="literal">default</code>, if you run:</p><div><pre class="programlisting">
<strong>docker-machine status default</strong>
</pre></div><p>It should tell you that the virtual machine is currently running. Finally, you should be able to SSH into the virtual machine by running the following command:</p><div><pre class="programlisting">
<strong>docker-machine ssh default</strong>
</pre></div><p>You will notice that when you SSH into the virtual machine, it is running the Boot2Docker distribution.</p><div><div><h3 class="title"><a id="note02"/>Note</h3><p>Boot2Docker <a id="id36" class="indexterm"/>is an extremely lightweight Linux distribution based on Tiny Core Linux, and its one purpose is to run Docker. Due to this, the entire distribution comes in at less than 30 MB, and it boots in around five seconds, which makes it perfect for running local development machines. For more information on Boot2Docker, refer to <a class="ulink" href="http://boot2docker.io/">http://boot2docker.io/</a>, and for Tiny Core Linux, refer to <a class="ulink" href="http://tinycorelinux.net/">http://tinycorelinux.net/</a>.</p></div></div><p>You should something similar to the following terminal session when running these commands:</p><div><img src="img/B05468_02_10.jpg" alt="Developing locally"/></div><p>There isn't <a id="id37" class="indexterm"/>much need to SSH into the virtual machine, though, as the Docker client that was installed by toolbox has been configured to connect to the Docker Engine on the virtual machine, this means that when you run the Docker commands locally, it passes all the calls through Docker on the virtual machine, try running the <code class="literal">hello-world</code> container:</p><div><pre class="programlisting">
<strong>docker run hello-world</strong>
</pre></div><p>You should see the following output:</p><div><img src="img/B05468_02_10.jpg" alt="Developing locally"/></div><p>At this<a id="id38" class="indexterm"/> stage, you may be thinking to yourself, this all is very good, but it's hardly a tool to get excited about. Well, you are wrong. Docker Machine has a few more tricks up its sleeve than being able to launch a Boot2Docker virtual machine locally.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec14"/>Heading into the cloud</h2></div></div></div><p>Docker Machine <a id="id39" class="indexterm"/>is able to connect to the following services, provision an instance, and configure your local Docker client to be able to communicate to the cloud-based instance.</p><p>The public cloud providers that currently are supported are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Amazon Web Services (AWS)</strong>: <a class="ulink" href="https://aws.amazon.com/">https://aws.amazon.com/</a></li><li class="listitem" style="list-style-type: disc"><strong>DigitalOcean</strong>: <a class="ulink" href="https://www.digitalocean.com/">https://www.digitalocean.com/</a></li><li class="listitem" style="list-style-type: disc"><strong>Microsoft Azure</strong>: <a class="ulink" href="https://azure.microsoft.com/">https://azure.microsoft.com/</a></li><li class="listitem" style="list-style-type: disc"><strong>Google Compute Engine</strong>: <a class="ulink" href="https://cloud.google.com/compute/">https://cloud.google.com/compute/</a></li><li class="listitem" style="list-style-type: disc"><strong>Rackspace</strong>: <a class="ulink" href="http://www.rackspace.co.uk/cloud/">http://www.rackspace.co.uk/cloud/</a></li><li class="listitem" style="list-style-type: disc"><strong>IBM SoftLayer</strong>: <a class="ulink" href="http://www.softlayer.com">http://www.softlayer.com</a></li><li class="listitem" style="list-style-type: disc"><strong>Exoscale</strong>: <a class="ulink" href="https://www.exoscale.ch/">https://www.exoscale.ch/</a></li><li class="listitem" style="list-style-type: disc"><strong>VMware vCloud Air</strong>: <a class="ulink" href="http://vcloud.vmware.com/">http://vcloud.vmware.com/</a></li></ul></div><p>The following self-hosted platforms can also be used:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>OpenStack</strong>: <a class="ulink" href="https://www.openstack.org/">https://www.openstack.org/</a></li><li class="listitem" style="list-style-type: disc"><strong>Microsoft Hyper-V</strong>: <a class="ulink" href="http://www.microsoft.com/virtualization/">http://www.microsoft.com/virtualization/</a></li><li class="listitem" style="list-style-type: disc"><strong>VMware vSphere</strong>: <a class="ulink" href="http://www.vmware.com/uk/products/vsphere/">http://www.vmware.com/uk/products/vsphere/</a></li></ul></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec01"/>The DigitalOcean driver</h3></div></div></div><p>Let's <a id="id40" class="indexterm"/>start <a id="id41" class="indexterm"/>creating some instances in the cloud. First, let's launch a machine in DigitalOcean.</p><p>There are two prerequisites for launching an instance with Docker Machine in DigitalOcean, the first is a DigitalOcean account and the second is an API token.</p><p>To sign up for a <a id="id42" class="indexterm"/>DigitalOcean account, visit <a class="ulink" href="https://www.digitalocean.com/">https://www.digitalocean.com/</a> and click on the <strong>Sign Up</strong> button. Once you have logged in to your account, you can generate an API token by clicking on the <strong>API</strong> link in the top menu.</p><p>To grab your token, click on the <strong>Generate New Token</strong> button and follow the on-screen instructions:</p><div><div><h3 class="title"><a id="tip03"/>Tip</h3><p>You only get one chance to make a record of your token, make sure that you store it somewhere safe, as it will allow anyone who has it to launch instances into your account.</p></div></div><div><img src="img/B05468_02_12.jpg" alt="The DigitalOcean driver"/></div><p>Once you have <a id="id43" class="indexterm"/>the token, you can launch your instance using Docker Machine. To do this, run the following command; make sure to replace the example API token with your own:</p><div><div><h3 class="title"><a id="tip04"/>Tip</h3><p>Using a backslash: As we have a lot options to pass to the <code class="literal">docker-machine</code> command, we are using \ to split the command over multiple lines so that it's easier to follow what is going on.</p></div></div><div><pre class="programlisting"> docker-machine create \
    --driver digitalocean \
    --digitalocean-access-token sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0 \
    dotest</pre></div><p>This will launch a <code class="literal">dotest</code> instance into your DigitalOcean account, you will see something similar to the following terminal output:</p><div><img src="img/B05468_02_13.jpg" alt="The DigitalOcean driver"/></div><p>If you check your DigitalOcean control panel, you will now see that the instance that was created by Docker Machine is listed here:</p><div><img src="img/B05468_02_14.jpg" alt="The DigitalOcean driver"/></div><p>Now we have two <a id="id44" class="indexterm"/>instances launched by Docker Machine, one running locally running on our machine called <code class="literal">default</code> and one hosted in DigitalOcean called <code class="literal">dotest</code>. We can confirm this by running the following command:</p><div><pre class="programlisting">
<strong>docker-machine ls</strong>
</pre></div><p>This will return all of the machines we have running and confirm their state, IP address, Docker version, and name. There is also a column that allows you to know which of the two machines your local environment is configured to communicate with:</p><div><img src="img/B05468_02_15.jpg" alt="The DigitalOcean driver"/></div><p>In the preceding example, our local Docker client is configured to communicate with the <code class="literal">default</code> instance, which is the run running locally. Let's change it so that it interacts with the DigitalOcean instance.</p><p>To do this, you <a id="id45" class="indexterm"/>have change some local environment variables, luckily, Docker Machine provides an easy way to find out what these are and also change them.</p><p>To find out what they all you have to do is simple, run the following command:</p><div><pre class="programlisting">
<strong>docker-machine env dotest</strong>
</pre></div><p>This will tell you exactly what you need to run to change from the <code class="literal">default</code> machine to <code class="literal">dotest</code>. The best thing is that the command itself formats the results in such a way that they can be executed, so we run the command again, but this time in a way where the output will be executed:</p><div><pre class="programlisting">
<strong>eval $(docker-machine env dotest)</strong>
</pre></div><p>Now if you get a listing from Docker Machine, you will notice that the <code class="literal">dotest</code> environment is now the active one:</p><div><img src="img/B05468_02_16.jpg" alt="The DigitalOcean driver"/></div><p>Now that we have our DigitalOcean instance active, you can run the <code class="literal">docker</code> command on your local machine, and they will have been executed on the DigitalOcean instance. Let's test this by running the hello-world container.</p><p>If you run the following command, you should see the image download and then the output of running the hello-world container:</p><div><pre class="programlisting">
<strong>docker run hello-world</strong>
</pre></div><p>If you then run the following command, you will see that the hello-world image exited a few seconds ago:</p><div><pre class="programlisting">
<strong>docker ps –a</strong>
</pre></div><p>This is demonstrated by the following Terminal output:</p><div><img src="img/B05468_02_17.jpg" alt="The DigitalOcean driver"/></div><p>As you can see, I <a id="id46" class="indexterm"/>used <code class="literal">ssh</code> to get into the DigitalOcean instance and ran the <code class="literal">docker ps –a</code> and <code class="literal">docker images</code> commands to demonstrate that the commands I ran locally were executed on the DigitalOcean instance; however, the beauty of this setup is that you shouldn't have to SSH instance often.</p><p>One thing you may have noticed is that all we told Docker Machine is that we want to use DigitalOcean and our API token; at no point did we tell it which region to launch the instance in, what specification we wanted, or which SSH key to use.</p><p>Docker Machine has some following sensible defaults:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">digitalocean-image = ubuntu-15-10-x64</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">digitalocean-region = nyc3</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">digitalocean-size = 512mb</code></li></ul></div><p>As I am based in the UK, let's look at changing the region and the specifications of the machine. First of all, we should remove the <code class="literal">dotest</code> instance by running the following command:</p><div><pre class="programlisting">
<strong>docker-machine rm dotest</strong>
</pre></div><p>This will terminate the <code class="literal">512mb</code> instance running in NYC3.</p><div><div><h3 class="title"><a id="tip05"/>Tip</h3><p>It is important to terminate instances that you are not using, as they will be costing you for each hour they are active. Remember one of the key advantages of using Docker Machine is that you can spin up instances both quickly and with as little interaction as possible.</p></div></div><p>Now that we have <a id="id47" class="indexterm"/>removed the old instance, let's add some additional flags to our <code class="literal">docker-machine</code> command to launch the new instance in the desired region and specification, we will be calling our new instance <code class="literal">douktest</code>. The updated <code class="literal">docker-machine create</code> command now looks similar to the following (remember to replace the example API token with your own):</p><div><pre class="programlisting">docker-machine create \
    --driver digitalocean \
    --digitalocean-access-token sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0 \
    --digitalocean-region lon1 \
    --digitalocean-size 1gb \
    douktest</pre></div><p>You should see similar output from the command as before, once the instance has been deployed, you can make it active by running:</p><div><pre class="programlisting">
<strong>eval $(docker-machine env douktest)</strong>
</pre></div><div><img src="img/B05468_02_18.jpg" alt="The DigitalOcean driver"/></div><p>When you enter the <a id="id48" class="indexterm"/>control panel, you will notice that the instance has launched in the specified region and at the desired specification:</p><div><img src="img/B05468_02_19.jpg" alt="The DigitalOcean driver"/></div><p>For full details on each of the regions and what machine types are available in each one, you can query the DigitalOcean API by running the following command (remember to replace the API token):</p><div><pre class="programlisting">
<strong>curl -X GET -H "Content-Type: application/json" -H "Authorization: Bearer sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0" "https://api.digitalocean.com/v2/regions" | python -mjson.tool</strong>
</pre></div><p>This will output information about each region.</p><p>One last thing, we still haven't found out about the SSH key. Each time you run Docker Machine, a new SSH key for the instance you are launching is created and uploaded to the provider, each key is stored in the <code class="literal">.docker</code> folder in your user's home directory. For example, the key for <code class="literal">douktest</code> can be found by running the following command:</p><div><pre class="programlisting">
<strong>cd ~/.docker/machine/machines/douktest/</strong>
</pre></div><p>Here, you will also find the certificates used to authenticate the Docker agent with the Docker installation <a id="id49" class="indexterm"/>on the instance and also the configuration:</p><div><img src="img/B05468_02_20.jpg" alt="The DigitalOcean driver"/></div><p>This covers DigitalOcean, what about other services? Let's quickly look at Amazon Web Services so that we can get an idea between the drivers for the different cloud providers.</p></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec02"/>The Amazon Web Services driver</h3></div></div></div><p>If <a id="id50" class="indexterm"/>you don't already have an Amazon <a id="id51" class="indexterm"/>Web Services account, you<a id="id52" class="indexterm"/> should sign up for one at <a class="ulink" href="http://aws.amazon.com/">http://aws.amazon.com/</a>. If you are new to AWS, then you will eligible for their free tier at <a class="ulink" href="http://aws.amazon.com/free/">http://aws.amazon.com/free/</a>.</p><p>I would recommend reading through Amazon's getting started guide if you are unfamiliar with AWS before working through this section of the chapter, you can find the guide at <a class="ulink" href="http://docs.aws.amazon.com/gettingstarted/latest/awsgsg-intro/gsg-aws-intro.html">http://docs.aws.amazon.com/gettingstarted/latest/awsgsg-intro/gsg-aws-intro.html</a>.</p><p>The AWS driver is similar to the DigitalOcean driver and it has some sensible defaults, rather than going into too much detail about how to customize the EC2 instance launched by Docker Machine, I will stick to the defaults. For AWS driver, the defaults are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">amazonec2-region = us-east-1 (North Virginia)</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">amazonec2-ami = ami-26d5af4c (Ubuntu 15.10)</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">amazonec2-instance-type = t2.micro</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">amazonec2-root-size = 16GB</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">amazonec2-security-group = docker-machine</code></li></ul></div><p>Before we launch our instance, we will also need to know our AWS access and secret keys, and also the VPC ID will be launching our instance. To get these, log in to the AWS console that can be found at <a class="ulink" href="https://console.aws.amazon.com/">https://console.aws.amazon.com/</a>.</p><p>You should already have a copy of your access and secret ID as these are created when your user was first created in AWS. If you have lost these, then you can generate a new pair by navigating to <strong>Services</strong> | <strong>IAM</strong> | <strong>Users</strong>, then selecting your user, and finally going to the <strong>Security Credentials</strong> tab. There you should see a button that says <strong>Create Access Key</strong>.</p><div><div><h3 class="title"><a id="note03"/>Note</h3><p>Amazon describes Amazon <strong>Virtual Private Cloud</strong> (<strong>VPC</strong>)<a id="id53" class="indexterm"/> as letting you provision a logically-isolated section of the AWS cloud, where you can launch resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways.</p></div></div><p>Before you find <a id="id54" class="indexterm"/>your VPC ID, you should make sure that you are in the correct region by ensuring that it says <strong>N. Virginia</strong> at the top right-hand corner of your AWS console, if it doesn't select it from the drop-down list.</p><p>Once you have ensured you are in the correct region, go to <strong>Services</strong> | <strong>VPC</strong> and click on <strong>Your VPCs</strong>. You don't need to worry about creating and configuring a VPC as Amazon provides you with a default VPC in each region. Select the VPC and you should see the something similar to the following screenshot:</p><div><img src="img/B05468_02_21.jpg" alt="The Amazon Web Services driver"/></div><p>Make a note of the VPC ID, you should now have enough information to launch your instance using Docker Machine. To do this, run the following command:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    --driver amazonec2 \</strong>
<strong>    --amazonec2-access-key JHFDIGJKBDS8639FJHDS \</strong>
<strong>    --amazonec2-secret-key sfvjbkdsvBKHDJBDFjbfsdvlkb+JLN873JKFLSJH \</strong>
<strong>    --amazonec2-vpc-id vpc-35c91750 \</strong>
<strong>    awstest</strong>
</pre></div><p>If all goes well, you<a id="id55" class="indexterm"/> should see something similar to the following output:</p><div><img src="img/B05468_02_22.jpg" alt="The Amazon Web Services driver"/></div><p>You should also be able to see an EC2 instance launched in the AWS Console by navigating to <strong>Services</strong> | <strong>EC2</strong> | <strong>Instances</strong>:</p><div><img src="img/B05468_02_23.jpg" alt="The Amazon Web Services driver"/></div><p>You may have <a id="id56" class="indexterm"/>noticed that Docker Machine created the security group and also assigned an SSH key to the instance without any need for us to get involved, keeping within the principle that you don't need to be an expert in configuring the environments that you are launching your Docker instance into.</p><p>Before we terminate the instance, let's switch our local Docker client over to use the AWS instance and launch the <code class="literal">Hello World</code> container:</p><div><img src="img/B05468_02_24.jpg" alt="The Amazon Web Services driver"/></div><p>As you can see, once<a id="id57" class="indexterm"/> you have launched an instance using Docker Machine and switched your local Docker client to it, there is no difference in usage between running Docker locally and on a cloud provider.</p><p>Before we start to rack up the cost, we should terminate our test AWS instance by running the following command:</p><div><pre class="programlisting">
<strong>docker-machine rm awstest</strong>
</pre></div><p>Then confirm that the instance has been terminated correctly in the AWS console:</p><div><img src="img/B05468_02_25.jpg" alt="The Amazon Web Services driver"/></div><p>If you don't do this, the <a id="id58" class="indexterm"/>EC2 instance will quite happily sit there costing you <strong>$0.013</strong> per hour until it is terminated.</p><div><div><h3 class="title"><a id="note04"/>Note</h3><p>Note that this is not Amazon's <strong>Elastic Container Service</strong> (<strong>ECS</strong>). We will be covering Amazon ECS in <a class="link" href="ch07.html" title="Chapter 7. Looking at Schedulers">Chapter 7</a>, <em>Looking at Schedulers</em>.</p></div></div></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec15"/>Other considerations</h2></div></div></div><p>As you <a id="id59" class="indexterm"/>can see from examples we have worked through, Docker Machine is a powerful part of Docker Toolbox as it allows users of all skill levels to be able to launch an instance either locally or in a cloud provider without having to roll their sleeves up and get stuck in configuring server instances or their local Docker client.</p><p>The examples we have used in this chapter have been launching either Boot2Docker or Ubuntu. Docker machine also supports the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Debian (8.0+)</strong>: <a class="ulink" href="https://www.debian.org/">https://www.debian.org/</a></li><li class="listitem" style="list-style-type: disc"><strong>Red Hat Enterprise Linux (7.0+)</strong>: <a class="ulink" href="https://www.redhat.com/">https://www.redhat.com/</a></li><li class="listitem" style="list-style-type: disc"><strong>CentOS (7+)</strong>: <a class="ulink" href="https://www.centos.org/">https://www.centos.org/</a></li><li class="listitem" style="list-style-type: disc"><strong>Fedora (21+)</strong>: <a class="ulink" href="https://getfedora.org/">https://getfedora.org/</a></li><li class="listitem" style="list-style-type: disc"><strong>RancherOS (0.3)</strong>: <a class="ulink" href="http://rancher.com/rancher-os/">http://rancher.com/rancher-os/</a></li></ul></div><p>The other thing to mention about Docker Machine is that, by default, it operates and opts in for crash reporting, considering the amount of different configuration/environment combinations Docker Machine can be used with, it is important that Docker get notified of any problems to help them make a better product. If, for any reason, you want to opt-out, then running the following command will disable crash reporting:</p><div><pre class="programlisting">
<strong>mkdir -p ~/.docker/machine &amp;&amp; touch ~/.docker/machine/no-error-report</strong>
</pre></div><p>For more information on Docker Machine, you can refer to the official documentation:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Docker Machine</strong>: <a class="ulink" href="https://docs.docker.com/machine/">https://docs.docker.com/machine/</a></li><li class="listitem" style="list-style-type: disc"><strong>Docker Machine Drivers</strong>: <a class="ulink" href="https://docs.docker.com/machine/drivers/">https://docs.docker.com/machine/drivers/</a></li><li class="listitem" style="list-style-type: disc"><strong>Docker Machine Command Reference</strong>: <a class="ulink" href="https://docs.docker.com/machine/reference/">https://docs.docker.com/machine/reference/</a></li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Docker Swarm</h1></div></div></div><p>Now that <a id="id60" class="indexterm"/>we have discussed how to launch individual Docker instances <a id="id61" class="indexterm"/>using Docker Machine, let's get a little more adventurous and create a cluster of instances. To do this, Docker ships a tool called Swarm. When deployed, it acts as a scheduler between your Docker client and host Docker instances, deciding where to launch containers based on scheduling rules.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec16"/>Creating a local cluster</h2></div></div></div><p>To start off, we <a id="id62" class="indexterm"/>are going to be using Docker Machine to create a cluster locally <a id="id63" class="indexterm"/>using VirtualBox (<a class="ulink" href="https://www.virtualbox.org">https://www.virtualbox.org</a>), which is bundled with Docker Toolbox. To start, we are going to launch a VM to generate a discovery token. To do this, run the following commands:</p><div><pre class="programlisting">
<strong>docker-machine create -d virtualbox discover</strong>
</pre></div><p>Then configure your Docker client to use the newly created local instance:</p><div><pre class="programlisting">
<strong>eval "$(docker-machine env discover)"</strong>
</pre></div><p>You can check that your Docker client is configured to use the <code class="literal">discover</code> instance by running <code class="literal">docker-machine ls</code> and making sure that <code class="literal">discover</code> has a star in the active column.</p><p>Finally, you can install the discovery service by running the following command:</p><div><pre class="programlisting">
<strong>docker run swarm create</strong>
</pre></div><p>This will download and run the discovery service and generate the token. At the end of the process, you will be given a token; it is important that you keep a note of this for the next steps. If everything went as planned, you should see something similar to the following output:</p><div><img src="img/B05468_02_26.jpg" alt="Creating a local cluster"/></div><p>In the preceding example, the token is <code class="literal">40c3bf4866eed5ad14ade6633fc4cefc</code>. Now that we have <a id="id64" class="indexterm"/>our token, we need to launch an instance that will act as the scheduler, this is know as a Swarm manager.</p><p>To do this, enter the following command, making sure that you replace the token with the one you generated:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    -d virtualbox \</strong>
<strong>    --swarm \</strong>
<strong>    --swarm-master \</strong>
<strong>    --swarm-discovery token://40c3bf4866eed5ad14ade6633fc4cefc \</strong>
<strong>    swarm-master</strong>
</pre></div><p>Now that we have the Swarm manager VM up and running, we can start launching VMs that act as nodes within the cluster. Again, using the discovery token, run the following commands to launch two nodes:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    -d virtualbox \</strong>
<strong>    --swarm \</strong>
<strong>    --swarm-discovery token://40c3bf4866eed5ad14ade6633fc4cefc \</strong>
<strong>    swarm-node-01</strong>
</pre></div><p>Then launch the second node using the following command:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    -d virtualbox \</strong>
<strong>    --swarm \</strong>
<strong>    --swarm-discovery token://40c3bf4866eed5ad14ade6633fc4cefc \</strong>
<strong>    swarm-node-02</strong>
</pre></div><p>We can check <a id="id65" class="indexterm"/>our VMs by running the <code class="literal">docker-machine ls</code> command and then switch our Docker client to use the cluster by running the following command:</p><div><pre class="programlisting">
<strong>eval $(docker-machine env --swarm swarm-master)</strong>
</pre></div><p>Now that your Docker client is communicating with the cluster, you can run <code class="literal">docker info</code> to find information about all the nodes and the cluster itself, you will see something similar to the following screenshot:</p><div><img src="img/B05468_02_27.jpg" alt="Creating a local cluster"/></div><p>So, now we have a three CPU, 3-GB cluster running over three nodes. To test it, let's run the <code class="literal">Hello World</code> container and then run <code class="literal">docker ps -a</code> so that we can see which node the container launched on:</p><div><img src="img/B05468_02_28.jpg" alt="Creating a local cluster"/></div><p>As you can see<a id="id66" class="indexterm"/> from the terminal output, the container was launched on <code class="literal">swarm-node-01</code>, running the container again should launch it on our second node:</p><div><img src="img/B05468_02_29.jpg" alt="Creating a local cluster"/></div><p>So there you have it, a really basic Docker Swarm cluster that you can launch your containers into using your local Docker client, all launched a managed using Docker Machine.</p><p>Before we move onto the next section, we should remove the local cluster. To do this, just run the following command:</p><div><pre class="programlisting">
<strong>docker-machine rm discover swarm-master swarm-node-01 swarm-node-02</strong>
</pre></div><p>Click on <code class="literal">yes</code> when prompted. You can then check whether the VMs have been terminated by running the <code class="literal">docker-machine ls</code> command.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Creating a Remote Cluster</h2></div></div></div><p>Before we move <a id="id67" class="indexterm"/>onto looking at the next tool, let's launch a cluster in the cloud. I am going to be using DigitalOcean for this.</p><p>First of all, let's create a new discovery token. As all we need to do is generate a discovery token, there is no need to launch an instance in DigitalOcean just for this task, so we will simply bring up <a id="id68" class="indexterm"/>a machine locally, make a note of the discovery token and then remove it:</p><div><pre class="programlisting">
<strong>docker-machine create -d virtualbox token</strong>
<strong>eval "$(docker-machine env token)"</strong>
<strong>docker run swarm create</strong>
<strong>docker-machine rm token</strong>
</pre></div><p>Now that we have our discovery token, let's launch our Swarm cluster in DigitalOcean, first of all we will look into Swarm manager:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    --driver digitalocean \</strong>
<strong>    --digitalocean-access-token sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0 \</strong>
<strong>    --digitalocean-region lon1 \</strong>
<strong>    --swarm \</strong>
<strong>    --swarm-master \</strong>
<strong>    --swarm-discovery token://453sdfjbnfvlknmn3435mwedvmndvnwe \</strong>
<strong>    swarm-master</strong>
</pre></div><p>Then the we will use the two nodes:</p><div><pre class="programlisting">
<strong>docker-machine create \</strong>
<strong>    --driver digitalocean \</strong>
<strong>    --digitalocean-access-token sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0 \</strong>
<strong>    --digitalocean-region lon1 \</strong>
<strong>    --digitalocean-size 1gb \</strong>
<strong>    --swarm \</strong>
<strong>    --swarm-discovery token://453sdfjbnfvlknmn3435mwedvmndvnwe \</strong>
<strong>    swarm-node-01</strong>

<strong>docker-machine create \</strong>
<strong>    --driver digitalocean \</strong>
<strong>    --digitalocean-access-token sdnjkjdfgkjb345kjdgljknqwetkjwhgoih314rjkwergoiyu34rjkherglkhrg0 \</strong>
<strong>    --digitalocean-region lon1 \</strong>
<strong>    --digitalocean-size 1gb \</strong>
<strong>    --swarm \</strong>
<strong>    --swarm-discovery token://453sdfjbnfvlknmn3435mwedvmndvnwe \</strong>
<strong>    swarm-node-02</strong>
</pre></div><p>As you can see in the following screenshot, I launched the cluster in DigitalOcean's London datacenter and gave the two nodes additional resources:</p><div><img src="img/B05468_02_30.jpg" alt="Creating a Remote Cluster"/></div><p>We will <a id="id69" class="indexterm"/>configure our local Docker client to use the remote cluster using the following command:</p><div><pre class="programlisting">
<strong>eval $(docker-machine env --swarm swarm-master)</strong>
</pre></div><p>This will give us the following information:</p><div><img src="img/B05468_02_31.jpg" alt="Creating a Remote Cluster"/></div><p>We are going to be using this cluster for the next part of this chapter, so try to keep it running for now. If you can't, then you can remove the cluster by running the following command:</p><div><pre class="programlisting">
<strong>docker-machine rm swarm-master swarm-node-01 swarm-node-02</strong>
</pre></div><p>You should also<a id="id70" class="indexterm"/> double the DigitalOcean control panel to ensure that your instances have terminated correctly.</p><div><div><h3 class="title"><a id="note05"/>Note</h3><p>Remember that with public cloud services, you are paying for that you use, so if you have an instance sat powered on, even if it is an <code class="literal">errored</code> state, with Docker Machine, the meter is running and you will be incurring cost.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Discovery backends</h2></div></div></div><p>At this <a id="id71" class="indexterm"/>point, it is worth pointing out that Docker allows you to swap out the Discovery backends, at the moment we are using the default one which the Hosted Discovery with Docker Hub, which isn't recommend for production.</p><p>Swarm supports the following <a id="id72" class="indexterm"/>discovery services:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>etcd</strong>: <a class="ulink" href="https://coreos.com/etcd/">https://coreos.com/etcd/</a></li><li class="listitem" style="list-style-type: disc"><strong>Consul</strong>: <a class="ulink" href="https://www.consul.io/">https://www.consul.io/</a></li><li class="listitem" style="list-style-type: disc"><strong>ZooKeeper</strong>: <a class="ulink" href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></li></ul></div><p>For the time being, we are just going to be looking at the tools Docker provides rather than any third-party options, so we are going to stick to the default Discovery backend.</p><p>Unfortunately, the one thing that the default Discovery backend doesn't give you is high availability, this means that our Swarm manager is a single point of failure. For our needs, this isn't a problem; however, I would not recommend running this configuration in production.</p><p>For more information on the different <a id="id73" class="indexterm"/>discovery backends and high availability with Swarm, refer to the following URLs:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Discovery backends</strong>: <a class="ulink" href="https://docs.docker.com/swarm/discovery/">https://docs.docker.com/swarm/discovery/</a></li><li class="listitem" style="list-style-type: disc"><strong>Swarm High Availability</strong>: <a class="ulink" href="https://docs.docker.com/swarm/multi-manager-setup/">https://docs.docker.com/swarm/multi-manager-setup/</a></li></ul></div><p>We are going to be looking a lot more at schedulers in later chapters, so for now, let's move onto the final service installed by Docker Toolbox.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Docker Compose</h1></div></div></div><p>So far in our <a id="id74" class="indexterm"/>exploration of the tools that ship with Docker Toolbox, we have been using services which manage our Docker host machines, the final service that we are going to look at in this chapter deals with containers. I am sure that you will agree that so far the tools provided by Docker are quite intuitive, Docker Compose is no different. It start off life as third-party service <a id="id75" class="indexterm"/>called Fig and was written by Orchard Labs (the project's original website is still available at <a class="ulink" href="http://fig.sh/">http://fig.sh/</a>).</p><p>The original project's goal was the following:</p><div><blockquote class="blockquote"><p><em>"Provide fast, isolated development environments using Docker"</em></p></blockquote></div><p>Since Fig became part of Docker, they haven't strayed too far from the original goal:</p><div><blockquote class="blockquote"><p><em>"Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file to configure your application's services. Then, using a single command, you create and start all the services from your configuration."</em></p></blockquote></div><p>Before we start looking at Compose files and start containers up, let's think of why a tool such as Compose is useful.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Why Compose?</h2></div></div></div><p>Launching<a id="id76" class="indexterm"/> individual containers is as simple as running the following command:</p><div><pre class="programlisting">
<strong>docker run -i -t ubuntu /bin/bash</strong>
</pre></div><p>This will launch and then attach to an Ubuntu container. As we have already touched upon, there is a little more to it than just launching simple containers though. Docker is not here to replace virtual machines, it is here to run a single application.</p><p>This means that you shouldn't really run an entire LAMP stack in single container, instead, you should look at running Apache and PHP in one container, which is then linked with a second container running MySQL. You could take this further, running a NGINX container, a PHP-FPM container, and also a MySQL container. This is where it gets complicated. All of sudden, your simple line for launching is now several lines, all of which have to executed in the correct order with the correct flags.</p><p>This is exactly the problem Docker Compose tries to fix. Rather than several long commands, you can define your containers using a YAML file. This means that you will be able to launch your application with a single command and leave the logic of the order in which the containers will be launched to Compose.</p><div><div><h3 class="title"><a id="note06"/>Note</h3><p>
<strong>YAML Ain't Markup Language</strong> (<strong>YAML</strong>) is <a id="id77" class="indexterm"/>a human-friendly data serialization standard for all programming languages.</p></div></div><p>It also means <a id="id78" class="indexterm"/>that you can ship your application's Compose file with your code base or directly to another developer/administrator and they will be able to launch your application exactly how you intended it be executed.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Compose files</h2></div></div></div><p>Almost<a id="id79" class="indexterm"/> everyone at some point would have installed, used, or read about WordPress, so for the next few examples, we will be using the official WordPress container from the Docker Hub, you can find details on the container at <a class="ulink" href="https://hub.docker.com/_/wordpress/">https://hub.docker.com/_/wordpress/</a>.</p><div><div><h3 class="title"><a id="note07"/>Note</h3><p>WordPress is <a id="id80" class="indexterm"/>web software that you can use to create a beautiful website, blog, or app. We like to say that WordPress is both free and priceless at the same time. For <a id="id81" class="indexterm"/>more information, check out <a class="ulink" href="https://wordpress.org/">https://wordpress.org/</a>.</p></div></div><p>Let's start by getting a basic WordPress installation up and running, first of all create a folder called <code class="literal">wordpress</code> and then add the following content to a file called <code class="literal">docker-compose.yml</code>:</p><div><pre class="programlisting">wordpress:
  container_name: my-wordpress-app
  image: wordpress
  ports:
    - "80:80"  
  links:
    - "mysql:mysql"
mysql:
  container_name: my-wordpress-database
  image: mysql
  environment:
    MYSQL_ROOT_PASSWORD: "password"</pre></div><p>You will be able to launch the application using your Swarm cluster by making sure that your local Docker client is configured to use it, run <code class="literal">docker-machine ls</code> and make sure that it is active and then run the following command:</p><div><pre class="programlisting">
<strong>eval $(docker-machine env --swarm swarm-master)</strong>
</pre></div><p>Once your client is configured to communicate with your Swarm cluster, run the following command within the folder containing the <code class="literal">docker-compose.yml</code> file:</p><div><pre class="programlisting">
<strong>docker-compose up -d</strong>
</pre></div><p>Using the <code class="literal">-d</code> flag at the end of the command launches the containers in detached mode, this means that they will run in the background. If we didn't use the -d flag, then our containers <a id="id82" class="indexterm"/>would have launched in the foreground and we would not have been able to carry on using the same terminal session without stopping the running containers.</p><p>You will see something similar to the following output:</p><div><img src="img/B05468_02_32.jpg" alt="Compose files"/></div><p>As you can see, you can find out the IP address of the node where the WordPress application has been launched by running <code class="literal">docker ps</code>. If you were to go to the IP address shown in the figure, where <code class="literal">port 80</code> is listed, you will see a WordPress installation screen:</p><div><img src="img/B05468_02_33.jpg" alt="Compose files"/></div><p>One of the interesting things to note is that although the <code class="literal">my-wordpress-app</code> container was defined first in the <code class="literal">docker-compose.yml</code> file, Compose recognized that it was linked to the <code class="literal">my-wordpress-database</code> container and it launched that one first. Also, you may <a id="id83" class="indexterm"/>have noticed that the <code class="literal">wordpress:latest</code> and <code class="literal">mysql:latest</code> images were pulled down on all of the nodes in the Swarm cluster.</p><p>So, what of the <code class="literal">docker-compose.yml</code> file itself? Let's look at it again, but this time with some comments.</p><p>As far as Compose is concerned, our WordPress application is split into two applications, one called <strong>wordpress</strong> and another called <strong>mysql</strong>. Let's look at the <code class="literal">docker-compose.yml</code> file again:</p><div><pre class="programlisting">wordpress:
  container_name: my-wordpress-app
  image: wordpress
  ports:
    - "80:80"  
  links:
    - "mysql:mysql"
mysql:
  container_name: my-wordpress-database
  image: mysql
  environment:
    MYSQL_ROOT_PASSWORD: "password"</pre></div><p>At the top level, we have the application name. From here, we then start to define the configuration for the application by giving a key and value, making sure that you pay close attention to the indentation. I tend to use two spaces to make it clear that the indent is there, but not so much that it becomes unreadable.</p><p>The first key that we are defining is <code class="literal">container_name</code>, we don't have to do this as Compose will name our containers automatically, based on the name of the folder we are in and the application name. If we hadn't defined this key, then our containers would have been called <code class="literal">wordpress_wordpress_01</code> and <code class="literal">wordpress_mysql_01</code>.</p><p>The next key tells the application which <code class="literal">image</code> to use, this will pull the image straight from the Docker Hub.</p><p>Then we define the <code class="literal">ports</code>, not that we only define the ports for the <strong>wordpress</strong> application and not the <strong>mysql</strong> one. As we want our <strong>wordpress</strong> application to listen on port of the host machine, we have given 80:80. In this case, the first 80 is the <strong>host</strong> port and the second one is the <strong>container</strong> port that we want to expose.</p><p>Again, the next key is only used on the <strong>wordpress</strong> application, this defines the <code class="literal">links</code>. Links are used to link containers together, exposing, in this case, the <strong>mysql</strong> container to the <strong>wordpress</strong> container. This means that when the <strong>wordpress</strong> container is launched, it will know the IP address of the <strong>mysql</strong> container and only its ports will be exposed to the <strong>wordpress</strong> container.</p><p>The final key <a id="id84" class="indexterm"/>we are defining is <code class="literal">environment</code>, here are we passing further keys and values that will be set as environment variables on the containers when they launch.</p><p>A full break down of all of the keys available in compose files can be found in the official documentation at <a class="ulink" href="https://docs.docker.com/compose/compose-file/">https://docs.docker.com/compose/compose-file/</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Launching more</h2></div></div></div><p>One of<a id="id85" class="indexterm"/> the advantages of using Compose is that each of the environments it launches is isolated, let's launch another WordPress installation using the following <code class="literal">docker-compose.yml</code> file:</p><div><pre class="programlisting">wordpress:
  container_name: my-other-wordpress-app
  image: wordpress
  ports:
    - "80:80"  
  links:
    - "mysql:mysql"
mysql:
  container_name: my-other-wordpress-database
  image: mysql
  environment:
    MYSQL_ROOT_PASSWORD: "password"</pre></div><p>As you can see, other than the container names, it is exactly the same as the previous environment we launched:</p><div><img src="img/B05468_02_34.jpg" alt="Launching more"/></div><p>The other thing you will notice is that the <code class="literal">my-other-wordpress</code> containers launched on the second node in the cluster. At the moment, each Compose environment will launch on a single node. As we launch more, we will start to have to change port assignments as they will start to<a id="id86" class="indexterm"/> clash on the hosts (that is, you can't have two <code class="literal">port 80</code> assigned to a single host).</p><div><div><h3 class="title"><a id="note08"/>Note</h3><p>Don't forget to remove any cloud-based instances that you have launched by using the <code class="literal">docker-machine rm</code> command and also check your cloud provider's control panel to ensure that the instances have correctly terminated.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Summary</h1></div></div></div><p>In this chapter, we have covered the additional client tools provided by Docker to extend the functionality of your core Docker installation, all of the tools that we have looked at have been designed to slot into your workflow and be as simple as possible to use. In the later chapters, we will be looking at how to expand some of the core functionality of Docker using third-party services. When we do, we will revisit a few of the tools that we have been through in this chapter and look at how they add additional functionality to them.</p></div></body></html>