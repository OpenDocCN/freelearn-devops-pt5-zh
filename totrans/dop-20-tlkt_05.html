<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Implementation of the Deployment Pipeline &#x2013; Initial Stages"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Implementation of the Deployment Pipeline – Initial Stages</h1></div></div></div><p>Let us start with some basic (and minimum) steps of the continuous deployment pipeline. We'll check out the code, run pre-deployment tests and, if they are successful, build a container and push it to the Docker registry. With the container safely available in the registry, we'll switch to a different VM that will serve as an imitation of a production server, run the container and perform post-deployment tests to ensure that everything works as expected.</p><p>Those steps will cover the most basic flow of what could be considered the continuous deployment process. Later on, in the next chapters, once we are comfortable with the process we did so far, we'll go ever further. We'll explore all the steps required for our microservice to safely and reliably reach the production servers with zero-downtime, in a way that allows us to scale easily, with the ability to rollback, and so on.</p><div class="section" title="Spinning Up the Continuous Deployment Virtual Machine"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec15"/>Spinning Up the Continuous Deployment Virtual Machine</h1></div></div></div><p>We'll start by <a class="indexterm" id="id212"/>creating the continuous delivery server. We'll do that by creating a VM with Vagrant. While using VMs is useful as a mean to perform easy to follow exercises, in the real world scenario you should skip VM altogether and install everything directly on the server. Remember, containers are in many cases a better substitute for some of the things we are used to doing with VMs and using both, as we'll do throughout this book, is in most case only a waste of resources. With that being said, let us create the <code class="literal">cd</code> and <code class="literal">prod</code> VMs. We'll use the first one as a continuous deployment server and the second as an imitation of the production environment.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd ..</strong></span>
<span class="strong"><strong>git clone https://github.com/vfarcic/ms-lifecycle.git</strong></span>
<span class="strong"><strong>cd ms-lifecycle</strong></span>
<span class="strong"><strong>vagrant up cd</strong></span>
<span class="strong"><strong>vagrant ssh cd</strong></span>
</pre></div><p>We cloned the GitHub repository, brought up the <code class="literal">cd</code> virtual machine and entered it.</p><p>There are a few<a class="indexterm" id="id213"/> basic Vagrant operations you might need to know to follow this book. Specifically, how to stop and run the VM again. You never know when you might be left with an empty battery on your laptop or have a need to free your resources for some other tasks. I wouldn't like you to get into a situation where you are not able to follow the rest of the book just because you shut down your laptop and was not able to get back to the same situation you were before. Therefore, let's go through two basic operations; stopping the VM and bringing it up again with the provisioners.</p><p>If you want to stop this VM, all you have to do is run the <code class="literal">vagrant halt</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant halt</strong></span>
</pre></div><p>After this, VM will be stopped and your resources free for other things. Later on, you can start the VMs again with the <code class="literal">vagrant up</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd --provision</strong></span>
<span class="strong"><strong>vagrant ssh cd</strong></span>
</pre></div><p>The <code class="literal">--provision</code> flag will, among other things, make sure that all the containers we need are indeed up and running. The <code class="literal">prod</code> VM, unlike the <code class="literal">cd</code>, does not use any provisioning, so the <code class="literal">--provision</code> argument is not needed.</p></div></div>
<div class="section" title="Deployment Pipeline Steps"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec16"/>Deployment Pipeline Steps</h1></div></div></div><p>With the <a class="indexterm" id="id214"/>VM up and running (or soon to be), let us quickly go through the process. We should perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Check out the code</li><li class="listitem">Run pre-deployment tests</li><li class="listitem">Compile and/or package the code</li><li class="listitem">Build the container</li><li class="listitem">Push the container to the registry</li><li class="listitem">Deploy the container to the production server</li><li class="listitem">Integrate the container</li><li class="listitem">Run post-integration tests</li><li class="listitem">Push the tests container to the registry<div class="mediaobject"><img alt="Deployment Pipeline Steps" src="graphics/B04858_05_01.jpg"/><div class="caption"><p>Figure 5-1 – The Docker deployment pipeline process</p></div></div></li></ol></div><p>At the<a class="indexterm" id="id215"/> moment we'll limit ourselves to manual execution and once we're comfortable with the way things work we'll transfer our knowledge to one of the CI/CD tools.</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec33"/></h2></div></div></div><div class="section" title="Checking Out the Code"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec05"/>Checking Out the Code</h3></div></div></div><p>Checking out<a class="indexterm" id="id216"/> the code is easy, and we already did it a couple of times:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>git clone https://github.com/vfarcic/books-ms.git</strong></span>
<span class="strong"><strong>cd books</strong></span>
<span class="strong"><strong>-ms</strong></span>
</pre></div></div></div><div class="section" title="Running Pre – Deployment Tests, Compiling, and Packaging the Code"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec34"/>Running Pre – Deployment Tests, Compiling, and Packaging the Code</h2></div></div></div><p>With the<a class="indexterm" id="id217"/> code checked out, we should run<a class="indexterm" id="id218"/> all the tests that do not require the service to be <a class="indexterm" id="id219"/>deployed. We already did the procedure when we tried different things we could do in the development environment.</p><div class="informalexample"><pre class="programlisting">docker build \
    -f Dockerfile.test \
    -t 10.100.198.200:5000/books-ms-tests \
    .
docker-compose \
    -f docker-compose-dev.yml \
    run --rm tests
ll target/scala-2.10/</pre></div><p>First we built the tests container defined in the Dockerfile.test file and tagged it with the <code class="literal">-t</code> argument. The name (or tag) of the container is <code class="literal">10.100.198.200:5000/books-ms-tests</code>. That is the special syntax with the first part being the address of the local registry and the second part the actual name of the container. We'll discuss and use the Registry later on. For now, it's important to know that we use it to store and retrieve containers we're building.</p><p>The second command run all the pre-deployment tests and compiled the Scala code into a JAR file ready for the distribution. The third command is only for demonstration purposes so that you can confirm that the JAR file is indeed created and resides in the <code class="literal">scala-2.10</code> directory.</p><p>Keep in mind that the reason for such a long time it took to build the container is because of a lot of things had to be downloaded for the first time. Each consecutive build will be much faster.</p><p>All we did up<a class="indexterm" id="id220"/> to now was running<a class="indexterm" id="id221"/> different commands without trying to understand <a class="indexterm" id="id222"/>what is behind them. Please note that commands to build Docker containers can be repeated in case of a failure. For example, you might lose your internet connection and, in such a case, building container would fail. If you repeat the build command, Docker will continue from the images that failed.</p><p>I wanted you to get a feeling of how Docker works from the perspective of those who just use pre-made containers or Dockerfile definitions created by others. Let us change this rhythm and dive into Dockerfile that is used to define containers.</p></div><div class="section" title="Building Docker Containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec35"/>Building Docker Containers</h2></div></div></div><p>With all the<a class="indexterm" id="id223"/> tests passed and the JAR file <a class="indexterm" id="id224"/>created, we can build the container that we'll deploy to production later on. Before we do that, let us examine the Dockerfile that contains all the information Docker needs for building the container. Contents of the Dockerfile are as follows:</p><div class="informalexample"><pre class="programlisting">FROM debian:jessie
MAINTAINER Viktor Farcic "viktor@farcic.com"
RUN apt-get update &amp;&amp; \
    apt-get install -y --force-yes --no-install-recommends openjdk-7-jdk &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
ENV DB_DBNAME books
ENV DB_COLLECTION books
COPY run.sh /run.sh
RUN chmod +x /run.sh
COPY target/scala-2.10/books-ms-assembly-1.0.jar /bs.jar
COPY client/components /client/components
CMD ["/run.sh"]
EXPOSE 8080</pre></div><p>You can <a class="indexterm" id="id225"/>find the <span class="emphasis"><em>Dockerfile</em></span> file together with the rest of the <code class="literal">books-ms</code> code in the <a class="ulink" href="https://github.com/vfarcic/books-ms">https://github.com/vfarcic/books-ms</a> GitHub repository.</p><p>Let us go through it line by line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>FROM debian:jessie</strong></span>
</pre></div><p>The first line specifies which image should be used as the base of the container we're building. In our case, we are using <span class="emphasis"><em>Debian</em></span> (version <span class="emphasis"><em>Jessie</em></span>). That means that we should have most of the functionality we would get with Debian OS. However, that is not to say that the whole OS is downloaded when we pull this container. Remember, Docker is using host kernel so when we specify that container should use, for example, Debian as its base, we<a class="indexterm" id="id226"/> are only downloading<a class="indexterm" id="id227"/> image that has things specific to the OS we specified, like, for instance, packaging mechanism (<span class="emphasis"><em>apt</em></span> in the case of Debian). What are the differences between various base images? Why did we choose the <span class="emphasis"><em>debian</em></span> image to part from?</p><p>In most cases the best choice for a base image is one of the official Docker images. Since Docker itself maintains those, they tend to be better controlled than those created by the community. The choice of the exact image one should use depends on the needs. Debian is my preference in many cases. Besides my liking of Debian-based Linux distributions, it is relatively small (~125 MB) and still a full distribution with everything you might need from a Debian OS. On the other hand, you might be familiar with RPM packaging and prefer, for example, CentOS. Its size is around 175 MB (approximately 50 % bigger than Debian). There are, however, some other cases when size is of utmost importance. That is especially true for images that would serve as utilities that are run once in a while to perform some specific actions. In such cases, Alpine might be a good start. Its size is 5 MB making it minuscule. However, bear in mind that, due to its minimalistic approach, this image might be hard to reason with when more complicated commands are run on top of it. Finally, in many cases, you might want to use more specific images as a base of your containers. For example, if you need a container with MongoDB but have few specific actions to perform on its initialization, you should use the mongo image.</p><p>In systems that host many containers, the size of the base image is less important than how many different base images are used. Remember, each image is cached on the server and reused across all containers that use it. If all your containers are, for example, extending from the <a class="indexterm" id="id228"/>
<span class="strong"><strong>debian</strong></span> image, the same cached copy will be reused in all cases meaning that it will be downloaded only once.</p><p>What we use as a base image is a container like any other. That means that you can use your containers as a base for others. For example, you might have many cases with applications that require NodeJS in combination with Gulp and few scripts specific to your organization. This scenario would be a good candidate for a container that would be extended (through the <code class="literal">FROM</code> instruction) by others.</p><p>Let us move to the next instruction:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>MAINTAINER Viktor Farcic "viktor@farcic.com"</strong></span>
</pre></div><p>The maintainer is purely informational providing information about the author; a person who maintains the container. Not much to do here. Moving on:</p><div class="informalexample"><pre class="programlisting">RUN apt-get update &amp;&amp; \
    apt-get install -y --force-yes --no-install-recommends openjdk-7-jdk &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*</pre></div><p>The <code class="literal">RUN</code> instruction executes any set of commands that run in the same way as if those commands are <a class="indexterm" id="id229"/>run in the command prompt. You might have noticed that each but the last line in our example ends with <code class="literal">&amp;&amp; \</code>. We<a class="indexterm" id="id230"/> are joining several separate commands instead of running each of them as a separate RUN instruction. The same result (from the operational perspective) could be accomplished with the following:</p><div class="informalexample"><pre class="programlisting">RUN apt-get update
RUN apt-get install -y --force-yes --no-install-recommends openjdk-7-jdk
RUN apt-get clean
RUN rm -rf /var/lib/apt/lists/*</pre></div><p>That certainly looks cleaner and easier to maintain. However, it has its set of problems. One of them is that each instruction in the Dockerfile generates a separate image. A container is a collection of images stacked one on top of the other. Knowing that, last two <code class="literal">RUN</code> instructions (<code class="literal">clean</code> and <code class="literal">rm</code>) do not provide any value. Let's illustrate it by putting (invented numbers) of the size of each image. First two instructions (<code class="literal">apt-get update</code> and <code class="literal">apt-get install</code>) are adding packages (let's say 100 MB). The second two (<code class="literal">apt-get clean</code> and <code class="literal">rm</code>) are removing files (let's say 10 MB). While removal of files on a normal system does reduce the size of what we have stored on the HD, in the case of Docker containers it only removes things from the current image. Since each image is <span class="emphasis"><em>immutable</em></span>, previous two images continue to have the size of 100 MB thus not removing the overall size of the container even though files removed later on are not accessible within the container. The size of those four images continues being 100 MB. If we go back to the first example where all commands are executed within the same <code class="literal">RUN</code> instruction thus creating a single image, the size is smaller (100 MB - 10 MB = 90 MB).</p><p>The important thing to note is that the size is not the only important consideration and we should try to balance it with maintainability. <span class="emphasis"><em>Dockerfile</em></span> needs to be readable, easy to maintain and with a clear intention behind it. That means that in some cases the benefits of having one huge <code class="literal">RUN</code> instruction might not be the best option if that means that it will be hard to maintain it later on.</p><p>All that being said, the purpose of the RUN command in our example is to update the system with latest packages (<code class="literal">apt-get update</code>), install JDK 7 (<code class="literal">apt-get install</code>) and remove unnecessary files created during the process (<code class="literal">apt-get clean</code> and <code class="literal">rm</code>).</p><p>The next set of instructions provides the container with environment variables that can be changed at runtime:</p><div class="informalexample"><pre class="programlisting">ENV DB_DBNAME books
ENV DB_COLLECTION books</pre></div><p>In this particular case, we are declaring variables <code class="literal">DB_DBNAME</code> and <code class="literal">DB_COLLECTION</code> with default values. The code of the service uses those variables to create the connection to the <span class="emphasis"><em>Mongo DB</em></span>. If, for some reason, we'd like to change those values, we could set them when executing the <code class="literal">docker run</code> command (as we'll see later on throughout the book).</p><p>In the container <a class="indexterm" id="id231"/>world, we are discouraged from <a class="indexterm" id="id232"/>passing environment specific files to containers running on different servers. Ideally, we should run a container without any other external files. While that is in some cases impractical (as, for example, with <span class="emphasis"><em>nginx</em></span> that we'll use later on for reverse proxy), environment variables are a preferred way of passing environment specific information to the container at runtime.</p><p>Next, in our example, are a couple of <code class="literal">COPY</code> instructions:</p><div class="informalexample"><pre class="programlisting">COPY run.sh /run.sh
RUN chmod +x /run.sh
COPY target/scala-2.10/books-ms-assembly-1.0.jar /bs.jar
COPY client/components /client/components</pre></div><p>
<code class="literal">COPY</code> instruction is true to its name. It copies files from the host file system to the container we are building. It should be written in the <code class="literal">COPY &lt;source&gt;... &lt;destination&gt;</code> format. The <code class="literal">source</code> is relative to the location of the <span class="emphasis"><em>Dockerfile</em></span> and must be inside the context of the build. What the latter statement means is that you cannot copy files that are not inside the directory where <span class="emphasis"><em>Dockerfile</em></span> resides or one of its child directories. For example, <code class="literal">COPY ../something /something</code> is not allowed. The source can be a file or a whole directory and can accept wildcards matching the Go's <code class="literal">filepath.Match</code> rules. The destination can also be a file or a directory. Destination matches the type of the source. If the source is a file, destination will be a file as well. Same is true when the source is a directory. To force destination to be a directory, end it with a slash (<code class="literal">/</code>).</p><p>While we haven't used <code class="literal">ADD</code> in our example, it is worth noting that it is very similar to <code class="literal">COPY</code>. In most cases I encourage you to use <code class="literal">COPY</code> unless you need additional features that <code class="literal">ADD</code> provides (most notably <code class="literal">TAR</code> extraction and URL support).</p><p>In our example, we are copying <code class="literal">run.sh</code> and making it executable through the <code class="literal">chmod RUN</code> instruction. Next, we are copying the rest of the files (back-end <code class="literal">JAR</code> and front-end <span class="emphasis"><em>components</em></span>).</p><p>Let us go through the last two instructions from our Dockerfile.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>CMD ["/run.sh"]</strong></span>
<span class="strong"><strong>EXPOSE 8080</strong></span>
</pre></div><p>The <code class="literal">CMD</code> specifies the command that will be executed when the container starts. The format is [<code class="literal">executable</code>, <code class="literal">parameter1</code>, <code class="literal">parameter2</code> and so on]. In our case <code class="literal">/run.sh</code> will run without any parameters. At the moment, the script contains a single command <code class="literal">java -jar bs.jar</code> that will start the Scala/Spray server. Keep in mind that <code class="literal">CMD</code> provides only the default executor that can be easily overwritten when a container is run.</p><p>The <code class="literal">EXPOSE</code> instruction specifies which port inside the container will be available at runtime.</p><p>The example <span class="emphasis"><em>Dockerfile</em></span> we explained does not contain all the instructions we could use. Throughout this book, we'll work with a couple of others and get more familiar with the format. In the meantime, please visit the Dockerfile reference for more information.</p><p>Equipped with this knowledge, let us build the container. The command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker build -t 10.100.198.200:5000/books-ms .</strong></span>
</pre></div><p>Let us use the<a class="indexterm" id="id233"/> time it takes for this command<a class="indexterm" id="id234"/> run (the first build always takes longer than the others) and go through the arguments we used. The first argument is <code class="literal">build</code> used for building containers. Argument <code class="literal">-t</code> allows us to tag the container with a particular name. If you'd like to push this container to the public Hub, the tag would be using the <code class="literal">/</code> format. If you have the account on Docker Hub, the username is used to identify you and can be used later on to push the container making it available for pulling on any server connected to the internet. Since I'm not willing to share my password, we took a different approach and used<a class="indexterm" id="id235"/> the <span class="emphasis"><em>registry</em></span> IP and port instead of the Docker Hub username. That allows us to push it to the private registry instead. This alternative is usually better because it provides us with a complete control over our containers, tends to be faster over the local network and won't give CEO of your company a heart attack for sending your applications to the cloud. Finally, the last argument is a dot (<code class="literal">.</code>) specifying that the Dockerfile is located in the current directory.</p><p>One important thing left to discuss is the order of instructions in the <span class="emphasis"><em>Dockerfile</em></span>. On one hand, it needs to be in logical. We can not, for example, run an executable before installing it or, as in our example, change permissions of the <code class="literal">run.sh</code> file before we copy it. On the other hand, we need to take in account Docker caching. When a <code class="literal">docker build</code> command is run, Docker will go instruction by instruction and check whether some other build process already created the image. Once an instruction that will build a new image is found, Docker will build not only that instruction but of all those that follow. That means that, in most cases, <code class="literal">COPY</code> and <code class="literal">ADD</code> instructions should be placed near the bottom of the <span class="emphasis"><em>Dockerfile</em></span>. Even within a group of <code class="literal">COPY</code> and <code class="literal">ADD</code> instructions, we should make sure to place higher those files that are less likely to change. In our example, we're adding <code class="literal">run.sh</code> before the <code class="literal">JAR</code> file and front-end components since latter are likely to change with every build. If you execute the <code class="literal">docker build</code> command the second time you'll notice that Docker outputs <code class="literal">---&gt; Using cache</code> in all steps. Later on, when we change the source code, Docker will continue outputting <code class="literal">---&gt; Using cache</code> only until it gets to one of the last two <code class="literal">COPY</code> instructions (which one it will be, depends on whether we changed the <code class="literal">JAR</code> file or the front-end components).</p><p>We'll be using <a class="indexterm" id="id236"/>Docker commands a<a class="indexterm" id="id237"/> lot, and you'll have plenty opportunity to get more familiar with them. In the meantime, please visit the Using the command line page for more information.</p><p>Hopefully, by this time, the container is already built. If not, take a short break. We are about to run our newly built container.</p><div class="section" title="Running Containers"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec06"/>Running Containers</h3></div></div></div><p>Running containers is easy as long as you know which arguments to use. The container we just built<a class="indexterm" id="id238"/> can be run with the following commands:</p><div class="informalexample"><pre class="programlisting">docker run -d --name books-db mongo
docker run -d --name books-ms \
    -p 8080:8080 \
    --link books-db:db \
    10.100.198.200:5000/books-ms</pre></div><p>The first command started the database container required by our service. The argument <code class="literal">-d</code> allows us to run a container in detached mode, meaning that it will run in the background. The second one, <code class="literal">--name books-db</code>, gives the container a name. If not specified, Docker would assign a random one. Finally, the last argument is the name of the image we want to use. In our<a class="indexterm" id="id239"/> case, we're using <span class="emphasis"><em>mongo</em></span>, the official Docker MongoDB image.</p><p>This command shows one of very useful Docker features. Just as GitHub revolutionized the way we share code between different developers and projects, Docker Hub changed the way we <a class="indexterm" id="id240"/>deploy not only applications we are building but also those built by others. Please feel free to visit <a class="ulink" href="https://hub.docker.com/">https://hub.docker.com/</a> and search for your <a class="indexterm" id="id241"/>favorite application, service, or a database. Chances are you'll find not only one (often official docker container) but many others done by the community. Efficient usage of Docker is often a combination of running images built by yourself and those built by others. Even if no image serves your purpose, it is often a good idea to use existing one as a base image. For example, you might want MongoDB with <span class="emphasis"><em>replication set</em></span> enabled. The best way to obtain such an image would be to use <span class="emphasis"><em>mongo</em></span> as the <code class="literal">FROM</code> instruction in your <span class="emphasis"><em>Dockerfile</em></span> and add replication commands below it.</p><p>The second <code class="literal">docker run</code> is a little bit more complicated. Besides running in detached mode and giving it a name, it also exposes port 8080 and links with the <code class="literal">books-ms-db</code> container. Exposing port is easy. We can provide a single port, for example <code class="literal">-p 8080</code>. In such a case, Docker will expose its internal port <code class="literal">8080</code> as a random port. We'll use this approach later on when we start <a class="indexterm" id="id242"/>working with <span class="emphasis"><em>service discovery tools</em></span>. In this example, we used two ports separated by a colon (<code class="literal">-p 8080:8080</code>). With such argument, Docker exposed its internal port 8080 to 8080. The next argument we used is <code class="literal">--link books-db:db</code> and allows us to link two containers. In this example, the name of the container we want to link to is <span class="emphasis"><em>books-ms-db</em></span>. Inside the container, this link will be converted into environment <a class="indexterm" id="id243"/>variables. Let see how those variables look like.</p><p>We can enter the running container using the <code class="literal">exec</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker exec -it books-ms bash</strong></span>
<span class="strong"><strong>env | grep DB</strong></span>
<span class="strong"><strong>exit</strong></span>
</pre></div><p>Arguments <code class="literal">-it</code> tells Docker that we want this execution to be interactive and with a terminal. It is followed by the name of the running container. Finally, we are overwriting the default command specified as the <code class="literal">CMD</code> instruction in the <span class="emphasis"><em>Dockerfile</em></span> with <span class="emphasis"><em>bash</em></span>. In other words, we entered into the running container by<a class="indexterm" id="id244"/> running <span class="emphasis"><em>bash</em></span>. Once inside the container, we listed all environment variables and filtered them so that only those containing <code class="literal">DB</code> are output. When we run the container, we specified that it should link with <code class="literal">books-ms-db</code> as <code class="literal">db</code>. Since all environment variables are always in uppercase, Docker created quite a few of them with names starting with <code class="literal">DB</code>. The output of <code class="literal">env</code> was as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>DB_NAME=/books-ms/db</strong></span>
<span class="strong"><strong>DB_PORT_27017_TCP=tcp://172.17.0.5:27017</strong></span>
<span class="strong"><strong>DB_PORT=tcp://172.17.0.5:27017</strong></span>
<span class="strong"><strong>DB_ENV_MONGO_VERSION=3.0.5</strong></span>
<span class="strong"><strong>DB_PORT_27017_TCP_PORT=27017</strong></span>
<span class="strong"><strong>DB_ENV_MONGO_MAJOR=3.0</strong></span>
<span class="strong"><strong>DB_PORT_27017_TCP_PROTO=tcp</strong></span>
<span class="strong"><strong>DB_PORT_27017_TCP_ADDR=172.17.0.5</strong></span>
<span class="strong"><strong>DB_COLLECTION=books</strong></span>
<span class="strong"><strong>DB_DBNAME=books</strong></span>
</pre></div><p>All but the last two are a result of linking with the other container. We got the name of the link, TCP, port, and so on. The last two (<code class="literal">DB_COLLECTION</code> and <code class="literal">DB_DBNAME</code>) are not the result of linking but variables we defined inside the <span class="emphasis"><em>Dockerfile</em></span>.</p><p>Finally, we exited the container.</p><p>There are few more things we can do to ensure that everything is running correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps -a</strong></span>
<span class="strong"><strong>docker logs books-ms</strong></span>
</pre></div><p>The <code class="literal">ps -a</code> command listed all (<code class="literal">-a</code>) containers. This command should output both <code class="literal">books-ms</code> and <code class="literal">books-ms-db</code>. The <code class="literal">logs</code> command, as the name says, outputs logs of the container <code class="literal">books-ms</code>.</p><p>Even though it was very easy to run the Mongo DB and our container, <code class="literal">books-ms</code>, we are still required to remember all the arguments. Much easier way to accomplish the same result is with <a class="indexterm" id="id245"/>
<span class="strong"><strong>Docker Compose</strong></span>. Before we see it in action, let us remove the container we are running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker rm -f books-ms books-db</strong></span>
<span class="strong"><strong>docker ps -a</strong></span>
</pre></div><p>The first command (<code class="literal">rm</code>) removes all listed containers. The argument <code class="literal">-f</code> forces that removal. Without it, only stopped containers could be removed. The <code class="literal">rm</code> command combined with<a class="indexterm" id="id246"/> the <code class="literal">-f</code> argument is equivalent to stopping containers with the <code class="literal">stop</code> command and then removing them with <code class="literal">rm</code>.</p><p>Let us run the same two containers (<code class="literal">mongo</code> and <code class="literal">books-ms</code>) with <span class="strong"><strong>Docker Compose</strong></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose -f docker-compose-dev.yml up -d app</strong></span>
</pre></div><p>The output of the command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Creating booksms_db_1</strong></span>
<span class="strong"><strong>Creating booksms_app_1</strong></span>
</pre></div><p>This time, we run both containers with a single <code class="literal">docker-compose</code> command. The <code class="literal">-f</code> argument specifies the specification file we want to use. I tend to define all development configurations in <code class="literal">docker-compose-dev.yml</code> and production in the default <code class="literal">docker-compose.yml</code>. When default file name is used, there is no need for the <code class="literal">-f</code> argument. Next is the <code class="literal">up</code> command that brought up the <code class="literal">app</code> container in detached mode (<code class="literal">-d</code>).</p><p>Let's take a look at the contents of the <code class="literal">docker-compose-dev.yml</code> file:</p><div class="informalexample"><pre class="programlisting">app:
  image: 10.100.198.200:5000/books-ms
  ports:- 8080:8080
  links:- db:db
db: image: mongo
...</pre></div><p>The above output only displays the targets we are interested right now. There are others primarily dedicated to testing and compiling. We used them before when we set up the development environment. We'll use them again later on. For now, let us discuss the <code class="literal">app</code> and <code class="literal">db</code> targets. Their definition is very similar to Docker commands and arguments we already used and should be easy to understand. The interesting one is <code class="literal">links</code>. Unlike linking with manual commands where we need first to start the source container (in our case <code class="literal">mongo</code>) and then the one that links to it (<code class="literal">books-ms</code>), <code class="literal">docker-compose</code> will start all dependant containers automatically. We run the <code class="literal">app</code> target and Docker compose realized that it depends on the <code class="literal">db</code> target, so it started it first.</p><p>As before, we can verify that both containers are up and running. This time, we'll do it with the Docker Compose:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose ps</strong></span>
</pre></div><p>The output should be similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    Name               Command          State           Ports</strong></span>
<span class="strong"><strong>----------------------------------------------------------------------</strong></span>
<span class="strong"><strong>booksms_app_1   /run.sh                 Up      0.0.0.0:8080-&gt;8080/tcp</strong></span>
<span class="strong"><strong>booksms_db_1    /entrypoint.sh mongod   Up      27017/tcp</strong></span>
</pre></div><p>Docker Compose, by default, names running containers using the combination of the project name (which default to the name of the directory), the name of the target (<code class="literal">app</code>) and the instance number (<code class="literal">1</code>). Later on, we'll run multiple instances of the same container distributed across<a class="indexterm" id="id247"/> multiple servers, and you'll have the chance to see this number increase.</p><p>With both containers up and running, we can check the logs of the containers we run with Docker Compose.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose logs</strong></span>
</pre></div><p>Please note that Docker Compose logs are in the <code class="literal">follow</code> mode, and you need to press <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span> to stop it.</p><p>I prefer as much testing as possible to be automatic, but that subject is left for later chapters so a brief manual verification will have to do for now.</p><div class="informalexample"><pre class="programlisting">curl -H 'Content-Type: application/json' -X PUT -d \
  '{"_id": 1,"title": "My First Book","author": "John Doe","description": "Not a very good book"}' \
  http://localhost:8080/api/v1/books | jq '.'
curl -H 'Content-Type: application/json' -X PUT -d \
  '{"_id": 2,"title": "My Second Book","author": "John Doe","description": "Not a bad as the first book"}' \
  http://localhost:8080/api/v1/books | jq '.'
curl -H 'Content-Type: application/json' -X PUT -d \
  '{"_id": 3,"title": "My Third Book","author": "John Doe","description": "Failed writers club"}' \
  http://localhost:8080/api/v1/books | jq '.'
curl http://localhost:8080/api/v1/books | jq '.'
curl http://localhost:8080/api/v1/books/_id/1 | jq '.'</pre></div><p>For those unfamiliar with <code class="literal">curl</code>, it is a command line tool and library for transferring data with URL syntax. In our case, we're using it to send three <code class="literal">PUT</code> requests to the service that, in turn, stored data to the MongoDB. Last two commands invoked the service APIs to retrieve a list of all books, as well as data related to a particular book with the ID 1. With those manual verifications, we confirmed that the service works and can communicate with the database. Please note that we used <code class="literal">jq</code> to format JSON output.</p><p>Remember, this service also contains front-end Web components, but we won't try them out at this time. That is reserved for later, when we deploy this service to production together with the Web site that will import them.</p><p>Containers that<a class="indexterm" id="id248"/> we are running are misplaced. The VM that we're using is supposed to be dedicated to continuous deployment, and the containers that we built should run on a separate production server (or in our case a separate VM that should simulate such a server). Before we start deploying to production, we should go through <span class="emphasis"><em>configuration management</em></span> that will allow us not only to streamline the deployment but also to setup the servers. We already used <code class="literal">Ansible</code> to create the <code class="literal">cd</code> VM, but we haven't had time to explain how it works. Even worst, we are yet to make a choice which tool to use.</p><p>For now, let us stop and remove the <code class="literal">books-ms</code> container and its dependencies thus freeing the <code class="literal">cd</code> server to do what it was intended to do in the first place; enable continuous deployment pipeline.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docke</strong></span>
<span class="strong"><strong>r-compose stop</strong></span>
<span class="strong"><strong>docker-compose rm -f</strong></span>
</pre></div></div><div class="section" title="Pushing Containers to the Registry"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec07"/>Pushing Containers to the Registry</h3></div></div></div><p>Docker Registry<a class="indexterm" id="id249"/> can be used to store and retrieve containers. We<a class="indexterm" id="id250"/> already run it with the <code class="literal">cd</code> VM we created at the beginning of this chapter. With the <code class="literal">books-ms</code> built, we can push it to the registry. That will allow us to pull the container from any place that can access the <code class="literal">cd</code> server. Please run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker push 10.100.198.200:5000/books-ms</strong></span>
</pre></div><p>Earlier in this chapter, we built the container using the <code class="literal">10.100.198.200:5000/books-ms</code> tag. That was a special format used for pushing to private registries; <code class="literal">:/</code>. After the container has been tagged, we pushed it to the registry running on IP <code class="literal">10.100.198.200</code> and port <code class="literal">5000. 10.100.198.200</code> is the IP of our <code class="literal">cd</code> VM.</p><p>With the container safely stored to the registry, we can run it on any server. Soon, once we go through configuration management, we'll have additional servers where we'll run containers stored in this registry.</p><p>Let's finish this chapter by destroying all the VMs. The next chapter will create those we need. That way you can take a break before continuing our adventure or jump into any chapter without the fear that something will fail due to tasks we did before. Each chapter is fully autonomous. While you will benefit from the knowledge obtained from previous chapters, technically, each of them works on its own. Before we destroy everything we did, we'll push<a class="indexterm" id="id251"/> the tests container so that we do not have to<a class="indexterm" id="id252"/> re-built it again from scratch. Registry container has a volume that maps our host directory to the internal path where images are stored. That way, all pushed images are stored on the host (directory <code class="literal">registry</code>) and do not depend on the VM where it's running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker push 10.100.198.200:5000</strong></span>
<span class="strong"><strong>/books-ms-tests</strong></span>
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div></div><div class="section" title="The Checklist"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec08"/>The Checklist</h3></div></div></div><p>We are still a few steps short of the basic implementation of the deployment pipeline. As a reminder, the <a class="indexterm" id="id253"/>steps are following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Checkout the code - Done</li><li class="listitem">Run pre-deployment tests - Done</li><li class="listitem">Compile and/or package the code - Done</li><li class="listitem">Build the container - Done</li><li class="listitem">Push the container to the registry - Done</li><li class="listitem">Deploy the container to the production server - Pending</li><li class="listitem">Integrate the container - Pending</li><li class="listitem">Run post-deployment tests - Pending</li><li class="listitem">Push the tests container to the registry - Pending</li></ol></div><p>It is important to notice that all the steps we run by now were performed on the <code class="literal">cd</code> VM. We want to reduce the impact on the production environment as much as possible so we'll continue running steps (or part of them) outside the destination server as much as possible:</p><div class="mediaobject"><img alt="The Checklist" src="graphics/B04858_05_02.jpg"/><div class="caption"><p>Figure 5-2 – The initial stages of the deployment pipeline with Docker</p></div></div><p>We did the first<a class="indexterm" id="id254"/> five steps, or, at least, their manual version. The rest will have to wait until we set up our production server. In the next chapter, we'll discuss the options we have to accomplish this task.</p></div></div></div></body></html>