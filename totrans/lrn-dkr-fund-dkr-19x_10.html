<html><head></head><body>
        

                            
                    <h1 class="header-title">Advanced Docker Usage Scenarios</h1>
                
            
            
                
<p class="mce-root">In the last chapter, we showed you how you can use tools to perform administrative tasks without having to install those tools on the host computer. We also illustrated the use of containers that host and run test scripts or code used to test and validate application services running in containers. Finally, we guided you through the task of building a simple Docker-based CI/CD pipeline using Jenkins as the automation server.</p>
<p>In this chapter, we will introduce advanced tips, tricks, and concepts that are useful when containerizing complex distributed applications, or when using Docker to automate sophisticated tasks.</p>
<p>This is a quick overview of all of the subjects we are going to touch on in this chapter:</p>
<ul>
<li>All of the tips and tricks of a Docker pro</li>
<li>Running your Terminal in a remote container and accessing it via HTTPS</li>
<li>Running your development environment inside a container</li>
<li>Running your code editor in a remote container and accessing it via HTTPS</li>
</ul>
<p>After finishing this chapter, you will be able to do the following:</p>
<ul>
<li>Successfully restore your Docker environment after it has been messed up completely</li>
<li>Run a remote Terminal in a container and access it with your browser via HTTPS</li>
<li>Edit code remotely with Visual Studio Code with your browser via HTTPS</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>In this chapter, if you want to follow along with the code, you need Docker for Desktop on your Mac or Windows machine and the Visual Studio Code editor. The example will also work on a Linux machine with Docker and Visual Studio Code installed. Docker Toolbox is not supported in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">All of the tips and tricks of a Docker pro</h1>
                
            
            
                
<p>In this section, I will present a few very useful tips and tricks that make the lives of advanced Docker users so much easier. We will start with some guidance on how to keep your Docker environment clean.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Keeping your Docker environment clean</h1>
                
            
            
                
<p>First, we want to learn how we can delete dangling images. According to Docker, dangling images are layers that have no relationship to any tagged images. Such image layers are certainly useless to us and can quickly fill up our disk—it's better to remove them from time to time. Here is the command:</p>
<pre><strong>$</strong> <strong>docker image prune -f</strong></pre>
<p>Please note that I have added the <kbd>-f</kbd> parameter to the <kbd>prune</kbd> command. This is to prevent the CLI from asking for a confirmation that we really want to delete those superfluous layers.</p>
<p>Stopped containers can waste precious resources too. If you're sure that you don't need these containers anymore, then you should remove them, either individually with the following:</p>
<pre><strong>$ docker container rm &lt;container-id&gt;</strong></pre>
<p>Or, you can remove them as a batch with the following:</p>
<pre><strong>$ docker container prune --force</strong></pre>
<p>It is worth mentioning once again that, instead of <kbd>&lt;container-id&gt;</kbd>, we can also use <kbd>&lt;container-name&gt;</kbd> to identify a container.</p>
<p>Unused Docker volumes too can quickly fill up disk space. It is a good practice to tender your volumes, specifically in a development or CI environment where you create a lot of mostly temporary volumes. But I have to warn you, Docker volumes are meant to store data. Often, this data must live longer than the life cycle of a container. This is specifically true in a production or production-like environment where the data is often mission-critical. Hence, be 100% sure of what you're doing when using the following command to prune volumes on your Docker host:</p>
<pre><strong>$ docker volume prune</strong><br/>WARNING! This will remove all local volumes not used by at least one container.<br/>Are you sure you want to continue? [y/N]</pre>
<p>I recommend using this command without the <kbd>-f</kbd> (or <kbd>--force</kbd>) flag. It is a dangerous and terminal operation and it's better to give yourself a second chance to reconsider your action. Without the flag, the CLI outputs the warning you see in the preceding. You have to explicitly confirm by typing <kbd>y</kbd> and pressing the <em>Enter</em> key. </p>
<p>On production or production-like systems, you should abstain from the preceding command and rather delete unwanted volumes one at a time by using this command:</p>
<pre><strong>$ docker volume rm &lt;volume-name&gt;</strong></pre>
<p>I should also mention that there is a command to prune Docker networks. But since we have not yet officially introduced networks, I will defer this to <a href="f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml" target="_blank">chapter 10</a>, <em>Single-Host Networking</em>.</p>
<p>In the next section, we are going to show how we can automate Docker from within a container.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running Docker in Docker</h1>
                
            
            
                
<p>At times, we may want to run a container hosting an application that automates certain Docker tasks. How can we do that? The Docker Engine and the Docker CLI are installed on the host, yet the application runs inside the container. Well, from early on, Docker has provided a means to bind-mount Linux sockets from the host into the container. On Linux, sockets are used as very efficient data communications endpoints between processes that run on the same host. The Docker CLI uses a socket to communicate with the Docker Engine; it is often called the Docker socket. If we can give access to the Docker socket to an application running inside a container then we can just install the Docker CLI inside this container, and we will then be able to run an application in the same container that uses this locally installed Docker CLI to automate container-specific tasks.</p>
<p>It is important to note that here we are not talking about running the Docker Engine inside the container but rather only the Docker CLI and bind-mount the Docker socket from the host into the container so that the CLI can communicate with the Docker Engine running on the host computer. This is an important distinction. Running the Docker Engine inside a container, although possible, is not recommended.</p>
<p>Let's assume we have the following script, called <kbd>pipeline.sh</kbd>, automating the building, testing, and pushing of a Docker image:</p>
<pre>#! /bin/bash<br/># *** Sample script to build, test and push containerized Node.js applications ***<br/># build the Docker image<br/>docker image build -t $HUB_USER/$REPOSITORY:$TAG .<br/># Run all unit tests<br/>docker container run $HUB_USER/$REPOSITORY:$TAG npm test<br/># Login to Docker Hub<br/>docker login -u $HUB_USER -p $HUB_PWD<br/># Push the image to Docker Hub<br/>docker image push $HUB_USER/$REPOSITORY:$TAG</pre>
<p>Note that we're using four environment variables: <kbd>$HUB_USER</kbd> and <kbd>$HUB_PWD</kbd> being the credentials for Docker Hub and <kbd>$REPOSITORY</kbd> and <kbd>$TAG</kbd> being the name and tag of the Docker image we want to build. Eventually, we will have to pass values for those environment variables in the <kbd>docker run</kbd> command.</p>
<p>We want to run that script inside a builder container. Since the script uses the Docker CLI, our builder container must have the Docker CLI installed, and to access the Docker Engine, the builder container must have the Docker socket bind-mounted. Let's start creating a Docker image for such a builder container:</p>
<ol>
<li>First, create a <kbd>builder</kbd> folder and navigate to it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir builder &amp;&amp; cd builder</strong></pre>
<ol start="2">
<li>Inside this folder, create a <kbd>Dockerfile</kbd> that looks like this:</li>
</ol>
<pre style="padding-left: 60px"><strong>FROM alpine:latest</strong><br/><strong>RUN apk update &amp;&amp; apk add docker<br/>WORKDIR /usr/src/app<br/>COPY . .<br/>CMD ./pipeline.sh</strong></pre>
<ol start="3">
<li>Now create a <kbd>pipeline.sh</kbd> file in the <kbd>builder</kbd> folder and add as content the pipeline script we have presented in the preceding file.</li>
<li>Save and make the file an executable:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ chmod +x ./pipeline.sh</strong></pre>
<ol start="5">
<li>Building an image is straightforward:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker image build -t builder .</strong></pre>
<p>We are now ready to try <kbd>builder</kbd> out with a real Node.js application, for example, the sample app we defined in the <kbd>ch08/sample-app</kbd> folder. Make sure you replace <kbd>&lt;user&gt;</kbd> and <kbd>&lt;password&gt;</kbd> with your own credentials for Docker Hub:</p>
<pre><strong>$ cd ~/fod/ch08/sample-app</strong><br/><strong>$ docker container run --rm \</strong><br/><strong>    --name builder \</strong><br/><strong>    -v /var/run/docker.sock:/var/run/docker.sock \<br/>    -v "$PWD":/usr/src/app \</strong><br/><strong>    -e HUB_USER=&lt;user&gt; \</strong><br/><strong>    -e HUB_PWD=&lt;password&gt;@j \</strong><br/><strong>    -e REPOSITORY=ch08-sample-app \</strong><br/><strong>    -e TAG=1.0 \</strong><br/><strong>    builder</strong></pre>
<p>Notice how, in the preceding command, we mounted the Docker socket into the container with <kbd>-v /var/run/docker.sock:/var/run/docker.sock</kbd>. If everything goes well, you should have a container image built for the sample application, the test should have been run, and the image should have been pushed to Docker Hub. This is only one of the many use cases where it is very useful to be able to bind-mount the Docker socket.</p>
<p>A special notice to all those of you who want to try Windows containers. On Docker for Windows, you can create a similar environment by bind-mounting Docker's <strong>named pipe</strong> instead of a socket. A named pipe on Windows is roughly the same as a socket on a Unix-based system. Assuming you're using a PowerShell Terminal, the command to bind-mount a named pipe when running a Windows container hosting Jenkins looks like this:<br/>
<br/>
<kbd><strong>PS&gt;</strong> <strong>docker container run `<br/></strong><strong>--name jenkins `<br/></strong><strong>-p 8080:8080 `<br/></strong><strong>-v \\.\pipe\docker_engine:\\.\pipe\docker_engine `<br/>
friism/jenkins</strong></kbd><br/>
<br/>
Note the special syntax, <kbd>\\.\pipe\docker_engine</kbd>, to access Docker's named pipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Formatting the output of common Docker commands</h1>
                
            
            
                
<p>Have you at times wished that your Terminal window was infinitely wide since the output of a Docker command such as <kbd>docker container ps</kbd> is scrambled across several lines per item? Worry not, as you can customize the output to your liking. Almost all commands that produce an output have a <kbd>--format</kbd> argument, which accepts a so-called Go template as a parameter. If you wonder why a Go template, it's because most of Docker is written in this popular low-level language. Let's look at an example. Assume we want to only show the name of the container, the name of the image, and the state of the container, separated by tabs, output by the <kbd>docker container ps</kbd> command. The format would then look like this:</p>
<pre><strong>$ docker container ps -a \<br/>--format "table {{.Names}}\t{{.Image}}\t{{.Status}}"</strong></pre>
<p>Please be aware that the <kbd>format</kbd> string is case sensitive. Also, note the addition of the <kbd>-a</kbd> parameter to include stopped containers in the output. A sample output could look like this:</p>
<pre>NAMES              IMAGE            STATUS<br/>elated_haslett     alpine           Up 2 seconds<br/>brave_chebyshev    hello-world      Exited (0) 3 minutes ago</pre>
<p>This is definitely nicer to display even on a narrow Terminal window than the unformatted one scattering wildly over multiple lines.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Filtering the output of common Docker commands</h1>
                
            
            
                
<p>Similar to what we have done in the previous section by pretty-printing the output of Docker commands, we can also filter what is output. There are quite a few filters that are supported. Please find the full list for each command in the Docker online documentation. The format of filters is straightforward and of the type <kbd>--filter &lt;key&gt;=&lt;value&gt;</kbd>. If we need to combine more than one filter, we can just combine multiple of these statements. Let's do an example with the <kbd>docker image ls</kbd> command as I have a lot of images on my workstation:</p>
<pre><strong>$</strong> <strong>docker image ls --filter dangling=false --filter "reference=*/*/*:latest"</strong></pre>
<p>The preceding filter only outputs images that are not dangling, that is, real images whose fully qualified name is of the form <kbd>&lt;registry&gt;/&lt;user|org&gt;&lt;repository&gt;:&lt;tag&gt;</kbd>, and the tag is equal to <kbd>latest</kbd>. The output on my machine looks like this:</p>
<pre>REPOSITORY                                  TAG     IMAGE ID      CREATED   SIZE<br/>docker.bintray.io/jfrog/artifactory-cpp-ce  latest  092f11699785  9 months  ago 900MB<br/>docker.bintray.io/jfrog/artifactory-oss     latest  a8a8901c0230  9 months  ago 897MB</pre>
<p>Having shown how to pretty print and filter output generated by the Docker CLI, it is now time to talk once more about building Docker images and how to optimize this process.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing your build process</h1>
                
            
            
                
<p>Many Docker beginners make the following mistake when crafting their first <kbd>Dockerfile</kbd>:</p>
<pre>FROM node:12.10-alpine<br/>WORKDIR /usr/src/app<br/>COPY . .<br/>RUN npm install<br/>CMD npm start</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Can you spot the weak point in this typical <kbd>Dockerfile</kbd> for a Node.js application? In <a href="527543ae-a569-47dc-975d-65c96d0f6ff0.xhtml" target="_blank">Chapter 4</a>, <em>Creating and Managing Container Images</em>, we have learned that an image consists of a series of layers. Each (logical) line in a <kbd>Dockerfile</kbd> creates a layer, except the lines with the <kbd>CMD</kbd> and/or <kbd>ENTRYPOINT</kbd> keyword. We have also learned that the Docker builder tries to do its best by caching layers and reusing them if they have not changed between subsequent builds. But the caching only uses cached layers that occur before the first changed layer. All subsequent layers need to be rebuilt. That said, the preceding structure of the <kbd>Dockerfile</kbd> busts the image layer cache!</p>
<p>Why? Well, from experience, you certainly know that <kbd>npm install</kbd> can be a pretty expensive operation in a typical Node.js application with many external dependencies. The execution of this command can take from seconds to many minutes. That said, each time one of the source files changes, and we know that happens frequently during development, line 3 in the <kbd>Dockerfile</kbd> causes the corresponding image layer to change. Hence, the Docker builder cannot reuse this layer from cache, nor can it reuse the subsequent layer created by <kbd>RUN npm install</kbd>. Any minor change in code causes a complete rerun of <kbd>npm install</kbd>. That can be avoided. The <kbd>package.json</kbd> file containing the list of external dependencies rarely changes. With all of that information, let's fix the <kbd>Dockerfile</kbd>:</p>
<pre>FROM node:12.10-alpine<br/>WORKDIR /usr/src/app<br/>COPY package.json ./<br/>RUN npm install<br/>COPY . .<br/>CMD npm start</pre>
<p>This time, on line 3, we only copy the <kbd>package.json</kbd> file into the container, which rarely changes. Hence, the subsequent <kbd>npm install</kbd> command has to be executed equally rarely. The <kbd>COPY</kbd> command on line 5 then is a very fast operation and hence rebuilding an image after some code has changed only needs to rebuild this last layer. Build times reduce to merely a fraction of a second.</p>
<p>The very same principle applies to most languages or frameworks, such as Python, .NET, or Java. Avoid busting your image layer cache!</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Limiting resources consumed by a container</h1>
                
            
            
                
<p>One of the great features of a container, apart from encapsulating application processes, is the possibility of limiting the resources a single container can consume at. This includes CPU and memory consumption. Let's have a look at how limiting the amount of memory (RAM) works:</p>
<pre><strong>$ docker container run --rm -it \<br/>    --name stress-test \</strong><br/><strong>    --memory 512M \</strong><br/><strong>    ubuntu:19.04 /bin/bash</strong></pre>
<p>Once inside the container, install the <kbd>stress</kbd> tool, which we will use to simulate memory pressure:</p>
<pre>/# <strong>apt-get update &amp;&amp; apt-get install -y stress</strong></pre>
<p>Open another Terminal window and execute the <kbd>docker stats</kbd> command. You should see something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8a674500-7732-44f7-98fd-a13d1aa38450.png" style="width:71.50em;height:3.50em;"/></p>
<p>docker stats showing a resource-limited container</p>
<p>Look at  <kbd>MEM USAGE</kbd> and <kbd>LIMIT</kbd>. Currently, the container uses only <kbd>1.87MiB</kbd> memory and has a limit of <kbd>512MB</kbd>. The latter corresponds to what we have configured for this container. Now, let's use <kbd>stress</kbd> to simulate four workers, which try to <kbd>malloc()</kbd> memory in blocks of <kbd>256MB</kbd>. Run this command inside the container to do so:</p>
<pre>/# <strong>stress -m 4</strong></pre>
<p>In the Terminal running Docker stats, observe how the value for <kbd>MEM USAGE</kbd> approaches but never exceeds <kbd>LIMIT</kbd>. This is exactly the behavior we expected from Docker. Docker uses Linux <kbd>cgroups</kbd> to enforce those limits.</p>
<p>We could similarly limit the amount of CPU a container can consume with the <kbd>--cpu</kbd> switch.</p>
<p>With this operation, engineers can avoid the noisy neighbor problem on a busy Docker host, where a single container starves all of the others by consuming an excessive amount of resources.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Read-only filesystem</h1>
                
            
            
                
<p>To protect your applications against malicious hacker attacks, it is often advised to define the filesystem of the container or part of it as read-only. This makes the most sense for stateless services. Assume that you have a billing service running in a container as part of your distributed, mission-critical application. You could run your billing service as follows:</p>
<pre><strong>$ docker container run -d --rm \</strong><br/><strong> --name billing \</strong><br/><strong> --read-only \</strong><br/><strong> acme/billing:2.0</strong></pre>
<p>The <kbd>--read-only</kbd> flag mounts the container's filesystem as read-only. If a hacker succeeds in entering your billing container and tries to change an application maliciously by, say, replacing one of the binaries with a compromised one, then this operation would fail. We can easily demonstrate that with the following commands:</p>
<pre><strong>$ docker container run --tty -d \<br/>    --name billing \<br/>    --read-only \<br/>    alpine /bin/sh<br/></strong><br/><strong>$ docker container exec -it billing \</strong><br/><strong>    sh -c 'echo "You are doomed!" &gt; ./sample.txt'<br/></strong>sh: can't create ./sample.txt: Read-only file system<strong><br/></strong></pre>
<p>The first command runs a container with a read-only filesystem and the second command tries to execute another process in this container, which is supposed to write something to the filesystem—in this case, a simple text file. This fails, as we can see in the preceding output, with the error message <kbd>Read-only file system</kbd>.</p>
<p>Another means to tighten the security of your applications running in containers is to avoid running them as <kbd>root</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avoid running a containerized app as root </h1>
                
            
            
                
<p>Most applications or application services that run inside a container do not need root access. To tighten security, it is helpful in those scenarios to run these processes with minimal necessary privileges. These applications should not be run as <kbd>root</kbd> nor assume that they have <kbd>root</kbd>-level privileges.</p>
<p class="mce-root"/>
<p>Once again, let's illustrate what we mean with an example. Assume we have a file with top-secret content. We want to secure this file on our Unix-based system using the <kbd>chmod</kbd> tool so that only users with root permission can access it. Let's assume I am logged in as <kbd>gabriel</kbd> on the <kbd>dev</kbd> host and hence my prompt is <kbd>gabriel@dev $</kbd>. I can use <kbd>sudo su</kbd> to impersonate a superuser. I have to enter the superuser password though:</p>
<pre><strong>gabriel@dev $ sudo su</strong><br/><strong>Password: &lt;root password&gt;</strong><br/><strong>root@dev $</strong></pre>
<p>Now, as the <kbd>root</kbd> user, I can create this file called <kbd>top-secret.txt</kbd> and secure it:</p>
<pre><strong>root@dev $ echo "You should not see this." &gt; top-secret.txt</strong><br/><strong>root@dev $ chmod 600 ./top-secret.txt<br/>root@dev $ exit</strong><br/><strong>gabriel@dev $</strong></pre>
<p>If I try to access the file as <kbd>gabriel</kbd>, the following happens:</p>
<pre><strong>gabriel@dev $ cat ./top-secret.txt</strong><br/>cat: ./top-secret.txt: Permission denied</pre>
<p>I get <kbd>Permission denied</kbd>, which is what we wanted. No other user except <kbd>root</kbd> can access this file. Now, let's build a Docker image that contains this secured file and when a container is created from it, tries to output its content. The <kbd>Dockerfile</kbd> could look like this:</p>
<pre><strong>FROM ubuntu:latest</strong><br/><strong>COPY ./top-secret.txt /secrets/</strong><br/><strong># simulate use of restricted file</strong><br/><strong>CMD cat /secrets/top-secret.txt</strong></pre>
<p>We can build an image from that Dockerfile (as <kbd>root</kbd>!) with the following:</p>
<pre><strong>gabriel@dev $ sudo su</strong><br/><strong>Password: &lt;root password&gt;</strong><br/><strong>root@dev $ docker image build -t demo-image .<br/>root@dev $ exit</strong><br/><strong>gabriel@dev $</strong></pre>
<p>Then, when running a container from that image we get:</p>
<pre><strong>gabriel@dev $</strong> <strong>docker container run demo-image<br/></strong>You should not see this.</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>OK, so although I am impersonating the <kbd>gabriel</kbd> user on the host and running the container under this user account, the application running inside the container automatically runs as <kbd>root</kbd>, and hence has full access to protected resources. That's bad, so let's fix it! Instead of running with the default, we define an explicit user inside the container. The modified <kbd>Dockerfile</kbd> looks like this:</p>
<pre>FROM ubuntu:latest<br/><strong>RUN groupadd -g 3000 demo-group |</strong><br/><strong>    &amp;&amp; useradd -r -u 4000 -g demo-group demo-user</strong><br/><strong>USER demo-user</strong><br/>COPY ./top-secret.txt /secrets/<br/># simulate use of restricted file<br/>CMD cat /secrets/top-secret.txt</pre>
<p>We use the <kbd>groupadd</kbd> tool to define a new group, <kbd>demo-group</kbd>, with the ID <kbd>3000</kbd>. Then, we use the <kbd>useradd</kbd> tool to add a new user, <kbd>demo-user</kbd>, to this group. The user has the ID <kbd>4000</kbd> inside the container. Finally, with the <kbd>USER demo-user</kbd> statement, we declare that all subsequent operations should be executed as <kbd>demo-user</kbd>.</p>
<p>Rebuild the image—again as <kbd>root</kbd>—and then try to run a container from it:</p>
<pre><strong>gabriel@dev $ sudo su</strong><br/><strong>Password: &lt;root password&gt;</strong><br/><strong>root@dev $ docker image build -t demo-image .</strong><br/><strong>root@dev $ exit</strong><br/><strong>gabriel@dev $ docker container run demo-image<br/></strong>cat: /secrets/top-secret.txt: Permission denied<strong><br/></strong></pre>
<p>And as you can see on the last line, the application running inside the container runs with restricted permissions and cannot access resources that need root-level access. By the way, what do you think would happen if I ran the container as <kbd>root</kbd>? Try it out!</p>
<p>These have been a few tips and tricks for pros that are useful in the day-to-day usage of containers. There are many more. Google them. It is worth it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running your Terminal in a remote container and accessing it via HTTPS</h1>
                
            
            
                
<p>There are situations where you need to access a remote server and only have the option to use a browser for that. Your laptop may be locked down by your employer so that you are not allowed to, for example, <kbd>ssh</kbd> into a server outside of the company's domain.</p>
<p class="mce-root"/>
<p>To test this scenario proceed as follows:</p>
<ol>
<li>Create a free account on Microsoft Azure, GCP, or AWS. Then, create a VM, preferably with Ubuntu 18.04 or higher as the operating system, to follow along more easily.</li>
<li>Once your VM is ready, SSH into it. The command to do so should look similar to this:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>$ ssh gnschenker@40.115.4.249</strong></pre>
<p>To get access, you may need to open port <kbd>22</kbd> for ingress first for the VM.</p>
<p style="padding-left: 60px">The user I have defined during the provisioning of the VM is <kbd>gnschenker</kbd> and the public IP address of my VM is <kbd>40.115.4.249</kbd>.</p>
<ol start="3">
<li>Install Docker on this VM using the description found here: <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" target="_blank">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a>.</li>
<li>On a special note, do not forget to add your user (<kbd>gnschenker</kbd>, in my case) to the <kbd>docker</kbd> group on the VM with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo usermod -aG docker &lt;user-name&gt;</strong></pre>
<p style="padding-left: 60px">With this, you avoid having to constantly use <kbd>sudo</kbd> for all Docker commands. You need to log out from and log in to the VM to make this change work.</p>
<ol start="5">
<li>Now, we are ready to run <strong>Shell in a Box</strong> (<a href="https://github.com/shellinabox/shellinabox" target="_blank">https://github.com/shellinabox/shellinabox</a>) in a container on the VM. There are quite a few people who have containerized Shell in a Box. We are using the Docker image, <kbd>sspreitzer/shellinabox</kbd>. At the time of writing, it is the most popular version by far on Docker Hub. With the following command, we are running the application with a user, <kbd>gnschenker</kbd>; password, <kbd>top-secret</kbd>; <kbd>sudo</kbd> for the user enabled; and with self-signed certificates:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run --rm \<br/>    --name shellinabox \</strong><br/><strong>    -p 4200:4200 \<br/>    -e SIAB_USER=gnschenker \</strong><br/><strong>    -e SIAB_PASSWORD=top-secret \</strong><br/><strong>    -e SIAB_SUDO=true \</strong><br/><strong>    -v `pwd`/dev:/usr/src/dev \</strong><br/><strong>    sspreitzer/shellinabox:latest</strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">Note that initially, we recommend running the container in interactive mode so that you can follow what's happening. Once you are more familiar with the service, consider running it in the background with the <kbd>-d</kbd> flag. Also, note that we are mounting the <kbd>~/dev</kbd> folder from the host to the <kbd>/usr/src/dev</kbd> folder inside the container. This is useful if we want to remotely edit code that we have, for example, cloned from GitHub into the <kbd>~/dev</kbd> folder.</p>
<p style="padding-left: 60px">Also, notice that we are mapping port <kbd>4200</kbd> of Shell in a Box to host port <kbd>4200</kbd>. This is the port over which we will be able to access the shell using a browser and HTTPS. Hence, you need to open port <kbd>4200</kbd> for ingress on your VM. As a protocol, select TCP.</p>
<ol start="6">
<li>Once the container is running and you have opened port <kbd>4200</kbd> for ingress, open a new browser window and navigate to <kbd>https://&lt;public-IP&gt;:4200</kbd>, where <kbd>&lt;public-IP&gt;</kbd> is the public IP address of your VM. Since we're using a self-signed certificate, you will be greeted with a warning,here shown when using Firefox:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/110c91ea-409e-47e7-b670-c5ab08fda9ba.png" style="width:70.67em;height:41.17em;"/></p>
<p>Browser warning due to the use of self-signed certificates</p>
<ol start="7">
<li>In our case, this is not a problem; we know the cause—it's the self-signed certificate. Hence, click the <strong>Advanced...</strong> button and then Accept Risk and Continue. Now, you will be redirected to the login screen. Log in with your username and password:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/6758f0cc-3405-4ede-b37e-8a0f2930a147.png" style="width:37.83em;height:22.00em;"/></p>
<p>Log in to the remote VM from your browser using HTTPS</p>
<p style="padding-left: 60px">We are logged in to the <strong>Shell in a Box</strong> application running on our remote VM, using the HTTPS protocol.</p>
<ol start="8">
<li>Now, we have full access to, for example, the files and folder mapped from the host VM to <kbd>/usr/src/dev</kbd>. We can, for example, use the <kbd>vi</kbd> text editor to create and edit files, although we have to first install vi with this:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo apt-get update &amp;&amp; sudo apt-get install -y vim</strong></pre>
<ol start="9">
<li>The possibilities are nearly endless. Please experiment with this setup.For example, run the Shell in a Box container with the Docker socket mounted, install Docker inside the container, and then try to use the Docker CLI from within the container. It is really cool because you can do all of this from within your browser!</li>
<li>If you intend to use this Shell in a Box container often and need some additional software installed, do not hesitate to create your own custom Docker image inheriting from <kbd>sspreitzer/shellinabox</kbd>.</li>
</ol>
<p>Next, we will see how to run your development environment inside a container.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running your development environment inside a container</h1>
                
            
            
                
<p>Imagine that you only have access to a workstation with Docker for Desktop installed, but no possibility to add or change anything else on this workstation. Now you want to do some proof of concepts and code some sample application using Python. Unfortunately, Python is not installed on your computer. What can you do? What if you could run a whole development environment inside a container, including code editor and debugger? What if, at the same time, you could still have your code files on your host machine?</p>
<p>Containers are awesome and genius engineers have come up with solutions for exactly this kind of problem.</p>
<p>Let's try this for a Python application:</p>
<ol>
<li>We will be using Visual Studio Code, our favorite code editor, to show how to run a complete Python development environment inside a container. But first, we need to install the necessary Visual Studio Code extension. Open Visual StudioCode and install the extension called Remote Development:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/9cd9233c-6976-4204-b2ed-9a00693a4547.png" style="width:30.08em;height:16.92em;"/></p>
<p>Remote Development extension for Visual Studio Code</p>
<ol start="2">
<li>Then, click the green quick actions status bar item in the lower-left of the Visual Studio Code window. In the popup, select <strong>Remote-Containers: Open Folder in Container...</strong>: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/32c52f7a-da9f-4d2f-b82c-14343336162d.png" style="width:43.08em;height:26.67em;"/></p>
<p>Opening a project in a remote container</p>
<ol start="3">
<li>Select the project folder you want to work with in the container. In our case, we selected the <kbd>~/fod/ch08/remote-app</kbd> folder. Visual StudioCode will start preparing the environment, which, the very first time, can take a couple of minutes or so. You will see a message like this while this is happening:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/00fddd11-d359-4cae-8003-bf8206513e2c.png" style="width:40.08em;height:4.83em;"/></p>
<p>Visual Studio Code preparing the development container</p>
<p style="padding-left: 60px">By default, this development container runs as a non-root user—called <kbd>python</kbd> in our case. We learned, in a prior section, that this is a highly recommended best practice. You can change though, and run as <kbd>root</kbd> by commenting out the line with <kbd>"runArgs": [ "-u", "python" ],</kbd> in the <kbd>.devcontainer/devcontainer.json</kbd> file.</p>
<ol start="4">
<li>Open a Terminal inside Visual Studio Code with <em>Shift</em> + <em>Ctrl</em> + <em>` </em>and run the Flask app with the <kbd>env FLASK_APP=main.py flask run</kbd> command. You should see output like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/d410a8ba-00c4-4722-a958-fba0fff9fcda.png" style="width:57.25em;height:13.50em;"/></p>
<p>Starting a Python Flask app from Visual Studio Code running inside a container </p>
<p style="padding-left: 60px">The <kbd>python@df86dceaed3d:/workspaces/remote-app$</kbd> prompt indicates that we are <strong>not</strong> running directly on our Docker host but from within a development container that Visual Studio Code spun up for us. The remote part of Visual Studio Code itself also runs inside that container. Only the client part of Visual Studio Code—the UI—continues to run on our host.</p>
<ol start="5">
<li>Open another Terminal window inside Visual Studio Code by pressing <em>Shift+Ctrl+`</em>. Then, use <kbd>curl</kbd> to test the application:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/6fb04349-0556-43fa-a13b-bc21d70cfe9f.png" style="width:61.08em;height:13.00em;"/></p>
<p>Testing the remote Flask app</p>
<ol start="6">
<li>Press <em>Ctrl</em> + <em>C</em> to stop the Flask application.</li>
<li>We can also debug the application like we're used when working directly on the host. Open the <kbd>.vscode/launch.json</kbd> file to understand how the Flask app is started and how the debugger is attached.</li>
<li>Open the <kbd>main.py</kbd> file and set a breakpoint on the <kbd>return</kbd> statement of the <kbd>home()</kbd> function.</li>
<li>Then, switch to the Debug view of Visual Studio Code and make sure the launch task, <kbd>Python: Flask</kbd>, is selected in the drop-down menu.</li>
<li>Next, press the green start arrow to start debugging. The output in the Terminal should look like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/63424e41-5a80-476c-8a90-0415284ac839.png" style="width:48.42em;height:19.42em;"/></p>
<p>Start debugging a remote app running in a container</p>
<ol start="11">
<li>Open another Terminal with <em>Shift</em> + <em>Ctrl</em> + <em>`</em> and test the application by running the <kbd>curl localhost:9000/</kbd> command. The debugger should hit the breakpoint and you can start analyzing:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/8d6e30fd-ec53-4dce-ae48-d9ec94e1d718.png" style="width:47.75em;height:27.67em;"/></p>
<p>Line-by-line debugging in Visual Studio Code running inside a container</p>
<p>I cannot say strongly enough how cool that is. The backend (non-UI part) of Visual Studio Code is running inside a container, as is Python, the Python debugger, and the Python Flask application itself. At the same time, the source code is mounted from the host into the container and the UI part of Visual Studio Code also runs on the host. This opens up unlimited possibilities for developers even on the most restricted workstations. You can do the same for all popular languages and frameworks, such as .NET, C#, Java, Go, Node.js, and Ruby. If one language is not supported out of the box, you can craft your own development container that will then work the same way as what we have shown with Python.</p>
<p>What if you are working on a workstation that does not have Docker for Desktop installed and is locked down even further? What are your options there?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running your code editor in a remote container and accessing it via HTTPS</h1>
                
            
            
                
<p>In this section, we will show how you can use Visual Studio Code to enable remote development inside a container. This is interesting when you are limited in what you can run on your workstation. Let's follow these steps:</p>
<ol>
<li>Download and extract the latest version of <kbd>code-server</kbd>. You can find out the URL by navigating to <a href="https://github.com/cdr/code-server/releases/latest" target="_blank">https://github.com/cdr/code-server/releases/latest</a>. At the time of writing, it is <kbd>1.1156-vsc1.33.1</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ VERSION=&lt;version&gt;</strong><br/><strong>$ wget https://github.com/cdr/code-server/releases/download/${VERSION}/code-server${VERSION}-linux-x64.tar.gz</strong><br/><strong>$ tar -xvzf code-server${VERSION}-linux-x64.tar.gz</strong></pre>
<p style="padding-left: 60px">Make sure to replace <kbd>&lt;version&gt;</kbd> with your specific version.</p>
<ol start="2">
<li>Navigate to the folder with the extracted binary, make it executable, and start it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd code-server${VERSION}-linux-x64</strong><br/><strong>$ chmod +x ./code-server</strong><br/><strong>$ sudo ./code-server -p 4200</strong></pre>
<p style="padding-left: 60px">The output should look similar to this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ea928591-0b62-4b04-9281-7d2ac4e551b7.png" style="width:52.00em;height:18.67em;"/></p>
<p>Starting Visual Studio Code remote-server on a remote VM</p>
<p style="padding-left: 60px">Code Server is using self-signed certificates to secure communication, so we can access it over HTTPS. Please make sure you note down the <kbd>Password</kbd> output on the screen since you need it when accessing Code Server from within your browser. Also note that we are using port <kbd>4200</kbd> to expose Code Server on the host, the reason being that we already opened that port for ingress on our VM. You can, of course, select any port you want—just make sure you open it for ingress.</p>
<ol start="3">
<li>Open a new browser page and navigate to <kbd>https://&lt;public IP&gt;:4200</kbd> ,where <kbd>&lt;public IP&gt;</kbd> is the public IP address of your VM. Since we are using self-signed certificates once again, the browser will greet you with a warning similar to what happened when we were using Shell in a Box earlier in this chapter. Accept the warning and you will be redirected to the login page of Code Server:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/d465a2e6-484a-44ef-98e5-f2a793ed4f9e.png" style="width:53.17em;height:30.58em;"/></p>
<p>Login page of Code Server</p>
<ol start="4">
<li>Enter the password that you noted down before and click ENTER IDE. Now you will be able to use Visual Studio Code remotely via your browser over a secure HTTPS connection:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/119fd2f9-696d-40d1-a25c-8292f546b98e.png" style="width:53.00em;height:30.50em;"/></p>
<p>Visual Studio Code running in the browser over HTTPS</p>
<ol start="5">
<li>Now you can do your development from, for example, a Chrome Book or a locked-down workstation, without restrictions. But wait a minute, you may say now! What does this have to do with containers? You're right—so far, there are no containers involved. I could say, though, that if your remote VM has Docker installed, you can use Code Server to do any container-specific development, and I would have saved the day. But that would be a cheap answer.</li>
<li>Let's run Code Server itself in a container. That should be easy, shouldn't it? Try using this command, which maps the internal port <kbd>8080</kbd> to the host port <kbd>4200</kbd> and mounts host folders containing Code Server settings and possibly your projects into the container:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run -it \</strong><br/><strong>    -p 4200:8080 \</strong><br/><strong>    -v "${HOME}/.local/share/code-server:/home/coder/.local/share/code-server" \</strong><br/><strong>    -v "$PWD:/home/coder/project" \</strong><br/><strong>    codercom/code-server:v2</strong></pre>
<p style="padding-left: 60px">Note, the preceding command runs Code Server in insecure mode as indicated in the output:</p>
<pre style="padding-left: 60px">info Server listening on http://0.0.0.0:8080<br/>info - No authentication<br/>info - Not serving HTTPS</pre>
<ol start="7">
<li>You can now access Visual Studio Code in your browser at <kbd>http://&lt;public IP&gt;:4200</kbd>. Please note the <kbd>HTTP</kbd> in the URL instead of <kbd>HTTPS</kbd>! Similarly to when running Code Server natively on the remote VM, you can now use Visual Studio Code from within your browser:</li>
</ol>
<div><img src="img/c7e66138-3253-4388-a724-f50fe7f064fc.png" style="width:55.58em;height:36.67em;"/></div>
<p>Developing within your browser</p>
<p>With this, I hope you have got a feel for the near-unlimited possibilities that the use of containers offers to you.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we have shown a few tips and tricks for the advanced Docker user that can make your life much more productive. We have also shown how you can leverage containers to serve whole development environments that run on remote servers and can be accessed from within a browser over a secure HTTPS connection.</p>
<p>In the next chapter, we will introduce the concept of a distributed application architecture and discuss the various patterns and best practices that are required to run a distributed application successfully. In addition to that, we will list some of the concerns that need to be fulfilled to run such an application in production or a production-like environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>Name the reasons why you would want to run a complete development environment inside a container.</li>
<li>Why should you avoid to run applications inside a container as <kbd>root</kbd>?</li>
<li>Why would you ever bind-mount the Docker socket into a container?</li>
<li>When pruning your Docker resources to make space, why do you need to handle volumes with special care?</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<ul>
<li>Using Docker in Docker at <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/" target="_blank">http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/</a></li>
<li>Shell in a Box at <a href="https://github.com/shellinabox/shellinabox" target="_blank">https://github.com/shellinabox/shellinabox</a></li>
<li>Remote development using SSH at <a href="https://code.visualstudio.com/docs/remote/ssh" target="_blank">https://code.visualstudio.com/docs/remote/ssh</a></li>
<li>Developing inside a container at <a href="https://code.visualstudio.com/docs/remote/containers" target="_blank">https://code.visualstudio.com/docs/remote/containers</a></li>
</ul>


            

            
        
    </body></html>