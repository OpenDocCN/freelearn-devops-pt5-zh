<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5.  Going Cloud </h1></div></div></div><p>In this chapter, we will see how to use Ansible for provisioning infrastructures in a matter of minutes. In my opinion, this is one of the most interesting and powerful capabilities of Ansible, since it allows you to (re-)create environments in a quick and consistent way. This is very important when you have multiple environments for the various stages of your deployment pipeline. In fact, it allows you to create equal environments and to keep them aligned when you need to make changes without any pain.</p><p>Letting Ansible provision your machines also has other advantages, and for those reasons I always suggest to do:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Audit trail</strong>: In the last few years, the IT sector swallowed a huge number of other sectors and as a consequence of this, the auditing processes are now looking at IT as a critical part of the process. When an auditor comes to the IT department asking for the history of a server, from its creation to the present moment, having Ansible playbooks for the whole process helps a lot.</li><li class="listitem" style="list-style-type: disc"><strong>Multiple staging environments</strong>: As we mentioned before, if you have multiple environments, provisioning servers with Ansible will help you a lot</li><li class="listitem" style="list-style-type: disc"><strong>Moving servers</strong>: When a company uses a global cloud provider (like AWS or DigitalOcean) they often choose the region closest to their offices or customers at the moment they create the first servers. Those providers often open new regions and if their new region is close to you; you may want to move or extend your infrastructure to the new region. This would be a nightmare if you had provisioned every resource manually.</li></ul></div><p>In this chapter, at a broad level, we'll cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Provisioning of machines in <strong>Amazon Web Services</strong> (<strong>AWS</strong>)</li><li class="listitem" style="list-style-type: disc">Provisioning of machines in DigitalOcean</li><li class="listitem" style="list-style-type: disc">Provisioning Docker containers</li></ul></div><p>Most of the new machine creations have two phases:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Provisioning a new machine or a new set of machines</li><li class="listitem" style="list-style-type: disc">Running playbooks to ensure the new machines are configured properly to play their role in your infrastructure</li></ul></div><p>We've looked at the configuration management aspect in the initial chapters. We'll focus a lot more on provisioning new machines in this chapter with a lesser focus on configuration management.</p><div><div><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Provisioning resources in the cloud</h1></div></div></div><p>With that, let's jump to the first topic. Teams managing infrastructures have a lot of choices today for running their builds, tests, and deployments. Providers such as Amazon, Rackspace, and DigitalOcean primarily provide <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>). When we speak about IaaS, it's better to speak about resources not virtual machines for different reasons:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The majority of the products that those companies allow you to provision are not machines but other critical resources such as networking and storage</li><li class="listitem" style="list-style-type: disc">Lately, many of those companies have started to provide many different kind of compute instances ranging from bare-metal machines to containers</li><li class="listitem" style="list-style-type: disc">Setting up machines with no networking (or storage) could be all you need for some very simple environments, but might not be enough in production environments</li></ul></div><p>Those companies usually provide API, CLI, GUI, and SDK utilities to create and manage cloud resources throughout their whole lifecycle. We're more interested in using their SDK as it will play an important part in our automation effort. Setting up new servers and provisioning them is interesting at first but at some stage it can become boring as it's quite repetitive in nature. Each provisioning step will involve several similar steps to get them up-and-running.</p><p>Imagine one fine morning you receive an e-mail asking for three new customer setups, where each customer setup has three to four instances and a bunch of services and dependencies. This might be an easy task for you, but would require running the same set of repetitive commands multiple times, followed by monitoring the servers once they come up to confirm that everything went well. In addition, anything you do manually has a chance of introducing problems. What if two of the customer setups come up correctly but, due to fatigue, you miss out a step for the third customer and hence introduce a problem? To deal with such situations, there exists automation.</p><p>Cloud provisioning automation makes it easy for an engineer to build up a new server as quickly as possible, allowing her to concentrate on other priorities. Using Ansible, you can easily perform these actions and automate cloud provisioning with minimal effort. Ansible provides you with the power to automate various different cloud platforms, such as Amazon, Azure, DigitalOcean, Google Cloud, Rackspace, and many more, with modules for different services available in the Ansible core or extended module packages.</p><div><div><h3 class="title"><a id="note25"/>Note</h3><p>As mentioned earlier, bringing up new machines is not the end of the game. We also need to make sure we configure them to play the required role.</p></div></div><p>In the next sections we will provision the environment that we have used in the previous chapters (two web servers and one database server) in the following environments:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Simple Amazon Web Service deployment</strong>: Where all machines will be placed in the same Availability Zone and same network</li><li class="listitem" style="list-style-type: disc"><strong>Complex Amazon Web Service deployment</strong>: Where the machines will be split in multiple Availability Zones as well as networks</li><li class="listitem" style="list-style-type: disc"><strong>DigitalOcean</strong>: DigitalOcean does not allow us to do many networking tweaks so it will be similar to the first one</li><li class="listitem" style="list-style-type: disc"><strong>Docker</strong>: We will create a simple deployment in this case</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec44"/>Amazon Web Service</h1></div></div></div><p>Amazon Web Service is the most used public cloud by a fair amount and it's often chosen due to their huge amount of available services as well as the huge amount of documentation, answered questions, and articles that can be expected from such a popular product.</p><p>Since AWS' goal is to be a complete virtual data center provider (and much more) we will need to create and manage our network as we would do if we had to set up a real data center. Obviously, we will not need to cable stuff since it's a virtual data center. Due to this, a few lines of an Ansible playbook will be enough.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec51"/>AWS global infrastructure</h2></div></div></div><p>Amazon has always been pretty discrete about sharing the location or the exact number of data centers that their cloud is actually composed of. While I'm writing this, AWS counts 13 regions (with 4 more regions already planned) with a total of 35 <strong>Availability Zones</strong> (<strong>AZ</strong>) and more than 50 edge locations. Amazon defines a region as a physical location in the world where we (Amazon) have multiple Availability Zones. Looking at Amazon's definition of Availability Zones, it says that an AZ consists of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. For edge location, there is no official definition.</p><p>As you can see, from a real life point of view, those definitions do not help you much. When I try to explain those concepts I usually use different definitions, created by myself:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Region</strong>: Group of AZs that are physically close</li><li class="listitem" style="list-style-type: disc"><strong>Availability Zone</strong>: A data center in a region (Amazon says that it could be more than one data center, but since there is no document listing the specific layout of every AZ, I assume the worst-case scenario)</li><li class="listitem" style="list-style-type: disc"><strong>Edge location</strong>: Internet exchanges or 3rd party data centers where Amazon has S3 and Route 53 endpoints</li></ul></div><p>Even though I tried to make those definitions as easy and as useful as possible, some of them are very cloudy. When we start to speak about real world differences, the definitions will become immediately clear. For instance, from a network speed perspective, when you move content within the same AZ, the bandwidth is very high. When you do the same operation with two AZs in the same region you get high bandwidth, while if you use two AZs from two different regions, the bandwidth will be much lower. Also, there is a price difference, since all traffic within the same region is free, while traffic between different regions is not free of charge.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec52"/>AWS Simple Storage Service</h2></div></div></div><p>Amazon S3 is the first AWS service to be launched and it's also one of the most well-known AWS services. Amazon S3 is an object storage service with public endpoints as well as private endpoints. It uses the concept of a bucket to allow you different kinds of files and to manage them in a simple way. Amazon S3 also gives the user more advanced features such as the capability of serving a bucket's contents using a built-in web server. This is one of the reasons why many people decide to host their website, or the pictures on their websites, on Amazon S3.</p><p>The advantages of S3 are mainly:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Price schema</strong>: You are billed by used gigabyte/month and by gigabyte transferred.</li><li class="listitem" style="list-style-type: disc"><strong>Reliability</strong>: Amazon affirms that the objects on AWS S3 have a 99.999999999% probability to survive any given year. This is orders of magnitude higher than any hard disk.</li><li class="listitem" style="list-style-type: disc"><strong>Tooling</strong>: Since S3 is a service that has been out there for many years now, a lot of tools have been implemented to leverage this service.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec53"/>AWS Elastic Compute Cloud (EC2)</h2></div></div></div><p>The second service launched by AWS is the EC2 service. This service allows you to spin up virtual machines on AWS infrastructure. You can think of those EC2 instances as OpenStack compute instances or VMware virtual machines. Initially, those machines were very similar to VPS, but after a while, Amazon decided to give much more flexibility on those machines introducing a very advanced networking option. The old kind of machines are still available in the oldest data centers with the name <strong>EC2 Classic</strong>, while the new kind is the current default and is just called <strong>EC2</strong>.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec54"/>AWS Virtual Private Cloud (VPC)</h2></div></div></div><p>The VPC is Amazon's networking implementation which we mentioned in the previous paragraph. The VPC is more a set of tools than a single tool, in fact, the capabilities it offers were offered by multiple metal boxes in the classic data center. The main things you can create with VPC are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Switches</li><li class="listitem" style="list-style-type: disc">Routers</li><li class="listitem" style="list-style-type: disc">DHCP</li><li class="listitem" style="list-style-type: disc">Gateways</li><li class="listitem" style="list-style-type: disc">Firewalls</li><li class="listitem" style="list-style-type: disc">Virtual Private Networks</li></ul></div><p>An important thing to understand when you use VPC is that the layout of your network is not completely arbitrary, since Amazon has created a few limitations to simplify their networking. The basic limitations are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">You cannot spawn a subnetwork between AZ</li><li class="listitem" style="list-style-type: disc">You cannot spawn a network between regions</li><li class="listitem" style="list-style-type: disc">You cannot route networks in different regions directly</li></ul></div><p>While, for the first two, the only solution is creating multiple networks and subnetworks, for the third, you can actually implement a workaround using a VPN service which could be self-provisioned or be provisioned using the official AWS VPN service.</p><p>We will be mainly using the switching and routing capabilities of VPC.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec55"/>AWS Route 53</h2></div></div></div><p>Like many other cloud services, Amazon offers a <strong>DNS as a Service</strong> (<strong>DNSaaS</strong>) feature and in Amazon case, it's called <strong>Route 53</strong>. Route 53 is a distributed DNS service with more than 50 endpoints worldwide (Route 53 is present in all AWS edge locations).</p><p>Route 53 allows you to create different zones for a domain allowing split-horizon situations in which, based on the fact that the client asking for a DNS resolution is inside or outside your VPC, will receive different responses. This is very useful when you want your applications to be easily moved in and out of your VPC without changes but at the same time, you want your traffic to stay on a private (virtual) network whenever possible.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec56"/>AWS Elastic Block Storage (EBS)</h2></div></div></div><p>AWS <strong>EBS</strong> is a block storage provider for allowing your EC2 instances to keep data that will survive reboots and is very flexible. From a user perspective, EBS seems a lot like any other SAN product with a simpler interface, since you only need to create the volume and tell EBS to which machine it needs to be attached, and EBS does the rest. You can attach multiple volumes to a single server, but every volume can be connected to only one server at any given time.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec57"/>AWS Identity and Access Management</h2></div></div></div><p>To allow you to manage users and access methods, Amazon provides the <strong>IAM</strong> service. The main features of the IAM service are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Create, edit, and delete users</li><li class="listitem" style="list-style-type: disc">Change user password</li><li class="listitem" style="list-style-type: disc">Create, edit, and delete groups</li><li class="listitem" style="list-style-type: disc">Manage users and group association</li><li class="listitem" style="list-style-type: disc">Manage tokens</li><li class="listitem" style="list-style-type: disc">Manage two-factor authentication</li><li class="listitem" style="list-style-type: disc">Manage SSH keys</li></ul></div><p>We will be using this service to set up our users and their permissions.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Amazon relational database service</h2></div></div></div><p>Setting up and maintaining relational databases is complex and very time-consuming. To simplify this, Amazon provides some widely used DBaaS, more specifically:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Aurora</li><li class="listitem" style="list-style-type: disc">MariaDB</li><li class="listitem" style="list-style-type: disc">MySQL</li><li class="listitem" style="list-style-type: disc">Oracle</li><li class="listitem" style="list-style-type: disc">PostgreSQL</li><li class="listitem" style="list-style-type: disc">SQL Server</li></ul></div><p>For each one of those engines, Amazon offers different features and price models but the specifics of each is beyond the goal of this book.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec59"/>Setting up an account with AWS</h2></div></div></div><p>The first thing we will need before starting to work on our Amazon Web Service is an account. Creating an account on Amazon Web Services is pretty straightforward and very well-documented by Amazon official documentation as well as by multiple independent sites and therefore it will not be covered in these pages.</p><p>After you have created your AWS account, you need to go into the AWS and do the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Upload your SSH key in <strong>EC2</strong> | <strong>Keypairs</strong></li><li class="listitem" style="list-style-type: disc">Create a new user in <strong>Identity &amp; Access Management</strong> | <strong>Users</strong> | <strong>Create new user</strong> and create a file in <code class="literal">~/.aws/credentials</code> with the following lines:</li></ul></div><pre class="programlisting">    [default] 
    aws_access_key_id = YOUR_ACCESS_KEY 
    aws_secret_access_key = YOUR_SECRET_KEY 
</pre><p>After you have created your AWS Keys and uploaded your SSH key, you need to set up Route53. In Route53 you need to create two zones for your domain (you can also use a subdomain if you don't have an unused domain): one <strong>public</strong> and one <strong>private</strong>.</p><p>If you create only the public zone, Route53 will propagate this zone everywhere, but if you create a public and a private zone, Route53 will serve your public zone everywhere but in the VPC you specified when creating the private zone. If you query those DNS entries from within that VPC, the private zone will be used. This approach has multiple advantages:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Only publicize the IP addresses of public machines</li><li class="listitem" style="list-style-type: disc">Always use DNS names instead of IP addresses, even for internal traffic</li><li class="listitem" style="list-style-type: disc">Ensure that your internal machines communicate directly without your traffic ever passing through the public web</li><li class="listitem" style="list-style-type: disc">Since the external IPs in Amazon Web Services are virtual IPs managed by Amazon and associated to your instances using NATs, this approach grants the least amount of hops and therefore latency</li></ul></div><div><div><h3 class="title"><a id="note26"/>Note</h3><p>If you declared an entry for your public zone but not in the private one, the machines in the VPC will not be able to resolve that entry.</p></div></div><p>After you have created the public zone, Amazon Web Services will give you a few name server IP addresses and you need to put those in your register/root zone DNS so that you can actually resolve those DNS.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec60"/>Simple AWS deployment</h2></div></div></div><p>As we said previously, the first thing that we will need is the networking up. For this example, we will need just one single network in one AZ and all our machines will stay there.</p><p>In this section, we will be working in the <code class="literal">playbooks/aws_simple_provision.yaml</code> file.</p><p>The first two lines are just used to declare the host that will perform the commands (<code class="literal">localhost</code>) and the beginning of the <code class="literal">tasks</code> section:</p><pre class="programlisting">    - hosts: localhost 
      tasks:

</pre><p>In AWS, we need to have a VPC network and subnetwork, but in case you need it, you can do the following to create the VPC network:</p><pre class="programlisting">    To create the VPC subnetwork: 
      - name: Ensure the VPC subnetwork is present 
        ec2_vpc_subnet: 
          state: present 
          az: AWS_AZ 
          vpc_id: '{{ aws_simple_net.vpc_id }}' 
          cidr: 10.0.1.0/24 
        register: aws_subnet 
</pre><p>Now we have all the information we need on the network and subnetwork, we can move to <strong>security groups</strong>. We can do this with the <code class="literal">ec2_group</code> module. In the Amazon Web Service world, security groups are used for firewalling. Security groups are very similar to groups of firewall rules that share the same destination (for ingress rules) or same destination (for egress rules). Three differences with standard firewalls rules are actually worth mentioning:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Multiple security groups can be applied to the same EC2 instance</li><li class="listitem" style="list-style-type: disc">As source (for ingress rules) or destination (for egress rules), you can specify one of the following:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An instance ID</li><li class="listitem" style="list-style-type: disc">Another security group</li><li class="listitem" style="list-style-type: disc">An IP range</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc">You don't have to specify a default deny rule at the end of the chain because AWS will add it by default</li></ul></div><pre class="programlisting">      - name: Ensure websg Security Group is present 
        ec2_group: 
          name: web 
          description: Web Security Group 
          region: AWS_AZ 
          vpc_id: VPC_ID 
          rules: 
          - proto: tcp 
            from_port: 80 
            to_port: 80 
            cidr_ip: 0.0.0.0/0 
          - proto: tcp 
            from_port: 443 
            to_port: 443 
            cidr_ip: 0.0.0.0/0 
          rules_egress: 
          - proto: all 
            cidr_ip: 0.0.0.0/0 
        register: aws_simple_websg 
</pre><p>So, in my case, the following code will be added to <code class="literal">playbooks/aws_simple_provision.yaml</code>:</p><pre class="programlisting">      - name: Ensure wssg Security Group is present 
        ec2_group: 
          name: wssg 
          description: Web Security Group 
          region: eu-west-1 
          vpc_id: '{{ aws_simple_net.vpcs.0.id }}' 
          rules: 
          - proto: tcp 
            from_port: 22 
            to_port: 22 
            cidr_ip: 0.0.0.0/0 
          - proto: tcp 
            from_port: 80 
            to_port: 80 
            cidr_ip: 0.0.0.0/0 
          - proto: tcp 
            from_port: 443 
            to_port: 443 
            cidr_ip: 0.0.0.0/0 
          rules_egress: 
          - proto: all 
            cidr_ip: 0.0.0.0/0 
        register: aws_simple_wssg 
</pre><p>We are now going to create another security group for our database. In this case, we only need to open port <code class="literal">3036</code> to the servers in the web security group:</p><pre class="programlisting">      - name: Ensure dbsg Security Group is present 
        ec2_group: 
          name: dbsg 
          description: DB Security Group 
          region: eu-west-1 
          vpc_id: '{{ aws_simple_net.vpcs.0.id }}' 
          rules: 
          - proto: tcp 
            from_port: 3036 
            to_port: 3036 
            group_id: '{{ aws_simple_wssg.group_id }}' 
          rules_egress: 
          - proto: all 
            cidr_ip: 0.0.0.0/0 
        register: aws_simple_dbsg 
</pre><div><div><h3 class="title"><a id="note27"/>Note</h3><p>As you can see, we allow all egress traffic to flow. This is not what security best practices suggest, and therefore you may need to regulate egress traffic as well. A case that frequently forces you to regulate egress traffic is if you want your target machine to be PCI-DSS compliant.</p></div></div><p>Now that we have the VPC, the subnet into the VPC, and the needed security groups, we can now move on to actually creating the EC2 instances:</p><pre class="programlisting">      - name: Setup instances 
        ec2: 
          assign_public_ip: '{{ item.assign_public_ip }}' 
          image: ami-7abd0209 
          region: eu-west-1 
          exact_count: 1 
          key_name: fale 
          count_tag: 
            Name: '{{ item.name }}' 
          instance_tags: 
            Name: '{{ item.name }}' 
          instance_type: t2.micro 
          group_id: '{{ item.group_id }}' 
          vpc_subnet_id: '{{ aws_simple_subnet.subnets.0.id }}' 
          volumes: 
            - device_name: /dev/sda1 
              volume_type: gp2 
              volume_size: 10 
              delete_on_termination: True 
        register: aws_simple_instances 
        with_items: 
        - name: ws01.simple.aws.fale.io 
          group_id: '{{ aws_simple_wssg.group_id }}' 
          assign_public_ip: True 
        - name: ws02.simple.aws.fale.io 
          group_id: '{{ aws_simple_wssg.group_id }}' 
          assign_public_ip: True 
        - name: db01.simple.aws.fale.io 
          group_id: '{{ aws_simple_dbsg.group_id }}' 
          assign_public_ip: False 
</pre><div><div><h3 class="title"><a id="note28"/>Note</h3><p>When we created the <code class="literal">db</code> machine we did not specify the <code class="literal">assign_public_ip</code>: <code class="literal">True</code> line. In this case, the machine will not receive a public IP and therefore it will not be reachable from outside our VPC. Since we used a very strict security group for this server, it would not be reachable from any machine outside the <code class="literal">wssg</code> anyway.</p></div></div><p>As you can guess, the piece of code we have just seen will create our three instances (two web servers and one database server).</p><p>We can now proceed to add those newly created instances to our Route 53 account so that we can resolve those machines' FQDN. To interact with AWS Route 53, we will be using the <code class="literal">route53</code> module, which allows us to create entries, query entries, and delete entries. To create a new entry, we will be using the following code:</p><pre class="programlisting">      - name: Add route53 entry for server SERVER_NAME 
        route53: 
          command: create 
          zone: ZONE_NAME 
          record: RECORD_TO_ADD 
          type: RECORD_TYPE 
          ttl: TIME_TO_LIVE 
          value: IP_VALUES 
          wait: True 
</pre><p>So to create the entries for our servers, we will add the following code:</p><pre class="programlisting">      - name: Add route53 rules for instances 
        route53: 
          command: create 
          zone: aws.fale.io 
          record: '{{ item.tagged_instances.0.tags.Name }}' 
          type: A 
          ttl: 1 
          value: '{{ item.tagged_instances.0.public_ip }}' 
          wait: True 
        with_items: '{{ aws_simple_instances.results }}' 
        when: item.tagged_instances.0.public_ip 
      - name: Add internal route53 rules for instances 
        route53: 
          command: create 
          zone: aws.fale.io 
          private_zone: True 
          record: '{{ item.tagged_instances.0.tags.Name }}' 
          type: A 
          ttl: 1 
          value: '{{ item.tagged_instances.0.private_ip }}' 
          wait: True 
        with_items: '{{ aws_simple_instances.results }}' 
</pre><div><div><h3 class="title"><a id="note29"/>Note</h3><p>Since the database server does not have a public address, it makes no sense to publish this machine in the public zone, so we have created this machine entry only in the internal zone.</p></div></div><p>Putting it all together, the <code class="literal">playbooks/aws_simple_provision.yaml</code> will be the following:</p><pre class="programlisting">    - hosts: localhost 
      tasks: 
      - name: Gather information of the EC2 VPC net in eu-west-1 
        ec2_vpc_net_facts: 
          region: eu-west-1 
        register: aws_simple_net 
      - name: Gather information of the EC2 VPC subnet in eu-west-1 
        ec2_vpc_subnet_facts: 
          region: eu-west-1 
          filters: 
            vpc-id: '{{ aws_simple_net.vpcs.0.id }}' 
        register: aws_simple_subnet 
      - name: Ensure wssg Security Group is present 
        ec2_group: 
          name: wssg 
          description: Web Security Group 
          region: eu-west-1 
          vpc_id: '{{ aws_simple_net.vpcs.0.id }}' 
          rules: 
          - proto: tcp 
            from_port: 22 
            to_port: 22 
            cidr_ip: 0.0.0.0/0 
          - proto: tcp 
            from_port: 80 
            to_port: 80 
            cidr_ip: 0.0.0.0/0 
          - proto: tcp 
            from_port: 443 
            to_port: 443 
            cidr_ip: 0.0.0.0/0 
          rules_egress: 
          - proto: all 
            cidr_ip: 0.0.0.0/0 
        register: aws_simple_wssg 
      - name: Ensure dbsg Security Group is present 
        ec2_group: 
          name: dbsg 
          description: DB Security Group 
          region: eu-west-1 
          vpc_id: '{{ aws_simple_net.vpcs.0.id }}' 
          rules: 
          - proto: tcp 
            from_port: 3036 
            to_port: 3036 
            group_id: '{{ aws_simple_wssg.group_id }}' 
          rules_egress: 
          - proto: all 
            cidr_ip: 0.0.0.0/0 
        register: aws_simple_dbsg 
      - name: Setup instances 
        ec2: 
          assign_public_ip: '{{ item.assign_public_ip }}' 
          image: ami-7abd0209 
          region: eu-west-1 
          exact_count: 1 
          key_name: fale 
          count_tag: 
            Name: '{{ item.name }}' 
          instance_tags: 
            Name: '{{ item.name }}' 
          instance_type: t2.micro 
          group_id: '{{ item.group_id }}' 
          vpc_subnet_id: '{{ aws_simple_subnet.subnets.0.id }}' 
          volumes: 
            - device_name: /dev/sda1 
              volume_type: gp2 
              volume_size: 10 
              delete_on_termination: True 
        register: aws_simple_instances 
        with_items: 
        - name: ws01.simple.aws.fale.io 
          group_id: '{{ aws_simple_wssg.group_id }}' 
          assign_public_ip: True 
        - name: ws02.simple.aws.fale.io 
          group_id: '{{ aws_simple_wssg.group_id }}' 
          assign_public_ip: True 
        - name: db01.simple.aws.fale.io 
          group_id: '{{ aws_simple_dbsg.group_id }}' 
          assign_public_ip: False 
      - name: Add route53 rules for instances 
        route53: 
          command: create 
          zone: aws.fale.io 
          record: '{{ item.tagged_instances.0.tags.Name }}' 
          type: A 
          ttl: 1 
          value: '{{ item.tagged_instances.0.public_ip }}' 
          wait: True 
        with_items: '{{ aws_simple_instances.results }}' 
        when: item.tagged_instances.0.public_ip 
      - name: Add internal route53 rules for instances 
        route53: 
          command: create 
          zone: aws.fale.io 
          private_zone: True 
          record: '{{ item.tagged_instances.0.tags.Name }}' 
          type: A 
          ttl: 1 
          value: '{{ item.tagged_instances.0.private_ip }}' 
          wait: True 
        with_items: '{{ aws_simple_instances.results }}' 
</pre><p>Running it with <code class="literal">ansible-playbook playbooks/aws_simple_provision.yaml</code>, we will have an output similar to:</p><pre class="programlisting">
<strong>PLAY [localhost] ***************************************************</strong>
<strong>TASK [setup] *******************************************************</strong>
<strong>ok: [localhost]

</strong>
<strong>TASK [Gather information of the EC2 VPC net in eu-west-1] **********</strong>
<strong>ok: [localhost]

</strong>
<strong>TASK [Gather information of the EC2 VPC subnet in eu-west-1] *******</strong>
<strong>ok: [localhost]

</strong>
<strong>TASK [Ensure wssg Security Group is present] ***********************</strong>
<strong>changed: [localhost]

</strong>
<strong>TASK [Ensure dbsg Security Group is present] ***********************</strong>
<strong>changed: [localhost]

</strong>
<strong>TASK [Setup instances] *********************************************</strong>
<strong>changed: [localhost] =&gt; (item={u'group_id': u'sg-950c2cf2', u'name': u'ws01.simple.aws.fale.io', u'assign_public_ip': True})</strong>
<strong>changed: [localhost] =&gt; (item={u'group_id': u'sg-950c2cf2', u'name': u'ws02.simple.aws.fale.io', u'assign_public_ip': True})</strong>
<strong>changed: [localhost] =&gt; (item={u'group_id': u'sg-940c2cf3', u'name': u'db01.simple.aws.fale.io', u'assign_public_ip': False})

</strong>
<strong>TASK [Add route53 rules for instances] *****************************</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>skipping: [localhost] =&gt;
    ....
</strong>
<strong>TASK [Add internal route53 rules for instances] ******************</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>PLAY RECAP ****************************************************</strong>
<strong>localhost                  : ok=7    changed=4    unreachable=0    failed=0</strong>
</pre></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec61"/>Complex AWS deployment</h2></div></div></div><p>In this paragraph, we will slightly change the previous example to move one of the web servers to another AZ within the same region. To do so, we are going to make a new file in <code class="literal">playbooks/aws_complex_provision.yaml</code> which will be very similar to the previous one, with one difference located in the part that helps us provision the machines. In fact, we will use the following code instead of the one we used on the previous run:</p><pre class="programlisting">      - name: Setup instances 
        ec2: 
          assign_public_ip: '{{ item.assign_public_ip }}' 
          image: ami-7abd0209 
          region: eu-west-1 
          exact_count: 1 
          key_name: fale 
          count_tag: 
            Name: '{{ item.name }}' 
          instance_tags: 
            Name: '{{ item.name }}' 
          instance_type: t2.micro 
          group_id: '{{ item.group_id }}' 
          vpc_subnet_id: '{{ item.vpc_subnet_id }}' 
          volumes: 
            - device_name: /dev/sda1 
              volume_type: gp2 
              volume_size: 10 
              delete_on_termination: True 
        register: aws_simple_instances 
        with_items: 
        - name: ws01.simple.aws.fale.io 
          group_id: '{{ aws_simple_wssg.group_id }}' 
          assign_public_ip: True 
          vpc_subnet_id: '{{ aws_simple_subnet.subnets.0.id }}' 
        - name: ws02.simple.aws.fale.io 
          group_id: '{{ aws_simple_wssg.group_id }}' 
          assign_public_ip: True 
          vpc_subnet_id: '{{ aws_simple_subnet.subnets.1.id }}' 
        - name: db01.simple.aws.fale.io 
          group_id: '{{ aws_simple_dbsg.group_id }}' 
          assign_public_ip: False 
          vpc_subnet_id: '{{ aws_simple_subnet.subnets.0.id }}' 
</pre><p>As you can see, we have put the <code class="literal">vpc_subnet_id</code> in a variable, so that we can use a different one for the <code class="literal">ws02</code> machine. Due to the fact that AWS already provides two subnets by default (and every subnet is tied to a different AZ), it's enough to use the following AZ. Security groups and Route 53 code does not need to be changed since it does not work at a subnet/AZ level, but at a VPC level (for security groups and internal Route 53 zone) or global level (for public Route 53).</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec45"/>DigitalOcean</h1></div></div></div><p>Compared to Amazon Web Services, DigitalOcean seems to be very incomplete. DigitalOcean, until a few months ago only provided droplets, SSH key management, and DNS management. At the time of writing this, DigitalOcean has very recently launched an additional block storage service. The advantages of DigitalOcean compared to many competitors are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Lower prices than AWS</li><li class="listitem" style="list-style-type: disc">Very easy APIs</li><li class="listitem" style="list-style-type: disc">Very well documented APIs</li><li class="listitem" style="list-style-type: disc">The droplets are very similar to standard virtual machines (they don't do weird customization)</li><li class="listitem" style="list-style-type: disc">The droplets are very quick to go up and down</li><li class="listitem" style="list-style-type: disc">Since DigitalOcean has a very simple networking stack, it's way more efficient than the AWS one</li></ul></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec62"/>Droplets</h2></div></div></div><p>Droplets are the main service offered by DigitalOcean and are compute instances which are very similar to Amazon EC2 classic. DigitalOcean relies on the <strong>Kernel Virtual Machine</strong> (<strong>KVM</strong>) to virtualize the machines, assuring very high performance and security. Since they do not change KVM in any sensible way, and since KVM is open source and available on any Linux machine, this allows system administrators to create identical environments on private and public clouds. DigitalOcean droplets will have one external IP and they can be eventually added to a virtual network that will allow your machines to use internal IPs.</p><p>Different from many other comparable services, DigitalOcean allows your droplets to have IPv6 IPs in addition to the IPv4 ones. This service is free of charge.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec63"/>SSH key management</h2></div></div></div><p>Every time you want to create a droplet, you have to specify if you want a specific SSH key assigned to the <code class="literal">root</code> user or if you want a password (which will have to be changed at the first login). To be able to choose an SSH key, you need an interface to upload it. DigitalOcean allows you to do this using a very simple interface which allows you to list the current keys, as well as create and delete keys.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec64"/>Private networking</h2></div></div></div><p>As mentioned in the droplet paragraph, DigitalOcean allows us to have a private network where our machine can communicate with another. This allows segregation of services (like a database service) only on the internal network to allow a higher level of security. Since by default, MySQL binds on all available interfaces, we will need to tweak the database role a little bit to only bind on the internal network.</p><p>To recognize the internal network from the external one there are many ways, due to some DigitalOcean peculiarities:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Private networks are always in the <code class="literal">10.0.0.0/8</code> network, while public IPs are never in that network</li><li class="listitem" style="list-style-type: disc">The public network is always <code class="literal">eth0</code> while the private network is always <code class="literal">eth1</code></li></ul></div><p>Based on your portability needs, you can use either one of those strategies to understand where to bind your services.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec65"/>Adding an SSH key in DigitalOcean</h2></div></div></div><p>You need to have a DigitalOcean user with the credit card set up, and have obtained API key. To perform those operations, you can use DigitalOcean web interface. We can now start to use Ansible to add our SSH key to our DigitalOcean cloud. To do so, we need to create a file called <code class="literal">playbooks/do_provision.yaml</code> with the following structure:</p><pre class="programlisting">    - hosts: localhost 
      tasks: 
      - name: Add the SSH Key to Digital Ocean 
        digital_ocean: 
          state: present 
          command: ssh 
          name: SSH_KEY_NAME 
          ssh_pub_key: 'ssh-rsa AAAA...' 
          api_token: XXX 
        register: ssh_key 
</pre><p>In my case, this is my file content:</p><pre class="programlisting">    - hosts: localhost 
      tasks: 
      - name: Add the SSH Key to Digital Ocean 
        digital_ocean: 
          state: present 
          command: ssh 
          name: faleKey 
          ssh_pub_key: 'ssh-rsa AAAA...==' 
          api_token: 259...b3b 
<strong>    </strong>    register: ssh_key 
</pre><p>Then we can execute it with:</p><pre class="programlisting">
<strong>    ansible-playbook -i localhost, playbooks/do_provision.yaml</strong>
</pre><p>and you will have a result similar to the following:</p><pre class="programlisting">
<strong>PLAY [localhost] **************************************************</strong>
<strong>TASK [setup] ******************************************************</strong>
<strong>ok: [localhost]

</strong>
<strong>TASK [Add the SSH Key to Digital Ocean] ***************************</strong>
<strong>changed: [localhost]

</strong>
<strong>PLAY RECAP ********************************************************</strong>
<strong>localhost                  : ok=2    changed=1    unreachable=0    failed=0</strong>
</pre><p>This task is idempotent so we can execute it multiple times. In case the key has already been uploaded, the SSH key ID will be returned at every run.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec66"/>Deployment in DigitalOcean</h2></div></div></div><p>At the time of writing, the only way to create a droplet in Ansible is by using the <code class="literal">digital_ocean</code> module which could be soon deprecated since many of its features are now done in a better, cleaner way by other modules and there is already a bug on Ansible bug tracker to track its complete rewrite and possible deprecation. My guess is that the new module will be called <code class="literal">digital_ocean_droplet</code> and will have a similar syntax, but at the moment there is no code so it's just my guess.</p><p>To create the droplets, we will have to use the <code class="literal">digital_ocean</code> module with a syntax similar to the following:</p><pre class="programlisting">      - name: Ensure the ws and db servers are present 
        digital_ocean: 
          state: present 
          ssh_key_ids: KEY_ID 
          name: '{{ item }}' 
          api_token: DIGITAL_OCEAN_KEY 
          size_id: 512mb 
          region_id: lon1 
          image_id: centos-7-0-x64 
          unique_name: True 
        with_items: 
        - WEBSERVER 1 
        - WEBSERVER 2 
        - DBSERVER 1 
</pre><p>To make sure that all our provisioning is done completely and in a sane way, I always suggest creating one single provision file for the whole infrastructure. So, in my case, I'll add the following task to the <code class="literal">playbooks/do_provision.yaml</code> file:</p><pre class="programlisting">      - name: Ensure the ws and db servers are present 
        digital_ocean: 
          state: present 
          ssh_key_ids: '{{ ssh_key.ssh_key.id }}' 
          name: '{{ item }}' 
          api_token: 259...b3b 
          size_id: 512mb 
          region_id: lon1 
          image_id: centos-7-0-x64 
          unique_name: True 
        with_items: 
        - ws01.do.fale.io 
        - ws02.do.fale.io 
        - db01.do.fale.io 
        register: droplets 
</pre><p>After this, we can add the domain with the <code class="literal">digital_ocean_domain</code> module:</p><pre class="programlisting">      - name: Ensure domain resolve properly
        digital_ocean_domain:
          api_token: 259...b3b
          state: present
          name: '{{ item.droplet.name }}'
          ip: '{{ item.droplet.ip_address }}'
        with_items: '{{ droplets.results }}'
</pre><p>So, putting all this together, our <code class="literal">playbooks/do_provision.yaml</code> will look like this:</p><pre class="programlisting">    - hosts: localhost 
      tasks: 
      - name: Add the SSH Key to Digital Ocean 
        digital_ocean: 
          state: present 
          command: ssh 
          name: faleKey 
          ssh_pub_key: 'ssh-rsa AAAA...==' 
          api_token: 7e7...f6f 
        register: ssh_key 
      - name: Ensure the ws and db servers are present 
        digital_ocean: 
          state: present 
          ssh_key_ids: '{{ ssh_key.ssh_key.id }}' 
          name: '{{ item }}' 
          api_token: 259...b3b 
          size_id: 512mb 
          region_id: lon1 
          image_id: centos-7-0-x64 
          unique_name: True 
        with_items: 
        - ws01.do.fale.io 
        - ws02.do.fale.io 
        - db01.do.fale.io 
        register: droplets 
      - name: Ensure domain resolve properly 
        digital_ocean_domain: 
          api_token: 259...b3b 
          state: present 
          name: '{{ item.droplet.name }}' 
          ip: '{{ item.droplet.ip_address }}' 
        with_items: '{{ droplets.results }}' 
</pre><p>So we can now run it with the following command:</p><pre class="programlisting">
<strong>ansible-playbook -i localhost, playbooks/do_provision.yaml</strong>
</pre><p>We will see a result similar to the following:</p><pre class="programlisting">
<strong>PLAY [localhost] **************************************************</strong>
<strong>TASK [setup] ******************************************************</strong>
<strong>ok: [localhost]

</strong>
<strong>TASK [Add the SSH Key to Digital Ocean] ***************************</strong>
<strong>changed: [localhost]

</strong>
<strong>TASK [Ensure the ws and db servers are present] *******************</strong>
<strong>changed: [localhost] =&gt; (item=ws01.do.fale.io)</strong>
<strong>changed: [localhost] =&gt; (item=ws02.do.fale.io)</strong>
<strong>changed: [localhost] =&gt; (item=db01.do.fale.io)

</strong>
<strong>TASK [Ensure domain resolve properly] *****************************</strong>
<strong>changed: [localhost] =&gt;
    ....

</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>changed: [localhost] =&gt;
    ....
</strong>
<strong>PLAY RECAP ************************************************************</strong>
<strong>localhost                  : ok=4    changed=3    unreachable=0    failed=0</strong>
</pre></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec46"/>Summary</h1></div></div></div><p>In this chapter, we have seen how we can provision our machines in both the AWS cloud and the DigitalOcean one. In the case of the AWS cloud, we have seen two different examples, one very simple and one slightly more complex.</p><p>In the next chapter, we will talk about getting notified by Ansible if something went wrong.</p></div></body></html>