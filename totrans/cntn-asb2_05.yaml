- en: Containers at Scale with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is by far one of the most popular open source projects to take the
    IT world by storm. It seems like almost everywhere you go, every blog you read,
    or news article you encounter, tells the tale of how Kubernetes has revolutionized
    the way DevOps and IT infrastructure are handled. With good reason, Kubernetes
    has truly taken a firm grasp of the IT landscape and introduced new concepts and
    ways of looking at infrastructure like no other platform before it. You might
    be in the camp of IT professionals who have heard of Kubernetes, but you have
    no idea what it is or how it can really benefit your infrastructure. Or, you could
    be where most of us are today, in the process of containerizing applications and
    workloads but don’t want to dabble in the additional complexity and murky water
    of Kubernetes just yet. Finally, you could be one of those lucky DevOps engineers
    or IT administrators who have successfully adopted containers and Kubernetes and
    is able to really reap the reliability and flexibility that Kubernetes provides.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this chapter is to provide an overview of what Kubernetes is,
    how it works (at a high level), and how to deploy your containerized applications
    to Kubernetes clusters using Ansible Container. Before we dive in too deep, you
    might ask, what is Kubernetes exactly? Kubernetes is a platform developed by Google
    for deploying, managing, configuring, and orchestrating containers at both small
    and large scales. Kubernetes was started as an internal project at Google, known
    as **Borg**, for managing the automatic deployment and scaling of containers across
    the vast Google infrastructure footprint. Based on some real-world lessons learned
    with Borg, Google released Kubernetes as an open source project so that other
    users and organizations could leverage the same flexibility to deploy containers
    at scale. Using Kubernetes, one can easily run containerized applications across
    multiple clustered nodes, automatically maintaining the desired number of replicas,
    service endpoints, and loadbalancing across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we have looked closely at how we can use the Ansible Container
    platform to quickly and reliably build container images using Ansible Playbooks
    and run those containers on our local workstation. Since we now understand quite
    well how to build version control and configuration management inside of our containers,
    the next step is using configuration management to declare how our applications
    should run outside of our container. This is the gap that Kubernetes fills. And,
    yes, it is just as awesome as it sounds. Ready? Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started using Google Cloud Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an application in Kubernetes using kubectl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a Kubernetes manifest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and updating containers in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief overview of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Admittedly, when one thinks of Kubernetes, one might immediately think of the
    complexity and multifaceted hierarchy of concepts associated with Kubernetes and
    be quick to think that this chapter will be over the reader's head in terms of
    how to understand and apply these concepts. Most users who have unsuccessfully
    attempted to venture into Kubernetes in the past may still feel the scars and
    be wary about moving forward with Kubernetes. Container automation using Kubernetes
    can quickly get quite complicated, but the rewards for learning and using Kubernetes
    are vast. Before we go forward, I must stress to the reader that Kubernetes is
    quite a complex platform. Attempting to explain in detail every feature and function
    of Kubernetes would take an entire book, if not longer. In fact, there has been
    a multitude of books written on container orchestration using Kubernetes that
    I would direct the reader's attention to should you would want to dig deeper into
    your understanding of these concepts. The point of this chapter is to introduce
    the reader to a basic understanding of what Kubernetes is, the primary functionality,
    and how the reader can quickly get started using it to optimize the deployment
    of containers. There is a lot more to be said about Kubernetes than the scope
    of this book has the time to go into, so if the reader wants to learn more about
    Kubernetes, I would strongly suggest checking out the documentation on the Kubernetes
    website at [https://kubernetes.io/docs](https://kubernetes.io/docs).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book so far, we have looked at using Ansible Container to build
    Docker containers that run on our local workstation or a remote server that has
    Docker installed and running on it. Docker provides us with a usable container
    runtime environment that has the functionality to start containers, expose ports,
    mount system volumes, and provide basic network connectivity using a bridged interface
    and IP Network Address Translation, or NAT. Docker does a very good job at running
    containers but does not provide the user with very much functionality beyond that.
    What happens when your container crashes? What do you do when you need to scale
    out your application to more nodes, racks, or data centers? What happens when
    a container on one host needs to access resources in a container running on a
    separate host? This is the exact type of use case that tools such as Kubernetes
    address. Think of Kubernetes essentially as a service that uses a scheduler and
    API to proactively monitor the current state of containers running in Docker (or
    another container runtime) and continuously attempts to drive it towards the desired
    state specified by the operator. For example, say you have a 4-node Kubernetes
    cluster running 3 instances of your application container. If the operator (you)
    instructs the Kubernetes API that you want the fourth instance of your application
    container running, Kubernetes will identify that you currently have three running
    instances and immediately schedule a fourth container to be created. Using a bin-packing
    algorithm, Kubernetes intrinsically understands that containers should be scheduled
    to run on separate hosts to provide high availability and make the most use of
    cluster resources. In the example above, the fourth container scheduled will most
    likely be scheduled to run on the fourth cluster host, provided no outstanding
    configuration has been put into place that would prevent new container workloads
    from running on that host. Furthermore, if one of the hosts in our cluster goes
    down, Kubernetes is intelligent enough to recognize the disparity and reschedule
    those workloads to run on different hosts until the downed node has been restored.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the flexible configuration management capabilities Kubernetes
    provides, it is also known for its unique ability to provide resilient networking
    resources to containers such as service discovery, DNS resolution, and load balancing
    across containers. In other words, Kubernetes has the innate ability to provide
    internal DNS resolution based on the services running in the cluster. When new
    pods are added to the service, Kubernetes will automatically see the new containers
    and update the DNS endpoints so that the new containers can be served by calling
    the internal service domain name within the cluster. This ensures that other containers
    can talk directly to other containerized services by calling internal domain names
    and cluster IP addresses within the Kubernetes overlay network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes incorporates many new concepts that might be somewhat foreign if
    you come from a background of working with static container deployments. Throughout
    this chapter, these concepts will be referred to as we learn more about Kubernetes,
    so it is important to have a grasp of what these terms mean as we go forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pod**: a pod represents one or more application containers running in the
    Kubernetes cluster. By default, a pod definition names at least one container
    that the user wishes to run in the cluster, including any additional environment
    variables, command or entrypoint configuration the user wants the pod to run with.
    If the pod includes more than one container definition, all containers running
    in the pod share the pod network and storage resources. For example, you could
    run a pod that consisted of a web server container as well a caching server. From
    the perspective of the pod, the web server might run on the localhost port 80,
    and the cache would likewise run on localhost port `11211`. From the perspective
    of Kubernetes, the pod itself would have a single IP address internal to the cluster
    the services would be exposed on, but in reality, would consist of two entirely
    separate containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment**: A deployment is an object in Kubernetes that defines pods which
    will be running in the cluster. Deployments consist of a variety of parameters,
    such as the name of the container image, volume mounts, and the number of replicas
    to run. In order to delete pods from a Kubernetes cluster, the deployment must
    be deleted. If you simply attempt to delete pods, you will see that Kubernetes
    attempts to recreate those pods. This happens due to the fact that the deployment
    object is informing the cluster that those pods should be running, and the controller
    manager (more on this later) will attempt to bring the cluster back into the desired
    state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels**: Labels are key-value pairs that can be assigned to almost any object
    in Kubernetes. Labels can be assigned to objects to organize subsets of resources
    in the cluster. For example, if you have a cluster that runs multiple deployments
    of the same pods, they can be labeled differently to indicate different uses.
    Labels can even be leveraged to by the scheduler to determine where and when pods
    should be running in across the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service**:A service defines a logical subgrouping of pods (usually by a label
    selector) and the methods by which they should be accessed by other resources
    in the cluster. For example, you could create a service that exposes a set of
    pods to the outside world. A selector such as a label could be used to determine
    which pods should be exposed. Later, if pods are added or removed from the cluster,
    Kubernetes will automatically scale the service, provided the new pods are running
    with the same selector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make this functionality transparent, Kubernetes provides multiple services
    running in the cluster that work in conjunction to ensure that the cluster and
    applications are continuously in the desired state. Collectively, these services
    are known as the Kubernetes Control Plane. The control plane is what allows the
    function, manage running containers, and maintain the state of  nodes and resources
    across the cluster. Let’s take a quick look at those now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KubeCTL**:`kubectl`, (pronounced kube-control), is the command-line tool
    for interacting with Kubernetes. `Kubectl` gives you direct access to the Kubernetes
    API to schedule new deployments, interact with Pods, expose deployments, and more.
    The `kubectl` tool requires a set of credentials in order to access the Kubernetes
    API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes API Server**: The Kubernetes API server is responsible for accepting
    input from the operator, either from the `kubectl` command-line tool or by direct
    access to the API itself. The Kubernetes API is responsible for coordinating information
    to the rest of the cluster to execute the desired state. It should also be noted
    that the Kubernetes API server depends on the ETCD service to store and retrieve
    information about the cluster nodes and services running in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes Scheduler**: The Kubernetes scheduler is responsible for scheduling
    new workloads across the cluster nodes. Core to this responsibility is monitoring
    the cluster to ensure that available resources are present in the cluster to run
    pods, as well as ensuring that servers are available and reachable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes Controller Manager**: The controller manager is primarily concerned
    with desired state compliance across the cluster. The controller manager service
    interacts with the ETCD service and watches for new jobs and requests coming in
    through the API server. When a new request is received and stored in ETCD, the
    controller manager kicks off a new job in tandem with the scheduler to ensure
    that the cluster is in the desired state defined by the operator. The controller
    manager accomplishes this by using control loops to continuously monitor the state
    of the cluster and immediately correct any discrepancies it sees between the current
    and desired state. When you delete a Kubernetes pod and a new one automatically
    gets created, you have the controller manager to thank for that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ETCD**: ETCD is a distributed key-value store created by CoreOS, which is
    used to store configuration information across the Kubernetes cluster. As stated
    previously, ETCD is primarily written to by the Kubernetes API server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container Networking Interface**: The Container Network Interface project,
    or CNI, attempts to bring additional network functionality then what comes out
    of the box with Kubernetes. CNI provides interfaces and plugin support to allow
    various network plugins to be deployed within Kubernetes clusters. This allows
    Kubernetes to provide overlay network connectivity to containers distributed across
    hosts so that containers do not have to rely on the relatively limited networking
    space provided on the Kubernetes hosts. Common third-party plugins that implement
    the CNI standard are Flannel, Weave, and Calico.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubelet**: Kubelet is a service which runs on every host in the Kubernetes
    cluster. The Kubelet’s primary responsibility is to leverage the underlying container
    runtime (Docker or rkt) to create and manage pods on cluster nodes according to
    the instructions received by the API, the scheduler, and controller manager. The
    Kubelet service does not manage containers or pods running on the host that were
    not created by Kubernetes. Think of the Kubelet as the translation layer between
    Docker and Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have an understanding of the Kubernetes platform and how it works,
    we can start using Kubernetes to run some of the containers we built earlier in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the Google Cloud platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the many chapters in this book, we have worked primarily in a single-node
    Vagrant lab that comes preloaded with most of the tools and utilities you need
    to get started using Docker and Ansible Container to initialize, build, run, and
    destroy containers through the various examples and lab exercises. Unfortunately,
    due to the complexity of Kubernetes, it is very difficult to run a Kubernetes
    environment within the Vagrant lab we have used so far. There are methods, but
    they would require more computing power and explanation that extends beyond the
    scope of this book. As a solution, I would suggest that the reader signs up for
    a free-tier account on Google Cloud Platform to quickly spin up a three-node Kubernetes
    cluster in only a few minutes, which can be used using `kubectl` command-line
    agent from the single-node Vagrant lab. At the time of writing, Google is offering
    a free $300.00 credit for signing up for a free-tier Google Cloud account. Once
    the $300.00 allowance has expired, Google will not charge you for further use
    without explicit authorization. In and of itself, this is more than enough to
    run our simple cluster and cover many of the major Kubernetes concepts.
  prefs: []
  type: TYPE_NORMAL
- en: If you are unable to sign up for a Google Cloud Platform account, you can spin
    up a local Kubernetes node on your workstation absolutely free of charge using
    the Minikube project. Configuring Minikube to work on your laptop with proper
    reachability for `kubectl` commands to work is fairly straightforward if you are
    using the Virtualbox hypervisor. If you are interested, you can find more information
    on Minikube at [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube).
  prefs: []
  type: TYPE_NORMAL
- en: Before we can proceed with creating our Google Cloud Kubernetes cluster, we
    need to first sign up for an account at [https://cloud.google.com/free/](https://cloud.google.com/free/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have created a free account, it will prompt you to create a new project.
    You can name your project anything you like, as Google Cloud will assign a unique
    identifier to it within the console. I named mine `AC-Kubernetes-Demo`. If the
    signup process does not prompt you to create a new project, from the main console
    you can select: Projects and click the + sign button to create a new project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d4b6058-b7cf-49f2-a96e-1ee79f563f02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once a project has been created, we can create a Kubernetes cluster using the
    Google Container Engine. From the main console window, on the left-hand side menu,
    select Container Engine| Container Clusters from the submenu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77686a44-c8ad-403c-a4ea-c73236d4a3fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the purposes of this example, and also to make the most of the free allowance
    provided to use the Google Container Engine, we will create a three-node container
    cluster using the minimum specifications. To do this, from the Container clusters
    dashboard, select the button Create Cluster. This will drop you into a form that
    will allow you to select your cluster specifications. I created mine to the following
    specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Name: Cluster-1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster Version: 1.6.9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 vCPU per Node (3 total vCPUs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container Optimized OS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disabled Automatic Upgrades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the cluster has been created, you should see a cluster that resembles
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c791484-ef85-4c0b-aa30-f1c5ca681b9f.png)'
  prefs: []
  type: TYPE_IMG
- en: The most recent versions of the Google Cloud interface may have changed since
    the time of writing. You may have to set up your Kubernetes cluster using a slightly
    different set of steps, or customization options. The default settings should
    be sufficient to create a cluster that isn’t so expensive that it quickly burns
    through your $300.00 allowance. Remember, the more resources you allocate to your
    cluster, the faster you will use up your credit!
  prefs: []
  type: TYPE_NORMAL
- en: 'Once our cluster has been fully deployed, we can connect to it from our Vagrant
    development lab. To do this, we need to first initialize the `kubectl` tool using
    the `Gcloud` interface. By default, these packages are not installed in the Vagrant
    lab to save on time and complexity when provisioning the VM. To enable this functionality,
    we need to modify the Vagrantfile, located in the `root` directory of the official
    Git repository for this book. Towards the bottom of the Vagrant file, you will
    see a section titled: `#Un-Comment this section to Install the Google Cloud SDK`.
    Un-commenting this section should result in the following changes to the Vagrantfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After making these changes, save the file, and launch the lab VM using the
    `vagrant up` command. If the lab VM is already running, you can use the `vagrant
    provision` command to re-provision the running VM, or simply destroy and re-create
    the VM as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the Vagrant lab VM has the `Google Cloud SDK` and `kubectl` installed,
    Execute the command `gcloud init` and, when prompted to log in, enter `Y` to confirm
    and continue logging in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `Gcloud` CLI tool should then return a hyperlink that will allow you to
    authorize your Vagrant lab with your Google Cloud account. Once you have granted
    permission to use your Google Cloud account, your web browser should return a
    code you can enter on the command line to complete the authorization process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CLI wizard should then prompt you to select a project. The project you
    just created should be displayed with a list of options. Select the project we
    just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It will then prompt you if you wish to configure the Google Compute Engine.
    This is not a necessary step, but if you opt to perform it, you will be presented
    with a list of geographic regions to select from. Select the one closest to you.
    Finally, your Google Cloud account should be connected to your Vagrant lab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can set up connectivity to our Kubernetes cluster using the `kubectl`
    tool. This can be accomplished by selecting the Connect button on the Container
    Engine dashboard, next to our cluster. A screen should pop up displaying details
    on how to connect to our cluster from our initialized Vagrant lab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7520e02d-5b01-48cf-8c72-854ae11710ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Copy and paste that command into your Vagrant environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This should cache the default Kubernetes credentials required for access to
    our cluster from the `kubectl` command-line tool. `kubectl` will already be installed
    in the Vagrant lab VM due to the changes made to the Vagrantfile earlier in the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `kubectl` is already installed, we can validate the connectivity to your
    Kubernetes cluster by executing `kubectl cluster-info` to view details about our
    running cluster. I censored the IP details for my cluster environment. However,
    your output will show all the relevant addresses for the core services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also run `kubectl get nodes` to see an output of the nodes the cluster
    consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Deploying an application in Kubernetes using kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KubeCTL or Kube Control is the official command line interface into the Kubernetes
    API Server and the rest of the Kubernetes Control Plane. Using the `kubectl` tool,
    you can view the status of pods, access cluster resources, and even exec into
    running pods for troubleshooting purposes. In this portion of the chapter, we
    will look at the basics of using `kubectl` to manually describe deployments, scale
    pods, and create services to access the pods. This is beneficial to understanding
    the basic concepts of Kubernetes to understand how Ansible Container is able to
    automate many of these tasks using the native Kubernetes modules available to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at some of the most common `kubectl` options and syntax you
    are likely to run into working with Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl get`: `kubectl get` is used to return resources that currently exist
    in the Kubernetes cluster. Commonly, this command is used to get a list of pods
    currently running or nodes in the cluster. Think of this command as being similar
    to the docker ps command. Examples of `get` commands are: `kubectl get pods` and
    `kubectl get deployments`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl describe`: `describe` is used to view more verbose details about a
    particular cluster resource. If you want to know the latest state of a resource
    or current details about how the resource is running you can use the describe
    command. `describe` is very helpful since you can call out a specific cluster
    resource, such as a pod, service, deployment, or replication controller to view
    the details pertaining directly to that instance. `describe` is also very useful
    for troubleshooting issues across Kubernetes environments. Examples of `describe`
    are: `kubectl describe pod`, and `kubectl describe node`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl run`: `kubectl run` functions quite similar to the `docker run` command
    we explored earlier in this book. Run is primarily used to start a new deployment
    and get pods up and running quickly in the Kubernetes cluster. The use case for
    run is rather limited, since more complex deployments are better suited for the
    `create` and `apply` commands. However, for testing or getting containers running
    quickly and efficiently, `run` is a fantastic tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl create`: Create is used to create new cluster resources such as pods,
    deployments, namespaces, or services. Create functions very similar to apply and
    run, with the caveat that it is used solely for launching new objects. Using create,
    you can use the `-f` flag to pass in a manifest file or direct URL to launch more
    complex items than you could with `kubectl` `run`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl apply`: Apply is often confused with `create` since the syntax and
    functionality is so similar. Apply is used to update Kubernetes resources that
    exist in the cluster, whereas `create` is used to create new resources. For example,
    if you created a series of pods based on a Kubernetes manifest that you launched
    using `kubectl` create, you could use `kubectl` apply to update any changes you
    may have made to the manifests. The Kubernetes Control Plane will analyze the
    manifest an attempt to make the changes necessary to bring the cluster resources
    into the desired state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl delete: delete` is rather self-explanatory since the primary function
    is used to delete objects from the Kubernetes cluster. Similar to create and apply,
    you can use the `-f` flag to pass in a Kubernetes manifest file that was created
    or updated previously and use that as an identifier to delete those resources
    from the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you will notice, the `kubectl` uses a verb/noun syntax that is quite easy
    to remember. Everything you do with `kubectl` will take a verb argument (get,
    describe, create, apply), followed by the objects in kubernetes you wish to act
    on: (pods, namespaces, nodes, and other specific resources). There are a lot more
    command options available to you using the `kubectl` tool, but these are by far
    some of the most common options you are likely to use when starting with Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: To view all of the possible parameters that Kubectl takes, you can use `kubectl
    --help or kubectl` subcommand `--help` to get help on a particular function or
    subcommand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we now have access to the Kubernetes cluster from our Vagrant lab, we
    can use the `kubectl` tool to explore the cluster resources and objects. The first
    command that we will look at is the `kubectl get pods` command. We will use this
    to return a list of pods that exist in all namespaces across the cluster. Simply
    typing in `kubectl get pods` will return nothing since Kubernetes resources are
    separated by namespaces. Namespaces provide a logical separation of Kubernetes
    resources based on DNS and networking rules, which allow users to have fine-grained
    control over multiple deployments that simultaneously exist in the same cluster.
    Currently, everything that exists in the Kubernetes cluster exists as running
    processes critical to the functionality of the Kubernetes control plane and exist
    in the `kube-system` namespace. To see a list of everything running in all namespaces,
    we can use the `kubectl get pods` command, passing in the `--all-namespaces` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Your list may look slightly different to the output I have provided here, based
    on the version of Kubernetes your cluster is running and any changes the Google
    Container Engine platform may have introduced since the time of writing. However,
    what you will see is a list of containers that are running to support the Kubernetes
    Control Plane, such as the `kube-proxy`, `kube-dns`, and logging mechanisms using
    `fluentd`. The default output will show the name of the pods, how long they have
    been running (the age), the number of running replicas, and the number of times
    the pods have restarted.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `-o wide` flag to see more details, such as the namespace and
    overlay network IP addresses assigned to the pods. For example, `kubectl get pods
    -o wide --all-namespaces`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a firm understanding of listing pods, we can use the `kubectl
    run` command to start our first deployment. In [Chapter 3](4b15cefb-8d9c-48b7-8927-126501886315.xhtml),
    *Your First Ansible Container Project* we learned how to build an NGINX container
    using a community-developed container-enabled role and uploaded it to our personal
    Docker Hub repository. We can use the `kubectl run` command to download our container,
    quickly create a new Kubernetes deployment called `nginx-web` and get this pod
    running in our cluster. In order to pull the pod from our repository, we will
    need to provide the fully qualified container name in the format: `image-repository/username/containername`.
    Furthermore, we need to map the port to port `8000` since the community-developed
    role leveraged that port by default. Finally, we will be launching this deployment
    in the `default` namespace, so no additional namespace configuration needs to
    be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we try running `kubectl get pods`, we will see a single NGINX pod running
    the default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can use the `kubctl get deployments` function to see what the
    current state of deployments for the default namespace looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the get pods and get deployments output, we have a single
    deployment called `nginx-web`, which consists of a single pod and a single container
    within that pod. This is in full agreement with the input we provided into the
    Kubernetes API server using the `kubectl run` command. If we attempt to delete
    this pod, there will be a delta between the desired state and the current status
    of our deployment. Kubernetes will then attempt to bring the cluster back into
    the desired state by recreating the deleted cluster resource. Let’s try doing
    a delete on the NGINX pod we created and see what happens. Usually, this happens
    so quickly, you will need to pay attention to the name of the pod as well as the
    age to see that the change has occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted to actually delete the pods from the cluster permanently, we could
    use the delete command on the deployment itself, using the syntax: `kubectl delete
    deployment nginx-web ` This would declare a new desired state, namely that we
    no longer want the deployment `nginx-web` present and all pods in that deployment
    should likewise be deleted.'
  prefs: []
  type: TYPE_NORMAL
- en: Describing Kubernetes resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes can also be used to view detailed information about the pods or other
    objects we have instantiated in our cluster. We can do this using the `kubectl
    describe` command. Describe can be used to see a detailed view of almost any resource
    in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a moment to describe our NGINX pod and ensure that it is running
    as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see describe displays a lot of pertinent information about our cluster,
    including details such as the namespace the pod is running in, any labels our
    pod is configured with, the name and location of the container image that is running,
    as well as the most recent events that have occurred to our pod. The describe
    output shows us a wealth of information that can help us to troubleshoot or optimize
    the deployments and containers in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing Kubernetes services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we now have a functional NGINX web server running in our cluster, we can
    expose this service to the outside world so that others can use our shiny new
    service. In order to do this, we can create a Kubernetes abstraction known as
    a service to control how our pod is granted outside access. By default, Kubernetes
    pods are assigned a cluster IP address by the overlay network fabric, which is
    only reachable within the cluster by other containers and across nodes. This is
    useful if you have a deployment that should never be directly exposed to the outside
    world. However, Kubernetes also supports exposing deployments using the service
    abstraction. Services can expose pods in a variety of ways, from allocating publicly
    routable IP addresses to the services and load balancing across the cluster to
    opening a simple node port on the master nodes, from which the service can listen.
    Google Container Engine provides native support for the `LoadBalancer` service
    type which can allocate a public virtual IP address to our deployment, making
    services extremely easy to expose. In order to allow our service to see the outside
    world, we can use the `kubectl expose deployment` command, providing the service
    type as `LoadBalancer`. Upon successful completion, you should see the message
    service `nginx-web` exposed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see our newly created service by running the `kubectl get services`
    command. You may notice that the `EXTERNAL IP` column may be in the pending state
    for a moment or two while Kubernetes allocates a public IP for the cluster. If
    you execute the `kubectl get services` command after a few minutes, you should
    notice it has an external IP and is ready to be accessed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After a minute or two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the external IP of `35.202.165.54` has been allocated to our
    deployment. You can access this IP address in a web browser to actually see the
    NGINX default web page in action. Remember, you have to access this service on
    TCP port `8000` since that is how the container-enabled role is configured out
    of the box. Bonus points if you want to go back and reconfigure your NGINX container
    to run on port `80`!
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Platform has native integration with the Google Cloud virtual load
    balancer resources, which allow Kubernetes to assign external IP addresses to
    services.  In baremetal environments or clusters running on other clouds, an extra
    configuration will be required to allow Kubernetes to seamlessly allocate publicly
    routed IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Kubernetes pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have pods running in our cluster, we can use the powerful Kubernetes
    primitives to scale out containers and running services across nodes for high
    availability. As mentioned previously, as soon as a desire state is declared that
    involves running more than one replica of a pod, Kubernetes will apply a bin-packing
    algorithm to the deployment in an effort to determine which nodes the service
    will run on. If you declare the same number of replicas as you have nodes in your
    cluster, Kubernetes will run one replica on each node by default. If you have
    more replicas declared then nodes, Kubernetes will run more than one replica on
    some of the nodes, and on others, it will run a single replica. This provides
    us with native high availability out of the box. One of the benefits of using
    Kubernetes is that, by leveraging these features and functionality, operators
    no longer worry as much about the underlying infrastructure the containers are
    running on as much as the cluster abstractions themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: Kubernetes can also use labels to control where certain deployments should
    run. For example, if you have a high compute capacity node, you could label that
    node as a compute node. You can customize your deployment so that those pods will
    only run on nodes with that particular label.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how powerful of a functionality this is, we use `kubectl` to
    scale out our existing web server deployment. Since we are currently running a
    three-node cluster, let’s scale out our NGINX deployment to four replicas. This
    way, we can best illustrate what decisions Kubernetes is making on where to place
    our containers. In order to scale our current deployment, we can use the `kubectl
    scale deployment` command to increase our replica count from one to four:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `kubectl get deployments`, we can see that Kubernetes is actively reconfiguring
    our cluster towards the desired state. It might take a few seconds for Kubernetes
    to get all four pods running, depending on the configuration you have chosen for
    your cluster. Following we can see the desired number of pods, the current number
    of pods running in the cluster, the number of pods that update, and the pods that
    are ready and available to accept traffic. It looks like our cluster is in our
    desired state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Running `kubectl get pods` with the `-o wide` flag, we can see that all four
    NGINX pods are running with different IP addresses allocated and on different
    cluster nodes. It is important to note that, since we specified four replicas
    and only have three nodes, Kubernetes made the decision to have two pod replicas
    running on the same host. Keep in mind that these are two separate and distinct
    pods with a different IP address, even though they are running the same host.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output is slightly truncated since the `-o` wide output is difficult
    to read properly in the context of a book page. Your output will be slightly more
    verbose than mine.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the public IP address again will result in the service now load balancing
    traffic across the pods in the cluster. Since we specified the service type as
    `LoadBalancer`, Kubernetes will use a round-robin algorithm to pass traffic to
    our pods with high availability. Unfortunately, this will not be obvious to the
    reader, since each pod is running the same NGINX test web page. One of the major
    benefits of Kubernetes is that services are usually tied to deployments. When
    you scale a deployment, the service will automatically scale and start passing
    traffic to the new pods!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move forward to the next exercise, let’s delete the deployment we
    just created, as well as the exposed service. This will return our cluster to
    a fresh state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Creating deployments using Kubernetes manifests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with the ability to create services and other objects directly from the
    command line, Kubernetes also gives you the ability to describe desired states
    using a manifest document. Kubernetes manifests give you the freedom to provide
    more customization options in an easier to read, understand, and repeatable format,
    as opposed to the command line, which is rather limited in its format. Since this
    chapter is not designed to be a deep dive into Kubernetes, we will not spend a
    lot of time going into all of the various configuration options that can be used
    in a Kubernetes manifest. Rather, my intention is to show the reader what manifests
    look like and how they work at a basic level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you are already familiar with creating deployments using the `kubectl`
    command-line tool, let’s demonstrate what our `nginx-web` deployment would look
    like using a Kubernetes manifest. These examples are available in the official
    git repository for the book, under the `Kubernetes/nginx-demo` directory. Open
    your text editor and create a file: `webserver-deployment.yml`. The content of
    this file should resemble the following. In this example, we are going to continue
    to use our previously created NGINX container. However, feel free to use other
    container URLs if you wish to experiment with using other types of services and
    ports.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Like all the YAML documents we have looked at thus far, Kubernetes manifest
    documents begin with three dashes to indicate the start of a YAML file. Kubernetes
    manifests always begin by specifying the API version, object kind, and metadata.
    This is colloquially known as the header data and indicates to the Kubernetes
    API the type of objects this document is going to create. Since we are creating
    a deployment, we will specify the `kind` parameter as `Deployment` and provide
    the name of the deployment as the metadata name. Everything listed underneath
    the `spec` section provides configuration option parameters that are specific
    to the pod object the document is creating. Since we are basically reverse engineering
    our previous deployment, we are specifying the number of replicas as `4`. The
    next few lines specify metadata we are going to configure our pods with, specifically
    a key-value pair label called, `app:http-webserver`. Keep this label in mind,
    as we are going to use it when we create the service resource next.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have another `spec` section, which lists the containers that are
    going to run inside our pod. Earlier in the chapter, I mentioned that a pod can
    be one or more containers running using shared network and cluster resources.
    Containers in a pod share a pod IP address and localhost namespace. Kubernetes
    deployments allow you to specify more than one pod under the `containers:` section,
    passing them in as listed key-value pair items. This example, however, will create
    a single-pod container known as `webserver-container`. It is here that we will
    specify the container image version, as well as the container port (`80`).
  prefs: []
  type: TYPE_NORMAL
- en: 'This manifest can be applied using the `kubectl create` command, passing in
    the `-f` flag, which indicates a manifest object, as well as the path to the manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful completion, you should see the pods getting created using `kubectl
    get pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating services using Kubernetes manifests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a similar way to creating our deployment using Kubernetes manifest, we can
    create other Kubernetes objects using manifests. The service we created earlier
    can be described using the following Kubernetes manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice in this example, we are specifying a different `kind` parameter to be
    `Service` as opposed to our previous example, which used `Deployment`. This tells
    the Kubernetes API to expect that the rest of the document will contain specifications
    that describe services instead of other types of Kubernetes objects. In the metadata
    section, we will name our service `webserver-service` (creative, no?). For the
    specification section, we will provide the type of service we are exposing, `LoadBalancer`,
    and provide the label we assigned to our deployment: `app: http-webserver`. When
    using `kubectl` to expose deployments, the service you create is inherently tied
    to the deployment you are exposing. When that deployment scales out, the service
    will be aware and will adjust according to how many backend pods are running.
    However, when creating a service using Kubernetes manifests, we can get more creative
    with how services are tied to the services they are exposing. In this example,
    we are creating a service that is associated with any pod that has the label `app:
    http-webserver`. In theory, this could be any number of pods across different
    namespaces and deployments. This allows for a lot of flexibility when designing
    applications around a Kubernetes architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: The final section of our manifest describes the ports we will perform load balancing
    across. Remember how our NGINX container uses the fixed port `8000` by virtue
    of the fact we built this container using the community-written role? Using the
    load balancer service, we can expose any port we want on the frontend to forward
    traffic to any port on the backend pods. The protocol we will use will be TCP.
    The port we want to expose on the virtual IP address will be `80` for standard
    HTTP requests. Finally, we will list the port that NGINX is listening to internally
    on our pods to forward traffic to. In this case, `8000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `kubectl create` command, we can create our service very similar
    to how we created our initial deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `kubectl get services`, we can see which external virtual IP address
    gets allocated to our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the `PORTS` column, we can see that TCP port `80` is exposed in
    our cluster. Using a web browser again, we can access our new public IP on port
    `80` and see whether it’s working:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dc64e1a-eb4f-49b6-88eb-e2a40805e508.png)'
  prefs: []
  type: TYPE_IMG
- en: Using Kubernetes manifests, we can describe in greater detail the ways we want
    our containerized applications to function. Manifests can easily be modified as
    well and reapplied using the `kubectl apply -f manifest.yml` command. If at any
    time, we wanted to update our application to a different version of the container
    image, or modify exposed ports on the service, `kubectl` apply would only make
    the changes necessary to bring our application into the desired state. Feel free
    to tweak these manifests on your own and reapply them to see in what ways you
    can configure the services to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at deploying containers to Kubernetes using Ansible Container.
    Before we move forward, let’s remove the pods in your Kubernetes cluster using
    the `kubectl delete` command, specifying the Kubernetes manifests we used to create
    or modify the deployment and service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: For now, we have finished working work the Kubernets cluster.  If you wish to
    delete the cluster from Google Cloud, you can do so now.  However, it is important
    to note that [Chapter 7](ef89f30f-00a9-4f4c-93b9-009474fc3022.xhtml), *Deploying
    Your First Project* covers deploying projects to Kubernetes. I would suggest you
    keep your cluster active until you have finished working on the material in [Chapter
    7](ef89f30f-00a9-4f4c-93b9-009474fc3022.xhtml)*, Deploying Your First Project*.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kubernetes Documentation**: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Cloud Platform**: [https://cloud.google.com](https://cloud.google.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running Kubernetes locally with Minikube**:  [https://kubernetes.io/docs/getting-started-guides/minikube/](https://kubernetes.io/docs/getting-started-guides/minikube/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is quickly becoming one of the most robust, flexible, and popular
    container deployment and orchestration platforms that is taking the IT industry
    by storm. Throughout this chapter, we have taken a close look at Kubernetes, learning
    about how it works as a platform and some of the key features that make it so
    useful and versatile. If you have worked in or around containers for very long,
    it will be clear to you that Kubernetes is rapidly being adopted by organizations
    throughout the world due to the extremely sophisticated mechanisms it uses to
    deploy and manage containers at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the native support for Kubernetes in Ansible Container, we can use the
    same workflow to build, run, test, and destroy containerized applications that
    we can deploy to robust services such as Kubernetes. Ansible Container truly provides
    the right tools to help drive complex deployments using a unified and reliable
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: However, Google Cloud and the Kubernetes framework are not the only cloud-based
    container orchestration solutions on the market in today’s world. OpenShift is
    quickly gaining popularity as a managed solution built by Red Hat that functions
    on top of the Kubernetes platform. Next, we will apply the Kubernetes concepts
    we learned in this chapter to deploy applications to the OpenShift software stack,
    using the powerful tools offered to us by the Ansible Container platform to drive
    large-scale application workloads.
  prefs: []
  type: TYPE_NORMAL
