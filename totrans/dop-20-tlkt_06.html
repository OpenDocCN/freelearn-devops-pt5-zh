<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Configuration Management in the Docker World"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Configuration Management in the Docker World</h1></div></div></div><p>Anyone managing more than a few servers can confirm that doing such a task manually is a waste of time and risky. <span class="strong"><strong>Configuration management</strong></span> (<span class="strong"><strong>CM</strong></span>) exists for a long time, and there is no single <a class="indexterm" id="id255"/>reason I can think of why one would not use one of the tools. The question is not whether to adopt one of them but which one to choose. Those that already embraced one or the other and invested a lot of time and money will probably argue that the best tool is the one they chose. As things usually go, the choices change over time and the reasons for one over the other might not be the same today as they were yesterday. In most cases, decisions are not based on available options but by the architecture of the legacy system, we are sworn to maintain. If such systems are to be ignored, or someone with enough courage and deep pockets would be willing to modernize them, today's reality would be dominated by containers and microservices. In such a situation, the choices we made yesterday are different from choices we could make today.</p><div class="section" title="CFEngine"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec17"/>CFEngine</h1></div></div></div><p>CFEngine<a class="indexterm" id="id256"/> can be considered the father of configuration management. It was created in 1993 and revolutionized the way we approach server setups and configurations. It started as an open source project and become commercialized in 2008 when the first enterprise version was released.</p><p>CFEngine is written in C, has only a few dependencies and is lightning fast. Actually, as to my knowledge, no other tool managed to overcome CFEngine's speed. That was, and still is its main strength. However, it had its weaknesses, with the requirement for coding skills being probably the main one. In many cases, an average operator was not able to utilize CFEngine. It requires a C developer to manage it. That did not prevent it from becoming widely adopted in some of the biggest enterprises. However, as youth usually wins over age, new tools were created, and today rarely anyone chooses CFEngine without being forced to do so due to the investment the company made into it.</p><div class="section" title="Puppet"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec36"/>Puppet</h2></div></div></div><p>Later on, Puppet<a class="indexterm" id="id257"/> came into being. It also started as an open source project <a class="indexterm" id="id258"/>followed by the enterprise version. It was considered more "operations friendly" thanks to its model-driven approach and small learning curve when compared to CFEngine. Finally, there was a configuration management tool that operations department could leverage. Unlike C utilized by CFEngine, Ruby proved to be easier to reason with and more accepted by ops. CFEngine's learning curve was probably the main reason Puppet got its footing into the configuration management market and slowly sent CFEngine into history. That does not mean that CFEngine is not used any more. It is, and it doesn't seem it will disappear anytime soon in the same way as Cobol is still present in many banks and other finance related businesses. However, it lost its reputation for being the weapon of choice.</p></div><div class="section" title="Chef"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec37"/>Chef</h2></div></div></div><p>Then came <a class="indexterm" id="id259"/>Chef promising to solve some of the nuances of Puppet. And it did, for a while. Later, as the popularity of both Puppet and Chef continued increasing, they entered the zero sum game. As soon as one of them came up with something <a class="indexterm" id="id260"/>new or some improvement, the other one adopted it. Both feature an ever increasing number of tools that tend to increase their learning curves and complexity. Chef is a bit more "developer friendly" while Puppet could be considered more oriented towards operations and sysadmin type of tasks. Neither has a clear enough advantage over the other, and the choice is often based on personal experience than anything else. Both Puppet and Chef are mature, widely adopted (especially in enterprise environments) and have an enormous number of open source contributions. The only problem is that they are too complicated for what we are trying to accomplish. Neither of them was designed with containers in mind. Neither of them could know that the game would change with Docker since it didn't exist at the time they were designed.</p><p>All of the configuration management tools we mentioned thus far are trying to solve problems that we should not have the moment we adopt containers and immutable deployments. The server mess that we had before is no more. Instead of hundreds or even thousands of packages, configuration files, users, logs, and so on, we are now trying to deal with a lot of containers and very limited amount of anything else. That does not mean that we do not need configuration management. We do! However, the scope of what the tool of choice should do is much smaller. In most cases, we need a user or two, Docker service up and running and a few more things. All the rest are containers. Deployment is becoming a subject of a different set of tools and redefining the scope of what CM should do. Docker Compose, Mesos, Kubernetes, and Docker Swarm, are only a few of a rapidly increasing number of deployment tools we might use today. In such a setting, our configuration management choice should value simplicity and immutability over other things. Syntax should be simple and easy to read even to those who never used the tool. Immutability can be accomplished by enforcing a push model that does not require<a class="indexterm" id="id261"/> anything<a class="indexterm" id="id262"/> to be installed on the destination server.</p><div class="section" title="Ansible"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec09"/>Ansible</h3></div></div></div><p>Ansible tries to <a class="indexterm" id="id263"/>solve the same problems as other configuration management tools<a class="indexterm" id="id264"/> but in a very different way. One significant difference is that it performs all its operations over SSH. CFEngine and Puppet require clients to be installed on all servers they are supposed to manage. While Chef claims that it doesn't, its support for agent-less running has limited features. That in itself is a huge difference when compared to Ansible that does not require servers to have anything special since SSH is (almost) always present. It leverages well defined and widely used protocol to run whatever commands need to be run to make sure that the destination servers comply with our specifications. The only requirement is Python that is already pre-installed on most Linux distributions. In other words, unlike competitors that are trying to force you to setup servers in a certain way, Ansible leverages existing realities and does not require anything. Due to its architecture, all you need is a single instance running on a Linux or OS X computer. We can, for example, manage all our servers from a laptop. While that is not advisable and Ansible should probably run on a real server (preferably the same one where other continuous integration and deployment tools are installed), laptop example illustrates its simplicity. In my experience, push-based systems like Ansible are much easier to reason with than pull based tools we discussed earlier.</p><p>Learning Ansible takes a fraction of the time when compared to all the intricacies required to master the other tools. Its syntax is based on YAML and with a single glimpse over a playbook, even a person who never used the tool would understand what's going on. Unlike Chef, Puppet and, especially CFEngine that are written by developers for developers, Ansible is written by developers for people who have better things to do than learn yet another language and/or DSL.</p><p>Some would point out that the major downside is Ansible's limited support for Windows. The client does not even run on Windows, and the number of modules that can be used in playbooks and run on it is very limited. This downside, assuming that we are using containers is, in my opinion, an advantage. Ansible developers did not waste time trying to create an all around tool and concentrated on what works best (commands over SSH on Linux). In any case, Docker is not yet ready to run containers in Windows. It might be in the future but at this moment (or, at least, the moment I was writing this text), this is on the roadmap. Even if we ignore containers and their questionable future on Windows, other tools are also performing much worse on Windows than Linux. Simply put, Windows architecture is not as friendly to the CM objectives as Linux is.</p><p>I probably went too far and should not be too harsh on Windows and question your choices. If you do <a class="indexterm" id="id265"/>prefer Windows servers over some Linux distribution, all my praise<a class="indexterm" id="id266"/> of Ansible is in vain. You should choose Chef or Puppet and, unless you already use it, ignore CFEngine.</p></div></div><div class="section" title="Final Thoughts"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec38"/>Final Thoughts</h2></div></div></div><p>If someone asked me few years ago which tool should we use I would have a hard time answering. Today, if one has the option to switch to containers (be it Docker or some other type) and immutable deployments, the choice is clear (at least among tools I mentioned). Ansible (when combined with Docker and Docker deployment tools) wins any time of the day. We might even argue whether CM tools are needed at all. There are examples when<a class="indexterm" id="id267"/> people fully rely upon, let's say, CoreOS, containers, and deployment tools like Docker Swarm or Kubernetes. I do not have such a radical opinion (yet) and think that CM continues being a valuable tool in the arsenal. Due to the scope of the tasks CM tools needs to perform, Ansible is just the tool we need. Anything more complicated or harder to learn would be overkill. I am yet to find a person who had trouble maintaining Ansible playbooks. As a result, configuration management can quickly become the responsibility of the whole team. I'm not trying to say that infrastructure should be taken lightly (it definitely shouldn't). However, having contributions from the entire team working on a project is a significant advantage for any type of tasks and CM should not be an exception. CFEngine, Chef, and Puppet are an overkill with their complex architecture and their steep learning curve, at least, when compared with Ansible.</p><p>The four tools we briefly went through are by no means the only ones we can choose from. You might easily argue that neither of those is the best and vote for something else. Fair enough. It all depends on preferences and objectives we are trying to archive. However, unlike the others, Ansible can hardly be a waste of time. It is so easy to learn that, even if you choose not to adopt it, you won't be able to say that a lot of valuable time was wasted. Besides, everything we learn brings something new and makes us better professionals.</p><p>You probably guessed by now that Ansible will be the tool we'll use for configuration management.</p></div><div class="section" title="Configuring the Production Environment"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec39"/>Configuring the Production Environment</h2></div></div></div><p>Let us see<a class="indexterm" id="id268"/> Ansible in action and then discuss how it is configured. We'll need two VMs up and running; the <code class="literal">cd</code> will be used as a server from which we'll set up the prod node.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd prod --provision</strong></span>
<span class="strong"><strong>vagrant ssh cd</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/prod.yml -i /vagrant/ansible/hosts/prod</strong></span>
</pre></div><p>The output<a class="indexterm" id="id269"/> should be similar to the following:</p><div class="informalexample"><pre class="programlisting">PPLAY [prod] *******************************************************************
 GATHERING FACTS ***************************************************************
 The authenticity of host '10.100.198.201 (10.100.198.201)' can't be established.
 ECDSA key fingerprint is 2c:05:06:9f:a1:53:2a:82:2a:ff:93:24:d0:94:f8:82.
 Are you sure you want to continue connecting (yes/no)? yes
 ok: [10.100.198.201]
 TASK: [common | JQ is present] ************************************************
 changed: [10.100.198.201]
 TASK: [docker | Debian add Docker repository and update apt cache] ************
 changed: [10.100.198.201]
 TASK: [docker | Debian Docker is present] *************************************
 changed: [10.100.198.201]
 TASK: [docker | Debian python-pip is present] *********************************
 changed: [10.100.198.201]
 TASK: [docker | Debian docker-py is present] **********************************
 changed: [10.100.198.201]
 TASK: [docker | Debian files are present] *************************************
 changed: [10.100.198.201]
 TASK: [docker | Debian Daemon is reloaded] ************************************
 skipping: [10.100.198.201]
 TASK: [docker | vagrant user is added to the docker group] ********************
 changed: [10.100.198.201]
 TASK: [docker | Debian Docker service is restarted] ***************************
 changed: [10.100.198.201]
 TASK: [docker-compose | Executable is present] ********************************
 changed: [10.100.198.201]
 PLAY RECAP ********************************************************************
 10.100.198.201             : ok=11   changed=9    unreachable=0    failed=0</pre></div><p>The important thing about Ansible (and configuration management in general) is that we are in most cases specifying the desired state of something instead commands we want to run. Ansible, in turn, will do its best to make sure that the servers are in that state. From the output <a class="indexterm" id="id270"/>above we can see that statuses of all tasks are <span class="emphasis"><em>changed</em></span> or <span class="emphasis"><em>skipping</em></span>. For example, we specified that we want Docker service. Ansible noticed that we do not have it on the destination server (<span class="emphasis"><em>prod</em></span>) and installed it.</p><p>What happens if we run the playbook again?</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook prod.yml -i hosts/prod</strong></span>
</pre></div><p>You'll notice that the status of all the tasks is <code class="literal">ok</code>:</p><div class="informalexample"><pre class="programlisting">PLAY [prod] *******************************************************************
GATHERING FACTS ***************************************************************
ok: [10.100.198.201]
TASK: [common | JQ is present] ************************************************
ok: [10.100.198.201]
TASK: [docker | Debian add Docker repository and update apt cache] ************
ok: [10.100.198.201]
TASK: [docker | Debian Docker is present] *************************************
ok: [10.100.198.201]
TASK: [docker | Debian python-pip is present] *********************************
ok: [10.100.198.201]
TASK: [docker | Debian docker-py is present] **********************************
ok: [10.100.198.201]
TASK: [docker | Debian files are present] *************************************
ok: [10.100.198.201]
TASK: [docker | Debian Daemon is reloaded] ************************************
skipping: [10.100.198.201]
TASK: [docker | vagrant user is added to the docker group] ********************
ok: [10.100.198.201]
TASK: [docker | Debian Docker service is restarted] ***************************
skipping: [10.100.198.201]
TASK: [docker-compose | Executable is present] ********************************
ok: [10.100.198.201]
PLAY RECAP ********************************************************************
10.100.198.201             : ok=10   changed=0    unreachable=0    failed=0</pre></div><p>Ansible went to the server and checked the status of all tasks, one at the time. Since this is the second run and we haven't modified anything in the server, Ansible concluded that there is nothing to do. The current state is as expected.</p><p>The command<a class="indexterm" id="id271"/> we just run (<code class="literal">ansible-playbook prod.yml -i hosts/prod</code>) is simple. The first argument is the path to the playbook and the second argument's value represents the path to the inventory file that contains the list of servers where this playbook should run.</p><p>That was a very simple example. We had to setup the production environment and, at this moment, all we needed is Docker, Docker Compose, and a few configuration files. Later on, we'll see more complicated examples.</p><p>Now that we've seen Ansible in action let us go through the configuration of the <span class="emphasis"><em>playbook</em></span> we just run (twice).</p></div><div class="section" title="Setting Up the Ansible Playbook"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec40"/>Setting Up the Ansible Playbook</h2></div></div></div><p>The content <a class="indexterm" id="id272"/>of the <code class="literal">prod.yml</code> Ansible playbook is as follows.</p><div class="informalexample"><pre class="programlisting">- hosts: prod
  remote_user: vagrant
  serial: 1
  sudo: yes
  roles: - common - docker</pre></div><p>Just by reading the playbook one should be able to understand what's it about. It is running on hosts called <span class="emphasis"><em>prod</em></span> as the user <span class="emphasis"><em>vagrant</em></span> and executes commands as <code class="literal">sudo</code>. At the bottom is the list of roles that, in our case, consists of only two; <code class="literal">common</code> and <code class="literal">docker</code>. Role is a set of tasks that we usually organize around one functionality, product, type of operations, and so on. The Ansible playbook organization is based on tasks that are grouped into roles that can be combined into playbooks.</p><p>Before we take a look at it, let us discuss what are the objectives of the <span class="emphasis"><em>docker</em></span> role. We want to make sure that the Docker Debian repository is present and that the latest <span class="emphasis"><em>docker-engine</em></span> package<a class="indexterm" id="id273"/> is installed. Later on, we'll need the <code class="literal">docker-py</code> (Python API client for Docker) that can be installed with <code class="literal">pip</code> so we're making sure that both are present in our system. Next, we need the standard Docker configuration to be replaced with our file located <a class="indexterm" id="id274"/>in the <span class="emphasis"><em>files</em></span> directory. Docker configurations require Docker service to be restarted, so we have to do just that every time there is a change to the <code class="literal">files/docker</code> file. Finally, we're making sure that the user <span class="emphasis"><em>vagrant</em></span> is added to the group <span class="emphasis"><em>docker</em></span> and, therefore, able to run Docker commands.</p><p>Let us take a look at the <code class="literal">roles/docker</code> directory that defines the role we're using. It consists of two sub-directories, <code class="literal">files</code>, and <code class="literal">tasks</code>. Tasks are the heart of any role and, by default, requires<a class="indexterm" id="id275"/> them to be defined in the <code class="literal">main.yml</code> file:</p><div class="informalexample"><pre class="programlisting">The content of the roles/docker/tasks/main.yml file is as follows.
- include: debian.yml
  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
- include: centos.yml
  when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'</pre></div><p>Since we'll be running Docker on both Debian (Ubuntu) and CentOS or Red Hat, roles are split into <code class="literal">debian.yml</code> and <code class="literal">centos.yml</code> files. Right now, we'll be using Ubuntu so let's take a look at the <code class="literal">roles/docker/tasks/debian.yml</code> role.</p><div class="informalexample"><pre class="programlisting">- name: Debian add Docker repository and update apt cache
  apt_repository:
    repo: deb https://apt.dockerproject.org/repo ubuntu-{{ debian_version }} main
    update_cache: yes
    state: present
  tags: [docker]
- name: Debian Docker is present
  apt:
    name: docker-engine
    state: latest
    force: yes
  tags: [docker]
- name: Debian python-pip is present
  apt: name=python-pip state=present
  tags: [docker]
- name: Debian docker-py is present
  pip: name=docker-py version=0.4.0 state=present
  tags: [docker]
- name: Debian files are present
  template:
    src: "{{ docker_cfg }}"
    dest: "{{ docker_cfg_dest }}"
  register: copy_result
  tags: [docker]
- name: Debian Daemon is reloaded
  command: systemctl daemon-reload
  when: copy_result|changed and is_systemd is defined
  tags: [docker]
- name: vagrant user is added to the docker group
  user:
    name: vagrant
    group: docker  register: user_resulttags: [docker]- name: Debian Docker service is restarted
  service:
    name: docker
    state: restarted
  when: copy_result|changed or user_result|changed
  tags: [docker]</pre></div><p>If this would be a different framework or a tool, I would pass through each of the tasks and explain them one by one, and you would be very grateful for acquiring more pieces of wisdom. However, I do not think there is a reason to do that. Ansible is very straightforward. Assuming that you have a basic Linux knowledge, I bet you can understand each of the tasks without any further explanation. In case I was wrong, and you do need an<a class="indexterm" id="id276"/> explanation, please look for the module in <a class="indexterm" id="id277"/>question in the <a class="ulink" href="http://docs.ansible.com/ansible/list_of_all_modules.html">http://docs.ansible.com/ansible/list_of_all_modules.html</a> of the Ansible documentation. For example, if you'd like to know what the second task does, you'd open the apt module. The only important thing to know for now is how the indentation works. YAML is based on <code class="literal">key: value</code>, <code class="literal">parent/child</code> structure. For example, the last task has <code class="literal">name</code> and <code class="literal">state</code> keys that are children of the <code class="literal">service</code> that, in turn, is one of the Ansible modules.</p><p>There is one more thing we used with our <code class="literal">prod.yml</code> playbook. The command we executed had the <code class="literal">-i hosts/prod</code> argument that we used to specify the inventory file with the list of hosts the playbook should run on. The <code class="literal">hosts/prod</code> inventory is quite big since it is used throughout the whole book. At the moment, we are interested only in the <code class="literal">prod</code> section since that is the value of the <code class="literal">hosts</code> argument we specified in the playbook:</p><div class="informalexample"><pre class="programlisting">...
[prod]
10.100.198.201
...</pre></div><p>If we'd like to apply the same configuration to more than one server all we'd have to do is add another IP.</p><p>We'll see more complex examples later on. I intentionally said more complex since nothing is truly complicated in Ansible but, depending on some tasks and their interdependency, some roles can be more or less complex. I hope that the playbook we just run gave you an approximation of the type of the tool Ansible is and I hope you liked it. We'll rely on it for all the configuration management tasks and more.</p><p>You might have noticed that we never entered the <code class="literal">prod</code> environment but run everything remotely from the <code class="literal">cd</code> server. The same practice will continue throughout the book. With Ansible and few other tools we'll get introduced to, later on, there is no need to ssh into servers and do <a class="indexterm" id="id278"/>manual tasks. In my opinion, our knowledge and creativity should be used for coding and everything else should be automatic; testing, building, deployment, scaling, logging, monitoring, and so on. That is one of the takeaways of this book. The key to success is massive automation that frees us to do exciting and more productive tasks.</p><p>As before, we'll end this chapter by destroying all the VMs. The next chapter will create those we need:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div><p>With the first production server up and running (at the moment only with Ubuntu OS, Docker, and Docker Compose) we can continue working on the basic implementation of the deployment pipeline.</p></div></div></div></body></html>