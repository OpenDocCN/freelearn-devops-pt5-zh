["```\n`1` `export` `AWS_ACCESS_KEY_ID``=[`...`]`\n`2` \n`3` `export` `AWS_SECRET_ACCESS_KEY``=[`...`]`\n`4` \n`5` `export` `AWS_DEFAULT_REGION``=`us-east-1 \n```", "``````````````````````````````````````````````````` You’re free to change the region to any that suits you as long as it has at least three availability zones.    Before we proceed, please make sure that [AWS CLI](https://aws.amazon.com/cli/) and [jq](https://stedolan.github.io/jq/download/) are installed. We’ll use the *CLI* to communicate with AWS and *jq* to format and filter JSON output returned by the *CLI*.  ![Figure 14-1: AWS Command Line Interface screen](img/00079.jpeg)  Figure 14-1: AWS Command Line Interface screen    Let’s take a look at the *Docker For AWS* template.    ``` `1` curl https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl `\\` `2 `    `|` jq `\".\"`  ```   `````````````````````````````````````````````````` The output is vast, and we won’t have time to go through the details of all the services it defines. Instead, we’ll focus on the parameters since they have to be specified during the execution of the template.    Let’s take another look at the template but, this time, limited to the `.Metadata` section.    ``` `1` curl https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl `\\` `2 `    `|` jq `\".Metadata\"`  ```   ````````````````````````````````````````````````` We output the template and used `jq` to filter the result. The output is as follows.    ```  `1` `{`  `2`  `\"AWS::CloudFormation::Interface\"``:` `{`  `3`    `\"ParameterGroups\"``:` `[`  `4`      `{`  `5`        `\"Label\"``:` `{`  `6`          `\"default\"``:` `\"Swarm Size\"`  `7`        `},`  `8`        `\"Parameters\"``:` `[`  `9`          `\"ManagerSize\"``,` `10 `          `\"ClusterSize\"` `11 `        `]` `12 `      `},` `13 `      `{` `14 `        `\"Label\"``:` `{` `15 `          `\"default\"``:` `\"Swarm Properties\"` `16 `        `},` `17 `        `\"Parameters\"``:` `[` `18 `          `\"KeyName\"``,` `19 `          `\"EnableSystemPrune\"``,` `20 `          `\"EnableCloudWatchLogs\"``,` `21 `          `\"EnableCloudStorEfs\"` `22 `        `]` `23 `      `},` `24 `      `{` `25 `        `\"Label\"``:` `{` `26 `          `\"default\"``:` `\"Swarm Manager Properties\"` `27 `        `},` `28 `        `\"Parameters\"``:` `[` `29 `          `\"ManagerInstanceType\"``,` `30 `          `\"ManagerDiskSize\"``,` `31 `          `\"ManagerDiskType\"` `32 `        `]` `33 `      `},` `34 `      `{` `35 `        `\"Label\"``:` `{` `36 `          `\"default\"``:` `\"Swarm Worker Properties\"` `37 `        `},` `38 `        `\"Parameters\"``:` `[` `39 `          `\"InstanceType\"``,` `40 `          `\"WorkerDiskSize\"``,` `41 `          `\"WorkerDiskType\"` `42 `        `]` `43 `      `}` `44 `    `],` `45 `    `\"ParameterLabels\"``:` `{` `46 `      `\"ClusterSize\"``:` `{` `47 `        `\"default\"``:` `\"Number of Swarm worker nodes?\"` `48 `      `},` `49 `      `\"EnableCloudStorEfs\"``:` `{` `50 `        `\"default\"``:` `\"Create EFS prerequsities for CloudStor?\"` `51 `      `},` `52 `      `\"EnableCloudWatchLogs\"``:` `{` `53 `        `\"default\"``:` `\"Use Cloudwatch for container logging?\"` `54 `      `},` `55 `      `\"EnableSystemPrune\"``:` `{` `56 `        `\"default\"``:` `\"Enable daily resource cleanup?\"` `57 `      `},` `58 `      `\"InstanceType\"``:` `{` `59 `        `\"default\"``:` `\"Agent worker instance type?\"` `60 `      `},` `61 `      `\"KeyName\"``:` `{` `62 `        `\"default\"``:` `\"Which SSH key to use?\"` `63 `      `},` `64 `      `\"ManagerDiskSize\"``:` `{` `65 `        `\"default\"``:` `\"Manager ephemeral storage volume size?\"` `66 `      `},` `67 `      `\"ManagerDiskType\"``:` `{` `68 `        `\"default\"``:` `\"Manager ephemeral storage volume type\"` `69 `      `},` `70 `      `\"ManagerInstanceType\"``:` `{` `71 `        `\"default\"``:` `\"Swarm manager instance type?\"` `72 `      `},` `73 `      `\"ManagerSize\"``:` `{` `74 `        `\"default\"``:` `\"Number of Swarm managers?\"` `75 `      `},` `76 `      `\"WorkerDiskSize\"``:` `{` `77 `        `\"default\"``:` `\"Worker ephemeral storage volume size?\"` `78 `      `},` `79 `      `\"WorkerDiskType\"``:` `{` `80 `        `\"default\"``:` `\"Worker ephemeral storage volume type\"` `81 `      `}` `82 `    `}` `83 `  `}` `84` `}`  ```   ```````````````````````````````````````````````` You should be familiar with those parameters. They are the same as those you saw when you created the cluster through AWS UI.    Now we are ready to create a cluster.    ```  `1` aws cloudformation create-stack `\\`  `2`    --template-url https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker`\\`  `3` .tmpl `\\`  `4`    --capabilities CAPABILITY_IAM `\\`  `5`    --stack-name devops22 `\\`  `6`    --parameters `\\`  `7`    `ParameterKey``=`ManagerSize,ParameterValue`=``3` `\\`  `8`    `ParameterKey``=`ClusterSize,ParameterValue`=``0` `\\`  `9`    `ParameterKey``=`KeyName,ParameterValue`=`devops22 `\\` `10 `    `ParameterKey``=`EnableSystemPrune,ParameterValue`=`yes `\\` `11 `    `ParameterKey``=`EnableCloudWatchLogs,ParameterValue`=`no `\\` `12 `    `ParameterKey``=`EnableCloudStorEfs,ParameterValue`=`yes `\\` `13 `    `ParameterKey``=`ManagerInstanceType,ParameterValue`=`t2.micro `\\` `14 `    `ParameterKey``=`InstanceType,ParameterValue`=`t2.micro  ```   ``````````````````````````````````````````````` We named the stack `devops22` and used the parameters to set the number of managers (`3`) and workers (`0`) and SSH key (`devops22`). We enabled prune and EFS, and disabled *CloudWatch*. This time we used `t2.micro` instances. We won’t deploy many services so 1 vCPU and 1GB of memory should be more than enough. At the same time, `t2.micro` is *free tier eligible* making it a perfect instance type for the exercises in this chapter.    We can use `aws cloudformation` command to list the resources defined in the stack and see their current status.    ``` `1` aws cloudformation describe-stack-resources `\\` `2 `    --stack-name devops22 `|` jq `\".\"`  ```   `````````````````````````````````````````````` The output is too big to be listed here so we’ll move on. Our immediate goal is to find out the status of the stack and confirm that it was created successfully before we SSH into it. We can describe a stack through the `aws cloudformation describe-stacks` command.    ``` `1` aws cloudformation describe-stacks `\\` `2 `    --stack-name devops22 `|` jq `\".\"`  ```   ````````````````````````````````````````````` We retrieved the description of the `devops22` stack. The output is as follows.    ```  `1` `{`  `2`  `\"Stacks\"``:` `[`  `3`    `{`  `4`      `\"StackId\"``:` `\"arn:aws:cloudformation:us-east-2:036548781187:stack/devops22/b\\`  `5` `f859420-99f1-11e7-92af-50a68a26e835\"``,`  `6`      `\"Description\"``:` `\"Docker CE for AWS 17.06.1-ce (17.06.1-ce-aws1)\"``,`  `7`      `\"Parameters\"``:` `[`  `8`        `{`  `9`          `\"ParameterValue\"``:` `\"yes\"``,` `10 `          `\"ParameterKey\"``:` `\"EnableCloudStorEfs\"` `11 `        `},` `12 `        `{` `13 `          `\"ParameterValue\"``:` `\"devops22\"``,` `14 `          `\"ParameterKey\"``:` `\"KeyName\"` `15 `        `},` `16 `        `{` `17 `          `\"ParameterValue\"``:` `\"t2.micro\"``,` `18 `          `\"ParameterKey\"``:` `\"ManagerInstanceType\"` `19 `        `},` `20 `        `{` `21 `          `\"ParameterValue\"``:` `\"0\"``,` `22 `          `\"ParameterKey\"``:` `\"ClusterSize\"` `23 `        `},` `24 `        `{` `25 `          `\"ParameterValue\"``:` `\"standard\"``,` `26 `          `\"ParameterKey\"``:` `\"ManagerDiskType\"` `27 `        `},` `28 `        `{` `29 `          `\"ParameterValue\"``:` `\"20\"``,` `30 `          `\"ParameterKey\"``:` `\"WorkerDiskSize\"` `31 `        `},` `32 `        `{` `33 `          `\"ParameterValue\"``:` `\"20\"``,` `34 `          `\"ParameterKey\"``:` `\"ManagerDiskSize\"` `35 `        `},` `36 `        `{` `37 `          `\"ParameterValue\"``:` `\"standard\"``,` `38 `          `\"ParameterKey\"``:` `\"WorkerDiskType\"` `39 `        `},` `40 `        `{` `41 `          `\"ParameterValue\"``:` `\"yes\"``,` `42 `          `\"ParameterKey\"``:` `\"EnableSystemPrune\"` `43 `        `},` `44 `        `{` `45 `          `\"ParameterValue\"``:` `\"no\"``,` `46 `          `\"ParameterKey\"``:` `\"EnableCloudWatchLogs\"` `47 `        `},` `48 `        `{` `49 `          `\"ParameterValue\"``:` `\"t2.small\"``,` `50 `          `\"ParameterKey\"``:` `\"InstanceType\"` `51 `        `},` `52 `        `{` `53 `          `\"ParameterValue\"``:` `\"3\"``,` `54 `          `\"ParameterKey\"``:` `\"ManagerSize\"` `55 `        `}` `56 `      `],` `57 `      `\"Tags\"``:` `[],` `58 `      `\"CreationTime\"``:` `\"2017-09-15T08:47:10.306Z\"``,` `59 `      `\"Capabilities\"``:` `[` `60 `        `\"CAPABILITY_IAM\"` `61 `      `],` `62 `      `\"StackName\"``:` `\"devops22\"``,` `63 `      `\"NotificationARNs\"``:` `[],` `64 `      `\"StackStatus\"``:` `\"CREATE_IN_PROGRESS\"``,` `65 `      `\"DisableRollback\"``:` `false` `66 `    `}` `67 `  `]` `68` `}`  ```   ```````````````````````````````````````````` Most of the description reflects the parameters we used to create the stack. The value we’re interested in is `StackStatus`. In my case, it is set to `CREATE_IN_PROGRESS` meaning that the cluster is still not ready. We should wait for a while and query the status again. This time, we’ll use `jq` to limit the output only to the `StackStatus` field.    ``` `1` aws cloudformation describe-stacks `\\` `2 `    --stack-name devops22 `|` `\\` `3 `    jq -r `\".Stacks[0].StackStatus\"`  ```   ``````````````````````````````````````````` If the output of the command is `CREATE_COMPLETE`, the cluster is created, and we can move on. Otherwise, please wait for a bit more and recheck the status. It should take around ten minutes to create the whole stack.    Now that the cluster is created, we need to get the DNS and the IP of one of the masters.    Cluster DNS is available through the `Outputs` section of the stack description.    ``` `1` aws cloudformation describe-stacks `\\` `2 `    --stack-name devops22 `|` `\\` `3 `    jq -r `\".Stacks[0].Outputs\"`  ```   `````````````````````````````````````````` The output is as follows.    ```  `1` [  `2`  {  `3`    \"Description\": \"Use this name to update your DNS records\",  `4`    \"OutputKey\": \"DefaultDNSTarget\",  `5`    \"OutputValue\": \"devops22-ExternalL-EEU3J540N4S0-1231273358.us-east-2.elb.ama\\  `6` zonaws.com\"  `7`  },  `8`  {  `9`    \"Description\": \"Availabilty Zones Comment\", `10 `    \"OutputKey\": \"ZoneAvailabilityComment\", `11 `    \"OutputValue\": \"This region has at least 3 Availability Zones (AZ). This is \\ `12` ideal to ensure a fully functional Swarm in case you lose an AZ.\" `13 `  }, `14 `  { `15 `    \"Description\": \"You can see the manager nodes associated with this cluster h\\ `16` ere. Follow the instructions here: https://docs.docker.com/docker-for-aws/deploy\\ `17` /\", `18 `    \"OutputKey\": \"Managers\", `19 `    \"OutputValue\": \"https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=\\ `20` us-east-2#Instances:tag:aws:autoscaling:groupName=devops22-ManagerAsg-RA4ECZRYJ3\\ `21` 7C;sort=desc:dnsName\" `22 `  }, `23 `  { `24 `    \"Description\": \"Use this as the VPC for configuring Private Hosted Zones\", `25 `    \"OutputKey\": \"VPCID\", `26 `    \"OutputValue\": \"vpc-99311ff0\" `27 `  }, `28 `  { `29 `    \"Description\": \"SecurityGroup ID of NodeVpcSG\", `30 `    \"OutputKey\": \"NodeSecurityGroupID\", `31 `    \"OutputValue\": \"sg-0d852c65\" `32 `  }, `33 `  { `34 `    \"Description\": \"Use this zone ID to update your DNS records\", `35 `    \"OutputKey\": \"ELBDNSZoneID\", `36 `    \"OutputValue\": \"Z3AADJGX6KTTL2\" `37 `  }, `38 `  { `39 `    \"Description\": \"SecurityGroup ID of ManagerVpcSG\", `40 `    \"OutputKey\": \"ManagerSecurityGroupID\", `41 `    \"OutputValue\": \"sg-4c832a24\" `42 `  }, `43 `  { `44 `    \"Description\": \"SecurityGroup ID of SwarmWideSG\", `45 `    \"OutputKey\": \"SwarmWideSecurityGroupID\", `46 `    \"OutputValue\": \"sg-aa852cc2\" `47 `  } `48` ]  ```   ````````````````````````````````````````` What we need is the `DefaultDNSTarget` value. We’ll have to refine our `jq` filters a bit more.    ``` `1` aws cloudformation describe-stacks `\\` `2 `    --stack-name devops22 `|` `\\` `3 `    jq -r `\".Stacks[0].Outputs[] | \\` `4 ``    select(.OutputKey==\\\"DefaultDNSTarget\\\")\\` `5 ``    .OutputValue\"`  ```   ```````````````````````````````````````` We used jq’s `select` statement to retrieve only the section with `OutputKey` set to `DefaultDNSTarget` and retrieved the `OutputValue`.    The output should be similar to the one that follows.    ``` `1` devops22-ExternalL-EEU3J540N4S0-1231273358.us-east-2.elb.amazonaws.com  ```   ``````````````````````````````````````` We should store the output of the previous command as an environment variable so that we can have it at hand if we need to open one of the services in a browser or, even better, to set it as the address of our domain.    ``` `1` `CLUSTER_DNS``=``$(`aws cloudformation `\\` `2 `    describe-stacks `\\` `3 `    --stack-name devops22 `|` `\\` `4 `    jq -r `\".Stacks[0].Outputs[] | \\` `5 ``    select(.OutputKey==\\\"DefaultDNSTarget\\\")\\` `6 ``    .OutputValue\"``)`  ```   `````````````````````````````````````` Even though we will not need the DNS in this chapter, it’s good to know how to retrieve it. We’ll need it later on in the chapters that follow.    The only thing left is to get the public IP of one of the managers. We can use `aws ec2 describe-instances` command to list all the EC2 instances running in the region.    ``` `1` aws ec2 describe-instances `|` jq -r `\".\"`  ```   ````````````````````````````````````` The output is too big to be presented in a book.    You should see three instances if the cluster we just created is the only one running in your region. Otherwise, there might be others. Since we do not want to risk retrieving anything but managers that belong to the `devops22` stack, we’ll refine the command to the one that follows.    ``` `1` aws ec2 describe-instances `|` `\\` `2 `    jq -r `\".Reservations[].Instances[] \\` `3 ``    | select(.SecurityGroups[].GroupName \\` `4 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `5 ``    .PublicIpAddress\"`  ```   ```````````````````````````````````` We used `jq` to filter the output and limit the results only to the instances attached to the security group with the name that starts with `devops22-ManagerVpcSG`. Further on, we retrieved the `PublicIpAddress` values.    The output is as follows.    ``` `1` 52.14.246.52 `2` 13.59.130.67 `3` 13.59.132.147  ```   ``````````````````````````````````` Those three IPs belong to the three managers that for the cluster.    We’ll use the previous command to set the environment variable `CLUSTER_IP`.    ``` `1` `CLUSTER_IP``=``$(`aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .PublicIpAddress\"` `\\` `7 `    `|` tail -n `1``)`  ```   `````````````````````````````````` Since we needed only one of the IPs, we piped the result of the `describe-instances` command to `tail` which limited the output to a single line.    Now that we have both the DNS and the IP of one of the managers, we can enter the cluster and confirm that all the nodes joined it.    ``` `1` ssh -i devops22.pem docker@`$CLUSTER_IP` `2`  `3` docker node ls  ```   ````````````````````````````````` The output of the `node ls` command is as follows (IDs are removed for brevity).    ``` `1` HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS `2` ip-172-31-21-57.us-east-2.compute.internal  Ready  Active       Reachable `3` ip-172-31-44-182.us-east-2.compute.internal Ready  Active       Reachable `4` ip-172-31-15-30.us-east-2.compute.internal  Ready  Active       Leader  ```   ```````````````````````````````` As expected, all three nodes joined the cluster, and we can explore self-healing applied to infrastructure through AWS services created with the *Docker For AWS* template.    ### Exploring Fault Tolerance    Since we are exploring self-healing (not self-adaptation), there’s no need to deploy all the stacks we used thus far. A single service will be enough to explore what happens when a node goes down. Our cluster, formed out of `t2.micro` instances, would not support much more anyways.    ``` `1` docker service create --name `test` `\\` `2 `    --replicas `10` alpine sleep `1000000`  ```   ``````````````````````````````` We created a service with ten replicas. Let’s confirm that they are spread across the three nodes of the cluster.    ``` `1` docker service ps `test`  ```   `````````````````````````````` The output is as follows (IDs are removed for brevity).    ```  `1` NAME    IMAGE         NODE                                        DESIRED STATE \\  `2` CURRENT STATE          ERROR PORTS  `3` test.1  alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running       \\  `4` Running 12 seconds ago  `5` test.2  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\  `6` Running 12 seconds ago  `7` test.3  alpine:latest ip-172-31-21-57.us-east-2.compute.internal  Running       \\  `8` Running 12 seconds ago  `9` test.4  alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running       \\ `10` Running 12 seconds ago `11` test.5  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\ `12` Running 12 seconds ago `13` test.6  alpine:latest ip-172-31-21-57.us-east-2.compute.internal  Running       \\ `14` Running 12 seconds ago `15` test.7  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\ `16` Running 12 seconds ago `17` test.8  alpine:latest ip-172-31-21-57.us-east-2.compute.internal  Running       \\ `18` Running 12 seconds ago `19` test.9  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\ `20` Running 12 seconds ago `21` test.10 alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running       \\ `22` Running 12 seconds ago  ```   ````````````````````````````` Let’s exit the cluster before we move onto a discussion how to simulate a failure of a node.    ``` `1` `exit`  ```   ```````````````````````````` We’ll simulate failure of an instance by terminating it. We’ll do that by executing `aws ec2 terminate-instances` command that requires `--instance-ids` argument. So, the first line of business is to figure out how to find ID of one of the nodes.    We already saw that we could use `aws ec2 describe-instances` command to get information about the instances of the cluster. This time we’ll output `InstanceId` of all the nodes that belong to the security group used by managers.    ``` `1` aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .InstanceId\"`  ```   ``````````````````````````` The output is as follows.    ``` `1` i-091ad925d0243f7ab `2` i-0e850f3073ec25acd `3` i-05b25bc6fb6730ce1  ```   `````````````````````````` We’ll repeat the same command, limit the output to only one row, and store the result as an environment variable.    ``` `1` `INSTANCE_ID``=``$(`aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .InstanceId\"` `\\` `7 `    `|` tail -n `1``)`  ```   ````````````````````````` Now that we have the ID, we can terminate the instance associated with it.    ``` `1` aws ec2 terminate-instances `\\` `2 `    --instance-ids `$INSTANCE_ID`  ```   ```````````````````````` The output is as follows.    ```  `1` {  `2`    \"TerminatingInstances\": [  `3`        {  `4`            \"InstanceId\": \"i-0fa78489dca8125e8\",  `5`            \"CurrentState\": {  `6`                \"Code\": 32,  `7`                \"Name\": \"shutting-down\"  `8`            },  `9`            \"PreviousState\": { `10 `                \"Code\": 16, `11 `                \"Name\": \"running\" `12 `            } `13 `        } `14 `    ] `15` }  ```   ``````````````````````` We can see that the previous state is `running` and that it changed to `shutting-down`.    Let’s see the state of the instances that form the cluster.    ``` `1` aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .State.Name\"`  ```   `````````````````````` We retrieved statuses of all the manager instances attached to the security group with a name starting with `devops22-ManagerVpcSG`. The output is as follows.    ``` `1` running `2` running  ```   ````````````````````` There are two manager instances in the cluster, and both are `running`. The node was indeed removed, and we are one server short from the desired setup. Let’s wait for a moment or two and take another look at the manager instances.    ``` `1` aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .State.Name\"`  ```   ```````````````````` This time the output is different.    ``` `1` pending `2` running `3` running  ```   ``````````````````` Besides the two running managers, the third was added and is currently pending. The auto-scaling group associated with the managers detected that one node is missing and started creating a new VM that will restore the cluster to the desired state. The new node is still not ready, so we’ll need to wait for a while longer.    ``` `1` aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .State.Name\"`  ```   `````````````````` The output is as follows.    ``` `1` running `2` running `3` running  ```   ````````````````` Auto-scaling group’s desired state was restored, and the cluster is operating at its full capacity.    We cannot be certain whether the node we destroyed is different than the one we were entering before. Therefore, we should retrieve IP of one of the nodes one more time, and place it in the environment variable `CLUSTER_IP`.    ``` `1` `CLUSTER_IP``=``$(`aws ec2 describe-instances `\\` `2 `    `|` jq -r `\".Reservations[] \\` `3 ``    .Instances[] \\` `4 ``    | select(.SecurityGroups[].GroupName \\` `5 ``    | contains(\\\"devops22-ManagerVpcSG\\\"))\\` `6 ``    .PublicIpAddress\"` `\\` `7 `    `|` tail -n `1``)`  ```   ```````````````` Even though we know that the new node was created automatically, we should still confirm that it also joined the cluster as a Swarm manager.    ``` `1` ssh -i devops22.pem docker@`$CLUSTER_IP` `2`  `3` docker node ls  ```   ``````````````` We entered into one of the managers and listed all the nodes. The output is as follows (IDs are removed for brevity).    ``` `1` HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS `2` ip-172-31-21-57.us-east-2.compute.internal  Down   Active       Unreachable `3` ip-172-31-44-182.us-east-2.compute.internal Ready  Active       Reachable `4` ip-172-31-15-30.us-east-2.compute.internal  Ready  Active       Leader  ```   `````````````` If the output of the `node ls` command is `Error response from daemon: This node is not a swarm manager...`, it means that you entered the node that was just created and it did not yet join the cluster. If that’s the case, all you have to do is wait for a while longer and try it again. I’ll assume that you entered to one of the “old” nodes.    The new node is not there. We can see only the three nodes that were initially created. One of them is `unreachable`.    Does that mean that the system does not work? Is self-healing working only partially and we need to join the new node manually? Should we create a script that will join new nodes to the cluster? The answer to all those questions is *no*. We were too impatient.    Even though AWS reported that the new node is running, it still requires a bit more time until it is fully initialized. Once that is finished, and the VM is fully operational, Docker’s system containers will run and automatically join the node to the cluster.    Let’s wait for a few moments and list the nodes one more time.    ``` `1` docker node ls  ```   ````````````` The output is as follows.    ``` `1` HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS `2` ip-172-31-26-141.us-east-2.compute.internal Ready  Active       Reachable `3` ip-172-31-21-57.us-east-2.compute.internal  Down   Active       Unreachable `4` ip-172-31-44-182.us-east-2.compute.internal Ready  Active       Reachable `5` ip-172-31-15-30.us-east-2.compute.internal  Ready  Active       Leader  ```   ```````````` The new node joined the cluster. Now we have four nodes, with one of them `unreachable`. Swarm cannot know that we destroyed the node. All it does know is that one manager is not reachable. That might be due to many reasons besides destruction.    The unreachable node will be removed from the list after a while.    Let’s see what happened to the replicas of the `test` service.    ``` `1` docker service ps `test`  ```   ``````````` The output is as follows (IDs are removed for brevity).    ```  `1` NAME       IMAGE         NODE                                        DESIRED STA\\  `2` TE CURRENT STATE          ERROR PORTS  `3` test.1     alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running    \\  `4`   Running 10 minutes ago  `5` test.2     alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running    \\  `6`   Running 10 minutes ago  `7` test.3     alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running    \\  `8`   Running 4 minutes ago  `9` \\_ test.3 alpine:latest ip-172-31-21-57.us-east-2.compute.internal  Shutdown   \\ `10 `   Running 4 minutes ago `11` test.4     alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running    \\ `12 `   Running 10 minutes ago `13` test.5     alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running    \\ `14 `   Running 10 minutes ago `15` test.6     alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running    \\ `16 `   Running 4 minutes ago `17 ` \\_ test.6 alpine:latest ip-172-31-21-57.us-east-2.compute.internal  Shutdown   \\ `18 `   Running 4 minutes ago `19` test.7     alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running    \\ `20 `   Running 10 minutes ago `21` test.8     alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running    \\ `22 `   Running 4 minutes ago `23 ` \\_ test.8 alpine:latest ip-172-31-21-57.us-east-2.compute.internal  Shutdown   \\ `24 `   Running 4 minutes ago `25` test.9     alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running    \\ `26 `   Running 10 minutes ago `27` test.10    alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running    \\ `28 `   Running 10 minutes ago  ```   `````````` When Swarm detected that one of the nodes is unreachable, it rescheduled replicas that were running there to the nodes that were healthy at the time. It did not wait for the new node to join the cluster.    Swarm cannot know whether we (or auto-scaling groups, or any other process) will restore the infrastructure to the desired state. What if we removed the node purposefully and had no intention to add a new one in its place? Even if Swarm would be confident that a new node will be added to the cluster, it would still not make sense to wait for it. Creating a new node is a costly operation. It takes too much time. Therefore, as soon as Swarm detected that some of the replicas are not running (those from the failed node), it rescheduled them to the other two nodes. As a result, the third node is currently empty. It will start getting replicas the next time we deploy something or update one of the existing services. Let’s try it out.    We’ll update our test service.    ``` `1` docker service update `\\` `2 `    --env-add `\"FOO=BAR\"` `test`  ```   ````````` Before we take a look at the service processes (or tasks), we should give Swarm a bit of time to perform rolling update to all the replicas. After a moment or two, we can execute `docker service ps` and discuss the result    ``` `1` docker service ps `\\` `2 `    -f desired-state`=`running `test`  ```   ```````` The output is as follows (IDs are removed for brevity).    ```  `1` NAME    IMAGE         NODE                                        DESIRED STATE \\  `2` CURRENT STATE              ERROR PORTS  `3` test.1  alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running       \\  `4` Running about a minute ago  `5` test.2  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\  `6` Running 32 seconds ago  `7` test.3  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\  `8` Running about a minute ago  `9` test.4  alpine:latest ip-172-31-26-141.us-east-2.compute.internal Running       \\ `10` Running about a minute ago `11` test.5  alpine:latest ip-172-31-26-141.us-east-2.compute.internal Running       \\ `12` Running about a minute ago `13` test.6  alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running       \\ `14` Running 55 seconds ago `15` test.7  alpine:latest ip-172-31-26-141.us-east-2.compute.internal Running       \\ `16` Running 20 seconds ago `17` test.8  alpine:latest ip-172-31-26-141.us-east-2.compute.internal Running       \\ `18` Running about a minute ago `19` test.9  alpine:latest ip-172-31-15-30.us-east-2.compute.internal  Running       \\ `20` Running 43 seconds ago `21` test.10 alpine:latest ip-172-31-44-182.us-east-2.compute.internal Running       \\ `22` Running 8 seconds ago  ```   ``````` Since containers are immutable, any update of a service always results in a rolling update process that replaces all the replicas. You’ll notice that, this time, they are spread across all the nodes of the cluster, including the new one.    ``` `1` `exit`  ```   `````` Self-healing applied to infrastructure works! We closed the circle. Swarm makes sure that our services are (almost) always in the desired state. With *Docker For AWS*, we accomplished a similar behavior with nodes.    The reason why over 50% of managers must be operational at any given moment lies in the Raft protocol that synchronizes data. Every piece of information is propagated to all the managers. An action is performed only if the majority agrees. That way we can guarantee data integrity. There is no majority if half or more members are absent.    You might be compelled to create clusters with five managers as a way to decrease chances of a complete cluster meltdown if two managers fail at the same time. In some cases that is a good strategy. However, the chances that two managers running in separate availability zones will go down at the same time are very slim. Don’t take this advice as a commandment. You should experiment with both approaches and make your own decision. I tend to run all my clusters smaller than ten nodes with three managers. When they are bigger, five is a good number.    You might go even further and opt for seven managers. The more, the better. Right? Wrong! Data synchronization between managers is a costly operation. The more managers, the more time is required until a consensus is reached. Seven managers often produce more overhead than benefit.    ### What Now?    We proved that self-healing works not only with services but also with infrastructure. We are getting close to having a self-sufficient system. The only thing missing is to find out a way to add self-adaptation applied to infrastructure. If we accomplish that, we’ll be able to leave our system alone. We can go on vacation knowing that it will be operational without us. We could even go to one of those exotic places that still do not have the Internet. Wouldn’t that be great?    Even though we are one step closer to our goal, we are still not there yet. We’ll take another break before moving on.    We’ll continue the practice from previous chapters. We’ll destroy the cluster and save us from unnecessary cost.    ``` `1` aws cloudformation delete-stack `\\` `2 `    --stack-name devops22 `3`  `4` aws cloudformation describe-stacks `\\` `5 `    --stack-name devops22 `|` `\\` `6 `    jq -r `\".Stacks[0].StackStatus\"`  ```   ````` The output of the `describe-stacks` command is as follows.    ``` `1` DELETE_IN_PROGRESS  ```   ```` Cluster will be removed soon.    Feel free to repeat the command if you don’t trust the system and want to see it through. You’ll know that the cluster is fully removed when you see the error output that follows.    ``` `1` An error occurred (ValidationError) when calling the DescribeStacks operation: S\\ `2` tack with id devops22 does not exist  ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` `````````````````` ``````````````````` ```````````````````` ````````````````````` `````````````````````` ``````````````````````` ```````````````````````` ````````````````````````` `````````````````````````` ``````````````````````````` ```````````````````````````` ````````````````````````````` `````````````````````````````` ``````````````````````````````` ```````````````````````````````` ````````````````````````````````` `````````````````````````````````` ``````````````````````````````````` ```````````````````````````````````` ````````````````````````````````````` `````````````````````````````````````` ``````````````````````````````````````` ```````````````````````````````````````` ````````````````````````````````````````` `````````````````````````````````````````` ``````````````````````````````````````````` ```````````````````````````````````````````` ````````````````````````````````````````````` `````````````````````````````````````````````` ``````````````````````````````````````````````` ```````````````````````````````````````````````` ````````````````````````````````````````````````` `````````````````````````````````````````````````` ```````````````````````````````````````````````````"]