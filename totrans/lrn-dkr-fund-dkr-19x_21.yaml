- en: Monitoring and Troubleshooting an App Running in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to deploy a multi-service application
    into a Kubernetes cluster. We configured application-level routing for the application
    and updated its services using a zero-downtime strategy. Finally, we provided
    confidential data to the running services by using Kubernetes Secrets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn the different techniques used to monitor an
    individual service or a whole distributed application running on a Kubernetes
    cluster. You will also learn how you can troubleshoot an application service that
    is running in production, without altering the cluster or the cluster nodes on
    which the service is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring an individual service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Prometheus to monitor your distributed application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting a service running in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After working through this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure application-level monitoring for a service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Prometheus to collect and centrally aggregate relevant application metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshoot a service running in production using a special tools container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to use Minikube on our local computer. Please refer
    to [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting Up a Working
    Environment*, for more information on how to install and use Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at: [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch17](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition/tree/master/ch17)[.](https://github.com/fundamentalsofdocker/labs/tree/2nd-edition/ch16/probes)'
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure you have cloned the GitHub repository as described in [Chapter
    2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting Up a Working Environment*.
  prefs: []
  type: TYPE_NORMAL
- en: In your Terminal, navigate to the `~/fod/ch17` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring an individual service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with a distributed mission-critical application in production or
    in any production-like environment, then it is of utmost importance to gain as
    much insight as possible into the inner workings of those applications. Have you
    ever had a chance to look into the cockpit of an airplane or the command center
    of a nuclear power plant? Both the airplane and the power plant are samples of
    highly complex systems that deliver mission-critical services. If a plane crashes
    or a power plant shuts down unexpectedly, a lot of people are negatively affected,
    to say the least. Thus the cockpit and the command center are full of instruments
    showing the current or past state of some part of the system. What you see is
    the visual representation of some sensors that are placed in strategic parts of
    the system, and constantly collect data such as the temperature or the flow rate.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the airplane or the power plant, our application needs to be instrumented
    with "sensors" that can feel the "temperature" of our application services or
    the infrastructure they run on. I put the temperature in double quotes since it
    is only a placeholder for things that matter in an application, such as the number
    of requests per second on a given RESTful endpoint, or the average latency of
    request to the same endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting values or readings that we collect, such as the average latency
    of requests, are often called metrics. It should be our goal to expose as many
    meaningful metrics as possible of the application services we build. Metrics can
    be both functional and non-functional. Functional metrics are values that say
    something business-relevant about the application service, such as how many checkouts
    are performed per minute if the service is part of an e-commerce application,
    or which are the five most popular songs over the last 24 hours if we're talking
    about a streaming application.
  prefs: []
  type: TYPE_NORMAL
- en: Non-functional metrics are important values that are not specific to the kind
    of business the application is used for, such as what is the average latency of
    a particular web request or how many `4xx` status codes are returned per minute
    by another endpoint, or how much RAM or how many CPU cycles a given service is
    using.
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed system where each part is exposing metrics, some overarching
    service should be collecting and aggregating the values periodically from each
    component. Alternatively, each component should forward its metrics to a central
    metrics server. Only if the metrics for all components of our highly distributed
    system are available for inspection in a central location are they of any value.
    Otherwise, monitoring the system becomes impossible. That's why pilots of an airplane
    never have to go and inspect individual and critical parts of the airplane in
    person during a flight; all necessary readings are collected and displayed in
    the cockpit.
  prefs: []
  type: TYPE_NORMAL
- en: Today one of the most popular services that is used to expose, collect, and
    store metrics is Prometheus. It is an open source project and has been donated
    to the **Cloud Native Computing Foundation** (**CNCF**). Prometheus has first-class
    integration with Docker containers, Kubernetes, and many other systems and programming
    platforms. In this chapter, we will use Prometheus to demonstrate how to instrument
    a simple service that exposes important metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting a Node.js-based service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we want to learn how to instrument a microservice authored
    in Node Express.js by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new folder called `node` and navigate to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Run `npm init` in this folder, and accept all defaults except the **entry point**,
    which you change from the `index.js` default to `server.js`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to add `express` to our project with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to install the Prometheus adapter for Node Express with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a file called `server.js` to the folder with this content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a very simple Node Express app with a single endpoint: `/hello`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To the preceding code, add the following snippet to initialize the Prometheus
    client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add an endpoint to expose the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s run this sample microservice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can see in the preceding output that the service is listening at port `3000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now try to access the metrics at the `/metrics` endpoint, as we defined
    in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: What we get as output is a pretty long list of metrics, ready for consumption
    by a Prometheus server.
  prefs: []
  type: TYPE_NORMAL
- en: This was pretty easy, wasn't it? By adding a node package and adding a few trivial
    lines of code to our application startup, we have gained access to a plethora
    of system metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s define our own custom metric. Let it be a `Counter` object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code snippet to `server.js` to define a custom counter called
    `my_hello_counter`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To our existing `/hello` endpoint, add code to increase the counter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Rerun the application with `npm start`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To test the new counter, let''s access our `/hello` endpoint twice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get this output when accessing the `/metrics` endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The counter we defined in code clearly works and is output with the `HELP` text
    we added.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to instrument a Node Express application, let's do the
    same for a .NET Core-based microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting a .NET Core-based service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by creating a simple .NET Core microservice based on the Web API
    template.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new `dotnet` folder, and navigate to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Use the `dotnet` tool to scaffold a new microservice called `sample-api:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the Prometheus adapter for .NET, which is available to us as a
    NuGet package called `prometheus-net.AspNetCore`. Add this package to the `sample-api`
    project, with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the project in your favorite code editor; for example, when using VS Code
    execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Locate the `Startup.cs` file, and open it. At the beginning of the file, add
    a `using` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then in the `Configure` method add the `endpoints.MapMetrics()` statement to
    the mapping of the endpoints. Your code should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that the above is valid for version 3.x of .NET Core. If you're on an earlier
    version, the configuration looks slightly different. Consult the following repo
    for more details, at [https://github.com/prometheus-net/prometheus-net.](https://github.com/prometheus-net/prometheus-net)
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, the Prometheus component will start publishing the request metrics
    of ASP.NET Core. Let''s try it. First, start the application with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output tells us that the microservice is listening at `https://localhost:5001`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use `curl` to call the metrics endpoint of the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'What we get is a list of system metrics for our microservice. That was easy:
    we only needed to add a NuGet package and a single line of code to get our service
    instrumented!'
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to add our own (functional) metrics? This is equally straightforward.
    Assume we want to measure the number of concurrent accesses to our `/weatherforecast `endpoint.
    To do this, we define a `gauge` and use it to wrap the logic in the appropriate
    endpoint with this gauge. We can do this by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate the `Controllers/WeatherForecastController.cs` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `using Prometheus;` to the top of the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a private instance variable of the `Gauge `type in the `WeatherForecastController`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Wrap the logic of the `Get` method with a `using` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Restart the microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Call the `/weatherforecast` endpoint a couple of times using `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `curl` to get the metrics, as earlier in this section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that there is now a new metric called `myapp_weather_forecasts_in_progress`
    available in the list. Its value will be zero, since currently you are not running
    any requests against the tracked endpoint, and a `gauge` type metric is only measuring
    the number of ongoing requests.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you have just defined your first functional metric. This is
    only a start; many more sophisticated possibilities are readily available to you.
  prefs: []
  type: TYPE_NORMAL
- en: Node.js or .NET Core-based application services are by no means special. It
    is just as straightforward and easy to instrument services written in other languages,
    such as Java, Python, or Go.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned how to instrument an application service so that it exposes important
    metrics, let's now have a look how we can use Prometheus to collect and aggregate
    those values to allow us to monitor a distributed application.
  prefs: []
  type: TYPE_NORMAL
- en: Using Prometheus to monitor a distributed application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned how to instrument an application service to expose
    Prometheus metrics, it's time to show how we can collect the metrics and forward
    them to a Prometheus server where all metrics will be aggregated and stored. We
    can then either use the (simple) web UI of Prometheus or a more sophisticated
    solution like Grafana to display important metrics on a dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike most other tools that are used to collect metrics from application services
    and infrastructure components, the Prometheus server takes the load of work and
    periodically scrapes all the defined targets. This way applications and services
    don't need to worry about forwarding data. You can also describe this as pulling
    metrics versus pushing them. This makes Prometheus servers an excellent fit for
    our case.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss how to deploy Prometheus to Kubernetes, followed by our
    two sample application services. Finally, we will deploy Grafana to the cluster,
    and use it to display our customer metrics on a dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s have a quick overview of the architecture of the planned system. As
    mentioned before, we have our microservices, the Prometheus server, and Grafana.
    Furthermore, everything will be deployed to Kubernetes. The following diagram
    shows the relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/567104a5-741d-48cd-9a0f-c6cb04042413.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level overview of an application using Prometheus and Grafana for monitoring
  prefs: []
  type: TYPE_NORMAL
- en: In the top center of the diagram, we have Prometheus, which periodically scrapes
    metrics from Kubernetes, shown on the left. It also periodically scrapes metrics
    from the services, in our case from the Node.js and the .NET sample services we
    created and instrumented in the previous section. Finally, on the right-hand side
    of the diagram, we have Grafana that is pulling data periodically from Prometheus
    to then display it on graphical dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Prometheus to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As indicated, we start by deploying Prometheus to Kubernetes. Let''s first
    define the Kubernetes YAML file that we can use to do so. First, we need to define
    a Kubernetes `Deployment` that will create a `ReplicaSet` of Prometheus server
    instances, and then we will define a Kubernetes service to expose Prometheus to
    us, so that we can access it from within a browser tab or that Grafana can access
    it. Let''s do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `ch17/kube` folder, and navigate to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Add a file called `prometheus.yaml` to this folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following code snippet to this file; it defines `Deployment` for Prometheus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We are defining a replica set with two instances of Prometheus. Each instance
    is assigned the two labels: `app: prometheus` and `purpose: monitoring-demo` for
    identification purposes. The interesting part is in the `volumeMounts` of container
    spec. There we mount a Kubernetes `ConfigMap` object, called `prometheus-cm` containing
    the Prometheus configuration, into the container to the location where Prometheus
    expects its configuration file(s). The volume of the `ConfigMap` type is defined
    on the last four lines of the above code snippet.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we will define the `config` map later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s define the Kubernetes service for Prometheus. Append this snippet
    to the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Please note the three dashes (`---`) at the beginning of the snippet are needed
    to separate individual object definitions in our YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: We call our service `prometheus-svc` and make it a `NodePort` (and not just
    a service of the `ClusterIP` type) to be able to access the Prometheus web UI
    from the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can define a simple configuration file for Prometheus. This file basically
    instructs the Prometheus server which services to scrape metrics from and how
    often to do so. First, create a `ch17/kube/config` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Please add a file called `prometheus.yml` to the last folder, and add the following
    content to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding file, we define three jobs for Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one called `prometheus` scrapes metrics every five seconds from the
    Prometheus server itself. It finds those metrics the at `localhost:9090` target.
    Note that by default the metrics should be exposed at the `/metrics` endpoint.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: The second job called `dotnet` scrapes metrics from a service found at `dotnet-api-svc:5000`,
    which will be our .NET Core service that we have defined and instrumented previously.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the third job does the same for our Node service. Note that we also
    have added a `group: ''production''` label to this job. This allows for further
    grouping of jobs or tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we can define the `ConfigMap` object in our Kubernetes cluster, with the
    next command. From within the `ch17/kube` folder execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now deploy Prometheus to our Kubernetes server with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s double-check that the deployment succeeded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Keep a close eye on the list of pods, and make sure they are all up and running.
    Please also note the port mapping of the `prometheus-svc` object. In my case,
    the `9090` port is mapped to the `31962` host port. In your case, the latter may
    be different, but it will also be in the `3xxxx` range.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now access the web UI of Prometheus. Open a new browser tab, and navigate
    to `http://localhost:<port>/targets` where `<port>` in my case is `31962`. You
    should see something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75ab2089-88ea-4bb2-b5c3-37e6e7c90d18.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus web UI showing the configured targets
  prefs: []
  type: TYPE_NORMAL
- en: In the last screenshot, we can see that we defined three targets for Prometheus.
    Only the third one in the list is up and accessible by Prometheus. It is the endpoint
    we defined in the configuration file for the job that scrapes metrics from Prometheus
    itself. The other two services are not running at this time, and thus their state
    is down.
  prefs: []
  type: TYPE_NORMAL
- en: Now navigate to Graph by clicking on the respective link in the top menu of
    the UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the metrics drop-down list, and inspect all the listed metrics that Prometheus
    found. In this case, it is only the list of metrics defined by the Prometheus
    server itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/73ba9c62-5f75-4962-8fff-84911faec999.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus web UI showing available metrics
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are ready to deploy the .NET and the Node sample services, we
    created earlier, to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our application services to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can use the sample services we created earlier and deploy them to
    Kubernetes, we must create Docker images for them and push them to a container
    registry. In our case, we will just push them to Docker Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the .NET Core sample:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate the `Program.cs` file in the .NET project and open it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Modify the `CreateHostBuilder` method so it looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `Dockerfile` with the following content to the `ch17/dotnet/sample-api`
    project folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Docker image by using this command from within the `dotnet/sample-api`
    project folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that you may want to replace `fundamentalsofdocker` with your own Docker
    Hub username in the preceding and subsequent command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Push the image to Docker Hub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we do the same with the Node sample API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add `Dockerfile` with the following content to the `ch17/node` project folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Docker image by using this command from within the `ch17/node` project
    folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note once again that you may want to replace `fundamentalsofdocker` with your
    own Docker Hub username in the preceding and subsequent command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Push the image to Docker Hub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: With this, we are ready to define the necessary Kubernetes objects for the deployment
    of the two services. The definition is somewhat lengthy and can be found in the `~/fod/ch17/kube/app-services.yaml` file
    in the repository. Please open that file and analyze its content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use this file to deploy the services:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Double-check that the services are up and running using the `kubectl get all` command.
    Make sure all the pods of the Node and .NET sample API services are up and running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List all Kubernetes services to find out the host ports for each application
    service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In my case, the .NET API is mapped to port `30822 `, and the Node API to port `31713`.
    Your ports may differ.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `curl` to access the `/metrics` endpoint for both services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Double-check the `/targets` endpoint in Prometheus to make sure the two microservices
    are now reachable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9500692f-aabe-4ceb-822b-71ef7c743735.png)'
  prefs: []
  type: TYPE_IMG
- en: Prometheus showing all targets are up and running
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure the custom metrics we defined for our Node.js and .NET services
    are defined and exposed, we need to access each service at least once. Thus use
    `curl` to access the respective endpoints a few times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The last step is to deploy Grafana to Kubernetes so that we have the ability
    to create sophisticated and graphically appealing dashboards displaying key metrics
    of our application services and/or infrastructure components.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Grafana to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's also deploy Grafana to our Kubernetes cluster, so that we can manage
    this tool the same way as all the other components of our distributed application.
    As the tool that allows us to create dashboards for monitoring the application,
    Grafana can be considered mission-critical and thus warrants this treatment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying Grafana to the cluster is pretty straightforward. Let''s do it as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a new file called `grafana.yaml` to the `ch17/kube` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To this file, add the definition for a Kubernetes `Deployment` for Grafana:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: There are no surprises in that definition. In this example, we are running a
    single instance of Grafana, and it uses the `app` and `purpose` labels for identification,
    similar to what we used for Prometheus. No special volume mapping is needed this
    time since we are only working with defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to expose Grafana, and thus add the following snippet to the preceding
    file to define a service for Grafana:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we are using a service of the `NodePort` type to be able to access
    the Grafana UI from our host.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now deploy Grafana with this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s find out what the port number will be, over which we can access Grafana:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a new browser tab, and navigate to `http://localhost:<port>` where `<port>` is
    the port you identified in the previous step, and in my case is `32379`. You should
    see something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bddbf845-8092-4f22-9777-19b297470767.png)'
  prefs: []
  type: TYPE_IMG
- en: Login screen of Grafana
  prefs: []
  type: TYPE_NORMAL
- en: Login with the default `admin` username, and the password is also `admin`. When
    asked to change the password click the Skip link for now. You will be redirected
    to the Home dashboard**.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the Home Dashboard, click on Create your first data source, and select Prometheus
    from the list of data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `http://prometheus-svc:9090` for the URL to Prometheus, and click the green Save
    & Testbutton.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Grafana, navigate back to the Home dashboard, and then select the New dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click Add query, and then from the Metrics drop-down menu, select the custom
    metric we defined in the .NET sample service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cc959094-a62e-4d8d-9af7-fb6b43dcfcfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the .NET custom metric in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: Change the value of Relative time from `1h` to `5m` ( five minutes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the dashboard refresh rate found in the upper-right corner of the view
    to `5s` (five seconds).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the same for the custom metric defined in the Node sample service, so
    that you will have two panels on your new dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the dashboard and its panels to your liking by consulting the documentation
    at [https://grafana.com/docs/grafana/latest/guides/getting_started/](https://grafana.com/docs/grafana/latest/guides/getting_started/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `curl` to access the two endpoints of the sample services, and observe
    the dashboard. It may look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/40be8912-dc9f-4b36-a80b-970944799afa.png)'
  prefs: []
  type: TYPE_IMG
- en: Grafana dashboard with our two custom metrics
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, we can say that Prometheus is a good fit to monitor our microservices
    because we just need to expose a metrics port, and thus don't need to add too
    much complexity or run additional services. Prometheus then is in charge of periodically
    scraping the configured targets, so that our services don't need to worry about
    emitting them.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting a service running in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a recommended best practice to create minimal images for production that
    don't contain anything that is not absolutely needed. This includes common tools
    that are usually used to debug and troubleshoot an application, such as netcat,
    iostat, ip, or others. Ideally, a production system only has the container orchestration
    software such as Kubernetes installed on a cluster node with a minimal OS, such
    as Core OS. The application container in turn ideally only contains the binaries
    absolutely necessary to run. This minimizes the attack surface and the risk of
    having to deal with vulnerabilities. Furthermore, a small image has the advantage
    of being downloaded quickly, using less space on disk and in memory and showing
    faster startup times.
  prefs: []
  type: TYPE_NORMAL
- en: But this can be a problem if one of the application services running on our
    Kubernetes cluster shows unexpected behavior and maybe even crashes. Sometimes
    we are not able to find the root cause of the problem just from the logs generated
    and collected, so we might need to troubleshoot the component on the cluster node
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: We may be tempted to SSH into the given cluster node and run some diagnostic
    tools. But this is not possible since the cluster node only runs a minimal Linux
    distro with no such tools installed. As a developer, we could now just ask the
    cluster administrator to install all the Linux diagnostic tools we intend to use.
    But that is not a good idea. First of all, this would open the door for potentially
    vulnerable software now residing on the cluster node, endangering all the other
    pods that run on that node, and also open a door to the cluster itself that could
    be exploited by hackers. Furthermore, it is always a bad idea to give developers
    direct access to nodes of a production cluster, no matter how much you trust your
    developers. Only a limited number of cluster administrators should ever be able
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is to have the cluster admin run a so-called bastion container
    on behalf of the developers. This bastion or troubleshoot container has all the
    tools installed that we need to pinpoint the root cause of the bug in the application
    service. It is also possible to run the bastion container in the host's network
    namespace; thus, it will have full access to all the network traffic of the container
    host.
  prefs: []
  type: TYPE_NORMAL
- en: The netshoot container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nicola Kabar, a former Docker employee, has created a handy Docker image called
    `nicolaka/netshoot` that field engineers at Docker use all the time to troubleshoot
    applications running in production on Kubernetes or Docker Swarm. We created a
    copy of the image for this book, available at `fundamentalsofdocker/netshoot`.
    The purpose of this container in the words of the creator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Purpose: Docker and Kubernetes network troubleshooting can become complex.
    With proper understanding of how Docker and Kubernetes networking works and the
    right set of tools, you can troubleshoot and resolve these networking issues.
    The `netshoot` container has a set of powerful networking troubleshooting tools
    that can be used to troubleshoot Docker networking issues."                   
                                                                                 
                                                                                 
                                                                                 
                                                                                 
     - *Nicola Kabar*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this container for debugging purposes, we can proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spin up a throwaway bastion container for debugging on Kubernetes, using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now use tools such as `ip` from within this container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'On my machine, this results in an output similar to the following if I run
    the pod on Docker for Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: To leave this troubleshoot container, just press *Ctrl* + *D* or type `exit`
    and then hit *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we need to dig a bit deeper and run the container in the same network namespace
    as the Kubernetes host, then we can use this command instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: If we run `ip` again in this container, we will see everything that the container
    host sees too, for example, all the `veth` endpoints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `netshoot` container has all the usual tools installed that an engineer
    ever needs to troubleshoot network-related problems. Some of the more familiar
    ones are `ctop`, `curl`, `dhcping`, `drill`, `ethtool`, `iftop`, `iperf`, and `iproute2`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned some techniques used to monitor an individual service
    or a whole distributed application running on a Kubernetes cluster. Furthermore,
    you investigated troubleshooting an application service that is running in production
    without having to alter the cluster or the cluster nodes on which the service
    is running.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter of this book, you will gain an overview of some
    of the most popular ways of running containerized applications in the cloud. The
    chapter includes samples on how to self-host and use hosted solutions and discuss
    their pros and cons. Fully managed offerings of vendors such as Microsoft Azure
    and Google Cloud Engine are briefly discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your learning progress, please answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is it important to instrument your application services?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you describe to an interested layperson what Prometheus is?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exporting Prometheus metrics is easy. Can you describe in simple words how you
    can do this for a Node.js application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You need to debug a service running on Kubernetes in production. Unfortunately,
    the logs produced by this service alone don't give enough information to pinpoint
    the root cause. You decide to troubleshoot the service directly on the respective
    Kubernetes cluster node. How do you proceed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few links that provide additional information on the topics discussed
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Monitoring with Prometheus*:* [https://sysdig.com/blog/kubernetes-monitoring-prometheus/](https://sysdig.com/blog/kubernetes-monitoring-prometheus/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus Client Libraries*: *[https://prometheus.io/docs/instrumenting/clientlibs/](https://prometheus.io/docs/instrumenting/clientlibs/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `netshoot` container*: *[https://github.com/nicolaka/netshoot](https://github.com/nicolaka/netshoot)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
