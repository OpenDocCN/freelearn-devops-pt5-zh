<html><head></head><body>
		<div><h1 id="_idParaDest-243"><em class="italic"><a id="_idTextAnchor242"/>Chapter 9</em>: Advanced Continuous Delivery</h1>
			<p>In the previous chapters, we started with nothing and ended with a complete continuous delivery pipeline. Now, it's time to present a mixture of different aspects that are also very important in the continuous delivery process, but which haven't been described yet.</p>
			<p>This chapter covers the following points:</p>
			<ul>
				<li>Managing database changes</li>
				<li>Pipeline patterns</li>
				<li>Release patterns</li>
				<li>Working with legacy systems</li>
			</ul>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor243"/>Technical requirements</h1>
			<p>To follow along with the instructions in this chapter, you'll need the following:</p>
			<ul>
				<li>Java 8+</li>
				<li>A Jenkins instance</li>
			</ul>
			<p>All the examples and solutions to the exercises can be found on GitHub at <a href="https://github.com/PacktPublishing/Continuous-Delivery-With-Docker-and-Jenkins-3rd-Edition/tree/main/Chapter09">https://github.com/PacktPublishing/Continuous-Delivery-With-Docker-and-Jenkins-3rd-Edition/tree/main/Chapter09</a>.</p>
			<p>Code in Action videos for this chapter can be viewed at <a href="https://bit.ly/3NVVOyi">https://bit.ly/3NVVOyi</a>.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/>Managing database changes</h1>
			<p>So far, we<a id="_idIndexMarker943"/> have focused on a continuous delivery process that was applied to a web service. A simple factor in this was that web services are inherently stateless. This fact means that they can easily be updated, restarted, cloned in many instances, and recreated from the given source code. A web service, however, is usually linked to its stateful part: a database that poses new challenges to the delivery process. These challenges can be grouped into the following categories:</p>
			<ul>
				<li><strong class="bold">Compatibility</strong>: The<a id="_idIndexMarker944"/> database schema, and the data itself, must be compatible with the web service all the time.</li>
				<li><strong class="bold">Zero-downtime deployment</strong>: In order to achieve zero-downtime deployment, we use rolling updates, which means that a database must be compatible with two different web service versions at the same time.</li>
				<li><strong class="bold">Rollback</strong>: A rollback of a database can be difficult, limited, or sometimes even impossible because not all operations are reversible (for example, removing a column that contains data).</li>
				<li><strong class="bold">Test data</strong>: Database-related <a id="_idIndexMarker945"/>changes are difficult to test because we need test data that is very similar to production data.</li>
			</ul>
			<p>In this section, I will explain how to address these challenges so that the continuous delivery process will be as safe as possible.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>Understanding schema updates</h2>
			<p>If you<a id="_idIndexMarker946"/> think about the delivery process, it's not really the data itself that causes difficulties, because we don't usually change the data when we deploy an application. The data is something that is collected while the system is live in production, whereas, during deployment, we only change the way we store and interpret this data. In other words, in the context of the continuous delivery process, we are interested in the structure of the database, not exactly in its content. This is why this section concerns mainly relational databases (and their schemas) and focuses less on other types of storage, such as NoSQL databases, where there is no structure definition.</p>
			<p>To better understand this, think of Hazelcast, which we have already used in this book. It stored the cached data so, effectively, it was a database. Nevertheless, it required zero effort from the continuous delivery perspective since it didn't have any data structure. All it stored were the key-value entries, which do not evolve over time.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">NoSQL databases<a id="_idIndexMarker947"/> usually don't have any restricting schema, and therefore, they simplify the continuous delivery process because there is no additional schema update step required. This is a huge benefit; however, it doesn't necessarily mean that writing applications with NoSQL databases is simpler because we have to put more effort into data validation in the source code.</p>
			<p>Relational databases have static schemas. If we would like to change it (for example, to add a new column to the table), we need to write and execute a SQL <strong class="bold">data definition language</strong> (<strong class="bold">DDL</strong>) script. Doing<a id="_idIndexMarker948"/> this manually for every change requires a lot of work and leads to error-prone solutions in which the operations team has to keep the code and the database structure in sync. A much better solution is to automatically update the schema in an incremental manner. Such a solution is called <strong class="bold">database migration</strong>.</p>
			<h3>Introducing database migration</h3>
			<p>Database schema migration<a id="_idIndexMarker949"/> is a process of incremental changes to the relational database structure. Let's take a look at the following diagram to understand it better:</p>
			<div><div><img src="img/B18223_09_01.jpg" alt="Figure 9.1 – Database schema migration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Database schema migration</p>
			<p>The database in version <em class="italic">v1</em> has the schema defined by the <code>V1_init.sql</code> file. Additionally, it stores the metadata related to the migration process, for example, its current schema version and the migration changelog. When we want to update the schema, we provide the changes in the form of a SQL file, such as <code>V2_add_table.sql</code>. Then, we <a id="_idIndexMarker950"/>need to run the migration tool that executes the given SQL file on the database (it also updates the metatables). In effect, the database schema is a result of all subsequently executed SQL migration scripts. Next, we will see an example of migration.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Migration scripts should be stored in the version control system, usually in the same repository as the source code.</p>
			<p>Migration tools and the strategies they use can be divided into two categories:</p>
			<ul>
				<li><strong class="bold">Upgrade and downgrade</strong>: This approach (as used by the Ruby on Rails framework, for example) means <a id="_idIndexMarker951"/>that we can migrate up (from <em class="italic">v1</em> to <em class="italic">v2</em>) and down (from <em class="italic">v2</em> to <em class="italic">v1</em>). It allows the database schema to roll back, which may sometimes end in data loss (if the migration is logically irreversible).</li>
				<li><strong class="bold">Upgrade only</strong>: This <a id="_idIndexMarker952"/>approach (as used by the Flyway tool, for example) only allows us to migrate up (from <em class="italic">v1</em> to <em class="italic">v2</em>). In many cases, the database updates are not reversible, for example, when removing a table from the database. Such a change cannot be rolled back, because even if we recreate the table, we have already lost all the data.</li>
			</ul>
			<p>There are <a id="_idIndexMarker953"/>many database migration tools available on the market, the most popular of which are <strong class="bold">Flyway</strong>, <strong class="bold">Liquibase</strong>, and <strong class="bold">Rail Migrations</strong> (from the Ruby on Rails framework). As a <a id="_idIndexMarker954"/>next <a id="_idIndexMarker955"/>step to understanding how such tools work, we will look at an example based on the Flyway tool.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">There are also commercial solutions provided for the particular databases, for example, Redgate (for SQL Server) and Optim Database Administrator (for DB2).</p>
			<h3>Using Flyway</h3>
			<p>Let's use Flyway <a id="_idIndexMarker956"/>to create a database schema for the calculator web service. The database will store the history of all operations that were executed on the service: the first parameter, the second parameter, and the result.</p>
			<p>We show how to use the SQL database and Flyway in three steps:</p>
			<ol>
				<li>Configuring the Flyway tool to work with Gradle</li>
				<li>Defining the SQL migration script to create the calculation history table</li>
				<li>Using the SQL database inside the Spring Boot application code</li>
			</ol>
			<p>Let's get started.</p>
			<h4>Configuring Flyway</h4>
			<p>In order to use<a id="_idIndexMarker957"/> Flyway with Gradle, we need to add the following content to the <code>build.gradle</code> file:</p>
			<pre>buildscript {
    dependencies {
        classpath('com.h2database:h2:1.4.200')
    }
}
...
plugins {
   id "org.flywaydb.flyway" version "8.5.0"
}
...
flyway {
   url = 'jdbc:h2:file:/tmp/calculator'
   user = 'sa'
}</pre>
			<p>Here are some quick comments on the configuration:</p>
			<ul>
				<li>We used the H2 database, which is an in-memory (and file-based) database.</li>
				<li>We store the database in the <code>/tmp/calculator</code> file.</li>
				<li>The default database user is called <code>sa</code> (system administrator).<p class="callout-heading">Tip</p><p class="callout">In the case of other SQL databases (for example, MySQL), the configuration would be very similar. The only difference is in the Gradle dependencies and the JDBC connection.</p></li>
			</ul>
			<p>After this<a id="_idIndexMarker958"/> configuration is applied, we should be able to run the Flyway tool by executing the following command:</p>
			<pre>$ ./gradlew flywayMigrate -i</pre>
			<p>The command created the database in the <code>/tmp/calculator.mv.db</code> file. Obviously, it has no schema, since we haven't defined anything yet.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Flyway can be used as a command-line tool, via the Java API, or as a plugin for the popular building tools Gradle, Maven, and Ant.</p>
			<h4>Defining the SQL migration script</h4>
			<p>The next step is to define the <a id="_idIndexMarker959"/>SQL file that adds the calculation table to the database schema. Let's create the <code>src/main/resources/db/migration/V1__Create_calculation_table.sql</code> file, with the following content:</p>
			<pre>create table CALCULATION (
   ID      int not null auto_increment,
   A       varchar(100),
   B       varchar(100),
   RESULT  varchar(100),
   primary key (ID)
);</pre>
			<p>Note the migration file naming convention, <code>&lt;version&gt;__&lt;change_description&gt;.sql</code>. The SQL file creates a table with four columns, <code>ID</code>, <code>A</code>, <code>B</code>, and <code>RESULT</code>. The <code>ID</code> column is an automatically incremented primary key of the table. Now, we are ready to run the <code>flyway</code> command to apply the migration:</p>
			<pre>$ ./gradlew flywayMigrate -i
...
Migrating schema "PUBLIC" to version "1 - Create calculation table"
Successfully applied 1 migration to schema "PUBLIC", now at version v1 (execution time 00:00.018s)</pre>
			<p>The command automatically<a id="_idIndexMarker960"/> detected the migration file and executed it on the database.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">The migration files should be always kept in the version control system, usually with the source code.</p>
			<h4>Accessing the database</h4>
			<p>We have executed our <a id="_idIndexMarker961"/>first migration, so the database is prepared. To see the complete example, we should also adapt our project so that it can access the database.</p>
			<p>Let's first configure the Gradle dependencies to use <code>h2database</code> from the Spring Boot project: </p>
			<ol>
				<li value="1">We can do this by adding the following lines to the <code>build.gradle</code> file:<pre>dependencies {
   implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
   implementation 'com.h2database:h2:1.4.200'
}</pre></li>
				<li>The next step is to set up the database location and the startup behavior in the <code>src/main/resources/application.properties</code> file:<pre>spring.datasource.url=jdbc:h2:file:/tmp/calculator;DB_CLOSE_ON_EXIT=FALSE 
spring.jpa.hibernate.ddl-auto=validate
spring.datasource.username=sa</pre></li>
			</ol>
			<p>The second line <a id="_idIndexMarker962"/>means that Spring Boot will not try to automatically generate the database schema from the source code model. On the contrary, it will only validate if the database schema is consistent with the Java model.</p>
			<ol>
				<li value="3">Now, let's create the Java ORM entity model for the calculation in the new <code>src/main/java/com/leszko/calculator/Calculation.java</code> file:<pre>package com.leszko.calculator;
import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.GenerationType;
import javax.persistence.Id;
@Entity
public class Calculation {
   @Id
   @GeneratedValue(strategy= GenerationType.IDENTITY)
   private Integer id;
   private String a;
   private String b;
   private String result;
   protected Calculation() {}
   public Calculation(String a, String b, String result) {
       this.a = a;
       this.b = b;
       this.result = result;
   }
}</pre></li>
			</ol>
			<p>The <code>Entity</code> class<a id="_idIndexMarker963"/> represents the database mapping in the Java code. A table is expressed as a class, with each column as a field. The next step is to create the repository for loading and storing the <code>Calculation</code> entities.</p>
			<ol>
				<li value="4">Let's create the <code>src/main/java/com/leszko/calculator/CalculationRepository.java</code> file:<pre>package com.leszko.calculator;
import org.springframework.data.repository.CrudRepository;
public interface CalculationRepository extends CrudRepository&lt;Calculation, Integer&gt; {}</pre></li>
				<li>Finally, we can use the <code>Calculation</code> and <code>CalculationRepository</code> classes to store the calculation history. Let's<a id="_idIndexMarker964"/> add the following code to the <code>src/main/java/com/leszko/calculator/CalculatorController.java</code> file:<pre>...
class CalculatorController {
   ...
   @Autowired
   private CalculationRepository calculationRepository;
   @RequestMapping("/sum")
   String sum(@RequestParam("a") Integer a, @RequestParam("b") Integer b) {
       String result = String.valueOf(calculator.sum(a, b));
       calculationRepository.save(new Calculation(a.toString(), b.toString(), result));
       return result;
   }
}</pre></li>
				<li>Now, we can finally start the service, for example, using the following command:<pre><strong class="bold">$ ./gradlew bootRun</strong></pre></li>
			</ol>
			<p>When we have started the service, we can send a request to the <code>/sum</code> endpoint. As a <a id="_idIndexMarker965"/>result, each summing operation is logged into the database.</p>
			<p class="callout-heading">Tip </p>
			<p class="callout">If you would like to browse the database content, you can add <code>spring.h2.console.enabled=true</code> to the <code>application.properties</code> file, and then browse the database via the <code>/h2-console</code> endpoint.</p>
			<p>We explained how the database schema migration works and how to use it inside a Spring Boot project built with Gradle. Now, let's take a look at how it integrates within the continuous delivery process.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/>Changing the database in continuous delivery</h2>
			<p>The first approach to use <a id="_idIndexMarker966"/>database updates inside the <a id="_idIndexMarker967"/>continuous delivery pipeline is to add a stage within the migration command execution. This simple solution works correctly for many cases; however, it has two significant drawbacks:</p>
			<ul>
				<li><strong class="bold">Rollback</strong>: As <a id="_idIndexMarker968"/>mentioned before, it's not always possible to roll back the database change (Flyway doesn't support downgrades at all). Therefore, in the case of service rollback, the database becomes incompatible.</li>
				<li><strong class="bold">Downtime</strong>: The service<a id="_idIndexMarker969"/> update and the database update are not executed at exactly the same time, which causes downtime.</li>
			</ul>
			<p>This leads us to two constraints that we will need to address:</p>
			<ul>
				<li>The database version needs to be compatible with the service version all the time.</li>
				<li>The database schema migration is not reversible.</li>
			</ul>
			<p>We will address these constraints for two different cases: backward-compatible updates and non-backward-compatible updates.</p>
			<h3>Backward-compatible changes</h3>
			<p>Backward-compatible changes are <a id="_idIndexMarker970"/>simpler. Let's look at the following figure to see how they work:</p>
			<div><div><img src="img/B18223_09_02.jpg" alt="Figure 9.2 – Backward-compatible database migration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Backward-compatible database migration</p>
			<p>Suppose that the <strong class="bold">Database v10</strong> schema migration is backward-compatible. If we need to roll back the <strong class="bold">Service v1.2.8</strong> release, then we deploy <strong class="bold">Service v1.2.7</strong>, and there is no need to do anything with the database (database migrations are not reversible, so we keep <strong class="bold">Database v11</strong>). Since the schema update is backward-compatible, <strong class="bold">Service v.1.2.7</strong> works perfectly fine with <strong class="bold">Database v11</strong>. The same applies if we need to roll back to <strong class="bold">Service v1.2.6</strong>, and so on. Now, suppose that <strong class="bold">Database v10</strong> and all other migrations are backward-compatible; then we could roll back to any service version, and everything would work correctly.</p>
			<p>There is also no problem with downtime. If the database migration has zero-downtime itself, then we can execute it first, and then use the rolling updates for the service.</p>
			<p>Let's look at an example of a backward-compatible change. We will create a schema update that adds a <code>created_at</code> column to the calculation table. The <code>src/main/resources/db/migration/V2__Add_created_at_column.sql</code> migration file looks as follows:</p>
			<pre>alter table CALCULATION
add CREATED_AT timestamp;</pre>
			<p>Aside from the migration script, the calculator service requires a new field in the <code>Calculation</code> class:</p>
			<pre>...
private Timestamp createdAt;
...</pre>
			<p>We also need to adjust its<a id="_idIndexMarker971"/> constructor, and then its usage in the <code>CalculatorController</code> class:</p>
			<pre>calculationRepository.save(new Calculation(a.toString(), b.toString(), result, Timestamp.from(Instant.now())));</pre>
			<p>After running the service, the calculation history is stored with the <code>created_at</code> column. Note that the change is backward-compatible because, even if we reverted the Java code and left the <code>created_at</code> column in the database, everything would work perfectly fine (the reverted code does not address the new column at all).</p>
			<h3>Non-backward-compatible changes</h3>
			<p>Non-backward-compatible changes <a id="_idIndexMarker972"/>are way more difficult. Looking at the previous diagram, if the <strong class="bold">v11</strong> database change was backward-incompatible, it would be impossible to roll back the service to <strong class="bold">1.2.7</strong>. In this case, how can we approach non-backward-compatible database migrations so that rollbacks and zero-downtime deployments would be possible?</p>
			<p>To cut a long story short, we can address this issue by converting a non-backward-compatible change into a change that is backward-compatible for a certain period of time. In other words, we need to put in the extra effort and split the schema migration into two parts:</p>
			<ul>
				<li>Backward-compatible update executed now, which usually means keeping some redundant data</li>
				<li>Non-backward-compatible update executed after the rollback period time that defines how far back we can revert our code</li>
			</ul>
			<p>To better illustrate this, let's look at the following diagram:</p>
			<div><div><img src="img/B18223_09_03.jpg" alt="Figure 9.3 – Non-backward-compatible database migration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Non-backward-compatible database migration</p>
			<p>Let's consider<a id="_idIndexMarker973"/> an example of dropping a column. A proposed method would include two steps:</p>
			<ul>
				<li>Stop using the column in the source code (<strong class="bold">v1.2.5</strong>, backward-compatible update, executed first).</li>
				<li>Drop the column from the database (<strong class="bold">v11</strong>, non-backward-compatible update, executed after the rollback period).</li>
			</ul>
			<p>All service versions until <strong class="bold">Database v11</strong> can be rolled back to any previous version; the services starting from <strong class="bold">Service v1.2.8</strong> can only be rolled back within the rollback period. Such an approach may sound trivial because all we did was delay the column removal from the database. However, it addresses both the rollback issue and the zero-downtime deployment issue. As a result, it reduces the risk associated with the release. If we adjust the rollback period to a reasonable amount of time (for example, in the case of multiple releases per day to 2 weeks), then the risk is negligible. We don't usually roll many versions back.</p>
			<p>Dropping a column was a very simple example. Let's take a look at a more difficult scenario and rename the result column in our calculator service. We show how to do this in a few steps:</p>
			<ol>
				<li value="1">Adding a new column to the database</li>
				<li>Changing the code to use both columns</li>
				<li>Merging the data in both columns</li>
				<li>Removing the old column from the code</li>
				<li>Dropping the old column from the database</li>
			</ol>
			<p>Let's look at these steps in detail.</p>
			<h4>Adding a new column to the database</h4>
			<p>Let's suppose<a id="_idIndexMarker974"/> that we need to rename the <code>result</code> column to <code>sum</code>. The first step is to add a new column that will be a duplicate. We must create a <code>src/main/resources/db/migration/V3__Add_sum_column.sql</code> migration file:</p>
			<pre>alter table CALCULATION
add SUM varchar(100);</pre>
			<p>As a result, after executing the migration, we will have two columns: <code>result</code> and <code>sum</code>.</p>
			<h4>Changing the code to use both columns</h4>
			<p>The next step is to <a id="_idIndexMarker975"/>rename the column in the source code model and to use both database columns for the <code>set</code> and <code>get</code> operations. We can change it in the <code>Calculation</code> class:</p>
			<pre>public class Calculation {
    ...
    private String sum;
    ...
    public Calculation(String a, String b, String sum, Timestamp createdAt) {
        this.a = a;
        this.b = b;
        this.sum = sum;
        this.result = sum;
        this.createdAt = createdAt;
    }
    public String getSum() {
        return sum != null ? sum : result;
    }
}</pre>
			<p class="callout-heading">Tip </p>
			<p class="callout">To be 100% accurate, in the <code>getSum()</code> method, we should compare something like the last modification column date. (It's not exactly necessary to always take the new column first.)</p>
			<p>From now on, every <a id="_idIndexMarker976"/>time we add a row into the database, the same value is written to both the <code>result</code> and <code>sum</code> columns. While reading <code>sum</code>, we first check whether it exists in the new column, and if not, we read it from the old column.</p>
			<p class="callout-heading">Tip </p>
			<p class="callout">The same result can be achieved with the use of database triggers that would automatically write the same values into both columns.</p>
			<p>All the changes that we have made so far are backward-compatible, so we can roll back the service anytime we want, to any version we want.</p>
			<h4>Merging the data in both columns</h4>
			<p>This step is <a id="_idIndexMarker977"/>usually done after some time when the release is stable. We need to copy the data from the old <code>result</code> column into the new <code>sum</code> column. Let's create a migration file called <code>V4__Copy_result_into_sum_column.sql</code>:</p>
			<pre>update CALCULATION
set CALCULATION.sum = CALCULATION.result
where CALCULATION.sum is null;</pre>
			<p>We still have no limits for the rollback; however, if we need to deploy the version before the change in <em class="italic">step 2</em>, then this database migration needs to be repeated.</p>
			<h4>Removing the old column from the code</h4>
			<p>At this point, we already have<a id="_idIndexMarker978"/> all data in the new column, so we can start to use it without the old column in the data model. In order to do this, we need to remove all code related to <code>result</code> in the <code>Calculation</code> class so that it would look as follows:</p>
			<pre>public class Calculation {
    ...
    private String sum;
    ...
    public Calculation(String a, String b, String sum, Timestamp createdAt) {
        this.a = a;
        this.b = b;
        this.sum = sum;
        this.createdAt = createdAt;
    }
    public String getSum() {
        return sum;
    }
}</pre>
			<p>After this operation, we will no longer use the <code>result</code> column in the code. Note that this operation is only <a id="_idIndexMarker979"/>backward-compatible up to <em class="italic">step 2</em>. If we need to roll back to <em class="italic">step 1</em>, then we could lose the data stored after this step.</p>
			<h4>Dropping the old column from the database</h4>
			<p>The last step is to drop the<a id="_idIndexMarker980"/> old column from the database. This migration should be performed after the rollback period when we are sure we won't need to roll back before <em class="italic">step 4</em>.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">The rollback period can be very long since we aren't using the column from the database anymore. This task can be treated as a cleanup task, so even though it's non-backward-compatible, there is no associated risk.</p>
			<p>Let's add the final migration, <code>V5__Drop_result_column.sql</code>:</p>
			<pre>alter table CALCULATION
drop column RESULT;</pre>
			<p>After this step, we will have finally completed the column renaming procedure. Note that the steps we took complicated the operation a little bit in order to stretch it in time. This reduced the risk of backward-incompatible database changes and allowed for zero-downtime deployments.</p>
			<h4>Separating database updates from code changes</h4>
			<p>So far, in all images, we<a id="_idIndexMarker981"/> showed that database migrations are run with service releases. In other words, each commit (which implies each release) took both database changes and code changes. However, the recommended approach is to make a clear separation that a commit to the repository is either a database update or a code change. This method is presented in the following diagram:</p>
			<div><div><img src="img/B18223_09_04.jpg" alt="Figure 9.4 – Separating database updates and code changes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Separating database updates and code changes</p>
			<p>The benefit of database-service change separation is that we get the backward-compatibility check for free. Imagine that the <strong class="bold">v11</strong> and <strong class="bold">v1.2.7</strong> changes concern one logical change, for example, adding a new column to the database. Then, we first commit <strong class="bold">Database v11</strong>, so the tests in the continuous delivery pipeline check whether <strong class="bold">Database v11</strong> works correctly with <strong class="bold">Service v.1.2.6</strong>. In other words, they check whether the <strong class="bold">Database v11</strong> update is backward-compatible. Then, we commit the <strong class="bold">v1.2.7</strong> change, so the pipeline checks whether <strong class="bold">Database v11</strong> works with <strong class="bold">Service v1.2.7</strong>.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">The database-code separation does not mean that we must have two separate Jenkins pipelines. The pipeline can always execute both, but we should keep it as a good practice that a commit is either a database update or a code change.</p>
			<p>To sum up, the database schema changes should never be done manually. Instead, we should always automate them using a migration tool executed as a part of the continuous delivery pipeline. We should also avoid non-backward-compatible database updates, and the best way to ensure this is to commit the database and code changes into the repository separately.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/>Avoiding a shared database</h2>
			<p>In many <a id="_idIndexMarker982"/>systems, we can spot that the database becomes the central point that is shared between multiple services. In such a case, any update to the database becomes much more challenging, because we need to coordinate it between all services.</p>
			<p>For example, imagine we are developing an online shop, and we have a <code>Customers</code> table that contains the following columns: <code>first name</code>, <code>last name</code>, <code>username</code>, <code>password</code>, <code>email</code>, and <code>discount</code>. There are three services that are interested in the customer's data:</p>
			<ul>
				<li><strong class="bold">Profile manager</strong>: This <a id="_idIndexMarker983"/>enables editing user's data.</li>
				<li><strong class="bold">Checkout processor</strong>: This<a id="_idIndexMarker984"/> processes the checkout (reads username and email).</li>
				<li><strong class="bold">Discount manager</strong>: This <a id="_idIndexMarker985"/>analyzes the customer's orders and applies a suitable discount.</li>
			</ul>
			<p>Let's look at the following diagram that shows this situation:</p>
			<div><div><img src="img/B18223_09_05.jpg" alt="Figure 9.5 – Shared database anti-pattern&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Shared database anti-pattern</p>
			<p>The three <a id="_idIndexMarker986"/>services are dependent on the same database schema. There are at least two issues with such an approach:</p>
			<ul>
				<li>When we want to update the schema, it must be compatible with all three services. While all backward-compatible changes are fine, any non-backward-compatible update becomes far more difficult, or even impossible.</li>
				<li>Each service has a separate delivery cycle and a separate continuous delivery pipeline. So, <em class="italic">which pipeline should we use for the database schema migrations?</em> Unfortunately, there is no good answer to this question.</li>
			</ul>
			<p>For the reasons mentioned previously, each service should have its own database and the services should communicate via their APIs. Using our example, we could apply the following refactoring:</p>
			<ul>
				<li>The checkout processor should communicate with the profile manager's API to fetch the customer's data.</li>
				<li>The discount column should be extracted to a separate database (or schema), and the discount manager should take ownership.</li>
			</ul>
			<p>The refactored version is presented in the following diagram:</p>
			<div><div><img src="img/B18223_09_06.jpg" alt="Figure 9.6 – Database per service pattern&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Database per service pattern</p>
			<p>Such an approach is consistent with the principles of the microservice architecture and should always be applied. Communication over APIs is far more flexible than direct database access.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">In the case of monolithic systems, a database is usually the integration point. Since such an approach causes a lot of issues, it's considered an anti-pattern.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor248"/>Preparing test data</h2>
			<p>We have already presented <a id="_idIndexMarker987"/>database migrations that keep the database schema consistent between the environments as a side effect. This is because if we run the same migration scripts on the development machine, in the staging environment, or in the production, then we would always get the result in the same schema. However, the data values inside the tables differ. How can we prepare the test data so that it would effectively test our system? This will be the focus of the next section.</p>
			<p>The answer to this question depends on the type of test, and it is different for unit testing, integration/acceptance testing, and performance testing. Let's examine each case.</p>
			<h3>Unit testing</h3>
			<p>In the case of unit testing, we don't use the real database. We either mock the test data on the level of the persistence <a id="_idIndexMarker988"/>mechanism (repositories and data access objects) or we fake the real database with an in-memory database (for example, an H2 database). Since unit tests are created by developers, the exact data values are also usually invented by developers and aren't as important.</p>
			<h3>Integration/acceptance testing</h3>
			<p>Integration<a id="_idIndexMarker989"/> and acceptance tests <a id="_idIndexMarker990"/>usually use the test/staging database, which should be as similar to the production as possible. One approach, adopted by many companies, is to snapshot the production data into staging that guarantees that it is exactly the same. This approach, however, is treated as an anti-pattern, for the following reasons:</p>
			<ul>
				<li><strong class="bold">Test isolation</strong>: Each test operates on the same database, so the result of one test may influence the input of the others.</li>
				<li><strong class="bold">Data security</strong>: Production instances usually store sensitive information and are, therefore, better secured.</li>
				<li><strong class="bold">Reproducibility</strong>: After every snapshot, the test data is different, which may result in flaky tests.</li>
			</ul>
			<p>For these reasons, the preferred approach is to manually prepare the test data by selecting a subset of the production data with the customer or the business analyst. When the production database grows, it's worth revisiting its content to see if there are any reasonable cases that should be added.</p>
			<p>The best way to add data to the staging database is to use the public API of a service. This approach is consistent with acceptance tests, which are usually black-box. Furthermore, using the API guarantees that the data itself is consistent and simplifies database refactoring by limiting direct database operations.</p>
			<h3>Performance testing</h3>
			<p>The test data for the performance testing<a id="_idIndexMarker991"/> is usually similar to acceptance testing. One significant difference is the amount of data. In order to test the performance correctly, we need to provide a sufficient volume of input data, as large as is available on the production (during the peak time). For this purpose, we can create data generators, which are usually shared between acceptance and performance tests.</p>
			<p>We have covered a lot about databases in the continuous delivery process. Now, let's move to something completely different. Let's move to the topic of improving our Jenkins pipeline using well-known pipeline patterns.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/>Pipeline patterns</h1>
			<p>We already know everything <a id="_idIndexMarker992"/>necessary to start a project and set up the continuous delivery pipeline with Jenkins, Docker, Kubernetes, Ansible, and Terraform. This section is intended to extend this knowledge with a few of the recommended Jenkins pipeline practices.</p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor250"/>Parallelizing pipelines</h2>
			<p>In this book, we have <a id="_idIndexMarker993"/>always executed the pipeline sequentially, stage by <a id="_idIndexMarker994"/>stage, step by step. This approach makes it easy to reason the state and the result of the build. If there is first the acceptance test stage and then the release stage, it means that the release won't ever happen until the acceptance tests are successful. Sequential pipelines are simple to understand and usually do not cause any surprises. That's why the first method to solve any problem is to do it sequentially.</p>
			<p>However, in some cases, the stages are time-consuming and it's worth running them in parallel. A very good example is performance tests. They usually take a lot of time, so, assuming that they are independent and isolated, it makes sense to run them in parallel. In Jenkins, we can parallelize the pipeline on two different levels:</p>
			<ul>
				<li><strong class="bold">Parallel steps</strong>: Within<a id="_idIndexMarker995"/> one stage, parallel processes run on the same agent. This method is simple because all Jenkins workspace-related files are located on one physical machine. However, as always with vertical scaling, the resources are limited to that single machine.</li>
				<li><code>stash</code> <code>Jenkinsfile</code> keyword) if a file created in the previous stage is needed on the other physical machine.</li>
			</ul>
			<p>Let's see how this looks in practice. If we want to run two steps in parallel, the <code>Jenkinsfile</code> script should look as follows:</p>
			<pre>pipeline {
   agent any
   stages {
       stage('Stage 1') {
           steps {
               parallel (
                       one: { echo "parallel step 1" },
                       two: { echo "parallel step 2" }
               )
           }
       }
       stage('Stage 2') {
           steps {
               echo "run after both parallel steps are completed"   
           }
       }
   }
}</pre>
			<p>In <code>Stage 1</code>, with the use<a id="_idIndexMarker997"/> of the <code>parallel</code> keyword, we execute two parallel <a id="_idIndexMarker998"/>steps, <code>one</code> and <code>two</code>. Note that <code>Stage 2</code> is only executed after both parallel steps are completed. That's why such solutions are perfectly safe to run tests in parallel; we can always be sure that the deployment stage only runs after all parallelized tests have already passed.</p>
			<p>The preceding code sample concerned the parallel steps level. The other solution would be to use parallel stages, and therefore, run each stage on a separate agent machine. The decision on which type of parallelism to use usually depends on two factors:</p>
			<ul>
				<li>How powerful the agent machines are</li>
				<li>How much time the given stage takes</li>
			</ul>
			<p>As a general recommendation, unit tests are fine to run in parallel steps, but performance tests are usually better off on separate machines.</p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor251"/>Reusing pipeline components</h2>
			<p>When the <code>Jenkinsfile</code> script <a id="_idIndexMarker999"/>grows in size and becomes more complex, we <a id="_idIndexMarker1000"/>may want to reuse its parts between similar pipelines.</p>
			<p>For example, we may want to have separate (but similar) pipelines for different environments (development, QA, and production). Another common example in the microservice world is that each service has a very similar <code>Jenkinsfile</code>. Then, how do we write <code>Jenkinsfile</code> scripts so that we don't repeat the same code all over again? There are two good patterns for<a id="_idIndexMarker1001"/> this purpose: parameterized builds, and shared libraries. Let's go over them individually.</p>
			<h3>Build parameters</h3>
			<p>We already<a id="_idIndexMarker1002"/> mentioned in <a href="B18223_04_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 4</em></a>, <em class="italic">Continuous Integration Pipeline</em>, that a pipeline can have input parameters. We can use them to provide different use cases with the same pipeline code. As an example, let's create a pipeline parameterized with the <code>environment</code> type:</p>
			<pre>pipeline {
   agent any
   parameters {
       string(name: 'Environment', defaultValue: 'dev', description: 'Which environment (dev, qa, prod)?')
   }
   stages {
       stage('Environment check') {
           steps {
               echo "Current environment: ${params.Environment}"   
           }
       }
   }
}</pre>
			<p>The build takes one input parameter, <code>Environment</code>. Then, all we do in this step is print the parameter. We can also add a condition to execute different code for different environments.</p>
			<p>With this configuration, when we start the build we will see a prompt for the input parameter, as follows:</p>
			<div><div><img src="img/B18223_09_07.jpg" alt="Figure 9.7 – Jenkins parametrized build&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Jenkins parametrized build</p>
			<p>A parameterized<a id="_idIndexMarker1003"/> build can help us reuse the pipeline code for scenarios that differ just a little bit. However, this feature should not be overused, because too many conditions can make a <code>Jenkinsfile</code> difficult to understand.</p>
			<h3>Shared libraries</h3>
			<p>The other solution to reuse the<a id="_idIndexMarker1004"/> pipeline is to extract its parts into a shared library.</p>
			<p>A shared library is a Groovy code that is stored as a separate, source-controlled project. This code can later be used in many <code>Jenkinsfile</code> scripts as pipeline steps. To make it clear, let's take a look at an example. A shared library technique always requires three steps:</p>
			<ol>
				<li value="1">Create a shared library project.</li>
				<li>Configure the shared library in Jenkins.</li>
				<li>Use the shared library in a <code>Jenkinsfile</code>.</li>
			</ol>
			<h4>Creating a shared library project</h4>
			<p>We start by <a id="_idIndexMarker1005"/>creating a new Git project, in which we put the shared library code. Each Jenkins step is expressed as a Groovy file located in the <code>vars</code> directory.</p>
			<p>Let's create a <code>sayHello</code> step that takes the <code>name</code> parameter and echoes a simple message. This should be stored in the <code>vars/sayHello.groovy</code> file:</p>
			<pre>/**
* Hello world step.
*/
def call(String name) {   
   echo "Hello $name!"
}</pre>
			<p class="callout-heading">Information </p>
			<p class="callout">Human-readable descriptions for shared library steps can be stored in the <code>*.txt</code> files. In our example, we could add the <code>vars/sayHello.txt</code> file with the step documentation.</p>
			<p>When the library code is done, we need to push it to the repository, for example, as a new GitHub project.</p>
			<h4>Configure the shared library in Jenkins</h4>
			<p>The next<a id="_idIndexMarker1006"/> step is to register the shared library in <a id="_idIndexMarker1007"/>Jenkins. We open <strong class="bold">Manage Jenkins</strong> | <strong class="bold">Configure System</strong> and find the <strong class="bold">Global Pipeline Libraries</strong> section. There, we can add the library giving it a chosen name, as follows:</p>
			<div><div><img src="img/B18223_09_08.jpg" alt="Figure 9.8 – Jenkins global pipeline library configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Jenkins global pipeline library configuration</p>
			<p>We specified the <a id="_idIndexMarker1008"/>name under which the library is registered and the<a id="_idIndexMarker1009"/> library repository address. Note that the latest version of the library will automatically be downloaded during the pipeline build.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">We showed importing the Groovy code as a <em class="italic">Global Shared Library</em>, but there are also other solutions. Read more at <a href="https://www.jenkins.io/doc/book/pipeline/shared-libraries/">https://www.jenkins.io/doc/book/pipeline/shared-libraries/</a>.</p>
			<h4>Using the shared library in a Jenkinsfile</h4>
			<p>Finally, we<a id="_idIndexMarker1010"/> can use the<a id="_idIndexMarker1011"/> shared library in a <code>Jenkinsfile</code>:</p>
			<pre>pipeline {
   agent any
   stages {
       stage("Hello stage") {
           steps {
           sayHello 'Rafal'
         }
       }
   }
}</pre>
			<p class="callout-heading">Tip</p>
			<p class="callout">If <code>@Library('example') _</code> at the beginning of the <code>Jenkinsfile</code> script.</p>
			<p>As you can see, we can use the Groovy code as a <code>sayHello</code> pipeline step. Obviously, after the pipeline build completes, we should see <code>Hello Rafal!</code> in the console output.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Shared libraries are not limited to one step. Actually, with the power of the Groovy language, they can even act as templates for entire Jenkins pipelines.</p>
			<p>After describing how to share the Jenkins pipeline code, let's also write a few words on rolling back deployments during the continuous delivery process.</p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor252"/>Rolling back deployments</h2>
			<p>I remember the words of my <a id="_idIndexMarker1012"/>colleague, a senior architect—<em class="italic">You don't need more QAs, you need a faster rollback</em>. While this statement is oversimplified<a id="_idIndexMarker1013"/> and the QA team is often of great value, there is a lot of truth in this sentence. Think about it; if you introduce a bug in the production but roll it back soon after the first user reports an error, then usually, nothing bad happens. On the other hand, if production errors are rare but no rollback is applied, then the process to debug the production usually ends in long, sleepless nights and some dissatisfied users. That's why we need to think about the rollback strategy upfront while creating the Jenkins pipeline.</p>
			<p>In the context of continuous delivery, there are two moments when the failure can happen:</p>
			<ul>
				<li>During the release process, in the pipeline execution</li>
				<li>After the pipeline build is completed, in production</li>
			</ul>
			<p>The first scenario is pretty simple and harmless. It concerns a case when the application is already deployed to production but the next stage fails, for example, the smoke test. Then, all we need to do is execute a script in the <code>post</code> pipeline section for the <code>failure</code> case, which downgrades the production service to the older Docker image version. If we use blue-green deployment (as we will describe later in this chapter), the risk of any downtime is minimal, since we usually execute the load-balancer switch as the last pipeline stage after the smoke test.</p>
			<p>The second scenario, in which we notice a production bug after the pipeline is successfully completed, is more difficult and requires a few words of comment. Here, the rule is that we should always release the rolled-back service using exactly the same process as the standard release. Otherwise, if we try to do something manually in a faster way, we are asking for trouble. Any non-repetitive task is risky, especially under stress when production is out of order.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">As a side note, if the pipeline completes successfully but there is a production bug, then it means that our tests are not good enough. So, the first thing after the rollback is to extend the unit/acceptance test suites with the corresponding scenarios.</p>
			<p>The most common continuous delivery process is a single, fully automated pipeline that starts by checking out the code and ends with release to the production.</p>
			<p>The following diagram <a id="_idIndexMarker1014"/>shows how this works:</p>
			<div><div><img src="img/B18223_09_09.jpg" alt="Figure 9.9 – Continuous delivery pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Continuous delivery pipeline</p>
			<p>We already presented the <a id="_idIndexMarker1015"/>classic continuous delivery pipeline in this book. If the rollback should use exactly the same process, then all we need to do is revert the latest code change from the repository. As a result, the pipeline automatically builds, tests, and finally, releases the right version.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Repository reverts and emergency fixes should never skip the testing stages in the pipeline, otherwise, we may end up with a release that is still not working correctly due to another issue that makes debugging even harder.</p>
			<p>The solution is very simple and elegant. The only drawback is the downtime that we need to spend on the complete pipeline build. This downtime can be avoided if we use blue-green deployment or canary releases, in which cases, we only change the load balancer setting to address the healthy environment.</p>
			<p>The rollback operation becomes far more complex in the case of orchestrated releases, during which many services are deployed at the same time. This is one of the reasons why orchestrated releases are treated as an anti-pattern, especially in the microservice world. The correct approach is to always maintain backward compatibility, at least for a time (as we showed for the database at the beginning of this chapter). Then, it's possible to release each service independently.</p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor253"/>Adding manual steps</h2>
			<p>In general, the continuous <a id="_idIndexMarker1016"/>delivery pipelines should be fully automated, triggered by a commit to the repository, and end after the release. Sometimes, however, we can't avoid having manual steps. The most common example is the release approval, which means that the process is fully automated, but there is a manual step to approve the new release. Another common example is manual tests. Some of them may exist because we operate on a legacy system; some others may occur when a test simply cannot be automated. No matter what the reason is, sometimes, there is no choice but to add a manual step.</p>
			<p>Jenkins syntax offers an <code>input</code> keyword for manual steps:</p>
			<pre>stage("Release approval") {
   steps {
       input "Do you approve the release?"
   }
}</pre>
			<p>The pipeline will stop execution on the <code>input</code> step and wait until it's manually approved.</p>
			<p>Remember that manual steps quickly become a bottleneck in the delivery process, and this is why they should always be treated as a solution that is inferior to complete automation.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It is sometimes useful to set a timeout for the input to avoid waiting forever for the manual interaction. After the configured time is elapsed, the whole pipeline is aborted.</p>
			<p>We have covered a lot of important pipeline patterns; now, let's focus on different deployment release patterns.</p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor254"/>Release patterns</h1>
			<p>In the last section, we discussed<a id="_idIndexMarker1017"/> the Jenkins pipeline patterns used to speed up the build execution (parallel steps), help with the code reuse (shared libraries), limit the risk of production bugs (rollback), and deal with manual approvals (manual steps). This section will focus on the next group of patterns; this time, related to the release process. They are designed to reduce the risk of updating the production to a new software version.</p>
			<p>We already described one of the release patterns, rolling updates, in <a href="B18223_06_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a>, <em class="italic">Clustering with Kubernetes</em>. Here, we will present two more: blue-green deployment and canary releases.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">A very convenient way to use the release patterns in Kubernetes is to use the Istio service mesh. Read more at <a href="https://istio.io/">https://istio.io/</a>.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor255"/>Blue-green deployment</h2>
			<p>Blue-green deployment <a id="_idIndexMarker1018"/>is a technique to reduce the downtime associated <a id="_idIndexMarker1019"/>with the release. It concerns having two identical production environments—one called <strong class="bold">green</strong>, the other called <strong class="bold">blue</strong>—as presented in the following diagram:</p>
			<div><div><img src="img/B18223_09_10.jpg" alt="Figure 9.10 – Blue-green deployment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Blue-green deployment</p>
			<p>In the figure, the currently accessible environment is blue. If we want to make a new release, then we deploy everything to the green environment and, at the end of the release process, change the load balancer to the green environment. As a result, the user suddenly starts using the new version. The next time we want to make a release, we make changes to the <a id="_idIndexMarker1020"/>blue environment and, in the end, we change the load <a id="_idIndexMarker1021"/>balancer to blue. We proceed the same every time, switching from one environment to another.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">The blue-green deployment technique works correctly with two assumptions: environmental isolation and no orchestrated releases.</p>
			<p>This solution provides the following benefits:</p>
			<ul>
				<li><strong class="bold">Zero downtime</strong>: All the downtime, from the user perspective, is a moment of changing the load balance switch, which is negligible.</li>
				<li><strong class="bold">Rollback</strong>: In order to roll back one version, it's enough to change back the load balance switch.</li>
			</ul>
			<p>Note that the blue-green deployment must include the following:</p>
			<ul>
				<li><strong class="bold">Database</strong>: Schema migrations can be tricky in case of a rollback, so it's worth using the patterns discussed at the beginning of this chapter.</li>
				<li><strong class="bold">Transactions</strong>: Running database transactions must be handed over to the new database.</li>
				<li><strong class="bold">Redundant infrastructure/resources</strong>: We need to have double the resources.</li>
			</ul>
			<p>There are techniques <a id="_idIndexMarker1022"/>and tools to overcome these challenges, so the blue-green deployment pattern is highly recommended and is widely used in the IT industry.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">You can read more about the <a id="_idIndexMarker1023"/>blue-green deployment technique on the excellent blog from Martin Fowler, at <a href="https://martinfowler.com/bliki/BlueGreenDeployment.html">https://martinfowler.com/bliki/BlueGreenDeployment.html</a>.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor256"/>Canary release</h2>
			<p>Canary release<a id="_idIndexMarker1024"/> is a technique to reduce the risk associated with introducing a<a id="_idIndexMarker1025"/> new version of the software. Similar to blue-green deployment, it uses two identical environments, as presented in the following diagram:</p>
			<div><div><img src="img/B18223_09_11.jpg" alt="Figure 9.11 – Canary release&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – Canary release</p>
			<p>Also, similar to the <a id="_idIndexMarker1026"/>blue-green deployment technique, the release process starts by deploying a new version in the environment that is currently unused. Here, however, the similarities end. The load balancer, instead of switching to the new environment, is set <a id="_idIndexMarker1027"/>to link only a selected group of users to the new environment. The rest still use the old version. This way, a new version can be tested by some users and, in case of a bug, only a small group will be affected. After the testing period, all users are switched to the new version.</p>
			<p>This approach has some great benefits:</p>
			<ul>
				<li><strong class="bold">Acceptance and performance testing</strong>: If the acceptance and performance testing are difficult to run in the staging environment, then it's possible to test them in production, minimizing<a id="_idIndexMarker1028"/> the impact on a small group of users.</li>
				<li><strong class="bold">Simple rollback</strong>: If a new change causes a failure, then rolling back is done by switching all users back to the old version.</li>
				<li><strong class="bold">A/B testing</strong>: If we are not sure whether the new version is better from the UX or the performance perspective, then it's possible to compare it with the old version.</li>
			</ul>
			<p>Canary release <a id="_idIndexMarker1029"/>shares the same drawbacks as the blue-green deployment. The additional challenge is that we have two production systems running at the same time. Nevertheless, canary release is an excellent technique used in most companies to help with the release and testing.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">You can read more about the<a id="_idIndexMarker1030"/> canary release technique on Martin Fowler's blog, at <a href="https://martinfowler.com/bliki/CanaryRelease.html">https://martinfowler.com/bliki/CanaryRelease.html</a>.</p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor257"/>Working with legacy systems</h1>
			<p>Everything we have<a id="_idIndexMarker1031"/> described so far applies to greenfield projects, for which setting up a continuous delivery pipeline is relatively simple.</p>
			<p>Legacy systems are, however, far more challenging, because they usually depend on manual tests and manual deployment steps. In this section, we will walk through the recommended scenario to incrementally apply continuous delivery to a legacy system.</p>
			<p>As the first step, I recommend reading a great book by Michael Feathers, <em class="italic">Working Effectively with Legacy Code</em>. His ideas on how to deal with testing, refactoring, and adding new features address most of the concerns about how to automate the delivery process for legacy systems.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">For many developers, it may be tempting to completely rewrite a legacy system rather than refactor it. While the idea is interesting from a developer's perspective, it is usually a bad business decision that results in a product failure. You can read more about the history of rewriting the Netscape browser in a brilliant blog post by Joel Spolsky, <em class="italic">Things You Should Never Do,</em> at <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i">https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i</a>.</p>
			<p>The way to apply the<a id="_idIndexMarker1032"/> continuous delivery process depends a lot on the current project's automation, the technology used, the hardware infrastructure, and the current release process. Usually, it can be split into three steps:</p>
			<ol>
				<li value="1">Automating build and deployment</li>
				<li>Automating tests</li>
				<li>Refactoring and introducing new features</li>
			</ol>
			<p>Let's look at these in detail.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor258"/>Automating build and deployment</h2>
			<p>The first step includes automating the deployment process. The good news is that in most legacy systems that I <a id="_idIndexMarker1033"/>have worked with, there <a id="_idIndexMarker1034"/>was already some automation in place (for example, in the form of shell scripts).</p>
			<p>In any case, the activities for automated deployment include the following:</p>
			<ol>
				<li value="1"><strong class="bold">Build and package</strong>: Some <a id="_idIndexMarker1035"/>automation usually already exists, in the form of Makefile, Ant, Maven, or any other build tool configuration, or a custom script.</li>
				<li><strong class="bold">Database migration</strong>: We need<a id="_idIndexMarker1036"/> to start incrementally managing the database schema. This requires putting the current schema as an initial migration and making all the further changes with tools such as Flyway or Liquibase, as already described in this chapter.</li>
				<li><strong class="bold">Deployment</strong>: Even if the deployment<a id="_idIndexMarker1037"/> process is fully manual, then there is usually a text/wiki page description that needs to be converted into an automated script.</li>
				<li><strong class="bold">Repeatable configuration</strong>: In<a id="_idIndexMarker1038"/> legacy systems, configuration files are usually changed manually. We need to extract the configuration and use a configuration management tool, as described in <a href="B18223_07_ePub.xhtml#_idTextAnchor185"><em class="italic">Chapter 7</em></a>, <em class="italic">Configuration Management with Ansible</em>.</li>
			</ol>
			<p>After the preceding steps, we can put everything into a deployment pipeline and use it as an automated phase after a manual <strong class="bold">user acceptance testing</strong> (<strong class="bold">UAT</strong>) cycle.</p>
			<p>From the process perspective, it's already worth starting to release more often. For example, if the release is yearly, try to do it quarterly, then monthly. The push for that factor will later result in faster-automated delivery adoption.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor259"/>Automating tests</h2>
			<p>The next step, usually much more<a id="_idIndexMarker1039"/> difficult, is to prepare the automated tests for the system. It requires communicating with the QA team in order to understand how they currently test the software so that we can move everything into an automated acceptance test suite. This phase requires two steps:</p>
			<ol>
				<li value="1"><strong class="bold">Acceptance/sanity test suite</strong>: We need to add automated tests that replace some of the regression activities of the QA team. Depending on the system, they can be provided as a black-box Selenium test or a Cucumber test.</li>
				<li><strong class="bold">(Virtual) test environments</strong>: At this point, we should already be thinking of the environments in which our tests would run. Usually, the best solution to save resources and limit the number of machines required is to virtualize the testing environment using Vagrant or Docker.</li>
			</ol>
			<p>The ultimate goal is to have an automated acceptance test suite that will replace the whole UAT phase from the development cycle. Nevertheless, we can start with a sanity test that will check if the system is correct, from the regression perspective.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">While adding test scenarios, remember that the test suite should execute in a reasonable time. For sanity tests, it is usually less than 10 minutes.</p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor260"/>Refactoring and introducing new features</h2>
			<p>When we have the fundamental regression testing suite (at a minimum), we are ready to add new features and refactor the old code. It's always better to do it in small pieces step by step, because refactoring<a id="_idIndexMarker1040"/> everything at once usually ends up in chaos, and that leads to production failures (not related to any particular change).</p>
			<p>This phase usually includes the following activities:</p>
			<ul>
				<li><strong class="bold">Refactoring</strong>: The best place to start refactoring the old code is where the new features are expected. Starting this way prepares us for the new feature requests yet to come.</li>
				<li><strong class="bold">Rewrite</strong>: If we plan to rewrite parts of the old code, we should start from the code that is the most difficult to test. This way, we can constantly increase the code coverage in our project.</li>
				<li><strong class="bold">Introducing new features</strong>: During the new feature implementation, it's worth using<a id="_idIndexMarker1041"/> the <strong class="bold">feature toggle</strong> pattern. Then, if anything bad happens, we can quickly turn off the new feature. The same pattern should also be used during refactoring.<p class="callout-heading">Information </p><p class="callout">For this phase, it's worth reading a very good book by Martin Fowler, <em class="italic">Refactoring: Improving the Design of Existing Code</em>.</p></li>
			</ul>
			<p>While touching on the old code, it's good to follow the rule to always add a passing unit test first, and only then change the code. With this approach, we can rely on automation to check that we don't accidentally change the business logic.</p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor261"/>Understanding the human element</h2>
			<p>While introducing the automated delivery process to a legacy system, you may feel the human factor more than anywhere else. In order to automate the build process, we need to communicate well with the operations team, and they must be willing to share their knowledge. The same story applies to the manual QA team; they need to be involved in writing automated tests because only they know how to test the software. If you think about it, both the operations and QA teams need to contribute to the project that will later automate their work. At some point, they may realize that their future in the company is not stable and become less helpful. Many companies struggle with introducing the continuous delivery process because teams do not want to get involved enough.</p>
			<p>In this section, we discussed how to approach legacy systems and the challenges they pose. If you are in the process of converting your project and organization to the continuous delivery approach, then you may want to take a look at the Continuous Delivery Maturity Model, which aims to give some structure to the process of adopting automated delivery.</p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor262"/>Summary</h1>
			<p>This chapter has been a mixture of various continuous delivery aspects that were not previously covered. The key takeaways from the chapter are as follows:</p>
			<ul>
				<li>Databases are an essential part of most applications, and should, therefore, be included in the continuous delivery process.</li>
				<li>Database schema changes are stored in the version control system and managed by database migration tools.</li>
				<li>There are two types of database schema changes: backward-compatible and backward-incompatible. While the first type is simple, the second requires a bit of overhead (split to multiple migrations spread over time).</li>
				<li>A database should not be the central point of the whole system. The preferred solution is to provide each service with its own database.</li>
				<li>The delivery process should always be prepared for a rollback scenario.</li>
				<li>Three release patterns should always be considered: rolling updates, blue-green deployment, and canary release.</li>
				<li>Legacy systems can be converted to the continuous delivery process in small steps, rather than all at once.</li>
			</ul>
			<p>Next, for the last part of the book, we will look into the best practices for your continuous delivery process.</p>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor263"/>Exercises</h1>
			<p>In this chapter, we covered various aspects of the continuous delivery process. Since practice makes perfect, we recommend the following exercises:</p>
			<ol>
				<li value="1">Use Flyway to create a non-backward-compatible change in the MySQL database:<ol><li>Use the official Docker image, <code>mysql</code>, to start the database.</li><li>Configure Flyway with a proper database address, username, and password.</li><li>Create an initial migration that creates a <code>USERS</code> table with three columns: <code>ID</code>, <code>EMAIL</code>, and <code>PASSWORD</code>.</li><li>Add sample data to the table.</li><li>Change the <code>PASSWORD</code> column to <code>HASHED_PASSWORD</code>, which will store the hashed passwords.</li><li>Split the non-backward-compatible change into three migrations, as described in this chapter.</li><li>You can use <code>MD5</code> or <code>SHA</code> for hashing.</li><li>Check that the database doesn't store any passwords in plain text as a result.</li></ol></li>
				<li>Create a Jenkins shared library with steps to build and unit test Gradle projects:<ol><li>Create a separate repository for the library.</li><li>Create two files in the library: <code>gradleBuild.groovy</code> and <code>gradleTest.groovy</code>.</li><li>Write the appropriate <code>call</code> methods.</li><li>Add the library to Jenkins.</li><li>Use the steps from the library in a pipeline.</li></ol></li>
			</ol>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor264"/>Questions</h1>
			<p>To verify the knowledge from this chapter, please answer the following questions:</p>
			<ol>
				<li value="1">What are database (schema) migrations?</li>
				<li>Can you name at least three database migration tools?</li>
				<li>What are the main two types of changes to the database schema?</li>
				<li>Why should one database not be shared between multiple services?</li>
				<li>What is the difference between the test data for unit tests and integration/acceptance tests?</li>
				<li>What Jenkins pipeline keyword do you use to make the steps run in parallel?</li>
				<li>What are different methods to reuse Jenkins pipeline components?</li>
				<li>What Jenkins pipeline keyword do you use to make a manual step?</li>
				<li>What are the three release patterns mentioned in this chapter?</li>
			</ol>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor265"/>Further reading</h1>
			<p>To read more about the advanced aspects of the continuous delivery process, please refer to the following resources:</p>
			<ul>
				<li><em class="italic">Databases as a Challenge for Continuous Delivery</em>: <a href="https://phauer.com/2015/databases-challenge-continuous-delivery/">https://phauer.com/2015/databases-challenge-continuous-delivery/</a>.</li>
				<li><em class="italic">Zero Downtime Deployment with a Database</em>: <a href="https://spring.io/blog/2016/05/31/zero-downtime-deployment-with-a-database">https://spring.io/blog/2016/05/31/zero-downtime-deployment-with-a-database</a>.</li>
				<li><em class="italic">Canary Release</em>: <a href="https://martinfowler.com/bliki/CanaryRelease.html">https://martinfowler.com/bliki/CanaryRelease.html</a>.</li>
				<li><em class="italic">Blue-Green Deployment</em>: <a href="https://martinfowler.com/bliki/BlueGreenDeployment.html">https://martinfowler.com/bliki/BlueGreenDeployment.html</a>.</li>
			</ul>
		</div>
	</body></html>