<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift HA Design for Single and Multiple DCs</h1>
                
            
            <article>
                
<p class="calibre2"> In the previous chapter, we briefly touched upon OpenShift HA and <strong class="calibre4">h</strong><span class="calibre11"><strong class="calibre4">igh availability</strong> (<strong class="calibre4">HA</strong>) in general</span>. We discussed how OpenShift provides redundancy in case of a failure and how you can prevent this from happening by designing your OpenShift cluster properly. Finally, we finished the chapter with backup and restore methods and procedures in OpenShift.  </p>
<p class="calibre2"><span class="calibre11">In this chapter,</span> we are going to talk about OpenShift scenarios in single and multiple data centers. This chapter will also explain how to properly design OpenShift in a<span class="calibre11"> </span>distributed<span class="calibre11"> and redundant </span>configuration across one or more data centers.</p>
<p class="calibre2">After reading this chapter, you will have an understanding of the following topics:</p>
<ul class="calibre9">
<li class="calibre10">OpenShift<span> </span>single-DC<span> </span>HA design</li>
<li class="calibre10">OpenShift<span> </span>multi-DC<span> </span>HA d<span>esign</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift single-DC HA design</h1>
                
            
            <article>
                
<p class="calibre2">In the previous chapter, we briefly covered HA and OpenShift HA in general, but we did'nt discuss how to practically design OpenShift in your data center environment.</p>
<p class="cdpaligncenter"><img class="alignnone71" src="../images/00095.jpeg"/></p>
<p class="calibre2">Let's recall what main OpenShift components we have and how they provide redundancy:</p>
<ul class="calibre9">
<li class="calibre10">Openshift infrastructure nodes </li>
<li class="calibre10">OpenShift master nodes </li>
<li class="calibre10">OpenShift nodes </li>
<li class="calibre10">Etcd key-value store</li>
<li class="calibre10">Persistent storage</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift infrastructure nodes</h1>
                
            
            <article>
                
<p class="calibre2">OpenShift infrastructure nodes are essential components that provide access from the outside of an OpenShift cluster. <span class="calibre11">OpenShift infrastructure nodes scale horizontally, which means that we can add as many nodes as we need in order to add network throughput. If you recall from the previous chapter, we need to consider which VIP method to use. We have two main VIP methods: </span></p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">VIP using an external load balancer</strong>:</li>
</ul>
<p class="cdpaligncenter"><img class="alignnone72" src="../images/00096.jpeg"/></p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">IP failover using keepalived</strong>:</li>
</ul>
<p class="cdpaligncenter"><img class="alignnone73" src="../images/00097.jpeg"/></p>
<p class="calibre2">Both methods have their own pros and cons, but the one that provides better scalability and smooth migration to multi-DC design is the VIP using an external load balancer method. This method allows you to dynamically distribute the load among all the infra nodes and add them dynamically without any interruption. If you are going to distribute the load across multiple data centers, the virtual IP with external load balancers method allows you to implement these changes with minimum downtime. We are going to discuss multi-DC design later in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift masters</h1>
                
            
            <article>
                
<p class="calibre2">Similar to OpenShift infrastructure nodes, OpenShift masters require redundancy and high availability. Redundancy is easily achieved by the number of horizontally scalable master nodes, and high availability is achieved by one of the VIP methods that we discussed previously. For the same reason, using an external load balancer is a way better and scalable solution compared to the keepalived and DNS method.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift nodes</h1>
                
            
            <article>
                
<p class="calibre2">OpenShift nodes do not have any specific HA requirements since they are running stateless pods in a redundant fashion. The only HA and redundancy consideration is to have enough OpenShift nodes to handle additional workload when a failure happens in your data center, whether a server, a rack, or a whole row of racks goes down. You need to distribute the workload and make sure that no matter what fails, you have redundant components and pods up and running.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Etcd key-value store</h1>
                
            
            <article>
                
<p class="calibre2">OpenShift etcd is a highly distributed key-value store where all critical OpenShift cluster-related data is kept. Etcd works in active/active configuration by default, which means it provides both redundancy and high availability by default. There is a general recommendation to install and configure your etcd cluster on dedicated nodes separately from OpenShift masters in a quantity of three, five, or seven members.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Persistent storage</h1>
                
            
            <article>
                
<p class="calibre2">External storage configuration <span class="calibre11">and design </span>for <span class="calibre11">OpenShift persistent data</span> is out of the scope of this book, but general advice is to make sure that you have your external storage available in a redundant and scalable fashion, <span class="calibre11">meaning that if one or several components fails, it does not affect overall storage performance and is always accessible by OpenShift. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Physical placement consideration</h1>
                
            
            <article>
                
<p class="calibre2">Considering that our OpenShift cluster is going to be up and running within a single data center, we need to take extra caution and make sure that we follow some simple rules: </p>
<ul class="calibre9">
<li class="calibre10">The same OpenShift components need to be connected to different switches and power circuits, and located in different racks or server rooms if possible</li>
<li class="calibre10">All hardware should be running on OpenShift components connected to physical networking using interface teaming with LACP and MC-LAG on networking switches</li>
<li class="calibre10">An external storage cluster for persistent data should follow the same rules and should be connected to different <span><span>switches and power circuits, and located in different racks or server rooms if possible</span></span></li>
<li class="calibre10">Use different load balancer clusters that work independently from each other and do not form a single point of failure</li>
<li class="calibre10">If you want to provide additional reliability for an OpenShift solution, you can also use the<span> server hardware RAID for OpenShift OS</span><span>, ECC-enabled RAM, multiple networking cards, dual-socket motherboards, and SSD disks</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Design considerations</h1>
                
            
            <article>
                
<p class="calibre2">Considering a design like this, you have to ask yourself several questions:</p>
<ul class="calibre9">
<li class="calibre10">What happens if any critical component (OpenStack, network, or storage) goes down?</li>
<li class="calibre10">What do I do if the OpenShift cluster upgrades?</li>
<li class="calibre10">What do I do if external storage for OpenShift persistent data becomes unavailable?</li>
<li class="calibre10">How much time does it take to recover an OpenShift cluster if the whole thing goes down?</li>
</ul>
<p class="calibre2">There are other questions, of course, that you have to ask yourself, but if you are able to answer these questions without any hesitation, then you are on the right track.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">OpenShift multi-DC HA design</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre11">OpenShift multi-DC is one of the most difficult topics when it comes to OpenShift design and implementation in a scalable and distributed environment. This happens mainly because there are not that many deployments and best practices developed around this topic. It may be relatively easy to deploy an OpenShift cluster in a single data center environment, but when it comes to a multi-DC design, this is where things will get complicated. The reason is that now we have to consider all OpenShift and adjacent components, like networking and storage, to be scalable and highly available across multiple data centers as well. </span>There are two main HA strategies for a design that involves more than one data center:</p>
<ul class="calibre9">
<li class="calibre10">Single OpenShift cluster across all data centers (for example, one cluster per three DCs)</li>
<li class="calibre10">One OpenShift cluster per data center <span>(for example, three clusters per three DCs)</span></li>
</ul>
<p class="calibre2">For all of these strategies, we need to use active/active scenarios because an active/passive scenario is a waste of resources and money. And although there are still many companies utilizing active/passive scenarios, they usually have plans to migrate to active/active <span class="calibre11">scenarios.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">One OpenShift cluster across all data centers</h1>
                
            
            <article>
                
<p class="calibre2">This particular design option is the most natural and the easiest to operate, but the most dangerous among all the other options. One data center environment brings one set of problems, and if you add another data center, it will give you twice as many problems. If you have an unreliable data center interconnect link, it will add up alongside the failure risks as well. Some of you may disagree, but this is what usually happens if you do not plan and design your solution properly:</p>
<p class="cdpaligncenter"><img class="alignnone74" src="../images/00098.jpeg"/></p>
<p class="calibre2">Considering a design like the preceding, you will have the following challenges:</p>
<ul class="calibre9">
<li class="calibre10">How do I load balance the traffic across all of these data centers from the internet?</li>
<li class="calibre10">How do I distribute each and every OpenShift component across these data centers so that, in case of a failure, OpenShift is still able to handle all the load? </li>
<li class="calibre10">How does my storage solution work across all three data centers?</li>
<li class="calibre10">Do I need to extend <span>the same network subnet</span> across all data centers?</li>
<li class="calibre10">How do I solve asymmetric routing problems?</li>
<li class="calibre10">What happens with the OpenShift cluster if there is a split brain scenario?</li>
<li class="calibre10">What happens if any critical component (OpenStack, network, or storage) goes down in one of the data centers?</li>
<li class="calibre10">How much time does it take if the whole OpenShift cluster goes down?</li>
<li class="calibre10">How do I scale out this solution, for example, if a new data center comes out?</li>
<li class="calibre10">What do I do if an OpenShift cluster upgrades?</li>
<li class="calibre10">What do I do if external storage for OpenShift persistent data becomes unavailable? </li>
</ul>
<p class="calibre2">So, as you can see, having just a single OpenShift cluster across all your data centers adds a lot more questions and problems compared to a single cluster in one data center. This solution has one main benefit<span class="calibre11">—</span>having one single OpenShift cluster is easier to operate. But you need to ask yourself whether you are building a solution that will be easy to operate or a reliable and stable solution that is going to be up and running during the most difficult and even catastrophic events. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">One OpenShift cluster per data center</h1>
                
            
            <article>
                
<p class="calibre2">While the previous solution has a lot of disadvantages, there is another, not so popular, but very stable, predictable, and scalable solution. That solution is where you have as many clusters as there are data centers:</p>
<p class="cdpaligncenter"><img class="alignnone75" src="../images/00099.jpeg"/></p>
<p class="calibre2">The main benefit of this solution is that all of your OpenShift clusters are going to be independent of each other and won't affect other OpenShift clusters if something goes wrong. You will still have challenges, but they are a bit different from single Openshift clusters across all DCs and easier to solve. Besides, this solution scales much better than the other ones. However, you should answer these questions before implementing this solution:    </p>
<ul class="calibre9">
<li class="calibre10">How do I load balance the traffic across all of these data centers from the internet?</li>
<li class="calibre10">What happens with the OpenShift cluster if there is a split brain scenario?</li>
<li class="calibre10">How does my storage data work across all three data centers?</li>
<li class="calibre10">Do I need database replication across all the data center? If yes, how will it work?</li>
<li class="calibre10">What happens if any critical component (OpenStack, network, or storage) goes down in one of the data centers?</li>
<li class="calibre10">How much time does it take if the whole OpenShift cluster goes down?</li>
<li class="calibre10">How do I scale out this solution, for example, if a new data center comes out?</li>
<li class="calibre10">What do I do regarding OpenShift cluster upgrades?</li>
<li class="calibre10">What do I do if external storage for OpenShift persistent data becomes unavailable? </li>
</ul>
<p class="calibre2">Following is a comparison table to consolidate the main differences in all <strong class="calibre4">OpenShift Container Platform</strong> (<strong class="calibre4">OCP</strong>) HA solutions that we have just discussed:</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">Name</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4">1xOCP-1xDC</strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4"><span class="calibre11">1xOCP-3xDC</span></strong></p>
</td>
<td class="calibre25">
<p class="calibre2"><strong class="calibre4"><span class="calibre11">3xOCP-3xDC</span></strong></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">DC redundancy</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Yes</span></p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Yes</span></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Intercluster redundancy</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Intercluster storage isolation</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">No</span></p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Scalability</p>
</td>
<td class="calibre25">
<p class="calibre2">Limited</p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Limited</span></p>
</td>
<td class="calibre25">
<p class="calibre2">Unlimited</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Solution implementation</p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
<td class="calibre25">
<p class="calibre2">Moderate</p>
</td>
<td class="calibre25">
<p class="calibre2">Hard</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Operations</p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Easy</span></p>
</td>
<td class="calibre25">
<p class="calibre2">Moderate</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Troubleshooting</p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
<td class="calibre25">
<p class="calibre2">Hard</p>
</td>
<td class="calibre25">
<p class="calibre2">Moderate</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Cluster seamless<br class="calibre5"/>
Upgrade and recovery</p>
</td>
<td class="calibre25">
<p class="calibre2">Moderate</p>
</td>
<td class="calibre25">
<p class="calibre2">Hard</p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2">Application development </p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Easy</span></p>
</td>
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Easy</span></p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Application deployment </span></p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
<td class="calibre25">
<p class="calibre2">Easy</p>
</td>
<td class="calibre25">
<p class="calibre2">Moderate </p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre2"><span class="calibre11">Requires external custom tools</span></p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
<td class="calibre25">
<p class="calibre2">No</p>
</td>
<td class="calibre25">
<p class="calibre2">Yes</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">As you can see, each and every HA solution has their own pros and cons:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">1xOCP-1xDC</strong>: This is easiest to implement, operate, and troubleshoot, but suffers from data center or OpenShift cluster failures and has limited scalability. </li>
<li class="calibre10"><strong class="calibre1">1xOCP-3xDC</strong>: In addition to all the benefits of the previous solution, it has better redundancy but adds a lot of troubleshooting effort if something goes wrong. This solution is also difficult to perform seamless upgrades and recoveries on.</li>
<li class="calibre10"><strong class="calibre1">3xOCP-3xDC</strong>: This is a next level solution architecture that is much harder to implement, operate, and troubleshoot, but is the most stable, scalable solution ever. This solution requires extensive experience and expertise, but ensures that you will always have your application up and running. </li>
</ul>
<p class="calibre2">In order to successfully implement the last solution where we have one OpenShift cluster per data center, we are required to take a closer look at the following topics:</p>
<ul class="calibre9">
<li class="calibre10">Networking</li>
<li class="calibre10">Storage</li>
<li class="calibre10">Application deployment</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Networking</h1>
                
            
            <article>
                
<p class="calibre2">The main problem in networking when you are building a solution like this is how to properly load balance the traffic, and when failure happens, how to re-route the traffic to other OpenShift clusters. The main technologies we can use here are the following:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Anycast </strong><span><strong class="calibre1">IP addresses</strong>: In order to effectively load balance the traffic across all our data centers, we can use an anycast IP address. This will help not only load balance the traffic but also provide IP failover if an application becomes unavailable in one of the data centers.</span></li>
<li class="calibre10"><strong class="calibre1">Application health checks</strong>: Application health checks are a must have in this solution. They will help you to identify the failure and reroute the traffic to other data centers.</li>
<li class="calibre10"><strong class="calibre1">Dynamic routing protocols</strong>: <span>Make sure that you have IGP/BGP connectivity between load balancers and network HW. When failure happens, IGP/BGP will withdraw its IP anycast address so that the traffic goes to other data centers. </span></li>
<li class="calibre10"><strong class="calibre1">SSL Offloading</strong>: <span>Depending on the implementation, you might need to configure SSL offloading on load balancers. This will take traffic decryption off the OpenShift cluster:</span></li>
</ul>
<p class="cdpaligncenter"><img class="alignnone76" src="../images/00100.jpeg"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Storage</h1>
                
            
            <article>
                
<p class="calibre2">Storage has always from a problem when it comes to geographically distributed application deployment across several platforms. Even major cloud providers have not solved it yet. This is the prime reason why application instances must be able to work independently from each other and be stateless. However, we can suggest a few architectures that will point you in the right direction and help solve the problem:</p>
<p class="calibre2"><strong class="calibre4">Storage geo-replication</strong>:</p>
<p class="cdpaligncenter"><img class="alignnone77" src="../images/00101.jpeg"/></p>
<p class="calibre2">You can set up Multi-DC storage replication so that you have data consistency across all data centers. One example of this is GlusterFS geo-replication, which supports different scenarios that should suit your case. As we discussed earlier in this book, GlusterFS is a perfect match for OpenShift as a persistent storage.</p>
<p class="calibre2"><strong class="calibre4">Database geo-replication</strong>:</p>
<p class="cdpaligncenter"><img class="alignnone78" src="../images/00102.jpeg"/></p>
<p class="calibre2">In most cases, the only stateful information you have in OpenShift will be kept in databases. Modern databases support multi-site, multi-DC, and multi-region replication architectures, such as Cassandra, MongoDB, Kafka, and Hazelcast.</p>
<div class="packt_infobox">You will still need to take care of backup and restore procedures if the database gets corrupted. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Application deployment</h1>
                
            
            <article>
                
<p class="calibre2">Once you are done with network and storage designs, the final step should be taken towards application deployment processes. Since we have several clusters, there must be a process regarding how to deliver your applications consistently across all OpenShift clusters and all data centers:</p>
<p class="cdpaligncenter"><img class="alignnone79" src="../images/00103.jpeg"/></p>
<p class="calibre2">This is where external tools come into the picture. We can use external CI/CD software to automate the application deployment process across all OpenShift clusters, or we can build a separate OpenShift cluster with CI/CD to develop, build, test, and release applications to production OpenShift clusters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, <span class="calibre11">we talked about OpenShift scenarios in single and multiple data centers. This chapter also explained how to properly design OpenShift in a</span><span class="calibre11"> </span><span class="calibre11">distributed</span><span class="calibre11"> and redundant </span><span class="calibre11">configuration across one or more data centers</span>.  </p>
<p class="calibre2">In the next chapter, we are going to cover main network aspects while designing an OpenShift cluster in one or across multiple data centers. We will also cover commonly made mistakes, solutions, and overall guidance from a networking point of view.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Questions</h1>
                
            
            <article>
                
<ol class="calibre13">
<li value="1" class="calibre10">Which OpenShift component has built-in HA and works in active/active mode?<span> </span>choose one:
<ol class="calibre14">
<li value="1" class="calibre10">OpenShift Etcd key-value store</li>
<li value="2" class="calibre10">OpenShift Masters</li>
<li value="3" class="calibre10">OpenShift infrastructure nodes</li>
<li value="4" class="calibre10"><span>OpenShift n</span>odes</li>
</ol>
</li>
<li value="2" class="calibre10">Which OpenShift HA solution out of the ones listed supports unlimited scalability? choose one:
<ol class="calibre14">
<li value="1" class="calibre10">1xOSP - 3xDC</li>
<li value="2" class="calibre10">3xOSP - 1xDC</li>
<li value="3" class="calibre10">3xOSP - 3xDC</li>
<li value="4" class="calibre10">1xOSP - 1xDC</li>
</ol>
</li>
<li value="3" class="calibre10"><span>Anycast </span>IP addresses ensure that application traffic is load balanced across several data centers:
<ol class="calibre14">
<li value="1" class="calibre10">True</li>
<li value="2" class="calibre10">False</li>
</ol>
</li>
<li value="4" class="calibre10"><span>What are the two options to ensure application data consistency in geo-replicated OpenShift deployments? </span>choose two: 
<ol class="calibre14">
<li value="1" class="calibre10">Persistent storage replication</li>
<li value="2" class="calibre10">Application database replication</li>
<li value="3" class="calibre10">Openshift cluster replication </li>
<li value="4" class="calibre10">Openshift etcd key-value store replication </li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Further reading</h1>
                
            
            <article>
                
<p class="calibre2">Here are a list of topics with links related to this chapter that you might want to deep dive into:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">OpeShift HA design</strong>:<span> </span><a href="http://v1.uncontained.io/playbooks/installation/" class="calibre8">http://v1.uncontained.io/playbooks/installation/</a></li>
<li class="calibre10"><strong class="calibre1">OpenShift high availability</strong>:<span> </span><a href="https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html" class="calibre8">https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html</a></li>
<li class="calibre10"><strong class="calibre1">Openshift for single and multiple DCs</strong>:<span> </span><a href="https://blog.openshift.com/deploying-openshift-applications-multiple-datacenters/" class="calibre8">https://blog.openshift.com/deploying-openshift-applications-multiple-datacenters/</a></li>
<li class="calibre10"><strong class="calibre1">GlusterFS geo-replication</strong>: <a href="https://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/" class="calibre8">https://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>