- en: Setting Up A Production Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explored quite a few techniques, processes, and tools that can help us build
    a self-sufficient system applied to services. Docker Swarm provides self-healing,
    and we created our own system for self-adaptation. By now, we should be fairly
    confident with our services and the time has come to explore how to accomplish
    similar goals applied to infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The system should be capable of recreating failed nodes, of upgrading without
    downtime, and to scale servers depending on the fluctuating needs. We cannot explore
    those topics using clusters based on Docker Machine nodes running locally. The
    capacity of our laptops is somewhat limited so we cannot scale nodes to a greater
    number. Even if we could, the infrastructure we’ll use for production clusters
    is quite different. We’ll need an API that will allow our system to communicate
    with infrastructure. Moreover, we did not have an opportunity to explore persistent
    storage of the services we used thus far. Those few examples are only a fraction
    of what we’ll need, and we won’t enter into details just yet. For now, we’ll try
    to create a production-ready cluster that will allow us to continue on our path
    towards a self-sufficient system.
  prefs: []
  type: TYPE_NORMAL
- en: The immediate goal is to transition from locally running Swarm cluster based
    on Docker machines into something more reliable. We’ll have to move into the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: There are too many hosting vendors we could choose from, and it would be impossible
    to explain the process for each one of them. Even if we would focus only on those
    that are very popular, we would still have at least ten vendors to go through.
    That would increase the scope of the book beyond manageable size so we’ll pick
    one hosting provider that we’ll use to demonstrate a setup of a production cluster.
    It had to be one and AWS is the most commonly used hosting vendor.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your current choice of a vendor, you might be very happy or extremely
    displeased with that. If you prefer using [Microsoft Azure](https://azure.microsoft.com),
    you’ll see that you’ll be able to follow the same steps as those we’ll explore
    for AWS. The chances are that you prefer [Google Compute Engine (GCE)](https://cloud.google.com/compute/),
    [Digital Ocean](https://www.digitalocean.com/), [OpenStack](https://www.openstack.org)
    running on-premise, or any other among thousands of solutions and vendors. I’ll
    do my best to explain the logic behind the setup we’ll do in AWS. Hopefully, you
    should be able to apply the same logic to your infrastructure. I’ll try to make
    it clear what you should do, and I’ll expect you to roll-up your sleeves and do
    it on your own. I’ll provide a blueprint, and you’ll do the work.
  prefs: []
  type: TYPE_NORMAL
- en: You might be tempted to start translating the exercises that follow to your
    hosting solution. Don’t! If you do not have it already, please create an account
    on [Amazon Web Services (AWS)](https://aws.amazon.com/) and follow the instructions
    as they are. By doing that, you should have a clear idea of what can be done and
    what is the path to take. Only after that, once you’re finished reading this book,
    you should try to translate the experience into your infrastructure. From my side,
    I’ll do my best to explain everything we’ll do in AWS in a way that the same principles
    can be translated to any other choice. Moreover, I’ll do my best to keep AWS costs
    to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: That was more than enough talk. We’ll move into a hands-on part of this chapter
    and create a Docker Swarm cluster. Once it’s up-and-running, we’ll proceed with
    deployment of all the services we used so far. Finally, we’ll discuss which services
    might be missing and which modifications we should do to our stacks to make them
    production-ready. Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker For AWS Cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [The DevOps 2.1 Toolkit: Docker Swarm](https://www.amazon.com/dp/1542468914),
    I argued that the best way to create a Swarm cluster in AWS is with a combination
    of [Packer](https://www.packer.io/) and [Terraform](https://www.terraform.io/).
    One of the alternatives was to use [Docker CE for AWS](https://store.docker.com/editions/community/docker-ce-aws).
    At that time *Docker for AWS* was too immature. Today, the situation is different.
    *Docker for AWS* provides a robust Docker Swarm cluster with most, if not all
    the services we would expect from it.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll create a *Docker for AWS* cluster and, while in progress, discuss some
    of its aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start creating a cluster, we should choose a region. The only thing
    that truly matters is whether a region of your choice supports at least three
    availability zones. If there’s only one availability zone, we’ll risk downtime
    if it would become unavailable. With two availability zones, we’d lose Docker
    manager’s quorum if one zone would go down. Just as we should always run an odd
    number of Docker managers, we should spread our cluster into an odd number of
    availability zones. Three is a good number. It fits most of the scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In case you’re new to AWS, an availability zone (AZ) is an isolated location
    inside a region. Each region is made up of one or more availability zones. Each
    AZ is isolated, but AZs in a region are connected through low-latency links. Isolation
    between AZs provides high-availability. A cluster spread across multiple AZs would
    continue operating even if a whole AZ goes down. When using AZs inside the same
    region, latency is low thus not affecting the performance. All in all, we should
    always run a cluster across multiple AZs within the same region.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check whether your favorite AWS region has at least three availability
    zones. Please open [EC2 screen](https://console.aws.amazon.com/ec2/v2/home) from
    the *AWS console*. You’ll see one of the availability zones selected in the top-right
    corner of the screen. If that’s not the location you’d like to use for your cluster,
    click on it to change it.
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down to the *Service Health* section. You’ll find *Availability Zone
    Status* inside it. If there are at least three zones listed, the region you selected
    is OK. Otherwise, please change the region and check one more time whether there
    are at least three availability zones.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13-1: The list of availability zones supported by the US East region](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-1: The list of availability zones supported by the US East region'
  prefs: []
  type: TYPE_NORMAL
- en: There’s one more prerequisite we need to fulfill before we create a cluster.
    We need to create an SSH key. Without it, we would not be able to access any of
    the nodes that form the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please go back to the *AWS console* and click the *Key Pairs* link from the
    left-hand menu. Click the *Create Key Pair* button, type *devops22* as the *Key
    pair name*, and, finally, click the *Create* button. The newly created SSH key
    will be downloaded to your laptop. Please copy it to the *docker-flow-monitor*
    directory. The project already has `/*.pem` entry in the `.gitignore` file so
    your key will not be accidentally committed to GitHub. Still, as an additional
    precaution, we should make sure that only you can read the contents of the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
