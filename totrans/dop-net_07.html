<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Using Continuous Integration Builds for Network Configuration"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Using Continuous Integration Builds for Network Configuration</h1></div></div></div><p>This chapter will focus on continuous integration, what the process entails, and why it is applicable to network operations. We will look at why continuous integration processes are vitally important when automating network operations.</p><p>This chapter will discuss the benefits of configuration management tooling and we will look at practical configuration management processes that can be used to set up continuous integration processes and tooling that is available to support continuous integration processes.</p><p>In this chapter, the following topics will be covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Continuous integration overview</li><li class="listitem" style="list-style-type: disc">Continuous integration tooling available</li><li class="listitem" style="list-style-type: disc">Network continuous integration</li></ul></div><div class="section" title="Continuous integration overview"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Continuous integration overview</h1></div></div></div><p>Continuous integration is a process used to improve the quality of development changes. A continuous <a id="id577" class="indexterm"/>integration process, when applied to developers, takes new code changes and integrates it with the rest of the code base. This is done early in the development lifecycle, creating an instant feedback loop and associated pass or failure against the change.</p><p>Within the remits of DevOps, continuous integration is a key component as it uses centralized tooling to make changes visible to other users and promotes collaboration and integration of changes earlier in the software development lifecycle. Continuous integration is often coupled with Continuous Delivery processes, where continuous integration is used as the first part of the software delivery lifecycle.</p><p>Prior to continuous integration being implemented, developers would sometimes only find out that code changes did not work when a release needed to be packaged. At this point, all developer changes were combined by a release management or operations team. By the time the release was ready to be packaged, a developer would have moved on to new tasks and not have <a id="id578" class="indexterm"/>been currently working on that piece of work anymore, meaning fixing the issue incurred more time delaying the release schedule.</p><p>A good continuous integration process should be triggered every time a developer commits a change, meaning that they have a prompt feedback cycle to tell them if their change is good, rather than finding out weeks or months later that their commit had an issue that will slow down the release process.</p><p>Continuous integration works on the premise of fixing as far left as possible, meaning at development time, with the furthest right being production. What this phrase really means is that if an issue is found earlier in the development cycle then it will cost less to fix and have less of an impact to the business as it will ideally never reach production.</p><p>A continuous <a id="id579" class="indexterm"/>integration process follows <a id="id580" class="indexterm"/>the following steps, <span class="strong"><strong>Commit Change</strong></span> to <span class="strong"><strong>Source Control Management</strong></span> (<span class="strong"><strong>SCM</strong></span>), the repository change is validated, and a pass or failure is issued back to the user:</p><div class="mediaobject"><img src="graphics/B05559_07_01.jpg" alt="Continuous integration overview"/></div><p>The output of the continuous integration process should be what is shipped to test environments and production servers. It is important to make sure that the same binary artifacts that have been through continuous integration and relative testing are the same ones that will eventually be deployed onto the production servers.</p><p>Processes such as continuous integration are used to create feedback loops that show issues as soon as they <a id="id581" class="indexterm"/>occur, which saves cost. This means the change is fresh in the implementers mind and they will be able to fix it or revert the change quickly, with developers currently iterating the code collaboratively and fixing issues as soon as they occur.</p><p>Although all IT staff may not follow identical deployment strategies, feedback loops and validation should not be unique to just developers. Sure, a compilation process may not be required when making network changes, but other validations can be done against a network device or a change on an SDN controller or load balancer to validate the changes are correct.</p><div class="section" title="Developer continuous integration"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec80"/>Developer continuous integration</h2></div></div></div><p>A continuous integration process in its purest form takes a developer code change, integrates it with <a id="id582" class="indexterm"/>other developers' latest changes and makes <a id="id583" class="indexterm"/>sure it compiles correctly. The continuous integration process can then optionally run a set of unit or integration <a id="id584" class="indexterm"/>tests on the code base, package <a id="id585" class="indexterm"/>the compiled binaries, and then upload the build package to an artifact repository, tagging the code repository and package with a unique version number.</p><p>So, a simple continuous integration process can be summarized as the following feedback loop:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The developer <a id="id586" class="indexterm"/>commits code change to the <span class="strong"><strong>SCM System</strong></span> and integrates it with the code base.</li><li class="listitem">The code <a id="id587" class="indexterm"/>base is pulled down to a <span class="strong"><strong>CI Build Server</strong></span>.</li><li class="listitem">The code is compiled to check that the new commit is valid and non-breaking and the repository is <a id="id588" class="indexterm"/>tagged with the build version number.</li><li class="listitem">Return <span class="strong"><strong>Pass</strong></span> or <span class="strong"><strong>Fail</strong></span> exit conditions and <span class="strong"><strong>Feedback result</strong></span> to users.</li><li class="listitem">Repeat steps 1-5 for the next code change.<div class="mediaobject"><img src="graphics/B05559_07_02.jpg" alt="Developer continuous integration"/></div></li></ol></div><p>Steps 1 (developer commit) and step 2 (creating a copy of the repository on the <span class="strong"><strong>CI Build Server</strong></span>) are processes taken care of by <span class="strong"><strong>SCM Systems</strong></span>.</p><p>Some of the popular SCM systems over the past 10 years have been Subversion, IBM Rational ClearCase, Microsoft Team Foundation Server, Perforce, and Telelogic CM Synergy. While distributed source control management systems have moved from centralized to distributed source control management systems such as Git and Mercurial in recent years.</p><p>Steps 3 (code compilation), 4 (code compilation feedback to users), and 5 (repetition of the process) in the <a id="id589" class="indexterm"/>process are carried out by a continuous <a id="id590" class="indexterm"/>integration building servers, which act as a scheduling agent for the continuous build process.</p><p>Tools such as Cruise Control, Hudson, or more recently Jenkins, Travis, and Thoughtworks Go are used to schedule continuous integration.</p><p>Step 4 (code <a id="id591" class="indexterm"/>compilation <a id="id592" class="indexterm"/>feedback to users) can be carried out using compilation tools such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Maven <a class="ulink" href="https://maven.apache.org/">https://maven.apache.org/</a></li><li class="listitem" style="list-style-type: disc">Ant <a class="ulink" href="http://ant.apache.org/">http://ant.apache.org/</a></li><li class="listitem" style="list-style-type: disc">MsBuild <a class="ulink" href="https://msdn.microsoft.com/en-us/library/ms171452(v=vs.90).aspx">https://msdn.microsoft.com/en-us/library/ms171452(v=vs.90).aspx</a></li><li class="listitem" style="list-style-type: disc">Rake <a class="ulink" href="http://rake.rubyforge.org/">http://rake.rubyforge.org/</a></li><li class="listitem" style="list-style-type: disc">Make <a class="ulink" href="http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/">http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/</a></li></ul></div><p>All these <a id="id593" class="indexterm"/>tools, and <a id="id594" class="indexterm"/>many more, can be used as the main validation step <a id="id595" class="indexterm"/>in the process depending on the type of code compilation that is required.</p><p>The continuous integration process is carried out, polling for every new developer commit, and carrying out the code compilation and repeating the same process over and over to provide a continuous feedback loop. If a developer breaks the CI build they need to immediately fix it so that it doesn't block other development changes from being compiled and validated. So developers use continuous integration to collaborate and make sure their changes successfully integrate.</p><p>Additional steps such as unit or integration tests can be subsequently bolted on to the process after the compilation is successful for increased validation of the change. Just because code compiles, it doesn't mean it is always functional. When all compilation and tests are packaged a sixth step may be introduced to package the software and deploy it to an artifact repository.</p><p>All good continuous integration processes should work on the premise of compile, test, and package. So a code release should be packaged once and the same package should be distributed to all servers at deployment time.</p></div><div class="section" title="Database continuous integration"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec81"/>Database continuous integration</h2></div></div></div><p>After continuous integration was set up to help improve the quality of code releases, developers that controlled database changes generally thought about doing similar processes for database <a id="id596" class="indexterm"/>changes. As database changes are <a id="id597" class="indexterm"/>always a big part of any enterprise release process having broken database releases can prevent software being deployed and released to customers.</p><p>As a result, database schema changes or database programmatic stored procedures would equally benefit from being integrated earlier on in the continuous integration process and tested in a similar way using quick validation and feedback loops.</p><p>In a way, developers have it easy when considering continuous integration as the compilation process is a binary pass or fail metric that is easy to understand. Scripting languages are of course the exception to this rule, but these can be supplemented using unit tests to provide the code validation on various code operations and both codes are improved by good test coverage.</p><p>When doing database schema changes, a number of test criteria need to be met prior to pushing the code to production. Good database developers will provide roll forward and roll-back scripts when making SQL changes, which will be applied to production databases and they normally test these on their development machines prior to checking them into a source control management system.</p><p>Database developers implement database changes using a roll-forward and roll-back release script and store them in SCM systems. The roll-back is only performed in the case of an emergency when it is being applied to production if the roll-forward for any reason fails.</p><p>So a typical database release process will have the two following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Apply SQL table or column creation, update, deletion, or stored procedure using release script.</li><li class="listitem" style="list-style-type: disc">If this fails, roll-back SQL table or column creation, update, deletion or stored procedure using the roll-back release script.</li></ul></div><p>So prior to any production release, a database developer's roll-forward and roll-back scripts should be tested. As multiple database developers are part of the same release, these database release scripts should be applied in the same sequenced order as they would be applied to production as one developers change could break another developers changes.</p><p>Before setting up a database continuous integration, a few prerequisites are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A database schema matching production with a relative dataset and all the same characteristics such as indexing so we are testing against a similar live version</li><li class="listitem" style="list-style-type: disc">The continuous integration process should also utilize the same deployment runner script that is used to sequence the database release scripts and provide roll-back in case of failure</li></ul></div><p>Testing roll-back scripts is as integral to testing roll-forward scripts so the database continuous integration process will need valid tests to encompass roll-back.</p><p>A common database deployment workflow applied by a database developer on their local workstation would look like this:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Apply roll-forward database script using deployment runner script to CI test database.</li><li class="listitem">Apply roll-back <a id="id598" class="indexterm"/>database script using <a id="id599" class="indexterm"/>deployment runner script to CI test database.</li><li class="listitem">Apply roll-forward database script using deployment runner script to CI test database.</li><li class="listitem">Apply roll-back database script using deployment runner script to CI test database.</li></ol></div><p>If the preceding set of steps is successful then the roll-forward and roll-back database scripts are sound in terms of syntax and won't fail when applied to the production database.</p><p>The preceding steps also check the validity of the sequencing using the deployment runner and check that the integrated database deployment scripts work together and do not conflict on roll-back either.</p><p>Using continuous integration, we have already ruled out multiple possible scenarios that could cause a failure in production. However, the preceding continuous integration process alone is not enough, as with a code compilation, just because SQL is not returning an error doesn't mean the database roll-forward and roll-back scripts are technically valid, so database changes still need to be supplemented with functional tests too.</p><p>Continuous integration is about putting quality checks earlier in the delivery lifecycle and creating feedback loops. Continuous integration is not about proving that a release is 100% valid, it should instead be looked at as a way of proving that a checking process has been followed, which proves that a release is not broken.</p><p>A simple continuous integration database process would provide the following feedback loop for database developers:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Developer commits roll-forward and roll-back change to <span class="strong"><strong>SCM System</strong></span> and it is integrated with the code base.</li><li class="listitem">The code base is <a id="id600" class="indexterm"/>pulled down to a <span class="strong"><strong>CI Build Server</strong></span>.</li><li class="listitem">Apply roll-forward database script using deployment runner script to CI test database.</li><li class="listitem">Apply roll-back <a id="id601" class="indexterm"/>database script using deployment runner script to CI test database.</li><li class="listitem">Apply roll-forward database script using deployment runner script to CI test database.</li><li class="listitem">Apply roll-back database script using deployment runner script to CI test database.</li><li class="listitem">Return <span class="strong"><strong>Pass</strong></span> or <span class="strong"><strong>Fail</strong></span> exit condition and feedback to users.</li><li class="listitem">Repeat steps 1-7 for the next database change.<div class="mediaobject"><img src="graphics/B05559_07_03.jpg" alt="Database continuous integration"/></div></li></ol></div><p>Once the release is ready to go live the database CI will have the final changes applied, preparing it for the next release, the next iteration of database changes and the next batch of database scripts that <a id="id602" class="indexterm"/>will be applied by the next release. Alternately, the CI database schema can be refreshed from production.</p><p>A good concept is to always create a baseline of the database so that if a database developer unwittingly commits a bad roll-forward and roll-back on a database then the CI database can be easily restored to the desired state and not prove a bottleneck for development.</p><p>Of course this is one way of dealing with validation of database changes and others are possible. Microsoft offers <a id="id603" class="indexterm"/>database projects for this very purpose, but the validation engine is not important, having validation of any changes early in the release lifecycle is the important takeaway.</p><p>It is important to make sure that nothing goes to production unless it goes through the CI process, there is no point setting up a great process and then skipping it as it makes the CI database schema invalid and could have massive consequences.</p></div></div></div>
<div class="section" title="Tooling available for continuous integration"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Tooling available for continuous integration</h1></div></div></div><p>Many different flavors <a id="id604" class="indexterm"/>of configuration management tooling are available to help build continuous integration processes, so there is a rich variety of different options to choose from, which can seem daunting at first.</p><p>Tools should be picked to facilitate processes and will be selected by teams or users. As described in <a class="link" href="ch03.html" title="Chapter 3. Bringing DevOps to Network Operations">Chapter 3</a>, <span class="emphasis"><em>Bringing DevOps to Network Operations</em></span>, it is important to first map out requirements that need to be solved and the desired process before selecting any tooling.</p><p>By the same token, it is important to avoid tools sprawl, which is all too common in large companies and have only one best fit tool for every operation rather than multiple tools doing the same thing as there is an operational overhead for the business.</p><p>If configuration management tooling already exists in a company for continuous integration then it will more than likely be able to meet the needs. When considering the tooling for carrying out continuous integration processes the following tools are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SCM system</li><li class="listitem" style="list-style-type: disc">Validation engine</li></ul></div><p>The SCM system is primarily used for storing code or configuration management configuration in a source control repository.</p><p>The validation engine is used to schedule the compilation of code or validate configuration. So continuous <a id="id605" class="indexterm"/>integration build servers are used for the scheduling and numerous compilation or test tools can be used to provide the validation.</p><div class="section" title="Source control management systems"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec82"/>Source control management systems</h2></div></div></div><p>SCM systems <a id="id606" class="indexterm"/>provide the center of a continuous <a id="id607" class="indexterm"/>integration process, but no matter the SCM system that is chosen; at a base level it should have the following essential features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Be accessible to all users that need to push changes</li><li class="listitem" style="list-style-type: disc">Store the latest version of files</li><li class="listitem" style="list-style-type: disc">Have a centralized URL that can be browsed by users to see available repositories</li><li class="listitem" style="list-style-type: disc">Have a role-based access permission model</li><li class="listitem" style="list-style-type: disc">Support roll-back of versions and version trees on files</li><li class="listitem" style="list-style-type: disc">Show which user committed a change along with the date and time of the change</li><li class="listitem" style="list-style-type: disc">Support tagging of repositories, this can be used to check out a tag to show all the files that contributed to a release</li><li class="listitem" style="list-style-type: disc">Support multiple repository branches for parallel development</li><li class="listitem" style="list-style-type: disc">Have the ability to merge files and deal with merge conflicts</li><li class="listitem" style="list-style-type: disc">Have a command line</li><li class="listitem" style="list-style-type: disc">Plug into Continuous Integration Build servers</li></ul></div><p>Most SCM systems will also support additional features such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A programmable API or SDK</li><li class="listitem" style="list-style-type: disc">Easily integrated with developer IDEs</li><li class="listitem" style="list-style-type: disc">Integrate with Active Directory or <span class="strong"><strong>Lightweight Directory Access Protocol</strong></span> (<span class="strong"><strong>LDAP</strong></span>) for role-based access</li><li class="listitem" style="list-style-type: disc">Support <a id="id608" class="indexterm"/>integration with change management tools, where a SCM commit can be associated with a change ticket</li><li class="listitem" style="list-style-type: disc">Support integration with peer review tools</li></ul></div><p>SCM systems can either be centralized or distributed, in recent years, distributed source control management systems have increased in popularity.</p><div class="section" title="Centralized SCM systems"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec10"/>Centralized SCM systems</h3></div></div></div><p>When SCM systems <a id="id609" class="indexterm"/>were originally created to facilitate development teams, a centralized architecture was used to build these systems. A centralized SCM system would be used to store code and developers would access the repository they were required to make code changes against and make edits against a live centralized system.</p><p>For developers to remain productive, the centralized SCM system would always need to be available and online:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Developers would access the repository where they wish to make code changes</li><li class="listitem" style="list-style-type: disc">They'd then check out the file they wished to edit</li><li class="listitem" style="list-style-type: disc">Make changes</li><li class="listitem" style="list-style-type: disc">Then check the file back into the code central branch</li></ul></div><p>The SCM system would have a locking mechanism to avoid collisions where only one file can be edited by one user at a time. If two developers accessed the file at the same time, the online SCM <a id="id610" class="indexterm"/>system would say it was locked by another developer and they would have to wait until the other developer made their change prior to being allowed to check out the code and make the subsequent change.</p><p>Developers when making changes would make a direct connection to repositories hosted in the centralized SCM system to make code updates. When a developer made a change, this in turn would write the changes in state to a centralized database, updating the state of the overall repository.</p><p>The state change would then be synchronized to other developer's views automatically. One of the criticisms of centralized SCM systems was the fact that developers sometimes wanted to work offline, so some centralized source control management systems introduced the concept of snapshot views, which was an alternative to the permanently live and updated repository view and also introduced offline update features.</p><p>A snapshot view in a centralized SCM system was a snapped copy of the live repository at a given point in time. Best practice would dictate that before committing any development changes to the centralized server, the snapshot view should be updated; any merge conflicts would be dealt with locally before checking in any changes that were made in the snapshot view.</p><p>Developers would integrate with the centralized SCM system using the command-line interface or GUI that was integrated with a developers IDE for ease of use so they didn't need to jump between the command line and the IDE.</p><p>Examples of <a id="id611" class="indexterm"/>good centralized source control management systems are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">IBM Rational ClearCase</li><li class="listitem" style="list-style-type: disc">Telelogic CM Synergy</li><li class="listitem" style="list-style-type: disc">IBM Rational Team Concert</li><li class="listitem" style="list-style-type: disc">Microsoft Team Foundation Server</li><li class="listitem" style="list-style-type: disc">Subversion</li><li class="listitem" style="list-style-type: disc">Perforce</li></ul></div></div><div class="section" title="Distributed SCM systems"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec11"/>Distributed SCM systems</h3></div></div></div><p>Distributed SCM systems do not have a central master and instead replicate changes to multiple places. Users will create replicas of a repository and then can pull or push using their own local copy sitting on their local development machine.</p><p>Each repository <a id="id612" class="indexterm"/>in a distributed system will have an owner or maintainer and users will submit changes in the form of pull requests. Developers will create a pull request, which is like a merge request, but instead the repository maintainer can then approve if they accept the pull request or not. Once accepted, the commit will be pulled into the branch.</p><p>One of the main benefits of a distributed SCM system is the ability to work on the repository offline. Changes can be committed to the local repository and then once it's back online, pushed to the master branch when developers are ready.</p><p>Distributed SCM systems are more merge-friendly and efficient, so they work better with agile development, which often means multiple small repositories for each microservice rather than large centralized code bases for monolith applications.</p><p>Examples of <a id="id613" class="indexterm"/>distributed SCM systems are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Git</li><li class="listitem" style="list-style-type: disc">Mercurial</li><li class="listitem" style="list-style-type: disc">Veracity</li></ul></div></div><div class="section" title="Branching strategies"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec12"/>Branching strategies</h3></div></div></div><p>Branching strategies are used to meet the needs of modern software development, with multiple branches serving <a id="id614" class="indexterm"/>different use cases and supporting multiple versions of the code.</p><p>SCM systems traditionally relied on a <span class="strong"><strong>Mainline</strong></span> branch, often referred to as the <span class="strong"><strong>Trunk</strong></span> or <span class="strong"><strong>Master</strong></span> branch. A mainline branching strategy meant that the mainline/trunk branch is always the clean and working version of the code, and the files on this branch are representative of the code in production.</p><p>Development branches were then created for active development on the latest releases, while release branches were used for maintenance releases if bugs were identified on the production system.</p><p>There are many different branching strategies that can be implemented; in the following example, the mainline branching strategy is illustrated.</p><p>The mainline/trunk/master branch <a id="id615" class="indexterm"/>is kept clean and all releases are done by merging changes to it, and this branch is tagged every time a release is done. This allows a diff to be done between tags to see what has changed.</p><p>The development branch is used for active development and creates version 1.0, then merges to the <span class="strong"><strong>Release Branch 1.0</strong></span>, which in turn immediately merges back to <span class="strong"><strong>Mainline/Trunk/Master</strong></span>.</p><p>The development branch then starts active development on version 2.0, while <span class="strong"><strong>Release Branch 1.0</strong></span> is used <a id="id616" class="indexterm"/>for 1.x maintenance releases if a bug fix is required:</p><div class="mediaobject"><img src="graphics/B05559_07_04.jpg" alt="Branching strategies"/></div><p>The mainline branching strategy meant a lot of merging and coordination and release managers were required to coordinate merges and releases of versions on release days.</p><p>Centralized configuration management systems were set up to favor a mainline approach to software development and this was good when supporting waterfall development.</p><p>Waterfall software development has rigid phases of the project, incorporating analysis, design, implementation, <a id="id617" class="indexterm"/>and testing phases, so the mainline branching strategy was sufficient when teams were producing only one release every few months as opposed to daily releases, so the laborious merge process was not such a bottleneck.</p><p>However, the transition to agile software development meant that implementing the mainline strategy became more difficult as teams release more frequently now that they have moved towards continuous deployment and delivery models.</p><p>An alternate branching strategy better suited to agile development is using feature branches. In agile software development work is split into sprints that last two weeks. So the master or mainline branch is still used but very short-lived feature branches are created by developers during a sprint. Distributed SCM systems put the developer in charge of the merging as opposed to using a centralized release management team for these operations.</p><p>In the following <a id="id618" class="indexterm"/>example, we can see an example of feature branching, where three different feature branches, <span class="strong"><strong>Feature A</strong></span>, <span class="strong"><strong>Feature B</strong></span>, and <span class="strong"><strong>Feature C</strong></span> are created during a two week sprint. When developers have finished development their features merged back into the <span class="strong"><strong>Trunk/Master</strong></span> branch.</p><p>Every time a commit is done from a feature branch then the change is merged directly to <span class="strong"><strong>Trunk/Master</strong></span> and a continuous integration process <a id="id619" class="indexterm"/>will be started which will validate the changes, every successful check-in then becomes a potential release candidate. After a release is packaged by the continuous integration process it is ready for deployment, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B05559_07_05.jpg" alt="Branching strategies"/></div><p>Some purists will argue against utilizing feature branches at all, preferring to always work against <span class="strong"><strong>Trunk/Master</strong></span>. However, this decision it down to the individual teams to govern which approach works best for them and it is subjective. Some will also argue that it adds an additional level of control until adequate testing is created on the Trunk/Master branch so that changes can be suitably peer-reviewed prior to merging.</p><p>When a commit is done against a branch it should trigger a CI build and associated validation of whatever <a id="id620" class="indexterm"/>change has been committed. This creates feedback loops at every stage of the process. Any change that goes into any branch should be governed by a CI build to gate-keep good changes and highlight breaking changes as soon as they happen so they can be fixed immediately.</p></div></div><div class="section" title="Continuous integration build servers"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec83"/>Continuous integration build servers</h2></div></div></div><p>Various continuous <a id="id621" class="indexterm"/>integration build servers <a id="id622" class="indexterm"/>are available to help schedule validation steps or tests. One of the first continuous integration build servers was Cruise Control from Thoughtworks, which has since evolved into Thoughtworks Go.</p><p>
<span class="strong"><strong>Cruise Control</strong></span> allowed users to <a id="id623" class="indexterm"/>configure an XML file that set up different continuous integration build jobs. Each build job ran a set of command-line options; normally, a compilation process against a code repository and it returned a green build if it was successful and a red build if the build was broken. Cruise Control would highlight the errors in the form of build logs providing feedback to users via the Cruise Control dashboard or by e-mail.</p><p>The market leading build server at the moment is Cloudbees Jenkins, which is an open source project and a fork of the original Hudson project. Jenkins really took away the need to configure XML files and moved all setup operations into the GUI or API. It comes with a plethora of plugins that can pretty much carry out any continuous integration operation possible. It also has recently delved into Continuous Delivery as of Jenkins 2.x release.</p><p>The next evolution of CI systems has moved towards cloud-based solutions with Travis being a popular choice for open source projects. This allows users to check in a Travis YAML file, which creates the build configuration from source control and can be versioned along with the code. This is something Jenkins 2.x is doing now using the Jenkinsfile and that the Jenkins job builder project had been doing for the OpenStack project.</p><p>There are many different options when looking for continuous integration build servers, consider the following; no matter the continuous integration build system that is chosen, at a base level it should have the following essential features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Dashboard for feedback</li><li class="listitem" style="list-style-type: disc">Notion of green and red builds</li><li class="listitem" style="list-style-type: disc">Scheduling capability for generic command lines</li><li class="listitem" style="list-style-type: disc">Pass or fail builds based on exit conditions, <code class="literal">0</code> being a pass</li><li class="listitem" style="list-style-type: disc">Plug-ins to well-known compilation tools</li><li class="listitem" style="list-style-type: disc">Ability to poll SCM systems</li><li class="listitem" style="list-style-type: disc">Ability to integrate with unit testing framework solutions such as Junit, Nunit, and more</li><li class="listitem" style="list-style-type: disc">Role-based access control</li><li class="listitem" style="list-style-type: disc">Ability to display change lists of the latest commits to a repository that has been built</li></ul></div><p>Most continuous integration build servers will <a id="id624" class="indexterm"/>also support additional features such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Have a programmable API or SDK</li><li class="listitem" style="list-style-type: disc">Provide e-mail <a id="id625" class="indexterm"/>or messaging integration</li><li class="listitem" style="list-style-type: disc">Integrate with Active Directory or LDAP for role-based access</li><li class="listitem" style="list-style-type: disc">Support integration <a id="id626" class="indexterm"/>with change management tools, where a SCM commit can be associated with a change ticket</li><li class="listitem" style="list-style-type: disc">Support integration with peer review tools</li></ul></div></div></div>
<div class="section" title="Network continuous integration"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Network continuous integration</h1></div></div></div><p>So why should network engineers be interested in continuous integration? A network team should be interested <a id="id627" class="indexterm"/>in continuous integration if they want to improve the following points, which were focused on in <a class="link" href="ch03.html" title="Chapter 3. Bringing DevOps to Network Operations">Chapter 3</a>,<span class="emphasis"><em> Bringing DevOps to Network Operations</em></span>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Velocity of change</li><li class="listitem" style="list-style-type: disc">Mean time to resolve</li><li class="listitem" style="list-style-type: disc">Improved uptime</li><li class="listitem" style="list-style-type: disc">Increased number of deployments</li><li class="listitem" style="list-style-type: disc">Cross skilling between teams</li><li class="listitem" style="list-style-type: disc">Removal of the bus factor of one</li></ul></div><p>The ability to easily trace what has changed on the network and see which engineer made a change is something that continuous integration brings to the table. This information will be available by looking at the latest commit on a continuous integration build system.</p><p>Roll-back will be as simple as deploying the last tagged release configuration as opposed to trawling through device logs to see what changes were applied to a network device if an error occurs.</p><p>Every network engineer can look at the job configuration on the continuous integration build system and see how it operates so every network engineer knows how the process works so it helps with cross skilling.</p><p>Having continual feedback loops will allow network teams to continuously improve processes, if a network process is sub-optimal then the network team can easily highlight the pain points in the process and fix them as the change process is evident to all engineers and done in a consistent manner.</p><p>When network teams use continuous integration processes it moves network teams out of firefighting mode and into tactical continuous improvement and optimization mode. Continuous integration means that the quality of network changes will improve as every network change has associated validation steps that are no longer manual and error prone.</p><p>Instead, these checks and validations are built in and carried out every time a network operator commits a network change to the SCM System. These changes can be built up over time to make network changes less error prone and give network engineers the same capabilities as developers and infrastructure teams.</p><p>Utilizing network continuous integration also takes the fear out of making production changes, as they are <a id="id628" class="indexterm"/>already validated and verified as part of the continuous integration process, so production changes can be viewed as just a business-as-usual activity, rather than something that needs to be planned weeks in advance or worried about. The view is: if an activity is problematic then do it more often, continually iterate it, improve it, and make people less afraid of doing it.</p><p>Having covered topics such as different SCM branching strategies, continuous integration build servers, and shown how continuous integration can be used for code and database changes, it should now be clear what continuous integration is and that it is not just about compilation of code. Instead, continuous integration is about validating parallel changes, making sure they all work together and providing feedback loops to users.</p><p>The DevOps movement is about interacting with others and removing bottlenecks, delivering products to market faster, and increasing accuracy so continuous integration is equally applicable to networking. The automation of processes and the collaboration between teams using similar concepts is very important so continuous integration really is the glue that holds infrastructure and networking as code together.</p><p>To a network engineer, concepts such as continuous integration may seem alien at first, but instead of talking about deep dive compilation processes, it should be focusing on processes. If any network engineer was asked if they could have a quick and easy-to-use process that validated all their network changes before production, providing quick feedback loops, then the answer would be yes. Continuous integration can therefore be a useful tool that would mean less broken production changes.</p><p>In this book, in <a class="link" href="ch04.html" title="Chapter 4. Configuring Network Devices Using Ansible">Chapter 4</a>, <span class="emphasis"><em>Configuring Network Devices Using Ansible</em></span>, <a class="link" href="ch05.html" title="Chapter 5. Orchestrating Load Balancers Using Ansible">Chapter 5</a>, <span class="emphasis"><em>Orchestrating Load Balancers Using Ansible</em></span>, and <a class="link" href="ch06.html" title="Chapter 6. Orchestrating SDN Controllers Using Ansible">Chapter 6</a>, <span class="emphasis"><em>Orchestrating SDN Controllers Using Ansible</em></span>, we looked at ways that network changes could be treated as code, using configuration management tooling such as Ansible to configure network devices, load balancers, and SDN controllers.</p><p>So when considering the following diagram, the question regarding continuous integration of network changes is not asking if continuous integration is possible for network changes. It should instead be questioning which validation engines can be used for network changes after a SCM commit has taken place to give a quick feedback loop of <span class="strong"><strong>Pass</strong></span> or <span class="strong"><strong>Fail</strong></span> to network operators:</p><div class="mediaobject"><img src="graphics/B05559_07_06.jpg" alt="Network continuous integration"/></div><div class="section" title="Network validation engines"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec84"/>Network validation engines</h2></div></div></div><p>The challenge when creating continuous integration builds for network changes is what to use for the validation <a id="id629" class="indexterm"/>engine. Network changes when using <a id="id630" class="indexterm"/>Ansible rely heavily on YAML configuration files, so the first validation that can be done is checking the YAML <code class="literal">var</code> files.</p><p>The <code class="literal">var</code> files are used to describe the desired state of the network, so checking that these YAML files are valid in terms of syntax is one valid check. So to do this, a tool such as <code class="literal">yamllint</code> can be used to check if the syntax of the files that are committed into source control management are valid.</p><p>Once the YAML <code class="literal">var</code> files are checked into source control, the continuous integration build should create a tag to state a new release has happened. All SCM systems should have a tagging or base-lining feature.</p><p>Tagging versions means that the current network release version can be diffed against the previous version to see what file changes have occurred on the YAML <code class="literal">var</code> files. If an issue is detected at any stage, all networks changes are made transparent.</p><p>So what other validation is possible? When focusing on configuration of network devices, we are pushing configuration changes to a networking operating system such as Juniper Junos or Arista Eos. So being able to run the newly committed changes and make sure the syntax is programmatically correct against those operating systems as part of the continuous integration process is highly desirable. Most network device operating systems as discussed in <a class="link" href="ch04.html" title="Chapter 4. Configuring Network Devices Using Ansible">Chapter 4</a>, <span class="emphasis"><em>Configuring Network Devices Using Ansible</em></span>, are Linux-based, so having a network operating system to issue commands to as part of the CI process doesn't seem too absurd.</p><p>The same can be said <a id="id631" class="indexterm"/>when checking the <a id="id632" class="indexterm"/>configuration used to orchestrate load balancers or SDN controllers, having a test environment attached to the continuous integration process is also highly desirable in theory. By utilizing a software version of the load balancer or emulated version of the SDN controller would be highly beneficial, so network engineers can pre-flight their network changes to make sure the API calls and syntax is correct.</p><p>However, there are challenges with simulating an SDN controller or creating or simulating a production environment depending on the vendor, they may have a huge overhead in terms of setting up a continuous integration environment due to cost. Network devices, load balancers, and SDN vendors are evolving to support automation and DevOps friendly processes such as continuous integration. Therefore, networking vendors are starting to appreciate the validity of giving small test environments; this is where virtualized or containerized versions of load balancers or SDN controllers would be useful as an API endpoint to validate the desired state that has been set up in YAML files.</p><p>Alternately, the vendors could provide a vagrant box to test if the desired configuration specified in YAML <code class="literal">var</code> files that is checked into SCM Systems is valid before it is propagated to the first test environment. Any enhancements that can be done to processes to make it fail as fast as possible and shift issues as far left as possible in the development lifecycle should be implemented where possible.</p><p>So with all of these validators, let's look at how these processes can be applied to network devices, or alternately orchestration. The number of validators used may depend on the network vendors that are being used, so we will look at the start point for a continuous integration build for network devices regardless of vendor and then look at more advanced options that could be used if the vendor provides a software load balancer or SDN emulation.</p></div><div class="section" title="Simple continuous integration builds for network devices"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec85"/>Simple continuous integration builds for network devices</h2></div></div></div><p>As network <a id="id633" class="indexterm"/>changes are always required daily by large organizations that are implementing microservice applications. To meet those demands networking should be as self-service as <a id="id634" class="indexterm"/>possible. To keep <a id="id635" class="indexterm"/>up with demand, network teams will probably need to use a feature branch SCM strategy or allow self-service YAML files to be committed directly to the master branch, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B05559_07_07.jpg" alt="Simple continuous integration builds for network devices"/></div><p>Each commit should be peer reviewed before it is merged. Ideally, the self-service process should allow development teams to package network changes alongside their code changes and follow a self-service approach.</p><p>The first continuous integration build that should be set up for network devices or orchestration should focus on version controlling the Ansible YAML files and running a simple YAML validation on the desired state.</p><p>Each continuous integration build that runs will also tag the repository. Tagging the SCM repository means that release versions can be compared or easily rolled back. It will also act as an audit log to show which user made changes and what exactly has changed in the environment. No changes should be made to a production system that has not gone through the continuous integration process.</p><p>So a simple network continuous integration build will follow these simple validation steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">YAML files are checked for syntax.</li><li class="listitem">The repository is tagged in the SCM System if successful.</li></ol></div><p>Therefore, a simple network continuous integration build would follow these steps. The network operator would commit the YAML files to the SCM system to change the desired state of the <a id="id636" class="indexterm"/>network; the continuous integration build server would tag the build if the YAML Lint <a id="id637" class="indexterm"/>operation finds <a id="id638" class="indexterm"/>that all the YAML files in the repository have valid syntax and return a positive result:</p><div class="mediaobject"><img src="graphics/B05559_07_08.jpg" alt="Simple continuous integration builds for network devices"/></div></div><div class="section" title="Configuring a simple Jenkins network CI build"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec86"/>Configuring a simple Jenkins network CI build</h2></div></div></div><p>This simple <a id="id639" class="indexterm"/>continuous <a id="id640" class="indexterm"/>integration build for network devices can be set up in the Jenkins CI build server. Rake and the <code class="literal">yamllint</code> gem should be configured on the Jenkins slave that the build will be executed on.</p><p>Once this has been completed, a new Jenkins CI build can be created in a matter of minutes.</p><p>First, select a new Jenkins freestyle job:</p><div class="mediaobject"><img src="graphics/B05559_07_09.jpg" alt="Configuring a simple Jenkins network CI build"/></div><p>Then configure the SCM <a id="id641" class="indexterm"/>system to <a id="id642" class="indexterm"/>use, in this instance Git, specifying <code class="literal">git@gitlab:devops/sdn.git</code> as the repository and the <code class="literal">*/master</code> branch of the project along with the SSH key required to provide access to the repository:</p><div class="mediaobject"><img src="graphics/B05559_07_10.jpg" alt="Configuring a simple Jenkins network CI build"/></div><p>Now for the validation step, a shell command build step is selected, which will run <code class="literal">rake yamllint</code> on the <a id="id643" class="indexterm"/>repository after configuring a <span class="strong"><strong>Rakefile</strong></span> in the <code class="literal">git@gitlab:devops/sdn.git</code> repository so the YAML files can be parsed:</p><div class="mediaobject"><img src="graphics/B05559_07_11.jpg" alt="Configuring a simple Jenkins network CI build"/></div><p>Finally, configure <a id="id644" class="indexterm"/>the build <a id="id645" class="indexterm"/>job to tag the Jenkins build version against the <code class="literal">devops/sdn.git</code> gitlab repository and <span class="strong"><strong>Save</strong></span> the build:</p><div class="mediaobject"><img src="graphics/B05559_07_12.jpg" alt="Configuring a simple Jenkins network CI build"/></div><p>This has configured a very simple Jenkins CI build process that will poll the Git repository for new <a id="id646" class="indexterm"/>changes, run <code class="literal">yamllint</code> against the repository, and then tag the Git repository if the build is <a id="id647" class="indexterm"/>successful.</p><p>The build health will be shown in Jenkins; the green ball means the build is in a healthy state so the YAML files are currently in a good state, and the duration of the check shows it took 6.2 seconds to execute the build, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_07_13.jpg" alt="Configuring a simple Jenkins network CI build"/></div></div><div class="section" title="Adding validations to network continuous integration builds"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec87"/>Adding validations to network continuous integration builds</h2></div></div></div><p>After highlighting the need for more robust validation to pre-flight configuration of network devices shifting failure as far left in the development lifecycle as possible to reduce the cost to fix. Having the ability to push mission-critical configuration changes to a networking operating system such as <a id="id648" class="indexterm"/>Cisco Nxos, Juniper Junos, or Arista Eos would be a good continuous integration validation.</p><p>So, like databases verifying that SQL syntax is correct, being able to run the newly committed changes and make sure the networking commands or orchestration commands applied to network devices syntax is programmatically correct should be part of the continuous integration build.</p><p>Continuous integration can then help the quality of network changes as an incorrect change would never be pushed to a network device, load balancer, or SDN controller. Of course, the functionality of the configuration pushed may not be what is required, but there should at least never be a situation where the configuration has a syntax error at deployment time.</p><p>As network devices, load balancer, and SDN controller changes are mission-critical, this brings an added layer of validation checks to any network changes and checks in a quick and automated way, providing quick feedback if a network change is not what is required.</p><div class="section" title="Continuous integration for network devices"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec13"/>Continuous integration for network devices</h3></div></div></div><p>Before setting up a <a id="id649" class="indexterm"/>network device, continuous integration of a few prerequisites is required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A network operating system will be required with production configuration pushed to it and all live settings, which can be hosted on a virtual appliance</li><li class="listitem" style="list-style-type: disc">The continuous integration build tools such as Jenkins will need to have an Ansible Control Host set up on the agent so it can execute Ansible playbooks</li><li class="listitem" style="list-style-type: disc">All playbooks should be written with a block rescue so subsequent cleanup is built-in if the execution of the playbook fails</li></ul></div><p>A typical network device release process will have the two following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Apply the network change self-service playbook.</li><li class="listitem">As the playbook is idempotent, changes will only be shown if a change has occurred.</li></ol></div><p>The Ansible playbook should provide resilience for roll-forward and roll-back in terms of state change. The previous steps also check the validity of the sequencing using the Ansible playbook and also check that the calls being made to the network device are valid.</p><p>A simple continuous <a id="id650" class="indexterm"/>integration network build process would provide the following feedback loop for network operators:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The network operator commits Ansible playbook or YAML <code class="literal">var</code> file's change to <span class="strong"><strong>SCM System</strong></span>, and it <a id="id651" class="indexterm"/>is integrated with the code base.</li><li class="listitem">The code base is <a id="id652" class="indexterm"/>pulled down to a <span class="strong"><strong>CI Build Server</strong></span>.</li><li class="listitem">YAML files are checked using <code class="literal">yamllint</code>.</li><li class="listitem">The Ansible playbook is applied to push network changes to the device.</li><li class="listitem">Return <span class="strong"><strong>Pass</strong></span> or <span class="strong"><strong>Fail</strong></span> exit conditions and feedback to users.</li><li class="listitem">Repeat steps 1-5 for the next network device change:<div class="mediaobject"><img src="graphics/B05559_07_14.jpg" alt="Continuous integration for network devices"/></div></li></ol></div></div></div><div class="section" title="Continuous integration builds for network orchestration"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec88"/>Continuous integration builds for network orchestration</h2></div></div></div><p>Before setting up a <a id="id653" class="indexterm"/>network orchestration <a id="id654" class="indexterm"/>for load balancers or SDN controllers, a few prerequisites are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A software load balancer or an emulated SDN controller will be required with production configuration pushed to it and all live settings</li><li class="listitem" style="list-style-type: disc">The continuous integration build tools such as Jenkins will need to have an Ansible controller set up on the agent so it can execute Ansible playbooks as well as the SDK that will allow the network orchestration modules to be executed</li><li class="listitem" style="list-style-type: disc">All playbooks should be written with a block rescue so subsequent cleanup is built in if the execution of the playbook fails</li></ul></div><p>A typical network device release process will have the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Apply network changes to the self-service playbook.</li><li class="listitem" style="list-style-type: disc">As the playbook is idempotent, changes will be only shown if a change has occurred.</li></ul></div><p>The Ansible playbook, like with the network device changes should provide resilience for roll-forward and roll-back in terms of state change. Some test servers may be needed on a virtualization platform to simulate the load balancing so health checks can be tested too.</p><p>A simple continuous<a id="id655" class="indexterm"/> integration network orchestration continuous integration process would provide the following <a id="id656" class="indexterm"/>feedback loop for network operators:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The network operator commits Ansible playbook or YAML <code class="literal">var</code> file change to <span class="strong"><strong>SCM System</strong></span> and it is <a id="id657" class="indexterm"/>integrated with the code base.</li><li class="listitem">The code base is <a id="id658" class="indexterm"/>pulled down to a <span class="strong"><strong>CI Build Server</strong></span>.</li><li class="listitem">YAML files are checked using <code class="literal">yamllint</code>.</li><li class="listitem">An Ansible playbook is applied to orchestrate the API and create the necessary load balancer or SDN changes.</li><li class="listitem">Return <span class="strong"><strong>Pass</strong></span> or <span class="strong"><strong>Fail</strong></span> exit condition and feedback to users.</li><li class="listitem">Repeat steps 1-5 for next network orchestration change:<div class="mediaobject"><img src="graphics/B05559_07_15.jpg" alt="Continuous integration builds for network orchestration"/></div></li></ol></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec40"/>Summary</h1></div></div></div><p>In this chapter, we have looked at what continuous integration is and how continuous integration processes can be applied to code and databases. The chapter then looked at ways that continuous integration can be applied to assist with network operations to provide feedback loops.</p><p>We also explored different SCM methodologies, the difference between centralized and distributed SCM systems and how branching strategies are used with waterfall and agile processes.</p><p>We then looked into the vast array of tools available for creating continuous integration processes focusing on some examples using Jenkins to set up a simple network continuous integration build.</p><p>In this chapter, you learned what continuous integration is, how it can be applied to network operations, SCM tooling, and the difference between centralized and distributed systems along with common SCM branching strategies.</p><p>Other key takeaways from this chapter include continuous integration build servers and their use, ways to integrate network changes into continuous integration, and potential continuous integration validation engines for network changes.</p><p>In the next chapter, we will look at various test tools and how they can be applied to continuous integration processes for added validation. This will allow unit tests to be created for network operations to make sure the desired state is actually implemented on devices before we will look at deploying the network changes in Continuous Delivery pipelines.</p></div></body></html>