- en: Orchestration Using Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about orchestration features. In this chapter,
    we will build on this by learning about Docker Swarm. It comes with Docker Engine
    (Docker installation packages) out of the box, so we don't need to install any
    other software. It is simpler to master the basics of Docker Swarm compared to
    the other orchestrators available, and it is powerful enough for production deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, in this chapter, we will learn how to deploy Docker Swarm in production.
    We will also review the new objects introduced by Docker Swarm and the steps required
    to deploy a complete application based on containers with orchestration. Networking
    is key for node-distributed applications, so we will examine how Docker Swarm
    provides solutions for internal networking, service discovery, and publishing
    deployed applications. At the end of the chapter, we will review how Docker Swarm
    can help us upgrade our application's components without service interruption.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Docker Swarm cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling workloads in the cluster – tasks and services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying applications using Stacks and other Docker Swarm resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking in Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about Docker Swarm's orchestrator features. We'll
    provide some labs at the end of this chapter that you can use to test your understanding
    and demonstrate the concepts you've learned. These labs can be run on your laptop
    or PC using the provided Vagrant "Docker Swarm" environment, or any already deployed
    Docker Swarm cluster of your own. Check out this book's GitHub code repository
    for the code we're going to be using in this chapter, along with additional information,
    at [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '"[https://bit.ly/31wfqmu](https://bit.ly/31wfqmu)"'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm is the built-in orchestrator that comes with Docker Engine out
    of the box. It was introduced in Docker Engine release 1.12 (the release numbers
    changed after 1.13 to four-digit numbers) as *swarm mode*. There was a previous
    swarm approach currently known as Legacy Swarm, which was closer in architecture
    to Kubernetes. It required an external key-value store database, among other components.
    Swarm mode is different from this because it includes everything needed for the
    orchestrator to work out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Swarm architecture is quite simple as it provides secure communications
    between components by default. Before deploying a Docker Swarm cluster, let''s
    review its main features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Container orchestration for multiple nodes is included on each Docker Engine**:
    This means that we can deploy a cluster without any other software. Docker Engine
    provides all the required components to deploy and manage the cluster out of the
    box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node roles can be changed at runtime**: Orchestration is based on different
    node roles. While the control plane is managed by managers or master nodes, computation
    or application deployment will be done on slave, worker, or minion nodes. Each
    orchestrator uses different names for these different roles, but they are essentially
    the same. Swarm allows us to change nodes from one role to another when one of
    them is unhealthy or when we need to do some maintenance tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workloads will be declared as services, defining a number of instances to
    be healthy**: The Docker orchestrator will keep the required number of replicas
    alive. If some of them die, the orchestrator will run new tasks to keep the required
    number alive. The orchestrator will manage this reconciliation process. If a node
    dies, the orchestrator will move all containers to a new, healthy node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**As workloads are based on the number of instances required, the orchestrator
    will allow us to change this number any time we require**: As a result, we can
    scale up or down the number of instances of a service (application component)
    to respond to a high demand for requests, for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We will deploy applications based on multiple service components, with all
    their requirements and connectivity between them**: As components may run on any
    cluster node, Docker Swarm will provide an internal overlay network to interconnect
    all application components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swarm will provide service discovery and internal load balancing**: In the
    *Service discovery and load balancing* section, we will learn how Docker Swarm
    can provide internal application DNS resolution so that all the components will
    easily be able to discover each other, along with load balancing between service
    replicas using a virtual IP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orchestration will allow us to update application components automatically**:
    In fact, all we need to decide is how these updates will be managed; orchestration
    will do the rest. This way, we can update application components without impacting
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We can ensure that our cluster runs securely by default**: Docker Swarm will
    deploy **Transport Layer Security** (**TLS**) to interconnect control plane components.
    It will manage certificates for all of our nodes, creating an internal CA and
    verifying all node certificates itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to know that only the control plane is secure by default. Users'
    access to features such as application publishing will require additional configuration.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in the previous chapter, orchestrators require databases to store
    and manage workloads and any other cluster resource information. Docker Swarm
    has a built-in key-value store under the `/var/lib/docker/swarm` path (this is
    on Linux; it can be found in its equivalent directory on Windows, under `C:\ProgramData\docker`).
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that the `/var/lib/docker/swarm` directory is
    essential, should we need to restore an unhealthy cluster. Take care of this directory
    and keep a backup of it.
  prefs: []
  type: TYPE_NORMAL
- en: We can lock users' access to the `/var/lib/docker/swarm` path using a key. This
    improves security. If it is unlocked, someone with enough system privileges can
    obtain Docker Swarm certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm overall architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned previously, Docker Swarm deploys its own secure control plane.
    There are two kinds of node roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Managers**: These manage the overall Swarm cluster environment. They share
    an internal key-value database. More specifically, one of the managers has a different
    role. This is the manager''s leader. There is only one leader per cluster and
    it makes all the necessary updates to the database. All other manager nodes will
    follow and sync their databases with the leader''s one. Managers maintain cluster
    health, serve the Swarm HTTP API, and schedule workloads on available compute
    nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workers**: Workloads will run on worker nodes. It is important to know that
    managers have worker roles too. This means that workloads can also run on managers
    if we do not specify any special scheduling location. Workers will never participate
    in scheduling decisions; they will just run assigned workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We manage workload locations either by using location constraints on each workload
    or by disabling container execution on some nodes.
  prefs: []
  type: TYPE_NORMAL
- en: On nodes with multiple interfaces, we will be able to choose which interface
    we will use for the control plane. Manager nodes will implement the Raft consensus
    algorithm to manage the Swarm cluster state. This algorithm requires multiple
    servers coming to an agreement on data and status. Once they reach a decision
    on a value, that decision is written to disk. This will ensure information distribution
    with consistency across multiple managers.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, there is a leader node that modifies and store changes
    on its database; all other nodes will sync their databases with it. To maintain
    this consistency, Swarm implements Raft. This algorithm will manage all changes
    in the database, as well as the election of a new leader when it is unhealthy.
    When the leader needs to make a change (such as to the application's component
    status, and its data), it will query all the other nodes for their opinions. If
    they all agree with the change, the leader will commit it, and the change will
    be synced to all the nodes. If the leader fails (as in, the node goes down, the
    server process dies, and so on), a new election is triggered. In this case, all
    the remaining manager nodes will vote for a new leader.
  prefs: []
  type: TYPE_NORMAL
- en: This process requires reaching a consensus, with the majority of nodes agreeing
    on the result of the election. If there is no majority, a new election process
    will be triggered until a new leader is elected. After that, the cluster will
    be healthy again. Keep these concepts in mind because they are key in Docker Swarm
    and other orchestrators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents the basic architecture of a Swarm orchestrator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8491519-b7cd-4e8d-b03b-49a057f12859.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's review each plane in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Management plane
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The management plane is the layer where all management tasks run. All cluster
    management traffic and workload maintenance will take place on this plane. The
    management plane provides high availability based on an odd number of manager
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: All communication in this plane is mutually encrypted using TLS (mutual TLS)
    by default. This is where the Raft protocol operates.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This plane manages the state of the cluster. The gossip protocol will periodically
    inform all nodes about the cluster state, reducing the amount of information required
    by nodes to simply having an overview of the health of the cluster. This protocol
    manages host-to-host communications and is called the *control plane* because
    each host communicates only with its closest companions, and information flows
    through this to reach all the nodes within the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Data plane
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data plane manages all the service's internal communications. It is based
    on VXLAN tunneling, encapsulating layer-2 packets within layer-3 headers. It will
    use UDP transport but VXLAN guarantees no dropped packets. We will be able to
    isolate the data plane from the control and management planes using the appropriate
    flags upon Docker Swarm creation (or joining).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we initialize a new Docker Swarm cluster, it generates a self-signed **Certificate
    Authority** (**CA**) and issues self-signed certificates to every node. This ensures
    mutual TLS communication. The following is a summary of the steps taken to ensure
    secure communications when a new node joins the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: When a node joins, it sends the manager its join token, along with a certificate
    request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, if the token is valid, the manager accepts the node's request and sends
    back a self-signed node certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The manager then registers the new node in the cluster and it will appear as
    part of the Docker Swarm cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the node is included in the cluster, it is ready (by default) to accept
    any new workload scheduled by the manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we will learn how to easily deploy a Docker Swarm cluster
    using common Docker command-line actions.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Docker Swarm cluster using the command line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the Docker `swarm` object to initialize a new cluster, join or leave
    a previously created one, and manage all Docker Swarm properties. Let''s take
    a look at the `docker swarm` actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init`: We will use `docker swarm init` to initialize a new cluster or recreate
    an existing one (we will describe this situation in more detail in the *High availability
    with Swarm* section). We will set many cluster options during cluster creation,
    but there are a few that can be changed later. The most important options are
    `--data-path-addr` and `--data-path-port` because they are used to set which node
    interface will be dedicated to the control plane on multi-homed nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the most commonly used arguments for creating the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--advertise-addr`: This option allows us to set which interface will be used
    to announce the cluster. All other nodes will use this interface''s IP address
    to join the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--data-path-addr`/`--data-path-port`: These options configure the interface
    and port used for the control plane. All traffic in this interface will be encrypted
    using TLS, and certificates will be managed internally by Swarm. We can use the
    IP address/port or host interface notation. The default port is `4789`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--external-ca`/`--cert-expiry`: Although Swarm will manage TLS for us, we
    can deploy our own CA for all certificates using this parameter. We can also specify
    how often certificates will be rotated. By default, they will be automatically
    recreated every 90 days (2,160 hours).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--listen-addr`: This option allows us to specify which host interface will
    be used to serve the cluster API. We can use IP address/port or host interface
    notation, with the default of `0.0.0.0:2377`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--autolock`: As we mentioned previously, we can lock access to internal Docker
    Swarm data. This is important because `/var/lib/docker/swarm` contains the CA
    and other certificates. If you are not sure about node access, it is better to
    lock this directory from users. Take care with this option because any system
    or Docker daemon restart will require the unlock key in order to enable this node
    in the cluster again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--dispatcher-heartbeat`: This option will manage how often nodes will report
    their health. It defaults to 5 seconds, but you can change it if your cluster
    has high latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--max-snapshots`/`--snapshot-interval`: Swarm will take database snapshots
    for manager synchronization. We can set the number of snapshots to keep. By default,
    none will be kept (just one for synchronization), but these can be useful for
    debugging or disaster recovery. We can also set the interval between snapshots.
    Take care when changing this option because having many snapshots will trigger
    a lot of sync operations to other nodes and can incur high-performance costs.
    But on the other hand, syncing less frequently can guide the cluster to non-synced
    states. This parameter defaults to 10,000 ms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join`: After cluster initialization, all the other nodes will join the previously
    created cluster, regardless of whether they are managers or workers. Joining Docker
    nodes to a cluster requires a cluster-specific token, with different tokens for
    the manager and worker nodes. We will always require a token and the leader IP
    address to join the cluster. Remember that the leader can change from time to
    time. We will also be able to set the control plane''s IP and port, the IP address
    to be announced to the other nodes, and the listen IP address for the API. We
    will execute this command on the joining node using the following format: `docker
    swarm join --token <MANAGER_OR_WORKER_TOKEN> <LEADER_IP:PORT>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leave`: Once a node is part of the cluster, it can leave it whenever we need
    it to. It is important to understand what it means to *leave* the cluster. The
    `leave` command will be executed on the node leaving the cluster. Manager nodes
    are not able to leave the cluster because this would force the cluster into an
    unhealthy state. We can use `--force` to make a node leave the cluster, even if
    it is a manager node, but this comes with risks that you need to understand before
    proceeding. Leaving the cluster will not remove the node from the internal Docker
    Swarm database. Rather, we need to inform the managers of this change by issuing
    `docker node rm <NODE_NAME_OR_ID>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update`: With this action, we can change some of the Docker Swarm cluster''s
    described properties, such as the external CA, certificate expiration settings,
    and snapshot behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ca`: As we mentioned previously, all internal control plane communication
    is based on TLS certificates. The `ca` option allows us to customize the CA and
    other certificate behavior. We can rotate them or choose our own CA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join-token`: With this action, we can review the current tokens for managers
    and workers. In fact, we can execute `join-token`, followed by the required role,
    to retrieve their values. We do not need to keep them safe since we can retrieve
    them as needed. These tokens are only used when joining the cluster. We can change
    them whenever we want, using `docker swarm join-token --rotate` to create a new
    one. This will not affect already joined nodes. We usually execute `docker swarm
    join-token worker` to retrieve the command line and token required to join the
    node to the cluster. We can use `--quiet` to retrieve only the token, which is
    useful for automating the joining process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unlock`/`unlock-key`: We mentioned previously that it is unsafe to allow users
    to access the `/var/lib/docker` directory. Access is only allowed to root by default,
    but it is even more secure to lock Docker Swarm information. For example, all
    cluster certificates will be stored under the `/var/lib/docker/swarm/certificates`
    directory. Locking Swarm information is a good practice, but be aware of losing
    your unlock key. Every time the cluster node starts (such as a Docker Engine or
    node restart, for example), the unlock key will be required. This leaves the cluster
    in a non-automatic, high-availability environment in some situations. The `unlock`
    option unlocks the Docker Swarm cluster information, while `unlock-key` allows
    us to manage the defined key used for this behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Swarm will also create new objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`swarm`: This is the cluster itself, along with its own properties, as described
    earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node`: These are the nodes that are part of the cluster. We will add labels
    to them and manage their roles as part of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service`: We deploy services on our Docker Swarm cluster. We won''t deploy
    a standalone container. We will learn more about services in the *Scheduling workloads
    in the cluster – tasks and services* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`secret` and `config`: Both objects allow us to share service configurations
    in the cluster. Remember, it is not easy to manage information on different hosts,
    even if the application is completely stateless.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stack`: We will use stacks to deploy applications. We will use a Docker Compose-like
    file format containing all application components and their interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these objects will have common actions associated with them, including listing,
    deploying/creating, removing, and inspecting their properties. Services and stacks
    will have containers associated with them, so we will be able to list their processes'
    distributions cluster-wide.
  prefs: []
  type: TYPE_NORMAL
- en: We can run a single node cluster on our laptop. It is not a problem running
    a single node cluster for testing or developing services or stacks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to deploy a Docker Swarm environment
    with high availability.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Docker Swarm with high availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned about the different roles in Docker Swarm clusters.
    However, in order to provide high availability, we will need to deploy more than
    one manager and worker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Raft consensus algorithm requires an odd number of healthy nodes to work
    because a majority of the nodes must agree on all the changes and resources states.
    This means that we will need at least *N/2+1* healthy nodes to agree before committing
    a change or resource state. In other words, we will not grant Docker Swarm availability
    if fewer than *N/2+1* manager nodes are healthy. Let''s review the options in
    the following table to get a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of managers** | **Required for consensus (*N/2+1)*** | **Allowed
    failures** | **Provides high availability?** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | No. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 0 | No. |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2 | 1 | Yes. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 | 1 | Yes, but this is not better than the three-manager option and
    can lead to election problems if the leader fails. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 | 2 | Yes. |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 4 | 2 | Yes, but this is not better than the five-manager option and
    can lead to election problems if the leader fails. |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 4 | 3 | Yes. |'
  prefs: []
  type: TYPE_TB
- en: When a manager fails in the 3-manager configuration, two nodes can agree and
    changes will be updated without problems. But if one of those fails, only one
    will be left and changes can't be committed. There is no consensus and no cluster
    operations can be deployed. This means that any service deployed in the cluster
    will stay running. Users will not be affected unless one service loses some replicas
    and Docker Swarm should have started new ones to achieve the required number.
    No automatic actions will be allowed because these have to update the database
    data, and this is not permitted. We will not be able to add or remove any nodes
    in that situation, so the cluster will be inconsistent.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, Swarm requires an odd number of managers to provide high availability.
    Although there is no limit regarding the number of manager nodes, more than seven
    is not recommended. Increasing the number of managers will reduce write performance
    because the leader will require more acknowledged responses from more nodes to
    update cluster changes. This will result in more round-trip network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: It is key to understand these behaviors. Even if we have deployed a three-node
    cluster, we can still lose quorum if a sufficient number of nodes become unhealthy.
    It is important to attend to node failures as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: We will usually deploy three-node clusters because they allow for the failure
    of 1 node. It is enough for production, but in some critical environments, we
    will deploy five-node clusters to allow for two node failures.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where a Swarm cluster needs to be distributed between different locations,
    the recommended number of managers is seven. It will allow distribution across
    multiple data centers. We will deploy three nodes in the first data center, two
    in the second data center, and a final two in the third data center (3+2+2). This
    distribution will allow us to handle a full data center failure with services
    being redistributed if worker nodes have sufficient resources.
  prefs: []
  type: TYPE_NORMAL
- en: What happens when a manager node fails? The leader will start to store committed
    changes in order to sync the unhealthy manager node when it is ready again. This
    will increase Docker Swarm's directory size. If you did not set your node disk
    space sufficiently to allow for these situations, your nodes will probably consume
    your entire filesystem if the failure doesn't recover soon. And then, you will
    get a second unhealthy node and your cluster will be inconsistent. This situation
    we've described is not a horror movie – it happens all too often on new installations
    where administrators think that the cluster will be alright with some unhealthy
    nodes for weeks at a time.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned one important option in the `docker swarm` command-line table when
    we talked about Docker Swarm cluster initialization. We will use `docker swarm
    init --force-new-cluster` in situations where the cluster is unhealthy, but at
    least one manager is working. If the cluster isn't quorate and no operations can
    be performed with cluster resources (that is, nodes can't be added/removed and
    services won't be repaired if they fail), we can force a new cluster. This is
    an extreme situation.
  prefs: []
  type: TYPE_NORMAL
- en: Take care of your environment before recreating the cluster. Forcing a new cluster
    will set the node where the command was executed as the leader. All other nodes
    in the cluster (including those managers that were insufficient for a quorum)
    will be set as workers. It is like a **cluster quorum reset**. Services and other
    resources will retain their states and configurations (as they were committed
    or retrieved from nodes). Therefore, we will end up with a one-manager node cluster
    with all the other nodes as workers. Services and other stuff should not be affected.
    In these situations, it is a good practice to review the manager node's logs because
    some containers can be left unmanaged if some cluster changes were not committed.
  prefs: []
  type: TYPE_NORMAL
- en: Although managers can act as workers, it is safer in production to run workloads
    on worker-role nodes only. A manager's processes may impact the application and
    vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: We will always deploy more than one worker in production environments. This
    will ensure the health of our services if one of the workers goes offline unexpectedly
    or if we need to perform any maintenance tasks, such as updating Docker Engine.
    We should usually deploy worker nodes according to our application's resource
    requirements. Adding workers will increase the total cluster workload capacity.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to deploy a Docker Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker Swarm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have reviewed the Docker Swarm architecture and the command-line
    actions required to initialize the cluster, we can create a cluster. By the end
    of this chapter, we will have a fully functional cluster with high availability.
    Let''s start by reviewing the Docker Swarm cluster creation process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize a Swarm cluster on a manager node. This node automatically
    becomes the cluster leader because no other manager is available. If we have a
    node with multiple interfaces, we will choose which interface will be associated
    with the control plane and which ones will be announced for other nodes and the
    Swarm API. The output will vary from the following in your environment. Let''s
    execute `docker swarm init`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the cluster has been created, we can review the cluster nodes and their
    properties by using `docker node ls`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first column shows the node object identifier. As we mentioned previously,
    new objects have been created with Docker Swarm. The second column shows its name
    from the internal host resolution service (this may contain a **Fully Qualified
    Domain Name** (**FQDN**)). Notice the asterisk near the hostname. This means that
    we are working on this node right now. All the commands are executed on that node,
    regardless of whether it is a leader.
  prefs: []
  type: TYPE_NORMAL
- en: On Docker Swarm, cluster commands related to cluster-wide objects are only available
    on manager nodes. We won't need to execute commands on the leader node, but we
    won't be able to execute any cluster commands on a worker node. We can't list
    nodes or deploy a service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last column shows each node''s Docker Engine version. Let''s take a look
    at the `STATUS`, `AVAILABILITY`, and `MANAGER STATUS` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`STATUS`, as its name suggests, shows the status of the node within the cluster.
    If it is not healthy, it will be shown here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MANAGER STATUS` shows the current role of the node (in this case, the node
    is the leader). We have three different states:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Leader`, when the node is the cluster leader.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Manager`, which means that the node is one of the cluster managers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An empty value will mean that the node has a worker role, and is therefore not
    part of the control plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AVAILABILITY` represents a node''s availability to receive workloads. Here,
    we can see that managers can receive workloads too. We can set this node property.
    In fact, there are three different states:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active`, which means that the node will be able to receive any workload.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`passive`, which means that the node will not run any other additional workload.
    Those already running will maintain their state, but no additional workloads will
    be allowed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drain` is the state that we get when we disable any workload on this node.
    When this happens, all running workloads on the node will be moved to any other
    healthy and available node.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can enforce the behavior of any node when joining the cluster, or even when
    we create the cluster, using the `--availability` flag with `docker swarm init`
    or `docker swarm join`. We will set the node availability for new workloads (`active`
    | `pause` | `drain`). By default, all the nodes will be active and ready to receive
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will join another node as a worker to demonstrate this, using the previously
    shown cluster initialization output with `docker swarm join`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can review the cluster node status (remember, this command will only
    be available on manager nodes) once more by executing `docker node ls`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are executing commands on the `sirius` node (marked with
    `*`), which is a leader and hence a manager. Notice that `antares` is a worker
    node because it has an empty value in the `MANAGER STATUS` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can review node information by executing the `docker node inspect` action
    (the following output is truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When we inspect a node, information regarding its status, node IP address, and
    TLS information will be shown in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: We can use labels on nodes to help Docker Swarm choose the best location for
    specific workloads. It uses node architectures to deploy workloads in the right
    place, but if we want a workload to run on a specific node, we can add a unique
    label and add a constraint to deploy the workload. We will learn more about service
    locations and labels in the *Chapter labs* section.
  prefs: []
  type: TYPE_NORMAL
- en: Under the `Spec` key, we can review the node role in the `docker node inspect`
    output. We can change the node role whenever necessary. This is a big improvement
    over other orchestrators, where roles are static. Keep in mind that role changes
    will affect your Docker Swarm architecture because it will change the number of
    managers and worker nodes. Keep high availability in mind, its requirement of
    an odd number of managers, and the consequences of this in case of node failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'A role is just a node property, which means we can change it just like any
    other object property. Remember that changes can only be deployed from manager
    nodes. We change a node''s role by executing `docker node update`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, let''s list all the nodes in the cluster by executing `docker node
    ls`, this time with a filter to retrieve only managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use `docker node inspect` to retrieve the `ManagerStatus` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Nodes can be removed from the cluster by using `docker node rm`, just as we
    did with other Docker objects. We will only remove worker nodes. The usual sequence
    for removing a manager node from a Docker Swarm cluster will require a previous
    step to change its role to a worker. Once a node role has changed to a worker,
    we can remove the node. If we need to remove a failed manager, we can force node
    removal using `--force`. However, this is not recommended as you can leave the
    cluster in an inconsistent state. The manager's database should be updated before
    you remove any node, which is why the removal sequence we've described here is
    so important.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to make sure that you have an odd number of manager nodes if you demote
    or remove any manager. If you have problems with the leader when you do not have
    an odd number of managers, you can reach an inconsistent state when other managers
    have to elect a new leader.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, labels are node properties. We can add and remove
    them at runtime. This is a big difference compared to the labels learned about
    in [Chapter 1](c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml), *Modern Infrastructures
    and Applications with Docker*. Those labels were set at the Docker daemon level
    and are static. We needed to add them to the `daemon.json` file, so we were required
    to restart the node's Docker Engine to make them effective. In this case, however,
    node labels are managed by Docker Swarm and can be changed with the common node
    object's `update` action (`docker node update`).
  prefs: []
  type: TYPE_NORMAL
- en: The Docker command line provides some shortcuts, as we have observed in previous
    chapters. In this case, we can change node roles by demoting a manager to a worker
    role, or by promoting a worker to a manager role. We use `docker node <promote|demote>
    <NODENAME_OR_ID>` to change between node roles.
  prefs: []
  type: TYPE_NORMAL
- en: We can also change a node's workload availability. This allows a node to receive
    (or not) cluster-deployed workloads. As with any other node property, we will
    use `docker node update --availability <available|drain|pause> <NODENAME_OR_ID>`
    to drain or pause a node when it was active. Both drain and pause will prevent
    us from scheduling any new workload on the node, while drain on its own will remove
    any currently running one from the affected node.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that when we drain a node, the scheduler will reassign any tasks running
    on the affected node to another available worker. Keep in mind that the other
    nodes should have enough resources before draining the given node.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will review how to back up and recover a faulty Docker
    Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering a faulty Docker Swarm cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will review a few steps to back up and restore Docker Swarm clusters. Losing
    your cluster quorum is not a big problem. As we have learned, we can recover the
    cluster by forcing the initialization of a new one, even with just one healthy
    manager. However, losing your cluster data will completely destroy your environment
    if you don't have any manager nodes that are operational and working correctly.
    In these situations, we can recover the cluster by restoring a copy containing
    healthy data that was taken when the cluster was running correctly. Let's learn
    how to take backups of our clusters now.
  prefs: []
  type: TYPE_NORMAL
- en: Backing up your Swarm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we learned in this chapter, `/var/lib/docker/swarm` (and its Microsoft Windows
    equivalent directory) contains the key-value store data, the certificates, and
    the encrypted Raft logs. Without them, we can't recover a faulty cluster, so let's
    back up this directory on any manager.
  prefs: []
  type: TYPE_NORMAL
- en: Having a consistent backup requires static files. If files are opened or some
    process is writing them, they will not be consistent. Therefore, we need to stop
    Docker Engine on the given node. Do not launch the backup procedure on the leader
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that while the backup operation is running, if the Docker daemon
    is stopped, the number of managers will be affected. The leader will continue
    managing changes and generating new sync points to recover synchronization with
    the lost manager. Your cluster will be vulnerable to losing quorum if other managers
    fail. If you plan to do daily backups, consider using five managers.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering your Swarm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In case we need to recover a completely failed cluster (where no managers can't
    achieve quorum and we can't force a new cluster), we will stop Docker Engine on
    one manager. Remove all `/var/lib/docker/swarm` directory content (or its Microsoft
    Windows equivalent) and restore the backed-up content to this directory. Then,
    start Docker Engine again and reinitialize the cluster with `docker swarm init
    --force-new-cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: When the single-manager cluster is healthy, start to add the other old Swarm
    cluster managers. Before adding those managers, ensure that they've left the old
    Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If we set up Swarm auto-lock, we will need the key that was stored with the
    restored backup. Even if you changed it after the backup was issued, you will
    still need the old one.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how workloads are deployed on the cluster
    and how Docker Swarm tracks the health of application components to ensure that
    services are not impacted when something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling workloads in the cluster – tasks and services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We don't run containers on a Swarm cluster; rather, we deploy services. These
    are atomic workloads that can be deployed in a Docker Swarm cluster. Services
    are defined by tasks, and each task is represented by a container in the Docker
    Swarm model. Swarm is based on SwarmKit and its logic is inherited. SwarmKit was
    created as a response to clustering any kind of task (such as virtual machines,
    for example), but Docker Swarm works with containers.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Swarm orchestrator uses a declarative model. This means that we define
    the desired state for our services and Docker Swarm will take care of the rest.
    If the defined number of replicas or tasks for a service is wrong – for example,
    if one of them died – Docker Swarm will take action to recover the correct state
    of the service. In this example, it will deploy a new replica to keep all the
    required nodes healthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents services and tasks in relation to containers.
    The `colors` service has five replicas (`colors.1` to `colors.5`). Each replica
    runs on one container from the same image, `codegazers/colors:1.13`, and these
    containers run distributed cluster-wide across `node1`, `node2`, and `node3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a777ffe3-5e76-482f-9bb3-415cbbed3f62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Service creation requires the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Which image will run the associated containers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many containers does this service require to be healthy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should the service be available to users on any port and protocol?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should service updates be managed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there any preferred location for this service to run?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Service creation will require all this information to be entered on the command
    line. Since services are Docker objects, we can use common actions such as listing,
    creating, removing, updating, and inspecting their properties. Docker Swarm will
    manage all our tasks'' integration with services. We will never deploy tasks or
    containers. We will just create and manage services. Let''s take a look at Docker
    command-line actions and options related to services:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create`: This is common to other objects, but services have many non-standard
    properties. We will not list and review all service arguments because most of
    them are inherited from containers. Here, we''ll review the most important ones
    related to service behavior:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--config`: We can create a service configuration only, not a real service.
    This will create all service environments and requirements but without running
    any task.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--container-label`/`--label`: We added this option here because it is important
    to understand that services and containers are different objects and that we can
    add labels to both. By default, Docker Swarm will create many labels on each service
    container to relate them to each other. We can easily use those labels to filter
    information regarding our services'' containers on any host.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--constraint`/`--placement-pref`: As we mentioned previously, we can specify
    which nodes should run a given service''s tasks. We use a list of key-value pairs
    as constraints to do this. All defined keys must be fulfilled to schedule the
    service''s tasks on a given node. If no node satisfies the defined constraints,
    the tasks will not be run because Docker Swarm''s scheduler will not find any
    node with those requirements. On the other hand, `placement-pref` will provide
    a placement preference. This will not limit which nodes will run the tasks, but
    we can use this to spread our services'' tasks across different nodes using a
    defined key. For example, we might distribute a given service''s tasks across
    different physical locations (such as data centers).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--mode`: There are two different service modes (in fact, there are three,
    as we will find out later in the *Networking in Docker Swarm* section, but at
    this point, just keep the following two in mind). By default, all services will
    use replication mode. This means that we will set a number of replicas to be healthy
    (by default, this is one replica). We also have global services. In this case,
    we will create as many replicas as nodes in the cluster, but we will just run
    one replica per node. This mode is very interesting for monitoring applications,
    for example, because all the nodes will receive their monitoring process. One
    important thing about these services is that every node that gets into the cluster
    will receive its own replica. Docker Swarm will deploy it on the new node for
    us automatically.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--with-registry-auth`: This is a very important option because it allows us
    to distribute credentials among cluster nodes so that we can use private images.
    It is also important to understand that Docker Swarm requires external or internal
    registries to work. We will not work with local images on cluster nodes anymore.
    Local images will lead to inconsistent deployments because image names can match,
    while content could be completely different across nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--endpoint-mode`: This option sets how services announce or manage their tasks.
    We can use `vip` and `dnsrr` for this. Services will default to `vip`, so each
    service will receive a virtual IP associated with its name, and an internal load
    balancer will route traffic to each replicated process (container/task) associated
    with it. On the other hand, `dnsrr` will use internal name resolution to associate
    each replica IP address whenever we ask for a service name. This way, internal
    name resolution will give us a different IP address when a given service has been
    deployed with more than one task.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--network`: We can attach new services to an existing network. As we did with
    containers, we can also use a host network namespace. The difference here is that
    we can''t execute privileged services, so our services will have to expose ports
    numbered higher than `1024`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--publish`: We will use this option to publish ports externally. Docker Swarm
    will expose ports using Docker Swarm''s router mesh on every node. If external
    requests arrive on a host that does not execute any service tasks, Docker Swarm
    will internally reroute requests to an appropriate node.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--replicas`/`--replicas-max-per-node`: Services are defined by how many replicas
    or tasks are deployed to maintain their healthy state. By default, all services
    deploy one single replica. As we will see later, we can change the number of replicas
    at any time we need. Not all application components (processes) will work well
    if we scale up or down their replicas. Imagine, for example, a SQL database. It
    is a completely stateful component because the database process will write data.
    If we add a new database replica accessing the same storage, the database will
    become corrupted. If each database replica has its own storage, they will manage
    different data. As a result, not all services can be scaled up or down.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--reserve-cpu`/`--reserve-memory`: We can reserve the amount of resources
    required for a service to work. If no node presents enough resources, it will
    not be scheduled.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--update-delay`/`--update-failure-action`/`--update-max-failure-ratio`/`--update-monitor`/`--update-order`/`--update-parallelism`:
    `update` options manage how changes are executed for a service. We will set how
    many services'' tasks will be updated at once, how many seconds we will wait between
    instances'' updates, and how the update process will be done. The `--update-order`
    option sets how this update process will be executed. By default, the running
    container will be stopped and a new one will be created after the old one is completely
    finished. With this setting, the service will be impacted. We can set a different
    order by starting a new container first. Then, once everything is fine, the old
    one will be stopped. This way, the service will not be impacted, but your application
    process must be able to allow for this situation. For example, it will not work
    on a standard SQL database, as we mentioned previously. We will also set what
    to do when some of the updates fail, either by executing an automatic rollback
    or by pausing the rest of the service updates until manual action is taken.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rollback-delay`/`--rollback-failure-action`/`--rollback-max-failure-ratio`/`--rollback-monitor`/`--rollback-order`/`--rollback-parallelism`:
    If the update process goes wrong, we can set an automatic rollback. These settings
    modify how rollbacks will be done. We have the same options we reviewed for the
    `update` process, but this time, the arguments will refer to the `rollback` process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ps`: With this, we can review all our service''s tasks and their distributions
    in the cluster. We can also use filters and output format. We will see a couple
    of examples of this in the *Chapter labs* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logs`: This is a very useful action because Docker Swarm will retrieve the
    logs of all our tasks for us. We can then review them from the manager command
    line instead of going to wherever the tasks were running to read the container''s
    logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update`: Service properties can be updated. For example, we can change image
    release versions, publish new ports, and change the number of replicas, among
    other things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rollback`: With this, we can return to the service''s previous properties.
    It is important to understand that images from previous executions should be kept
    in our hosts to allow for the application''s rollbacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect`/`ls`/`rm`: These are the common actions we encounter with all other
    kinds of objects. We''ve already learned how to use them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that privileged containers are not allowed on services.
    Therefore, if we want to use the host network namespace, container processes should
    expose and use non-privileged ports (higher than `1024`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker service constraints can be set with custom labels, but there are some
    internal ones that are created by default that are very useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Label** | **Attribute** |'
  prefs: []
  type: TYPE_TB
- en: '| `node.id` | Node ID |'
  prefs: []
  type: TYPE_TB
- en: '| `node.hostname` | Node hostname; for example, `node.hostname==antares` |'
  prefs: []
  type: TYPE_TB
- en: '| `node.role` | Node Swarm role; for example, `node.role!=manager` |'
  prefs: []
  type: TYPE_TB
- en: '| `node.labels` | Swarm node-assigned labels; for example, `node.labels.environment==production`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `engine.labels` | Docker Engine-defined labels; for example, `engine.labels.operatingsystem==ubuntu
    18.04` |'
  prefs: []
  type: TYPE_TB
- en: 'We can use variables to define service properties. In the following example,
    we''re using internal Docker Swarm variables in the container''s hostname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To summarize, before continuing with other Swarm resources: services are a
    group of tasks, each executing one container. All these containers run together
    to maintain the service''s state. Docker Swarm will monitor the service''s state
    and if one container dies, it will run a new container to maintain the number
    of instances. It is important to note that a container''s IDs and names will change.
    However, while new tasks can be created, the task''s name will not be changed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at a quick example before moving on to the next topic. We
    will create a simple NGINX web server service using `docker service create`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can review where the created task is running by using `docker service ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we move to the node where the task is running. Once there, we kill the
    associated container using `docker container kill`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, a new task will be created automatically with a new container.
    The task name hasn''t changed, but it is a new task, as we can tell from its ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can review some of the labels that were created by Swarm to fully
    identify containers using their services. We use `docker container inspect` for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: There are some service options that can be set using strings to help us identify
    their configuration and other associated resources. This is very important when
    we need to isolate resources attached to a specific service's tasks or use some
    special information to access other services, such as the container's hostname.
    We can use labels to add meta-information to containers, but there are also Docker
    Swarm-defined variables that we can use within strings. These variables use Go's
    template syntax (as we also learned when formatting the listing command's output)
    and can be used with `docker service create` and the `--hostname`, `--mount`,
    and `--env` arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can set an associated service container's hostname to be unique
    between tasks using these variables; for example, `--hostname="{{.Service.Name}}-{{.Task.ID}}"`.
    We can even use the node's name to identify this task with the node in which it
    is running using `--hostname="{{.Node.Hostname}}"`. This can be very useful with
    global services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a quick list of valid service template substitutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service**: `.Service.ID`, `.Service.Name`, and `.Service.Labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node**: `.Node.ID` and `.Node.Hostname`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task**: `.Task.ID`, `.Task.Name`, and `.Task.Slot`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will introduce some new Docker Swarm objects that will
    help us deploy our applications on clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications using Stacks and other Docker Swarm resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about other Docker Swarm objects that will help
    us to fully deploy applications within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We've already learned how to configure applications using environment variables.
    This is not recommended for production because anyone with system Docker access
    can read their values. To avoid this situation, we will use external data sources.
    We also learned how to integrate host resources inside containers. We can set
    configurations and passwords in files shared between hosts and containers. This
    will work on standalone environments but not for distributed workloads, where
    containers can run on different hosts. We will need to sync those files on all
    cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid syncing files on multiple nodes, Docker Swarm provides two different
    objects for managing them. We can have private files or secrets and configurations.
    Both objects store their values in the Swarm key-value store. Stored values will
    be available for every cluster node that requires them. These objects are similar,
    but secrets are used for passwords, certificates, and so on, while config objects
    are used for application configuration files. Now, let's examine them in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A secret is a blob of data that contains passwords, certificates, and any other
    information that should not be shared over the network. They will be stored in
    an encrypted fashion so that they're safe from snoopers. Docker Swarm will manage
    and store secrets for us. Because this kind of data is stored in the key-value
    store, only managers will have access to any secrets we create. When a container
    needs to use that stored secret, the host responsible for running that container
    (a service task container) will have access too. The container will receive a
    temporal filesystem (in-memory `tmpfs` on Linux hosts) containing that secret.
    When the container dies, the secret will not be accessible on the host. Secrets
    will only be available to running containers when they are required.
  prefs: []
  type: TYPE_NORMAL
- en: Since secrets are Docker Swarm objects, we can use all of the usual actions
    (`list`, `create`, `remove`, `inspect`, and so on). Do not expect to read secret
    data with the `inspect` action. Once created, it is not possible to read or change
    a secret's content. We create secrets with files or by using standard input for
    data. We can add labels for easy listing in big cluster environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a secret has been created, we can use it within our services. We have
    both short and long notations. By default, using the short format, a file with
    secret data will be created under `/run/secrets/<SECRET_NAME>`. This file will
    be mounted in a `tmpfs` filesystem on Linux. Windows is different because it does
    not support on-memory filesystems. We can use the long format to specify the filename
    to be used for the secret file under `/run/secrets`, along with its ownership
    and file permissions. This will help us avoid root usage inside the container
    in order to access the file. Let''s create a secret with `docker secret create`
    and then use it on a service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned previously, it is not possible to retrieve secret data. We
    can inspect previously created secrets using the common `docker secret inspect`
    action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will learn about configuration objects.
  prefs: []
  type: TYPE_NORMAL
- en: Config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Config objects are similar to secrets, but they aren't encrypted on the Docker
    Swarm Raft log and will not be mounted on a `tmpfs` filesystem in containers.
    Configs can be added or removed while service tasks are running. In fact, we can
    even update service configurations. We will use these objects to store configurations
    for applications. They can contain strings or binaries (up to 500 KB, which is
    more than enough for configurations).
  prefs: []
  type: TYPE_NORMAL
- en: When we create a config object, Docker Swarm will store it in the Raft log,
    which is encrypted, and it will be replicated to other managers by mutual TLS.
    Therefore, all the managers will have the new config object value.
  prefs: []
  type: TYPE_NORMAL
- en: Using config files on services requires there to be a mount path inside the
    containers. By default, the mounted configuration file will be world-readable
    and owned by the user running the container, but we can adjust both properties
    should we need to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a quick example. We will create a configuration file using `docker
    config create` and then use it inside a service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we can review the config content and see that it is readable.
    Using `docker config inspect`, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Let's move on to stacks.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stacks help us deploy complete applications. They are **Infrastructure-as-Code**
    (**IaC**) files with all their component definitions, their interactions, and
    the external resources required to deploy an application. We will use `docker-compose`
    file definitions (`docker-compose.yaml`). Not all `docker-compose` file primitive
    keys will be available. For example, `depends_on`will not be available for stacks
    because they don't have dependency declarations. This is something you have to
    manage in your own application logic.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned with the `docker-compose` command in [Chapter 5](1c86479c-e4f5-4508-9eca-d29bb3dbaf4b.xhtml),
    *Deploying Multi-Container Applications*, every application that's deployed will
    run by default in its own network. When using stacks on Docker Swarm, application
    components are deployed cluster-wide. Overlay networks will be used because each
    component should reach others, regardless of where they are running. Stacks will
    also be deployed on their own networks by default.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks deploy applications based on services. Therefore, we will keep our service
    definitions in the `docker-compose` file. To be able to identify these services
    from other stacks, we will set the stacks' names.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that `docker-compose` will deploy multi-container
    applications on one Docker Engine, while `docker stack` will deploy multi-service
    applications on a Swarm cluster. Note that, nonetheless, both use the same kind
    of IaC file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a quick look at the `docker stack` command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`deploy`: Deploying Stacks requires a `docker-compose` file version of 3.0
    and above. We will use the `deploy` action to create and run all application components
    at once. It is also possible to use a Docker Application Bundle file, which is
    something that will not be covered in this book, but it is good to know that we
    have multiple options with Docker Stacks for deploying applications on Docker
    Swarm. As we mentioned previously, we will need to name our stack''s deployment
    to fully identify all its components within the cluster. All of the stack''s resources
    will receive the stack''s name as a prefix unless they were externally created
    from the stack''s file definition. In this latter case, they will retain their
    original names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the main options for `docker stack deploy`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--compose-file`/`-c`: We use `docker-compose.yaml` as the stack definition
    file unless we specify a custom filename with this option.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--orchestrator`: This option was recently added and allows us to choose which
    orchestrator will deploy and manage the stack. We will be able to choose between
    Docker Swarm and Kubernetes when both are available in our environment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--with-registry-auth`: As we learned with services, sharing authentication
    is vital when using private registries. Without this option, we can''t ensure
    all the nodes are using the same image or that they have access to the registry
    because this will depend on locally stored authentication.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`services`: The `services` option shows us a list of the deployed stack''s
    services. As with all other listing actions, we can format and filter its output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ps`: This action lists all the services and where tasks were deployed. It
    is easy to filter and format its output, as we will see in the *Chapter labs*
    section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ls`/`rm`: These are common object actions for listing and removing them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is not much more to say about stacks. IaC requires that every deployment
    is reproducible. Even for a simple standalone service, make sure to use a stack
    file to deploy it. The *Chapter l**abs* section will cover these actions and options
    with some more examples. In the next section, we will learn how Swarm can change
    application networking cluster-wide.
  prefs: []
  type: TYPE_NORMAL
- en: Networking in Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we talk about Docker Swarm, we need to introduce a new concept regarding
    networks: *overlay* networks. As we mentioned at the beginning of this chapter,
    a new network driver will be available because Docker Swarm will distribute all
    application components across multiple nodes. They have to be reachable no matter
    where they run. The overlay network will work over VXLAN tunnels using the **User
    Datagram Protocol** (**UDP**). We will be able to encrypt this communication,
    but some overhead should normally be expected.'
  prefs: []
  type: TYPE_NORMAL
- en: The overlay network driver will create a distributed network across cluster
    nodes and automatically provides routing of packets to interconnect distributed
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'When Swarm is first initialized, two networks are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker_gwbridge`: This bridge network will connect all Docker daemons that
    are part of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ingress`: This is an overlay network that will manage Docker Swarm services''
    control and data traffic. All the services will be connected to this network so
    that they can reach each other if we do not specify any custom overlay network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm will only manage overlay networks. We can create new overlay networks
    for our applications that will be isolated from each other. The same happens when
    working locally with custom bridged networks. We will be able to connect services
    to different networks at once, as we did with bridged environments. We will also
    be able to connect containers to overlay networks, although this is not something
    that's commonly done. Remember that we will not run standalone containers in Docker
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'If firewalls are enabled in your environment, you''ll need to allow the following
    traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Port or Range of Ports** | **Protocol** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| **2377** | TCP | Cluster management traffic |'
  prefs: []
  type: TYPE_TB
- en: '| **7946** | TCP/UDP | Swarm node intercommunication |'
  prefs: []
  type: TYPE_TB
- en: '| **4789** | UDP | Overlay networking |'
  prefs: []
  type: TYPE_TB
- en: Docker Swarm management traffic is always encrypted by default, as we learned
    in previous sections. We can also encrypt overlay networking. When we use encryption
    arguments on overlay network creation, Docker Swarm creates **Internet Protocol
    Security** (**IPSEC**) encryption on overlay VXLANs. It adds security, though
    a performance overhead is to be expected. It is up to you to manage the balance
    between security and performance in your applications. As encryption is done upon
    network creation, it can't be changed once the network has been created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating overlay networks is easy – we just specify the overlay driver with
    `docker network create`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, it is created unencrypted and non-attachable. This means that containers
    will not be able to connect to this network. Only services will be allowed. Let''s
    verify this by trying to attach a simple container to the created network using
    `docker container run`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid this, we need to declare the network as attachable from the very beginning.
    This second example also adds an encryption option using `docker network create
    --attachable --opt encrypted`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We connected to the newer sample encrypted network without any problem because
    it was created with the `attachable` property.
  prefs: []
  type: TYPE_NORMAL
- en: All services that are connected to the same overlay network will see each other
    by their names, and all their exposed ports will be available internally, regardless
    of whether they are published.
  prefs: []
  type: TYPE_NORMAL
- en: By default, all Swarm overlay networks will have 24-bit masks, which means we
    will be able to allocate 255 IP addresses. Each service that's deployed may consume
    multiple IP addresses, as well as one for each node peering on a given overlay
    network. You may run into IP exhaustion in some situations. To avoid this, consider
    creating bigger networks if many services need to use them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a closer look at service discovery and how
    Docker routes traffic to all service replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery and load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker Swarm has internal **Internet Protocol Address Management** (**IPAM**)
    and **Domain Name System** (**DNS**) components to automatically assign a virtual
    IP address and a DNS entry for each service that's created. Internal load balancing
    will distribute requests among a service's tasks based on the service's DNS name.
    As we mentioned earlier, all the services on the same network will know each other
    and will be reachable on their exposed ports.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm managers (in fact, the leader) will use the created ingress overlay
    network to publish the services we declared as accessible from outside the cluster.
    If no port was declared during service creation, Docker Swarm will automatically
    assign one for each exposed port that's declared in the `30000`-`32767` range.
    We have to manually declare any port above `1024` because we can't create privileged
    services.
  prefs: []
  type: TYPE_NORMAL
- en: All the nodes will participate in this ingress router mesh. Therefore, the nodes
    will accept connections on the published port, regardless of whether they run
    one of the requested tasks. The router mesh will route all incoming requests to
    published ports on all the nodes to running tasks (containers). Therefore, published
    ports will be allocated on all Swarm nodes and hence only one service will be
    able to use declared ports. In other words, if we publish a service on port `8080`,
    we will not be able to reuse that port for another service. This will limit the
    maximum number of services that can run on the cluster to the number of free ports
    available on the Linux or Windows systems used. We learned that Docker Engine
    will not be able to publish more than one container on the same port using NAT.
    In this case, all the nodes will fix ports to published services.
  prefs: []
  type: TYPE_NORMAL
- en: The router mesh listens on the published ports on all the node's available IP
    addresses. We will use cluster-external load balancers to route traffic to the
    cluster's hosts. We usually use a couple of them for publishing, with the load
    balancer forwarding all requests to them.
  prefs: []
  type: TYPE_NORMAL
- en: We can use `docker service update` to modify or remove already declared ports
    or add new ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema shows how a router mesh works on a three-node cluster
    publishing a service with two replicas. The colors service runs two tasks. Therefore,
    one container runs on NODE1 and NODE2, respectively (these are Docker Swarm-scheduled
    tasks on the nodes in the following diagram). Internally, these containers expose
    their application on port `3000`. The service that defined that container''s port
    as `3000` will be published on the host''s port; that is, `8080`. This port will
    be published on all the nodes, even if they do not run any service tasks. Internal
    load balancing will route requests to the appropriate containers using the ingress
    overlay network. Finally, users will access the published service through an external
    load balancer. This is not part of the Docker Swarm environment, but it helps
    us to provide high-availability forwarding requests to a set of available nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7af011c5-ed04-4aa2-b68d-5c7e7d199f86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will have short and long formats for publishing services. Long formats always
    provide more options. In the following example, we''re publishing an NGINX process
    on cluster port `8080` and forwarding its traffic to the container''s port, `80`,
    using `docker service create --publish`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'On any node, we will be able to access the NGINX service on port `8080`. We
    can test this using the `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can retrieve the current service tasks' IP addresses by querying the DNS
    for `tasks.<SERVICE_NAME>`.
  prefs: []
  type: TYPE_NORMAL
- en: By default, all the services use the router mesh. However, we can avoid this
    default behavior, as we will see in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Bypassing the router mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using host mode or a **Round-Robin** **DNS** (**RRDNS**) endpoint, we can bypass
    the router mesh. This will allow us to access instances on given nodes on defined
    ports or apply our own load balancer. In some situations, we need to include special
    load balancing features such as weights or persistence of users' sessions. The
    default Docker Swarm's router mesh behavior will route requests to all available
    services' backend instances. It is important to identify your application's requirements
    to determine whether you should deploy its components using Docker Swarm's default
    load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Docker's internal load balancer will just do L3 routing. It will not provide
    any weight-based routing or special features.
  prefs: []
  type: TYPE_NORMAL
- en: Using host mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using host mode, only nodes with running instances will receive traffic. We
    can label nodes so that they only schedule some tasks on them and route traffic
    to them from load balancers. In this case, we can't run more replicas for this
    service than the defined and labeled number of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will run one NGINX process on each node in the
    cluster since we defined a global service. We will use `docker service create
    --mode global --publish mode=host`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The service's defined port will be available on all the nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using Round-Robin DNS mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also use RRDNS mode to avoid the service's virtual IP address. In this
    situation, Docker Swarm will not assign a virtual IP for the service, so it will
    create a service DNS entry with all its replicas' IP addresses. This is useful
    when we want to use our own load balancer inside the Docker Swarm cluster to deploy
    this load balancer as another service. It is not easy to maintain the IP addresses
    of replicas inside the load balancer service. We will probably use DNS resolution
    inside the load balancer's configuration, querying the DNS to retrieve all instances'
    IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will help us understand the concepts we've learned in this
    chapter with some labs.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter labs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will complete this chapter's lab to help us improve our understanding
    of the concepts we've learned. Deploy `environments/swarm-environment` from this
    book's GitHub repository ([https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git))
    if you have not done so yet. You can use your own Linux server. Use `vagrant up`
    from the `environments/swarm`folder to start your virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait until all your nodes are running. We can check the nodes'' status using
    `vagrant status`. Connect to your lab node using `vagrant ssh swarm-node1`. Vagrant
    has deployed four nodes for you. You will be using the `vagrant` user with root
    privileges using `sudo`. You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Nodes will have three interfaces (IP addresses and virtual hardware resources
    can be modified by changing the `config.yml` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '`eth0 [10.0.2.15]`: Internal, required for Vagrant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eth1 [10.10.10.X/24]`: Prepared for Docker Swarm internal communication. The
    first node will get the IP address `10.10.10.11`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eth2 [192.168.56.X/24]`: A host-only interface for communication between your
    host and the virtual nodes. The first node will get the IP address `192.168.56.11`,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the `eth1` interface for Docker Swarm and we will be able to connect
    to published applications using the `192.168.56.X/24` IP address range. All nodes
    have Docker Engine Community Edition installed and the `vagrant` user is allowed
    to execute `docker`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can connect to the first deployed virtual node using `vagrant ssh swarm-node1`.
    This process may vary if you''ve already deployed a Docker Swarm virtual environment
    before and just started it using `vagrant up`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, you are ready to start the labs. Let's start by creating a Docker Swarm
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker Swarm cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once Vagrant (or your own environment) has been deployed, we will have four
    nodes (named `node<index>`, from `1` to `4`) with Ubuntu Xenial and Docker Engine
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: First, review your lab node's IP addresses (`10.10.10.11` to `10.10.10.14` if
    you used Vagrant since the first interface will be Vagrant's internal host-to-node
    interface). Once you are familiar with the environment's IP addresses, we can
    initiate a cluster on `node1`, for example.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Linux as a VirtualBox host, you can execute `alias vssh='vagrant
    ssh'` on your Terminal to use `vssh` instead of `vagrant ssh` to connect to nodes
    as it will be more familiar with non-Vagrant-based real environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our environment ready for the labs, along with four nodes
    and Docker Engine already installed, let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to `node1` and initialize a new cluster using `docker swarm init`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is normal if you are using Vagrant as nodes will have at least two interfaces.
    The first interface is internal to Vagrant, while the other is the one fixed for
    the labs. In this case, we will need to specify which interface to use for the
    cluster with `--advertise-addr`. We will execute `docker swarm init --advertise-addr`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, Swarm is initialized correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a second node that''s connecting to `node2` and executing the command described
    in the initialization output. We will join the cluster using the `docker swarm
    join` command with the obtained token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: With this, a node is added as a worker.
  prefs: []
  type: TYPE_NORMAL
- en: 'On `node1`, verify that the new node was added by using `docker node ls`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `swarm-node1` is the leader because this is the node that initialized
    the cluster. We couldn't have executed `docker node ls` on `swarm-node2` because
    it was not a manager node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will execute the same joining process on `swarm-node3`, using `docker swarm
    join` again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will review the token for managers, so the next node will be added
    as a manager. We will use `docker swarm join-token manager`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we connect to `swarm-node4` and execute the shown joining command (`docker
    swarm join`) with the new token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The cluster now has four nodes: two managers and two workers. This will not
    provide high availability should the leader fail. Let''s promote `swarm-node2`
    to the manager role too, for example, by executing `docker node update --role
    manager`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We can change node roles using the `promote` and `demote` commands as well,
    but it is more convenient to know what they really mean for node property updates.
    Also, notice that we can change node roles whenever we want, but we should maintain
    the number of healthy managers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can review the node''s status again. Managers are shown as `Reachable` or
    `Leader`, indicating that this node is the cluster leader. Using `docker node
    ls`, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we executed these commands on `node4`. We can do this because it
    is a manager (not a leader, but a manager). We can use any manager to manage the
    cluster, but only the leader will perform updates on the internal database.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will just leave one manager for the rest of the labs, but first, we will
    kill the `node1` Docker Engine daemon to see what happens in the cluster. We will
    stop the Docker daemon using `systemctl stop docker`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect to the other manager (`node2`, for example; that is, the recently promoted
    node). Now, let''s review the node''s status with `docker node ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'A new leader was elected from among the other running managers. Now, we can
    start the `node1` Docker Engine daemon again using `systemctl start docker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The node remains as a manager but is no longer the leader of the cluster because
    a new one was elected when it failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s demote all non-leader nodes to workers for the rest of the labs using
    `docker node update --role worker`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the error when listing again. `node1` is not a manager now, so we can''t
    manage the cluster from this node anymore. All management commands will now run
    from `node4` for the rest of the labs. `node4` is the only manager, which makes
    it the cluster leader, as we can observe using `docker node ls` once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the next lab, we will deploy a simple web server service.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a simple replicated service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From `swarm-node4`, we will create a replicated service (by default) and test
    how we can distribute more replicas on different nodes. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy the `webserver` service using a simple `nginx:alpine` image by executing
    `docker service create`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we had to wait a few seconds until all the instances were correctly
    running. The amount of time this takes may vary if the image has some configured
    health check.
  prefs: []
  type: TYPE_NORMAL
- en: We can overwrite the image-defined health checks on service creation or by updating
    the configuration using `--health-cmd` and other related arguments. In fact, we
    can change almost everything on a used image, just as we did with containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it is deployed, we can review where the replica was started by using `docker
    service ps`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `nginx` was deployed on `swarm-node3`. This may vary in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can scale the number of replicas to `3` and review how they were distributed.
    We will use `docker service update --replicas` for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If we review the replicas'' distribution, we can discover where the containers
    are running using `docker service ps webserver`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, in this case, `swarm-node2` did not receive a replica, but we can
    force replicas to run there.
  prefs: []
  type: TYPE_NORMAL
- en: 'To force specific locations, we can add labels to specific nodes and add constraints
    to nodes. We''ll add a label using `docker node update --label-add`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can modify the current service so that it runs on specific nodes labeled
    as `tier==front`. We will use `docker service update --constraint-add node.labels.tier`
    and then review its distributed tasks again with `docker service ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now, all the replicas are running on `swarm-node2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will perform some maintenance tasks on `node2`. In this situation,
    we will remove the `service` constraint before draining `swarm-node2`. If we do
    not do that, no other node will receive workloads because they are restricted
    to `tier=front` node labels. We removed the service''s constraints using `docker
    service update --constraint-rm node.labels.tier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The tasks did not move to other nodes because the tasks were already satisfying
    the service constraints (no constraint in the new situation).
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm will never move tasks if it is not really necessary because it
    will always try to avoid any service disruption. We can force an update regarding
    service task redistribution by using `docker service update --force <SERVICE_NAME>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we will pause `swarm-node3` and drain `swarm-node2`. We will
    use `docker node update --availability pause` and `docker node update --availability
    drain` to do so, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s review our service replica distribution again using `docker service
    ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Notice that only `swarm-node1` and `swarm-node4` get some tasks because `swarm-node3`
    is paused and we removed all tasks on `swarm-node2`.
  prefs: []
  type: TYPE_NORMAL
- en: We can use `docker node ps <NODE>` to get all the tasks from all the services
    running on the specified node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will remove the `webserver` service and enable nodes `node2` and `node3`
    again. We will execute `docker service rm` to remove the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In the next lab, we will create a global service.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a global service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lab, we will deploy a global service. It will run one task on each
    cluster node. Let''s learn how to use `global` mode:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we learned that global services will deploy one replica on
    each node. Let''s create one and review its distribution. We will use `docker
    service create --mode global`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'All the nodes receive their own replicas, as we can see with `docker service
    ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now drain `swarm-node1`, for example, and review the new task distribution.
    We will drain the node using `docker node update --availability drain`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: None of the nodes received the `swarm-node1` task because global services will
    only run one replica of a defined service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we enable `swarm-node1` once more using `docker node update --availability
    active`, its replica will start to run again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Swarm will run one task of any global service on each node. When a new node
    joins the cluster, it will also receive one replica of each global service defined
    in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will remove the `webserver` service again to clear the cluster for the following
    labs by using `docker service rm webserver`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We will now take a quick look at service updates to learn how to update a service's
    base image.
  prefs: []
  type: TYPE_NORMAL
- en: Updating a service's base image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s learn how to refresh a new image version of a deployed and running service
    while *avoiding* user access interruption:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a 6-replica `webserver` service using `docker service create
    --replicas 6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we update to a specific `nginx:alpine` version with `perl` support, for
    example. We use `docker service update --image` to change only its base image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The update took more than 60 seconds because Swarm updated tasks one by one
    at 10-second intervals. It will first start the new container with the newly defined
    image. Once it is healthy, it will stop the old version of the container. This
    must be done on each task and therefore takes more time, but this way, we can
    ensure that there is always a `webserver` task running. In this example, we have
    not published any `webserver` ports, so no user interaction is expected. It is
    just a simple lab – but real-life environments will be the same, and internal
    Docker Swarm load balancing will always guide the user's requests to alive instances
    while an update is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new version is running now, as we can observe by using `docker service
    ps` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We will remove the `webserver` service again to clear the cluster for the following
    labs using `docker service rm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: In the next lab, we will deploy applications using stacks instead of creating
    services manually, which might lead to us making configuration errors, for example.
    Using stacks will provide environment reproducibility because we will always run
    the same IaC definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying using Docker Stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lab, we will deploy a PostgreSQL database using secrets, configurations,
    and volumes on an IaC file. This file will contain all the application''s requirements
    and will be used to deploy the application as a Docker Stack. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a secret for the required PostgreSQL admin user password.
    We will execute `docker service create` with the standard input as the secret
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We will use it as an external secret inside the `docker-compose` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to create a simple initialization script to create a new database
    when PostgreSQL starts. We will create a simple file in the current directory
    named `create-docker-database.sh` with the following content and appropriate `755`
    permissions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a config file with the file''s content. We will use this file
    to create a database named `docker` on starting up PostgreSQL. This is something
    we can use because it is provided by the official Docker Hub PostgreSQL image.
    We will use `docker config create` with the `create-docker-database.sh` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add labels to some of the nodes to ensure the database is always running
    there since we will create an external volume only on that node. For this example,
    we will use `node2`. We will create a volume using `docker volume create`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This volume will only exist on `swarm-node2`, so we will create a constraint
    based on a node label to run the service task only on `swarm-node2`. We will use
    `docker node update --label-add tier=database` for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple sample. In your production environment, you will never use
    local volumes. We will need to define and use some plugin that allows us to share
    the same volume on different hosts, such as NFS and RexRay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create the following Docker Compose file, named `postgres-stack.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Take note of the following things in this file; we added a lot of learned information
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: We defined the `postgres:alpine` image for the `database` service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `database` service will only be scheduled on worker nodes with a `tier`
    label key and a value of `database`. In this case, it will run tasks only on `node2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `postgres` image can use Docker Swarm secret files as environment variables,
    and in this case, it will use `postgres_password` mounted on `/run/secrets/postgres_password`.
    The secret is declared externally because it was previously created outside of
    this file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also added a config file to create an initial database called `docker`. The
    config file is external as well because we added it outside the `postgres-stack.yaml`
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also added an external volume named `PGDATA`. We will use this volume for
    the database but it will only exist on `node2`. It is defined as external because
    we manually create the `PGDATA` volume locally on `node2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We published the PostgreSQL application's port `5432` on the host's port; that
    is, `15432`. We changed the published port to recognize that they are not the
    same because `5432` will be an internal port on the defined network named `net`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we defined the `net` network as `attachable` to be able to test our
    database with a simple container running a `postgres` client. We added two aliases
    to the `database` service inside this network: `postgres` and `mydatabase`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that all the objects that were created for the stack will use the stack's
    name as a prefix. This will not happen on externally defined objects. They will
    be used, but we create them manually, outside of the stack's life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'We deploy the `postgres` stack using `docker stack deploy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We can easily review the stack's status using `docker stack ps`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: It is running on `swarm-node2`, as we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We published port `5432` on port `15432`. We can connect to this port from
    any node IP address in the cluster because Swarm uses a routing mesh. We use the
    `curl` command to review the port''s availability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We get this response to `curl` because we are not using the right software client
    (but the ports are listening). Let's run a simple `alpine` container with the
    `postgres` client.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run a simple `alpine` container attached to the stack''s deployed
    network. In this example, it is `postgres_net`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we ran a simple `alpine` container and installed the `postgresql-client`
    package using `docker container run` with an appropriate network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that we added the `mydatabase` and `postgres` aliases to the `database`
    service. Therefore, any of them will be valid for testing database connectivity
    since Swarm added these entries to the internal DNS. We can test this by running
    a simple `ping` command inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the installed client to test our deployed PostgreSQL. Remember
    to use the previously defined password that we created as a secret, `SuperSecretPassword`.
    We will test our database''s connectivity using the `psql` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We listed the deployed databases using `\l` and the `docker` database, which
    was created with our `create-db.sh` script. Notice that we used the default PostgreSQL
    database port `5432` (we omitted any port customization on client request) instead
    of `15432`. This is because the `docker` container was connecting to the database
    internally. Both the `postgres_database.1` task and the externally run container
    are using the same network, `postgres_net`.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we can use all learned options with the created stack service, `postgres_database`.
    Anyway, we can modify the Docker Compose file and redeploy the same stack again
    with some changes. Swarm will review the required updates and take the necessary
    actions on all components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s exit the running container by executing the `exit` command, and then
    remove the `postgres` stack and `node2` volume to clean up for the following labs
    using `docker stack rm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: In the next lab, we will launch a simple replicated service and review internal
    ingress load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm ingress internal load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lab, we will use the `codegazers/colors:1.13` image. This is a simple
    application that will show different random front page colors or texts. Let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a service named `colors` based on the `codegazers/colors:1.13`
    image. Since we won''t be setting any specific color using environment variables,
    random ones will be chosen for us. Use `docker service create --constraint node.role==worker`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We chose not to run a replica on the manager node because we will use `curl`
    from `node4` in this lab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test local connectivity from the `swarm-node4` manager with `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We deployed one replica and it is running the `orange` color. Take note of the
    container's IP address and its name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run five more replicas by executing `docker service update --replicas
    6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'If we test service port `8080` with `curl` once more, we will get different
    colors. This is because the containers were launched without color settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We get different colors on different containers. The router mesh is guiding
    our requests to the `colors` tasks' containers using the ingress overlay network.
  prefs: []
  type: TYPE_NORMAL
- en: We can access all the `colors` service task logs using `docker service logs
    colors`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s remove the `colors` service for the next and final lab using `docker
    service rm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: In the next lab, we will review service endpoint modes and consider how DNS
    resolves `vip` and `dnsrr` situations.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lab, we will create a test overlay attachable network and review DNS
    entries for the `vip` and `dnsrr` endpoint modes. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create an attachable overlay `test` network using `docker
    network create --attachable -d overlay`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create two different `colors` services. Each one will use different
    endpoint modes. For the `vip` mode, we will use `docker service create`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the second one for `dnsrr` using `docker service create --endpoint-mode
    dnsrr`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run a simple `alpine` container on the `test` network using `docker
    container run` and test the internal name resolution functionality. We will need
    to install the `bind-tools` package to be able to use the `host` and `nslookup`
    tools:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: As expected, when using the `vip` endpoint mode, the service receives a virtual
    IP address. All requests will be redirected to that address and ingress will route
    to the appropriate container using internal load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, using the `dnsrr` endpoint will not provide a virtual IP
    address. The internal DNS will add an entry for each container IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also take a look at the containers attached to the `test` network. These
    containers will get one internal IP address and one that will be routed on the
    overlay network. We can launch the `ip add show` command attached to one of the
    running `colors-dnsrr` tasks'' containers using `docker container exec`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: All Vagrant environments can easily be removed by executing `vagrant destroy
    -f` to remove all previously created nodes for this lab. This command should be
    executed on your `environments/swarm` local directory.
  prefs: []
  type: TYPE_NORMAL
- en: Remove all the services that you created for this last lab with `docker service
    rm colors-dnsrr colors-vip`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed how to deploy and work with the Docker Swarm orchestrator.
    This is the default orchestrator in Docker as it comes out of the box with Docker
    Engine.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about Docker Swarm's features and how to deploy applications using
    stacks (IaC files) and services instead of containers. Orchestration will manage
    the application's components to keep them running, helping us to even upgrade
    them without impacting users. Docker Swarm also introduced new objects such as
    secrets and config, which help us distribute workloads within cluster nodes. Volumes
    and networks should be managed cluster-wide. We also learned about overlay networking
    and how Docker Swarm's router mesh has simplified application publishing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the Kubernetes orchestrator. Currently,
    Kubernetes is a small part of the Docker Certified Associate exam, but this will
    probably be increased in the following releases. It is also useful for you to
    know and understand the concepts of Kubernetes alongside Docker Swarm. Docker
    Enterprise provides both and we can make them work together.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choose all of the false statements from the following options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Docker Swarm is the only orchestrator that can work with Docker.
  prefs: []
  type: TYPE_NORMAL
- en: b) Docker Swarm comes included out of the box with Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: c) Docker Swarm will allow us to deploy applications on a pool of nodes working
    together, known as a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the preceding statements are false.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements are false regarding what Swarm provides by
    default?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Service discovery
  prefs: []
  type: TYPE_NORMAL
- en: b) Internal load balancing
  prefs: []
  type: TYPE_NORMAL
- en: c) Overlay networking among distributed containers on cluster nodes
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the preceding statements are false
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements are true in relation to managers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) We can't create replicated services with tasks running on managers.
  prefs: []
  type: TYPE_NORMAL
- en: b) There is just one leader node on each cluster that manages all Swarm cluster
    changes and object statuses.
  prefs: []
  type: TYPE_NORMAL
- en: c) If the leader node dies, all the changes will be frozen until the leader
    node is healthy again.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the preceding statements are true.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements are false in relation to workers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Worker nodes just run workloads.
  prefs: []
  type: TYPE_NORMAL
- en: b) If we drain a worker node, all the workloads running on that node will be
    moved to other available nodes.
  prefs: []
  type: TYPE_NORMAL
- en: c) Swarm roles can be changed for any node in the cluster whenever this is required.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the preceding statements are true.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements are false about Swarm Stacks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By default, all Stacks will be deployed on their own networks.
  prefs: []
  type: TYPE_NORMAL
- en: b) Stacks will use Docker Compose files to define all application components.
  prefs: []
  type: TYPE_NORMAL
- en: c) Everything that's used for a Stack should be defined inside the `docker-compose`
    file. We can't add external objects.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the preceding statements are true.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following links for more information regarding the topics that
    were covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Swarm overview: [https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deploying applications on Docker Swarm: [https://docs.docker.com/get-started/swarm-deploy/](https://docs.docker.com/get-started/swarm-deploy/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orchestration with Docker Swarm: [https://hub.packtpub.com/orchestration-docker-swarm/](https://hub.packtpub.com/orchestration-docker-swarm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Native Docker clustering with Swarm: [https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm](https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
