- en: Orchestration Using Docker Swarm
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 进行编排
- en: In the previous chapter, we learned about orchestration features. In this chapter,
    we will build on this by learning about Docker Swarm. It comes with Docker Engine
    (Docker installation packages) out of the box, so we don't need to install any
    other software. It is simpler to master the basics of Docker Swarm compared to
    the other orchestrators available, and it is powerful enough for production deployments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了编排功能。在本章中，我们将基于此继续学习 Docker Swarm。它与 Docker 引擎（Docker 安装包）一起捆绑提供，因此我们无需安装其他软件。相比其他编排器，掌握
    Docker Swarm 的基础更为简单，而且它足够强大，可以用于生产部署。
- en: In summary, in this chapter, we will learn how to deploy Docker Swarm in production.
    We will also review the new objects introduced by Docker Swarm and the steps required
    to deploy a complete application based on containers with orchestration. Networking
    is key for node-distributed applications, so we will examine how Docker Swarm
    provides solutions for internal networking, service discovery, and publishing
    deployed applications. At the end of the chapter, we will review how Docker Swarm
    can help us upgrade our application's components without service interruption.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本章中，我们将学习如何在生产环境中部署 Docker Swarm。我们还将回顾 Docker Swarm 引入的新对象以及部署基于容器的完整应用程序所需的步骤。网络是节点分布式应用的关键，因此我们将探讨
    Docker Swarm 如何提供解决方案来处理内部网络、服务发现和发布已部署的应用程序。在本章结束时，我们将回顾 Docker Swarm 如何帮助我们在不中断服务的情况下升级应用程序的组件。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Deploying Docker Swarm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 Docker Swarm
- en: Creating a Docker Swarm cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 Docker Swarm 集群
- en: Scheduling workloads in the cluster – tasks and services
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中调度工作负载 – 任务与服务
- en: Deploying applications using Stacks and other Docker Swarm resources
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Stacks 和其他 Docker Swarm 资源部署应用程序
- en: Networking in Docker Swarm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm 中的网络
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will learn about Docker Swarm's orchestrator features. We'll
    provide some labs at the end of this chapter that you can use to test your understanding
    and demonstrate the concepts you've learned. These labs can be run on your laptop
    or PC using the provided Vagrant "Docker Swarm" environment, or any already deployed
    Docker Swarm cluster of your own. Check out this book's GitHub code repository
    for the code we're going to be using in this chapter, along with additional information,
    at [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习 Docker Swarm 的编排器功能。我们将在本章末尾提供一些实验，供你测试理解并展示你学到的概念。这些实验可以在你的笔记本电脑或个人电脑上使用提供的
    Vagrant “Docker Swarm”环境运行，或者使用你自己已经部署的 Docker Swarm 集群。请查看本书的 GitHub 代码库，获取我们将在本章中使用的代码以及更多信息，地址是
    [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git)。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码演示：
- en: '"[https://bit.ly/31wfqmu](https://bit.ly/31wfqmu)"'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '"[https://bit.ly/31wfqmu](https://bit.ly/31wfqmu)"'
- en: Deploying Docker Swarm
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Docker Swarm
- en: Docker Swarm is the built-in orchestrator that comes with Docker Engine out
    of the box. It was introduced in Docker Engine release 1.12 (the release numbers
    changed after 1.13 to four-digit numbers) as *swarm mode*. There was a previous
    swarm approach currently known as Legacy Swarm, which was closer in architecture
    to Kubernetes. It required an external key-value store database, among other components.
    Swarm mode is different from this because it includes everything needed for the
    orchestrator to work out of the box.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 是与 Docker 引擎捆绑在一起的内置编排器。它是在 Docker 引擎 1.12 版本中引入的（版本号在 1.13 后改为四位数字），以*swarm
    模式*呈现。之前有一种称为传统 Swarm 的方法，它的架构更接近 Kubernetes，需要一个外部的键值存储数据库等组件。Swarm 模式与此不同，因为它包含了运行编排器所需的所有内容，可以开箱即用。
- en: 'The Swarm architecture is quite simple as it provides secure communications
    between components by default. Before deploying a Docker Swarm cluster, let''s
    review its main features:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 架构相当简单，因为它默认提供了组件之间的安全通信。在部署 Docker Swarm 集群之前，让我们回顾一下它的主要特点：
- en: '**Container orchestration for multiple nodes is included on each Docker Engine**:
    This means that we can deploy a cluster without any other software. Docker Engine
    provides all the required components to deploy and manage the cluster out of the
    box.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个Docker Engine都包含多节点容器编排**：这意味着我们可以部署一个不需要其他软件的集群。Docker Engine提供了部署和管理集群所需的所有组件。'
- en: '**Node roles can be changed at runtime**: Orchestration is based on different
    node roles. While the control plane is managed by managers or master nodes, computation
    or application deployment will be done on slave, worker, or minion nodes. Each
    orchestrator uses different names for these different roles, but they are essentially
    the same. Swarm allows us to change nodes from one role to another when one of
    them is unhealthy or when we need to do some maintenance tasks.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点角色可以在运行时更改**：编排基于不同的节点角色。虽然控制平面由管理者或主节点管理，但计算或应用程序部署将在从节点、工作节点或随从节点上完成。每个编排器对这些不同角色使用不同名称，但它们本质上是相同的。Swarm允许我们在一个角色不健康或需要执行维护任务时将节点从一个角色更改为另一个角色。'
- en: '**Workloads will be declared as services, defining a number of instances to
    be healthy**: The Docker orchestrator will keep the required number of replicas
    alive. If some of them die, the orchestrator will run new tasks to keep the required
    number alive. The orchestrator will manage this reconciliation process. If a node
    dies, the orchestrator will move all containers to a new, healthy node.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作负载将被声明为服务，定义需要保持健康的实例数量**：Docker编排器将保持所需数量的副本存活。如果其中一些实例失败，编排器将运行新任务以保持所需数量的实例存活。编排器将管理此调解过程。如果一个节点失败，编排器将所有容器迁移到一个新的、健康的节点。'
- en: '**As workloads are based on the number of instances required, the orchestrator
    will allow us to change this number any time we require**: As a result, we can
    scale up or down the number of instances of a service (application component)
    to respond to a high demand for requests, for example.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由于工作负载基于所需实例数量，编排器允许我们随时更改此数字**：因此，我们可以根据请求的高需求来扩展或减少服务（应用程序组件）的实例数量。'
- en: '**We will deploy applications based on multiple service components, with all
    their requirements and connectivity between them**: As components may run on any
    cluster node, Docker Swarm will provide an internal overlay network to interconnect
    all application components.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们将基于多个服务组件部署应用程序，并满足它们的所有需求和连接要求**：由于组件可能在任何集群节点上运行，Docker Swarm将提供内部叠加网络以连接所有应用程序组件。'
- en: '**Swarm will provide service discovery and internal load balancing**: In the
    *Service discovery and load balancing* section, we will learn how Docker Swarm
    can provide internal application DNS resolution so that all the components will
    easily be able to discover each other, along with load balancing between service
    replicas using a virtual IP.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swarm将提供服务发现和内部负载均衡**：在*服务发现和负载均衡*部分，我们将了解Docker Swarm如何提供内部应用程序DNS解析，以便所有组件可以轻松发现彼此，并通过虚拟IP在服务副本之间进行负载均衡。'
- en: '**Orchestration will allow us to update application components automatically**:
    In fact, all we need to decide is how these updates will be managed; orchestration
    will do the rest. This way, we can update application components without impacting
    users.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编排将允许我们自动更新应用程序组件**：事实上，我们只需决定如何管理这些更新；编排将完成其余工作。这样，我们可以更新应用程序组件而不会影响用户。'
- en: '**We can ensure that our cluster runs securely by default**: Docker Swarm will
    deploy **Transport Layer Security** (**TLS**) to interconnect control plane components.
    It will manage certificates for all of our nodes, creating an internal CA and
    verifying all node certificates itself.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们可以确保默认情况下集群安全运行**：Docker Swarm将部署**传输层安全**（**TLS**）来连接控制平面组件。它将为所有节点管理证书，创建内部CA并验证所有节点证书。'
- en: It is important to know that only the control plane is secure by default. Users'
    access to features such as application publishing will require additional configuration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 只有控制平面默认安全，用户访问应用发布等功能需要额外配置。
- en: As we learned in the previous chapter, orchestrators require databases to store
    and manage workloads and any other cluster resource information. Docker Swarm
    has a built-in key-value store under the `/var/lib/docker/swarm` path (this is
    on Linux; it can be found in its equivalent directory on Windows, under `C:\ProgramData\docker`).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中所学到的，调度器需要数据库来存储和管理工作负载及其他集群资源信息。Docker Swarm 在`/var/lib/docker/swarm`路径下有一个内建的键值存储（这是
    Linux 上的路径；在 Windows 上，它可以在`C:\ProgramData\docker`目录下找到其对应的路径）。
- en: It is important to understand that the `/var/lib/docker/swarm` directory is
    essential, should we need to restore an unhealthy cluster. Take care of this directory
    and keep a backup of it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，`/var/lib/docker/swarm`目录是必不可少的，万一我们需要恢复一个不健康的集群时，请务必保护好这个目录，并保持备份。
- en: We can lock users' access to the `/var/lib/docker/swarm` path using a key. This
    improves security. If it is unlocked, someone with enough system privileges can
    obtain Docker Swarm certificates.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用密钥锁定用户对`/var/lib/docker/swarm`路径的访问权限，从而提高安全性。如果路径被解锁，拥有足够系统权限的人可以获取 Docker
    Swarm 证书。
- en: Docker Swarm overall architecture
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker Swarm 整体架构
- en: 'As we mentioned previously, Docker Swarm deploys its own secure control plane.
    There are two kinds of node roles:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Docker Swarm 部署了其自己的安全控制平面。节点角色有两种：
- en: '**Managers**: These manage the overall Swarm cluster environment. They share
    an internal key-value database. More specifically, one of the managers has a different
    role. This is the manager''s leader. There is only one leader per cluster and
    it makes all the necessary updates to the database. All other manager nodes will
    follow and sync their databases with the leader''s one. Managers maintain cluster
    health, serve the Swarm HTTP API, and schedule workloads on available compute
    nodes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理节点**：这些节点管理整个 Swarm 集群环境。它们共享一个内部的键值数据库。更具体地说，其中一个管理节点担任不同的角色，即集群的领导者。每个集群只有一个领导者，领导者负责对数据库进行所有必要的更新。其他所有管理节点都会跟随并将它们的数据库与领导者的数据库同步。管理节点维护集群健康，提供
    Swarm HTTP API 服务，并在可用计算节点上调度工作负载。'
- en: '**Workers**: Workloads will run on worker nodes. It is important to know that
    managers have worker roles too. This means that workloads can also run on managers
    if we do not specify any special scheduling location. Workers will never participate
    in scheduling decisions; they will just run assigned workloads.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：工作负载将在工作节点上运行。需要注意的是，管理节点也有工作节点角色。这意味着，如果我们没有指定特殊的调度位置，工作负载也可以在管理节点上运行。工作节点永远不会参与调度决策，它们只会运行分配的工作负载。'
- en: We manage workload locations either by using location constraints on each workload
    or by disabling container execution on some nodes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对每个工作负载使用位置约束或禁用某些节点上的容器执行来管理工作负载的位置。
- en: On nodes with multiple interfaces, we will be able to choose which interface
    we will use for the control plane. Manager nodes will implement the Raft consensus
    algorithm to manage the Swarm cluster state. This algorithm requires multiple
    servers coming to an agreement on data and status. Once they reach a decision
    on a value, that decision is written to disk. This will ensure information distribution
    with consistency across multiple managers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多个网络接口的节点上，我们将能够选择用于控制平面的接口。管理节点将实现 Raft 共识算法来管理 Swarm 集群状态。该算法要求多个服务器就数据和状态达成一致。一旦它们就某个值做出决定，该决定会被写入磁盘。这将确保信息在多个管理节点间一致地分发。
- en: As we mentioned previously, there is a leader node that modifies and store changes
    on its database; all other nodes will sync their databases with it. To maintain
    this consistency, Swarm implements Raft. This algorithm will manage all changes
    in the database, as well as the election of a new leader when it is unhealthy.
    When the leader needs to make a change (such as to the application's component
    status, and its data), it will query all the other nodes for their opinions. If
    they all agree with the change, the leader will commit it, and the change will
    be synced to all the nodes. If the leader fails (as in, the node goes down, the
    server process dies, and so on), a new election is triggered. In this case, all
    the remaining manager nodes will vote for a new leader.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，存在一个领导节点，它修改并存储数据库中的变更；所有其他节点将与其同步数据库。为了保持一致性，Swarm实现了Raft协议。该算法将管理数据库中的所有更改，并在领导节点不健康时进行新领导的选举。当领导节点需要做出更改（例如，修改应用程序的组件状态及其数据）时，它会询问所有其他节点的意见。如果所有节点都同意更改，领导节点将提交该更改，并将其同步到所有节点。如果领导节点失败（例如，节点宕机，服务器进程崩溃等），将触发新的选举。在这种情况下，所有剩余的管理节点将投票选举新的领导节点。
- en: This process requires reaching a consensus, with the majority of nodes agreeing
    on the result of the election. If there is no majority, a new election process
    will be triggered until a new leader is elected. After that, the cluster will
    be healthy again. Keep these concepts in mind because they are key in Docker Swarm
    and other orchestrators.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程需要达成共识，绝大多数节点需同意选举结果。如果没有达成多数意见，将触发新的选举过程，直到选出新的领导节点。选举完成后，集群将恢复健康。记住这些概念，因为它们是Docker
    Swarm和其他协调器的关键。
- en: 'The following diagram represents the basic architecture of a Swarm orchestrator:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示表示Swarm协调器的基本架构：
- en: '![](img/e8491519-b7cd-4e8d-b03b-49a057f12859.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8491519-b7cd-4e8d-b03b-49a057f12859.png)'
- en: Let's review each plane in detail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细回顾每个平面的内容。
- en: Management plane
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理平面
- en: The management plane is the layer where all management tasks run. All cluster
    management traffic and workload maintenance will take place on this plane. The
    management plane provides high availability based on an odd number of manager
    nodes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 管理平面是执行所有管理任务的层次。所有集群管理流量和工作负载维护都将在此平面进行。管理平面提供基于奇数个管理节点的高可用性。
- en: All communication in this plane is mutually encrypted using TLS (mutual TLS)
    by default. This is where the Raft protocol operates.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有在此平面中的通信都使用TLS（双向TLS）进行加密。这是Raft协议运行的地方。
- en: Control plane
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制平面
- en: This plane manages the state of the cluster. The gossip protocol will periodically
    inform all nodes about the cluster state, reducing the amount of information required
    by nodes to simply having an overview of the health of the cluster. This protocol
    manages host-to-host communications and is called the *control plane* because
    each host communicates only with its closest companions, and information flows
    through this to reach all the nodes within the control plane.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该平面管理集群的状态。Gossip协议会定期通知所有节点集群状态，减少节点所需的相关信息，仅提供集群健康概况。这一协议管理主机之间的通信，因此被称为*控制平面*，因为每个主机只与它最近的伙伴通信，信息通过该平面流动，传递至控制平面内的所有节点。
- en: Data plane
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据平面
- en: The data plane manages all the service's internal communications. It is based
    on VXLAN tunneling, encapsulating layer-2 packets within layer-3 headers. It will
    use UDP transport but VXLAN guarantees no dropped packets. We will be able to
    isolate the data plane from the control and management planes using the appropriate
    flags upon Docker Swarm creation (or joining).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面管理所有服务的内部通信。它基于VXLAN隧道，将二层数据包封装在三层头部内。它将使用UDP传输，但VXLAN保证没有丢包。我们可以在创建（或加入）Docker
    Swarm时使用适当的标志，将数据平面与控制和管理平面隔离开来。
- en: 'When we initialize a new Docker Swarm cluster, it generates a self-signed **Certificate
    Authority** (**CA**) and issues self-signed certificates to every node. This ensures
    mutual TLS communication. The following is a summary of the steps taken to ensure
    secure communications when a new node joins the cluster:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化一个新的Docker Swarm集群时，它会生成一个自签名的**证书颁发机构**（**CA**），并向每个节点发放自签名证书。这确保了双向TLS通信。以下是当一个新节点加入集群时，确保安全通信的步骤概述：
- en: When a node joins, it sends the manager its join token, along with a certificate
    request.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当一个节点加入时，它会将其加入令牌和证书请求一起发送给管理节点。
- en: Then, if the token is valid, the manager accepts the node's request and sends
    back a self-signed node certificate.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，如果令牌有效，管理节点接受该节点的请求并返回一个自签名的节点证书。
- en: The manager then registers the new node in the cluster and it will appear as
    part of the Docker Swarm cluster.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，管理节点将新节点注册到集群中，它将成为Docker Swarm集群的一部分。
- en: Once the node is included in the cluster, it is ready (by default) to accept
    any new workload scheduled by the manager.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦节点被包含在集群中，它将准备好（默认情况下）接受由管理节点调度的任何新工作负载。
- en: In the next section, we will learn how to easily deploy a Docker Swarm cluster
    using common Docker command-line actions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何使用常见的Docker命令行操作轻松部署Docker Swarm集群。
- en: Deploying a Docker Swarm cluster using the command line
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用命令行部署Docker Swarm集群
- en: 'We can use the Docker `swarm` object to initialize a new cluster, join or leave
    a previously created one, and manage all Docker Swarm properties. Let''s take
    a look at the `docker swarm` actions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Docker的`swarm`对象来初始化一个新集群，加入或离开一个先前创建的集群，并管理所有Docker Swarm属性。让我们来看一下`docker
    swarm`操作：
- en: '`init`: We will use `docker swarm init` to initialize a new cluster or recreate
    an existing one (we will describe this situation in more detail in the *High availability
    with Swarm* section). We will set many cluster options during cluster creation,
    but there are a few that can be changed later. The most important options are
    `--data-path-addr` and `--data-path-port` because they are used to set which node
    interface will be dedicated to the control plane on multi-homed nodes.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`：我们将使用`docker swarm init`来初始化一个新集群或重新创建一个已有集群（我们将在*高可用性与Swarm*部分更详细地描述这种情况）。在集群创建过程中，我们将设置许多集群选项，但有些选项可以稍后更改。最重要的选项是`--data-path-addr`和`--data-path-port`，因为它们用于设置在多网卡节点上哪个节点接口将专门用于控制平面。'
- en: 'These are the most commonly used arguments for creating the cluster:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是创建集群时最常用的参数：
- en: '`--advertise-addr`: This option allows us to set which interface will be used
    to announce the cluster. All other nodes will use this interface''s IP address
    to join the cluster.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--advertise-addr`：此选项允许我们设置用于宣布集群的接口。所有其他节点将使用该接口的IP地址来加入集群。'
- en: '`--data-path-addr`/`--data-path-port`: These options configure the interface
    and port used for the control plane. All traffic in this interface will be encrypted
    using TLS, and certificates will be managed internally by Swarm. We can use the
    IP address/port or host interface notation. The default port is `4789`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--data-path-addr`/`--data-path-port`：这些选项配置用于控制平面的接口和端口。该接口的所有流量将使用TLS加密，证书将由Swarm内部管理。我们可以使用IP地址/端口或主机接口表示法。默认端口是`4789`。'
- en: '`--external-ca`/`--cert-expiry`: Although Swarm will manage TLS for us, we
    can deploy our own CA for all certificates using this parameter. We can also specify
    how often certificates will be rotated. By default, they will be automatically
    recreated every 90 days (2,160 hours).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--external-ca`/`--cert-expiry`：尽管Swarm会为我们管理TLS，但我们可以使用此参数部署自己的CA以管理所有证书。我们还可以指定证书的旋转频率。默认情况下，证书每90天（2160小时）自动重新创建。'
- en: '`--listen-addr`: This option allows us to specify which host interface will
    be used to serve the cluster API. We can use IP address/port or host interface
    notation, with the default of `0.0.0.0:2377`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--listen-addr`：此选项允许我们指定将用于提供集群API的主机接口。我们可以使用IP地址/端口或主机接口表示法，默认值为`0.0.0.0:2377`。'
- en: '`--autolock`: As we mentioned previously, we can lock access to internal Docker
    Swarm data. This is important because `/var/lib/docker/swarm` contains the CA
    and other certificates. If you are not sure about node access, it is better to
    lock this directory from users. Take care with this option because any system
    or Docker daemon restart will require the unlock key in order to enable this node
    in the cluster again.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--autolock`：正如我们之前提到的，我们可以锁定对内部Docker Swarm数据的访问。这很重要，因为`/var/lib/docker/swarm`包含了CA和其他证书。如果您不确定节点访问权限，最好将此目录锁定，以防止用户访问。使用此选项时要小心，因为任何系统或Docker守护进程的重启都需要解锁密钥才能再次启用此节点。'
- en: '`--dispatcher-heartbeat`: This option will manage how often nodes will report
    their health. It defaults to 5 seconds, but you can change it if your cluster
    has high latency.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--dispatcher-heartbeat`：此选项将管理节点报告健康状况的频率。默认值为5秒，但如果您的集群存在高延迟，可以进行更改。'
- en: '`--max-snapshots`/`--snapshot-interval`: Swarm will take database snapshots
    for manager synchronization. We can set the number of snapshots to keep. By default,
    none will be kept (just one for synchronization), but these can be useful for
    debugging or disaster recovery. We can also set the interval between snapshots.
    Take care when changing this option because having many snapshots will trigger
    a lot of sync operations to other nodes and can incur high-performance costs.
    But on the other hand, syncing less frequently can guide the cluster to non-synced
    states. This parameter defaults to 10,000 ms.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--max-snapshots`/`--snapshot-interval`: Swarm将为管理节点的同步创建数据库快照。我们可以设置保留的快照数量。默认情况下，不会保留任何快照（只为同步保留一个），但这些快照在调试或灾难恢复时可能非常有用。我们还可以设置快照之间的间隔。更改此选项时要小心，因为保存过多的快照会触发大量的同步操作到其他节点，可能会导致高性能开销。但另一方面，较少的同步也可能使集群进入不同步的状态。此参数的默认值为10,000毫秒。'
- en: '`join`: After cluster initialization, all the other nodes will join the previously
    created cluster, regardless of whether they are managers or workers. Joining Docker
    nodes to a cluster requires a cluster-specific token, with different tokens for
    the manager and worker nodes. We will always require a token and the leader IP
    address to join the cluster. Remember that the leader can change from time to
    time. We will also be able to set the control plane''s IP and port, the IP address
    to be announced to the other nodes, and the listen IP address for the API. We
    will execute this command on the joining node using the following format: `docker
    swarm join --token <MANAGER_OR_WORKER_TOKEN> <LEADER_IP:PORT>`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`join`: 集群初始化后，所有其他节点将加入先前创建的集群，无论它们是管理节点还是工作节点。将Docker节点加入集群需要集群特定的令牌，管理节点和工作节点的令牌不同。我们始终需要一个令牌和领导者的IP地址来加入集群。请记住，领导者的IP可能会发生变化。我们还可以设置控制平面的IP和端口、要向其他节点公布的IP地址以及API的监听IP地址。我们将在加入节点上执行以下命令：`docker
    swarm join --token <MANAGER_OR_WORKER_TOKEN> <LEADER_IP:PORT>`。'
- en: '`leave`: Once a node is part of the cluster, it can leave it whenever we need
    it to. It is important to understand what it means to *leave* the cluster. The
    `leave` command will be executed on the node leaving the cluster. Manager nodes
    are not able to leave the cluster because this would force the cluster into an
    unhealthy state. We can use `--force` to make a node leave the cluster, even if
    it is a manager node, but this comes with risks that you need to understand before
    proceeding. Leaving the cluster will not remove the node from the internal Docker
    Swarm database. Rather, we need to inform the managers of this change by issuing
    `docker node rm <NODE_NAME_OR_ID>`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leave`: 一旦节点成为集群的一部分，我们可以根据需要让其退出集群。理解*退出*集群的含义很重要。`leave`命令将在退出集群的节点上执行。管理节点无法退出集群，因为这样会导致集群进入不健康状态。我们可以使用`--force`强制让节点退出集群，即使它是管理节点，但这会带来一些风险，必须在操作前充分理解。退出集群并不会将节点从内部的Docker
    Swarm数据库中移除。相反，我们需要通过执行`docker node rm <NODE_NAME_OR_ID>`命令来通知管理节点这一变化。'
- en: '`update`: With this action, we can change some of the Docker Swarm cluster''s
    described properties, such as the external CA, certificate expiration settings,
    and snapshot behavior.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update`: 通过此操作，我们可以更改Docker Swarm集群的一些描述属性，例如外部CA、证书过期设置和快照行为。'
- en: '`ca`: As we mentioned previously, all internal control plane communication
    is based on TLS certificates. The `ca` option allows us to customize the CA and
    other certificate behavior. We can rotate them or choose our own CA.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ca`: 如前所述，所有内部控制平面的通信都是基于TLS证书的。`ca`选项允许我们自定义CA和其他证书行为。我们可以轮换证书或选择自己的CA。'
- en: '`join-token`: With this action, we can review the current tokens for managers
    and workers. In fact, we can execute `join-token`, followed by the required role,
    to retrieve their values. We do not need to keep them safe since we can retrieve
    them as needed. These tokens are only used when joining the cluster. We can change
    them whenever we want, using `docker swarm join-token --rotate` to create a new
    one. This will not affect already joined nodes. We usually execute `docker swarm
    join-token worker` to retrieve the command line and token required to join the
    node to the cluster. We can use `--quiet` to retrieve only the token, which is
    useful for automating the joining process.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`join-token`：通过此操作，我们可以查看当前的管理器和工作节点令牌。事实上，我们可以执行`join-token`，后跟所需的角色，以检索它们的值。我们不需要将其保管，因为我们可以根据需要随时检索这些令牌。这些令牌仅在加入集群时使用。我们可以随时更改它们，使用`docker
    swarm join-token --rotate`来创建一个新的令牌，这不会影响已经加入的节点。我们通常执行`docker swarm join-token
    worker`来检索加入节点到集群所需的命令行和令牌。我们可以使用`--quiet`仅检索令牌，这对自动化加入过程非常有用。'
- en: '`unlock`/`unlock-key`: We mentioned previously that it is unsafe to allow users
    to access the `/var/lib/docker` directory. Access is only allowed to root by default,
    but it is even more secure to lock Docker Swarm information. For example, all
    cluster certificates will be stored under the `/var/lib/docker/swarm/certificates`
    directory. Locking Swarm information is a good practice, but be aware of losing
    your unlock key. Every time the cluster node starts (such as a Docker Engine or
    node restart, for example), the unlock key will be required. This leaves the cluster
    in a non-automatic, high-availability environment in some situations. The `unlock`
    option unlocks the Docker Swarm cluster information, while `unlock-key` allows
    us to manage the defined key used for this behavior.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unlock`/`unlock-key`：我们之前提到过，允许用户访问`/var/lib/docker`目录是不安全的。默认情况下，只有 root
    用户允许访问，但将 Docker Swarm 信息加锁会更安全。例如，所有集群证书将存储在`/var/lib/docker/swarm/certificates`目录下。加锁
    Swarm 信息是一个好习惯，但要注意不要丢失解锁密钥。每次集群节点启动时（例如 Docker Engine 或节点重启时），都需要解锁密钥。这在某些情况下会导致集群处于非自动、高可用环境。`unlock`选项用于解锁
    Docker Swarm 集群信息，而`unlock-key`允许我们管理用于此行为的密钥。'
- en: 'Docker Swarm will also create new objects:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 还会创建新的对象：
- en: '`swarm`: This is the cluster itself, along with its own properties, as described
    earlier.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`swarm`：这就是集群本身及其相关的属性，如前所述。'
- en: '`node`: These are the nodes that are part of the cluster. We will add labels
    to them and manage their roles as part of the cluster.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node`：这些是集群中的节点。我们将为它们添加标签，并作为集群的一部分管理它们的角色。'
- en: '`service`: We deploy services on our Docker Swarm cluster. We won''t deploy
    a standalone container. We will learn more about services in the *Scheduling workloads
    in the cluster – tasks and services* section.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service`：我们在 Docker Swarm 集群上部署服务。我们不会部署独立的容器。我们将在*集群中的工作负载调度——任务和服务*部分学习更多关于服务的内容。'
- en: '`secret` and `config`: Both objects allow us to share service configurations
    in the cluster. Remember, it is not easy to manage information on different hosts,
    even if the application is completely stateless.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`secret` 和 `config`：这两个对象允许我们在集群中共享服务配置。请记住，即使应用程序完全无状态，管理不同主机上的信息也并不容易。'
- en: '`stack`: We will use stacks to deploy applications. We will use a Docker Compose-like
    file format containing all application components and their interactions.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stack`：我们将使用堆栈来部署应用程序。我们将使用类似 Docker Compose 的文件格式，其中包含所有应用程序组件及其交互。'
- en: All these objects will have common actions associated with them, including listing,
    deploying/creating, removing, and inspecting their properties. Services and stacks
    will have containers associated with them, so we will be able to list their processes'
    distributions cluster-wide.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些对象都将与之关联常见的操作，包括列出、部署/创建、删除和检查其属性。服务和堆栈将与容器相关联，因此我们将能够列出集群范围内的进程分布。
- en: We can run a single node cluster on our laptop. It is not a problem running
    a single node cluster for testing or developing services or stacks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在笔记本电脑上运行单节点集群。对于测试或开发服务或堆栈，运行单节点集群并不成问题。
- en: In the next section, we will learn how to deploy a Docker Swarm environment
    with high availability.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何部署具有高可用性的 Docker Swarm 环境。
- en: Deploying Docker Swarm with high availability
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署具有高可用性的 Docker Swarm
- en: So far, we have learned about the different roles in Docker Swarm clusters.
    However, in order to provide high availability, we will need to deploy more than
    one manager and worker.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了Docker Swarm集群中不同的角色。然而，为了提供高可用性，我们需要部署多个管理节点和工作节点。
- en: 'The Raft consensus algorithm requires an odd number of healthy nodes to work
    because a majority of the nodes must agree on all the changes and resources states.
    This means that we will need at least *N/2+1* healthy nodes to agree before committing
    a change or resource state. In other words, we will not grant Docker Swarm availability
    if fewer than *N/2+1* manager nodes are healthy. Let''s review the options in
    the following table to get a better understanding:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Raft一致性算法要求有奇数个健康节点才能工作，因为大多数节点必须就所有变更和资源状态达成一致。这意味着我们需要至少*N/2+1*个健康节点达成一致才能提交变更或资源状态。换句话说，如果少于*N/2+1*个管理节点健康，我们将无法保证Docker
    Swarm的可用性。让我们通过下表回顾这些选项，以便更好地理解：
- en: '| **Number of managers** | **Required for consensus (*N/2+1)*** | **Allowed
    failures** | **Provides high availability?** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **管理节点数量** | **所需共识数量 (*N/2+1)** | **允许的故障数** | **提供高可用性？** |'
- en: '| 1 | 1 | 0 | No. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 否。 |'
- en: '| 2 | 2 | 0 | No. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 0 | 否。 |'
- en: '| 3 | 2 | 1 | Yes. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1 | 是的。 |'
- en: '| 4 | 3 | 1 | Yes, but this is not better than the three-manager option and
    can lead to election problems if the leader fails. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 1 | 是的，但这不比三节点管理选项更好，如果领导节点失败，可能会导致选举问题。 |'
- en: '| 5 | 3 | 2 | Yes. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 3 | 2 | 是的。 |'
- en: '| 6 | 4 | 2 | Yes, but this is not better than the five-manager option and
    can lead to election problems if the leader fails. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 4 | 2 | 是的，但这不比五节点管理选项更好，如果领导节点失败，可能会导致选举问题。 |'
- en: '| 7 | 4 | 3 | Yes. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4 | 3 | 是的。 |'
- en: When a manager fails in the 3-manager configuration, two nodes can agree and
    changes will be updated without problems. But if one of those fails, only one
    will be left and changes can't be committed. There is no consensus and no cluster
    operations can be deployed. This means that any service deployed in the cluster
    will stay running. Users will not be affected unless one service loses some replicas
    and Docker Swarm should have started new ones to achieve the required number.
    No automatic actions will be allowed because these have to update the database
    data, and this is not permitted. We will not be able to add or remove any nodes
    in that situation, so the cluster will be inconsistent.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个管理节点在3节点管理配置中失败时，两个节点可以达成一致，变更将不受问题影响地更新。但如果其中一个节点失败，只剩下一个节点，变更将无法提交。没有共识，也无法进行集群操作。这意味着，集群中部署的任何服务将继续运行。除非某个服务失去了一些副本，并且Docker
    Swarm应当启动新的副本以达到所需数量，否则用户不会受到影响。由于这些操作需要更新数据库数据，而这在此情况下是不允许的，因此不允许自动操作。在这种情况下，我们将无法添加或移除任何节点，集群将变得不一致。
- en: Consequently, Swarm requires an odd number of managers to provide high availability.
    Although there is no limit regarding the number of manager nodes, more than seven
    is not recommended. Increasing the number of managers will reduce write performance
    because the leader will require more acknowledged responses from more nodes to
    update cluster changes. This will result in more round-trip network traffic.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Swarm需要奇数个管理节点来提供高可用性。虽然管理节点的数量没有限制，但不推荐超过七个。增加管理节点的数量会降低写入性能，因为领导节点需要更多节点的确认响应才能更新集群变更。这将导致更多的网络往返流量。
- en: It is key to understand these behaviors. Even if we have deployed a three-node
    cluster, we can still lose quorum if a sufficient number of nodes become unhealthy.
    It is important to attend to node failures as soon as possible.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些行为至关重要。即使我们已经部署了一个三节点集群，如果有足够多的节点变得不健康，我们仍然可能会失去法定人数。重要的是要尽快处理节点故障。
- en: We will usually deploy three-node clusters because they allow for the failure
    of 1 node. It is enough for production, but in some critical environments, we
    will deploy five-node clusters to allow for two node failures.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会部署三节点集群，因为它们允许1个节点的故障。这对生产环境来说已经足够，但在一些关键环境中，我们将部署五节点集群，以便允许两个节点的故障。
- en: In cases where a Swarm cluster needs to be distributed between different locations,
    the recommended number of managers is seven. It will allow distribution across
    multiple data centers. We will deploy three nodes in the first data center, two
    in the second data center, and a final two in the third data center (3+2+2). This
    distribution will allow us to handle a full data center failure with services
    being redistributed if worker nodes have sufficient resources.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要将 Swarm 集群分布在不同位置的情况下，推荐的管理节点数量是七个。这将允许跨多个数据中心进行分布。我们将在第一个数据中心部署三个节点，在第二个数据中心部署两个节点，在第三个数据中心部署两个节点（3+2+2）。这种分布将使我们能够处理整个数据中心的故障，如果工作节点有足够的资源，服务可以重新分配。
- en: What happens when a manager node fails? The leader will start to store committed
    changes in order to sync the unhealthy manager node when it is ready again. This
    will increase Docker Swarm's directory size. If you did not set your node disk
    space sufficiently to allow for these situations, your nodes will probably consume
    your entire filesystem if the failure doesn't recover soon. And then, you will
    get a second unhealthy node and your cluster will be inconsistent. This situation
    we've described is not a horror movie – it happens all too often on new installations
    where administrators think that the cluster will be alright with some unhealthy
    nodes for weeks at a time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当管理节点失败时会发生什么？领导节点将开始存储已提交的更改，以便在该管理节点重新恢复时同步它。这将增加 Docker Swarm 的目录大小。如果没有足够的磁盘空间来应对这种情况，节点可能会消耗整个文件系统，特别是在故障未能及时恢复的情况下。然后，你将得到第二个不健康的节点，集群将变得不一致。我们所描述的情况并不是恐怖电影——在新安装中，管理员通常认为集群可以在几周内容忍一些不健康的节点，这种情况发生得太频繁。
- en: We mentioned one important option in the `docker swarm` command-line table when
    we talked about Docker Swarm cluster initialization. We will use `docker swarm
    init --force-new-cluster` in situations where the cluster is unhealthy, but at
    least one manager is working. If the cluster isn't quorate and no operations can
    be performed with cluster resources (that is, nodes can't be added/removed and
    services won't be repaired if they fail), we can force a new cluster. This is
    an extreme situation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论 Docker Swarm 集群初始化时提到过一个重要的选项，即 `docker swarm` 命令行表中的选项。在集群不健康但至少有一个管理节点正常工作的情况下，我们将使用
    `docker swarm init --force-new-cluster`。如果集群未能达到法定人数并且无法对集群资源执行任何操作（即无法添加/删除节点，服务在失败后无法修复），我们可以强制创建一个新集群。这是一个极端情况。
- en: Take care of your environment before recreating the cluster. Forcing a new cluster
    will set the node where the command was executed as the leader. All other nodes
    in the cluster (including those managers that were insufficient for a quorum)
    will be set as workers. It is like a **cluster quorum reset**. Services and other
    resources will retain their states and configurations (as they were committed
    or retrieved from nodes). Therefore, we will end up with a one-manager node cluster
    with all the other nodes as workers. Services and other stuff should not be affected.
    In these situations, it is a good practice to review the manager node's logs because
    some containers can be left unmanaged if some cluster changes were not committed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新创建集群之前，请先处理好环境。强制创建新集群会将执行命令的节点设置为领导节点。集群中的所有其他节点（包括那些未能达到法定人数的管理节点）将被设置为工作节点。这就像是**集群法定人数重置**。服务和其他资源将保留其状态和配置（无论是已提交的还是从节点中恢复的）。因此，最终我们将得到一个单管理节点的集群，所有其他节点都将是工作节点。服务和其他内容不应受到影响。在这种情况下，查看管理节点的日志是一个好习惯，因为如果某些集群更改没有被提交，某些容器可能会被遗留在未管理状态。
- en: Although managers can act as workers, it is safer in production to run workloads
    on worker-role nodes only. A manager's processes may impact the application and
    vice versa.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然管理节点可以充当工作节点，但在生产环境中，最好仅在工作角色节点上运行工作负载。管理节点的进程可能会影响应用程序，反之亦然。
- en: We will always deploy more than one worker in production environments. This
    will ensure the health of our services if one of the workers goes offline unexpectedly
    or if we need to perform any maintenance tasks, such as updating Docker Engine.
    We should usually deploy worker nodes according to our application's resource
    requirements. Adding workers will increase the total cluster workload capacity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在生产环境中始终会部署多个工作节点。这将确保我们的服务在某个工作节点意外下线或需要执行维护任务（例如更新 Docker 引擎）时，服务的健康状态不会受到影响。通常，我们应该根据应用程序的资源需求部署工作节点。增加工作节点将提高集群的总工作负载容量。
- en: In the next section, we will learn how to deploy a Docker Swarm cluster.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何部署 Docker Swarm 集群。
- en: Creating a Docker Swarm cluster
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Docker Swarm 集群
- en: 'Now that we have reviewed the Docker Swarm architecture and the command-line
    actions required to initialize the cluster, we can create a cluster. By the end
    of this chapter, we will have a fully functional cluster with high availability.
    Let''s start by reviewing the Docker Swarm cluster creation process:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了 Docker Swarm 架构以及初始化集群所需的命令行操作，我们可以创建一个集群。到本章结束时，我们将拥有一个具有高可用性的完全功能集群。让我们先回顾一下
    Docker Swarm 集群创建过程：
- en: 'First, we initialize a Swarm cluster on a manager node. This node automatically
    becomes the cluster leader because no other manager is available. If we have a
    node with multiple interfaces, we will choose which interface will be associated
    with the control plane and which ones will be announced for other nodes and the
    Swarm API. The output will vary from the following in your environment. Let''s
    execute `docker swarm init`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们在管理节点上初始化一个 Swarm 集群。由于没有其他管理节点可用，当前节点自动成为集群领导者。如果我们有一个具有多个接口的节点，我们将选择哪个接口与控制平面关联，哪些接口将用于其他节点和
    Swarm API。输出将在您的环境中有所不同。让我们执行`docker swarm init`：
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the cluster has been created, we can review the cluster nodes and their
    properties by using `docker node ls`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦集群创建完成，我们可以使用`docker node ls`来查看集群节点及其属性：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first column shows the node object identifier. As we mentioned previously,
    new objects have been created with Docker Swarm. The second column shows its name
    from the internal host resolution service (this may contain a **Fully Qualified
    Domain Name** (**FQDN**)). Notice the asterisk near the hostname. This means that
    we are working on this node right now. All the commands are executed on that node,
    regardless of whether it is a leader.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列显示节点对象标识符。正如我们之前提到的，新的对象已通过 Docker Swarm 创建。第二列显示其来自内部主机解析服务的名称（这可能包含**完全限定域名**（**FQDN**））。请注意主机名旁边的星号。这意味着我们当前正在操作该节点。所有命令都在该节点上执行，无论它是否是领导节点。
- en: On Docker Swarm, cluster commands related to cluster-wide objects are only available
    on manager nodes. We won't need to execute commands on the leader node, but we
    won't be able to execute any cluster commands on a worker node. We can't list
    nodes or deploy a service.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker Swarm 上，与集群范围对象相关的集群命令仅在管理节点上可用。我们不需要在领导节点上执行命令，但在工作节点上无法执行任何集群命令。我们不能列出节点或部署服务。
- en: 'The last column shows each node''s Docker Engine version. Let''s take a look
    at the `STATUS`, `AVAILABILITY`, and `MANAGER STATUS` columns:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一列显示每个节点的 Docker Engine 版本。让我们来看一下`STATUS`、`AVAILABILITY`和`MANAGER STATUS`列：
- en: '`STATUS`, as its name suggests, shows the status of the node within the cluster.
    If it is not healthy, it will be shown here.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STATUS`，顾名思义，显示节点在集群中的状态。如果节点不健康，它将在此显示。'
- en: '`MANAGER STATUS` shows the current role of the node (in this case, the node
    is the leader). We have three different states:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MANAGER STATUS`显示节点的当前角色（在这种情况下，该节点是领导者）。我们有三种不同的状态：'
- en: '`Leader`, when the node is the cluster leader.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Leader`，当该节点是集群领导者时。'
- en: '`Manager`, which means that the node is one of the cluster managers.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Manager`，意味着该节点是集群的管理节点之一。'
- en: An empty value will mean that the node has a worker role, and is therefore not
    part of the control plane.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果值为空，则表示该节点具有工作节点角色，因此不属于控制平面的一部分。
- en: '`AVAILABILITY` represents a node''s availability to receive workloads. Here,
    we can see that managers can receive workloads too. We can set this node property.
    In fact, there are three different states:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AVAILABILITY`表示节点接收工作负载的可用性。在这里，我们可以看到管理节点也能接收工作负载。我们可以设置这个节点属性。实际上，有三种不同的状态：'
- en: '`active`, which means that the node will be able to receive any workload.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`active`，意味着该节点能够接收任何工作负载。'
- en: '`passive`, which means that the node will not run any other additional workload.
    Those already running will maintain their state, but no additional workloads will
    be allowed.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`passive`，意味着该节点不会运行任何额外的工作负载。已经在运行的工作负载将保持其状态，但不会允许新的工作负载。'
- en: '`drain` is the state that we get when we disable any workload on this node.
    When this happens, all running workloads on the node will be moved to any other
    healthy and available node.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drain`是当我们禁用该节点上的任何工作负载时的状态。发生这种情况时，所有正在运行的工作负载将被移到任何其他健康且可用的节点上。'
- en: We can enforce the behavior of any node when joining the cluster, or even when
    we create the cluster, using the `--availability` flag with `docker swarm init`
    or `docker swarm join`. We will set the node availability for new workloads (`active`
    | `pause` | `drain`). By default, all the nodes will be active and ready to receive
    workloads.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在节点加入集群时，甚至在我们创建集群时，通过使用`docker swarm init`或`docker swarm join`命令中的`--availability`标志来强制节点的行为。我们将为新工作负载设置节点可用性（`active`
    | `pause` | `drain`）。默认情况下，所有节点都将处于活动状态，准备接收工作负载。
- en: 'We will join another node as a worker to demonstrate this, using the previously
    shown cluster initialization output with `docker swarm join`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加入另一个节点作为工作节点来演示此操作，使用之前显示的集群初始化输出和`docker swarm join`：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we can review the cluster node status (remember, this command will only
    be available on manager nodes) once more by executing `docker node ls`:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过执行`docker node ls`再次查看集群节点状态（记住，这个命令仅在管理节点上可用）：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we are executing commands on the `sirius` node (marked with
    `*`), which is a leader and hence a manager. Notice that `antares` is a worker
    node because it has an empty value in the `MANAGER STATUS` column.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们在`*`标记的`sirius`节点上执行命令，它是一个领导者，因此是一个管理节点。注意，`antares`是一个工作节点，因为它在`MANAGER
    STATUS`列中没有值。
- en: 'We can review node information by executing the `docker node inspect` action
    (the following output is truncated):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行`docker node inspect`操作来查看节点信息（以下输出已被截断）：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When we inspect a node, information regarding its status, node IP address, and
    TLS information will be shown in JSON format.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查一个节点时，关于其状态、节点 IP 地址和 TLS 信息将以 JSON 格式显示。
- en: We can use labels on nodes to help Docker Swarm choose the best location for
    specific workloads. It uses node architectures to deploy workloads in the right
    place, but if we want a workload to run on a specific node, we can add a unique
    label and add a constraint to deploy the workload. We will learn more about service
    locations and labels in the *Chapter labs* section.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在节点上使用标签，帮助 Docker Swarm 为特定工作负载选择最佳位置。它使用节点架构将工作负载部署到合适的地方，但如果我们希望工作负载在特定节点上运行，可以添加一个独特的标签并添加约束来部署该工作负载。我们将在*章节实验*部分进一步学习服务位置和标签。
- en: Under the `Spec` key, we can review the node role in the `docker node inspect`
    output. We can change the node role whenever necessary. This is a big improvement
    over other orchestrators, where roles are static. Keep in mind that role changes
    will affect your Docker Swarm architecture because it will change the number of
    managers and worker nodes. Keep high availability in mind, its requirement of
    an odd number of managers, and the consequences of this in case of node failures.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Spec`键下，我们可以在`docker node inspect`输出中查看节点角色。我们可以在必要时更改节点角色。这相较于其他编排器是一个巨大改进，因为其他编排器的角色是静态的。请记住，角色更改将影响
    Docker Swarm 架构，因为它会改变管理节点和工作节点的数量。记住高可用性要求管理节点数量为奇数，以及在节点故障时的后果。
- en: 'A role is just a node property, which means we can change it just like any
    other object property. Remember that changes can only be deployed from manager
    nodes. We change a node''s role by executing `docker node update`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 角色只是一个节点属性，这意味着我们可以像更改其他对象属性一样更改它。记住，更改只能从管理节点部署。我们可以通过执行`docker node update`来更改节点的角色：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once again, let''s list all the nodes in the cluster by executing `docker node
    ls`, this time with a filter to retrieve only managers:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次执行`docker node ls`列出集群中的所有节点，这次使用筛选器只获取管理节点：
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now use `docker node inspect` to retrieve the `ManagerStatus` key:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`docker node inspect`来获取`ManagerStatus`键：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Nodes can be removed from the cluster by using `docker node rm`, just as we
    did with other Docker objects. We will only remove worker nodes. The usual sequence
    for removing a manager node from a Docker Swarm cluster will require a previous
    step to change its role to a worker. Once a node role has changed to a worker,
    we can remove the node. If we need to remove a failed manager, we can force node
    removal using `--force`. However, this is not recommended as you can leave the
    cluster in an inconsistent state. The manager's database should be updated before
    you remove any node, which is why the removal sequence we've described here is
    so important.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`docker node rm`将节点从集群中移除，就像我们对其他 Docker 对象所做的那样。我们只会移除工作节点。通常，将管理节点从 Docker
    Swarm 集群中移除需要先将其角色更改为工作节点。一旦节点角色更改为工作节点，我们就可以移除该节点。如果需要移除故障的管理节点，可以使用`--force`强制移除节点。但不推荐这么做，因为这可能会使集群处于不一致状态。必须在移除任何节点之前更新管理节点的数据库，这也是我们在这里描述的移除顺序如此重要的原因。
- en: Remember to make sure that you have an odd number of manager nodes if you demote
    or remove any manager. If you have problems with the leader when you do not have
    an odd number of managers, you can reach an inconsistent state when other managers
    have to elect a new leader.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，如果降级或移除任何管理节点，确保管理节点的数量为奇数。如果没有奇数个管理节点，且在没有奇数个管理节点的情况下出现领导节点问题，当其他管理节点需要选举新领导时，可能会导致集群状态不一致。
- en: As we mentioned previously, labels are node properties. We can add and remove
    them at runtime. This is a big difference compared to the labels learned about
    in [Chapter 1](c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml), *Modern Infrastructures
    and Applications with Docker*. Those labels were set at the Docker daemon level
    and are static. We needed to add them to the `daemon.json` file, so we were required
    to restart the node's Docker Engine to make them effective. In this case, however,
    node labels are managed by Docker Swarm and can be changed with the common node
    object's `update` action (`docker node update`).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标签是节点的属性。我们可以在运行时添加和删除它们。这与[第1章](c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml)《使用
    Docker 的现代基础设施与应用程序》中介绍的标签有很大的不同。那些标签是在 Docker 守护进程级别设置的，并且是静态的。我们需要将它们添加到`daemon.json`文件中，因此必须重新启动节点的
    Docker 引擎才能使其生效。而在本例中，节点标签由 Docker Swarm 管理，可以通过常见的节点对象的`update`操作（`docker node
    update`）来更改。
- en: The Docker command line provides some shortcuts, as we have observed in previous
    chapters. In this case, we can change node roles by demoting a manager to a worker
    role, or by promoting a worker to a manager role. We use `docker node <promote|demote>
    <NODENAME_OR_ID>` to change between node roles.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前几章中观察到的那样，Docker 命令行提供了一些快捷方式。在这种情况下，我们可以通过将管理节点降级为工作节点角色，或将工作节点提升为管理节点角色来更改节点角色。我们使用`docker
    node <promote|demote> <NODENAME_OR_ID>`来在节点角色之间进行切换。
- en: We can also change a node's workload availability. This allows a node to receive
    (or not) cluster-deployed workloads. As with any other node property, we will
    use `docker node update --availability <available|drain|pause> <NODENAME_OR_ID>`
    to drain or pause a node when it was active. Both drain and pause will prevent
    us from scheduling any new workload on the node, while drain on its own will remove
    any currently running one from the affected node.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更改节点的工作负载可用性。这使得节点可以接收（或不接收）集群部署的工作负载。与任何其他节点属性一样，我们将使用`docker node update
    --availability <available|drain|pause> <NODENAME_OR_ID>`来排空或暂停处于活动状态的节点。无论是排空还是暂停，都会阻止我们在该节点上调度任何新工作负载，而仅排空则会将任何当前运行的工作负载从受影响的节点上移除。
- en: Remember that when we drain a node, the scheduler will reassign any tasks running
    on the affected node to another available worker. Keep in mind that the other
    nodes should have enough resources before draining the given node.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当我们排空一个节点时，调度器会将任何正在该节点上运行的任务重新分配到其他可用的工作节点。请记住，其他节点在排空该节点之前应该有足够的资源。
- en: In the next section, we will review how to back up and recover a faulty Docker
    Swarm cluster.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾如何备份和恢复故障的 Docker Swarm 集群。
- en: Recovering a faulty Docker Swarm cluster
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恢复故障的 Docker Swarm 集群
- en: We will review a few steps to back up and restore Docker Swarm clusters. Losing
    your cluster quorum is not a big problem. As we have learned, we can recover the
    cluster by forcing the initialization of a new one, even with just one healthy
    manager. However, losing your cluster data will completely destroy your environment
    if you don't have any manager nodes that are operational and working correctly.
    In these situations, we can recover the cluster by restoring a copy containing
    healthy data that was taken when the cluster was running correctly. Let's learn
    how to take backups of our clusters now.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回顾一些备份和恢复Docker Swarm集群的步骤。丢失集群法定人数并不是什么大问题。正如我们所学，即使只有一个健康的管理节点，我们也可以通过强制初始化一个新集群来恢复集群。但是，如果丢失了集群数据，将完全摧毁您的环境，前提是没有任何一个操作正常的管理节点。在这些情况下，我们可以通过恢复在集群正常运行时采集的包含健康数据的副本来恢复集群。现在让我们学习如何备份我们的集群。
- en: Backing up your Swarm
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备份您的Swarm
- en: As we learned in this chapter, `/var/lib/docker/swarm` (and its Microsoft Windows
    equivalent directory) contains the key-value store data, the certificates, and
    the encrypted Raft logs. Without them, we can't recover a faulty cluster, so let's
    back up this directory on any manager.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中所学，`/var/lib/docker/swarm`（以及其对应的微软Windows目录）包含了键值存储数据、证书和加密的Raft日志。没有它们，我们无法恢复故障集群，因此让我们在任何一个管理节点上备份这个目录。
- en: Having a consistent backup requires static files. If files are opened or some
    process is writing them, they will not be consistent. Therefore, we need to stop
    Docker Engine on the given node. Do not launch the backup procedure on the leader
    node.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 保持一致的备份需要静态文件。如果文件被打开或某些进程正在写入它们，那么它们将不一致。因此，我们需要停止指定节点上的Docker Engine。不要在主节点上启动备份操作。
- en: Keep in mind that while the backup operation is running, if the Docker daemon
    is stopped, the number of managers will be affected. The leader will continue
    managing changes and generating new sync points to recover synchronization with
    the lost manager. Your cluster will be vulnerable to losing quorum if other managers
    fail. If you plan to do daily backups, consider using five managers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在备份操作进行时，如果Docker守护进程被停止，管理节点的数量将会受到影响。领导节点将继续管理更改，并生成新的同步点以恢复与丢失管理节点的同步。如果其他管理节点失败，您的集群将容易丧失法定人数。如果您计划进行每日备份，建议使用五个管理节点。
- en: Recovering your Swarm
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复您的Swarm
- en: In case we need to recover a completely failed cluster (where no managers can't
    achieve quorum and we can't force a new cluster), we will stop Docker Engine on
    one manager. Remove all `/var/lib/docker/swarm` directory content (or its Microsoft
    Windows equivalent) and restore the backed-up content to this directory. Then,
    start Docker Engine again and reinitialize the cluster with `docker swarm init
    --force-new-cluster`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要恢复一个完全失败的集群（即所有管理节点无法达到法定人数，并且我们无法强制创建一个新集群），我们将停止一个管理节点上的Docker Engine。删除所有`/var/lib/docker/swarm`目录内容（或其微软Windows对应目录）并将备份的内容恢复到该目录。然后，重新启动Docker
    Engine，并使用`docker swarm init --force-new-cluster`重新初始化集群。
- en: When the single-manager cluster is healthy, start to add the other old Swarm
    cluster managers. Before adding those managers, ensure that they've left the old
    Swarm cluster.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当单节点管理的集群健康时，开始添加其他旧Swarm集群的管理节点。在添加这些管理节点之前，确保它们已经退出了旧的Swarm集群。
- en: If we set up Swarm auto-lock, we will need the key that was stored with the
    restored backup. Even if you changed it after the backup was issued, you will
    still need the old one.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们启用了Swarm自动锁定功能，我们将需要与恢复的备份一起存储的密钥。即使您在备份发出后更改了密钥，您仍然需要使用旧的密钥。
- en: In the next section, we will learn how workloads are deployed on the cluster
    and how Docker Swarm tracks the health of application components to ensure that
    services are not impacted when something goes wrong.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何在集群上部署工作负载，以及Docker Swarm如何跟踪应用程序组件的健康状况，以确保在出现故障时服务不受影响。
- en: Scheduling workloads in the cluster – tasks and services
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在集群中调度工作负载 —— 任务和服务
- en: We don't run containers on a Swarm cluster; rather, we deploy services. These
    are atomic workloads that can be deployed in a Docker Swarm cluster. Services
    are defined by tasks, and each task is represented by a container in the Docker
    Swarm model. Swarm is based on SwarmKit and its logic is inherited. SwarmKit was
    created as a response to clustering any kind of task (such as virtual machines,
    for example), but Docker Swarm works with containers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不在 Swarm 集群上运行容器，而是部署服务。这些是可以在 Docker Swarm 集群中部署的原子工作负载。服务由任务定义，每个任务在 Docker
    Swarm 模型中由一个容器表示。Swarm 基于 SwarmKit，并继承了其逻辑。SwarmKit 是应对任何任务（例如虚拟机）集群化的需求而创建的，但
    Docker Swarm 是与容器协同工作的。
- en: The Docker Swarm orchestrator uses a declarative model. This means that we define
    the desired state for our services and Docker Swarm will take care of the rest.
    If the defined number of replicas or tasks for a service is wrong – for example,
    if one of them died – Docker Swarm will take action to recover the correct state
    of the service. In this example, it will deploy a new replica to keep all the
    required nodes healthy.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 调度器使用声明式模型。这意味着我们定义服务的期望状态，Docker Swarm 会处理其余的部分。如果服务的副本数或任务数不正确——例如，如果其中一个副本挂掉——Docker
    Swarm 将采取措施恢复服务的正确状态。在这个例子中，它会部署一个新的副本，以保持所有所需节点的健康。
- en: 'The following diagram represents services and tasks in relation to containers.
    The `colors` service has five replicas (`colors.1` to `colors.5`). Each replica
    runs on one container from the same image, `codegazers/colors:1.13`, and these
    containers run distributed cluster-wide across `node1`, `node2`, and `node3`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示表示了与容器相关的服务和任务。`colors` 服务有五个副本（`colors.1` 到 `colors.5`）。每个副本都在同一镜像 `codegazers/colors:1.13`
    的一个容器上运行，这些容器在 `node1`、`node2` 和 `node3` 上分布式运行：
- en: '![](img/a777ffe3-5e76-482f-9bb3-415cbbed3f62.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a777ffe3-5e76-482f-9bb3-415cbbed3f62.jpg)'
- en: 'Service creation requires the following information:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务需要以下信息：
- en: Which image will run the associated containers?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个镜像将运行关联的容器？
- en: How many containers does this service require to be healthy?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该服务需要多少个容器才能保持健康？
- en: Should the service be available to users on any port and protocol?
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该服务是否应该在任何端口和协议上对用户可用？
- en: How should service updates be managed?
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务更新应如何管理？
- en: Is there any preferred location for this service to run?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有该服务运行的首选位置？
- en: 'Service creation will require all this information to be entered on the command
    line. Since services are Docker objects, we can use common actions such as listing,
    creating, removing, updating, and inspecting their properties. Docker Swarm will
    manage all our tasks'' integration with services. We will never deploy tasks or
    containers. We will just create and manage services. Let''s take a look at Docker
    command-line actions and options related to services:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务时需要在命令行输入所有这些信息。由于服务是 Docker 对象，我们可以使用常见的操作，如列出、创建、删除、更新和检查它们的属性。Docker
    Swarm 将管理所有任务与服务的集成。我们永远不会部署任务或容器，只会创建和管理服务。让我们来看一下与服务相关的 Docker 命令行操作和选项：
- en: '`create`: This is common to other objects, but services have many non-standard
    properties. We will not list and review all service arguments because most of
    them are inherited from containers. Here, we''ll review the most important ones
    related to service behavior:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create`：这与其他对象相似，但服务有许多非标准属性。我们不会列出并审查所有服务参数，因为其中大多数是从容器继承的。在这里，我们将回顾与服务行为相关的最重要的几个：'
- en: '`--config`: We can create a service configuration only, not a real service.
    This will create all service environments and requirements but without running
    any task.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--config`：我们仅能创建服务配置，而不是一个实际的服务。这将创建所有服务环境和要求，但不会运行任何任务。'
- en: '`--container-label`/`--label`: We added this option here because it is important
    to understand that services and containers are different objects and that we can
    add labels to both. By default, Docker Swarm will create many labels on each service
    container to relate them to each other. We can easily use those labels to filter
    information regarding our services'' containers on any host.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--container-label`/`--label`：我们在这里添加这个选项是因为理解服务和容器是不同的对象很重要，我们可以为两者添加标签。默认情况下，Docker
    Swarm 会在每个服务容器上创建许多标签，以便将它们相互关联。我们可以轻松地使用这些标签来过滤关于我们服务容器的任何主机信息。'
- en: '`--constraint`/`--placement-pref`: As we mentioned previously, we can specify
    which nodes should run a given service''s tasks. We use a list of key-value pairs
    as constraints to do this. All defined keys must be fulfilled to schedule the
    service''s tasks on a given node. If no node satisfies the defined constraints,
    the tasks will not be run because Docker Swarm''s scheduler will not find any
    node with those requirements. On the other hand, `placement-pref` will provide
    a placement preference. This will not limit which nodes will run the tasks, but
    we can use this to spread our services'' tasks across different nodes using a
    defined key. For example, we might distribute a given service''s tasks across
    different physical locations (such as data centers).'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--constraint`/`--placement-pref`：如前所述，我们可以指定哪些节点应该运行某个服务的任务。我们使用一组键值对作为约束条件来实现这一点。所有定义的键必须满足，才能在指定节点上调度该服务的任务。如果没有节点满足这些定义的约束条件，任务将无法执行，因为
    Docker Swarm 的调度器找不到符合要求的节点。另一方面，`placement-pref` 提供了一种放置偏好设置。这不会限制哪些节点会运行任务，但我们可以通过定义的键来将服务的任务分布到不同的节点上。例如，我们可能会将某个服务的任务分布到不同的物理位置（例如数据中心）中。'
- en: '`--mode`: There are two different service modes (in fact, there are three,
    as we will find out later in the *Networking in Docker Swarm* section, but at
    this point, just keep the following two in mind). By default, all services will
    use replication mode. This means that we will set a number of replicas to be healthy
    (by default, this is one replica). We also have global services. In this case,
    we will create as many replicas as nodes in the cluster, but we will just run
    one replica per node. This mode is very interesting for monitoring applications,
    for example, because all the nodes will receive their monitoring process. One
    important thing about these services is that every node that gets into the cluster
    will receive its own replica. Docker Swarm will deploy it on the new node for
    us automatically.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--mode`：有两种不同的服务模式（事实上，后来我们会在 *Docker Swarm 网络配置* 部分了解到三种，但此时只需要记住以下两种）。默认情况下，所有服务都将使用复制模式。这意味着我们将设置要保持健康的副本数（默认情况下是一个副本）。我们还有全局服务。在这种情况下，我们会创建与集群节点数量相等的副本，但每个节点上只运行一个副本。这种模式对于监控应用程序非常有趣，因为所有节点都会接收到自己的监控进程。关于这些服务，有一个重要的事情是每个加入集群的节点都会收到它自己的副本。Docker
    Swarm 会自动将其部署到新的节点上。'
- en: '`--with-registry-auth`: This is a very important option because it allows us
    to distribute credentials among cluster nodes so that we can use private images.
    It is also important to understand that Docker Swarm requires external or internal
    registries to work. We will not work with local images on cluster nodes anymore.
    Local images will lead to inconsistent deployments because image names can match,
    while content could be completely different across nodes.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--with-registry-auth`：这是一个非常重要的选项，因为它允许我们在集群节点之间分发凭证，从而使用私有镜像。还需要理解的是，Docker
    Swarm 需要外部或内部的注册表来工作。我们不再在集群节点上使用本地镜像。使用本地镜像会导致不一致的部署，因为镜像名称可能匹配，但不同节点上的内容可能完全不同。'
- en: '`--endpoint-mode`: This option sets how services announce or manage their tasks.
    We can use `vip` and `dnsrr` for this. Services will default to `vip`, so each
    service will receive a virtual IP associated with its name, and an internal load
    balancer will route traffic to each replicated process (container/task) associated
    with it. On the other hand, `dnsrr` will use internal name resolution to associate
    each replica IP address whenever we ask for a service name. This way, internal
    name resolution will give us a different IP address when a given service has been
    deployed with more than one task.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--endpoint-mode`：此选项设置服务如何宣布或管理它们的任务。我们可以使用 `vip` 和 `dnsrr` 来设置。服务默认为 `vip`，这意味着每个服务将接收一个与其名称相关联的虚拟
    IP，并且内部负载均衡器会将流量路由到与之关联的每个副本进程（容器/任务）。另一方面，`dnsrr` 将使用内部名称解析来关联每个副本的 IP 地址，每当我们请求服务名称时。这样，当给定服务以多个任务部署时，内部名称解析会为我们提供一个不同的
    IP 地址。'
- en: '`--network`: We can attach new services to an existing network. As we did with
    containers, we can also use a host network namespace. The difference here is that
    we can''t execute privileged services, so our services will have to expose ports
    numbered higher than `1024`.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--network`：我们可以将新服务连接到现有的网络。就像我们在容器中做的那样，我们也可以使用主机网络命名空间。这里的区别在于，我们不能执行特权服务，因此我们的服务必须暴露高于
    `1024` 的端口。'
- en: '`--publish`: We will use this option to publish ports externally. Docker Swarm
    will expose ports using Docker Swarm''s router mesh on every node. If external
    requests arrive on a host that does not execute any service tasks, Docker Swarm
    will internally reroute requests to an appropriate node.'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--publish`: 我们将使用此选项来将端口公开到外部。Docker Swarm将通过每个节点上的Docker Swarm路由器网格公开端口。如果外部请求到达一个没有执行任何服务任务的主机，Docker
    Swarm将内部重新路由请求到适当的节点。'
- en: '`--replicas`/`--replicas-max-per-node`: Services are defined by how many replicas
    or tasks are deployed to maintain their healthy state. By default, all services
    deploy one single replica. As we will see later, we can change the number of replicas
    at any time we need. Not all application components (processes) will work well
    if we scale up or down their replicas. Imagine, for example, a SQL database. It
    is a completely stateful component because the database process will write data.
    If we add a new database replica accessing the same storage, the database will
    become corrupted. If each database replica has its own storage, they will manage
    different data. As a result, not all services can be scaled up or down.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--replicas`/`--replicas-max-per-node`: 服务是通过部署多少个副本或任务来维持其健康状态的。默认情况下，所有服务部署一个副本。正如我们稍后会看到的，我们可以随时更改副本数量。如果我们扩展或缩减副本数量，并不是所有的应用组件（进程）都会正常工作。例如，想象一个SQL数据库。它是一个完全有状态的组件，因为数据库进程会写入数据。如果我们增加一个新的数据库副本来访问相同的存储，数据库将会损坏。如果每个数据库副本都有自己的存储，它们将管理不同的数据。因此，并不是所有服务都可以扩展或缩减。'
- en: '`--reserve-cpu`/`--reserve-memory`: We can reserve the amount of resources
    required for a service to work. If no node presents enough resources, it will
    not be scheduled.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--reserve-cpu`/`--reserve-memory`: 我们可以为服务保留所需的资源。如果没有节点提供足够的资源，该服务将不会被调度。'
- en: '`--update-delay`/`--update-failure-action`/`--update-max-failure-ratio`/`--update-monitor`/`--update-order`/`--update-parallelism`:
    `update` options manage how changes are executed for a service. We will set how
    many services'' tasks will be updated at once, how many seconds we will wait between
    instances'' updates, and how the update process will be done. The `--update-order`
    option sets how this update process will be executed. By default, the running
    container will be stopped and a new one will be created after the old one is completely
    finished. With this setting, the service will be impacted. We can set a different
    order by starting a new container first. Then, once everything is fine, the old
    one will be stopped. This way, the service will not be impacted, but your application
    process must be able to allow for this situation. For example, it will not work
    on a standard SQL database, as we mentioned previously. We will also set what
    to do when some of the updates fail, either by executing an automatic rollback
    or by pausing the rest of the service updates until manual action is taken.'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--update-delay`/`--update-failure-action`/`--update-max-failure-ratio`/`--update-monitor`/`--update-order`/`--update-parallelism`:
    `update`选项管理服务变更的执行方式。我们将设置每次更新多少个服务任务、在每个实例更新之间等待多少秒以及如何执行更新过程。`--update-order`选项设置更新过程的执行顺序。默认情况下，正在运行的容器将在旧容器完全停止后被停止并创建一个新容器。使用此设置时，服务将受到影响。我们可以通过首先启动新容器来设置不同的顺序。然后，一旦一切正常，旧容器将被停止。这样，服务将不受影响，但您的应用进程必须能够适应这种情况。例如，它在标准SQL数据库上将无法工作，正如我们之前提到的那样。我们还将设置当某些更新失败时该如何处理，您可以选择执行自动回滚，或者暂停其余服务的更新，直到手动采取行动。'
- en: '`--rollback-delay`/`--rollback-failure-action`/`--rollback-max-failure-ratio`/`--rollback-monitor`/`--rollback-order`/`--rollback-parallelism`:
    If the update process goes wrong, we can set an automatic rollback. These settings
    modify how rollbacks will be done. We have the same options we reviewed for the
    `update` process, but this time, the arguments will refer to the `rollback` process.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--rollback-delay`/`--rollback-failure-action`/`--rollback-max-failure-ratio`/`--rollback-monitor`/`--rollback-order`/`--rollback-parallelism`:
    如果更新过程出错，我们可以设置自动回滚。这些设置会修改回滚的方式。我们有与`update`过程相同的选项，但这次参数会涉及到`rollback`过程。'
- en: '`ps`: With this, we can review all our service''s tasks and their distributions
    in the cluster. We can also use filters and output format. We will see a couple
    of examples of this in the *Chapter labs* section.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ps`: 有了这个，我们可以回顾所有服务的任务及其在集群中的分布。我们还可以使用过滤器和输出格式。在*章节实验*部分，我们将看到一些例子。'
- en: '`logs`: This is a very useful action because Docker Swarm will retrieve the
    logs of all our tasks for us. We can then review them from the manager command
    line instead of going to wherever the tasks were running to read the container''s
    logs.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logs`：这是一个非常有用的操作，因为 Docker Swarm 会为我们获取所有任务的日志。我们可以从管理节点的命令行查看它们，而不是去任务运行的地方读取容器的日志。'
- en: '`update`: Service properties can be updated. For example, we can change image
    release versions, publish new ports, and change the number of replicas, among
    other things.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update`：服务属性可以进行更新。例如，我们可以更改镜像的发布版本，发布新端口，改变副本数量等。'
- en: '`rollback`: With this, we can return to the service''s previous properties.
    It is important to understand that images from previous executions should be kept
    in our hosts to allow for the application''s rollbacks.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rollback`：通过这个操作，我们可以恢复服务的先前属性。需要理解的是，之前执行过的镜像应该保留在我们的主机上，以便支持应用程序的回滚。'
- en: '`inspect`/`ls`/`rm`: These are the common actions we encounter with all other
    kinds of objects. We''ve already learned how to use them.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inspect`/`ls`/`rm`：这些是我们在所有其他类型的对象中都会遇到的常见操作。我们已经学习了如何使用它们。'
- en: It is important to note that privileged containers are not allowed on services.
    Therefore, if we want to use the host network namespace, container processes should
    expose and use non-privileged ports (higher than `1024`).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，服务中不允许使用特权容器。因此，如果我们想要使用主机网络命名空间，容器进程应该暴露并使用非特权端口（大于 `1024`）。
- en: 'Docker service constraints can be set with custom labels, but there are some
    internal ones that are created by default that are very useful:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 服务约束可以通过自定义标签进行设置，但还有一些是默认创建的内部标签，它们非常有用：
- en: '| **Label** | **Attribute** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **标签** | **属性** |'
- en: '| `node.id` | Node ID |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `node.id` | 节点 ID |'
- en: '| `node.hostname` | Node hostname; for example, `node.hostname==antares` |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `node.hostname` | 节点主机名；例如，`node.hostname==antares` |'
- en: '| `node.role` | Node Swarm role; for example, `node.role!=manager` |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `node.role` | 节点的 Swarm 角色；例如，`node.role!=manager` |'
- en: '| `node.labels` | Swarm node-assigned labels; for example, `node.labels.environment==production`
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| `node.labels` | Swarm 节点分配的标签；例如，`node.labels.environment==production` |'
- en: '| `engine.labels` | Docker Engine-defined labels; for example, `engine.labels.operatingsystem==ubuntu
    18.04` |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| `engine.labels` | Docker 引擎定义的标签；例如，`engine.labels.operatingsystem==ubuntu
    18.04` |'
- en: 'We can use variables to define service properties. In the following example,
    we''re using internal Docker Swarm variables in the container''s hostname:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用变量来定义服务属性。在以下示例中，我们在容器的主机名中使用了内部的 Docker Swarm 变量：
- en: '[PRE8]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To summarize, before continuing with other Swarm resources: services are a
    group of tasks, each executing one container. All these containers run together
    to maintain the service''s state. Docker Swarm will monitor the service''s state
    and if one container dies, it will run a new container to maintain the number
    of instances. It is important to note that a container''s IDs and names will change.
    However, while new tasks can be created, the task''s name will not be changed.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在继续处理其他 Swarm 资源之前：服务是一组任务，每个任务执行一个容器。所有这些容器一起运行以保持服务的状态。Docker Swarm 会监控服务的状态，如果一个容器宕机，它会运行一个新容器以保持实例的数量。需要注意的是，容器的
    ID 和名称会发生变化。然而，尽管可以创建新的任务，任务的名称不会改变。
- en: 'Let''s have a look at a quick example before moving on to the next topic. We
    will create a simple NGINX web server service using `docker service create`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一个主题之前，让我们看一个快速的示例。我们将使用 `docker service create` 创建一个简单的 NGINX Web 服务器服务：
- en: '[PRE9]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can review where the created task is running by using `docker service ps`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `docker service ps` 查看创建的任务运行在哪个节点：
- en: '[PRE10]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we move to the node where the task is running. Once there, we kill the
    associated container using `docker container kill`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们移动到任务所在的节点。一旦到达，我们使用 `docker container kill` 杀死相关容器：
- en: '[PRE11]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After a few seconds, a new task will be created automatically with a new container.
    The task name hasn''t changed, but it is a new task, as we can tell from its ID:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，一个新的任务将自动创建并伴随一个新的容器。任务名称没有改变，但它是一个新任务，从其 ID 我们可以看出这一点：
- en: '[PRE12]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can review some of the labels that were created by Swarm to fully
    identify containers using their services. We use `docker container inspect` for
    this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看一些由 Swarm 创建的标签，以便通过它们来完整识别使用服务的容器。我们使用 `docker container inspect`
    来查看：
- en: '[PRE13]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: There are some service options that can be set using strings to help us identify
    their configuration and other associated resources. This is very important when
    we need to isolate resources attached to a specific service's tasks or use some
    special information to access other services, such as the container's hostname.
    We can use labels to add meta-information to containers, but there are also Docker
    Swarm-defined variables that we can use within strings. These variables use Go's
    template syntax (as we also learned when formatting the listing command's output)
    and can be used with `docker service create` and the `--hostname`, `--mount`,
    and `--env` arguments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些服务选项可以通过字符串设置，帮助我们识别其配置和其他相关资源。当我们需要隔离特定服务任务的资源或使用一些特殊信息访问其他服务时，这是非常重要的，例如容器的主机名。我们可以使用标签为容器添加元信息，但也有一些
    Docker Swarm 定义的变量，我们可以在字符串中使用它们。这些变量使用 Go 的模板语法（正如我们在格式化列出命令输出时学到的），并可以与 `docker
    service create` 以及 `--hostname`、`--mount` 和 `--env` 参数一起使用。
- en: Therefore, we can set an associated service container's hostname to be unique
    between tasks using these variables; for example, `--hostname="{{.Service.Name}}-{{.Task.ID}}"`.
    We can even use the node's name to identify this task with the node in which it
    is running using `--hostname="{{.Node.Hostname}}"`. This can be very useful with
    global services.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用这些变量将关联服务容器的主机名设置为任务间唯一；例如，`--hostname="{{.Service.Name}}-{{.Task.ID}}"`。我们甚至可以使用节点的名称，通过`--hostname="{{.Node.Hostname}}"`来标识任务所属的节点。这在全局服务中非常有用。
- en: 'The following is a quick list of valid service template substitutions:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有效的服务模板替换的快速列表：
- en: '**Service**: `.Service.ID`, `.Service.Name`, and `.Service.Labels`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**：`.Service.ID`、`.Service.Name` 和 `.Service.Labels`'
- en: '**Node**: `.Node.ID` and `.Node.Hostname`'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：`.Node.ID` 和 `.Node.Hostname`'
- en: '**Task**: `.Task.ID`, `.Task.Name`, and `.Task.Slot`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：`.Task.ID`、`.Task.Name` 和 `.Task.Slot`'
- en: In the next section, we will introduce some new Docker Swarm objects that will
    help us deploy our applications on clusters.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一些新的 Docker Swarm 对象，它们将帮助我们在集群中部署应用程序。
- en: Deploying applications using Stacks and other Docker Swarm resources
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Stacks 和其他 Docker Swarm 资源部署应用程序
- en: In this section, we will learn about other Docker Swarm objects that will help
    us to fully deploy applications within the cluster.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习其他 Docker Swarm 对象，它们将帮助我们在集群内完全部署应用程序。
- en: We've already learned how to configure applications using environment variables.
    This is not recommended for production because anyone with system Docker access
    can read their values. To avoid this situation, we will use external data sources.
    We also learned how to integrate host resources inside containers. We can set
    configurations and passwords in files shared between hosts and containers. This
    will work on standalone environments but not for distributed workloads, where
    containers can run on different hosts. We will need to sync those files on all
    cluster nodes.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何使用环境变量配置应用程序。这不推荐在生产环境中使用，因为任何具有系统 Docker 访问权限的人都可以读取它们的值。为了避免这种情况，我们将使用外部数据源。我们还学会了如何将主机资源集成到容器中。我们可以将配置和密码设置在主机和容器之间共享的文件中。这在独立环境中可以正常工作，但对于分布式工作负载（容器可能在不同主机上运行）来说则不行。我们将需要在所有集群节点上同步这些文件。
- en: To avoid syncing files on multiple nodes, Docker Swarm provides two different
    objects for managing them. We can have private files or secrets and configurations.
    Both objects store their values in the Swarm key-value store. Stored values will
    be available for every cluster node that requires them. These objects are similar,
    but secrets are used for passwords, certificates, and so on, while config objects
    are used for application configuration files. Now, let's examine them in depth.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在多个节点之间同步文件，Docker Swarm 提供了两种不同的对象来管理它们。我们可以拥有私有文件、密钥和配置。两者都将其值存储在 Swarm
    键值存储中。存储的值将对每个需要它们的集群节点可用。这些对象是相似的，但密钥用于密码、证书等，而配置对象则用于应用程序配置文件。现在，让我们深入了解它们。
- en: Secrets
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密钥
- en: A secret is a blob of data that contains passwords, certificates, and any other
    information that should not be shared over the network. They will be stored in
    an encrypted fashion so that they're safe from snoopers. Docker Swarm will manage
    and store secrets for us. Because this kind of data is stored in the key-value
    store, only managers will have access to any secrets we create. When a container
    needs to use that stored secret, the host responsible for running that container
    (a service task container) will have access too. The container will receive a
    temporal filesystem (in-memory `tmpfs` on Linux hosts) containing that secret.
    When the container dies, the secret will not be accessible on the host. Secrets
    will only be available to running containers when they are required.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 密钥是包含密码、证书以及任何其他不应在网络上传输的信息的数据块。它们将以加密方式存储，以防止被窥探。Docker Swarm 会为我们管理和存储密钥。由于这些数据存储在键值存储中，只有管理节点才能访问我们创建的任何密钥。当容器需要使用存储的密钥时，负责运行该容器的主机（服务任务容器）也将能够访问该密钥。容器将接收到一个临时的文件系统（在
    Linux 主机上为内存中的 `tmpfs`），其中包含该密钥。当容器停止时，该密钥将无法在主机上访问。密钥仅在容器运行时需要时才会可用。
- en: Since secrets are Docker Swarm objects, we can use all of the usual actions
    (`list`, `create`, `remove`, `inspect`, and so on). Do not expect to read secret
    data with the `inspect` action. Once created, it is not possible to read or change
    a secret's content. We create secrets with files or by using standard input for
    data. We can add labels for easy listing in big cluster environments.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于密钥是 Docker Swarm 对象，因此我们可以使用所有常规操作（`list`、`create`、`remove`、`inspect` 等）。不要指望通过
    `inspect` 操作读取密钥数据。一旦创建，无法读取或更改密钥的内容。我们可以通过文件或使用标准输入来创建密钥。我们还可以添加标签，以便在大型集群环境中轻松列出密钥。
- en: 'Once a secret has been created, we can use it within our services. We have
    both short and long notations. By default, using the short format, a file with
    secret data will be created under `/run/secrets/<SECRET_NAME>`. This file will
    be mounted in a `tmpfs` filesystem on Linux. Windows is different because it does
    not support on-memory filesystems. We can use the long format to specify the filename
    to be used for the secret file under `/run/secrets`, along with its ownership
    and file permissions. This will help us avoid root usage inside the container
    in order to access the file. Let''s create a secret with `docker secret create`
    and then use it on a service:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了密钥，我们可以在服务中使用它。我们有短格式和长格式两种表示方式。默认情况下，使用短格式时，一个包含密钥数据的文件将会在 `/run/secrets/<SECRET_NAME>`
    下创建。该文件将在 Linux 上的 `tmpfs` 文件系统中挂载。Windows 不支持内存文件系统，因此不同于 Linux。我们可以使用长格式指定密钥文件在
    `/run/secrets` 下的文件名，以及其所有权和文件权限。这样可以帮助我们避免在容器内使用 root 权限来访问该文件。让我们通过 `docker
    secret create` 创建一个密钥，并在服务中使用它：
- en: '[PRE14]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As we mentioned previously, it is not possible to retrieve secret data. We
    can inspect previously created secrets using the common `docker secret inspect`
    action:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，无法检索密钥数据。我们可以使用常见的 `docker secret inspect` 操作来检查之前创建的密钥：
- en: '[PRE15]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next section, we will learn about configuration objects.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习配置对象。
- en: Config
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: Config objects are similar to secrets, but they aren't encrypted on the Docker
    Swarm Raft log and will not be mounted on a `tmpfs` filesystem in containers.
    Configs can be added or removed while service tasks are running. In fact, we can
    even update service configurations. We will use these objects to store configurations
    for applications. They can contain strings or binaries (up to 500 KB, which is
    more than enough for configurations).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象与密钥类似，但它们不会在 Docker Swarm Raft 日志中加密，并且不会在容器中的 `tmpfs` 文件系统上挂载。配置可以在服务任务运行时添加或删除。事实上，我们甚至可以更新服务配置。我们将使用这些对象来存储应用程序的配置。它们可以包含字符串或二进制文件（最多
    500 KB，这对配置来说足够了）。
- en: When we create a config object, Docker Swarm will store it in the Raft log,
    which is encrypted, and it will be replicated to other managers by mutual TLS.
    Therefore, all the managers will have the new config object value.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个配置对象时，Docker Swarm 会将其存储在加密的 Raft 日志中，并通过相互 TLS 协议将其复制到其他管理节点。因此，所有管理节点将拥有新的配置对象值。
- en: Using config files on services requires there to be a mount path inside the
    containers. By default, the mounted configuration file will be world-readable
    and owned by the user running the container, but we can adjust both properties
    should we need to.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务中使用配置文件需要在容器内有一个挂载路径。默认情况下，挂载的配置文件将是世界可读的，并且由运行容器的用户拥有，但如果需要，我们可以调整这两个属性。
- en: 'Let''s look at a quick example. We will create a configuration file using `docker
    config create` and then use it inside a service:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。我们将使用`docker config create`创建一个配置文件，然后在服务中使用它：
- en: '[PRE16]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this case, we can review the config content and see that it is readable.
    Using `docker config inspect`, we get the following output:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以查看配置内容并确认其可读性。使用`docker config inspect`，我们得到以下输出：
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let's move on to stacks.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论堆栈。
- en: Stacks
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆栈
- en: Stacks help us deploy complete applications. They are **Infrastructure-as-Code**
    (**IaC**) files with all their component definitions, their interactions, and
    the external resources required to deploy an application. We will use `docker-compose`
    file definitions (`docker-compose.yaml`). Not all `docker-compose` file primitive
    keys will be available. For example, `depends_on`will not be available for stacks
    because they don't have dependency declarations. This is something you have to
    manage in your own application logic.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈帮助我们部署完整的应用程序。它们是**基础设施即代码**（**IaC**）文件，包含所有组件定义、它们的交互方式以及部署应用所需的外部资源。我们将使用`docker-compose`文件定义（`docker-compose.yaml`）。并非所有`docker-compose`文件中的基本键都会可用。例如，`depends_on`在堆栈中不可用，因为它们没有依赖声明。这是需要在自己的应用逻辑中管理的。
- en: As we learned with the `docker-compose` command in [Chapter 5](1c86479c-e4f5-4508-9eca-d29bb3dbaf4b.xhtml),
    *Deploying Multi-Container Applications*, every application that's deployed will
    run by default in its own network. When using stacks on Docker Swarm, application
    components are deployed cluster-wide. Overlay networks will be used because each
    component should reach others, regardless of where they are running. Stacks will
    also be deployed on their own networks by default.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第五章](1c86479c-e4f5-4508-9eca-d29bb3dbaf4b.xhtml)《部署多容器应用程序》中所学到的，*部署多容器应用程序*，每个已部署的应用程序默认会在自己的网络中运行。当在Docker
    Swarm上使用堆栈时，应用组件会在整个集群中部署。将使用覆盖网络，因为每个组件应该能够相互访问，无论它们运行在哪里。堆栈也会默认在自己的网络中部署。
- en: Stacks deploy applications based on services. Therefore, we will keep our service
    definitions in the `docker-compose` file. To be able to identify these services
    from other stacks, we will set the stacks' names.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈通过服务来部署应用程序。因此，我们将把服务定义保存在`docker-compose`文件中。为了能够将这些服务与其他堆栈区分开来，我们将设置堆栈的名称。
- en: It is important to understand that `docker-compose` will deploy multi-container
    applications on one Docker Engine, while `docker stack` will deploy multi-service
    applications on a Swarm cluster. Note that, nonetheless, both use the same kind
    of IaC file.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，`docker-compose`将在一个Docker引擎上部署多容器应用程序，而`docker stack`将在Swarm集群上部署多服务应用程序。请注意，尽管如此，它们都使用相同类型的IaC文件。
- en: 'Let''s have a quick look at the `docker stack` command line:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下`docker stack`命令行：
- en: '`deploy`: Deploying Stacks requires a `docker-compose` file version of 3.0
    and above. We will use the `deploy` action to create and run all application components
    at once. It is also possible to use a Docker Application Bundle file, which is
    something that will not be covered in this book, but it is good to know that we
    have multiple options with Docker Stacks for deploying applications on Docker
    Swarm. As we mentioned previously, we will need to name our stack''s deployment
    to fully identify all its components within the cluster. All of the stack''s resources
    will receive the stack''s name as a prefix unless they were externally created
    from the stack''s file definition. In this latter case, they will retain their
    original names.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deploy`：部署堆栈需要`docker-compose`文件版本为3.0及以上。我们将使用`deploy`操作一次性创建并运行所有应用组件。也可以使用Docker应用包文件，这是本书不涉及的内容，但值得知道的是，我们有多个选项可以通过Docker堆栈在Docker
    Swarm上部署应用程序。如前所述，我们需要为堆栈的部署命名，以便在集群中完全识别其所有组件。堆栈的所有资源将以堆栈的名称作为前缀，除非它们是从堆栈的文件定义外部创建的。在这种情况下，它们将保留原始名称。'
- en: 'These are the main options for `docker stack deploy`:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些是`docker stack deploy`的主要选项：
- en: '`--compose-file`/`-c`: We use `docker-compose.yaml` as the stack definition
    file unless we specify a custom filename with this option.'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--compose-file`/`-c`：我们使用`docker-compose.yaml`作为堆栈定义文件，除非我们使用此选项指定自定义文件名。'
- en: '`--orchestrator`: This option was recently added and allows us to choose which
    orchestrator will deploy and manage the stack. We will be able to choose between
    Docker Swarm and Kubernetes when both are available in our environment.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--orchestrator`：此选项是最近添加的，它允许我们选择哪个编排工具来部署和管理堆栈。在我们的环境中，当Docker Swarm和Kubernetes都可用时，我们可以选择其中之一。'
- en: '`--with-registry-auth`: As we learned with services, sharing authentication
    is vital when using private registries. Without this option, we can''t ensure
    all the nodes are using the same image or that they have access to the registry
    because this will depend on locally stored authentication.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--with-registry-auth`：正如我们在使用服务时所学到的，使用私有注册表时共享身份验证至关重要。没有这个选项，我们无法确保所有节点使用相同的镜像，或者它们能访问注册表，因为这将依赖于本地存储的身份验证。'
- en: '`services`: The `services` option shows us a list of the deployed stack''s
    services. As with all other listing actions, we can format and filter its output.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`services`：`services`选项显示已部署堆栈的服务列表。与所有其他列出操作一样，我们可以格式化并过滤其输出。'
- en: '`ps`: This action lists all the services and where tasks were deployed. It
    is easy to filter and format its output, as we will see in the *Chapter labs*
    section of this chapter.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ps`：此操作列出所有服务及其任务部署的位置。它的输出可以轻松进行过滤和格式化，正如我们将在*本章实验*部分看到的那样。'
- en: '`ls`/`rm`: These are common object actions for listing and removing them.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ls`/`rm`：这些是常见的对象操作，用于列出和删除对象。'
- en: There is not much more to say about stacks. IaC requires that every deployment
    is reproducible. Even for a simple standalone service, make sure to use a stack
    file to deploy it. The *Chapter l**abs* section will cover these actions and options
    with some more examples. In the next section, we will learn how Swarm can change
    application networking cluster-wide.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 关于堆栈没什么更多要说的了。基础设施即代码（IaC）要求每个部署都是可重现的。即使是一个简单的独立服务，也请确保使用堆栈文件来部署它。*本章实验*部分将通过更多示例来讲解这些操作和选项。在下一节中，我们将学习Swarm如何在集群范围内更改应用程序的网络。
- en: Networking in Docker Swarm
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm中的网络
- en: 'When we talk about Docker Swarm, we need to introduce a new concept regarding
    networks: *overlay* networks. As we mentioned at the beginning of this chapter,
    a new network driver will be available because Docker Swarm will distribute all
    application components across multiple nodes. They have to be reachable no matter
    where they run. The overlay network will work over VXLAN tunnels using the **User
    Datagram Protocol** (**UDP**). We will be able to encrypt this communication,
    but some overhead should normally be expected.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论Docker Swarm时，我们需要引入一个关于网络的新概念：*覆盖*网络。正如我们在本章开头提到的，由于Docker Swarm将把所有应用组件分布到多个节点上，它们必须无论运行在哪个位置都能互相访问。因此，将提供一个新的网络驱动程序，覆盖网络将在使用**用户数据报协议**（**UDP**）的VXLAN隧道上工作。我们将能够加密这种通信，但通常会有一些额外的开销。
- en: The overlay network driver will create a distributed network across cluster
    nodes and automatically provides routing of packets to interconnect distributed
    containers.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络驱动程序将跨集群节点创建一个分布式网络，并自动提供数据包路由以互连分布式容器。
- en: 'When Swarm is first initialized, two networks are created:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当Swarm首次初始化时，创建了两个网络：
- en: '`docker_gwbridge`: This bridge network will connect all Docker daemons that
    are part of the cluster.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker_gwbridge`：此桥接网络将连接集群中所有的Docker守护进程。'
- en: '`ingress`: This is an overlay network that will manage Docker Swarm services''
    control and data traffic. All the services will be connected to this network so
    that they can reach each other if we do not specify any custom overlay network.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ingress`：这是一个覆盖网络，将管理Docker Swarm服务的控制和数据流量。所有服务将连接到该网络，以便它们可以相互访问，如果我们没有指定任何自定义的覆盖网络。'
- en: Docker Swarm will only manage overlay networks. We can create new overlay networks
    for our applications that will be isolated from each other. The same happens when
    working locally with custom bridged networks. We will be able to connect services
    to different networks at once, as we did with bridged environments. We will also
    be able to connect containers to overlay networks, although this is not something
    that's commonly done. Remember that we will not run standalone containers in Docker
    Swarm.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm只会管理覆盖网络。我们可以为应用程序创建新的覆盖网络，并且这些网络彼此隔离。当我们在本地使用自定义桥接网络时也会发生类似的情况。我们将能够将服务连接到多个网络，就像我们在桥接环境中所做的那样。我们还可以将容器连接到覆盖网络，尽管这不是常见的做法。请记住，我们不会在Docker
    Swarm中运行独立容器。
- en: 'If firewalls are enabled in your environment, you''ll need to allow the following
    traffic:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的环境中启用了防火墙，您需要允许以下流量：
- en: '| **Port or Range of Ports** | **Protocol** | **Purpose** |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| **端口或端口范围** | **协议** | **用途** |'
- en: '| **2377** | TCP | Cluster management traffic |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| **2377** | TCP | 集群管理流量 |'
- en: '| **7946** | TCP/UDP | Swarm node intercommunication |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| **7946** | TCP/UDP | Swarm节点之间的相互通信 |'
- en: '| **4789** | UDP | Overlay networking |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| **4789** | UDP | 覆盖网络 |'
- en: Docker Swarm management traffic is always encrypted by default, as we learned
    in previous sections. We can also encrypt overlay networking. When we use encryption
    arguments on overlay network creation, Docker Swarm creates **Internet Protocol
    Security** (**IPSEC**) encryption on overlay VXLANs. It adds security, though
    a performance overhead is to be expected. It is up to you to manage the balance
    between security and performance in your applications. As encryption is done upon
    network creation, it can't be changed once the network has been created.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 的管理流量默认始终是加密的，正如我们在前面的章节中所了解的。我们还可以加密覆盖网络。当我们在创建覆盖网络时使用加密参数时，Docker
    Swarm 会在覆盖 VXLAN 上创建 **互联网协议安全** (**IPSEC**) 加密。虽然这增加了安全性，但也会带来性能开销。您需要在应用程序中管理安全性和性能之间的平衡。由于加密发生在网络创建时，因此一旦网络创建完成，就无法更改。
- en: 'Creating overlay networks is easy – we just specify the overlay driver with
    `docker network create`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 创建覆盖网络很简单——我们只需通过 `docker network create` 指定覆盖驱动程序：
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'By default, it is created unencrypted and non-attachable. This means that containers
    will not be able to connect to this network. Only services will be allowed. Let''s
    verify this by trying to attach a simple container to the created network using
    `docker container run`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，它是以未加密且不可附加的方式创建的。这意味着容器将无法连接到此网络，只有服务才能连接。我们可以通过尝试使用 `docker container
    run` 将一个简单的容器附加到已创建的网络来验证这一点：
- en: '[PRE19]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To avoid this, we need to declare the network as attachable from the very beginning.
    This second example also adds an encryption option using `docker network create
    --attachable --opt encrypted`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们需要从一开始就将网络声明为可附加。这第二个示例还使用 `docker network create --attachable --opt
    encrypted` 添加了加密选项：
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We connected to the newer sample encrypted network without any problem because
    it was created with the `attachable` property.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功连接到了新创建的加密网络，因为它是通过具有`attachable`属性的方式创建的。
- en: All services that are connected to the same overlay network will see each other
    by their names, and all their exposed ports will be available internally, regardless
    of whether they are published.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 所有连接到同一覆盖网络的服务将通过它们的名称彼此识别，并且所有暴露的端口将在内部可用，无论它们是否已发布。
- en: By default, all Swarm overlay networks will have 24-bit masks, which means we
    will be able to allocate 255 IP addresses. Each service that's deployed may consume
    multiple IP addresses, as well as one for each node peering on a given overlay
    network. You may run into IP exhaustion in some situations. To avoid this, consider
    creating bigger networks if many services need to use them.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有 Swarm 覆盖网络都会有 24 位掩码，这意味着我们可以分配 255 个 IP 地址。每个部署的服务可能会消耗多个 IP 地址，同时每个节点与给定覆盖网络上的节点进行对等连接时也会占用一个
    IP 地址。在某些情况下，可能会遇到 IP 地址耗尽的问题。为避免这种情况，如果有很多服务需要使用网络，可以考虑创建更大的网络。
- en: In the next section, we will take a closer look at service discovery and how
    Docker routes traffic to all service replicas.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细了解服务发现以及 Docker 如何将流量路由到所有服务副本。
- en: Service discovery and load balancing
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现与负载均衡
- en: Docker Swarm has internal **Internet Protocol Address Management** (**IPAM**)
    and **Domain Name System** (**DNS**) components to automatically assign a virtual
    IP address and a DNS entry for each service that's created. Internal load balancing
    will distribute requests among a service's tasks based on the service's DNS name.
    As we mentioned earlier, all the services on the same network will know each other
    and will be reachable on their exposed ports.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 具有内部的 **互联网协议地址管理** (**IPAM**) 和 **域名系统** (**DNS**) 组件，用于自动为每个创建的服务分配虚拟
    IP 地址和 DNS 条目。内部负载均衡将基于服务的 DNS 名称将请求分发到服务的任务。正如我们之前提到的，所有在同一网络上的服务都会相互识别，并且能够通过它们的暴露端口进行访问。
- en: Docker Swarm managers (in fact, the leader) will use the created ingress overlay
    network to publish the services we declared as accessible from outside the cluster.
    If no port was declared during service creation, Docker Swarm will automatically
    assign one for each exposed port that's declared in the `30000`-`32767` range.
    We have to manually declare any port above `1024` because we can't create privileged
    services.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 管理器（实际上是领导者）将使用创建的入口覆盖网络来发布我们声明为可从集群外部访问的服务。如果在服务创建时没有声明端口，Docker
    Swarm 会自动为每个暴露的端口分配一个端口，端口范围在 `30000`-`32767` 之间。我们必须手动声明所有大于 `1024` 的端口，因为我们不能创建特权服务。
- en: All the nodes will participate in this ingress router mesh. Therefore, the nodes
    will accept connections on the published port, regardless of whether they run
    one of the requested tasks. The router mesh will route all incoming requests to
    published ports on all the nodes to running tasks (containers). Therefore, published
    ports will be allocated on all Swarm nodes and hence only one service will be
    able to use declared ports. In other words, if we publish a service on port `8080`,
    we will not be able to reuse that port for another service. This will limit the
    maximum number of services that can run on the cluster to the number of free ports
    available on the Linux or Windows systems used. We learned that Docker Engine
    will not be able to publish more than one container on the same port using NAT.
    In this case, all the nodes will fix ports to published services.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 所有节点都将参与此入口路由器网格。因此，无论节点是否运行请求的任务，这些节点都会接受已发布端口上的连接。路由器网格将所有传入的请求路由到所有节点上已发布端口的正在运行的任务（容器）。因此，已发布的端口将在所有Swarm节点上分配，因此只有一个服务能够使用已声明的端口。换句话说，如果我们在端口`8080`上发布一个服务，我们将无法将该端口重新用于另一个服务。这将限制集群中可以运行的最大服务数量，受限于所使用的Linux或Windows系统中可用的端口数量。我们了解到，Docker
    Engine无法通过NAT在同一端口上发布多个容器。在这种情况下，所有节点将固定端口以供已发布的服务使用。
- en: The router mesh listens on the published ports on all the node's available IP
    addresses. We will use cluster-external load balancers to route traffic to the
    cluster's hosts. We usually use a couple of them for publishing, with the load
    balancer forwarding all requests to them.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器网格监听所有节点的可用IP地址上的已发布端口。我们将使用集群外部的负载均衡器将流量路由到集群的主机。我们通常使用它们中的几个进行发布，并且负载均衡器会将所有请求转发到这些主机。
- en: We can use `docker service update` to modify or remove already declared ports
    or add new ones.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker service update`修改或删除已声明的端口，或添加新的端口。
- en: 'The following schema shows how a router mesh works on a three-node cluster
    publishing a service with two replicas. The colors service runs two tasks. Therefore,
    one container runs on NODE1 and NODE2, respectively (these are Docker Swarm-scheduled
    tasks on the nodes in the following diagram). Internally, these containers expose
    their application on port `3000`. The service that defined that container''s port
    as `3000` will be published on the host''s port; that is, `8080`. This port will
    be published on all the nodes, even if they do not run any service tasks. Internal
    load balancing will route requests to the appropriate containers using the ingress
    overlay network. Finally, users will access the published service through an external
    load balancer. This is not part of the Docker Swarm environment, but it helps
    us to provide high-availability forwarding requests to a set of available nodes:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了路由器网格在一个三节点集群中如何工作，该集群发布了一个带有两个副本的服务。颜色服务运行两个任务。因此，一个容器分别运行在NODE1和NODE2上（这些是Docker
    Swarm调度的任务，如下图所示）。在内部，这些容器通过端口`3000`暴露它们的应用程序。定义该容器端口为`3000`的服务将会发布在主机的端口上，也就是`8080`。即使某些节点未运行任何服务任务，该端口也会在所有节点上发布。内部负载均衡将使用入口覆盖网络将请求路由到合适的容器。最终，用户将通过外部负载均衡器访问发布的服务。这不是Docker
    Swarm环境的一部分，但它有助于我们提供高可用性，将请求转发到一组可用的节点：
- en: '![](img/7af011c5-ed04-4aa2-b68d-5c7e7d199f86.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7af011c5-ed04-4aa2-b68d-5c7e7d199f86.png)'
- en: 'We will have short and long formats for publishing services. Long formats always
    provide more options. In the following example, we''re publishing an NGINX process
    on cluster port `8080` and forwarding its traffic to the container''s port, `80`,
    using `docker service create --publish`:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供短格式和长格式用于发布服务。长格式通常提供更多选项。在以下示例中，我们将在集群端口`8080`上发布一个NGINX进程，并通过`docker
    service create --publish`将其流量转发到容器的端口`80`：
- en: '[PRE21]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'On any node, we will be able to access the NGINX service on port `8080`. We
    can test this using the `curl` command:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何节点上，我们都能通过端口`8080`访问NGINX服务。我们可以使用`curl`命令进行测试：
- en: '[PRE22]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can retrieve the current service tasks' IP addresses by querying the DNS
    for `tasks.<SERVICE_NAME>`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查询DNS中的`tasks.<SERVICE_NAME>`来获取当前服务任务的IP地址。
- en: By default, all the services use the router mesh. However, we can avoid this
    default behavior, as we will see in the following section.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有服务都使用路由器网格。然而，我们可以避免这种默认行为，正如在接下来的部分中所看到的那样。
- en: Bypassing the router mesh
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绕过路由器网格
- en: Using host mode or a **Round-Robin** **DNS** (**RRDNS**) endpoint, we can bypass
    the router mesh. This will allow us to access instances on given nodes on defined
    ports or apply our own load balancer. In some situations, we need to include special
    load balancing features such as weights or persistence of users' sessions. The
    default Docker Swarm's router mesh behavior will route requests to all available
    services' backend instances. It is important to identify your application's requirements
    to determine whether you should deploy its components using Docker Swarm's default
    load balancing.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主机模式或**循环** **DNS**（**RRDNS**）端点，我们可以绕过路由器网格。这将使我们能够在指定的节点上通过定义的端口访问实例，或应用我们自己的负载均衡器。在某些情况下，我们需要包括特殊的负载均衡特性，如权重或用户会话的持久性。Docker
    Swarm的默认路由器网格行为将请求路由到所有可用服务的后端实例。识别应用程序的需求非常重要，以决定是否应该使用Docker Swarm的默认负载均衡来部署其组件。
- en: Docker's internal load balancer will just do L3 routing. It will not provide
    any weight-based routing or special features.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Docker的内部负载均衡器只会执行L3路由。它不会提供基于权重的路由或特殊功能。
- en: Using host mode
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用主机模式
- en: Using host mode, only nodes with running instances will receive traffic. We
    can label nodes so that they only schedule some tasks on them and route traffic
    to them from load balancers. In this case, we can't run more replicas for this
    service than the defined and labeled number of nodes.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主机模式，只有运行实例的节点才会接收流量。我们可以为节点打标签，使它们只调度一些任务，并从负载均衡器接收流量。在这种情况下，我们不能为该服务运行比定义的标签节点数更多的副本。
- en: 'In the following example, we will run one NGINX process on each node in the
    cluster since we defined a global service. We will use `docker service create
    --mode global --publish mode=host`:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将在集群中的每个节点上运行一个NGINX进程，因为我们定义了一个全局服务。我们将使用`docker service create --mode
    global --publish mode=host`：
- en: '[PRE23]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The service's defined port will be available on all the nodes in the cluster.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的定义端口将在集群中的所有节点上可用。
- en: Using Round-Robin DNS mode
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用循环DNS模式
- en: We can also use RRDNS mode to avoid the service's virtual IP address. In this
    situation, Docker Swarm will not assign a virtual IP for the service, so it will
    create a service DNS entry with all its replicas' IP addresses. This is useful
    when we want to use our own load balancer inside the Docker Swarm cluster to deploy
    this load balancer as another service. It is not easy to maintain the IP addresses
    of replicas inside the load balancer service. We will probably use DNS resolution
    inside the load balancer's configuration, querying the DNS to retrieve all instances'
    IP addresses.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用RRDNS模式来避免服务的虚拟IP地址。在这种情况下，Docker Swarm不会为服务分配虚拟IP，而是为该服务创建一个DNS条目，包含其所有副本的IP地址。当我们希望在Docker
    Swarm集群内使用自己的负载均衡器，将其作为另一个服务部署时，这很有用。在负载均衡器服务中维护副本的IP地址并不容易。我们可能会在负载均衡器的配置中使用DNS解析，查询DNS以获取所有实例的IP地址。
- en: The next section will help us understand the concepts we've learned in this
    chapter with some labs.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将通过一些实验帮助我们理解本章所学的概念。
- en: Chapter labs
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章实验
- en: Now, we will complete this chapter's lab to help us improve our understanding
    of the concepts we've learned. Deploy `environments/swarm-environment` from this
    book's GitHub repository ([https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git))
    if you have not done so yet. You can use your own Linux server. Use `vagrant up`
    from the `environments/swarm`folder to start your virtual environment.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将完成本章的实验，以帮助我们加深对所学概念的理解。如果你还没有部署本书GitHub仓库中的`environments/swarm-environment`，请部署它（[https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git)）。你也可以使用自己的Linux服务器。从`environments/swarm`文件夹中使用`vagrant
    up`启动虚拟环境。
- en: 'Wait until all your nodes are running. We can check the nodes'' status using
    `vagrant status`. Connect to your lab node using `vagrant ssh swarm-node1`. Vagrant
    has deployed four nodes for you. You will be using the `vagrant` user with root
    privileges using `sudo`. You should get the following output:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 等待所有节点运行完毕。我们可以使用`vagrant status`检查节点的状态。使用`vagrant ssh swarm-node1`连接到实验节点。Vagrant已为你部署了四个节点。你将使用`vagrant`用户，并通过`sudo`获取root权限。你应该能看到以下输出：
- en: '[PRE24]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Nodes will have three interfaces (IP addresses and virtual hardware resources
    can be modified by changing the `config.yml` file):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 节点将有三个接口（IP地址和虚拟硬件资源可以通过更改`config.yml`文件进行修改）：
- en: '`eth0 [10.0.2.15]`: Internal, required for Vagrant.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth0 [10.0.2.15]`：内部接口，Vagrant所需。'
- en: '`eth1 [10.10.10.X/24]`: Prepared for Docker Swarm internal communication. The
    first node will get the IP address `10.10.10.11`, and so on.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth1 [10.10.10.X/24]`：用于Docker Swarm内部通信。第一个节点将获得IP地址`10.10.10.11`，依此类推。'
- en: '`eth2 [192.168.56.X/24]`: A host-only interface for communication between your
    host and the virtual nodes. The first node will get the IP address `192.168.56.11`,
    and so on.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth2 [192.168.56.X/24]`：这是一个仅主机接口，用于主机与虚拟节点之间的通信。第一个节点将获得IP地址`192.168.56.11`，依此类推。'
- en: We will use the `eth1` interface for Docker Swarm and we will be able to connect
    to published applications using the `192.168.56.X/24` IP address range. All nodes
    have Docker Engine Community Edition installed and the `vagrant` user is allowed
    to execute `docker`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`eth1`接口进行Docker Swarm通信，并且可以通过`192.168.56.X/24`的IP地址范围连接到已发布的应用程序。所有节点都安装了Docker
    Engine Community Edition，并且`vagrant`用户被允许执行`docker`命令。
- en: 'Now, we can connect to the first deployed virtual node using `vagrant ssh swarm-node1`.
    This process may vary if you''ve already deployed a Docker Swarm virtual environment
    before and just started it using `vagrant up`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`vagrant ssh swarm-node1`连接到第一个已部署的虚拟节点。如果你之前已经部署过Docker Swarm虚拟环境，并通过`vagrant
    up`启动了它，这个过程可能会有所不同：
- en: '[PRE25]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, you are ready to start the labs. Let's start by creating a Docker Swarm
    cluster.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好开始实验了。让我们先从创建Docker Swarm集群开始。
- en: Creating a Docker Swarm cluster
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Docker Swarm集群
- en: Once Vagrant (or your own environment) has been deployed, we will have four
    nodes (named `node<index>`, from `1` to `4`) with Ubuntu Xenial and Docker Engine
    installed.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Vagrant（或你自己的环境）部署完成，我们将有四个节点（命名为`node<index>`，从`1`到`4`），每个节点都安装了Ubuntu Xenial和Docker
    Engine。
- en: First, review your lab node's IP addresses (`10.10.10.11` to `10.10.10.14` if
    you used Vagrant since the first interface will be Vagrant's internal host-to-node
    interface). Once you are familiar with the environment's IP addresses, we can
    initiate a cluster on `node1`, for example.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查实验节点的IP地址（如果你使用了Vagrant，地址范围为`10.10.10.11`到`10.10.10.14`，因为第一个接口是Vagrant的内部主机到节点接口）。当你熟悉了环境的IP地址后，我们可以在`node1`上启动集群，例如。
- en: If you are using Linux as a VirtualBox host, you can execute `alias vssh='vagrant
    ssh'` on your Terminal to use `vssh` instead of `vagrant ssh` to connect to nodes
    as it will be more familiar with non-Vagrant-based real environments.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Linux作为VirtualBox主机，可以在终端执行`alias vssh='vagrant ssh'`，使用`vssh`代替`vagrant
    ssh`连接节点，这样会更加符合非Vagrant环境的使用习惯。
- en: 'Now that we have our environment ready for the labs, along with four nodes
    and Docker Engine already installed, let''s get started:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始实验环境，并且四个节点和Docker Engine已经安装好，让我们开始吧：
- en: 'Connect to `node1` and initialize a new cluster using `docker swarm init`:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到`node1`并使用`docker swarm init`初始化一个新集群：
- en: '[PRE26]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is normal if you are using Vagrant as nodes will have at least two interfaces.
    The first interface is internal to Vagrant, while the other is the one fixed for
    the labs. In this case, we will need to specify which interface to use for the
    cluster with `--advertise-addr`. We will execute `docker swarm init --advertise-addr`:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Vagrant，这种情况是正常的，因为节点至少会有两个接口。第一个接口是Vagrant内部的主机到节点的接口，另一个接口是为实验室准备的。在这种情况下，我们需要使用`--advertise-addr`指定集群使用哪个接口。我们将执行`docker
    swarm init --advertise-addr`：
- en: '[PRE27]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now, Swarm is initialized correctly.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Swarm已经正确初始化。
- en: 'Add a second node that''s connecting to `node2` and executing the command described
    in the initialization output. We will join the cluster using the `docker swarm
    join` command with the obtained token:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加第二个节点，它将连接到`node2`并执行初始化输出中描述的命令。我们将使用获取的令牌，通过`docker swarm join`命令将其加入集群：
- en: '[PRE28]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With this, a node is added as a worker.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，一个节点就作为工作节点被添加了。
- en: 'On `node1`, verify that the new node was added by using `docker node ls`:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`node1`上，通过使用`docker node ls`验证新节点是否已被添加：
- en: '[PRE29]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Notice that `swarm-node1` is the leader because this is the node that initialized
    the cluster. We couldn't have executed `docker node ls` on `swarm-node2` because
    it was not a manager node.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`swarm-node1`是领导者，因为这是初始化集群的节点。我们不能在`swarm-node2`上执行`docker node ls`，因为它不是管理节点。
- en: 'We will execute the same joining process on `swarm-node3`, using `docker swarm
    join` again:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在`swarm-node3`上执行相同的加入过程，再次使用`docker swarm join`：
- en: '[PRE30]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we will review the token for managers, so the next node will be added
    as a manager. We will use `docker swarm join-token manager`:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看管理节点的令牌，以便将下一个节点作为管理节点添加。我们将使用`docker swarm join-token manager`：
- en: '[PRE31]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, we connect to `swarm-node4` and execute the shown joining command (`docker
    swarm join`) with the new token:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们连接到 `swarm-node4`，并使用新的令牌执行所示的加入命令（`docker swarm join`）：
- en: '[PRE32]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The cluster now has four nodes: two managers and two workers. This will not
    provide high availability should the leader fail. Let''s promote `swarm-node2`
    to the manager role too, for example, by executing `docker node update --role
    manager`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前集群有四个节点：两个管理节点和两个工作节点。如果领导节点失败，这将无法提供高可用性。让我们将 `swarm-node2` 也提升为管理节点，例如，通过执行
    `docker node update --role manager`：
- en: '[PRE33]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can change node roles using the `promote` and `demote` commands as well,
    but it is more convenient to know what they really mean for node property updates.
    Also, notice that we can change node roles whenever we want, but we should maintain
    the number of healthy managers.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 `promote` 和 `demote` 命令来更改节点角色，但了解它们对节点属性更新的实际含义会更方便。此外，请注意，我们可以随时更改节点角色，但应保持健康的管理节点数量。
- en: 'We can review the node''s status again. Managers are shown as `Reachable` or
    `Leader`, indicating that this node is the cluster leader. Using `docker node
    ls`, we get the following output:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次查看节点的状态。管理节点显示为 `Reachable` 或 `Leader`，表示该节点是集群的领导者。使用 `docker node ls`，我们得到如下输出：
- en: '[PRE34]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Notice that we executed these commands on `node4`. We can do this because it
    is a manager (not a leader, but a manager). We can use any manager to manage the
    cluster, but only the leader will perform updates on the internal database.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在 `node4` 上执行了这些命令。我们之所以能这样做，是因为它是一个管理节点（不是领导者，但仍是管理节点）。我们可以使用任何管理节点来管理集群，但只有领导节点会执行内部数据库的更新。
- en: 'We will just leave one manager for the rest of the labs, but first, we will
    kill the `node1` Docker Engine daemon to see what happens in the cluster. We will
    stop the Docker daemon using `systemctl stop docker`:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将只保留一个管理节点用于接下来的实验，但首先，我们将停止 `node1` 的 Docker 引擎守护进程，看看集群中会发生什么。我们将使用 `systemctl
    stop docker` 来停止 Docker 守护进程：
- en: '[PRE35]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Connect to the other manager (`node2`, for example; that is, the recently promoted
    node). Now, let''s review the node''s status with `docker node ls`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到另一个管理节点（例如 `node2`，即最近被提升的节点）。现在，让我们使用 `docker node ls` 来查看该节点的状态：
- en: '[PRE36]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'A new leader was elected from among the other running managers. Now, we can
    start the `node1` Docker Engine daemon again using `systemctl start docker`:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 从其他正在运行的管理节点中选举出了一个新的领导者。现在，我们可以重新启动 `node1` 的 Docker 引擎守护进程，使用 `systemctl start
    docker`：
- en: '[PRE37]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The node remains as a manager but is no longer the leader of the cluster because
    a new one was elected when it failed.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点仍然是管理节点，但由于在故障时选举出了新的领导者，它不再是集群的领导者。
- en: 'Let''s demote all non-leader nodes to workers for the rest of the labs using
    `docker node update --role worker`:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `docker node update --role worker` 将所有非领导节点降级为工作节点，供接下来的实验使用：
- en: '[PRE38]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Notice the error when listing again. `node1` is not a manager now, so we can''t
    manage the cluster from this node anymore. All management commands will now run
    from `node4` for the rest of the labs. `node4` is the only manager, which makes
    it the cluster leader, as we can observe using `docker node ls` once more:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意再次列出时的错误。`node1` 现在不是管理节点，因此我们无法再从该节点管理集群。所有管理命令将在接下来的实验中从 `node4` 运行。`node4`
    是唯一的管理节点，因此它成为了集群的领导者，正如我们通过 `docker node ls` 再次观察到的那样：
- en: '[PRE39]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In the next lab, we will deploy a simple web server service.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将部署一个简单的 Web 服务器服务。
- en: Deploying a simple replicated service
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署一个简单的复制服务
- en: 'From `swarm-node4`, we will create a replicated service (by default) and test
    how we can distribute more replicas on different nodes. Let''s get started:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `swarm-node4` 上，我们将创建一个复制服务（默认情况下），并测试如何将更多副本分布到不同的节点。让我们开始吧：
- en: 'Deploy the `webserver` service using a simple `nginx:alpine` image by executing
    `docker service create`:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用简单的 `nginx:alpine` 镜像，通过执行 `docker service create` 来部署 `webserver` 服务：
- en: '[PRE40]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice that we had to wait a few seconds until all the instances were correctly
    running. The amount of time this takes may vary if the image has some configured
    health check.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要等几秒钟，直到所有实例正确运行。如果镜像配置了健康检查，所需时间可能会有所不同。
- en: We can overwrite the image-defined health checks on service creation or by updating
    the configuration using `--health-cmd` and other related arguments. In fact, we
    can change almost everything on a used image, just as we did with containers.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在服务创建时或通过更新配置使用 `--health-cmd` 和其他相关参数来覆盖镜像定义的健康检查。实际上，我们可以更改几乎所有使用过的镜像，就像我们处理容器一样。
- en: 'Once it is deployed, we can review where the replica was started by using `docker
    service ps`:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，我们可以使用 `docker service ps` 来查看副本启动的位置：
- en: '[PRE41]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In this case, `nginx` was deployed on `swarm-node3`. This may vary in your environment.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`nginx`已部署在`swarm-node3`上。这在你的环境中可能会有所不同。
- en: 'We can scale the number of replicas to `3` and review how they were distributed.
    We will use `docker service update --replicas` for this:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将副本数量扩展到`3`，并查看它们的分布情况。我们将使用`docker service update --replicas`命令：
- en: '[PRE42]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If we review the replicas'' distribution, we can discover where the containers
    are running using `docker service ps webserver`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾副本的分布情况，可以通过`docker service ps webserver`命令发现容器的运行位置：
- en: '[PRE43]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Notice that, in this case, `swarm-node2` did not receive a replica, but we can
    force replicas to run there.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，`swarm-node2`没有收到副本，但我们可以强制副本在该节点上运行。
- en: 'To force specific locations, we can add labels to specific nodes and add constraints
    to nodes. We''ll add a label using `docker node update --label-add`:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要强制特定位置，我们可以为特定节点添加标签并添加约束。我们将使用`docker node update --label-add`命令添加标签：
- en: '[PRE44]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can modify the current service so that it runs on specific nodes labeled
    as `tier==front`. We will use `docker service update --constraint-add node.labels.tier`
    and then review its distributed tasks again with `docker service ps`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以修改当前的服务，使其仅在标记为`tier==front`的特定节点上运行。我们将使用`docker service update --constraint-add
    node.labels.tier`命令，然后再次使用`docker service ps`来查看其分布的任务：
- en: '[PRE45]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now, all the replicas are running on `swarm-node2`.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有副本都运行在`swarm-node2`上。
- en: 'Now, we will perform some maintenance tasks on `node2`. In this situation,
    we will remove the `service` constraint before draining `swarm-node2`. If we do
    not do that, no other node will receive workloads because they are restricted
    to `tier=front` node labels. We removed the service''s constraints using `docker
    service update --constraint-rm node.labels.tier`:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将对`node2`执行一些维护任务。在这种情况下，我们将在排空`swarm-node2`之前，先移除`service`约束。如果不这样做，其他节点将无法接收工作负载，因为它们被限制为`tier=front`节点标签。我们通过`docker
    service update --constraint-rm node.labels.tier`命令移除了服务的约束：
- en: '[PRE46]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The tasks did not move to other nodes because the tasks were already satisfying
    the service constraints (no constraint in the new situation).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 任务没有迁移到其他节点，因为这些任务已经满足了服务约束（在新的情况下没有约束）。
- en: Docker Swarm will never move tasks if it is not really necessary because it
    will always try to avoid any service disruption. We can force an update regarding
    service task redistribution by using `docker service update --force <SERVICE_NAME>`.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm永远不会迁移任务，除非真的有必要，因为它总是尽量避免任何服务中断。我们可以通过`docker service update --force
    <SERVICE_NAME>`强制更新服务任务的重新分配。
- en: 'In this step, we will pause `swarm-node3` and drain `swarm-node2`. We will
    use `docker node update --availability pause` and `docker node update --availability
    drain` to do so, respectively:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将暂停`swarm-node3`并排空`swarm-node2`。我们分别使用`docker node update --availability
    pause`和`docker node update --availability drain`来执行这两个操作：
- en: '[PRE47]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, let''s review our service replica distribution again using `docker service
    ps`:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次使用`docker service ps`命令回顾一下我们的服务副本分配情况：
- en: '[PRE48]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Notice that only `swarm-node1` and `swarm-node4` get some tasks because `swarm-node3`
    is paused and we removed all tasks on `swarm-node2`.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有`swarm-node1`和`swarm-node4`接收了一些任务，因为`swarm-node3`已暂停，而且我们已移除`swarm-node2`上的所有任务。
- en: We can use `docker node ps <NODE>` to get all the tasks from all the services
    running on the specified node.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker node ps <NODE>`命令来获取指定节点上所有服务的任务。
- en: 'We will remove the `webserver` service and enable nodes `node2` and `node3`
    again. We will execute `docker service rm` to remove the service:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将移除`webserver`服务并重新启用`node2`和`node3`节点。我们将执行`docker service rm`命令来移除该服务：
- en: '[PRE49]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the next lab, we will create a global service.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将创建一个全球服务。
- en: Deploying a global service
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署一个全球服务
- en: 'In this lab, we will deploy a global service. It will run one task on each
    cluster node. Let''s learn how to use `global` mode:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将部署一个全球服务。它将在每个集群节点上运行一个任务。让我们学习如何使用`global`模式：
- en: 'In this chapter, we learned that global services will deploy one replica on
    each node. Let''s create one and review its distribution. We will use `docker
    service create --mode global`:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到，全球服务将在每个节点上部署一个副本。让我们创建一个服务并回顾其分布情况。我们将使用`docker service create --mode
    global`命令：
- en: '[PRE50]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'All the nodes receive their own replicas, as we can see with `docker service
    ps`:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 所有节点都会收到自己的副本，正如我们通过`docker service ps`命令看到的那样：
- en: '[PRE51]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will now drain `swarm-node1`, for example, and review the new task distribution.
    We will drain the node using `docker node update --availability drain`:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将对`swarm-node1`进行排空操作，并查看新的任务分配情况。我们将使用`docker node update --availability
    drain`命令来排空该节点：
- en: '[PRE52]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: None of the nodes received the `swarm-node1` task because global services will
    only run one replica of a defined service.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 没有节点接收到`swarm-node1`任务，因为全局服务只会在每个节点上运行一个定义好的服务副本。
- en: 'If we enable `swarm-node1` once more using `docker node update --availability
    active`, its replica will start to run again:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们再次使用`docker node update --availability active`启用`swarm-node1`，其副本将重新启动：
- en: '[PRE53]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Swarm will run one task of any global service on each node. When a new node
    joins the cluster, it will also receive one replica of each global service defined
    in the cluster.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm将在每个节点上运行任何全局服务的一个任务。当一个新节点加入集群时，它也会接收到集群中定义的每个全局服务的一个副本。
- en: 'We will remove the `webserver` service again to clear the cluster for the following
    labs by using `docker service rm webserver`:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将再次删除`webserver`服务，以便通过`docker service rm webserver`清理集群，为接下来的实验做准备：
- en: '[PRE54]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We will now take a quick look at service updates to learn how to update a service's
    base image.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将快速了解服务更新，以学习如何更新服务的基础镜像。
- en: Updating a service's base image
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新服务的基础镜像
- en: 'Let''s learn how to refresh a new image version of a deployed and running service
    while *avoiding* user access interruption:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何刷新已部署并正在运行的服务的新镜像版本，同时*避免*用户访问中断：
- en: 'First, we create a 6-replica `webserver` service using `docker service create
    --replicas 6`:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用`docker service create --replicas 6`创建一个6副本的`webserver`服务：
- en: '[PRE55]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we update to a specific `nginx:alpine` version with `perl` support, for
    example. We use `docker service update --image` to change only its base image:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将更新到一个支持`perl`的特定`nginx:alpine`版本，例如。我们使用`docker service update --image`仅更改其基础镜像：
- en: '[PRE56]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The update took more than 60 seconds because Swarm updated tasks one by one
    at 10-second intervals. It will first start the new container with the newly defined
    image. Once it is healthy, it will stop the old version of the container. This
    must be done on each task and therefore takes more time, but this way, we can
    ensure that there is always a `webserver` task running. In this example, we have
    not published any `webserver` ports, so no user interaction is expected. It is
    just a simple lab – but real-life environments will be the same, and internal
    Docker Swarm load balancing will always guide the user's requests to alive instances
    while an update is running.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 更新花费了超过60秒，因为Swarm会每隔10秒更新一个任务。它首先会启动一个使用新定义镜像的新容器。一旦它变得健康，它将停止旧版本的容器。这必须在每个任务上执行，因此需要更多时间，但这样我们可以确保始终有一个`webserver`任务在运行。在这个例子中，我们没有发布任何`webserver`端口，所以不会有用户交互。它只是一个简单的实验，但现实环境也会是如此，内部Docker
    Swarm负载均衡会始终将用户的请求引导到存活的实例上，即使更新正在进行中。
- en: 'The new version is running now, as we can observe by using `docker service
    ps` again:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本现在正在运行，我们可以通过再次使用`docker service ps`来观察：
- en: '[PRE57]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We will remove the `webserver` service again to clear the cluster for the following
    labs using `docker service rm`:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将再次删除`webserver`服务，以便通过`docker service rm`清理集群，为接下来的实验做准备：
- en: '[PRE58]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In the next lab, we will deploy applications using stacks instead of creating
    services manually, which might lead to us making configuration errors, for example.
    Using stacks will provide environment reproducibility because we will always run
    the same IaC definitions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将使用堆栈部署应用程序，而不是手动创建服务，这样可以避免例如配置错误等问题。使用堆栈将提供环境的可重现性，因为我们始终会运行相同的IaC定义。
- en: Deploying using Docker Stacks
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Docker Stacks进行部署
- en: 'In this lab, we will deploy a PostgreSQL database using secrets, configurations,
    and volumes on an IaC file. This file will contain all the application''s requirements
    and will be used to deploy the application as a Docker Stack. Let''s get started:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将使用密钥、配置和卷在IaC文件中部署PostgreSQL数据库。该文件将包含所有应用程序的需求，并将用于作为Docker Stack部署应用程序。让我们开始吧：
- en: 'First, we will create a secret for the required PostgreSQL admin user password.
    We will execute `docker service create` with the standard input as the secret
    content:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个用于所需PostgreSQL管理员用户密码的密钥。我们将执行`docker service create`，并将标准输入作为密钥内容：
- en: '[PRE59]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We will use it as an external secret inside the `docker-compose` file.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它作为`docker-compose`文件中的外部密钥来使用。
- en: 'We are going to create a simple initialization script to create a new database
    when PostgreSQL starts. We will create a simple file in the current directory
    named `create-docker-database.sh` with the following content and appropriate `755`
    permissions:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个简单的初始化脚本，在PostgreSQL启动时创建一个新的数据库。我们将在当前目录中创建一个名为`create-docker-database.sh`的文件，内容如下，并设置适当的`755`权限：
- en: '[PRE60]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then, we create a config file with the file''s content. We will use this file
    to create a database named `docker` on starting up PostgreSQL. This is something
    we can use because it is provided by the official Docker Hub PostgreSQL image.
    We will use `docker config create` with the `create-docker-database.sh` file:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个配置文件，并将文件的内容添加到其中。我们将使用这个文件在启动PostgreSQL时创建一个名为`docker`的数据库。这是我们可以使用的功能，因为它是官方Docker
    Hub PostgreSQL镜像提供的。我们将使用`docker config create`与`create-docker-database.sh`文件：
- en: '[PRE61]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We will add labels to some of the nodes to ensure the database is always running
    there since we will create an external volume only on that node. For this example,
    we will use `node2`. We will create a volume using `docker volume create`:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为一些节点添加标签，以确保数据库始终在这些节点上运行，因为我们只会在该节点上创建外部卷。对于这个示例，我们将使用`node2`。我们将使用`docker
    volume create`来创建一个卷：
- en: '[PRE62]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This volume will only exist on `swarm-node2`, so we will create a constraint
    based on a node label to run the service task only on `swarm-node2`. We will use
    `docker node update --label-add tier=database` for this:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 这个卷仅存在于`swarm-node2`上，所以我们将基于节点标签创建约束，以便仅在`swarm-node2`上运行服务任务。我们将使用`docker
    node update --label-add tier=database`来实现：
- en: '[PRE63]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This is a simple sample. In your production environment, you will never use
    local volumes. We will need to define and use some plugin that allows us to share
    the same volume on different hosts, such as NFS and RexRay.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的示例。在生产环境中，你永远不会使用本地卷。我们需要定义并使用一些插件，允许我们在不同的主机之间共享同一个卷，例如NFS和RexRay。
- en: 'Now, we will create the following Docker Compose file, named `postgres-stack.yaml`:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建以下的Docker Compose文件，命名为`postgres-stack.yaml`：
- en: '[PRE64]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Take note of the following things in this file; we added a lot of learned information
    here:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意文件中的以下内容；我们在这里添加了许多已学到的信息：
- en: We defined the `postgres:alpine` image for the `database` service.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为`database`服务定义了`postgres:alpine`镜像。
- en: The `database` service will only be scheduled on worker nodes with a `tier`
    label key and a value of `database`. In this case, it will run tasks only on `node2`.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`database`服务将只在具有`tier`标签键和值为`database`的工作节点上调度。在这种情况下，它只会在`node2`上运行任务。'
- en: The `postgres` image can use Docker Swarm secret files as environment variables,
    and in this case, it will use `postgres_password` mounted on `/run/secrets/postgres_password`.
    The secret is declared externally because it was previously created outside of
    this file.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`postgres`镜像可以使用Docker Swarm的密钥文件作为环境变量，在这种情况下，它将使用挂载在`/run/secrets/postgres_password`上的`postgres_password`。该密钥声明为外部密钥，因为它之前在这个文件之外已经创建。'
- en: We also added a config file to create an initial database called `docker`. The
    config file is external as well because we added it outside the `postgres-stack.yaml`
    file.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还添加了一个配置文件，用来创建一个名为`docker`的初始数据库。这个配置文件也是外部的，因为我们将其放在了`postgres-stack.yaml`文件之外。
- en: We also added an external volume named `PGDATA`. We will use this volume for
    the database but it will only exist on `node2`. It is defined as external because
    we manually create the `PGDATA` volume locally on `node2`.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还添加了一个名为`PGDATA`的外部卷。我们将使用这个卷来存储数据库数据，但它仅存在于`node2`上。它被定义为外部卷，因为我们手动在`node2`上创建了`PGDATA`卷。
- en: We published the PostgreSQL application's port `5432` on the host's port; that
    is, `15432`. We changed the published port to recognize that they are not the
    same because `5432` will be an internal port on the defined network named `net`.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将PostgreSQL应用程序的端口`5432`发布到主机的端口；即`15432`。我们更改了发布的端口，以便能够识别它们不相同，因为`5432`将是定义的名为`net`网络中的一个内部端口。
- en: 'Finally, we defined the `net` network as `attachable` to be able to test our
    database with a simple container running a `postgres` client. We added two aliases
    to the `database` service inside this network: `postgres` and `mydatabase`.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将`net`网络定义为`attachable`，以便能够用一个简单的容器运行`postgres`客户端来测试我们的数据库。我们在这个网络内的`database`服务中添加了两个别名：`postgres`和`mydatabase`。
- en: Notice that all the objects that were created for the stack will use the stack's
    name as a prefix. This will not happen on externally defined objects. They will
    be used, but we create them manually, outside of the stack's life cycle.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有为堆栈创建的对象都将使用堆栈名称作为前缀。外部定义的对象不会这样做。它们将会被使用，但我们手动创建它们，超出了堆栈的生命周期。
- en: 'We deploy the `postgres` stack using `docker stack deploy`:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`docker stack deploy`来部署`postgres`堆栈：
- en: '[PRE65]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We can easily review the stack's status using `docker stack ps`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker stack ps`轻松查看堆栈的状态。
- en: '[PRE66]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: It is running on `swarm-node2`, as we expected.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行在`swarm-node2`上，正如我们预期的那样。
- en: 'We published port `5432` on port `15432`. We can connect to this port from
    any node IP address in the cluster because Swarm uses a routing mesh. We use the
    `curl` command to review the port''s availability:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将端口`5432`发布到端口`15432`。我们可以从集群中任何节点的IP地址连接到此端口，因为Swarm使用路由网格。我们使用`curl`命令检查端口的可用性：
- en: '[PRE67]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We get this response to `curl` because we are not using the right software client
    (but the ports are listening). Let's run a simple `alpine` container with the
    `postgres` client.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到此`curl`响应是因为我们没有使用正确的软件客户端（但端口正在监听）。让我们运行一个简单的`alpine`容器，并使用`postgres`客户端。
- en: 'Now, we can run a simple `alpine` container attached to the stack''s deployed
    network. In this example, it is `postgres_net`:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行一个简单的`alpine`容器，连接到堆栈部署的网络。在本示例中，它是`postgres_net`：
- en: '[PRE68]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Here, we ran a simple `alpine` container and installed the `postgresql-client`
    package using `docker container run` with an appropriate network:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们运行了一个简单的`alpine`容器，并使用`docker container run`和适当的网络安装了`postgresql-client`软件包：
- en: '[PRE69]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Remember that we added the `mydatabase` and `postgres` aliases to the `database`
    service. Therefore, any of them will be valid for testing database connectivity
    since Swarm added these entries to the internal DNS. We can test this by running
    a simple `ping` command inside the container:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们将`mydatabase`和`postgres`别名添加到了`database`服务中。因此，任何一个都可以用于测试数据库连接性，因为Swarm已将这些条目添加到内部DNS中。我们可以通过在容器内运行简单的`ping`命令来测试：
- en: '[PRE70]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We will use the installed client to test our deployed PostgreSQL. Remember
    to use the previously defined password that we created as a secret, `SuperSecretPassword`.
    We will test our database''s connectivity using the `psql` command:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用已安装的客户端测试我们部署的PostgreSQL。记得使用我们之前作为机密创建的密码`SuperSecretPassword`。我们将使用`psql`命令测试数据库的连接性：
- en: '[PRE71]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We listed the deployed databases using `\l` and the `docker` database, which
    was created with our `create-db.sh` script. Notice that we used the default PostgreSQL
    database port `5432` (we omitted any port customization on client request) instead
    of `15432`. This is because the `docker` container was connecting to the database
    internally. Both the `postgres_database.1` task and the externally run container
    are using the same network, `postgres_net`.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`\l`列出了已部署的数据库，并列出了通过`create-db.sh`脚本创建的`docker`数据库。请注意，我们使用的是默认的PostgreSQL数据库端口`5432`（应客户请求未进行端口定制），而不是`15432`。这是因为`docker`容器在内部连接到数据库。`postgres_database.1`任务和外部运行的容器都使用相同的网络`postgres_net`。
- en: Notice that we can use all learned options with the created stack service, `postgres_database`.
    Anyway, we can modify the Docker Compose file and redeploy the same stack again
    with some changes. Swarm will review the required updates and take the necessary
    actions on all components.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以使用所有已学习的选项与创建的堆栈服务`postgres_database`。无论如何，我们可以修改Docker Compose文件并重新部署相同的堆栈并进行一些更改。Swarm将审查所需的更新并对所有组件采取必要的行动。
- en: 'Let''s exit the running container by executing the `exit` command, and then
    remove the `postgres` stack and `node2` volume to clean up for the following labs
    using `docker stack rm`:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行`exit`命令退出正在运行的容器，然后使用`docker stack rm`删除`postgres`堆栈和`node2`卷，以便清理为后续实验做准备：
- en: '[PRE72]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In the next lab, we will launch a simple replicated service and review internal
    ingress load balancing.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将启动一个简单的复制服务并审查内部入口负载均衡。
- en: Swarm ingress internal load balancing
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swarm入口内部负载均衡
- en: 'In this lab, we will use the `codegazers/colors:1.13` image. This is a simple
    application that will show different random front page colors or texts. Let''s
    get started:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们将使用`codegazers/colors:1.13`镜像。这是一个简单的应用程序，将显示不同的随机前端颜色或文本。让我们开始吧：
- en: 'Let''s create a service named `colors` based on the `codegazers/colors:1.13`
    image. Since we won''t be setting any specific color using environment variables,
    random ones will be chosen for us. Use `docker service create --constraint node.role==worker`,
    as follows:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们基于`codegazers/colors:1.13`镜像创建一个名为`colors`的服务。由于我们不会使用环境变量设置特定的颜色，因此将为我们选择随机颜色。使用`docker
    service create --constraint node.role==worker`，如下所示：
- en: '[PRE73]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We chose not to run a replica on the manager node because we will use `curl`
    from `node4` in this lab.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择不在管理节点上运行副本，因为在本实验中我们将从`node4`使用`curl`。
- en: 'Let''s test local connectivity from the `swarm-node4` manager with `curl`:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`curl`从`swarm-node4`管理节点测试本地连接性：
- en: '[PRE74]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We deployed one replica and it is running the `orange` color. Take note of the
    container's IP address and its name.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署了一个副本，并且它正在运行`orange`颜色。请注意容器的IP地址及其名称。
- en: 'Let''s run five more replicas by executing `docker service update --replicas
    6`:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过执行`docker service update --replicas 6`来运行更多的副本：
- en: '[PRE76]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'If we test service port `8080` with `curl` once more, we will get different
    colors. This is because the containers were launched without color settings:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们再一次使用`curl`测试服务端口`8080`，我们会得到不同的颜色。这是因为容器启动时没有设置颜色：
- en: '[PRE77]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We get different colors on different containers. The router mesh is guiding
    our requests to the `colors` tasks' containers using the ingress overlay network.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同的容器上得到了不同的颜色。路由器网格正在使用入口覆盖网络将请求引导到`colors`任务的容器。
- en: We can access all the `colors` service task logs using `docker service logs
    colors`.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker service logs colors`查看所有`colors`服务任务的日志。
- en: 'Let''s remove the `colors` service for the next and final lab using `docker
    service rm`:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`docker service rm`命令移除`colors`服务，为下一个也是最后一个实验做准备：
- en: '[PRE78]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In the next lab, we will review service endpoint modes and consider how DNS
    resolves `vip` and `dnsrr` situations.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将回顾服务端点模式，并考虑DNS如何解析`vip`和`dnsrr`情况。
- en: Service discovery
  id: totrans-471
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'In this lab, we will create a test overlay attachable network and review DNS
    entries for the `vip` and `dnsrr` endpoint modes. Let''s get started:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们将创建一个测试用的覆盖附加网络，并回顾`vip`和`dnsrr`端点模式的DNS条目。让我们开始吧：
- en: 'First, we need to create an attachable overlay `test` network using `docker
    network create --attachable -d overlay`, as follows:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要使用`docker network create --attachable -d overlay`命令创建一个可附加的覆盖`test`网络，如下所示：
- en: '[PRE79]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, let''s create two different `colors` services. Each one will use different
    endpoint modes. For the `vip` mode, we will use `docker service create`:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建两个不同的`colors`服务。每个服务将使用不同的端点模式。对于`vip`模式，我们将使用`docker service create`：
- en: '[PRE80]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Let''s create the second one for `dnsrr` using `docker service create --endpoint-mode
    dnsrr`, as follows:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`docker service create --endpoint-mode dnsrr`命令创建第二个服务，使用`dnsrr`模式，如下所示：
- en: '[PRE81]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, let''s run a simple `alpine` container on the `test` network using `docker
    container run` and test the internal name resolution functionality. We will need
    to install the `bind-tools` package to be able to use the `host` and `nslookup`
    tools:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`docker container run`在`test`网络上运行一个简单的`alpine`容器，并测试内部名称解析功能。我们需要安装`bind-tools`包，以便使用`host`和`nslookup`工具：
- en: '[PRE82]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: As expected, when using the `vip` endpoint mode, the service receives a virtual
    IP address. All requests will be redirected to that address and ingress will route
    to the appropriate container using internal load balancing.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，使用`vip`端点模式时，服务会获得一个虚拟IP地址。所有请求都会重定向到该地址，入口会使用内部负载均衡将请求路由到适当的容器。
- en: On the other hand, using the `dnsrr` endpoint will not provide a virtual IP
    address. The internal DNS will add an entry for each container IP.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用`dnsrr`端点模式时不会提供虚拟IP地址。内部DNS会为每个容器IP添加一个条目。
- en: 'We can also take a look at the containers attached to the `test` network. These
    containers will get one internal IP address and one that will be routed on the
    overlay network. We can launch the `ip add show` command attached to one of the
    running `colors-dnsrr` tasks'' containers using `docker container exec`:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以查看附加到`test`网络的容器。这些容器将获得一个内部IP地址，以及一个将在覆盖网络上路由的IP地址。我们可以使用`docker container
    exec`命令，查看运行中的`colors-dnsrr`任务的容器，执行`ip add show`命令：
- en: '[PRE83]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: All Vagrant environments can easily be removed by executing `vagrant destroy
    -f` to remove all previously created nodes for this lab. This command should be
    executed on your `environments/swarm` local directory.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Vagrant环境可以通过执行`vagrant destroy -f`来轻松移除，从而删除本实验中之前创建的所有节点。此命令应该在你的`environments/swarm`本地目录中执行。
- en: Remove all the services that you created for this last lab with `docker service
    rm colors-dnsrr colors-vip`.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`docker service rm colors-dnsrr colors-vip`命令移除你为上一个实验创建的所有服务。
- en: Summary
  id: totrans-487
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we reviewed how to deploy and work with the Docker Swarm orchestrator.
    This is the default orchestrator in Docker as it comes out of the box with Docker
    Engine.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了如何部署和使用Docker Swarm调度器。它是Docker的默认调度器，因为它是Docker Engine自带的。
- en: We learned about Docker Swarm's features and how to deploy applications using
    stacks (IaC files) and services instead of containers. Orchestration will manage
    the application's components to keep them running, helping us to even upgrade
    them without impacting users. Docker Swarm also introduced new objects such as
    secrets and config, which help us distribute workloads within cluster nodes. Volumes
    and networks should be managed cluster-wide. We also learned about overlay networking
    and how Docker Swarm's router mesh has simplified application publishing.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了 Docker Swarm 的功能，以及如何使用栈（基础设施即代码文件）和服务来部署应用程序，而不是使用容器。编排将管理应用程序的组件，以保持它们的运行，帮助我们在不影响用户的情况下进行升级。Docker
    Swarm 还引入了新的对象，如 secrets 和 config，帮助我们在集群节点之间分发工作负载。卷和网络应该在集群范围内进行管理。我们还学习了覆盖网络，以及
    Docker Swarm 的路由网格如何简化应用程序发布。
- en: In the next chapter, we will learn about the Kubernetes orchestrator. Currently,
    Kubernetes is a small part of the Docker Certified Associate exam, but this will
    probably be increased in the following releases. It is also useful for you to
    know and understand the concepts of Kubernetes alongside Docker Swarm. Docker
    Enterprise provides both and we can make them work together.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习 Kubernetes 编排器。目前，Kubernetes 是 Docker Certified Associate 考试的一个小部分，但在未来的版本中这一部分可能会增加。同时，了解和理解
    Kubernetes 的概念与 Docker Swarm 一起使用是很有用的。Docker Enterprise 提供了这两者，并且我们可以让它们协同工作。
- en: Questions
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Choose all of the false statements from the following options:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下选项中选择所有错误的陈述：
- en: a) Docker Swarm is the only orchestrator that can work with Docker.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: a) Docker Swarm 是唯一可以与 Docker 配合使用的编排器。
- en: b) Docker Swarm comes included out of the box with Docker Engine.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: b) Docker Swarm 已经随 Docker Engine 一起包含在内。
- en: c) Docker Swarm will allow us to deploy applications on a pool of nodes working
    together, known as a cluster.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: c) Docker Swarm 允许我们在多个节点池上部署应用程序，这些节点池一起工作，被称为集群。
- en: d) All of the preceding statements are false.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有前述陈述均为假。
- en: Which of the following statements are false regarding what Swarm provides by
    default?
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些陈述关于 Swarm 默认提供的功能是错误的？
- en: a) Service discovery
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: a) 服务发现
- en: b) Internal load balancing
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: b) 内部负载均衡
- en: c) Overlay networking among distributed containers on cluster nodes
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: c) 在集群节点之间进行分布式容器的覆盖网络
- en: d) All of the preceding statements are false
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有前述陈述均为假。
- en: Which of the following statements are true in relation to managers?
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些陈述关于管理节点是正确的？
- en: a) We can't create replicated services with tasks running on managers.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: a) 我们不能在管理节点上创建带有任务的复制服务。
- en: b) There is just one leader node on each cluster that manages all Swarm cluster
    changes and object statuses.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: b) 每个集群中只有一个领导节点，负责管理所有 Swarm 集群的变化和对象状态。
- en: c) If the leader node dies, all the changes will be frozen until the leader
    node is healthy again.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: c) 如果领导节点故障，所有更改将被冻结，直到领导节点恢复健康。
- en: d) All of the preceding statements are true.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有前述陈述均为真。
- en: Which of the following statements are false in relation to workers?
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些陈述关于工作节点是错误的？
- en: a) Worker nodes just run workloads.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: a) 工作节点只运行工作负载。
- en: b) If we drain a worker node, all the workloads running on that node will be
    moved to other available nodes.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: b) 如果我们排空一个工作节点，所有运行在该节点上的工作负载将转移到其他可用节点。
- en: c) Swarm roles can be changed for any node in the cluster whenever this is required.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: c) Swarm 角色可以在集群中的任何节点上根据需要进行更改。
- en: d) All of the preceding statements are true.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有前述陈述均为真。
- en: Which of the following statements are false about Swarm Stacks?
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些关于 Swarm 栈的陈述是错误的？
- en: a) By default, all Stacks will be deployed on their own networks.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: a) 默认情况下，所有栈将部署在各自的网络上。
- en: b) Stacks will use Docker Compose files to define all application components.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: b) 栈将使用 Docker Compose 文件来定义所有应用组件。
- en: c) Everything that's used for a Stack should be defined inside the `docker-compose`
    file. We can't add external objects.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: c) 用于栈的所有内容应该在`docker-compose`文件中定义。我们不能添加外部对象。
- en: d) All of the preceding statements are true.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有前述陈述均为真。
- en: Further reading
  id: totrans-517
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following links for more information regarding the topics that
    were covered in this chapter:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下链接，了解本章所涉及的更多信息：
- en: 'Docker Swarm overview: [https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm 概述：[https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)
- en: 'Deploying applications on Docker Swarm: [https://docs.docker.com/get-started/swarm-deploy/](https://docs.docker.com/get-started/swarm-deploy/)'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在Docker Swarm上部署应用程序: [https://docs.docker.com/get-started/swarm-deploy/](https://docs.docker.com/get-started/swarm-deploy/)'
- en: 'Orchestration with Docker Swarm: [https://hub.packtpub.com/orchestration-docker-swarm/](https://hub.packtpub.com/orchestration-docker-swarm/)'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用Docker Swarm进行编排: [https://hub.packtpub.com/orchestration-docker-swarm/](https://hub.packtpub.com/orchestration-docker-swarm/)'
- en: 'Native Docker clustering with Swarm: [https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm](https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm)'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用Swarm进行本地Docker集群: [https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm](https://www.packtpub.com/virtualization-and-cloud/native-docker-clustering-swarm)'
