<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Best Practices for the Real World</h1></div></div></div><p>We have really covered a lot of ground in this book so far. We are now on our final chapter. Here, we are going to look at how to combine all the skills you have learned and create a production-ready module. Just to go out with a bang, we will create a module that configures and deploys Kubernetes as the frontend (note that running Kuberbetes as frontend has limitations and would not be the best idea for production. The UCP component of the module will be production ready). Since there will be a lot of sensitive data, we will take advantage of Hiera. We will create a custom fact to automate the retrieval of the UCP fingerprint, and we will split out all the kubernetes components and use interlock to proxy our API service. We will also take UCP further and look at how to repoint the Docker daemon to use the UCP cluster. The server architecture will follow the same design as we discussed in the scheduler chapter. We will use three nodes, all running Ubuntu 14.04 with an updated kernel to support the native Docker network namespace. We will cover the following topics in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hiera</li><li class="listitem" style="list-style-type: disc">The code</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec38"/>Hiera</h1></div></div></div><p>In this topic, we will look<a id="id297" class="indexterm"/> at how to make our modules stateless. We will move all the data that is<a id="id298" class="indexterm"/> specific to the node that is applied to the <a id="id299" class="indexterm"/>module in Hiera (<a class="ulink" href="https://docs.puppetlabs.com/hiera/3.1/">https://docs.puppetlabs.com/hiera/3.1/</a>). There are two main drivers behind this. The first is to remove any sensitive data such as passwords, keys, and so on, out of our modules. The second is if we remove node-specific data or state out our modules so that they are generic. We can apply them to any number of hosts without changing the logic of the module. This gives us the flexibility to publish our modules for other members of the Puppet community.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec51"/>What data belongs in Hiera</h2></div></div></div><p>When we first sit down and start development on a new module, some of the things that we should consider are: <a id="id300" class="indexterm"/>whether we can make our module OS agnostic, how we can run the module on multiple machines without logic changes or extra development, and how we can protect our sensitive data?</p><p>The answer to all these questions is Hiera.</p><p>We are able to leverage Hiera by parameterizing our classes. This will allow Puppet to automatically look up Hiera at the beginning of the catalogue compilation. So, let's explore some examples of the data that you will put in Hiera.</p><p>Note that in our chapter about container schedulers, we briefly used Hiera. We set the following values:</p><div><img src="img/B05201_09_01.jpg" alt="What data belongs in Hiera"/></div><p>As you can see, we are setting parameters such as versions. Why would we want to set versions in Hiera and not straight in the module? If we set the version of Consul, for example, we might be running version 5.0 in production. Hashicorp just released version 6.0 by changing the Hiera<a id="id301" class="indexterm"/> value for the environment (for more information about Puppet environments, go to <a class="ulink" href="https://docs.puppetlabs.com/puppet/latest/reference/environments.html">https://docs.puppetlabs.com/puppet/latest/reference/environments.html</a>). In Hiera versions from dev to 6.0, we can run multiple versions of the application with no module development.</p><p>The same can be done with IP addresses or URLs. So in the hieradata for dev, your Swarm cluster URL could be <code class="literal">dev.swarm.local</code> and production could be just <code class="literal">swarm.local</code>.</p><p>Another type of data that you will want to separate is passwords/keys. You wouldn't want the same password/key in your dev environment as there are in production. Again, Hiera will let you obfuscate this data.</p><p>We can then take the protection of this data further using eyaml, which Puppet supports. This allows you to use keys to encrypt your Hiera <code class="literal">.yaml</code> files. So, when the files are checked into source<a id="id302" class="indexterm"/> control, they are encrypted. This helps prevent data leakage. For more information on eyaml, visit <a class="ulink" href="https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml">https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml</a>.</p><p>As you see, Hiera gives you the flexibility to move your data from the module to Hiera to externalize configurations, making the module stateless.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec52"/>Tips and tricks for Hiera</h2></div></div></div><p>There are some really<a id="id303" class="indexterm"/> handy tips that you can refer to when using Hiera. Puppet allows you to call functions, lookups, and query facts from Hiera. Mastering these<a id="id304" class="indexterm"/> features will come in very handy. If this is new to you, read the document available at <a class="ulink" href="https://docs.puppetlabs.com/hiera/3.1/variables.html">https://docs.puppetlabs.com/hiera/3.1/variables.html</a> before moving on in the chapter.</p><p>So, let's look at an example of looking up facts from Hiera. First, why would you want to do this? One really good reason is IP address lookups. If you have a class that you are applying to three nodes and you have to advertise an IP address like we did in our <code class="literal">consul</code> module, setting the IP address in Hiera will not work, as the IP address will be different for each machine. We create a file called <code class="literal">node.yaml</code> and add the IP address there. The issue is that we will now have multiple Hiera files. Every time Puppet loads the catalogue, it looks up all the Hiera files to check whether any values have changed. The more files we have, the more load it puts on the master and the slower our Puppet runs will become. So we can tell Hiera to look up the fact and the interface we want to advertise. Here is an example of what the code would look like:</p><div><pre class="programlisting">
<strong>ucpconfig::ucp_host_address: "%{::ipaddress_eth1}"</strong>
</pre></div><p>The one call out is that if we called the fact from a module, we would call it with the fully qualified name, <code class="literal">$::ipaddress_eth1</code>. Unfortunately, Hiera does not support the use of this. So we can use the short name for the <code class="literal">::ipaddress_eth1</code> fact.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec39"/>The code</h1></div></div></div><p>Now that we have a <a id="id305" class="indexterm"/>good understanding of how to make our module stateless, let's go ahead and start coding. We will split the coding into two parts. In the first part we will write the module to install and configure Docker UCP. The final topic will be to run Kubernetes as the frontend.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec53"/>UCP</h2></div></div></div><p>The first thing that we need to do is create a new Vagrant repo for this chapter. By now, we should be masters at <a id="id306" class="indexterm"/>creating a new Vagrant repo. Once we have created that, we will create a new module called <code class="literal">&lt;AUTHOR&gt;-ucpconfig</code> and move it to our <code class="literal">modules</code> directory in the root of our Vagrant repo. We will first set up our <code class="literal">servers.yml</code> file by adding the code shown in the following screenshot:</p><div><img src="img/B05201_09_02.jpg" alt="UCP"/></div><p>As you can see, we are setting up three servers, where <code class="literal">ucp-01</code> will be the master of the cluster and the other two nodes will join the cluster. We will add two more files: <code class="literal">config.json</code> and <code class="literal">docker_subscription.lic</code>. Here, <code class="literal">config.json</code> will contain the authorization key to Docker Hub, and <code class="literal">docker_subscription.lic</code> will contain our trial license for UCP. Note that we covered both of these files in the container scheduler chapter. If you are having issues setting up these files, refer to that chapter.</p><p>The file we will now look at is the puppetfile. We need to add the code shown in the following screenshot to it:</p><div><img src="img/B05201_09_03.jpg" alt="UCP"/></div><p>Now that we have our Vagrant repo <a id="id307" class="indexterm"/>set up, we can move on to our module. We will need to create four files: <code class="literal">config.pp</code>, <code class="literal">master.pp</code>, <code class="literal">node.pp</code>, and <code class="literal">params.pp</code>.</p><p>The first file we will look at is <code class="literal">params.pp</code>. Again, I'd like to code this file first as it sets a good foundation for the rest of the module. We do this as follows:</p><div><img src="img/B05201_09_04.jpg" alt="UCP"/></div><p>As you can see, we are setting all our parameters. We will look at each one in depth as we apply it to the class. This way, we will have context on the value we are setting. You might have noted that we have <a id="id308" class="indexterm"/>set a lot of variables to empty strings. This is because we will use Hiera to look up the values. I have hardcoded some values in as well, such as the UCP version as 1.0.0. This is just a default value if there is no value in Hiera. However, we will be setting the value for UCP to 1.0.3. The expected behavior is that the Hiera value will take precedence. You will note that there is a fact that we are referencing, which is <code class="literal">$::ucp_fingerprint</code>. This is a custom fact. This will automate the passing of the UCP fingerprint. If you remember, in the container scheduler chapter, we had to build <code class="literal">ucp-01</code> to get the fingerprint and add it to Hiera for the other nodes' benefit. With the custom fact, we will automate the process.</p><p>To create a custom fact, we will first need to create a <code class="literal">lib</code> folder in the root of the module. Under that folder, we will create a folder called <code class="literal">facter</code>. In that folder, we will create a file called <code class="literal">ucp_fingerprint.rb</code>. When writing a custom fact, the filename needs to be the same as the fact's name. The code that we will add to our custom fact is as follows:</p><div><img src="img/B05201_09_05.jpg" alt="UCP"/></div><p>In this code, you can see that<a id="id309" class="indexterm"/> we are adding a <code class="literal">bash</code> command to query our UCP master in order to get the fingerprint. I have hardcoded the IP address of the master in this fact. In our environment, we would write logic for that to be more fluid so that we are allowed to have multiple environments, hostnames, and so on. The main part to take away from the custom fact is the command itself.</p><p>We will now move back to our <code class="literal">init.pp</code> file, which is as follows:</p><div><img src="img/B05201_09_06.jpg" alt="UCP"/></div><p>The first thing you can see it that<a id="id310" class="indexterm"/> we are declaring all our variables at the top of our class. This is where Puppet will look up Hiera and match any variable that we have declared. The first block of code in the module is going to set up our Docker daemon. We are going to add the extra configuration to tell the daemon where it can find the backend for the Docker native network. Then, we will declare a case statement on the <code class="literal">$::hostname</code> fact. You will note that we have set a parameter for the hostname. This is to make our module more portable. Inside the case statement, if the host is a master, you can see that we will use our Consul container as the backend for our Docker network. Then, we will order the execution of our classes that are applied to the node.</p><p>In the next block of code<a id="id311" class="indexterm"/> in the case statement, we have declared the <code class="literal">$ucp_deploy_node</code> variable for the <code class="literal">$::hostname</code> fact. We will use this node to deploy Kubernetes from. We will get back to this later in the topic. The final block of code in our case statement is the <code class="literal">catch all</code> or <code class="literal">default</code>. If Puppet cannot find the fact of <code class="literal">$::hostname</code> in either of our declared variables, it will apply these classes.</p><p>We will now move on to our <code class="literal">master.pp</code> file, which is as follows:</p><div><img src="img/B05201_09_07.jpg" alt="UCP"/></div><p>The first thing you will note is that we are declaring our variables again at the top of this class. We are doing this as we are declaring many parameters, and this makes our module a lot more readable. Doing this in complex modules is a must, and I would really recommend you to follow this practice. It will make your life a lot easier when it comes to debugging. As you can see in the following screenshot, we are tying the parameters back to our <code class="literal">init.pp</code> file:</p><div><img src="img/B05201_09_08.jpg" alt="UCP"/></div><p>As you can see from the <a id="id312" class="indexterm"/>preceding code, there are very few types which we are values that we are setting in the module.</p><p>We will now move on to our <code class="literal">node.pp</code> file, as follows:</p><div><img src="img/B05201_09_09.jpg" alt="UCP"/></div><p>As you can see, we are declaring<a id="id313" class="indexterm"/> the parameters at the top of the class, again tying them back to our <code class="literal">init.pp</code> file. We have declared most of our values as we will use Hiera.</p><p>We will now move on to our <code class="literal">config.pp</code> file, as follows:</p><div><img src="img/B05201_09_10.jpg" alt="UCP"/></div><p>In this class, we are going to make<a id="id314" class="indexterm"/> a few vital configurations for our cluster. So, we will walk through each block of code individually. Let's take a look at the first one:</p><div><img src="img/B05201_09_11.jpg" alt="UCP"/></div><p>In this block, we will <a id="id315" class="indexterm"/>declare our variables as we have in all the classes in this module. One call out that I have not mentioned yet is that we are only declaring the variables that are applied to its class. We are not declaring all the parameters. Let's look at the next set of code now:</p><div><img src="img/B05201_09_12.jpg" alt="UCP"/></div><p>In this block of code, we will pass an array of packages that we need to curl our UCP master in order to get the SSL bundle that we will need for TLS coms between nodes. Now, let's see the third block:</p><div><img src="img/B05201_09_13.jpg" alt="UCP"/></div><p>In this code, we are going to create a shell script called <code class="literal">get_ca.sh.erb</code> to get the bundle from the master. Let's create the file, and the first thing that we will need to do is create a <code class="literal">templates</code> folder in the root of the module. Then, we can create our <code class="literal">get_ca.sh.erb</code> file in the <code class="literal">templates</code> folder. We will add the following code to the file:</p><div><img src="img/B05201_09_14.jpg" alt="UCP"/></div><p>As you can see in the script, we need<a id="id316" class="indexterm"/> to create an <code class="literal">auth</code> token and pass it to the API. Puppet does not natively handle tasks like these well, as we are using variables inside the <code class="literal">curl</code> command. Creating a template file and running an <code class="literal">exec</code> function is fine as long as we make it idempotent. In the next block of code, we will do this:</p><div><img src="img/B05201_09_15.jpg" alt="UCP"/></div><p>In this block of code, we will run the preceding script. You can see that we have set the parameters, <code class="literal">command</code> <code class="literal">path</code>, and <strong>cwd</strong> (<strong>current working directory</strong>). The next resource is <code class="literal">creates</code> that tells Puppet that<a id="id317" class="indexterm"/> there should be a file called <code class="literal">ca.pem</code> in the current working directory. If Puppet finds that the file does not exist, it will execute the exec, and if the file does exist, Puppet will do nothing. This will give our exec idempotency.</p><p>In the next block of code, we will create a file in <code class="literal">/etc/profile.d</code>, which will then point the Docker daemon on each node to the master's IP address, allowing us to schedule containers across the cluster:</p><div><img src="img/B05201_09_16.jpg" alt="UCP"/></div><p>Now, let's create a <code class="literal">docker.sh</code> file in our <code class="literal">templates</code> directory. In the file, we will put the following code:</p><div><img src="img/B05201_09_17.jpg" alt="UCP"/></div><p>In this file, we are telling the<a id="id318" class="indexterm"/> Docker daemon to use TLS, setting the location of the key files that we got from the bundle earlier in the class. The last thing we are setting is the Docker host that will point to the UCP master.</p><p>This last block of code in this class should look very familiar, as we are setting up a Docker network:</p><div><img src="img/B05201_09_18.jpg" alt="UCP"/></div><p>We can now move on to the the file where all our data is, our Hiera file. That file is located in the <code class="literal">hieradata</code> folder, which is present in the root of our Vagrant repo. The following screenshot shows the various data present in the Hiera file:</p><div><img src="img/B05201_09_19.jpg" alt="UCP"/></div><p>So, let's list out all the data we have defined here. We are defining the UCP master as <code class="literal">ucp-01</code>, and our deploy node is <code class="literal">ucp-03</code> (this is the node that we are deploying Kubernetes from). The UCP URL of the master is <code class="literal">https://172.17.10.101</code>. This is used when we connect nodes to the master and also when we get our <code class="literal">ca</code> bundle and UCP fingerprint. We keep the username and password as <code class="literal">admin</code> and <code class="literal">orca</code>. We will use UCP version <code class="literal">1.0.3</code>. We will then use<a id="id319" class="indexterm"/> the Hiera fact lookup that we discussed earlier in the book to set the host address and alternate names for UCP.</p><p>The next parameter will tell UCP that we will use an internal CA. We will then set our scheduler to use <code class="literal">spread</code>, define the ports for both Swarm and the controller, tell UCP to persevere the certs, and send the location of the license file for UCP. Next, we will set some date for Consul, such as the master IP, the interface to advertise, the image to use, and how many nodes to expect at the time of booting the Consul cluster. Lastly, we will set the variables we want our Docker daemon to use, such as the name of our Docker network, <code class="literal">swarm-private</code>, the network driver, <code class="literal">overlay</code>, the path to our certs, and the Docker host that we set in our <code class="literal">docker.sh</code> file placed at <code class="literal">/etc/profile.d/</code>.</p><p>So, as you can see, we have a whole lot of data in Hiera. However, as we have already discussed, there is data that could be changed for different environments. So as you can see, there is a massive benefit in making your module stateless and abstracting your data to Hiera, especially if you want to write modules that can scale easily.</p><p>We will now add the following code to our manifest file, <code class="literal">default.pp</code> located in the <code class="literal">manifests</code> folder in the root of our Vagrant repo. The following code defines our node definition:</p><div><img src="img/B05201_09_20.jpg" alt="UCP"/></div><p>We can then open our terminal and <a id="id320" class="indexterm"/>change the directory to the root of our Vagrant repo. We will then issue the <code class="literal">vagrant up</code> command to run Vagrant. Once the three boxes are built, you should get the following terminal output:</p><div><img src="img/B05201_09_21.jpg" alt="UCP"/></div><p>We can then log in to<a id="id321" class="indexterm"/> the web URL at <code class="literal">https://127.0.0.1:8443</code>:</p><div><img src="img/B05201_09_22.jpg" alt="UCP"/></div><p>We will log in with the <code class="literal">admin</code> <a id="id322" class="indexterm"/>username and <code class="literal">orca</code> password. In the following screenshot, we can see that our cluster is up and healthy:</p><div><img src="img/B05201_09_23.jpg" alt="UCP"/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec54"/>Kubernetes</h2></div></div></div><p>Since I thought to finish off with a bang, we will do something pretty cool now. I wanted to deploy an application that had<a id="id323" class="indexterm"/> multiple containers in which we could use interlock to showcase application routing/load balancing. What better application than Kubernetes. As I mentioned earlier, there are limitations to running Kubernetes like this, and it is only for lab purposes. The skills we can take away and apply to our Puppet modules is application routing/load balancing. In our last topic, we set the parameter for <code class="literal">$ucp_deploy_node</code> in the case statement in our <code class="literal">init.pp</code> file. In that particular block of code, we had a class called <code class="literal">compose.pp</code>. This is the class that will deploy Kubernetes across our UCP cluster. Let's look at the file:</p><div><img src="img/B05201_09_24.jpg" alt="Kubernetes"/></div><p>The first resource just creates a directory called <code class="literal">kubernetes</code>. This is where we will place our Docker Compose file. Now, you will notice something different about how we are running Docker Compose. An exec? Why would we use an exec when we have a perfectly good provider that is<a id="id324" class="indexterm"/> tried and trusted. The reason we are using the exec in this case is because we are changing <code class="literal">$PATH</code> during the last Puppet run. What do I mean by that? Remember the file we added to the <code class="literal">/etc/profile.d/</code> directory? It changed the shell settings for where <code class="literal">DOCKER_HOST</code> is pointing. This will only come into effect in the next Puppet run, so the Docker daemon will not be pointing to the cluster. This will mean that all the kubernetes containers will come up on one host. This will cause a failure in the catalogue, as we will get a port collision from two containers using <code class="literal">8080</code>. Now, this will only come into effect when we run the module all at once, as a single catalogue.</p><p>Now, let's have a look at our Docker Compose file:</p><div><img src="img/B05201_09_25.jpg" alt="Kubernetes"/></div><p>In this book, I have been stressing on how I prefer using the Docker Compose method to deploy my container apps. This Docker Compose file is a perfect example why. We have seven containers in this compose file. I find it easy as we have all the logic right there in front of us and we have to <a id="id325" class="indexterm"/>write minimal code. Now, let's look at the code. The first thing that is different and that we wouldn't have seen so far is that we are declaring <code class="literal">version 2</code>. This is because version 1.6.2 of Docker Compose<a id="id326" class="indexterm"/> has been released (<a class="ulink" href="https://github.com/docker/compose/releases/tag/1.6.2">https://github.com/docker/compose/releases/tag/1.6.2</a>). So, to take advantage of the new features, we need to declare that we want to use <code class="literal">version 2</code>.</p><p>The first container we are declaring is interlock. We are going to use interlock as our application router that will make server requests to the Kubernetes API. For this container, we are going to forward ports, <code class="literal">443</code>, <code class="literal">8080</code>, and <code class="literal">8443</code>, to the host. We will then map <code class="literal">/etc/docker</code> from the host machine to the container. The reason for this is we need the keys to connect to the Swarm API. So, we will take advantage of the bundle we installed earlier in the chapter.</p><p>In the command resource, we will tell interlock where to find the certs, the Swarm URL, and lastly, that we want to use <code class="literal">haproxy</code>. We will then add this container to our overlay network, <code class="literal">swarm-private</code>. The next thing is in the environment resource, we will set a constraint and we will tell Compose that we can only run interlock on <code class="literal">ucp-03</code>. We will do this to avoid port collision with the Kubernetes API service. The next container is <code class="literal">etcd</code>. Not much has changed regarding this since we configured Kubernetes in the scheduler chapter, so we will move on. The next container is the <code class="literal">kubernetes</code> API service. The one thing we need to call out with this is that we are declaring in the environment resource <code class="literal">- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}</code>. This is the URL that interlock will look for when it sends the request to the API.</p><p>This is the main reason we are running kubernetes to gain the skills of application routing. So, I won't go through the<a id="id327" class="indexterm"/> rest of the containers. Kubernetes is very well documented at <a class="ulink" href="http://kubernetes.io/">http://kubernetes.io/</a>. I would recommend that you read up on the features and explore Kubernetes—its a beast, there is so much to learn.</p><p>So, now we have all our code. Let's run it!</p><p>Just to see the end-to-end build<a id="id328" class="indexterm"/> process, we will open our terminal and change the directory to the root of our Vagrant repo. If you have the servers built from the earlier topics, issue <code class="literal">vagrant destroy -f &amp;&amp; vagrant up</code>; if not, just a simple <code class="literal">vagrant up</code> command will do. Once the Puppet run is complete, our terminal should have the following output:</p><div><img src="img/B05201_09_26.jpg" alt="Kubernetes"/></div><p>We can then log in to our web UI at <code class="literal">https://127.0.0.1:8443</code>:</p><div><img src="img/B05201_09_27.jpg" alt="Kubernetes"/></div><p>Then, we will log in with the<a id="id329" class="indexterm"/> <code class="literal">admin</code> username and the <code class="literal">orca</code> password. We should see the following screenshot once we login successfully:</p><div><img src="img/B05201_09_28.jpg" alt="Kubernetes"/></div><p>You will note that we have one <a id="id330" class="indexterm"/>application now. If we click on the application twice, we can see that Kubernetes is up and running:</p><div><img src="img/B05201_09_29.jpg" alt="Kubernetes"/></div><p>You will note that we have our container split across <code class="literal">ucp-02</code> and <code class="literal">ucp-03</code> due to the environment settings in our Docker Compose file. One thing to take note of is that interlock in on <code class="literal">ucp-03</code> and the Kubernetes API service is on <code class="literal">ucp-02</code>.</p><p>Now that we have built everything<a id="id331" class="indexterm"/> successfully, we need to log in to <code class="literal">ucp-03</code> and download the <code class="literal">kubectl</code> client. We can achieve that by issuing the following command:</p><div><pre class="programlisting">
<strong>$'wget https://storage.googleapis.com/kubernetes-release/release/v1.1.8/bin/linux/amd64/kubectl'</strong>
</pre></div><p>So let's log in to <code class="literal">ucp-03</code> and issue the <code class="literal">vagrant ssh ucp-03</code> command from the root of our Vagrant repo. We will then change to root (<code class="literal">sudo -i</code>). Then, we will issue the <code class="literal">wget</code> command, as shown in the following screenshot:</p><div><img src="img/B05201_09_30.jpg" alt="Kubernetes"/></div><p>We will then make the file executable by issuing the following command:</p><div><pre class="programlisting">
<strong>$'chmod +x kubectl'</strong>
</pre></div><p>Now remember that we set a URL in the environment settings for the <code class="literal">- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}</code> API container. We will need to set that value in our host file. So, use your favorite way to edit files on the local machine, such as vim, nano sed, and so on, and add <code class="literal">172.17.10.103 kubernetes.ucp-demo.local</code>. The IP address points to <code class="literal">ucp-03</code> as that is where interlock is running.</p><p>Now we are ready to test our cluster. We will do that by issuing the <code class="literal">./kubectl -s kubernetes.ucp-demo.local get nodes</code> command. We should get the following output after this:</p><div><img src="img/B05201_09_31.jpg" alt="Kubernetes"/></div><p>As you can see, everything is up and running. If we go back to our UCP console, you can see that the API server is running on <code class="literal">ucp-02</code> (with the IP address <code class="literal">172.17.10.102</code>). So, how are we doing this? Interlock is <a id="id332" class="indexterm"/>processing the HTTP calls on <code class="literal">8080</code> and routing them to our API server. This tells us that our application routing is in place. This is a very basic example, but something you should play with as you can really design some slick solutions using interlock.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec40"/>Summary</h1></div></div></div><p>In this chapter, we really focused on how to build a Puppet module that is shippable. The use of Hiera and the separation of data from the logic of the module is not only applicable to modules that deploy containers, but for any Puppet modules you write. At some point in your Puppet career, you will either open source a module or contribute to an already open-sourced module. What you have learned in this chapter will be invaluable in both of those use cases. Lastly, we finished with something fun, deploying Kubernetes as the frontend to UCP. In doing this, we also looked at application routing/load balancing. This is obviously a great skill to master as your container environment grows, especially to stay away from issues such as port collision.</p></div></body></html>