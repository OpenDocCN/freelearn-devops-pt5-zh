<html><head></head><body><div class="chapter" title="Chapter&#xA0;14.&#xA0;Clustering and Scaling Services"><div class="titlepage"><div><div><h1 class="title"><a id="ch14"/>Chapter 14. Clustering and Scaling Services</h1></div></div></div><div class="blockquote"><table border="0" cellpadding="0" cellspacing="0" class="blockquote" summary="Block quote" width="100%"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>Organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td align="right" colspan="2" style="text-align: center" valign="top">--<span class="attribution"><span class="emphasis"><em>M. Conway</em></span></span></td></tr></table></div><p>Many will tell you that they have a <span class="emphasis"><em>scalable system</em></span>. After all, scaling is easy. Buy a server, install WebLogic (or whichever other monster application server you're using) and deploy your applications. Then wait for a few weeks until you discover that everything is so fast that you can click a button, have some coffee, and, by the time you get back to your desk, the result will be waiting for you. What do you do? You scale. You buy few more servers, install your monster applications servers and deploy your monster applications on top of them. Which part of the system was the bottleneck? Nobody knows. Why did you duplicate everything? Because you must. And then some more time passes, and you continue scaling until you run out of money and, simultaneously, people working for you go crazy. Today we do not approach scaling like that. Today we understand that scaling is about many other things. It's about elasticity. It's about being able to quickly and easily scale and de-scale depending on variations in your traffic and growth of your business, and that, during that process, you should not go bankrupt. It's about the need of almost every company to scale their business without thinking that IT department is a liability. It's about getting rid of those monsters.</p><div class="section" title="Scalability"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl1sec34"/>Scalability</h1></div></div></div><p>Let us, for a moment take a step back and discuss why we want to scale applications. The main reason is <span class="emphasis"><em>high availability</em></span>. Why do we want high availability? We want it because we want our<a class="indexterm" id="id577"/> business to be available under any load. The bigger the load, the better (unless you are under DDoS). It means that our business is booming. With high availability our users are happy. We all want speed, and many of us simply leave the site if it takes too long to load. We want to avoid having outages because every minute our business is not operational can be translated into a money loss. What would you do if an online store is not available? Probably go to another. Maybe not the first time, maybe not the second, but, sooner or later, you would get fed up and switch it for another. We are used to everything being fast and responsive, and there are so many alternatives that we do not think twice before trying something else. And if that something else turns up to be better... One man's loss is another man's gain. Do we solve all our problems with scalability? Not even close. Many other factors decide the availability of our applications. However, scalability is an important part of it, and it happens to be the subject of this chapter.</p><p>What is scalability? It is a property of a system that indicates its ability to handle increased load in a graceful manner or its potential to be enlarged as demand increases. It is the ability to <a class="indexterm" id="id578"/>accept increased volume or traffic.</p><p>The truth is that the way we design our applications dictates the scaling options available. Applications will not scale well if they are not designed to scale. That is not to say that an application not designed for scaling cannot scale. Everything can scale, but not everything can scale well.</p><p>Commonly observed scenario is as follows:</p><p>We start with a simple architecture, sometimes with load balancer sometimes without, setup a few application servers and one database. Everything is great, complexity is low, and we can develop new features very fast. The cost of operations is low, income is high (considering that we just started), and everyone is happy and motivated.</p><p>Business is growing, and the traffic is increasing. Things are beginning to fail, and performance is dropping. Firewalls are added, additional load balancers are set up, the database is scaled, more application servers are added and so on. Things are still relatively simple. We are faced with new challenges, but obstacles can be overcome in time. Even though the complexity is increasing, we can still handle it with relative ease. In other words, what we're doing is still more or less the same but bigger. Business is doing well, but it is still relatively small.</p><p>And then it happens. The big thing you've been waiting for. Maybe one of the marketing campaigns hit the spot. Maybe there was a negative change in your competition. Maybe that last feature was indeed a killer one. No matter the reasons, business got a big boost. After a short period of happiness due to this change, your pain increases tenfold. Adding more databases does not seem to be enough. Multiplying application servers does not appear to fulfill the needs. You start adding caching and what so not. You start getting the feeling that every time you multiply something, benefits are not equally big. Costs increase, and you are still not able to meet the demand. Database replications are too slow. New application servers do not make such a big difference anymore. Operational costs are increasing faster than you expected. The situation hurts the business and the team. You are starting to realize that the architecture you were so proud of cannot fulfill this increase in load. You can not split it. You cannot scale things that hurt the most. You cannot start over. All you can do is continue multiplying with ever decreasing benefits of such actions.</p><p>The situation <a class="indexterm" id="id579"/>described above is quite common. What was good at the beginning, is not necessarily right when the demand increases. We need to balance the need for <span class="strong"><strong>You ain't going to need it</strong></span> (<span class="strong"><strong>YAGNI</strong></span>) principle and the longer term vision. We cannot start <a class="indexterm" id="id580"/>with the system optimized for large companies because it is too expensive and does not provide enough benefits when business is small. On the other hand, we cannot lose the focus from one of the main objectives of any business. We cannot not think about scaling from the very first day. Designing scalable architecture does not mean that we need to start with a cluster of a hundred servers. It does not mean that we have to develop something big and complex from the start. It means that we should start small, but in the way that, when it becomes big, it is easy to scale. While microservices are not the only way to accomplish that goal, they are indeed a good way to approach this problem. The cost is not in development but operations. If operations are automated, that cost can be absorbed quickly and does not need to represent a massive investment. As you already saw (and will continue seeing throughout the rest of the book), there are excellent open source tools at our disposal. The best part of automation is that the investment tends to have lower maintenance cost than when things are done manually.</p><p>We already discussed microservices and automation of their deployments on a tiny scale. Now it's time to convert this small scale to something bigger. Before we jump into practical parts, let us explore what are some of the different ways one might approach scaling.</p><p>We are often limited by our design and choosing the way applications are constructed limits our choices severely. Although there are many different ways to scale, most common one is called <span class="emphasis"><em>Axis Scali</em></span>
<span class="emphasis"><em>ng</em></span>.</p><div class="section" title="Axis scaling"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec72"/>Axis scaling</h2></div></div></div><p>Axis scaling <a class="indexterm" id="id581"/>can be best represented through three dimensions of a<a class="indexterm" id="id582"/> cube; <span class="emphasis"><em>x-axis</em></span>, <span class="emphasis"><em>y-axis</em></span> and <span class="emphasis"><em>z-axis</em></span>. Each of those dimensions describes a type of scaling.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>X-Axis</strong></span>: Horizontal duplication</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Y-Axis</strong></span>: Functional decomposition</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Z-Axis</strong></span>: Data partitioning<div class="mediaobject"><img alt="Axis scaling" src="graphics/B05848_14_01.jpg"/><div class="caption"><p>Figure 14-1 – Scale cube</p></div></div></li></ul></div><p>Let's <a class="indexterm" id="id583"/>go<a class="indexterm" id="id584"/> through axes, one at the time.</p><div class="section" title="X-Axis scaling"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec30"/>X-Axis scaling</h3></div></div></div><p>In a nutshell, <span class="emphasis"><em>x-axis scaling</em></span> is accomplished by running multiple instances of an application or<a class="indexterm" id="id585"/> a service. In most cases, there is a load balancer <a class="indexterm" id="id586"/>on top that makes sure that the traffic is shared among all those instances. The biggest advantage of x-axis scaling is simplicity. All we have to do is deploy the same application on multiple servers. For that reason, this is the most commonly used type of scaling. However, it comes with its set of disadvantages when applied to monolithic applications. Having a huge application usually requires big cache that demands heavy usage of memory. When such an application is multiplied, everything is multiplied by it, including the cache.</p><p>Another, often more important, problem is inappropriate usage of resources. Performance problems are almost never related to the whole application. Not all modules are equally affected, and, yet, we multiply everything. That means that even though we could be better of by scaling only part of the application that require such an action, we scale everything. Never the less, x-scaling is important no matter the architecture. The major difference is the effect that such a scaling has. By using microservices, we are not removing the need for x-axis scaling but making sure that due to their architecture such scaling has more effect than with alternative and more traditional approaches to architecture. With microservices we have the option to fine-tune scaling. We can have many instances of services that suffer a lot under heavy load and only a few instances of those that are<a class="indexterm" id="id587"/> used less often or require fewer resources. On top of that, since they are <a class="indexterm" id="id588"/>small, we might never reach a limit of a service. A small service in a big server would need to receive a truly massive amount of traffic before the need for scaling arises. Scaling microservices is more often related to fault tolerance than performance problems. We want to have multiple copies running so that, if one of them dies, the others can take over until recovery is performed:</p><div class="mediaobject"><img alt="X-Axis scaling" src="graphics/B05848_14_02.jpg"/><div class="caption"><p>Figure 14-2 – Monolithic application scaled inside a cluster</p></div></div></div><div class="section" title="Y-Axis scaling"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec31"/>Y-Axis scaling</h3></div></div></div><p>Y-axis scaling is<a class="indexterm" id="id589"/> all about decomposition of an application into <a class="indexterm" id="id590"/>smaller services. Even though there are different ways to accomplish this decomposition, microservices are probably the best approach we can take. When they are combined with immutability and self-sufficiency, there is indeed no better alternative (at least from the prism of y-axis scaling). Unlike x-axis scaling, the y-axis is not accomplished by<a class="indexterm" id="id591"/> running multiple<a class="indexterm" id="id592"/> instances of the same application but by having multiple different services distributed across the cluster.</p></div><div class="section" title="Z-Axis scaling"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec32"/>Z-Axis scaling</h3></div></div></div><p>Z-axis scaling is<a class="indexterm" id="id593"/> rarely applied to applications or services. Its<a class="indexterm" id="id594"/> primary and most common usage is among databases. The idea behind this type of scaling is to distribute data among multiple servers thus reducing the amount of work that each of them needs to perform. Data is partitioned and distributed so that each server needs to deal only with a subset of the data. This type of the<a class="indexterm" id="id595"/> separation is often called sharding, and there are many databases specially designed for this purpose. Benefits of z-axis scaling are most noticeable in I/O and cache and memory utilization.</p></div></div><div class="section" title="Clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec73"/>Clustering</h2></div></div></div><p>A server cluster consists of a set of connected servers that work together and can be seen as a single <a class="indexterm" id="id596"/>system. They are usually connected through fast local area network (LAN). The major difference between a cluster and simply a group of servers is that the <a class="indexterm" id="id597"/>cluster acts as a single system trying to provide high availability, load balancing, and parallel processing.</p><p>If we deploy applications, or services, to individually managed servers and treat them as separate units, the utilization of resources is sub-optimum. We cannot know in advance which group of services should be deployed to a server and utilize resources to their maximum. More importantly, resource usage tends to fluctuate. While, in the morning, some service might require a lot of memory, during the afternoon that usage might be lower. Having predefined servers does not allow us elasticity that would balance that usage in the best possible way. Even if such a high level of dynamism is not required, predefined servers tend to create problems when something goes wrong, resulting in manual actions to redeploy the affected services to a healthy node:</p><div class="mediaobject"><img alt="Clustering" src="graphics/B05848_14_03.jpg"/><div class="caption"><p>Figure 14-3 – Cluster with containers deployed to predefined servers</p></div></div><p>Real clustering is accomplished when we stop thinking in terms of individual servers and start thinking of a cluster; of all servers as one big entity. That can be better explained if we drop to a bit lower level. When we deploy an application, we tend to specify how much memory or CPU it might need. However, we do not decide which memory slots our application will use nor which CPUs it should utilize. For example, we don't specify that some application should use CPUs 4, 5 and 7. That would be inefficient and potentially dangerous. We only decide that three CPUs are required. The same approach should be taken on a higher level. We should not care where an application or a service will be deployed but what it needs. We should be able to define that the service has certain requirements and tell some tool to deploy it to whichever server in our cluster, as long as it fulfills the needs we have. The best (if not the only) way to accomplish that is to consider the <a class="indexterm" id="id598"/>whole cluster as one entity. We can increase or decrease the capacity <a class="indexterm" id="id599"/>of that cluster by adding or removing servers but, no matter what we do, it should still be a single entity. We define a strategy and let our services be deployed somewhere inside the cluster. Those using cloud providers<a class="indexterm" id="id600"/> like <span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>), Microsoft's Azure and <span class="strong"><strong>Google Cloud Engine</strong></span> (<span class="strong"><strong>GCP</strong></span>) are already accustomed to this approach, even<a class="indexterm" id="id601"/> though they might not be aware of it.</p><p>Throughout the rest of this chapter, we'll explore ways to create our cluster and explore tools that can help us with that objective. The fact that we'll be simulating the cluster locally does not mean that the same strategies cannot be applied to public or private clouds and data centers. Quite the opposite:</p><div class="mediaobject"><img alt="Clustering" src="graphics/B05848_14_04.jpg"/><div class="caption"><p>Figure 14-4 – Cluster with containers deployed to servers based on a predefined strategy</p></div></div></div><div class="section" title="Docker Clustering Tools Compared – Kubernetes versus Docker Swarm versus Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec74"/>Docker Clustering Tools Compared – Kubernetes versus Docker Swarm versus Mesos</h2></div></div></div><p>Kubernetes<a class="indexterm" id="id602"/> and Docker Swarm are<a class="indexterm" id="id603"/> probably the two most commonly<a class="indexterm" id="id604"/> used tools to deploy containers <a class="indexterm" id="id605"/>inside a cluster. Both are created as helper platforms that can be used to manage a cluster of containers and treat all servers as a single unit. While their goals are, somewhat, similar, they differ considerably in their approach.</p><div class="section" title="Kubernetes"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec33"/>Kubernetes</h3></div></div></div><p>Kubernetes is<a class="indexterm" id="id606"/> based on Google's experience of many years working with Linux containers. It is, in a way, a replica of what Google has been doing for a long time but, this time, adapted to Docker. That approach is great in many ways, most important being that they used their experience from the start. If you started using Kubernetes around Docker version 1.0 (or earlier), the experience with Kubernetes was great. It solved many of the problems that Docker itself had. We can mount persistent volumes that allow us to move containers without losing data, it uses flannel to create networking between containers, it has load balancer integrated, it uses etcd for service discovery, and so on. However, Kubernetes comes at a cost. When compared with Docker, it uses a different CLI, different API, and different YAML definitions. In other words, you cannot use Docker CLI, nor you can use Docker Compose to define containers. Everything needs to be done from scratch exclusively for Kubernetes. It's as if the tool was not written for Docker (which is partly true). Kubernetes brought clustering to a new level but at the expense of usability and steep learning curve.</p></div><div class="section" title="Docker Swarm"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec34"/>Docker Swarm</h3></div></div></div><p>Docker Swarm took a different approach. It is a native clustering for Docker. The best part is that it <a class="indexterm" id="id607"/>exposes standard Docker API meaning that any tool that you used to communicate with Docker (Docker CLI, Docker Compose, Dokku, Krane, and so on) can work equally well with Docker Swarm. That in itself is both an advantage and a disadvantage at the same time. Being able to use familiar tools of your choosing is great but for the same reasons we are bound by the limitations of Docker API. If the Docker API doesn't support something, there is no way around it through Swarm API, and some clever tricks need to be performed.</p></div><div class="section" title="Apache Mesos"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec35"/>Apache Mesos</h3></div></div></div><p>The next in<a class="indexterm" id="id608"/> line of tools that can be used to manage a cluster is Apache Mesos. It is the clustering veteran. Mesos abstracts CPU, memory, storage, and other resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to be easily built and run efficiently.</p><p>Mesos is made using the same principles as the Linux kernel, only at a different level of abstraction. Mesos kernel runs on every machine and provides applications with APIs for resource management and scheduling across entire datacenter and cloud environments. Unlike Kubernetes and Docker Swarm, Mesos is not limited to containers. It can work with almost any type of deployments including Docker containers.</p><p>Mesos uses Zookeeper for service discovery. It uses Linux containers to isolate processes. If, for example, we deploy Hadoop without using Docker, Mesos will run it as a native Linux container providing similar features as if we packed it as a Docker container.</p><p>Mesos provides few features that Swarm doesn't have at this moment, mainly more powerful scheduler. Apart from the scheduler, what makes Mesos attractive is that we can use it for both Docker and non-Docker deployments. Many organizations might not want to use Docker, or they might decide to use a combination of both Docker and non-Docker deployments. In such a case, Mesos is truly an excellent option if we do not want to deal with two sets of clustering tools; one for containers and the other for the rest of deployments.</p><p>However, Mesos is<a class="indexterm" id="id609"/> old and too big for what we're trying to accomplish. More importantly, Docker containers are an afterthought. The platform was not designed with them in mind but added Docker support later on. Working with Docker and Mesos feels awkward, and it becomes apparent from the very start that those two were not meant to be used together. Given the existence of Swarm and Kubernetes, there is nothing that Mesos can offer to those decided to embrace Docker. Mesos is falling behind. The main advantage it has over the other two tools is its wide adoption. Many started using it before the emergence of Docker and might choose stick with it. For those that have the option to start fresh, the choice should fall between Kubernetes and Docker Swarm.</p><p>We'll explore Kubernetes and Docker Swarm in more details and leave Mesos behind. The exploration will be based on their setup and features they provide for running containers in a cluster.</p></div></div><div class="section" title="Setting It Up"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec75"/>Setting It Up</h2></div></div></div><p>Setting up Docker Swarm is easy, straightforward and flexible. All we have to do is install one of the<a class="indexterm" id="id610"/> service discovery tools and run the <code class="literal">swarm</code> container on all nodes. Since the distribution itself is packed in a Docker container, it works in the same way no matter the operating system. We run the <code class="literal">swarm</code> container, expose a port and inform it about the address of the service discovery. It could hardly be easier than that. We can even start using it without any service discovery tool, see whether we like it and when our usage of it becomes more serious, add etcd, Consul or some of the other supported tools.</p><p>Kubernetes setup is <a class="indexterm" id="id611"/>quite more complicated and obfuscated. Installation instructions differ from OS to OS and provider to provider. Each OS or a hosting provider comes with its set of instructions, each of them having a separate maintenance team with a different set of problems. As an example, if you choose to try it out with Vagrant, you are stuck with Fedora. That does not mean that you cannot run it with Vagrant and, let's say, Ubuntu or CoreOS. You can, but you need to start searching for instructions outside the official Kubernetes Getting Started page. Whatever your needs are, it's likely that the community has the solution, but you still need to spend some time searching for it and hoping that it works from the first attempt. The bigger problem is that the installation relies on a bash script. That would not be a big deal in itself if we would not live in the era where configuration management is a must. We might not want to run a script but make Kubernetes be part of our Puppet, Chef, or Ansible definitions. Again, this can be overcome as well. You can find Ansible playbooks for running Kubernetes, or you can write your own. None of those issues are a big problem but, when <a class="indexterm" id="id612"/>compared with Swarm, they are a bit painful. With Docker, we were supposed not to have installation instructions (aside from a few <code class="literal">docker run</code> arguments). We were supposed to run containers. Swarm fulfills that promise, and Kubernetes doesn't.</p><p>While some might not <a class="indexterm" id="id613"/>care about which discovery tool is used, I love the simplicity of Swarm and the logic "batteries included but removable". Everything works out-of-the-box, but we still have the option to substitute one component for the other. Unlike Swarm, Kubernetes is an opinionated tool. You need to live with the choices it made for you. If you want to use Kubernetes, you have to use etcd. I'm not trying to say that etcd is bad (quite contrary), but if you prefer, for example, to use Consul, you're in a very complicated situation and would need to use one for Kubernetes and the other for the rest of your service discovery needs. Another thing I dislike about Kubernetes is its need to know things in advance, before the setup. You need to tell it the addresses of all your nodes, which role each of them has, how many minions there are in the cluster and so on. With Swarm, we just bring up a node and tell it to join the network. Nothing needs to be set in advance since the information about the cluster is propagated through the <code class="literal">gossip</code> protocol.</p><p>Setup might not be the most significant difference between those tools. No matter which tool you choose, sooner or later everything will be up and running, and you'll forget any trouble you might have had during the process. You might say that we should not choose one tool over the other only because one is easier to set up. Fair enough. Let's move on and speak about differences in how you define containers that should be run with those tools.</p></div><div class="section" title="Running Containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec76"/>Running Containers</h2></div></div></div><p>How do you <a class="indexterm" id="id614"/>define all the arguments needed for running Docker containers with Swarm? You don't! Actually, you do, but not in any form or way different from the way you were defining them before Swarm. If you are used to running containers through Docker CLI, you can keep using it with (almost) the same commands. If you prefer to use Docker Compose to run containers, you can continue using it to run them inside the Swarm cluster. Whichever way you've used to run your containers, the chances are that you can continue doing the same with Swarm but on a much larger scale.</p><p>Kubernetes requires you to learn its CLI and configurations. You cannot use <code class="literal">docker-compose.yml</code> definitions you created earlier. You'll have to create Kubernetes equivalents. You cannot use Docker CLI commands you learned before. You'll have to learn Kubernetes CLI and, likely, make sure that the whole organization learns it as well.</p><p>No matter which<a class="indexterm" id="id615"/> tool you choose for deployments to your cluster, chances are you are already familiar with Docker. You are probably already used to Docker Compose as a way to define arguments for the containers you'll run. If you played with it for more than a few hours, you are using it as a substitute for Docker CLI. You run containers with it, tail their logs, scale them, and so on. On the other hand, you might be a hard-core Docker user who does not like Docker Compose and prefers running everything through Docker CLI or you might have your bash scripts that run containers for you. No matter what you choose, it should work with Docker Swarm.</p><p>If you adopt Kubernetes, be prepared to have multiple definitions of the same thing. You will need Docker Compose to run your containers outside Kubernetes. Developers will continue needing to run containers on their laptops, your staging environments might or might not be a big cluster, and so on.</p><p>In other words, once you adopt Docker, Docker Compose or Docker CLI are unavoidable. You have to use them one way or another. Once you start using Kubernetes you will discover that all your Docker Compose definitions (or whatever else you might be using) need to be translated to Kubernetes way of describing things and, from there on, you will have to maintain both. With Kubernetes, everything will have to be duplicated resulting in higher cost of maintenance. And it's not only about duplicated configurations. Commands you'll run outside the cluster will be different from those inside the cluster. All those Docker commands you learned and love will have to get their Kubernetes equivalents inside the cluster.</p><p>Guys behind Kubernetes are not trying to make your life miserable by forcing you to do things "their way". The reason for such a big differences is in different approaches Swarm and Kubernetes are using to tackle the same problem. Swarm team decided to match their API with the one from Docker. As a result, we have (almost) full compatibility. Almost everything we can do with Docker we can do with Swarm as well only on a much larger scale. There's nothing new to do, no configurations to be duplicated and nothing new to learn. No matter whether you use Docker CLI directly or go through Swarm, API is (more or less) the same. The negative side of that story is that if there is something you'd like Swarm to do and that something is not part of the Docker API, you're in for a disappointment. Let us simplify this a bit. If you're looking for a tool for deploying containers in a cluster that will use Docker API, Swarm is the solution. On the other hand, if you want a tool that will overcome Docker limitations, you should go with Kubernetes. It is power (Kubernetes) against simplicity (Swarm). Or, at least, that's how it was until recently. But, I'm jumping ahead of myself.</p><p>The only question unanswered is what those limitations are. Two of the major ones were networking, persistent volumes and automatic failover in case one or more containers or a whole node stopped working.</p><p>Until Docker Swarm release 1.0 we could not link containers running on different servers. We still cannot link them, but now we have <code class="literal">multi-host networking</code> to help us connect containers running on different servers. It is a very powerful feature. Kubernetes used <code class="literal">flannel</code> to accomplish networking and now, since the Docker release 1.9, that feature is available as part of Docker CLI.</p><p>Another problem <a class="indexterm" id="id616"/>was persistent volumes. Docker introduced them in release 1.9. Until recently, if you persist a volume, that container was tied to the server that volume resides. It could not be moved around without, again, resorting to some nasty tricks like copying volume directory from one server to another. That in itself is a slow operation that defies the goals of the tools like Swarm. Besides, even if you have time to copy a volume from one to the other server, you do not know where to copy since clustering tools tend to treat your whole datacenter as a single entity. Your containers will be deployed to a location most suitable for them (least number of containers running, most CPUs or memory available, and so on). Now we have persistent volumes supported by Docker natively.</p><p>Finally, automatic failover is probably the only feature advantage Kubernetes has over Swarm. However, failover solution provided by Kuberentes is incomplete. If a container goes down, Kubernetes will detect that and start it again on a healthy node. The problem is that containers or whole nodes often do not fail for no reason. Much more needs to be done than a simple re-deployment. Someone needs to be notified, information before a failure needs to be evaluated, and so on. If re-deployment is all you need, Kubernetes is a good solution. If more is needed, Swarm, due to its "batteries included but removable" philosophy, allows you to build your solution. Regarding the failover, it's a question whether to aim for an out-of-the-box solution (Kubernetes) that is hard to extend or go for a solution that is built with the intention to be easily extended (Swarm).</p><p>Both networking and persistent volumes problems were one of the features supported by Kubernetes for quite some time and the reason many were choosing it over Swarm. That advantage disappeared with Docker release 1.9. Automatic fail-over remains an advantage Kubernetes has over Swarm when looking at out-of-the-box solutions. In the case of Swarm, we need to develop failover strategies ourselves.</p></div><div class="section" title="The Choice"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec77"/>The Choice</h2></div></div></div><p>When trying to <a class="indexterm" id="id617"/>make a choice between <a class="indexterm" id="id618"/>Docker Swarm and Kubernetes, think in following terms. Do you want to depend on Docker solving problems related to clustering. If you do, choose Swarm. If Docker does not support something, it will be unlikely that it will be supported by Swarm since it relies on Docker API. On the other hand, if you want a tool that works around Docker limitations, Kubernetes might be the right one for you. Kubernetes was not built around Docker but is based on Google's experience with containers. It is opinionated and tries to do things in its own way.</p><p>The real question is whether Kubernetes' way of doing things, which is quite different from how we use Docker, is overshadowed by advantages it gives. Or, should we place our bets into Docker itself and hope that it will solve those problems? Before you answer those questions, take a look at the Docker release 1.9. We got persistent volumes and software networking. We also got <code class="literal">unless-stopped</code> restart policy that will manage our unwanted failures. Now, there are three things less of a difference between Kubernetes and Swarm. Actually, these days there are very few advantages Kubernetes has over Swarm. Automatic failover featured by Kubernetes is a blessing and a curse at the same time. On the other hand, Swarm uses Docker API meaning that you get to keep all your commands and <a class="indexterm" id="id619"/>Docker Compose <a class="indexterm" id="id620"/>configurations. Personally, I'm placing my bets on Docker engine getting improvements and Docker Swarm running on top of it. The difference between the two is small. Both are production ready but Swarm is easier to set up, easier to use and we get to keep everything we built before moving to the cluster; there is no duplication between cluster and non-cluster configurations.</p><p>My recommendation is to go with Docker Swarm. Kubernetes is too opinionated, hard to set up, too different from Docker CLI/API and at the same time, besides automatic failover, it doesn't have real advantages over Swarm since the Docker release 1.9. That doesn't mean that there are no features available in Kubernetes that are not supported by Swarm. There are feature differences in both directions. However, those differences are, in my opinion, not significant ones and the gap is getting smaller with each Docker release. Actually, for many use cases, there is no gap at all while Docker Swarm is easier to set up, learn and use.</p><p>Let us give Docker Swarm a spin and see how it fares.</p></div></div></div>
<div class="section" title="Docker Swarm walkthrough"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl1sec35"/>Docker Swarm walkthrough</h1></div></div></div><p>To set up <a class="indexterm" id="id621"/>Docker Swarm, we need one of the service discovery tools. Consul served us well, and we'll continue using it for this purpose. It is a great tool and works well with Swarm. We'll set up three servers. One will act as master and the other two as cluster nodes:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_05.jpg"/><div class="caption"><p>Figure 14-5 – Docker Swarm cluster with Consul used for service discovery</p></div></div><p>Swarm will use <a class="indexterm" id="id622"/>Consul instances to register and retrieve information about nodes and services deployed in them. Whenever we bring up a new node or halt an existing one, that information will be propagated to all Consul instances and reach Docker Swarm, which, in turn, will know where to deploy our containers. The master node will have Swarm master running. We'll use its API to instruct Swarm what to deploy and what the requirements are (number of CPUs, the amount of memory, and so on). Node servers will have Swarm nodes deployed. Each time Swarm master receives an instruction to deploy a container, it will evaluate the current situation of the cluster and send instructions to one of the nodes to perform the deployment:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_06.jpg"/><div class="caption"><p>Figure 14-6 – Docker Swarm cluster with one master and two nodes</p></div></div><p>We'll start <a class="indexterm" id="id623"/>with the <span class="emphasis"><em>spread</em></span> strategy that will deploy containers to a node that has the least number of containers running. Since, in the beginning, nodes will be empty, when given instruction to deploy the first container, Swarm master will propagate it to one of the nodes since both are empty at the moment:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_07.jpg"/><div class="caption"><p>Figure 14-7 – Docker Swarm cluster with the first container deployed</p></div></div><p>When given the<a class="indexterm" id="id624"/> second instruction to deploy a container, Swarm master will decide to propagate it to the other Swarm node, since the first already has one container running:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_08.jpg"/><div class="caption"><p>Figure 14-8 – Docker Swarm cluster with the second container deployed</p></div></div><p>If we continue <a class="indexterm" id="id625"/>deploying containers, at some point our tiny cluster will become saturated, and something would need to be done before the server collapses:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_09.jpg"/><div class="caption"><p>Figure 14-9 – Docker Swarm cluster with all nodes full</p></div></div><p>The only thing <a class="indexterm" id="id626"/>we would need to do to increase the cluster capacity is to bring up a new server with Consul and Swarm node. As soon as such a node is brought up, its information would be propagated throughout Consul instances as well as to Swarm master. From that moment on, Swarm would have that node in the account for all new deployments. Since this server would start with no containers and we are using a <a class="indexterm" id="id627"/>simple <span class="emphasis"><em>spread</em></span> strategy, all new deployments would be performed on that node until it reaches the same number of running containers as the others:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_10.jpg"/><div class="caption"><p>Figure 14-10 – Docker Swarm cluster with container deployed to the new node</p></div></div><p>Opposite scenario <a class="indexterm" id="id628"/>can be observed in case one node stops responding due to a failure. Consul cluster would detect that one of it's members is not responding and propagate that information throughout the cluster, thus reaching Swarm master. From that moment on, all new deployments would be sent to one of the healthy nodes:</p><div class="mediaobject"><img alt="Docker Swarm walkthrough" src="graphics/B05848_14_11.jpg"/><div class="caption"><p>Figure 14-11 – Docker Swarm cluster one node failed and containers distributed over healthy nodes</p></div></div><p>Let us dive into<a class="indexterm" id="id629"/> simple examples we just discussed. Later on, we'll explore other strategies as well as the ways Swarm behaves when certain constraints are set; CPU, memory and the like.</p></div>
<div class="section" title="Setting Up Docker Swarm"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl1sec36"/>Setting Up Docker Swarm</h1></div></div></div><p>To see Docker Swarm in action, we'll simulate an Ubuntu cluster. We'll bring up the <code class="literal">cd</code> node that we'll use <a class="indexterm" id="id630"/>for orchestration, one node that will act as Swarm master and two nodes that will form the cluster. Up to this point, we always used Ubuntu 14.04 LTS (long term support) since it is considered stable and supported for a long time. The next long term support version will be 15.04 LTS (not released at the time this book was written). Since some of the features we'll explore later on, throughout this chapter, will need a relatively new Kernel, the <code class="literal">swarm</code> nodes will be running Ubuntu 15.04. If you open the Vagrantfile, you'll notice that Swarm master and nodes have the following line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>d.vm.box = "ubuntu/vivid64"</strong></span>
</pre></div><p>
<code class="literal">Vivid64</code> is the code name for Ubuntu 15.04.</p><p>Let us bring up the nodes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd swarm-master swarm-node-1 swarm-node-2</strong></span>
</pre></div><p>With all the four nodes up and running, we can proceed, and create the Swarm cluster. As before, we'll do the provisioning using Ansible:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant ssh cd</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/swarm.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>
</pre></div><p>Let us use our time<a class="indexterm" id="id631"/> wisely and explore the <code class="literal">swarm.yml</code> playbook, while Ansible is provisioning our servers. The content of the <code class="literal">swarm.yml</code> file is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: swarm</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  vars:</strong></span>
<span class="strong"><strong>    - debian_version: vivid</strong></span>
<span class="strong"><strong>    - docker_cfg_dest: /lib/systemd/system/docker.service</strong></span>
<span class="strong"><strong>    - is_systemd: true</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - docker</strong></span>
<span class="strong"><strong>    - consul</strong></span>
<span class="strong"><strong>    - swarm</strong></span>
<span class="strong"><strong>    - registrator</strong></span>
</pre></div><p>We started by setting up <span class="emphasis"><em>docker</em></span>. Since this time we're using a different version of Ubuntu, we had to specify those differences as variables, so that the correct repository is used (<code class="literal">debian_version</code>), as well as to reload service configuration (<code class="literal">is_systemd</code>). We also set the <code class="literal">docker_cfg_dest</code> variable so that the configuration file is sent to the correct location.</p><p>We have few more variables set in the <code class="literal">hosts/prod</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[swarm]</strong></span>
<span class="strong"><strong>10.100.192.200 swarm_master=true consul_extra="-server -bootstrap-expect 1" docker_cfg=docker-swarm-master.service</strong></span>
<span class="strong"><strong>10.100.192.20[1:2] swarm_master_ip=10.100.192.200 consul_server_ip=10.100.192.200 docker_cfg=docker-swarm-node.service</strong></span>
</pre></div><p>We'll explore <code class="literal">swarm_master</code> and <code class="literal">swarm_master_ip</code> later on. For now, please remember that they are defined in the <code class="literal">prod</code> file so that they can be applied (or omitted) based on the server type (master or node). Depending on whether we are provisioning master or node, Docker configuration file is <code class="literal">docker-swarm-master.service</code> or <code class="literal">docker-swarm-node.service</code>.</p><p>Let's take a look at the <code class="literal">ExecStart</code> part of the master node Docker configuration (the rest is the same as the standard one that comes with the Docker package) defined in <code class="literal">roles/docker/templates/docker-swarm-master.service</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ExecStart=/usr/bin/docker daemon -H fd:// \</strong></span>
<span class="strong"><strong>          --insecure-registry 10.100.198.200:5000 \</strong></span>
<span class="strong"><strong>          --registry-mirror=http://10.100.198.200:5001 \</strong></span>
<span class="strong"><strong>          --cluster-store=consul://{{ ip }}:8500/swarm \</strong></span>
<span class="strong"><strong>          --cluster-advertise={{ ip }}:2375 {{ docker_extra }}</strong></span>
</pre></div><p>We're telling <a class="indexterm" id="id632"/>Docker to allow insecure registry on the IP/port where our private registry runs (located in the <code class="literal">cd</code> node). We're also specifying that Swarm cluster information should be stored in Consul running on the same node, as well as that it should be advertised to the port <code class="literal">2375</code>:</p><p>The node configuration defined in <code class="literal">roles/docker/templates/docker-swarm-node.service</code> has few more arguments:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ExecStart=/usr/bin/docker daemon -H fd:// \</strong></span>
<span class="strong"><strong>          -H tcp://0.0.0.0:2375 \</strong></span>
<span class="strong"><strong>          -H unix:///var/run/docker.sock \</strong></span>
<span class="strong"><strong>          --insecure-registry 10.100.198.200:5000 \</strong></span>
<span class="strong"><strong>          --registry-mirror=http://10.100.198.200:5001 \</strong></span>
<span class="strong"><strong>          --cluster-store=consul://{{ ip }}:8500/swarm \</strong></span>
<span class="strong"><strong>          --cluster-advertise={{ ip }}:2375 {{ docker_extra }}</strong></span>
</pre></div><p>Apart from those arguments that are the same as in the master node, we're telling Docker to allow communication on the port <code class="literal">2375</code> (<code class="literal">-H tcp://0.0.0.0:2375</code>) as well as through the socket (<code class="literal">-H unix:///var/run/docker.sock</code>):</p><p>Both <code class="literal">master</code> and <code class="literal">node</code> configurations are following the standard settings recommended by the official Docker Swarm documentation when used in conjunction with Consul.</p><p>The rest of the roles used in the <code class="literal">swarm.yml</code> playbook are <code class="literal">consul</code>, <code class="literal">swarm</code>, and <code class="literal">registrator</code>. Since we already used and saw Consul and Registrator roles, we'll explore only tasks belonging to the <code class="literal">swarm</code> role defined in the <code class="literal">roles/swarm/tasks/main.yml</code> file:</p><div class="informalexample"><pre class="programlisting">- name: Swarm node is running
  docker:
    name: swarm-node
    image: swarm
    command: join --advertise={{ ip }}:2375 consul://{{ ip }}:8500/swarm
    env:
      SERVICE_NAME: swarm-node
  when: not swarm_master is defined
  tags: [swarm]

- name: Swarm master is running
  docker:
    name: swarm-master
    image: swarm
    ports: 2375:2375
    command: manage consul://{{ ip }}:8500/swarm
    env:
      SERVICE_NAME: swarm-master
  when: swarm_master is defined
  tags: [swarm]</pre></div><p>As you can see, running Swarm is pretty straightforward. All we have to do is run the <code class="literal">swarm</code> container<a class="indexterm" id="id633"/> and, depending on whether it's master or node, specify one command or the other. If server acts as a Swarm node, the command is <code class="literal">join --advertise={{ ip }}:2375 consul://{{ ip }}:8500/swarm</code> which, translated into plain words, means that it should join the cluster, advertise its existence on port <code class="literal">2375</code> and use Consul running on the same server for service discovery. The command that should be used in the Swarm master is even shorter; <code class="literal">manage consul://{{ ip }}:8500/swarm</code>. All we had to do is specify that this Swarm container should be used to manage the cluster and, as with Swarm nodes, use Consul for service discovery.</p><p>Hopefully, the playbook we run earlier finished its execution. If it didn't, grab a coffee and continue reading once it's done. We're about to check whether our Swarm cluster is working as expected.</p><p>Since we are still inside the <code class="literal">cd</code> node, we should tell Docker CLI to use a different host.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://10.100.192.200:2375</strong></span>
</pre></div><p>With the Docker client running on <code class="literal">cd</code> and using the <code class="literal">swarm-master</code> node as a host, we can control the Swarm cluster remotely. For a start, we can check the information of our cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker info</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Containers: 4</strong></span>
<span class="strong"><strong>Images: 4</strong></span>
<span class="strong"><strong>Role: primary</strong></span>
<span class="strong"><strong>Strategy: spread</strong></span>
<span class="strong"><strong>Filters: health, port, dependency, affinity, constraint</strong></span>
<span class="strong"><strong>Nodes: 2</strong></span>
<span class="strong"><strong> swarm-node-1: 10.100.192.201:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>  └ Labels: executiondriver=native-0.2, kernelversion=3.19.0-42-generic, operatingsystem=Ubuntu 15.04, storagedriver=devicemapper</strong></span>
<span class="strong"><strong> swarm-node-2: 10.100.192.202:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>  └ Labels: executiondriver=native-0.2, kernelversion=3.19.0-42-generic, operatingsystem=Ubuntu 15.04, storagedriver=devicemapper</strong></span>
<span class="strong"><strong>CPUs: 2</strong></span>
<span class="strong"><strong>Total Memory: 3.07 GiB</strong></span>
<span class="strong"><strong>Name: b358fe59b011</strong></span>
</pre></div><p>Isn't this great? With a single command, we have an overview of the whole cluster. While, at this moment, we have only two servers (<code class="literal">swarm-node-1</code> and <code class="literal">swarm-node-2</code>), if there would be hundred, thousand, or even more nodes, <code class="literal">docker info</code> would provide information about <a class="indexterm" id="id634"/>all of them. In this case, we can see that four containers are running and four images. That is correct since each node is running Swarm and Registrator containers. Further on, we can see the <code class="literal">Role</code>, <code class="literal">Strategy</code>, and <code class="literal">Filters</code>. Next in the line are nodes that constitute our cluster followed by information about each of them. We can see how many containers each is running (currently two), how many CPUs and memory is reserved for our containers, and labels associated with each node. Finally, we can see the total number of CPUs and memory of the whole cluster. Everything presented by <code class="literal">docker info</code> acts not only as information but also a functionality of the Swarm cluster. For now, please note that all this information is available for inspection. Later on, we'll explore how to utilize it for our benefit.</p><p>The best part of Docker Swarm is that it shares the same API as Docker, so all the commands we already used throughout this book are available. The only difference is that instead of operating Docker on a single server, with Swarm we are operating a whole cluster. For example, we can list all images and processes throughout the entire Swarm cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker images</strong></span>

<span class="strong"><strong>docker ps -a</strong></span>
</pre></div><p>By running <code class="literal">docker images</code> and <code class="literal">docker ps -a</code> we can observe that there are two images pulled into the cluster and four containers running (two containers on each of the two servers). The only visual difference is that names of running containers are prefixed with the name of the server they are running on. For example, the container named <code class="literal">registrator</code> is presented as <code class="literal">swarm-node-1/registrator</code> and <code class="literal">swarm-node-2/registrator</code>. The combined output of those two commands is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>REPOSITORY               TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</strong></span>
<span class="strong"><strong>swarm                    latest              a9975e2cc0a3        4 weeks ago         17.15 MB</strong></span>
<span class="strong"><strong>gliderlabs/registrator   latest              d44d11afc6cc        4 months ago        20.93 MB</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                           NAMES</strong></span>
<span class="strong"><strong>a2c7d156c99d        gliderlabs/registrator   "/bin/registrator -ip"   2 hours ago         Up 2 hours                                          swarm-node-2/registrator</strong></span>
<span class="strong"><strong>e9b034aa3fc0        swarm                    "/swarm join --advert"   2 hours ago         Up 2 hours          2375/tcp                        swarm-node-2/swarm-node</strong></span>
<span class="strong"><strong>a685cdb09814        gliderlabs/registrator   "/bin/registrator -ip"   2 hours ago         Up 2 hours                                          swarm-node-1/registrator</strong></span>
<span class="strong"><strong>5991e9bd2a40        swarm                    "/swarm join --advert"   2 hours ago         Up 2 hours          2375/tcp                        swarm-node-1/swarm-node</strong></span>
</pre></div><p>Now that we <a class="indexterm" id="id635"/>know that Docker commands work in the same way when run against the remote server (<code class="literal">swarm-master</code>) and can be used to control the whole cluster (<code class="literal">swarm-node-1</code> and <code class="literal">swarm-nod</code>
<code class="literal">e-2</code>), let's try to deploy our <code class="literal">books-ms</code> service.</p><div class="section" title="Deploying with Docker Swarm"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec78"/>Deploying with Docker Swarm</h2></div></div></div><p>We'll start by <a class="indexterm" id="id636"/>repeating the same deployment process we did before, but, this time, we'll be sending commands to the Swarm master:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>git clone https://github.com/vfarcic/books-ms.git</strong></span>

<span class="strong"><strong>cd ~/books-ms</strong></span>
</pre></div><p>We cloned the <code class="literal">books-ms</code> repository and, now, we can run the service through Docker Compose:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose up -d app</strong></span>
</pre></div><p>Since the <code class="literal">app</code> target is linked with the <code class="literal">db</code>, Docker Compose run both. So far, everything looks the same as if we run the same command without Docker Swarm. Let us take a look at the processes that were created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app_1</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app_1/booksms_db_1,swarm-node-2/booksms_app_1/db,swarm-node-2/booksms_app_1/db_1,swarm-node-2/booksms_db_1</strong></span>
</pre></div><p>As we can see, both<a class="indexterm" id="id637"/> containers are running on <code class="literal">swarm-node-2</code>. In your case, it could be <code class="literal">swarm-node-1</code>. We did not make the decision where to deploy the containers. Swarm did that for us. Since we are using the default strategy that, without specifying additional constraints, runs containers on a server that has the least number of them running. Since both <code class="literal">swarm-node-1</code> and <code class="literal">swarm-node-2</code> were equally empty (or full), Swarm had an easy choice and could have placed containers on either one of those servers. In this case, it chose <code class="literal">swarm-node-2</code>.</p><p>The problem with the deployment we just performed is that the two targets (<code class="literal">app</code> and <code class="literal">db</code>) are linked. In such a case, Docker has no other option but to place both containers on the same server. That, in a way, defies the objective we're trying to accomplish. We want to deploy containers to the cluster and, as you'll soon discover, be able to scale them easily. If both containers need to be run on the same server, we are limiting Swarm's ability to distribute them properly. In this example, those two containers would be better of running on separate servers. If, before deploying those containers, both servers had the equal number of containers running, it would make more sense to run the <code class="literal">app</code> on one and the <code class="literal">db</code> on the other. That way we'd distribute resource usage much better. As it is now, the <code class="literal">swarm-node-2</code> needs to do all the work, and the <code class="literal">swarm-node-1</code> is empty. The first thing we should do is to get rid of the link.</p><p>Let's stop the containers we're running and start over:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose stop</strong></span>

<span class="strong"><strong>docker-compose rm -f</strong></span>
</pre></div><p>That was another example of advantages Swarm provides. We sent the <code class="literal">stop</code> and <code class="literal">rm</code> commands to the Swarm master and it located containers for us. From now on, all the behavior will be the same, in the sense that, through the Swarm master, we'll treat the whole cluster as one single unit oblivious of the specifics of each server.</p></div><div class="section" title="Deploying with Docker Swarm without Links"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec79"/>Deploying with Docker Swarm without Links</h2></div></div></div><p>To deploy <a class="indexterm" id="id638"/>containers to Docker Swarm cluster properly, we'll use a different file for Docker Compose definition; <code class="literal">docker-compose-no-links.yml</code>. The targets are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>app:</strong></span>
<span class="strong"><strong>  image: 10.100.198.200:5000/books-ms</strong></span>
<span class="strong"><strong>  ports:</strong></span>
<span class="strong"><strong>    - 8080</strong></span>

<span class="strong"><strong>db:</strong></span>
<span class="strong"><strong>  image: mongo</strong></span>
</pre></div><p>The only significant difference between <code class="literal">app</code> and <code class="literal">db</code> targets defined in <code class="literal">docker-compose.yml</code> and <code class="literal">docker-compose-swarm.yml</code> is that the latter does not use links. As you will see soon, this will allow us to distribute freely containers inside the cluster.</p><p>Let's take a look at what happens<a class="indexterm" id="id639"/> if we bring up <span class="emphasis"><em>db</em></span> and <span class="emphasis"><em>app</em></span> containers without the <a class="indexterm" id="id640"/>link.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose -f docker-compose-no-links.yml up -d db app</strong></span>

<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>
</pre></div><p>The output<a class="indexterm" id="id641"/> of the <code class="literal">docker ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_db_1</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app_1</strong></span>
</pre></div><p>As you can see, this time, Swarm decided to place each container on a different server. It brought up the first container and, since from that moment on one server had more containers than the other, it choose to bring up the second on the other node.</p><p>By removing linking between containers, we solved one problem but introduced another. Now our containers can be distributed much more efficiently, but they cannot communicate with each other. We can address this issue by using a <code class="literal">proxy</code> service (nginx, HAProxy, and so on). However, our <code class="literal">db</code> target does not expose any ports to the outside world. A good practice is to expose only ports of services that are publicly accessible. For that reason, the <code class="literal">app</code> target exposes port <code class="literal">8080</code> and the <code class="literal">db</code> target doesn't expose any. The <code class="literal">db</code> target is meant to be used internally, and only by the <code class="literal">app</code>. Since the Docker release 1.9, linking can be considered deprecated, for <a class="indexterm" id="id642"/>a new feature called <span class="emphasis"><em>networking</em></span>:</p><p>Let's remove the containers and try to bring them up networking enabled:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose -f docker-compose-no-links.yml sto</strong></span>
<span class="strong"><strong>p</strong></span>

<span class="strong"><strong>docker-compose -f docker-compose-no-links.yml rm -f</strong></span>
</pre></div></div><div class="section" title="Deploying with Docker Swarm and Docker Networking"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec80"/>Deploying with Docker Swarm and Docker Networking</h2></div></div></div><p>At the time I <a class="indexterm" id="id643"/>was writing this chapter, Docker introduced<a class="indexterm" id="id644"/> the new release 1.9. It is, without a doubt, the most significant release, since version 1.0. It gave us two long awaited features; multi-host networking and persistent volumes. Networking makes linking deprecated and is the feature we need to connect containers across multiple hosts. There is no more need for proxy services to connect containers internally. That is not to say that proxy is not useful, but that we should use a proxy as a public interface towards our services and networking for connecting containers that form a logical group. The new Docker networking and proxy services have different advantages and should be used for different use cases. Proxy services provide load balancing and can control the access to our services. Docker networking is a convenient way to connect separate containers that form a single<a class="indexterm" id="id645"/> service and reside on the same network. A <a class="indexterm" id="id646"/>typical use case for Docker networking would be a service that requires a connection to a database. We can connect those two through networking. Furthermore, the service itself might need to be scaled and have multiple instances running. A proxy service with load balancer should fulfill that requirement. Finally, other services might need to access this service. Since we want to take advantage of load balancing, that access should also be through a proxy:</p><div class="mediaobject"><img alt="Deploying with Docker Swarm and Docker Networking" src="graphics/B05848_14_12.jpg"/><div class="caption"><p>Figure 14-12 – Multi-host networking combined with a proxy and load balancing service</p></div></div><p>The figure 14-12 represents one common use case. We have a scaled service with two instances running on <code class="literal">nodes-01</code> and <code class="literal">nodes-03</code>. All communication to those services is performed through a proxy service that takes care of load balancing and security. Any service (be it external or internal) that wants to access our service needs to go through the proxy. Internally, the service uses the database. The communication between the service instances and the database is internal and performed through the multi-host network. This setting allows us to scale easily within the cluster while keeping internal all communication between containers that compose a single service. In other words, all communication between containers that compose a single service is done through networking while the communication between services is performed through the proxy.</p><p>There are different ways to create a multi-host network. We can set up the network manually:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker network create my-network</strong></span>

<span class="strong"><strong>docker network ls</strong></span>
</pre></div><p>The output of the <code class="literal">network ls</code> command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NETWORK ID          NAME                  DRIVER</strong></span>
<span class="strong"><strong>5fc39aac18bf        swarm-node-2/host     host</strong></span>
<span class="strong"><strong>aa2c17ae2039        swarm-node-2/bridge   bridge</strong></span>
<span class="strong"><strong>267230c8d144        my-network            overlay</strong></span>
<span class="strong"><strong>bfc2a0b1694b        swarm-node-2/none     null</strong></span>
<span class="strong"><strong>b0b1aa45c937        swarm-node-1/none     null</strong></span>
<span class="strong"><strong>613fc0ba5811        swarm-node-1/host     host</strong></span>
<span class="strong"><strong>74786f8b833f        swarm-node-1/bridge   bridge</strong></span>
</pre></div><p>You can see that <a class="indexterm" id="id647"/>one of the networks is <code class="literal">my-network</code> we <a class="indexterm" id="id648"/>created earlier. It spans the whole Swarm cluster and we can use it with the <code class="literal">--net</code> argument:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --name books-ms-db \</strong></span>
<span class="strong"><strong>    --net my-network \</strong></span>
<span class="strong"><strong>    mongo</strong></span>

<span class="strong"><strong>docker run -d --name books-ms \</strong></span>
<span class="strong"><strong>    --net my-network \</strong></span>
<span class="strong"><strong>    -e DB_HOST=books-ms-db \</strong></span>
<span class="strong"><strong>    -p 8080 \</strong></span>
<span class="strong"><strong>    10.100.198.200:5000/books-ms</strong></span>
</pre></div><p>We started two containers that compose a single service; <code class="literal">books-ms</code> is the API that communicates with <code class="literal">books-ms-db</code> that acts as a database. Since both containers had the <code class="literal">--net my-network</code> argument, they both belong to the <code class="literal">my-network</code> network. As a result, Docker updated hosts file providing each container with an alias that can be used for internal communication.</p><p>Let's enter the <code class="literal">books-ms</code> container and take a look at the hosts file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker exec -it books-ms bash</strong></span>

<span class="strong"><strong>cat /etc/hosts</strong></span>

<span class="strong"><strong>exit</strong></span>
</pre></div><p>The output of the <code class="literal">exec</code> command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>10.0.0.2    3166318f0f9c</strong></span>
<span class="strong"><strong>127.0.0.1   localhost</strong></span>
<span class="strong"><strong>::1 localhost ip6-localhost ip6-loopback</strong></span>
<span class="strong"><strong>fe00::0 ip6-localnet</strong></span>
<span class="strong"><strong>ff00::0 ip6-mcastprefix</strong></span>
<span class="strong"><strong>ff02::1 ip6-allnodes</strong></span>
<span class="strong"><strong>ff02::2 ip6-allrouters</strong></span>
<span class="strong"><strong>10.0.0.2    books-ms-db</strong></span>
<span class="strong"><strong>10.0.0.2    books-ms-db.my-network</strong></span>
</pre></div><p>The interesting part of the <code class="literal">hosts</code> file are the last two entries. Docker detected that the <code class="literal">books-ms-db</code> container uses the same network as the <code class="literal">books-ms</code> container, and updated the <code class="literal">hosts</code> file by<a class="indexterm" id="id649"/> adding <code class="literal">books-ms-db</code> and <code class="literal">books-ms-db.my-network</code> aliases. If some convention is used, it is trivial to code our services in a <a class="indexterm" id="id650"/>way that they use aliases like that one to communicate with resources located in a separate container (in this case with the database).</p><p>We also passed an environment variable <code class="literal">DB_HOST</code> to the <code class="literal">book-ms</code>. That indicates to our service which host to use to connect to the database. We can see this by outputting environments of the container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker exec -it books-ms env</strong></span>
</pre></div><p>The output of the command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</strong></span>
<span class="strong"><strong>HOSTNAME=eb3443a66355</strong></span>
<span class="strong"><strong>DB_HOST=books-ms-db</strong></span>
<span class="strong"><strong>DB_DBNAME=books</strong></span>
<span class="strong"><strong>DB_COLLECTION=books</strong></span>
<span class="strong"><strong>HOME=/root</strong></span>
</pre></div><p>As you can see, one of the environment variables is <code class="literal">DB_HOST</code> with the value <code class="literal">books-ms-db</code>.</p><p>What we have right now is Docker networking that created hosts alias <code class="literal">books-ms-db</code> pointing to the IP of the network Docker created. We also have an environment variable <code class="literal">DB_HOST</code> with value <code class="literal">books-ms-db</code>. The code of the service uses that variable to connect to the database.</p><p>As expected, we can specify <code class="literal">network</code> as part of our Docker Compose specification. Before we try it out, let's remove those two containers and the network.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker rm -f books-ms books-ms-db</strong></span>

<span class="strong"><strong>docker network rm my-network</strong></span>
</pre></div><p>This time, we'll run containers through Docker Compose. We'll use the <code class="literal">net</code> argument inside <code class="literal">docker-compose-swarm.yml</code> and, in that way, do the same process as we did earlier. The alternative would be to use new Docker Compose argument <code class="literal">--x-networking</code> that would create the network for us but, at this moment, it is in the experimental stage and not entirely reliable. Before we proceed, let us take a quick look at the relevant targets inside the <code class="literal">docker-compose-swarm.yml</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>app:</strong></span>
<span class="strong"><strong>  image: 10.100.198.200:5000/books-ms</strong></span>
<span class="strong"><strong>  ports:</strong></span>
<span class="strong"><strong>    - 8080</strong></span>
<span class="strong"><strong>  net: books-ms</strong></span>
<span class="strong"><strong>  environment:</strong></span>
<span class="strong"><strong>    - SERVICE_NAME=books-ms</strong></span>
<span class="strong"><strong>    - DB_HOST=books-ms-db</strong></span>

<span class="strong"><strong>db:</strong></span>
<span class="strong"><strong>  container_name: books-ms-db</strong></span>
<span class="strong"><strong>  image: mongo</strong></span>
<span class="strong"><strong>  net: books-ms</strong></span>
<span class="strong"><strong>  environment:</strong></span>
<span class="strong"><strong>    - SERVICE_NAME=books-ms-db</strong></span>
</pre></div><p>The only<a class="indexterm" id="id651"/> important difference is the addition <a class="indexterm" id="id652"/>of the <code class="literal">net</code> argument. Everything else is, more or less, the same as in many other targets we explored by now.</p><p>Let us create the network and run our containers through Docker Compose:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker network create books-ms</strong></span>

<span class="strong"><strong>docker-compose -f docker-compose-swarm.yml \</strong></span>
<span class="strong"><strong>    up -d db app</strong></span>
</pre></div><p>The output of the command we just run is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Creating booksms_app_1</strong></span>
<span class="strong"><strong>Creating books-ms-db</strong></span>
</pre></div><p>Before creating the services <code class="literal">app</code> and <code class="literal">db</code>, we created a new network called <code class="literal">books-ms</code>. The name of the network is the same as the value of the <span class="emphasis"><em>net</em></span> argument specified in the <code class="literal">docker-compose-swarm.yml</code> file.</p><p>We can confirm that the network was created by running the <code class="literal">docker network ls</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker network ls</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NETWORK ID          NAME                           DRIVER</strong></span>
<span class="strong"><strong>6e5f816d4800        swarm-node-1/host              host</strong></span>
<span class="strong"><strong>aa1ccdaefd70        swarm-node-2/docker_gwbridge   bridge</strong></span>
<span class="strong"><strong>cd8b1c3d9be5        swarm-node-2/none              null</strong></span>
<span class="strong"><strong>ebcc040e5c0c        swarm-node-1/bridge            bridge</strong></span>
<span class="strong"><strong>6768bad8b390        swarm-node-1/docker_gwbridge   bridge</strong></span>
<span class="strong"><strong>8ebdbd3de5a6        swarm-node-1/none              null</strong></span>
<span class="strong"><strong>58a585d09bbc        books-ms                       overlay</strong></span>
<span class="strong"><strong>de4925ea50d1        swarm-node-2/bridge            bridge</strong></span>
<span class="strong"><strong>2b003ff6e5da        swarm-node-2/host              host</strong></span>
</pre></div><p>As you can see, the <code class="literal">overlay</code> network <code class="literal">books-ms</code> has been created.</p><p>We can also <a class="indexterm" id="id653"/>double check that the <code class="literal">hosts</code> file inside<a class="indexterm" id="id654"/> containers has been updated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker exec -it booksms_app_1 bash</strong></span>

<span class="strong"><strong>cat /etc/hosts</strong></span>

<span class="strong"><strong>exit</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>10.0.0.2    3166318f0f9c</strong></span>
<span class="strong"><strong>127.0.0.1   localhost</strong></span>
<span class="strong"><strong>::1 localhost ip6-localhost ip6-loopback</strong></span>
<span class="strong"><strong>fe00::0 ip6-localnet</strong></span>
<span class="strong"><strong>ff00::0 ip6-mcastprefix</strong></span>
<span class="strong"><strong>ff02::1 ip6-allnodes</strong></span>
<span class="strong"><strong>ff02::2 ip6-allrouters</strong></span>
<span class="strong"><strong>10.0.0.3    books-ms-db</strong></span>
<span class="strong"><strong>10.0.0.3    books-ms-db.my-network</strong></span>
</pre></div><p>Finally, let's see how did Swarm distribute our containers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/books-ms-db</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app_1</strong></span>
</pre></div><p>Swarm deployed the <code class="literal">app</code> container to the <code class="literal">swarm-node-1</code> and the <code class="literal">db</code> container to the <code class="literal">swarm-node-2</code>.</p><p>Finally, let's test whether the <code class="literal">book-ms</code> service is working properly. We do not know where did Swarm deploy the container nor which port is exposed. Since we do not (yet) have a proxy, we'll retrieve the IP and the port of the service from Consul, send a PUT request to store some data in the database residing in a different container and, finally, send a GET request to check whether we can retrieve the record. Since we do not have a proxy service that <a class="indexterm" id="id655"/>would make sure that requests are<a class="indexterm" id="id656"/> redirected to the correct server and port, we'll have to retrieve the IP and the port from Consul:</p><div class="informalexample"><pre class="programlisting">ADDRESS=`curl \
    10.100.192.200:8500/v1/catalog/service/books-ms \
    | jq -r '.[0].ServiceAddress + ":" + (.[0].ServicePort | tostring)'`

curl -H 'Content-Type: application/json' -X PUT -d \
  '{"_id": 2,
  "title": "My Second Book",
  "author": "John Doe",
  "description": "A bit better book"}' \
  $ADDRESS/api/v1/books | jq '.'

curl $ADDRESS/api/v1/books | jq '.'
The output of the last command is as follows.
[
  {
    "author": "John Doe",
    "title": "My Second Book",
    "_id": 2
  }
]</pre></div><p>If the service could not communicate with the database located on a different node, we would not be able to put, nor to get data. Networking between containers deployed to separate servers worked! All we had to do is use an additional argument with Docker Compose (<span class="emphasis"><em>net</em></span>) and make sure that the service code utilizes information from the hosts file.</p><p>Another advantage of Docker networking is that, if one container stops working, we can redeploy it (potentially to a separate server) and, assuming that the service can handle the temporary connection loss, continue using it as if nothing happened.</p></div><div class="section" title="Scaling Services with Docker Swarm"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec81"/>Scaling Services with Docker Swarm</h2></div></div></div><p>As you've<a class="indexterm" id="id657"/> already seen, scaling with Docker Compose is <a class="indexterm" id="id658"/>easy. While examples we run by now were limited to a single server, with Docker Swarm we can extend scaling to the whole cluster. Now that we have one instance of <code class="literal">books-ms</code> running, we can scale it to, let's say, three:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose -f docker-compose-swarm.yml \</strong></span>
<span class="strong"><strong>    scale app=3</strong></span>

<span class="strong"><strong>docker ps --filter name=books \</strong></span>
<span class="strong"><strong>    --format "table {{.Names}}"</strong></span>
</pre></div><p>The output of the <code class="literal">ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app_3</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app_2</strong></span>
<span class="strong"><strong>swarm-node-2/books-ms-db</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app_1</strong></span>
</pre></div><p>We can see that <a class="indexterm" id="id659"/>Swarm continues distributing containers<a class="indexterm" id="id660"/> evenly. Each node is currently running two containers. Since we asked Docker Swarm to scale the <code class="literal">books-ms</code> containers to three, two of them are now running alone and the third one is deployed together with the database. Later on, when we start working on the automation of the deployment to the Docker Swarm cluster, we'll also make sure that all the instances of the service are properly set in the proxy.</p><p>For the future reference, we might want to store the number of instances in Consul. Later on, it might come in handy if, for example, we want to increase or decrease that number:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -X PUT -d 3 \</strong></span>
<span class="strong"><strong>    10.100.192.200:8500/v1/kv/books-ms/instances</strong></span>
<span class="strong"><strong>Services can be as easily descaled. For example, the traffic might drop, later during the day, and we might want to free resources for other services.</strong></span>
<span class="strong"><strong>docker-compose -f docker-compose-swarm.yml \</strong></span>
<span class="strong"><strong>    scale app=1</strong></span>

<span class="strong"><strong>curl -X PUT -d 1 \</strong></span>
<span class="strong"><strong>    10.100.192.200:8500/v1/kv/books-ms/instances</strong></span>

<span class="strong"><strong>docker ps --filter name=books \</strong></span>
<span class="strong"><strong>    --format "table {{.Names}}"</strong></span>
</pre></div><p>Since we told Swarm to scale (down) to one instance and, at that moment, there were three of them running, Swarm removed instances two and three leaving the system with only one running. That can be observed from the output of the <code class="literal">docker ps</code> command that is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/books-ms-db</strong></span>
<span class="strong"><strong>swarm-node-1/booksms_app_1</strong></span>
</pre></div><p>We descaled and went back to the beginning, with one instance of each target running.</p><p>We are about to<a class="indexterm" id="id661"/> explore few more Swarm options. Before <a class="indexterm" id="id662"/>we proceed, let us stop and remove running containers, and start over:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose stop</strong></span>

<span class="strong"><strong>docker-compose rm -f</strong></span>
</pre></div></div><div class="section" title="Scheduling Containers Depending on Reserved CPUs and Memory"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec82"/>Scheduling Containers Depending on Reserved CPUs and Memory</h2></div></div></div><p>Up until<a class="indexterm" id="id663"/> now, Swarm was scheduling<a class="indexterm" id="id664"/> deployments to servers that have the least number of them running. That is the default strategy applied when there is no other constraint specified. It is often not realistic to expect that all containers require equal access to resources. We can further refine Swarm deployments by giving hints of what we expect from containers. For example, we can specify how many CPUs we need for a particular container. Let's give it a spin.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker info</strong></span>
</pre></div><p>The relevant parts of the output of the command are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Nodes: 2</strong></span>
<span class="strong"><strong> swarm-node-1: 10.100.192.201:2375</strong></span>
<span class="strong"><strong>  └ Containers: 2</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong> swarm-node-2: 10.100.192.202:2375</strong></span>
<span class="strong"><strong>  └ Containers: 2</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Even though we are already running two containers on each node (<code class="literal">Registrator</code> and <code class="literal">Swarm</code>), there are no reserved CPUs, nor reserved memory. When we run those containers, we did not specify that CPU or memory should be reserved.</p><p>Let's try running Mongo DB with one CPU reserved for the process. Keep in mind that this is only a hint and will not prevent other containers already deployed on those servers from using that CPU.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --cpu-shares 1 --name db1 mongo</strong></span>

<span class="strong"><strong>docker info</strong></span>
</pre></div><p>Since each node has only one CPU assigned, we could not assign more than one. The relevant parts of the output of the <code class="literal">docker info</code> command are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Nodes: 2</strong></span>
<span class="strong"><strong> swarm-node-1: 10.100.192.201:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 1 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong> swarm-node-2: 10.100.192.202:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 2</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>This time, <code class="literal">swarm-node-1</code> has one (out of one) CPU reserved. Since there are no more available <a class="indexterm" id="id665"/>CPUs on that node, if we<a class="indexterm" id="id666"/> repeat the process and bring up one more Mongo DB with the same constraint, Swarm will have no option but to deploy it to the second node. Let's try it out:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --cpu-shares 1 --name db2 mongo</strong></span>

<span class="strong"><strong>docker info</strong></span>
</pre></div><p>The relevant parts of the output of the <code class="literal">ps</code> command are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Nodes: 2</strong></span>
<span class="strong"><strong> swarm-node-1: 10.100.192.201:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 1 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong> swarm-node-2: 10.100.192.202:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 1 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>This time, both nodes have all the CPUs reserved.</p><p>We can take a look at the processes and confirm that both DBs are indeed running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=db --format "table {{.Names}}"</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/db2</strong></span>
<span class="strong"><strong>swarm-node-1/db1</strong></span>
</pre></div><p>Indeed, both containers are running, one on each node.</p><p>Let's see <a class="indexterm" id="id667"/>what happens if we try to bring <a class="indexterm" id="id668"/>up one more container that requires one CPU:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --cpu-shares 1 --name db3 mongo</strong></span>
</pre></div><p>This time, Swarm returned the following error message:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Error response from daemon: no resources available to schedule container</strong></span>
</pre></div><p>We requested deployment of a container that requires one CPU, and Swarm got back to us saying that there are no available nodes that fulfill that requirement. Before we proceed to explore other constraints, please bear in mind that <span class="emphasis"><em>CPU Shares</em></span> do not work in the same way with Swarm as when applied to a Docker running on a single server. For more information <a class="indexterm" id="id669"/>regarding such a case, please consult <a class="ulink" href="https://docs.docker.com/engine/reference/run/#cpu-share-constraint">https://docs.docker.com/engine/reference/run/#cpu-share-constraint</a> page for more information.</p><p>Let's remove our containers and start over:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker rm -f db1 db2</strong></span>
</pre></div><p>We can also use memory as a constraint. For example, we can direct Swarm to deploy a container reserving one CPU and one GB of memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --cpu-shares 1 -m 1g --name db1 mongo</strong></span>

<span class="strong"><strong>docker info</strong></span>
</pre></div><p>The output of the <code class="literal">docker info</code> command is as follows (limited to relevant parts):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Nodes: 2</strong></span>
<span class="strong"><strong> swarm-node-1: 10.100.192.201:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 1 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 1 GiB / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong> swarm-node-2: 10.100.192.202:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 2</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 0 B / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>This time not only that one CPU is reserved, but almost all of the memory as well. While we could not demonstrate much when using CPU constraints, since our nodes have only one each, with <a class="indexterm" id="id670"/>memory we have a bit bigger<a class="indexterm" id="id671"/> margin to experiment. For example, we can bring up three instances of Mongo DB with 100 MB reserved for each:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d -m 100m --name db2 mongo</strong></span>

<span class="strong"><strong>docker run -d -m 100m --name db3 mongo</strong></span>

<span class="strong"><strong>docker run -d -m 100m --name db4 mongo</strong></span>

<span class="strong"><strong>docker info</strong></span>
</pre></div><p>The output of the <code class="literal">docker info</code> command is as follows (limited to relevant parts):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Nodes: 2</strong></span>
<span class="strong"><strong> swarm-node-1: 10.100.192.201:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 3</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 1 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 1 GiB / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong> swarm-node-2: 10.100.192.202:2375</strong></span>
<span class="strong"><strong>  └ Status: Healthy</strong></span>
<span class="strong"><strong>  └ Containers: 5</strong></span>
<span class="strong"><strong>  └ Reserved CPUs: 0 / 1</strong></span>
<span class="strong"><strong>  └ Reserved Memory: 300 MiB / 1.535 GiB</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>It is obvious that all of those three containers were deployed to the <code class="literal">swarm-node-2</code>. Swarm realized that the second node had less available memory on the <code class="literal">swarm-node-1</code> and decided to deploy the new container to the <code class="literal">swarm-node-2</code>. That decision was repeated two more times since the same constraints were used. As a result, the <code class="literal">swarm-node-2</code> now has all those three containers running and 300 MB of memory reserved. We can confirm that by checking the running processes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps --filter name=db --format "table {{.Names}}"</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/db4</strong></span>
<span class="strong"><strong>swarm-node-2/db3</strong></span>
<span class="strong"><strong>swarm-node-2/db2</strong></span>
<span class="strong"><strong>swarm-node-1/db1</strong></span>
</pre></div><p>There are many<a class="indexterm" id="id672"/> other ways we can give hints to <a class="indexterm" id="id673"/>Swarm where to deploy containers. We won't explore all of them. I invite you to check Docker documentation<a class="indexterm" id="id674"/> for <a class="indexterm" id="id675"/>Strategies (<a class="ulink" href="https://docs.docker.com/swarm/scheduler/strategy/">https://docs.docker.com/swarm/scheduler/strategy/</a>)and Filters (<a class="ulink" href="https://docs.docker.com/swarm/scheduler/filter/">https://docs.docker.com/swarm/scheduler/filter/</a>).</p><p>At this moment, we have more than enough knowledge to attempt deployment automation to the Docker Swarm cluster.</p><p>Before we proceed, let's remove the containers we run until now:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker rm -f db1 db2 db3 db4</strong></span>
</pre></div></div></div>
<div class="section" title="Automating Deployment with Docker Swarm and Ansible"><div class="titlepage"><div><div><h1 class="title"><a id="ch14lvl1sec37"/>Automating Deployment with Docker Swarm and Ansible</h1></div></div></div><p>We are <a class="indexterm" id="id676"/>already familiar with Jenkins Workflow, and it<a class="indexterm" id="id677"/> should be relatively easy to extend this knowledge to Docker Swarm deployments.</p><p>First things first. We need to provision our <code class="literal">cd</code> node with Jenkins:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook /vagrant/ansible/jenkins-node-swarm.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>

<span class="strong"><strong>ansible-playbook /vagrant/ansible/jenkins.yml \</strong></span>
<span class="strong"><strong>    -c local</strong></span>
</pre></div><p>The two playbooks deployed the familiar Jenkins instance with two nodes. This time, the slaves we are running are <code class="literal">cd</code> and <code class="literal">swarm-master</code>. Among other jobs, the playbook created the <code class="literal">books-ms-swarm</code> job based on the <code class="literal">Multibranch Workflow</code>. The only difference between <a class="indexterm" id="id678"/>this and the other multibranch jobs <a class="indexterm" id="id679"/>we used earlier is in the <code class="literal">Include branches</code> filter that, this time, is set to <code class="literal">swarm</code>:</p><div class="mediaobject"><img alt="Automating Deployment with Docker Swarm and Ansible" src="graphics/B05848_14_13.jpg"/><div class="caption"><p>Figure 14-13 – Configuration screen of the books-ms-swarm Jenkins job</p></div></div><p>Let's index the branches and let the job run while we explore the Jenkinsfile located in the <code class="literal">books-ms swarm</code> branch.</p><p>Please open the <code class="literal">books-ms-swarm</code> job and click <span class="strong"><strong>Branch Indexing</strong></span> followed by <span class="strong"><strong>Run Now</strong></span>. Since there is only one branch matching the specified filter, Jenkins will create one subproject called <code class="literal">swarm</code> and start building it. If you are curious about the progress of the build, you can monitor the progress by opening the build console.</p><div class="section" title="Examining the Swarm Deployment Playbook"><div class="titlepage"><div><div><h2 class="title"><a id="ch14lvl2sec83"/>Examining the Swarm Deployment Playbook</h2></div></div></div><p>The content <a class="indexterm" id="id680"/>of the Jenkins workflow defined in the Jenkinsfile is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node("cd") {</strong></span>
<span class="strong"><strong>    def serviceName = "books-ms"</strong></span>
<span class="strong"><strong>    def prodIp = "10.100.192.200" // Modified</strong></span>
<span class="strong"><strong>    def proxyIp = "10.100.192.200" // Modified</strong></span>
<span class="strong"><strong>    def proxyNode = "swarm-master"</strong></span>
<span class="strong"><strong>    def registryIpPort = "10.100.198.200:5000"</strong></span>
<span class="strong"><strong>    def swarmPlaybook = "swarm.yml" // Modified</strong></span>
<span class="strong"><strong>    def proxyPlaybook = "swarm-proxy.yml" // Added</strong></span>
<span class="strong"><strong>    def instances = 1 // Added</strong></span>

<span class="strong"><strong>    def flow = load "/data/scripts/workflow-util.groovy"</strong></span>

<span class="strong"><strong>    git url: "https://github.com/vfarcic/${serviceName}.git"</strong></span>
<span class="strong"><strong>    flow.provision(swarmPlaybook) // Modified</strong></span>
<span class="strong"><strong>    flow.provision(proxyPlaybook) // Added</strong></span>
<span class="strong"><strong>    flow.buildTests(serviceName, registryIpPort)</strong></span>
<span class="strong"><strong>    flow.runTests(serviceName, "tests", "")</strong></span>
<span class="strong"><strong>    flow.buildService(serviceName, registryIpPort)</strong></span>

<span class="strong"><strong>    def currentColor = flow.getCurrentColor(serviceName, prodIp)</strong></span>
<span class="strong"><strong>    def nextColor = flow.getNextColor(currentColor)</strong></span>

<span class="strong"><strong>    flow.deploySwarm(serviceName, prodIp, nextColor, instances) // Modified</strong></span>
<span class="strong"><strong>    flow.runBGPreIntegrationTests(serviceName, prodIp, nextColor)</strong></span>
<span class="strong"><strong>    flow.updateBGProxy(serviceName, proxyNode, nextColor)</strong></span>
<span class="strong"><strong>    flow.runBGPostIntegrationTests(serviceName, prodIp, proxyIp, proxyNode, currentColor, nextColor)</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>I added <a class="indexterm" id="id681"/>comments to the modified and added lines (when compared with Jenkinsfile from the previous chapter) so that we can explore the differences from the Jenkinsfile defined in the blue-green branch.</p><p>The variables <code class="literal">prodIp</code> and <code class="literal">proxyIp</code> have been changed to point to the <code class="literal">swarm-master</code> node. This time, we are using two Ansible playbooks to provision the cluster. The <code class="literal">swarmPlaybook</code> variable holds the name of the playbook that configures the whole <code class="literal">Swarm</code> cluster while the <code class="literal">proxyPlaybook</code> variable references the playbook in charge of setting up the <code class="literal">nginx</code> proxy on the <code class="literal">swarm-master</code> node. In real world situations, Swarm master and the proxy service should be separated but, in this case, I opted against an additional VM to save a bit of resources on your laptop. Finally, the <code class="literal">instances</code> variable with the default value of <code class="literal">1</code> is added to the script. We'll explore its usage shortly.</p><p>The only truly notable difference is the usage of the <code class="literal">deploySwarm</code> function that replaces <code class="literal">deployBG</code>. It is one more utility function defined in the <code class="literal">workflow-util.groovy</code> script. Its contents are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def deploySwarm(serviceName, swarmIp, color, instances) {</strong></span>
<span class="strong"><strong>    stage "Deploy"</strong></span>
<span class="strong"><strong>    withEnv(["DOCKER_HOST=tcp://${swarmIp}:2375"]) {</strong></span>
<span class="strong"><strong>        sh "docker-compose pull app-${color}"</strong></span>
<span class="strong"><strong>        try {</strong></span>
<span class="strong"><strong>            sh "docker network create ${serviceName}"</strong></span>
<span class="strong"><strong>        } catch (e) {}</strong></span>
<span class="strong"><strong>        sh "docker-compose -f docker-compose-swarm.yml \</strong></span>
<span class="strong"><strong>            -p ${serviceName} up -d db"</strong></span>
<span class="strong"><strong>        sh "docker-compose -f docker-compose-swarm.yml \</strong></span>
<span class="strong"><strong>            -p ${serviceName} rm -f app-${color}"</strong></span>
<span class="strong"><strong>        sh "docker-compose -f docker-compose-swarm.yml \</strong></span>
<span class="strong"><strong>            -p ${serviceName} scale app-${color}=${instances}"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>    putInstances(serviceName, swarmIp, instances)</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>As before, we start by pulling the latest container from the registry. The new addition is the creation <a class="indexterm" id="id682"/>of a Docker network. Since it can be created only once, and all subsequent attempts will result in an error, the <code class="literal">sh</code> command is enclosed inside a <code class="literal">try/catch</code> block that will prevent the script from failing.</p><p>The creation of the network is followed by deployment of the <code class="literal">db</code> and <code class="literal">app</code> targets. Unlike DB that, in this scenario, is always deployed as a single instance, the <code class="literal">app</code> target might need to be scaled. For that reason, the first one is deployed through the <code class="literal">up</code> and the other through the <code class="literal">scale</code> command available through Docker Compose. The <code class="literal">scale</code> command utilizes the <code class="literal">instances</code> variable to determine how many copies of the release should be deployed. We can increase or decrease their number simply by changing the <code class="literal">instances</code> variable in the Jenkinsfile. Once such a change is committed to the repository, Jenkins will run a new build and deploy as many instances as we specified.</p><p>Finally, we are putting the number of instances to Consul by invoking the helper function <code class="literal">putInstances</code> which, in turn. executed a simple Shell command. Even though we won't be using the information right now, it will come in handy in the next chapter when we start building a self-healing system.</p><p>That's it. There were only a few changes we had to apply to the Jenkinsfile to have the <code class="literal">blue-green</code> deployment extended from a single server to the whole Swarm cluster. Both Docker Swarm and Jenkins Workflow proved to be very easy to work with, even easier to maintain, and, yet, very powerful.</p><p>By this time, the build of the <code class="literal">swarm</code> sub-project probably finished. We can validate that from the build console screen or, directly, by opening the <code class="literal">books-ms-swarm</code> job and confirming that the status of the last build is represented with the <code class="literal">blue</code> ball. If you are curious why the success is represented with blue instead of green color, please read the <span class="emphasis"><em>Why does Jenkins have blue balls?</em></span> article at <a class="ulink" href="https://jenkins.io/blog/2012/03/13/why-does-jenkins-have-blue-balls/">https://jenkins.io/blog/2012/03/13/why-does-jenkins-have-blue-balls/</a>:</p><div class="mediaobject"><img alt="Examining the Swarm Deployment Playbook" src="graphics/B05848_14_14.jpg"/><div class="caption"><p>Figure 14-14 – The books-ms-swarm Jenkins job screen</p></div></div><p>Now that <a class="indexterm" id="id683"/>we understand what is behind the <span class="emphasis"><em>Jenkinsfile</em></span> script and the build is finished, we can manually validate that everything seems to be working correctly.</p><div class="section" title="Running the Swarm Jenkins Workflow"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec36"/>Running the Swarm Jenkins Workflow</h3></div></div></div><p>The first run<a class="indexterm" id="id684"/> of the swarm subproject was initiated by Jenkins automatically once it finished indexing branches. All that's left for us is to double check that the whole process was indeed executed correctly.</p><p>This was the first deployment so the blue release should be running somewhere inside the cluster. Let's take a look where did Swarm decide to deploy our containers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://10.100.192.200:2375</strong></span>

<span class="strong"><strong>docker ps --filter name=books --format "table {{.Names}}"</strong></span>
</pre></div><p>The output of the ps command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app-blue_1</strong></span>
<span class="strong"><strong>swarm-node-1/books-ms-db</strong></span>
</pre></div><p>In this case, Swarm deployed the <code class="literal">books-ms</code> container to the <code class="literal">swarm-node-2</code> and the Mongo DB to the <code class="literal">swarm-node-1</code>. We can also verify whether the service was correctly stored in Consul:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl swarm-master:8500/v1/catalog/service/books-ms-blue \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>

<span class="strong"><strong>curl swarm-master:8500/v1/kv/books-ms/color?raw</strong></span>

<span class="strong"><strong>curl swarm-master:8500/v1/kv/books-ms/instances?raw</strong></span>
</pre></div><p>The output of all three commands is as follows:</p><div class="informalexample"><pre class="programlisting">[
  {
    "ServicePort": 32768,
    "ServiceAddress": "10.100.192.202",
    "ServiceTags": null,
    "ServiceName": "books-ms-blue",
    "ServiceID": "swarm-node-2:booksms_app-blue_1:8080",
    "Address": "10.100.192.202",
    "Node": "swarm-node-2"
  }
]
...
blue
...
1</pre></div><p>According<a class="indexterm" id="id685"/> to Consul, the release was deployed to <code class="literal">swarm-node-2</code> (<code class="literal">10.100.192.202</code>) and has the port <code class="literal">32768</code>. We are currently running the <code class="literal">blue</code> release, and have only one instance running.</p><p>Finally, we can double check that the service is indeed working by sending a few requests to it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -H 'Content-Type: application/json' -X PUT -d \</strong></span>
<span class="strong"><strong>  '{"_id": 1,</strong></span>
<span class="strong"><strong>  "title": "My First Book",</strong></span>
<span class="strong"><strong>  "author": "John Doe",</strong></span>
<span class="strong"><strong>  "description": "Not a very good book"}' \</strong></span>
<span class="strong"><strong>  swarm-master/api/v1/books | jq '.'</strong></span>

<span class="strong"><strong>curl swarm-master/api/v1/books | jq '.'</strong></span>
</pre></div><p>The first request was PUT, sending a signal to the service that we want to store the book. The second retrieved the list of all books.</p><p>The automated process seems to be working correctly when run for the first time. We'll execute the build again and deploy the green release.</p></div><div class="section" title="The Second Run of the Swarm Deployment Playbook"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec37"/>The Second Run of the Swarm Deployment Playbook</h3></div></div></div><p>Let's <a class="indexterm" id="id686"/>deploy the next release.</p><p>Please open the swarm subproject and click the Build Now link. The build will start, and we can monitor it from the Console screen. After a few minutes, the build will finish executing, and we'll be able to check the result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps -a --filter name=books --format "table {{.Names}}\t{{.Status}}"</strong></span>
</pre></div><p>The output of the ps command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES                              STATUS</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app-green_1   Up 7 minutes</strong></span>
<span class="strong"><strong>swarm-node-2/booksms_app-blue_1    Exited (137) 15 seconds ago</strong></span>
<span class="strong"><strong>swarm-node-1/books-ms-db           Up 10 hours</strong></span>
</pre></div><p>Since we <a class="indexterm" id="id687"/>run the <code class="literal">green</code> release, the <code class="literal">blue</code> release is in the <code class="literal">Exited</code> status. We can observe the information about the currently running release from Consul:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl swarm-master:8500/v1/catalog/service/books-ms-green \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>The response from the Consul request is as follows:</p><div class="informalexample"><pre class="programlisting">[
  {
    "ModifyIndex": 3314,
    "CreateIndex": 3314,
    "Node": "swarm-node-2",
    "Address": "10.100.192.202",
    "ServiceID": "swarm-node-2:booksms_app-green_1:8080",
    "ServiceName": "books-ms-green",
    "ServiceTags": [],
    "ServiceAddress": "10.100.192.202",
    "ServicePort": 32770,
    "ServiceEnableTagOverride": false
  }
]</pre></div><p>Now we can test the service itself:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl swarm-master/api/v1/books | jq '.'</strong></span>
</pre></div><p>Since we already have the Consul UI running, please open the <code class="literal">http://10.100.192.200:8500/ui</code> address in your favorite browser to get a visual representation of services we deployed.</p><p>As an exercise, fork the <code class="literal">books-ms</code> repository and modify the job to use you repository. Open the <span class="emphasis"><em>Jenkinsfile</em></span> inside the <code class="literal">swarm</code> branch, change it to deploy three instances of the service, and push the changes. Run the build again and, once it's finished, confirm that three instances were deployed to the cluster.</p></div><div class="section" title="Cleaning Up"><div class="titlepage"><div><div><h3 class="title"><a id="ch14lvl3sec38"/>Cleaning Up</h3></div></div></div><p>This concludes<a class="indexterm" id="id688"/> our tour of Docker Swarm. We'll use it more throughout the next chapters. Before moving to the next subject, lets destroy the VMs. We'll create them again when we need them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>

<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div><p>The solution we developed still has quite a few problems. The system is not fault tolerant, and is difficult to monitor. The next chapter will address the first of those problems through creation of a self-healing system.</p></div></div></div></body></html>