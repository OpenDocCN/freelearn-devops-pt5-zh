- en: Building and Deploying a Multi-Container Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, throughout the course of this book, we have explored the many facets
    of Ansible Container and containerized application deployments. We have looked
    at building Docker containers from basic Dockerfiles, using Ansible Container
    to install roles, build containers, and even deploy applications to cloud solutions
    such as Kubernetes and OpenShift. However, you may have noticed that our discussion
    so far has been centered around deploying single microservice applications such
    as Apache2, Memcached, NGINX, and MariaDB. These applications can be deployed
    standalone, without any dependency on other services or applications aside from
    a basic Docker daemon. While learning containerization from building single-container
    microservices is a great way to learn the core concepts of containerization, it
    isn't an accurate reflection of real-world application infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: As you may already know, applications usually comprise stacks of interconnected
    software that work together to deliver a service to end users. A typical application
    stack might involve a web frontend that receives input from a user. The web interface
    might be responsible for knowing how to contact a database backend to store the
    data provided to it by the user, as well as retrieve previously stored data. Big
    data applications might periodically analyze the data within the database in an
    attempt to figure out trends in data, analyze usage, or perform other functions
    that give data scientists insight into how users are operating the application.
    These applications live in a delicate balance that's dependent on network connectivity,
    DNS resolution, and service discovery in order to talk to each other and perform
    their overarching functions.
  prefs: []
  type: TYPE_NORMAL
- en: The world of containers is not very different at the outset. After all, the
    containerized software still draws dependencies on other containerized and non-containerized
    applications to store, retrieve, and process data, and perform distinct functions.
    As we touched on in [Chapter 5](ccc07e61-25e7-4984-953b-586b28b12aab.xhtml), *Containers
    at Scale with Kubernetes,* and [Chapter 6](d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml),
    *Managing Containers with OpenShift*, containers bring a lot more versatility
    and much reduced management complexity to the problem of deploying and scaling
    multi-tiered applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining complex applications using Docker networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Ansible Container django-gulp-nginx project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the django-gulp-nginx project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development and production configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the project to OpenShift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining complex applications using Docker networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containerized environments are dynamic and apt to change state quickly. Unlike
    traditional infrastructure, containers are continually scaling up and down, perhaps
    even migrating between hosts. It is critical that containers are able to discover
    other containers, establish network connectivity, and share resources quickly
    and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: As we touched on in previous chapters, Docker, Kubernetes, and OpenShift have
    the native functionality to automatically discover and access other containers
    using various networking protocols and DNS resolution, not unlike bare-metal or
    virtualized servers. When deploying containers on a single Docker host, Docker
    will assign each container an IP address in a virtual subnet that can be used
    to talk to other container IP addresses in the same subnet. Likewise, Docker will
    also provide simple DNS resolution that can be used to resolve container names
    internally. When scaled out across multiple hosts using container orchestration
    systems such as Kubernetes, OpenShift, or Docker Swarm, containers use an overlay
    network to establish network connectivity between hosts and run as though they
    exist on the same host. As we saw in [Chapter 5](ccc07e61-25e7-4984-953b-586b28b12aab.xhtml),
    *Containers at Scale with Kubernetes*, Kubernetes provides a sophisticated internal
    DNS system to resolve containers based on namespaces within the larger Kubernetes
    DNS domain. There is a lot to be said about container networking, so for the purposes
    of this chapter, we will look at Docker networking for service discovery. In this
    section, we will create a dedicated Docker network subnet and create containers
    that leverage DNS to establish network connectivity to other running containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate basic network connectivity between Docker containers, let''s
    use the Docker environment in our Vagrant lab host to create a new virtual container
    network using the bridge networking driver. Bridge networking is one of the most
    basic types of container networks that is limited to a single Docker host. We
    can create this using the `docker network create` command. In this example, we
    will create a network called `SkyNet` using the `172.100.0.0/16` CIDR block, with
    the `bridge` networking driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can validate this network has been successfully created using the `docker
    network ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see detailed information about this network in JSON format using the
    `docker network inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have established a network on our Docker host, we can create containers
    to connect to this network to test the functionality. Let''s create two Alpine
    Linux containers to connect to this network and use them to test DNS resolution
    and reachability. The Alpine Linux Docker image is an extremely lightweight container
    image that can be used to quickly spin up containers for testing purposes. In
    this example, we will create two Alpine Linux containers named `service1` and
    `service2`, connected to the SkyNet Docker network using `--network` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In a similar way, we can start the `service2` container, using the `SkyNet`
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Although these containers are not running a service, they are running by allocating
    a pseudo-tty instance to them using the `-t` flag. Allocating a pseudo-tty to
    the container will keep it from immediately exiting, but will cause the container
    to exit if the TTY session is terminated. Throughout this book, we have looked
    at running containers using command and entrypoint arguments, which is the recommended
    approach. Running containers by allocating a pseudo-tty is great for quickly spinning
    up containers for testing purposes, but not a recommended way to run traditional
    application containers.  Application containers should always run based on the
    status of the **process ID** (**PID**) running within it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example, we can see that our local Docker host pulled down the
    latest Alpine container image and ran it using the parameters we passed into the
    `docker run` command. Likewise, the second `docker run` command created a second
    instance of this container image using the same parameters. Using the `docker
    inspect` command, we can see which IP addresses the Docker daemon assigned our
    containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can do the same for `service2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, Docker assigned the IP address of `172.100.0.2` to our `service1`
    container, and the IP address of `172.100.0.3` to our `service2` container. These
    IP addresses provide network connectivity exactly as  you would expect between
    two hosts attached to the same network segment. If we use `docker exec` to log
    into the `service1` container, we can check to see whether `service1` can ping
    `service2` using the IP addresses Docker assigned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since these containers are running using a pseudo-tty instead of a command
    or entrypoint, simply typing `exit` in the container shell will kill the TTY session
    and stop the container. To keep the container running when exiting the shell,
    use the Docker escape sequence from your keyboard: *Ctrl* + *P* *Ctrl* + *Q*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can as well do this test likewise from the  `service2` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It is easy to see that IP-based networking works well to establish network
    connectivity between running containers. The downside of this approach is that
    we cannot always know ahead of time what IP addresses the container runtime environment
    will assign our containers. For example, a container may require an entry in a
    configuration file to point to a service it depends on. Although you might be
    tempted to plug an IP address into your container role and build it, this container
    role would have to be rebuilt for each and every environment it is deployed into.
    Furthermore, when containers get stopped and restarted, they could take on different
    IP addresses, which will cause the application to break down. Luckily, as a solution
    to this issue, Docker provides a DNS resolution based on the container name, which
    will actively keep track of running containers and resolve the correct IP address
    in the event that a container should change IP addresses. Container names, unlike
    IP addresses, can be known in advance and be used to point containers to the correct
    services inside of configuration files, or stored in memory as environment variables.
    We can see this in action by logging back into the `service1` container and using
    the `ping` command to ping the name `service2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we can create a third service container and check to see if the
    new container has the ability to resolve the names of `service1` and `service2`
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if we log into the `service2` container, we can use the `nslookup`
    command to resolve the IP address of the newly created `service3` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Docker creates DNS resolution using the name of the Docker network as a domain.
    As such, the `nslookup` results are showing the fully qualified domain name of
    `service3` as `service3.SkyNet`. However, as I'm sure you could imagine, having
    DNS resolution for containers is an incredibly powerful tool for building reliable
    and robust containerized infrastructures. Just by knowing the name of the container,
    you can establish links and dependencies between containers that will scale with
    your infrastructure. This concept extends far beyond learning the individual IP
    addresses of containers. For example, as we saw in [Chapter 5](ccc07e61-25e7-4984-953b-586b28b12aab.xhtml),
    *Containers at Scale with Kubernetes*, and [Chapter 6](d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml), *Managing
    Your Applications with OpenShift*, Kubernetes and OpenShift allow for the creation
    of services that logically connect to backend pods using labels or other identifiers.
    When other pods pass traffic to the service DNS entry, Kubernetes will load-balance
    traffic to the running pods that match the label rules configured in the service
    entry. The only thing the containers that rely on that service need to know is
    how to resolve the service FQDN, and the container orchestrator takes care of
    the rest. The backend pods could scale up or down, but as long as the container
    orchestrator's DNS service is able to resolve the service entry, the other containers
    calling the service will not notice a difference.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Ansible Container django-gulp-nginx project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of container networking concepts and
    Docker DNS resolution, we can build projects that have multi-container dependencies.
    Ansible Container has a concept of creating fully reusable full stack containerized
    applications, aptly named Container Apps. Container Apps are able to be downloaded
    and deployed quickly from Ansible Galaxy very similar to container-enabled roles.
    Container Apps have the benefit of allowing users to get started developing quickly
    against fully functional multi-tier applications that run as separate microservice
    containers. In this example, we will use a community-developed web application
    project that spins up a Python-based Django, Gulp, and NGINX environment we can
    deploy locally and to a container orchestration environment such as OpenShift
    or Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can explore a wide range of container apps using Ansible Galaxy by simply
    going to the Ansible Galaxy website at [https://galaxy.ansible.com](https://galaxy.ansible.com/),
    selecting BROWSE ROLES, clicking on Role Type from the Keyword drop-down box,
    and selecting Container App from the search dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c64ca50-00dd-44b7-81ad-52584fdf6e2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Searching for Container Apps in Ansible Galaxy'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are going to leverage the pre-built Ansible `django-gulp-nginx`
    Container App, which is an official Ansible Container project. This container
    app creates a containerized Django framework web application that leverages NGINX
    as a web server, Django and Gulp as a framework, and PostgreSQL as a database
    server. In this project is an entirely self-contained demo environment we can
    use to explore how Ansible Container works with other services and dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get started with using this project, we need to first install it
    in a clean directory on our Vagrant Lab VM. First, create a new directory (I will
    call mine `demo`), and run the `ansible-container init` command followed by the
    name of the Container App we want to install, `ansible.django-gulp-nginx`. You
    can find the full name for this project on Ansible Galaxy, using the preceding
    steps to search for Container Apps. Following code demonstrates creating a new
    directory and initializing the Django-Gulp-NGINX project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successfully initializing the project, you should see the Ansible Container
    initialized from Galaxy Container App `ansible.django-gulp-nginx` message appear.
    This indicates that the container app was successfully installed from Ansible
    Galaxy. Executing the `ls` command in the `demo/ directory` should display project
    files similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A lot of the files listed are configuration files that support the Gulp/Django
    framework for the application we are going to create. The primary file we are
    concerned with for the purposes of this demonstration is the core file in all
    Ansible Container projects: `container.yml`. If you open the `container.yml` file
    in a text editor, it should resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output shown here is a reflection of the contents of `container.yml` at
    the time of writing. Yours may look slightly different if updates have been made
    to this project since the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this `container.yml` file contains many of the same specifications
    we have covered already in previous chapters of the book. Out of the box, this
    project contains the service declarations to build the Gulp, Django, NGINX, and
    Postgres containers, complete with the role paths and various role variables defined
    to ensure the project is able to run in a completely self-contained format. Also
    built into this project is support for deploying this project to OpenShift. One
    of the benefits of this project is that it exposes virtually every possible configuration
    option available in an Ansible Container project, as well as the proper syntax
    to activate these features. Personally, I like to use this project as a reference
    guide in case I forget the proper syntax to use in my project''s `container.yml`
    files. Following is a list of sections from the `container.yml` that are useful
    for the user to have an understanding of, starting from the top and moving towards
    the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conductor`: As we have seen throughout this book, this section defines the
    conductor container and the base container image to build the conductor from.
    In this case, the conductor image will be a Centos 7 container that leverages
    a volume mount from the `temp-space` directory in the `root` of the project to
    the `/tmp` directory inside of the container. It is important to note here that
    the conductor image can leverage volume mounts in order to store data during the
    build process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defaults`: This section is known as the top-level defaults section and is
    used to instantiate variables that can be used throughout the project. Here, you
    can define variables that can be used in the service section of the project as
    role variable overrides, or simply in place of hardcoding the same values over
    and over again in the `container.yml` file. It is important to note that in the
    order that, Ansible Container evaluates variable precedence, the top-level defaults
    section has the lowest precedence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`services`: In the `services` section, we see entries for the core service
    that will run in this stack (`django`, `gulp`, `nginx`, and `postgresql`). This
    section, for the most part, should be reviewed based on what we have covered in
    previous chapters up until this point. However, you will notice that, in the container
    definition for the `django` container, there is a `link` line that specifies the
    `postgresql` container name. You will notice this as well in the other container
    definitions that list the name of the `django` container. In previous versions
    of Docker, links were a way of establishing networking connectivity and container
    name resolution for individual containers. However, recent versions of Docker
    have deprecated the `link` syntax in favor of the native container name resolution
    built into the Docker networking stack. It is important to note that many projects
    still use links as a way to establish network dependencies and container name
    resolution, but will most likely be removed in future versions of Docker. Container
    orchestration tools such as Kubernetes and OpenShift also ignore the `link` syntax
    since they only use native DNS services to resolve other containers and services.
    Another aspect I would like to draw the readers attention to in the `services`
    section is the `nginx`, `gulp`, and `django` containers have a new sub-section
    titled `dev-overrides`. This section is for specifying container configuration
    that will only be present when building testing containers locally. Usually, developers
    use `dev-overrides` to run containers with verbose debugging output turned on,
    or other similar logging mechanisms are used to troubleshoot potential issues.
    The `dev-override` configuration will be ignored when using the `--production`
    flag when executing `ansible-container run`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`volumes`: The top-level volumes section is used to specify **persistent volume
    claims** (**PVCs**) that continue to exist even if the container is stopped or
    destroyed. This section normally maps volumes that have already been created in
    the container-specific services section of the `container.yml` file to provide
    a more verbose configuration for how the container orchestrator should handle
    the persistent volume claim. In this case, the `postgres-data` volume that has
    been mapped in the PostgreSQL container is given the OpenShift specific configuration
    of `ReadWriteMany` access mode, as well as 3 GB of storage. PVCs are usually required
    for applications dependent on storing and retrieving data, such as databases or
    storage APIs. The overall goal of PVCs is that we do not want to lose data if
    need to redeploy, upgrade, or migrate the container to another host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the django-gulp-nginx project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a firm understanding of some of the more advanced Ansible
    Container syntax that is commonly found in Container Apps, we can apply the knowledge
    we have learned so far of the Ansible Container workflow to build and run the
    container App. Since container apps are full Ansible Container projects complete
    with roles, a `container.yml` file, and other supporting project data, the same
    Ansible Container workflow commands we used previously can be used here with no
    modifications. When you are ready, execute the `ansible-container build` command
    in the `root` directory of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the Container App is building four service containers, it may take a
    little longer than usual for the build process to complete. If you are following
    along, you will see Ansible Container go through each playbook role individually
    as it creates the containers and works to bring them into the desired state described
    in the playbooks. When the build has completed successfully, we can execute `ansible-container
    run` command to start the containers and bring our new web service online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When the run playbooks have finished executing, the service containers should
    be running on the Vagrant VM in developer mode, since the `container.yml` file
    specifies `dev-overrides` for many of the services. It is important to note that
    `ansible-container run` will, by default, run the service containers according
    to any `dev-override` configuration listed in the `container.yml` file. For example,
    one developer override configured is to not run the NGINX container when running
    in developer mode. This is accomplished by setting a developer override option
    for the NGINX container so that it will run `/bin/false` as the initial container
    command, immediately killing it. Executing the `docker ps -a` command will show
    that the `postgresql`, `django`, and `gulp` containers are running, with NGINX
    in a stopped state. Using the developer overrides, NGINX is stopped and `gulp`
    is responsible for serving up the HTML page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the containers have started, the `django-gulp-nginx` Container App will
    be listening on the Vagrant lab VM''s localhost address at port `8080`. We can
    use the `curl` command to test the application and ensure we are able to get the
    default Hello World simple HTML page response the service is designed to provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Development versus production configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, executing the `ansible-container run` command on a project that
    specifies developer-overrides for a given service will run the service with the
    developer overrides active. Often, the developer overrides expose verbose logging
    or debugging options in an application that a developer would not want a general
    end user to be exposed to, not to mention that it can be quite resource-intensive
    to run applications with verbose logging stack tracing running constantly. The
    `ansible-container run` command has the ability to be run with the `--production`
    flag to specify when to run services in a mode that mimics a production-style
    deployment. Using the `--production` flag ignores the `dev_overrides` sections
    in the `container.yml` file and runs the services as explicitly defined in the
    `container.yml` file. Now that we have verified that our web service is able to
    run and function in developer mode, we can try running our service in production
    mode to mimic a full production deployment on our local workstation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to run `ansible-container stop` in order to stop all running
    container instances in developer mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s re-run the `ansible-container run` command, this time providing
    the `--production` flag to indicate that we wish to ignore the developer overrides
    and run this service in production mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now look at the services running, you will notice that the NGINX server
    container is now running and acting as the frontend service for the web traffic
    on port `8080` instead of the Gulp container. Meanwhile, the Gulp container has
    been started with the default command `/bin/false`, which instantly kills the
    container. In this example, we have introduced a production configuration that
    terminates a development HTTP web server, in favor of a production-ready NGINX
    web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can finally test the web service once more to ensure that the service is
    reachable and running on the Vagrant Lab VM on localhost port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Deploying the project to OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at how to run the demo web application locally using
    the production and development configurations provided by the `dev_override` syntax.
    Now that we have an understanding of how the web application functions and leverages
    other services, we can look at how to deploy this application in a production-grade
    container orchestration environment such as OpenShift or Kubernetes. In this section
    of the book, we will deploy this project using the production configuration into
    the local Minishift cluster we created in [Chapter 6](d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml),
    *Managing* *Applications with OpenShift*. Prior to starting this example, make
    sure you have a valid OpenShift credentials file that works with your local cluster,
    in the `/home/ubuntu/.kube/config` directory. If new OpenShift credentials need
    to be created, be sure to turn back to [Chapter 7](ef89f30f-00a9-4f4c-93b9-009474fc3022.xhtml),
    *Deploying Your First Project*, for more details.
  prefs: []
  type: TYPE_NORMAL
- en: In order to ensure our application can be deployed to OpenShift, we need to
    modify the container app's `container.yml` file so it points to our Kubernetes
    configuration file as well as to the Docker Hub registry for pushing our container
    images.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift comes with an integrated container registry you can use to push container
    images to during the `ansible-container deploy` process. However, it requires
    some additional configuration that is beyond the scope of this book. For now,
    it will be sufficient to use the Docker Hub registry as we have throughout this
    book so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `settings` section of the `container.yml` file, we will add a `k8s_auth`
    stanza to point to the Kubernetes configuration file the OC generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in the `registries` section, we will add an entry for the Docker Hub
    container registry, using our user credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have OpenShift and Docker Hub configured in our project, we can
    use the `ansible-container deploy` command with the `--engine openshift` flag
    to generate the OpenShift deployment and push the image artifacts to Docker Hub.
    In order to differentiate the images, we can push them to Docker Hub using the
    `containerapp` tag. Since we are pushing multiple images to Docker Hub, depending
    on your internet connection speed, it may take a few minutes for this process
    to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the deploy process has completed successfully, we can use the `ansible-container
    run` command with the `--engine openshift` flag to launch our application and
    run it in our simulated OpenShift production environment. Don''t forget to specify
    the `--production` flag so that our service gets deployed using the production
    configuration and not the developer overrides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the process has completed successfully, we can log into the OpenShift
    web console to validate the service is running as expected. Unless it''s otherwise
    changed, the Container App was deployed into a new project called `demo`, but
    will be displayed with the name `Ansible Container Demo` in the web interface,
    as per our `container.yml` configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7264f1dd-2d91-435a-8e7b-e1336f13e59b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The Ansible Container Demo project deployed to OpenShift'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the Ansible Container Demo project display name will show you the
    standard OpenShift dashboard demonstrating the running pods according to the production
    configuration. You should see the `django`, `ngnix`, and `postgresql` pods running,
    along with a link to the route created to access the web application in the upper-right
    corner of the console display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c909b54-6e3c-4230-aa07-ec720ef32d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Running pods in the demo project'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test to ensure our application is running by clicking on the `nip.io`
    route created in OpenShift and ensuring the NGINX web server container is reachable.
    Clicking on the link should show the simple `Hello you!` Django application in
    its full glory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c2bedf4-ada0-487c-80ae-5c36d766c946.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The Hello World page as viewed running in OpenShift'
  prefs: []
  type: TYPE_NORMAL
- en: That looks a lot nicer then the `curl` testing we were running in the local
    Vagrant lab, don't you think? Congratulations, you have successfully deployed
    a multi-container application into a simulated production environment!
  prefs: []
  type: TYPE_NORMAL
- en: 'From the OpenShift console, we can validate that the various aspects of our
    deployment are present and functioning as intended. For example, you can click
    on the `Storage` link in the left-hand navigation bar to validate that the PVC
    Postgres data was created and is functional in OpenShift. Clicking on postgres-data
    will show the details of the PVC object, including the allocated storage (3 GiB),
    and the access modes configured in the `container.yml` file, `Read-Write-Many`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88cc7495-f897-4295-8133-daa930c8d392.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: PostgreSQL PVC'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ansible django-gulp-nginx project**: [https://github.com/ansible/django-gulp-nginx/](https://github.com/ansible/django-gulp-nginx/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker networking documentation**: [https://docs.docker.com/engine/userguide/networking/](https://docs.docker.com/engine/userguide/networking/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are nearing the end of our journey with Ansible Container, we have covered
    what is perhaps the final hurdle in our quest to learn about automating containers
    using the Ansible Container project, working with multi-container projects. Due
    to the inherent networking functionality available in almost all container runtime
    environments, such as Docker, Kubernetes, and OpenShift, building streamlined
    microservice software stacks is a breeze. As we have seen throughout this chapter,
    microservice containers can easily be connected with Lego-like efficiency to build
    and deploy robust applications in production.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we have looked at how container runtime environments
    establish dependencies on other containers using the container networking fabric,
    as well as creating link dependencies. We observed how these concepts work together
    to build a rather complex multi-container application using Gulp, Django, NGINX,
    and Postgres containers. We tested this stack in developer mode using `dev_overrides`,
    as well as in production mode according to the project configuration. Finally,
    we deployed this application into our local OpenShift cluster to simulate a real-world
    production deployment, complete with container networking and persistent volume
    claims.
  prefs: []
  type: TYPE_NORMAL
- en: The final chapter of the book will cover ways in which you can expand your knowledge
    of Ansible Container and cover some practical tips on how to go forward in your
    knowledge of Ansible Container, carrying forward the knowledge you have obtained
    so far in this book.
  prefs: []
  type: TYPE_NORMAL
