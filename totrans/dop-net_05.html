<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Orchestrating Load Balancers Using Ansible"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Orchestrating Load Balancers Using Ansible</h1></div></div></div><p>This chapter will focus on some of the popular load balancing solutions that are available today and the approaches that they take to load balancing applications.</p><p>With the emergence of cloud solutions, such as AWS, Microsoft Azure, Google Cloud, and OpenStack, we will look at the impact this has had on load balancing with distributed load and centralized load balancing strategies. This chapter will show practical configuration management processes that can be used to orchestrate load balancers using Ansible to help automate the load balancing needs for applications.</p><p>In this chapter, the following topics will be covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Centralized and distributed load balancers</li><li class="listitem" style="list-style-type: disc">Popular load balancing solutions</li><li class="listitem" style="list-style-type: disc">Load balancing immutable and static servers</li><li class="listitem" style="list-style-type: disc">Using Ansible to orchestrate load balancers</li></ul></div><div class="section" title="Centralized and distributed load balancers"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Centralized and distributed load balancers</h1></div></div></div><p>With the introduction of <a id="id368" class="indexterm"/>microservice architectures allowing development teams to make changes to production applications more frequently, developers no longer just need to release software on a quarterly basis.</p><p>With the move towards <a id="id369" class="indexterm"/>Continuous Delivery and DevOps, applications are <a id="id370" class="indexterm"/>now released weekly, daily, or even hourly with only one or a subset of those microservices being updated and released.</p><p>Organizations have found microservice architectures to be easier to manage and have moved away from building monolith applications. Microservice applications break a larger application into smaller manageable chunks. This allows application features to be released to customers on a more frequent basis, as the business does not have to redeploy the whole product each time they release. This means only a small microservice needs to be redeployed to deploy a feature. As the release process is more frequent and continuous, then it is better understood, normally completely automated, and ultimately load balanced.</p><p>Microservice architectures can also be beneficial for large businesses, which are distributed across many <a id="id371" class="indexterm"/>offices or countries as different teams can own different microservices and release <a id="id372" class="indexterm"/>them independently of one another.</p><p>This, of course, means that development teams need a way of testing dependency management, and the onus is put on adequate testing to make sure that a microservice doesn't break other microservices when it is released.</p><p>As a result, developers need to create mocking and stubbing services, so microservice applications can be effectively tested against multiple software versions without having to deploy the full production estate.</p><p>Creating a microservice architecture is a huge mindset shift for a business but a necessary one to remain competitive. Releasing monolithic applications is often difficult and time-consuming for an organization, and businesses that have quarterly release cycles will eventually lose out to competitors that can release their features in a quicker, more granular way.</p><p>The use of microservice architectures has meant that being able to utilize the same load balancing in test environments as production has become even more important due to how dynamic environments need to be.</p><p>So having test environments load balancing configuration as close to production environments as possible is a must. Configuration management tooling can be used to control the desired state of the load balancer.</p><p>The delegation of responsibilities also needs to be reviewed to support microservice architectures, so control of some of the load balancing provisioning should move to development teams as opposed to being a request to the network team to make it manageable and not to impede development teams. This, of course, is a change in culture that needs sponsorship from senior management to make the required changes to the operational model.</p><p>Load balancing requirements when using microservice applications will evolve as an application is developed or scaled up and down in size, so it is important that these aspects are made available to developers to self-service requests rather than wait on a centralized network team to make load balancing changes.</p><p>As a result of the shift towards microservices architectures, the networking and load balancing landscape has needed to evolve too to support those needs with PaaS solutions being created by many vendors to handle application deployment across hybrid cloud and load balancing.</p><p>Off-the-shelf PaaS solutions are a great option for companies that maybe aren't tech-savvy and are unable to <a id="id373" class="indexterm"/>create their own deployment pipelines using configuration management tooling, such as Chef, Puppet, Ansible, and Salt, to deploy their applications into cloud environments.</p><p>Regardless of the approach to deployment, roll your own or off-the-shelf PaaS. Both microservice and monolith applications still need to be supported when considering public, private, and hybrid clouds.</p><p>As a result, networking and load balancing need to be adaptable to support varied workloads. Although the end goal for an organization is ultimately a microservice architecture, the reality for most companies is having to adopt a hybrid approach catering to centralized and distributed load balancing methods to support both monolithic and cloud native microservices.</p><div class="section" title="Centralized load balancing"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Centralized load balancing</h2></div></div></div><p>Traditionally, load <a id="id374" class="indexterm"/>balancers were installed as external physical appliances with very complex designs and used very expensive equipment. Load balancers would be configured to serve web content with SSL requests terminated on the expensive physical appliances.</p><p>The load balancer would have complex configuration to route requests to applications using context switching, and requests would be served directly to the static backend servers.</p><p>This was optimal for monolith configurations as applications typically were self-contained and followed a three-tier model:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A frontend webserver</li><li class="listitem" style="list-style-type: disc">A business logic layer</li><li class="listitem" style="list-style-type: disc">A database layer</li></ul></div><p>This didn't require a lot of east to west traffic within the network as the traffic was north to south, traversing the frontend, business logic, and database. Networks were designed to minimize the amount of time taken to process the request and serve it back to the end user, and it was always served by the core network each time.</p></div><div class="section" title="Distributed load balancing"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/>Distributed load balancing</h2></div></div></div><p>With the evolution towards <a id="id375" class="indexterm"/>microservice architectures, the way that applications operate has changed somewhat. Applications are less self-contained and need to talk to dependent microservices applications that exist within the same tenant network, or even across multiple tenants.</p><p>This means that east-west traffic within the data center is much higher, and that traffic in the data center doesn't always go through the core network like it once did.</p><p>Clusters of microservices applications are instead instantiated and then load balanced within the tenant network using x86 software load balancing solutions with the endpoint of the microservices <a id="id376" class="indexterm"/>clusters <span class="strong"><strong>Virtual IP</strong></span> (<span class="strong"><strong>VIP</strong></span>) exposed to adjacent microservices that need to utilize it.</p><p>With the growing <a id="id377" class="indexterm"/>popularity of virtual machines, containers, and software-defined overlay networks, this means that software load balancing solutions are now used to load balance applications within the tenant network, as opposed to having to pin back to a centralized load balancing solution.</p><p>As a result load balancing vendors have had to adapt and produce virtualized or containerized versions of their physical appliances to stay competitive with open source software load balancing solutions, which are routinely used with microservices.</p></div></div></div>
<div class="section" title="Popular load balancing solutions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Popular load balancing solutions</h1></div></div></div><p>As applications have moved from monoliths to microservices, load balancing requirements have <a id="id378" class="indexterm"/>undoubtedly changed. Today, we have seen a move towards open source load balancing solutions, which are tightly integrated with virtual machines and containers to serve east to west traffic between VPC in AWS or a tenant network in OpenStack as opposed to pinning out to centralized physical appliances.</p><p>Open source <a id="id379" class="indexterm"/>load balancing solutions are now available from <span class="strong"><strong>Nginx</strong></span> and <span class="strong"><strong>HAProxy</strong></span> to help <a id="id380" class="indexterm"/>developers load balance their applications or AWS elastic <a id="id381" class="indexterm"/>load balancing feature:</p><p>
<a class="ulink" href="https://aws.amazon.com/elasticloadbalancing/">https://aws.amazon.com/elasticloadbalancing/</a>
</p><p>Just a <a id="id382" class="indexterm"/>few years ago, Citrix NetScalers (<a class="ulink" href="https://www.citrix.com/products/netscaler-adc/">https://www.citrix.com/products/netscaler-adc/</a>) and F5 <a id="id383" class="indexterm"/>Big-IP (<a class="ulink" href="https://f5.com/products/big-ip">https://f5.com/products/big-ip</a>) solutions had the monopoly in the enterprise load balancing space, but the load balancing landscape has changed significantly with a multitude of new solutions available.</p><p>New load balancing start-ups such as <a id="id384" class="indexterm"/>Avi networks (<a class="ulink" href="https://avinetworks.com/">https://avinetworks.com/</a>) focus on x86 compute and software solutions to deliver load balancing solutions, which have been created to assist with both modern micros-service applications and monolith applications to support both distributed and centralized load balancing strategies.</p><p>The aim of this book is not about which load balancing vendor solution is the best; there is no <span class="emphasis"><em>one size fits all</em></span> solution, and the load balancing solution chosen will depend on traffic patterns, performance, and portability that is required by an organization.</p><p>This book will not delve into <a id="id385" class="indexterm"/>performance metrics; its goal is to look at the different load balancing strategies that are available today from each vendor and the configuration management methods that could be utilized to fully automate and orchestrate load balancers which will in turn help network teams automate load balancing network operations.</p><div class="section" title="Citrix NetScaler"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Citrix NetScaler</h2></div></div></div><p>
<span class="strong"><strong>Citrix NetScaler</strong></span> provides <a id="id386" class="indexterm"/>a portfolio of products to service an organization's load balancing requirements. Citrix provide various different products to end <a id="id387" class="indexterm"/>users, such as the <span class="strong"><strong>MPX</strong></span>, <span class="strong"><strong>SDX</strong></span>, <span class="strong"><strong>VPX</strong></span>, and <a id="id388" class="indexterm"/>more recently the <span class="strong"><strong>CPX</strong></span> appliances, with flexible license costs available for <a id="id389" class="indexterm"/>each product based on the throughput they support.</p><p>MPX and SDX are the <a id="id390" class="indexterm"/>NetScaler hardware appliances, whereas the VPX is a virtualized NetScaler and the <a id="id391" class="indexterm"/>CPX is a containerized NetScaler.</p><p>All of these products <a id="id392" class="indexterm"/>support differing amounts of throughput based on the license that is purchased (<a class="ulink" href="https://www.citrix.com/products/netscaler-adc/platforms.html">https://www.citrix.com/products/netscaler-adc/platforms.html</a>).</p><p>All of the Citrix NetScaler family of products share the same common set of APIs and code, so the software is completely consistent. NetScaler has a REST API and a Python, Java, and C# Nitro SDK, which exposes all the NetScaler operations that are available in the GUI to the end user. All the NetScaler products allow programmatic control of NetScaler objects and entities that need to be set up to control load balancing or routing on MPX, SDX, VPX, or CPX.</p><p>The NetScaler MPX appliance is a centralized physical load balancing appliance that is used to deal with <a id="id393" class="indexterm"/>a high number of <span class="strong"><strong>Transactions Per Second</strong></span> (<span class="strong"><strong>TPS</strong></span>); MPX has <a id="id394" class="indexterm"/>numerous security features and complies with <span class="strong"><strong>Restriction of Hazardous Substances</strong></span> (<span class="strong"><strong>RoHS</strong></span>) and <span class="strong"><strong>Federal Information Processing Standard</strong></span> (<span class="strong"><strong>FIPS</strong></span>), so the solution <a id="id395" class="indexterm"/>can be used by heavily regulated industries that require businesses to comply with certain regulatory standards.</p><p>MPX is typically used to do SSL offloading; it supports a massive amount of SSL throughput, which can be very useful for very highly performant applications, so the SSL offloading can be done on the hardware appliance.</p><p>MPX can be used to direct traffic to different tenant networks using layer 4 load balancing and layer 7 context switching or alternately direct traffic to a second load balancing tier.</p><p>The NetScaler SDX appliance is also a centralized physical appliance that is used to deal with a high number of TPS. SDX allows multiple VPX appliances to be set up as HA pairs and deployed on SDX to allow increased throughput and resiliency.</p><p>NetScaler <a id="id396" class="indexterm"/>also supports <span class="strong"><strong>Global Server Load Balancing</strong></span> (<span class="strong"><strong>GSLB</strong></span>), which allows load to be distributed across multiple VPX HA pairs in a scale out <a id="id397" class="indexterm"/>model utilizing <span class="strong"><strong>CNAME,</strong></span> which directs traffic across <a id="id398" class="indexterm"/>multiple <a id="id399" class="indexterm"/>HA pairs:</p><div class="mediaobject"><img src="graphics/B05559_05_01.jpg" alt="Citrix NetScaler"/></div><p>The VPX can be installed on any x86 hypervisor and be utilized as a VM appliance, and a new CPX is now available that puts the NetScaler inside a Docker container, so they can be deployed within a tenant network as opposed to being set up in a centralized model. All appliances allow SSL certificates to be assigned and used.</p><p>Every NetScaler appliance, be it MPX, SDX, VPX, or CPX, utilize IP the same object model and code that has the <a id="id400" class="indexterm"/>following prominent entities defined in software to carry <a id="id401" class="indexterm"/>out application load balancing:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Server</strong></span>: A server <a id="id402" class="indexterm"/>entity on NetScaler binds a virtual machine or bare metal server's IP address to the server entity. This means the IP address is a candidate for load balancing once it is bound to other NetScaler entities.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Monitor</strong></span>: The monitor <a id="id403" class="indexterm"/>entity on NetScaler are attached to services or service groups and provide health checks that are used to monitor the health of attached server entities. If the health checks, which could be as simple as a web-ping, are not positive, the service or service group will be marked as down, and NetScaler will not direct traffic to it.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Service group</strong></span>: A service <a id="id404" class="indexterm"/>group is a NetScaler entity used to bind a group of one or more servers to an <code class="literal">lbvserver</code> entity; a service group can have one or more monitors associated with it to health check the associated servers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Service</strong></span>: The service <a id="id405" class="indexterm"/>entity is used to bind one server entity and one or more monitor health checks to an <code class="literal">lbvserver</code> entity, which specifies the protocol and port to check the server on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>lbvserver</strong></span>: An <code class="literal">lbvserver</code> entity <a id="id406" class="indexterm"/>determines the load balancing policy such as round robin or least connection and is connected to a service group entity or multiple service entities and will expose a virtual IP address that can be served to end users to access web applications or a web service endpoints.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>gslbvserver</strong></span>: When DNS <a id="id407" class="indexterm"/>load balancing between NetScaler appliances is required, a <code class="literal">gslbvserver</code> entity is used to specify the <code class="literal">gslb</code> domain name and TTL.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>csvserver</strong></span>: The <code class="literal">csvserver</code> entity <a id="id408" class="indexterm"/>is used to provide layer 7 context switching from a gslbvserver domain or lbvserver IP address to other lbvservers. This is used to route traffic using the NetScaler appliance.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>gslbservice</strong></span>: The <code class="literal">gslbvservice</code> entity binds the <code class="literal">gslbvserver</code> domain to one or more <a id="id409" class="indexterm"/><code class="literal">gslbservers </code>entities to distribute traffic across NetScaler appliances.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>gslbserver</strong></span>: The <code class="literal">gslbserver</code> entities <a id="id410" class="indexterm"/>are is the gslb-enabled IP addresses of the NetScaler appliances.</li></ul></div><p>Simple load <a id="id411" class="indexterm"/>balancing can be done utilizing the server, monitor, service group/service, and lbvserver <a id="id412" class="indexterm"/>combination. With <code class="literal">gslbvserver</code> and <code class="literal">csvserver</code>, context switching allows more complex requirements for complex routing and resiliency.</p></div><div class="section" title="F5 Big-IP"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec59"/>F5 Big-IP</h2></div></div></div><p>The <span class="strong"><strong>F5 Big-IP</strong></span> suite is<a id="id413" class="indexterm"/> based on F5's very own custom TMOS real-time operating <a id="id414" class="indexterm"/>system, which is self-contained and runs on Linux. TMOS has a collection of operating systems and firmware, which all run on BIG-IP hardware appliances or within the BIG-IP virtual instances. BIG-IP and TMOS (and even TMM) can be used interchangeably depending on the use case.</p><p>TMOS is at the heart of every F5 appliance and allows inspection of traffic. It makes forwarding decisions based on the type of traffic acting much in the same way as a firewall would, only allowing predefined protocols to flow through the F5 system.</p><p>TMOS also features iRules, which are programmatic scripts written using F5's very own <span class="strong"><strong>Tool Command Language</strong></span> (<span class="strong"><strong>TCL</strong></span>) that enables users to create unique functions triggered by specific <a id="id415" class="indexterm"/>events. This could be used to content switch traffic or red-order HTTP cookies; TCL is fully extensible and programmable and can carry out numerous operations.</p><p>The F5 Big-IP solution is primarily a hardware load balancing solution, that provides multiple sets of physical hardware boxes that customers can purchase based on their throughput requirements, and the hardware can be clustered together for redundancy.</p><p>The F5 Big-IP suite provides a multitude of products that provide services catering for load balancing, traffic management, and even firewalling.</p><p>The main load balancing services provided by the F5 Big-IP Suite are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Big-IP DNS:</strong></span> F5's global load <a id="id416" class="indexterm"/>balancing solution</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Local traffic manager:</strong></span> The main <a id="id417" class="indexterm"/>load balancing product of the F5 Big-IP suite</li></ul></div><p>The F5 Big-IP solution, like the Citrix NetScaler, implements an object model to allow load balancing to be programmatically defined and virtualized. F5 allows SSL certificates to be associated with entities.</p><p>The following local traffic manager object entities allow F5 Big-IP to load balance applications:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pool members</strong></span>: The pool <a id="id418" class="indexterm"/>member entity is mapped to a virtual or physical server's IP address and can be bound to one or more pools. A pool member can have health monitors associated.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Monitor</strong></span>: The monitor <a id="id419" class="indexterm"/>entity returns the status on specific pool members and acts as a health check.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pool</strong></span>: The pool <a id="id420" class="indexterm"/>entity is a logical grouping of a cluster of pool members that are associated; a pool can have health monitors <a id="id421" class="indexterm"/>associated with it as well as <span class="strong"><strong>Quality of Service</strong></span> (<span class="strong"><strong>QoS</strong></span>).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Virtual servers</strong></span>: The virtual <a id="id422" class="indexterm"/>server entity is associated with a pool or multiple pools, and the virtual server determines the load balancing policy, such as round robin or least connections. The F5 solution also will offer load balancing solutions based on capacity or fastest connection. Layer 7 profiles utilizing iRules can be <a id="id423" class="indexterm"/>configured against a virtual server and is used to expose an IP address to access pool members.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>iRules</strong></span>: iRules utilize the <a id="id424" class="indexterm"/>programmatic TCL, so users can author particular load balancing rules based on events such as context switching to different pools.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rate classes</strong></span>: Rate classes <a id="id425" class="indexterm"/>implement rate shaping, and they are used to control bandwidth consumption on particular load balancing operations to cap throughput.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Traffic classes</strong></span>: Traffic class <a id="id426" class="indexterm"/>entities are used to regulate traffic flow based on particular events.</li></ul></div></div><div class="section" title="Avi Networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec60"/>Avi Networks</h2></div></div></div><p>
<span class="strong"><strong>Avi Networks</strong></span> are a relatively <a id="id427" class="indexterm"/>new start-up but have a very interesting load <a id="id428" class="indexterm"/>balancing product, which truly embraces the software-defined mandate. It is an enterprise software load balancing <a id="id429" class="indexterm"/>solution that comprises the <span class="strong"><strong>Avi Controller</strong></span> that can be deployed on x86 compute. Avi is a pure software solution that deploys distributed Avi service engines into tenant networks and integrates with an AWS VPC and an OpenStack tenant:</p><div class="mediaobject"><img src="graphics/B05559_05_02.jpg" alt="Avi Networks"/></div><p>The Avi Networks solution offers automated provisioning of load balancing services on x86 hypervisors, and it can automatically scale out to meet load balancing needs elastically based on utilization rules that users can configure.</p><p>The Avi Networks solution supports multiple or isolated tenants and has a real-time application monitoring and analytics engine that can work out where latency is occurring on the network and the location's packets are being routed from.</p><p>Avi also supports a rich graphical interface that shows load balancing entities so users have a visual view of load balancing, and it additionally supports anti-DDoS support.</p><p>All commands that are <a id="id430" class="indexterm"/>issued via GUI or API utilize the same REST API calls. The <a id="id431" class="indexterm"/>Avi Networks solution supports a Python and REST API. The Net Networks object model has numerous entities that are used to define load balancing in much the same way as NetScalers and F5:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Health monitor profile</strong></span>: The health <a id="id432" class="indexterm"/>monitor pool profile entity specifies health checks for a pool of servers using health attributes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pool</strong></span>: The pool entity <a id="id433" class="indexterm"/>specifies the IP addresses of virtual or physical servers in the form of a server list and has associated health monitor profiles; it also allows an event to be specified using a data script if a pool goes down. One or more pools are bound to the virtual service entity.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Custom policy</strong></span>: The custom <a id="id434" class="indexterm"/>policy allows users to programmatically specify policies against a virtual service.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>App profile</strong></span>: The app profile <a id="id435" class="indexterm"/>entity allows each application to be modeled with associated http attributes, security, DDoS, caching, compression, and PKI attributes specified as part of the app profile associated with a virtual service.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Analytics profile</strong></span>: The analytics <a id="id436" class="indexterm"/>profile makes use of the Avi analytics engine and captures threat, metrics, health score as well as latency thresholds and failure codes that are mapped to the virtual service entity.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>TCP/UDP profile</strong></span>: The TCP/UDP profile <a id="id437" class="indexterm"/>governs if TCP or UDP is used and any DDoS L3/L4 profiles are set.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SSL profile</strong></span>: The SSL entity <a id="id438" class="indexterm"/>governs SSL ciphers that will be used by a virtual service entity.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>PKI profile</strong></span>: The PKI profile <a id="id439" class="indexterm"/>entity is bound to the virtual service entity and specifies the certificate authority for the virtual service.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Policy set</strong></span>: The policy set <a id="id440" class="indexterm"/>entity allows users to set security teams to set policies against each virtual service governing request and response polices.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Virtual service</strong></span>: The virtual <a id="id441" class="indexterm"/>service entity is the entry point IP address to the load balanced pool of servers and is associated with all profiles to define the application pools load balancing and is bound to the TCP/UDP, app, SSL, SSL cert, policy, and analytics profiles.</li></ul></div></div><div class="section" title="Nginx"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"/>Nginx</h2></div></div></div><p>
<span class="strong"><strong>Nginx</strong></span> (<a class="ulink" href="https://www.nginx.com/">https://www.nginx.com/</a>) supports <a id="id442" class="indexterm"/>both commercial and open source versions. It is an x86 <a id="id443" class="indexterm"/>software load balancing solution. Nginx can be used as both an <a id="id444" class="indexterm"/>HTTP and TCP load balancer supporting HTTP, TCP, and even UDP, and can also support SSL/TLS termination.</p><p>Nginx can be set up for redundancy in a highly available fashion using <span class="emphasis"><em>keepalived</em></span>,<span class="emphasis"><em> </em></span>so if there is an outage on one Nginx load balancer, it will seamlessly fail over to a backup with zero downtime.</p><p>Nginx Plus is the commercial offering and is more fully featured than the open source version, supporting features such as active health checks, session persistence, and caching.</p><p>Load balancing on Nginx is set up by declaring syntax in the <code class="literal">nginx.conf</code> file. It works on the principle of wanting to simplify load balancing configuration. Unlike NetScalers, F5s, and Avi Networks, it does not utilize an object model to define load balancing rules, instead Nginx describes load balanced virtual or physical machines as backend servers using declarative syntax.</p><p>In the following simple example, we see three servers, <code class="literal">10.20.1.2</code>, <code class="literal">10.20.1.3</code>, and <code class="literal">10.20.1.4</code>, all load balanced on port <code class="literal">80</code> using Nginx declarative syntax, and it is served on <code class="literal">http://www.devopsfornetworking.com/devops_for_networking</code>:</p><div class="mediaobject"><img src="graphics/B05559_05_03.jpg" alt="Nginx"/></div><p>By default, Nginx will load balance servers using round-robin load balancing method, but it also supports other load balancing methods.</p><p>The Nginx <code class="literal">least_conn</code> load balancing method forwards to backend servers with the least connections at any particular time, whereas the Nginx <code class="literal">ip_hash</code> method of load balancing means that users can tie the same source address to the same target backend server for the entirety <a id="id445" class="indexterm"/>of a request. </p><p>This is useful as some applications require <a id="id446" class="indexterm"/>that all requests are tied to the same server using sticky sessions while transactions are processed.</p><p>However, the proprietary Nginx Plus version supports an additional load balancing method named <code class="literal">least_time</code>, which calculates the lowest latency of backend servers based on the number of active connections and subsequently forwards requests appropriately based on those calculations.</p><p>The Nginx load balancer uses a weighting system at all times when load balancing; all servers by default have a weight of <code class="literal">1</code>. If a weight other than <code class="literal">1</code> is placed on a server, it will not receive requests unless the other servers on a backend are not available to process requests. This can be useful when throttling specific amounts of traffic to backend servers.</p><p>In the following example, we can see that the backend servers have load balancing method least connection configured. Server <code class="literal">10.20.1.3</code> has a weight of <code class="literal">5</code>, meaning only when <code class="literal">10.20.1.2</code> and <code class="literal">10.20.1.4</code> are maxed out will requests is sent to the <code class="literal">10.20.1.3</code> backend server:</p><div class="mediaobject"><img src="graphics/B05559_05_04.jpg" alt="Nginx"/></div><p>By default, using round-robin load balancing in Nginx won't stop forwarding requests to servers that are not responding, so it utilizes <code class="literal">max_fails</code> and <code class="literal">fail_timeouts</code> for this.</p><p>In the following <a id="id447" class="indexterm"/>example, we can see server <code class="literal">10.20.1.2</code> and <code class="literal">10.20.1.4</code> <a id="id448" class="indexterm"/>have the <code class="literal">max_fail</code> count of <code class="literal">2</code> and a <code class="literal">fail_timeout</code> of <code class="literal">1</code> second; if this is exceeded then Nginx will stop directing traffic to these servers:</p><div class="mediaobject"><img src="graphics/B05559_05_05.jpg" alt="Nginx"/></div></div><div class="section" title="HAProxy"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec62"/>HAProxy</h2></div></div></div><p>
<span class="strong"><strong>HAProxy</strong></span> (<a class="ulink" href="http://www.haproxy.org/">http://www.haproxy.org/</a>) is an open source x86 software load balancer that is session aware and can provide layer 4 load balancing. The HAproxy load balancer can also carry out layer 7 context switching <a id="id449" class="indexterm"/>based on the content of the request as well as SSL/TLS <a id="id450" class="indexterm"/>termination.</p><p>HAProxy is <a id="id451" class="indexterm"/>primarily used for HTTP load balancing and can be set up in a redundant fashion using <code class="literal">keepalived</code> configuration using two apache configurations, so if the master fails, the slave will become the master to make sure there is no interruption in service for end users.</p><p>HAProxy uses declarative configuration files to support load balancing as opposed to an object model that proprietary load balancing solutions, such as NetScaler, F5 and Avi Networks, have adopted.</p><p>The HAProxy configuration file has the following declarative configuration sections to allow load balancing to be set up:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Backend</strong></span>: A backend <a id="id452" class="indexterm"/>declaration can contain one or more servers in it; backend servers are added in the format of a DNS record or an IP address. Multiple backend declarations can be set up on a HAProxy server. The load balancing algorithm can also be selected, such as round robin or least connection.<p>In the following example, we see two backend servers, <code class="literal">10.11.0.1</code> and <code class="literal">10.11.0.2</code>, load balanced using the round-robin algorithm on port <code class="literal">80</code>:</p><div class="mediaobject"><img src="graphics/B05559_05_06.jpg" alt="HAProxy"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Check</strong></span>: Checks avoid <a id="id453" class="indexterm"/>users having to manually remove a server from the backend if for any reason, it becomes unavailable and this mitigates outages. HAProxy's default health always attempts to establish a TCP connection to the server using the default port and IP. HAProxy will automatically disable servers that are <a id="id454" class="indexterm"/>unable to serve requests to avoid outages. Servers <a id="id455" class="indexterm"/>will only be re-enabled when it passes its check. HAProxy will report whole backends as unavailable if all servers on a backend have failed their health checks.<p>A number of different health checks can be put against backend servers by utilizing the option <code class="literal">{health-check}</code> line item; for instance, <code class="literal">tcp-check</code> in the following example can check on the health of port <code class="literal">8080</code> even though port <code class="literal">443</code> is being balanced:</p><div class="mediaobject"><img src="graphics/B05559_05_07.jpg" alt="HAProxy"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Access Control List (ACL)</strong></span>: ACL declarations are used to inspect headers and forward to specific <a id="id456" class="indexterm"/>backend servers based on the headers. An ACL in HAProxy will try to find conditions and trigger actions based on this.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Frontend</strong></span>: The frontend <a id="id457" class="indexterm"/>declaration allows different kinds of traffic to be supported by the HAProxy load balancer.<p>In the following <a id="id458" class="indexterm"/>example, HAProxy will accept http traffic on port <code class="literal">80</code>, with an<a id="id459" class="indexterm"/> ACL matching requests only if the request starts with <code class="literal">/network</code> and it is then forwarded to the <code class="literal">high-perf-backend</code> if the ACL <code class="literal">/web-network</code> is matched:</p><div class="mediaobject"><img src="graphics/B05559_05_08.jpg" alt="HAProxy"/></div></li></ul></div></div></div>
<div class="section" title="Load balancing immutable and static infrastructure"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Load balancing immutable and static infrastructure</h1></div></div></div><p>With the introduction of public and private cloud solutions such as AWS and OpenStack, there has been a shift towards utilizing immutable infrastructure instead of traditional static servers.</p><p>This has raised a point of <a id="id460" class="indexterm"/>contention with <span class="emphasis"><em>pets versus cattle</em></span> or, as Gartner defines it <span class="emphasis"><em>bi-modal</em></span> (<a class="ulink" href="http://www.gartner.com/it-glossary/bimodal/">http://www.gartner.com/it-glossary/bimodal/</a>).</p><p>Gartner has said that two different strategies need to be adopted, one for new microservices, <span class="emphasis"><em>cattle,</em></span> and one for legacy<a id="id461" class="indexterm"/> infrastructure, <span class="emphasis"><em>pets</em></span>. <span class="emphasis"><em>Cattle</em></span> are servers that are killed off once they have served their purpose or have an issue, typically<a id="id462" class="indexterm"/> lasting one release iteration. Alternately, <span class="emphasis"><em>pets</em></span> are servers that will have months or years of uptime and will be patched and cared for by operations staff.</p><p>Gartner defines <span class="emphasis"><em>pets</em></span> as Mode 1 and <span class="emphasis"><em>cattle</em></span> as Mode 2. It is said that a <span class="emphasis"><em>cattle</em></span> approach favors the stateless microservice cloud-native  applications, whereas a <span class="emphasis"><em>pet</em></span>, on the other hand, is any application that is a monolith, or potentially a single appliance or something that contains data, such as a database.</p><p>Immutable infrastructure and solutions such as OpenStack and AWS are said by many to favor only the <span class="emphasis"><em>cattle</em></span>, with monoliths and databases remaining pets still need a platform that caters for long-lived servers.</p><p>Personally, I find the <span class="emphasis"><em>pets</em></span> versus <span class="emphasis"><em>cattle</em></span> debate to be a very lazy argument and somewhat tiresome. Instead of dumping applications into two buckets, applications should be treated as a software delivery problem, which becomes a question of stateless read applications and stateful applications with caching and data. Cloud-native microservice applications still need data and state, so I am puzzled by the distinction.</p><p>However, it is undisputed that the load balancer is <a id="id463" class="indexterm"/>key to immutable infrastructure, as at least one version of the application always needs to be exposed to a customer or other microservices to maintain that applications incur zero downtime and remain operational at all times.</p><div class="section" title="Static and immutable servers"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec63"/>Static and immutable servers</h2></div></div></div><p>Historically, an operations team was used by companies to perform the following operations on servers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Rack and cable</li><li class="listitem" style="list-style-type: disc">Providing firmware updates</li><li class="listitem" style="list-style-type: disc">Configuring the RAID configuration</li><li class="listitem" style="list-style-type: disc">Installing an operating system</li><li class="listitem" style="list-style-type: disc">Patching the operating system</li></ul></div><p>This was <a id="id464" class="indexterm"/>all before making the servers available to developers. Static infrastructure can still exist within a cloud environment; for example, databases are <a id="id465" class="indexterm"/>still typically deployed as static, physical servers, given the volume of data that needs to be persisted on their local disk.</p><p>Static servers mean a set of long-lived servers that typically will contain state.</p><p>Immutable servers, on the other hand, mean that every time a virtual machine is changed, a new virtual machine is deployed, complete with a new operating system and new software released on them, delete please. Immutable infrastructure means no in-place changes to a server's state.</p><p>This moves away from the pain of doing in-place  upgrades and makes sure that snowflake server configurations are a thing of the past, where every server, despite the best intentions, has drifted slightly from its desired state over a period of time.</p><p>How many times when releasing software has a release worked on four out of five machines, and hours or days were wasted debugging why a particular software upgrade wasn't working on a particular server.</p><p>Immutable infrastructure builds servers from a known state promoting the same configuration to quality assurance, integration, performance testing, and production environments.</p><p>Parts of cloud infrastructure can be made completely immutable to reap these benefits. The operating system is one such candidate; rather than doing in-place  patching, a single golden image can be created and patched using automation tooling such as Packer in a fully automated fashion.</p><p>Applications that require a caching layer are more stateful by nature so that cache needs to be available at all times to <a id="id466" class="indexterm"/>serve other applications. These caching applications should be <a id="id467" class="indexterm"/>deployed as clusters, which are load balanced, and rolling updates will be done to make sure one version of the cache data is always available. A new software release of that caching layer should synchronize the cache to the new release before the pervious release is destroyed.</p><p>Data, on the other hand, is always persistent, so can be stored on persistent storage and then mounted by the operating system. When doing an immutable rolling update, the operating system layer can mount the data on either persistent or shared storage as part of the release process.</p><p>It is possible to separate the operating system and the data to make all virtual machines stateless, for instance, <a id="id468" class="indexterm"/>OpenStack Cinder (<a class="ulink" href="https://wiki.openstack.org/wiki/Cinder">https://wiki.openstack.org/wiki/Cinder</a>) can be utilized to store persistent data on volumes that can be attached to virtual machines.</p><p>With all these use cases considered, most applications can be designed to be deployed immutably through proper configuration management, even monoliths, as long as they are not a single point of failure. If any applications are single points of failure, they should be rearchitected as releasing software should never result in downtime. Although applications are stateful, each state can be updated in stages so that an overall immutable infrastructure model can be maintained.</p></div><div class="section" title="Blue/green deployments"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec64"/>Blue/green deployments</h2></div></div></div><p>The <span class="strong"><strong>blue green deployment </strong></span>process is not a new concept. Before cloud solutions came to prominence, <a id="id469" class="indexterm"/>production servers would typically have a set of servers consisting of blue (no live traffic) and green (serving customer traffic) that would be utilized. These are typically known as blue and green servers, which alternated per release.</p><p>The blue green model in simple terms means that when a software upgrade needed to be carried out, the blue servers would be upgraded to the latest software version. Once the upgrade had been completed, the blue servers would become the new green servers with live traffic switched to serve from the newly upgraded servers.</p><p>The switching of live traffic was typically done by switching DNS entries to point at the newly upgraded servers. So once the DNS <span class="strong"><strong>Time To Live</strong></span> (<span class="strong"><strong>TTL</strong></span>) had propagated, end user requests would be served by the newly upgraded servers.</p><p>This means that if there was an issue with a software release, rollback could be achieved by switching back the DNS entries to point at the previous software version.</p><p>A typical blue green deployment process is described here:</p><p>
<span class="strong"><strong>Release 1.1</strong></span> would be <a id="id470" class="indexterm"/>deployed on servers 1, 2, and 3 and served on a load balancer to customers and made Green (live):</p><div class="mediaobject"><img src="graphics/B05559_05_09.jpg" alt="Blue/green deployments"/></div><p>
<span class="strong"><strong>Release 1.2</strong></span> would be deployed on servers 4, 5, and 6 and then be patched to the latest patch <a id="id471" class="indexterm"/>version, upgraded to the latest release and tested. When ready, the operations team would toggle the load<a id="id472" class="indexterm"/> balancer to serve boxes 4, 5, and 6 as the new production release, as shown later, and the previously green (live) deployment would become blue, and vice versa:</p><div class="mediaobject"><img src="graphics/B05559_05_10.jpg" alt="Blue/green deployments"/></div><p>When the operations team came to do the next release, servers 1, 2, and 3 would be patched<a id="id473" class="indexterm"/> to the latest version, upgraded to <span class="strong"><strong>Release 1.3</strong></span> from <span class="strong"><strong>Release 1.1</strong></span>, tested, and when ready, the operations team would direct traffic to the new release using the load balancer, making <span class="strong"><strong>Release 1.2</strong></span> blue and <span class="strong"><strong>Release 1.3</strong></span> green, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05559_05_11.jpg" alt="Blue/green deployments"/></div><p>This was traditionally the procedure of running a blue green deployment using static servers.</p><p>However, when using an<a id="id474" class="indexterm"/> immutable model, instead of using long-lived static servers, such as Servers 1, 2, 3, 4, 5, and 6, after a release was successful, the servers would be destroyed, as shown here, as they have served their purpose:</p><div class="mediaobject"><img src="graphics/B05559_05_12.jpg" alt="Blue/green deployments"/></div><p>The next time servers 4, 5, and 6 were required, instead of doing an in-place upgrade, three new virtual machines <a id="id475" class="indexterm"/>would be created from the golden base image in a cloud environment. These golden images would already be patched up to the latest version, so brand new servers 7, 8, and 9 with the old servers destroyed and the new <span class="strong"><strong>Release 1.4</strong></span> would be deployed on them, as shown later. </p><p>Once server 7, 8, and 9 were live, servers 1, 2, and 3 would be destroyed as they have served their purpose:</p><div class="mediaobject"><img src="graphics/B05559_05_13.jpg" alt="Blue/green deployments"/></div></div></div>
<div class="section" title="Using Ansible to Orchestrate load balancers"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Using Ansible to Orchestrate load balancers</h1></div></div></div><p>In <a class="link" href="ch04.html" title="Chapter 4. Configuring Network Devices Using Ansible">Chapter 4</a>, <span class="emphasis"><em>Configuring Network Devices Using Ansible</em></span>, we covered the basics of Ansible and how to use an Ansible Control Host, playbooks, and roles for configuration management of network devices. Ansible, though, has multiple <a id="id476" class="indexterm"/>different core operations that can help with orchestrating load balancers, which we will look at in this chapter.</p><div class="section" title="Delegation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec65"/>Delegation</h2></div></div></div><p>Ansible delegation is a powerful mechanism that means from a playbook or role, Ansible can carry out actions <a id="id477" class="indexterm"/>on the target servers specified in the inventory file by <a id="id478" class="indexterm"/>connecting to them using SSH or WinRM, or alternately execute commands from the Ansible Control Host. WinRM is the Microsoft remote management standard and the equivalent of SSH for Windows that allows administrators to connect to Windows guests and execute programs.</p><p>The following diagram shows these two alternative connection methods with the Ansible Control Host either logging in to boxes using SSH or WinRM to configure them or running an API call from the Ansible Control Host directly:</p><div class="mediaobject"><img src="graphics/B05559_05_26.jpg" alt="Delegation"/></div><p>Both of these options can be carried out from the same role or playbook using <code class="literal">delegate_to</code>, which <a id="id479" class="indexterm"/>makes playbooks and roles extremely flexible as they can combine API calls and server-side <a id="id480" class="indexterm"/>configuration management tasks.</p><p>An example of delegation can be found later where the Ansible extras HAProxy modules are used, with <code class="literal">delegate_to</code> used to trigger an orchestration action that disables all backend services in the inventory file:</p><div class="mediaobject"><img src="graphics/B05559_05_15.jpg" alt="Delegation"/></div></div><div class="section" title="Utilizing serial to control roll percentages"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec66"/>Utilizing serial to control roll percentages</h2></div></div></div><p>In order to release <a id="id481" class="indexterm"/>software without interruptions to service, a zero downtime approach is preferable, as it doesn't require a maintenance window to schedule a change or release. Ansible supports a <span class="emphasis"><em>serial</em></span> option, which passes a percentage value to a playbook.</p><p>The <span class="emphasis"><em>serial</em></span> option allows Ansible to iterate over the inventory and only carry out the action against a percentage of the boxes, completing the necessary playbook, before moving onto the next portion of the inventory. It is important to note that Ansible passes inventory as an unordered dictionary, so the percentage of the inventory that is processed will not be in a specific order.</p><p>Using the <span class="emphasis"><em>serial</em></span> option that a blue/green strategy could be employed in Ansible, so boxes will need to be taken out of the load balancer and upgraded before being put back into service. Rather than doubling up on the number of boxes, three boxes are required, as shown in the following image, which all serve <span class="strong"><strong>Release 1.4</strong></span>:</p><div class="mediaobject"><img src="graphics/B05559_05_16.jpg" alt="Utilizing serial to control roll percentages"/></div><p>Utilizing the following Ansible <a id="id482" class="indexterm"/>playbook, using a combination of <code class="literal">delegate_to</code> and <code class="literal">serial</code>, each of the servers can be upgraded using a rolling update:</p><div class="mediaobject"><img src="graphics/B05559_05_17.jpg" alt="Utilizing serial to control roll percentages"/></div><p>The playbook will execute the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The <code class="literal">serial 30%</code> will mean that<a id="id483" class="indexterm"/> only one server at a time is upgraded. So, <span class="strong"><strong>Server 7</strong></span> will be taken out of the HAProxy <code class="literal">backend_nodes</code> pool by disabling the service calling the HAProxy using a local <code class="literal">delegate_to</code> action on the Ansible Control Host. A <code class="literal">yum</code> update will then be executed to upgrade the server version new <code class="literal">application1</code> release <span class="strong"><strong>version 1.5</strong></span> on <span class="strong"><strong>server 7</strong></span>, as follows:<div class="mediaobject"><img src="graphics/B05559_05_18.jpg" alt="Utilizing serial to control roll percentages"/></div></li><li class="listitem"><span class="strong"><strong>Server 7</strong></span> will then be <a id="id484" class="indexterm"/>enabled again and put into service on the load balancer using a local <code class="literal">delegate_to</code> action. The serial command will iterate onto <code class="literal">server 8</code> and disable it on HAProxy, before doing a <code class="literal">yum</code> update to upgrade the server version new <code class="literal">application1</code> release <span class="strong"><strong>version 1.5</strong></span>, as follows:<div class="mediaobject"><img src="graphics/B05559_05_19.jpg" alt="Utilizing serial to control roll percentages"/></div></li><li class="listitem">The rolling update will then enable <span class="strong"><strong>Server 8</strong></span> on the load balancer, and the serial command will <a id="id485" class="indexterm"/>iterate onto <span class="strong"><strong>Server 9</strong></span>, disabling it on HAProxy before doing a yum update, which will upgrade the server with the new <code class="literal">application1</code> release <span class="strong"><strong>version 1.5</strong></span> alternating when necessary between execution on the local server and the server, as shown here:<div class="mediaobject"><img src="graphics/B05559_05_20.jpg" alt="Utilizing serial to control roll percentages"/></div></li><li class="listitem">Finally, the playbook <a id="id486" class="indexterm"/>will finish by enabling <span class="strong"><strong>server 9</strong></span> on the load balancer, and all servers will be upgraded to <span class="strong"><strong>Release 1.5</strong></span> using Ansible as follows:<div class="mediaobject"><img src="graphics/B05559_05_21.jpg" alt="Utilizing serial to control roll percentages"/></div></li></ol></div></div><div class="section" title="Dynamic inventories"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec67"/>Dynamic inventories</h2></div></div></div><p>When dealing with cloud <a id="id487" class="indexterm"/>platforms, using just static inventories is <a id="id488" class="indexterm"/>sometimes not enough. It is useful to understand the inventory of servers that are already deployed within the estate and target subsets of them based on characteristics or profiles.</p><p>Ansible has an enhanced feature named the dynamic inventory. It allows users to query a cloud platform of their choosing with a Python script; this will act as an autodiscovery tool that can be connected to AWS or OpenStack, returning the server inventory in JSON format.</p><p>This allows Ansible to load this JSON file into a playbook or role so that it can be iterated over. In the same way, a static inventory file can be via variables.</p><p>The dynamic inventory fits into the same command-line constructs instead of passing the following static inventory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook –i inevntories/inevtontory –l qa –e current_build=9 playbooks/add_hosts_to_netscaler.yml</strong></span>
</pre></div><p>Then, a dynamic inventory script, <code class="literal">openstack.py</code>, for the OpenStack cloud provider could be passed instead:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook –i inevntories/openstack.py –l qa –e environment=qa playbooks/devops-for_networking.yml</strong></span>
</pre></div><p>The dynamic inventory script <a id="id489" class="indexterm"/>can be set up to allow specific limits. In the <a id="id490" class="indexterm"/>preceding case, the only server inventory that has been returned is the quality assurance servers, which is controlled using the <code class="literal">–l qa</code> limit.</p><p>When using Ansible with immutable servers, the static inventory file can be utilized to spin up new virtual machines, whereas the static inventory can be used to query the estate and do supplementary actions when they have already been created.</p></div><div class="section" title="Tagging metadata"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec68"/>Tagging metadata</h2></div></div></div><p>When using dynamic inventory in Ansible, metadata becomes a very important component, as servers deployed in a<a id="id491" class="indexterm"/> cloud environment can be sorted and filtered using metadata that is tagged against virtual or physical machines.</p><p>When provisioning AWS, Microsoft Azure, or OpenStack instances in a public or private cloud, metadata can be tagged against servers to group them.</p><p>In the following example, we can see a playbook creating new OpenStack servers using the <code class="literal">os_server</code> OpenStack module. It will iterate over the static inventory, tagging each newly created group, and release metadata on the machine:</p><div class="mediaobject"><img src="graphics/B05559_05_22.jpg" alt="Tagging metadata"/></div><p>The dynamic inventory can then be filtered using the <code class="literal">–l</code> argument to specify boxes with <code class="literal">group: qa</code>. This will <a id="id492" class="indexterm"/>return a consolidated list of servers.</p></div><div class="section" title="Jinja2 filters"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec69"/>Jinja2 filters</h2></div></div></div><p>
<span class="strong"><strong>Jinja2 filters</strong></span> allow Ansible to filter a playbook or role, allowing it to control which conditions need to be <a id="id493" class="indexterm"/>satisfied before executing a particular command or module. There <a id="id494" class="indexterm"/>are a wide variety of different jinja2 filters available out of the box with Ansible or custom filters can be written.</p><p>An example of a playbook using a jinja2 filter would only add the server to the NetScaler if its metadata <code class="literal">openstack.metadata.build</code> value is equal to the current build version:</p><div class="mediaobject"><img src="graphics/B05559_05_23.jpg" alt="Jinja2 filters"/></div><p>Executing the ansible-playbook <code class="literal">add_hosts_to_netscaler.yml command</code> with a limit <code class="literal">–l</code> on <code class="literal">qa</code> would only return boxes in the <code class="literal">qa</code> metadata group as the inventory. Then, the boxes can be further filtered at playbook or role using the when jinja2 filter to only execute the <code class="literal">add into load balancer pool</code> command if the <code class="literal">openstack.metadata.build</code> number of the box matches the <code class="literal">current_build</code> variable of <code class="literal">9</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook –I inevntories/openstack.py –l qa –e environment=qa –e current_build=9 playbooks/add_hosts_to_netscaler.yml</strong></span>
</pre></div><p>The result of this would be that only the new boxes would be added to the NetScaler <code class="literal">lbvserver</code> VIP.</p><p>The boxes could be removed in a similar way in the same playbook with a <span class="emphasis"><em>not equal to</em></span> condition:</p><div class="mediaobject"><img src="graphics/B05559_05_24.jpg" alt="Jinja2 filters"/></div><p>This could all be combined along with the serial percentage to roll percentages of the new release into service on the <a id="id495" class="indexterm"/>load balancer and decommission the old release utilizing <a id="id496" class="indexterm"/>dynamic inventory, delegation, jinja2 filters, and the serial rolling update features of Ansible together for simple orchestration of load balancers.</p></div><div class="section" title="Creating Ansible networking modules"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec70"/>Creating Ansible networking modules</h2></div></div></div><p>As Ansible can be used to schedule API commands against a load balancer, it can be easily utilized <a id="id497" class="indexterm"/>to build out a load balancer object model that <a id="id498" class="indexterm"/>popular networking solutions, such as Citrix NetScaler, F5 Big-IP, or Avi Networks, utilize.</p><p>With the move to microservice architectures, load balancing configuration needs to be broken out to remain manageable, so it is application-centric , as opposed to living in a centralized monolith configuration file.</p><p>This means that there are operational concerns when doing load balancing changes, so Ansible can be utilized by network operators to build out the complex load balancing rules, apply SSL certificates, and set up more complex layer 7 context switching or public IP addresses and provide this as a service to developers.</p><p>Utilizing the Python APIs provided by load balancing vendors, each operation could then be created as a module with a set of YAML <code class="literal">var</code> files describing the intended state of the load balancer.</p><p>In the example mentioned later, we look at how Ansible var files could be utilized by developers to create a service and health monitor for every new virtual server on a NetScaler. These services are then bound to the <code class="literal">lbvserver</code> entity, which was created by the network team, with a roll percentage of 10%, which can be loaded into the playbook's <code class="literal">serial</code> command. The playbook or role is utilized to create services, lbmonitors, 34 servers and bind <a id="id499" class="indexterm"/>services to lbvservers, whereas the var file <a id="id500" class="indexterm"/>describes the desired state of those NetScaler objects:</p><div class="mediaobject"><img src="graphics/B05559_05_25.jpg" alt="Creating Ansible networking modules"/></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter, we saw that the varied load balancing solutions are available from proprietary vendors to open source solutions, and discussed the impact that microservices have had on load balancing, moving it from a centralized to distributed model to help serve east-west traffic.</p><p>We then looked at blue/green deployment models, the merits of immutable and static servers, and how software releases can be orchestrated using Ansible in either model. In the process, we illustrated how useful Ansible is at orchestrating load balancers by utilizing dynamic inventory, rolling updates, delegation, and jinja2 filters can all be used to help fulfill load balancing requirements.</p><p>The key takeaways from this chapter are that microservice applications have changed the way applications need to be load balanced, and distributed load balancing is better suited when deploying microservice applications, which have more east-west traffic patterns.</p><p>The reasons that immutable infrastructure is well-suited to microservice applications should now be clear.. The chapter also defined ways that state and data can be separated from the operating system and that different rolling update models are required to support stateless and stateful applications. In the next chapter, we will look at applying these same automation principles to SDN Controllers, primarily focusing on the Nuage solution. It will cover configuring firewall rules and other SDN commands, so the whole network can be programmatically controlled and automated.</p></div></body></html>