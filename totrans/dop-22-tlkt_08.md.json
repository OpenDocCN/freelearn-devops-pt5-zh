["```\n`1` chmod +x scripts/dm-swarm-04.sh\n`2` \n`3` ./scripts/dm-swarm-04.sh\n`4` \n`5` `eval` `$(`docker-machine env swarm-1`)` \n```", "`````````````````````````` We executed the `dm-swarm-04.sh` script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks and deployed the stacks. Now we should wait a few moments until all the services in the `monitor` stack are up and running. Please use `docker stack ps monitor` command to confirm that the status of all the services in the stack is *Running*.    Finally, we’ll confirm that everything is deployed correctly by opening Prometheus in a browser.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/monitor\"`  ```   ````````````````````````` Now the state of our cluster is the same as it was at the end of the previous chapter and we can proceed towards deploying exporters.    ### Deploying Exporters    Exporters provide data Prometheus can scrape and put into its database.    The stack we’ll deploy is as follows.    ```  `1` `version``:` `\"3\"`  `2`   `3` `services``:`  `4`   `5`  `ha``-``proxy``:`  `6`    `image``:` `quay``.``io``/``prometheus``/``haproxy``-``exporter``:``$``{``HA_PROXY_TAG``:-``latest``}`  `7`    `networks``:`  `8`      `-` `proxy`  `9`      `-` `monitor` `10 `    `deploy``:` `11 `      `labels``:` `12 `        `-` `com``.``df``.``notify``=``true` `13 `        `-` `com``.``df``.``scrapePort``=``9101` `14 `    `command``:` `-``haproxy``.``scrape``-``uri=``\"http://admin:admin@proxy/admin?stats;csv\"` `15`  `16 `  `cadvisor``:` `17 `    `image``:` `google``/``cadvisor``:``$``{``CADVISOR_TAG``:-``latest``}` `18 `    `networks``:` `19 `      `-` `monitor` `20 `    `volumes``:` `21 `      `-` `/:/rootfs` `22 `      `-` `/``var``/``run``:``/``var``/``run` `23 `      `-` `/``sys``:``/``sys` `24 `      `-` `/``var``/``lib``/``docker``:``/``var``/``lib``/``docker` `25 `    `deploy``:` `26 `      `mode``:` `global` `27 `      `labels``:` `28 `        `-` `com``.``df``.``notify``=``true` `29 `        `-` `com``.``df``.``scrapePort``=``8080` `30`  `31 `  `node``-``exporter``:` `32 `    `image``:` `basi``/``node``-``exporter``:``$``{``NODE_EXPORTER_TAG``:-``v1``.13.0``}` `33 `    `networks``:` `34 `      `-` `monitor` `35 `    `environment``:` `36 `      `-` `HOST_HOSTNAME``=/etc``/``host_hostname` `37 `    `volumes``:` `38 `      `-` `/``proc``:``/``host``/``proc` `39 `      `-` `/``sys``:``/``host``/``sys` `40 `      `-` `/:/rootfs` `41 `      `-` `/``etc``/``hostname``:``/``etc``/``host_hostname` `42 `    `deploy``:` `43 `      `mode``:` `global` `44 `      `labels``:` `45 `        `-` `com``.``df``.``notify``=``true` `46 `        `-` `com``.``df``.``scrapePort``=``9100` `47 `    `command``:` `'-collector.procfs /host/proc -collector.sysfs /host/sys -collector\\` `48` `.filesystem.ignored-mount-points \"^/(sys|proc|dev|host|etc)($$|/)\" -collector.te\\` `49` `xtfile.directory /etc/node-exporter/ -collectors.enabled=\"conntrack,diskstats,en\\` `50` `tropy,filefd,filesystem,loadavg,mdadm,meminfo,netdev,netstat,stat,textfile,time,\\` `51` `vmstat,ipvs\"'` `52`  `53` `networks``:` `54 `  `monitor``:` `55 `    `external``:` `true` `56 `  `proxy``:` `57 `    `external``:` `true`  ```   ```````````````````````` As you can see, the stack definition contains the `node` and `haproxy` exporters as well as `cadvisor` service. `haproxy-exporter` provides proxy metrics, `node-exporter` collects server data, while `cadvisor` outputs information about containers inside our cluster. You’ll notice that `cadvisor` and `node-exporter` are running in the `global mode`. A replica will run on each server so that we can obtain an accurate picture of all the nodes that form the cluster.    The important parts of the stack definition are `com.df.notify` and `com.df.scrapePort` labels. The first one tells `swarm-listener` that it should notify the monitor when those services are created (or destroyed). The `scrapePort` labels are defining ports of the exporters.    Please visit [Scrape Parameter](http://monitor.dockerflow.com/usage/#scrape-parameters) section of the documentation for more information how to define scrape parameters.    Let’s deploy the stack and see it in action.    ``` `1` docker stack deploy `\\` `2 `    -c stacks/exporters.yml `\\` `3 `    exporter  ```   ``````````````````````` Please wait until all the services in the stack and running. You can monitor their status with `docker stack ps exporter` command.    Once the `exporter` stack is up-and-running, we can confirm that all the services were added to the `monitor` config.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/monitor/config\"`  ```  `````````````````````` ![Figure 4-1: Configuration with exporters](img/00015.jpeg)  Figure 4-1: Configuration with exporters    We can also confirm that all the targets are indeed working.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/monitor/targets\"`  ```   ````````````````````` There should be three targets. If they are still not registered, please wait a few moments and refresh your screen.    Two of the targets (`exporter_cadvisor` and `exporter_node-exporter`) are running as global services. As a result, each has three endpoints, one on each node. The last target is `exporter_ha-proxy`. Since we did not deploy it globally nor specified multiple replicas, in has only one endpoint.  ![Figure 4-2: Targets and endpoints](img/00016.jpeg)  Figure 4-2: Targets and endpoints    If we used the “official” Prometheus image, setting up those targets would require an update of the config file and reload of the service. On top of that, we’d need to persist the configuration. Instead, we let *Swarm Listener* notify *Docker Flow Monitor* that there are new services that should, in this case, generate new scraping targets. Instead of splitting the initial information into multiple locations, we specified scraping info as service labels and let the system take care of the distribution of that data.  ![Figure 4-3: Prometheus scrapes metrics from exporters](img/00017.jpeg)  Figure 4-3: Prometheus scrapes metrics from exporters    Let’s take a closer look into the exporters running in our cluster.    ### Exploring Exporter Metrics    All the exporters we deployed expose metrics in Prometheus format. We can observe them by sending a simple HTTP request. Since the services do not publish any ports, the only way we can communicate with them is through the `monitor` network attached to those exporters.    We’ll create a new utility service and attach it to the `monitor` network.    ``` `1` docker service create `\\` `2 `    --name util `\\` `3 `    --network monitor `\\` `4 `    --mode global `\\` `5 `    alpine sleep `100000000`  ```   ```````````````````` We created a service based on the `alpine` image, named it `util`, and attached it to the `monitor` network so that it can communicate with exporters we deployed. We made the service `global` so that it runs on every node. That guaranteed that a replica runs on the node we’re in. Since `alpine` does not have a long running process, without `sleep`, it would stop as soon as it started, Swarm would reschedule it, only to detect that it stopped again, and so on. Without `sleep` it would enter a never ending loop of failures and rescheduling.    ``` `1` `ID``=``$(`docker container ls -q `\\` `2 `    -f `\"label=com.docker.swarm.service.name=util\"``)` `3`  `4` docker container `exec` -it `$ID` `\\` `5 `    apk add --update curl  ```   ``````````````````` Next, we found the `ID` of the container, entered it, and installed `curl`.    Now we’re ready to send requests to the exporters.    ``` `1` docker container `exec` -it `$ID` `\\` `2 `    curl node-exporter:9100/metrics  ```   `````````````````` Partial output of the request to the `node-exporter` is as follows.    ```  `1` ...  `2` # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.  `3` # TYPE process_cpu_seconds_total counter  `4` process_cpu_seconds_total 3.05  `5` # HELP process_max_fds Maximum number of open file descriptors.  `6` # TYPE process_max_fds gauge  `7` process_max_fds 1.048576e+06  `8` # HELP process_open_fds Number of open file descriptors.  `9` # TYPE process_open_fds gauge `10` process_open_fds 7 `11` # HELP process_resident_memory_bytes Resident memory size in bytes. `12` # TYPE process_resident_memory_bytes gauge `13` process_resident_memory_bytes 1.6228352e+07 `14` # HELP process_start_time_seconds Start time of the process since unix epoch in \\ `15` seconds. `16` # TYPE process_start_time_seconds gauge `17` process_start_time_seconds 1.49505618366e+09 `18` # HELP process_virtual_memory_bytes Virtual memory size in bytes. `19` # TYPE process_virtual_memory_bytes gauge `20` process_virtual_memory_bytes 2.07872e+07 `21` ...  ```   ````````````````` As you can see, each metric contains a help entry that describes it, states the type, and displays metric name followed with a value.    We won’t go into details of all the metrics provided by `node-exporter`. The list is quite big, and it would require a whole chapter (maybe even a book) to go through all of them. The important thing, at this moment, is to know that almost anything hardware and OS related is exposed as a metric.    Please note that Overlay network load-balanced our request and forwarded it to one of the replicas of the exporter. We don’t know what the origin of those metrics is. It could be a replica running on any of the nodes of the cluster. That should not be a problem since, at this moment, we’re interested only in observing how metrics look like. If you go back to the configuration screen, you’ll notice that targets are configured to use `tasks.[SERVICE_NAME]` format for addresses. When a service name is prefixed with `tasks.`, Swarm returns the list of all replicas (or tasks) of a service.    Let’s move to `cadvisor` metrics.    ``` `1` docker container `exec` -it `$ID` `\\` `2 `    curl cadvisor:8080/metrics  ```   ```````````````` Partial output of the request to `cadvisor` metrics is as follows.    ```  `1` ...  `2` # HELP container_network_receive_bytes_total Cumulative count of bytes received  `3` # TYPE container_network_receive_bytes_total counter  `4` container_network_receive_bytes_total{id=\"/\",interface=\"dummy0\"} 0  `5` container_network_receive_bytes_total{id=\"/\",interface=\"eth0\"} 6.6461026e+07  `6` container_network_receive_bytes_total{id=\"/\",interface=\"eth1\"} 1.3054141e+07  `7` ...  `8` container_network_receive_bytes_total{container_label_com_docker_stack_namespace\\  `9` =\"proxy\",container_label_com_docker_swarm_node_id=\"zvn1kazstoa12pu3rfre9j4sw\",co\\ `10` ntainer_label_com_docker_swarm_service_id=\"gfoias8w9bf1cve5dujzzlpfh\",container_\\ `11` label_com_docker_swarm_service_name=\"proxy_swarm-listener\",container_label_com_d\\ `12` ocker_swarm_task=\"\",container_label_com_docker_swarm_task_id=\"39hgd75s8vt051smew\\ `13` 3ke4imw\",container_label_com_docker_swarm_task_name=\"proxy_swarm-listener.1.39hg\\ `14` d75s8vt051smew3ke4imw\",id=\"/docker/f2232d2ddf801b1ff41120bb1b95213be15767fe0e6d4\\ `15` 5266b3b8bba149b3634\",image=\"vfarcic/docker-flow-swarm-listener:latest@sha256:d67\\ `16` 494f08aa3efba86d5231adba8ee7281c29fd401a5f67377ee026cc436552b\",interface=\"eth0\",\\ `17` name=\"proxy_swarm-listener.1.39hgd75s8vt051smew3ke4imw\"} 112764 `18` ...  ```   ``````````````` The major difference, when compared to `node-exporter`, is that `cadvisor` provides a lot of labels. They help a lot when querying metrics, and we’ll use them soon.    Just like with `node-exporter`, we won’t go into details of each metric exposed through `cadvisor`. Instead, as we’re progressing towards creating a *self-healing* system, we’ll gradually increase the number of metrics we’re using and comment on them as they come.    Now that we have the metrics and that Prometheus is scraping and storing them in its database, we can turn our attention to queries we can execute.    ### Querying Metrics    Targets are up and running, and Prometheus is scraping their data. We should generate some traffic that would let us see Prometheus query language in action.    We’ll deploy *go-demo* stack. It contains a service with an API and a corresponding database. We’ll use it as a demo service that will allow us to explore better some of the metrics we can use.    ``` `1` docker stack deploy `\\` `2 `    -c stacks/go-demo.yml `\\` `3 `    go-demo  ```   `````````````` We should wait a few moments for the services from the `go-demo` stack to become operational. Please execute `docker stack ps go-demo` to confirm that all the replicas are running.    Now that the demo service is running, we can explore some of the metrics we have at our disposal.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/monitor/graph\"`  ```   ````````````` Please type `haproxy_backend_connections_total` in the *Expression* field, and press the *Execute* button. The result should be zero connections on the backend `go-demo_main-be8080`. Let’s spice it up by creating a bit of traffic.    ``` `1` `for` `((``n``=``0``;`n<`200``;`n++`))``;` `do` `2 `    curl `\"http://``$(`docker-machine ip swarm-1`)``/demo/hello\"` `3` `done`  ```   ```````````` We sent 200 requests to the `go-demo` service.    If we go back to the Prometheus UI and repeat the execution of the `haproxy_backend_connections_total` expression, the result should be different. In my case, there are *200* backend connections from *go-demo_main-be8080*.  ![Figure 4-4: HA Proxy metrics](img/00018.jpeg)  Figure 4-4: HA Proxy metrics    We could display the data as a graph by clicking the *Graph* tab.    How about memory usage? We have the data through `cadvisor` so we might just as well use it.    Please type `container_memory_usage_bytes{container_label_com_docker_swarm_service_name=\"go-demo_main\"}` in the expression field and click the *Execute* button.    The result is memory usage limited to the Docker service `go-demo_main`. Depending on the view, you should see three values in *Console* or three lines in the *Graph* tab. They represent memory usage of the three replicas of the `go-demo_main` service.  ![Figure 4-5: cAdvisor metrics](img/00019.jpeg)  Figure 4-5: cAdvisor metrics    Finally, let’s explore one of the `node-exporter` metrics. We can, for example, display the amount of available memory from each of the nodes.    Please type `sum by (instance) (node_memory_MemFree)` in the expression field and click the *Execute* button.    The result is a representation of free memory for each of the nodes of the cluster.  ![Figure 4-6: Graph with available memory](img/00020.jpeg)  Figure 4-6: Graph with available memory    Now that we had a very brief overview of the ways we can query metrics, we should start using them.    ### Updating Service Constraints    The services we created so far are scheduled without any constraints. The exceptions are those that tie some of the services to one of the Swarm managers.    Without constraints, Swarm will distribute service replicas evenly. It will place them on a node that has fewest containers. Such a strategy can be disastrous. For example, we might end up with Prometheus, ElasticSearch, and MongoDB on the same node. Since all three of them require a fair amount of memory, their performance can deteriorate quickly. At the same time, the rest of the nodes might be running very undemanding services like `go-demo`. As a result, we can end up with a very uneven distribution of replicas from the resource perspective.    We cannot blame Swarm for a poor distribution of service replicas. We did not give it any information to work with. As a minimum, we should have defined how much memory it should reserve for each service as well as memory limits.    Memory reservation gives Swarm a hint how much it should reserve for a service. If, for example, we specify that a replica of a service should reserve 1GB of memory, Swarm will make sure to run it on a node that has that amount available. Bear in mind that it does not compare reservation with the actual memory usage but, instead, it compares it with the reservations made for other services and the total amount of memory allocated to each node.    Memory limit, on the other hand, should be set to the maximum amount we expect a service to use. If the actual usage surpasses it, the container will be shut down and, consequently, Swarm will reschedule it. Memory limit is, among other things, a useful protection against memory leaks and a way of preventing a single service abducting all the resources.    Let us revisit the services we are currently running and try to set their memory reservations and limits.    What should be the constraint values? How do we know how much memory should be reserved and what should be the limit? As it happens, there are quite a few different approaches we can take.    We could visit a fortune teller and consult a crystal ball, or we can make a lot of very inaccurate assumptions. Either of those is a bad way of defining constraints. You might be inclined to say that databases need more memory than backend services. We can assume that those written in Java require more resources than those written in Go. There is no limit to the number of guesses we could make. However, more often than not, they will be false and inaccurate. If those two would be the only options, I would strongly recommend visiting a fortune teller instead guessing. Since the result will be, more or less, the same, a fortune teller can, at least, provide a fun diversion from day to day monotony and lead to very popular photos uploaded to Instagram.    The correct approach is to let the services run for a while and consult metrics. Then let them run a while longer and revisit the metrics. Then wait some more and consult again. The point is that the constraints should be reviewed and, if needed, updated periodically. They should be redefined and adapted as a result of new data. It’s a task that should be repeated every once in a while. Fortunately, we can create alerts that will tell us when to revisit constraints. However, you’ll have to wait a while longer until we get there. For now, we are only concerned with the initial set of constraints.    While we should let the services run for at least a couple of hours before consulting metrics, my patience is reaching the limit. Instead, we’ll imagine that enough metrics were collected and consult Prometheus.    The first step is to get a list of the stacks we are currently running.    ``` `1` docker stack ls  ```   ``````````` The output is as follows.    ``` `1` NAME      SERVICES `2` exporter  3 `3` go-demo   2 `4` monitor   2 `5` proxy     2  ```   `````````` Let us consult the current memory usage of those services.    Please open Prometheus’ graph screen.    ``` `1` open `\"http://``$(`docker-machine ip swarm-1`)``/monitor/graph\"`  ```   ````````` Type `container_memory_usage_bytes{container_label_com_docker_stack_namespace=\"exporter\"}` in the *Expression* field, click the *Execute* button, and switch to the *Graph* view.    If you hover over the lines in the graph, you’ll see that one of the labels is `container_label_com_docker_swarm_service_name`. It contains the name of a service allowing you to identify how much memory it is consuming.    While the exact numbers will differ from one case to another, `exporter_cadvisor` should be somewhere between 20MB and 30MB, while `exporter_node-exporter` and `exporter_ha-proxy` should have lower usage that is around 10MB.    With those numbers in mind, our `exporter` stack can be as follows (limited to relevant parts).    ```  `1` ...  `2`   `3`  ha-proxy:  `4`    ...  `5`    deploy:  `6`      ...  `7`      resources:  `8`        reservations:  `9`          memory: 20M `10 `        limits: `11 `          memory: 50M `12 `    ... `13`  `14 `  cadvisor: `15 `    ... `16 `    deploy: `17 `      ... `18 `      resources: `19 `        reservations: `20 `          memory: 30M `21 `        limits: `22 `          memory: 50M `23`  `24 `  node-exporter: `25 `    ... `26 `    deploy: `27 `      ... `28 `      resources: `29 `        reservations: `30 `          memory: 20M `31 `        limits: `32 `          memory: 50M `33 `    ...  ```   ```````` We set memory reservations similar to the upper bounds of the current usage. That will help Swarm schedule the containers better, unless they are global and have to run everywhere. More importantly, it allows Swarm to calculate future schedules by excluding these reservations from the total available memory.    Memory limits, on the other hand, will provide limitations on how much memory containers created from those services can use. Without memory limits, a container might “go wild” and abduct all the memory on a node for itself. Good example are in-memory databases like Prometheus. If we would deploy it without any limitation, it could easily take over all the resources leaving the rest of the services running on the same node struggling.    Let’s deploy the updated version of the `exporter` stack.    ``` `1` docker stack deploy `\\` `2 `    -c stacks/exporters-mem.yml `\\` `3 `    exporter  ```   ``````` Since most of the stack are global services, we will not see much difference in the way Swarm schedules them. No matter the reservations, a replica will run on each node when the mode is global. Later on, we’ll see more benefits behind memory reservations. For now, the important thing to note is that Swarm has a better picture about the reserved memory on each node and will be able to do future scheduling with more precision.    We’ll continue with the rest of the stacks. The next in line is `go-demo`.    Please go back to Prometheus’ Graph screen, type `container_memory_usage_bytes{container_label_com_docker_stack_namespace=\"go-demo\"}` in the *Expression* field, and click the *Execute* button.    The current usage of `go-demo_db` should be between 30MB and 40MB while `go-demo_main` is probably below 5MB. We’ll update the stack accordingly.    The new `go-demo` stack is as follows (limited to relevant parts).    ```  `1` ...  `2`  main:  `3`    ...  `4`    deploy:  `5`      ...  `6`      resources:  `7`        reservations:  `8`          memory: 5M  `9`        limits: `10 `          memory: 10M `11`  `12 `  db: `13 `    ... `14 `    deploy: `15 `      resources: `16 `        reservations: `17 `          memory: 40M `18 `        limits: `19 `          memory: 80M `20` ...  ```   `````` Now we can deploy the updated version of the `go-demo` stack.    ``` `1` docker stack deploy `\\` `2 `    -c stacks/go-demo-mem.yml `\\` `3 `    go-demo  ```   ````` Two stacks are done, and two are still left to be updated. The `monitor` and `proxy` stacks should follow the same process. I’m sure that by now you can query Prometheus by yourself. You’ll notice that `monitor_monitor` service (Prometheus) is the one that uses the most memory (over 100MB). Since we can expect Prometheus memory usage to rise with time, we should be generous with its reservations and set it to 500MB. Similarly, a reasonable limit could be 800MB. The rest of the services are very moderate with their memory consumption.    Once you’re done exploring the rest of the stacks through Prometheus, the only thing left is to deploy of the updated versions.    ``` `1` `DOMAIN``=``$(`docker-machine ip swarm-1`)` `\\` `2 `    docker stack deploy `\\` `3 `    -c stacks/docker-flow-monitor-mem.yml `\\` `4 `    monitor `5`  `6` docker stack deploy `\\` `7 `    -c stacks/docker-flow-proxy-mem.yml `\\` `8 `    proxy  ```   ```` Now that our stacks are better-defined thanks to metrics, we can proceed and try to improve our queries through memory reservations and limits.    ### Using Memory Reservations and Limits in Prometheus    Metrics obtained through *cAdvisor* are not restricted to actual usage. We have, among others, metrics based on container specs. We can, for example, retrieve memory limits with the metric `container_spec_memory_limit_bytes`.    Please type `container_spec_memory_limit_bytes{container_label_com_docker_stack_namespace!=\"\"}` in the *Expression* field and click the *Execute* button. The result should be straight lines that represent memory limits we defined in our stacks.    The usage of the `container_label_com_docker_stack_namespace` label is important. We used it to filter the metrics so that only those that come from the stacks are included. That way, we excluded root metrics from `cAdvisor` that provide summarized totals.    In Prometheus, memory limits are not very useful in themselves. However, if we combine them with the actual memory usage, we can get percentages that can provide indications of the health of our system.    Please type `container_memory_usage_bytes{container_label_com_docker_stack_namespace!=\"\"} / container_spec_memory_limit_bytes{container_label_com_docker_stack_namespace!=\"\"} * 100` in the *Expression* field and click the *Execute* button.  ![Figure 4-7: Graph percentages based on memory limits and the actual usage](img/00021.jpeg)  Figure 4-7: Graph percentages based on memory limits and the actual usage    The result consists of percentages based on memory limits and the actual usage. The should all be below 60%. We will leverage this information later when we start working on alerts.    ### What Now?    We did not go deep into metrics and queries. There are too many of them. Listing each metric would be the repetition of the `HELP` entries that already explain them (even though often not in much detail). More importantly, I believe that the best way to learn something is through a practical usage. We’ll use those metrics soon when we start creating alerts, and you will have plenty of opportunities to get a better understanding how they work. The same holds true for queries. They will be indispensable for creating alerts and will be explained in more details in the next chapter. Still, even though we’ll go through quite a few metrics and queries, the book will not provide a detailed documentation of every combination you can apply. [Querying Prometheus](https://prometheus.io/docs/querying/basics/) is a much better place to learn how queries work. Instead, we’ll focus on practical hands-on experience.    Now it’s time for another break. Remove the VMs, grab a coffee, do something fun, and come back fresh. Alerts are coming next.    ``` `1` docker-machine rm -f `\\` `2 `    swarm-1 swarm-2 swarm-3  ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` `````````````````` ``````````````````` ```````````````````` ````````````````````` `````````````````````` ``````````````````````` ```````````````````````` ````````````````````````` ``````````````````````````"]