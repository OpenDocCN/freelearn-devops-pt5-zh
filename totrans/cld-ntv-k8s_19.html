<html><head></head><body>
		<div><p><a id="_idTextAnchor321"/></p>
			<h1 id="_idParaDest-305"><em class="italic"><a id="_idTextAnchor322"/>Chapter 15</em>: Stateful Workloads on Kubernetes</h1>
			<p>This chapter details the current state of the industry when it comes to running stateful workloads in databases. We will discuss the use of Kubernetes (and popular open source projects) for running databases, storage, and queues on Kubernetes. Case study tutorials will include running object storage, a database, and a queue system on Kubernetes.</p>
			<p>In this chapter, we will first understand how stateful applications run on Kubernetes and then learn how to use Kubernetes storage for stateful applications. We will then learn how to run databases on Kubernetes, as well as covering messaging and queues. Let's start with a discussion of why stateful applications are much more complex than stateless applications on Kubernetes.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding stateful applications on Kubernetes</li>
				<li>Using Kubernetes storage for stateful applications</li>
				<li>Running databases on Kubernetes</li>
				<li>Implementing messaging and queues on Kubernetes</li>
			</ul>
			<h1 id="_idParaDest-306"><a id="_idTextAnchor323"/>Technical requirements</h1>
			<p>In order to run the commands detailed in this chapter, you will need a computer that supports the <code>kubectl</code> command-line tool along with a working Kubernetes cluster. See <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, for several methods for getting up and running with Kubernetes quickly, and for instructions on how to install the kubectl tool.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository: </p>
			<p><a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter15">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter15</a></p>
			<h1 id="_idParaDest-307"><a id="_idTextAnchor324"/>Understanding stateful applications on Kubernetes</h1>
			<p>Kubernetes <a id="_idIndexMarker843"/>provides excellent primitives for running both stateless <a id="_idIndexMarker844"/>and stateful applications, but stateful workloads have taken longer to mature on Kubernetes. However, in recent years, some high-profile Kubernetes-based stateful application frameworks and projects have proven the increasing maturity of stateful applications on Kubernetes. Let's review some of these first in order to set the stage for the rest of the chapter.</p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor325"/>Popular Kubernetes-native stateful applications</h2>
			<p>There are many types of<a id="_idIndexMarker845"/> stateful applications. Though most applications are stateful, only certain components in those applications store <em class="italic">state</em> data. We can remove these specific stateful components from applications and focus on those components in our review. In this book, we'll talk about databases, queues, and object storage, leaving out persistent storage components such as those we reviewed in <a href="B14790_07_Final_PG_ePub.xhtml#_idTextAnchor166"><em class="italic">Chapter 7</em></a>, <em class="italic">Storage on Kubernetes</em>. We'll also go over a few, less generic components as honorable mentions. Let's start with databases!</p>
			<h3>Kubernetes-compatible databases</h3>
			<p>In addition to<a id="_idIndexMarker846"/> typical <strong class="bold">databases</strong> (<strong class="bold">DBs</strong>) and key-value <a id="_idIndexMarker847"/>stores such as <strong class="bold">Postgres</strong>, <strong class="bold">MySQL</strong>, and <strong class="bold">Redis</strong> that can<a id="_idIndexMarker848"/> be deployed on Kubernetes with StatefulSets or community operators, there are some major made-for-Kubernetes options:</p>
			<ul>
				<li><strong class="bold">CockroachDB</strong>: A distributed SQL database that can be deployed seamlessly on Kubernetes</li>
				<li><strong class="bold">Vitess</strong>: A MySQL sharding orchestrator that allows global scalability for MySQL, also installable on Kubernetes via an operator</li>
				<li><strong class="bold">YugabyteDB</strong>: A distributed SQL database similar to <strong class="bold">CockroachDB</strong> that also supports Cassandra-like querying</li>
			</ul>
			<p>Next, let's look at<a id="_idIndexMarker849"/> queuing and messaging on Kubernetes.</p>
			<h3>Queues, streaming, and messaging on Kubernetes</h3>
			<p>Again, there <a id="_idIndexMarker850"/>are <a id="_idIndexMarker851"/>industry-standard <a id="_idIndexMarker852"/>options <a id="_idIndexMarker853"/>such <a id="_idIndexMarker854"/>as <strong class="bold">Kafka</strong> and <strong class="bold">RabbitMQ</strong> that <a id="_idIndexMarker855"/>can be deployed on<a id="_idIndexMarker856"/> Kubernetes<a id="_idIndexMarker857"/> using community Helm charts and operators, in addition to some purpose-made open- and closed-source options:</p>
			<ul>
				<li><strong class="bold">NATS</strong>: Open source messaging and streaming system</li>
				<li><strong class="bold">KubeMQ</strong>: Kubernetes-native message broker</li>
			</ul>
			<p>Next, let's look at object storage on Kubernetes.</p>
			<h3>Object storage on Kubernetes</h3>
			<p>Object storage takes <a id="_idIndexMarker858"/>volume-based persistent storage from<a id="_idIndexMarker859"/> Kubernetes and adds on an object storage layer, similar to (and in many cases compatible with the API of) Amazon S3:</p>
			<ul>
				<li><strong class="bold">Minio</strong>: S3-compatible object storage built for high performance.</li>
				<li><strong class="bold">Open IO</strong>: Similar to <em class="italic">Minio</em>, this has high performance and supports S3 and Swift storage.</li>
			</ul>
			<p>Next, let's look at a few honorable mentions.</p>
			<h3>Honorable mentions</h3>
			<p>In addition to the<a id="_idIndexMarker860"/> preceding generic components, there are some more specialized (but still categorical) stateful applications that can be run on Kubernetes:</p>
			<ul>
				<li><strong class="bold">Key and auth management</strong>: <strong class="bold">Vault</strong>, <strong class="bold">Keycloak</strong></li>
				<li><strong class="bold">Container registries</strong>: <strong class="bold">Harbor</strong>, <strong class="bold">Dragonfly</strong>, <strong class="bold">Quay</strong></li>
				<li><strong class="bold">Workflow management</strong>: <strong class="bold">Apache Airflow</strong> with a Kubernetes Operator</li>
			</ul>
			<p>Now that we've reviewed a few categories of stateful applications, let's talk about how these state-heavy <a id="_idIndexMarker861"/>applications are typically implemented on Kubernetes.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor326"/>Understanding strategies for running stateful applications on Kubernetes</h2>
			<p>Though there is<a id="_idIndexMarker862"/> nothing inherently wrong with deploying a stateful application on Kubernetes with a ReplicaSet or Deployment, you will find that the majority of stateful applications on Kubernetes use StatefulSets. We talked about StatefulSets in <a href="B14790_04_Final_PG_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 4</em></a>, <em class="italic">Scaling and Deploying Your Application</em>, but why are they so useful for applications? We will review and answer this question in this chapter.</p>
			<p>The main reason is Pod identity. Many distributed stateful applications have their own clustering mechanism or consensus algorithm. In order to smooth over the process for these types of applications, StatefulSets provide static Pod naming based on an ordinal system, starting from <code>0</code> to <code>n</code>. This, in combination with a rolling update and creation method, makes it much easier for applications to cluster themselves, which is extremely important for cloud-native databases such as CockroachDB. </p>
			<p>To illustrate how and why StatefulSets can help run stateful applications on Kubernetes, let's look at how we might run MySQL on Kubernetes with StatefulSets.</p>
			<p>Now, to be clear, running a single Pod of MySQL on Kubernetes is extremely simple. All we need to do is find a MySQL container image and ensure that it has the proper configuration and <code>startup</code> command.</p>
			<p>However, when we look to scale our database, we start to run into issues. Unlike a simple stateless application, where we can scale our deployment without creating new state, MySQL (like many other DBs) has its own method of clustering and consensus. Each member of a MySQL cluster knows about the other members, and most importantly, it knows which member of the cluster is the leader. This is how databases like MySQL can offer consistency guarantees and <strong class="bold">Atomicity, Consistency, Isolation, Durability</strong> (<strong class="bold">ACID</strong>) compliance.</p>
			<p>Therefore, since each member in a MySQL cluster needs to know about the other members (and most importantly, the master), we need to run our DB Pods in a way that means they have a common way to find and communicate with the other members of the DB cluster.</p>
			<p>The way that StatefulSets offer this is, as we mentioned at the beginning of the section, via ordinal Pod numbering. This way, applications that need to self-cluster while running on Kubernetes know that a common naming scheme starting from <code>0</code> to <code>n</code> will be used. In addition, when a Pod at a specific ordinal restarts – for instance, <code>mysql-pod-2</code> – the same PersistentVolume will be mounted to the new Pod that starts in that ordinal spot. This allows for stateful consistency between restarts for a single Pod in a StatefulSet, which <a id="_idIndexMarker863"/>makes it much easier for applications to form a stable cluster.</p>
			<p>To see how this works in practice, let's look at a StatefulSet specification for MySQL. </p>
			<h3>Running MySQL on StatefulSets</h3>
			<p>The following <a id="_idIndexMarker864"/>YAML spec is adapted from the Kubernetes <a id="_idIndexMarker865"/>documentation version. It shows how we can run MySQL clusters on StatefulSets. We will review each part of the YAML spec separately, so we can understand exactly how the mechanisms interact with StatefulSet guarantees.</p>
			<p>Let's start with the first part of the spec:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">statefulset-mysql.yaml</p>
			<pre>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql</pre>
			<p>As you can see, we are going to be creating a MySQL cluster with three <code>replicas</code>. </p>
			<p>There isn't much else exciting about this piece, so let's move onto the start of <code>initContainers</code>. There<a id="_idIndexMarker866"/> will be quite a few containers running in this<a id="_idIndexMarker867"/> Pod between <code>initContainers</code> and regular containers, so we will explain each separately. What follows is the first <code>initContainer</code> instance:</p>
			<pre>    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf
          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map</pre>
			<p>This first <code>initContainer</code>, as you can see, is the MySQL container image. Now, this doesn't mean that we won't have the MySQL container running constantly in the Pod. This is a pattern<a id="_idIndexMarker868"/> you will tend to see fairly often with complex <a id="_idIndexMarker869"/>applications. Sometimes the same container image is used as both an <code>initContainer</code> instance and a normally running container in a Pod. This is because that container has the correct embedded scripts and tools to do common setup tasks programmatically.</p>
			<p>In this example, the MySQL <code>initContainer</code> creates a file, <code>/mnt/conf.d/server-id.cnf</code>, and adds a <code>server</code> ID, corresponding to the Pod's <code>ordinal</code> ID in the StatefulSet, to the file. When writing the <code>ordinal</code> ID, it adds <code>100</code> as an offset, to get around the reserved value in MySQL of a <code>server-id</code> ID of <code>0</code>.</p>
			<p>Then, depending on whether the Pod <code>ordinal</code> D is <code>0</code> or not, it copies configuration for either a master or slave MySQL server to the volume.</p>
			<p>Next, let's look at the second <code>initContainer</code> in the following section (we've left out some code with volume mount information for brevity, but the full code is available in the GitHub repository of the book):</p>
			<pre>      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          xtrabackup --prepare --target-dir=/var/lib/mysql</pre>
			<p>As you can see, this <code>initContainer</code> isn't MySQL at all! Instead, the container image is a tool called Xtra Backup. Why do we need this container? </p>
			<p>Consider a situation<a id="_idIndexMarker870"/> where a brand-new Pod, with a brand-new, empty<a id="_idIndexMarker871"/> PersistentVolume joins the cluster. In this scenario, the data replication processes will need to copy all of the data via replication from the other members in the MySQL cluster. With large databases, this process could be exceedingly slow. </p>
			<p>For this reason, we have an <code>initContainer</code> instance that loads in data from another MySQL Pod in the StatefulSet, so that the data replication capabilities of MySQL have something to start with. In a case where there is already data in the MySQL Pod, this loading of data does not occur. The <code>[[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0</code> line is the one that checks to see whether there is existing data.</p>
			<p>Once these two <code>initContainer</code> instances have successfully completed their tasks, we have all our MySQL configuration courtesy of the first <code>initContainer</code>, and we have a somewhat recent set of data from another member in the MySQL StatefulSet.</p>
			<p>Now, let's move on to the actual containers in the StatefulSet definition, starting with MySQL itself:</p>
			<pre>      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d</pre>
			<p>As you can see, this <a id="_idIndexMarker872"/>MySQL container setup is fairly basic. In addition<a id="_idIndexMarker873"/> to an environment variable, we mount the previously created configuration. This pod also has some liveness and readiness probe configuration – check the GitHub repository of this book for those.</p>
			<p>Now, let's move on and check out our final container, which will look familiar – it's actually another <a id="_idIndexMarker874"/>instance of Xtra Backup! Let's see how it is configured:</p>
			<pre>- name: xtrabackup
containerPort: 3307
command:
- bash
- "-c"
- |
set -ex
cd /var/lib/mysql if [[ -f xtrabackup_slave_info &amp;&amp; "x$(&lt;xtrabackup_slave_info)" != "x" ]]; thencat xtrabackup_slave_info | sed -E 's/;$//g' &gt; change_master_to.sql.inrm -f xtrabackup_slave_info xtrabackup_binlog_info
elif [[ -f xtrabackup_binlog_info ]]; then[[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
rm -f xtrabackup_binlog_info xtrabackup_slave_info
echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
MASTER_LOG_POS=${BASH_REMATCH[2]}" &gt; change_master_to.sql.in
fi if [[ -f change_master_to.sql.in ]]; then
echo "Waiting for mysqld to be ready (accepting connections)"
until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
echo "Initializing replication from clone position"
mysql -h 127.0.0.1 \
-e "$(&lt;change_master_to.sql.in), \
MASTER_HOST='mysql-0.mysql', \
MASTER_USER='root', \
MASTER_PASSWORD='', \
MASTER_CONNECT_RETRY=10; \
START SLAVE;" || exit 1
mv change_master_to.sql.in change_master_to.sql.orig
fi exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"</pre>
			<p>This container setup is a bit complex, so let's review it section by section. </p>
			<p>We know from our <code>initContainers</code> that Xtra Backup loads in data from another Pod in the StatefulSet in order to get the Pod somewhat ready for replicating, to and from other members in the StatefulSet.</p>
			<p>The Xtra Backup container in this case is the one that actually starts that replication! This container will first <a id="_idIndexMarker875"/>check to see whether the Pod it is running on is <a id="_idIndexMarker876"/>supposed to be a slave Pod in the MySQL cluster. If so, it will start a data replication process from the master.</p>
			<p>Finally, the Xtra Backup container will also open a listener on port <code>3307</code>, which will send a clone of the data in the Pod, if requested. This is the setup that sends clone data to the other Pods in the StatefulSet when they request a clone. Remember that the first <code>initContainer</code> looks at other Pods in the StatefulSet, in order to get a clone. In the end, each Pod in the StatefulSet is able to request clones in addition to running a process that can send data clones to other Pods. </p>
			<p>Finally, to wrap up our spec, let's look at <code>volumeClaimTemplate</code>. This section of the spec also lists volume mounts for the previous container and the volume setup for the Pod (but we've left that out for brevity. Check the GitHub repository of this book for the rest):</p>
			<pre>  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi</pre>
			<p>As you can see, there's nothing especially interesting about the volume setup for the last container or the volume list. However, it's worthwhile to note the <code>volumeClaimTemplates</code> section, because the data will remain the same as long as a Pod restarts at the same ordinal spot. A new Pod added to the cluster will instead start with a blank PersistentVolume, which will trigger the initial data clone.</p>
			<p>All together, these features of StatefulSets, in combination with the correct configuration of Pods and tooling, allow for the easy scaling of a stateful DB on Kubernetes.</p>
			<p>Now that we've talked<a id="_idIndexMarker877"/> about why stateful Kubernetes applications <a id="_idIndexMarker878"/>may use StatefulSets, let's go ahead and implement some to prove it! We'll start with an object storage application.</p>
			<h1 id="_idParaDest-310"><a id="_idTextAnchor327"/>Deploying object storage on Kubernetes</h1>
			<p>Object storage is<a id="_idIndexMarker879"/> different from filesystem or block storage. It presents a higher-level abstraction that encapsulates a file, gives it an identifier, and often includes versioning. The file can then be accessed via its specific identifier.</p>
			<p>The most popular object storage service is probably AWS S3, but Azure Blob Storage and Google Cloud Storage are similar alternatives. In addition, there are several self-hosted object storage technologies that can be run on Kubernetes, which we reviewed in the previous section.</p>
			<p>For this book, we will review the configuration and usage of <strong class="bold">Minio</strong> on Kubernetes. Minio is an object <a id="_idIndexMarker880"/>storage engine that emphasizes high performance and<a id="_idIndexMarker881"/> can be deployed on Kubernetes, in addition to other orchestration technologies such as <strong class="bold">Docker Swarm</strong> and <strong class="bold">Docker Compose</strong>.</p>
			<p>Minio supports Kubernetes deployments using both an operator and a Helm chart. In this book, we will focus on the operator, but for more information on the Helm chart, check out the Minio docs at <a href="https://docs.min.io/docs">https://docs.min.io/docs</a>. Let's get started with the Minio Operator, which will let us review some cool community extensions to kubectl.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor328"/>Installing the Minio Operator</h2>
			<p>Installing the Minio <a id="_idIndexMarker882"/>Operator will be quite different from anything we have done so far. Minio actually provides a <code>kubectl</code> plugin in order to manage the installation and configuration of the operator and Minio as a whole.</p>
			<p>We haven't spoken much about <code>kubectl</code> plugins in this book, but they are a growing part of the Kubernetes ecosystem. <code>kubectl</code> plugins can provide additional functionality in the form of new <code>kubectl</code> commands.</p>
			<p>In order to install the <code>minio</code> kubectl plugin, we use Krew, which is a plugin manager for <code>kubectl</code> that makes it easy to search and add <code>kubectl</code> plugins with a single command.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor329"/>Installing Krew and the Minio kubectl plugin</h2>
			<p>So first, let's install<a id="_idIndexMarker883"/> Krew. The installation process varies depending on<a id="_idIndexMarker884"/> your OS and environment, but for macOS, it looks like the following (check out the Krew docs at <a href="https://krew.sigs.k8s.io/docs">https://krew.sigs.k8s.io/docs</a> for more information):</p>
			<ol>
				<li>First, let's install the Krew CLI tool with the following Terminal commands:<pre><strong class="bold">(</strong>
<strong class="bold">  set -x; cd "$(mktemp -d)" &amp;&amp;</strong>
<strong class="bold">  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz" &amp;&amp;</strong>
<strong class="bold">  tar zxvf krew.tar.gz &amp;&amp;</strong>
<strong class="bold">  KREW=./krew-"$(uname | tr '[:upper:]' '[:lower:]')_$(uname -m | sed -e 's/x86_64/amd64/' -e 's/arm.*$/arm/')" &amp;&amp;</strong>
<strong class="bold">  "$KREW" install krew</strong>
<strong class="bold">)</strong></pre></li>
				<li>Now, we can add Krew to our <code>PATH</code> variable with the following command:<pre><code>kubectl krew</code> commands.</p></li>
				<li>To install the Minio kubectl plugin, you can run the following <code>krew</code> command:<pre><strong class="bold">kubectl krew install minio</strong></pre></li>
			</ol>
			<p>Now, with the Minio <a id="_idIndexMarker885"/>kubectl plugin installed, let's look at getting Minio set up on our cluster<a id="_idIndexMarker886"/>.</p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor330"/>Starting the Minio Operator</h2>
			<p>First off, we need <a id="_idIndexMarker887"/>to actually install the Minio Operator on our cluster. This deployment will control all the Minio tasks that we need to do later:</p>
			<ol>
				<li value="1">We can install the Minio Operator using the following command:<pre><strong class="bold">kubectl minio init</strong></pre><p>This will result in the following output:</p><pre>CustomResourceDefinition tenants.minio.min.io: created
ClusterRole minio-operator-role: created
ServiceAccount minio-operator: created
ClusterRoleBinding minio-operator-binding: created
MinIO Operator Deployment minio-operator: created</pre></li>
				<li>To check whether the Minio Operator is ready to go, let's check on our Pods with the following command:<pre><strong class="bold">kubectl get pods</strong></pre></li>
			</ol>
			<p>You should see the Minio Operator Pod running in the output:</p>
			<pre>NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
default       minio-operator-85ccdcfb6-r8g8b     1/1     Running   0          5m37s</pre>
			<p>We now have the Minio Operator running properly on Kubernetes. Next up, we can create a Minio <a id="_idIndexMarker888"/>tenant.</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor331"/>Creating a Minio tenant</h2>
			<p>The next step is <a id="_idIndexMarker889"/>to create a <strong class="bold">tenant</strong>. Since Minio is a multi-tenant system, each tenant has its own namespace separation for buckets and objects, in addition to separate PersistentVolumes. Additionally, the Minio Operator starts Minio in Distributed Mode with a highly available setup and data replication.</p>
			<p>Before creating our Minio <a id="_idIndexMarker890"/>tenant, we need to install a <strong class="bold">Container Storage Interface</strong> (<strong class="bold">CSI</strong>) driver for Minio. CSI is a standardized way to interface between storage providers and containers – and Kubernetes implements CSI in order to allow third-party storage providers to write their own drivers for seamless integration to Kubernetes. Minio recommends the Direct CSI driver in order to manage PersistentVolumes for Minio.</p>
			<p>To install the Direct CSI driver, we need to run a <code>kubectl apply</code> command with Kustomize. However, the Direct CSI driver installation requires some environment variables to be set in order to create the Direct CSI configuration with the proper configuration, as shown:</p>
			<ol>
				<li value="1">First, let's go ahead and create this environment file based on the Minio recommendations:<pre>DIRECT_CSI_DRIVES=data{1...4}
DIRECT_CSI_DRIVES_DIR=/mnt
KUBELET_DIR_PATH=/var/lib/kubelet</pre><p>As you can see, this environment file determines where the Direct CSI driver will mount volumes.  </p></li>
				<li>Once we've created <code>default.env</code>, let's load these variables into memory using the following command:<pre><strong class="bold">export $(cat default.env)</strong></pre></li>
				<li>Finally, let's install the Direct CSI driver with the following command:<pre><strong class="bold">kubectl apply -k github.com/minio/direct-csi</strong></pre><p>This should result in the following output:</p><pre>kubenamespace/direct-csi created
storageclass.storage.k8s.io/direct.csi.min.io created
serviceaccount/direct-csi-min-io created
clusterrole.rbac.authorization.k8s.io/direct-csi-min-io created
clusterrolebinding.rbac.authorization.k8s.io/direct-csi-min-io created
configmap/direct-csi-config created
secret/direct-csi-min-io created
service/direct-csi-min-io created
deployment.apps/direct-csi-controller-min-io created
daemonset.apps/direct-csi-min-io created
csidriver.storage.k8s.io/direct.csi.min.io created</pre></li>
				<li>Before we go<a id="_idIndexMarker891"/> ahead and create our Minio tenant, let's check to see whether our CSI Pods started up properly. Run the following command to check:<pre><strong class="bold">kubectl get pods –n direct-csi</strong></pre><p>You should see output similar to the following if the CSI Pods have started:</p><pre>NAME                                          READY   STATUS    RESTARTS   AGE
direct-csi-controller-min-io-cd598c4b-hn9ww   2/2     Running   0          9m
direct-csi-controller-min-io-cd598c4b-knvbn   2/2     Running   0          9m
direct-csi-controller-min-io-cd598c4b-tth6q   2/2     Running   0          9m
direct-csi-min-io-4qlt7                       3/3     Running   0          9m
direct-csi-min-io-kt7bw                       3/3     Running   0          9m
direct-csi-min-io-vzdkv                       3/3     Running   0          9m</pre></li>
				<li>Now with our CSI driver installed, let's create our Minio tenant – but first, let's take a look at the YAML that the <code>kubectl minio tenant create</code> command generates:<pre><code>Tenant</code> <code>Tenant</code> CRD. This first part of our spec has two containers specified, a container for the Minio console and one for the Minio <code>server</code> itself. In addition, the <code>replicas</code> value mirrors what we specified in our <code>kubectl minio tenant create</code> command. Finally, it specifies the name of a secret for the Minio <code>console</code>.</p><p>Next, let's look at the<a id="_idIndexMarker894"/> bottom portion of the Tenant CRD:</p><pre> liveness:
    initialDelaySeconds: 10
    periodSeconds: 1
    timeoutSeconds: 1
  mountPath: /export
  requestAutoCert: true
  zones:
  - resources: {}
    servers: 2
    volumeClaimTemplate:
      apiVersion: v1
      kind: persistentvolumeclaims
      metadata:
        creationTimestamp: null
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 256Mi
      status: {}
    volumesPerServer: 2
status:
  availableReplicas: 0
  currentState: ""</pre><p>As you can see, the <code>Tenant</code> resource specifies a number of servers (also specified by the <code>creation</code> command) that matches the number of replicas. It also specifies the <a id="_idIndexMarker895"/>name of the internal Minio Service, as well as a <code>volumeClaimTemplate</code> instance to be used.</p><p>This spec, however, does not work for our purposes, since we are using the Direct CSI. Let's update the <code>zones</code> key with a new <code>volumeClaimTemplate</code> that uses the Direct CSI, as follows (save this file as <code>my-updated-minio-tenant.yaml</code>). Here's just the <code>zones</code> portion of that file, which we updated: </p><pre>zones:
  - resources: {}
    servers: 2
    volumeClaimTemplate:
      metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 256Mi
        storageClassName: direct.csi.min.io</pre></li>
				<li>Let's now go ahead and create our Minio tenant! We can do this using the following command:<pre><strong class="bold">kubectl apply -f my-updated-minio-tenant.yaml</strong></pre></li>
			</ol>
			<p>This should result in the following output:</p>
			<pre>tenant.minio.min.io/my-tenant created
secret/my-tenant-creds-secret created
secret/my-tenant-console-secret created</pre>
			<p>At this point, the Minio Operator will start creating the necessary resources for our new Minio tenant, and after <a id="_idIndexMarker896"/>a couple of minutes, you should see some Pods start up in addition to the operator, which will look similar to the following:</p>
			<div><div><img src="img/B14790_15_001.jpg" alt="Figure 15.1 – Minio Pods output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.1 – Minio Pods output</p>
			<p>We now have our Minio tenant completely up and running! Next, let's take a look at the Minio console to see how our tenant looks.</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor332"/>Accessing the Minio console</h2>
			<p>First, in order to get<a id="_idIndexMarker897"/> the login information for the console, we will need to fetch the content of two keys, which are kept in the autogenerated <code>&lt;TENANT NAME&gt;-console-secret</code> secret.</p>
			<p>To fetch the <code>access</code> key and the <code>secret</code> key (which in our case will be autogenerated) for the console, let's use the two following commands. In our case, we use our <code>my-tenant</code> tenant to get the <code>access</code> key:</p>
			<pre>echo $(kubectl get secret my-tenant-console-secret -o=jsonpath='{.data.CONSOLE_ACCESS_KEY}' | base64 --decode)</pre>
			<p>And to get the <code>secret</code> key, we use this:</p>
			<pre>echo $(kubectl get secret my-tenant-console-secret -o=jsonpath='{.data.CONSOLE_SECRET_KEY}' | base64 --decode)</pre>
			<p>Now, our Minio console will be available on a service, <code>&lt;TENANT NAME&gt;-console</code>. </p>
			<p>Let's access this console using a <code>port-forward</code> command. In our case, this will be as follows:</p>
			<pre>kubectl port-forward service/my-tenant-console 8081:9443</pre>
			<p>Our Minio console will then be available at <code>https://localhost:8081</code> on your browser. You will need to accept the browser security warning since we haven't set up TLS certificates for the console for localhost in this example. Put in the <code>access</code> key and <code>secret</code> key you got from the previous steps to log in!</p>
			<p>Now that we're logged into the console, we can start adding to our Minio tenant. First, let's create a bucket. To do this, click <strong class="bold">Buckets</strong> on the left sidebar, then click the <strong class="bold">Create Bucket</strong> button.</p>
			<p>In the popup, enter the name of the bucket (in our case, we will use <code>my-bucket</code>) and submit the form. You should see a new bucket in the list – see the following screenshot for an example:</p>
			<div><div><img src="img/B14790_15_002.jpg" alt="Figure 15.2 – Bucket"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.2 – Bucket</p>
			<p>We now have our distributed Minio setup ready, together with a bucket to upload to. Let's wrap this example up by uploading a file to our brand-new object storage system!</p>
			<p>We're going to do this upload using the Minio CLI, which makes the process of interacting with S3-compatible storage such as Minio much easier. Instead of using the Minio CLI from our<a id="_idIndexMarker898"/> local machine, we will run a container image preloaded with the Minio CLI from within Kubernetes, since the TLS setup will only work when accessing Minio from within the cluster.</p>
			<p>First, we'll need to fetch the Minio <code>access</code> key and <code>secret</code>, which are different from the console <code>access</code> key and <code>secret</code> we fetched earlier. To get these keys, run the following console commands (in our case, our tenant is <code>my-tenant</code>). First, get the <code>access</code> key:</p>
			<pre>echo $(kubectl get secret my-tenant-creds-secret -o=jsonpath='{.data.accesskey}' | base64 --decode)</pre>
			<p>Then, get the <code>secret</code> key:</p>
			<pre>echo $(kubectl get secret my-tenant-creds-secret -o=jsonpath='{.data.secretkey}' | base64 --decode)</pre>
			<p>Now, let's start up that pod with the Minio CLI. To do this, let's use this Pod spec:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">minio-mc-pod.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: minio-mc
spec:
  containers:
  - name: mc
    image: minio/mc
    command: ["/bin/sh", "-c", "sleep 10000000s"]
  restartPolicy: OnFailure</pre>
			<p>Create this Pod using this:</p>
			<pre>kubectl apply -f minio-mc-pod.yaml</pre>
			<p>Then, to <code>exec</code> into this <code>minio-mc</code> Pod, we run the usual <code>exec</code> command:</p>
			<pre>Kubectl exec -it minio-mc -- sh</pre>
			<p>Now, let's configure <a id="_idIndexMarker899"/>access for our newly created Minio distributed cluster in the Minio CLI. We can do this with the following command (the <code>--insecure</code> flag is required in this config):</p>
			<pre>mc config host add my-minio https://&lt;MINIO TENANT POD IP&gt;:9000 --insecure</pre>
			<p>The Pod IP for this command can be the IP for either of our tenant Minio Pods – in our case, these are <code>my-tenant-zone-0-0</code> and <code>my-tenant-zone-0-1</code>. Once you run this command, you will be prompted for the access key and secret key. Enter them, and you will see a confirmation message if successful, which will look like this:</p>
			<pre>Added `my-minio` successfully.</pre>
			<p>Now, to test that the CLI configuration is working, we can create another test bucket using the following command:</p>
			<pre>mc mb my-minio/my-bucket-2 --insecure</pre>
			<p>This should result in the following output:</p>
			<pre>Bucket created successfully `my-minio/my-bucket-2`.</pre>
			<p>As a final test of our setup, let's upload a file to our Minio bucket!</p>
			<p>First, still on the <code>minio-mc</code> Pod, create a text file named <code>test.txt</code>. Fill the file with whatever text you'd like.</p>
			<p>Now, let's upload it to our recently created bucket using this:</p>
			<pre>mc mv test.txt my-minio/my-bucket-2 --insecure</pre>
			<p>You should see a loading bar with the upload, which should end with the entire file size as uploaded.</p>
			<p>As one last check, go to the <strong class="bold">Dashboard</strong> page on the Minio console and see whether the object shows up, as shown in the following figure:</p>
			<div><div><img src="img/B14790_15_003.jpg" alt="Figure 15.3 – Dashboard"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.3 – Dashboard</p>
			<p>As you can see, our file<a id="_idIndexMarker900"/> was successfully uploaded!</p>
			<p>That's it for Minio – there is a lot more you can do in terms of configuration, but that is outside the scope of this book. Check the docs at <a href="https://docs.min.io/">https://docs.min.io/</a> for more information.</p>
			<p>Next up, let's look at running DBs on Kubernetes.</p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor333"/>Running DBs on Kubernetes</h1>
			<p>Now that we've taken <a id="_idIndexMarker901"/>a look at object storage workloads on<a id="_idIndexMarker902"/> Kubernetes, we can move on to databases. As we've discussed previously in this chapter and elsewhere in the book, many databases support running on Kubernetes, with varying levels of maturity.</p>
			<p>First off, there are several legacy and existing DB engines that support deploying to Kubernetes. Often, these engines will have supported Helm charts or operators. For instance, SQL databases such as PostgreSQL and MySQL have Helm charts and operators supported by various different organizations. NoSQL databases such as MongoDB also have supported ways to deploy to Kubernetes.</p>
			<p>In addition to these previously existing database engines, container orchestrators such as Kubernetes have lead to the creation of a new category – the <strong class="bold">NewSQL</strong> database.</p>
			<p>These databases offer the incredible scalability of NoSQL databases in addition to SQL-compliant APIs. They can be thought of as a way to easily scale SQL on Kubernetes (and other orchestrators). CockroachDB is a popular choice here, as is <strong class="bold">Vitess</strong>, which isn't so much a replacement <a id="_idIndexMarker903"/>NewSQL database as it is a way to easily scale the<a id="_idIndexMarker904"/> MySQL engine.</p>
			<p>In this chapter, we will focus on deploying CockroachDB, which is a modern NewSQL database built for distributed environments and perfect for Kubernetes. </p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor334"/>Running CockroachDB on Kubernetes</h2>
			<p>To run<a id="_idIndexMarker905"/> CockroachDB on our cluster, we will use the official<a id="_idIndexMarker906"/> CockroachDB Helm chart:</p>
			<ol>
				<li value="1">The first thing we need to do is to add the CockroachDB Helm chart repository, using the following command:<pre><strong class="bold">helm repo add cockroachdb https://charts.cockroachdb.com/</strong></pre><p>This should result in the following output:</p><pre><strong class="bold">"cockroachdb" has been added to your repositories</strong></pre></li>
				<li>Before we install the chart, let's create a custom <code>values.yaml</code> file in order to tweak some of the default settings for CockroachDB. Our file for this demo looks like the following:<pre>storage:
  persistentVolume:
    size: 2Gi
statefulset:
  resources:
    limits:
      memory: "1Gi"
    requests:
      memory: "1Gi"
conf:
  cache: "256Mi"
  max-sql-memory: "256Mi"</pre><p>As you can see, we are specifying a PersistentVolume size of <code>2</code> GB, Pod memory limits and requests of <code>1</code> GB, and the contents of a configuration file for CockroachDB. This configuration file includes settings for <code>cache</code> and max <code>memory</code>, which<a id="_idIndexMarker907"/> are set to 25% of the size of the<a id="_idIndexMarker908"/> memory limits at <code>256</code> MB. This ratio is a CockroachDB best practice. Keep in mind that these are not all production-ready settings, but they will work for our demo.</p></li>
				<li>At this point, let's go ahead and create our CockroachDB cluster using the following Helm command:<pre><strong class="bold">helm install cdb --values cockroach-db-values.yaml cockroachdb/cockroachdb</strong></pre><p>If successful, you will see a lengthy deploy message from Helm, which we will not reproduce here. Let's check to see exactly what was deployed on our cluster using the following command:</p><pre><strong class="bold">kubectl get po</strong> </pre><p>You will see output similar to the following:</p><pre>NAMESPACE     NAME                                          READY   STATUS      RESTARTS   AGE
default       cdb-cockroachdb-0                             0/1     Running     0          57s
default       cdb-cockroachdb-1                             0/1     Running     0          56s
default       cdb-cockroachdb-2                             1/1     Running     0          56s
default       cdb-cockroachdb-init-8p2s2                    0/1     Completed   0          57s</pre><p>As you can see, we have three Pods in a StatefulSet in addition to a setup Pod that was used for some initialization tasks.</p></li>
				<li>In order to check<a id="_idIndexMarker909"/> to see whether our cluster is<a id="_idIndexMarker910"/> functional, we can use a command that is handily given to us in the CockroachDB Helm chart output (it will vary depending on your Helm release name):<pre>kubectl run -it --rm cockroach-client \
        --image=cockroachdb/cockroach \
        --restart=Never \
        --command -- \
        ./cockroach sql --insecure --host=cdb-cockroachdb-public.default</pre></li>
			</ol>
			<p>If successful, a console will be opened with a prompt similar to the following:</p>
			<pre>root@cdb-cockroachdb-public.default:26257/defaultdb&gt;</pre>
			<p>In the next section, we will test CockroachDB with SQL.</p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor335"/>Testing CockroachDB with SQL</h2>
			<p>Now, we can<a id="_idIndexMarker911"/> run SQL commands to our new CockroachDB <a id="_idIndexMarker912"/>database! </p>
			<ol>
				<li value="1">First, let's create a database with the following command:<pre><strong class="bold">CREATE DATABASE mydb;</strong></pre></li>
				<li>Next, let's create a simple table:<pre>CREATE TABLE mydb.users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    first_name STRING,
    last_name STRING,
    email STRING
 );</pre></li>
				<li>Then, let's<a id="_idIndexMarker913"/> add<a id="_idIndexMarker914"/> some data with this command:<pre><strong class="bold">INSERT INTO mydb.users (first_name, last_name, email)</strong>
<strong class="bold">  VALUES</strong>
<strong class="bold">      ('John', 'Smith', 'jsmith@fake.com');</strong></pre></li>
				<li>Finally, let's confirm the data using this:<pre><strong class="bold">SELECT * FROM mydb.users;</strong></pre></li>
			</ol>
			<p>That should give you the following output:</p>
			<pre>                  id                  | first_name | last_name |      email
---------------------------------------+------------+-----------+------------------
  e6fa342f-8fe5-47ad-adde-e543833ffd28 | John       | Smith     | jsmith@fake.com
(1 row)</pre>
			<p>Success!</p>
			<p>As you can see, we have a fully functional distributed SQL database. Let's move on to the final stateful workload type that we will review: messaging.</p>
			<h1 id="_idParaDest-319"><a id="_idTextAnchor336"/>Implementing messaging and queues on Kubernetes</h1>
			<p>For messaging, we<a id="_idIndexMarker915"/> will be implementing RabbitMQ, an open <a id="_idIndexMarker916"/>source message queue system that supports Kubernetes. Messaging systems are typically used in applications to decouple various components of the application in order to support the scale and throughput, as well as asynchronous patterns such as retries and service worker fleets. For instance, instead of one service calling another service directly, a service could place a message onto a persistent message queue, at which point it would be picked up by a worker container that is listening to the queue. This allows for easy horizontal scaling and greater tolerance of entire component downtime as compared to a load balancing approach.</p>
			<p>RabbitMQ is one of many options for message queues. As we mentioned in the first section of the chapter, RabbitMQ is an industry-standard option for message queues, not necessarily a queue system built for Kubernetes specifically. However, it's still a great choice and very easy to deploy, as we will see shortly.</p>
			<p>Let's start with implementing RabbitMQ on Kubernetes!</p>
			<h2 id="_idParaDest-320"><a id="_idTextAnchor337"/>Deploying RabbitMQ on Kubernetes</h2>
			<p>Installing <a id="_idIndexMarker917"/>RabbitMQ on Kubernetes can be easily done via an <a id="_idIndexMarker918"/>operator or via a Helm chart. For the purposes of this tutorial, we will use the Helm chart:</p>
			<ol>
				<li value="1">First, let's add the proper <code>helm</code> repository (provided by <strong class="bold">Bitnami</strong>):<pre><strong class="bold">helm repo add bitnami https://charts.bitnami.com/bitnami</strong></pre></li>
				<li>Next, let's create a custom values file to tweak some parameters:<pre>auth:
  user: user
  password: test123
persistence:
  enabled: false</pre><p>As you can see, in this case, we are disabling persistence, which is great for a quick demo. </p></li>
				<li>Then, RabbitMQ can easily be installed on the cluster using the following command:<pre><strong class="bold">helm install rabbitmq bitnami/rabbitmq --values values-rabbitmq.yaml</strong></pre><p>Once successful, you will see a confirmation message from Helm. The RabbitMQ Helm chart <a id="_idIndexMarker919"/>also includes a management UI, so<a id="_idIndexMarker920"/> let's use that to validate that our installation worked.</p></li>
				<li>First, let's start a port forward to the <code>rabbitmq</code> service:<pre><code>http://localhost:15672</code>. It will look like the following:</p><div><img src="img/B14790_15_004.jpg" alt="Figure 15.4 – RabbitMQ management console login"/></div><p class="figure-caption">Figure 15.4 – RabbitMQ management console login</p></li>
				<li>Now, we should be able to log in to the dashboard using the username and password specified in the values file. Upon login, you will see the RabbitMQ dashboard main view.<p>Importantly, you will see a list of the nodes in your RabbitMQ cluster. In our case, we only have a single node, which will display as follows:</p><div><img src="img/B14790_15_005.jpg" alt="Figure 15.5 – RabbitMQ management console node item"/></div><p class="figure-caption">Figure 15.5 – RabbitMQ management console node item</p><p>For each node, you can see the name and some metadata, including memory, uptime, and more.</p></li>
				<li>In order to add a new queue navigate to <strong class="bold">Queues</strong> on the top bar, click <strong class="bold">Add a new queue</strong> toward<a id="_idIndexMarker921"/> the bottom of the screen. Fill out the <a id="_idIndexMarker922"/>form as follows, then click <strong class="bold">Add queue</strong>:<div><img src="img/B14790_15_006.jpg" alt="Figure 15.6 – RabbitMQ management console queue creation"/></div><p class="figure-caption">Figure 15.6 – RabbitMQ management console queue creation</p><p>If successful, the screen should refresh with your new queue added to the list. This means our RabbitMQ setup is working properly!</p></li>
				<li>Finally, now that we have a queue, we can publish a message to it. To do this, click on your newly created queue on the <strong class="bold">Queues</strong> page, then click <strong class="bold">Publish Message</strong>.</li>
				<li>Write any text in the <strong class="bold">Payload</strong> text box and click <strong class="bold">Publish Message</strong>. You should see a confirmation popup telling you that your message has been published successfully, and the screen should refresh, showing your message on the queue, as shown in the following figure:<div><img src="img/B14790_15_007.jpg" alt="Figure 15.7 – RabbitMQ management console queue status"/></div><p class="figure-caption">Figure 15.7 – RabbitMQ management console queue status</p></li>
				<li>Finally, to emulate<a id="_idIndexMarker923"/> fetching messages from<a id="_idIndexMarker924"/> the queue, click on <strong class="bold">Get messages</strong> near the bottom of the page, which should expand to show a new section, and then click the <strong class="bold">Get Message(s)</strong> button. You should see an output of the message you sent, proving that the queue system works!</li>
			</ol>
			<h1 id="_idParaDest-321"><a id="_idTextAnchor338"/>Summary</h1>
			<p>In this chapter, we learned about running stateful workloads on Kubernetes. First, we reviewed a high-level overview of some of the types of stateful workloads and some examples of each. Then, we moved on to actually deploying one of these workloads – an object storage system – on Kubernetes. Next, we did the same with a NewSQL database, CockroachDB, showing you how to easily deploy a CockroachDB cluster on Kubernetes.</p>
			<p>Finally, we showed you how to deploy the RabbitMQ message queue on Kubernetes using a Helm chart. The skills you used in this chapter will help you deploy and use popular stateful application patterns on Kubernetes.</p>
			<p>If you've made it this far, thanks for sticking with us through all 15 chapters of this book! I hope that you have learned how to use a broad spectrum of Kubernetes functionality and that you now have all the tools you need in order to build and deploy complex applications on Kubernetes.</p>
			<h1 id="_idParaDest-322"><a id="_idTextAnchor339"/>Questions</h1>
			<ol>
				<li value="1">What cloud storage offering is Minio's API compatible with?</li>
				<li>What are the benefits of a StatefulSet for a distributed database?</li>
				<li>In your words, what makes stateful applications difficult to run on Kubernetes?</li>
			</ol>
			<h1 id="_idParaDest-323"><a id="_idTextAnchor340"/>Further reading</h1>
			<ul>
				<li>Minio Quickstart Documentation: <a href="https://docs.min.io/docs/minio-quickstart-guide.html">https://docs.min.io/docs/minio-quickstart-guide.html</a> </li>
				<li>CockroachDB Kubernetes Guide: <a href="https://www.cockroachlabs.com/docs/v20.2/orchestrate-a-local-cluster-with-kubernetes">https://www.cockroachlabs.com/docs/v20.2/orchestrate-a-local-cluster-with-kubernetes</a></li>
			</ul>
		</div>
	</body></html>