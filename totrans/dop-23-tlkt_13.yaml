- en: Managing Resources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理资源
- en: Without an indication how much CPU and memory a container needs, Kubernetes
    has no other option than to treat all containers equally. That often produces
    a very uneven distribution of resource usage. Asking Kubernetes to schedule containers
    without resource specifications is like entering a taxi driven by a blind person.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有指明容器需要多少 CPU 和内存，Kubernetes 只能将所有容器视为平等。这往往导致资源使用分配极不均衡。如果没有资源规格就请求 Kubernetes
    调度容器，就像进入一辆由盲人驾驶的出租车一样。
- en: We have come a long way, from humble beginnings, towards understanding many
    of the essential Kubernetes object types and principles. One of the most important
    things we're missing is resource management. Kubernetes was blindly scheduling
    the applications we deployed so far. We never gave it any indication how much
    resources we expect those applications to use, nor established any limits. Without
    them, Kubernetes was carrying out its tasks in a very myopic fashion. Kubernetes
    could see a lot, but not enough. We'll change that soon. We'll give Kubernetes
    a pair of glasses that will provide it a much better vision.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走了很长一段路，从最初的起点开始，逐步了解了许多 Kubernetes 的基本对象类型和原则。现在我们最缺少的就是资源管理。到目前为止，Kubernetes
    一直在盲目地调度我们部署的应用程序。我们从未告诉它这些应用程序需要多少资源，也没有设定任何限制。没有这些，Kubernetes 执行任务的方式就显得非常短视。Kubernetes
    看到的很多，但远远不够。我们很快就会改变这一点，给 Kubernetes 配上一副眼镜，让它有更清晰的视野。
- en: Once we learn how to define resources, we'll go further and make sure that certain
    limitations are set, that some defaults are determined, and that there are quotas
    that will prevent applications from overloading the cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们学会了如何定义资源，我们将进一步确保设定一些限制，确定默认值，并设置配额，以防止应用程序超载集群。
- en: This chapter is the last piece of the puzzle. Once we solve it, we'll be ready
    to start thinking about using Kubernetes in production. You won't know everything
    you should know about operating Kubernetes. No one does. But, you will know just
    enough to get you going in the right direction.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是拼图的最后一块。解决了这一部分，我们就可以开始考虑在生产环境中使用 Kubernetes 了。你可能还不能掌握操作 Kubernetes 所需的所有知识，实际上没有人能做到这一点。但你会知道足够多的内容，能够把你引导到正确的方向。
- en: Creating a cluster
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: We'll go through almost the same routine as we did in the previous chapters.
    We'll enter the directory where we cloned the `vfarcic/k8s-specs` repository,
    pull the latest code, start a Minikube cluster, and so on and so forth. The only
    new thing we'll do this time is to enable one more addon. We'll add Heapster to
    the cluster. It's too soon to explain what it does and why we'll need it. That
    will come later. For now, just remember that there will soon be something in your
    cluster called Heapster. If you do not already know what it is, consider this
    a teaser meant to build suspense.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行几乎与前几章相同的操作。我们会进入我们克隆的 `vfarcic/k8s-specs` 仓库所在的目录，拉取最新代码，启动 Minikube 集群，等等。这次唯一不同的地方是我们将启用一个额外的插件——Heapster。现在解释它的功能和为什么需要它还为时过早，稍后会详细说明。暂时只需记住，你的集群里很快会有一个叫
    Heapster 的东西。如果你还不知道它是什么，认为这是一个悬念引导，稍后会有更多解释。
- en: All the commands from this chapter are available in the [`13-resource.sh`](https://gist.github.com/cc8c44e1e84446dccde3d377c131a5cd)
    ([https://gist.github.com/vfarcic/cc8c44e1e84446dccde3d377c131a5cd](https://gist.github.com/vfarcic/cc8c44e1e84446dccde3d377c131a5cd))
    Gist.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 [`13-resource.sh`](https://gist.github.com/cc8c44e1e84446dccde3d377c131a5cd)
    ([https://gist.github.com/vfarcic/cc8c44e1e84446dccde3d377c131a5cd](https://gist.github.com/vfarcic/cc8c44e1e84446dccde3d377c131a5cd))
    Gist 中找到。
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that the latest code is pulled, the cluster is running, and the add-ons
    are enabled, we can proceed and explore how to define container memory and CPU
    resources.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经拉取了最新的代码，集群也在运行，插件已启用，我们可以继续并探索如何定义容器的内存和 CPU 资源。
- en: Defining container memory and CPU resources
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义容器的内存和 CPU 资源
- en: So far, we did not specify how much memory and CPU containers should use, nor
    what their limits should be. If we do that, Kubernetes' scheduler will have a
    much better idea about the needs of those containers, and it'll make much better
    decisions on which nodes to place the Pods and what to do if they start "misbehaving".
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有指定容器应该使用多少内存和 CPU，也没有设定它们的限制。如果我们这样做，Kubernetes 的调度器将能更好地了解这些容器的需求，从而做出更好的决策，决定将
    Pods 放置在哪些节点上，以及当容器“行为异常”时应该如何处理。
- en: 'Let''s take a look at a modified `go-demo-2` definition:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下修改后的 `go-demo-2` 定义：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The specification is almost the same as those we used before. The only new entries
    are in the `resources` section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 规范几乎与我们之前使用的相同。唯一新增的条目是在`resources`部分。
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We specified `limits` and `requests` entries in the `resources` section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`resources`部分指定了`limits`和`requests`条目。
- en: CPU resources are measured in `cpu` units. The exact meaning of a `cpu` unit
    depends on where we host our cluster. If servers are virtualized, one `cpu` unit
    is equivalent to one **virtualized processor** (**vCPU**). When running on bare-metal
    with Hyperthreading, one `cpu` equals one Hyperthread. For the sake of simplification,
    we'll assume that one `cpu` resource is one CPU processor (even though that is
    not entirely true).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CPU资源以`cpu`单位进行度量。`cpu`单位的确切含义取决于我们托管集群的位置。如果服务器是虚拟化的，那么一个`cpu`单位相当于一个**虚拟化处理器**（**vCPU**）。当在裸机上运行且启用了超线程时，一个`cpu`等于一个超线程。为了简化，我们假设一个`cpu`资源等于一个CPU处理器（尽管这并不完全准确）。
- en: If one container is set to use `two` CPU, and the other is set to `one` CPU,
    the later is guaranteed half as much processing power.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个容器设置为使用`两个`CPU，而另一个设置为使用`一个`CPU，则后者的处理能力将仅为前者的一半。
- en: CPU values can be fractioned. In our example, the `db` container has the CPU
    requests set to `0.5` which is equivalent to half CPU. The same value could be
    expressed as `500m`, which translates to five hundred millicpu. If you take another
    look at the CPU specs of the `api` container, you'll see that its CPU limit is
    set to `400m` and the requests to `200m`. They are equivalent to `0.4` and `0.2`
    CPUs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CPU值可以被分割。在我们的示例中，`db`容器的CPU请求设置为`0.5`，这相当于半个CPU。这个值也可以表示为`500m`，即五百毫CPU。如果你再看看`api`容器的CPU规格，你会看到它的CPU限制设置为`400m`，请求为`200m`，它们分别等于`0.4`和`0.2`个CPU。
- en: Memory resources follow a similar pattern as CPU. The significant difference
    is in the units. Memory can be expressed as **K** (**kilobyte**), **M** (**Megabyte**),
    **G** (**Gigabyte**), **T** (**Terabyte**), **P** (**Petabyte**), and **E** (**Exabyte**).
    We can also use the power-of-two equivalents `Ki`, `Mi`, `Gi`, `Ti`, `Pi`, and
    `Ei`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 内存资源的模式与CPU类似。主要区别在于单位。内存可以表示为**K**（**千字节**）、**M**（**兆字节**）、**G**（**千兆字节**）、**T**（**太字节**）、**P**（**拍字节**）和**E**（**艾字节**）。我们也可以使用二的幂次方单位`Ki`、`Mi`、`Gi`、`Ti`、`Pi`和`Ei`。
- en: If we go back to the `go-demo-2-random.yml` definition, we'll see that the `db`
    container has the limit set to **200Mi** (**two hundred megabytes**) and the requests
    to **100Mi** (**one hundred megabytes**).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到`go-demo-2-random.yml`定义文件，我们会看到`db`容器的限制设置为**200Mi**（**两百兆字节**），请求设置为**100Mi**（**一百兆字节**）。
- en: We have already mentioned `limits` and `requests` quite a few times and yet
    we have not explained what each of them mean.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过`limits`和`requests`好几次，但仍然没有解释它们各自的含义。
- en: A limit represents the amount of resources that a container should not pass.
    The assumption is that we define limits as upper boundaries which, when reached,
    indicate that something went wrong, as well as a way to guard our resources from
    being overtaken by a single rouge container due to memory leaks or similar problems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 限制表示容器不应超过的资源量。假设我们将限制定义为上限，当达到该上限时，表示出现了问题，并且是一种防止单个恶性容器因内存泄漏或类似问题而占用过多资源的方式。
- en: If a container is restartable, Kubernetes will restart a container that exceeds
    its memory limit. Otherwise, it might terminate it. Bear in mind that a terminated
    container will be recreated if it belongs to a Pod (as all Kubernetes-controlled
    containers do).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器是可重启的，Kubernetes会重启超出内存限制的容器。否则，它可能会终止该容器。请记住，如果容器属于一个Pod（所有Kubernetes管理的容器都是如此），即使它被终止，也会被重新创建。
- en: Unlike memory, CPU limits never result in termination or restarts. Instead,
    a container will not be allowed to consume more than the CPU limit for an extended
    period.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与内存不同，CPU限制永远不会导致容器终止或重启。相反，容器将不会允许在较长时间内消耗超过CPU限制的资源。
- en: Requests represent the expected resource utilization. They are used by Kubernetes
    to decide where to place Pods depending on actual resource utilization of the
    nodes that form the cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请求代表预期的资源利用率。Kubernetes使用它们来决定如何根据节点的实际资源使用情况，将Pods放置在集群中。
- en: If a container exceeds its memory requests, the Pod it resides in might be evicted
    if a node runs out of memory. Such eviction usually results in the Pod being scheduled
    on a different node, as long as there is one with enough available memory. If
    a Pod cannot be scheduled to any of the nodes due to lack of available resources,
    it enters the pending state waiting until resources on one of the nodes are freed,
    or a new node is added to the cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器超出了其内存请求，且节点的内存不足，Pod 可能会被驱逐。这种驱逐通常会导致 Pod 被调度到另一节点，只要该节点有足够的可用内存。如果由于资源不足，Pod
    无法调度到任何节点，它将进入待处理状态，直到某个节点的资源被释放，或者集群中新增一个节点。
- en: 'Simply discussing the theory of `resources` might be confusing if not followed
    by practical examples. Therefore, we''ll move on and create the resources defined
    in the `go-demo-2-random.yml` file:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仅仅讨论 `资源` 的理论，而没有实际的例子，可能会让人感到困惑。因此，我们将继续并创建在 `go-demo-2-random.yml` 文件中定义的资源：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We created the resources and waited until the `go-demo-2-api` Deployment was
    rolled out. The output of the later command should be as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了资源并等待 `go-demo-2-api` Deployment 部署完成。后续命令的输出应该如下所示：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s describe the `go-demo-2-api` Deployment and see its `limits` and `requests`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述 `go-demo-2-api` Deployment，并查看其 `limits` 和 `requests`：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output, limited to the `limits` and the `requests`, is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 限制为 `limits` 和 `requests` 的输出如下：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see that the `limits` and the `requests` correspond to those we defined
    in the `go-demo-2-random.yml` file. That should come as no surprise.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`limits` 和 `requests` 对应于我们在 `go-demo-2-random.yml` 文件中定义的内容。这个结果并不令人惊讶。
- en: Let's describe the nodes that form the cluster (even though there's only one).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述形成集群的节点（尽管这里只有一个）。
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output, limited to the resource-related entries, is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 限制为资源相关条目的输出如下：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `Capacity` represents the overall capacity of a node. In our case, the `minikube`
    node has 2 CPUs, 2GB of RAM, and can run up to one hundred and ten Pods. Those
    are the upper limits imposed by the hardware or, in our case, the size of the
    VM created by Minikube.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`容量` 表示节点的总体容量。在我们的例子中，`minikube` 节点有 2 个 CPU、2GB 的内存，最多可运行 110 个 Pods。这些是硬件或在我们案例中，由
    Minikube 创建的虚拟机大小所强加的上限。'
- en: Further down is the `Non-terminated Pods` section. It lists all the Pods with
    the CPU and memory limits and requests. We can, for example, see that the `go-demo-2-db`
    Pod has the memory limit set to `100Mi`, which is `5%` of the capacity. Similarly,
    we can see that not all Pods have specified resources. For example, the `heapster-snq2f`
    Pod has all the values set to `0`. Kubernetes will not be able to handle those
    Pods appropriately. However, since this is a demo cluster, we'll give the Minikube
    authors a pass and ignore the lack of resource specification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `未终止的 Pods` 部分。它列出了所有具有 CPU 和内存限制及请求的 Pods。例如，我们可以看到 `go-demo-2-db` Pod
    的内存限制设置为 `100Mi`，即占总容量的 `5%`。类似地，我们可以看到并非所有 Pods 都指定了资源。例如，`heapster-snq2f` Pod
    的所有值都设置为 `0`。Kubernetes 无法适当处理这些 Pods。然而，由于这是一个演示集群，我们可以宽容对待，忽略资源未指定的问题。
- en: Finally, the `Allocated resources` section provides summed values from all the
    Pods. We can, for example, see that the CPU limits are `55%`. Limits can be even
    higher than `100%`, and that would not necessarily be a thing to worry about.
    Not all the containers will have memory and CPU bursts over the requested values.
    Even if that happens, Kubernetes will know what to do.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`已分配资源` 部分提供了所有 Pods 的汇总值。例如，我们可以看到 CPU 限制为 `55%`。限制值甚至可以超过 `100%`，这并不一定是需要担心的事情。并非所有容器都会超过请求的内存和
    CPU 值。即使发生这种情况，Kubernetes 也会知道如何处理。
- en: What truly matters is that the total amount of requested memory and CPU is within
    the limits of the capacity. That, however, leads us to an interesting question.
    What is the basis for the resources we defined so far?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 真正重要的是，请求的内存和 CPU 总量在容量的限制范围内。然而，这也引出了一个有趣的问题。我们到目前为止定义资源的依据是什么？
- en: Measuring actual memory and CPU consumption
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量实际的内存和 CPU 消耗
- en: How did we come up with the current memory and CPU values? Why did we set the
    memory of the MongoDB to `100Mi`? Why not `50Mi` or `1Gi`? It is embarrassing
    to admit that the values we have right now are random. I guessed that the containers
    based on the `vfarcic/go-demo-2` image require less resources than Mongo database,
    so their values are comparatively smaller. That was the only criteria I used to
    define the resources.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何得出当前内存和CPU值的？为什么将MongoDB的内存设置为`100Mi`？为什么不是`50Mi`或`1Gi`？现在承认我们有的值是随机的是令人尴尬的。我猜测基于`vfarcic/go-demo-2`镜像的容器需要比Mongo数据库更少的资源，因此它们的值相对较小。这是我定义资源的唯一标准。
- en: Before you frown upon my decision to put random values for resources, you should
    know that we do not have any metrics to back us up. Anybody's guess is as good
    as mine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在您对我为资源放置随机值感到不满之前，您应该知道我们没有任何支持我们的指标。任何人的猜测与我的一样好。
- en: The only way to truly know how much memory and CPU an application uses is by
    retrieving metrics. We'll use Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster))
    for that purpose.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 了解一个应用程序使用多少内存和CPU的唯一方法是检索指标。我们将使用Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster))
    来实现这一目的。
- en: Heapster collects and interprets various signals like compute resource usage,
    lifecycle events, and so on. In our case, we're interested only in CPU and memory
    consumption of the containers we're running in our cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster收集和解释各种信号，如计算资源使用情况，生命周期事件等。在我们的情况下，我们只关注我们集群中正在运行的容器的CPU和内存消耗。
- en: When we created the cluster, we enabled the `heapster` addon and Minikube deployed
    it as a system application. Not only that, but it also deployed InfluxDB ([https://github.com/influxdata/influxdb](https://github.com/influxdata/influxdb))
    and Grafana ([https://grafana.com/](https://grafana.com/)). The former is the
    database where Heapster stores data and the latter can be used to visualize it
    through dashboards.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群时，我们启用了`heapster`插件，并且Minikube将其部署为系统应用程序。不仅如此，它还部署了InfluxDB ([https://github.com/influxdata/influxdb](https://github.com/influxdata/influxdb))
    和Grafana ([https://grafana.com/](https://grafana.com/))。前者是Heapster存储数据的数据库，后者可以通过仪表板可视化数据。
- en: You might be inclined to think that Heapster, InfluxDB, and Grafana might be
    the solution for your monitoring needs. I advise against such a decision. We're
    using Heapster only because it's readily available as a Minikube addon. The idea
    to develop Heapster as a tool for monitoring needs is mostly abandoned. Its primary
    focus is to serve as an internal tool required for some of the Kubernetes features.
    Instead, I'd suggest a combination of Prometheus ([https://prometheus.io/](https://prometheus.io/))
    combined with the Kubernetes API as the source of metrics and Alertmanager ([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))
    for your alerting needs. However, those tools are not in the scope of this chapter,
    so you might need to educate yourself from their documentation, or wait until
    the sequel to this book is published (the tentative name is *Advanced Kubernetes*).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能倾向于认为Heapster、InfluxDB和Grafana可能是您监控需求的解决方案。我建议不要做出这样的决定。我们仅使用Heapster，因为它作为Minikube插件readily可用。将Heapster开发为监控工具的想法在很大程度上已被放弃。它的主要重点是作为一些Kubernetes特性所需的内部工具。相反，我建议结合使用Prometheus
    ([https://prometheus.io/](https://prometheus.io/)) 和 Kubernetes API 作为指标的来源，以及Alertmanager
    ([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))
    作为您的警报需求。然而，这些工具不在本章的范围内，所以您可能需要从它们的文档中学习，或者等待本书的续集出版（暂定名为*高级Kubernetes*）。
- en: Use Heapster only as a quick-and-dirty way to retrieve metrics. Explore the
    combination of Prometheus and Alertmanager for your monitoring and alerting needs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用Heapster作为快速检索指标的一种方法。探索Prometheus和Alertmanager的组合，以满足您的监控和警报需求。
- en: Now that we clarified what Heapster is good for, as well as what it isn't, we
    can proceed and confirm that it is indeed running inside our cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们澄清了Heapster的作用及其非作用，我们可以继续确认它确实在我们的集群内运行。
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the `heapster` and `influxdb-grafana` Pods are running.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，`heapster` 和 `influxdb-grafana` Pods 正在运行。
- en: 'We''ll explore Heapster just enough to retrieve the data we need. For that,
    we''ll need access to its API. However, Minikube didn''t expose its port so that''ll
    be the first thing we''ll do:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅探索足够多的Heapster来检索我们需要的数据。为此，我们需要访问其API。然而，Minikube没有暴露其端口，这将是我们要做的第一件事：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll need to find out which `NodePort` was created for us. To do that, we
    need to get familiar with the JSON definition of the service:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找出为我们创建了哪个`NodePort`。为此，我们需要熟悉服务的JSON定义：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are looking for the `nodePort` entry inside the `spec.ports` array. The
    command that retrieves it and assigns the output to the `PORT` variable is as
    follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找`spec.ports`数组中的`nodePort`条目。检索并将其输出分配给`PORT`变量的命令如下：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We used the `jsonpath` output to retrieve only `nodePort` of the first (and
    the only) entry of the `spec.ports` array.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`jsonpath`输出仅检索`spec.ports`数组中的第一个（也是唯一的）条目的`nodePort`。
- en: Let's try a very simple query of the Heapster API.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一次非常简单的Heapster API查询。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of the `curl` request is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl`请求的输出如下：'
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We don't really need Heapster to retrieve the list of Pods. What we do need
    are metrics of one of the Pods. For that, we need it's name.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上不需要Heapster来检索Pod列表。我们真正需要的是其中一个Pod的指标。为此，我们需要它的名称。
- en: We'll use a similar command we used to retrieve Heapster's service port.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个类似的命令来检索Heapster的服务端口。
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We retrieved all the Pods with the labels `service=go-demo-2` and `type=db`,
    and formatted the output so that `metadata.name` from the first item is retrieved.
    The value is stored as the `DB_POD_NAME` variable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索了所有标签为`service=go-demo-2`和`type=db`的Pod，并将输出格式化，以便从第一个项中检索`metadata.name`。该值被存储为`DB_POD_NAME`变量。
- en: Now we can take a look at the available metrics of the `db` container inside
    the Pod.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以查看Pod中`db`容器的可用指标。
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, most of the available metrics are related to memory and CPU.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，大多数可用的指标与内存和CPU相关。
- en: Let's see whether memory usage indeed corresponds with the memory resources
    we defined for the `go-demo-2-db` Deployment. As a reminder, we set memory request
    to `100Mi` and memory limit to `200Mi`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看内存使用是否确实与我们为`go-demo-2-db`部署定义的内存资源相匹配。提醒一下，我们将内存请求设置为`100Mi`，内存限制设置为`200Mi`。
- en: A request that retrieves memory usage of the `db` container is as follows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 检索`db`容器内存使用情况的请求如下：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output, limited only to a few entries, is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于几个条目，如下所示：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can see that memory usage is somewhere around 38 megabytes. That's quite
    a big difference from `100Mi` we set. Sure, this service is not under real production
    load but, since we're simulating a "real" cluster, we'll pretend that `38Mi` is
    indeed memory usage under "real" conditions. That means that we overestimated
    the requests by assigning a value almost three times larger than the actual usage.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到内存使用量大约是38MB。与我们设置的`100Mi`相比，这是一个相当大的差距。当然，这项服务并没有承受真实的生产负载，但由于我们正在模拟一个“真实”的集群，我们假装`38Mi`确实是在“真实”条件下的内存使用情况。这意味着我们通过分配一个几乎是实际使用量三倍的值，过高地估计了请求。
- en: How about CPU? Did we make such a colossal mistake with it as well? As a reminder,
    we set the CPU request to `0.3` and the limit to `0.5`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那CPU怎么样呢？我们在这方面也犯了如此巨大的错误吗？提醒一下，我们将CPU请求设置为`0.3`，限制为`0.5`。
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output, limited to only a few entries, is as follows.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于几个条目，如下所示。
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we can see, the CPU usage is around `5m` or `0.005` CPU. We, again, made
    a huge mistake with resource specification. Our value is around sixty times higher.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，CPU使用量大约为`5m`或`0.005` CPU。我们再次在资源规范上犯了一个巨大的错误。我们的值大约高出六十倍。
- en: Such deviations between our expectations (resource requests and limits) and
    the actual usage can lead to very unbalanced scheduling with undesirable effects.
    We'll correct the resources soon. For now, we'll explore what happens if the amount
    of resources is below the actual usage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望（资源请求和限制）与实际使用之间的这种偏差可能会导致调度极度不平衡，产生不良后果。我们将很快修正资源配置。现在，我们将探讨一下如果资源量低于实际使用情况会发生什么。
- en: Exploring the effects of discrepancies between resource specifications and resource
    usage
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索资源规范与资源使用之间差异的影响
- en: 'Let''s take a look at a slightly modified version of the `go-demo-2` definition:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看稍微修改过的`go-demo-2`定义：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When compared with the previous definition, the difference is only in `resources`
    of the `db` container in the `go-demo-2-db` Deployment.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的定义相比，差异仅在于`go-demo-2-db`部署中`db`容器的`resources`。
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于相关部分，如下所示：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The memory limit is set to `10Mi` and the request to `5Mi`. Since we already
    know from Heapster's data that MongoDB requires around `38Mi`, memory resources
    are, this time, much lower than the actual usage.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 内存限制设置为`10Mi`，请求为`5Mi`。由于我们已经从Heapster的数据中知道MongoDB大约需要`38Mi`，这次内存资源远低于实际使用量。
- en: 'Let''s see what will happen when we apply the new configuration:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当应用新配置时会发生什么：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We applied the new configuration and retrieved the Pods. The output is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了新配置并检索了Pods。输出如下：
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In your case, the status might not be `OOMKilled`. If so, wait for a while longer
    and retrieve the Pods again. The status should eventually change to `CrashLoopBackOff`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的情况下，状态可能不是`OOMKilled`。如果是这样，请再等待一段时间并重新检索Pods。状态最终应更改为`CrashLoopBackOff`。
- en: As you can see, the status of the `go-demo-2-db` Pod is `OOMKilled` (**Out Of
    Memory Killed**). Kubernetes detected that the actual usage is way above the limit
    and it declared the Pod as a candidate for termination. The container was terminated
    shortly afterwards. Kubernetes will recreate the terminated container a while
    later only to discover that the memory usage is still above the limit. And so
    on, and so forth. The loop will continue.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`go-demo-2-db` Pod的状态是`OOMKilled`（**内存溢出终止**）。Kubernetes检测到实际使用量远超限制，且将Pod标记为终止候选者。容器随后被终止。Kubernetes稍后会重新创建已终止的容器，但很快会发现内存使用仍然超出限制。如此反复循环，直到继续下去。
- en: A container can exceed its memory request if the node has enough available memory.
    On the other hand, a container is not allowed to use more memory than the limit.
    When that happens, it becomes a candidate for termination.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果节点有足够的可用内存，容器可以超过其内存请求。另一方面，容器不能使用超过限制的内存。当发生这种情况时，它将成为终止的候选者。
- en: 'Let''s describe the Deployment and see the status of the `db` container:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下部署并查看`db`容器的状态：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output, limited to relevant parts, is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 限制输出的相关部分如下：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can see that the last state of the `db` container is `OOMKilled`. When we
    explore the events, we can see that, so far, the container was restarted eight
    times with the reason `BackOff`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`db`容器的最后状态是`OOMKilled`。当我们查看事件时，可以看到到目前为止，该容器因`BackOff`原因已重启了八次。
- en: 'Let''s explore another possible situation through yet another updated definition:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过另一个更新的定义来探索另一种可能的情况：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Just as before, the change is only in the `resources` of the `go-demo-2-db`
    Deployment. The output, limited to the relevant parts, is as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所述，更改仅影响`go-demo-2-db`部署中的`resources`部分。相关部分的输出如下：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This time, we specified that the requested memory is twice as much as the total
    memory of the node (2GB). The memory limit is even higher.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们指定请求的内存是节点总内存（2GB）的两倍。内存限制甚至更高。
- en: 'Let''s apply the change and observe what happens:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个更改并观察会发生什么：
- en: '[PRE31]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of the latter command is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 后续命令的输出如下：
- en: '[PRE32]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This time, the status of the Pod is `Pending`. Kubernetes could not place it
    anywhere in the cluster and is waiting until the situation changes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，Pod的状态是`Pending`。Kubernetes无法将其放置在集群中的任何地方，并正在等待直到情况有所变化。
- en: Even though memory requests are associated with containers, it often makes sense
    to translate them into Pods requirements. We can say that the requested memory
    of a Pod is the sum of the requests of all the containers that form it. In our
    case, the Pod has only one container, so the requests of the two are equal. The
    same can be said for limits.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管内存请求与容器相关，但通常更有意义的是将它们转化为Pod的需求。我们可以说，Pod的请求内存是构成该Pod的所有容器请求的总和。在我们的案例中，Pod只有一个容器，因此它们的请求是相等的。限制也是如此。
- en: During the scheduling process, Kubernetes sums the requests of a Pod and looks
    for a node that has enough available memory and CPU. If Pod's request cannot be
    satisfied, it is placed in the pending state in the hope that resources will be
    freed on one of the nodes, or that a new server will be added to the cluster.
    Since such a thing will not happen in our case, the Pod created through the `go-demo-2-db`
    Deployment will be pending forever, unless we change the memory request again.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度过程中，Kubernetes会将一个Pod的请求进行求和，并寻找一个有足够可用内存和CPU的节点。如果Pod的请求无法满足，Pod将进入待处理状态，期待某个节点释放资源，或者集群中加入新服务器。由于在我们这种情况下不会发生这样的事情，通过`go-demo-2-db`部署创建的Pod将永远处于待处理状态，除非我们再次更改内存请求。
- en: When Kubernetes cannot find enough free resources to satisfy the resource requests
    of all the containers that form a Pod, it changes its state to `Pending`. Such
    Pods will remain in this state until requested resources become available.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Kubernetes 无法找到足够的空闲资源来满足构成 Pod 的所有容器的资源请求时，它会将状态更改为`Pending`。这些 Pod 将保持此状态，直到请求的资源变得可用。
- en: Let's describe the `go-demo-2-db` Deployment and see whether there is some additional
    useful information in it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下 `go-demo-2-db` 部署，看看是否能从中获取一些额外的有用信息。
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output, limited to the events section, is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 限制在事件部分的输出如下：
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can see that it has already `FailedScheduling` seven times and that the message
    clearly indicates that there is `Insufficient memory`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它已经 `FailedScheduling` 了七次，且消息清楚地表明存在 `Insufficient memory`。
- en: 'We''ll revert to the initial definition. Even though we know that its resources
    are incorrect, we know that it satisfies all the requirements and that all the
    Pods will be scheduled successfully:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将恢复到初始定义。尽管我们知道它的资源配置不正确，但我们知道它满足所有要求，且所有 Pod 将成功调度：
- en: '[PRE35]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that all the Pods are running, we should try to write a better definition.
    For that, we need to observe memory and CPU usage and use that information to
    decide the requests and the limits.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有 Pod 都在运行，我们应该尝试编写更好的定义。为此，我们需要观察内存和 CPU 使用情况，并利用这些信息来决定请求和限制。
- en: Adjusting resources based on actual usage
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于实际使用情况调整资源
- en: We saw some of the effects that can be caused by a discrepancy between resource
    usage and resource specification. It's only natural that we should adjust our
    specification to reflect the actual memory and CPU usage better.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了资源使用和资源规格之间差异可能带来的一些影响。调整我们的规格以更好地反映实际的内存和 CPU 使用情况是理所当然的。
- en: 'Let''s start with the database:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数据库开始：
- en: '[PRE36]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We retrieved the name of the database Pod and used it to obtain memory and CPU
    usage of the `db` container. As a result, we now know that memory usage is somewhere
    between `30Mi` and `40Mi`. Similarly, we know that the CPU consumption is somewhere
    around `5m`
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索到了数据库 Pod 的名称，并使用它获取了 `db` 容器的内存和 CPU 使用情况。因此，我们现在知道内存使用量在 `30Mi` 和 `40Mi`
    之间。同样，我们知道 CPU 消耗大约在 `5m` 附近。
- en: Let's take the same metrics for the `api` container.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拿 `api` 容器的相同指标来看看。
- en: '[PRE37]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As expected, an `api` container uses even less resources than MongoDB. Its memory
    is somewhere between `3Mi` and `7Mi`. Its CPU usage is so low that Heapster rounded
    it to `0m`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，`api` 容器使用的资源甚至比 MongoDB 还少。它的内存在 `3Mi` 和 `7Mi` 之间。它的 CPU 使用率低到 Heapster
    将其四舍五入为 `0m`。
- en: Equipped with this knowledge, we can proceed to update our YAML definition.
    Still, before we do that, I need to clarify a few things.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 具备了这些知识，我们可以继续更新我们的 YAML 定义。然而，在此之前，我需要澄清一些事情。
- en: The metrics we collected are based on applications that do nothing. Once they
    start getting real load and start hosting production size data, the metrics would
    change drastically. What you need is a way to predict how much resources an application
    will use in production, not in a simple test environment. You might be inclined
    to run stress tests that would simulate production setup. Do that. It's significant,
    but it does not necessarily result in real production-like behavior.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集的指标基于一些什么也不做的应用程序。一旦它们开始承载实际负载并开始处理生产规模的数据，指标将会发生剧烈变化。你需要的是一种方法来预测应用程序在生产环境中将使用多少资源，而不是在简单的测试环境中。你可能倾向于进行压力测试，以模拟生产环境。这样做是有意义的，但不一定能得到与实际生产环境相似的行为。
- en: Replicating production and behavior of real users is tough. Stress tests will
    get you half-way. For the other half, you'll have to monitor your applications
    in production and, among other things, adjust resources accordingly. There are
    many additional things you should take into account but, for now, I wanted to
    stress that applications that do nothing are not a good measure of resource usage.
    Still, we're going to imagine that the applications we're currently running are
    under production-like load and that the metrics we retrieved represent how the
    applications would behave in production.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 复制生产环境和模拟真实用户行为是很困难的。压力测试能够帮助你走一半的路。剩下的一半，你需要在生产环境中监控你的应用，并且，除了其他事情外，还需要根据实际情况调整资源。你应该考虑许多额外的因素，但目前，我想强调的是，什么都不做的应用并不是衡量资源使用的好标准。不过，我们将假设当前运行的应用程序承受的是类似生产环境的负载，且我们收集的指标代表了这些应用在生产中的表现。
- en: Simple test environments do not reflect production usage of resources. Stress
    tests are a good start, but not a complete solution. Only production provides
    real metrics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的测试环境无法反映生产环境中资源的使用情况。压力测试是一个好的开始，但并不是一个完整的解决方案。只有生产环境提供真实的度量数据。
- en: Let's take a look at a new definition that better represents resource usage
    of the applications.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一个更能代表应用程序资源使用的新定义。
- en: '[PRE38]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分如下：
- en: '[PRE39]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: That is much better. The resource requests are only slightly higher than the
    current usage. We set the memory limits value to double that of the requests so
    that the applications have ample resources for occasional (and short-lived) bursts
    of additional memory consumption. CPU limits are much higher than requests mostly
    because I was too embarrassed to put anything less than a tenth of a CPU as the
    limit. Anyways, the point is that requests are close to the observed usage and
    limits are higher so that applications have some space to breathe in case of a
    temporary spike in resource usage.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就好多了。资源请求仅略高于当前使用量。我们将内存限制设置为请求值的两倍，以便应用程序在偶尔（且短暂）需要额外内存时，有充足的资源。CPU 限制远高于请求值，主要是因为我太不好意思将
    CPU 限制设置为不到十分之一的 CPU。无论如何，关键在于请求值接近实际使用情况，而限制值较高，以便在资源使用暂时激增时，应用程序有些喘息空间。
- en: 'All that''s left is to apply the new definition:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是应用新的定义：
- en: '[PRE40]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `deployment "go-demo-2-api"` was successfully rolled out, and we can move
    onto the next subject.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`deployment "go-demo-2-api"` 已成功发布，我们可以进入下一个主题。'
- en: Exploring quality of service (QoS) contracts
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索服务质量（QoS）合同
- en: When we send a request to Kubernetes API to create a Pod (directly or through
    one of the Controllers), it initiates the scheduling process. What happens next
    or, to be more precise, where it will decide to run a Pod, depends hugely on the
    resources we defined for the containers that form the Pod. In a nutshell, Kubernetes
    will decide to deploy a Pod, whenever it is possible, inside one of the nodes
    that has enough available memory.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向 Kubernetes API 发送请求创建一个 Pod（直接或通过其中一个控制器）时，它会启动调度过程。接下来会发生什么，或者更准确地说，它决定在哪个地方运行
    Pod，主要取决于我们为组成该 Pod 的容器定义的资源。简而言之，Kubernetes 会决定在某个节点上部署 Pod，只要该节点有足够的可用内存。
- en: When memory requests are defined, Pods will get the memory they requested. If
    memory usage of one of the containers exceeds the requested amount, or if some
    other Pod needs that memory, the Pod hosting it might be killed. Please note that
    I wrote that a Pod *might* be killed. Whether that will happen depends on the
    requests from other Pods and the available memory in the cluster. On the other
    hand, containers that exceed their memory limits are always killed (unless it
    was a temporary situation).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义了内存请求时，Pod 将获得它们请求的内存。如果某个容器的内存使用量超过请求的数量，或者其他 Pod 需要这些内存，那么托管该容器的 Pod 可能会被终止。请注意，我说的是
    Pod *可能* 被终止。是否发生这种情况取决于其他 Pod 的请求和集群中可用的内存。另一方面，超出内存限制的容器总是会被终止（除非是暂时的情况）。
- en: CPU requests and limits work a bit differently. Containers that exceed specified
    CPU resources are not killed. Instead, they are throttled.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 请求和限制有些不同。超过指定 CPU 资源的容器不会被终止。相反，它们会被限制。
- en: Now that we shed a bit of light around Kubernetes' killing activities, we should
    note that (almost) nothing happens randomly. When there aren't enough resources
    to serve the needs of all the Pods, Kubernetes will destroy one or more containers.
    The decision which one it will be is anything but random. Who will be the unlucky
    one depends on the assigned **Quality of Service** (**QoS**). Those with the lowest
    priority are killed first.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们稍微了解了 Kubernetes 的终止机制，我们应该注意到（几乎）没有什么是随机发生的。当没有足够的资源来满足所有 Pod 的需求时，Kubernetes
    会销毁一个或多个容器。决定销毁哪一个是绝非随机的。谁将成为不幸的那个，取决于分配的 **服务质量**（**QoS**）。优先级最低的会最先被销毁。
- en: Since this might be the first time you heard about QoS, we'll spend some time
    explaining what they are and how they work.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这可能是你第一次听说 QoS，我们将花一些时间来解释它们是什么以及如何工作。
- en: Pods are the smallest units in Kubernetes. Since almost everything ends up as
    a Pod (one way or another), it is no wonder that Kubernetes promises specific
    guarantees to all the Pods running inside the cluster. Whenever we send a request
    to the API to create or update a Pod, it gets assigned one of the QoS classes.
    They are used to make decisions such as where to schedule a Pod or whether to
    evict it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是 Kubernetes 中最小的单元。由于几乎所有东西都最终作为 Pod 存在（无论以何种方式），因此 Kubernetes 承诺为集群中运行的所有
    Pods 提供特定的保证也就不足为奇了。每当我们向 API 发送请求以创建或更新 Pod 时，Pod 会被分配一个 QoS 类别。这些 QoS 用于做出决策，例如在哪里调度
    Pod 或是否驱逐它。
- en: We do not specify QoS directly. Instead, they are assigned based on the decisions
    we make with resource requests and limits.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会直接指定 QoS。相反，它们是根据我们在资源请求和限制上的决策来分配的。
- en: At the moment, three QoS classes are available. Each Pod can have the *Guaranteed*,
    the *Burstable*, or the *BestEffort* QoS.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有三种 QoS 类别可用。每个 Pod 都可以拥有 *Guaranteed*、*Burstable* 或 *BestEffort* QoS。
- en: '**Guaranteed QoS** is assigned only to Pods which have set both CPU requests
    and limits, and memory requests and limits for all of their containers. The Pods
    we created with the last definition match that criteria. However, there''s one
    more necessary condition that must be met. The requests and limits values must
    be the same per container. Still, there is a catch. When a container specifies
    only limits, requests are automatically set to the same values. In other words,
    containers without requests will have Guaranteed QoS if their limits are defined.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**Guaranteed QoS** 只分配给那些为所有容器设置了 CPU 请求和限制以及内存请求和限制的 Pods。我们使用上一个定义创建的 Pods
    符合这一标准。然而，还有一个必须满足的必要条件。每个容器的请求和限制值必须相同。不过，这里有个陷阱。当容器仅指定限制时，请求会自动设置为与限制相同的值。换句话说，当容器没有指定请求时，只要它们的限制已定义，它们将拥有
    Guaranteed QoS。'
- en: 'We can summarize criteria for Guaranteed QoS as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结出保证 QoS 的标准如下：
- en: Both memory and CPU limits must be set
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须设置内存和 CPU 限制
- en: Memory and CPU requests must be set to the same values as the limits, or they
    can be left empty, in which case they default to the limits (we'll explore them
    soon)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存和 CPU 请求必须设置为与限制相同的值，或者可以留空，在这种情况下，它们默认为限制值（我们稍后会详细探讨）。
- en: Pods with Guaranteed QoS assigned are the top priority and will never be killed
    unless they exceed their limits or are unhealthy. They are the last to go when
    things go wrong. As long as their resource usage is within limits, Kubernetes
    will always choose to kill Pods with other QoS assignments when resource usage
    is over the capacity.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 分配了 Guaranteed QoS 的 Pods 是最高优先级，除非它们超出限制或不健康，否则永远不会被杀掉。当出现问题时，它们是最后一个被终止的。只要它们的资源使用量在限制范围内，Kubernetes
    总是会选择在资源使用超出容量时终止具有其他 QoS 分配的 Pods。
- en: Let's move to the next QoS.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个 QoS。
- en: '**Burstable QoS** is assigned to Pods that do not meet the criteria for Guaranteed
    QoS but have at least one container with memory or CPU requests defined.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**Burstable QoS** 分配给不符合 Guaranteed QoS 标准但至少有一个容器定义了内存或 CPU 请求的 Pods。'
- en: Pods with the Burstable QoS are guaranteed minimal (requested) memory usage.
    They might be able to use more resources if they are available. If the system
    is under pressure and needs more available memory, containers belonging to the
    Pods with the Burstable QoS are more likely to be killed than those with Guaranteed
    QoS when there are no Pods with the BestEffort QoS. You can consider the Pods
    with this QoS as medium priority.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 具有 Burstable QoS 的 Pods 保证最小（请求的）内存使用量。如果有更多资源可用，它们可能会使用更多资源。如果系统处于压力状态并需要更多可用内存，则具有
    Burstable QoS 的容器比那些具有 Guaranteed QoS 的容器更容易被杀掉，前提是没有具有 BestEffort QoS 的 Pods。你可以将具有该
    QoS 的 Pods 看作中等优先级。
- en: Finally, we reached the last QoS.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们到达了最后一个 QoS。
- en: '**BestEffort QoS** is given to the Pods that do not qualify as Guaranteed or
    Burstable. They are Pods that consist of containers that have none of the resources
    defined. Containers in Pods qualified as BestEffort can use any available memory
    they need.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**BestEffort QoS** 分配给不符合 Guaranteed 或 Burstable 标准的 Pods。它们是由没有定义任何资源的容器组成的
    Pods。符合 BestEffort 条件的 Pods 中的容器可以使用任何它们需要的可用内存。'
- en: When in need of more resources, Kubernetes will start killing containers residing
    in the Pods with BestEffort QoS. They are the lowest priority, and they are the
    first to disappear when more memory is needed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要更多资源时，Kubernetes 将开始终止位于 BestEffort QoS 的 Pods 中的容器。它们的优先级最低，当需要更多内存时，它们是最先被终止的。
- en: Let's take a look which QoS our `go-demo-2-db` Pod got assigned.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下我们的 `go-demo-2-db` Pod 被分配了哪种 QoS。
- en: '[PRE41]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，限制为相关部分，如下所示：
- en: '[PRE42]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The Pod was assigned Burstable QoS. Its limits are different from requests,
    so it did not qualify for Guaranteed QoS. Since its resources are set, and it
    is not eligible for Guaranteed QoS, Kubernetes assigned it the second best QoS.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该 Pod 被分配了可突发的 QoS。它的限制与请求不同，因此不符合保证的 QoS。由于它的资源已设置，并且不符合保证的 QoS，Kubernetes
    将其分配为第二优先级的 QoS。
- en: 'Now, let''s take a look at a slightly modified definition:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下稍作修改的定义：
- en: '[PRE43]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，限制为相关部分，如下所示：
- en: '[PRE44]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This time, we specified that both `cpu` and `memory` should have the same values
    for both the `requests` and the `limits` for the containers that will be created
    with the `go-demo-2-db` Deployment. As a result, it should be assigned Guaranteed
    QoS.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们指定了 `cpu` 和 `memory` 对容器的 `requests` 和 `limits` 应具有相同的值，以便通过 `go-demo-2-db`
    Deployment 创建的容器。因此，它应该被分配为 `Guaranteed` QoS。
- en: The containers of the `go-demo-2-api` Deployment are void of any `resources`
    definitions and, therefore, will be assigned BestEffort QoS.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo-2-api` Deployment 的容器没有任何 `resources` 定义，因此将被分配为 BestEffort QoS。'
- en: Let's confirm that both assumptions (not to say guesses) are indeed correct.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认这两个假设（不说是猜测）是否确实正确。
- en: '[PRE45]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We applied the new definition and output the rollout status of the `go-demo-2-db`
    Deployment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了新的定义并输出了 `go-demo-2-db` Deployment 的滚动更新状态。
- en: Now we can describe the Pod created thought the `go-demo-2-db` Deployment and
    check its QoS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以描述通过 `go-demo-2-db` Deployment 创建的 Pod，并检查其 QoS。
- en: '[PRE46]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，限制为相关部分，如下所示：
- en: '[PRE47]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Memory and CPU limits and requests are the same and, as a result, the QoS is
    `Guaranteed`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 内存和 CPU 的限制与请求相同，因此 QoS 为 `Guaranteed`。
- en: Let's check the QoS of the Pods created through the `go-demo-2-api` Deployment.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查通过 `go-demo-2-api` Deployment 创建的 Pods 的 QoS。
- en: '[PRE48]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，限制为相关部分，如下所示：
- en: '[PRE49]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The three Pods created through the `go-demo-2-api` Deployment are without any
    resources definitions and, therefore, their QoS is set to `BestEffort`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `go-demo-2-api` Deployment 创建的三个 Pods 没有任何资源定义，因此它们的 QoS 被设置为 `BestEffort`。
- en: We won't be needing the objects we created so far so we'll remove them before
    moving onto the next subject.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再需要之前创建的对象，所以我们在进入下一个主题之前会先将它们删除。
- en: '[PRE50]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Defining resource defaults and limitations within a namespace
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在命名空间内定义资源默认值和限制
- en: We already learned how to leverage Kubernetes namespaces to create clusters
    within a cluster. When combined with RBAC, we can create namespaces and give users
    permissions to use them without exposing the whole cluster. Still, one thing is
    missing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何利用 Kubernetes 命名空间在一个集群内创建多个子集群。当与 RBAC 结合使用时，我们可以创建命名空间并授予用户使用这些命名空间的权限，而不暴露整个集群。然而，仍然有一件事缺失。
- en: We can, let's say, create a `test` namespace and allow users to create objects
    without permitting them to access other namespaces. Even though that is better
    than allowing everyone full access to the cluster, such a strategy would not prevent
    people from bringing the whole cluster down or affecting the performance of applications
    running in other namespaces. The piece of the puzzle we're missing is resource
    control on the namespace level.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设，创建一个 `test` 命名空间，并允许用户在不允许其访问其他命名空间的情况下创建对象。尽管这样比允许所有人完全访问集群要好，但这种策略并不能防止人们将整个集群宕机或影响在其他命名空间中运行的应用程序的性能。我们缺少的那一块拼图就是在命名空间级别上的资源控制。
- en: We already discussed that every container should have resource `limits` and
    `requests` defined. That information helps Kubernetes schedule Pods more efficiently.
    It also provides it with the information it can use to decide whether a Pod should
    be evicted or restarted. Still, the fact that we can specify `resources` does
    not mean that we are forced to define them. We should have the ability to set
    default `resources` that will be applied when we forget to specify them explicitly.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过每个容器都应该定义资源的 `limits` 和 `requests`。这些信息帮助 Kubernetes 更高效地调度 Pods。它还为
    Kubernetes 提供了可以用来决定是否驱逐或重启 Pod 的信息。然而，虽然我们可以指定 `resources`，并不意味着我们必须强制定义它们。我们应该有能力设置默认的
    `resources`，以便在我们忘记显式指定时自动应用。
- en: Even if we define default `resources`, we also need a way to set limits. Otherwise,
    everyone with permissions to deploy a Pod can potentially run an application that
    requests more resources than we're willing to give.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们定义了默认的 `resources`，我们也需要一种设置限制的方式。否则，所有具有部署 Pod 权限的人都有可能运行请求资源超过我们愿意提供的应用程序。
- en: All in all, our next task is to define default requests and limits as well as
    to specify minimum and maximum values someone can define for a Pod.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，我们下一步的任务是定义默认请求和限制，并指定可以为 Pod 定义的最小和最大值。
- en: We'll start by creating a `test` Namespace.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建一个 `test` 命名空间开始。
- en: '[PRE51]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: With a playground namespace created, we can take a look at a new definition.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个 playground 命名空间后，我们可以查看一个新的定义。
- en: '[PRE52]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE53]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We specified that the resource should be of `LimitRange` kind. It's `spec` has
    four `limits`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定资源应该是 `LimitRange` 类型。它的 `spec` 有四个 `limits`。
- en: The `default` limit and `defaultRequest` entries will be applied to the containers
    that do not specify resources. If a container does not have memory or CPU limits,
    it'll be assigned the values set in the `LimitRange`. The `default` entries are
    used as limits, and the `defaultRequest` entries are used as requests.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有指定资源的容器，将应用 `default` 限制和 `defaultRequest` 条目。如果容器没有内存或 CPU 限制，它将被分配到 `LimitRange`
    中设置的值。`default` 条目用作限制，`defaultRequest` 条目用作请求。
- en: When a container does have the resources defined, they will be evaluated against
    `LimitRange` thresholds specified as `max` and `min`. If a container does not
    meet the criteria, the Pod that hosts the containers will not be created.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器确实定义了资源时，它们将根据 `LimitRange` 中指定的 `max` 和 `min` 阈值进行评估。如果容器不符合标准，将不会创建承载容器的
    Pod。
- en: 'We''ll see a practical implementation of the four `limits` soon. For now, the
    next step is to create the `limit-range` resource:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将看到四个 `limits` 的实际实现。目前，下一步是创建 `limit-range` 资源：
- en: '[PRE54]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We created the `LimitRange` resource.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了 `LimitRange` 资源。
- en: Let's describe the `test` namespace where the resource was created.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述创建资源的 `test` 命名空间。
- en: '[PRE55]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于相关部分如下。
- en: '[PRE56]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We can see that the `test` namespace has the resource limits we specified. We
    set four out of five possible values. The `maxLimitRequestRatio` is missing and
    we'll describe it only briefly. When `MaxLimitRequestRatio` is set, container
    request and limit resources must both be non-zero, and the limit divided by the
    request must be less than or equal to the enumerated value.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `test` 命名空间拥有我们指定的资源限制。我们设置了五个可能的值中的四个。`maxLimitRequestRatio` 没有被设置，我们将简要描述它。当设置了
    `MaxLimitRequestRatio` 时，容器的请求和限制资源必须同时非零，并且限制除以请求必须小于或等于枚举值。
- en: 'Let''s take a look at yet another variation of the `go-demo` definition:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个 `go-demo` 定义的变体：
- en: '[PRE57]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The only thing to note is that none of the containers have any resources defined.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，没有一个容器定义了任何资源。
- en: Next, we'll create the objects defined in the `go-demo-2-no-res.yml` file.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建在 `go-demo-2-no-res.yml` 文件中定义的对象。
- en: '[PRE58]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We created the objects inside the `test` namespace and waited until the `deployment
    "go-demo-2-api"` was successfully rolled out.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `test` 命名空间内创建了对象，并等待成功部署 `deployment "go-demo-2-api"`。
- en: 'Let''s describe one of the Pods we created:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述我们创建的一个 Pod：
- en: '[PRE59]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于相关部分如下：
- en: '[PRE60]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Even though we did not specify the resources of the `db` container inside the
    `go-demo-2-db` Pod, the resources are set. The `db` container was assigned the
    `default` limits of the `test` Namespace as the container limit. Similarly, the
    `defaultRequest` limits were used as container requests.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们没有明确指定 `go-demo-2-db` Pod 内 `db` 容器的资源，资源已经被设置。`db` 容器被分配了 `test` 命名空间的
    `default` 限制作为容器的限制。类似地，`defaultRequest` 限制被用作容器的请求。
- en: As we can see, any attempt to create Pods hosting containers without resources
    will result in the namespace limits applied.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试创建未指定资源的容器宿主 Pod 时，将会受到命名空间限制的影响。
- en: We should still define container resources instead of relying on namespace default
    limits. They are, after all, only a fallback in case someone forgot to define
    resources.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然应该定义容器资源，而不是依赖命名空间的默认限制。毕竟，它们只是在有人忘记定义资源时的备选项。
- en: Let's see what happens when resources are defined, but they do not match the
    namespace `min` and `max` limits.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当资源被定义但不符合命名空间 `min` 和 `max` 限制时会发生什么。
- en: We'll use the same `go-demo-2.yml` we used before.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: What matters is that the `resources` for both Deployments are defined.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Let's create the objects and retrieve the events. They will help us understand
    better what is happening.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output of the latter command, limited to the relevant parts, is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We can see that we are forbidden from creating either of the two Pods. The difference
    between those events is in what caused Kubernetes to reject our request.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The `go-demo-2-db-*` Pod could not be created because its `maximum memory usage
    per Container is 80Mi, but limit is 100Mi`. On the other hand, we are forbidden
    from creating the `go-demo-2-api-*` Pods because the `minimum memory usage per
    Container is 10Mi, but request is 5Mi`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: All the containers within the `test` namespace will have to comply with the
    `min` and `max` limits. Otherwise, we are forbidden from creating them. Container
    limits cannot be higher than the namespace `max` limits. On the other hand, container
    resource requests cannot be smaller than namespace `min` limits.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: If we think about namespace limits as lower and upper thresholds, we can say
    that container requests cannot be below them, and that container limits can't
    be above.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Press the *Ctrl* + *C* keys to stop watching the events.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: It might be easier to observe the effects of the `max` and `min` limits if we
    create Pods directly, instead of through Deployments.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We tried to create a Pod with the memory request set to `100Mi`. Since the namespace
    limit is `80Mi`, the API returned the error message stating that the `Pod "test"
    is invalid`. Even though the `max` limit refers to container `limit`, memory request
    was used in its absence.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We'll run a similar exercise but, this time, with only `1Mi` set as memory request.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This time, the error is slightly different. We can see that `pods "test" is
    forbidden: minimum memory usage per Container is 10Mi, but request is 1Mi`. What
    we requested is below the `min` limit of the `test` namespace and, therefore,
    we are forbidden from creating the Pod.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: We'll delete the `test` namespace before we move into the next subject.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Defining resource quotas for a namespace
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resource defaults and limitations are a good first step towards preventing malicious
    or accidental deployment of Pods that can potentially produce adverse effects
    on the cluster. Still, any user with the permissions to create Pods in a namespace
    can overload the system. Even if `max` values are set to some reasonably small
    amount of memory and CPU, a user could deploy thousands, or even millions of Pods,
    and "eat" all the available cluster resources. Such an effect might not be even
    produced out of malice but accidentally. A Pod might be attached to a system that
    scales it automatically without defining upper bounds and, before we know it,
    it might scale to too many replicas. There are also many other ways things might
    get out of control.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: What we need is to define namespace boundaries through quotas.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: With quotas, we can guarantee that each namespace gets its fair share of resources.
    Unlike `LimitRange` rules that are applied to each container, `ResourceQuota`
    defines namespace limits based on aggregate resource consumption.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We can use `ResourceQuota` objects to define the total amount of compute resources
    (memory and CPU) that can be spent in a namespace. We can also use it to limit
    storage utilization or the number of objects of a certain type that can be created
    in a namespace.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the cluster resources we have in our Minikube cluster.
    It is small, and it's not even a real cluster. However, it's the only one we have
    (for now), so please use your imagination and pretend that it's "real".
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Our cluster has 2 CPUs and 2 GB of memory. Now, let's say that this cluster
    serves only development and production purposes. We can use the `default` namespace
    for production and create a `dev` namespace for development. We can assume that
    the production should consume all the resources of the cluster minus those given
    to the `dev` namespace which, on the other hand, should not exceed a specific
    limit.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that with 2 CPUs and 2 GB of memory, there isn't much we can give
    to developers. Still, we'll try to be generous. We'll give them 500 MB and 0.8
    CPUs for requests. We'll allow occasional bursts in resource usage by defining
    limits of 1 CPU and 1 GB of memory. Furthermore, we might want to limit the number
    of Pods to ten. Finally, as a way to reduce risks, we will deny developers the
    right to expose node ports.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Isn't that a decent plan? I'll imagine that, at this moment, you are nodding
    as a sign of approval so we'll move on and create the quotas we discussed.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `dev.yaml` definition:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Besides creating the `dev` namespace, we're also creating a `ResourceQuota`.
    It specifies a set of `hard` limits. Remember, they are based on aggregated data,
    and not on per-container basis like LimitRanges.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: We set requests quotas to `0.8` CPUs and `500Mi` of RAM. Similarly, limit quotas
    as set to `1` CPU and `1Gi` of memory. Finally, we specified that the `dev` namespace
    can have only `10` Pods and that there can be no NodePorts. That's the plan we
    formulated and defined. Now let's create the objects and explore the effects.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We can see from the output that the `namespace "dev"` was created as well as
    the `resourcequota "dev"`. To be on the safe side, we'll describe the newly created
    `devquota`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We can see that the hard limits are set and that there's currently no usage.
    That was to be expected since we're not running any objects in the `dev` namespace.
    Let's spice it up a bit by creating the already too familiar `go-demo-2` objects.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We created the objects from the `go-demo-2.yml` file and waited until the `go-demo-2-api`
    Deployment rolled out. Now we can revisit the values of the `dev` quota:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE75]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Judging from the `Used` column, we can see that we are, for example, currently
    running `4` Pods and that we are still below the limit of `10`. One of those Pods
    was created through the `go-demo-2-db` Deployment, and the other three with the
    `go-demo-2-api`. If you summarize resources we specified for the containers that
    form those Pods, you'll see that the values match the used `limits` and `requests`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `Used` 列中，我们可以看到例如我们目前正在运行 `4` 个 Pod，并且仍然低于 `10` 的限制。其中一个 Pod 是通过 `go-demo-2-db`
    部署创建的，另外三个是通过 `go-demo-2-api` 创建的。如果你总结这些 Pod 中容器的资源配置，你会看到这些值与已使用的 `limits` 和
    `requests` 匹配。
- en: So far, we did not reach any of the quotas. Let's try to break at least one
    of them.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有达到任何配额。让我们尝试打破至少一个配额。
- en: '[PRE76]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 仅限相关部分的输出如下：
- en: '[PRE77]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The definition of the `go-demo-2-scaled.yml` is almost the same as the one in
    `go-demo-2.yml`. The only difference is that the number of replicas of the `go-demo-2-api`
    Deployment is increased to fifteen. As you already know, that should result in
    fifteen Pods created through that Deployment.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo-2-scaled.yml` 的定义几乎与 `go-demo-2.yml` 相同。唯一的不同是 `go-demo-2-api` 部署的副本数增加到十五个。正如你所知道的，这将导致通过该部署创建十五个
    Pod。'
- en: I'm sure you can guess what will happen if we apply the new definition. We'll
    do it anyway.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我敢肯定你能猜到如果我们应用新的定义会发生什么，但我们还是会继续这样做。
- en: '[PRE78]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We applied the new definition. We'll give Kubernetes a few moments to do the
    work before we take a look at the events it'll generate. So, take a deep breath
    and count from one to the number of processors in your Laptop. In my case, it's
    one Mississippi, two Mississippi, three Mississippi, all the way until sixteen
    Mississippi.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了新的定义。我们将给 Kubernetes 一些时间来完成工作，然后再查看它将生成的事件。因此，深呼吸，从一数到你笔记本电脑上处理器的数量。以我为例，应该是“一
    Mississippi，二 Mississippi，三 Mississippi”，一直数到十六 Mississippi。
- en: '[PRE79]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The output of a few of the events generated inside the `dev` namespace is as
    follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `dev` 命名空间内生成的一些事件的输出如下：
- en: '[PRE80]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: We can see that we reached two of the limits imposed by the namespace quota.
    We reached the maximum amount of CPU (`1`) and Pods (`10`). As a result, ReplicaSet
    controller was forbidden from creating new Pods.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们达到了命名空间配额施加的两个限制。我们达到了 CPU 的最大值（`1`）和 Pods 的最大值（`10`）。因此，ReplicaSet
    控制器被禁止创建新的 Pod。
- en: We should be able to confirm which hard limits were reached by describing the
    `dev` namespace.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该能够通过描述 `dev` 命名空间来确认达到的硬限制。
- en: '[PRE81]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The output, limited to the `Resource Quotas` section, is as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 仅限于 `Resource Quotas` 部分的输出如下：
- en: '[PRE82]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: As the events showed us, the values of `limits.cpu` and `pods` resources are
    the same in both `User` and `Hard` columns. As a result, we won't be able to create
    any more Pods, nor will we be allowed to increase CPU limits for those that are
    already running.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如事件所示，`limits.cpu` 和 `pods` 资源在 `User` 和 `Hard` 列中的值是相同的。因此，我们将无法再创建更多的 Pod，也无法为已运行的
    Pod 增加 CPU 限制。
- en: Finally, let's take a look at the Pods inside the `dev` namespace.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看一下 `dev` 命名空间中的 Pod。
- en: '[PRE83]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Following is the output of the preceding command:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面命令的输出：
- en: '[PRE84]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The `go-demo-2-api` Deployment managed to create nine Pods. Together with the
    Pod created through the `go-demo-2-db`, we reached the limit of ten.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo-2-api` 部署成功创建了九个 Pod。再加上通过 `go-demo-2-db` 创建的 Pod，我们达到了十个的上限。'
- en: We confirmed that the limit and the Pod quotas work. We'll revert to the previous
    definition (the one that does not reach any of the quotas) before we move onto
    the next verification.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认了限制和 Pod 配额有效。我们将在进行下一次验证之前恢复到先前的定义（即不会达到任何配额的定义）。
- en: '[PRE85]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The output of the latter command should indicate that the `deployment "go-demo-2-api"
    was successfully rolled out`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 后者命令的输出应表明 `deployment "go-demo-2-api" 已成功发布`。
- en: 'Let''s take a look at yet another slightly modified definition of the `go-demo-2`
    objects:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下另一个稍微修改过的 `go-demo-2` 对象定义：
- en: '[PRE86]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output, limited to the relevant parts, is as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 仅限相关部分的输出如下：
- en: '[PRE87]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Both memory request and limit of the `api` container of the `go-demo-2-api`
    Deployment is set to `200Mi` while the database remains with the memory request
    of `50Mi`. Knowing that the `requests.memory` quota of the `dev` namespace is
    `500Mi`, it's enough to do simple math and come to the conclusion that we won't
    be able to run all three replicas of the `go-demo-2-api` Deployment.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Just as before, we should wait for a while before taking a look at the events
    of the `dev` namespace.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The output, limited to one of the entries, is as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: We reached the quota of the `requests.memory`. As a result, creation of at least
    one of the Pods is forbidden. We can see that we requested creation of a Pod that
    requests `200Mi` of memory. Since the current summary of the memory requests is
    `455Mi`, creating that Pod would exceed the allocated `500Mi`.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look at the namespace.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The output, limited to the `Resource Quotas` section, is as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Indeed, the amount of used memory requests is `455Mi`, meaning that we could
    create additional Pods with up to `45Mi`, not `200Mi`.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: We'll revert to the `go-demo-2.yml` one more time before we explore the last
    quota we defined.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: The only quota we did not yet verify is `services.nodeports`. We set it to `0`
    and, as a result, we should not be allowed to expose any node ports. Let's confirm
    that is indeed true.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: All our quotas work as expected. But, there are others. We won't have time to
    explore examples of all the quotas we can use. Instead, we'll list them all for
    future reference.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'We can divide quotas into several groups:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute resource quotas** limit the total sum of the compute resources. They
    are as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '| **Resource name** | **Description** |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| `cpu` | Across all pods in a non-terminal state, the sum of CPU requests
    cannot exceed this value. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| `limits.cpu` | Across all pods in a non-terminal state, the sum of CPU limits
    cannot exceed this value. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| `limits.memory` | Across all pods in a non-terminal state, the sum of memory
    limits cannot exceed this value. |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| `memory` | Across all pods in a non-terminal state, the sum of memory requests
    cannot exceed this value. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| `requests.cpu` | Across all pods in a non-terminal state, the sum of CPU
    requests cannot exceed this value. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| `requests.memory` | Across all pods in a non-terminal state, the sum of memory
    requests cannot exceed this value. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '**Storage resource quotas** limit the total sum of the storage resources. We
    did not yet explore storage (beyond a few local examples) so you might want to
    keep the list that follows for future reference:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '| **Resource name** | **Description** |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| `requests.storage` | Across all persistent volume claims, the sum of storage
    requests cannot exceed this value. |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| `persistentvolumeclaims` | The total number of persistent volume claims that
    can exist in the namespace. |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| `[PREFIX]/requests.storage` | Across all persistent volume claims associated
    with the storage-class-name, the sum of storage requests cannot exceed this value.
    |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| `[PREFIX]/requests.storage` | 与存储类名称相关的所有持久卷索赔的总存储请求不能超过此值。 |'
- en: '| `[PREFIX]/persistentvolumeclaims` | Across all persistent volume claims associated
    with the storage-class-name, the total number of persistent volume claims that
    can exist in the namespace. |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| `[PREFIX]/persistentvolumeclaims` | 与存储类名称相关的所有持久卷索赔的总数，可存在于命名空间中。 |'
- en: '| `requests.ephemeral-storage` | Across all pods in the namespace, the sum
    of local ephemeral storage requests cannot exceed this value. |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| `requests.ephemeral-storage` | 命名空间中所有Pod的本地临时存储请求总和不能超过此值。 |'
- en: '| `limits.ephemeral-storage` | Across all pods in the namespace, the sum of
    local ephemeral storage limits cannot exceed this value. |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| `limits.ephemeral-storage` | 命名空间中所有Pod的本地临时存储限制的总和不能超过此值。 |'
- en: Please note that `[PREFIX]` should be replaced with `<storage-class-name>.storageclass.storage.k8s.io`.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`[PREFIX]`应替换为`<storage-class-name>.storageclass.storage.k8s.io`。
- en: '**Object count quotas** limit the number of objects of a given type. They are
    as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**对象计数配额** 限制了给定类型对象的数量。它们如下：'
- en: '| **Resource name** | **Description** |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| **资源名称** | **描述** |'
- en: '| `configmaps` | The total number of config maps that can exist in the namespace.
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| `configmaps` | 可存在于命名空间中的配置映射总数。 |'
- en: '| `persistentvolumeclaims` | The total number of persistent volume claims that
    can exist in the namespace. |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| `persistentvolumeclaims` | 可存在于命名空间中的持久卷索赔总数。 |'
- en: '| `pods` | The total number of pods in a non-terminal state that can exist
    in the namespace. A pod is in a terminal state if status.phase in (Failed, Succeeded)
    is true. |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| `pods` | 可存在于命名空间中的非终端状态下的Pod总数。如果status.phase为（Failed，Succeeded），则Pod处于终止状态。
    |'
- en: '| `replicationcontrollers` | The total number of replication controllers that
    can exist in the namespace. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| `replicationcontrollers` | 可存在于命名空间中的复制控制器总数。 |'
- en: '| `resourcequotas` | The total number of resource quotas that can exist in
    the namespace. |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| `resourcequotas` | 可存在于命名空间中的资源配额总数。 |'
- en: '| `services` | The total number of services that can exist in the namespace.
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| `services` | 可存在于命名空间中的服务总数。 |'
- en: '| `services.loadbalancers` | The total number of services of type load balancer
    that can exist in the namespace. |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| `services.loadbalancers` | 可存在于命名空间中的负载均衡器类型的服务总数。 |'
- en: '| `services.nodeports` | The total number of services of type node port that
    can exist in the namespace. |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| `services.nodeports` | 可存在于命名空间中的节点端口类型的服务总数。 |'
- en: '| `secrets` | The total number of secrets that can exist in the namespace.
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| `secrets` | 可存在于命名空间中的秘密总数。 |'
- en: What now?
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: Wasn't that a ride?
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 那不是一次很好的体验吗？
- en: Kubernetes relies heavily on available resources spread throughout the cluster.
    Still, it cannot do magic. We need to help it out by defining resources we expect
    our containers will consume.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在整个集群中广泛依赖可用资源。但它不能变出魔法。我们需要通过定义我们预期容器将消耗的资源来帮助它。
- en: Even though Heapster is not the best solution for collecting metrics, it is
    already available in our Minikube cluster, and we used it to learn how much resources
    our applications use and, through that information, we refined our resource definitions.
    Without metrics, our definitions are pure guesses. When we guess, Kubernetes needs
    to guess as well. A stable system is a predictable system based on facts, not
    someone's imagination. Heapster helped us transform our assumptions into measurable
    facts which we fed into Kubernetes which, in turn, used them in its scheduling
    algorithms.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Heapster不是收集指标的最佳解决方案，但它已经在我们的Minikube集群中可用，我们用它来了解我们的应用程序使用多少资源，通过这些信息，我们优化了我们的资源定义。没有指标，我们的定义就是纯粹的猜测。当我们猜测时，Kubernetes也需要猜测。一个稳定的系统是基于事实而不是想象的可预测系统。Heapster帮助我们将假设转化为可衡量的事实，然后我们将这些事实提供给Kubernetes，在其调度算法中使用。
- en: Exploration of resource definitions led us to **Quality Of Service** (**QoS**).
    Even though Kubernetes decides which QoS will be used, knowing the rules used
    in the decision process is essential if we are to prioritize applications and
    their availability.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 资源定义的探索引导我们进入**服务质量**（**QoS**）。尽管Kubernetes决定使用哪种QoS，但如果我们要优先考虑应用程序及其可用性，了解决策过程中使用的规则是至关重要的。
- en: All that leads us to the culmination of the strategies that make our clusters
    secure, stable, and robust. Dividing a cluster into Namespaces and employing RBAC
    is not enough. RBAC prevents unauthorized users from accessing the cluster and
    provides permissions to those we trust. However, RBAC does not prevent users from
    accidentally (or intentionally) putting the cluster in danger through too many
    deployments, too big applications, or inaccurate sizing. Only by combining RBAC
    with resource defaults, limitations, and quotas can we hope for a fault tolerant
    and robust cluster capable of reliably hosting our applications.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: We learned almost all the essential Kubernetes objects and principles. The time
    has come to move to a "real" cluster. We are about to delete the Minikube cluster
    for the last time (at least in this book).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Kubernetes resource management compared to docker swarm equivalent
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resource management can be divided into a few categories. We need to define
    how much memory and CPU we except a container will use and what are the limits.
    This information is crucial for a scheduler to make "intelligent" decisions when
    calculating where to place containers. In this aspect, there is no essential difference
    between Kubernetes and Docker Swarm. Both are using requested resources to decide
    where to deploy containers and limits when to evict them. Both of them are, more
    or less, the same in this aspect.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: How can we know how much memory and CPU to dedicate to each of our containers?
    That's one of the questions I heard way too many times. The answer is simple.
    Collect metrics, evaluate them, adjust resources, take a break, repeat. Where
    do we collect metrics? Wherever you want. Prometheus is a good choice. Where will
    it get metrics? Well, it depends which scheduler you use. If it's Docker Swarm,
    you'll need to run a bunch of exporters. Or, you might be brave enough and try
    the experimental feature that exposes Docker's internal metrics in Prometheus
    format. You might even be enthusiastic enough to think that they will be enough
    for all your monitoring and alerting needs. Maybe, by the time you read this,
    the feature is not experimental anymore. On the other hand, Kubernetes has it
    all, and so much more. You can use Heapster, or you might discover that it is
    too limiting and configure Prometheus to scrape metrics directly from Kubernetes
    API. Kubernetes exposes a vast amount of data. More than you'll probably ever
    need. You will be able to fetch memory, CPU, IO, network, and a myriad of other
    metrics and make intelligent decisions not only about the resources your containers
    require but about so much more.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: To make things clear, you can get the same metrics no matter whether you're
    running Kubernetes or Docker Swarm. The major difference is that Kubernetes exposes
    them through its API, while with Swarm you'll have to struggle between the decisions
    whether to use its limited metrics or go into the trouble of setting up the exporters
    like cAdvisor and Node Exporter. Most likely, you'll discover that you'll need
    both the metrics from Swarm's API and those from the exporters. Kubernetes has
    a more robust solution, even though you might still need an exporter or two. Still,
    having most, if not all, of the metrics you'll need from its API is a handy thing
    to have.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Frankly, the differences in the way we retrieve metrics from the two schedulers
    are not of great importance. If this would be where the story about resources
    ends, I'd conclude that both solutions are, more or less, equally good. But, the
    narrative continues. This is where similarities stop. Or, to be more precise,
    this is where Docker Swarm ends, and Kubernetes only just began.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes allows us to define resource defaults and limitations that are applied
    to containers that do not specify resources. It allows us to specify resource
    quotas that prevent accidental or malicious over-usage of resources. The two combined
    with Namespaces provide very powerful safeguards. They give us some of the means
    with which we can design the system that is truly fault tolerant by preventing
    rogue containers, uncontrolled scaling, and human errors from bringing our clusters
    to a grinding halt. Don't think, even for a second, that quotas are the only thing
    required for building a robust system. It isn't the only piece of the puzzle,
    but it is a significant one never the less.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces combined with quotas are important. I'd even say that they are crucial.
    Without them, we would be forced to create a cluster for every group, team, or
    a department in our organizations. Or, we might have to resort to further tightening
    of the processes that prevent our teams from exploiting the benefits behind container
    orchestrators. If the goal is to provide freedom to our teams without sacrificing
    cluster stability, Kubernetes has a clear edge over Docker Swarm.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: This battle is won by Kubernetes, but the war still rages.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: OK. I exaggerated a bit with the words *battle* and *war*. It's not a conflict,
    and both communities are increasing collaboration and sharing ideas and solutions.
    Both platforms are merging. Still, for now, Kubernetes has a clear edge over Docker
    Swarm on the subject of resource management.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
