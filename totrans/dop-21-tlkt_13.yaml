- en: Creating and Managing a Docker Swarm Cluster in DigitalOcean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*                               Plan to throw one (implementation) away; you
    will, anyhow.**–Fred Brooks*'
  prefs: []
  type: TYPE_NORMAL
- en: We already saw a few ways to create and operate a Swarm cluster in AWS. Now
    we'll try to do the same in *DigitalOcean* ([https://www.digitalocean.com/](https://www.digitalocean.com/)).
    We'll explore some of the tools and configurations that can be used with this
    hosting provider.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike AWS that is known to everyone, DigitalOcean is relatively new and less
    well known. You might be wondering why I chose DigitalOcean before some other
    providers like Azure and GCE. The reason lies in the differences between AWS (and
    other similar providers) and DigitalOcean. The two differ in many aspects. Comparing
    them is like comparing David and Goliath. One is small while the other (AWS) is
    huge. DigitalOcean understands that it cannot compete with AWS on its own ground,
    so it decided to play a different game.
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean launched in 2011 and is focused on very specific needs. Unlike
    AWS with its *everything-to-everyone* approach, DigitalOcean provides virtual
    machines. There are no bells and whistles. You do not get lost in their catalog
    of services since it is almost non-existent. If you need a place to host your
    cluster and you do not want to use services designed to lock you in, DigitalOcean
    might be the right choice.
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean's main advantages are pricing, high performance, and simplicity.
    If that's what you're looking for, it is worth trying it out.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through these three advantages one by one.
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean's pricing is probably the best among all cloud providers. No matter
    whether you are a small company in need of only a couple of servers, or a big
    entity that is looking for a place to instantiate hundreds or even thousands of
    servers, chances are that DigitalOcean will be cheaper than any other provider.
    That might leave you wondering about the quality. After all, cheaper things tend
    to sacrifice it. Is that the case with DigitalOcean?
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean offers very high-performance machines. All disk drives are SSD,
    network speed is 1 Gbps, and it takes less than a minute to create and initialize
    droplets (their name for VMs). As a comparison, AWS EC2 instance startup time
    can vary between one and three minutes.
  prefs: []
  type: TYPE_NORMAL
- en: The last advantages DigitalOcean offers are their UI and API. Both are clean
    and easy to understand. Unlike AWS that can have a steep learning curve, you should
    have no trouble learning how to use them in a few hours.
  prefs: []
  type: TYPE_NORMAL
- en: Enough with the words of praise. Not everything can be great. What are the disadvantages?
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean does not offer a plethora of services. It does a few things, and
    it does them well. It is a bare-bone **Infrastructure as a service **(**IaaS**)
    provider. It assumes that you will set up the services yourself. There is no load
    balancing, centralized logging, sophisticated analytics, hosted databases, and
    so on. If you need those things, you are expected to set them up yourself. Depending
    on your use case, that can be an advantage or a disadvantage.
  prefs: []
  type: TYPE_NORMAL
- en: A comparison between DigitalOcean and AWS is unfair since the scope of what
    each does is different. DigitalOcean is not trying to compete with AWS as a whole.
    If pressed to compare something, that would be DigitalOcean against AWS EC2\.
    In such a case, DigitalOcean wins hands down.
  prefs: []
  type: TYPE_NORMAL
- en: I will assume that you already have a DigitalOcean account. If that's not the
    case, please register using: [https://m.do.co/c/ee6d08525457](https://m.do.co/c/ee6d08525457)
    . You'll get 10 in credit. That should be more than enough to run the examples
    in this chapter. DigitalOcean is so cheap that you will probably finish this chapter
    with more than 9 remaining balance.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you've already made a firm decision to use a different cloud computing
    provider or on-premise servers, I highly recommend going through this chapter.
    It will help you compare DigitalOcean with your provider of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Let's give DigitalOcean a spin and judge through examples whether it is a good
    choice to host our Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that some parts of this chapter are very similar, or even the
    same as those you read in the other cloud computing chapters like Chapter 12, *Creating
    and Managing a Docker Swarm Cluster in Amazon Web Services*. The reason for the
    partial repetition is the goal to make the cloud computing chapters useful not
    only to those who read everything, but also those who skipped other providers
    and jumped right here.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move into practical exercises, we'll need to get the access keys and
    decide the region where we'll run the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Chapter 12, *Creating And Managing A Docker Swarm Cluster in Amazon Web
    Services,* we installed AWS **Command Line Interface** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)) that
    helped us with some of the tasks. DigitalOcean has a similar interface called
    doctl. Should we install it? I don't think we need a CLI for DigitalOcean. Their
    API is clean and well defined, and we can accomplish everything a CLI would do
    with simple curl requests. DigitalOcean proves that a well designed API goes a
    long way and can be the only entry point into the system, saving us the trouble
    of dealing with middle-man applications like CLIs.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start using the API, we should generate an access token that will
    serve as the authentication method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please open the *DigitalOcean tokens* screen ([https://cloud.digitalocean.com/settings/api/tokens](https://cloud.digitalocean.com/settings/api/tokens))
    and click theGenerate New Token button. You''ll be presented with the New personal
    access token popup, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/new-personal-access-token.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-1: DigitalOcean new personal access token screen'
  prefs: []
  type: TYPE_NORMAL
- en: Type `devops21` as Token name and click the Generate Token button. You'll see
    the newly generated token. We'll put it into the environment variable `DIGITALOCEAN_ACCESS_TOKEN`.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the Gist `12-digital-ocean.sh` ([https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e](https://gist.github.com/vfarcic/81248d2b6551f6a1c2bcfb76026bae5e)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the token before running the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual token.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can decide which region our cluster will run in.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the currently available regions by sending a request to [https://api.digitalocean.com/v2/regions](https://api.digitalocean.com/v2/regions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We sent an HTTP `GET` request to the regions API. The request contains the access
    token. The response is piped to `jq`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the bottom of the response, DigitalOcean currently supports
    twelve regions. Each contains the information about available droplet sizes and
    the supported features.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, I will be using **San Francisco 2 **(**sfo2**) region.
    Feel free to change it to the region closest to your location. If you choose to
    run the examples in a different region, please make sure that it contains the
    `private_networking` feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll put the region inside the environment variable `DIGITALOCEAN_REGION`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we are all set with the prerequisites that will allow us to create the first
    Swarm cluster in DigitalOcean. Since we used Docker Machine throughout most of
    the book, it will be our first choice.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster with Docker Machine and DigitalOcean API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll continue using the `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository. It contains configurations and scripts that''ll help us out. You already
    have it cloned. To be on the safe side, we''ll pull the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the first droplet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We specified that the *Docker Machine* should use the `digitalocean` driver
    to create an instance in the region we defined as the environment variable `DIGITALOCEAN_REGION`.
    The size of the droplet is 1 GB and it has private networking enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Machine launched a DigitalOcean droplet, provisioned it with Ubuntu,
    and installed and configured Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: As you no doubt already noticed, everyone is trying to come up with a different
    name for the same thing. DigitalOcean is no exception. They came up with the term
    *droplet*. It is a different name for a virtual private server. Same thing, different
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can initialize the cluster. We should use private IPs for all communication
    between nodes. Unfortunately, `docker-machine ip` command returns only the public
    IP, so we'll have to resort to a different method to get the private IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can send a `GET` request to the droplets API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `droplets` API returned all information about all the droplets we own (at
    the moment only one). We are interested only in the private IP of the newly created
    instance called `swarm-1`. We can get it by filtering the results to include only
    the droplet named `swarm-1` and selecting the `v4` element with the type `private`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use `jq` ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    to filter the output and get what we need. If you haven't already, please download
    and install the jq distribution suited for your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that sends the request, filters the result, and stores the private
    IP as an environment variable is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We sent a `GET` request to the `droplets` API, used the `jq select` statement
    to discard all the entries except the one with the name `swarm-1`. That was followed
    with another select statement that returned only the private address. The output
    was stored as the environment variable `MANAGER_IP`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be on the safe side, we can echo the value of the newly created variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can execute the `swarm init` command in the same way as we did in the
    previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that the cluster is indeed initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we initialized the cluster, we can add more nodes. We''ll start by
    creating two new instances and joining them as managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There's no need to explain the commands we just executed since they are the
    combination of those we used before.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll add a few worker nodes as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that all five nodes are indeed forming the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Our cluster is ready. The only thing left is to deploy a few services
    and confirm that the cluster is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already created the services quite a few times, we''ll speed up the
    process with the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    Compose stacks. They''ll create the `proxy`, `swarm-listener`, `go-demo-db`, and
    `go-demo` services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Non-Windows users do not need to enter the `swarm-1` machine and can accomplish
    the same result by deploying the stacks directly from their laptops.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''ll take a few moments until all the images are downloaded. After a while,
    the output of the `service ls command` should be as follows (IDs are removed for
    brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that the `go-demo` service is accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We set up the whole Swarm cluster using Docker Machine and the DigitalOcean
    API. Is that everything we need? That depends on the requirements we might define
    for our cluster. We should probably add a few Floating IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: A DigitalOcean Floating IP is a publicly-accessible static IP address that can
    be assigned to one of your Droplets. A Floating IP can also be instantly remapped,
    via the DigitalOcean Control Panel or API, to one of your other Droplets in the
    same data center. This instant remapping capability grants you the ability to
    design and create **High Availability **(**HA**) server infrastructure setups
    that do not have a single point of failure, by adding redundancy to the entry
    point, or gateway, to your servers.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we should probably set at least two Floating IPs and map them
    to two of the droplets in the cluster. Those two (or more) IPs would be set as
    our DNS records. That way, when an instance fails, and we replace it with a new
    one, we can remap the Elastic IP without affecting our users.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few other improvements we could do. However, that would put
    us in an awkward position. We would be using a tool that is not meant for setting
    up a complicated cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The creation of VMs was quite slow. Docker Machine spent too much time provisioning
    it with Ubuntu and installing Docker Engine. We can reduce that time by creating
    snapshots with Docker Engine pre-installed. However, with such an action, the
    main reason for using Docker Machine would be gone. Its primary usefulness is
    simplicity. Once we start complicating the setup with other resources, we'll realize
    that the simplicity is being replaced with too many ad-hoc commands.
  prefs: []
  type: TYPE_NORMAL
- en: Running `docker-machine` combined with API requests works great when we are
    dealing with a small cluster, especially when we want to create something fast
    and potentially not very durable. The biggest problem is that everything we've
    done so far has been ad-hoc commands. Chances are that we would not be able to
    reproduce the same steps the second time. Our infrastructure is not documented
    so our team would not know what constitutes our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: My recommendation is to use `docker-machine` in DigitalOcean as a quick and
    dirty way to create a cluster mostly for demo purposes. It can be useful for production
    as well, as long as the cluster is relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: We should look at alternatives if we'd like to set up a more complex, bigger,
    and potentially more permanent solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us delete the cluster we created and explore the alternatives with a clean
    slate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If you read the previous chapter, at this point you are probably expecting to
    see a sub-chapter named *Docker for DigitalOcean*. There is no such thing. At
    least not at the time I'm writing this chapter. Therefore, we'll jump right into
    *Packer* and *Terraform*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster with Packer and Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This time we'll use a set of tools completely unrelated to Docker. It'll be
    *Packer *([https://www.packer.io/](https://www.packer.io/)) and *Terraform* ([https://www.terraform.io/](https://www.terraform.io/)).
    Both are coming from *HashiCorp* ([https://www.hashicorp.com/](https://www.hashicorp.com/)).
    Packer allows us to create machine images. With Terraform we can create, change,
    and improve cluster infrastructure. Both tools support almost all the major providers.
  prefs: []
  type: TYPE_NORMAL
- en: They can be used with Amazon EC2, CloudStack, DigitalOcean, **Google Compute
    Engine** (**GCE**), Microsoft Azure, VMWare, VirtualBox, and quite a few others.
    The ability to be infrastructure agnostic allows us to avoid vendor lock-in. With
    a minimal change in configuration, we can easily transfer our cluster from one
    provider to another. Swarm is designed to work seamlessly no matter which hosting
    provider we use, as long as the infrastructure is properly defined. With Packer
    and Terraform we can define infrastructure in such a way that transitioning from
    one to another is as painless as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Using Packer to create DigitalOcean snapshots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository already has the Packer and Terraform configurations we''ll use. They
    are located in the directory `terraform/do`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to use Packer to create a snapshot. To do that, we''ll need
    our DigitalOcean API token set as the environment variable `DIGITALOCEAN_API_TOKEN`.
    It is the same token we set as the environment variable `DIGITALOCEAN_ACCESS_TOKEN`.
    Unfortunately, Docker Machine and Packer have different naming standards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual token.
  prefs: []
  type: TYPE_NORMAL
- en: We'll instantiate all Swarm nodes from the same snapshot. It'll be based on
    Ubuntu and have the latest Docker Engine installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON definition of the image we are about to build is in `terraform/do/packer-ubuntu-docker.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/packer-ubuntu-docker.json)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration consists of two sections: `builders` and `provisioners`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `builders` section defines all the information Packer needs to build a snapshot.
    The `provisioners` section describes the commands that will be used to install
    and configure software for the machines created by builders. The only required
    section is builders.
  prefs: []
  type: TYPE_NORMAL
- en: Builders are responsible for creating machines and generating images from them
    for various platforms. For example, there are separate builders for EC2, VMware,
    VirtualBox, and so on. Packer comes with many builders by default, and can also
    be extended to add new builders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The builders section we''ll use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Each type of builder has specific arguments that can be used. We specified that
    the `type` is `digitalocean`. Please visit the DigitalOcean Builder page ([https://www.packer.io/docs/builders/digitalocean.html](https://www.packer.io/docs/builders/digitalocean.html)) for
    more info.
  prefs: []
  type: TYPE_NORMAL
- en: Please note, when using the `digitalocean` type, we have to provide the token.
    We could have specified it through the `api_token` field. However, there is an
    alternative. If the field is not specified, Packer will try to get the value from
    the environment variable `DIGITALOCEAN_API_TOKEN`. Since we already exported it,
    there's no need to repeat ourselves with the token inside Packer configuration.
    Moreover, the token should be secret.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it inside the config would risk exposure.The region is critical since
    a snapshot can be created only within one region. If we wanted to share the same
    machine across multiple regions, each would need to be specified as a separate
    builder.
  prefs: []
  type: TYPE_NORMAL
- en: We set the image to `ubuntu-16-04-x64`. That will be the base image which we'll
    use to create our own. The size of the snapshot is not directly related to the
    size of the droplets we'll create, so there's no need to make it big. We set it
    to 1 GB.
  prefs: []
  type: TYPE_NORMAL
- en: By default, DigitalOcean enables only public networking, so we defined `private_networking`
    as `true`. Later on, we'll set up Swarm communication to be available only through
    the private network.
  prefs: []
  type: TYPE_NORMAL
- en: The `snapshot_name` field is the name we'll give to this snapshot. Since there
    is no option to overwrite an existing snapshot, the name must be unique so we
    added `timestamp` to the name.
  prefs: []
  type: TYPE_NORMAL
- en: Please visit the DigitalOcean Builder page ([https://www.packer.io/docs/builders/digitalocean.html](https://www.packer.io/docs/builders/digitalocean.html)) for
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: The second section is provisioners. It contains an array of all the provisioners
    that Packer should use to install and configure software within running machines
    before turning them into snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few `provisioner` types we can use. If you read *The DevOps
    2.0 Toolkit *([https://www.amazon.com/dp/B01BJ4V66M](https://www.amazon.com/dp/B01BJ4V66M)),
    you know that I advocated Ansible as the `provisioner` of choice. Should we use
    it here as well? In most cases, when building images meant to run Docker containers,
    I opt for a simple shell. The reasons for a change from Ansible to Shell lies
    in the different objectives `provisioners` should fulfill when running on live
    servers, as opposed to building images.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Shell, Ansible (and most other `provisioners`) are idempotent. They verify
    the actual state and execute one action or another depending on what should be
    done for the desired state to be fulfilled. That's a great approach since we can
    run Ansible as many times as we want and the result will always be the same. For
    example, if we specify that we want to have JDK 8, Ansible will SSH into a destination
    server, discover that the JDK is not present and install it. The next time we
    run it, it'll discover that the JDK is already there and do nothing. Such an approach
    allows us to run Ansible playbooks as often as we want and it'll always result
    in JDK being installed. If we tried to accomplish the same through a Shell script,
    we'd need to write lengthy `if/else` statements. If JDK is installed, do nothing,
    if it's not installed, install it, if it's installed, but the version is not correct,
    upgrade it, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: So, why not use it with Packer? The answer is simple. We do not need idempotency
    since we'll run it only once while creating an image. We won't use it on running
    instances. Do you remember the *pets vs cattle* discussion? Our VMs will be instantiated
    from an image that already has everything we need. If the state of that VM changes,
    we'll terminate it and create a new one. If we need to do an upgrade or install
    additional software, we won't do it inside the running instance, but create a
    new image, destroy running instances, and instantiate new ones based on the updated
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Is idempotency the only reason we would use Ansible? Definitely not! It is a
    very handy tool when we need to define complicated server setup. However, in our
    case the setup is simple. We need Docker Engine and not much more. Almost everything
    will be running inside containers. Writing a few Shell commands to install Docker
    is easier and faster than defining Ansible playbooks.
  prefs: []
  type: TYPE_NORMAL
- en: It would probably take the same number of commands to install Ansible as to
    install Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Long story short, we'll use shell as our `provisioner` of choice for building
    AMIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `provisioners` section we''ll use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The shell type is followed by a set of commands. They are the same as the commands
    we can find in the Install Docker *on Ubuntu* ([https://docs.docker.com/engine/installation/linux/ubuntulinux/](https://docs.docker.com/engine/installation/linux/ubuntulinux/))
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a general idea how Packer configuration works, we can proceed
    and build an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We run the `packer build` of the `packer-ubuntu-docker.json` with the `machine-readable`
    output sent to the `packer-ubuntu-docker.log` file. Machine readable output will
    allow us to parse it easily and retrieve the ID of the snapshot we just created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the confirmation that the build was successful, the relevant part
    of the output is the line `id,sfo2:21373017`. It contains the snapshot ID we'll
    need to instantiate VMs based on the image. You might want to store the `packer-ubuntu-docker.log`
    in your code repository in case you need to get the ID from a different server.
  prefs: []
  type: TYPE_NORMAL
- en: The flow of the process we executed can be described through *figure 12-2:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cloud-architecture-images-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-2: The flow of the Packer process'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to create a Swarm cluster with VMs based on the snapshot we
    built.
  prefs: []
  type: TYPE_NORMAL
- en: Using Terraform to create a Swarm cluster in DigitalOcean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Terraform is the third member of the *everyone-uses-a-different-environment-variable-for-the-token*
    club. It expects the token to be stored as environment variable `DIGITALOCEAN_TOKEN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual token.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform does not force us to have any particular file structure. We can define
    everything in a single file. However, that does not mean that we should. Terraform
    configs can get big, and separation of logical sections into separate files is
    often a good idea. In our case, we'll have three `tf` files. The `terraform/do/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf))
    holds all the variables. If we need to change any parameter, we'll know where
    to find it. The `terraform/do/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf))
    file contains definitions of the elements that might be potentially reusable on
    other occasions. Finally, the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    file has the Swarm-specific resources. We'll explore each of the Terraform configuration
    files separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of the `terraform/do/variables.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/variables.tf))
    file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `swarm_manager_token` and `swarm_worker_token` will be required to join
    the nodes to the cluster. The `swarm_snapshot_id` will hold the ID of the snapshot
    we created with Packer. The `swarm_manager_ip` variable is the IP of one of the
    managers that we'll need to provide for the nodes to join the cluster. The `swarm_managers`
    and `swarm_workers` define how many nodes we want of each. The `swarm_region`
    defines the region our cluster will run in while the `swarm_instance_size` is
    set to 1 GB. Feel free to change it to a bigger size if you start using this Terraform
    config to create a real cluster. Finally, the `swarm_init` variable allows us
    to specify whether this is the first run and the node should initialize the cluster.
    We'll see its usage very soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of the `terraform/do/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/common.tf))
    file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Each resource is defined with a type (For example, `digitalocean_ssh_key`) and
    a name (For example, `docker`). The type determines which resource should be created
    and must be one of those currently supported.
  prefs: []
  type: TYPE_NORMAL
- en: The first resource `digitalocean_ssh_key` allows us to manage SSH keys for droplet
    access. Keys created with this resource can be referenced in your droplet configuration
    via their ID or fingerprint. We set it as the value of the `devops21-do.pub` file
    that we'll create soon.
  prefs: []
  type: TYPE_NORMAL
- en: The second resource we're using is the `digitalocean_floating_ip`. It represents
    a publicly-accessible static IP address that can be mapped to one of our droplets.
    We defined three of those. They would be used in our DNS configuration. That way,
    when a request is made to your domain, DNS redirects it to one of the floating
    IPs. If one the droplets is down, DNS should use one of the other entries. That
    way, you'd have time to change the floating IP from the failed to a new droplet.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult the *DIGITALOCEAN_SSH_KEY* ([https://www.terraform.io/docs/providers/do/r/ssh_key.html](https://www.terraform.io/docs/providers/do/r/ssh_key.html))
    and *DIGITALOCEAN_FLOATING_IP* ([https://www.terraform.io/docs/providers/do/r/floating_ip.html](https://www.terraform.io/docs/providers/do/r/floating_ip.html))
    pages for more info.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the resources, we also defined a few outputs. They represent values
    that will be displayed when Terraform apply is executed and can be queried easily
    using the output command.
  prefs: []
  type: TYPE_NORMAL
- en: When building potentially complex infrastructure, Terraform stores hundreds
    or thousands of attribute values for all resources. But, as a user, we may only
    be interested in a few values of importance, such as manager IPs. Outputs are
    a way to tell Terraform what data is relevant.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the outputs are the addresses of the floating IPs.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult the *Output Configuration* ([https://www.terraform.io/docs/configuration/outputs.html](https://www.terraform.io/docs/configuration/outputs.html))
    page for more info.
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the real deal. The `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    file contains the definition of all the instances we'll create.
  prefs: []
  type: TYPE_NORMAL
- en: Since the content of this file is a bit bigger than the others, we'll examine
    each resource separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first resource in line is the `digitalocean_droplet` type named `swarm-manager`.
    Its purpose is to create Swarm manager nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The resource contains the image that references the snapshot we created with
    Packer. The actual value is a variable that we'll define at runtime. The size
    specifies the size of the instance we want to create. The default value is fetched
    from the variable `swarm_instance_size`. By default, it is set to 1 GB. Just as
    any other variable, it can be overwritten at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The count field defines how many managers we want to create. The first time
    we run terraform, the value should be 1 since we want to start with one manager
    that will initialize the cluster. Afterward, the value should be whatever is defined
    in variables. We'll see the use case of both combinations soon.
  prefs: []
  type: TYPE_NORMAL
- en: The name, the region, and the `private_networking` should be self-explanatory.
    The ssh-keys type is an array that, at this moment, contains only one element;
    the ID of the `digitalocean_ssh_key` resource we defined in the `common.tf` file.
  prefs: []
  type: TYPE_NORMAL
- en: The connection field defines the SSH connection details. The user will be root.
    Instead of a password, we'll use the `devops21-do` key.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `provisioner` is defined. The idea is to do as much provisioning
    as possible during the creation of the images. That way, instances are created
    much faster since the only action is to create a VM out of an image. However,
    there is often a part of provisioning that cannot be done when creating an image.
    The `swarm init` command is one of those. We cannot initialize the first Swarm
    node until we get the IP of the server. In other words, the server needs to be
    running (and therefore has an IP) before the `swarm init` command is executed.
  prefs: []
  type: TYPE_NORMAL
- en: Since the first node has to initialize the cluster while any other should join,
    we're using `if` statements to distinguish one case from the other. If the variable
    `swarm_init` is `true`, the `docker swarm init` command will be executed. On the
    other hand, if the variable `swarm_init` is set to `false`, the command will be
    `docker swarm join`. In that case, we are using another variable `swarm_manager_ip` to
    tell the node which manager to use to join the cluster. Please note that the IP
    is obtained using the special syntax `self.ipv4_address_private`. We are referencing
    oneself and getting the `ipv4_address_private`. There are many other attributes
    we can get from a resource. Please consult the *DIGITALOCEAN_DROPLET* ([https://www.terraform.io/docs/providers/do/r/droplet.html](https://www.terraform.io/docs/providers/do/r/droplet.html))
    page for more info.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `digitalocean_droplet` resource named `swarm-worker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `swarm-worker` resource is almost identical to `swarm-manager`. The only
    difference is in the count field that uses the `swarm_workers` variable and the
    `provisioner`. Since a worker cannot initialize a cluster, there was no need for
    `if` statements, so the only command we want to execute is `docker swarm join`.
    Terraform uses a naming convention that allows us to specify values as environment
    variables by adding the `TF_VAR_ prefix`. For example, we can specify the value
    of the variable `swarm_snapshot_id` by setting the environment variable `TF_VAR_swarm_snapshot_id`.
    The alternative is to use the `-var` argument. I prefer environment variables
    since they allow me to specify them once instead of adding `-var` to every command.
    The last part of the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    specification are outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outputs we defined are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: They are public and private IPs of the managers. Since there are only a few
    (if any) reasons to know worker IPs, we did not define them as outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''ll use the snapshot we created with Packer, we need to retrieve the
    ID from the `packer-ubuntu-docker.log`. Let''s take another look at the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The important line of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The command that follows parses the output and extracts the ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s double-check that the command worked as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We got the ID of the snapshot. Before we start creating the resources, we need
    to create the SSH key `devops21-do` we referenced in the Terraform config.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create the SSH key using `ssh-keygen`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: When asked to *enter file in which to save the key*, please answer with `devops21-do`.
    The rest of the questions can be answered in any way you see fit. I'll leave them
    all empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should be similar to the one that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now that the `devops21-do` key is created, we can start using Terraform. Before
    we create our cluster and the infrastructure around it, we should ask Terraform
    to show us the execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to Terraform v0.8+users. **'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, we would not need to specify targets to see the whole execution plan.
    However, Terraform *v0.8* introduced a bug that sometimes prevents us from outputting
    a plan if a resource has a reference to another, not yet created, resource. In
    this case, the `digitalocean_floating_ip.docker_2` and `digitalocean_floating_ip.docker_3`
    are such resources. The targets from the command that follows are intended to
    act as a workaround until the problem is fixed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an extensive list of resources and their properties. Since the
    output is too big to be printed, I''ll limit the output only to the resource types
    and names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Since this is the first execution, all the resources would be created if we
    were to execute `terraform apply`. We would get five droplets; three managers
    and two workers. That would be accompanied by three floating IPs and one SSH key.
  prefs: []
  type: TYPE_NORMAL
- en: If you see the complete output, you'll notice that some of the property values
    are set to `<computed>`. That means that Terraform cannot know what will be the
    actual values until it creates the resources. A good example is IPs. They do not
    exist until the droplet is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also output the plan using the `graph` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: That, in itself, is not very useful.
  prefs: []
  type: TYPE_NORMAL
- en: The `graph` command is used to generate a visual representation of either a
    configuration or an execution plan. The output is in the DOT format, which can
    be used by GraphViz to make graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Please open *Graphviz* Download page ([http://www.graphviz.org/Download.php](http://www.graphviz.org/Download.php))
    and download and install the distribution compatible with your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can combine the `graph` command with `dot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The output should be the same as in the *Figure 11-10:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/terraform-graph-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-3: The image generated by Graphviz from the output of the terraform
    graph command'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization of the plan allows us to see the dependencies between different
    resources. In our case, all resources will use the `digitalocean` provider. Both
    instance types will depend on the SSH key docker, and the floating IPs will be
    attached to manager droplets.
  prefs: []
  type: TYPE_NORMAL
- en: When dependencies are defined, we don't need to specify explicitly all the resources
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s take a look at the plan Terraform will generate when
    we limit it only to one Swarm manager node so that we can initialize the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The runtime variables `swarm_init` and `swarm_managers` will be used to tell
    Terraform that we want to initialize the cluster with one manager. The plan command
    takes those variables into account and outputs the execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output, limited only to resource types and names, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Even though the specified that we want only the plan for the `swarm-manager`
    resource, Terraform noticed that it depends on the SSH key docker, and included
    it in the execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start small and create only one manager instance that will initialize
    the cluster. As we saw from the plan, it depends on the SSH key, so Terraform
    will create it as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The output is too big to be presented fully in the book. If you look at it from
    your terminal, you'll notice that the SSH key is created first since `swarm-manager`
    depends on it. Please note that we did not specify the dependency explicitly.
    However, since the resource has it specified in the `ssh_keys` field, Terraform
    understood that it is the dependency.
  prefs: []
  type: TYPE_NORMAL
- en: Once the `swarm-manager` instance is created, Terraform waited until SSH access
    is available. After it had managed to connect to the new instance, it executed
    provisioning commands that initialized the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The outputs are defined at the bottom of the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    file. Please note that not all outputs are listed but only those of the resources
    that were created.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the public IP of the newly created droplet and SSH into it.
  prefs: []
  type: TYPE_NORMAL
- en: You might be inclined to copy the IP. There's no need for that. Terraform has
    a command that can be used to retrieve any information we defined as the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that retrieves the public IP of the first, and currently the only
    manager is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can leverage the output command to construct SSH commands. As an example,
    the command that follows will SSH into the machine and retrieve the list of Swarm
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'From now on, we won''t be limited to a single manager node that initialized
    the cluster. We can create all the rest of the nodes. However, before we do that,
    we need to discover the manager and worker tokens. For security reasons, it is
    better that they are not stored anywhere, so we''ll create environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also need to set the environment variable `swarm_manager_ip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Even though we could use `digitalocean_droplet.swarm-manager.0.private_ip` inside
    the `terraform/do/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/do/swarm.tf))
    it is a good idea to have it defined as an environment variable. That way, if
    the first manager fails, we can easily change it to `swarm_manager_2_private_ip`
    without modifying the `.tf` files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us see the plan for the creation of the rest of Swarm nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the plan is to create four new resources. Since we already have
    one manager running and specified that the desired number is three, two additional
    managers will be created together with two workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply the execution plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The last lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: All four resources were created, and we got the output of the manager public
    and private IPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s enter into one of the managers and confirm that the cluster indeed works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `node ls` command is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: All the nodes are present, and the cluster seems to be working.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be fully confident that everything works as expected, we''ll deploy a few
    services. Those will be the same services we were creating throughout the book,
    so we''ll save us some time and deploy the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))
    stacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We downloaded the stacks from the repositories and executed the `stack deploy`
    commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we have to do now is wait for a few moments, execute the `service ls` command,
    and confirm that all the replicas are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `service ls` command should be as follows (IDs are removed
    for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s send a request to the `go-demo` service through the `proxy`.
    If it returns the correct response, we''ll know that everything works correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: It works!
  prefs: []
  type: TYPE_NORMAL
- en: 'Are we finished? We probably are. As a last check, let''s validate that the
    `proxy` is accessible from outside the servers. We can confirm that by exiting
    the server and sending a request from our laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We are still missing floating IPs. While they are not necessary for this demo,
    we would create them if this were a production cluster, and use them to configure
    our DNSes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we can create the plan without specifying the targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant parts of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Terraform detected that all the resources except floating IPs
    are already created so it created the plan that would execute the creation of
    only three resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply the plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The floating IPs were created and we can see the output of their IPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing left is to confirm that floating IPs are indeed created and
    configured correctly. We can do that by sending a request through one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the output is status `200 OK`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Let's see what happens if we simulate a failure of an instance.
  prefs: []
  type: TYPE_NORMAL
- en: We'll delete an instance using the DigitalOcean API. We could use Terraform
    to remove an instance. However, removing it with the API will be a closer simulation
    of an unexpected failure of a node.
  prefs: []
  type: TYPE_NORMAL
- en: To remove an instance, we need to find its ID. We can do that with the `terraform
    show` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we want to remove the second worker. The command to find all
    its information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Among other pieces of data, we got the ID. In my case, it is `33909722`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the command that follows, please change the ID to the one you
    got from the `terraform state show` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: DigitalOcean does not provide any response content to `DELETE` requests, so
    the status `204` indicates that the operation was successful.
  prefs: []
  type: TYPE_NORMAL
- en: It will take a couple of moments until the droplet is removed entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `terraform plan` command one more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant parts of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Terraform deduced that one resource `swarm-worker.1` needs to be added to reconcile
    the discrepancy between the state it has stored locally and the actual state of
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we have to do to restore the cluster to the desirable state is to run `terraform
    apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant parts of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We can see that one resource was added. The terminated worker has been recreated,
    and the cluster continues operating at its full capacity.
  prefs: []
  type: TYPE_NORMAL
- en: The state of the cluster is stored in the `terraform.tfstate` file. If you are
    not running it always from the same computer, you might want to store that file
    in your repository together with the rest of your configuration files. The alternative
    is to use Remote State ([https://www.terraform.io/docs/state/remote/index.html](https://www.terraform.io/docs/state/remote/index.html))
    and, for example, store it in Consul.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the desired state of the cluster is easy as well. All we have to to
    is add more resources and rerun `terraform apply`.
  prefs: []
  type: TYPE_NORMAL
- en: We are finished with the brief introduction to Terraform for DigitalOcean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of the process we executed can be described through *Figure 12-4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cloud-architecture-instances-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-4: The flow of the Terraform process'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s destroy what we did before we compare the different approaches we took
    to create and manage a Swarm cluster in DigitalOcean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The cluster is gone as if it never existed, saving us from unnecessary expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to remove a snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: Before we remove the snapshot we created, we need to find its `ID`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The request that will return the list of all snapshots is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use `jq` to get the snapshot ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: We sent an `HTTP GET` request to retrieve all snapshots and used `jq` to retrieve
    only the ID. The result was stored in the environment variable `SNAPSHOT_ID`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can send a `DELETE` request that will remove the snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant output of the response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: The snapshot has been removed. No resources are running on the DigitalOcean
    account, and you will not be charged anything more than what you spent from running
    the exercises in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right tools to create and manage Swarm clusters in DigitalOcean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We tried two different combinations to create a Swarm cluster in DigitalOcean.
    We used *Docker Machine* with the *DigitalOcean API* and *Packer* with *Terraform*.
    That is, by no means, the final list of the tools we can use. The time is limited,
    and I promised to myself that this book will be shorter than *War and Peace*,
    so I had to draw the line somewhere. Those two combinations are, in my opinion,
    the best candidates as your tools of choice. Even if you do choose something else,
    this chapter, hopefully, gave you an insight into the direction you might want
    to take.
  prefs: []
  type: TYPE_NORMAL
- en: Most likely you won't use both combinations so the million dollar question is
    which one should it be?
  prefs: []
  type: TYPE_NORMAL
- en: Only you can answer that question. Now you have the practical experience that
    should be combined with the knowledge of what you want to accomplish. Each use
    case is different, and no combination would be the best fit for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I will provide a brief overview and some of the use-cases that
    might be a good fit for each combination.
  prefs: []
  type: TYPE_NORMAL
- en: To Docker Machine or not to Docker Machine?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Machine is the weaker solution we explored. It is based on ad-hoc commands
    and provides little more than a way to create droplets and install Docker Engine.
    It uses *Ubuntu 15.10* as the base snapshot. Not only that it is old but is a
    temporary release. If we choose to use Ubuntu, the correct choice is 16.04 **Long
    Term Support **(**LTS**).
  prefs: []
  type: TYPE_NORMAL
- en: If Docker Machine would, at least, provide the minimum setup for Swarm Mode
    (as it did with the old Standalone Swarm), it could be a good choice for a small
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As it is now, the only true benefit Docker Machine provides when working with
    a Swarm cluster in DigitalOcean is Docker Engine installation on a remote node
    and the ability to use the `docker-machine env` command to make our local Docker
    client seamlessly communicate with the remote cluster. Docker Engine installation
    is simple so that alone is not enough. On the other hand, `docker-machine env`
    command should not be used in a production environment. Both benefits are too
    weak.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the current problems with Docker Machine can be fixed with some extra
    arguments (For example, `--digitalocean-image`) and in combination with other
    tools. However, that only diminishes the primary benefit behind Docker Machine.
    It was supposed to be simple and work out of the box. That was partly true before
    Docker 1.12\. Now, at least in DigitalOcean, it is lagging behind.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean we should discard Docker Machine when working with DigitalOcean?
    Not always. It is still useful when we want to create an ad-hoc cluster for demo
    purposes or maybe experiment with some new features. Also, if you don't want to
    spend time learning other tools and just want something you're familiar with,
    Docker Machine might be the right choice. I doubt that's your case. The fact that
    you reached this far in this book tells me that you do want to explore better
    ways of managing a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The final recommendation is to keep Docker Machine as the tool of choice when
    you want to simulate a Swarm cluster locally as we did in the previous chapters.
    There are better choices for DigitalOcean.
  prefs: []
  type: TYPE_NORMAL
- en: To Terraform or not to Terraform?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Terraform, when combined with Packer, is an excellent choice. HashiCorp managed
    to make yet another tool that changes the way we configure and provision servers.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration management tools have as their primary objective the task of making
    a server always be in the desired state. If a web server stops, it will be started
    again. If a configuration file is changed, it will be restored. No matter what
    happens to a server, its desired state will be restored. Except, when there is
    no fix to the issue. If a hard disk fails, there's nothing configuration management
    can do.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with configuration management tools is that they were designed to
    work with physical, not virtual servers. Why would we fix a faulty virtual server
    when we can create a new one in a matter of seconds? Terraform understands how
    cloud computing works better than anyone and embraces the idea that our servers
    are not pets anymore. They are cattle. It'll make sure that all your resources
    are available.
  prefs: []
  type: TYPE_NORMAL
- en: When something is wrong on a server, it will not try to fix it. Instead, it
    will destroy it and create a new one based on the image we choose.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean that there is no place for Puppet, Chef, Ansible, and other similar
    tools? Are they obsolete when operating in the cloud? Some are more outdated than
    others. Puppet and Chef are designed to run an agent on each server, continuously
    monitoring its state and modifying it if things go astray. There is no place for
    such tools when we start treating our servers as cattle. Ansible is in a bit better
    position since it is more useful than others, as a tool designed to configure
    a server instead of monitor it. As such, it could be very helpful when creating
    images.
  prefs: []
  type: TYPE_NORMAL
- en: We can combine Ansible with Packer. Packer would create a new VM, Ansible would
    provision that VM with everything we need, and leave it to Packer to create an
    image out of it. If the server setup is complicated, that makes a lot of sense.
    We don't create a lot of system users since we do not log into a machine to deploy
    software. Swarm does that for us. We do not install web servers and runtime dependencies
    anymore. They are inside containers. Is there a true benefit from using configuration
    management tools to install a few things into VMs that will be converted into
    images? More often than not, the answer is no. The few things we need can be just
    as easily installed and configured with a few Shell commands. Configuration management
    of our cattle can, and often should, be done with bash.
  prefs: []
  type: TYPE_NORMAL
- en: I might have been too harsh. Ansible is still a great tool if you know when
    to use it and for what purpose. If you prefer it over bash to install and configure
    a server before it becomes an image, go for it. If you try to use it to control
    your nodes and create DigitalOcean resources, you're on a wrong path. Terraform
    does that much better. If you think that it is better to provision a running node
    instead of instantiating images that already have everything inside, you must
    have much more patience than I do.
  prefs: []
  type: TYPE_NORMAL
- en: The final recommendation is to use *Terraform* with *Packer* if you want to
    have control of all the pieces that constitute your cluster, or if you already
    have a set of rules that need to be followed. Be ready to spend some time tuning
    the configs until you reach the optimum setup. Unlike AWS that, for good or bad,
    forces us to deal with many types of resources, DigitalOcean is simple. You create
    droplets, add a few floating IPs, and that's it. You might want to install a firewall
    on the machines. If you do, the best way would be to do that during the creation
    of a snapshot with Packer. It is questionable whether a firewall is needed when
    using Swarm networking, but that's a discussion for some other time.
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no such thing as *Docker for DigitalOcean*, Terraform is a clear
    winner. DigitalOcean is simple, and that simplicity is reflected through Terraform
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The final verdict
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Terraform wins over Docker Machine in a landsilde. If there is such a thing
    as *Docker for DigitalOcean*, this discussion would be longer. As it is now, the
    choice is easy. If you choose DigitalOcean, manage your cluster with Packer and
    Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: To DigitalOcean or not to DigitalOcean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally speaking, I like products and services that are focused on very few
    things and do them well. DigitalOcean is one of those. It is an **Infrastructure
    as a Service**(**IaaS**) provider and nothing else. The services it provides are
    few in number (For example, floating IP) and limited to those that are a real
    necessity. If you're looking for a provider that will offer you everything you
    can imagine, choose **Amazon Web Services**(**AWS**), Azure, GCE, or any other
    cloud computing provider that aims at delivering not only hosting but also numerous
    services on top. The fact that you reached this far in this book tells me that
    there are strong chances that you are interested in setting up infrastructure
    services yourself. If that's the case, DigitalOcean is worth a try. Doing everything
    often means not doing anything really well. DigitalOcean does a few things, and
    it does them well. What is does, it does better than most.
  prefs: []
  type: TYPE_NORMAL
- en: The real question is whether you need only an **Infrastructure as a Service **(**IaaS**)
    provider or you need **Platform as a Service **(**PaaS**) as well. In my opinion,
    containers make PaaS obsolete. It will be gradually replaced with containers managed
    by schedulers (For example, *Docker Swarm*) or **Containers as a Service **(**CaaS**).
    You might not agree with me. If you do, a huge part of AWS becomes obsolete leaving
    it with EC2, storage, VPCs, and only a handful of other services. In such a case,
    DigitalOcean is a mighty competitor and an excellent choice. The few things it
    does, it does better than AWS at a lower price. The performance is impressive.
    It's enough to measure the time it requires to create a droplet and compare it
    with the time AWS requires to create and initiate an EC2 instance. The difference
    is huge. The first time I created a droplet, I thought that there's something
    wrong. My brain could not comprehend that it could be done in less than a minute.
  prefs: []
  type: TYPE_NORMAL
- en: Did I mention simplicity? DigitalOcean is simple, and I love simplicity. Therefore,
    the logical conclusion is that I love DigitalOcean. The real mastery is to make
    complex things easy to use. That's where both Docker and DigitalOcean shine.
  prefs: []
  type: TYPE_NORMAL
