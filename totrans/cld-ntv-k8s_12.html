<html><head></head><body>
		<div><p><a id="_idTextAnchor211"/></p>
			<h1 id="_idParaDest-201"><em class="italic"><a id="_idTextAnchor212"/>Chapter 9</em>: Observability on Kubernetes</h1>
			<p>This chapter dives into capabilities that are highly recommended to implement when running Kubernetes in production. First, we discuss observability in the context of distributed systems such as Kubernetes. Then, we look at the built-in Kubernetes observability stack and what functionality it implements. Finally, we learn how to supplement the built-in observability tooling with additional observability, monitoring, logging, and metrics infrastructure from the ecosystem. The skills you learn in this chapter will help you deploy observability tools to your Kubernetes cluster and enable you to understand how your cluster (and applications running on it) are functioning.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding observability on Kubernetes</li>
				<li>Using default observability tooling – metrics, logging, and the dashboard</li>
				<li>Implementing the best of the ecosystem</li>
			</ul>
			<p>To start, we will learn the out-of-the-box tools and processes that Kubernetes provides for observability.</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor213"/>Technical requirements</h1>
			<p>In order to run the commands detailed in this chapter, you will need a computer that supports the <code>kubectl</code> command-line tool along with a working Kubernetes cluster. See <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, for several methods for getting up and running with Kubernetes quickly, and for instructions on how to install the kubectl tool.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository: </p>
			<p><a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9</a></p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor214"/>Understanding observability on Kubernetes</h1>
			<p>No production system is <a id="_idIndexMarker428"/>complete without a way to monitor it. In software, we define observability as the ability to, at any point in time, understand how our system is performing (and, in the best case, why). Observability grants significant benefits in security, performance, and operational capacity. By knowing how your system is responding at the VM, container, and application level, you can tune it to perform efficiently, react quickly to events, and more easily troubleshoot bugs.</p>
			<p>For instance, let's take a scenario where your application is running extremely slowly. In order to find the bottleneck, you may look at the application code itself, the resource specifications of the Pod, the number of Pods in the deployment, the memory and CPU usage at the Pod level or Node level, and externalities such as a MySQL database running outside your cluster.</p>
			<p>By adding observability tooling, you would be able to diagnose many of these variables and figure out what issues may be contributing to your application slowdown. </p>
			<p>Kubernetes, as a production-ready container orchestration system, gives us some default tools to monitor our applications. For the purposes of this chapter, we will separate observability into four ideas: metrics, logs, traces, and alerts. Let's look at each of them:</p>
			<ul>
				<li><strong class="bold">Metrics</strong> here represents <a id="_idIndexMarker429"/>the ability to see numerical representations of the system's current state, with specific attention paid to CPU, memory, network, disk space, and more. These numbers allow us to judge the gap in current state with the system's maximum capacity and ensure that the system remains available to users.</li>
				<li><strong class="bold">Logs</strong> refers to the<a id="_idIndexMarker430"/> practice of collecting text logs from applications and systems. Logs will likely be a combination of Kubernetes control plane logs and logs from your application Pods themselves. Logs can help us diagnose the availability of the Kubernetes system, but they also can help with triaging application bugs.</li>
				<li><strong class="bold">Traces</strong> refers to collecting<a id="_idIndexMarker431"/> distributed traces. Traces are an observability pattern that delivers end-to-end visibility of a chain of requests – which can be HTTP requests or otherwise. This topic is especially important in a distributed cloud-native setting where microservices are used. If you have many microservices and they call each other, it can be difficult to<a id="_idIndexMarker432"/> find bottlenecks or issues when many services are involved in a single end-to-end request. Traces allow you to view requests broken down by each leg of a service-to-service call.</li>
				<li><strong class="bold">Alerts</strong> correspond to<a id="_idIndexMarker433"/> the practice of setting automated touch points when certain events happen. Alerts can be set on both <em class="italic">metrics</em> and <em class="italic">logs</em>, and delivered through a host of mediums, from text messages to emails to third-party applications and everything in between.</li>
			</ul>
			<p>Between these four aspects of observability, we should be able to understand the health of our cluster. However, it is possible to configure many different possible data points for metrics, logs, and even alerting. Therefore, knowing what to look for is important. The next section will discuss the most important observability areas for Kubernetes cluster and application health.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor215"/>Understanding what matters for Kubernetes cluster and application health</h2>
			<p>Among the vast <a id="_idIndexMarker434"/>number of possible metrics and logs that Kubernetes or <a id="_idIndexMarker435"/>third-party observability solutions for Kubernetes can provide, we can narrow down some of the ones that are most likely to cause major issues with your cluster. You should keep these pieces front and center in whichever observability solution you end up using. First, let's look at the connection between CPU usage and cluster health.</p>
			<h3>Node CPU usage</h3>
			<p>The state of CPU <a id="_idIndexMarker436"/>usage across the Nodes in your Kubernetes cluster is a very important metric to keep an eye on across your observability solution. We've discussed in previous chapters how Pods can define resource requests and limits for CPU usage. However, it is still possible for Nodes to oversubscribe their CPU usage when the limits are set higher than the maximum CPU capacity of the cluster. Additionally, the master Nodes that run our control plane can also encounter CPU capacity issues.</p>
			<p>Worker Nodes with maxed-out CPUs may perform poorly or throttle workloads running on Pods. This can easily occur if no limits are set on Pods – or if a Node's total Pod resource limits are greater than its max capacity, even if its total resource requests are lower. Master Nodes with capped-out CPUs may hurt the performance of the scheduler, kube-apiserver, or any of the other control plane components.</p>
			<p>In general, CPU usage across worker and master Nodes should be visible in your observability solution. This is best done via a combination of metrics (for instance on a charting solution such as Grafana, which you'll learn about later in this chapter) – and alerts for high CPU <a id="_idIndexMarker437"/>usage across the nodes in your cluster.</p>
			<p>Memory usage is also an extremely important metric to keep track of, similar to with CPU.</p>
			<h3>Node memory usage</h3>
			<p>As with CPU usage, memory <a id="_idIndexMarker438"/>usage is an extremely important metric to observe across your cluster. Memory usage can be oversubscribed using Pod Resource Limits – and many of the same issues as with CPU usage can apply for both the master and worker Nodes in the cluster.</p>
			<p>Again, a combination of alerting and metrics is important for visibility into cluster memory usage. We will learn some tools for this later in this chapter.</p>
			<p>For the next major observability piece, we will look not at metrics but at logs.</p>
			<h3>Control plane logging</h3>
			<h3>The components of the<a id="_idIndexMarker439"/> Kubernetes control plane, when running, output logs that can be used to get an in-depth view of cluster operations. These logs can also significantly help with troubleshooting, as we'll see in <a href="B14790_10_Final_PG_ePub.xhtml#_idTextAnchor230"><em class="italic">Chapter 10</em></a>, <em class="italic">Troubleshooting Kubernetes</em>. Logs for the Kubernetes API server, controller manager, scheduler, kube proxy, and kubelet can all be very useful for certain troubleshooting or observability reasons.</h3>
			<h3>Application logging</h3>
			<p>Application logging can <a id="_idIndexMarker440"/>also be incorporated into an observability stack for Kubernetes – being able to view application logs along with other metrics can be very helpful to operators. </p>
			<h3>Application performance metrics </h3>
			<p>As with application<a id="_idIndexMarker441"/> logging, application performance metrics and monitoring are highly relevant to the performance of your applications on Kubernetes. Memory usage and CPU profiling at the application level can be a valuable piece of the observability stack. </p>
			<p>Generally, Kubernetes provides the data infrastructure for application monitoring and logging but stays away from providing higher-level functionality such as charting and searching. With this in mind, let's review the tools that Kubernetes gives us by default to address these concerns.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor216"/>Using default observability tooling</h1>
			<p>Kubernetes provides<a id="_idIndexMarker442"/> observability tooling even without adding any third-party solutions. These native Kubernetes tools form the basis of many of the more robust solutions, so they are important to discuss. Since observability includes metrics, logs, traces, and alerts, we will discuss each in turn, focusing first on the Kubernetes-native solutions. First, let's discuss metrics.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor217"/>Metrics on Kubernetes</h2>
			<p>A lot of information<a id="_idIndexMarker443"/> about your applications can be gained by simply running <code>kubectl describe pod</code>. We can see information about our Pod's spec, what state it is in, and key issues preventing its functionality.</p>
			<p>Let's assume we are having some trouble with our application. Specifically, the Pod is not starting. To investigate, we run <code>kubectl describe pod</code>. As a reminder on kubectl aliases mentioned in <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, <code>kubectl describe pod</code> is the same as <code>kubectl describe pods</code>. Here is an example output from the <code>describe pod</code> command – we've stripped out everything apart from the <code>Events</code> information:</p>
			<div><div><img src="img/B14790_09_001_new.jpg" alt="Figure 9.1 – Describe Pod Events output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Describe Pod Events output</p>
			<p>As you can see, this Pod is not being scheduled because our Nodes are all out of memory! That would be a good thing to investigate further.</p>
			<p>Let's keep going. By running <code>kubectl describe nodes</code>, we can learn a lot about our Kubernetes Nodes. Some of this information can be very relevant to how our system is performing. Here's another example output, this time from the <code>kubectl describe nodes</code> command. Rather than putting the entire output here, which can be quite lengthy, let's zero in on two important sections – <code>Conditions</code> and <code>Allocated resources</code>. First, let's review the <code>Conditions</code> section:</p>
			<div><div><img src="img/B14790_09_002_new.jpg" alt="Figure 9.2 – Describe Node Conditions output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Describe Node Conditions output</p>
			<p>As you can see, we <a id="_idIndexMarker444"/>have included the <code>Conditions</code> block of the <code>kubectl describe nodes</code> command output. It's a great place to look for any issues. As we can see here, our Node is actually experiencing issues. Our <code>MemoryPressure</code> condition is true, and the <code>Kubelet</code> has insufficient memory. No wonder our Pods won't schedule! </p>
			<p>Next, check out the <code>Allocated resources</code> block:</p>
			<pre>Allocated resources:
 (Total limits may be over 100 percent, i.e., overcommitted.)
 CPU Requests	CPU Limits    Memory Requests  Memory Limits
 ------------	----------    ---------------  -------------
 8520m (40%)	4500m (24%)   16328Mi (104%)   16328Mi (104%)</pre>
			<p>Now we're seeing some metrics! It looks like our Pods are requesting too much memory, leading to our Node and Pod issues. As you can tell from this output, Kubernetes is already collecting metrics data about our Nodes, by default. Without that data, the scheduler would not be able to do its job properly, since maintaining Pod resources requests with Node capacity is one of its most important functions.</p>
			<p>However, by default, these metrics are not surfaced to the user. They are in fact being collected by each Node's <code>Kubelet</code> and delivered to the scheduler for it to do its job. Thankfully, we can easily get these metrics by deploying Metrics Server to our cluster.</p>
			<p>Metrics Server is an <a id="_idIndexMarker445"/>officially supported Kubernetes application that collects metrics information and surfaces it on an API endpoint for use. Metrics Server is in fact required to make the Horizontal Pod Autoscaler work, but it is not always included by default, depending on the Kubernetes distribution.</p>
			<p>Deploying Metrics Server is very quick. As of the writing of this book, the newest version can be installed using the following:</p>
			<pre>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">Full documentation on how to use Metrics Server can be found at <a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a>. </p>
			<p>Once Metrics Server is running, we can use a brand-new Kubernetes command. The <code>kubectl top</code> command can be used with either Pods or Nodes to see granular information about how much memory and CPU capacity is in use.</p>
			<p>Let's take a look at some example usage. Run <code>kubectl top nodes</code> to see Node-level metrics. Here's the output of the command:</p>
			<div><div><img src="img/B14790_09_003_new.jpg" alt="Figure 9.3 – Node Metrics output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Node Metrics output</p>
			<p>As you can see, we are able to see both absolute and relative CPU and memory usage. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">CPU cores are measured in <code>millcpu</code> or <code>millicores</code>. 1,000 <code>millicores</code> is equivalent to one virtual CPU. Memory is measured in bytes.</p>
			<p>Next, let's take a look <a id="_idIndexMarker446"/>at the <code>kubectl top pods</code> command. Run it with the <code>–namespace kube-system</code> flag to see Pods in the <code>kube-system</code> namespace. </p>
			<p>To do this, we run the following command:</p>
			<pre>Kubectl top pods -n kube-system </pre>
			<p>And we get the following output:</p>
			<pre>NAMESPACE     NAME                CPU(cores)   MEMORY(bytes)   
default       my-hungry-pod       8m           50Mi            
default       my-lightweight-pod  2m           10Mi       </pre>
			<p>As you can see, this command uses the same absolute units as <code>kubectl top nodes</code> – millicores and bytes. There are no relative percentages when looking at Pod-level metrics.</p>
			<p>Next, we'll look at how Kubernetes handles logging.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor218"/>Logging on Kubernetes</h2>
			<p>We can split up logging<a id="_idIndexMarker447"/> on Kubernetes into two areas – <em class="italic">application logs</em> and <em class="italic">control plane logs</em>. Let's start with control plane logs.</p>
			<h3>Control plane logs</h3>
			<p>Control plane logs<a id="_idIndexMarker448"/> refers to the logs created by the Kubernetes control plane components, such as the scheduler, API server, and others. For a vanilla Kubernetes install, control plane logs can be found on the Nodes themselves and require direct access to the Nodes in order to see. For clusters with components set up to use <code>systemd</code>, logs are found using the <code>journalctl</code> CLI tool (refer to the following link for more information: <a href="https://manpages.debian.org/stretch/systemd/journalctl.1.en.html">https://manpages.debian.org/stretch/systemd/journalctl.1.en.html</a> ).</p>
			<p>On master Nodes, you can find logs in the following locations on the filesystem:</p>
			<ul>
				<li>At <code>/var/log/kube-scheduler.log</code>, you can find the Kubernetes scheduler logs.</li>
				<li>At <code>/var/log/kube-controller-manager.log</code>, you can find the controller manager logs (for instance, to see scaling events).</li>
				<li>At <code>/var/log/kube-apiserver.log</code>, you can find the Kubernetes API server logs.</li>
			</ul>
			<p>On worker Nodes, logs are available in two locations on the filesystem:</p>
			<ul>
				<li>At <code>/var/log/kubelet.log</code>, you can find the kubelet logs.</li>
				<li>At <code>/var/log/kube-proxy.log</code>, you can find the kube proxy logs.</li>
			</ul>
			<p>Although, generally, cluster health is influenced by the health of the Kubernetes master and worker Node components, it is of course also important to keep track of your application's logs.</p>
			<h3>Application logs</h3>
			<p>It's very easy to find <a id="_idIndexMarker449"/>application logs on Kubernetes. Before we explain how it works, let's look at an example.</p>
			<p>To check logs for a specific Pod, you can use the <code>kubectl logs &lt;pod_name&gt;</code> command. The output of the command will display any text written to the container's <code>stdout</code> or <code>stderr</code>. If a Pod has multiple containers, you must include the container name in the command:</p>
			<pre>kubectl logs &lt;pod_name&gt; &lt;container_name&gt; </pre>
			<p>Under the hood, Kubernetes handles Pod logs by using the container engine's logging driver. Typically, any logs to <code>stdout</code> or <code>stderr</code> are persisted to each Node's disk in the <code>/var/logs</code> folder. Depending on the Kubernetes distribution, log rotations may be set up to prevent overuse of Node disk space by logs. In addition, Kubernetes components such as the scheduler, kubelet, and kube-apiserver also persist logs to Node disk space, usually within the <code>/var/logs</code> folder. It is important to note how limited this default logging capability is – a robust observability stack for Kubernetes would certainly include <a id="_idIndexMarker450"/>a third-party solution for log forwarding, as we'll see shortly.</p>
			<p>Next, for general Kubernetes observability, we can use Kubernetes Dashboard.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor219"/>Installing Kubernetes Dashboard</h2>
			<p>Kubernetes<a id="_idIndexMarker451"/> Dashboard provides all of the functionality of kubectl – including viewing logs and editing resources – in a GUI. It's very easy to get the dashboard set up – let's see how.</p>
			<p>The dashboard can be installed in a single <code>kubectl apply</code> command. For customizations, check out the Kubernetes Dashboard GitHub page at <a href="https://github.com/kubernetes/dashboard">https://github.com/kubernetes/dashboard</a>.</p>
			<p>To install a version of Kubernetes Dashboard, run the following <code>kubectl</code> command, replacing the <code>&lt;VERSION&gt;</code> tag with your desired version, based on the version of Kubernetes you are using (again, check the Dashboard GitHub page for version compatibility):</p>
			<pre>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/&lt;VERSION&gt; /aio/deploy/recommended.yaml</pre>
			<p>In our case, as of the writing of this book, we will use v2.0.4 – the final command looks like this: </p>
			<pre>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml</pre>
			<p>Once Kubernetes Dashboard has been installed, there are a few methods to access it. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is not usually recommended to use Ingress or a public load balancer service, because Kubernetes Dashboard allows users to update cluster objects. If for some reason your login methods for the dashboard are compromised or easy to figure out, you could be looking at a large security risk.</p>
			<p>With that in mind, we can use either <code>kubectl port-forward</code> or <code>kubectl proxy</code> in order to view our dashboard from our local machine.</p>
			<p>For this example, we will <a id="_idIndexMarker452"/>use the <code>kubectl proxy</code> command, because we haven't used it in an example yet.</p>
			<p>The <code>kubectl proxy</code> command, unlike the <code>kubectl port-forward</code> command, requires only one command to proxy to every service running on your cluster. It does this by proxying the Kubernetes API directly to a port on your local machine, which is by default <code>8081</code>. For a full discussion of the <code>Kubectl proxy</code> command, check the docs at <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy</a>.</p>
			<p>In order to access a specific Kubernetes service using <code>kubectl proxy</code>, you just need to have the right path. The path to access Kubernetes Dashboard after running <code>kubectl proxy</code> will be the following:</p>
			<pre>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</pre>
			<p>As you can see, the <code>kubectl proxy</code> path we put in our browser is on localhost port <code>8001</code>, and mentions the namespace (<code>kubernetes-dashboard</code>), the service name and selector (<code>https:kubernetes-dashboard</code>), and a proxy path.</p>
			<p>Let's put our Kubernetes Dashboard URL in a browser and see the result:</p>
			<div><div><img src="img/B14790_09_004_new.jpg" alt="Figure 9.4 – Kubernetes Dashboard login"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Kubernetes Dashboard login</p>
			<p>When we deploy and access Kubernetes Dashboard, we are met with a login screen. We can either<a id="_idIndexMarker453"/> create a Service Account (or use our own) to log in to the dashboard, or simply link our local <code>Kubeconfig</code> file. By logging in to Kubernetes Dashboard with a specific Service Account's token, the dashboard user will inherit that Service Account's permissions. This allows you to specify what type of actions a user will be able to take using Kubernetes Dashboard – for instance, read-only permissions.</p>
			<p>Let's go ahead and create a brand-new Service Account for our Kubernetes Dashboard. You could customize this Service Account and limit its permissions, but for now we will give it admin permissions. To do this, follow these steps:</p>
			<ol>
				<li>We can create a Service Account imperatively using the following Kubectl command:<pre><strong class="bold">kubectl create serviceaccount dashboard-user</strong></pre><p>This results in the following output, confirming the creation of our Service Account:</p><pre><strong class="bold">serviceaccount/dashboard-user created</strong></pre></li>
				<li>Now, we need to link our Service Account to a ClusterRole. You could also use a Role, but we want our dashboard user to be able to access all namespaces. To link a Service Account to the <code>cluster-admin</code> default ClusterRole using a single command, we can run the following:<pre><strong class="bold">kubectl create clusterrolebinding dashboard-user \--clusterrole=cluster-admin --serviceaccount=default:dashboard-user</strong></pre><p>This command will result in the following output:</p><pre><strong class="bold">clusterrolebinding.rbac.authorization.k8s.io/dashboard-user created</strong></pre></li>
				<li>After this command is run, we should be able to log in to our dashboard! First, we just need to find the token that we will use to log in. A Service Account's token is stored as<a id="_idIndexMarker454"/> a Kubernetes secret, so let's see what it looks like. Run the following command to see which secret our token is stored in:<pre><strong class="bold">kubectl get secrets</strong></pre><p>In the output, you should see a secret that looks like the following:</p><pre><strong class="bold">NAME                         TYPE                                  DATA   AGE</strong>
<strong class="bold">dashboard-user-token-dcn2g   kubernetes.io/service-account-token   3      112s</strong></pre></li>
				<li>Now, to get our token for signing in to the dashboard, we only need to describe the secret contents using the following:<pre><strong class="bold">kubectl describe secret dashboard-user-token-dcn2g</strong>   </pre><p>The resulting output will look like the following:</p><pre><strong class="bold">Name:         dashboard-user-token-dcn2g</strong>
<strong class="bold">Namespace:    default</strong>
<strong class="bold">Labels:       &lt;none&gt;</strong>
<strong class="bold">Annotations:  kubernetes.io/service-account.name: dashboard-user</strong>
              <strong class="bold">kubernetes.io/service-account.uid: 9dd255sd-426c-43f4-88c7-66ss91h44215</strong>
<strong class="bold">Type:  kubernetes.io/service-account-token</strong>
<strong class="bold">Data</strong>
<strong class="bold">====</strong>
<strong class="bold">ca.crt:</strong>     <strong class="bold">1025 bytes</strong>
<strong class="bold">namespace:  7 bytes</strong>
<strong class="bold">token: &lt; LONG TOKEN </strong><strong class="bold">HERE &gt;</strong></pre></li>
				<li>To log in to the <a id="_idIndexMarker455"/>dashboard, copy the string next to <code>token</code>, copy it into the token input on the Kubernetes Dashboard login screen, and click <strong class="bold">Sign In</strong>. You should be greeted with the Kubernetes Dashboard overview page!</li>
				<li>Go ahead and click around the dashboard – you should be able to see all the same resources you would be able to using kubectl, and you can filter by namespace in the left-hand sidebar. For instance, here's a view of the <strong class="bold">Namespaces</strong> page:<div><img src="img/B14790_09_005_new.jpg" alt="Figure 9.5 – Kubernetes Dashboard detail"/></div><p class="figure-caption">Figure 9.5 – Kubernetes Dashboard detail</p></li>
				<li>You can also click on individual resources, and even edit those resources using the dashboard as long as<a id="_idIndexMarker456"/> the Service Account you used to log in has the proper permissions. <p>Here's a view of editing a Deployment resource from the deployment detail page:</p></li>
			</ol>
			<div><div><img src="img/B14790_09_006_new.jpg" alt="Figure 9.6 – Kubernetes Dashboard edit view"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Kubernetes Dashboard edit view</p>
			<p>Kubernetes Dashboard<a id="_idIndexMarker457"/> also lets you view Pod logs and dive into many other resource types in your cluster. To understand the full capabilities of the dashboard, check the docs at the previously mentioned GitHub page.</p>
			<p>Finally, to round out our discussion of default observability on Kubernetes, let's take a look at alerting.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor220"/>Alerts and traces on Kubernetes</h2>
			<p>Unfortunately, the last <a id="_idIndexMarker458"/>two pieces of the observability puzzle – <em class="italic">alerts</em> and <em class="italic">traces</em> – are not yet native pieces of functionality on Kubernetes. In order to create this type of functionality, let's move on to our next section – incorporating<a id="_idIndexMarker459"/> open source tooling from the Kubernetes ecosystem.</p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor221"/>Enhancing Kubernetes observability using the best of the ecosystem</h1>
			<p>As we've discussed, though<a id="_idIndexMarker460"/> Kubernetes provides the basis for powerful visibility functionality, it is generally up to the community and vendor ecosystem to create higher-level tooling for metrics, logging, traces, and alerting. For the purposes of this book, we will focus on fully open source, self-hosted solutions. Since many of these solutions fulfill multiple visibility pillars between metrics, logs, traces, and alerting, instead of categorizing solutions into each visibility pillar during our review, we will review each solution separately.</p>
			<p>Let's start with an often-used combination of technologies for metrics and alerts: <strong class="bold">Prometheus</strong> and <strong class="bold">Grafana</strong>.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor222"/>Introducing Prometheus and Grafana</h2>
			<p>Prometheus and <a id="_idIndexMarker461"/>Grafana <a id="_idIndexMarker462"/>are a typical combination of visibility technologies on Kubernetes. Prometheus is a time series database, query layer, and alerting system with many integrations, while Grafana is a sophisticated graphing and visualization layer that integrates with Prometheus. We'll walk you through the installation and usage of these tools, starting with Prometheus.</p>
			<h3>Installing Prometheus and Grafana</h3>
			<p>There are many <a id="_idIndexMarker463"/>ways<a id="_idIndexMarker464"/> to install Prometheus on Kubernetes, but most use Deployments in order to scale the service. For our purposes, we will be using the <code>kube-prometheus</code> project (<a href="https://github.com/coreos/kube-prometheus">https://github.com/coreos/kube-prometheus</a>). This project includes an <code>operator</code> as well as several <strong class="bold">custom resource definitions</strong> (<strong class="bold">CRDs</strong>). It will also automatically install Grafana <a id="_idIndexMarker465"/>for us!</p>
			<p>An operator is essentially an application controller on Kubernetes (deployed like other applications in a Pod) that happens to make commands to the Kubernetes API in order to correctly run or operate its application. </p>
			<p>A CRD, on the other hand, allows us to model custom functionality inside of the Kubernetes API. We'll learn a lot more about operators and CRDs in <a href="B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289"><em class="italic">Chapter 13</em></a>, <em class="italic">Extending Kubernetes with CRDs</em>, but for now just think of operators as a way to create <em class="italic">smart deployments</em> where the application can control itself properly and spin up other Pods and Deployments as necessary – and think of CRDs as a way to use Kubernetes to store application-specific concerns.</p>
			<p>To install Prometheus, first we need to download a release, which may be different depending on the newest version of Prometheus or your intended version of Kubernetes:</p>
			<pre>curl -LO https://github.com/coreos/kube-prometheus/archive/v0.5.0.zip</pre>
			<p>Next, unzip the file using any tool. First, we're going to need to install the CRDs. In general, most Kubernetes tooling installation instructions will have you create the CRDs on Kubernetes first, since any additional setup that uses the CRD will fail if the underlying CRD has not already been created on Kubernetes.</p>
			<p>Let's install <a id="_idIndexMarker466"/>them using<a id="_idIndexMarker467"/> the following command:</p>
			<pre>kubectl apply -f manifests/setup</pre>
			<p>We'll need to wait a few seconds while the CRDs are created. This command will also create a <code>monitoring</code> namespace for our resources to live in. Once everything is ready, let's spin up the rest of the Prometheus and Grafana resources using the following:</p>
			<pre>kubectl apply -f manifests/</pre>
			<p>Let's talk about what this command will actually create. The entire stack consists of the following:</p>
			<ul>
				<li><strong class="bold">Prometheus Deployment</strong>: Pods of the Prometheus application</li>
				<li><strong class="bold">Prometheus Operator</strong>: Controls and operates the Prometheus app Pods</li>
				<li><strong class="bold">Alertmanager Deployment</strong>: A Prometheus component to specify and trigger alerts</li>
				<li><strong class="bold">Grafana</strong>: A powerful visualization dashboard</li>
				<li><strong class="bold">Kube-state-metrics agent</strong>: Generates metrics from the Kubernetes API state</li>
				<li><strong class="bold">Prometheus Node Exporter</strong>: Exports Node hardware- and OS-level metrics to Prometheus</li>
				<li><strong class="bold">Prometheus Adapter for Kubernetes Metrics</strong>: Adapter for Kubernetes Resource Metrics API and Custom Metrics API for ingest into Prometheus</li>
			</ul>
			<p>Together, all these components will provide sophisticated visibility into our cluster, from the command plane down to the application containers themselves.</p>
			<p>Once the stack has been created (check by using the <code>kubectl get po -n monitoring</code> command), we <a id="_idIndexMarker468"/>can start using our components. Let's dive <a id="_idIndexMarker469"/>into usage, starting with plain Prometheus.</p>
			<h3>Using Prometheus</h3>
			<p>Though the real<a id="_idIndexMarker470"/> power of Prometheus is in its data store, query, and alert layer, it does provide a simple UI to developers. As you'll see later, Grafana provides many more features and customizations, but it is worth it to get acquainted with the Prometheus UI.</p>
			<p>By default, <code>kube-prometheus</code> will only create ClusterIP services for Prometheus, Grafana, and Alertmanager. It's up to us to expose them outside the cluster. For the purposes of this tutorial, we're simply going to port forward the service to our local machine. For production, you may want to use Ingress to route requests to the three services.</p>
			<p>In order to <code>port-forward</code> to the Prometheus UI service, use the <code>port-forward</code> kubectl command:</p>
			<pre>Kubectl -n monitoring port-forward svc/prometheus-k8s 3000:9090</pre>
			<p>We need to use port <code>9090</code> for the Prometheus UI. Access the service on your machine at <code>http://localhost:3000</code>.</p>
			<p>You should see something like the following screenshot:</p>
			<div><div><img src="img/B14790_09_007_new.jpg" alt="Figure 9.7 – Prometheus UI"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Prometheus UI</p>
			<p>As you can see, the Prometheus UI has a <strong class="bold">Graph</strong> page, which is what you can see in <em class="italic">Figure 9.4</em>. It also has its <a id="_idIndexMarker471"/>own UI for seeing configured alerts – but it doesn't allow you to create alerts via the UI. Grafana and Alertmanager will help us for that task.</p>
			<p>To perform a query, navigate to the <code>PromQL</code> – we won't present it fully to you in this book, but the Prometheus docs are a great way to learn. You can refer to it using the following link: <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">https://prometheus.io/docs/prometheus/latest/querying/basics/</a>.</p>
			<p>To show how this works, let's enter a basic query, as follows:</p>
			<pre>kubelet_http_requests_total</pre>
			<p>This query will list the total number of HTTP requests made to the kubelet on each Node, for each request category, as shown in the following screenshot:</p>
			<div><div><img src="img/B14790_09_008_new.jpg" alt="Figure 9.8 – HTTP requests query"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – HTTP requests query</p>
			<p>You can also see the<a id="_idIndexMarker472"/> requests in graph form by clicking the <strong class="bold">Graph</strong> tab next to <strong class="bold">Table</strong> as shown in the following screenshot:</p>
			<div><div><img src="img/B14790_09_009_new.jpg" alt="Figure 9.9 – HTTP requests query – graph view"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – HTTP requests query – graph view</p>
			<p>This provides a time<a id="_idIndexMarker473"/> series graph view of the data from the preceding screenshot. As you can see, the graphing capability is fairly simple.</p>
			<p>Prometheus also provides an <strong class="bold">Alerts</strong> tab for configuring Prometheus alerts. Typically, these alerts are configured via code instead of using the <strong class="bold">Alerts</strong> tab UI, so we will skip that page in our review. For more information, you can check the official Prometheus documentation at <a href="https://prometheus.io/docs/alerting/latest/overview/">https://prometheus.io/docs/alerting/latest/overview/</a>.</p>
			<p>Let's move on to Grafana, where <a id="_idIndexMarker474"/>we can extend Prometheus powerful data tooling with visualizations.</p>
			<h3>Using Grafana</h3>
			<p>Grafana provides <a id="_idIndexMarker475"/>powerful tools for visualizing metrics, with many supported charting types that can update in real time. We can connect Grafana to Prometheus in order to see our cluster metrics charted on the Grafana UI.</p>
			<p>To get started with Grafana, do the following:</p>
			<ol>
				<li value="1">We will end our current port forwarding (<em class="italic">CTRL</em> + <em class="italic">C</em> will do the trick) and set up a new port forward listener to the Grafana UI:<pre><strong class="bold">Kubectl -n monitoring port-forward svc/grafana 3000:3000</strong></pre></li>
				<li>Again, navigate to <code>localhost:3000</code> to see the Grafana UI. You should be able to log in with <code>admin</code> and <code>admin</code>, at which point you should be able to change the initial password as shown in the following screenshot:<div><img src="img/B14790_09_010_new.jpg" alt="Figure 9.10 – Grafana Change Password screen"/></div><p class="figure-caption">Figure 9.10 – Grafana Change Password screen</p></li>
				<li>Upon login, you will see the following screen. Grafana does not come preconfigured with any dashboards, but we can add them easily by clicking the <strong class="bold">+</strong> sign as shown in the following screenshot: <div><img src="img/B14790_09_011_new.jpg" alt="Figure 9.11 – Grafana main page"/></div><p class="figure-caption">Figure 9.11 – Grafana main page</p></li>
				<li>Each Grafana dashboard<a id="_idIndexMarker476"/> includes one or more graphs for different sets of metrics. To add a preconfigured dashboard (instead of creating one yourself), click the plus sign (<strong class="bold">+</strong>) on the left-hand menu bar and click <strong class="bold">Import</strong>. You should see a page like the following screenshot:<div><img src="img/B14790_09_012_new.jpg" alt="Figure 9.12 – Grafana Dashboard Import"/></div><p class="figure-caption">Figure 9.12 – Grafana Dashboard Import</p><p>We can add a dashboard via this page either using the JSON configuration or by pasting in a public dashboard ID. </p></li>
				<li>You can find public dashboards and their associated IDs at <a href="https://grafana.com/grafana/dashboards/315">https://grafana.com/grafana/dashboards/315</a>. Dashboard #315 is a great starter dashboard for Kubernetes – let's add it to the textbox labeled <strong class="bold">Grafana.com Dashboard</strong> and click <strong class="bold">Load</strong>.</li>
				<li>Then, on the next page, select the <strong class="bold">Prometheus</strong> data source from the <strong class="bold">Prometheus</strong> option dropdown, which is used to pick between multiple data sources if available. Click <strong class="bold">Import</strong>, and the dashboard should be loaded, which will look like the<a id="_idIndexMarker477"/> following screenshot:</li>
			</ol>
			<div><div><img src="img/B14790_09_013_new.jpg" alt="Figure 9.13 – Grafana dashboard"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – Grafana dashboard</p>
			<p>This particular Grafana dashboard provides a good high-level overview of network, memory, CPU, and filesystem utilization across the cluster, and it is broken down per Pod and container. It is configured with real-time graphs for <strong class="bold">Network I/O pressure</strong>, <strong class="bold">Cluster memory usage</strong>, <strong class="bold">Cluster CPU usage</strong>, and <strong class="bold">Cluster filesystem usage</strong> – though this last option<a id="_idIndexMarker478"/> may not be enabled depending on how you have installed Prometheus.</p>
			<p>Finally, let's look at the Alertmanager UI.</p>
			<h3>Using Alertmanager</h3>
			<p>Alertmanager is an<a id="_idIndexMarker479"/> open source solution for managing alerts generated from Prometheus alerts. We installed Alertmanager previously as part of our stack – let's take a look at what it can do:</p>
			<ol>
				<li value="1">First, let's <code>port-forward</code> the Alertmanager service using the following command:<pre><strong class="bold">Kubectl -n monitoring port-forward svc/alertmanager-main 3000:9093</strong></pre></li>
				<li>As usual, navigate to <code>localhost:3000</code> to see the UI as shown in the following screenshot. It looks similar to the Prometheus UI:</li>
			</ol>
			<div><div><img src="img/B14790_09_014_new.jpg" alt="Figure 9.14 – Alertmanager UI"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – Alertmanager UI</p>
			<p>Alertmanager works<a id="_idIndexMarker480"/> together with Prometheus alerts. You can use the Prometheus server to specify alert rules, and then use Alertmanager to group similar alerts into single notifications, perform deduplications, and create <em class="italic">silences</em>, which are essentially a way to mute alerts if they match specific rules.</p>
			<p>Next, we will review a popular logging stack for Kubernetes – Elasticsearch, FluentD, and Kibana.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor223"/>Implementing the EFK stack on Kubernetes</h2>
			<p>Similar to<a id="_idIndexMarker481"/> the <a id="_idIndexMarker482"/>popular ELK stack (Elasticsearch, Logstash, and Kibana), the EFK stack swaps out Logstash for the FluentD log forwarder, which is well supported on Kubernetes. Implementing this stack is easy and allows us to get started with log aggregation and search functionalities using purely open source tooling on Kubernetes.</p>
			<h3>Installing the EFK stack</h3>
			<p>There are many ways<a id="_idIndexMarker483"/> to install the EFK Stack on Kubernetes, but the Kubernetes GitHub repository itself has some supported YAML, so let's just use that: </p>
			<ol>
				<li value="1">First, clone or download the Kubernetes repository using the following command:<pre><strong class="bold">git clone https://github.com/kubernetes/kubernetes</strong></pre></li>
				<li>The manifests are located in the <code>kubernetes/cluster/addons</code> folder, specifically under <code>fluentd-elasticsearch</code>:<pre><strong class="bold">cd kubernetes/cluster/addons</strong></pre><p>For a production workload, we would likely make some changes to these manifests in order to properly customize the configuration for our cluster, but for the purposes of this tutorial we will leave everything as default. Let's start the process of bootstrapping our EFK stack.</p></li>
				<li>First, let's create the Elasticsearch cluster itself. This runs as a StatefulSet on Kubernetes, and also provides a Service. To create the cluster, we need to run two <code>kubectl</code> commands:<pre><strong class="bold">kubectl apply -f ./fluentd-elasticsearch/es-statefulset.yaml</strong>
<strong class="bold">kubectl apply -f ./fluentd-elasticsearch/es-service.yaml</strong></pre><p class="callout-heading">Important note</p><p class="callout">A word of warning for the Elasticsearch StatefulSet – by default, the resource request for each Pod is 3 GB of memory, so if none of your Nodes have that available, you will not be able to deploy it as configured by default.</p></li>
				<li>Next, let's deploy the FluentD logging agents. These will run as a DaemonSet – one per Node – and forward logs from the Nodes to Elasticsearch. We also need to create the ConfigMap YAML, which contains the base FluentD agent configuration. This can be further customized to add things such as log filters and new sources. </li>
				<li>To install the DaemonSet for the agents and their configuration, run the following two <code>kubectl</code> commands:<pre><strong class="bold">kubectl apply -f ./fluentd-elasticsearch/fluentd-es-configmap.yaml</strong>
<strong class="bold">kubectl apply -f ./fluentd-elasticsearch/fluentd-es-ds.yaml</strong></pre></li>
				<li>Now that we've created the ConfigMap and the FluentD DaemonSet, we can create our Kibana application, which is a GUI for interacting with Elasticsearch. This piece runs as<a id="_idIndexMarker484"/> a Deployment, with a Service. To deploy Kibana to our cluster, run the final two <code>kubectl</code> commands:<pre><strong class="bold">kubectl apply -f ./fluentd-elasticsearch/kibana-deployment.yaml</strong>
<strong class="bold">kubectl apply -f ./fluentd-elasticsearch/kibana-service.yaml</strong></pre></li>
				<li>Once everything has been initiated, which may take several minutes, we can access the Kibana UI in the same way that we did Prometheus and Grafana. To check the status of the resources we just created, we can run the following:<pre><strong class="bold">kubectl get po -A</strong></pre></li>
				<li>Once all Pods for FluentD, Elasticsearch, and Kibana are in the <code>addons</code> folder for more information.</li>
				<li>Once we've confirmed that our components are working properly, let's use the <code>port-forward</code> command to access the Kibana UI. By the way, our EFK stack pieces will live in the <code>kube-system</code> namespace – so our command needs to reflect that. So, let's use the following command:<pre><code>port-forward</code> to your local machine's port <code>8080</code> from the Kibana UI. </p></li>
				<li>Let's check out the Kibana UI at <code>localhost:8080</code>. It should look something like the following, depending<a id="_idIndexMarker485"/> on your exact version and configuration:<div><img src="img/B14790_09_015_new.jpg" alt="Figure 9.15 – Basic Kibana UI"/></div><p class="figure-caption">Figure 9.15 – Basic Kibana UI</p><p>Kibana provides several different features for searching and visualizing logs, metrics, and more. The most important section of the dashboard for our purposes is <strong class="bold">Logging</strong>, since we are using Kibana solely as a log search UI in our example. </p><p>However, Kibana has many other functions, some of which are comparable to Grafana. For instance, it includes a full visualization engine, <strong class="bold">application performance monitoring</strong> (<strong class="bold">APM</strong>) capabilities, and Timelion, an expression engine for time series <a id="_idIndexMarker486"/>data very similar to what is found in Prometheus's PromQL. Kibana's metrics functionality is similar to Prometheus and Grafana.</p></li>
				<li>In order to get Kibana working, we will first need to specify an index pattern. To do this, click on the <strong class="bold">Visualize</strong> button, then click <strong class="bold">Add an Index Pattern</strong>. Select an<a id="_idIndexMarker487"/> option from the list of patterns and choose the index with the current date on it, then create the index pattern.</li>
			</ol>
			<p>Now that we're set up, the <code>h</code>:</p>
			<div><div><img src="img/B14790_09_016_new.jpg" alt="Figure 9.16 – Discover UI"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – Discover UI</p>
			<p>When Kibana cannot find<a id="_idIndexMarker488"/> any results, it gives you a handy set of possible solutions including query examples, as you can see in <em class="italic">Figure 9.13</em>. </p>
			<p>Now that you know how to create search queries, you can create visualizations from queries on the <strong class="bold">Visualize</strong> page. These can be chosen from a selection of visualization types including graphs, charts, and more, and then customized with specific queries as shown in the following screenshot:</p>
			<div><div><img src="img/B14790_09_017_new.jpg" alt="Figure 9.17 – New visualization"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – New visualization </p>
			<p>Next, these visualizations can be combined into dashboards. This works similarly to Grafana where multiple<a id="_idIndexMarker489"/> visualizations can be added to a dashboard, which can then be saved and reused.</p>
			<p>You can also use the search bar to further filter your dashboard visualizations – pretty nifty! The following screenshot shows how a dashboard can be tied to a specific query:</p>
			<div><div><img src="img/B14790_09_018_new.jpg" alt="Figure 9.18 – Dashboard UI"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18 – Dashboard UI</p>
			<p>As you can see, a dashboard can be created for a specific query using the <strong class="bold">Add</strong> button.</p>
			<p>Next, Kibana provides a tool called <em class="italic">Timelion</em>, which is a time series visualization synthesis tool. Essentially, it allows you to combine separate data sources into a single visualization. Timelion is very powerful, but a full discussion of its feature set is outside the scope of this book. The following screenshot shows the Timelion UI – you may notice some similarities to<a id="_idIndexMarker490"/> Grafana, as these two sets of tools offer very similar capabilities:</p>
			<div><div><img src="img/B14790_09_019_new.jpg" alt="Figure 9.19 – Timelion UI"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19 – Timelion UI</p>
			<p>As you can see, in Timelion a query can be used to drive a real-time updating graph, just like in Grafana.</p>
			<p>Additionally, though less relevant to this book, Kibana provides APM functionality, which requires some further setup, especially with Kubernetes. In this book we lean on Prometheus for this type of information while using the EFK stack to search logs from our applications.</p>
			<p>Now that we've covered Prometheus and Grafana for metrics and alerting, and the EFK stack for logging, only one piece of the observability puzzle is left. To solve this, we will use another excellent piece of open source software – Jaeger.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor224"/>Implementing distributed tracing with Jaeger</h2>
			<p>Jaeger is an open<a id="_idIndexMarker491"/> source distributed <a id="_idIndexMarker492"/>tracing solution compatible with Kubernetes. Jaeger implements the OpenTracing specification, which is a set of standards for defining distributed traces.</p>
			<p>Jaeger exposes a UI for viewing traces and integrates with Prometheus. The official Jaeger documentation can be found at <a href="https://www.jaegertracing.io/docs/">https://www.jaegertracing.io/docs/</a>. Always check the docs for new information, since things may have changed since the publishing of this book.</p>
			<h3>Installing Jaeger using the Jaeger Operator</h3>
			<p>To install Jaeger, we are<a id="_idIndexMarker493"/> going to use the Jaeger Operator, which<a id="_idIndexMarker494"/> is the first operator that we've come across in this book. An <em class="italic">operator</em> in Kubernetes is simply a pattern for creating custom application controllers that speak Kubernetes's language. This means that instead of having to deploy all the various Kubernetes resources for an application, you can deploy a single Pod (or usually, single Deployment) and that application will talk to Kubernetes and spin up all the other required resources for you. It can even go further and self-operate the application, making resource changes when necessary. Operators can be highly complex, but they make it easier for us as end users to deploy commercial or open source software on our Kubernetes clusters.</p>
			<p>To get started with the Jaeger Operator, we need to create a few initial resources for Jaeger, and then the operator will do the rest. A prerequisite for this installation of Jaeger is that the <code>nginx-ingress</code> controller is installed on our cluster, since that is how we will access the Jaeger UI.</p>
			<p>First, we need to create a namespace for Jaeger to live in. We can get this via the <code>kubectl create namespace</code> command:</p>
			<pre>kubectl create namespace observability</pre>
			<p>Now that our namespace is created, we need to create some <strong class="bold">CRDs</strong> that Jaeger and the operator will use. We will discuss CRDs in depth in our chapter on extending Kubernetes, but for now, think of them as a way to co-opt the Kubernetes API to build custom functionality for applications. Using the following steps, let's install Jaeger:</p>
			<ol>
				<li value="1">To create the Jaeger CRDs, run the following command:<pre><strong class="bold">kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/crds/jaegertracing.io_jaegers_crd.yaml</strong></pre><p>With our CRDs created, the operator needs a few Roles and Bindings to be created in order to do its work. </p></li>
				<li>We want Jaeger to have cluster-wide permission in our cluster, so we will create some optional ClusterRoles and ClusterRoleBindings as well. To accomplish this, we run <a id="_idIndexMarker495"/>the <a id="_idIndexMarker496"/>following commands:<pre><strong class="bold">kubectl create -n observability -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml</strong>
<strong class="bold">kubectl create -n observability -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role.yaml</strong>
<strong class="bold">kubectl create -n observability -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role_binding.yaml</strong>
<strong class="bold">kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/cluster_role.yaml</strong>
<strong class="bold">kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/cluster_role_binding.yaml</strong></pre></li>
				<li>Now, we finally have all the pieces necessary for our operator to work. Let's install the operator with one last <code>kubectl</code> command:<pre><strong class="bold">kubectl create -n observability -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/operator.yaml</strong></pre></li>
				<li>Finally, check to see if the operator is running, using the following command:<pre><strong class="bold">kubectl get deploy -n observability</strong></pre></li>
			</ol>
			<p>If the operator is running correctly, you will see something similar to the following output, with one available Pod for the deployment:</p>
			<div><div><img src="img/B14790_09_020_new.jpg" alt="Figure 9.20 – Jaeger Operator Pod output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20 – Jaeger Operator Pod output</p>
			<p>We now have our Jaeger Operator up and running – but Jaeger itself isn't running. Why is this the case? Jaeger is a highly complex system and can run in different configurations, and the operator makes it easier to deploy these configurations. </p>
			<p>The Jaeger Operator uses a CRD called <code>Jaeger</code> to read a configuration for your Jaeger instance, at which time the operator will deploy all the necessary Pods and other resources on Kubernetes.</p>
			<p>Jaeger can <a id="_idIndexMarker497"/>run <a id="_idIndexMarker498"/>in three main configurations: <em class="italic">AllInOne</em>, <em class="italic">Production</em>, and <em class="italic">Streaming</em>. A full discussion of these configurations is outside the scope of this book (check the Jaeger docs link shared previously), but we will be using the AllInOne configuration. This configuration combines the Jaeger UI, Collector, Agent, and Ingestor into a single Pod, without any persistent storage included. This is perfect for demo purposes – to see production-ready configurations, check the Jaeger docs.</p>
			<p>In order to create our Jaeger deployment, we need to tell the Jaeger Operator about our chosen configuration. We do that using the CRD that we created earlier – the Jaeger CRD. Create a new file for this CRD instance:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Jaeger-allinone.yaml</p>
			<pre>apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: all-in-one
  namespace: observability
spec:
  strategy: allInOne</pre>
			<p>We are just using a small subset of the possible Jaeger type configurations – again, check the docs for the full story.</p>
			<p>Now, we can create our Jaeger instance by running the following:</p>
			<pre>Kubectl apply -f jaeger-allinone.yaml</pre>
			<p>This command creates an instance of the Jaeger CRD we installed previously. At this point, the Jaeger Operator should realize that the CRD has been created. In less than a minute, our actual Jaeger Pod should be running. We can check for it by listing all the Pods in the observability namespace, with the following command:</p>
			<pre>Kubectl get po -n observability</pre>
			<p>As an output, you should see the newly created Jaeger Pod for our all-in-one instance:</p>
			<pre>NAME                         READY   STATUS    RESTARTS   AGE
all-in-one-12t6bc95sr-aog4s  1/1     Running   0          5m</pre>
			<p>The Jaeger Operator <a id="_idIndexMarker499"/>creates an Ingress record when we also <a id="_idIndexMarker500"/>have an Ingress controller running on our cluster. This means that we can simply list our Ingress entries using kubectl to see where to access the Jaeger UI.</p>
			<p>You can list ingresses using this command:</p>
			<pre>Kubectl get ingress -n observability</pre>
			<p>The output should show your new Ingress for the Jaeger UI as shown:</p>
			<div><div><img src="img/B14790_09_021_new.jpg" alt="Figure 9.21 – Jaeger UI Service output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21 – Jaeger UI Service output</p>
			<p>Now you can navigate to the address listed in your cluster's Ingress record to see the Jaeger UI. It should look like the following:</p>
			<div><div><img src="img/B14790_09_022_new.jpg" alt="Figure 9.22 – Jaeger UI"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22 – Jaeger UI</p>
			<p>As you can see, the Jaeger<a id="_idIndexMarker501"/> UI is pretty simple. There are three<a id="_idIndexMarker502"/> tabs at the top – <strong class="bold">Search</strong>, <strong class="bold">Compare</strong>, and <strong class="bold">System Architecture</strong>. We will focus on the <strong class="bold">Search</strong> tab, but for more information about the other two, check the Jaeger docs at <a href="https://www.jaegertracing.io">https://www.jaegertracing.io</a>.</p>
			<p>The Jaeger <strong class="bold">Search</strong> page lets us search for traces based on many inputs. We can search based on which Service is included in the trace, or based on tags, duration, or more. However, right now there's nothing in our Jaeger system.</p>
			<p>The reason for this is that even though we have Jaeger up and running, our apps still need to be configured to send traces to Jaeger. This usually needs to be done at the code or framework level and is out of the scope of this book. If you want to play around with Jaeger's tracing capabilities, a sample app is available to install – see the Jaeger docs page at <a href="https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod">https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod</a>.</p>
			<p>With services sending traces to Jaeger, it is possible to see traces. A trace in Jaeger looks like the following. We've cropped out some of the later parts of the trace for readability, but this should give you a good idea of what a trace can look like:</p>
			<div><div><img src="img/B14790_09_023_new.jpg" alt="Figure 9.23 – Trace view in Jaeger"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23 – Trace view in Jaeger</p>
			<p>As you can see, the Jaeger UI view for a trace splits up service traces into constituent parts. Each service-to-service call, as well as any specific calls within the services themselves, have their own line in the trace. The horizontal bar chart you see moves from left to right with time, and each individual call in the trace has its own line. In this trace, you can see we <a id="_idIndexMarker503"/>have HTTP calls, SQL calls, as well as <a id="_idIndexMarker504"/>some Redis statements.</p>
			<p>You should be able to see how Jaeger and tracing in general can help developers make sense of a web of service-to-service calls and can help find bottlenecks.</p>
			<p>With that review of Jaeger, we have a fully open source solution to every problem in the observability bucket. However, that does not mean that there is no use case where a commercial solution makes sense – in many cases it does.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor225"/>Third-party tooling</h2>
			<p>In addition to many<a id="_idIndexMarker505"/> open source libraries, there are many commercially available products for metrics, logging, and alerting on Kubernetes. Some of these can be much more powerful than the open source options.</p>
			<p>Generally, most tooling in metrics and logging will require you to provision resources on your cluster to forward metrics and logs to your service of choice. In the examples we've used in this chapter, these services are running in the cluster, though in commercial products these can often be separate SaaS applications where you log on to analyze your logs and see your metrics. For instance, with the EFK stack we provisioned in this chapter, you can pay Elastic for a hosted solution where the Elasticsearch and Kibana pieces of the solution would be hosted on Elastic's infrastructure, reducing complexity in the solution. There are also many other solutions in this space, from vendors including Sumo Logic, Logz.io, New Relic, DataDog, and AppDynamics.</p>
			<p>For a production environment, it is common to use separate compute (either a separate cluster, service, or SaaS tool) to perform log and metric analytics. This ensures that the cluster running your actual <a id="_idIndexMarker506"/>software can be dedicated to the application alone, and any costly log searching or querying functionality can be handled separately. It also means that if our application cluster goes down, we can still view logs and metrics up until the point of the failure.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor226"/>Summary</h1>
			<p>In this chapter, we learned about observability on Kubernetes. We first learned about the four major tenets of observability: metrics, logging, traces, and alerts. Then we discovered how Kubernetes itself provides tooling for observability, including how it manages logs and resource metrics and how to deploy Kubernetes Dashboard. Finally, we learned how to implement and use some key open source tools to provide visualization, searching, and alerting for the four pillars. This knowledge will help you build robust observability infrastructure for your future Kubernetes clusters and help you decide what is most important to observe in your cluster.</p>
			<p>In the next chapter, we will use what we learned about observability to help us troubleshoot applications on Kubernetes.</p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor227"/>Questions</h1>
			<ol>
				<li value="1">Explain the difference between metrics and logs.</li>
				<li>Why would you use Grafana instead of simply using the Prometheus UI?</li>
				<li>When running an EFK stack in production (so as to keep as much compute off the production app cluster as possible), which piece(s) of the stack would run on the production app cluster? And which piece(s) would run off the cluster?</li>
			</ol>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor228"/>Further reading</h1>
			<ul>
				<li>In-depth review of Kibana Timelion: <a href="https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html">https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html</a></li>
			</ul>
		</div>
	</body></html>