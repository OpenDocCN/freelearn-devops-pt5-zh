- en: Chapter 8. Onto Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker came out of dotCloud's PaaS, where it fulfils the needs of IT to develop
    and deploy web applications in a fast and scalable manner. This is needed to keep
    up with the ever-accelerating pace of using the Web. Keeping everything running
    in our Docker container in production is no simple feat.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will wrap up what you learned about optimizing Docker and
    illustrate how it relates to operating our web applications in production. It
    consists of the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing web operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting our application with Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading on web operations in general
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing web operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping a web application running 24/7 on the Internet poses challenges in both
    software development and systems administration. Docker positions itself as the
    glue that allows both disciplines to come together by creating Docker images that
    can be built and deployed in a consistent manner.
  prefs: []
  type: TYPE_NORMAL
- en: However, Docker is not a silver bullet to the Web. It is still important to
    know the fundamental concepts in software development and systems administration
    as web applications become more complex. The complexity naturally arises because
    these days, with Internet technologies in particular, the multitude of web applications
    is becoming more ubiquitous in people's lives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dealing with the complexity of keeping web applications up and running involves
    mastering the ins and outs of web operations, and like any road to mastery, Theo
    Schlossnagle boils it down to four basic pursuits: knowledge, tools, experience,
    and discipline. *Knowledge* refers to absorbing information about web operations
    available on the Internet, in conferences, and technology meetings like a sponge.
    Understanding them and knowing how to filter out the signal from the noise will
    aid us in designing our application''s architecture when they burn in production.
    With Docker and Linux containers increasing in popularity, it is important to
    be aware of the different technologies that support it and dive into its basics.
    In [Chapter 7](part0046_split_000.html#1BRPS2-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 7. Troubleshooting Containers"), *Troubleshooting Containers*, we showed
    that regular Linux debugging tools are still useful in debugging running Docker
    containers. By knowing how containers interact with our Docker host''s operating
    system, we were able to debug the problems occurring in Docker.'
  prefs: []
  type: TYPE_NORMAL
- en: The second aspect is mastering our *tools*. This book basically revolved around
    mastering the use of Docker by looking at how it works and how to optimize its
    usage. In [Chapter 2](part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 2. Optimizing Docker Images"), *Optimizing Docker Images*, we learned
    how to optimize Docker images based on how Docker builds the images and runs the
    container using its copy-on-write filesystem underneath. This was guided by our
    knowledge of web operations on why optimized Docker images are important both
    from a scalability and deployability standpoint. Knowing how to use Docker effectively
    does not happen overnight. Its mastery can only be gained by a continuous practice
    of using Docker in production. Sure, we might be paged at 2 am for our first Docker
    deployment in production, but as time goes by, the experience we gain from continuous
    usage will make Docker an extension of our limbs and senses, as Schlossnagle puts
    it.
  prefs: []
  type: TYPE_NORMAL
- en: By applying the knowledge and continuously using our tools, we gain *experience*
    that we can draw upon in the future. This aids us in making good judgments based
    on bad decisions that we made in the past. It is the place where we can see the
    theory of container technology and the practice of running Docker in production
    collide. Scholassnagle mentioned the challenges of acquiring experience in web
    operations and how to survive the bad judgments and draw experiences from them.
    He suggests having limited environments in which a bad decision's impact is minimal.
    Docker is the best place to draw these types of experiences. Having a standard
    format of ready-to-deploy Docker images, junior web operations engineers can have
    their own environments that they can experiment with and learn from their mistakes
    in. Also, since Docker environments look very similar when they move forward to
    production, these engineers will already have their experience to draw upon.
  prefs: []
  type: TYPE_NORMAL
- en: The last part in the pursuit of mastering web operations is *discipline*. However,
    as it is a very young discipline, such processes are not well defined. Even with
    Docker, it took a few years for people to realize the best ways to use container
    technologies. Before this, the convenience of including the whole kitchen sink
    in Docker images was very common. However, as we can see in [Chapter 2](part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 2. Optimizing Docker Images"), *Optimizing Docker Images*, reducing the
    footprint of Docker images helps aid in managing the complexity of the applications
    that we have to debug. This makes the experience of debugging in [Chapter 7](part0046_split_000.html#1BRPS2-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 7. Troubleshooting Containers"), *Troubleshooting Containers*, much simpler
    because we have fewer components and factors to think about. These disciplines
    of using Docker do not come overnight just by reading Docker blogs (well, some
    do). It involves continuous exposure to the knowledge of the Docker community
    and the practice of using Docker in various settings for production use.
  prefs: []
  type: TYPE_NORMAL
- en: In the remaining sections, we will show how the theory and practice of using
    Docker's container technology can aid in the operation of our web applications.
  prefs: []
  type: TYPE_NORMAL
- en: Supporting web applications with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows the typical architecture of a web application.
    We have the load balancer tier that receives traffic from the Internet and then
    the traffic, which is typically composed of user requests, is relayed to a farm
    of web application servers in a load-balanced fashion. Depending on the nature
    of the request, some states will be grabbed by the web application from the persistent
    storage tier, similar to database servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supporting web applications with Docker](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding diagram, each tier is run inside a Docker container
    on top of a Docker host. With this layout for each component, we can take advantage
    of Docker's uniform way of deploying load balancers, applications, and databases,
    as we did in [Chapter 2](part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 2. Optimizing Docker Images"), *Optimizing Docker Images*, and [Chapter
    6](part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e "Chapter 6. Load
    Balancing"), *Load Balancing*. However, in addition to the Docker daemons in each
    Docker host, we need supporting infrastructure to manage and observe the whole
    stack of our web architecture in a scalable fashion. On the right-hand side, we
    can see that each of our Docker hosts sends diagnostic information—for example,
    application and system events such as log messages and metrics—to our centralized
    logging and monitoring system. We deployed such a system in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*, where we rolled out Graphite and an ELK stack. In addition, there
    might be another system that listens for specific signals in the logs and metrics
    and sends alerts to the engineers responsible for the operation of our Docker-based
    web application stack. These events can relate to critical events, such as the
    availability and performance of our application, that we need to take action on
    to ensure that our application is fulfilling the needs of our business as expected.
    An internally managed system, such as Nagios, or a third-party one, such as PagerDuty,
    is used for our Docker deployments to call and wake us up at 2 am for deeper monitoring
    and troubleshooting sessions as in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*, and [Chapter 7](part0046_split_000.html#1BRPS2-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 7. Troubleshooting Containers"), *Troubleshooting Containers*.
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side of the diagram contains the configuration management system.
    This is the place where each of the Docker hosts downloads all the settings it
    needs to function properly. In [Chapter 3](part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 3. Automating Docker Deployments with Chef"), *Automating Docker Deployments
    with Chef*, we used a Chef server to store the configuration of our Docker host.
    It contained information such as a Docker host's role in our architecture's stack.
    The Chef server stores information on which Docker containers to run in each tier
    and how to run them using the Chef recipes we wrote. Finally, the configuration
    management system also tells our Docker hosts where the Graphite and Logstash
    monitoring and logging endpoints are.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, it takes various components to support our web application in production
    aside from Docker. Docker allows us to easily set up this infrastructure because
    of the speed and flexibility of deploying containers. Nonetheless, we shouldn't
    skip doing our homework about having these supporting infrastructures in place.
    In the next section, we will see the supporting infrastructure of deploying web
    applications in Docker using the skills you learned in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An important component when tuning the performance of Docker containers is
    the feedback telling us that we were able to improve our web application correctly.
    The deployment of Graphite and the ELK stack in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*, gave us visibility on the effects of what we changed in our Docker-based
    web application. As much as it is important to gather feedback, it is more important
    to gather feedback in a timely manner. Therefore, the deployment of our Docker
    containers needs to be in a fast and scalable manner. Being able to configure
    a Docker host automatically, as we did in [Chapter 3](part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 3. Automating Docker Deployments with Chef"), *Automating Docker Deployments
    with Chef*, is an important component for a fast and automated deployment system.
    The rest of the components are described in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying applications](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Whenever we submit changes to our application's code or the `Dockerfile` describing
    how it is run and built, we need supporting infrastructure to propagate this change
    all the way to our Docker hosts. In the preceding diagram, we can see that the
    changes we submit to our version control system, such as Git, generate a trigger
    to build the new version of our code. This is usually done through Git's postreceive
    hooks in the form of shell scripts. The triggers will be received by a build server,
    such as Jenkins. The steps to propagate the change will be similar to the blue-green
    deployment process we made in [Chapter 6](part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 6. Load Balancing"), *Load Balancing*. After receiving the trigger to
    build the new changes we submitted, Jenkins will take a look at the new version
    of our code and run `docker build` to create the Docker image. After the build,
    Jenkins will push the new Docker image to a Docker registry, such as Docker Hub,
    as we set up in [Chapter 2](part0018_split_000.html#H5A42-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 2. Optimizing Docker Images"), *Optimizing Docker Images*. In addition,
    it will update the target Docker hosts indirectly by updating the entry in the
    Chef server configuration management system we laid out in [Chapter 3](part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 3. Automating Docker Deployments with Chef"), *Automating Docker Deployments
    with Chef*. With the artifacts of changes available in the Chef server and Docker
    registry, our Docker host will now notice the new configuration and download,
    deploy, and run the new version of our web application inside a Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how a similar process is used to scale
    out our Docker application.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we receive alerts from our monitoring system, as in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*, that the pool of Docker containers running our web application
    is not loaded, it is time to scale out. We accomplished this using load balancers
    in [Chapter 6](part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 6. Load Balancing"), *Load Balancing*. The following diagram shows the
    high-level architecture of the commands we ran in [Chapter 6](part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 6. Load Balancing"), *Load Balancing*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling applications](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Once we decide to scale out and add an additional Docker host, we can automate
    the process with a scale-out orchestrator component. This can be a series of simple
    shell scripts that we will install inside a build server, such as Jenkins. The
    orchestrator will basically ask the cloud provider API to create a new Docker
    host. This request will then provision the Docker host and run the initial bootstrap
    script to download the configuration from our configuration management system
    in [Chapter 3,](part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 3. Automating Docker Deployments with Chef") *Automating Docker Deployments
    with Chef*. This will automatically set up the Docker host to download our application's
    Docker image from the Docker registry. After this whole provisioning process is
    finished, our scale-out orchestrator will then update the load balancer in our
    Chef server with the new list of application servers to forward traffic to. So,
    the next time the `chef-client` inside our load balancer Docker host polls the
    Chef Server, it will add the new Docker host and start forwarding traffic to it.
  prefs: []
  type: TYPE_NORMAL
- en: As we can note, learning the way to automate setting up our Docker host in [Chapter
    3](part0022_split_000.html#KVCC1-afc4585f6623427885a0b0c8e5b2e22e "Chapter 3. Automating
    Docker Deployments with Chef"), *Automating Docker Deployments with Chef*, is
    crucial to realizing the scalable load balancing architecture setup we did in
    [Chapter 6](part0041_split_000.html#173721-afc4585f6623427885a0b0c8e5b2e22e "Chapter 6. Load
    Balancing"), *Load Balancing*.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The supporting architecture to help our web applications use Docker is nothing
    but a scratch on the surface. The fundamental concepts in this chapter are described
    in greater detail in the following books:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Web Operations: Keeping the Data On Time*, which is edited by J. Allspaw and
    J. Robbins. 2010 O''Reilly Media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continuous Delivery*, by J. Humble and D. Farley. 2010 Addison-Wesley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jenkins: The Definitive Guide*, J. F. Smart. 2011 O''Reilly Media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Art of Capacity Planning: Scaling Web Resources*, J. Allspaw. 2008 O''Reilly
    Media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pro Git*, S. Chacon and B. Straub. 2014 Apress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You learned a lot about how Docker works throughout this book. In addition to
    the basics of Docker, we looked back at some fundamental concepts of web operations
    and how it helps us realize the full potential of Docker. You gained knowledge
    of key Docker and operating systems concepts to get a deeper understanding of
    what is happening behind the scenes. You now have an idea of how our application
    goes from our code down to the actual call in the operating system of our Docker
    host. You learned a lot about the tools to deploy and troubleshoot our Docker
    containers in production in a scalable and manageable fashion.
  prefs: []
  type: TYPE_NORMAL
- en: However, this should not stop you from continuing to develop and practice using
    Docker to run our web applications in production. We should not be afraid to make
    mistakes and gain further experience on the best ways to run Docker in production.
    As the Docker community evolves, so do these practices through the collective
    experience of the community. So, we should continue and be disciplined in learning
    the fundamentals we started to master little by little. Don't hesitate to run
    Docker in production!
  prefs: []
  type: TYPE_NORMAL
