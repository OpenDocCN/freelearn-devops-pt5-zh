<html><head></head><body>
        

                            
                    Orchestration Using Kubernetes
                
            
            
                
<p class="mce-root">This chapter is dedicated to the most widely used container orchestrator today—Kubernetes. In 2018, Kubernetes was adopted by 51% of container users as their main orchestrator. Kubernetes adoption has increased in recent years, and it is now at the core of most <strong>Container-as-a-S</strong><strong>ervice</strong> (<strong>CaaS</strong>) platforms.</p>
<p>Cloud providers have followed the expansion of Kubernetes, and most of them (including Amazon, Google, and Azure) now provide their own <strong>Kubernetes-as-a-Service</strong> (<strong>KaaS</strong>) platforms where users do not have to take care of Kubernetes' administrative tasks. These services are designed for simplicity and availability on cloud platforms. Users just run their workloads on them and the cloud providers manage complicated maintenance tasks.</p>
<p>In this chapter, we will learn how Kubernetes works and what features it provides. We'll review what is required to deploy a Kubernetes cluster with high availability. We will then learn about Kubernetes objects, such as pods and services, among others. Networking is key to distributing workloads within a cluster; we will learn how Kubernetes networking works and how it provides service discovery and load balancing. Finally, we will review some of the special security features provided by Kubernetes to manage cluster authentication and authorization.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Deploying Kubernetes</li>
<li>High availability with Kubernetes</li>
<li>Pods, services, and other Kubernetes resources</li>
<li>Deploying orchestrated resources</li>
<li>Kubernetes networking</li>
<li>Publishing applications</li>
</ul>
<p>Kubernetes is not part of the Docker Certified Associate exam yet, but it probably will be in the next release as Docker Enterprise comes with a fully compatible Kubernetes platform deployed on top of the Docker Swarm orchestrator. Docker Enterprise is the only container platform that provides both orchestrators at the same time. We will learn about Docker Enterprise's components and features in the third section of this book, with a chapter dedicated to each component.</p>
<h1 id="uuid-946dd34f-2b6d-409f-b280-0a1cabfcd1ae">Technical requirements</h1>
<p class="mce-root">In this chapter, we will learn about the features of the Docker Swarm orchestrator. We also provide some labs at the end of the chapter to help you to understand and learn about the concepts that we will cover. These labs can be run on your laptop or PC using the provided Vagrant <em>Kubernetes environment</em> or any already-deployed Docker Swarm cluster by yourself. You can view additional information in this book's GitHub code repository, which is available at <a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a>.</p>
<p>Check out the following video to see the Code in Action:</p>
<p>"<a href="https://bit.ly/3gzAnS3" target="_blank">https://bit.ly/3gzAnS3</a>"</p>
<h1 id="uuid-88016faa-1dc6-4c99-8afe-262ba24e3a73">Deploying Kubernetes using Docker Engine</h1>
<p>Kubernetes has many features and is more complex than Docker Swarm. It provides additional features not available on Docker Swarm without having to modify our application code. Docker Swarm is more aligned with microservices logic, while Kubernetes is closer to the virtual machine application's <strong>lift and shift</strong> approach (move application as is to a new infrastructure). This is because the Kubernetes pod object can be compared to virtual machines (with application processes running as containers inside a pod).</p>
<p class="mce-root">Before we begin discussing Kubernetes architecture, let's review some of the concepts that we've learned about orchestration.</p>
<p>Orchestration should provide all that's required for deploying a solution to execute, manage, and publish applications based on the containers distributed on a pool of nodes. Therefore, it should provide a control plane to ensure cluster availability, a scheduler for deploying applications, and a network plane to interconnect distributed applications. It should also provide features for publishing cluster-distributed applications. Application health will also be managed by the orchestrator. As a result, if one application component dies, a new one will be deployed to ensure the application's health.</p>
<p>Kubernetes provides all of these features, and so does Docker Swarm too. However, Kubernetes has many more features, is extensible, and has a bigger community behind the project. Docker also adopted Kubernetes in its Docker Enterprise 2.0 release. It is the only platform that supports Docker Swarm and Kubernetes on the same infrastructure.</p>
<p>Kubernetes provides more container density because it is able to run more than one container at once for each application component. It also provides autoscale features and other advanced scheduling features.</p>
<p>Because Kubernetes is a big community project, some of its components have also been decoupled on different projects to provide faster deployment. The main open source project is hosted by the <strong>Cloud Native Computing Foundation</strong> (<strong>CNCF</strong>). Kubernetes releases a new version every 6 months—imagine updating old legacy applications in production every 6 months. As previously mentioned, it is not easy to follow this application life cycle for many other products, but Kubernetes provides a methodology to upgrade to new software releases easily.</p>
<p>Kubernetes' architectural model is based on the usual orchestration components. We deploy master nodes to execute management tasks and worker nodes (also known as minions) to run application workloads. We also deploy an <kbd>etcd</kbd> key-value database to store all of the cluster object data.</p>
<p>Let's introduce the Kubernetes components. Masters and workers run different processes, and their number may vary depending on the functionalities provided by each role. Most of these components could be installed as either system services or containers. Here is a list of Kubernetes cluster components:</p>
<ul>
<li><kbd>kube-apiserver</kbd></li>
<li><kbd>kube-scheduler</kbd></li>
<li><kbd>kube-controller-manager</kbd></li>
<li><kbd>etcd</kbd></li>
<li><kbd><kbd>kubelet</kbd></kbd></li>
<li><kbd>kube-proxy</kbd></li>
<li>Container runtime</li>
</ul>
<p>Note that this list is very different from what we learned about Docker Swarm, where everything was built-in. Let's review each component's features and properties. Remember, this is not a Kubernetes book—we will only learn the basics.</p>
<p>We will run dedicated master nodes to provide an isolated cluster control plane. The following components will run on these nodes:</p>
<ul>
<li><kbd>kube-apiserver</kbd>: This is the Kubernetes core, and it exposes the Kubernetes API via HTTP (HTTPS if we use TLS certificates). We will connect to this component in order to deploy and manage applications.</li>
<li><kbd>kube-scheduler</kbd>: When we deploy an application's components, the scheduler will decide where to run each one if no node-specific location has been defined. To decide where to run deployed workloads, it will review workload properties, such as specific resources, limits, architecture requirements, affinities, or constraints.</li>
<li><kbd>kube-controller-manager</kbd>: This component will manage controllers, which are processes that are always watching for a cluster object's state changes. This, for example, will manage the node's and workload's states to ensure the desired number of instances are running.</li>
<li><kbd>etcd</kbd>: This is the key-value store for all Kubernetes objects' information and states. Some production environments will run <kbd>etcd</kbd> out of the master nodes' infrastructure to avoid performance issues and to improve components' high availability.</li>
</ul>
<p>Worker processes, on the other hand, can run on any node. As we learned with Docker Swarm, we can decide to run application workloads on worker and master nodes. These are the required components for compute nodes:</p>
<ul>
<li><kbd>kubelet</kbd>: This is the core Kubernetes agent component. It will run on any cluster node that is able to execute application workloads. This process will also ensure that node-assigned Kubernetes workloads are running and are healthy (it will only manage pods created within Kubernetes).</li>
</ul>
<p>We are talking about scheduling containers or workloads on a Kubernetes cluster. The fact is that we will schedule pods, which are Kubernetes-specific objects. Kubernetes will run pods; it will never run standalone containers.</p>
<ul>
<li><kbd>kube-proxy</kbd>: This component will manage the workload's network interactions using operating system packet filtering and routing features. <kbd>kube-proxy</kbd> should run on any worker node (that is, nodes that run workloads).</li>
</ul>
<p>Earlier, we mentioned the container runtime as one of the Kubernetes cluster's components. In fact, it is a requirement because Kubernetes itself does not provide one. We will use Docker Engine as it is the most widely used engine, and we have already discussed it in previous chapters.</p>
<p>The following workflow represents all Kubernetes components distributed on five nodes (notice that the master has worker components too and that <kbd>etcd</kbd> is also deployed out of it):</p>
<div><img src="img/698fa4f7-056a-4506-a8aa-de77fe895290.jpg" style=""/></div>
<p>As discussed in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>, external load balancers will provide L4 and L7 routing on replicated services. In this case, cluster management components do not use router mesh-like services. We will provide high availability for core components using replicated processes on different nodes. A virtual IP address will be required and we will also use <strong>Fully Qualified Domain Name</strong> (<strong>FQDN</strong>) names for <strong>Transport Layer Security</strong> (<strong>TLS</strong>) certificates. This will ensure secure communications and access to and from Kubernetes components.</p>
<p>The following diagram shows the TLS certificates that will be created to ensure secure communication between components:</p>
<div><img src="img/86153b0f-f81f-4726-a0d3-52bf4519857b.jpg" style=""/></div>
<p>We will use the <kbd>kubectl</kbd> command line to interact with the Kubernetes cluster, and we will always connect to the <kbd>kube-apiserver</kbd> processes.</p>
<p>In the next section, we will learn how to implement high-availability Kubernetes cluster environments.</p>
<h1 id="uuid-b78aee61-a44d-4d71-aa95-3d5e2a7b40ae">Deploying a Kubernetes cluster with high availability</h1>
<p class="mce-root">Docker Swarm was easy to implement. To provide high availability, we simply changed the node roles to accomplish the required odd number of managers. In Kubernetes, this is not so easy; roles cannot be changed, and, usually, administrators do not change the initial number of master nodes.</p>
<p>Therefore, installing a Kubernetes cluster with high-availability components requires some planning. The good thing here is that Docker Enterprise will deploy the cluster for you (since the 2.0 release). We will review this method in <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>, as <strong>Universal Control Plane</strong> (<strong>UCP</strong>) will deploy Kubernetes on top of Docker Swarm.</p>
<p>To provide high availability, we will deploy an odd number of control plane components. It is usual to deploy <kbd>etcd</kbd> on three additional nodes. In this scenario, nodes would be neither masters nor workers because <kbd>etcd</kbd> will be deployed out of the Kubernetes nodes. We will require access to this external <kbd>etcd</kbd> from the master nodes only. Therefore, in this situation, we will run a cluster of eight nodes: three nodes will run <kbd>etcd</kbd>, three masters nodes will run all of the other control plane components (cluster management), and there will be at least two workers to provide redundancy if one of them dies. This is appropriate for many Kubernetes environments. We isolate <kbd>etcd</kbd> from the control plane components to provide better management performance.</p>
<p>We can deploy <kbd>etcd</kbd> on master nodes. This is similar to what we learned about Docker Swarm. We can have <em>pure masters—</em>running only management components—and worker nodes for workloads.</p>
<p>Installing Kubernetes is not easy, and there are many software vendors that have developed their own KaaS platforms to provide different methods of installation.</p>
<p>For high availability we will run distributed copies of <kbd>etcd</kbd>. In this scenario, <kbd>kube-apiserver</kbd> will connect to a list of nodes instead of just one <kbd>etcd</kbd> node. The <kbd>kube-apiserver</kbd>, <kbd>kube-scheduler</kbd>, and <kbd>kube-controller-manager</kbd> processes will run duplicated on different master nodes (one instance on each master node).</p>
<p>We will use <kbd>kube-apiserver</kbd> to manage the cluster. The Kubernetes client will connect to this server process using the HTTP/HTTPS protocol. We will use an external load balancer to distribute traffic between different replicas running on the master nodes. Kubernetes works with the Raft algorithm because <kbd>etcd</kbd> uses it.</p>
<p>Applications deployed in the cluster will have high availability based on resilience by default (just like in Docker Swarm clusters). Once an application is deployed with all of its components, if one of them fails, <kbd>kube-controller-manager</kbd> will run a new one. There are different controllers processes, for different deployments that are responsible for executing applications based on replicas, on all nodes at the same time, and other specific execution situations.</p>
<p>In the next section, we will introduce the pod concept, which is key to understanding the differences between Kubernetes and Docker Swarm.</p>
<h1 id="uuid-ab5efedb-27d7-4450-bb86-565105f169f7">Pods, services, and other Kubernetes resources</h1>
<p class="mce-root">The pod concept is key to understanding Kubernetes. A pod is a group of containers that run together. It is very simple. All of these containers share a network namespace and storage. It is like a small logical host because we run many processes together, sharing the same IP addresses and volumes. The isolation methods that we learned about in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>, are applicable here.</p>
<h2 id="uuid-16942bec-0526-4a2b-bd6f-9d213642d3b4">Pods</h2>
<p>Pods are the smallest scheduling unit in Kubernetes environments. Containers within a pod will share the same IP address and can find each other using <kbd>localhost</kbd>. Therefore, assigned ports must be unique within pods. We cannot reuse ports for other containers and inter-process communication because processes will run as if they were executed on the same logical host. A pod's life relies on the healthiness of a container.</p>
<p>Pods can be used to integrate full application stacks, but it is true that they are usually used with a few containers. In fact, microservices rely on small functionalities; therefore, we will run just one container per node. As pods are the smallest Kubernetes scheduling unit, we scale pods up and down, not containers. Therefore, complete stacks will be replicated if many grouped application components are executed together within a pod.</p>
<p>On the other hand, pods allow us, for example, to execute a container in order to initialize some special features or properties for another container. Remember the <em>Deploying using Docker Stacks</em> section from <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>? In that lab, we launched a PostgreSQL database and we added an initialization script to create a specific database. We can do this on Kubernetes using the initial containers within a pod.</p>
<p>Terminating and removing pods will depend on how much time it will take to stop or delete all of the containers running within a pod.</p>
<p>The following diagram represents a pod with some containers inside, sharing the same IP address and volume, among other features (we will be able to apply a special security context to all containers within a pod):</p>
<div><img src="img/f23d1b15-275f-4766-8466-05b44fb71f75.jpg" style=""/></div>
<p class="CDPAlignLeft CDPAlign">Let's now review the service resources on Kubernetes.</p>
<h2 id="uuid-5c3427a3-d3db-44ec-a630-8efe64ed2db2">Services</h2>
<p>Services have a different meaning in Kubernetes. Services are abstract objects for the cluster; we do not schedule services in Kubernetes. They define a logical set of pods that work together to serve an application component. We can also associate a service with an external resource (endpoint). This service will be used inside a cluster like any other, but with external IP addresses and ports, for example. </p>
<p>We also use services to publish applications inside and outside a Kubernetes cluster. For these purposes, there are different types of services. All of them, except headless services, provide internal load balancing between all pod replicas for a common service:</p>
<ul>
<li><strong>Headless</strong>: We use headless services to interface with non-Kubernetes service discovery solutions. No virtual IP will be allocated. There will be no load balancing or proxy to reach the service's pods. This behavior is similar to Docker Swarm's DNSRR mode.</li>
<li><strong>ClusterIP</strong>: This is the default service type. Kubernetes will provide an internal virtual IP address chosen from a configurable pool. This will allow only internal cluster objects to reach the defined service.</li>
<li><strong>NodePort</strong>: NodePort services also receive a virtual IP (ClusterIP), but exposed services' ports will be available on all cluster nodes. Kubernetes will route requests to the service's ClusterIP address, no matter which node received them. Therefore, the service's defined port will be available on <kbd>&lt;ANY_CLUSTER_NODE&gt;:&lt;NODEPORT_PORT&gt;</kbd>. This effectively reminds us of the routing mesh's behavior on Docker Swarm. In this case, we need to add some cluster nodes to external load balancers to reach the defined and exposed service's ports.</li>
<li><strong>LoadBalancer</strong>: This service type is available only in a cloud provider's Kubernetes deployment. We expose a service externally using automatically created (using the cloud provider's API integration) load balancers. It uses both a ClusterIP virtual IP for internal routing and a NodePort concept for reaching service-defined ports from load balancers.</li>
<li><strong>ExternalName</strong>: This is not very common nowadays because it relies on DNS CNAME records and is a new implementation. It is used to add external services, out of the Kubernetes cluster. External services will be reachable by their names as if they were running inside<br/>
Kubernetes cluster.</li>
</ul>
<p>The following schema represents the NodePort service type's usual configuration. In this example, the service is reachable on port <kbd>7000</kbd> from an external load balancer, while pods are reachable internally on port <kbd>5000</kbd>. All traffic will be internally load balanced between all of the service's pod endpoints:</p>
<div><img src="img/d68704cd-fb33-4e9a-95f6-0d1afacbed24.jpg" style=""/></div>
<p>There are many other resources in Kubernetes. We will take a quick look at some of them before going into how we deploy applications on Kubernetes clusters in depth.</p>
<h2 id="uuid-83e2a342-e588-4a92-b732-a2ed51e8493b">ConfigMaps and secrets</h2>
<p>We learned how to distribute the required application information cluster-wide with Docker Swarm. Kubernetes also provides solutions for this. We will use ConfigMaps, instead of Docker Swarm config objects, and secrets.</p>
<p>In both cases, we can use either files or standard input (using the <kbd>--from-literal</kbd> option) to create these resources. The literal option will allow us to create these objects using the command line instead of a YAML file. </p>
<p>The Kubernetes <kbd>kubectl</kbd> command line provides two different approaches to create cluster resources/objects (imperative and declarative). We will use either command-line generators or resource files, usually in YAML format. The first method is usually known as imperative, but is not available for all kinds of resources, and using files is known as declarative. This will apply to all Kubernetes resources; therefore, we will be able to use either <kbd>kubectl create pod</kbd> with arguments or <kbd>kubectl create -f &lt;POD_DEFINITION_FILE_IN_YAML_FORMAT&gt;</kbd>. We can export a previously generated command-line object into YAML format easily to allow resource reproducibility, to save its definition somewhere safe.</p>
<p>ConfigMaps and secrets allow us to decouple configurations from image content without using unsecured runtime-visible variables or local files shared on some nodes. We will use secrets for sensitive data, while ConfigMaps will be used for common configurations.</p>
<h2 id="uuid-33e0d88d-d810-48a1-b6a7-cb0653efe2de">Namespaces</h2>
<p>Namespaces can be understood as scopes based on names. They allow us to isolate resources between them. The names of resources are unique within each namespace. Resources can only be within one namespace; therefore, we can divide access to them using namespaces.</p>
<p>One of the simplest uses for namespaces is to limit user access and the usage of Kubernetes' objects and resources' quotas. Based on namespaces, we will allow a specific set of host resources for users. For example, different groups of users or teams will have their own resources and a quota that will limit their environment's behavior.</p>
<h2 id="uuid-49d2435e-3919-46f9-9fc6-ae635fdeacbb">Persistent volumes</h2>
<p>We learned about volumes in <a href="e7804d8c-ed8c-4013-8449-b746ee654210.xhtml">Chapter 4</a>, <em>Container Persistency and Networking</em>. In Kubernetes, volumes are attached to pods, not containers; therefore, volumes will follow a pod's life cycle.</p>
<p>There are many volume types in Kubernetes and we can mix them inside pods. Volumes are available to any container running within a pod. There are volumes specially designed for cloud providers and storage solutions that are available in most data centers. Let's review a couple of interesting, commonly used volumes:</p>
<ul>
<li><kbd>emptyDir</kbd>: This volume is created when a pod is assigned to a node and is removed with the pod. It starts off empty and is usually used to share information between containers running within a pod.</li>
<li><kbd>hostPath</kbd>: We have already used this type of volume on Docker. These volumes allow us to mount a file or directory from the host into pods.</li>
</ul>
<p>Each volume type has its own special options to enable its unique features.</p>
<p>These volumes are designed to be used within pods, but they are not prepared for Kubernetes clustering and storing permanent data. For these situations, we use <strong>Persistent Volumes</strong> (<strong>PVs</strong>).</p>
<p>PVs allow us to abstract how storage is provided. It doesn't matter how storage hosts arrive in the cluster; we only care about how to use them. A PV is provisioned by an administrator, for example, and users are allowed to use it. PVs are Kubernetes resources; hence, we can associate them with namespaces and they have their own life cycle. They are pod-independent.</p>
<p>PVs are requested by <strong>Persistent Volume Claims</strong> (<strong>PVCs</strong>). Therefore, PVCs consume defined PVs. This is the way to associate a pod with a PV.</p>
<p>Therefore, PVCs allow users to consume storage. We can designate storage according to internal properties, such as speed, how it is provided on the hosts, and more, and allow dynamic provisioning using <strong>storage classes</strong>. With these objects, we describe all of the storage solutions available in the cluster with their properties as profiles and Kubernetes prepares the persistent storage to be used.</p>
<p>It is important to know that we can decide the behavior of the PV data once pods die. The <strong>retail reclaim</strong> policy describes what to do with volumes and their content once pods no longer use them. Therefore, we will choose between deleting the volume, retaining the volume and its content, and recycling it.</p>
<p>We can say that PVs are Kubernetes cluster resources designated for application persistent storage and PVCs are the requests to use them.</p>
<p>Storage classes are a new feature that allow administrators to integrate dynamic provisions into our cluster. This helps us to provide storage without having to manually configure each volume. We will just define profiles and features for storage and the provisioners will give the best solution for the required volume.</p>
<p>In the next section, we will learn how to deploy workloads on Kubernetes clusters.</p>
<h1 id="uuid-e8624eb0-8b4f-4f4c-876c-82cab9c0e34b">Deploying orchestrated resources</h1>
<p class="mce-root">Deploying workloads in Kubernetes is easy. We will use <kbd>kubectl</kbd> to specify the resources to be created and interact with <kbd>kube-apiserver</kbd>.</p>
<p>As mentioned earlier, we can use the command line to either use built-in generators or YAML files. Depending on the Kubernetes API version, some options may not be available, but we will assume Kubernetes 1.11 or higher.</p>
<p>In this chapter, all examples use Kubernetes 1.14 because it is the version available on the current Docker Enterprise release, 3.0, at the time of writing this book.</p>
<p>Let's start by creating a simple pod. We will review both options—imperative, using the command-line, and declarative, using YAML manifests.</p>
<p>Using the pod generator, we will run the <kbd>kubectl run --generator=run-pod/v1</kbd> command:</p>
<pre><strong>$ kubectl run --generator=run-pod/v1 --image=nginx:alpine myfirstpod --labels=example=myfirstpod</strong><br/><strong>pod/myfirstpod created</strong></pre>
<p>Using a YAML definition file, we will describe all of the required properties of the pod:</p>
<pre>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/> name: myfirstpod<br/>  labels:<br/>    example: myfirstpod<br/>spec:<br/>  containers:<br/>  - name: myfirstpodcontainer<br/>    image: nginx:alpine</pre>
<p>To deploy this <kbd>.yaml</kbd> definition file, we will just run <kbd>kubectl create -f &lt;YAML_DEFINITION_FILE&gt;</kbd>. This will create all of the defined resources in the file on the specified namespace. Because we are not using an argument to specify a namespace, they will be created on the user-defined one. In our case, we are using the <kbd>default</kbd> namespace by default.</p>
<p>We can define the namespace either on each YAML file or by using a command-line argument. The latter will overwrite the YAML definition.</p>
<p>Both examples will create the same pod, with one container inside, running an <kbd>nginx:alpine</kbd> image.</p>
<p>Take care when using the <kbd>args</kbd> and <kbd>command</kbd> definitions on Kubernetes. These keys differ from the definitions we used for Docker containers or images. Kubernetes' <kbd>command</kbd> will represent <kbd>ENTRYPOINT</kbd>, while <kbd>args</kbd> will represent the container/image <kbd>CMD</kbd> definition.</p>
<p>We can kill this pod by simply removing it using <kbd>kubectl delete</kbd>. To get a list of pods running within a namespace, we will use <kbd>kubectl get pods</kbd>. If the namespace is omitted on the <kbd>kubectl</kbd> execution, the user-assigned namespace will be used:</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>NAME         READY   STATUS    RESTARTS   AGE</strong><br/><strong>myfirstpod   1/1     Running   0          11s</strong></pre>
<p>But this just created a simple pod; we cannot create more NGINX replicas with this kind of resource. To use replicas, we will use ReplicaSets instead of single pods.</p>
<p>We will set up a pod template section and pod selectors to identify which deployed pods belong to this <kbd>ReplicaSet</kbd> resource within a new YAML file. This will help the controller to watch the pods' health.</p>
<p>Here, to the previous pod definition, we add a <kbd>template</kbd> section and a <kbd>selector</kbd> key with labels:</p>
<pre>apiVersion: apps/v1<br/>kind: ReplicaSet<br/>metadata:<br/>  name: myfirstrs<br/>  labels:<br/>    example: myfirstrs<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      example: myfirstrs<br/>  template:<br/>    metadata:<br/>      name: myfirstpod<br/>      labels:<br/>        example: myfirstrs<br/>    spec:<br/>      containers:<br/>      - name: myfirstpodcontainer<br/>        image: nginx:alpine</pre>
<p>Therefore, we created three replicas using the same pod definition as we did earlier. This pod's definition was used as a template for all of the replicas. We can review all of the resources deployed using <kbd>kubectl get all</kbd>. In the following command, we filter the results to retrieve only resources with the <kbd>example</kbd> label and the <kbd>myfirstrs</kbd> value:</p>
<pre><strong>$ kubectl get all -l example=myfirstrs</strong><br/><strong>NAME                  READY   STATUS    RESTARTS   AGE</strong><br/><strong>pod/myfirstrs-2xrpk   1/1     Running   0          47s</strong><br/><strong>pod/myfirstrs-94rb5   1/1     Running   0          47s</strong><br/><strong>pod/myfirstrs-jm6lc   1/1     Running   0          47s</strong><br/><br/><strong>NAME                        DESIRED   CURRENT   READY   AGE</strong><br/><strong>replicaset.apps/myfirstrs   3         3         3       47s</strong></pre>
<p>Each replica will have the same prefix name, but its own ID will be part of the name. This uniquely identifies the resource in the Kubernetes cluster.</p>
<p>We are using <kbd>kubectl get all -l &lt;KEY=VALUE&gt;</kbd> to filter all of the resources we labeled with the <kbd>example</kbd> key and the <kbd>myfirstrs</kbd> value.</p>
<p>We can use <kbd>DaemonSet</kbd> to deploy a replica on each node in the cluster, just as we did with Docker Swarm's global services:</p>
<pre>apiVersion: apps/v1<br/>kind: DaemonSet<br/>metadata:<br/>  name: myfirstds<br/>  labels:<br/>    example: myfirstds<br/>spec:<br/>  selector:<br/>    matchLabels:<br/>      example: myfirstds<br/>  template:<br/>    metadata:<br/>      name: myfirstpod<br/>      labels:<br/>        example: myfirstds<br/>    spec:<br/>      containers:<br/>      - name: myfirstpodcontainer<br/>        image: nginx:alpine<br/>        resources:<br/>          limits:<br/>            memory: 100Mi<br/>          requests:<br/>            cpu: 100m<br/>            memory: 10Mi</pre>
<p>We can now review the pod distribution again using <kbd>kubectl get all</kbd>.</p>
<p>Notice that we added the container's resource limits and resource requests. The <kbd>limits</kbd> key allows us to specify resource limits for each container. On the other hand, <kbd>requests</kbd> informs the scheduler about the minimal resources required to run this component. A pod will not be able to run on a node if there are not enough resources to achieve the requested CPU, memory, and more. If any containers exceed their limits, they will be terminated:</p>
<pre><strong>$ kubectl get all -l example=myfirstds -o wide</strong><br/><strong>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</strong><br/><strong>pod/myfirstds-cr7xc 1/1 Running 0 84s 192.168.135.5 node3 &lt;none&gt; &lt;none&gt;</strong><br/><strong>pod/myfirstds-f6x8n 1/1 Running 0 84s 192.168.104.6 node2 &lt;none&gt; &lt;none&gt;</strong><br/><br/><strong>NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTOR</strong><br/><strong>daemonset.apps/myfirstds 2 2 2 2 2 &lt;none&gt; 84s myfirstpodcontainer nginx:alpine example=myfirstds</strong></pre>
<p>The <kbd>Deployment</kbd> resource is a higher-level concept, as it manages <kbd>ReplicaSet</kbd> and allows us to issue application component updates. It is recommended that you use <kbd>Deployment</kbd> instead of <kbd>ReplicaSet</kbd>. We will again use the <kbd>template</kbd> and <kbd>select</kbd> sections:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: myfirstdeployment<br/>  labels:<br/>    example: myfirstds<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      example: myfirstdeployment<br/>  template:<br/>    metadata:<br/>      name: myfirstpod<br/>      labels:<br/>        example: myfirstdeployment<br/>    spec:<br/>      containers:<br/>      - name: myfirstpodcontainer<br/>        image: nginx:alpine<br/>        ports:<br/>        - containerPort: 80</pre>
<p>Therefore, the deployment will run three replicas of <kbd>nginx:alpine</kbd>, distributed again on cluster nodes:</p>
<pre><strong>$  kubectl get all -l example=myfirstdeployment -o wide</strong><br/><strong>NAME                                     READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES</strong><br/><strong>pod/myfirstdeployment-794f9bfcd7-9m8vg   1/1     Running   0          12s   192.168.135.9    node3   &lt;none&gt;           &lt;none&gt;</strong><br/><strong>pod/myfirstdeployment-794f9bfcd7-f7499   1/1     Running   0          12s   192.168.104.10   node2   &lt;none&gt;           &lt;none&gt;</strong><br/><strong>pod/myfirstdeployment-794f9bfcd7-kfzfk   1/1     Running   0          12s   192.168.104.11   node2   &lt;none&gt;           &lt;none&gt;</strong><br/><br/><strong>NAME                                           DESIRED   CURRENT   READY   AGE   CONTAINERS            IMAGES         SELECTOR</strong><br/><strong>replicaset.apps/myfirstdeployment-794f9bfcd7   3         3         3       12s   myfirstpodcontainer   nginx:alpine   pod-template-hash=794f9bfcd7,example=myfirstdeployment</strong></pre>
<p>Notice that replicas are only running on some nodes. This is because we have some taints<strong> </strong>on the other nodes (some Kubernetes deployments avoid workloads on master nodes by default). Taints and tolerations help us to allow the scheduling of pods on only specific nodes. In this example, the master node will not run a workload, although it also has a worker role (it runs the Kubernetes worker processes that we learned about, <kbd>kubelet</kbd> and <kbd>kube-proxy</kbd>). These features remind us of Docker Swarm's node availability concepts. In fact, we can also execute <kbd>kubectl cordon &lt;NODE&gt;</kbd> to set a node as non-schedulable.</p>
<p>This chapter is a brief introduction to the main concepts of Kubernetes. We highly recommend that you view the Kubernetes documentation for further information: <a href="https://kubernetes.io">https://kubernetes.io</a>.</p>
<p>We can set replication based on a pod's performance and limits. This is known as <strong>autoscaling</strong>, and it is an interesting feature that is not available in Docker Swarm.</p>
<p>When an application's replicated components require persistence, we use another kind of resource. StatefulSets guarantee the order and uniqueness of pods.</p>
<p>Now that we know how to deploy applications, let's review how Kubernetes manages and deploys a network locally with components distributed on different nodes.</p>
<h1 id="uuid-68c6aed0-601e-4a31-9b3b-768a61261ea3">Kubernetes networking</h1>
<p class="mce-root">Kubernetes, like any other orchestrator, provides local and distributed networking. There are a few important communication assumptions that Kubernetes has to accomplish:</p>
<ul>
<li>Container-to-container communication</li>
<li>Pod-to-pod communication</li>
<li>Pod-to-service communication</li>
<li>User access and communication between external or internal applications<br/></li>
</ul>
<p>Container-to-container communication is easy because we learned that containers within a pod share the same IP and network namespace.</p>
<p>We know that each pod gets its own IP address. Therefore, Kubernetes needs to provide routing and accessibility to and from pods running on different hosts. Following the Docker concepts that we learned about in <a href="e7804d8c-ed8c-4013-8449-b746ee654210.xhtml">Chapter 4</a>, <em>Container Persistency and Networking</em>, Kubernetes also uses bridge networking for pods running on the same host. Therefore, all pods running on a host will be able to talk with each other using bridge networking.</p>
<p>Remember how Docker allowed us to deploy different bridge networks on a single host? This way, we were able to isolate applications on a host using different networks. Using this local concept, overlaying networks on a Docker Swarm cluster also deployed bridged interfaces. And these interfaces will be connected using tunnels created  between hosts using VXLAN. Isolation was something simple on Docker standalone hosts and Docker Swarm. Docker Engine had to manage all of the backstage magic to make this work with firewall rules and routing, but overlay networking is available out of the box.</p>
<p>Kubernetes provides a simpler approach. All pods run on the same network; hence, every pod will see other pods within the same host. In fact, we can go further—pods are locally accessible from hosts.</p>
<p>Let's consider this concept with a couple of pods. We will run <kbd>example-webserver</kbd> and <kbd>example-nettools</kbd> at the same time, executing simple <kbd>nginx:alpine</kbd> and <kbd>frjaraur/nettools:minimal</kbd> (this is a small alpine image with some helpful network tools) pods. First, we will create a deployment for <kbd>example-webserver</kbd> using <kbd>kubectl create deployment</kbd>:</p>
<pre><strong>$ kubectl create deployment example-webserver --image=nginx:alpine</strong><br/><strong>deployment.apps/example-webserver created</strong></pre>
<p>We review the pod's IP address using <kbd>kubectl get pods</kbd>:</p>
<pre><strong>$ kubectl get pods -o wide</strong><br/><strong>NAME                                READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES</strong><br/><strong>example-webserver-7789c6d697-kts7l   1/1     Running   0          69s   192.168.104.16   node2   &lt;none&gt;           &lt;none&gt;</strong></pre>
<p>As we said, <kbd>localhost</kbd> communications to the pod will work. Let's try a simple <kbd>ping</kbd> command from the host to the pod's IP address:</p>
<pre><strong>node3:~$ ping -c 2 192.168.104.16 </strong><br/><strong>PING 192.168.104.16 (192.168.104.16) 56(84) bytes of data.</strong><br/><strong>64 bytes from 192.168.104.16: icmp_seq=1 ttl=63 time=0.483 ms</strong><br/><strong>64 bytes from 192.168.104.16: icmp_seq=2 ttl=63 time=0.887 ms</strong><br/><br/><strong>--- 192.168.104.16 ping statistics ---</strong><br/><strong>2 packets transmitted, 2 received, 0% packet loss, time 1001ms</strong><br/><strong>rtt min/avg/max/mdev = 0.483/0.685/0.887/0.202 ms</strong></pre>
<p>Additionally, we can also have access to its running <kbd>nginx</kbd> process. Let's try <kbd>curl</kbd> using the pod's IP again, but this time, we will use port <kbd>80</kbd>:</p>
<pre><strong>node3:~$ curl -I 192.168.104.16:80</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Sun, 05 Jan 2020 22:20:42 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong></pre>
<p>Therefore, the host can communicate with all of the pods running on top of Docker Engine.</p>
<p>We can get a pod's IP address using <kbd>jsonpath</kbd>, to format the pod's information output, which is very interesting when we have hundreds of pods: <kbd>kubectl get pod example-webserver -o jsonpath='{.status.podIP}'</kbd>.</p>
<p>Let's execute an interactive pod with the aforementioned <kbd>frjaraur/nettools:minimal</kbd><em><strong> </strong></em>image. We will use <kbd>kubectl run --generator=run-pod/v1</kbd> to execute this new pod. Notice that we added <kbd>-ti -- sh</kbd> to run an interactive shell within this pod. From this pod, we will run <kbd>curl</kbd> again, connecting to the <kbd>example-webserver</kbd> pod's IP address:</p>
<pre><strong>$ kubectl run --generator=run-pod/v1 example-nettools --image=frjaraur/nettools:minimal -ti -- sh </strong><br/><strong>If you don't see a command prompt, try pressing enter.</strong><br/><strong>/ # ping -c 2 192.168.104.16 </strong><br/><strong>PING 192.168.104.16 (192.168.104.16): 56 data bytes</strong><br/><strong>64 bytes from 192.168.104.16: seq=0 ttl=62 time=0.620 ms</strong><br/><strong>64 bytes from 192.168.104.16: seq=1 ttl=62 time=0.474 ms</strong><br/><br/><strong>--- 192.168.104.16 ping statistics ---</strong><br/><strong>2 packets transmitted, 2 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max = 0.474/0.547/0.620 ms</strong><br/><br/><strong>/ # curl -I 192.168.104.16:80</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Sun, 05 Jan 2020 22:22:16 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong></pre>
<p>We have successfully accessed the deployed <kbd>example-webserver</kbd> pod using <kbd>ping</kbd> and <kbd>curl</kbd>, sending some requests to its <kbd>nginx</kbd> running process. It is clear that both containers can see each other.</p>
<p>There is something even more interesting in this example: we have not reviewed where these pods are running. In fact, they are running on different hosts, as we can read from the <kbd>kubectl get pods -o wide</kbd> command's output:</p>
<pre><strong>$ kubectl get pods -o wide</strong><br/><strong>NAME                                READY   STATUS    RESTARTS   AGE    IP               NODE    NOMINATED NODE   READINESS GATES</strong><br/><strong>example-nettools                     1/1     Running   1          85s    192.168.135.13   node3   &lt;none&gt;           &lt;none&gt;</strong><br/><strong>example-webserver-7789c6d697-kts7l   1/1     Running   0          5m8s   192.168.104.16   node2   &lt;none&gt;           &lt;none&gt;</strong></pre>
<p>Networking between hosts is controlled by another component that will allow these distributed communications. In this case, this component is Calico, which is a <strong>container network interface</strong> (<strong>CNI</strong>) applied to this Kubernetes cluster. The Kubernetes network model provides a flat network (all pods are distributed on the same network), and data plane networking is based on interchangeable plugins. We will use the plugin that best affords all of the required features in our environment.</p>
<p>There are other CNI implementations apart from Calico, such as Flannel, Weave, Romana, Cillium, and more. Each one provides its own features and host-to-host implementations. For example, Calico uses <strong>Border Gateway Protocol</strong> (<strong>BGP</strong>) to route real container IP addresses inside the cluster. Once a CNI is deployed, all of the container IP addresses will be managed by its implementation. They are usually deployed at the beginning of a Kubernetes cluster implementation. Calico allows us to implement network policies, which are very important to ensure security in this flat network where every pod sees other pods.</p>
<p>We have not looked at any service networking yet, which is also important here. If a pod dies, a new IP will be allocated, hence access will be lost on the previous IP address; that is why we use services. Remember, services are logical groupings of pods, usually with a virtual IP address. This IP address will be assigned from another pool of IP addresses (the service IP addresses pool). Pods and services do not share the same IP address pool. A service's IP will not change when new pods are recreated.</p>
<h2 id="uuid-7417803e-613e-40d5-85cf-21eeab0c3fd1">Service discovery</h2>
<p>Let's create a service associated with the currently deployed <kbd>example-webserver</kbd> deployment. We'll use <kbd>kubectl expose</kbd>:</p>
<pre><strong>$ kubectl expose deployment example-webserver \</strong><br/><strong>--name example-webserver-svc --type=NodePort --port=80</strong><br/> <br/><strong>service/example-webserver-svc exposed</strong></pre>
<p>We could have done this using either <kbd>kubectl create service</kbd> (imperative format) or a YAML definition file (declarative format). We used <kbd>kubectl expose</kbd> because it's simpler to quickly publish any kind or resource. We can review a service's IP addresses using <kbd>kubectl get services</kbd>:</p>
<pre><strong>$ kubectl get services -o wide</strong><br/><strong>NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR</strong><br/><strong>kubernetes             ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        11h   &lt;none&gt;</strong><br/><strong>example-webserver-svc   NodePort    10.98.107.31   &lt;none&gt;        80:30951/TCP   39s   app=example-webserver</strong></pre>
<p>Remember that we define the services associated with pods using selectors. In this case, the service will group all pods with the <kbd>app</kbd> label and the <kbd>example-webserver</kbd> value. This label was automatically created because we created <kbd>Deployment</kbd>. As a result, all pods grouped for this service will be accessible on the <kbd>10.98.107.31</kbd> IP address and the internal TCP port <kbd>80</kbd>. We defined which pod's port will be associated with this service—in both cases, we set port <kbd>80</kbd>:</p>
<pre><strong>$ curl -I 10.98.107.31:80</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Sun, 05 Jan 2020 22:26:09 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong></pre>
<p>It is accessible, as expected. Kubernetes' internal network has published this service on the defined ClusterIP address.</p>
<p>Because we created this service as <kbd>NodePort</kbd>, a random port has been associated with the service. In this case, it is port <kbd>30951</kbd>. As a result, requests will be routed to the application's pods within the cluster when we reach the cluster nodes' IP addresses in the randomly chosen port.</p>
<div><kbd>NodePort</kbd> ports are assigned randomly by default, but we can set them manually in the range between <kbd>30000</kbd> and <kbd>32767</kbd>.</div>
<p>Let's verify this feature. We will send some requests to the port that is listening on cluster nodes. In this example, we'll use the <kbd>curl</kbd> command on the local <kbd>0.0.0.0</kbd> IP address and port <kbd>30951</kbd> on various nodes:</p>
<pre><strong>node1:~$ curl -I 0.0.0.0:30951</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Sun, 05 Jan 2020 22:26:57 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong><br/><br/><br/><strong>node3:~$ curl -I 0.0.0.0:30951</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Sun, 05 Jan 2020 22:27:41 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong></pre>
<p>Communication between pods happens even if they are not running on the same node. The following output shows that pods are not running in either <kbd>node1</kbd> or <kbd>node3</kbd>. The application's pod is running on <kbd>node2</kbd>. The internal routing works:</p>
<pre><strong>$ kubectl get pods -o wide -l app=example-webserver</strong><br/><strong>NAME                                READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES</strong><br/><strong>example-webserver-7789c6d697-kts7l   1/1     Running   0          10m   192.168.104.16   node2   &lt;none&gt;           &lt;none&gt;</strong></pre>
<p>There is something more interesting, though—services create a DNS entry with their names following this pattern:</p>
<pre>&lt;SERVICE_NAME&gt;.&lt;NAMESPACE&gt;.svc.&lt;CLUSTER&gt;.&lt;DOMAIN&gt;</pre>
<p>In our example, we have not used a namespace or a domain. The service resolution will be simple: <kbd>example-webserver.default.svc.cluster.local</kbd>. This resolution is only available in the Kubernetes cluster by default. Therefore, we can test this resolution by executing a pod with the <kbd>host</kbd> or <kbd>nslookup</kbd> tools. We will attach our terminal interactively to the running <kbd>example-nettools</kbd><em><strong> </strong></em>pod using <kbd>kubectl attach</kbd> and run <kbd>host</kbd> and <kbd>curl</kbd> to test the DNS resolution:</p>
<pre><strong>$ kubectl attach example-nettools -c example-nettools -i -t</strong><br/><strong>If you don't see a command prompt, try pressing enter.</strong><br/><strong>/ # host example-webserver.default.svc.cluster.local</strong><br/><strong>example-webserver.default.svc.cluster.local has address 10.101.195.251</strong><br/><strong>/ # curl -I example-webserver.default.svc.cluster.local:80</strong><br/><strong>HTTP/1.1 200 OK</strong><br/><strong>Server: nginx/1.17.6</strong><br/><strong>Date: Sun, 05 Jan 2020 21:58:37 GMT</strong><br/><strong>Content-Type: text/html</strong><br/><strong>Content-Length: 612</strong><br/><strong>Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT</strong><br/><strong>Connection: keep-alive</strong><br/><strong>ETag: "5dd406e1-264"</strong><br/><strong>Accept-Ranges: bytes</strong></pre>
<p>We have confirmed that the service has a DNS entry that is reachable by any other Kubernetes cluster resource. We have also published the service using <kbd>NodePort</kbd>, so it is accessible on any node IP address. We could have an external load balancer routing requests to this deployed service on any cluster node's IP address and a chosen (or manually set) port. This port will be fixed for this service until it is removed.</p>
<p>Notice that we used <kbd>kubectl attach example-nettools -c example-nettools -i -t</kbd> to reconnect to a running pod left in the background.</p>
<p>In the next section, we will learn how scaling will change the described behavior.</p>
<h2 id="uuid-4e340af5-450a-48c7-85eb-5475caa6128e">Load balancing</h2>
<p>If we now scale up to three replicas, without changing anything on the deployed service, we will add load balancing features. Let's scale up using <kbd>kubectl scale</kbd>:</p>
<pre><strong>$ kubectl scale --replicas=3 deployment/example-webserver</strong><br/><strong>deployment.extensions/example-webserver scaled</strong></pre>
<p>Now we will have three running instances or pods for the <kbd>example-webserver</kbd> deployment.</p>
<p>Notice that we have scaled from the command line using the resource's type and its name: <kbd>kubectl scale --replicas=&lt;NUMBER_OF_REPLICAS&gt; &lt;RESOURCE_TYPE&gt;/&lt;NAME&gt;</kbd>.</p>
<p>We can review deployment pods using <kbd>kubectl get pods</kbd> with the associated label:</p>
<pre><strong>$ kubectl get pods -o wide -l app=example-webserver</strong><br/><strong>NAME                                READY   STATUS    RESTARTS   AGE    IP               NODE    NOMINATED NODE   READINESS GATES</strong><br/><strong>example-webserver-7789c6d697-dnx6l   1/1     Running   0          4m8s   192.168.135.14   node3   &lt;none&gt;           &lt;none&gt;</strong><br/><strong>example-webserver-7789c6d697-kts7l   1/1     Running   0          23m    192.168.104.16   node2   &lt;none&gt;           &lt;none&gt;</strong><br/><strong>example-webserver-7789c6d697-zdrtr   1/1     Running   0          4m8s   192.168.104.17   node2   &lt;none&gt;           &lt;none&gt;</strong></pre>
<p>If we now test the service's access again, we will reach each one of the three replicas. We execute the next simple loop to reach the service's backend pods five times:</p>
<pre><strong>$ for I in $(seq 5);do curl -I 10.98.107.31:80;done</strong></pre>
<p>If we review one of the deployed pod's logs using <kbd>kubectl logs</kbd>, we will notice that not all requests were logged. Although we made more than two requests using the service's IP address, we just logged a few:</p>
<pre><strong>$ kubectl logs example-webserver-7789c6d697-zdrtr</strong><br/><strong>192.168.166.128 - - [05/Jan/2020:22:44:32 +0000] "HEAD / HTTP/1.1" 200 0 "-" "curl/7.47.0" "-"</strong><br/><strong>192.168.166.128 - - [05/Jan/2020:22:45:38 +0000] "HEAD / HTTP/1.1" 200 0 "-" "curl/7.47.0" "-"</strong></pre>
<p>Only one-third of the requests are logged on each pod; therefore, the internal load balancer is distributing the traffic between all available applications' pods. Internal load balancing is deployed by default between all pods associated with a service.</p>
<p>As we have seen, Kubernetes provides flat networks for pods and services, simplifying networking and internal application accessibility. On the other hand, it is insecure because any pod can reach any other pods or services. In the next section, we will learn how to avoid this situation.</p>
<h2 id="uuid-d30eac09-40e3-4cf2-8331-e26c38d79666" class="">Network policies</h2>
<p>Network policies define rules to allow communication between groups of pods and other components. Using labels, we apply specific rules to matching pods for ingress and egress traffic on defined ports. These rules can be set using IP ranges, namespaces, or even other labels to include or exclude resources.</p>
<p>Network policies are applied using network plugins; therefore, the CNI deployed on our cluster must support them. For example, Calico supports <kbd>NetworkPolicy</kbd> resources.</p>
<p>We will be able to define default rules to all pods in the cluster, isolating all internet traffic, for example, or a defined group of hosts.</p>
<p>This YAML file represents an example of a <kbd>NetworkPolicy</kbd> resource applying ingress and egress traffic rules:</p>
<pre>apiVersion: networking.k8s.io/v1<br/>kind: NetworkPolicy<br/>metadata:<br/>  name: database-traffic<br/>spec:<br/>  podSelector:<br/>    matchLabels:<br/>      tier: database<br/>  policyTypes:<br/>  - Ingress<br/>  - Egress<br/>  ingress:<br/>  - from:<br/>    - ipBlock:<br/>        cidr: 172.17.10.0/24<br/>    - podSelector:<br/>        matchLabels:<br/>          tier: frontend<br/>    ports:<br/>    - protocol: TCP<br/>      port: 5432<br/>  egress:<br/>  - to:<br/>    - ipBlock:<br/>        cidr: 10.0.0.0/24<br/>    ports:<br/>    - protocol: TCP<br/>      port: 5978</pre>
<p>In this example, we will apply defined ingress and egress rules to all pods including the <kbd>tier</kbd> label with the <kbd>database</kbd> value.</p>
<p>The ingress rule allows traffic from any pod on the same namespace with the <kbd>tier</kbd> label and the <kbd>frontend</kbd><em><strong> </strong></em>value. All IP addresses in subnet <kbd>172.17.10.0/24</kbd> will also be allowed to access defined <kbd>database</kbd> pods.</p>
<p>The egress rule allows traffic from defined <kbd>database</kbd> pods to port <kbd>5978</kbd> on all IP addresses on subnet <kbd>10.0.0.0/24</kbd>.</p>
<p>If we do not apply a <kbd>NetworkPolicy</kbd> resource to a namespace, all traffic is allowed. We can change this behavior using <kbd>podSelector: {}</kbd>. This will match all pods in the namespace. For example, to disallow all egress traffic, we can use the following <kbd>NetworkPolicy</kbd> YAML definition:</p>
<pre>apiVersion: networking.k8s.io/v1<br/>kind: NetworkPolicy<br/>metadata:<br/>  name: default-deny<br/>spec:<br/>  podSelector: {}<br/>  policyTypes:<br/>  - Egress</pre>
<p class="mce-root">So, we have learned that we can ensure security even on a Kubernetes flat network with <kbd>NetworkPolicy</kbd> resources. Let's review the ingress resources.</p>
<h1 id="uuid-80dd5657-c4b4-4d44-b244-1ba8f28e3596">Publishing applications</h1>
<p>Ingress resources help us to publish applications deployed on Kubernetes clusters. They work very well with HTTP and HTTPS services, providing many features for distributing and managing traffic between services. This traffic will be located on the OSI model's transport and application layers; they are also known as layers 4 and 7, respectively. It also works with raw TCP and UDP services; however, in these cases, traffic will be load balanced at layer 4 only.</p>
<p>These resources route traffic from outside the cluster to services running within the cluster. Ingress resources require the existence of a special service called an <strong>ingress controller</strong>. These services will load balance or route traffic using rules created by ingress resources. Therefore, publishing an application using this feature requires two components:</p>
<ul>
<li><strong>Ingress resource</strong>: The rules to apply to incoming traffic</li>
<li><strong>Ingress controller</strong>: The load balancer that will automatically convert or translate ingress rules to load balance configurations</li>
</ul>
<p>A combination of both objects provides the dynamic publishing of applications. If one application's pod dies, a new one will be created and the service and ingress controller will automatically route all traffic to the new one. This will also isolate services from external networks. We will publish one single endpoint instead of the <kbd>NodePort</kbd> or <kbd>LoadBalancer</kbd> service types for all services, which will consume many nodes' ports or cloud IP addresses. This endpoint is the load balancer that will use the ingress controller and ingress resource rules to route traffic internally to deployed services:</p>
<div><img src="img/64584d31-c831-4590-a961-cc5877468a3a.jpg" style=""/></div>
<p>This chapter's labs show us an interesting load balancing example using <strong>NGINX Ingress Controller</strong>. Let's review a quick example YAML configuration file:</p>
<pre>apiVersion: networking.k8s.io/v1beta1<br/>kind: Ingress<br/>metadata:<br/>  name: simple-fanout-example<br/>  annotations:<br/>    nginx.ingress.kubernetes.io/rewrite-target: /<br/>spec:<br/>  rules:<br/>  - host: example.local<br/>    http:<br/>      paths:<br/>      - path: /example1<br/>        backend:<br/>          serviceName: example-webserver<br/>          servicePort: 80<br/>      - path: /example2<br/>        backend:<br/>          serviceName: another-service<br/>          servicePort: 8080</pre>
<p class="mce-root">This example outlines the rules that should be applied to route requests to a specific <kbd>example.local</kbd>-specific host header. Any request containing <kbd>/example1</kbd> in its URL will be guided to <kbd>example-webserver</kbd>, while <kbd>another-service</kbd> will receive requests containing the <kbd>/example2</kbd> path in its URL. Notice that we have used the internal service's ports; therefore, no additional service exposure is required. One ingress controller endpoint will redirect traffic to the <kbd>example-webserver</kbd> and <kbd>another-service</kbd> services. This saves up the host's ports (and/or IP addresses on the cloud providers because the <kbd>LoadBalancer</kbd> service type uses one published public IP address per service).</p>
<p>We can provide as many ingress controllers as needed. In fact, in multi-tenant environments, we usually deploy more than just one to isolate publishing planes between different tenants.</p>
<p>This brief look at publishing applications on Kubernetes has finished this review of the main Kubernetes networking features. Let's now move on to Kubernetes security properties.</p>
<h1 id="uuid-f9b6b8a0-a1ca-48b1-9ac0-c45778a49c62">Kubernetes security components and features</h1>
<p>Kubernetes provides mechanisms to authenticate and authorize access to its API. This allows us to apply different levels of privileges for users or roles within a cluster. This prevents unauthorized access to some core resources, such as scheduling or nodes in the cluster.</p>
<p>Once users are allowed to use cluster resources, we use namespaces to isolate their own resources from other users. This works even in multi-tenant environments where a higher level of security is required.</p>
<p>Kubernetes works with the very elaborate <strong>Role-Based Access Control</strong> (<strong>RBAC</strong>) environment, which provides a great level of granularity to allow specific actions on some resources while other actions are denied.</p>
<p>We manage the <kbd>Role</kbd> and <kbd>ClusterRole</kbd> resources to describe permissions for different resources. We use <kbd>Role</kbd> to define permissions within namespaces and <kbd>ClusterRole</kbd> for permissions on cluster-wide resources. Rules are supplied using some defined verbs, such as <kbd>list</kbd>, <kbd>get</kbd>, <kbd>update</kbd>, and more, and these verbs are applied to resources (or even specific resource names). The <kbd>RoleBinding</kbd> and <kbd>ClusterRoleBinding</kbd> resources grant permissions defined in roles to users or sets of users.</p>
<p>Kubernetes also provides the following:</p>
<ul>
<li>Service accounts to identify processes within pods to other resources</li>
<li>Pod security policies to control the special behaviors of pods, such as privileged containers, host namespaces, restrictions on running containers with root users, or enabling read-only root filesystems on containers, among other features</li>
<li>Admission controllers to intercept API requests, allowing us to validate or modify them to ensure image freshness and security, forcing the creation of pods to always pull from registries, to set the default storage, to deny the execution of processes within privileged containers, or to specify the default host resource limit ranges if none are declared, among other security features</li>
</ul>
<p>It is very important in production environments to limit a host's resource usage because non-limited pods can consume all of their resources by default.</p>
<p>Kubernetes provides many features to ensure cluster security at all levels. It is up to you to use them because most of them are not applied by default. We will learn more about the roles and grants applied to resources in <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>, because many of these configurations are integrated into Docker Enterprise.</p>
<p>We are not going to go deeper into this topic because Kubernetes is not part of the current Docker Certified Associate curriculum, and this is just a quick introduction.</p>
<p>It is recommended that you take a closer look at Kubernetes' security features because it has many more compared to Docker Swarm. On the other hand, it is true that Docker Enterprise provides many of these features to Docker Swarm.</p>
<h1 id="uuid-0903b6b8-6b8b-4378-9fa0-4fd977aec86a">Comparing Docker Swarm and Kubernetes side by side</h1>
<p>In this section, we will compare the Docker Swarm and Kubernetes features side by side to get a good idea of how they solve common problems. We have discussed these concepts in both this chapter and in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm.</em> They have common approaches to many problems:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Parameters</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Docker Swarm</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Kubernetes</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">High-availability solution</td>
<td>Provides high availability for core components.</td>
<td>Provides high availability for core components.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Resilience</td>
<td>All services run with resilience based on the state definition.</td>
<td>All resources based on replication controllers will provide resilience (<kbd>ReplicaSet</kbd>, <kbd>DaemonSet</kbd>, <kbd>Deployment</kbd>, and <kbd>StatefulSet</kbd>) based on the state definition.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Infrastructure as code</td>
<td>The Docker Compose file format will allow us to deploy stacks.</td>
<td>We will use YAML to format resource files. These will allow us to deploy workloads using a declarative format.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Dynamic distribution</td>
<td>Application components and their replicas will be automatically distributed cluster-wide, although we can provide some constraints.</td>
<td>Kubernetes also distributes components, but we can provide advanced constraints using labels and other features.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Automatic updates</td>
<td>Application components can be upgraded using rolling updates and rollbacks in the case of a failure.</td>
<td>Kubernetes also provides rolling updates and rollbacks.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Publishing applications</td>
<td>Docker Swarm provides internal load balancing between service replicas and <strong>router mesh</strong> to publish an application's service's ports on all of the cluster nodes at the same time.</td>
<td>Kubernetes also provides internal load balancing, and <kbd>NodePort</kbd> type services will also publish the application's components on all of the nodes at the same time. But Kubernetes also provides load balancing services (among other types) to auto-configure external load balancers to route requests to deployed services.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Cluster-internal networking</td>
<td>Containers that are deployed as tasks for each service can communicate with other containers deployed in the same network. Internal IP management will provide their IP addresses, and services can be consumed by their names so that there is internal DNS resolution.</td>
<td style="width: 37.123%">Pod-to-pod communication works and IP addresses are provided by internal <strong>Internet Protocol Address Management</strong> (IPAM). We will also have service-to-service communication and resolution.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Key-value store</td>
<td>Docker Swarm provides an internal store to manage all objects and their statuses. This store will have high availability with an odd number of master nodes.</td>
<td>Kubernetes also requires a key-value store to manage its resources. This component is provided using <kbd>etcd</kbd> and we can deploy it externally out of Kubernetes cluster nodes. We should provide an odd number of <kbd>etcd</kbd> nodes to provide high availability.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The preceding table showed us the main similarities regarding solving common problems. The next table will show the main differences:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Parameters</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Docker Swarm</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Kubernetes</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Pods versus tasks</td>
<td>Docker Swarm deploys tasks for services. Each task will run one container at a time. If the container dies, a new one will be created to ensure the required number of replicas (tasks).Services are the smallest unit of deployment. We will deploy applications running their components as services.</td>
<td>Kubernetes has the concept of a pod. Each pod can run more than one container inside. All of them share the same IP address (networking namespace). Containers inside a pod share volumes and will always run on the same host. A pod's life relies on containers. If one of them dies, a pod is unhealthy. Pods are the smallest unit of deployment in Kubernetes; therefore, we scale pods up and down, with all of their containers.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Services</td>
<td>Services in Docker Swarm are objects with an IP address for internal load balancing between replicas (by default, we can avoid this using the <kbd>dnsrr</kbd> endpoint mode). We create services to execute our application components, and we scale up or down the number of replicas required to be healthy.</td>
<td>In Kubernetes, services are different. They are logical resources. This means that they are deployed only to publish a group of pod resources. Kubernetes services are logical groupings of pods that work together. Kubernetes services also get an IP address for internal load balancing (<kbd>clusterIP</kbd>), and we can also avoid this situation by using the "headless" feature.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Networking</td>
<td>Docker Swarm deploys overlay networking by default. This ensures communications between an application's components are deployed on different hosts.Stacks in Docker Swarm will be deployed on different networks. This means that we can provide a subnet for each application. Multiple networks for deployments will provide a good level of security because they are isolated from each other. This can be improved using available network encryption (disabled by default). However, on the other hand, they are difficult to manage and things can get complicated when we need to provide isolation on services integrated into multiple stacks.</td>
<td>Kubernetes provides a flat network using a common interface called a CNI. Networking has been decoupled from Kubernetes' core to allow us to use multiple and different networking solutions. Each solution has its own features and implementation for routing on a cluster environment. A flat network makes things easier. All pods and services will see each other by default. On the other hand, security is not provided. We will deploy <kbd>NetworkPolicy</kbd> resources to ensure secure communications between resources in the cluster. These policies will manage who can talk to who in the Kubernetes world.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Authentication and authorization</td>
<td>Docker Swarm, by default, does not provide any mechanism to authenticate or authorize specific requests. Once a Docker Swarm node has published its daemon access (in a <kbd>daemon.json</kbd> configuration file), anyone can connect to it and manage the cluster if we use a manager node. This is a security risk that should always be avoided. We can create a secure client configuration with SSL/TLS certificates. But certificates in Docker Swarm will ensure secure communication only. There is no authorization validation. Docker Enterprise will provide the required features to provide RBAC to Docker Swarm's clusters. </td>
<td>Kubernetes does provide authentication and authorization. In fact, it includes a full-featured RBAC system to manage users' and applications' accesses to the resources deployed within the Kubernetes cluster. This RBAC system allows us to set specific permissions for a user's or team's access. Using Kubernetes namespaces will also improve security in multi-tenant or team scenarios.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Secrets</td>
<td>Docker encrypts secrets by default. They will only be readable inside containers while they are running.</td>
<td>Kubernetes will encode secrets using the Base64 algorithm by default. We will need to use external secret providers or additional encryption configuration (<kbd>EncryptionConfig</kbd>) to ensure a secret's integrity.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">Publishing applications</td>
<td>Docker Swarm just provides a router mesh for publishing applications. This will publish application ports on all cluster nodes. This can be insecure because all nodes will have all the applications published and we will use a lot of ports (at least one for each published application). Docker Enterprise will provide Interlock, which has many features in common with ingress controllers.</td>
<td>Kubernetes provides ingress controller resources. Ingress controllers publish a few endpoints (using <kbd>NodePort</kbd> or any other cloud service definition), and this internal ingress will talk to services' backends (pods). This will require fewer ports for applications (only those required to publish ingress controllers). Requests will be routed by these resources to real backend services. Security is improved because we add a smart piece of software in the middle of the requests to help us to decide which backends will process requests. The ingress controller acts as a reverse-proxy and it will verify whether a valid host header is used on every request. If none is used, requests will be forwarded to a default backend. If requests contain a valid header, they will be forwarded to the defined service's virtual IP and the internal load balancer will choose which pod will finally receive them. The orchestrator will manage defined rules and clusters, and the internal or external load balancer will interpret them to ensure the right backend receives the user's request.<br/></td>
</tr>
</tbody>
</table>
<p> </p>
<p>So far, we have learned that there are several similarities and differences between Docker Swarm and Kubernetes. We can note the following:</p>
<ul>
<li>Kubernetes provides more container density.</li>
<li>Docker Swarm provides cluster-wide networking with subnets for isolation by default.</li>
<li>Kubernetes provides role-based access to cluster resources.</li>
<li>Publishing applications in Kubernetes is better using ingress controllers.</li>
</ul>
<p>Let's now review some of the topics we have learned by applying them to some easy labs.</p>
<h1 id="uuid-2d275d80-722a-4b3b-899d-5ec244c5a7ff">Chapter labs</h1>
<p>We will now work through a long lab that will help us to review the concepts we've learned so far.</p>
<p>Deploy <kbd>environments/kubernetes</kbd> from this book's GitHub repository (<a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a>) if you have not done so yet. You can use your own Linux server. Use <kbd>vagrant up</kbd> from the <kbd>environments/kubernetes</kbd> folder to start your virtual environment. All files used during these labs can be found inside the <kbd>chapter9</kbd> folder.</p>
<p>Wait until all of the nodes are running. We can check the status of the nodes using <kbd>vagrant status</kbd>. Connect to your lab node using <kbd>vagrant ssh kubernetes-node1</kbd>. Vagrant deploys three nodes for you, and you will be using the <kbd>vagrant</kbd> user with root privileges using <kbd>sudo</kbd>. You should have the following output:</p>
<pre><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/kubernetes$ vagrant up<br/>--------------------------------------------------------------------------------------------<br/> KUBERNETES Vagrant Environment<br/> Engine Version: current<br/> Kubernetes Version: 1.14.0-00<br/> Kubernetes CNI: https://docs.projectcalico.org/v3.8/manifests/calico.yaml<br/>--------------------------------------------------------------------------------------------<br/>Bringing machine 'kubernetes-node1' up with 'virtualbox' provider...<br/>Bringing machine 'kubernetes-node2' up with 'virtualbox' provider...<br/>Bringing machine 'kubernetes-node3' up with 'virtualbox' provider...<br/></strong><br/><strong>...</strong><br/><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/kubernetes$</strong></pre>
<p>Nodes will have three interfaces (IP addresses and virtual hardware resources can be modified by changing the <kbd>config.yml</kbd> file):</p>
<ul>
<li><kbd>eth0 [10.0.2.15]</kbd>: This is an internal interface, required for Vagrant.</li>
<li><kbd>eth1 [10.10.10.X/24]</kbd>: This is prepared for Docker Kubernetes' internal communication. The first node will get the <kbd>10.10.10.11</kbd> IP address and so on.</li>
<li><kbd>eth2 [192.168.56.X/24]</kbd>: This is a host-only interface for communication between your host and the virtual nodes. The first node will get the <kbd>192.168.56.11</kbd> IP address and so on.</li>
</ul>
<p>We will use the <kbd>eth1</kbd> interface for Kubernetes, and we will be able to connect to published applications using the <kbd>192.168.56.X/24</kbd> IP address' range. All nodes have Docker Engine Community Edition installed and a Vagrant user is allowed to execute <kbd>docker</kbd>. A small Kubernetes cluster with one master (<kbd>kubernetes-node1</kbd>) and two worker nodes (<kbd>kubernetes-node2</kbd> and <kbd>kubernetes-node3</kbd>) will be deployed for you.</p>
<p class="mce-root">We can now connect to the first deployed virtual node using <kbd>vagrant ssh kubernetes-node1</kbd>. The process may vary if you have already deployed a Kubernetes virtual environment and have just started it using <kbd>vagrant up</kbd>:</p>
<pre><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/kubernetes$ vagrant ssh kubernetes-node1</strong><br/><strong>vagrant@kubernetes-node1:~$</strong></pre>
<p>Now you are ready to start the labs. We will start these labs by deploying a simple application.</p>
<h2 id="uuid-0c563b01-759b-45a0-b85e-c906df96c44c">Deploying applications in Kubernetes</h2>
<p>Once Vagrant (or your own environment) is deployed, we will have three nodes (named <kbd>kubernetes-node&lt;index&gt;</kbd> from <kbd>1</kbd> to <kbd>3</kbd>) with Ubuntu Xenial and Docker Engine installed. Kubernetes will also be up and running for you, with one master node and two workers. The Calico CNI will also be deployed for you automatically.</p>
<p>First, review your node IP addresses (<kbd>10.10.10.11</kbd> to <kbd>10.10.10.13</kbd> if you used Vagrant, because the first interface will be Vagrant-internal).</p>
<p>The steps for deploying our application are as follows:</p>
<ol>
<li>Connect to <kbd>kubernetes-node1</kbd> and review the deployed Kubernetes cluster using <kbd>kubectl get nodes</kbd>. A file, named <kbd>config</kbd>, including the required credentials and the Kubernetes API endpoint will be copied under the <kbd>~/.kube</kbd><em><strong> </strong></em>directory automatically. We'll also refer to this file as <strong>Kubeconfig</strong>. This file configures the <kbd>kubectl</kbd> command line for you:</li>
</ol>
<pre style="padding-left: 60px"><strong>Docker-Certified-Associate-DCA-Exam-Guide/environments/kubernetes$ vagrant ssh kubernetes-node1</strong><br/><br/><strong>vagrant@kubernetes-node1:~$ kubectl get nodes</strong><br/><strong>NAME STATUS ROLES AGE VERSION<br/>kubernetes-node1 Ready master 6m52s v1.14.0<br/>kubernetes-node2 Ready &lt;none&gt; 3m57s v1.14.0<br/>kubernetes-node3 Ready &lt;none&gt; 103s v1.14.0<br/></strong></pre>
<p style="padding-left: 60px">Kubernetes cluster version 1.14.00 has been deployed and is running. Notice that <kbd>kubernetes-node1</kbd> is the only master node in this cluster; therefore, we are not providing high availability.</p>
<p style="padding-left: 60px">Currently, we are using the <kbd>admin</kbd> user, and, by default, all deployments will run on the <kbd>default</kbd> namespace, unless another is specified. This configuration is also done in the <kbd>~/.kube/config</kbd> file.</p>
<p style="padding-left: 60px">The Calico CNI was also deployed; hence, host-to-container networking should work cluster-wide.</p>
<ol start="2">
<li>Create a deployment file, named <kbd>blue-deployment-simple.yaml</kbd>, using your favorite editor with the following content:</li>
</ol>
<pre style="padding-left: 60px">apiVersion: extensions/v1beta1<br/>kind: Deployment<br/>metadata:<br/>  name: blue-app<br/>  labels:<br/>    color: blue<br/>    example: blue-app<br/>spec:<br/>  replicas: 2<br/>  selector:<br/>    matchLabels:<br/>      app: blue<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: blue<br/>    spec:<br/>      containers:<br/>      - name: blue<br/>        image: codegazers/colors:1.12<br/>        env:<br/>        - name: COLOR<br/>          value: blue<br/>        ports:<br/>        - containerPort: 3000</pre>
<p style="padding-left: 60px">This will deploy two replicas of the<em><strong> </strong></em><kbd>codegazers/colors:1.12</kbd> image. We will expect two running pods after it is deployed. We set the <kbd>COLOR</kbd> environment variable to <kbd>blue</kbd> and, as a result, all of the application components will be <kbd>blue</kbd>. Containers will expose port <kbd>3000</kbd> internally within the cluster.</p>
<ol start="3">
<li>Let's deploy this <kbd>blue-app</kbd> application using <kbd>kubectl create -f &lt;KUBERNETES_RESOURCES_FILE&gt;.yaml</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl create -f blue-deployment-simple.yaml</strong><br/><strong>deployment.extensions/blue-app created</strong></pre>
<p style="padding-left: 60px">This command line has created a deployment, named <kbd>blue-app</kbd>, with two replicas. Let's review the deployment created using <kbd>kubectl get deployments</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl get deployments -o wide</strong><br/><strong>NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR</strong><br/><strong>blue-app 2/2 2 2 103s blue codegazers/colors:1.12 app=blue</strong></pre>
<p style="padding-left: 60px">Therefore, two pods will be running, associated with the <kbd>blue-app</kbd><em><strong> </strong></em>deployment. Let's now review the deployed pods using <kbd>kubectl get pods</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl get pods -o wide</strong><br/><strong>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</strong><br/><strong>blue-app-54485c74fc-wgw7r 1/1 Running 0 2m8s 192.168.135.2 kubernetes-node3 &lt;none&gt; &lt;none&gt;</strong><br/><strong>blue-app-54485c74fc-x8p92 1/1 Running 0 2m8s 192.168.104.2 kubernetes-node2 &lt;none&gt; &lt;none&gt;</strong></pre>
<p style="padding-left: 60px">In this case, one pod runs on <kbd>kubernetes-node2</kbd> and another one runs on <kbd>kubernetes-node3</kbd>. Let's try to connect to their virtual assigned IP addresses on the exposed port. Remember that IP addresses will be assigned randomly, hence they may vary on your environment. We will just use <kbd>curl</kbd> against the IP address of <kbd>kubernetes-node1</kbd> and the pod's internal port:</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ curl 192.168.104.2:3000/text</strong><br/><strong>APP_VERSION: 1.0</strong><br/><strong>COLOR: blue</strong><br/><strong>CONTAINER_NAME: blue-app-54485c74fc-x8p92</strong><br/><strong>CONTAINER_IP: 192.168.104.2</strong><br/><strong>CLIENT_IP: ::ffff:192.168.166.128</strong><br/><strong>CONTAINER_ARCH: linux</strong></pre>
<p style="padding-left: 60px">We can connect from <kbd>kubernetes-node1</kbd> to pods running on other hosts correctly. So, Calico is working correctly.</p>
<p style="padding-left: 60px">We should be able to connect to any pods' deployed IP addresses. These IP addresses will change whenever a container dies and a new pod is deployed. We will never connect to pods to consume their application processes. We will use services instead of pods to publish applications, as we have already discussed in this chapter. They will not change their IP addresses when application components, running as pods, have to be recreated.</p>
<ol start="4">
<li>Let's create a service to load balance requests between deployed pods with a fixed virtual IP address. Create the <kbd>blue-service-simple.yaml</kbd> file with the following content:</li>
</ol>
<pre style="padding-left: 60px">apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: blue-svc<br/>spec:<br/>  ports:<br/>  - port: 80<br/>    targetPort: 3000<br/>    protocol: TCP<br/>    name: http<br/>  selector:<br/>    app: blue</pre>
<p style="padding-left: 60px">A random IP address will be associated with this service. This IP address will be fixed, and it will be valid even if pods die. Notice that we have exposed a new port for the service. This will be the service's port, and requests reaching the defined port <kbd>80</kbd><em><strong> </strong></em>will be routed to port <kbd>3000</kbd> on each pod. We will use <kbd>kubectl get svc</kbd> to retrieve the service's port and IP address:</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl create -f blue-service-simple.yaml</strong><br/><strong>service/blue-svc created</strong><br/><br/><strong>vagrant@kubernetes-node1:~$ kubectl get svc</strong><br/><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong><br/><strong>blue-svc ClusterIP 10.100.207.49 &lt;none&gt; 80/TCP 7s</strong><br/><strong>kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 53m</strong></pre>
<ol start="5">
<li>Let's verify the internal load balance by sending some requests to the <kbd>blue-svc</kbd> service using <kbd>curl</kbd> against its IP address, accessing port <kbd>80</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ curl 10.100.207.49:80/text</strong><br/>APP_VERSION: 1.0<br/>COLOR: blue<br/><strong>CONTAINER_NAME: blue-app-54485c74fc-x8p92</strong><br/><strong>CONTAINER_IP: 192.168.104.2</strong><br/>CLIENT_IP: ::ffff:192.168.166.128<br/>CONTAINER_ARCH: linux</pre>
<ol start="6">
<li>Let's try again using <kbd>curl</kbd>. We will test the internal load balancing by executing some requests to the service's IP address and port:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ curl 10.100.207.49:80/text</strong><br/>APP_VERSION: 1.0<br/>COLOR: blue<br/><strong>CONTAINER_NAME: blue-app-54485c74fc-wgw7r</strong><br/><strong>CONTAINER_IP: 192.168.135.2</strong><br/>CLIENT_IP: ::ffff:192.168.166.128<br/>CONTAINER_ARCH: linux</pre>
<p style="padding-left: 60px">The service has load-balanced our requests between both pods. Let's now try to expose this service to be accessible to the application's users.</p>
<ol start="7">
<li>Now we will remove the previous service's definition and deploy a new one with the service's <kbd>NodePort</kbd> type. We will use <kbd>kubectl delete -f &lt;KUBERNETES_RESOURCES_FILE&gt;.yaml</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl delete -f blue-service-simple.yaml</strong><br/><strong>service "blue-svc" deleted</strong></pre>
<p style="padding-left: 60px" class="mce-root">Create a new definition, <kbd>blue-service-nodeport.yaml</kbd>, with the following content:</p>
<pre style="padding-left: 60px">apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: blue-svc<br/>spec:<br/>  type: NodePort<br/>  ports:<br/>  - port: 80<br/>    targetPort: 3000<br/>    protocol: TCP<br/>    name: http<br/>  selector:<br/>    app: blue</pre>
<ol start="8">
<li>We now just create a service definition and notice a random port associated with it. We will also use <kbd>kubectl create</kbd> and <kbd>kubectl get svc</kbd> after it is deployed:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl create -f blue-service-nodeport.yaml</strong><br/><strong>service/blue-svc created</strong><br/><br/><strong>vagrant@kubernetes-node1:~$ kubectl get svc</strong><br/><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong><br/><strong>blue-svc NodePort 10.100.179.60 &lt;none&gt; 80:32648/TCP 5s</strong><br/><strong>kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 58m</strong></pre>
<ol start="9">
<li>We learned that the <kbd>NodePort</kbd> service will act as Docker Swarm's router mesh. Therefore, the service's port will be fixed on every node. Let's verify this feature using <kbd>curl</kbd> against any node's IP address and assigned port. In this example, it is <kbd>32648</kbd>. This port may vary on your environment because it will be assigned dynamically:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ curl 0.0.0.0:32648/text</strong><br/><strong>APP_VERSION: 1.0</strong><br/><strong>COLOR: blue</strong><br/><strong>CONTAINER_NAME: blue-app-54485c74fc-x8p92</strong><br/><strong>CONTAINER_IP: 192.168.104.2</strong><br/><strong>CLIENT_IP: ::ffff:192.168.166.128</strong><br/><strong>CONTAINER_ARCH: linux</strong></pre>
<ol start="10">
<li class="mce-root">Locally, on <kbd>node1</kbd> port <kbd>32648</kbd>, the service is accessible. It should be accessible on any of the nodes on the same port. Let's try on <kbd>node3</kbd>, for example, using <kbd>curl</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node3:~$ curl 10.10.10.13:32648/text</strong><br/><strong>APP_VERSION: 1.0</strong><br/><strong>COLOR: blue</strong><br/><strong>CONTAINER_NAME: blue-app-54485c74fc-wgw7r</strong><br/><strong>CONTAINER_IP: 192.168.135.2</strong><br/><strong>CLIENT_IP: ::ffff:10.0.2.15</strong><br/><strong>CONTAINER_ARCH: linux</strong></pre>
<p style="padding-left: 60px">We learned that even if a node does not run a related workload, the service will be accessible on the defined (or, in this case, random) port using <kbd>NodePort</kbd>.</p>
<ol start="11">
<li>We will finish this lab by upgrading the deployment images to a newer version. We will use <kbd>kubectl set image deployment</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl set image deployment blue-app blue=codegazers/colors:1.15</strong><br/><strong>deployment.extensions/blue-app image updated</strong></pre>
<ol start="12">
<li>Let's review the deployment again to verify that the update was done. We will use <kbd>kubectl get all -o wide</kbd> to retrieve all of the created resources and their locations:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl get all -o wide</strong><br/><strong>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</strong><br/><strong>pod/blue-app-787648f786-4tz5b 1/1 Running 0 76s 192.168.104.3 node2 &lt;none&gt; &lt;none&gt;</strong><br/><strong>pod/blue-app-787648f786-98bmf 1/1 Running 0 76s 192.168.135.3 node3 &lt;none&gt; &lt;none&gt;</strong><br/><br/><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR</strong><br/><strong>service/blue-svc NodePort 10.100.179.60 &lt;none&gt; 80:32648/TCP 22m app=blue</strong><br/><strong>service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 81m &lt;none&gt;</strong><br/><br/><strong>NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR</strong><br/><strong>deployment.apps/blue-app 2/2 2 2 52m blue codegazers/colors:1.15 app=blue</strong><br/><br/><strong>NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR</strong><br/><strong>replicaset.apps/blue-app-54485c74fc 0 0 0 52m blue codegazers/colors:1.12 app=blue,pod-template-hash=54485c74fc</strong><br/><strong>replicaset.apps/blue-app-787648f786 2 2 2 76s blue codegazers/colors:1.15 app=blue,pod-template-hash=787648f786</strong></pre>
<ol start="13">
<li>Notice that new pods were created with a newer image. We can verify the update using <kbd>kubectl rollout status</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl rollout status deployment.apps/blue-app</strong><br/><strong>deployment "blue-app" successfully rolled out</strong></pre>
<ol start="14">
<li>We can go back to the previous image version just by executing <kbd>kubectl rollout undo</kbd>. Let's go back to the previous image version:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl rollout undo deployment.apps/blue-app</strong><br/><strong>deployment.apps/blue-app rolled back</strong></pre>
<ol start="15">
<li>And now, we can verify that the current <kbd>blue-app</kbd> deployment runs the <kbd>codegazers/colors:1.12</kbd> images again. We will again review deployment locations using <kbd>kubectl get all</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ kubectl get all -o wide</strong><br/><strong>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</strong><br/><strong>pod/blue-app-54485c74fc-kslgw 1/1 Running 0 62s 192.168.104.4 node2 &lt;none&gt; &lt;none&gt;</strong><br/><strong>pod/blue-app-54485c74fc-lrkxv 1/1 Running 0 62s 192.168.135.4 node3 &lt;none&gt; &lt;none&gt;</strong><br/><br/><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR</strong><br/><strong>service/blue-svc NodePort 10.100.179.60 &lt;none&gt; 80:32648/TCP 29m app=blue</strong><br/><strong>service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 87m &lt;none&gt;</strong><br/><br/><strong>NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR</strong><br/><strong>deployment.apps/blue-app 2/2 2 2 58m blue codegazers/colors:1.12 app=blue</strong><br/><br/><strong>NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR</strong><br/><strong>replicaset.apps/blue-app-54485c74fc 2 2 2 58m blue codegazers/colors:1.12 app=blue,pod-template-hash=54485c74fc</strong><br/><strong>replicaset.apps/blue-app-787648f786 0 0 0 7m46s blue codegazers/colors:1.15 app=blue,pod-template-hash=787648f786</strong></pre>
<p style="padding-left: 60px">Going back to the previous state was very easy.</p>
<p>We can set comments for each change using the <kbd>--record</kbd> option on the <kbd>update</kbd> commands.</p>
<h2 id="uuid-e74b21c2-f687-4cc9-9272-6be8f2f7ed62">Using volumes</h2>
<p>In this lab, we will deploy a simple web server using different volumes. We will use <kbd>webserver.deployment.yaml</kbd>.</p>
<p class="mce-root">We have prepared the following volumes:</p>
<ul>
<li><kbd>congigMap</kbd>: Config volume with <kbd>/etc/nginx/conf.d/default.conf</kbd>—the configuration file)</li>
<li><kbd>emptyDir</kbd>: Empty volume for NGINX logs, <kbd>/var/log/nginx</kbd></li>
<li><kbd>secret</kbd>: Secret volume to specify some variables to compose the <kbd>index.html</kbd> page</li>
<li><kbd>persistentVolumeClaim</kbd>: Data volume bound to the <kbd>hostPath</kbd> defined as <kbd>persistentVolume</kbd> using the host's <kbd>/mnt</kbd> content</li>
</ul>
<div><p>We have declared one specific node for our web server to ensure the <kbd>index.html</kbd> file location under the <kbd>/mnt</kbd> directory. We have used <kbd>nodeName: kubernetes-node2</kbd> in our deployment file, <kbd>webserver.deployment.yaml</kbd>:</p>
</div>
<ol>
<li>First, we verify that there is no file under the <kbd>/mnt</kbd> directory in the <kbd>kubernetes-node2</kbd> node. We connect to <kbd>kubernetes-node2</kbd> and then we review the <kbd>/mnt</kbd> content:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ vagrant ssh kubernetes-node2<br/><br/>vagrant@kubernetes-node2:~$ ls  /mnt/</strong></pre>
<ol start="2">
<li>Then, we change to <kbd>kubernetes-node1</kbd> to clone our repository and launch the web server deployment:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ vagrant ssh kubernetes-node1<br/><br/>vagrant@kubernetes-node1:~$ git clone https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</strong></pre>
<p style="padding-left: 60px">We move to <kbd>chapter9/nginx-lab/yaml</kbd>:</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~$ cd Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml/</strong><br/><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$</strong></pre>
<ol start="3">
<li>We will use the <kbd>ConfigMap</kbd>, <kbd>Secret</kbd>, <kbd>Service</kbd>, <kbd>PersistentVolume</kbd>, and <kbd>PersistentVolumeClaim</kbd> resources in this lab using YAML files. We will deploy all of the resource files in the <kbd>yaml</kbd> directory:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ kubectl create -f .<br/>configmap/webserver-test-config created<br/>deployment.apps/webserver created<br/>persistentvolume/webserver-pv created<br/>persistentvolumeclaim/werbserver-pvc created<br/>secret/webserver-secret created<br/>service/webserver-svc created</strong></pre>
<ol start="4">
<li>Now we will review all of the resources created. We have not defined a namespace; therefore, the <kbd>default</kbd> namespace will be used (we omitted it in our commands because it is our default namespace). We will use <kbd>kubectl get all</kbd> to list all of the resources available in the default namespace:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ kubectl get all<br/>NAME                            READY   STATUS    RESTARTS   AGE<br/>pod/webserver-d7fbbf4b7-rhvvn   1/1     Running   0          31s<br/>NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE<br/>service/kubernetes      ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        107m<br/>service/webserver-svc   NodePort    10.97.146.192   &lt;none&gt;        80:30080/TCP   31s<br/>NAME                        READY   UP-TO-DATE   AVAILABLE   AGE<br/>deployment.apps/webserver   1/1     1            1           31s<br/>NAME                                  DESIRED   CURRENT   READY   AGE<br/>replicaset.apps/webserver-d7fbbf4b7   1         1         1       31s</strong></pre>
<p style="padding-left: 60px">However, not all of the resources are listed. The <kbd>PersistentVolume</kbd> and <kbd>PersistentVolumeClaim</kbd> resources are not shown. Therefore, we will ask the Kubernetes API about these resources using <kbd>kubectl get pv</kbd> (<kbd>PersisteVolumes</kbd>) and <kbd>kubectl get pvs</kbd> (<kbd>PersistenVolumeClaims</kbd>):</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ kubectl get pv<br/>NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE<br/>webserver-pv   500Mi      RWO            Retain           Bound    default/werbserver-pvc   manual                  6m13s<br/><br/>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ kubectl get pvc<br/>NAME             STATUS   VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br/>werbserver-pvc   Bound    webserver-pv   500Mi      RWO            manual         6m15s</strong></pre>
<ol start="5">
<li>Let's send some requests to our web server. You can see, in <kbd>kubectl get all</kbd> output, that <kbd>webserver-svc</kbd> is published using <kbd>NodePort</kbd> on port <kbd>30080</kbd>, associating the host's port <kbd>30080</kbd> with the service's port <kbd>80</kbd>. As mentioned earlier, all hosts will publish port <kbd>30080</kbd>; therefore, we can use <kbd>curl</kbd> on the current host (<kbd>kubernetes-node1</kbd>) and port <kbd>30080</kbd> to try to reach our web server's pods:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ curl 0.0.0.0:30080</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>&lt;title&gt;DEFAULT_TITLE&lt;/title&gt;</strong><br/><strong>&lt;style&gt;</strong><br/><strong>    body {</strong><br/><strong>        width: 35em;</strong><br/><strong>        margin: 0 auto;</strong><br/><strong>        font-family: Tahoma, Verdana, Arial, sans-serif;</strong><br/><strong>    }</strong><br/><strong>&lt;/style&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>&lt;h1&gt;DEFAULT_BODY&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<ol start="6">
<li>We have used ;a <kbd>ConfigMap</kbd> resource to specify an NGINX configuration file, <kbd>webserver.configmap.yaml</kbd>:</li>
</ol>
<div><pre style="padding-left: 60px">apiVersion: v1<br/>kind: ConfigMap<br/>metadata:<br/>  creationTimestamp: null<br/>  name: webserver-test-config<br/>data:<br/>  default.conf: |+<br/>        server {<br/>            listen       80;<br/>            server_name  test;<br/>            location / {<br/>                root   /wwwroot;<br/>                index  index.html index.htm;<br/>            }<br/>            error_page   500 502 503 504  /50x.html;<br/>            location = /50x.html {<br/>                root   /usr/share/nginx/html;<br/>            }<br/>        }</pre></div>
<p style="padding-left: 60px" class="mce-root">This configuration is included inside our deployment file, <kbd>webserver.deployment.yaml</kbd> . Here is the piece of code where it is defined:</p>
<pre style="padding-left: 60px">...<br/>        volumeMounts:<br/>        - name: config-volume<br/>          mountPath: /etc/nginx/conf.d/<br/>...<br/>      volumes:<br/>      - name: config-volume<br/>        configMap:<br/>          name: webserver-test-config<br/>...</pre>
<p style="padding-left: 60px">The first piece declares where this configuration file will be mounted, while the second part links the defined resource: <kbd>webserver-test-config</kbd>. Therefore, the data defined inside the <kbd>ConfigMap</kbd> resource will be integrated inside the web server's pod as <kbd>/etc/nginx/conf.d/default.conf</kbd> (take a look at the data block).</p>
<div><ol start="7">
<li>As mentioned earlier, we also have a <kbd>Secret</kbd> resource (<kbd>webserver.secret.yaml</kbd>):</li>
</ol>
</div>
<pre style="padding-left: 60px">apiVersion: v1<br/>data:<br/>  PAGEBODY: SGVsbG9fV29ybGRfZnJvbV9TZWNyZXQ=<br/>  PAGETITLE: RG9ja2VyX0NlcnRpZmllZF9EQ0FfRXhhbV9HdWlkZQ==<br/>kind: Secret<br/>metadata:<br/>  creationTimestamp: null<br/>  name: webserver-secret</pre>
<p style="padding-left: 60px">We can verify, here, that keys are visible while values are not (encoded using the Base64 algorithm).</p>
<div><p>We can also create this secret using the imperative format with the <kbd>kubectl</kbd> command line:</p>
<kbd>kubectl create secret generic webserver-secret \<br/>--from-literal=PAGETITLE="Docker_Certified_DCA_Exam_Guide" \<br/>--from-literal=PAGEBODY="Hello_World_from_Secret"</kbd></div>
<p style="padding-left: 60px">We also used this secret resource in our deployment:</p>
<pre style="padding-left: 60px">...<br/>        env:<br/>...<br/>        - name: PAGETITLE<br/>          valueFrom:<br/>            secretKeyRef:<br/>              name: webserver-secret<br/>              key: PAGETITLE<br/>        - name: PAGEBODY<br/>          valueFrom:<br/>            secretKeyRef:<br/>              name: webserver-secret<br/>              key: PAGEBODY<br/>...</pre>
<p style="padding-left: 60px">In this case, the <kbd>PAGETITLE</kbd> and <kbd>PAGEBODY</kbd> keys will be integrated as environment variables inside the web server's pod. These values will be used in our lab as values for the <kbd>index.html</kbd> page. <kbd>DEFAULT_BODY</kbd> and <kbd>DEFAULT_TITLE</kbd> will be changed from the pod's container process.</p>
<ol start="8">
<li>This lab has another volume definition. In fact, we have <kbd>PersistentVolumeclaim</kbd> included as a volume in our deployment's definition:</li>
</ol>
<pre style="padding-left: 60px">...<br/>        volumeMounts:<br/>...<br/>        - mountPath: /wwwroot<br/>          name: data-volume<br/>...<br/>      - name: data-volume<br/>        persistentVolumeClaim:<br/>          claimName: werbserver-pvc<br/>...</pre>
<p style="padding-left: 60px">The volume claim is used here and is mounted in <kbd>/wwwroot</kbd> inside the web server's pod. <kbd>PersistentVolume</kbd> and <kbd>PersistentVolumeClaim</kbd> are defined in <kbd>webserver.persistevolume.yaml</kbd> and <kbd>webserver.persistevolumeclaim.yaml</kbd>, respectively.</p>
<ol start="9">
<li>Finally, we have an <kbd>emptyDir</kbd> volume definition. This will be used to bypass the container's filesystem and save the NGINX logs:</li>
</ol>
<pre style="padding-left: 60px">...<br/>        volumeMounts:<br/>...<br/>        - mountPath: /var/log/nginx<br/>          name: empty-volume<br/>          readOnly: false<br/>...<br/>      volumes:<br/>...<br/>      - name: empty-volume<br/>        emptyDir: {}<br/>...</pre>
<ol start="10">
<li>The first pod execution will create a default <kbd>/wwwroot/index.html</kbd> file inside it. This is mounted inside the <kbd>kubernetes-node2</kbd> node's filesystem, inside the <kbd>/mount</kbd> directory. Therefore, after this first execution, we find that <kbd>/mnt/index.html</kbd> was created (you can verify this by following <em>step 1</em> again). The file was published, and we get it when we execute <kbd>curl 0.0.0.0:30080</kbd> in <em>step 5</em>.</li>
</ol>
<ol start="11">
<li>Our application is quite simple, but it is prepared to modify the content of the <kbd>index.html</kbd> file. As mentioned earlier, the default title and body will be changed with the values defined in the secret resource. This will happen after the creation of the container if the <kbd>index.html</kbd> file already exists. Now that it has been created, as verified in <em>step 10</em>, we can delete the web server's pod. Kubernetes will create a new one, and, therefore, the application will change its content. We use <kbd>kubectl delete pod</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ kubectl delete pod/webserver-d7fbbf4b7-rhvvn</strong><br/><strong>pod "webserver-d7fbbf4b7-rhvvn" deleted</strong></pre>
<p style="padding-left: 60px">After a few seconds, a new pod is created (we are using a deployment and Kubernetes takes care of the application's component resilience):</p>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ kubectl get pods</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>webserver-d7fbbf4b7-sz6dx 1/1 Running 0 17s</strong></pre>
<ol start="12">
<li>Let's again verify the content of our web server using <kbd>curl</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>vagrant@kubernetes-node1:~/Docker-Certified-Associate-DCA-Exam-Guide/chapter9/nginx-lab/yaml$ curl 0.0.0.0:30080</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>&lt;title&gt;Docker_Certified_DCA_Exam_Guide&lt;/title&gt;</strong><br/><strong>&lt;style&gt;</strong><br/><strong> body {</strong><br/><strong> width: 35em;</strong><br/><strong> margin: 0 auto;</strong><br/><strong> font-family: Tahoma, Verdana, Arial, sans-serif;</strong><br/><strong> }</strong><br/><strong>&lt;/style&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>&lt;h1&gt;Hello_World_from_Secret&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<p style="padding-left: 60px">Now the content has changed inside the defined <kbd>PersistentVolume</kbd> resource.</p>
<ol start="13">
<li>We can also verify the <kbd>/mnt/index.html</kbd> content in <kbd>kubernetes-node2</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ vagrant ssh kubernetes-node2</strong><br/><br/><strong>vagrant@kubernetes-node2:~$ cat /mnt/index.html</strong><br/><strong>&lt;!DOCTYPE html&gt;</strong><br/><strong>&lt;html&gt;</strong><br/><strong>&lt;head&gt;</strong><br/><strong>&lt;title&gt;Docker_Certified_DCA_Exam_Guide&lt;/title&gt;</strong><br/><strong>&lt;style&gt;</strong><br/><strong> body {</strong><br/><strong> width: 35em;</strong><br/><strong> margin: 0 auto;</strong><br/><strong> font-family: Tahoma, Verdana, Arial, sans-serif;</strong><br/><strong> }</strong><br/><strong>&lt;/style&gt;</strong><br/><strong>&lt;/head&gt;</strong><br/><strong>&lt;body&gt;</strong><br/><strong>&lt;h1&gt;Hello_World_from_Secret&lt;/h1&gt;</strong><br/><strong>&lt;/body&gt;</strong><br/><strong>&lt;/html&gt;</strong></pre>
<p>In this lab, we have used four different volume resources, with different definitions and features. These labs were very simple, showing you how to deploy a small application on Kubernetes. All of the labs can be easily removed by destroying all the Vagrant nodes using <kbd>vagrant destroy</kbd> from the <kbd>environments/kubernetes</kbd> directory.</p>
<p>We highly recommend going further with Kubernetes because it will become a part of the exam in the near future. However, right now, Kubernetes is outside the scope of the Docker Certified Associate exam.</p>
<h1 id="uuid-733e7c3b-e9e2-4791-8213-3ca9ab53aefd" class="mce-root">Summary</h1>
<p class="mce-root">In this chapter, we quickly reviewed some of Kubernetes' main features. We compared most of the must-have orchestration features with those discussed in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>. Both provide workload deployment and the management of a distributed pool of nodes. They monitor an application's health and allow us to upgrade components without service interruption. They also provide networking and publishing solutions.</p>
<p>Pods provide higher container density, allowing us to run more than one container at once. This concept is closer to applications running on virtual machines and makes container adoption easier. Services are logical groups of pods and we can use them to expose applications. Service discovery and load balancing work out of the box dynamically.</p>
<p>Cluster-wide networking requires additional plugins in Kubernetes, and we also learned that a flat network can facilitate routing on different hosts and make some things easier; however, it does not provide security by default. Kubernetes provides enough mechanisms to ensure network security using network policies and single endpoints for multiple services with ingress. Publishing applications is even easier with ingress. It adds internal load balancing features dynamically with rules managed using ingress resources. This allows us to save up node ports and public IP addresses within the environment.</p>
<p>At the end of the chapter, we reviewed a number of points about Kubernetes security. We discussed how RBAC provides different environments to users running their workloads on the same cluster. We also talked about some features provided by Kubernetes to ensure default security on resources.</p>
<p>There is much more to learn about Kubernetes, but we will have to end this chapter here. We highly recommend that you follow the Kubernetes documentation and the release notes on the project's website (<a href="https://kubernetes.io/">https://kubernetes.io/</a>).</p>
<p>In the next chapter, we'll look at the differences and similarities between Swarm and Kubernetes, side by side.</p>
<h1 id="uuid-ca29e35d-b2d2-4c52-8526-bd78ed570dfa" class="mce-root">Questions</h1>
<ol>
<li>Which of these features is not included in Kubernetes by default?</li>
</ol>
<p style="padding-left: 90px">a) An internal key-value store.<br/>
b) Network communication between containers distributed on different Docker hosts.<br/>
c) Controllers for deploying workload updates without service interruptions.<br/>
d) None of these features are included.</p>
<ol start="2">
<li>Which of these statements is true about pods?</li>
</ol>
<p style="padding-left: 90px">a) Pods always run in pairs to provide an application with high availability.<br/>
b) Pods are the minimum unit of deployment on Kubernetes.<br/>
c) We can deploy more than one container per pod.<br/>
d) We need to choose which containers in a pod should be replicated when pods are scaled.</p>
<ol start="3">
<li>Which of these statements is true about pods?</li>
</ol>
<p style="padding-left: 90px">a) All pod containers run using a unique network namespace.<br/>
b) All containers within a pod can share volumes.<br/>
c) All pods running on Docker Engine are accessible from the host using their IP addresses.<br/>
d) All of these statements are true.</p>
<ol start="4">
<li>Kubernetes provides different controllers to deploy application workloads. Which of these statements is true?</li>
</ol>
<p style="padding-left: 90px">a) <kbd>DaemonSet</kbd> will run one replica on each cluster node.<br/>
b) <kbd>ReplicaSet</kbd> will allow us to scale application pods up or down.<br/>
c) Deployments are higher-level resources. They manage <kbd>ReplicaSet</kbd>.<br/>
d) All of these statements are true.</p>
<ol start="5">
<li>How can we expose services to users in Kubernetes? (Which of these statements is false?)</li>
</ol>
<p style="padding-left: 90px">a) ClusterIP services provide a virtual IP accessible to users.<br/>
b) NodePort services listen on all nodes and route traffic using the provided ClusterIP to reach all service backends.<br/>
c) LoadBalancer creates simple load balancers on cloud providers to load balance requests to service backends.<br/>
d) Ingress controllers help us to use single endpoints (one per ingress controller) to load balance requests to non-published services.</p>
<h1 id="uuid-80353813-1854-437e-af34-c88198dca81c">Further reading</h1>
<p>You can refer to the following links for more information on topics covered in this chapter:</p>
<ul>
<li>Kubernetes documentation: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
<li>Kubernetes concepts: <a href="https://kubernetes.io/docs/concepts/">https://kubernetes.io/docs/concepts/</a></li>
<li>Kubernetes learning tasks: <a href="https://kubernetes.io/docs/tasks/">https://kubernetes.io/docs/tasks/</a></li>
<li>Kubernetes on Docker Enterprise: <a href="https://docs.docker.com/ee/ucp/kubernetes/kube-resources">https://docs.docker.com/ee/ucp/kubernetes/kube-resources</a></li>
<li><em>Getting Started with Kubernetes</em>: <a href="https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition">https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition</a></li>
</ul>


            

            
        
    </body></html>