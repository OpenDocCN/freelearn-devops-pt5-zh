<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Benchmarking</h1></div></div></div><p class="calibre8">In optimizing our Docker applications, it is important to validate the parameters that we tuned. Benchmarking is an experimental way of identifying if the elements we modified in our Docker containers performed as expected. Our application will have a wide area of options to be optimized. The Docker hosts running them have their own set of parameters such as memory, networking, CPU, and storage as well. Depending on the nature of our application, one or more of these parameters can become a bottleneck. Having a series of tests to validate each component with benchmarks is important for guiding our optimization strategy.</p><p class="calibre8">Additionally, by creating proper performance tests, we can also identify the limits of the current configuration of our Docker-based application. With this information, we can start exploring infrastructure parameters such as scaling out our application by deploying them on more Docker hosts. We can also use this information to scale up the same application by moving our workload to a Docker host with higher storage, memory, or CPU. And when we have hybrid cloud deployments, we can use these measurements to identify which cloud provider gives our application its optimum performance.</p><p class="calibre8">Measuring how our application responds to these benchmarks is important when planning the capacity needed for our Docker infrastructure. By creating a test workload simulating peak and normal conditions, we can predict how our application will perform once it is released to production.</p><p class="calibre8">In this chapter, we will cover the following topics to benchmark a simple web application deployed in our Docker infrastructure:</p><div><ul class="itemizedlist"><li class="listitem">Setting up Apache JMeter for benchmarking</li><li class="listitem">Creating and designing a benchmark workload</li><li class="listitem">Analyzing application performance</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec29" class="calibre1"/>Setting up Apache JMeter</h1></div></div></div><p class="calibre8">Apache JMeter <a id="id197" class="calibre1"/>is a popular application used to test the performance of web servers. Besides load testing web servers, the open source project grew to support testing other network protocols such as LDAP, FTP, and even raw TCP packets. It is highly configurable, and powerful enough to design complex workloads of different usage patterns. This feature can be used to simulate thousands of users suddenly visiting our web application thus inducing a spike in the load.</p><p class="calibre8">Another feature expected in any load-testing software is its data capture and analysis functions. JMeter has such a wide variety of data recording, plotting, and analysis features that we can explore the results of our benchmarks right away. Finally, it has a wide variety of plugins that may already have the load pattern, analysis, or network connection that we plan to use.</p><div><h3 class="title2"><a id="note28" class="calibre1"/>Note</h3><p class="calibre8">More information <a id="id198" class="calibre1"/>about the features and how to use Apache JMeter can be found on its website at <a class="calibre1" href="http://jmeter.apache.org">http://jmeter.apache.org</a>.</p></div><p class="calibre8">In this section, we will deploy an example application to benchmark, and prepare our workstation to run our first JMeter-based benchmark.</p></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec25" class="calibre1"/>Deploying a sample application</h2></div></div></div><p class="calibre8">We can also <a id="id199" class="calibre1"/>bring our own web application we want to benchmark if we please. But for the rest of this chapter, we will benchmark the following application described in this section. The application is a simple Ruby web application deployed using Unicorn, a popular Ruby application server. It receives traffic via a Unix socket from Nginx. This setup is very typical for most Ruby applications found in the wild.</p><p class="calibre8">In this section, we will deploy this Ruby application in a Docker host called <code class="literal">webapp</code>. We will use separate Docker hosts for the application, benchmark tools, and monitoring. This separation is important so that the benchmark and monitoring instrumentation we run doesn't affect the benchmark results.</p><p class="calibre8">The next few steps show us how to build and deploy our simple Ruby web application stack:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, create the Ruby application by creating the following Rack <code class="literal">config.ru</code> file:<div><pre class="programlisting">app =  proc do |env|

  <strong class="calibre2">Math.sqrt rand</strong>
  [200, {}, %w(hello world)]
end
run app</pre></div></li><li class="listitem" value="2">Next, we package the application as a Docker container with the following <code class="literal">Dockerfile</code>:<div><pre class="programlisting">FROM ruby:2.2.3

RUN gem install unicorn
WORKDIR /app

COPY . /app

VOLUME /var/run/unicorn

CMD unicorn -l /var/run/unicorn/unicorn.sock</pre></div></li><li class="listitem" value="3">Now we <a id="id200" class="calibre1"/>will create the Nginx configuration file <code class="literal">nginx.conf</code>. It will forward requests to our Unicorn application server through the Unix socket that we created in the previous step. In logging the request, we will record <code class="literal">$remote_addr</code> and <code class="literal">$response_time</code>. We will pay particular attention to these metrics later when we analyze our benchmark results:<div><pre class="programlisting">events { }

http {
  log_format unicorn ''$remote_addr [$time_local]''
             '' ""$request"" $status''
             '' $body_bytes_sent $request_time'';
  access_log /var/log/nginx/access.log unicorn; 

  upstream app_server {
    server unix:/var/run/unicorn/unicorn.sock;
  }
  server {
    location / {
      proxy_pass http://app_server;
    }
  }
}</pre></div></li><li class="listitem" value="4">The preceding Nginx configuration will then be packaged as a Docker container with the following <code class="literal">Dockerfile</code>:<div><pre class="programlisting">FROM nginx:1.9.4

COPY nginx.conf /etc/nginx/nginx.conf</pre></div></li><li class="listitem" value="5">The last component will be a <code class="literal">docker-compose.yml</code> file to tie the two Docker containers together for deployment:<div><pre class="programlisting">web:
  log_opt:
    syslog-tag: nginx
  build: ./nginx
  ports:
    - 80:80
  volumes_from:
    - app
app:
  build: ./unicorn</pre></div></li></ol><div></div><p class="calibre8">In the end, we will <a id="id201" class="calibre1"/>have the files shown in the following screenshot in our code base:</p><div><img src="img/00022.jpeg" alt="Deploying a sample application" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">After preparing our Dockerized web application, let us now deploy it to our Docker host by typing the following command:</p><div><pre class="programlisting">
<strong class="calibre2">webapp$ docker-compose up -d</strong>
</pre></div><div><h3 class="title2"><a id="note29" class="calibre1"/>Note</h3><p class="calibre8">Docker Compose is a tool for creating multi-container applications. It has a schema defined in YML to describe how we want our Docker containers to run and link to each other.</p><p class="calibre8">Docker Compose supports a curl | bash type of installation. To quickly install it on our Docker host, type the following command:</p><div><pre class="programlisting">
<strong class="calibre2">dockerhost$ curl -L https://github.com/docker/compose/releases/download/1.5.2/docker-compose-`uname -s`-`uname -m` \&gt; /usr/local/bin/docker-compose</strong>
</pre></div><p class="calibre8">We only covered Docker Compose in passing in this chapter. However, we can get more information <a id="id202" class="calibre1"/>about Docker Compose on the documentation website found at <a class="calibre1" href="http://docs.docker.com/compose">http://docs.docker.com/compose</a>.</p></div><p class="calibre8">Finally, let us <a id="id203" class="calibre1"/>conduct a preliminary test to determine if our application works properly:</p><div><pre class="programlisting">
<strong class="calibre2">$ curl http://webapp.dev</strong>
<strong class="calibre2">hello world</strong>
</pre></div><p class="calibre8">Now we are done preparing the application that we want to benchmark. In the next section, we will prepare our workstation to perform the benchmarks by installing Apache JMeter.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec26" class="calibre1"/>Installing JMeter</h2></div></div></div><p class="calibre8">For the rest <a id="id204" class="calibre1"/>of this chapter, we will use Apache JMeter version 2.13 to perform our benchmarks. In this section, we will download and install it in our workstation. Follow the next few steps to set up JMeter properly:</p><div><ol class="orderedlist"><li class="listitem" value="1">To begin, go to <a id="id205" class="calibre1"/>JMeter's download web page at <a class="calibre1" href="http://jmeter.apache.org/download_jmeter.cgi">http://jmeter.apache.org/download_jmeter.cgi</a>.</li><li class="listitem" value="2">Select the link for <strong class="calibre2">apache-jmeter-2.13.tgz</strong> to begin downloading the binary.</li><li class="listitem" value="3">When the download finishes, extract the tarball by typing the following command:<div><pre class="programlisting">
<strong class="calibre2">$ tar -xzf apache-jmeter-2.13.tgz</strong>
</pre></div></li><li class="listitem" value="4">Next, we will add the <code class="literal">bin/</code> directory to our <code class="literal">$PATH</code> so that JMeter can be easily launched from the command line. To do this, we will type the following command in our terminal:<div><pre class="programlisting">
<strong class="calibre2">$ export PATH=$PATH:`pwd`/apache-jmeter-2.13/bin</strong>
</pre></div></li><li class="listitem" value="5">Finally, launch JMeter by typing the following command:<div><pre class="programlisting">
<strong class="calibre2">$ jmeter</strong>
</pre></div></li></ol><div></div><p class="calibre8">We will now see the JMeter UI just like the following screenshot. Now we are finally ready to write the <a id="id206" class="calibre1"/>benchmark for our application!:</p><div><img src="img/00023.jpeg" alt="Installing JMeter" class="calibre10"/></div><p class="calibre11"> </p><div><h3 class="title2"><a id="note31" class="calibre1"/>Note</h3><p class="calibre8">Note that Apache JMeter is a Java application. According to the JMeter website, it requires at <a id="id207" class="calibre1"/>least Java 1.6 to work. Make sure you have a <strong class="calibre2">Java Runtime Environment</strong> (<strong class="calibre2">JRE</strong>) properly set up before installing JMeter.</p><p class="calibre8">If we were in a Mac OSX environment, we could use Homebrew and just type the following command:</p><div><pre class="programlisting">
<strong class="calibre2">$ brew install jmeter</strong>
</pre></div><p class="calibre8">For other platforms, the <a id="id208" class="calibre1"/>instructions described earlier should be sufficient to get started. More information on how to install JMeter can be found at <a class="calibre1" href="http://jmeter.apache.org/usermanual/get-started.html">http://jmeter.apache.org/usermanual/get-started.html</a>.</p></div></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec30" class="calibre1"/>Building a benchmark workload</h1></div></div></div><p class="calibre8">Writing benchmarks <a id="id209" class="calibre1"/>for an application is an open-ended area to explore. Apache JMeter can be overwhelming at first. It has several options to tune in order to write our benchmarks. To begin, we can use the "story" of our application as a start. The following are some of the questions we can ask ourselves:</p><div><ul class="itemizedlist"><li class="listitem">What does our application do?</li><li class="listitem">What is the persona of our users?</li><li class="listitem">How do they interact with our application?</li></ul></div><p class="calibre8">Starting with these questions, we can then translate them into actual requests to our application.</p><p class="calibre8">In the sample application that we wrote in the earlier section, we have a web application that displays <code class="literal">Hello World</code> to our users. In web applications, we are typically interested with the throughput and response time. Throughput refers to the number of users that can receive <code class="literal">Hello World</code> at a <a id="id210" class="calibre1"/>time. Response time describes the time lag before the user receives the <code class="literal">Hello World</code> message from the moment they requested it.</p><p class="calibre8">In this section, we will create a preliminary benchmark in Apache JMeter. Then we will begin analyzing our initial results with JMeter's analysis tools and the monitoring stack that we deployed in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>. After that, we will iterate on the benchmarks we developed, and tune it. This way, we know that we are benchmarking our application properly.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec27" class="calibre1"/>Creating a test plan in JMeter</h2></div></div></div><p class="calibre8">A series of <a id="id211" class="calibre1"/>benchmarks in Apache JMeter is described in a test plan. A test plan describes a series of steps that JMeter will execute like <a id="id212" class="calibre1"/>performing requests to a web application. Each step in a test plan is called an element. These elements themselves can have one or more elements as well. In the end, our test plan will look like a tree—an hierarchy of elements to describe the benchmark we want for our application.</p><p class="calibre8">To add an element into our test plan, we simply right-click on the parent element that we want, and then select <strong class="calibre2">Add</strong>. This opens a context menu of elements that can be added to the selected parent element. In the following screenshot, we add a <strong class="calibre2">Thread Group</strong> element to the main element, <strong class="calibre2">Test Plan</strong>:</p><div><img src="img/00024.jpeg" alt="Creating a test plan in JMeter" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The next few steps show the way to create a test plan conducting the benchmark that we want:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, let us rename the <strong class="calibre2">Test Plan</strong> to something more appropriate. Click on the <strong class="calibre2">Test Plan</strong> element. This will update the main JMeter window on the right. In the form field labeled <strong class="calibre2">Name:</strong>, set the value to <strong class="calibre2">Unicorn Capacity</strong>.</li><li class="listitem" value="2">Under the <strong class="calibre2">Unicorn Capacity</strong> test plan, create a thread group. Name this <strong class="calibre2">Application Users</strong>. We will configure this thread group to send 10,000 requests to our application from a single thread in the beginning. Use the following parameters for filling out the form to achieve this setting:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre2">Number of Threads</strong>: 1</li><li class="listitem"><strong class="calibre2">Ramp-up Period</strong>: 0 seconds</li><li class="listitem"><strong class="calibre2">Loop Count</strong>: 120,000 times<div><h3 class="title2"><a id="tip05" class="calibre1"/>Tip</h3><p class="calibre8">When we <a id="id213" class="calibre1"/>start developing our <a id="id214" class="calibre1"/>test plans, having a low loop count is useful. Instead of 120,000 loop counts, we can begin with 10,000 or even just 10 instead. Our benchmarks are shorter, but we get immediate feedback when developing it such as when we proceed to the next step. When we finish the whole test plan, we can always revert and tune it later to generate more requests.</p></div></li></ul></div></li><li class="listitem" value="3">Next, under the <strong class="calibre2">Application Users</strong> thread group, we create the actual request by adding <strong class="calibre2">Sampler, HTTP Request</strong>. This is the configuration where we set the details of how we make a request to our web application:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre2">Name</strong>: Go to <code class="literal">http://webapp/</code></li><li class="listitem"><strong class="calibre2">Server Name</strong>: <code class="literal">webapp</code></li></ul></div></li><li class="listitem" value="4">Finally, we configure how to save the test results by adding a listener under the <strong class="calibre2">Unicorn Capacity</strong> test plan. For this, we will add a <strong class="calibre2">Simple Data Writer,</strong> and name it <strong class="calibre2">Save Result</strong>. We set the <strong class="calibre2">Filename</strong> field to <code class="literal">result.jtl</code> to save our benchmark results in the said file. We will refer to this file later when we analyze the result of the benchmark.</li></ol><div></div><p class="calibre8">Now we have a basic benchmark workload that generates 120,000 HTTP requests to <code class="literal">http://webapp/</code>. Then the test plan saves the result of each request in a file called <code class="literal">result.jtl</code>. The following is a screenshot of JMeter after the last step in creating the test plan:</p><div><img src="img/00025.jpeg" alt="Creating a test plan in JMeter" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Finally, it is time to run our benchmark. Go to the <strong class="calibre2">Run</strong> menu, then select <strong class="calibre2">Start</strong> to begin executing the <a id="id215" class="calibre1"/>test plan. While the benchmark is running, the <strong class="calibre2">Start</strong> button is grayed-out and disabled. When the execution finishes, it will be enabled again.</p><p class="calibre8">After running the <a id="id216" class="calibre1"/>benchmark, we will analyze the results by looking at the <code class="literal">result.jtl</code> file using JMeter's analysis tools in the next section.</p><div><h3 class="title2"><a id="note33" class="calibre1"/>Note</h3><p class="calibre8">There are various types of elements that can be placed in a JMeter test plan. Besides the three elements we used previously to create a basic benchmark for our application, there <a id="id217" class="calibre1"/>are several others that regulate requests, perform other network requests, and analyze data.</p><p class="calibre8">A comprehensive list <a id="id218" class="calibre1"/>of test plan elements and their description can be found on the JMeter page at <a class="calibre1" href="http://jmeter.apache.org/usermanual/component_reference.html">http://jmeter.apache.org/usermanual/component_reference.html</a>.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec31" class="calibre1"/>Analyzing benchmark results</h1></div></div></div><p class="calibre8">In this section, we <a id="id219" class="calibre1"/>will analyze the benchmark results, and identify how the 120,000 requests affected our application. In creating web application benchmarks, there are typically two things we are usually interested in:</p><div><ul class="itemizedlist"><li class="listitem">How many requests can our application handle at a time?</li><li class="listitem">For how long is each request being processed by our application?</li></ul></div><p class="calibre8">These two low-level web performance metrics can easily translate to the business implications of our application. For example, how many customers are using our application? Another one is, how are they perceiving the responsiveness of our application from a user experience perspective? We can correlate secondary metrics in our application such as <a id="id220" class="calibre1"/>CPU, memory, and network to determine our system capacity.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec28" class="calibre1"/>Viewing the results of JMeter runs</h2></div></div></div><p class="calibre8">Several listener <a id="id221" class="calibre1"/>elements of JMeter have features that render graphs. Enabling this when running the benchmark is useful when developing the test plan. But the time taken by the UI to render the results in real time, in addition to the actual benchmark requests, affects the performance of the test. Hence, it is better for us to separate the execution and analysis components of our benchmark. In this section, we will create a new test plan, and look at a few JMeter listener elements to analyze the data we acquired in <code class="literal">result.jtl</code>.</p><p class="calibre8">To begin our analysis, we first create a new test plan, and name this <strong class="calibre2">Analyze Results</strong>. We will add various listener elements under this test plan parent element. After this, follow the next few steps to add various JMeter listeners that can be used to analyze our benchmark result.</p><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec04" class="calibre1"/>Calculating throughput</h3></div></div></div><p class="calibre8">For our first <a id="id222" class="calibre1"/>analysis, we will use the <strong class="calibre2">Summary Report</strong> listener. This listener will show the throughput of our application. A measurement of throughput will show the number of transactions our application can handle per second.</p><p class="calibre8">To display the throughput, perform the following steps:</p><p class="calibre8">After loading the listener, fill out the <strong class="calibre2">Filename</strong> field by selecting the <code class="literal">result.jtl</code> file that we generated when we ran our benchmark. For the run we did earlier, the following screenshot shows that the 120,000 HTTP requests were sent to <code class="literal">http://webapp/</code> at a throughput of 746.7 requests per second:</p><div><img src="img/00026.jpeg" alt="Calculating throughput" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can also look at how throughput evolved over the course of our benchmark with the <strong class="calibre2">Graph Results</strong> listener. Create this listener under the <strong class="calibre2">Analyze Results</strong> test plan element and name it <strong class="calibre2">Throughput over time</strong>. Make sure that only the <strong class="calibre2">Throughput</strong> checkbox is marked (feel free to look at the other data points later though). After creating the listener, load our <code class="literal">result.jtl</code> test result again. The following screenshot shows how the throughput <a id="id223" class="calibre1"/>evolved over time:</p><div><img src="img/00027.jpeg" alt="Calculating throughput" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As we can see in the preceding screenshot, the throughput started slow while JMeter tries to warm up its single-thread pool of requests. But after our benchmark continues to run, the throughput level settles at a stable level. By having a large number of loop counts earlier in our thread group, we were able to minimize the effect of the earlier ramp-up period.</p><p class="calibre8">This way, the throughput displayed in the <strong class="calibre2">Summary Report</strong> earlier is more or less a consistent result. Take note that the <strong class="calibre2">Graph Results</strong> listener wraps around its data points after several <a id="id224" class="calibre1"/>samples.</p><div><h3 class="title2"><a id="tip06" class="calibre1"/>Tip</h3><p class="calibre8">Remember that in benchmarking, the more samples we get, the more precise our observations can be!</p></div></div><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec05" class="calibre1"/>Plotting response time</h3></div></div></div><p class="calibre8">Another metric we are interested in when we benchmark our application is the <strong class="calibre2">response time</strong>. The response time shows the duration for which JMeter has to wait before receiving the web <a id="id225" class="calibre1"/>page response from our application. In terms of real users, we can look at this as the time our users typed our web application's URL to the time everything got displayed in their web browser (it may not represent the real whole picture if our application renders some slow JavaScript, but for the application we made earlier, this analogy should suffice).</p><p class="calibre8">To view the response time of our application, we will use the <strong class="calibre2">Response Time Graph</strong> listener. As an initial setting, we can set the interval to 500 milliseconds. This will average some of the response times along 500 milliseconds in <code class="literal">result.jtl</code>. In the following image, you can see that our application's response time is mostly at around 1 millisecond:</p><div><img src="img/00028.jpeg" alt="Plotting response time" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">If we want to display the response time in finer detail, we can decrease the interval to as low as 1 millisecond. Take note that this will take more time to display as the JMeter UI tries to plot more <a id="id226" class="calibre1"/>points in the application. Sometimes, when there are too many samples, JMeter may crash, because our workstation doesn't have enough memory to display the entire graph. In case of large benchmarks, we would be better off observing the results with our monitoring system. We will look at this data in the next section.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec29" class="calibre1"/>Observing performance in Graphite and Kibana</h2></div></div></div><p class="calibre8">There might be a case <a id="id227" class="calibre1"/>when our workstation is so old that Java is not <a id="id228" class="calibre1"/>able to handle displaying 120,000 data points in its JMeter UI. To solve this, we can reduce the amount of data we have by either generating less requests in our benchmark or averaging out some of the data like we did earlier, when graphing response time. However, sometimes we want to see the full resolution of our data. This full view is useful when we want to inspect the finer details of how our application behaves. Fortunately, we already have a monitoring system in place for our Docker infrastructure that we built in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>.</p><div><h3 class="title2"><a id="note34" class="calibre1"/>Note</h3><p class="calibre8">In this section, our monitoring and logging systems are deployed in a Docker host called <code class="literal">monitoring</code>. Our Docker host <code class="literal">webapp</code> that runs our application containers will have Collected and Rsyslog send events to the Docker host <code class="literal">monitoring</code>.</p></div><p class="calibre8">Remember the Nginx configuration mentioned when describing our benchmarks? The access log generated from the standard of the Nginx container is captured by Docker. If we use the same setup of our Docker daemon in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>, these log events are captured by the local Rsyslog service. These Syslog entries will then be forwarded to the Logstash Syslog collector, and stored to Elasticsearch. We can then use the visualize feature of Kibana to look at the throughput of our application. The following analysis was made by counting the number of access log entries that Elasticsearch received per second:</p><div><img src="img/00029.jpeg" alt="Observing performance in Graphite and Kibana" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can also plot our application's response time during the course of the benchmark in Kibana. To do this, we first need to reconfigure our Logstash configuration to parse the data being received from the access log, and extract out the response time as a metric using filters. To do <a id="id229" class="calibre1"/>this, update <code class="literal">logstash.conf</code> from <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>, to add the <code class="literal">grok {}</code> filter as follows:</p><div><pre class="programlisting">input {
  syslog {
    port =&gt; 1514
    type =&gt; syslog
  }
}

<strong class="calibre2">filter {</strong>
<strong class="calibre2">  if [program] == ""docker/nginx"" {</strong>
<strong class="calibre2">    grok {</strong>
<strong class="calibre2">      patterns_dir =&gt; [""/etc/logstash/patterns""]</strong>
<strong class="calibre2">      match =&gt; {</strong>
<strong class="calibre2">        ""message"" =&gt; ""%{NGINXACCESS}""</strong>
<strong class="calibre2">      }</strong>
<strong class="calibre2">    }</strong>
<strong class="calibre2">  }</strong>
<strong class="calibre2">}</strong>

output {
  elasticsearch {
    host =&gt; ""elasticsearch""
  }
}</pre></div><div><h3 class="title2"><a id="note35" class="calibre1"/>Note</h3><p class="calibre8">Logstash's Filter plugins are used to intermediately process events before they reach our target storage endpoint such as Elasticsearch. It transforms raw data such as lines of text to a <a id="id230" class="calibre1"/>richer data schema in JSON that we can then use later for further analysis. More information about Logstash Filter plugins can be found at <a class="calibre1" href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">https://www.elastic.co/guide/en/logstash/current/filter-plugins.html</a>.</p></div><p class="calibre8">The <code class="literal">NGINXACCESS</code> pattern being referred to in the preceding code is defined externally in what the <code class="literal">grok {}</code> filter calls a <code class="literal">patterns</code> file. Write the following as its content:</p><div><pre class="programlisting">REQUESTPATH \""%{WORD:method} %{URIPATHPARAM} HTTP.*\""
HTTPREQUEST %{REQUESTPATH} %{NUMBER:response_code}
WEBMETRICS %{NUMBER:bytes_sent:int} %{NUMBER:response_time:float}
NGINXSOURCE %{IP:client} \[%{HTTPDATE:requested_at}\]
NGINXACCESS %{NGINXSOURCE} %{HTTPREQUEST} %{WEBMETRICS}</pre></div><p class="calibre8">Finally, rebuild our <code class="literal">hubuser/logstash</code> Docker container from <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>. Don't forget to update the <code class="literal">Dockerfile</code> as follows to add the patterns file to our Docker context:</p><div><pre class="programlisting">FROM logstash:1.5.3

ADD logstash.conf /etc/logstash.conf
<strong class="calibre2">ADD patterns /etc/logstash/patterns/nginx</strong>
EXPOSE 1514/udp
EXPOSE 25826/udp</pre></div><p class="calibre8">Now that we extracted <a id="id231" class="calibre1"/>the response times from the Nginx access logs, we <a id="id232" class="calibre1"/>can plot these data points in a Kibana visualization. The following is a screenshot of Kibana showing the average response time per second of the benchmark we ran earlier:</p><div><img src="img/00030.jpeg" alt="Observing performance in Graphite and Kibana" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Another result that we can explore is the way our Docker host <code class="literal">webapp</code> responds with the load received from our benchmark. First we can check how our web application consumes the CPU of our Docker host. Let's log in to our monitoring system's graphite-web dashboard and plot the metrics <code class="literal">webapp.cpu-0.cpu-*</code> except <code class="literal">cpu-idle</code>. As we can see in the following image, the CPU of our Docker host goes to 100 percent usage the moment we start sending our application a lot of requests:</p><div><img src="img/00031.jpeg" alt="Observing performance in Graphite and Kibana" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can explore other system measurements of our Docker host to see how it is affected by the load of HTTP requests that it gets. The important point is that we use this data and correlate it to see how <a id="id233" class="calibre1"/>our web application behaved.</p><div><h3 class="title2"><a id="note36" class="calibre1"/>Note</h3><p class="calibre8">Apache JMeter <a id="id234" class="calibre1"/>version 2.13 and later include a backend listener that we can use to send JMeter data measurements in real time to external endpoints. By default, it ships with support for the Graphite wire protocol. We can use this feature to send benchmark results to the Graphite monitoring infrastructure that we built in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers</em>. More <a id="id235" class="calibre1"/>information on how to use this feature is available at <a class="calibre1" href="http://jmeter.apache.org/usermanual/realtime-results.html">http://jmeter.apache.org/usermanual/realtime-results.html</a>.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec32" class="calibre1"/>Tuning the benchmark</h1></div></div></div><p class="calibre8">At this point, we <a id="id236" class="calibre1"/>already have a basic workflow of creating a test plan in Apache JMeter and analyzing the preliminary results. From here, there are several parameters we can adjust to achieve our benchmark objectives. In this section, we will iterate on our test plan to identify the limits of our Docker application.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec30" class="calibre1"/>Increasing concurrency</h2></div></div></div><p class="calibre8">The first parameter <a id="id237" class="calibre1"/>that we may want to tune is increasing the <strong class="calibre2">Loop Count</strong> of our test plan. Driving our test plan to generate more requests will allow us to see the effects of the load we induced to our application. This increases the precision of our benchmark experiments, because outlier events such as a slow network connection or hardware failure (unless we are testing that specifically!) affect our tests.</p><p class="calibre8">After having enough data points for our benchmarks, we may realize that the load being generated is not enough against our Docker application. For example, the current throughput we received from our first analysis may not simulate the behavior of real users. Let us say that we want to have 2000 requests per second. To increase the rate at which JMeter generates the requests, we can increase the number of threads in the thread group that we created earlier. This increases the number of concurrent requests that JMeter is creating at a time. If we want to simulate a gradual increase in the number of users, we can adjust the ramp-up period to be longer.</p><div><h3 class="title2"><a id="tip07" class="calibre1"/>Tip</h3><p class="calibre8">For workloads where we want to simulate a sudden increase of users, we can stick with a ramp-up period of 0 to start all the threads right away. In cases where we want to tune other behaviors such as a constant load and then a sudden spike, we can use the <strong class="calibre2">Stepping Thread Group</strong> plugin.</p></div><p class="calibre8">We may also want to limit it to precisely just 100 requests per second. Here, we can use <code class="literal">Timer</code> elements to control how our threads generate the request. To start limiting throughput, we can use <a id="id238" class="calibre1"/>the <strong class="calibre2">Constant Throughput Timer</strong>. This will make JMeter automatically slow down threads when it perceives that the throughput it is <a id="id239" class="calibre1"/>receiving from our web application is increasing too much.</p><p class="calibre8">Some of the benchmark techniques here are difficult to apply with the built-in Apache JMeter components. There are a variety of plugins that make it simpler to generate the load to drive our application. They <a id="id240" class="calibre1"/>are available as plugins. The Apache JMeter list of popularly used community plugins is found at <a class="calibre1" href="http://jmeter-plugins.org">http://jmeter-plugins.org</a>.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec31" class="calibre1"/>Running distributed tests</h2></div></div></div><p class="calibre8">After tuning the <a id="id241" class="calibre1"/>concurrency parameters for a while, we realize that our result does not change. We may set JMeter to generate 10,000 requests at a time, but that will most likely crash our UI! In this case, we are already reaching the performance limits of our workstation while building the benchmarks. From this point, we can start exploring using a pool of servers that run JMeter to create distributed tests. Distributed tests are useful, because we can grab several servers from the cloud with higher performance to simulate spikes. It is also useful for creating load coming from several sources. This distributed setup is useful for simulating high-latency scenarios, where our users are accessing our Docker application from halfway across the world.</p><p class="calibre8">Execute the following <a id="id242" class="calibre1"/>steps for deploying Apache JMeter on several Docker hosts to perform a distributed benchmark:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, create the following <code class="literal">Dockerfile</code> to create a Docker image called <code class="literal">hubuser/jmeter</code>:<div><pre class="programlisting">FROM java:8u66-jre

# Download URL for JMeter
RUN curl http://www.apache.org/dist/jmeter/binaries/apache-jmeter-2.13.tgz  | tar xz
WORKDIR /apache-jmeter-2.13

EXPOSE 1099
EXPOSE 1100

ENTRYPOINT ["./bin/jmeter", "-j", "/dev/stdout", "-s", \
            "-Dserver_port=1099", "-Jserver.rmi.localport=1100"]</pre></div></li><li class="listitem" value="2">Next, provision the number of Docker hosts we want according to our cloud or server provider. Take note of the hostname or IP address of each Docker host. For our case, we created two Docker hosts called <code class="literal">dockerhost1</code> and <code class="literal">dockerhost2</code>.</li><li class="listitem" value="3">Now, we will run the JMeter server on our Docker hosts. Log in to each of them, and type the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost1$ docker run -p 1099:1099 -p 1100:1100 \</strong>
<strong class="calibre2">         hubuser/jmeter -Djava.rmi.server.hostname=dockerhost1</strong>
<strong class="calibre2">dockerhost2$ docker run -p 1099:1099 -p 1100:1100 \</strong>
<strong class="calibre2">         hubuser/jmeter -Djava.rmi.server.hostname=dockerhost2</strong>
</pre></div></li><li class="listitem" value="4">To finalize our JMeter cluster, we will type the following command to launch the JMeter UI client connected to the JMeter servers:<div><pre class="programlisting">
<strong class="calibre2">$ jmeter -Jremote_hosts=dockerhost1,dockerhost2</strong>
</pre></div></li></ol><div></div><p class="calibre8">With an Apache JMeter cluster at our disposal, we are now ready to run distributed tests. Note that the number of threads in the test plan specifies the thread count on each JMeter server. In the case of the test plan we made in the earlier section, our JMeter benchmark will generate 240,000 requests. We should adjust these counts according to the test workload we have in mind. Some of the guidelines we mentioned in the previous section can be used to tune our remote tests.</p><p class="calibre8">Finally, to start the remote tests, select <strong class="calibre2">Remote Start All</strong> from the <strong class="calibre2">Run</strong> menu. This will spawn the thread groups we created in our test plan to our JMeter servers in <code class="literal">dockerhost1</code> and <code class="literal">dockerhost2</code>. When we look at our access logs of Nginx, we can now see that the IP sources are coming from two different sources. The following IP addresses come from <a id="id243" class="calibre1"/>each of our Docker hosts:</p><div><pre class="programlisting">
<strong class="calibre2">172.16.132.216 [14/Sep/2015:16:...] ""GET / HTTP/1.1"" 200 20 0.003</strong>
<strong class="calibre2">172.16.132.187 [14/Sep/2015:16:...] ""GET / HTTP/1.1"" 200 20 0.003</strong>
<strong class="calibre2">172.16.132.216 [14/Sep/2015:16:...] ""GET / HTTP/1.1"" 200 20 0.003</strong>
<strong class="calibre2">172.16.132.187 [14/Sep/2015:16:...] ""GET / HTTP/1.1"" 200 20 0.002</strong>
<strong class="calibre2">172.16.132.216 [14/Sep/2015:16:...] ""GET / HTTP/1.1"" 200 20 0.002</strong>
<strong class="calibre2">172.16.132.187 [14/Sep/2015:16:...] ""GET / HTTP/1.1"" 200 20 0.003</strong>
</pre></div><div><h3 class="title2"><a id="note37" class="calibre1"/>Note</h3><p class="calibre8">More information on <a id="id244" class="calibre1"/>distributed and remote testing can be found at <a class="calibre1" href="http://jmeter.apache.org/usermanual/remote-test.html">http://jmeter.apache.org/usermanual/remote-test.html</a>.</p></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec33" class="calibre1"/>Other benchmarking tools</h1></div></div></div><p class="calibre8">There are a few other benchmarking tools specifically for benchmarking web-based applications. The following is a short list of such tools with their links:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Apache </strong><a id="id245" class="calibre1"/><strong class="calibre2">Bench</strong>: <a class="calibre1" href="http://httpd.apache.org/docs/2.4/en/programs/ab.html">http://httpd.apache.org/docs/2.4/en/programs/ab.html</a></li><li class="listitem"><strong class="calibre2">HP Lab's </strong><a id="id246" class="calibre1"/><strong class="calibre2">Httperf</strong>: <a class="calibre1" href="http://www.hpl.hp.com/research/linux/httperf">http://www.hpl.hp.com/research/linux/httperf</a></li><li class="listitem"><strong class="calibre2">Siege</strong>: <a id="id247" class="calibre1"/><a class="calibre1" href="https://www.joedog.org/siege-home">https://www.joedog.org/siege-home</a></li></ul></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec34" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we created benchmarks for gauging the performance of our Docker application. By using Apache JMeter and the monitoring system we set up in <a class="calibre1" title="Chapter 4. Monitoring Docker Hosts and Containers" href="part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e">Chapter 4</a>, <em class="calibre9">Monitoring Docker Hosts and Containers,</em> we analyzed how our application behaved under various conditions. We now have an idea about the limitations of our application, and will use it to further optimize it or to scale it out.</p><p class="calibre8">In the next chapter, we will talk about load balancers for scaling-out our application to increase its capacity.</p></div></body></html>