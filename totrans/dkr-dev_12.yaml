- en: '*Chapter 10*: Monitoring Docker Using Prometheus, Grafana, and Jaeger'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章*：使用 Prometheus、Grafana 和 Jaeger 监控 Docker'
- en: In order to understand how an application behaves when it runs in production,
    developers and system operators rely on logging, monitoring, and alerting systems.
    These systems can both give insight into whether an application and its environment
    are operating normally and provide clues to follow if troubleshooting is needed.
    As systems become more complex, the need for deeper insights into both applications
    and their support software also grows. Systems that allow for deep inspection
    of all these concerns without having to alter the code that runs on the system
    can be said to have good **observability** characteristics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解应用程序在生产环境中运行时的行为，开发人员和系统运维人员依赖于日志记录、监控和警报系统。这些系统既能洞察应用程序及其环境是否正常运行，又能在故障排除时提供线索。随着系统变得越来越复杂，对应用程序及其支持软件的深入洞察需求也在增加。那些能够在不修改运行系统代码的情况下深入检查所有这些问题的系统，可以被认为具有良好的**可观察性**特征。
- en: In this chapter, you will learn how to instrument your application and its runtime
    environment to improve the observability of the entire system. You will learn
    about many aspects of logging, monitoring, and alerting. Specifically, you will
    learn how to view, query, and store logs from the Kubernetes cluster both within
    the cluster and in CloudWatch and Amazon **Simple Storage Service** (**S3**).
    You will learn how to implement liveness and readiness probes specific to the
    needs of a cloud-native application, get alerts when something goes wrong, and
    capture application metrics with Prometheus. You will learn how to visualize performance
    and availability metrics using Grafana. Finally, we will dive deep into the application-specific
    metrics at the code and database layer using Jaeger.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将教你如何为你的应用程序及其运行时环境添加监控，以提高整个系统的可观察性。你将学习日志记录、监控和警报的许多方面。具体来说，你将学习如何查看、查询和存储来自
    Kubernetes 集群的日志，既可以在集群内查看，也可以存储到 CloudWatch 和 Amazon **简单存储服务**（**S3**）。你将学习如何实现特定于云原生应用程序需求的存活性和就绪性探针，在出现问题时获取警报，并使用
    Prometheus 捕获应用程序度量。你还将学习如何使用 Grafana 可视化性能和可用性指标。最后，我们将深入探讨通过 Jaeger 获取应用程序特定的代码和数据库层级的度量。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Docker logging and container runtime logging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 日志记录和容器运行时日志记录
- en: Use liveness and readiness probes in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中使用存活性和就绪性探针
- en: Gathering metrics and sending alerts with Prometheus
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus 收集度量并发送警报
- en: Visualizing operational data with Grafana
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Grafana 可视化操作数据
- en: Application performance monitoring with Jaeger
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Jaeger 进行应用程序性能监控
- en: Next up, let's make sure that you are ready to test out these systems and learn
    how to use them in concert to achieve observability for your system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们确保你准备好测试这些系统，并学习如何将它们协同使用，以实现系统的可观察性。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter focuses on the integration of Kubernetes with some AWS services,
    including CloudWatch, Kinesis, and S3, so you must have a working AWS account
    with administrator privileges. You will need to have a working Kubernetes cluster
    in AWS, as set up in a previous chapter with AWS `eksctl`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章侧重于 Kubernetes 与一些 AWS 服务的集成，包括 CloudWatch、Kinesis 和 S3，因此你必须拥有一个具有管理员权限的有效
    AWS 账户。你需要在 AWS 中拥有一个有效的 Kubernetes 集群，正如前一章中使用 AWS `eksctl` 设置的那样。
- en: You will also need to have a current version of the AWS CLI, `kubectl`, and
    `helm` 3.x installed on your local workstation, as described in the previous chapter.
    The `helm` commands in this chapter use the `helm` 3.x syntax.  The EKS cluster
    must have a working ALB Ingress Controller setup.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要在本地工作站上安装当前版本的 AWS CLI、`kubectl` 和 `helm` 3.x，如前一章所述。本章中的 `helm` 命令使用的是
    `helm` 3.x 语法。EKS 集群必须设置有有效的 ALB Ingress Controller。
- en: You could use Spinnaker and Jenkins, as set up in previous chapters, to deploy
    the applications in this chapter, but it is not required.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像前几章中设置的那样使用 Spinnaker 和 Jenkins 来部署本章中的应用程序，但这不是必需的。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码是如何运行的：
- en: '[https://bit.ly/3iIqgvM](https://bit.ly/3iIqgvM)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3iIqgvM](https://bit.ly/3iIqgvM)'
- en: Setting up a demo application – ShipIt Clicker v7
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置演示应用程序 – ShipIt Clicker v7
- en: 'In order to have a sample application to instrument and monitor, we will use
    the version of ShipIt Clicker in the `chapter10` directory in the following GitHub
    repository:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有一个样例应用进行仪表监控，我们将使用以下 GitHub 仓库中 `chapter10` 目录下的 ShipIt Clicker 版本：
- en: '[https://github.com/PacktPublishing/Docker-for-Developers/](https://github.com/PacktPublishing/Docker-for-Developers/)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Docker-for-Developers/](https://github.com/PacktPublishing/Docker-for-Developers/)'
- en: This version of the application has some important production-readiness updates
    in contrast to the version in the previous chapter. Instead of being tightly coupled
    with a specific Redis installation, this version uses a Redis server installed
    separately. We will need to deploy the Redis cluster onto Kubernetes before installing
    the latest version of ShipIt Clicker.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本版本的应用相比上一章的版本有一些重要的生产就绪更新。它不再紧密依赖于特定的 Redis 安装，而是使用单独安装的 Redis 服务器。我们需要在安装最新版本的
    ShipIt Clicker 之前，先将 Redis 集群部署到 Kubernetes 上。
- en: To prepare our Kubernetes environments, both in the local learning environment
    and the AWS cloud EKS cluster, we will first need to install Redis using Helm.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备我们的 Kubernetes 环境，既包括本地学习环境，也包括 AWS 云中的 EKS 集群，我们首先需要使用 Helm 安装 Redis。
- en: Installing Redis from the Bitnami Helm repository
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Bitnami Helm 仓库安装 Redis
- en: In order to deploy this version, we are going to have to deploy the Redis server
    independently of the ShipIt Clicker pod. This represents a more realistic scenario
    than the one where the ShipIt Clicker Kubernetes pod had both the Redis server
    and the stateless application container running in it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了部署这个版本，我们将需要将 Redis 服务器独立部署，而不是将其与 ShipIt Clicker pod 一起部署。这代表了比之前章节中 ShipIt
    Clicker Kubernetes pod 中同时运行 Redis 服务器和无状态应用容器的场景更现实的情况。
- en: 'We are going to use the version of Redis maintained by Bitnami ([https://bitnami.com/](https://bitnami.com/)),
    which offers separate reader and writer endpoints. Deploy Redis first through
    Helm, both to your local Kubernetes installation and then to your cloud Kubernetes
    installation (replace `docker-desktop` and the AWS ARN with the context IDs for
    your installation when you run the following commands):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由 Bitnami 维护的 Redis 版本（[https://bitnami.com/](https://bitnami.com/)），它提供了独立的读取和写入端点。首先通过
    Helm 部署 Redis，先在本地 Kubernetes 安装中，然后在你的云 Kubernetes 安装中（当你运行以下命令时，将 `docker-desktop`
    和 AWS ARN 替换为你安装的上下文 ID）：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will deploy a Redis cluster with one node that accepts read and write,
    and multiple nodes that are replicas that are read-only. The version of ShipIt
    Clicker in this chapter has been adapted to use this external Redis service, which
    uses a Kubernetes secret to store a password needed for authentication.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这将部署一个包含一个接受读写的节点和多个只读的副本节点的 Redis 集群。本章中的 ShipIt Clicker 版本已适配为使用此外部 Redis
    服务，该服务通过 Kubernetes secret 存储用于身份验证的密码。
- en: Offensive terms – master and slave considered harmful
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击性术语——master 和 slave 被认为是有害的
- en: 'The Bitnami Redis template, and Redis itself, use *master* and *slave* terminology
    to describe the roles of nodes in a distributed system. Please know that while
    these terms are common in information technology, many people find this terminology
    backward and offensive. Other terms, such as primary/secondary or reader/writer,
    convey the same information without the negative connotations. See this article
    for more on this controversial issue:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Bitnami Redis 模板和 Redis 本身使用 *master* 和 *slave* 的术语来描述分布式系统中节点的角色。请注意，尽管这些术语在信息技术中很常见，但许多人认为这些术语是过时且冒犯的。其他术语，例如
    primary/secondary 或 reader/writer，传达相同的信息，但没有负面的含义。更多关于这个争议性问题的内容请参见这篇文章：
- en: '[https://medium.com/@zookkini/masters-and-slaves-in-the-tech-world-132ef1c87504](mailto:https://medium.com/@zookkini/masters-and-slaves-in-the-tech-world-132ef1c87504)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/@zookkini/masters-and-slaves-in-the-tech-world-132ef1c87504](mailto:https://medium.com/@zookkini/masters-and-slaves-in-the-tech-world-132ef1c87504)'
- en: Next, let's build and install ShipIt Clicker into our learning environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建并安装 ShipIt Clicker 到我们的学习环境中。
- en: Installing the latest version of ShipIt Clicker locally
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本地安装最新版本的 ShipIt Clicker
- en: 'Next, we will build the ShipIt Clicker Docker container, tag it, and push it
    to Docker Hub, as we did in previous chapters. Issue these commands, replacing
    `dockerfordevelopers` with your Docker Hub username:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建 ShipIt Clicker Docker 容器，标记它，并将其推送到 Docker Hub，正如我们在前面的章节中所做的那样。执行以下命令，将
    `dockerfordevelopers` 替换为你的 Docker Hub 用户名：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Inspect the running pods and services using `kubectl get all` to verify that
    the pod is running, note its name, then inspect the logs with `kubectl logs` to
    see the startup logs. There should be no errors in the log.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl get all`检查正在运行的pods和服务，以验证pod是否正在运行，记下其名称，然后使用`kubectl logs`查看启动日志。日志中不应有任何错误。
- en: Next, let's install this version on EKS.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在EKS上安装这个版本。
- en: Installing the latest version of ShipIt Clicker on EKS through ECR
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过ECR在EKS上安装最新版本的ShipIt Clicker
- en: 'Now that you have built the Docker containers and installed this locally, install
    it to AWS EKS via `values.yaml` to give this a hostname in the Route 53 zone,
    such as `shipit-v7.eks.example.com` (replace the ECR reference with the one corresponding
    to your AWS account and region, and replace `example.com` with your domain name):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了Docker容器并在本地安装了它，将其安装到AWS EKS，通过`values.yaml`为其在Route 53区域中设置主机名，如`shipit-v7.eks.example.com`（将ECR引用替换为与你的AWS账户和区域对应的引用，并将`example.com`替换为你的域名）：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Inspect the Kubernetes logs to make sure the application has deployed cleanly
    to the cluster:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 检查Kubernetes日志，确保应用程序已成功部署到集群：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If all is well with the deployment, get the AWS ALB Ingress Controller ingress
    address, as described in the previous chapter, and create DNS entries in the Route
    53 console for the deployed application with the ALB address. You should then
    be able to reach your application at a URL similar to `https://shipit-v7.eks.example.com/`
    (replace `example.com` with your domain name).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果部署没有问题，获取AWS ALB Ingress Controller的入口地址，如前一章所述，并在Route 53控制台中为部署的应用程序创建DNS条目，使用ALB地址。然后，你应该能够通过类似`https://shipit-v7.eks.example.com/`的URL访问你的应用程序（将`example.com`替换为你的域名）。
- en: Configuring Jenkins and Spinnaker for this chapter
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章Jenkins和Spinnaker的配置
- en: 'You might wonder whether you can use the same Jenkins and Spinnaker configuration
    you set up previously for this chapter. You can, by making a few simple configuration
    changes to the Jenkins job in the `Spinnaker` multi-branch pipeline item and the
    Spinnaker pipeline definitions. Start by fixing up Jenkins. Edit the configuration
    of the job and change the `chapter10/Jenkinsfile`, and then hit the **Save** button:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，是否可以使用你之前为本章设置的Jenkins和Spinnaker配置。可以，只需对Jenkins作业中的`Spinnaker`多分支管道项和Spinnaker管道定义做一些简单的配置更改。首先修复Jenkins。编辑作业的配置并更改`chapter10/Jenkinsfile`，然后点击**保存**按钮：
- en: '![](img/B11641_10_001.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_001.jpg)'
- en: Figure 10.1 – The Jenkins Build Configuration setting for the Spinnaker multi-branch
    pipeline item
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – Spinnaker多分支管道项的Jenkins构建配置设置
- en: Jenkins will rescan the repository and use the files from `chapter10` instead
    of `chapter9`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins将重新扫描仓库，并使用来自`chapter10`的文件，而不是`chapter9`的文件。
- en: Then, go to Spinnaker and edit the pipeline for the staging environment in the
    configuration pipeline stage, and change all the `chapter9` references to `chapter10`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，进入Spinnaker，在配置管道阶段编辑用于暂存环境的管道，并将所有`chapter9`的引用更改为`chapter10`。
- en: You can then use `git push --force origin HEAD:staging` as described in the
    previous chapter to trigger a Kubernetes deployment from Spinnaker.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以按照前一章中描述的使用`git push --force origin HEAD:staging`来触发Spinnaker的Kubernetes部署。
- en: 'The Helm templates for ShipIt Clicker in this chapter have been packaged into
    an archive file, `chapter10/helm.tar.gz`, using the following commands:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中ShipIt Clicker的Helm模板已经通过以下命令打包成一个归档文件`chapter10/helm.tar.gz`：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you alter the Helm Charts and you are using Spinnaker, be sure to use the
    preceding commands to repackage the `helm.tar.gz` file, as Spinnaker expects the
    charts in that specific file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更改了Helm Charts并且正在使用Spinnaker，确保使用前面的命令重新打包`helm.tar.gz`文件，因为Spinnaker期望在该特定文件中找到这些chart。
- en: Next, let's take a detailed look at logging for both the Docker containers and
    the container runtime logs, such as those for the Kubernetes control plane.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们详细了解Docker容器和容器运行时日志，例如Kubernetes控制平面的日志。
- en: Docker logging and container runtime logging
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker日志和容器运行时日志
- en: When you are trying to troubleshoot problems with your application, it helps
    to have detailed logs for both the application itself and from whatever system
    it runs. Every Docker container, whether it is run locally or with a cloud container
    runtime manager such as Kubernetes, produces its own logs that you can query.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在排查应用程序问题时，拥有详细的日志对解决问题非常有帮助，包括来自应用程序本身和它运行的任何系统的日志。每个Docker容器，无论是在本地运行还是使用像Kubernetes这样的云容器运行时管理器运行，都生成自己的日志，你可以查询这些日志。
- en: In previous chapters, we've used both the `docker logs` command and the `kubectl
    logs` command in order to examine logs for the demo application when run both
    on a local workstation and in the cloud with Kubernetes. These commands can yield
    insight into events that are critical to your system, including both application
    logging messages and error and exception logs. They are still the bedrock tools
    you will reach for; but particularly when we need to scale out our application
    with Kubernetes, we will need a more sophisticated approach.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用了`docker logs`命令和`kubectl logs`命令，以便在本地工作站和Kubernetes云环境中运行演示应用程序时检查日志。这些命令可以为您的系统提供有关事件的关键见解，包括应用程序日志消息以及错误和异常日志。它们仍然是您最常使用的基本工具；但是，特别是当我们需要使用Kubernetes扩展应用程序时，我们将需要一种更复杂的方法。
- en: Understanding Kubernetes container logging
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Kubernetes容器日志
- en: Every Docker container running in every Kubernetes pod produces logs. The Kubernetes
    runtime, by default, will temporarily store the last 10 MB of logs for every running
    container. This makes it possible to sample the logs for every running application
    using only the `kubectl logs` tool. When a pod is evicted from a node, or when
    a container restarts, *Kubernetes will delete these ephemeral log files*; it will
    *not* automatically save the logs to permanent storage. This is far from ideal
    if you need to troubleshoot a problem, especially if the problem happened long
    ago enough that those logs have rolled over and the older log entries are unavailable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个在Kubernetes pod中运行的Docker容器都会产生日志。默认情况下，Kubernetes运行时会临时存储每个运行中容器的最后10MB日志。这使得只需使用`kubectl
    logs`工具，就能对每个运行中的应用程序进行日志采样。当pod从节点上驱逐或容器重启时，*Kubernetes将删除这些临时日志文件*；它*不会*自动将日志保存到永久存储中。如果您需要排查问题，尤其是当问题发生在很久之前，导致那些日志已经滚动，且较旧的日志条目无法获取时，这样的做法远非理想。
- en: You can use `kubectl` to examine multiple logs at once, as shown in the previous
    chapter, with respect to showing multiple Spinnaker container logs, and you can
    use common command-line tools, such as `grep`, `awk`, `jq`, and `less`, to carry
    out further basic searching and filtering on logs. However, the issue with logs
    rolling over will thwart some search attempts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所示，您可以使用`kubectl`一次检查多个日志，针对显示多个Spinnaker容器日志，您还可以使用常见的命令行工具，如`grep`、`awk`、`jq`和`less`，对日志进行进一步的基本搜索和过滤。然而，日志滚动的问题会阻碍一些搜索尝试。
- en: Given the constraints on the basic features of the Kubernetes system with respect
    to both log retention and searching, it would be prudent to explore how we might
    want to mitigate these issues. Let's talk about the characteristics we would want
    from a log management system next.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Kubernetes系统在日志保留和搜索方面的基本功能限制，明智的做法是探索如何缓解这些问题。接下来，让我们讨论我们希望日志管理系统具备的特点。
- en: Ideal characteristics for a log management system
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理想的日志管理系统特性
- en: 'Ideally, you would want to use a system for managing your logs that has some
    of the following characteristics:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，您希望使用具有以下某些特征的日志管理系统：
- en: Having log messages be available to view in a central console
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在中央控制台查看日志消息
- en: Low latency from when a log event happens to when it is available for searches
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日志事件发生到可供搜索的延迟时间较低
- en: Collection of logs from multiple sources, including Kubernetes objects such
    as pods, nodes, deployments, and Docker containers
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多个来源收集日志，包括Kubernetes对象，如pods、节点、部署和Docker容器
- en: An easy-to-use search interface, with the ability to save and reuse ad hoc queries
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于使用的搜索界面，能够保存并重复使用临时查询
- en: A way to visualize a histogram of search results that includes the ability to
    zoom in on the graph by clicking and dragging over the graph (a feature known
    as *brushing*)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种可视化搜索结果直方图的方式，支持通过点击和拖动图表来缩放图表（这一功能被称为*刷选*）
- en: A way to send alerts based on the contents of log messages
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据日志消息内容发送警报的方式
- en: A way to configure the retention period of the log messages
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置日志消息保留期限的方式
- en: 'Various organizations have built many excellent log storage and analysis systems
    over the past 20 years, including the following third-party log management systems:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的20年里，各种组织构建了许多优秀的日志存储和分析系统，包括以下第三方日志管理系统：
- en: Splunk ([https://www.splunk.com/](https://www.splunk.com/))
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Splunk ([https://www.splunk.com/](https://www.splunk.com/))
- en: Elasticsearch ([https://www.elastic.co/](https://www.elastic.co/))
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch ([https://www.elastic.co/](https://www.elastic.co/))
- en: Loggly ([https://www.loggly.com/](https://www.loggly.com/))
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loggly ([https://www.loggly.com/](https://www.loggly.com/))
- en: Papertrail ([https://www.papertrail.com/](https://www.papertrail.com/))
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papertrail ([https://www.papertrail.com/](https://www.papertrail.com/))
- en: New Relic Logs ([https://newrelic.com/products/logs](https://newrelic.com/products/logs))
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: New Relic Logs ([https://newrelic.com/products/logs](https://newrelic.com/products/logs))
- en: Datadog Log Management ([https://docs.datadoghq.com/logs/](https://docs.datadoghq.com/logs/))
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Datadog Log Management ([https://docs.datadoghq.com/logs/](https://docs.datadoghq.com/logs/))
- en: 'Cloud providers also have built excellent integrated log storage and analysis
    systems, including the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商还拥有出色的集成日志存储和分析系统，包括以下几种：
- en: AWS CloudWatch ([https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/))
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CloudWatch ([https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/))
- en: Google Cloud Logging ([https://cloud.google.com/logging](https://cloud.google.com/logging))
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud Logging ([https://cloud.google.com/logging](https://cloud.google.com/logging))
- en: Microsoft Azure Monitor Logs ([https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform-logs](https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform-logs))
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Azure Monitor Logs ([https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform-logs](https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform-logs))
- en: As a developer or system operator, you can use these systems to store and search
    log entries. However, in order to do so, you must use a **log shipper** to extract
    the logs from their origins and forward them to the log management system.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者或系统运维人员，你可以使用这些系统来存储和搜索日志条目。然而，为了做到这一点，你必须使用 **日志传输工具** 从日志源提取日志并将其转发到日志管理系统。
- en: 'We will examine how to forward Kubernetes container logs to one of these systems
    shortly, but first, let''s examine another critical system aspect: logging for
    the Kubernetes control plane that provides orchestration for nodes, pods, and
    the rest of the family of Kubernetes objects.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快探讨如何将 Kubernetes 容器日志转发到这些系统中的一个，但首先，让我们研究另一个关键系统方面：Kubernetes 控制平面的日志记录，它负责节点、Pod
    以及 Kubernetes 对象家族的协调工作。
- en: Troubleshooting Kubernetes control plane issues with logs
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用日志排查 Kubernetes 控制平面问题
- en: 'If you run your own Kubernetes cluster, where you manage the control plane
    servers, you may have a difficult time troubleshooting system-level issues. The
    Kubernetes troubleshooting guide offers guidance about looking at various log
    files on individual machines in the control plane cluster, which could be a painful
    exercise:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你自己运行 Kubernetes 集群并管理控制平面服务器，可能会在排查系统级别问题时遇到困难。Kubernetes 故障排除指南提供了关于查看控制平面集群中各个机器日志文件的指导，这可能是一个痛苦的过程：
- en: '[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/
    )'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/)'
- en: 'However, if you are using managed Kubernetes services, such as AWS EKS, you
    will not have direct access to these systems. You might ask, *how do I get those
    logs*? The managed Kubernetes service providers all have ways to ship those logs
    to another system in order to aid in troubleshooting. Fortunately, AWS EKS has
    an optional configuration setting that tells it to ship logs from its control
    plane directly to CloudWatch:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用的是托管的 Kubernetes 服务，例如 AWS EKS，你将无法直接访问这些系统。你可能会问，*我该如何获取这些日志*？托管的 Kubernetes
    服务提供商都有将这些日志转发到另一个系统的方式，以帮助故障排除。幸运的是，AWS EKS 提供了一个可选的配置设置，允许它将控制平面的日志直接转发到 CloudWatch：
- en: '[https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html](https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html](https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html)'
- en: 'If you used the AWS EKS Quick Start described in [*Chapter 8*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157),
    *Deploying Docker Apps to Kubernetes*, to create your EKS cluster, it sets this
    up for you. You can go to the CloudWatch Logs console in the `us-east-2` region
    to verify: [https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logs:](https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logs:)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了 [*第 8 章*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157) 中描述的 AWS EKS
    快速启动来创建 EKS 集群，系统会为你自动设置这一配置。你可以前往 `us-east-2` 区域的 CloudWatch Logs 控制台进行验证：[https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logs:](https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logs:)
- en: 'You will see a listing of log groups similar to the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到类似以下的日志组列表：
- en: '![](img/B11641_10_002.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_002.jpg)'
- en: Figure 10.2 – CloudWatch log groups showing EKS control plane logs
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 显示 EKS 控制平面日志的 CloudWatch 日志组
- en: The main Kubernetes control plan log group will be named similarly to `/aws/eks/EKS-8PWG76O8/cluster`,
    but with your EKS cluster name. You can navigate to this and examine the logs
    there in detail through the console.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的 Kubernetes 控制平面日志组将类似于 `/aws/eks/EKS-8PWG76O8/cluster`，但名称会包含你的 EKS 集群名称。你可以导航到这里，并通过控制台详细查看日志。
- en: 'If you used `eksctl` to create your EKS cluster, you may not have enabled CloudWatch
    logging. You can use the instructions here to add CloudWatch logging to EKS through
    `eksctl`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 `eksctl` 创建了 EKS 集群，你可能没有启用 CloudWatch 日志记录。你可以使用这里的说明通过 `eksctl` 启用 EKS
    的 CloudWatch 日志记录：
- en: '[https://eksctl.io/usage/cloudwatch-cluster-logging/](https://eksctl.io/usage/cloudwatch-cluster-logging/)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://eksctl.io/usage/cloudwatch-cluster-logging/](https://eksctl.io/usage/cloudwatch-cluster-logging/)'
- en: Now that you have verified that your EKS cluster control plane is logging to
    CloudWatch and have learned how to get a basic viewing of the logs, let's proceed
    to capture the rest of the Kubernetes logs in CloudWatch Logs and analyze them
    with CloudWatch Logs Insights.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经验证了 EKS 集群控制平面日志已发送到 CloudWatch，并学习了如何基本查看日志，我们将继续在 CloudWatch Logs 中捕获其余的
    Kubernetes 日志，并使用 CloudWatch Logs Insights 进行分析。
- en: Storing logs with CloudWatch Logs
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CloudWatch Logs 存储日志
- en: AWS operates a cloud-scale service to handle logging, time-series metrics, data
    ingestion, storage, and analysis called **CloudWatch**. Many AWS services, including
    EKS, offer logging integration through CloudWatch. As with so many AWS services,
    you only pay for what you use. You can learn more about the basics of CloudWatch
    at [https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了一个云规模的服务，处理日志、时间序列度量、数据摄取、存储和分析，称为 **CloudWatch**。许多 AWS 服务，包括 EKS，都通过
    CloudWatch 提供日志集成功能。像许多 AWS 服务一样，你只为实际使用的部分付费。你可以在 [https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/)
    学习有关 CloudWatch 的基础知识。
- en: We saw in the previous section that AWS allows us to configure the EKS control
    plane to send logs directly to CloudWatch. This is good, but if we are going to
    manage our logs in a central place, we should try to store *all* of our logs there.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中看到，AWS 允许我们配置 EKS 控制平面直接将日志发送到 CloudWatch。这很好，但如果我们要在一个中央位置管理日志，我们应该尽量将
    *所有* 日志都存储在那里。
- en: Next, we will look at how we can ship more logs to CloudWatch, using the solution
    that AWS recommends in the EKS documentation – Fluent Bit ([https://fluentbit.io/](https://fluentbit.io/)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解如何通过 AWS 推荐的解决方案将更多日志发送到 CloudWatch，该解决方案在 EKS 文档中有所描述——Fluent Bit
    ([https://fluentbit.io/](https://fluentbit.io/))。
- en: AWS provides an excellent tutorial on setting up Fluent Bit with EKS at [https://aws.amazon.com/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/](https://aws.amazon.com/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了一个关于如何在 EKS 上设置 Fluent Bit 的优秀教程，网址是 [https://aws.amazon.com/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/](https://aws.amazon.com/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/)。
- en: The scripts and configuration files described later in this chapter are inspired
    by and partially derived from that article.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后描述的脚本和配置文件灵感来源于并部分借鉴了那篇文章。
- en: Next, we will learn how we can use a script to install Fluent Bit and supporting
    AWS resources quickly and repeatably.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用脚本快速且可重复地安装 Fluent Bit 及其支持的 AWS 资源。
- en: Installing Fluent Bit to ship logs to CloudWatch
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Fluent Bit 以将日志发送到 CloudWatch
- en: 'While you could go through the steps in the previously referenced AWS blogs
    by hand, in order to streamline these operations and make them work more seamlessly
    with the AWS EKS Quick Start, you can use the `install-fluentbit-daemonset.sh`
    script in this chapter to install Fluent Bit as a DaemonSet in your EKS cluster,
    with a configuration that ships logs to CloudWatch Logs. Give it the name of the
    CloudFormation template for your EKS cluster CloudFormation template as a command-line
    parameter:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以手动按照之前引用的 AWS 博客中的步骤操作，但为了简化这些操作并使其与 AWS EKS 快速启动更加无缝配合，你可以使用本章中的 `install-fluentbit-daemonset.sh`
    脚本，以 DaemonSet 的形式在 EKS 集群中安装 Fluent Bit，并使用一个将日志发送到 CloudWatch Logs 的配置。将 EKS
    集群的 CloudFormation 模板名称作为命令行参数传入：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Setting up Fluent Bit to work with AWS requires a bit more work than it does
    with some other cloud platforms; for example, if you were using Google Cloud Platform's
    GKE, it would be installed automatically for you.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Fluent Bit 以与 AWS 配合使用，比与其他一些云平台配合使用要多一些工作；例如，如果你使用的是 Google Cloud Platform
    的 GKE，它会为你自动安装。
- en: Once you have the logs for the containers streaming into CloudWatch, you can
    use the CloudWatch AWS console to view the container logs, as well as the control
    plane logs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将容器的日志流式传输到 CloudWatch，你可以使用 CloudWatch AWS 控制台查看容器日志以及控制平面日志。
- en: Changing the CloudWatch log retention periods
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更改 CloudWatch 日志保留周期
- en: 'By default, CloudWatch will store logs indefinitely. To save on log storage
    fees, you should consider setting a relatively short retention period for your
    CloudWatch logs – such as 60 days. You can do that from the console or the command
    line, as follows, where this command sets the period for the `fluentbit-cloudwatch`
    log group created by the `install-fluentbit-daemonset.sh` script:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CloudWatch 会无限期地存储日志。为了节省日志存储费用，你应该考虑为 CloudWatch 日志设置相对较短的保留期限——例如 60
    天。你可以通过控制台或命令行执行此操作，以下命令设置由 `install-fluentbit-daemonset.sh` 脚本创建的 `fluentbit-cloudwatch`
    日志组的保留周期：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You might consider doing this for each of the CloudWatch log groups, even the
    ones created by the AWS EKS Quick Start CloudFormation template.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会考虑为每个 CloudWatch 日志组进行此操作，即使是由 AWS EKS 快速入门 CloudFormation 模板创建的日志组。
- en: Next, let's see how we can store logs in S3.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下如何将日志存储到 S3 中。
- en: Storing logs for the long term with AWS S3
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS S3 长期存储日志
- en: In order to economically store log data for the long term, over a period of
    months or years, you can use an inexpensive cloud object storage system, such
    as Amazon S3 ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了经济高效地长期存储日志数据，可以使用廉价的云对象存储系统，如 Amazon S3（[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)）。
- en: If you have a serious need to retain logs for the long term – for example, if
    you have a sensitive financial application where regulations mandate 5 years of
    storage for all application logs – S3 could be a good fit. You can make long-term
    storage even less expensive by setting up S3 life cycle rules on the bucket to
    move objects to less expensive storage tiers, migrate them to Amazon Glacier ([https://aws.amazon.com/glacier/](https://aws.amazon.com/glacier/)),
    or expire and delete older records.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有长期保留日志的严重需求——例如，如果你有一个敏感的金融应用程序，其中规定要求所有应用日志必须存储 5 年——S3 可能是一个不错的选择。你可以通过为存储桶设置
    S3 生命周期规则，将对象迁移到更便宜的存储层，迁移到 Amazon Glacier（[https://aws.amazon.com/glacier/](https://aws.amazon.com/glacier/)），或使旧记录过期并删除，从而使长期存储更加经济实惠。
- en: AWS published a blog article ([https://aws.amazon.com/blogs/opensource/centralized-container-logging-fluent-bit/](https://aws.amazon.com/blogs/opensource/centralized-container-logging-fluent-bit/))
    that outlines a path that you could use to stream the logs into S3 using Kinesis
    Firehose as an additional Fluent Bit target. You could follow the instructions
    in the blog under the *Log analysis across clusters* section to get the logs streaming
    to S3 that way, but it will probably be challenging to do so as you would have
    to adapt the scripts to the EKS Quick Start in many ways, including changing the
    AWS region and dealing with the assumption that you used `eksctl` to set up your
    cluster.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 发布了一篇博客文章（[https://aws.amazon.com/blogs/opensource/centralized-container-logging-fluent-bit/](https://aws.amazon.com/blogs/opensource/centralized-container-logging-fluent-bit/)），概述了你可以使用
    Kinesis Firehose 作为额外的 Fluent Bit 目标将日志流式传输到 S3 的路径。你可以按照博客中 *跨集群的日志分析* 部分的说明，将日志以这种方式流式传输到
    S3，但这可能会比较具有挑战性，因为你需要以多种方式调整脚本，以适应 EKS 快速入门，包括更改 AWS 区域并假设你使用 `eksctl` 来设置集群。
- en: A project called `CloudWatch2S3` that was inspired by that blog can help with
    this process by deploying one CloudFormation template. This has the advantage
    that it can send *all* of the CloudWatch log groups to S3, and you can install
    it by applying a single CloudFormation template. It can also collect CloudWatch
    logs from multiple AWS accounts should you choose to do that. Clone the GitHub
    repository at  [https://github.com/CloudSnorkel/CloudWatch2S3](https://github.com/CloudSnorkel/CloudWatch2S3)
    to your workstation and follow the directions there to set up the streaming of
    CloudWatch logs to S3\. Before you proceed, you might consider creating an Amazon
    **Key Management Service** (**KMS**) key to encrypt the Kinesis Firehose and S3
    bucket contents. Install the CloudFormation template using the AWS console or
    CLI, as you prefer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为 `CloudWatch2S3` 的项目，灵感来自于那篇博客，可以通过部署一个 CloudFormation 模板来帮助完成这个过程。它的优点是可以将
    *所有* CloudWatch 日志组发送到 S3，并且你可以通过应用单个 CloudFormation 模板来安装它。它还可以收集来自多个 AWS 账户的
    CloudWatch 日志，如果你选择这么做的话。克隆 GitHub 仓库 [https://github.com/CloudSnorkel/CloudWatch2S3](https://github.com/CloudSnorkel/CloudWatch2S3)
    到你的工作站，并按照仓库中的指引来设置将 CloudWatch 日志流式传输到 S3。在继续之前，你可能考虑创建一个 Amazon **Key Management
    Service** (**KMS**) 密钥来加密 Kinesis Firehose 和 S3 存储桶的内容。根据需要，你可以使用 AWS 控制台或 CLI
    安装 CloudFormation 模板。
- en: Now that we have seen how to store logs in both CloudWatch and S3, it would
    be nice to learn how we might query those logs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在 CloudWatch 和 S3 中存储日志，接下来学习如何查询这些日志会非常有用。
- en: Analyzing logs with CloudWatch Insights and Amazon Athena
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CloudWatch Insights 和 Amazon Athena 分析日志
- en: Now that you have logs stored in both CloudWatch and S3, you can query them
    with either CloudWatch Insights or Amazon Athena.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经在 CloudWatch 和 S3 中存储了日志，你可以使用 CloudWatch Insights 或 Amazon Athena 来查询这些日志。
- en: Analyzing logs stored in CloudWatch with CloudWatch Insights
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 CloudWatch Insights 分析存储在 CloudWatch 中的日志
- en: The easiest way to perform queries on the logs stored in AWS is with CloudWatch
    Insights. This web-based query interface provides an interactive query builder
    and a way to visualize the results in both histogram and tabular data formats.
    It features a saved query manager, which is a key feature because it lets you
    build and refine a set of queries that can span one or more log groups. The documentation
    for CloudWatch Insights is available at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 执行查询 AWS 中存储日志的最简单方法是使用 CloudWatch Insights。这个基于 Web 的查询接口提供了一个交互式查询构建器，并且能够以直方图和表格数据格式可视化结果。它具备保存查询管理器的功能，这是一个关键功能，因为它让你能够构建并优化一组可以跨越一个或多个日志组的查询。有关
    CloudWatch Insights 的文档可以参考 [https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html)。
- en: You can explore the sample queries in the AWS console for that service to get
    a better feel for what CloudWatch Insights has to offer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 AWS 控制台中探索该服务的示例查询，以更好地了解 CloudWatch Insights 提供的功能。
- en: Analyzing logs stored in S3 with AWS Athena
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS Athena 分析存储在 S3 中的日志
- en: 'When logs are stored in S3, you won''t be able to query them in exactly the
    same way you would if you used CloudWatch Insights or another log management system.
    However, there are ways to efficiently query logs stored in S3\. The most direct
    way is with a query tool called Amazon Athena:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当日志存储在 S3 中时，你无法以与使用 CloudWatch Insights 或其他日志管理系统完全相同的方式进行查询。然而，仍然有方法可以高效地查询存储在
    S3 中的日志。最直接的方法是使用一个名为 Amazon Athena 的查询工具：
- en: '[https://aws.amazon.com/athena/](https://aws.amazon.com/athena/)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/athena/](https://aws.amazon.com/athena/)'
- en: 'Athena will let you use a SQL-like query language on semi-structured data stored
    in S3 buckets. You pay by the query, according to how much data is scanned and
    how much processing time it requires. In order to get Athena to understand the
    structure of your S3 data, you would need to configure virtual tables using the
    AWS Glue catalog:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Athena 允许你使用类似 SQL 的查询语言查询存储在 S3 存储桶中的半结构化数据。你按照查询付费，费用根据扫描的数据量和所需的处理时间来计算。为了让
    Athena 理解你的 S3 数据的结构，你需要使用 AWS Glue 目录配置虚拟表：
- en: '[https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html)'
- en: Setting up the combination of AWS Glue and Athena is pretty complex and is beyond
    the scope of what we can show in this chapter. See the links in the *Further reading*
    section at the end of this chapter for more information on setting up Athena so
    that you can use it to query the data stored in S3.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 AWS Glue 和 Athena 的组合相当复杂，超出了本章内容的范围。有关设置 Athena 以便查询存储在 S3 中的数据的更多信息，请参阅本章末尾的
    *进一步阅读* 部分中的链接。
- en: Exercise – finding the number of ShipIt Clicker games played
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 – 查找玩过的 ShipIt 点击器游戏数量
- en: 'The ShipIt Clicker demo application emits a log message every time a game is
    started of the form:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ShipIt 点击器演示应用程序每当启动一个游戏时都会发出如下形式的日志信息：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create a query in CloudWatch Insights that counts the total number of games
    that have been created. For CloudWatch Insights, you will have to select the `fluentbit-cloudwatch`
    log group.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CloudWatch Insights 中创建一个查询，计算已创建的游戏总数。对于 CloudWatch Insights，你需要选择 `fluentbit-cloudwatch`
    日志组。
- en: Solution
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Refer to the following file for the solution:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下文件获取解决方案：
- en: '[https://github.com/PacktPublishing/Docker-for-Developers/tree/master/chapter10/cloudwatch-insights.txt](https://github.com/PacktPublishing/Docker-for-Developers/tree/master/chapter10/cloudwatch-insights.txt)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Docker-for-Developers/tree/master/chapter10/cloudwatch-insights.txt](https://github.com/PacktPublishing/Docker-for-Developers/tree/master/chapter10/cloudwatch-insights.txt)'
- en: Using the liveness, readiness, and startup probes in Kubernetes
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中使用存活性、就绪性和启动探针
- en: Kubernetes has multiple types of health checks, called **probes**, to ensure
    that the Docker containers it runs are in shape to process traffic. You can read
    about them in detail at [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有多种类型的健康检查，称为 **探针**，用于确保其运行的 Docker 容器能够处理流量。你可以在[https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)中详细阅读它们。
- en: 'The types of probes deal with different concerns:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 探针的类型处理不同的关注点：
- en: '**Liveness**: Determines whether an application can process requests at all.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存活性**：确定应用程序是否能够处理请求。'
- en: '**Readiness**: Determines whether a container is ready to receive real traffic,
    especially if it depends on external resources that have to be reachable or connected.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**就绪性**：确定容器是否已准备好接收真实流量，特别是当容器依赖于必须可访问或已连接的外部资源时。'
- en: '**Startup**: Determines whether a container is ready to start taking the other
    two types of traffic, intended for slow-starting legacy applications to give them
    time to start. As these are mostly needed for legacy applications, we won''t cover
    them in detail.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启动**：确定容器是否准备好开始接收另外两种类型的流量，适用于启动较慢的传统应用程序，给它们时间启动。由于这些探针主要用于传统应用程序，我们将不会详细讨论它们。'
- en: You can configure probes to execute commands inside a running container, perform
    a TCP port check, or check an HTTP endpoint. Probes have sensible default values
    for timeouts and check intervals—by default, a probe will check every 10 seconds
    and will fail with a timeout with 1 second. By default, a probe must fail three
    times in a row before the probe enters the failure state, and it must succeed
    once before it enters a success state. You can override these values through template
    annotations, in `deployment.yaml` in your Helm Charts, for example.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以配置探针在运行中的容器内执行命令、进行 TCP 端口检查或检查 HTTP 端点。探针有合理的默认超时和检查间隔值——默认情况下，探针每 10 秒检查一次，并在
    1 秒内超时失败。默认情况下，探针必须连续失败三次才会进入失败状态，必须成功一次才会进入成功状态。你可以通过模板注解覆盖这些值，例如在 Helm Charts
    中的 `deployment.yaml` 文件中。
- en: If a liveness probe for a container fails enough times, Kubernetes will kill
    the container and restart it. If a readiness probe for a container in a pod is
    failing, Kubernetes will not direct any traffic for a service depending on that
    pod to the container. We are going to examine liveness and readiness probes in
    detail next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器的存活性探针失败足够多次，Kubernetes 将终止容器并重启它。如果一个 Pod 中容器的就绪性探针失败，Kubernetes 将不会将依赖该
    Pod 的服务的流量指向该容器。接下来，我们将详细探讨存活性和就绪性探针。
- en: Using a liveness probe to see whether a container can respond
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用存活性探针查看容器是否能够响应
- en: 'For a service such as ShipIt Clicker, a good liveness check would be one where
    the application can rely solely on internally configured resources to respond
    – for example, relying on containers deployed in the same pod. In previous chapters,
    the liveness and readiness checks for this application were set to retrieve the
    `/` resource via HTTP. The liveness check stays the same for this chapter, as
    the ability to serve a simple HTML page is a good liveness check for an Express
    application. Observe the following excerpt from `chapter10/shipitclicker/templates/deployment.yaml`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 ShipIt Clicker 这样的服务，一个好的存活检查是，应用程序可以完全依赖内部配置的资源进行响应——例如，依赖于部署在同一 pod 中的容器。在前几章中，应用程序的存活和就绪检查是通过
    HTTP 获取 `/` 资源进行的。本章的存活检查保持不变，因为能够提供一个简单的 HTML 页面是 Express 应用程序的一个不错的存活检查。请查看以下来自
    `chapter10/shipitclicker/templates/deployment.yaml` 的片段：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This makes Express serve the file in `chapter10/src/public/index.html`. This
    makes a decent liveness probe, but it does not mean that a pod is ready to process
    requests that reach out to external resources. For that, we should use a readiness
    check.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 Express 提供 `chapter10/src/public/index.html` 文件。这个做法提供了一个不错的存活探针，但它并不意味着
    pod 已准备好处理需要访问外部资源的请求。为此，我们应该使用就绪探针。
- en: Using a readiness probe to ensure that a service can receive traffic
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用就绪探针来确保服务能够接收流量
- en: Some applications have to complete a wave of initialization where they make
    database calls and call on external services before they are ready to take traffic.
    For ShipIt Clicker, the application must be able to contact Redis before it is
    ready to receive traffic. Next, we are going to examine a defect in the prior
    versions of ShipIt Clicker and the fix that had to be made to support both liveness
    and readiness probes, as these changes are illustrative of the type of changes
    that you might have in your application.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序需要完成一系列初始化操作，比如进行数据库调用和调用外部服务，才能准备好接收流量。对于 ShipIt Clicker，应用程序必须在准备好接收流量之前能够联系到
    Redis。接下来，我们将检查 ShipIt Clicker 早期版本中的缺陷，以及为了支持存活和就绪探针所做的修复，因为这些变化是你在应用程序中可能会遇到的更改的典型示例。
- en: Changing ShipIt Clicker to support separate liveness and readiness probes
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改 ShipIt Clicker 以支持独立的存活和就绪探针
- en: Previous versions of ShipIt Clicker would suffer a fatal exception if any connection
    to Redis failed. This would happen as soon as the initialization routines in `src/server/index.js`
    loaded, as the modules it loaded would instantiate the `RedisDatabase` class in
    `src/server/api/services/redis-service.js`, which would immediately connect to
    the Redis server. This class lacked a Redis error handler, so the error it threw
    was fatal and caused the process to terminate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ShipIt Clicker 的早期版本会在连接 Redis 失败时遇到致命异常。只要 `src/server/index.js` 中的初始化例程加载，这种情况就会发生，因为它加载的模块会实例化位于
    `src/server/api/services/redis-service.js` 的 `RedisDatabase` 类，而该类会立即连接到 Redis
    服务器。这个类缺少 Redis 错误处理程序，因此抛出的错误是致命的，会导致进程终止。
- en: This failure would repeat immediately as Kubernetes tried to start another container
    and would trigger a series of crashes that engaged the Kubernetes crash loop detector.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故障会立即重复，因为 Kubernetes 尝试启动另一个容器，并触发一系列崩溃，进而激活 Kubernetes 崩溃循环检测器。
- en: 'The new error handler in the `RedisDatabase.init()` method in `chapter10/src/server/api/services/redis.service.js`
    looks like this, and will log all Redis errors to the console – and, therefore,
    to the Kubernetes logging system – to make it easier to troubleshoot:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`chapter10/src/server/api/services/redis.service.js` 中 `RedisDatabase.init()`
    方法的新错误处理程序如下所示，它将所有 Redis 错误记录到控制台——因此，也会记录到 Kubernetes 日志系统——以便更容易进行故障排除：'
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This chapter''s code also uses a lazy loading pattern to avoid having to immediately
    connect to Redis when the classes are instantiated. With lazy loading, you defer
    the creation of an object or resource until you actually need it. We achieve lazy
    loading by using by the `RedisDatabase.instance()` method, which uses the singleton
    design pattern for the Redis client connection:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码还采用了懒加载模式，避免在类实例化时立即连接 Redis。通过懒加载，你将对象或资源的创建推迟到实际需要时。我们通过 `RedisDatabase.instance()`
    方法实现了懒加载，该方法使用了 Redis 客户端连接的单例设计模式：
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using lazy loading will allow us to defer connecting to the Redis server until
    a request arrives that really requires it. Recall that in this version of the
    application, we split the Redis server out from the ShipIt Clicker service and
    have it running separately. Given this, a readiness probe should reach out to
    the Redis server and make sure that ShipIt Clicker can indeed talk to it, before
    accepting traffic. This version has a new API endpoint, `/api/v2/games/ready`,
    which performs a Redis `PING` operation to ensure that the application is ready
    to take traffic:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用懒加载将允许我们推迟连接 Redis 服务器，直到到达一个真正需要它的请求。回想一下，在这个版本的应用程序中，我们将 Redis 服务器从 ShipIt
    Clicker 服务中拆分出来，并单独运行。鉴于此，准备探针应该连接到 Redis 服务器，并确保 ShipIt Clicker 确实可以与它通信，然后再接受流量。这个版本有一个新的
    API 端点 `/api/v2/games/ready`，它执行 Redis 的 `PING` 操作以确保应用程序准备好接收流量：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If the Redis server is not available, this readiness probe will fail and Kubernetes
    will remove the container that fails the health check from the service.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Redis 服务器不可用，则此就绪探针将失败，Kubernetes 将从服务中移除未通过健康检查的容器。
- en: Exercise – forcing ShipIt Clicker to fail the readiness check
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 – 强制 ShipIt Clicker 失败就绪检查
- en: 'Next, we will run an experiment to see what happens when the liveness probe
    passes but the readiness check fails. Use `kubectl` to switch to your local learning
    environment Kubernetes context. Temporarily alter the `chapter10/shipitclicker/template/configmap.yaml`
    file to break the Redis installation by changing the `REDIS_PORT` value to an
    invalid number, such as `1234`. Then, use Helm to install the chart with the alternative
    `shipit-ready-fail` name. Use `kubectl get pods` to verify that the new pod is
    in the `RUNNING` state but has `0/1` pods that are marked `READY`. Your output
    should look something like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一个实验，看看当存活探针通过但就绪检查失败时会发生什么。使用 `kubectl` 切换到本地学习环境的 Kubernetes 上下文。暂时更改
    `chapter10/shipitclicker/template/configmap.yaml` 文件，修改 `REDIS_PORT` 值为无效数字，例如
    `1234`，以破坏 Redis 安装。然后，使用 Helm 安装替代名称为 `shipit-ready-fail` 的 chart。使用 `kubectl
    get pods` 验证新 pod 的状态为 `RUNNING`，但标记为 `READY` 的 pod 数量为 `0/1`。你的输出应该类似于以下内容：
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The readiness checks for this installation of ShipIt Clicker will start failing
    immediately. If you describe the pod, you will see that it is no longer ready.
    When you are done, use Helm to uninstall the `shipit-ready-fail` chart and revert
    the value in the `configmap.yaml` file to its original value.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个安装版本的 ShipIt Clicker 的准备检查将立即开始失败。如果你描述这个 pod，你会看到它不再是就绪状态。当你完成后，使用 Helm 卸载
    `shipit-ready-fail` chart，并将 `configmap.yaml` 文件中的值恢复为原始值。
- en: Gathering metrics and sending alerts with Prometheus
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Prometheus 收集指标并发送警报
- en: Prometheus is the dominant Kubernetes-based system for collecting metrics on
    cluster operations. Prometheus sports a wide range of features related to handling
    time-series data, visualizing data, querying it, and sending alerts based on metrics
    data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 是一个主导的基于 Kubernetes 的系统，用于收集集群操作的指标数据。Prometheus 提供了与处理时间序列数据、可视化数据、查询数据和基于指标数据发送警报相关的广泛功能。
- en: This metrics data might include a variety of time-series data for CPU usage,
    both for nodes and for pods; storage utilization; application health, as defined
    by readiness probes; and other application-specific metrics. Prometheus uses a
    pull model where it polls endpoints for numeric data. Pods, DaemonSets, and other
    Kubernetes resources supporting Prometheus use annotations to advertise that Kubernetes
    should scrape them for metrics data via HTTP, usually via a `/metrics` endpoint.
    This can include data from Nodes, surfaced through a DaemonSet called `node_exporter`
    that runs on each Node.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标数据可能包括多种时间序列数据，例如节点和 pod 的 CPU 使用情况；存储利用率；由就绪探针定义的应用程序健康状况；以及其他特定于应用程序的指标。Prometheus
    使用拉取模型，它会定期轮询端点以获取数值数据。支持 Prometheus 的 Pods、DaemonSets 以及其他 Kubernetes 资源通过注解声明
    Kubernetes 应该通过 HTTP 从它们那里抓取指标数据，通常是通过 `/metrics` 端点。这可能包括来自节点的数据，通过一个名为 `node_exporter`
    的 DaemonSet 来暴露该数据，该 DaemonSet 在每个节点上运行。
- en: It stores the metrics data it receives by associating this data with a metric
    name and a set of labels in key-value pair format, along with a millisecond-resolution
    timestamp. This labeling allows both efficient storage and the querying of the
    metrics in a time-series database. System operators and automated systems can
    then query this database to investigate the system's health and performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过将接收到的指标数据与指标名称以及键值对格式的标签集合关联，并附带毫秒分辨率的时间戳来存储这些数据。这种标签方式不仅允许高效存储，还可以在时间序列数据库中查询这些指标。系统管理员和自动化系统可以查询此数据库，以调查系统的健康状况和性能。
- en: It not only provides a time-series database for metrics but also an alerting
    subsystem so that system operators can proactively take action when applications
    encounter trouble.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅提供了一个时间序列数据库用于存储指标，还提供了一个警报子系统，帮助系统管理员在应用程序遇到问题时主动采取措施。
- en: You can read more about the overall Prometheus architecture and its feature
    set at [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)了解更多关于
    Prometheus 的整体架构和功能。
- en: Prometheus' history
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus 的历史
- en: While Prometheus was originally developed by SoundCloud in 2012, it became a
    **Cloud Native Computing Foundation** (**CNCF**) top-level project in 2016 and
    it is independent of any single company, just like Kubernetes itself. Its design
    is inspired by Google's Borgmon system.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Prometheus 最初是由 SoundCloud 于 2012 年开发的，但它在 2016 年成为了**云原生计算基金会**（**CNCF**）的顶级项目，并且它独立于任何单一公司，就像
    Kubernetes 本身一样。它的设计灵感来自 Google 的 Borgmon 系统。
- en: Exploring Prometheus through its query and graph web interface
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过查询和图形网页界面探索 Prometheus
- en: 'If you installed an EKS cluster using the AWS EKS Quick Start CloudFormation
    templates as described in [*Chapter 8*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157),
    *Deploying Docker Apps to Kubernetes*, you should already have a working Prometheus
    application. If not, you can follow the instructions here to install it using
    Helm:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照[*第 8 章*](B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157)中描述的方式，使用 AWS
    EKS 快速启动 CloudFormation 模板安装了 EKS 集群，*部署 Docker 应用到 Kubernetes*，你应该已经有一个可以工作的
    Prometheus 应用程序。如果没有，你可以按照此处的说明使用 Helm 安装它：
- en: '[https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html](https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html](https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html)'
- en: 'You can connect to the Prometheus service and start exploring it by using `kubectl`
    to create a port forwarding proxy to the Prometheus console web application. You
    should connect the `prometheus-server` Kubernetes service to your local workstation
    as follows (replace the expression after `use-context` with your AWS EKS cluster
    ARN):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`kubectl`创建一个端口转发代理连接到 Prometheus 服务并开始探索它。你应该按如下方式将`prometheus-server`
    Kubernetes 服务连接到本地工作站（将`use-context`后的表达式替换为你的 AWS EKS 集群 ARN）：
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, open a web browser and visit `http://localhost:9090/`, and you will see
    the Prometheus query console.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开一个网页浏览器并访问`http://localhost:9090/`，你将看到 Prometheus 查询控制台。
- en: A good starter query to use to test Prometheus is the `node_load1` term, which
    shows the 1-minute load averages of the underlying Kubernetes nodes. Enter that
    into the query field and hit the **Execute** button, and then activate the **Graph**
    tab. You will see a graph showing those load averages.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的初学者查询是使用`node_load1`，它显示了底层 Kubernetes 节点的 1 分钟负载平均值。将其输入查询字段并点击**执行**按钮，然后激活**图形**选项卡。你将看到一个显示负载平均值的图表。
- en: The **Prometheus query language** is called **PromQL** and is quite different
    from other time-series database query languages. You will need to learn more about
    PromQL to formulate your own queries. Read more about that at [https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085](mailto:https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prometheus 查询语言**叫做**PromQL**，它与其他时间序列数据库查询语言非常不同。你需要学习更多关于 PromQL 的内容，以便编写自己的查询。在[https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085](mailto:https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085)上阅读更多相关内容。'
- en: While Prometheus can graph query results on its own, Kubernetes users typically
    use Grafana in conjunction with Prometheus to provide more sophisticated graphs
    and dashboards. We will explore Grafana further later in this chapter. Next, let's
    examine how you might add a Prometheus metric to an application.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Prometheus 可以独立绘制查询结果的图表，但 Kubernetes 用户通常会将 Grafana 与 Prometheus 一起使用，以提供更复杂的图表和仪表盘。我们将在本章后面深入探讨
    Grafana。接下来，我们将研究如何将 Prometheus 指标添加到应用程序中。
- en: Adding Prometheus metrics to an application
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向应用程序添加 Prometheus 指标
- en: In order to integrate an application with Prometheus, you need to expose a specially
    structured HTTP API via a Prometheus client library. Prometheus offers official
    client libraries for several languages, and the community has created many other
    client libraries for different languages. You can read more about the general
    process in the Prometheus documentation at [https://prometheus.io/docs/instrumenting/clientlibs/](https://prometheus.io/docs/instrumenting/clientlibs/).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将应用程序与 Prometheus 集成，您需要通过 Prometheus 客户端库暴露一个特殊结构的 HTTP API。Prometheus 为多种语言提供官方客户端库，社区也为其他语言创建了许多客户端库。您可以在
    Prometheus 文档中阅读更多关于一般过程的内容，链接为 [https://prometheus.io/docs/instrumenting/clientlibs/](https://prometheus.io/docs/instrumenting/clientlibs/)。
- en: 'To demonstrate this integration, the version of ShipIt Clicker in this chapter
    exposes both a set of default metrics and a custom metric in the form of a counter,
    labeled `shipitclicker_deployments_total`. To do this, we integrate the Prometheus
    client for JavaScript applications using Node.js:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个集成，本章中的 ShipIt Clicker 版本暴露了默认指标集和一个名为 `shipitclicker_deployments_total`
    的自定义计数器指标。为了做到这一点，我们通过 Node.js 集成了用于 JavaScript 应用程序的 Prometheus 客户端：
- en: '[https://github.com/siimon/prom-client](https://github.com/siimon/prom-client)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/siimon/prom-client](https://github.com/siimon/prom-client)'
- en: To perform the integration, we installed and saved the prom-client Node module
    with an `npm install prom-client --save` command, and then integrated the client
    loosely following the provided example code at [https://github.com/siimon/prom-client/blob/master/example/server.js](https://github.com/siimon/prom-client/blob/master/example/server.js).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行集成，我们通过 `npm install prom-client --save` 命令安装并保存了 prom-client Node 模块，然后根据提供的示例代码，松散地集成了客户端，参考链接为
    [https://github.com/siimon/prom-client/blob/master/example/server.js](https://github.com/siimon/prom-client/blob/master/example/server.js)。
- en: The structure of a metrics-enabled ShipIt Clicker program
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用指标的 ShipIt Clicker 程序结构
- en: 'The Prometheus metrics publishing code in ShipIt Clicker is organized conventionally
    for a Node application written with the Express framework, with routes for metrics
    added to the main router in `chapter10/src/server/routes.js` in the same modular
    pattern as the routes for the game API. The main route imports `chapter10/src/server/api/controllers/metrics/router.js`,
    which defines the HTTP routes for `/metrics` and a special route for `/metrics/shipitclicker_deployment_total`,
    using the controller class defined in `chapter10/src/server/api/controllers/metrics/controller.js`.
    This controller has methods that integrate with a Prometheus service class defined
    in `chapter10/src/server/api/services/prometheus.service.js`, which integrates
    with the `prom-client` library and exposes both the default metrics and the custom
    `shipitclicker_deployments_total` metric. Refer to the following code excerpt
    from the service to see how we encapsulate the `prom-client` library:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ShipIt Clicker 中的 Prometheus 指标发布代码在结构上遵循了使用 Express 框架编写的 Node 应用程序的常规方式，指标路由被添加到与游戏
    API 路由相同的模块化模式中的主路由 `chapter10/src/server/routes.js`。主路由导入了 `chapter10/src/server/api/controllers/metrics/router.js`，该路由定义了
    `/metrics` 和特殊路由 `/metrics/shipitclicker_deployment_total` 的 HTTP 路由，并使用 `chapter10/src/server/api/controllers/metrics/controller.js`
    中定义的控制器类。该控制器包含与位于 `chapter10/src/server/api/services/prometheus.service.js` 中定义的
    Prometheus 服务类集成的方法，该服务类与 `prom-client` 库集成并暴露了默认指标以及自定义的 `shipitclicker_deployments_total`
    指标。请参考以下服务代码摘录，了解我们如何封装 `prom-client` 库：
- en: '[PRE14]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The controller classes that serve up the metrics have proper exception-handling
    and error-logging scaffolding that the baseline example from `prom-client` lacks.
    If you wanted to, you could easily adapt the router, controller, and service classes
    to a new application with minimal effort.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提供指标的控制器类具有适当的异常处理和错误日志记录框架，而 `prom-client` 的基础示例中缺少这些功能。如果您愿意，您可以轻松地将路由器、控制器和服务类适配到新应用程序中，几乎不需要额外的工作。
- en: 'In order to simplify troubleshooting, the metrics are bound to the same HTTP
    port as the rest of the application: port `3000`. This means that you can retrieve
    the metrics from any installed version of ShipIt Clicker that has this code integrated
    – for example, from [https://shipit-v7.eks.example.com/metrics](https://shipit-v7.eks.example.com/metrics)
    (replace `example.com` with your domain name). You should see a long list of metrics,
    starting with the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化故障排除，度量指标与应用程序的其余部分绑定在同一个 HTTP 端口：端口 `3000`。这意味着你可以从任何安装了 ShipIt Clicker
    并集成了此代码的版本中获取度量指标——例如，从 [https://shipit-v7.eks.example.com/metrics](https://shipit-v7.eks.example.com/metrics)（将
    `example.com` 替换为你的域名）。你应该会看到一长串度量指标，以下列内容为开头：
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that we have seen the raw metrics, let's examine how the configuration that
    allows Prometheus to discover the demo application works.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了原始度量指标，让我们检查一下让 Prometheus 发现演示应用程序的配置是如何工作的。
- en: Getting Prometheus to discover the ShipIt Clicker application
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让 Prometheus 发现 ShipIt Clicker 应用程序
- en: 'The installation of Prometheus configured through the AWS EKS Quick Start CloudFormation
    template is configured to perform service discovery of pods that support Prometheus
    metrics. In order for your Kubernetes pods to be discovered, they must be annotated
    with Prometheus-specific metadata, including the `prometheus.io/scrape: "true"`
    annotation. Refer to `chapter10/shipitclicker/template/deployment.yaml` for the
    annotations used to expose ShipIt Clicker to Prometheus:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '通过 AWS EKS 快速启动 CloudFormation 模板配置的 Prometheus 安装已配置为执行支持 Prometheus 度量指标的
    Pod 的服务发现。为了使你的 Kubernetes Pod 被发现，它们必须带有 Prometheus 特定的元数据注释，包括 `prometheus.io/scrape:
    "true"` 注释。请参阅 `chapter10/shipitclicker/template/deployment.yaml`，了解用于将 ShipIt
    Clicker 暴露给 Prometheus 的注释：'
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As long as these annotations are on the pod, Prometheus will know that it must
    scrape the pod's `/metrics` endpoint for data.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 只要这些注释存在于 Pod 上，Prometheus 就会知道必须抓取 Pod 的 `/metrics` 端点以获取数据。
- en: Now that we have seen how the program and its configuration templates have been
    extended to support Prometheus metrics, let's query Prometheus for the custom
    metric.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到程序及其配置模板是如何扩展以支持 Prometheus 度量指标的，让我们查询 Prometheus 以获取自定义度量指标。
- en: Querying Prometheus for a custom metric
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询 Prometheus 以获取自定义度量指标
- en: Play the game deployed at [https://shipit-v7.eks.example.com/](https://shipit-v7.eks.example.com/)
    for a minute or two (replace `example.com` with your domain name). Then, connect
    to the Prometheus console using the port forwarding method explained earlier in
    this chapter, and issue a query for `shipitclicker_deployments_total`, then switch
    to the `Graph` tab. You should see a graph that shows an increasing number of
    deployments over time.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://shipit-v7.eks.example.com/](https://shipit-v7.eks.example.com/) 上玩游戏一两分钟（将
    `example.com` 替换为你的域名）。然后，使用本章前面解释的端口转发方法连接到 Prometheus 控制台，并发出查询 `shipitclicker_deployments_total`，然后切换到
    `Graph` 标签。你应该会看到一张图表，显示随着时间推移，部署数量不断增加。
- en: If you keep playing the game and keep re-issuing the query in the Prometheus
    console, you will see the number of deployments go up. The default scrape interval
    and targets that Prometheus uses are defined in a `prometheus.yml` file embedded
    in the `prometheus-server` ConfigMap in the `prometheus` namespace. By default,
    it is set to `30` seconds, so you will not see instantaneous changes in the query
    results from Prometheus.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续玩游戏并在 Prometheus 控制台中持续重新发出查询，你将看到部署数量增加。Prometheus 使用的默认抓取间隔和目标定义在 `prometheus.yml`
    文件中，该文件嵌入在 `prometheus-server` ConfigMap 中，位于 `prometheus` 命名空间内。默认情况下，抓取间隔设置为
    `30` 秒，因此你不会看到 Prometheus 查询结果的即时变化。
- en: Next, let's explore Prometheus' support for alerts.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索 Prometheus 对警报的支持。
- en: Configuring Prometheus alerts
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Prometheus 警报
- en: Prometheus has the capability to query itself periodically in order to detect
    important conditions – this is the basis of the alerts system. You can apply the
    powerful Prometheus query language to detect when parts of your system that have
    Prometheus metrics are overloaded, responding too slowly, or are not available.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 具有定期自我查询的能力，以检测重要的条件——这是警报系统的基础。你可以应用强大的 Prometheus 查询语言来检测系统中具有
    Prometheus 度量指标的部分是否过载、响应过慢或不可用。
- en: For most applications, the foundational alert item must answer the question
    *is the application available*? If the application is up, it is ready and available
    to serve user requests. Prometheus has a metric called `up` that can help answer
    that question – it will have a value of `1` if the service is up, and `0` if it
    is down. If you query Prometheus for `up`, you will see the basic availability
    status of every service it monitors. You might want to raise an alert if any service
    has a value other than `1` for 5 minutes or more. That is the basic example given
    in the Prometheus documentation for alerts (refer to [https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)).
    Next, we will show how to add the example `InstanceDown` rule from the documentation
    to our Prometheus service configuration.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数应用程序，基础告警项必须回答的问题是 *应用程序是否可用*？如果应用程序正在运行，它就准备好为用户请求提供服务。Prometheus 有一个名为
    `up` 的指标，可以帮助回答这个问题 – 如果服务可用，它的值为 `1`，如果不可用，则为 `0`。如果你查询 Prometheus 中的 `up`，你将看到它监控的每个服务的基本可用性状态。如果任何服务的值不是
    `1` 且持续 5 分钟或更长时间，你可能会想要触发告警。这是 Prometheus 文档中给出的告警基础示例（参见 [https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)）。接下来，我们将展示如何将文档中的
    `InstanceDown` 示例规则添加到我们的 Prometheus 服务配置中。
- en: The AWS EKS Quick Start templates have a Prometheus installation that has no
    alerts defined at the start, so we will have to define one or more ourselves.
    If you installed Prometheus on your local workstation, you would edit configuration
    files in the `/etc` directory to do this, and then trigger a configuration file
    reload. However, in a Kubernetes setup, there has to be another mechanism in place
    to allow the editing of these values.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EKS 快速启动模板中的 Prometheus 安装最初没有定义告警，因此我们需要自己定义一个或多个告警。如果你在本地工作站上安装了 Prometheus，你将编辑
    `/etc` 目录中的配置文件来完成此操作，然后触发配置文件的重新加载。然而，在 Kubernetes 设置中，必须有另一个机制来允许编辑这些值。
- en: 'The AWS EKS Quick Start Prometheus setup uses a Kubernetes ConfigMap in the
    `prometheus` namespace called `prometheus-service` that has multiple embedded
    YAML configuration files defined within it, and a container running in each Prometheus
    server pod (refer to [https://github.com/jimmidyson/configmap-reload](https://github.com/jimmidyson/configmap-reload))
    that monitors the ConfigMap files for changes and then sends an HTTP `POST` to
    the Prometheus server running in the pod to get it to reload the changes. The
    ConfigMap files are updated once per minute inside the pods. The editing cycle
    for making config changes to alerts looks like this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EKS 快速启动 Prometheus 设置使用了一个位于 `prometheus` 命名空间中的 Kubernetes ConfigMap，名为
    `prometheus-service`，该 ConfigMap 内嵌了多个 YAML 配置文件，并且在每个 Prometheus 服务器 Pod 中运行一个容器（参见
    [https://github.com/jimmidyson/configmap-reload](https://github.com/jimmidyson/configmap-reload)），该容器监控
    ConfigMap 文件的变化，并向 Pod 中运行的 Prometheus 服务器发送 HTTP `POST` 请求，触发其重新加载更改。ConfigMap
    文件每分钟更新一次。编辑告警配置的周期如下所示：
- en: Edit the `prometheus-service` ConfigMap using `kubectl`.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `kubectl` 编辑 `prometheus-service` ConfigMap。
- en: Wait 1 minute for the ConfigMap changes to propagate to the pods.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待 1 分钟，直到 ConfigMap 更改传播到 Pods。
- en: View the alerts via the port-forwarded Prometheus console at `http://localhost:9090/alerts`.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过端口转发的 Prometheus 控制台查看告警，地址为 `http://localhost:9090/alerts`。
- en: 'In order to add the monitoring, we run the following command to edit the ConfigMap
    and add the rules under the `alerts:` stanza, as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了添加监控，我们运行以下命令来编辑 ConfigMap，并在 `alerts:` 部分下添加规则，如下所示：
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Look at the top of the file and make the `alerts:` stanza match the following
    text:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 查看文件顶部，并使`alerts:`部分与以下文本匹配：
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After you have edited the file, save it and it will propagate to the pods within
    1 minute.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑完文件后，保存它，文件将在 1 分钟内传播到 Pods。
- en: Troubleshooting note – YAML format files are exacting
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除提示 – YAML 格式文件要求严格
- en: The capitalization and spacing in the `prometheus-server` pods) – or worse,
    a silent failure to add the alert you intended.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`prometheus-server` Pods 中的大小写和空格问题）– 或者更糟糕的是，无法添加你打算设置的告警。'
- en: 'You should then be able to see the alert definition in the Prometheus console
    in the **Alerts** section; click on **InstanceDown** and it should show the alert
    definition:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你应该能在 Prometheus 控制台的 **Alerts** 部分看到告警定义；点击 **InstanceDown**，应该能显示告警定义：
- en: '![](img/B11641_10_003.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_003.jpg)'
- en: Figure 10.3 – Prometheus alerts showing InstanceDown
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 显示 `InstanceDown` 的 Prometheus 告警
- en: Now that you have an alert defined, you can configure Prometheus to send notifications
    based on the alert.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经定义了警报，你可以配置 Prometheus 根据该警报发送通知。
- en: Sending notifications with the Prometheus Alertmanager
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Prometheus Alertmanager 发送通知
- en: One of the most powerful aspects of Prometheus is its support for sending notifications
    of alerts, powered by a component called **Alertmanager**. This component takes
    the raw alert information from Prometheus, performs additional processing on it,
    and then sends notifications. You can find an in-depth overview of Prometheus
    alerting at [https://prometheus.io/docs/alerting/overview/](https://prometheus.io/docs/alerting/overview/).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 最强大的功能之一就是它支持通过名为 **Alertmanager** 的组件发送警报通知。该组件从 Prometheus 获取原始警报信息，进行进一步处理，然后发送通知。你可以在
    [https://prometheus.io/docs/alerting/overview/](https://prometheus.io/docs/alerting/overview/)
    找到 Prometheus 警报的详细概述。
- en: This alerting system supports multiple channels, including email, PagerDuty,
    Pushover, Slack, and more through webhooks. We are going to configure a Slack
    integration to demonstrate sending an alert. In order to do this, we are going
    to alter the Alertmanager configuration, which is stored in a Kubernetes ConfigMap
    called `prometheus-alertmanager`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 该警报系统支持多个渠道，包括电子邮件、PagerDuty、Pushover、Slack 等，均通过 webhooks 实现。我们将配置一个 Slack
    集成，演示如何发送警报。为此，我们将修改存储在名为 `prometheus-alertmanager` 的 Kubernetes ConfigMap 中的
    Alertmanager 配置。
- en: 'To add the Slack integration, make sure you have a Slack account that is signed
    in via a web browser, then go to [https://api.slack.com/](https://api.slack.com/)
    and build a new app for Slack. In the **Features** configuration screen, configure
    a new incoming webhook and select a channel in Slack to receive the notifications.
    Then, copy the URL of the incoming hook to the clipboard and store it in a local
    text file. You will need that when you configure Alertmanager. Configure any other
    settings that you feel are relevant, including an icon for the Slack integration.
    Then, edit the ConfigMap for the Alertmanager using the following command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加 Slack 集成，确保你有一个通过浏览器登录的 Slack 帐户，然后访问 [https://api.slack.com/](https://api.slack.com/)，为
    Slack 创建一个新应用。在 **Features** 配置屏幕中，配置一个新的 incoming webhook，并选择一个 Slack 渠道接收通知。然后，将
    incoming hook 的 URL 复制到剪贴板，并将其存储在本地文本文件中。配置 Alertmanager 时需要用到它。配置你认为相关的其他设置，包括
    Slack 集成的图标。然后，使用以下命令编辑 Alertmanager 的 ConfigMap：
- en: '[PRE19]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The ConfigMap will have an empty `{}` clause for the `global:` section, which
    we will remove, and then we add `slack_api_url` and the `slack_configs` section,
    as follows (replace the value in single quotes for the Slack API URL with your
    incoming webhook URL from the Slack application, and replace the channel with
    the hashtag name of your Slack channel where alerts should appear):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMap 中将会有一个空的 `{}` 子句用于 `global:` 部分，我们将删除它，然后添加 `slack_api_url` 和 `slack_configs`
    部分，具体如下（将单引号中的值替换为来自 Slack 应用的 incoming webhook URL，并将频道替换为 Slack 频道的 hashtag
    名称，警报将在该频道中显示）：
- en: '[PRE20]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will give you a very basic alerting setup that you can expand on in order
    to get notified of downtime. You can test that the Alertmanager is hooked up by
    sending a test alert via the Prometheus Alertmanager API. First, port-forward
    the Alertmanager service to your local machine:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供一个非常基础的警报设置，你可以在此基础上扩展，获取停机的通知。你可以通过 Prometheus Alertmanager API 发送测试警报来测试
    Alertmanager 是否已经连接。首先，将 Alertmanager 服务的端口转发到本地机器：
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In a different console window, issue the following command:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个控制台窗口中，输入以下命令：
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should get a `{"status":"success"}` response from that `curl` command,
    and then you should see the `Hello World` alert in your Slack:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该从 `curl` 命令中收到 `{"status":"success"}` 响应，然后你应该在你的 Slack 中看到 `Hello World`
    警报：
- en: '![](img/B11641_10_004.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_004.jpg)'
- en: Figure 10.4 – Prometheus alert in Slack
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – Prometheus 警报在 Slack 中
- en: Exercise – deploy a broken ShipIt Clicker, expect an AlertManager notification
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 - 部署一个故障的 ShipIt Clicker，期待 AlertManager 通知
- en: Edit the `chapter10/shipitclicker/templates/deployment.yaml`file to redirect
    Prometheus probes to port `3001` and deploy this broken ShipIt Clicker application
    using Helm to see the alerting in action. Call the application `shipit-broken`.
    Check the Prometheus console to verify that the alert enters the pending state.
    This should happen in less than 1 minute. Within 10 minutes, you should see an
    alert in Slack of the `[FIRING:1] (InstanceDown shipit-broken shipitclicker 10.0.87.39:3000
    kubernetes-pods default shipit-broken-shipitclicker-6658f47599-pkxwk 6658f47599
    page)` form. Once you get the alert, uninstall the`shipit-broken` Helm Chart,
    revert the change to `deployment.yaml`, and you should stop getting notifications
    about that specific issue.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 `chapter10/shipitclicker/templates/deployment.yaml` 文件，将 Prometheus 探针重定向到端口
    `3001`，并使用 Helm 部署这个损坏的 ShipIt Clicker 应用程序，以查看警报的实际效果。将应用程序命名为 `shipit-broken`。检查
    Prometheus 控制台，验证警报进入待处理状态。这应该在 1 分钟以内发生。10 分钟内，您应该会在 Slack 上看到类似 `[FIRING:1]
    (InstanceDown shipit-broken shipitclicker 10.0.87.39:3000 kubernetes-pods default
    shipit-broken-shipitclicker-6658f47599-pkxwk 6658f47599 page)` 的警报。收到警报后，卸载 `shipit-broken`
    Helm Chart，恢复 `deployment.yaml` 文件的更改，您应该不再收到该特定问题的通知。
- en: Once you get the alert, uninstall the `shipit-broken` Helm Chart and you should
    stop getting notifications about that specific issue.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收到警报，卸载 `shipit-broken` Helm Chart，您应该不再收到关于该特定问题的通知。
- en: Exploring Prometheus queries and external monitoring in-depth
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探索 Prometheus 查询和外部监控
- en: The topics about how to build Prometheus queries and how to extend Prometheus
    to monitor external systems are quite deep and beyond the scope of this chapter.
    Please consult the Prometheus documentation and the links in the *Further reading*
    section at the end of this chapter to learn more about creating Prometheus queries
    and configuring it to use additional metrics data sources.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何构建 Prometheus 查询以及如何扩展 Prometheus 以监控外部系统的话题非常深入，超出了本章的范围。请查阅 Prometheus
    文档以及本章末尾的 *进一步阅读* 部分，了解更多关于创建 Prometheus 查询和配置它以使用额外的指标数据源的信息。
- en: Next, let's examine how we can use Grafana to visualize the data that Prometheus
    gathers.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用 Grafana 来可视化 Prometheus 收集的数据。
- en: Visualizing operational data with Grafana
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Grafana 可视化操作数据
- en: Prometheus is often deployed with Grafana ([https://grafana.com/](https://grafana.com/))
    to provide sophisticated dashboards and a more sophisticated UI for monitoring.
    The installation of Kubernetes from the AWS EKS Quick Start includes Grafana,
    configured with a few dashboards. Let's explore the Grafana installation and see
    how it integrates with Prometheus.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 通常与 Grafana 一起部署（[https://grafana.com/](https://grafana.com/)），以提供复杂的仪表板和更精密的监控
    UI。AWS EKS 快速启动中安装的 Kubernetes 包含了 Grafana，并配置了一些仪表板。让我们来探索 Grafana 安装并看看它是如何与
    Prometheus 集成的。
- en: Gaining access to Grafana
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 Grafana 访问权限
- en: 'The Grafana installation is exposed by default over a Kubernetes LoadBalancer,
    which in EKS creates an AWS EC2-Classic `EXTERNAL-IP` field for the actual DNS
    name of the ELB:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 安装默认通过 Kubernetes LoadBalancer 暴露，在 EKS 中，它会为实际的 ELB DNS 名称创建一个 AWS
    EC2-Classic `EXTERNAL-IP` 字段：
- en: '[PRE23]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Put that DNS address into your web browser, prefixed with `http://`, and you
    will get the Grafana login screen. You will need to retrieve the administrative
    username and password from the Kubernetes secret to login:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将该 DNS 地址放入您的网页浏览器中，前缀加上 `http://`，您将看到 Grafana 登录界面。您需要从 Kubernetes 密钥中获取管理员用户名和密码才能登录：
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Use these values to log in to the Grafana console. You can then explore the
    UI, including the dashboards and the Prometheus query explorer. Some of the dashboards
    might not have values fully populated, such as the **Kubernetes All Nodes** dashboard,
    but don't fret too much about it, as it is possible to add community-provided
    dashboards that are extremely detailed and fully populated with cluster-wide statistics.
    Look at the **Kubernetes Pods** dashboard and select different pods, including
    the Redis pods and the ShipIt Clicker pod, to get a feel for how to use the dashboards.
    Change the time window with the widget in the upper-right corner to show data
    for a day or a week, and then click and drag over an interesting area to zoom
    in.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值登录到 Grafana 控制台。然后，您可以探索 UI，包括仪表板和 Prometheus 查询浏览器。某些仪表板可能没有完全填充数据，例如
    **Kubernetes All Nodes** 仪表板，但无需过于担心，因为可以添加社区提供的详细且完全填充集群统计数据的仪表板。查看 **Kubernetes
    Pods** 仪表板并选择不同的 Pod，包括 Redis Pod 和 ShipIt Clicker Pod，以了解如何使用仪表板。使用右上角的控件更改时间窗口，以显示一天或一周的数据，然后点击并拖动感兴趣的区域进行缩放。
- en: Next, let's add a couple of community-provided dashboards to get a flavor for
    the full power that this system can deliver.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一些社区提供的仪表板，以便体验系统所能提供的全部功能。
- en: Adding a community-provided dashboard
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加社区提供的仪表板
- en: Grafana provides a repository of both official and community-provided dashboards
    at [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 提供了一个官方和社区提供的仪表板仓库，地址为 [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)。
- en: These include a bewildering variety of dashboards. You should explore this in
    detail with your own needs in mind.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仪表板包含种类繁多的选项。你应该根据自己的需求，详细探索这些仪表板。
- en: When you add a dashboard, one of the options presented is **Import**. Choose
    this and it will ask you for a dashboard ID or URL from the community site.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当你添加一个仪表板时，其中一个选项是 **Import**。选择这个选项后，它会要求你输入社区网站的仪表板 ID 或 URL。
- en: 'Here are four general-purpose dashboards that are worth adding to your installation:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是四个值得添加到你的安装中的通用仪表板：
- en: '**Cluster Monitoring for Kubernetes**: This compact dashboard from Pivotal
    Observatory lets you see what pods are consuming the most CPU, memory, and network
    resources at a glance – [https://grafana.com/grafana/dashboards/10000](https://grafana.com/grafana/dashboards/10000).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cluster Monitoring for Kubernetes**：这个来自 Pivotal Observatory 的紧凑型仪表板让你一眼就能看到哪些
    Pod 消耗了最多的 CPU、内存和网络资源 – [https://grafana.com/grafana/dashboards/10000](https://grafana.com/grafana/dashboards/10000)。'
- en: '**Kubernetes Cluster (Prometheus)**: A concise dashboard showing critical cluster-wide
    metrics – [https://grafana.com/grafana/dashboards/6417](https://grafana.com/grafana/dashboards/6417).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes Cluster (Prometheus)**：一个简洁的仪表板，展示了集群范围内的关键指标 – [https://grafana.com/grafana/dashboards/6417](https://grafana.com/grafana/dashboards/6417)。'
- en: '**1 Node Exporter for Prometheus Dashboard EN v20191102**: A cluster-wide complex
    dashboard that exposes many CPU, disk, and network metrics – [https://grafana.com/grafana/dashboards/11074](https://grafana.com/grafana/dashboards/11074).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1 Node Exporter for Prometheus Dashboard EN v20191102**：一个集群级的复杂仪表板，展示了许多
    CPU、磁盘和网络指标 – [https://grafana.com/grafana/dashboards/11074](https://grafana.com/grafana/dashboards/11074)。'
- en: '**Node Exporter Full**: This exposes every possible metric from the **Prometheus
    Node Exporter**, a very popular dashboard on the site with over two million downloads
    – [https://grafana.com/grafana/dashboards/1860](https://grafana.com/grafana/dashboards/1860).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Node Exporter Full**：这是一个展示 **Prometheus Node Exporter** 所有可能指标的仪表板，网站上非常受欢迎，下载量超过两百万次
    – [https://grafana.com/grafana/dashboards/1860](https://grafana.com/grafana/dashboards/1860)。'
- en: Adding a new dashboard with a custom query
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加一个自定义查询的新仪表板
- en: 'The steps to add a new dashboard with a custom query are as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 添加自定义查询的新仪表板的步骤如下：
- en: 'In the left menu, click on the **+** sign to add a new dashboard. Then, in
    the **New Panel** area, click **Add Query**. Add the following query to the field
    next to **Metrics**:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧菜单中，点击 **+** 图标以添加新的仪表板。然后，在 **New Panel** 区域，点击 **Add Query**。在 **Metrics**
    旁边的字段中添加以下查询：
- en: '[PRE25]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'It should look something like this:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它应该看起来像这样：
- en: '![](img/B11641_10_005.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B11641_10_005.jpg)'
- en: Figure 10.5 – Grafana custom dashboard item definition
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.5 – Grafana 自定义仪表板项目定义
- en: Then, in the `ShipIt Clicker Deployments`, and then click on the left-pointing
    arrow in the top-left corner of the screen to return to defining the widget.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在 `ShipIt Clicker Deployments` 中，点击屏幕左上角的左箭头，返回定义小部件的界面。
- en: In the top menu, click on the graph with the plus sign to add another widget:![](img/B11641_10_006.jpg)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在顶部菜单中，点击带有加号的图表以添加另一个小部件：![](img/B11641_10_006.jpg)
- en: Figure 10.6 – The Grafana add widget
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.6 – Grafana 添加小部件
- en: 'Add another similar panel with the following query with the title `ShipIt Clicker
    Deployments Rate`:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个类似的面板，使用以下查询并设置标题为 `ShipIt Clicker Deployments Rate`：
- en: '[PRE26]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Then, click on the gear icon in the top menu and change the name of the dashboard
    to `ShipIt Clicker Dashboard`, and then save the dashboard.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击顶部菜单中的齿轮图标，将仪表板的名称更改为 `ShipIt Clicker Dashboard`，然后保存仪表板。
- en: 'Next, take a break and play the ShipIt Clicker game for a few minutes. This
    will generate traffic that you will be able to see on the graph. A few minutes
    after you stop playing, your dashboard might look like this:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，休息一下，玩几分钟的 ShipIt Clicker 游戏。这将生成你可以在图表中看到的流量。停止游戏几分钟后，你的仪表板可能会看起来像这样：
- en: '![](img/B11641_10_007.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_007.jpg)'
- en: Figure 10.7 – The ShipIt Clicker custom dashboard in Grafana
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – Grafana 中的 ShipIt Clicker 自定义仪表板
- en: Understanding rates and counters
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 理解速率和计数器
- en: Note that the rate dashboard drops back down to 0 after you stopped playing,
    but the one that counts only the total increases and stays as it is. Choosing
    a rate query for a variable ending in `total` in Prometheus is usually what you
    want to measure throughput.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在停止播放后，速率仪表板会回落到 0，但仅计算总数的仪表板会继续增加并保持不变。在 Prometheus 中选择以 `total` 结尾的速率查询通常是您想要测量吞吐量的方式。
- en: 'Now that we have seen how to graph application metrics and build dashboards
    with Grafana, let''s explore another topic: application performance monitoring
    and distributed tracing with Jaeger.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用 Grafana 绘制应用程序指标并构建仪表板，接下来让我们探索另一个主题：使用 Jaeger 进行应用程序性能监控和分布式追踪。
- en: Application performance monitoring with Jaeger
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Jaeger 进行应用程序性能监控
- en: We are now going to take a brief tour of Jaeger to see how it can be used for
    performance monitoring in a microservices architecture. One of the key problems
    faced when implementing performance and error tracking in a microservice architecture
    versus a monolithic application is that a microservices architecture is inherently
    a distributed environment.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将简要浏览 Jaeger，了解它如何在微服务架构中用于性能监控。在实现微服务架构与单体应用程序的性能和错误追踪时，面临的一个关键问题是，微服务架构本身就是一个分布式环境。
- en: Early attempts at solving this problem, such as OpenCensus ([https://opencensus.io/tracing/](https://opencensus.io/tracing/)),
    suffered from disparate terminology and approaches and incompatible systems. To
    solve this problem, the performance monitoring community created the OpenTracing
    API.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 早期尝试解决这个问题的工具，如 OpenCensus ([https://opencensus.io/tracing/](https://opencensus.io/tracing/))，由于使用了不同的术语、方法和不兼容的系统，面临了一些问题。为了克服这些问题，性能监控社区创建了
    OpenTracing API。
- en: Understanding the OpenTracing API
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 OpenTracing API
- en: The **OpenTracing** project ([https://opentracing.io/](https://opentracing.io/))
    is designed to allow engineers to add performance-monitoring features to their
    projects using a common API specification that is non-vendor specific.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenTracing** 项目 ([https://opentracing.io/](https://opentracing.io/)) 旨在允许工程师使用一种非特定厂商的通用
    API 规范，将性能监控功能添加到他们的项目中。'
- en: 'Some of the key features of OpenTracing that realize this goal are as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这个目标的 OpenTracing 的一些关键特性如下：
- en: The API specification itself ([https://github.com/opentracing/specification](https://github.com/opentracing/specification))
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 规范本身 ([https://github.com/opentracing/specification](https://github.com/opentracing/specification))
- en: Frameworks and libraries that implement the API specification
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 API 规范的框架和库
- en: Comprehensive documentation ([https://opentracing.io/docs/](https://opentracing.io/docs/))
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 综合文档 ([https://opentracing.io/docs/](https://opentracing.io/docs/))
- en: 'Let''s now look at the two most important core concepts of the specification:
    spans and tracing.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下这个规范的两个最重要的核心概念：span 和 tracing。
- en: Spans
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Span
- en: A span represents a unit of work and is the basic building block of this type
    of tracing system. Each span contains an operation name, the start and finish
    time, a **SpanContext**, and finally, **tags** and **logs** key-value pairs.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Span 代表一个工作单元，是这种追踪系统的基本构建块。每个 span 包含一个操作名称、开始和结束时间、一个 **SpanContext**，以及最后的
    **tags** 和 **logs** 键值对。
- en: Your tag key-value pairs apply to the whole span and include information such
    as `db.type` and `http.url`. A list of conventional tags can be found on GitHub
    at [https://github.com/opentracing/specification/blob/master/semantic_conventions.md](https://github.com/opentracing/specification/blob/master/semantic_conventions.md).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 您的标签键值对适用于整个 span，并包含诸如 `db.type` 和 `http.url` 等信息。常见标签的列表可以在 GitHub 上找到：[https://github.com/opentracing/specification/blob/master/semantic_conventions.md](https://github.com/opentracing/specification/blob/master/semantic_conventions.md)。
- en: The logs key-value pair is used to define logging messages that refer to a specific
    incident or event, rather than the span as a whole. For example, you could use
    this collection of key-value pairs to record debugging information.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 日志键值对用于定义与特定事件或事故相关的日志消息，而不是整个 span。例如，您可以使用这组键值对来记录调试信息。
- en: The final concept in a span is the SpanContext, which is used to carry data
    across process boundaries. Its two key components are the state that denotes a
    specific span within a trace and a concept known as **baggage items**. These are
    essentially key-value pairs that cross a process boundary.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Span 中的最终概念是 SpanContext，它用于跨进程边界传递数据。它的两个关键组成部分是表示 trace 中特定 span 的状态，以及被称为
    **baggage items** 的概念。这些本质上是跨进程边界的键值对。
- en: You can read more about spans at the OpenTracing website's documentation at
    [https://opentracing.io/docs/overview/spans/](https://opentracing.io/docs/overview/spans/).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 OpenTracing 网站的文档中阅读更多关于 spans 的内容，链接地址为 [https://opentracing.io/docs/overview/spans/](https://opentracing.io/docs/overview/spans/)。
- en: Traces and tracers
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Traces 和 tracers
- en: The next concept we will look at is traces and tracers.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将研究的概念是 traces 和 tracers。
- en: A trace is a way of grouping one or more spans under a single identifier known
    as the **trace identifier**. This can be used to understand a workflow through
    a distributed system, such as a microservices architecture.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Trace 是将一个或多个 spans 按照一个称为 **trace 标识符** 的单一标识符进行分组的方式。它可以用来理解分布式系统中的工作流，例如微服务架构。
- en: The tracer is the actual implementation of the OpenTracing API specification
    that collects spans and publishes them. Examples of tracers that implement OpenTracing
    include Datadog (which we will explore in [*Chapter 14*](B11641_14_Final_NM_ePub.xhtml#_idTextAnchor316),
    *Advanced Docker Security – Secrets, Secret Commands, Tagging, and Labels*), Instana,
    Lightstep, and Jaeger.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Tracer 是 OpenTracing API 规范的实际实现，它收集 spans 并发布它们。实现 OpenTracing 的一些 tracer 示例包括
    Datadog（我们将在 [*第14章*](B11641_14_Final_NM_ePub.xhtml#_idTextAnchor316), *高级 Docker
    安全性 – 密码、秘密命令、标签和标签* 中进行探索）、Instana、Lightstep 和 Jaeger。
- en: If you want to read more around tracers and traces, you can find the official
    documentation at [https://opentracing.io/docs/overview/tracers/](https://opentracing.io/docs/overview/tracers/).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想阅读更多关于 tracers 和 traces 的内容，可以在官方文档中找到，链接为 [https://opentracing.io/docs/overview/tracers/](https://opentracing.io/docs/overview/tracers/)。
- en: Let's explore a tool that implements the OpenTracing API – Jaeger.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一个实现了 OpenTracing API 的工具 —— Jaeger。
- en: Introduction to Jaeger
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jaeger 介绍
- en: Jaeger is an open source application-tracing framework that allows developers
    and system operators to gather information from a running application and determine
    both how the application spends its time and how it interacts with other distributed
    system components, using the OpenTracing API. The Jaeger website is [https://www.jaegertracing.io/](https://www.jaegertracing.io/).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 是一个开源的应用程序追踪框架，允许开发人员和系统操作员收集正在运行的应用程序的信息，并确定应用程序如何消耗时间以及它如何与其他分布式系统组件进行交互，使用的是
    OpenTracing API。Jaeger 网站地址为 [https://www.jaegertracing.io/](https://www.jaegertracing.io/)。
- en: Jaeger's history
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jaeger 的历史
- en: Jaeger, named after the German word for hunter, originally came from the transportation
    company Uber. Engineers there, led by Yuri Shkuro, built this distributed tracing
    framework. Inspired by the Google paper on their tracing framework, Dapper ([https://research.google/pubs/pub36356/](https://research.google/pubs/pub36356/)),
    and the Zipkin tracing framework ([https://zipkin.io/](https://zipkin.io/)), they
    created Jaeger as a cloud-native tracing framework. Uber has been using Jaeger
    since 2015 and contributed it to the CNCF in 2017; the CNCF promoted it to a top-level
    project in 2019\. You can read more about the history of Jaeger on the Uber engineering
    blog at [https://eng.uber.com/distributed-tracing/](https://eng.uber.com/distributed-tracing/).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger，源自德语中的“猎人”一词，最初由运输公司 Uber 开发。由 Yuri Shkuro 领导的工程师团队在此基础上构建了这个分布式追踪框架。受到
    Google 关于其追踪框架 Dapper（[https://research.google/pubs/pub36356/](https://research.google/pubs/pub36356/)）以及
    Zipkin 追踪框架（[https://zipkin.io/](https://zipkin.io/)）的论文启发，他们创造了 Jaeger 作为一个云原生追踪框架。Uber
    自 2015 年以来一直在使用 Jaeger，并于 2017 年将其贡献给 CNCF；CNCF 在 2019 年将其晋升为顶级项目。你可以在 Uber 工程博客上阅读更多关于
    Jaeger 历史的内容，链接为 [https://eng.uber.com/distributed-tracing/](https://eng.uber.com/distributed-tracing/)。
- en: Jaeger's components
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jaeger 的组件
- en: 'Some of the important components that make up the Jaeger ecosystem include
    the following:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 构成 Jaeger 生态系统的一些重要组件包括以下内容：
- en: The client libraries available as packages or directly from GitHub
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端库，可作为包或直接从 GitHub 获取
- en: Jaeger agents used to listen for spans
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaeger 代理用于监听 spans
- en: The collector, responsible for aggregating data sent from agents
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collector，负责汇总从代理发送的数据
- en: Jaeger query, for analyzing data via a UI
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaeger 查询，用于通过 UI 分析数据
- en: The Ingester, which allows us to gather data from Kafka topics and then write
    the data to services such as AWS Elasticsearch
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingester，它允许我们从 Kafka 主题收集数据，然后将数据写入 AWS Elasticsearch 等服务
- en: Let's test Jaeger and see how it works in practice.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试 Jaeger，看看它在实践中如何运作。
- en: Exploring the Jaeger UI
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 Jaeger UI
- en: 'To explore Jaeger, we can run the all-in-one latest image using Docker:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索 Jaeger，我们可以使用 Docker 运行最新的 all-in-one 镜像：
- en: '[PRE27]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we can open a web browser and visit `http://localhost:16686/` to see
    the UI. The Jaeger search interface itself is instrumented to send traces to the
    collector, so once you see the UI, reload the page once to make some more traces,
    and populate the **Service** drop-down box. Then, press the **Find Traces** button.
    It should look something like this:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以打开一个网页浏览器并访问`http://localhost:16686/`查看UI界面。Jaeger搜索界面本身已被设置为向收集器发送追踪信息，所以一旦看到UI界面，刷新页面一次以生成更多的追踪信息，并填充**服务**下拉框。接着，点击**查找追踪**按钮。它应该看起来像这样：
- en: '![](img/B11641_10_008.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_008.jpg)'
- en: Figure 10.8 – The Jaeger UI search interface
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – Jaeger UI搜索界面
- en: When you are done exploring, stop the running Docker container by pressing *Ctrl*
    + *C*. Next, lets explore how you might instrument an application by seeing how
    ShipIt Clicker is integrated with OpenTracing and Jaeger.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 探索完成后，通过按下*Ctrl* + *C*停止运行中的Docker容器。接下来，让我们探索如何通过查看ShipIt Clicker与OpenTracing和Jaeger的集成来为应用程序添加追踪功能。
- en: Exploring the Jaeger client with ShipIt Clicker
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ShipIt Clicker探索Jaeger客户端
- en: 'The Jaeger client is available in a number of languages. Our example will use
    Node.js, but there is also support for Go, Java, and Python, among others. You
    can check the official client documentation at the following URL to learn more:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger客户端支持多种编程语言。我们的示例将使用Node.js，但也支持Go、Java和Python等语言。你可以访问以下网址查看官方客户端文档以了解更多信息：
- en: '[https://www.jaegertracing.io/docs/1.18/client-libraries/](https://www.jaegertracing.io/docs/1.18/client-libraries/)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.jaegertracing.io/docs/1.18/client-libraries/](https://www.jaegertracing.io/docs/1.18/client-libraries/)'
- en: 'ShipIt Clicker v7 already has a Jaeger client, a piece of OpenTracing JavaScript
    Express middleware, and the OpenTracing API client installed:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ShipIt Clicker v7已经安装了Jaeger客户端、OpenTracing JavaScript Express中间件和OpenTracing
    API客户端：
- en: 'The Jaeger client: [https://github.com/jaegertracing/jaeger-client-node](https://github.com/jaegertracing/jaeger-client-node)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaeger客户端：[https://github.com/jaegertracing/jaeger-client-node](https://github.com/jaegertracing/jaeger-client-node)
- en: 'Express middleware: [https://github.com/opentracing-contrib/javascript-express](https://github.com/opentracing-contrib/javascript-express)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Express中间件：[https://github.com/opentracing-contrib/javascript-express](https://github.com/opentracing-contrib/javascript-express)
- en: 'The OpenTracing client: [https://github.com/opentracing/opentracing-javascript](
    https://github.com/opentracing/opentracing-javascript)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenTracing客户端：[https://github.com/opentracing/opentracing-javascript]( https://github.com/opentracing/opentracing-javascript)
- en: 'If you have an Express application that you want to use with Jaeger, you would
    issue the following command to install the same combination of libraries:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个希望与Jaeger一起使用的Express应用程序，你可以执行以下命令安装相同的库组合：
- en: '[PRE28]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the GitHub repository ([https://github.com/PacktPublishing/Docker-for-Developers](https://github.com/PacktPublishing/Docker-for-Developers)),
    the Jaeger client configuration in `chapter10/src/server/common/jaeger.js` has
    an example of how to configure the Jaeger client using a mixture of environment
    variables and default values. Both the `docker-compose` configuration files and
    the Helm templates for ShipIt Clicker have been updated to use some environment
    variables to configure Jaeger, to give `jaeger.js` the right context for those
    environments; this file imports the `jaeger-client` module, configures it, and
    exports a `tracer` object. We use the `tracer` object from the `express-opentracing`
    middleware in the `chaper10/src/server/common/server.js` file:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库中（[https://github.com/PacktPublishing/Docker-for-Developers](https://github.com/PacktPublishing/Docker-for-Developers)），`chapter10/src/server/common/jaeger.js`中的Jaeger客户端配置示例展示了如何使用环境变量和默认值的混合方式配置Jaeger客户端。`docker-compose`配置文件和ShipIt
    Clicker的Helm模板都已更新，使用一些环境变量来配置Jaeger，为`jaeger.js`提供适当的环境上下文；该文件导入了`jaeger-client`模块，进行配置，并导出`tracer`对象。我们在`chaper10/src/server/common/server.js`文件中的`express-opentracing`中间件使用了这个`tracer`对象：
- en: '[PRE29]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Using middleware or other software that can hook into common libraries processes
    provides us with lift and lets us avoid writing boilerplate code. The `express-opentracing`
    middleware object decorates the Express `res` response object with a `span` attribute,
    which lets us use an OpenTracing span from within our controllers and request
    handlers.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 使用中间件或其他可以与常见库进程挂钩的软件，能够为我们提供提升，并让我们避免编写样板代码。`express-opentracing`中间件对象为Express的`res`响应对象装饰了一个`span`属性，这使我们能够在控制器和请求处理程序中使用OpenTracing的跨度。
- en: 'We can use a more explicit style also, where we create the spans and log entries
    programmatically:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用更明确的风格，在其中程序化地创建跨度和日志条目：
- en: 'To see this in action, inspect the ShipItClicker''s API controller at `chapter10/src/server/api/controllers/games/controller.js`:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看实际操作，请检查 ShipItClicker 的 API 控制器`chapter10/src/server/api/controllers/games/controller.js`：
- en: '[PRE30]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next stanza shows how to create a tag in the span that holds more detailed
    tracing information:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一段代码展示了如何在 span 中创建一个标签，用于保存更详细的跟踪信息：
- en: '[PRE31]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code initializes a child span for Redis, using the main span
    through `req.span`. Then, we immediately call Redis, log the results, and finish
    the span:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上面的代码通过`req.span`使用主 span 初始化了一个子 span，用于 Redis。然后，我们立即调用 Redis，记录结果，并完成 span：
- en: '[PRE32]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we log a message in the span associated with the parent span:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在与父 span 关联的 span 中记录一条消息：
- en: '[PRE33]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we log the message using the regular logging mechanism and update the
    Prometheus custom metric if this request increments the `deploys` element:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用常规日志机制记录消息，并在此请求递增`deploys`元素时更新 Prometheus 自定义指标：
- en: '[PRE34]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If we get this far, the Redis request has been successful, and we can return
    a JSON response to the client:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们到达这里，Redis 请求已经成功，我们可以向客户端返回一个 JSON 响应：
- en: '[PRE35]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If the request fails – for example, if Redis is unavailable – we must carry
    out error processing. First, we construct a message that has the detailed error
    in it:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果请求失败——例如 Redis 无法访问——我们必须进行错误处理。首先，我们构造一条包含详细错误信息的消息：
- en: '[PRE36]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we log the error to both the OpenTracing span and our regular error log,
    and return a 404 Not Found HTTP response to the client:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将错误记录到 OpenTracing span 和常规错误日志中，并返回一个 404 Not Found HTTP 响应给客户端：
- en: '[PRE37]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The preceding code shows how you can use the tracer object to initiate a child
    span of the main span in `req.span`, and has logging elements that annotate both
    spans with the results of the Redis operation.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码展示了如何使用 tracer 对象在`req.span`中启动一个子 span，并包含记录 Redis 操作结果的日志元素。
- en: 'In order to make it easy to demonstrate the Jaeger integration, this chapter
    has a Docker Compose file, `chapter10/docker-compose.yml`, that integrates the
    ShipIt Clicker container, Redis, and Jaeger. You can run all of them by issuing
    the following commands from the `chapter10` directory:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便演示 Jaeger 集成，本章提供了一个 Docker Compose 文件`chapter10/docker-compose.yml`，它集成了
    ShipIt Clicker 容器、Redis 和 Jaeger。你可以通过在`chapter10`目录下执行以下命令来运行它们：
- en: '[PRE38]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can then visit `http://localhost:3010/` to play the ShipIt Clicker game
    for a minute to generate some traces, then visit `http://localhost:16686/` to
    see the Jaeger query interface in action. Query the `shipitclicker-v7` service,
    click on one of the traces in the graph, and then expand the two spans and the
    logs revealed within and you should see something like this:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以访问`http://localhost:3010/`，玩一分钟的 ShipIt Clicker 游戏以生成一些跟踪数据，再访问`http://localhost:16686/`查看
    Jaeger 查询界面的实际操作。查询`shipitclicker-v7`服务，点击图表中的一个跟踪记录，然后展开两个 spans 和其中显示的日志，你应该会看到类似这样的内容：
- en: '![](img/B11641_10_009.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B11641_10_009.jpg)'
- en: Figure 10.9 – Jaeger trace showing the ShipIt Clicker HTTP transaction and Redis
    spans
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 显示 ShipIt Clicker HTTP 事务和 Redis spans 的 Jaeger 跟踪
- en: In contrast to the `docker-compose.yml` file presented in [*Chapter 6*](B11641_06_Final_NM_ePub.xhtml#_idTextAnchor102),
    *Deploying Applications with Docker Compose*, the one in this chapter is deliberately
    set up for development, not as a production-hardened configuration. It exposes
    both the Redis and Jaeger ports for convenience, so it is not suitable for production
    use without additional hardening. However, this makes it very convenient for debugging
    and developing the application. You can even run the ShipIt Clicker application
    code on your local workstation by running `npm run dev` and have it connect to
    the Docker-hosted Redis and Jaeger services – which is probably the fastest way
    to try out changes.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 与[*第六章*](B11641_06_Final_NM_ePub.xhtml#_idTextAnchor102)《使用 Docker Compose 部署应用程序》中展示的`docker-compose.yml`文件相比，本章中的文件是专门为开发环境设置的，而不是作为生产环境的配置。它暴露了
    Redis 和 Jaeger 的端口，方便使用，因此在没有额外加固的情况下，不适合用于生产环境。然而，这使得它在调试和开发应用程序时非常方便。你甚至可以通过运行`npm
    run dev`在本地工作站上运行 ShipIt Clicker 应用代码，并让它连接到 Docker 托管的 Redis 和 Jaeger 服务——这可能是尝试更改最快的方式。
- en: You could also install Jaeger in Kubernetes, both to your local learning environment
    and to the AWS EKS Kubernetes cluster. To do that, we will use the Jaeger Operator.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 Kubernetes 中安装 Jaeger，既可以在本地学习环境中安装，也可以在 AWS EKS Kubernetes 集群中安装。为此，我们将使用
    Jaeger Operator。
- en: Installing the Jaeger Operator
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Jaeger Operator
- en: We've seen how we can use Jaeger locally through both a raw `docker` command
    and through `docker-compose`. Next, we will learn how to deploy Jaeger to Kubernetes.
    The Helm Charts for Jaeger ([https://github.com/jaegertracing/helm-charts](https://github.com/jaegertracing/helm-charts))
    are not fully supported, and they may have issues with Helm 3\. The Jaeger team
    is actively investing in Jaeger Operator as the primary method to install and
    maintain this system. A Kubernetes **Operator** is a special type of resource
    that orchestrates the installation and maintenance of a whole set of related objects
    and configurations, often comprising a complex distributed system.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何通过原始的`docker`命令和`docker-compose`在本地使用Jaeger。接下来，我们将学习如何将Jaeger部署到Kubernetes。Jaeger的Helm
    Charts（[https://github.com/jaegertracing/helm-charts](https://github.com/jaegertracing/helm-charts)）并没有完全得到支持，且在Helm
    3中可能会出现问题。Jaeger团队正在积极投资Jaeger Operator，作为安装和维护该系统的主要方法。Kubernetes **Operator**是一种特殊类型的资源，它协调整个相关对象和配置的安装和维护，通常涉及复杂的分布式系统。
- en: 'To deploy to a Kubernetes environment, we can use the following GitHub repository
    as a guide:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署到Kubernetes环境中，我们可以使用以下GitHub仓库作为指南：
- en: '[https://github.com/jaegertracing/jaeger-operator](https://github.com/jaegertracing/jaeger-operator)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jaegertracing/jaeger-operator](https://github.com/jaegertracing/jaeger-operator)'
- en: 'Use the set of `kubectl` commands listed there to install the operator namespace
    and the related Kubernetes objects for the Jaeger operator. Run not only the main
    `kubectl` commands but also the set of `kubectl` commands to give the operator
    cluster-wide permissions through a role binding. To get Jaeger to work smoothly
    with all the namespaces, edit the deployment and remove the value from the `WATCH_NAMESPACE`
    variable:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用那里的`kubectl`命令集来安装Operator命名空间及相关的Kubernetes对象。运行不仅是主要的`kubectl`命令，还包括一组`kubectl`命令，以通过角色绑定赋予Operator集群范围的权限。为了使Jaeger在所有命名空间中顺利运行，请编辑部署并从`WATCH_NAMESPACE`变量中移除值：
- en: '[PRE39]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The part of the file with `WATCH_NAMESPACE` should then look as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中包含`WATCH_NAMESPACE`部分的内容应该如下所示：
- en: '[PRE40]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now that you have done this, you can install a Jaeger Operator instance that
    will itself spin up the services, pods, and DaemonSets for Jaeger. An example
    Operator definition suitable for development or lightweight production use that
    deploys Jaeger using a DaemonSet on all nodes using only memory for trace storage
    is in `chapter10/jaeger.yaml`. Install it with `kubectl`:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，你可以安装Jaeger Operator实例，它将自动启动Jaeger的服务、Pod和DaemonSets。一个适用于开发或轻量级生产使用的示例Operator定义，使用DaemonSet在所有节点上部署Jaeger并仅使用内存存储跟踪数据，位于`chapter10/jaeger.yaml`文件中。使用`kubectl`进行安装：
- en: '[PRE41]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This will install all the required components, including a `jaeger-query` Ingress
    Controller that does not have any annotations, so the EKS cluster will not connect
    it to anything. See the `chapter10/jaeger-ingress.yaml` file for a version that
    has annotations to connect it to the internet with the ALB Ingress Controller.
    You can use the same basic procedures you used with other Kubernetes services
    and Route 53 to expose the Jaeger console from Kubernetes; or, you can leave it
    alone and connect to the Jaeger console only when you need to via port forwarding.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装所有必需的组件，包括一个没有任何注解的`jaeger-query` Ingress Controller，因此EKS集群将不会将其连接到任何内容。有关具有注解的版本，请参见`chapter10/jaeger-ingress.yaml`文件，该版本使用ALB
    Ingress Controller将其连接到互联网。你可以使用与其他Kubernetes服务和Route 53相同的基本程序，从Kubernetes暴露Jaeger控制台；或者，你也可以保持不变，仅在需要时通过端口转发连接到Jaeger控制台。
- en: If you are installing this on your local Kubernetes learning environment, you
    could alternatively add NGINX Ingress Controller annotations to the Ingress Controller.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本地的Kubernetes学习环境中进行安装，你还可以将NGINX Ingress Controller注解添加到Ingress Controller中。
- en: To further extend Jaeger, you might consider adding one of the storage backends,
    such as Cassandra or Elasticsearch, so that traces will persist beyond the lifetime
    of the Jaeger pod. We're going to leave it there with Jaeger, but feel free to
    explore it in more detail.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步扩展Jaeger，你可能考虑添加一个存储后端，如Cassandra或Elasticsearch，这样跟踪数据就可以在Jaeger Pod的生命周期之外持久化。我们将在此处暂时停止对Jaeger的讨论，但你可以自由深入探讨。
- en: Next, we will review what we have learned in this chapter.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾一下本章所学的内容。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned all about observability – how to perform logging
    and monitoring for Docker applications using both Kubernetes cloud-native approaches
    and using AWS services.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经了解了关于可观测性的一切——如何使用Kubernetes原生方法以及AWS服务对Docker应用进行日志记录和监控。
- en: You learned about decoupling applications from common services (such as Redis)
    to increase production-readiness. In order to aid troubleshooting and the analysis
    of application and system problems, you learned how to extend logging beyond the
    running containers in a Kubernetes cluster into AWS CloudWatch and S3, as well
    as how to query those log storage systems using both CloudWatch Insights and AWS
    Athena. You saw how you might add more sophisticated Kubernetes liveness and readiness
    probes to an application, and how to make error handling more robust.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习了如何将应用程序与常见服务（如 Redis）解耦，以提高生产就绪性。为了帮助故障排除和分析应用程序及系统问题，你学习了如何将日志扩展到 Kubernetes
    集群中运行的容器之外，进入 AWS CloudWatch 和 S3，以及如何使用 CloudWatch Insights 和 AWS Athena 查询这些日志存储系统。你还了解到如何为应用程序添加更复杂的
    Kubernetes 活跃性和就绪性探针，以及如何使错误处理更加健壮。
- en: Then, you learned how to collect detailed metrics from both the application
    and the supporting systems using Prometheus, how to query those metrics, and how
    to set up alerts with the Prometheus Alertmanager. Prometheus and Grafana go hand
    in hand; you discovered how to configure Grafana dashboards provided by the community
    and how to add a custom dashboard that shows application-specific metrics. Finally,
    you learned how to use Jaeger and the OpenTracing API to instrument an application
    with traces that give deep insight into the performance of an application by using
    both open source middleware and explicitly annotating the application.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你学习了如何使用 Prometheus 从应用程序和支持系统中收集详细的指标，如何查询这些指标，以及如何使用 Prometheus Alertmanager
    设置警报。Prometheus 和 Grafana 是相辅相成的；你发现了如何配置社区提供的 Grafana 仪表板，并如何添加一个显示应用程序特定指标的自定义仪表板。最后，你学习了如何使用
    Jaeger 和 OpenTracing API 为应用程序添加追踪，通过使用开源中间件并明确标注应用程序，深入了解应用程序的性能。
- en: In the next chapter, we will explore how we can scale out the application using
    autoscaling, protect it from overloading using Envoy and the circuit breaker pattern,
    and perform load testing using k6.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索如何通过自动扩展来扩展应用程序，如何使用 Envoy 和断路器模式保护它免于过载，以及如何使用 k6 进行负载测试。
- en: Further reading
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can explore the following resources to expand your knowledge of observability,
    Kubernetes logging, Prometheus monitoring, Grafana, Jaeger, and managing Kubernetes
    clusters:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以探索以下资源，以扩展你对可观测性、Kubernetes 日志、Prometheus 监控、Grafana、Jaeger 以及管理 Kubernetes
    集群的知识：
- en: 'Introduction to observability: [https://docs.honeycomb.io/learning-about-observability/intro-to-observability/](https://docs.honeycomb.io/learning-about-observability/intro-to-observability/).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可观测性简介：[https://docs.honeycomb.io/learning-about-observability/intro-to-observability/](https://docs.honeycomb.io/learning-about-observability/intro-to-observability/)。
- en: 'Manage your Kubernetes clusters in style with k9s – a quick and easy terminal
    interface similar to Midnight Commander that is an alternative to using `kubectl`
    to query and control a Kubernetes cluster: [https://k9scli.io/](https://k9scli.io/).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 k9s 风格管理你的 Kubernetes 集群 – 一个快速简便的终端界面，类似于 Midnight Commander，是使用 `kubectl`
    查询和控制 Kubernetes 集群的替代方案：[https://k9scli.io/](https://k9scli.io/)。
- en: 'Kail – Kubernetes'' log tail utility: [https://github.com/boz/kail](https://github.com/boz/kail).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kail – Kubernetes 日志尾部工具：[https://github.com/boz/kail](https://github.com/boz/kail)。
- en: 'Getting started with Athena: [https://docs.aws.amazon.com/athena/latest/ug/getting-started.html](https://docs.aws.amazon.com/athena/latest/ug/getting-started.html).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena 入门：[https://docs.aws.amazon.com/athena/latest/ug/getting-started.html](https://docs.aws.amazon.com/athena/latest/ug/getting-started.html)。
- en: 'Query data from S3 files using AWS Athena: [https://towardsdatascience.com/query-data-from-s3-files-using-aws-athena-686a5b28e943](https://towardsdatascience.com/query-data-from-s3-files-using-aws-athena-686a5b28e943).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Athena 查询 S3 文件中的数据：[https://towardsdatascience.com/query-data-from-s3-files-using-aws-athena-686a5b28e943](https://towardsdatascience.com/query-data-from-s3-files-using-aws-athena-686a5b28e943)。
- en: 'Getting started with Kubernetes – Observability: Are Your Applications Healthy?  Liveness
    and Readiness Probes: [https://www.alibabacloud.com/blog/getting-started-with-kubernetes-%7C-observability-are-your-applications-healthy_596077](https://www.alibabacloud.com/blog/getting-started-with-kubernetes-%7C-observability-are-your-applications-healthy_596077).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 入门 – 可观测性：你的应用程序健康吗？ 活跃性和就绪性探针：[https://www.alibabacloud.com/blog/getting-started-with-kubernetes-%7C-observability-are-your-applications-healthy_596077](https://www.alibabacloud.com/blog/getting-started-with-kubernetes-%7C-observability-are-your-applications-healthy_596077)。
- en: '*Kubernetes Liveness and Readiness Probes: How to Avoid Shooting Yourself in
    the Foot*: [https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/](https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes 存活探针和就绪探针：如何避免自毁前程*：[https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/](https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/)'
- en: 'Awesome Prometheus alerts – the mother lode of rules for not only Kubernetes
    but also other systems that Prometheus can monitor, available under a Creative
    Commons Attribution license: [https://awesome-prometheus-alerts.grep.to/rules](https://awesome-prometheus-alerts.grep.to/rules).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超棒的 Prometheus 告警——不仅适用于 Kubernetes，还适用于 Prometheus 能监控的其他系统，且采用了知识共享许可协议：[https://awesome-prometheus-alerts.grep.to/rules](https://awesome-prometheus-alerts.grep.to/rules)。
- en: 'Configuring Prometheus Operator Helm Chart with AWS EKS has good examples of
    more detailed Alertmanager configurations: [https://medium.com/zolo-engineering/configuring-prometheus-operator-helm-chart-with-aws-eks-c12fac3b671a](https://medium.com/zolo-engineering/configuring-prometheus-operator-helm-chart-with-aws-eks-c12fac3b671a).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Prometheus Operator Helm Chart 与 AWS EKS 的方法中包含了更多详细的 Alertmanager 配置示例：[https://medium.com/zolo-engineering/configuring-prometheus-operator-helm-chart-with-aws-eks-c12fac3b671a](https://medium.com/zolo-engineering/configuring-prometheus-operator-helm-chart-with-aws-eks-c12fac3b671a)。
- en: 'Monitoring Distributed Systems – from the Google SRE book – pay special attention
    to the **four Golden Signals**: [https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式系统监控——来自 Google SRE 书中的内容——特别关注**四个黄金信号**：[https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/)。
- en: 'How to monitor Golden Signals in Kubernetes: [https://sysdig.com/blog/golden-signals-kubernetes/](https://sysdig.com/blog/golden-signals-kubernetes/).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 Kubernetes 中监控黄金信号：[https://sysdig.com/blog/golden-signals-kubernetes/](https://sysdig.com/blog/golden-signals-kubernetes/)。
- en: 'PromQL tutorial for beginners and humans: [https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085](mailto:https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085).'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PromQL 初学者教程：[https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085](mailto:https://medium.com/@valyala/promql-tutorial-for-beginners-9ab455142085)。
- en: 'Understanding delays on Prometheus alerting: [https://pracucci.com/prometheus-understanding-the-delays-on-alerting.html](https://pracucci.com/prometheus-understanding-the-delays-on-alerting.html).'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Prometheus 告警的延迟：[https://pracucci.com/prometheus-understanding-the-delays-on-alerting.html](https://pracucci.com/prometheus-understanding-the-delays-on-alerting.html)。
- en: 'Kubernetes in Production – the Ultimate Guide to Monitoring Resource Metrics
    with Prometheus: [https://www.replex.io/blog/kubernetes-in-production-the-ultimate-guide-to-monitoring-resource-metrics](https://www.replex.io/blog/kubernetes-in-production-the-ultimate-guide-to-monitoring-resource-metrics).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 在生产环境中的监控——使用 Prometheus 监控资源指标的终极指南：[https://www.replex.io/blog/kubernetes-in-production-the-ultimate-guide-to-monitoring-resource-metrics](https://www.replex.io/blog/kubernetes-in-production-the-ultimate-guide-to-monitoring-resource-metrics)。
- en: 'Kubernetes Monitoring with Prometheus – the ultimate guide (part 1) – yes,
    it''s funny that multiple articles claim to be the ultimate guide, but this one
    has really detailed information and a part 2 that also covers Grafana: [https://sysdig.com/blog/kubernetes-monitoring-prometheus/](https://sysdig.com/blog/kubernetes-monitoring-prometheus/).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 与 Prometheus 监控——终极指南（第 1 部分）——是的，有多篇文章声称自己是终极指南，但这篇确实提供了详细信息，并且第
    2 部分也涉及到 Grafana：[https://sysdig.com/blog/kubernetes-monitoring-prometheus/](https://sysdig.com/blog/kubernetes-monitoring-prometheus/)。
- en: 'Kubernetes: Monitoring with Prometheus — exporters, Service Discovery, and
    its roles. Has a section on setting up a Redis exporter that you could use to
    explore ShipIt Clicker''s operation better: [https://itnext.io/kubernetes-monitoring-with-prometheus-exporters-a-service-discovery-and-its-roles-ce63752e5a1](https://itnext.io/kubernetes-monitoring-with-prometheus-exporters-a-service-discovery-and-its-roles-ce63752e5a1).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes：使用 Prometheus 监控——exporter、服务发现及其角色。包含一个关于设置 Redis exporter 的章节，您可以通过它更好地探索
    ShipIt Clicker 的操作：[https://itnext.io/kubernetes-monitoring-with-prometheus-exporters-a-service-discovery-and-its-roles-ce63752e5a1](https://itnext.io/kubernetes-monitoring-with-prometheus-exporters-a-service-discovery-and-its-roles-ce63752e5a1)。
- en: 'Taking Advantage of Deadman''s Switch in Prometheus: [https://jpweber.io/blog/taking-advantage-of-deadmans-switch-in-prometheus/](https://jpweber.io/blog/taking-advantage-of-deadmans-switch-in-prometheus/)
    (combine with [https://deadmanssnitch.com/](https://deadmanssnitch.com/) for a
    complete Deadman''s Switch alerting system).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Prometheus 中利用 Deadman's Switch：[https://jpweber.io/blog/taking-advantage-of-deadmans-switch-in-prometheus/](https://jpweber.io/blog/taking-advantage-of-deadmans-switch-in-prometheus/)（结合
    [https://deadmanssnitch.com/](https://deadmanssnitch.com/) 来创建完整的 Deadman's Switch
    警报系统）。
- en: 'Using Prometheus Metrics in Amazon CloudWatch: [https://aws.amazon.com/blogs/containers/using-prometheus-metrics-in-amazon-cloudwatch/](https://aws.amazon.com/blogs/containers/using-prometheus-metrics-in-amazon-cloudwatch/).'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Amazon CloudWatch 中使用 Prometheus 指标：[https://aws.amazon.com/blogs/containers/using-prometheus-metrics-in-amazon-cloudwatch/](https://aws.amazon.com/blogs/containers/using-prometheus-metrics-in-amazon-cloudwatch/)。
- en: 'An alternative solution to the periodic export of CloudWatch logs to S3 via
    a scheduled Lambda function: [https://medium.com/searce/exporting-cloudwatch-logs-to-s3-through-lambda-before-retention-period-f425df06d25f](https://medium.com/searce/exporting-cloudwatch-logs-to-s3-through-lambda-before-retention-period-f425df06d25f).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计划的 Lambda 函数定期将 CloudWatch 日志导出到 S3 的替代方案：[https://medium.com/searce/exporting-cloudwatch-logs-to-s3-through-lambda-before-retention-period-f425df06d25f](https://medium.com/searce/exporting-cloudwatch-logs-to-s3-through-lambda-before-retention-period-f425df06d25f)。
