<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Persisting State</h1>
                </header>
            
            <article>
                
<div class="packt_tip">Having fault-tolerance and high-availability is of no use if we lose application state during rescheduling. Having state is unavoidable, and we need to preserve it no matter what happens to our applications, servers, or even a whole datacenter.</div>
<p>The way to preserve the state of our applications depends on their architecture. Some are storing data in-memory and rely on periodic backups. Others are capable of synchronizing data between multiple replicas, so that loss instance of one does not result in loss of data. Most, however, are relying on disk to store their state. We'll focus on that group of stateful applications.</p>
<p>If we are to build fault-tolerant systems, we need to make sure that failure of any part of the system is recoverable. Since speed is of the essence, we cannot rely on manual operations to recuperate from failures. Even if we could, no one wants to be the person sitting in front of a screen, waiting for something to fail, only to bring it back to its previous state.</p>
<p>We already saw that Kubernetes would, in most cases, recuperate from a failure of an application, of a server, or even of a whole datacenter. It'll reschedule Pods to healthy nodes. We also experienced how AWS and kops accomplish more or less the same effect on the infrastructure level. Auto-scaling groups will recreate failed nodes and, since they are provisioned with kops startup processes, new instances will have everything they need, and they will join the cluster.</p>
<p>The only thing that prevents us from saying that our system is (mostly) highly available and fault tolerant is the fact that we did not solve the problem of persisting state across failures. That's the subject we'll explore next.</p>
<p>We'll try to preserve our data no matter what happens to our stateful applications or the servers where they run.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Kubernetes cluster</h1>
                </header>
            
            <article>
                
<p>We'll start by recreating a similar cluster as the one we used in the previous chapter:</p>
<div class="packt_infobox">All the commands from this chapter are available in the <a href="https://gist.github.com/41c86eb385dfc5c881d910c5e98596f2"><kbd>15-pv.sh</kbd></a> (<a href="https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2" target="_blank"><span class="URLPACKT">https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2</span></a>) Gist.</div>
<pre><strong>cd k8s-specs</strong>
    
<strong>git pull</strong>
    
<strong>cd cluster</strong>  </pre>
<p>We entered the local copy of the <kbd>k8s-specs</kbd> repository, pulled the latest code, and went into the <kbd>cluster</kbd> directory.</p>
<p>In the previous chapter, we stored the environment variables we used in the <kbd>kops</kbd> file. Let's take a quick look at them.</p>
<pre><strong>cat kops</strong>  </pre>
<p>The output, without the keys, is as follows:</p>
<pre><strong>export AWS_ACCESS_KEY_ID=...</strong>
<strong>export AWS_SECRET_ACCESS_KEY=...</strong>
<strong>export AWS_DEFAULT_REGION=us-east-2</strong>
<strong>export ZONES=us-east-2a,us-east-2b,us-east-2c</strong>
<strong>export NAME=devops23.k8s.local</strong>
<strong>export KOPS_STATE_STORE=s3://devops23-1520933480</strong>  </pre>
<p>By storing the environment variables in a file, we can fast-track the process by loading them using the <kbd>source</kbd> command.</p>
<div class="packt_infobox">In the older editions of the book, there was an error in the command we used to store the environment variables in the <kbd>kops</kbd> file. The <kbd>export</kbd> commands were missing. Please ensure that your copy of the file has all the lines starting with <kbd>export</kbd>. If that's not the case, please update it accordingly.</div>
<pre><strong>source kops</strong>  </pre>
<p>Now that the environment variables are set, we can proceed to create an <kbd>S3</kbd> bucket:</p>
<pre><strong>export BUCKET_NAME=devops23-$(date +%s)</strong>
    
<strong>aws s3api create-bucket \ </strong>
<strong>    --bucket $BUCKET_NAME \</strong>
<strong>    --create-bucket-configuration \</strong>
<strong>    LocationConstraint=$AWS_DEFAULT_REGION</strong>
    
<strong>export KOPS_STATE_STORE=s3://$BUCKET_NAME</strong>  </pre>
<p>The command that creates the <kbd>kops</kbd> alias is as follows. Execute it only if you are a <strong>Windows user</strong>:</p>
<pre><strong>alias kops="docker run -it --rm \</strong>
<strong>    -v $PWD/devops23.pub:/devops23.pub \</strong>
<strong>    -v $PWD/config:/config \</strong>
<strong>    -e KUBECONFIG=/config/kubecfg.yaml \</strong>
<strong>    -e NAME=$NAME -e ZONES=$ZONES \</strong>
<strong>    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \</strong>
<strong>    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \</strong>
<strong>    -e KOPS_STATE_STORE=$KOPS_STATE_STORE \</strong>
<strong>    vfarcic/kops"</strong>  </pre>
<p>Now we can, finally, create a new Kubernetes cluster in AWS.</p>
<pre><strong>kops create cluster \</strong>
<strong>    --name $NAME \</strong>
<strong>    --master-count 3 \</strong>
<strong>    --master-size t2.small \</strong>
<strong>    --node-count 2 \</strong>
<strong>    --node-size t2.medium \</strong>
<strong>   --zones $ZONES \</strong>
<strong>    --master-zones $ZONES \</strong>
<strong>    --ssh-public-key devops23.pub \</strong>
<strong>    --networking kubenet \</strong>
<strong>  </strong><strong>  --yes</strong>  </pre>
<p>If we compare that command with the one we executed in the previous chapter, we'll notice only a few minor changes. We increased <kbd>node-count</kbd> to <kbd>2</kbd> and <kbd>node-size</kbd> to <kbd>t2.medium</kbd>. That will give us more than enough capacity for the exercises we'll run in this chapter.</p>
<p>Let's validate the cluster:</p>
<pre><strong>kops validate cluster</strong>  </pre>
<p>Assuming that enough time passed since we executed <kbd>kops create cluster</kbd>, the output should indicate that the <kbd>cluster devops23.k8s.local is ready</kbd>.</p>
<div class="packt_infobox"><span class="packt_screen">A note to Windows users<br/></span>Kops was executed inside a container. It changed the context inside the container that is now gone. As a result, your local <kbd>kubectl</kbd> context was left intact. We'll fix that by executing <kbd>kops export kubecfg --name ${NAME}</kbd> and <kbd>export KUBECONFIG=$PWD/config/kubecfg.yaml</kbd>. The first command exported the config to <kbd>/config/kubecfg.yaml</kbd>. That path was specified through the environment variable <kbd>KUBECONFIG</kbd> and is mounted as <kbd>config/kubecfg.yaml</kbd> on local hard disk. The latter command exports <kbd>KUBECONFIG</kbd> locally. Through that variable, <kbd>kubectl</kbd> is now instructed to use the configuration in <kbd>config/kubecfg.yaml</kbd> instead of the default one. Before you run those commands, please give AWS a few minutes to create all the EC2 instances and for them to join the cluster. After waiting and executing those commands, you'll be all set.</div>
<p>We'll need Ingress if we'd like to access the applications we'll deploy.</p>
<pre><strong>kubectl create \</strong>
<strong>    -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx/v1.6.0.yaml</strong>
  </pre>
<p>Ingress will not help us much without the ELB DNS, so we'll get that as well:</p>
<pre><strong>CLUSTER_DNS=$(aws elb \</strong>
<strong>    describe-load-balancers | jq -r \</strong>
<strong>    ".LoadBalancerDescriptions[] \</strong>
<strong>    | select(.DNSName \</strong>
<strong>    | contains (\"api-devops23\") \</strong>
<strong>    | not).DNSName")</strong>
    
<strong>echo $CLUSTER_DNS</strong>  </pre>
<p>The output of the latter command should end with <kbd>us-east-2.elb.amazonaws.com</kbd>.</p>
<p>Finally, now that we are finished with the cluster setup, we can go back to the repository root directory.</p>
<pre><strong>cd ..</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying stateful applications without persisting state</h1>
                </header>
            
            <article>
                
<p>We'll start the exploration by deploying a stateful application without any mechanism to persist its state. That will give us a better insight into benefits behind of some of the Kubernetes concepts and resources we'll use in this chapter.</p>
<p>We already deployed Jenkins a few times. Since it is a stateful application, it is an excellent candidate to serve as a playground.</p>
<p>Let's take a look at a definition stored in the <kbd>pv/jenkins-no-pv.yml</kbd> file.</p>
<pre><strong>cat pv/jenkins-no-pv.yml</strong>  </pre>
<p>The YAML defines the <kbd>jenkins</kbd> namespace, an Ingress controller, and a service. We're already familiar with those types of resources so we'll skip explaining them and jump straight to the Deployment definition.</p>
<p>The output of the <kbd>cat</kbd> command, limited to the <kbd>jenkins</kbd> Deployment, is as follows:</p>
<pre><strong>...</strong>
<strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: Jenkins</strong>
    <strong>  namespace: Jenkins</strong>
    <strong>spec:</strong>
    <strong>  selector:</strong>
    <strong>    matchLabels:</strong>
    <strong>      app: Jenkins</strong>
    <strong>  strategy:</strong>
    <strong>    type: Recreate</strong>
    <strong>  template:</strong>
    <strong>    metadata:</strong>
    <strong>      labels:</strong>
    <strong>        app: Jenkins</strong>
    <strong>    spec:</strong>
    <strong>      containers:</strong>
    <strong>      - name: Jenkins</strong>
    <strong>        image: vfarcic/Jenkins</strong>
    <strong>        env:</strong>
    <strong>        - name: JENKINS_OPTS</strong>
    <strong>          value: --prefix=/Jenkins</strong>
    <strong>        volumeMounts:</strong>
    <strong>        - name: jenkins-creds</strong>
    <strong>          mountPath: /etc/secrets</strong>
    <strong>        resources:</strong>
    <strong>          limits:</strong>
    <strong>            memory: 2Gi</strong>
    <strong>            cpu: 1</strong>
    <strong>          requests:</strong>
    <strong>            memory: 1Gi</strong>
    <strong>            cpu: 0.5</strong>
    <strong>      volumes:</strong>
    <strong>      - name: jenkins-creds</strong>
    <strong>        secret:</strong>
    <strong>          secretName: jenkins-creds</strong></pre>
<p>There's nothing special about this Deployment. We already used a very similar one. Besides, by now, you're an expert at Deployment controllers.</p>
<p>The only thing worth mentioning is that there is only one volume mount and it references a secret we're using to provide Jenkins with the initial administrative user. Jenkins is persisting its state in <kbd>/var/jenkins_home</kbd>, and we are not mounting that directory.</p>
<p>Let's create the resources defined in <kbd>pv/jenkins-no-pv.yml</kbd>:</p>
<pre><strong>kubectl create \</strong>
<strong>    -f pv/jenkins-no-pv.yml \</strong>
<strong>    --record --save-config</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>namespace "jenkins" created</strong>
<strong>ingress "jenkins" created</strong>
<strong>service "jenkins" created</strong>
<strong>deployment "jenkins" created</strong>  </pre>
<p>We'll take a quick look at the events as a way to check that everything was deployed successfully:</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    get events</strong></pre>
<p>The output, limited to relevant parts, is as follows:</p>
<pre><strong>...</strong>
<strong>2018-03-14 22:36:26 +0100 CET   2018-03-14 22:35:54 +0100 CET   7         jenkins-8768d486-lmv6b.151be70fd682e40d   Pod                 Warning   FailedMount  kubelet, ip-172-20-99-208.us-east-2.compute.internal   MountVolume.SetUp <br/></strong></pre>
<pre><strong> failed for volume "jenkins-creds" : secrets "jenkins-creds" not found</strong>
    <strong>...</strong></pre>
<p>We can see that the setup of the only volume failed since it could not find the secret referenced as <kbd>jenkins-creds</kbd>. Let's create it:</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    create secret \</strong>
<strong>    generic jenkins-creds \</strong>
<strong>    --from-literal=jenkins-user=jdoe \</strong>
<strong>    --from-literal=jenkins-pass=incognito</strong>  </pre>
<p>Now, with the secret <kbd>jenkins-creds</kbd> created in the <kbd>jenkins</kbd> namespace, we can confirm that the rollout of the Deployment was successful.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    rollout status \</strong>
<strong>    deployment jenkins</strong>  </pre>
<p>We can see, from the output, that the <kbd>deployment "jenkins" was successfully rolled out</kbd>.</p>
<p>Now that everything is up and running, we can open Jenkins UI in a browser:</p>
<pre><strong>open "http://$CLUSTER_DNS/jenkins"</strong></pre>
<div class="packt_tip"><span class="packt_screen">A note to Windows users<br/></span>Git Bash might not be able to use the <kbd>open</kbd> command. If that's the case, please replace the <kbd>open</kbd> command with <kbd>echo</kbd>. As a result, you'll get the full address that should be opened directly in your browser of choice.</div>
<p>Please click the <span class="packt_screen">Log in</span> link, type <kbd>jdoe</kbd> as the <span class="packt_screen">User,</span> and <kbd>incognito</kbd> as the <span class="packt_screen">Password</span>. When finished, click the <span class="packt_screen">log in</span> button.</p>
<p>Now that we are authenticated as <span class="packt_screen">jdoe</span> administrator, we can proceed and create a job. That will generate a state that we can use to explore what happens when a stateful application fails.</p>
<p>Please click the <span class="packt_screen">create new jobs</span> link, type <kbd>my-job</kbd> as the item name, select <span class="packt_screen">Pipeline</span> as the job type, and press the <span class="packt_screen">OK</span> button.</p>
<p>You'll be presented with the job configuration screen. There's no need to do anything here since we are not, at the moment, interested in any specific Pipeline definition. It's enough to click the <span class="packt_screen">Save</span> button.</p>
<p>Next, we'll simulate a failure by killing <kbd>java</kbd> process running inside the Pod created by the <kbd>jenkins</kbd> Deployment. To do that, we need to find out the name of the Pod.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    get pods \</strong>
<strong>    --selector=app=jenkins \</strong>
<strong>    -o json</strong>  </pre>
<p>We retrieved the Pods from the <kbd>jenkins</kbd> namespace, filtered them with the selector <kbd>api=jenkins</kbd>, and formatted the output as <kbd>json</kbd>.</p>
<p>The output, limited to the relevant parts, as is follows:</p>
<pre><strong>{</strong>
<strong>  "apiVersion": "v1",</strong>
<strong>    "items": [</strong>
<strong>    {</strong>
<strong>      ...</strong>
<strong>      "metadata": {</strong>
<strong>        ...</strong>
<strong>        "name": "jenkins-8768d486-lmv6b",</strong>
<strong>        ...</strong></pre>
<p>We can see that the name is inside <kbd>metadata</kbd> entry of one of the <kbd>items</kbd>. We can use that to formulate <kbd>jsonpath</kbd> that will retrieve only the name of the Pod:</p>
<pre><strong>POD_NAME=$(kubectl \</strong>
<strong>    --namespace jenkins \</strong>
<strong>    get pods \</strong>
<strong>    --selector=app=jenkins \</strong>
<strong>    -o jsonpath="{.items[*].metadata.name}")</strong>
    
<strong>echo $POD_NAME</strong>  </pre>
<p>The name of the Pod is now stored in the environment variable <kbd>POD_NAME</kbd>.</p>
<p>The output of the latter command is as follows:</p>
<pre><strong>jenkins-8768d486-lmv6b</strong>  </pre>
<p>Now that we know the name of the Pod hosting Jenkins, we can proceed and kill the <kbd>java</kbd> process:</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    exec -it $POD_NAME pkill java</strong>  </pre>
<p>The container failed once we killed Jenkins process. We already know from experience that a failed container inside a Pod will be recreated. As a result, we had a short downtime, but Jenkins is running once again.</p>
<p>Let's see what happened to the job we created earlier. I'm sure you know the answer, but we'll check it anyway:</p>
<pre><strong>open "http://$CLUSTER_DNS/jenkins"</strong>  </pre>
<p>As expected, <kbd>my-job</kbd> is nowhere to be found. The container that was hosting <kbd>/var/jenkins_home</kbd> directory failed, and it was replaced with a new one. The state we created is lost.</p>
<p>Truth be told, we already saw in the <a href="a8a54c79-1dda-41ec-8ad5-4986c17b7041.xhtml">Chapter 8</a>, <em>Using Volumes to Access Host's File System</em> that we can mount a volume in an attempt to preserve state across failures. However, in the past, we used <kbd>emptyDir</kbd> which mounts a local volume. Even though that's better than nothing, such a volume exists only as long as the server it is stored in is up and running. If the server would fail, the state stored in <kbd>emptyDir</kbd> would be gone. Such a solution would be only slightly better than not using any volume. By using local disk we would only postpone inevitable, and, sooner or later, we'd get to the same situation. We'd be left wondering why we lost everything we created in Jenkins. We can do better than that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating AWS volumes</h1>
                </header>
            
            <article>
                
<p>If we want to persist state that will survive even server failures, we have two options we can choose. We could, for example, store data locally and replicate it to multiple servers. That way, a container could use local storage knowing that the files are available on all the servers. Such a setup would be too complicated if we'd like to implement the process ourselves. Truth be told, we could use one of the volume drivers for that. However, we'll opt for a more commonly used method to persist the state across failures. We'll use external storage.</p>
<p>Since we are running our cluster in AWS, we can choose between <a href="https://aws.amazon.com/s3/"><kbd>S3</kbd></a> (<a href="https://aws.amazon.com/s3/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/s3/</span></a>), <a href="https://aws.amazon.com/efs/"><strong>Elastic File System</strong> (<strong>EFS</strong>)</a> (<a href="https://aws.amazon.com/efs/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/efs/</span></a>), and <a href="https://aws.amazon.com/ebs/"><strong>Elastic Block Store</strong></a> (<strong>EBS</strong>) (<a href="https://aws.amazon.com/ebs/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/ebs/</span></a>).</p>
<p>S3 is meant to be accessed through its API and is not suitable as a local disk replacement. That leaves us with EFS and EBS.</p>
<p>EFS, has a distinct advantage that it can be mounted to multiple EC2 instances spread across multiple availability zones. It is closest we can get to fault-tolerant storage. Even if a whole zone (datacenter) fails, we'll still be able to use EFS in the rest of the zones used by our cluster. However, that comes at a cost. EFS introduces a performance penalty. It is, after all, a <strong>network file system</strong> (<strong>NFS</strong>), and that entails higher latency.</p>
<p><strong>Elastic Block Store</strong> (<strong>EBS</strong>) is the fastest storage we can use in AWS. Its data access latency is very low thus making it the best choice when performance is the primary concern. The downside is availability. It doesn't work in multiple availability zones. Failure of one will mean downtime, at least until the zone is restored to its operational state.</p>
<p>We'll choose EBS for our storage needs. Jenkins depends heavily on IO, and we need data access to be as fast as possible. However, there is another reason for such a choice. EBS is fully supported by Kubernetes. EFS will come but, at the time of this writing, it is still in the experimental stage. As a bonus advantage, EBS is much cheaper than EFS.</p>
<p>Given the requirements and what Kubernetes offers, the choice is obvious. We'll use EBS, even though we might run into trouble if the availability zone where our Jenkins will run goes down. In such a case, we'd need to migrate EBS volume to a healthy zone. There's no such thing as a perfect solution.</p>
<p>We are jumping ahead of ourselves. We'll leave Kubernetes aside for a while and concentrate on creating an EBS volume.</p>
<p>Each EBS volume is tied to an availability zone. Unlike EFS, EBS cannot span multiple zones. So, the first thing we need to do is to find out which are the zones worker nodes are running in. We can get that information by describing the EC2 instances belonging to the security group <kbd>nodes.devops23.k8s.local</kbd>.</p>
<pre><strong>aws ec2 describe-instances</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>{</strong>
<strong>  "Reservations": [</strong>
<strong>    {</strong>
<strong>      "Instances": [</strong>
<strong>        {</strong>
<strong>          ...</strong>
    <strong>      "SecurityGroups": [</strong>
    <strong>        {</strong>
    <strong>          "GroupName": "nodes.devops23.k8s.local",</strong>
    <strong>          "GroupId": "sg-33fd8c58"</strong>
    <strong>        }</strong>
    <strong>      ],</strong>
    <strong>      ...</strong>
    <strong>      "Placement": {</strong>
    <strong>        "Tenancy": "default",</strong>
    <strong>        "GroupName": "",</strong>
    <strong>        "AvailabilityZone": "us-east-2a"</strong>
    <strong>      },</strong>
    <strong>      ...</strong></pre>
<p>We can see that the information is inside the <kbd>Reservations.Instances</kbd> array. To get the zone, we need to filter the output by the <kbd>SecurityGroups.GroupName</kbd> field. Zone name is located in the <kbd>Placement.AvailabilityZone</kbd> field.</p>
<p>The command that does the filtering and retrieves the availability zones of the worker nodes is as follows:</p>
<pre><strong>aws ec2 describe-instances \</strong>
<strong>    | jq -r \</strong>
<strong>    ".Reservations[].Instances[] \</strong>
<strong>    | select(.SecurityGroups[]\</strong>
<strong>    .GroupName==\"nodes.$NAME\")\</strong>
<strong>    .Placement.AvailabilityZone"</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>us-east-2a</strong>
<strong>us-east-2c</strong>  </pre>
<p>We can see that the two worker nodes are located in the zones <kbd>us-east-2a</kbd> and <kbd>us-east-2c</kbd>.</p>
<p>The commands that retrieve the zones of the two worker nodes and store them in environment variables is as follows:</p>
<pre><strong>aws ec2 describe-instances \</strong>
<strong>    | jq -r \</strong>
<strong>    ".Reservations[].Instances[] \</strong>
<strong>    | select(.SecurityGroups[]\</strong>
<strong>    .GroupName=="\nodes.$NAME\")\</strong>
<strong>    .Placement.AvailabilityZone" \</strong>
<strong>    | tee zones</strong>
    
<strong>AZ_1=$(cat zones | head -n 1)</strong>
    
<strong>AZ_2=$(cat zones | tail -n 1)</strong>  </pre>
<p>We retrieved the zones and stored the output into the <kbd>zones</kbd> file. Further on, we retrieved the first row with the <kbd>head</kbd> command and stored it in the environment variable <kbd>AZ_1</kbd>. Similarly, we stored the last (the second) row in the variable <kbd>AZ_2</kbd>.</p>
<p>Now we have all the information we need to create a few volumes.</p>
<div class="packt_infobox">The command that follows requires a relatively newer version of <kbd>aws</kbd>. If it fails, please update your AWS CLI binary to the latest version.</div>
<pre><strong>VOLUME_ID_1=$(aws ec2 create-volume \</strong>
<strong>    --availability-zone $AZ_1 \</strong>
<strong>    --size 10 \</strong>
<strong>    --volume-type gp2 \</strong>
<strong>    --tag-specifications "ResourceType=volume,Tags=[{Key=KubernetesCluster,Value=$NAME}]" \</strong>
<strong>    | jq -r '.VolumeId')</strong>
    
<strong>VOLUME_ID_2=$(aws ec2 create-volume \</strong>
<strong>    --availability-zone $AZ_1 \</strong>
<strong>    --size 10 \</strong>
<strong>    --volume-type gp2 \</strong>
<strong>    --tag-specifications "ResourceType=volume,Tags=[{Key=KubernetesCluster,Value=$NAME}]" \</strong>
    <strong>| jq -r '.VolumeId')</strong>
    
<strong>VOLUME_ID_3=$(aws ec2 create-volume \</strong>
    <strong>--availability-zone $AZ_2 \</strong>
    <strong>--size 10 \</strong>
    <strong>--volume-type gp2 \</strong>
    <strong>--tag-specifications "ResourceType=volume,Tags=[{Key=KubernetesCluster,Value=$NAME}]" \</strong>
<strong>    | jq -r '.VolumeId')</strong></pre>
<p>We executed <kbd>aws ec2 create-volume</kbd> command three times. As a result, we created three EBS volumes. Two of them are in one zone, while the third is in another. They all have <kbd>10</kbd> GB of space. We chose <kbd>gp2</kbd> as the type of the volumes. The other types either require bigger sizes or are more expensive. When in doubt, <kbd>gp2</kbd> is usually the best choice for EBS volumes.</p>
<p>We also defined a tag that will help us distinguish the volumes dedicated to this cluster from those we might have in our AWS account for other purposes.</p>
<p>Finally, <kbd>jq</kbd> filtered the output so that only the volume ID is retrieved. The results are stored in the environment variables <kbd>VOLUME_ID_1</kbd>, <kbd>VOLUME_ID_2</kbd>, and <kbd>VOLUME_ID_3</kbd>.</p>
<p>Let's take a quick look at one of the IDs we stored as an environment variable:</p>
<pre><strong>echo $VOLUME_ID_1</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>vol-092b8980b1964574a</strong>  </pre>
<p>Finally, to be on the safe side, we'll list the volume that matches the ID and thus confirm, without doubt, that the EBS was indeed created.</p>
<pre><strong>aws ec2 describe-volumes \</strong>
<strong>    --volume-ids $VOLUME_ID_1</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>  "Volumes": [</strong>
<strong>    {</strong>
<strong>      "AvailabilityZone": "us-east-2c",</strong>
<strong>      "Attachments": [],</strong>
<strong>      "Tags": [</strong>
<strong>        {</strong>
<strong>          "Value": "devops23.k8s.local",</strong>
<strong>          "Key": "KubernetesCluster"</strong>
<strong>        }</strong>
<strong>      ],</strong>
<strong>      "Encrypted": false,</strong>
<strong>      "VolumeType": "gp2",</strong>
<strong>      "VolumeId": "vol-092b8980b1964574a",</strong>
<strong>      "State": "available",</strong>
<strong>      "Iops": 100,</strong>
<strong>      "SnapshotId": "",</strong>
<strong>      "CreateTime": "2018-03-14T21:47:13.242Z",</strong>
<strong>      "Size": 10</strong>
<strong>    }</strong>
<strong>  ]</strong>
<strong>}</strong>  </pre>
<p>Now that the EBS volumes are indeed <kbd>available</kbd> and in the same zones as the worker nodes, we can proceed and create Kubernetes persistent volumes.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/3c2542d1-2653-4569-9932-9be7c575aa3f.png" style="width:32.33em;height:28.58em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 15-1: EBS volumes created in the same zones as the worker nodes</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating Kubernetes persistent volumes</h1>
                </header>
            
            <article>
                
<p>The fact that we have a few EBS volumes available does not mean that Kubernetes knows about their existence. We need to add PersistentVolumes that will act as a bridge between our Kubernetes cluster and AWS EBS volumes.</p>
<p>PersistentVolumes allow us to abstract details of how storage is provided (for example, EBS) from how it is consumed. Just like Volumes, PersistentVolumes are resources in a Kubernetes cluster. The main difference is that their lifecycle is independent of individual Pods that are using them.</p>
<p>Let's take a look at a definition that will create a few PersistentVolumes:</p>
<pre><strong>cat pv/pv.yml</strong>  </pre>
<p>The output, limited to the first of the three volumes, is as follows:</p>
<pre><strong>kind: PersistentVolume</strong>
<strong>apiVersion: v1</strong>
<strong>metadata:</strong>
<strong>  name: manual-ebs-01</strong>
<strong>  labels:</strong>
<strong>    type: ebs</strong>
<strong>spec:</strong>
<strong>  storageClassName: manual-ebs</strong>
<strong>  capacity:</strong>
<strong>    storage: 5Gi</strong>
<strong>  accessModes:</strong>
<strong>    - ReadWriteOnce</strong>
<strong>  awsElasticBlockStore:</strong>
<strong>    volumeID: REPLACE_ME_1</strong>
<strong>    fsType: ext4</strong>
<strong>...</strong>  </pre>
<p>The <kbd>spec</kbd> section features a few interesting details. We set <kbd>manual-ebs</kbd> as the storage class name. We'll see later what is its function. For now, just remember the name.</p>
<p>We defined that the storage capacity is <kbd>5Gi</kbd>. It does not need to be the same as the capacity of the EBS we created earlier, as long as it is not bigger. Kubernetes will try to match <kbd>PersistentVolume</kbd> with, in this case, EBS that has a similar, if not the same capacity. Since we have only one EBS volume with 10 GB, it is the closest (and the only) match to the <kbd>PersistentVolume</kbd> request of <kbd>5Gi</kbd>. Ideally, persistent volumes capacity should match EBS size, but I wanted to demonstrate that any value equal to or less then the actual size should do.</p>
<p>We specified that the access mode should be <kbd>ReadWriteOnce</kbd>. That means that we'll be able to mount the volume as read-write only once. Only one Pod will be able to use it at any given moment. Such a strategy fits us well since EBS cannot be mounted to multiple instances. Our choice of the access mode is not truly a choice, but more an acknowledgment of the way how EBS works. The alternative modes are <kbd>ReadOnlyMany</kbd> and <kbd>ReadWriteMany</kbd>. Both modes would result in volumes that could be mounted to multiple Pods, either as read-only or read-write. Those modes would be more suitable for NFS like, for example, EFS, which can be mounted by multiple instances.</p>
<p>The <kbd>spec</kbd> fields we explored so far are common to all persistent volume types. Besides those, there are entries specific to the actual volume we are associating with a Kubernetes <kbd>PersistentVolume</kbd>. Since we're going to use EBS, we specified <kbd>awsElasticBlockStore</kbd> with the volume ID and file system type. Since I could not know in advance what will be the ID of your EBS volume, the definition has the value set to <kbd>REPLACE_ME</kbd>. Later on, we'll replace it with the ID of the EBS we created earlier.</p>
<p>There are many other types we could have specified instead. If this cluster would run on Azure, we could use <kbd>azureDisk</kbd> or <kbd>azureFile</kbd>. In <strong>Google Compute Engine</strong> (<strong>GCE</strong>) it would be <kbd>GCEPersistentDisk</kbd>. We could have setup <kbd>Glusterfs</kbd>. Or, if we would have this cluster running in an on-prem data center, it would probably be <kbd>nfs</kbd>. There are quite a few others we could use but, since we're running the cluster in AWS, many would not work, while others could be too difficult to set up. Since EBS is already available, we'll just roll with it. All in all, this cluster is in AWS, and <kbd>awsElasticBlockStore</kbd> is the easiest, if not the best choice.</p>
<p>Now that we have an understanding of the YAML definition, we can proceed and create the <kbd>PersistentVolume</kbd>:</p>
<pre><strong>cat pv/pv.yml \</strong>
<strong>    | sed -e \</strong>
<strong>    "s@REPLACE_ME_1@$VOLUME_ID_1@g" \</strong>
<strong>    | sed -e \</strong>
<strong>    "s@REPLACE_ME_2@$VOLUME_ID_2@g" \</strong>
<strong>    | sed -e \</strong>
<strong>    "s@REPLACE_ME_3@$VOLUME_ID_3@g" \</strong>
<strong>    | kubectl create -f - \</strong>
<strong>    --save-config --record</strong>  </pre>
<p>We used <kbd>cat</kbd> to output the contents of the <kbd>pv/pv.yml</kbd> file and pipe it into <kbd>sed</kbd> commands which, in turn, replaced the <kbd>REPLACE_ME_*</kbd> strings with the IDs of the EBS volumes we created earlier. The result was sent to the <kbd>kubectl create</kbd> command that created persistent volumes. As a result, we can see from the output that all three PersistentVolumes were created.</p>
<p>Let's take a look at the persistent volumes currently available in our cluster.</p>
<pre><strong>kubectl get pv</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS    CLAIM STORAGECLASS REASON AGE</strong>
<strong>manual-ebs-01 5Gi      RWO          Retain         Available       manual-ebs          11s</strong>
<strong>manual-ebs-02 5Gi      RWO          Retain         Available       manual-ebs          11s</strong>
<strong>manual-ebs-03 5Gi      RWO          Retain         Available       manual-ebs          11s</strong></pre>
<p>It should come as no surprise that we have three volumes:</p>
<p>The interesting part of the information we're seeing are the statuses. The persistent volumes are <kbd>available</kbd>. We created them, but no one is using them. They just sit there waiting for someone to claim them.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/44bc1af7-6d59-4e75-96e4-ef26a0e9ff49.png" style="width:30.00em;height:27.42em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 15-2: Kubernetes persistent volumes tied to EBS volumes</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Claiming persistent volumes</h1>
                </header>
            
            <article>
                
<p>Kubernetes persistent volumes are useless if no one uses them. They exist only as objects with relation to, in our case, specific EBS volumes. They are waiting for someone to claim them through the <kbd>PersistentVolumeClaim</kbd> resource.</p>
<p>Just like Pods which can request specific resources like memory and CPU, <kbd>PersistentVolumeClaims</kbd> can request particular sizes and access modes. Both are, in a way, consuming resources, even though of different types. Just as Pods should not specify on which node they should run, <kbd>PersistentVolumeClaims</kbd> cannot define which volume they should mount. Instead, Kubernetes scheduler will assign them a volume depending on the claimed resources.</p>
<p>We'll use <kbd>pv/pvc.yml</kbd> to explore how we could claim a persistent volume:</p>
<pre><strong>cat pv/pvc.yml</strong> </pre>
<p>The output is as follows:</p>
<pre><strong>kind: PersistentVolumeClaim</strong>
<strong>apiVersion: v1</strong>
<strong>metadata:</strong>
<strong>  name: jenkins</strong>
<strong>  namespace: jenkins</strong>
<strong>spec:</strong>
<strong>  storageClassName: manual-ebs</strong>
<strong>  accessModes:</strong>
<strong>    - ReadWriteOnce</strong>
<strong>  resources:</strong>
<strong>    requests:</strong>
<strong>      storage: 1Gi</strong>  </pre>
<p>The YAML file defines a <kbd>PersistentVolumeClaim</kbd> with the storage class name <kbd>manual-ebs</kbd>. That is the same class as the persistent volumes <kbd>manual-ebs-*</kbd> we created earlier. The access mode and the storage request are also matching what we defined for the persistent volume.</p>
<p>Please note that we are not specifying which volume we'd like to use. Instead, this claim specifies a set of attributes (<kbd>storageClassName</kbd>, <kbd>accessModes</kbd>, and <kbd>storage</kbd>). Any of the volumes in the system that match those specifications might be claimed by the <kbd>PersistentVolumeClaim</kbd> named <kbd>jenkins</kbd>. Bear in mind that <kbd>resources</kbd> do not have to be the exact match. Any volume that has the same or bigger amount of storage is considered a match. A claim for <kbd>1Gi</kbd> can be translated to <em>at least 1Gi</em>. In our case, a claim for <kbd>1Gi</kbd> matches all three persistent volumes since they are set to <kbd>5Gi</kbd>.</p>
<p>Now that we explored the definition of the claim, we can proceed, and create it:</p>
<pre><strong>kubectl create -f pv/pvc.yml \</strong>
<strong>    --save-config --record</strong>  </pre>
<p>The output indicates that the <kbd>persistentvolumeclaim "jenkins" was created</kbd>.</p>
<p>Let's list the claims and see what we got:</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    get pvc</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME    STATUS VOLUME        CAPACITY ACCESS MODES STORAGECLASS AGE</strong>
<strong>jenkins Bound  manual-ebs-02 5Gi      RWO          manual-ebs   17s</strong>  </pre>
<p>We see from the output that the status of the claim is <kbd>Bound</kbd>. That means that the claim found a matching persistent volume and bounded it. We can confirm that by listing the volumes:</p>
<pre><strong>kubectl get pv</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS    CLAIM           STORAGECLASS REASON AGE</strong>
<strong>manual-ebs-01 5Gi      RWO          Retain         Available                 manual-ebs          7m</strong>
<strong>manual-ebs-02 5Gi      RWO          Retain         Bound     jenkins/jenkins manual-ebs          7m</strong>
<strong>manual-ebs-03 5Gi      RWO          Retain         Available                 manual-ebs          7m</strong>
  </pre>
<p>We can see that one of the volumes (<kbd>manual-ebs-02</kbd>) changed the status from <kbd>Available</kbd> to <kbd>Bound</kbd>. That is the volume bound to the claim we created a moment ago. We can see that the claim comes from <kbd>jenkins</kbd> namespace and <kbd>jenkins</kbd><kbd>PersistentVolumeClaim</kbd>.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ba1b318f-887f-4c15-a39a-4affbbabffe2.png" style="width:31.25em;height:28.50em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign">Figure 15-3: Creation of a Persistent Volume Claim</div>
<p>Please note that if a PersistentVolumeClaim cannot find a matching volume, it will remain unbound indefinitely, unless we add a new PersistentVolume with the matching specifications.</p>
<p>We still haven't accomplished our goal. The fact that we claimed a volume does not mean that anyone uses it. On the other hand, our Jenkins needs to persist its state. We'll join our <kbd>PersistentVolumeClaim</kbd> with a Jenkins container.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Attaching claimed volumes to Pods</h1>
                </header>
            
            <article>
                
<pre><strong>cat pv/jenkins-pv.yml</strong>  </pre>
<p>The relevant parts of the output is as follows:</p>
<pre><strong>...</strong>
<strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: jenkins</strong>
<strong>  namespace: jenkins</strong>
<strong>spec:</strong>
<strong>  ...</strong>
<strong>  template:</strong>
<strong>    ...</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: jenkins</strong>
<strong>        ...</strong>
<strong>        volumeMounts:</strong>
<strong>        - name: jenkins-home</strong>
<strong>          mountPath: /var/jenkins_home</strong>
<strong>        ...</strong>
<strong>      volumes:</strong>
<strong>      - name: jenkins-home</strong>
<strong>        persistentVolumeClaim:</strong>
<strong>          claimName: jenkins</strong>
<strong>      ...</strong>  </pre>
<p>You'll notice that, this time, we added a new volume <kbd>jenkins-home</kbd>, which references the <kbd>PersistentVolumeClaim</kbd> called <kbd>jenkins</kbd>. From the container's perspective, the claim is a volume.</p>
<p>Let's deploy Jenkins resources and confirm that everything works as expected.</p>
<pre><strong>kubectl apply \</strong>
<strong>    -f pv/jenkins-pv.yml \</strong>
<strong>    --record</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>namespace "jenkins" configured</strong>
<strong>ingress "jenkins" configured</strong>
<strong>service "jenkins" configured</strong>
<strong>deployment "jenkins" configured</strong>  </pre>
<p>We'll wait until the Deployment rolls out before proceeding with a test that will confirm whether Jenkins state is now persisted.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    rollout status \</strong>
<strong>    deployment jenkins</strong>  </pre>
<p>Once the rollout is finished, we'll see a message stating that the <kbd>deployment "jenkins" was successfully rolled out</kbd>.</p>
<p>We sent a request to the Kubernetes API to create a Deployment. As a result, we got a <kbd>ReplicaSet</kbd> that, in turn, created the <kbd>jenkins</kbd> Pod. It mounted the <kbd>PersistentVolumeClaim</kbd>, which is bound to the <kbd>PersistenceVolume</kbd>, that is tied to the EBS volume. As a result, the EBS volume was mounted to the <kbd>jenkins</kbd> container running in a Pod.</p>
<p>A simplified version of the sequence of events is depicted in the <em>Figure 15-4</em>.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0b4b1b21-dd3f-4e03-9ba1-7d22b85a55ac.png" style="width:31.83em;height:27.67em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Figure 15-4: The sequence of events initiated with a request to create a Jenkins Pod with the PersistentVolumeClaim</div>
<ol>
<li>We executed <kbd>kubectl</kbd> command</li>
<li><kbd>kubectl</kbd> sent a request to <kbd>kube-apiserver</kbd> to create the resources defined in <kbd>pv/jenkins-pv.yml</kbd></li>
<li>Among others, the <kbd>jenkins</kbd> Pod was created in one of the worker nodes</li>
<li>Since <kbd>jenkins</kbd> container in the Pod has a <kbd>PersistentVolumeClaim</kbd>, it mounted it as a logical volume</li>
<li>The <kbd>PersistentVolumeClaim</kbd> was already bound to one of the PersistentVolumes</li>
<li>The PersistentVolume is associated with one of the EBS volumes</li>
<li>The EBS volume was mounted as a physical volume to the <kbd>jenkins</kbd> Pod</li>
</ol>
<p>Now that Jenkins is up-and-running, we'll execute a similar set of steps as before, and validate that the state is persisted across failures.</p>
<pre><strong>open "http://$CLUSTER_DNS/jenkins"</strong>  </pre>
<p>We opened Jenkins home screen. If you are not authenticated, please click the <span class="packt_screen">Log in</span> link and type <kbd>jdoe</kbd> as the <span class="packt_screen">User</span> and <kbd>**incognito*</kbd> as the <span class="packt_screen">Password</span>. Click the <span class="packt_screen">log in</span> button.</p>
<p>You'll see the <span class="packt_screen">create new jobs</span> link. Click it. Type <kbd>my-job</kbd> as the item name, select <kbd>Pipeline</kbd> as the job type, and click the <span class="packt_screen">OK</span> button. Once inside the job configuration screen, all we have to do is click the <span class="packt_screen">Save</span> button. An empty job will be enough to test persistence.</p>
<p>Now we need to find out the name of the Pod created through the <kbd>jenkins</kbd> Deployment.</p>
<pre><strong>POD_NAME=$(kubectl \</strong>
<strong>    --namespace jenkins \</strong>
<strong>    get pod \</strong>
<strong>    --selector=app=jenkins \</strong>
<strong>    -o jsonpath="{.items[*].metadata.name}")</strong></pre>
<p>With the name of the Pod stored in the environment variable <kbd>POD_NAME</kbd>, we can proceed and kill <kbd>java</kbd> process that's running Jenkins.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    exec -it $POD_NAME pkill java</strong>  </pre>
<p>We killed the Jenkins process and thus simulated failure of the container. As a result, Kubernetes detected the failure and recreated the container.</p>
<p>A minute later, we can open Jenkins home screen again, and check whether the state (the job we created) was preserved.</p>
<pre><strong>open "http://$CLUSTER_DNS/jenkins"</strong>  </pre>
<p>As you can see, the job is still available thus proving that we successfully mounted the EBS volume as the directory where Jenkins preserves its state.</p>
<p>If instead of destroying the container, we terminated the server where the Pod is running, the result, from the functional perspective, would be the same. The Pod would be rescheduled to a healthy node. Jenkins would start again and restore its state from the EBS volume. Or, at least, that's what we'd hope. However, such behavior is not guaranteed to happen in our cluster.</p>
<p>We have only two worker nodes, distributed in two (out of three) availability zones. If the node that hosted Jenkins failed, we'd be left with only one node. To be more precise, we'd have only one worker node running in the cluster until the auto-scaling group detects that an EC2 instance is missing and recreates it. During those few minutes, the single node we're left with is not in the same zone. As we already mentioned, each EBS instance is tied to a zone, and the one we mounted to the Jenkins Pod would not be associated with the zone where the other EC2 instance is running. As a result, the PersistentVolume could not re-bound the EBS volume and, therefore, the failed container could not be recreated, until the failed EC2 instance is recreated.</p>
<p>The chances are that the new EC2 instance would not be in the same zone as the one where the failed server was running. Since we're using three availability zones, and one of them already has an EC2 instance, AWS would recreate the failed server in one of the other two zones. We'd have fifty percent chances that the new EC2 would be in the same zone as the one where the failed server was running. Those are not good odds.</p>
<p>In the real-world scenario, we'd probably have more than two worker nodes. Even a slight increase to three nodes would give us a very good chance that the failed server would be recreated in the same zone. Auto-scaling groups are trying to distribute EC2 instances more or less equally across all the zones. However, that is not guaranteed to happen. A good minimum number of worker nodes would be six.</p>
<p>The more servers we have, the higher are the chances that the cluster is fault tolerant. That is especially true if we are hosting stateful applications. As it goes, we almost certainly have those. There's hardly any system that does not have a state in some form or another.</p>
<p>If it's better to have more servers than less, we might be in a complicated position if our system is small and needs, let's say, less than six servers. In such cases, I'd recommend running smaller VMs. If, for example, you planned to use three <kbd>t2.xlarge</kbd> EC2 instances for worker nodes, you might reconsider that and switch to six <kbd>t2.large</kbd> servers. Sure, more nodes mean more resource overhead spent on operating systems, Kubernetes system Pods, and few other things. However, I believe that is compensated with bigger stability of your cluster.</p>
<p>There is still one more situation we might encounter. A whole availability zone (data center) might fail. Kubernetes will continue operating correctly. It'll have two instead of three master nodes, and the failed worker nodes will be recreated in healthy zones. However, we'd run into trouble with our stateful services. Kubernetes would not be able to reschedule those that were mounted to EBS volumes from the failed zone. We'd need to wait for the availability zone to come back online, or we'd need to move the EBS volume to a healthy zone manually. The chances are that, in such a case, the EBS would not be available and, therefore, could not be moved.</p>
<p>We could create a process that would be replicating data in (near) real-time between EBS volumes spread across multiple availability zones, but that also comes with a downside. Such an operation would be expensive and would likely slow down state retrieval while everything is fully operational. Should we choose lower performance over high-availability? Is the increased operational overhead worth the trouble? The answer to those questions will differ from one use-case to another.</p>
<p>There is yet another option. We could use <a href="https://aws.amazon.com/efs/">EFS</a> (<a href="https://aws.amazon.com/efs/" target="_blank"><span class="URLPACKT">https://aws.amazon.com/efs/</span></a>) instead of EBS. But, that would also impact performance since EFS tends to be slower than EBS. On top of that, there is no production-ready EFS support in Kubernetes. At the time of this writing, the EFS provisioner (<a href="https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs" target="_blank"><span class="URLPACKT">https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs</span></a>) is still in beta phase. By the time you read this, things might have changed. Or maybe they didn't. Even when the <em>efs provisioner</em> becomes stable, it will still be slower and more expensive solution than EBS.</p>
<p>Maybe you'll decide to ditch EBS (and EFS) in favor of some other type of persistent storage. There are many different options you can choose. We won't explore them since an in-depth comparison of all the popular solutions would require much more space than what we have left. Consider them an advanced topic that will be covered in the next book. Or maybe it won't. I do not yet know the scope of <em>The DevOps 2.4 Toolkit</em> book.</p>
<p>All in all, every solution has pros and cons and none would fit all use-cases. For good or bad, we'll stick with EBS for the remainder of this book.</p>
<p>Going back to PersistentVolumes tied to EBS...</p>
<p>Now that we explored how to manage static persistent volumes, we'll try to accomplish the same results using dynamic approach. But, before we do that, we'll see what happens when some of the resources we created are removed.</p>
<p>Let's delete the <kbd>jenkins</kbd> Deployment.</p>
<pre><strong>kubectl --namespace jenkins delete \</strong>
<strong>    deploy jenkins</strong>  </pre>
<p>The output shows us that the <kbd>deployment "jenkins" was deleted</kbd>.</p>
<p>Did anything happen with the PersistentVolumeClaim and the PersistentVolume?</p>
<pre><strong>kubectl --namespace jenkins get pvc</strong>
    
<strong>kubectl get pv</strong>  </pre>
<p>The combined output of both commands is as follows:</p>
<pre><strong>NAME    STATUS VOLUME        CAPACITY ACCESS MODES STORAGECLASS   AGE</strong>
<strong>jenkins Bound  manual-ebs-02 5Gi      RWO          manual-ebs     57s</strong>
    
<strong>NAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS    CLAIM           STORAGECLASS REASON AGE</strong>
<strong>manual-ebs-01 5Gi      RWO          Retain         Available jenkins/jenkins manual-ebs          10m</strong>
<strong>manual-ebs-02 5Gi      RWO          Retain         Bound     jenkins/jenkins manual-ebs          10m</strong>
<strong>manual-ebs-03 5Gi      RWO          Retain         Available jenkins/jenkins manual-ebs          10m</strong></pre>
<p>Even though we removed Jenkins Deployment and, with it, the Pod that used the claim, both the PersistentVolumeClaim and PersistentVolumes are intact. The <kbd>manual-ebs-01</kbd> volume is still bound to the <kbd>jenkins</kbd> claim.</p>
<p>What would happen if we remove the PersistentVolumeClaim <kbd>jenkins</kbd>?</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    delete pvc jenkins</strong>  </pre>
<p>The output shows that the <kbd>persistentvolumeclaim "jenkins" was deleted</kbd>.</p>
<p>Now, let's see what happened with the PersistentVolumes:</p>
<pre><strong>kubectl get pv</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS   CLAIM          STORAGECLASS REASON AGE</strong>
<strong>manual-ebs-01 5Gi      RWO          Retain         Available jenkins/jenkins manual-ebs          10m</strong>
<strong>manual-ebs-02 5Gi      RWO          Retain         Released  jenkins/jenkins manual-ebs          10m</strong>
<strong>manual-ebs-03 5Gi      RWO          Retain         Available jenkins/jenkins manual-ebs          10m</strong></pre>
<p>This time, the <kbd>manual-ebs-2</kbd> volume is <kbd>Released</kbd>.</p>
<p>This might be a good moment to explain the <kbd>Retain</kbd> policy applied to the PersistentVolumes we created.</p>
<p><kbd>ReclaimPolicy</kbd> defines what should be done with a volume after it's released from its claim. The policy was applied the moment we deleted the PersistentVolumeClaim that was bound to <kbd>manual-ebs-02</kbd>. When we created the PersistentVolumes, we did not specify <kbd>ReclaimPolicy</kbd>, so the volumes were assigned the default policy which is <kbd>Retain</kbd>.</p>
<p>The <kbd>Retain</kbd> reclaim policy enforces manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists, and the volume is considered <kbd>released</kbd>. But it is not yet available for other claims because the previous claimant's data remains on the volume. In our case, that data is Jenkins state. If we'd like this PersistentVolume to become available, we'd need to delete all the data on the EBS volume.</p>
<p>Since we are running the cluster in AWS, it is easier to delete than to recycle resources, so we'll remove the released PersistentVolume instead of trying to clean everything we generated inside the EBS. Actually, we'll remove all the volumes since we are about to explore how we can accomplish the same effects dynamically.</p>
<p>The other two reclaim policies are <kbd>Recycle</kbd> and <kbd>Delete</kbd>. <kbd>Recycle</kbd> is considered deprecated so we won't waste time explaining it. The <kbd>Delete</kbd> policy requires dynamic provisioning, but we'll postpone the explanation until we explore that topic.</p>
<p>Let's delete some stuff:</p>
<pre><strong>kubectl delete -f pv/pv.yml</strong> </pre>
<p>The output is as follows:</p>
<pre><strong>persistentvolume "manual-ebs-01" deleted</strong>
<strong>persistentvolume "manual-ebs-02" deleted</strong>
<strong>persistentvolume "manual-ebs-03" deleted</strong> </pre>
<p>We can see that all three PersistentVolumes were deleted. However, only Kubernetes resources were removed. We still need to manually delete the EBS volumes.</p>
<p>If you go to your AWS console, you'll see that all three EBS volumes are now in the <kbd>available</kbd> state and waiting to be mounted. We'll delete them all:</p>
<pre><strong>aws ec2 delete-volume \</strong>
<strong>    --volume-id $VOLUME_ID_1</strong>
    
<strong>aws ec2 delete-volume \</strong>
<strong>    --volume-id $VOLUME_ID_2</strong>
    
<strong>aws ec2 delete-volume \</strong>
<strong>    --volume-id $VOLUME_ID_3</strong></pre>
<p>We are finished with our tour around manual creation of persistent volumes. If we'd use this approach to volume management, cluster administrator would need to ensure that there is always an extra number of available volumes that can be used by new claims. It is tedious work that often results in having more volumes than we need. On the other hand, if we don't have a sufficient number of available (unused) volumes, we're risking that someone will create a claim that will not find a suitable volume to mount.</p>
<p>Manual volume management is sometimes unavoidable, especially if chose to use on-prem infrastructure combined with NFS. However, this is not our case. AWS is all about dynamic resource provisioning, and we'll exploit that to its fullest.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using storage classes to dynamically provision persistent volumes</h1>
                </header>
            
            <article>
                
<p>So far, we used static PersistentVolumes. We had to create both EBS volumes and Kubernetes PersistentVolumes manually. Only after both became available were we able to deploy Pods that are mounting those volumes through PersistentVolumeClaims. We'll call this process static volume provisioning.</p>
<p>In some cases, static volume provisioning is a necessity. Our infrastructure might not be capable of creating dynamic volumes. That is often the case with on-premise infrastructure with volumes based on NFS. Even then, with a few tools, a change in processes, and right choices for supported volume types, we can often reach the point where volume provisioning is dynamic. Still, that might prove to be a challenge with legacy processes and infrastructure.</p>
<p>Since our cluster is in AWS, we cannot blame legacy infrastructure for provisioning volumes manually. Indeed, we could have jumped straight into this section. After all, AWS is all about dynamic infrastructure management. However, I felt that it will be easier to understand the processes by exploring manual provisioning first. The knowledge we obtained thus far will help us understand better what's coming next. The second reason for starting with manual provisioning lies in my inability to predict your plans. Maybe you will run a Kubernetes cluster on infrastructure that has to be static. Even though we're using AWS for the examples, everything you learned this far can be implemented on static infrastructure. You'll only have to change EBS with NFS and go through NFSVolumeSource (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core</span></a>) documentation. There are only three NFS-specific fields so you should be up-and-running in no time.</p>
<p>Before we discuss how to enable dynamic persistent volume provisioning, we should understand that it will be used only if none of the static PersistentVolumes match our claims. In other words, Kubernetes will always select statically created PersistentVolumes over dynamic ones.</p>
<p>Dynamic volume provisioning allows us to create storage on-demand. Instead of manually pre-provisioning storage, we can provision it automatically when a resource requests it.</p>
<p>We can enable dynamic provisioning through the usage of StorageClasses from the <kbd>storage.k8s.io</kbd> API group. They allow us to describe the types of storage that can be claimed. On the one hand, cluster administrator can create as many StorageClasses as there are storage flavours. On the other hand, the users of the cluster do not have to worry about the details of each available external storage. It's a win-win situation where the administrators do not have to create PersistentVolumes in advance, and the users can simply claim the storage type they need.</p>
<p>To enable dynamic provisioning, we need to create at least one StorageClass object. Luckily for us, kops already set up a few, so we might just as well take a look at the StorageClasses currently available in our cluster:</p>
<pre><strong>kubectl get sc</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME          PROVISIONER           AGE</strong>
<strong>default       kubernetes.io/aws-ebs 44m</strong>
<strong>gp2 (default) kubernetes.io/aws-ebs 44m</strong>  </pre>
<p>We can see that there are two StorageClasses in our cluster. Both are using the same <kbd>aws-ebs</kbd> provisioner. Besides the names, the only difference, at least in this output, is that one of them is marked as <kbd>default</kbd>. We'll explore what that means a bit later. For now, we'll trust that kops configured those classes correctly and try to claim a PersistentVolume.</p>
<p>Let's take a quick look at yet another <kbd>jenkins</kbd> definition:</p>
<pre><strong>cat pv/jenkins-dynamic.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>...</strong>
<strong>kind: PersistentVolumeClaim</strong>
<strong>apiVersion: v1</strong>
<strong>metadata:</strong>
<strong>  name: jenkins</strong>
<strong>  namespace: jenkins</strong>
<strong>spec:</strong>
<strong>  storageClassName: gp2</strong>
<strong>  accessModes:</strong>
<strong>    - ReadWriteOnce</strong>
<strong>  resources:</strong>
<strong>    requests:</strong>
<strong>      storage: 1Gi</strong>
<strong>...</strong>  </pre>
<p>This Jenkins definition is almost the same as the one we used before. The only difference is in the PersistentVolumeClaim that, this time, specified <kbd>gp2</kbd> as the <kbd>StorageClassName</kbd>. There is one more difference though. This time we do not have any PersistentVolume pre-provisioned. If everything works as expected, a new PersistentVolume will be created dynamically.</p>
<pre><strong>kubectl apply \</strong>
<strong>    -f pv/jenkins-dynamic.yml \</strong>
<strong>    --record</strong></pre>
<p>We can see that some of the resources were re-configured, while others were created.</p>
<p>Next, we'll wait until the <kbd>jenkins</kbd> Deployment is rolled out successfully:</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    rollout status \</strong>
<strong>    deployment jenkins</strong></pre>
<p>Now we should be able to see what happened through the <kbd>jenkins</kbd> namespace events.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    get events</strong></pre>
<p>The output, limited to the last few lines, is as follows:</p>
<pre><strong>...</strong>
<strong>20s 20s 1 jenkins.... Deployment            Normal ScalingReplicaSet     deployment-controller       Scaled up replica set jenkins-... to 1</strong>
<strong>20s 20s 1 jenkins.... PersistentVolumeClaim Normal ProvisioningSucceeded persistentvolume-controller Successfully provisioned volume pvc-... using kubernetes.io/aws-ebs</strong></pre>
<p>We can see that a new PersistentVolume was <kbd>successfully provisioned</kbd>.</p>
<p>Let's take a look at the status of the PersistentVolumeClaim.</p>
<pre><strong>kubectl --namespace jenkins get pvc</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME    STATUS VOLUME  CAPACITY ACCESS MODES STORAGECLASS AGE<br/>jenkins Bound  pvc-... 1Gi      RWO          gp2          1m</strong></pre>
<p>The part of the output that matters is the status. We can see that it <kbd>Bound</kbd> with the PersistentVolume thus confirming, again, that the volume was indeed created dynamically.</p>
<p>To be on the safe side, we'll list the PersistentVolumes as well:</p>
<pre><strong>kubectl get pv</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME    CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM                STORAGECLASS REASON AGE</strong>
<strong>pvc-... 1Gi      RWO          Delete         Bound  jenkins/jenkins gp2                      4m</strong>
  </pre>
<p>As expected, the PersistentVolume was created, it is bound to the PersistentVolumeClaim, and its reclaim policy is <kbd>Delete</kbd>. We'll see the policy in action soon.</p>
<p>Finally, the last verification we'll perform is to confirm that the EBS volume was created as well:</p>
<pre><strong>aws ec2 describe-volumes \</strong>
<strong>    --filters 'Name=tag-key,Values="kubernetes.io/created-for/pvc<br/> /name"'</strong>
  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>{</strong>
<strong>  "Volumes": [</strong>
<strong>    {</strong>
<strong>      "AvailabilityZone": "us-east-2c",</strong>
<strong>      ...</strong>
<strong>      "VolumeType": "gp2",</strong>
<strong>      "VolumeId": "vol-0a4d5cfa4699e5c6f",</strong>
<strong>      "State": "in-use",</strong>
<strong>      ...</strong>
<strong>    }</strong>
<strong>  ]</strong>
<strong>}</strong>  </pre>
<p>We can see that a new EBS volume was created in the availability zone <kbd>us-east-2c</kbd>, that the type is <kbd>gp2</kbd>, and that its state is <kbd>in-use</kbd>.</p>
<p>Dynamic provisioning works! Given that we're using AWS, it is a much better solution than using static resources.</p>
<p>Before we move into a next subject, we'll explore the effect of the reclaim policy <kbd>Delete</kbd>. To do so, we'll delete the Deployment and the PersistentVolumeClaim.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    delete deploy,pvc jenkins</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>deployment "jenkins" deleted</strong>
<strong>persistentvolumeclaim "jenkins" deleted</strong></pre>
<p>Now that the claim to the volume was removed, we can check what happened with the dynamically provisioned PersistentVolumes.</p>
<pre><strong>kubectl get pv</strong>  </pre>
<p>The output shows that <kbd>no resources</kbd> were found, clearly indicating that the PersistentVolume that was created through the claim is now gone.</p>
<p>How about the AWS EBS volume? Was it removed as well?</p>
<pre><strong>aws ec2 describe-volumes \</strong>
<strong>    --filters 'Name=tag-key,Values="kubernetes.io/created-for/pvc/name"'</strong>
  </pre>
<p>The output is as follows:</p>
<pre><strong>{</strong>
<strong>  "Volumes": []</strong>
<strong>}</strong>  </pre>
<p>We got an empty array proving that the EBS volume was removed as well.</p>
<p>Through dynamic volume provisioning, not only that volumes are created when resources claim them, but they are also removed when the claims are released. Dynamic removal is accomplished through the reclaim policy <kbd>Delete</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using default storage classes</h1>
                </header>
            
            <article>
                
<p>Working with dynamic provisioning simplifies a few things. Still, a user needs to know which volume type to use. While in many cases that is an important choice, there are often situations when a user might not want to worry about that. It might be easier to use the cluster administrator's choice for volume types and let all claims that do not specify <kbd>storageClassName</kbd> get a default volume. We'll try to accomplish that through one of the admission controllers.</p>
<p>Admission controllers are intercepting requests to the Kubernetes API server. We won't go into details of admission controllers since the list of those supported by Kubernetes is relatively big. We are interested only in the <kbd>DefaultStorageClass</kbd> which happens to be already enabled in the cluster we created with kops.</p>
<p><kbd>DefaultStorageClass</kbd> admission controller observes creation of PersistentVolumeClaims. Through it, those that do not request any specific storage class are automatically added a default storage class to them. As a result, PersistentVolumeClaims that do not request any special storage class are bound to PersistentVolumes created from the default <kbd>StorageClass</kbd>. From user's perspective, there's no need to care about volume types since they will be provisioned based on the default type unless they choose a specific class.</p>
<p>Let's take a look at the storage classes currently available in our cluster:</p>
<pre><strong>kubectl get sc</strong></pre>
<p>The output is as follows:</p>
<pre><strong>NAME          PROVISIONER           AGE</strong>
<strong>default       kubernetes.io/aws-ebs 56m</strong>
<strong>gp2 (default) kubernetes.io/aws-ebs 56m</strong>  </pre>
<p>This is not the first time we're listing the storage classes in our cluster. However, we did not discuss that one of the two (<kbd>gp2</kbd>) is marked as the default <kbd>StorageClass</kbd>.</p>
<p>Let's describe the <kbd>gp2</kbd> class.</p>
<pre><strong>kubectl describe sc gp2</strong> </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>Name:            gp2</strong>
<strong>IsDefaultClass:  Yes</strong>
<strong>Annotations:     kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.beta.kubernetes.io/is-default-class":"true"},"labels":{"k8s-addon":"storage-aws.addons.k8s.io"},"name":"gp2","namespace":""},"parameters":{"type":"gp2"},"provisioner":"kubernetes.io/aws-ebs"}</strong>
<strong>,storageclass.beta.kubernetes.io/is-default-class=true</strong>
<strong>Provisioner:    kubernetes.io/aws-ebs</strong>
<strong>Parameters:     type=gp2</strong>
<strong>ReclaimPolicy:  Delete</strong>
<strong>Events:         &lt;none&gt;</strong>  </pre>
<p>The important part lies in the annotations. One of them is <kbd>".../is-default-class":"true"</kbd>. It sets that <kbd>StorageClass</kbd> as default. As a result, it will be used to create PersistentVolumes by any PersistentVolumeClaim that does not specify StorageClass name.</p>
<p>Let's try to adapt Jenkins stack to use the ability to dynamically provision a volume associated with the <kbd>DefaultStorageClass</kbd>.</p>
<p>The new Jenkins definition is as follows:</p>
<pre><strong>cat pv/jenkins-default.yml</strong></pre>
<p>The output, limited to the <kbd>PersistentVolumeClaim</kbd>, is as follows.</p>
<pre><strong>...</strong>
<strong>kind: PersistentVolumeClaim</strong>
<strong>apiVersion: v1</strong>
<strong>metadata:</strong>
<strong>  name: jenkins</strong>
<strong>  namespace: jenkins</strong>
<strong>spec:</strong>
<strong>  accessModes:</strong>
<strong>    - ReadWriteOnce</strong>
<strong>  resources:</strong>
<strong>    requests:</strong>
<strong>      storage: 1Gi</strong>
<strong>...</strong>  </pre>
<p>It's hard to spot the difference between that YAML file and the one we used before. It is very small and hard to notice change so we'll execute <kbd>diff</kbd> to compare the two:</p>
<pre><strong>diff pv/jenkins-dynamic.yml \</strong>
<strong>    pv/jenkins-default.yml</strong></pre>
<p>The output is as follows:</p>
<pre><strong>48d47</strong>
<strong>&lt;   storageClassName: gp2</strong>  </pre>
<p>As you can see, the only difference is that <kbd>pv/jenkins-dynamic.yml</kbd> doesn't have <kbd>storageClassName: gp2</kbd>. That field is omitted from the new definition. Our new <kbd>PersistentVolumeClaim</kbd> does not have an associated StorageClass.</p>
<p>Let's <kbd>apply</kbd> the new definition:</p>
<pre><strong>kubectl apply \</strong>
<strong>    -f pv/jenkins-default.yml \</strong>
<strong>    --record</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>namespace "jenkins" configured</strong>
<strong>ingress "jenkins" configured</strong>
<strong>service "jenkins" configured</strong>
<strong>persistentvolumeclaim "jenkins" created</strong>
<strong>deployment "jenkins" created</strong>  </pre>
<p>What we're interested in are PersistentVolumes, so let's retrieve them.</p>
<pre><strong>kubectl get pv</strong>
<strong>NAME    CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM                STORAGECLASS REASON AGE</strong>
<strong>pvc-... 1Gi      RWO          Delete         Bound  jenkins/jenkins gp2                      16s</strong>
  </pre>
<p>As you can see, even though we did not specify any StorageClass, a volume was created based on the <kbd>gp2</kbd> class, which happens to be the default one.</p>
<p>We'll delete the <kbd>jenkins</kbd> Deployment and PersistentVolumeClaim before we explore how we can create our own StorageClasses.</p>
<pre><strong>kubectl --namespace jenkins \</strong>
<strong>    delete deploy,pvc jenkins</strong></pre>
<p>The output is as follows:</p>
<pre><strong>deployment "jenkins" deleted</strong>
<strong>persistentvolumeclaim "jenkins" deleted</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating storage classes</h1>
                </header>
            
            <article>
                
<p>Even though kops created two StorageClasses, both are based on <kbd>gp2</kbd>. While that is the most commonly used EBS type, we might want to create volumes based on one of the other three options offered by AWS.</p>
<p>Let's say that we want the fastest EBS volume type for our Jenkins. That would be <kbd>io1</kbd>. Since kops did not create a StorageClass of that type, we might want to create our own.</p>
<p>YAML file that creates StorageClass based on EBS <kbd>io1</kbd> is defined in <kbd>pv/sc.yml</kbd>. Let's take a quick look.</p>
<pre><strong>cat pv/sc.yml</strong>  </pre>
<p><span>The output is as follows:</span></p>
<pre><strong>kind: StorageClass</strong>
<strong>apiVersion: storage.k8s.io/v1</strong>
<strong>metadata:</strong>
<strong>  name: fast</strong>
<strong>  labels:</strong>
<strong>    type: ebs<br/>provisioner: kubernetes.io/aws-ebs</strong>
<strong>parameters:</strong>
<strong>  type: io1</strong>
<strong>reclaimPolicy: Delete</strong>  </pre>
<p>We used <kbd>kubernetes.io/aws-ebs</kbd> as the <kbd>provisioner</kbd>. It is a mandatory field that determines the plugin that will be used for provisioning PersistentVolumes. Since we are running the cluster in AWS, <kbd>aws-ebs</kbd> is the logical choice. There are quite a few other provisioners we could choose. Some of them are specific to a hosting provider (for example, <kbd>GCEPersistentDisk</kbd> and <kbd>AzureDisk</kbd>) while others can be used anywhere (for example, <kbd>GlusterFS</kbd>).</p>
<p>The list of supported provisioners is growing. At the time of this writing, the following types are supported:</p>
<div>
<div>
<table border="1" class="MsoTableGrid" style="width: 602px;height: 1443px">
<tbody>
<tr>
<td style="width: 382px">
<p class="CDPAlignCenter CDPAlign">Volume Plugin</p>
</td>
<td style="width: 204px">
<p class="CDPAlignCenter CDPAlign">Internal Provisioner</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>AWSElasticBlockStore</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>AzureFile</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>AzureDisk</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>CephFS</p>
</td>
<td style="width: 204px">
<p>no</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>Cinder</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>FC</p>
</td>
<td style="width: 204px">
<p>no</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>FlexVolume</p>
</td>
<td style="width: 204px">
<p>no</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>Flocker</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>GCEPersistentDisk</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>Glusterfs</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>iSCSI</p>
</td>
<td style="width: 204px">
<p>no</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>PhotonPersistentDisk</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>Quobyte</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>NFS</p>
</td>
<td style="width: 204px">
<p>no</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>RBD</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>VsphereVolume</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>PortworxVolume</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>ScaleIO</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>StorageOS</p>
</td>
<td style="width: 204px">
<p>yes</p>
</td>
</tr>
<tr>
<td style="width: 382px">
<p>Local</p>
</td>
<td style="width: 204px">
<p>no</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p class="mce-root">The internal provisioners are those with names prefixed with <kbd>kubernetes.io</kbd> (for example, <kbd>kubernetes.io/aws-ebs</kbd>). They are shipped with Kubernetes. External provisioners, on the other hand, are independent programs shipped separately from Kubernetes. An example of a commonly used external provisioner is <kbd>NFS</kbd>. The parameters depend on the StorageClass. We used the <kbd>aws-ebs</kbd> provisioner which allows us to specify the <kbd>type</kbd> parameter that defines one of the supported Amazon EBS volume types. It can be EBS Provisioned IOPS SSD (<kbd>io1</kbd>), EBS <strong>General Purpose SSD</strong> (<strong>gp2</strong>), Throughput Optimized HDD (<kbd>st1</kbd>), and Cold HDD (<kbd>sc1</kbd>). We set it to <kbd>io1</kbd> which is the highest performance SSD volume. Please consult <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters"><em>Parameters</em></a> (<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters" target="_blank"><span class="URLPACKT">https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters</span></a>) section of the <em>Storage Classes</em> documentation for more info. Finally, we set the <kbd>reclaimPolicy</kbd> to <kbd>Delete</kbd>. Unlike <kbd>Retain</kbd> that forces us to delete the contents of the released volume before it becomes available to new PersistentVolumeClaims, <kbd>Delete</kbd> removes both the PersistentVolume as well as the associated volume in the external architecture. The <kbd>Delete</kbd> reclaim policy works only with some of the external volumes like AWS EBS, Azure Disk, or Cinder volume. Now that we dipped our toes into the StorageClass definition, we can proceed and create it.</p>
<pre class="mce-root"><strong>kubectl create -f pv/sc.yml</strong></pre>
<p class="mce-root">The output shows that the <kbd>storageclass "fast" was created</kbd>, so we'll list, one more time, the StorageClassses in our cluster.</p>
<pre class="mce-root"><strong>kubectl get sc</strong></pre>
<p class="mce-root">The output is as follows:</p>
<pre class="mce-root"><strong>NAME          PROVISIONER           AGE<br/></strong><strong>default       kubernetes.io/aws-ebs 58m<br/></strong><strong>fast          kubernetes.io/aws-ebs 19s<br/></strong><strong>gp2 (default) kubernetes.io/aws-ebs 58m</strong></pre>
<p class="mce-root">We can see that, this time, we have a new StorageClass.</p>
<p class="mce-root">Let's take a look at yet another Jenkins definition.</p>
<pre class="mce-root"><strong>cat pv/jenkins-sc.yml</strong></pre>
<p class="mce-root">The output, limited to the relevant parts, is as follows:</p>
<pre class="mce-root"><strong>...<br/></strong><strong>kind: PersistentVolumeClaim<br/></strong><strong>apiVersion: v1<br/></strong><strong>metadata:<br/></strong><strong>  name: jenkins<br/></strong><strong>  namespace: jenkins<br/></strong><strong>spec:<br/></strong><strong>  storageClassName: fast<br/></strong><strong>  accessModes:<br/></strong><strong>    - ReadWriteOnce<br/></strong><strong>  resources:<br/></strong><strong>    requests:<br/></strong><strong>      storage: 4Gi<br/></strong><strong>...</strong></pre>
<p class="mce-root">The only difference, when compared with the previous definition, is that we are now using the newly created StorageClass named <kbd>fast</kbd>.</p>
<p class="mce-root">Finally, we'll confirm that the new StorageClass works by deploying the new <kbd>jenkins</kbd> definition.</p>
<pre class="mce-root"><strong>kubectl apply \<br/></strong><strong>    -f pv/jenkins-sc.yml \<br/></strong><strong>    --record</strong></pre>
<p class="mce-root">The output is as follows:</p>
<pre class="mce-root"><strong>namespace "jenkins" configured<br/></strong><strong>ingress "jenkins" configured<br/></strong><strong>service "jenkins" configured<br/></strong><strong>persistentvolumeclaim "jenkins" created<br/></strong><strong>deployment "jenkins" created</strong></pre>
<p class="mce-root">As the final verification, we'll list the EBS volumes and confirm that a new one was created based on the new class.</p>
<pre class="mce-root">aws ec2 describe-volumes \ <br/>    --filters 'Name=tag-key,Values="kubernetes.io/created-for/pvc/name"' </pre>
<p class="mce-root">The output, limited to the relevant parts, is as follows:</p>
<pre class="mce-root"><strong>{<br/></strong><strong>   "Volumes": [<br/></strong><strong>   {<br/></strong><strong>      ...</strong> <strong>"VolumeType": "io1",<br/></strong><strong>      "VolumeId": "vol-0e0af4f2a7a54354d",<br/></strong><strong>      "State": "in-use",<br/></strong><strong>      ...<br/></strong>    <strong>}<br/></strong><strong>  ]<br/>}</strong></pre>
<p class="mce-root">We can see that the type of the newly created EBS volume is <kbd>io1</kbd> and that it is <kbd>in-use</kbd>.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a3596ac4-ca51-4225-aaee-457ec369a86e.png" style="width:31.75em;height:26.83em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 15-5: The sequence of events initiated with a request to create a Jenkins Pod with the PersistentVolumeClaim using a custom StorageClass</div>
<p class="mce-root">A simplified version of the flow of events initiated with the creation of the <kbd>jenkins</kbd> Deployment is as follows:</p>
<ol>
<li>We created the <kbd>jenkins</kbd> Deployment, which created a ReplicaSet, which, in turn, created a Pod.</li>
<li>The Pod requested persistent storage through the PersistentVolumeClaim.</li>
<li>The PersistentVolumeClaim requested PersistentStorage with the StorageClass name <kbd>fast</kbd>.</li>
<li>StorageClass <kbd>fast</kbd> is defined to create a new EBS volume, so it requested one from the AWS API.</li>
<li>AWS API created a new EBS volume.</li>
<li>EBS volume was mounted to the <kbd>jenkins</kbd> Pod.</li>
</ol>
<p class="mce-root">We're finished exploring persistent volumes. You should be equipped with the knowledge how to persist your stateful applications, and the only pending action is to remove the volumes and the cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What now?</h1>
                </header>
            
            <article>
                
<p class="mce-root">There's nothing left to do but to destroy what we did so far.</p>
<p class="mce-root">This time, we cannot just delete the cluster. Such an action would leave the EBS volumes running. So, we need to remove them first.</p>
<p class="mce-root">We could remove EBS volumes through AWS CLI. However, there is an easier way. If we delete all the claims to EBS volumes, they will be deleted as well since our PersistentVolumes are created with the reclaim policy set to <kbd>Delete</kbd>. EBS volumes are created when needed and destroyed when not.</p>
<p class="mce-root">Since all our claims are in the <kbd>jenkins</kbd> namespace, removing it is the easiest way to delete them all.</p>
<pre class="mce-root"><strong>kubectl delete ns jenkins</strong></pre>
<p class="mce-root">The output shows that the <kbd>namespace "jenkins" was deleted</kbd> and we can proceed to delete the cluster as well.</p>
<pre class="mce-root"><strong>kops delete cluster \<br/></strong><strong>--name $NAME \<br/></strong><strong>--yes</strong></pre>
<p class="mce-root">We can see from the output that the cluster <kbd>devops23.k8s.local</kbd> was deleted and we are left only with the S3 bucket used for kops state. We'll delete it as well.</p>
<pre class="mce-root"><strong>aws s3api delete-bucket \<br/></strong><strong>--bucket $BUCKET_NAME</strong></pre>
<p class="mce-root">Before you leave, please consult the following API references for more information about volume-related resources.</p>
<ul>
<li class="mce-root"><a href="https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolume-v1-core">PersistentVolume v1 core</a> <span>(</span><a href="https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage" target="_blank"><span class="URLPACKT">https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage</span></a><span>)</span></li>
<li class="mce-root"><a href="https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core">PersistentVolumeClaim v1 core</a> (<a href="https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core" target="_blank"><span class="URLPACKT">https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core</span></a>)</li>
<li class="mce-root"><a href="https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage">StorageClass v1 storage</a> (<a href="https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage" target="_blank"><span class="URLPACKT">https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage</span></a>)</li>
</ul>
<p class="mce-root">That's it. There's nothing left.</p>


            </article>

            
        </section>
    </body></html>