- en: Chapter 8. Optimize for Scale and Cost
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 优化规模和成本
- en: 'On the subject of optimization, we shall start from the top, that is to say
    the earliest stage: the design stage.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化方面，我们将从顶部开始，也就是说从设计阶段开始。
- en: Imagine iterating over your architecture plan time and time again, until you
    have convinced yourself and your colleagues that this is the best you can do with
    the information available at that time. Now imagine that, unless you have a very
    unusual use case, other people have already done similar iterations and have kindly
    shared the outcome.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一遍又一遍地审视你的架构计划，直到你和你的同事们都确信，这是在当前信息条件下你能做到的最好的。现在想象一下，除非你有一个非常特殊的用例，否则其他人已经进行了类似的迭代，并慷慨地分享了结果。
- en: Back to reality and fortunately, we were not far off. There is indeed a collective
    AWS knowledge base in the form of blog posts, case studies, and white papers available
    to anybody embarking on their first cloud deployment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回到现实中，幸运的是，我们并不远离。确实存在一个AWS的集体知识库，以博客文章、案例研究和白皮书的形式向任何开始他们的第一个云部署的人提供。
- en: We are going to take a distilled sample of that knowledge base and apply it
    to a common architecture example, in an attempt to achieve maximum scalability,
    whilst remaining cost efficient.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这些知识的精华样本开始，并将其应用于一个常见的架构示例中，试图在保持成本效益的同时实现最大的可伸缩性。
- en: 'The example is going to be one of a typical frontend (NGINX nodes), backend
    (DB cluster) and a storage layer deployment within a VPC:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将是一个典型的前端（NGINX节点）、后端（数据库集群）和VPC内的存储层部署：
- en: '![Optimize for Scale and Cost](img/image_08_001.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![优化规模和成本](img/image_08_001.jpg)'
- en: Whilst, technically, our whole deployment is on the Internet, the visual segregation
    above is to emphasize the network isolation properties of a VPC.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从技术上讲，我们整个部署都在互联网上，但上面的视觉分隔是为了强调VPC的网络隔离属性。
- en: Architectural considerations
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构考虑
- en: Let us now examine this deployment one component at a time, starting with the
    VPC itself.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐个组件地检查这个部署，从VPC本身开始。
- en: The VPC
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VPC
- en: I am proceeding under the assumption that if you are still holding this book,
    you have likely accepted the way of the VPC.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设如果你仍然持有这本书，你可能已经接受了VPC的方式。
- en: CIDR
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CIDR
- en: How many VPCs are you foreseeing having? Would they be linked (VPC peering)
    or would you be bridging other networks in (VPN)?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你预计会有多少个VPC？它们会连接（VPC对等连接）还是会在其他网络中进行桥接（VPN）？
- en: The answers to these questions play a role when choosing the CIDR for a VPC.
    As a general rule it is recommended to avoid common (household router) network
    addresses such as `192.168.1.0` or `10.0.0.0`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的答案在选择VPC的CIDR时起到了作用。作为一个通则，建议避免常见的（家庭路由器）网络地址，如`192.168.1.0`或`10.0.0.0`。
- en: Keep track of and assign different CIDRs if you have more than one VPC, even
    if you don't have an immediate need to peer them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个VPC，即使你没有立即需要将它们对等，也要跟踪和分配不同的CIDR。
- en: Consider a CIDR that will allow for large enough subnets to accommodate potential
    instance scaling with minimal fragmentation (number of subnets).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个CIDR，它将允许足够大的子网以便在最小碎片化的情况下扩展实例（子网数量）。
- en: Subnets and Availability Zones
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子网和可用区
- en: '**Availability Zones** (**AZs**) are how we add resilience to a deployment,
    so we should have at least two of those. There might be configurations in which
    you have to use three, for example where a cluster quorum is needed, such as **ZooKeeper**.
    In that case, it is advisable to keep quorum members in separate zones in order
    to handle network partitions better. To accommodate this and to keep charges low,
    we could create subnets in three zones, deploy quorum clusters in all three, and
    other components (say **NGINX** hosts) in only two of those.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**可用区**（**AZs**）是我们如何为部署增加弹性的方式，因此我们应该至少有两个这样的区域。在某些配置中，例如需要集群仲裁的情况下，比如**ZooKeeper**，可能需要使用三个区域。在这种情况下，建议将仲裁成员放在不同的区域以更好地处理网络分区。为了适应这一点并保持费用低廉，我们可以在三个区域创建子网，在所有三个区域部署仲裁集群，并在其中的两个区域中部署其他组件（比如**NGINX**主机）。'
- en: 'Let us illustrate an example where we have a Zookeeper and a web server (NGINX)
    component within our VPC. We have decided to use three AZs and maintain two sets
    of subnets: **public** and **private**. The former routing through the IGW, the
    latter via NAT:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子，在这个例子中，我们在VPC中有一个Zookeeper和一个Web服务器（NGINX）组件。我们决定使用三个AZ并维护两组子网：**公共**和**私有**。前者通过IGW进行路由，后者通过NAT：
- en: '![Subnets and Availability Zones](img/image_08_002.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![子网和可用区](img/image_08_002.jpg)'
- en: Here we have the ELB spanning across all three AZs and public subnets respectively.
    In the private subnet space, we find two web servers plus our cluster of three
    ZooKeeper nodes giving us a good balance of resilience at optimal cost.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有跨越所有三个可用区（AZs）和公共子网的 ELB。在私有子网空间中，我们有两个 Web 服务器和三个 ZooKeeper 节点的集群，这为我们提供了在最优成本下的良好弹性平衡。
- en: VPC limits
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VPC 限制
- en: 'AWS enforces certain initial limits on every account, which might catch you
    by surprise when your environment starts scaling up. Important ones to check are: **Instances**,
    **EBS** and **Networking** limits found on the **EC2 dashboard**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 对每个账户强制实施某些初始限制，当你的环境开始扩展时，这些限制可能会让你感到意外。需要检查的重要限制有：**实例**、**EBS** 和 **网络**
    限制，这些可以在 **EC2 控制台**中找到：
- en: '![VPC limits](img/image_08_003.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![VPC 限制](img/image_08_003.jpg)'
- en: When requesting an increase, select a number that is high enough to provide
    a buffer for scaling, but not inadequately high as after all the limits are there
    to protect against accidental/erroneous overprovisioning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求扩展时，选择一个足够高的数字以提供扩展缓冲，但不要选择过高的数字，因为毕竟，限制是为了防止意外或错误的过度配置。
- en: The frontend layer
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前端层
- en: With the subnets in place, we can start thinking about our VPC inhabitants.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好子网后，我们可以开始考虑我们的 VPC 内的居民。
- en: The frontend or application layer consists of our Auto Scaling Groups and the
    first decision that we'll face would be that of an EC2 instance type.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 前端或应用层由我们的自动扩展组构成，第一个我们将面临的决定是选择 EC2 实例类型。
- en: The profile of the frontend application would very much dictate the choice between
    a memory, compute or a storage optimized instance. With some help from fellow
    developers (in the case of an in-house application) and a suitable performance
    testing tool (or service) you should be able to ascertain which system resource
    does the given application make most use of.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前端应用的配置将很大程度上决定选择内存优化型、计算优化型或存储优化型实例。借助来自同事（在内部应用的情况下）的帮助和合适的性能测试工具（或服务），你应该能够确定该应用程序主要使用哪种系统资源。
- en: Let us assume we have picked the **C4 Compute Optimized** instance class which
    AWS suggests for webservers. The next question will be - what size?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择了**C4 计算优化型**实例类，这是 AWS 推荐用于 Web 服务器的实例。下一个问题是——选择什么大小？
- en: Well, one way to guess our way through, is to take the average number of requests
    per second that we would like to be able to support, deploy the minimum number
    of instances we can afford (two for resilience) of the smallest size available
    in the chosen class and run a load test against them. Ideally the average utilization
    across the two nodes would remain under 50% to allow for traffic spikes and events
    of failure where the remaining host takes all the load. If the results are far
    below that mark, then we should look for a different class with smaller instance
    types for better value. Otherwise we keep increasing the C4 size.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一种猜测方法是：取我们希望能够支持的每秒请求的平均数量，部署我们能够承受的最小实例数量（为了弹性，至少选择两个实例），并选择该类中最小的实例大小，然后进行负载测试。理想情况下，两个节点的平均利用率应该保持在
    50% 以下，以便应对流量激增和故障事件，其中剩余的主机会承担所有负载。如果结果远低于这个标准，那么我们应该寻找一个不同的类，选择更小的实例类型，以获得更好的性价比。否则，我们将继续增加
    C4 实例的大小。
- en: 'Next comes the question of Auto Scaling. We have the right class and instance
    size to work with, and now we need scaling thresholds. Firstly, if you are fortunate
    enough to have predicable loads, then your problems end here with the use of **Scheduled
    Actions**:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是自动扩展的问题。我们已经选择了合适的类和实例大小，现在需要设置扩展阈值。首先，如果你幸运地拥有可预测的负载，那么使用**定时操作**，你的问题就可以解决了：
- en: '![The frontend layer](img/image_08_004.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![前端层](img/image_08_004.jpg)'
- en: You can simply tell AWS scale me up at *X* o'clock then back down at *Y*. The
    rest of us, we have to set alarms and thresholds.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以简单地告诉 AWS 在 *X* 点钟时扩展我，再在 *Y* 点钟时缩减。其他人则需要设置警报和阈值。
- en: We've already decided that a 50% average utilization (let us say CPU) is our
    upper limit and by that time we should already have scaling in progress. Otherwise,
    if one of our two nodes fails, at that rate the other one will have to work at
    maximum capacity. As an example a **CloudWatch** alarm could be >40% average CPU
    used for five minutes, triggering an Auto Scaling Group action to increase the
    group size by 50% (which is one instance).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经决定，50%的平均利用率（假设是CPU）是我们的上限，到那时我们应该已经开始扩展。否则，如果我们的两个节点中的一个发生故障，在这个速率下，另一个节点将不得不以最大容量工作。举个例子，**CloudWatch**告警可以设置为超过40%的平均CPU使用率持续五分钟，触发自动扩展组的动作，将组的大小增加50%（即一个实例）。
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In order to prevent unnecessary scaling events, it is important to adjust the
    value of the **Cooldown period**. It should reflect the expected time a newly
    launched instance will take to become fully operational and start affecting the
    **CloudWatch** metric.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止不必要的扩展事件，调整**冷却时间**的值非常重要。它应该反映出新启动的实例完全投入操作并开始影响**CloudWatch**指标所需的预期时间。
- en: 'For even finer control over how Auto Scaling reacts to the alarm we could use
    Step Scaling (ref: [http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html](http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html)).
    **Step Adjustments** allow for a varied response based on the severity of the
    threshold breach. For example, if the load increases from 40% to 50%, then scale
    up with only a single instance, but if the hop is from 40% to 70%, go straight
    to two or more.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更精细地控制自动扩展如何响应告警，我们可以使用步进扩展（参考：[http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html](http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html)）。**步进调整**根据阈值突破的严重程度，允许有不同的响应。例如，如果负载从40%增加到50%，则仅扩展一个实例；但如果负载从40%跃升到70%，则直接扩展到两个或更多实例。
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: With Step Scaling the **Cooldown period** is set via the **Instance Warmup**
    option.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步进扩展时，**冷却时间**是通过**实例预热**选项设置的。
- en: While we aim to scale up relatively quickly to prevent any service disruption,
    scaling down should be timely to save hourly charges, but not premature which
    could cause a scaling loop.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们希望快速扩展以防止任何服务中断，但缩减应该及时进行以节省按小时计费的费用，但又不能过早进行，以免导致扩展循环。
- en: The **CloudWatch** alarm for scaling down should act over a much longer period
    of time than the five minutes we observed earlier. Also the gap between the threshold
    for scaling up and the one for scaling down should be wide enough not to have
    instances launch, only to be terminated shortly after.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**CloudWatch**用于缩减的告警应持续的时间应远长于我们之前观察到的五分钟。而且，扩展和缩减的阈值之间的间隔应足够宽，以避免实例启动后很快被终止。'
- en: EC2 Instance utilization is just one example of a trigger; it is also worth
    considering ELB metrics such as sum of total request, non-2XX responses or response
    latency. If you choose to use any of those, ensure that your scale down alarms
    react to the **INSUFFICIENT_DATA** state which is observed during periods of no
    traffic (perhaps late at night).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: EC2实例利用率只是触发器的一个例子；还可以考虑ELB指标，如总请求数、非2XX响应或响应延迟。如果选择使用其中任何一种，请确保您的缩减告警能够响应**INSUFFICIENT_DATA**状态，这种状态通常出现在没有流量的时期（例如，深夜）。
- en: The backend layer
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后端层
- en: Behind the application we are likely to find a database cluster of some sort.
    For this example, we have chosen RDS (MySQL/PostgreSQL). However, the scaling
    and resilience ideas can be easily translated to suit a custom DB cluster on EC2
    instances.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序背后，我们很可能会发现某种类型的数据库集群。以这个例子为例，我们选择了RDS（MySQL/PostgreSQL）。然而，扩展和弹性设计可以轻松地转化为适用于EC2实例上的自定义数据库集群。
- en: Starting with high-availability, in terms of RDS, the feature is called a **Multi-AZ**
    deployment. This gives us a Primary RDS instance with a hot **STANDBY** replica
    as a failover solution. Unfortunately, the Standby cannot be used for anything
    else, that is to say we cannot have it, for example, serving read-only queries.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从高可用性的角度出发，对于RDS来说，相关功能称为**多可用区（Multi-AZ）**部署。这为我们提供了一个主RDS实例以及一个热备份**备用**副本作为故障转移解决方案。不幸的是，备用副本无法用于其他用途，也就是说，我们不能让它服务于只读查询等。
- en: 'A Multi-AZ setup within our VPC would look like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们VPC中的多可用区设置看起来是这样的：
- en: '![The backend layer](img/image_08_005.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![后端层](img/image_08_005.jpg)'
- en: In the case of a **PRIMARY** outage, RDS automatically fails over to the **STANDBY**,
    updating relevant DNS records in the process. According to the documentation,
    a typical failover takes one to two minutes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在**PRIMARY**发生故障时，RDS会自动切换到**STANDBY**，并在此过程中更新相关的DNS记录。根据文档，典型的故障切换大约需要一到两分钟。
- en: The triggers include the Primary becoming unavailable (thus failing AWS health-checks),
    a complete AZ outage, or a user interruption such as an RDS instance reboot.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 触发因素包括主节点不可用（因此AWS健康检查失败）、完整的AZ故障，或用户中断，如RDS实例重启。
- en: 'So far, with Multi-AZ we have a reasonably resilient, but perhaps not very
    scalable setup. In a busy environment it is common to dedicate a primary DB node
    for writes, while reading is done off of replicas. The inexpensive option would
    be to add a single replica to our current arrangement:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，使用Multi-AZ，我们拥有一个相对弹性强的设置，但可能不是非常可扩展。在忙碌的环境中，通常会为写入操作分配一个主DB节点，而读取操作则从副本中进行。便宜的选择是向当前配置中添加一个副本：
- en: '![The backend layer](img/image_08_006.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![后端层](img/image_08_006.jpg)'
- en: Here we write to **PRIMARY** and read from **REPLICA**, or for read-intensive
    applications reads can go to both.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据写入**PRIMARY**并从**REPLICA**读取，或者对于读密集型应用程序，读取可以同时从两者进行。
- en: 'If our budget allows, we can take this a step further and provide a **REPLICA**
    in both subnets in which we deploy frontend/application nodes:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的预算允许，我们可以进一步提升，并在我们部署前端/应用节点的两个子网中提供一个**REPLICA**：
- en: '![The backend layer](img/image_08_007.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![后端层](img/image_08_007.jpg)'
- en: Latency across AWS zones is already pretty low, but with such a per-zone RDS
    distribution, we reduce it even further. All hosts would write to the **PRIMARY**.
    However they can assign a higher priority to their local (same zone) **REPLICA**
    when reading.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 跨AWS区域的延迟已经非常低，但通过这种每区域的RDS分布，我们可以进一步降低延迟。所有主机都会写入**PRIMARY**，但是它们可以在读取时优先选择本地（同一区域）的**REPLICA**。
- en: And since we are on a spending spree, additional RDS performance boost can be
    gained with provisioned IOPS. This is something to consider if you are running
    a heavy workload and in need of high RDS Storage I/O.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们正在大手笔花费，额外的RDS性能提升可以通过预配置的IOPS来实现。如果你运行的是重负载并且需要高RDS存储I/O，这一点值得考虑。
- en: Although indirectly, caching is another very effective way to increase RDS scalability
    by alleviating the load.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管是间接的，缓存也是一种非常有效的方式，通过减轻负载来提高RDS的可扩展性。
- en: 'Popular software choices here are **Memcached** and **Redis**. Either is simple
    to setup locally (on each application host). If you would like to benefit from
    a shared cache then you could run a cluster on EC2 or use the AWS managed ElastiCache
    service. With the latter, we can have again a **Multi-AZ** configuration plus
    multiple replicas for resilience and low-latency:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里流行的软件选择包括**Memcached**和**Redis**。它们都可以轻松在本地设置（在每个应用主机上）。如果你希望利用共享缓存，可以在EC2上运行集群，或者使用AWS托管的ElastiCache服务。使用后者，我们可以再次拥有**Multi-AZ**配置，并且配备多个副本以提高弹性和降低延迟：
- en: '![The backend layer](img/image_08_008.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![后端层](img/image_08_008.jpg)'
- en: You will notice that the failover scenario differs from RDS in that there is
    no standby instance. In the event of a **PRIMARY** failure **ELASTICACHE** promotes
    the most up-to-date **REPLICA** instead.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，故障切换场景与RDS不同，因为没有备用实例。在**PRIMARY**发生故障时，**ELASTICACHE**会提升最新的**REPLICA**。
- en: Tip
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Note that after the promotion the **PRIMARY** endpoint remains the same, however
    the promoted Replica's address changes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，升级后，**PRIMARY**端点保持不变，但升级后的副本地址会发生变化。
- en: The object storage layer
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象存储层
- en: In the effort of achieving effortless scalability, we must put emphasis on building
    stateless applications where possible. Not keeping state on our application nodes
    would mean storing any valuable data away from them. A classic example is **WordPress**,
    where user uploads are usually kept locally, making it difficult to scale such
    a setup horizontally.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现轻松的可扩展性，我们必须在可能的情况下重点构建无状态应用程序。不在应用节点上保持状态意味着将任何有价值的数据存储在其他地方。一个经典的例子是**WordPress**，其中用户上传的文件通常保存在本地，这使得横向扩展这种设置变得困难。
- en: While it is possible to have a shared file system across your EC2 instances
    using **Elastic File System** (**EFS**), for reliability and scalability we are
    much better off using an object storage solution such as **AWS S3**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以通过使用**弹性文件系统**（**EFS**）在EC2实例之间共享文件系统，但为了提高可靠性和可扩展性，我们最好使用像**AWS S3**这样的对象存储解决方案。
- en: 'It is fair to say that accessing S3 objects is not as trivial as working with
    an EFS volume, however the AWS tools and SDKs lower the barrier considerably.
    For easy experimenting, you could start with the S3 CLI. Eventually you would
    want to build S3 capabilities into your application using one of the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以公平地说，访问 S3 对象并不像使用 EFS 卷那样简单，但 AWS 工具和 SDK 大大降低了门槛。为了便于实验，你可以从 S3 CLI 开始。最终，你会希望在应用程序中集成
    S3 功能，可以使用以下其中之一：
- en: 'Java/.NET/PHP/Python/Ruby or other SDKs (ref: [https://aws.amazon.com/tools/](https://aws.amazon.com/tools/))'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java/.NET/PHP/Python/Ruby 或其他 SDK（参考：[https://aws.amazon.com/tools/](https://aws.amazon.com/tools/)）
- en: 'REST API (ref: [http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html](http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html))'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REST API (参考：[http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html](http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html))
- en: 'In previous chapters we examined IAM Roles as a convenient way of granting
    S3 bucket access to EC2 instances. We can also enhance the connectivity between
    those instances and S3 using VPC Endpoints:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们探讨了 IAM 角色作为一种方便的方式，将 S3 存储桶的访问权限授予 EC2 实例。我们还可以通过使用 VPC 端点来增强实例与
    S3 之间的连接：
- en: '|   | *A VPC endpoint enables you to create a private connection between your
    VPC and another AWS service without requiring access over the Internet, through
    a NAT device, a VPN connection, or AWS Direct Connect. Endpoints are virtual devices.
    They are horizontally scaled, redundant, and highly available VPC components that
    allow communication between instances in your VPC and AWS services without imposing
    availability risks or bandwidth constraints on your network traffic.* |   |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|   | *VPC 端点使你能够在 VPC 和其他 AWS 服务之间创建私有连接，无需通过 Internet、NAT 设备、VPN 连接或 AWS
    Direct Connect。端点是虚拟设备。它们是横向扩展、冗余且高度可用的 VPC 组件，允许你的 VPC 实例与 AWS 服务之间进行通信，同时不会对网络流量带来可用性风险或带宽限制。*
    |   |'
- en: '|   | --[http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html](http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|   | --[http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html](http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html)
    |'
- en: 'If you have clients in a different geographic location uploading content to
    your bucket, then S3 transfer acceleration (ref: [http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html](http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html))
    can be used to improve their experience. It is simply a matter of clicking **Enable**
    on the bucket''s settings page:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的客户端位于不同的地理位置并且向你的存储桶上传内容，则可以使用 S3 传输加速（参考：[http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html](http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html)）来提升他们的体验。只需在存储桶的设置页面点击**启用**即可：
- en: '![The object storage layer](img/image_08_009.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![对象存储层](img/image_08_009.jpg)'
- en: We have now covered speed improvements; scalability comes built into the S3
    service itself and for cost optimization we have the different storage classes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经覆盖了速度改进；可扩展性是 S3 服务本身内置的，对于成本优化，我们有不同的存储类别。
- en: S3 currently supports four types (classes) of storage. The most expensive and
    most durable being the **Standard class**, which is also the default. This is
    followed by the **Infrequent Access class (Standard_IA)** which is cheaper, however
    keep in mind that it is indeed intended for rarely accessed objects otherwise
    the associated retrieval cost would be prohibitive. Next is the **Reduced Redundancy
    class** which, despite the scary name, is still pretty durable at 99.99%. And
    lastly, comes the **Glacier storage class** which is akin to a tape backup in
    that objects are archived and there is a 3-5 hour retrieval time (with 1-5 minute
    urgent retrievals available at a higher cost).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: S3 目前支持四种类型（类别）的存储。最昂贵且最耐用的是**标准类**，也是默认存储类型。接下来是**低频访问类（Standard_IA）**，价格较便宜，但需要注意的是，它确实是为较少访问的对象设计的，否则与之相关的检索成本会非常高。接下来是**减少冗余类**，尽管名字有些吓人，但它的耐用性依然相当高，达到了
    99.99%。最后是**冰川存储类**，它类似于磁带备份，意味着对象会被归档，并且有 3-5 小时的检索时间（紧急检索可在 1-5 分钟内完成，但会有更高的费用）。
- en: 'You can specify the storage class (except for Glacier) of an object at time
    of upload or change it retrospectively using the AWS console, CLI or SDK. Archiving
    to Glacier is done using a bucket lifecycle policy (bucket''s settings page):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上传时指定对象的存储类（冰川存储类除外），或者通过 AWS 控制台、CLI 或 SDK 后续更改存储类。归档到冰川存储类是通过存储桶生命周期策略（存储桶设置页面）来完成的：
- en: '![The object storage layer](img/image_08_010.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![对象存储层](img/image_08_010.jpg)'
- en: 'We need to add a new rule, describing the conditions under which an object
    gets archived:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加一条新规则，描述在什么条件下对象会被归档：
- en: '![The object storage layer](img/image_08_011.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![对象存储层](img/image_08_011.jpg)'
- en: Incidentally, Lifecycle rules can also help you clean up old files.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，生命周期规则也可以帮助您清理旧文件。
- en: The load balancing layer
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡层
- en: The days of the *Wild Wild West* when one used to setup web servers with public
    IPs and DNS round-robin have faded away and the load balancer has taken over.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的*疯狂时代*，人们曾通过公共 IP 和 DNS 轮询来搭建 Web 服务器，但如今负载均衡器已经取而代之。
- en: We are going to look at the AWS ELB service, but this is certainly not the only
    available option. As a matter of fact, if your use case is highly sensitive to
    latency or you observe frequent, short lived traffic surges then you might want
    to consider rolling your own EC2 fleet of load balancing nodes using NGINX or
    HAProxy.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究 AWS ELB 服务，但这绝不是唯一的可选方案。实际上，如果您的使用场景对延迟非常敏感，或者您观察到频繁且短暂的流量激增，您可能希望考虑使用
    NGINX 或 HAProxy 来部署自己的 EC2 负载均衡节点集群。
- en: The ELB service is priced at a flat per-hour fee plus bandwidth charges, so
    perhaps not much we can do to reduce costs, but we can explore ways of boosting
    performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ELB 服务按每小时固定费用加带宽费用计费，因此我们可能无法通过减少费用来节省成本，但我们可以探索提高性能的方法。
- en: Cross-zone load balancing
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨区域负载均衡
- en: 'Under normal conditions, a Classic ELB would deploy its nodes within the zones
    which our backend (application) instances occupy and forward traffic according
    to those zones. That is to say, the ELB node in zone **A** will talk to the backend
    instance in the same zone, and the same principle applies for zone **B**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，Classic ELB 会将其节点部署在我们的后台（应用程序）实例所在的区域，并根据这些区域转发流量。也就是说，区域**A**中的 ELB
    节点将与同一区域的后台实例通信，区域**B**也适用相同的原则：
- en: '![Cross-zone load balancing](img/image_08_012.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![跨区域负载均衡](img/image_08_012.jpg)'
- en: 'This is sensible as it clearly ensures lowest latency, but there are a couple
    of things to note:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是合理的，因为它可以确保最低的延迟，但有几点需要注意：
- en: An equal number of backend nodes should be maintained in each zone for best
    load spread
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获得最佳负载分布，每个区域中应保持相等数量的后台节点。
- en: Clients caching the IP address for an ELB node would stick to the respective
    backend instance
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端缓存 ELB 节点的 IP 地址将始终保持与相应后台实例的连接
- en: 'To improve the situation at the expense of some (minimal) added latency, we
    can enable **Cross-Zone Load Balancing** in the Classic ELB''s properties:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善这种情况，虽然会带来一些（最小的）延迟增加，我们可以在 Classic ELB 的属性中启用**跨区域负载均衡**：
- en: '![Cross-zone load balancing](img/image_08_013.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![跨区域负载均衡](img/image_08_013.jpg)'
- en: 'This will change the traffic distribution policy, so that requests to a given
    ELB node will be evenly spread across all registered (status: InService) backend
    instances, changing our earlier diagram to this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将改变流量分配策略，使得对特定 ELB 节点的请求将均匀分布到所有已注册（状态：InService）后台实例上，从而将我们之前的示意图更改为如下：
- en: '![Cross-zone load balancing](img/image_08_014.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![跨区域负载均衡](img/image_08_014.jpg)'
- en: An unequal number of backend nodes per zone would no longer have an impact on
    load balancing, nor would external parties targeting a single ELB instance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个区域中后台节点数量不均将不再影响负载均衡，外部方也无法仅针对单个 ELB 实例进行攻击。
- en: ELB pre-warming
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELB 预热
- en: An important aspect of the ELB service is that it runs across a cluster of EC2
    instances of a given type, very much like our backend nodes. With that in mind,
    it should not come as a surprise that ELB scales based on demand, again much like
    our Auto Scaling Group does.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ELB 服务的一个重要方面是它在一组特定类型的 EC2 实例上运行，这与我们的后台节点非常相似。考虑到这一点，ELB 会根据需求进行扩展，这一点与我们的自动扩展组类似，因此不会让人感到意外。
- en: This is all very well when incoming traffic fluctuates within certain boundaries,
    so that it can be absorbed by the ELB or increases gradually, allowing enough
    time for the ELB to scale and accommodate. However, sharp surges can result in
    ELB dropping connections if large enough.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当传入流量波动在一定范围内时，这一方案效果很好，因为 ELB 可以吸收流量，或者流量逐渐增加，为 ELB 扩展和适应提供足够的时间。然而，剧烈的流量激增可能会导致
    ELB 丢失连接，如果流量大到一定程度的话。
- en: This can be prevented with a technique called **pre-warming** or essentially
    scaling up an ELB ahead of anticipated traffic spikes. Currently this is not something
    that can be done at the user end, meaning you would need to contact AWS Support
    with an ELB pre-warming request.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况可以通过一种叫做**预热**的技术来防止，实际上就是在预期流量高峰前扩展ELB。目前这不是用户端可以执行的操作，这意味着你需要联系AWS支持来提交ELB预热请求。
- en: The CDN layer
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CDN层
- en: '**CloudFront** or AWS''s CDN solution is yet another method of improving the
    performance of the ELB and S3 services. If you are not familiar with CDN networks,
    those, generally speaking, provide faster access to any clients you might have
    in a different geographic location from your deployment location. In addition,
    a CDN would also cache data so that subsequent requests won''t even reach your
    server (also called **origin**) greatly reducing load.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**CloudFront**或AWS的CDN解决方案是另一种提高ELB和S3服务性能的方法。如果你不熟悉CDN网络，一般来说，它们为不同地理位置的客户端提供更快的访问速度。此外，CDN还会缓存数据，以便后续请求无需到达你的服务器（也叫**源**），从而大大减轻负载。'
- en: So, given our VPC deployment in the US, if we were to setup a **CloudFront distribution**
    in front of our ELB and/or S3 bucket, then requests from clients originating in
    say Europe would be routed to the nearest *European CloudFront Point-of-Presence*
    which in turn would either serve a cached response or fetch the requested data
    from the ELB/S3 over a high-speed, internal AWS network.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，假设我们的VPC部署在美国，如果我们在ELB和/或S3存储桶前设置**CloudFront分发**，那么来自欧洲等地的客户端请求会被路由到最近的*欧洲CloudFront节点*，该节点会根据需要提供缓存响应或通过AWS内部高速网络从ELB/S3获取请求的数据。
- en: 'To setup a basic **web distribution** we can use the **CloudFront dashboard**:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置一个基本的**Web分发**，我们可以使用**CloudFront仪表盘**：
- en: '![The CDN layer](img/image_08_015.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![CDN层](img/image_08_015.jpg)'
- en: 'Once we **Get Started** then the second page presents the distribution properties:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们**开始使用**，第二页会展示分发的属性：
- en: '![The CDN layer](img/image_08_016.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![CDN层](img/image_08_016.jpg)'
- en: Conveniently, resources within the same AWS account are suggested. The origin
    is the source of data that CloudFront needs to talk to, for example the ELB sitting
    in front of our application. In the **Alternate Domain Names** field we would
    enter our website address (say `www.example.org`), the rest of the settings can
    remain with their defaults for now.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，同一AWS账户中的资源会被建议。源是CloudFront需要连接的数据源，例如，位于我们应用程序前面的ELB。在**备用域名**字段中，我们将输入我们的网站地址（比如`www.example.org`），其余设置目前可以保留默认值。
- en: After the distribution becomes active all that is left to do is to update the
    DNS record for `www.example.org` currently pointing at the ELB to point to the
    distribution address instead.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当分发变为活动状态后，剩下的工作就是更新指向ELB的`www.example.org`的DNS记录，将其改为指向分发地址。
- en: Spot instances
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spot实例
- en: 'Our last point is on making further EC2 cost savings using **Spot** instances.
    These represent unused resources across the EC2 platform, which users can bid
    on at any given time. Once a user has placed a winning bid and has been allocated
    the EC2 instance, it remains theirs for as long as the current Spot price stays
    below their bid, else it gets terminated (a notice is served via the instance
    meta-data, ref: [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html)).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一点是利用**Spot**实例进一步节省EC2成本。Spot实例代表了EC2平台上未使用的资源，用户可以随时竞标。一旦用户成功竞标并分配到EC2实例，只要当前的Spot价格低于他们的出价，该实例就会持续存在，否则它会被终止（实例元数据中会提供通知，参考：[http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html)）。
- en: These conditions make Spot instances ideal for workflows, where the job start
    time is flexible and any tasks can be safely resumed in case of instance termination.
    For example, one can run short-lived Jenkins jobs on Spot instances (there is
    even a plugin for this) or use it to run a workflow which performs a series of
    small tasks that save state regularly to S3/RDS.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些条件使得Spot实例非常适合于工作流，其中任务开始时间灵活，且如果实例终止，可以安全地恢复任务。例如，可以在Spot实例上运行短期的Jenkins作业（甚至有插件支持），或者用它来运行执行一系列小任务的工作流，这些任务会定期将状态保存到S3/RDS。
- en: AWS Calculators
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS计算器
- en: 'Lastly, a simple yet helpful tool to give you an idea of how much your planned
    deployment would cost: [http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html)
    (remember to untick the **FREE USAGE TIER** near the top of the page)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是一款简单却实用的工具，可以帮助你了解计划部署的成本：[http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html)（记得取消勾选页面顶部的**FREE
    USAGE TIER**选项）。
- en: 'And if you were trying to compare the cost of on-premise to cloud, then this
    might be of interest: [https://aws.amazon.com/tco-calculator/](https://aws.amazon.com/tco-calculator/).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正试图比较本地部署与云计算的成本，那么以下内容可能对你有帮助：[https://aws.amazon.com/tco-calculator/](https://aws.amazon.com/tco-calculator/)。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we examined different ways in which to optimize both the scalability
    and running costs of an AWS deployment.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们研究了优化AWS部署的可扩展性和运行成本的不同方法。
- en: We started with the underlying VPC and its core properties such as the CIDR,
    subnets and how to plan for growth. We covered methods of improving the performance
    of the frontend, backend, storage and load balancing components. Then we looked
    at AWS Spot instances as a very cost efficient solution for executing lower-priority,
    batch processing jobs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基础的VPC及其核心属性开始，包括CIDR、子网以及如何规划增长。我们讨论了提高前端、后端、存储和负载均衡组件性能的方法。接着，我们探讨了AWS
    Spot实例，作为执行低优先级批处理任务的一种非常高效的成本解决方案。
- en: In the next chapter we move into the realm of security and explore the topic
    of how to better harden an AWS environment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进入安全领域，探讨如何更好地加强AWS环境的安全性。
