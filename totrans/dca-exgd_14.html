<html><head></head><body>
        

                            
                    Universal Control Plane
                
            
            
                
<p class="mce-root">In this chapter, we will learn everything about Docker's <strong>Universal Control Plane</strong> (<strong>UCP</strong>) that's required for the Docker Certified Associate exam. Universal Control Plane is the Docker Enterprise component in charge of managing clusters. First, we will introduce UCP's components and their features. It is important to know that UCP has changed a lot in recent years. The Docker Enterprise platform was previously known as Docker Datacenter. Docker changed its name when version 2.0 was released. That version was also important because it was the first one to include Kubernetes as a second orchestrator. In this chapter, we will learn how Kubernetes is integrated and how to deploy a production-ready platform.</p>
<p>In November 2019, Mirantis Inc. acquired the Docker Enterprise platform business, including its products, customers, and employees. Therefore, Docker Enterprise is currently a Mirantis Inc. product.</p>
<p class="mce-root">We will discover UCP's main components and learn how to deploy a production-ready environment with high availability. Enterprise environments have many security requirements and UCP includes its own authentication and authorization systems based on RBAC, all of which can be easily integrated with an enterprise's user management platform. Docker Enterprise is based on Docker Swarm but also includes an enterprise-ready Kubernetes environment within the cluster. We will learn about UCP's administration tasks, security configurations, special features, and how to provide a disaster recovery strategy based on backup and restore features. We will finish this chapter by reviewing what should be monitored on this platform to ensure its health.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Understanding UCP components and features</li>
<li>Deploying UCP with high availability</li>
<li>Reviewing Docker UCP's environment</li>
<li>Role-based access control and isolation</li>
<li>UCP's Kubernetes integration</li>
<li>UCP administration and security</li>
<li>Backup strategies</li>
<li>Upgrades, health checks, and troubleshooting</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-a83df44f-afcf-4b8d-8ad5-ccbea32a99fd">Technical requirements</h1>
<p>You can find the code for this chapter in the GitHub repository: <a href="https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git">https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git</a></p>
<p>Check out the following video to see the Code in Action:</p>
<p>"<a href="https://bit.ly/34BHHdj" target="_blank">https://bit.ly/34BHHdj</a>"</p>
<h1 id="uuid-c900db49-f934-4bee-8c6f-82fcd7e07301" class="mce-root">Understanding UCP components and features</h1>
<p class="mce-root">Docker's UCP provides the control plane for the Docker Enterprise platform. It is based on Docker Swarm but also integrates the Kubernetes orchestrator. The following is a quick list of its current features:</p>
<ul>
<li class="mce-root">A centralized cluster management interface</li>
<li class="mce-root">A cluster resource environment</li>
<li class="mce-root">Role-based access control</li>
<li class="mce-root">A client environment via WebGUI or the CLI</li>
</ul>
<p>As we mentioned previously, UCP is based on Docker Swarm orchestration. We will deploy a Docker Swarm cluster with managers and worker roles.</p>
<p>First, we will install a manager node. This will be the leader during the installation process. All the components will be deployed as containers, so we only require a Docker Enterprise Engine to run them.</p>
<p>Once the first manager has been installed, and with all the UCP components up and running, we will continue adding nodes to the cluster. This is a really simple process.</p>
<p>All the components will be managed by a master agent process called <kbd>ucp-agent</kbd>. This process will deploy all the other components according to the role of the installed node.</p>
<p class="mce-root">Let's review the different components that are deployed on manager and worker role nodes.</p>
<h2 id="uuid-c09429d9-2e41-47e1-a741-239dbadef1b9">UCP components on manager nodes</h2>
<p>In <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>, we learned how these clusters work. Manager nodes run all management processes. We will deploy an odd number of manager nodes to provide high availability because Docker Swarm is based on the Raft protocol and requires consensus or quorum in the management plane.</p>
<p>Manager nodes run all UCP core services, including the web UI and data stores that persist the state of UCP. These are the UCP services running on manager nodes:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Component</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-agent</kbd></p>
</td>
<td>
<p>This component is the agent running on each node to monitor and ensure the required services are running.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-swarm-manager</kbd></p>
</td>
<td>
<p>To provide compatibility with a Docker Swarm environment, this component runs on manager nodes.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-proxy</kbd></p>
</td>
<td>
<p>This component provides secure access to each Docker Engine on the platform using TLS and forwarding requests to a local socket.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-auth-api</kbd></p>
</td>
<td>
<p>Authentication is managed with this component running on manager nodes, exposing its API to authorize access.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-auth-store</kbd></p>
</td>
<td>
<p>This component stores data and the configurations of users, organizations, and teams on the UCP platform.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-auth-worker</kbd></p>
</td>
<td>
<p>An authentication worker periodically runs synchronization tasks with external authentication backends.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-client-root-ca</kbd></p>
</td>
<td>
<p>This component provides a certificate authority to sign users' bundles on the platform. Bundles are packages issued by the management platform to provide users with access.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-cluster-root-ca</kbd></p>
</td>
<td>
<p>To ensure secure communication between platform components, this component provides a CA for signing TLS certificates.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-kv</kbd></p>
</td>
<td>
<p>This component provides a key-value database to store cluster configurations. It was only used for Legacy Swarm (we have not seen how Docker Swarm was deployed in the past, but it is similar to Kubernetes these days) but currently, it is also used as a Kubernetes key value.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-controller</kbd></p>
</td>
<td>
<p>The UCP web UI is key for management. <kbd>ucp-controller</kbd> provides this feature. It is the first point of failure when users cannot access the cluster.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-reconcile</kbd></p>
</td>
<td>
<p>To monitor components' health, UCP runs <kbd>ucp-agent</kbd>, and if some of them fail, it will try to restart them. This component will just run when something goes wrong.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-dsinfo</kbd></p>
</td>
<td>
<p>UCP can run troubleshooting reports. It executes this component to retrieve all available information. We use support dumps to send information to support services.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-metrics</kbd></p>
</td>
<td>
<p>This component recovers node metrics. This data can be used in other monitoring environments.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ucp-interlock</kbd>/<kbd>ucp-interlock-proxy</kbd></p>
</td>
<td>
<p>Interlock components allow advanced users to publish applications deployed in the cluster. <kbd>ucp-interlock</kbd> queries the UCP API for changes to be configured to publish services in the <kbd>ucp-interlock-proxy</kbd> component, as well as a reverse proxy to be deployed and configured automatically for you by UCP.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The following processes also run on master nodes but are separated into a different table because they are related to Kubernetes:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 32.813%" class="CDPAlignCenter CDPAlign">
<p><strong>Process</strong></p>
</td>
<td style="width: 64.187%" class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>ucp-kube-apiserver</kbd></p>
</td>
<td style="width: 64.187%">
<p>This is the Kubernetes master API component. All Kubernetes processes will be deployed as containers in our host. Using containers to deploy applications helps us to maintain applications' components and their upgrades.</p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>ucp-kube-controller-manager</kbd></p>
</td>
<td style="width: 64.187%">
<p>This Kubernetes process will manage all controllers required to control, replicate, and monitor Pods.</p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>ucp-kube-scheduler</kbd></p>
</td>
<td style="width: 64.187%">
<p><kbd>kube-scheduler</kbd> schedules workloads within cluster nodes.</p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>ucp-kubelet</kbd></p>
</td>
<td style="width: 64.187%">
<p><kbd>kubelet</kbd> is the Kubernetes agent. It is the endpoint used by Kubernetes to manage nodes and their interactions.</p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>ucp-kube-proxy</kbd></p>
</td>
<td style="width: 64.187%">
<p><kbd>kube-proxy</kbd> manages a Pod's publishing and communications.</p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>k8s_ucp-kube-dns</kbd>/</p>
<p><kbd>k8s_ucp-kubedns-sidecar</kbd>/</p>
<p><kbd>k8s_ucp-dnsmasq-nanny</kbd></p>
</td>
<td style="width: 64.187%">
<p>These containers manage and monitor DNS procedures and the resolution required for UCP and Kubernetes.</p>
</td>
</tr>
<tr>
<td style="width: 32.813%">
<p><kbd>k8s_calico-node</kbd>/</p>
<p><kbd>k8s_install-cni_calico-node</kbd>/</p>
<p><kbd>k8s_calico-kube-controllers</kbd></p>
</td>
<td style="width: 64.187%">
<p>Calico is the default <strong>container network interface</strong> (<strong>CNI</strong>) for Kubernetes and it is automatically deployed during UCP installation. </p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>There is also an important component on newer releases to help with the interaction between Docker Swarm and Kubernetes interactions:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 17.524%" class="CDPAlignCenter CDPAlign">
<p><strong>Process</strong></p>
</td>
<td style="width: 81.476%" class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 17.524%">
<p><kbd>k8s_ucp-kube-compose</kbd></p>
</td>
<td style="width: 81.476%">
<p><kbd>kube-compose</kbd> allows us to deploy Docker Compose's workloads either on Docker Swarm as stacks or Kubernetes.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>These are the components that can be deployed on manager nodes. We will usually deploy at least three manager nodes because either Docker Swarm or Kubernetes requires an odd number of nodes for a distributed consensus.</p>
<p>Now, let's review worker components.</p>
<h2 id="uuid-9b4d3236-8f98-429a-a06b-88e2bb192ca6">UCP components on worker nodes</h2>
<p>The following are the components that are deployed on worker nodes once they are joined to the cluster:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 28.69%" class="CDPAlignCenter CDPAlign">
<p><strong>Components</strong></p>
</td>
<td style="width: 70.31%" class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 28.69%">
<p><kbd>ucp-agent</kbd>, <kbd>ucp-proxy</kbd>, <kbd>ucp-dsinfo</kbd>, and <kbd>ucp-reconcile</kbd></p>
</td>
<td style="width: 70.31%">
<p>These processes have the same functionality that was described for manager nodes.</p>
</td>
</tr>
<tr>
<td style="width: 28.69%">
<p><kbd>ucp-interlock-extension</kbd></p>
</td>
<td style="width: 70.31%">
<p><kbd>interlock-extension</kbd> prepares configurations for <kbd>interlock-proxy</kbd> based on changes retrieved from Docker Swarm service configurations. This process is based on templates reconfigured dynamically to accomplish all updates that happen within cluster-wide published workloads.</p>
</td>
</tr>
<tr>
<td style="width: 28.69%">
<p><kbd>ucp-interlock-proxy</kbd></p>
</td>
<td style="width: 70.31%">
<p><kbd>interlock-proxy</kbd> runs a proxy process configured dynamically thanks to all other Interlock processes. These prepare a configuration file for the proxy component with all the running backends required for each published service.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For Kubernetes to work, worker nodes also execute the following processes:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 40.722%" class="CDPAlignCenter CDPAlign">
<p><strong>Processes</strong></p>
</td>
<td style="width: 56.278%" class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 40.722%">
<p><kbd>ucp-kubelet</kbd> and <kbd>ucp-kube-proxy</kbd></p>
</td>
<td style="width: 56.278%">
<p>Only these two processes are required for Kubernetes on worker nodes.</p>
</td>
</tr>
<tr>
<td style="width: 40.722%">
<p><kbd>k8s_calico-node</kbd> and <kbd>k8s_install-cni_calico-node</kbd></p>
</td>
<td style="width: 56.278%">
<p>Networking within cluster nodes requires Calico, so its processes are also deployed on worker nodes.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>From these lists, it is easy to understand how Kubernetes is deployed on Docker Enterprise. Manager nodes run Kubernetes' control plane while workers receive workloads. Notice that managers run <kbd>kube-proxy</kbd> and <kbd>kubelet</kbd>, so they are also able to receive workloads. This is also true for Docker Swarm, as we learned in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>. Docker Enterprise allows managers to execute application workloads by default.</p>
<p>We will also review the volumes that are deployed on UCP. We will divide them into two categories:</p>
<ul>
<li><strong>Volumes for certificates</strong>: All these volumes are associated with certificate management within the cluster. Take care of them because if we lose them, we will have serious authentication problems between UCP/Kubernetes processes: 
<ul>
<li><kbd>ucp-auth-api-certs</kbd></li>
<li><kbd>ucp-auth-store-certs</kbd></li>
<li><kbd>ucp-auth-worker-certs</kbd></li>
<li><kbd>ucp-client-root-ca</kbd></li>
<li><kbd>ucp-cluster-root-ca</kbd></li>
<li><kbd>ucp-controller-client-certs</kbd></li>
<li><kbd>ucp-controller-server-certs</kbd></li>
<li><kbd>ucp-kv-certs</kbd></li>
<li><kbd>ucp-node-certs</kbd></li>
</ul>
</li>
<li><strong>Volumes for data</strong>: These are data volumes and are used to store different databases deployed within the cluster, as well as the metrics that have been retrieved from different components:
<ul>
<li><kbd>ucp-auth-store-data</kbd></li>
<li><kbd>ucp-auth-worker-data</kbd></li>
<li><kbd>ucp-kv</kbd></li>
<li><kbd>ucp-metrics-data</kbd></li>
<li><kbd>ucp-metrics-inventory</kbd></li>
</ul>
</li>
</ul>
<p>All these volumes are important for the cluster. Key-value pairs, common certificates, and authentication data volumes are replicated on control plane nodes. They are created using the default local volume driver unless we have already created them using a different driver. Keep in mind that they should be created before deploying the cluster if we want to store data in a non-standard location (<kbd>/var/lib/docker/volumes</kbd> or the defined <kbd>data-root</kbd> path in your environment).</p>
<p>This section is very important for the Docker Certified Associate exam because we need to know where components are distributed and their functionality on the platform.</p>
<p>Now that we know what components will be deployed on each cluster role, we will learn how to install production-ready environments.</p>
<h1 id="uuid-19880dc4-9ccb-4871-a6b5-34d2ff4f142a" class="mce-root">Deploying UCP with high availability</h1>
<p class="mce-root">First, we will take a look at the hardware and software requirements for the platform. We will use version 3.0 – the current version at the time of writing this book. It is known that the DCA exam was prepared even before Docker Enterprise version 2.0 was released. Neither Docker Desktop nor Kubernetes were part of the Docker Enterprise platform on that release. We will deploy the current version because the exam has evolved to cover important topics in newer versions. Let's quickly review the current maintenance life cycle:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 45.5129%" class="CDPAlignCenter CDPAlign">
<p><strong>Docker Enterprise 2.1</strong></p>
</td>
<td style="width: 52.4871%" class="CDPAlignCenter CDPAlign">
<p><strong>Docker Enterprise 3.0</strong></p>
</td>
</tr>
<tr>
<td style="width: 45.5129%" class="CDPAlignLeft CDPAlign">
<p>End of life on 2020-11-06</p>
</td>
<td style="width: 52.4871%" class="CDPAlignLeft CDPAlign">
<p>End of life on 2021-07-21</p>
</td>
</tr>
<tr>
<td style="width: 45.5129%" class="CDPAlignLeft CDPAlign">
<p class="mce-root">Components:</p>
<p class="mce-root">- Enterprise Engine 18.09.z<br/>
- Universal Control Plane 3.1.z<br/>
- Docker Trusted Registry 2.6.z</p>
</td>
<td style="width: 52.4871%" class="CDPAlignLeft CDPAlign">
<p class="mce-root">Components:</p>
<p>- Enterprise Engine 19.03.z</p>
<p>- Universal Control Plane 3.2.z</p>
<p>- Docker Trusted Registry 2.7.z</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Docker provides 2 years of support from the release date. We recommend taking a look at Docker's website for updated information on the maintenance life cycle and compatibility matrix:</p>
<ul>
<li><a href="https://success.docker.com/article/maintenance-lifecycle">https://success.docker.com/article/maintenance-lifecycle</a></li>
<li><a href="https://success.docker.com/article/compatibility-matrix">https://success.docker.com/article/compatibility-matrix</a></li>
</ul>
<p>All of the listed versions include Kubernetes, but the deployed version will be different. At the time of writing this book, Docker Enterprise deploys Kubernetes v1.14.8.</p>
<p>We can deploy the Docker Enterprise platform on-premises or on cloud providers. Before deploying the first node, let's review the minimum node requirements:</p>
<ul>
<li class="mce-root">8 GB and 4 GB of RAM for the manager and worker nodes, respectively.</li>
<li class="mce-root">2 vCPUs for manager nodes. Worker nodes' vCPUs will depend on the applications to be deployed.</li>
<li class="mce-root">10 GB of free disk space for the <kbd>/var</kbd> partition for manager nodes (a minimum of 6 GB is recommended because Kubernetes will verify disk space before installation) and at least 500 MB of free disk space for the <kbd>/var</kbd> partition for worker nodes. Worker node space will depend on the applications to be deployed, how big their images are, and how many image releases should be present on our nodes.</li>
</ul>
<p class="mce-root">A more realistic approach to resources for control plane and image management would probably be the following:</p>
<ul>
<li>16 GB of RAM for manager nodes and workers with DTR (we will learn about DTR in <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, <em>Implementing an Enterprise-Grade Registry with DTR</em>).</li>
<li>Four vCPUs for manager nodes and workers with DTR. Control plane CPU and image scanning can take forever if there is not enough CPU available.</li>
</ul>
<p>Keep in mind that a cluster's size will really depend on the applications being deployed. It is usually recommended to distribute application components in several nodes because clusters with fewer nodes are harder to maintain. A few nodes with many resources is worse than having the same resources distributed on many nodes. It gives you better cluster life cycle management and a better workload distribution when some nodes fail.</p>
<p>On control plane nodes, we will deploy Docker Enterprise Engine version 19.03 (the latest release at the time of writing). These nodes should be deployed with static IP addresses and Linux kernel version 3.10 or higher. Because we will deploy more than one replica, we will require an external load balancer to route control plane requests to any of the available manager nodes. We will use a virtual IP address and a <strong>Fully Qualified Domain Name</strong> (<strong>FQDN</strong>) associated with this load balancer. We will add them as <strong>Subject Alternative Names</strong> (<strong>SANs</strong>) to ensure valid certificates. Certificates should be associated (as a SAN) with any node that can be reached as part of UCP's service. In this case, manager nodes will run control plane components, so certificates should be valid for any of them, including all possible FQDN names associated with UCP's management endpoints (ports <kbd>443</kbd> and <kbd>6443</kbd> by default for Docker Swarm and Kubernetes, respectively).</p>
<p>We will expose TCP ports <kbd>443</kbd> and <kbd>6443</kbd> to users by default. Both can be changed to other ones more appropriate for our environment. The first port allows user interaction with UCP's control plane either using the web browser, the API, or the Docker command line. The second port described publishes the Kubernetes API server. It allows us to interact directly with the Kubernetes orchestrator.</p>
<p>Worker nodes do not require static IP addresses but they should be accessible by their names using DNS.</p>
<p>We cannot deploy user namespaces within UCP. (We learned about the user namespaces that are used to improve host security in <a href="c2dd78c4-066f-40b4-94e7-a7e2904d7ec2.xhtml">Chapter 3</a>, <em>Running Docker Containers</em>.) It is not easy to use this feature under UCP conditions, which is why it is not supported.</p>
<p>A minimum environment with high availability will include three managers and at least two workers. The following diagram shows the smallest environment (DTR nodes are not included). We can say that UCP has three major logical components:</p>
<div><img src="img/47319578-6162-481e-9844-036acbb22ea9.jpg" style=""/></div>
<p>Therefore, as a summary, we will need the following logical requirements for the deployment:</p>
<ul>
<li>Static IP addresses for manager nodes</li>
<li>A VIP address and FQDN for the control plane</li>
<li>An external load balancer owning a VIP and TCP pass through to managers on ports <kbd>443</kbd> and <kbd>6443</kbd> (by default)</li>
</ul>
<p>The following ports and protocols should be permitted (TCP ports unless explicitly described):</p>
<ul>
<li>On managers:
<ul>
<li>Port <kbd>443</kbd> for the UCP web UI and API</li>
<li>Port <kbd>6443</kbd> for the Kubernetes API server</li>
<li>Ports <kbd>2376</kbd> and <kbd>2377</kbd> for Docker Swarm communication</li>
<li>Ports ranging from <kbd>12379</kbd> to <kbd>12388</kbd> for internal UCP component communication</li>
</ul>
</li>
<li>On workers and managers:
<ul>
<li>Port <kbd>7946</kbd> (TCP and UDP) for Docker Swarm gossip</li>
<li>Port <kbd>4789</kbd> (UDP) for overlay networking</li>
<li>Port <kbd>12376</kbd> for TLS authentication proxy to access Docker Engine</li>
<li>Port <kbd>6444</kbd> for the Kubernetes API reverse proxy</li>
<li>Port <kbd>179</kbd> for BGP peers for Kubernetes networking</li>
<li>Port <kbd>9099</kbd> for Calico health checks</li>
<li>Port <kbd>10250</kbd> for Kubernetes Kubelet</li>
</ul>
</li>
</ul>
<p>Users will use ports <kbd>443</kbd> and <kbd>6443</kbd> to access UCP services via the HTTPS protocol.</p>
<p>All cluster nodes will run containers. Some of these nodes will act as managers and they will run management components while others just run a few worker components and workloads. But there are two common elements on managers and workers: UCP agent and Docker Engine.</p>
<p>Docker Engine is always required because we need to run containers. Docker Enterprise requires Docker Enterprise Engine. The installation process is easy and it will be based on the license key file and the specific packages available for each customer at <kbd>https://hub.docker.com/u/&lt;YOUR_USER_OR_ORGANIZATION&gt;/content</kbd>. First, we will go to <kbd>https://hub.docker.com/</kbd> and register for a Docker Hub account. Docker provides a 1-month trial of the Docker Enterprise platform, available at <a href="https://hub.docker.com/editions/enterprise/docker-ee-trial">https://hub.docker.com/editions/enterprise/docker-ee-trial</a>. In <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>,<a href="ab131f1f-ca6e-4815-9a3a-8c92c93c9dbc.xhtml"> Chapter 12</a>, <em>Publishing Applications in Docker Enterprise</em>, and <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, <em>Implementing an Enterprise-Grade Registry with DTR</em>, we will show my own account (<kbd>frjaraur</kbd>) for example purposes, as well as the different steps and pictures to help you understand this.</p>
<p>The following screenshot shows the <kbd>frjaraur</kbd> content URL. You will have your own content once you log into the Docker Hub website. We will find the required license and our package repository URL on this page after signing up for a Docker Enterprise 30-day trial:</p>
<div><img src="img/56d1cce9-3d94-453a-aa50-ae233b1cd2b4.png" style=""/></div>
<p>At the bottom right, we will read our package's URL. Click on the copy button and follow the next procedure to install Docker Enterprise Engine. This procedure will be different for each Linux distribution. In this book, we will follow Ubuntu's procedure. The process is described at the previously provided customer content URL. Microsoft Windows nodes are also supported within the Docker Enterprise platform, although they can just be used as workers at the time of writing this book.</p>
<p>These are the steps to follow to install UCP with high availability on Ubuntu nodes:</p>
<ol>
<li>Export the Docker Engine version and the previously shown URL on the <kbd>DOCKER_EE_VERSION</kbd> and <kbd>DOCKER_EE_URL</kbd> variables, respectively. At the time of writing this book, the latest Docker Enterprise Engine version is 19.03:</li>
</ol>
<pre style="padding-left: 60px"><strong># export DOCKER_EE_URL="https://storebits.docker.com/ee/trial/sub-76c16081-298d-4950-8d02-7f5179771813"</strong><br/><strong># export DOCKER_EE_VERSION=19.03</strong></pre>
<p style="padding-left: 60px">Notice that your <kbd>DOCKER_EE_URL</kbd> will be completely different. You can ask for a trial license to follow these steps.</p>
<ol start="2">
<li>Then, we need to add the Docker customer's package repository to our environment:</li>
</ol>
<pre style="padding-left: 60px"><strong># curl -fsSL "${DOCKER_EE_URL}/ubuntu/gpg" | sudo apt-key add -</strong><br/><strong># apt-key fingerprint 6D085F96</strong><br/><strong># add-apt-repository \  </strong><br/><strong>"deb [arch=$(dpkg --print-architecture)] $DOCKER_EE_URL/ubuntu \   $(lsb_release -cs) \   stable-$DOCKER_EE_VERSION"</strong><br/><strong># apt-get update -qq</strong></pre>
<ol start="3">
<li>Finally, we will install the required packages:</li>
</ol>
<pre style="padding-left: 60px"><strong># apt-get install -qq docker-ee docker-ee-cli containerd.io</strong></pre>
<p>These procedures must be applied to all cluster nodes before installing UCP. As we mentioned previously, we will have different procedures for different Linux flavors, but we will also be able to include Microsoft Windows nodes in the cluster. Microsoft Windows Docker Engine's installation is completely different and is shown at <kbd>https://hub.docker.com/u/&lt;YOUR_USER_OR_ORGANIZATION&gt;/content</kbd>.</p>
<p>Always review your Docker customer's content page before installing Docker Enterprise Engine because the installation procedure may change.</p>
<p>When all the cluster nodes have Docker Engine installed, we can continue with Docker UCP's installation. This workflow is not required but it is recommended because we can avoid any problems before installing UCP. This is because its components will run as containers in your hosts.</p>
<p>Docker provides support for different infrastructures and also certifies running the Docker Enterprise platform on them. Amazon Web Services and Microsoft Azure are the certified environments at the time of writing this book. In both cases, Docker also provides infrastructure scripts and/or step-by-step documentation for successfully deploying the Docker Enterprise platform on them.</p>
<p>The Docker Enterprise platform is based on Docker Swarm, although Kubernetes is also deployed. Therefore, we will create a Docker Swarm cluster using the UCP installer, and then we will add other nodes as managers or workers.</p>
<p>The installation will require launching a container named <kbd>ucp</kbd>. This is very important because it ensures just one installation at once. We will also use Docker Engine's local socket as the volume inside the installation container. The UCP installation process has many options – we will cover the most important ones here.</p>
<p>To install UCP, we will launch <kbd>docker container run --name ucp docker/ucp:&lt;RELEASE_TO_INSTALL&gt;</kbd>. It is important to install a specific release because the <kbd>docker/ucp</kbd> container will also be used for backup/recovery and other tasks.</p>
<p>Let's write down and execute a usual installation command line for the first manager in the cluster:</p>
<pre class="mce-root"><strong>(first manager node) # docker container run --rm -it --name ucp \</strong><br/><strong>  -v /var/run/docker.sock:/var/run/docker.sock \</strong><br/><strong>  docker/ucp:3.2.5 install \</strong><br/><strong>  --host-address &lt;MANAGEMENT_HOST_IP_ADDRESS&gt; \</strong><br/><strong>  --san &lt;LOAD_BALANCED_FQDN&gt; \</strong><br/><strong>  --san &lt;OTHER_FQDN_ALIAS_OR_IP&gt; \</strong><br/><strong>  --admin-username &lt;ADMIN_USER&gt; \</strong><br/><strong>  --admin-password &lt;ADMIN_USER_PASSWORD&gt;</strong><br/><br/><strong> INFO[0000] Your Docker daemon version 19.03.5, build 2ee0c57608 (4.4.0-116-generic) is compatible with UCP 3.2.5 (57c1024)</strong><br/>  WARN[0000] None of the Subject Alternative Names we'll be using in the UCP certificates ["&lt;NODE_IP_ADDRESS&gt;" "&lt;NODE_NAME&gt;"] contain a domain component. Your generated certs may fail TLS validation unless you only use one of these short names or IP addresses to connect. You can use the --san flag to add more aliases<br/><strong> INFO[0000] Checking required ports for connectivity</strong><br/><strong> INFO[0004] Checking required container images</strong><br/><strong> INFO[0007] Running install agent container ...</strong><br/><strong> INFO[0000] Loading install configuration</strong><br/><strong> INFO[0000] Running Installation Steps</strong><br/><strong> INFO[0000] Step 1 of 35: [Setup Internal Cluster CA]</strong><br/><strong> ...</strong><br/><strong> INFO[0014] Step 16 of 35: [Deploy UCP Controller Server]</strong><br/><strong> INFO[0016] Step 17 of 35: [Deploy Kubernetes API Server]</strong><br/><strong> ...</strong><br/><strong> INFO[0033] Step 24 of 35: [Install Kubernetes CNI Plugin]</strong><br/><strong> INFO[0063] Step 25 of 35: [Install KubeDNS]</strong><br/><strong> INFO[0064] Step 26 of 35: [Create UCP Controller Kubernetes Service Endpoints]</strong><br/><strong> INFO[0066] Step 27 of 35: [Install Metrics Plugin]</strong><br/><strong> INFO[0067] Step 28 of 35: [Install Kubernetes Compose Plugin]</strong><br/><strong> INFO[0073] Step 29 of 35: [Deploy Manager Node Agent Service]</strong><br/><strong> INFO[0073] Step 30 of 35: [Deploy Worker Node Agent Service]</strong><br/><strong> INFO[0073] Step 31 of 35: [Deploy Windows Worker Node Agent Service]</strong><br/><strong> INFO[0073] Step 32 of 35: [Deploy Cluster Agent Service]</strong><br/><strong> INFO[0073] Step 33 of 35: [Set License]</strong><br/><strong> INFO[0073] Step 34 of 35: [Set Registry CA Certificates]</strong><br/><strong> INFO[0073] Step 35 of 35: [Wait for All Nodes to be Ready]</strong><br/><strong> INFO[0078] All Installation Steps Completed</strong> </pre>
<p>After 35 steps, your UCP's environment will be installed on the first Linux node. Take care and use DNS resolution and an external load balancer. As we mentioned in the previous sections, all the managers will run the same control plane components. Therefore, an external load balancer is required to guide requests to any of them. This can be done by following the round-robin algorithm, for example (it does not matter which UCP manager node receives the requests, but at least one should be reachable).</p>
<p>The external load balancer will provide a virtual IP address to the UCP control plane and we will also provide pass-through port-routing for ports <kbd>443</kbd> and <kbd>6443</kbd> (or customized ones if you changed them). We will add this external load balancer's virtual IP address and the associated fully qualified domain name as a SAN. In fact, we will add as many SANs as required for our environment using the <kbd>--san</kbd> argument.</p>
<p>These steps are key for your organization access and <strong>Docker Trusted Registry</strong> (<strong>DTR</strong>) because it is usual to integrate both within UCP. In this case, DTR will ask UCP for user authentication, so it has to have access and resolution to UCP's FQDN and ports.</p>
<p>We will use a pass-through or transparent proxy on external load balancers to allow UCP's backends to manage TLS certificates and connections.</p>
<p>The UCP image will allow us to do the following:</p>
<ul>
<li>Install and uninstall UCP using the <kbd>install</kbd> and <kbd>uninstall-ucp</kbd> actions. The uninstall option will remove all UCP components from all cluster nodes. We do not have to execute any other procedure to completely remove UCP from our nodes. Docker Engine will not be removed.</li>
<li>Download the required Docker images from Docker Hub using the <kbd>images</kbd> option.</li>
<li>Backup and restore UCP manager nodes using the <kbd>backup</kbd> and <kbd>restore</kbd> actions.</li>
<li>Provide a UCP cluster ID and its certificates using the <kbd>id</kbd> and <kbd>dump-certs</kbd> options. Dumping certificates allows us to store them securely to avoid certificate problems if we accidentally remove any required volume.</li>
<li>Create a support-dump using the <kbd>support</kbd> action. These dumps will contain all the useful information about our environment, including application/container logs.</li>
<li>Upgrade the UCP platform by executing the <kbd>upgrade</kbd> option. This option will upgrade all UCP components and may impact our services. It is preferred to add the <kbd>--manual-worker-upgrade</kbd> argument to avoid worker nodes from being auto-upgraded. We will need to take care of our workloads and move them within worker nodes and manually upgrade UCP on them.</li>
<li>Create an example UCP configuration file and verify the required port status. UCP can be configured using either the provided web UI or using configuration files. Using configuration files will allow us to maintain reproducibility, and changes can be managed with any configuration management application. This method can be achieved once UCP is installed or during installation by customizing the example config file generated with the <kbd>example-config</kbd> option and using <kbd>docker/ucp install --existing-config</kbd> with this modified file. The available options are described at the following link: <a href="https://docs.docker.com/ee/ucp/admin/configure/ucp-configuration-file">https://docs.docker.com/ee/ucp/admin/configure/ucp-configuration-file</a>.</li>
</ul>
<p>The following are the most commonly used UCP installation options:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 25.952%">
<p class="CDPAlignLeft CDPAlign"><strong>Options</strong></p>
</td>
<td style="width: 67.048%" class="CDPAlignCenter CDPAlign">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 25.952%">
<p><kbd>--swarm-grpc-port</kbd>, </p>
<p><kbd>--controller-port</kbd>,</p>
<p><kbd>--kube-apiserver-port</kbd>, and <kbd>--swarm-port</kbd></p>
</td>
<td style="width: 67.048%">
<p>These options allow us to modify the default ports used in several services. The most important ones, probably customized in your environment, will be <kbd>kube-apiserver-port</kbd> (defaults to <kbd>6443</kbd>) and <kbd>controller-port</kbd> (defaults to <kbd>443</kbd>). They publish Kubernetes and UCP user endpoints to allow us to interact with the cluster.</p>
</td>
</tr>
<tr>
<td style="width: 25.952%">
<p><kbd>--host-address</kbd> and</p>
<p><kbd>--data-path-addr</kbd></p>
</td>
<td style="width: 67.048%">
<p>The first option sets which node's IP address will be allocated for publishing the control plane. The second option allows us to isolate the control plane from the data plane. We set a different interface or IP address for the data plane.</p>
</td>
</tr>
<tr>
<td style="width: 25.952%">
<p><kbd>--pod-cidr</kbd>, <kbd>--service-cluster-ip-range</kbd> and <kbd>--nodeport-range</kbd></p>
</td>
<td style="width: 67.048%">
<p>These options allow us to customize Kubernetes Pods' and Services' IP address ranges and publishing ports for <kbd>NodePort</kbd> services.</p>
</td>
</tr>
<tr>
<td style="width: 25.952%">
<p><kbd>--external-server-cert</kbd></p>
</td>
<td style="width: 67.048%">
<p>With this option, we configure our own certificate within the UCP cluster.</p>
</td>
</tr>
<tr>
<td style="width: 25.952%">
<p><kbd>--san</kbd></p>
</td>
<td style="width: 67.048%">
<p>We include as many SANs as required to add these aliases to UCP certificates. Ask yourself how users and admins will consume the UCP cluster and add the FQDN names related to these services.</p>
</td>
</tr>
<tr>
<td style="width: 25.952%">
<p><kbd>--admin-username</kbd> and <kbd>--admin-password</kbd></p>
</td>
<td style="width: 67.048%">
<p>It is recommended to set up an admin username and password during installation to provide a reproducible workflow. We will avoid the <kbd>--interactive</kbd> option to have an <strong>Infrastructure-as-Code</strong> (<strong>IaC</strong>) UCP installation process.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Once UCP is installed on the first manager node, we will just join other manager nodes and workers to the cluster, as we learned in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>. To get the required <kbd>docker join</kbd> command line, we just execute <kbd>docker swarm join-token manager</kbd> for manager nodes and <kbd>docker swarm join-token worker</kbd> for worker nodes. We just copy their output and execute the <kbd>docker join</kbd> command on each manager and worker node. It is quite easy.</p>
<p>It is also possible to obtain the required joining instructions from the web UI by going to the Shared Resources | Nodes menu.</p>
<p>Nodes in a UCP cluster can work with either Docker Swarm or Kubernetes, or even in mixed mode. This allows nodes to run Docker Swarm and Kubernetes workloads at the same time.</p>
<p>Mixed mode is not recommended in production because orchestrators do not share their load information. Therefore, a node can be almost full for one orchestrator and empty for another. In this situation, it can continue receiving new workloads for a non-full orchestrator, hence impacting other orchestrator's application performance.</p>
<p>As a summary, we installed Docker Engine and then we installed UCP. We reviewed this process and the main arguments required to install the Docker Enterprise platform in our environment.</p>
<p>If you plan to install Docker Enterprise on Amazon AWS or the Microsoft Azure cloud, you should read the specific instructions and options in the Docker documentation (for AWS, <a href="https://docs.docker.com/ee/ucp/admin/install/cloudproviders/install-on-aws">https://docs.docker.com/ee/ucp/admin/install/cloudproviders/install-on-aws</a>; for Azure, <a href="https://docs.docker.com/ee/ucp/admin/install/cloudproviders/install-on-azure">https://docs.docker.com/ee/ucp/admin/install/cloudproviders/install-on-azure</a>).</p>
<p>Now, we will review the UCP environment.</p>
<h1 id="uuid-8c365526-3acb-4634-b399-1459faa04193">Reviewing the Docker UCP environment</h1>
<p>In this section, we will review the Docker UCP environment. We will be able to use either the web UI, the command line, or its published REST API. In this book, we will cover the web application interface and how to integrate our <kbd>docker</kbd> client command line with UCP. </p>
<p>First, we will introduce the web UI.</p>
<h2 id="uuid-8a01bf64-38f7-45ee-8188-92b8cdda10b7">The web UI</h2>
<p>The web UI will run on all manager nodes. We will use UCP's fully qualified domain name, which is associated with its virtual IP address. Port <kbd>443</kbd> will be used unless you manually configured a different one. If we open <kbd>https://&lt;UCP_FQDN&gt;:&lt;UCP_PORT&gt;</kbd> on our browser, we will access the UCP login page. If we have used autogenerated certificates, the browser will warn us about an untrusted CA. This is normal because UCP generates an internal CA automatically for us to sign all internal and external certificates. We can upload our corporate or private certificates into UCP.</p>
<p>Remember to apply a passthrough (or transparent proxy) configuration on your external load balancer to access UCP backends. We will use <kbd>https://&lt;MANAGER_IP&gt;:&lt;UCP_PORT&gt;/_ping</kbd> for the backend's health check.</p>
<p>Let's have a quick review of UCP's web UI. The following screenshot shows the main login interface. We set the admin's password during installation, either executing this process interactively with the <kbd>--interactive</kbd> argument or automating these settings by adding the <kbd>--admin-username</kbd> and <kbd>--admin-password</kbd> arguments. The username and password that are used during installation should be used to log into UCP.</p>
<p>The main page will also ask us to add a license file if we have not applied it during installation. This can be done using the <kbd>--license</kbd> argument for <kbd>docker run docker/ucp:&lt;RELEASE&gt; install</kbd>:</p>
<div><img src="img/8a593cc8-4833-4389-b117-c518e784dd92.png" style=""/></div>
<p>The following screenshot shows the UCP Dashboard. Each user will have access to their own. The left panel will provide access to the user's profile, Dashboard, Access Control, Shared Resources, and resources specific to Kubernetes and Swarm:</p>
<div><img src="img/df34de10-6b1d-4d8f-87ae-cb4dcf15d115.png" style=""/></div>
<p>The Dashboard screen shows us a quick review of the cluster's components' status. It also provides a summary of the Swarm and Kubernetes workloads and an overview of the cluster's load. Do not think this is enough for monitoring as this is too simple. We should add monitoring tools to improve alerting, performance reporting, and capacity planning features.</p>
<p>Access Control will only appear when UCP administrators access the cluster's Web UI. Administrators will be able to manage all of RBAC's behavior:</p>
<ul>
<li>Orgs &amp; Teams provides an interface to create organizations and teams and we will integrate users into them from these entries.</li>
<li>The Users endpoint will allow us to manage users as expected. We will learn how to create and manage users in the next topic.</li>
<li>Roles provides an interface for Kubernetes and UCP roles. Kubernetes resources should be managed using declarative methods using YAML resource files, while Docker Swarm's resources (managed by UCP) will be created using the web UI.</li>
<li>Grants helps us manage Kubernetes role bindings and Swarm roles and collection integrations.</li>
</ul>
<p>Shared Resources provides access to resources for either Kubernetes or Docker Swarm. We will manage collections, stacks, containers, images, and nodes:</p>
<div><img src="img/208027ae-f984-4e25-b748-f8cbf662233b.png"/></div>
<p>Nodes can be managed from the Nodes entry point. We will set node properties and the orchestrator mode. Adding new nodes is easy, as we have seen. The Add Node option shows us the cluster's <kbd>docker join</kbd> command line. We will just copy this instruction to the new node's Terminal. This will also apply to Microsoft Windows nodes.</p>
<p>Stacks will show either multi-container or multi-service applications deployed on a Docker Swarm cluster. This view also shows Kubernetes workloads.</p>
<p>Kubernetes and Swarm endpoints show us each orchestrator's specific resources:</p>
<ul>
<li>Kubernetes shows namespaces, service accounts, controllers, services, ingress resources, Pod configurations, and storage. We will be able to change which namespace will be used for all users' web UI endpoints. We will also review and create Kubernetes resources using the declarative method.</li>
<li>Swarm allows us to create and review services, volumes, networks, secrets, and configurations.</li>
</ul>
<p>We can review the UCP documentation as well as Kubernetes' and the UCP API. This will help us implement automation procedures based on REST API integrations.</p>
<h2 id="uuid-2c2aee46-67d8-4163-8860-c1d50c408b12">The command line using the UCP bundle</h2>
<p>The UCP bundle is probably the most important part to access for your users and administrators. Although every user can have access to UCP's web UI, CI/CD, monitoring tools, and DevOps, users will review and launch their workloads using the Docker command line. Therefore, this access should be secure. Remember that Docker Swarm deploys an encrypted control plane. All its internal communications will be secured by TLS. Users' access is not secured. UCP, on the other hand, provides a completely secure solution. Security is ensured using TLS for users and admin access. This is managed using personalized certificates. Each user will get their own group of certificates. Kubernetes access is also secured using the UCP user bundle.</p>
<p>To obtain this UCP bundle, users will use either the web UI or a simple <kbd>curl</kbd> command – or any command-line web client – to  download this package file, compressed as a ZIP folder:</p>
<div><img src="img/612e0ee9-344c-4c0f-8733-e1cb3353672d.png"/></div>
<p>The preceding screenshot shows web GUI access to the user bundle file. We will just download it using a web browser. Once it is on our computer, we will decompress it. This file contains certificates, configuration, and scripts to load the client environment on our computer, regardless of whether it is running Linux, Mac, or Windows operating systems. There is an environment file for each one. We will look at its content and the procedure in Linux, but it is similar in Windows (the commands will vary).</p>
<p>We can use <kbd>curl</kbd> and <kbd>jq</kbd> to download the user bundle from the command line:</p>
<pre><strong>$ AUTHTOKEN=$(curl -sk -d '{"username":"&lt;username&gt;","password":"&lt;password&gt;"}' https://&lt;UCP_FQDN&gt;:&lt;UCP_PORT&gt;/auth/login | jq -r .auth_token)</strong><br/><br/><strong>$ curl -k -H "Authorization: Bearer $AUTHTOKEN" https://&lt;UCP_FQDN&gt;:&lt;UCP_PORT&gt;/api/clientbundle -o ucp-bundle-admin.zip</strong></pre>
<p>If we decompress the admin bundle file, <kbd>ucp-bundle-admin.zip</kbd>, using <kbd>unzip</kbd>, we will obtain all the files required to connect to our cluster:</p>
<pre><strong>$ unzip ucp-bundle-admin.zip</strong></pre>
<p>We will then load this environment. We will use <kbd>env.ps1</kbd> in Microsoft Windows PowerShell or the <kbd>env.cmd</kbd> Command Prompt. On Linux hosts, we will use <kbd>env.sh</kbd>. When we load the environment on our shell, we will be able to connect remotely to the UCP cluster using the <kbd>docker</kbd> or <kbd>kubectl</kbd> client software:</p>
<pre><strong>$ source env.sh</strong><br/><strong> Cluster "ucp_&lt;UCP_FQDN&gt;:6443_admin" set.</strong><br/><strong> User "ucp_&lt;UCP_FQDN&gt;:6443_admin" set.</strong><br/><strong> Context "ucp_&lt;UCP_FQDN&gt;:6443_admin" created.</strong></pre>
<p>Notice that the Kubernetes context has also been set. Therefore, we will be able to manage the cluster and deploy workloads on either Kubernetes or Docker Swarm. Each user's UCP bundle must be stored securely. We can generate a new one if we remove it, but keep it safe; someone could potentially use it and obtain access to your environment files.</p>
<p>UCP provides client software for Microsoft Windows and Linux on our manager nodes at <kbd>https://&lt;UCP_FQDN&gt;:&lt;UCP_PORT&gt;/manage/dashboard/dockercli</kbd>. We can download them to connect to the cluster.</p>
<p>It is key to use the UCP bundle instead of connecting to the cluster using SSH or any other local access. We will never allow local access to cluster nodes. Everyone must access the cluster either using the command line or the web UI.</p>
<p>UCP's REST API is also secured using certificates. We will require the UCP bundle's certificate files to access the cluster using its API.</p>
<p>We will review UCP's access control in the next section and provide a simple example that will help us understand RBAC concepts.</p>
<h1 id="uuid-eece54f6-e92f-4b1f-88b5-4b71201520c4" class="mce-root">Role-based access control and isolation</h1>
<p class="mce-root"><strong>Role-based access control</strong> (<strong>RBAC</strong>) manages authorization for Docker Swarm and Kubernetes. Docker Enterprise lets us manage users' access to resources. We use roles to allow users to view, edit, and use cluster resources.</p>
<p>Authorization is based on the following concepts:</p>
<ul>
<li><strong>Subjects</strong>: We manage users, teams, and service accounts within organizations. Users are part of teams, included in organizations.</li>
<li><strong>Resources</strong>: These are the groups of Docker objects we were talking about in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>. As Kubernetes is also integrated into the UCP cluster, Kubernetes resources are also part of these groupings. UCP manages resources grouped in collections.</li>
<li><strong>Collections</strong>: These are sets of resources, including different kinds of objects, such as volumes, secrets, configs, networks, services, and so on.</li>
<li><strong>Roles</strong>: These group sets of permissions and we assign them to different subjects. Roles define what can be done by whom.</li>
<li><strong>Grants</strong>: Combining subjects with roles and resource sets, we obtain grants. They are effective user permissions applied to groups of resources.</li>
</ul>
<p>Service accounts are only valid for Kubernetes. These are not user accounts; they are associated with applications or APIs assigned to manage their access.</p>
<p class="mce-root">There are some predefined roles but we can create our own. This is a list of the default ones included with Docker Enterprise's UCP:</p>
<ul>
<li class="mce-root">None: This role does not provide access to any Docker Swarm resources. This should be the default role for new users.</li>
<li class="mce-root">View Only: Users with this role can view resources such as services, volumes, and networks but they cannot create new resources.</li>
<li class="mce-root">Restricted Control: Users with this role can view and edit resources but they cannot use bind mounts (hosts' directories) or execute new processes within containers using <kbd>docker exec</kbd>. They cannot run privileged containers or with enhanced capabilities.</li>
<li class="mce-root">Scheduler: This role allows users to view nodes so that they can schedule workloads on them. By default, all users get a grant with the <kbd>Scheduler</kbd> role against the <kbd>/Shared</kbd> collection.</li>
<li class="mce-root">Full Control: This role should be restricted to advanced users only. These can view and edit volumes, networks, and images. They also can create privileged containers.</li>
</ul>
<p>Users will only be able to manage their own containers or Pods. This behavior allows integrating namespaces (Kubernetes) and collections (Docker Swarm) in this equation. Therefore, users with full control access to a set of resources included in a collection will have all privileges on them in Docker Swarm. The same will happen if we add resources within a namespace and the user is included in a fully privileged role in Kubernetes.</p>
<p>There is also a more advanced role that's assigned to Docker Enterprise administrators. They will have full control and management privileges for the UCP environment. This can be managed using the Is Admin checkbox on the user's properties page.</p>
<p>Grants interconnect users and permissions with the set of resources where they should be applied. The grants management workflow includes their creation, user assignment, the role that should be applied, and resource association. This way, we ensure that the appropriate privileges are applied to a collection of resources assigned to a group of users (or just one user).</p>
<p>Collections are hierarchical and contain resources. They are represented by using a directory-like structure and every user has their own private collection, along with the user's default permissions. Using this, we can nest collections. Once a user has been granted access to a collection, they will have access to all its hierarchical children. There are three main collections: <kbd>/</kbd>, <kbd>/System</kbd>, and <kbd>/Shared</kbd>.</p>
<p>Under the <kbd>/System</kbd> collection, we will find UCP's manager nodes and UCP's and DTR's system services. By default, only administrators will have access to this collection. On the other hand, the <kbd>/Shared</kbd> collection will contain all the worker nodes ready for running workloads. We can add additional collections and move some workers to isolate them and provide multi-tenant features. Distributing workers on different collections will also distribute workload execution for different groups or tenants.</p>
<p>Each user has a private collection by default under <kbd>/Collections/Swarm/Shared/Private/&lt;USER_NAME&gt;</kbd>. This ensures that users' workloads are secure by default and only administrators will have access. Therefore, users have to deploy workloads on their team-shared collections.</p>
<p>Labels associated with collections manage users' access to resources, which makes it easy to allow or disallow a user's visibility dynamically.</p>
<p>Let's review these concepts with a short example.</p>
<p>We have two projects in our organization (<kbd>myorganization</kbd>): <kbd>projectA</kbd> and <kbd>projectB</kbd>. We will also assume that we have three teams in our organization: developers, quality and assurance, and DevOps. Let's describe some users and their roles within our organization:</p>
<ul>
<li><strong>Developers</strong>: <kbd>dev1</kbd> and <kbd>dev2</kbd></li>
<li><strong>Quality and assurance:</strong> <kbd>qa1</kbd> and <kbd>qa2</kbd></li>
<li><strong>DevOps:</strong> <kbd>devops1</kbd> and <kbd>devops2</kbd></li>
<li><strong>UCP Admin</strong></li>
</ul>
<p>The following image shows some screenshots of the user creation process. First, we create an organization and then teams and users inside the previously created organization:</p>
<div><img src="img/63c62974-88fc-4628-b990-2d220a8445c9.png"/></div>
<p>Each user will have their own user account in UCP. We will create developers, quality and assurance, and DevOps teams and we will add their users. We will also create three main collections as stages and child collections for each project. Therefore, we will have the following:</p>
<ul>
<li><kbd>development/projectA</kbd> and <kbd>development/projectB</kbd></li>
<li><kbd>certification/projectA</kbd> and <kbd>certification/projectB</kbd></li>
<li><kbd>production/projectA</kbd> and <kbd>production/projectB</kbd></li>
</ul>
<p>Let's suppose that each developer works on one project at a time. They should have full access to their projects during the development stage but they should have view-only access in the certification stage. Quality and assurance users will only have access to create and modify their deployments in the certification stage. DevOps will have access to create and modify resources in production and they will allow view-only access to developers, but only on <kbd>projectA</kbd>. In fact, <kbd>projectB</kbd> should be secure and only a <kbd>devops2</kbd> user should be able to modify resources for this project.</p>
<p>The following screenshots show the process of adding grants to allow a user access to collections. First, we create a collection, and then we add that collection to a new role:</p>
<div><img src="img/e10abfc9-b802-4ef5-9ee3-855379fe31e0.png" style=""/></div>
<p>We will launch two deployments with different users and will review how they view these deployments.</p>
<p>We will assume that all the required users have been created and that each user's <kbd>ucp-bundle</kbd> has been downloaded. As user <kbd>dev1</kbd>, we will create a simple <kbd>nginx</kbd> deployment for <kbd>projectB</kbd>, using <kbd>com.docker.ucp.access.label=/development/projectB</kbd> as the label:</p>
<pre><strong>(as user dev1)$ source env.sh </strong><br/><strong>Cluster "ucp_192.168.56.11:6443_dev1" set.</strong><br/><strong>User "ucp_192.168.56.11:6443_dev1" set.</strong><br/><strong>Context "ucp_192.168.56.11:6443_dev1" modified.</strong><br/><br/><strong>(as user dev1)$ docker service create --name nginx-dev --label com.docker.ucp.access.label=/development/projectB nginx</strong><br/><strong>k7hsizrvlc0cmy9va78bri06k</strong><br/><strong>overall progress: 1 out of 1 tasks </strong><br/><strong>1/1: running [==================================================&gt;] </strong><br/><strong>verify: Service converged </strong><br/><br/><strong>(as user dev1)$ $ docker service ls</strong><br/><strong>ID NAME MODE REPLICAS IMAGE PORTS</strong><br/><strong>k7hsizrvlc0c nginx-dev replicated 1/1 nginx:latest </strong></pre>
<p>If we now impersonate user <kbd>qa1</kbd>, we will get different results:</p>
<pre><strong>(as user qa1)$ source env.sh </strong><br/><strong>Cluster "ucp_192.168.56.11:6443_qa1" set.</strong><br/><strong>User "ucp_192.168.56.11:6443_qa1" set.</strong><br/><strong>Context "ucp_192.168.56.11:6443_qa1" modified.</strong><br/><br/><strong>(as user qa1)$ docker service ls</strong><br/><strong>ID NAME MODE REPLICAS IMAGE PORTS</strong></pre>
<p>User <kbd>qa1</kbd> will not list any services because it does not have access to the <kbd>dev1</kbd> collection. But if we review this list with the <kbd>devops2</kbd> user, we will obtain a list that includes the <kbd>dev1</kbd> user's services:</p>
<pre><strong>(as user devops2)$ docker service ls</strong><br/><strong>ID NAME MODE REPLICAS IMAGE PORTS</strong><br/><strong>k7hsizrvlc0c nginx-dev replicated 1/1 nginx:latest </strong></pre>
<p>If we try to modify this resource (the <kbd>nginx-dev</kbd> service), we will obtain an access error because we only have view authorization. On the other hand, the <kbd>dev2</kbd> user can scale up the number of replicas because they are in the developer group:</p>
<pre><strong>(as user devops2)$ docker service update --replicas 2 nginx-dev</strong><br/><strong>Error response from daemon: access denied:</strong><br/><strong>no access to Service Create, Service Update, on collection 8185981a-5e15-4906-9fbf-465e9f712918</strong><br/><strong>no access to Service Create, Service Update, on collection 8185981a-5e15-4906-9fbf-465e9f712918</strong><br/><strong>no access to Service Update, on collection 8185981a-5e15-4906-9fbf-465e9f712918</strong><br/><br/><strong>(as user dev2)$ docker service update --replicas 2 nginx-dev</strong><br/><strong>nginx-dev</strong><br/><strong>overall progress: 2 out of 2 tasks </strong><br/><strong>1/2: running [==================================================&gt;] </strong><br/><strong>2/2: running [==================================================&gt;] </strong><br/><strong>verify: Service converged </strong></pre>
<p>To finish off this example, we will create two different services as user <kbd>devops2</kbd>. We will deploy secure and unsecured services from <kbd>projectB</kbd> and <kbd>projectA</kbd>, respectively:</p>
<pre><strong>(as user devops2)$ docker service create --quiet --name nginx-prod-secure \</strong><br/><strong>--label com.docker.ucp.access.label=/production/projectB nginx</strong><br/><br/><strong>4oeuld63v96ck26efype57320</strong><br/><br/><strong>(as user devops2)$ docker service create --quiet --name nginx-prod-unsecure \</strong><br/><strong>--label com.docker.ucp.access.label=/production/projectA nginx</strong><br/><br/><strong>txmuqfcr751n8cb445hqu73td</strong><br/><br/><strong>(as user devops2)$ docker service ls</strong><br/><strong>ID NAME MODE REPLICAS IMAGE PORTS</strong><br/><strong>k7hsizrvlc0c nginx-dev replicated 2/2 nginx:latest </strong><br/><strong>4oeuld63v96c nginx-prod-secure replicated 1/1 nginx:latest </strong><br/><strong>txmuqfcr751n nginx-prod-unsecure replicated 1/1 nginx:latest</strong></pre>
<p>In this case, the <kbd>devops1</kbd> user should only be able to manage <kbd>nginx-prod-unsecure</kbd>, which is associated with <kbd>projectA</kbd>:</p>
<pre><strong>(as user devops1)$ docker service ls</strong><br/><strong>ID NAME MODE REPLICAS IMAGE PORTS</strong><br/><strong>k7hsizrvlc0c nginx-dev replicated 2/2 nginx:latest </strong><br/><strong>txmuqfcr751n nginx-prod-unsecure replicated 1/1 nginx:latest</strong></pre>
<p>This was a simple example of authorization management using labels. In this case, we manually added these labels, but we can set a default collection for each user if we wish. This will provide a default label associated with their workflows, instead of us using the out-of-the-box <kbd>/Collections/Swarm/Shared/Private/&lt;USER&gt;</kbd> collection. We can also associate constraints with collections to ensure specific locations. This is very important in multi-tenant environments. </p>
<h1 id="uuid-b8066bdb-a6aa-443b-9870-406a6ebcb9dd">UCP's Kubernetes integration</h1>
<p>As we have learned, Kubernetes is deployed alongside Docker Swarm when installing UCP. If we take a look at all the required Kubernetes components, we will notice that all of them run as containers within our cluster. The required key-value store will also be provided. Port <kbd>6443</kbd> (by default) will provide Kubernetes access, and users and administrators will use this port to manage the cluster or execute their workloads.</p>
<p>We will use the Docker bundle's certificates and configuration file, <kbd>kube.yml</kbd>. As we learned in this chapter, we will load our user's bundle environment and then get access to the Kubernetes cluster using the <kbd>kubectl</kbd> command line.</p>
<p>Once <kbd>env.sh</kbd> has been loaded using <kbd>source env.sh</kbd>, we will have the required environment variables and access to our certificates. If we get Kubernetes cluster nodes using <kbd>kubectl get nodes</kbd>, we will obtain their status:</p>
<pre><strong>$ kubectl get nodes</strong><br/><strong>NAME STATUS ROLES AGE VERSION</strong><br/><strong>node1 Ready master 4d13h v1.14.8-docker-1</strong><br/><strong>node2 Ready master 4d13h v1.14.8-docker-1</strong><br/><strong>node3 Ready master 4d13h v1.14.8-docker-1</strong><br/><strong>node4 Ready &lt;none&gt; 4d12h v1.14.8-docker-1</strong></pre>
<p>If we review the running Pods in the <kbd>kube-system</kbd> namespace using <kbd>kubectl get pods -n kube-system</kbd>, we will notice that <kbd>calico</kbd> and <kbd>compose</kbd> for Kubernetes are also deployed:</p>
<pre><strong>$ kubectl get pods -n kube-system</strong><br/><strong> NAME READY STATUS RESTARTS AGE</strong><br/><strong> calico-kube-controllers-5c48d7d966-cncw2 1/1 Running 3 4d13h</strong><br/><strong> calico-node-8sxh2 2/2 Running 6 4d13h</strong><br/><strong> calico-node-k2fgh 2/2 Running 6 4d13h</strong><br/><strong> calico-node-nrk62 2/2 Running 6 4d13h</strong><br/><strong> calico-node-wgl9c 2/2 Running 6 4d13h</strong><br/><strong> compose-779494d49d-wk8m4 1/1 Running 3 4d13h</strong><br/><strong> compose-api-85c67b79bd-7sbhj 1/1 Running 4 4d13h</strong><br/><strong> kube-dns-6b8f7bdd9-g6tfq 3/3 Running 9 4d13h</strong><br/><strong> kube-dns-6b8f7bdd9-ls2z2 3/3 Running 9 4d13h</strong><br/><strong> ucp-metrics-6nfz4 3/3 Running 9 4d13h</strong><br/><strong> ucp-metrics-hnsfb 3/3 Running 9 4d13h</strong><br/><strong> ucp-metrics-xdl24 3/3 Running 9 4d13h</strong></pre>
<p>These components are very important because Calico is the default CNI deployed with UCP. This allows us to deploy applications distributed cluster-wide. Pods and services are able to communicate within the cluster even if they do not run on the same host. This is not required in Docker Swarm because overlay networking is included by default. Calico allows us also to improve Kubernetes security because it can deploy network policies to isolate and manage Pods' and services' communications.</p>
<p>On the other hand, Compose for Kubernetes provides a standard interface for Docker Swarm and Kubernetes. Docker stacks could be deployed either on Docker Swarm or Kubernetes.</p>
<p>We can also notice that <kbd>ucp-metrics</kbd> also runs Kubernetes workloads as other system-related deployments, obtained using <kbd>kubectl get deployments -A</kbd>:</p>
<pre><strong>$ kubectl get deployments -A </strong><br/><strong>NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE </strong><br/><strong>kube-system calico-kube-controllers 1/1 1 1 4d14h </strong><br/><strong>kube-system compose 1/1 1 1 4d14h </strong><br/><strong>kube-system compose-api 1/1 1 1 4d14h </strong><br/><strong>kube-system kube-dns 2/2 2 2 4d14h </strong></pre>
<p>Kubernetes roles and role bindings are managed from the command line and the web UI. All Kubernetes features from the 1.14.8 release are available. This is also very important. Docker Enterprise provides a vanilla Kubernetes release and product releases also upgrade Kubernetes, but you cannot upgrade Kubernetes manually.</p>
<p>In the next section, we will review the main administration tasks and security improvements.</p>
<h1 id="uuid-28b52d0c-a50f-4adb-9f31-b3e907fdf288">UCP administration and security</h1>
<p>UCP administrators manage Docker Swarm and Kubernetes clusters. They integrate external LDAP/AD authentication. Authentication can be delegated but UCP manages authorizations, as we learned in the <em>Role-based access control and isolation</em> section.</p>
<p>The following screenshot shows the Admin Settings endpoint:</p>
<div><img src="img/4236eb4d-0505-4dc0-ba92-9ea1cfa7eee4.png" style=""/></div>
<p>Docker Enterprise license can be introduced during installation, but it also can be manage from the web UI in Admin Settings. This endpoint also allow us to do the following administration tasks:</p>
<ul>
<li>Rotate Docker Swarm's tokens to improve a cluster's security. Tokens are only used to join nodes to the cluster; we can change them whenever we need to.</li>
<li>Manage Interlock's ports and enable publishing applications using this feature. We will talk about Interlock in <a href="ab131f1f-ca6e-4815-9a3a-8c92c93c9dbc.xhtml">Chapter 12</a>, <em>Publishing Applications in Docker Enterprise</em>.</li>
<li>Configure some cluster configurations such as UCP's port and key-value database snapshots.</li>
<li>Integrate external LDAP and configure the default role to apply for new users and some session settings. This option delegates authentication to external LDAP/AD systems and UCP will just be used as an authentication cache if it is not available. We set user filters using attributes to only integrate subsets of users in the UCP environment. UCP synchronizes LDAP changes periodically.</li>
<li>Change UCP's application and audit logging levels.</li>
<li>Execute and configure backups from the web UI.</li>
<li>Integrate Docker Trusted Registry and Docker Content Trust to allow only signed images. This will be applied to all the nodes within the cluster.</li>
<li>Set the default orchestrator for new nodes. We can choose between Docker Swarm, Kubernetes, and mixed mode.</li>
<li>Authorize administrators or users to execute workloads on UCP managers or worker nodes running DTR. We will decide who can run workloads on management nodes. It is recommended to avoid any non-control-plane workload on managers.</li>
<li>Customize and launch platform upgrades. This will allow us to decide between a completely automated process and deploying manual upgrades to worker nodes to avoid impacting an application's service.</li>
</ul>
<p>It is recommended to disallow application workloads on UCP managers. These nodes should only run on the UCP system and DTR containers. This will avoid any application performance issues due to UCP's control plane. On the other hand, if any application component consumes too many resources, this will not affect the control plane.</p>
<p>Allowing only signed images in production is key. This will ensure image provenance and a CI/CD workflow. It is also possible to require some specific signs for images. For example, we can ensure that only images signed by <kbd>Operations Team</kbd>, <kbd>Developer's Chief</kbd>, and <kbd>IT manager</kbd> will run in production. This will apply to all the nodes in the cluster.</p>
<p>Many of UCP's and Kubernetes' features can be queried or modified via UCP's REST API. We should review the documentation at <kbd>https://&lt;UCP_FQDN&gt;[:443]/apidocs/</kbd> and <kbd>https://&lt;UCP_FQDN&gt;[:443]/kubernetesdocs/</kbd>.</p>
<p>UCP also provides some Pod security policies that are applied by default on a Kubernetes cluster. These Pod security policies will do the following:</p>
<ul>
<li>Manage privileged containers</li>
<li>Configure the host's namespaces (IPC, PID, network, and ports)</li>
<li>Manage the host's paths and their permissions and volume types</li>
<li>Manage users and groups for the container process execution and <kbd>setuid</kbd> capabilities inside the container</li>
<li>Change the default container's capabilities</li>
<li>Integrate Linux security modules</li>
<li>Allow host kernel configurations using <kbd>sysctl</kbd></li>
</ul>
<p class="mce-root">By default, only administrators will be able to deploy privileged containers in UCP's Kubernetes. This is configured on a privileged Pod security policy. By default, UCP just provides two special policies, as we can see in the <kbd>kubectl get PodSecurityPolicies</kbd> output:</p>
<pre><strong>$ kubectl get PodSecurityPolicies</strong><br/><strong>NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES </strong><br/><strong>privileged true * RunAsAny RunAsAny RunAsAny RunAsAny false *</strong><br/><strong>unprivileged false RunAsAny RunAsAny RunAsAny RunAsAny false *</strong></pre>
<p>You can read more about the Pod security policies included with Docker Enterprise and how to create new ones in the Kubernetes documentation or by going to this blog post: <a href="https://www.mirantis.com/blog/understanding-kubernetes-security-on-docker-enterprise-3-0/">https://www.mirantis.com/blog/understanding-kubernetes-security-on-docker-enterprise-3-0/</a>.</p>
<p>Admission controllers are other valuable pieces in Kubernetes' security. They intercept Kubernetes API requests to allow or modify them before scheduling or executing any action. This allows us to enforce default security on resources, even if users try to execute an action that isn't allowed. Admission controllers are applied to the Kubernetes API process. Therefore, we should inspect the <kbd>ucp-kube-apiserver</kbd> container's command-line options to verify which admission controllers have been applied to our environment. As Kubernetes is not part of the DCA exam yet, we will stop here regarding this topic. But it is important to understand that Docker Enterprise applies security in Kubernetes using well-known Kubernetes mechanisms. UCP applies three special admission controllers to prevent anyone from removing core Kubernetes roles required by UCP, to ensure image signing if required, and to manage the execution of non-system Kubernetes Pods only on non-mixed nodes.</p>
<p>In the next section, we will review how to create and restore UCP's backups.</p>
<h1 id="uuid-ca31630f-514d-48be-aa4e-cd437e5f9a17">Backup strategies</h1>
<p>In this section, we will learn how to backup and restore the Docker Enterprise UCP platform.</p>
<p>As UCP runs on top of Docker Swarm, this is the first component to review when preparing a good backup strategy.</p>
<p>We should run periodic backups of Docker Swarm. These backups will allow us to recover cluster configuration and workloads.</p>
<h2 id="uuid-d0b8eefc-3d2f-45e8-b6e6-42d5de32519d">Docker Swarm's backup</h2>
<p>We introduced how to execute a Docker Swarm backup in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>. In that chapter, we described the content we should take care of. Let's learn about the steps to follow to implement a production-ready backup of Docker Swarm for the Docker Enterprise platform.</p>
<p>Make sure you have applied the auto-lock feature to improve secure access to Docker Swarm data as we will need it. The lock key will not be stored with a backup. You should store it in a safe place.</p>
<p>We will execute the backup steps on all non-leader manager nodes. This will ensure that any of the managers except the leader (at that moment) can be restored. In fact, it should not be easy to completely destroy a cluster, so backing up just a node should be fine. If all the clusters are completely lost, we will recover that node and then add others, as we did during the installation process. Your cluster health should not rely on backup-restore features. That is why we run the Raft protocol for cluster components syncing and running more than one manager node.</p>
<p>Application deployments and their configuration should be stored in code repositories, as we have recommended a couple of times in this book. Sometimes, it is even easier to deploy a new cluster and launch all the applications again using automation tools.</p>
<p>The following steps are recommended to create a good backup of Docker Swarm orchestrator data:</p>
<ol>
<li>Verify that the platform is healthy before executing this backup procedure.</li>
<li>We will stop Docker Engine on the non-leader manager by executing <kbd>systemctl stop docker</kbd>.</li>
<li>Create a <kbd>.tar</kbd> file with <kbd>/var/lib/docker/swarm</kbd> directory content: <kbd>tar -cvzf "&lt;DIRECTORY_FOR_YOUR_BACKUPS&gt;/swarm-backup-$(hostname -s)-$(date +%y%m%d).tgz" /var/lib/docker/swarm/</kbd>.</li>
<li>Start Docker Engine again executing <kbd>systemctl start docker</kbd>.</li>
<li>We can execute this procedure on other non-leader manager nodes, although we will be able to restore Docker Swarm with one node only if the backup was successful.</li>
</ol>
<p>UCP runs on top of Docker Swarm. Let's review the required steps for backing up UCP.</p>
<h2 id="uuid-9318eb84-8fc6-48d7-9780-31fc5d75a1a8">Backing up UCP</h2>
<p>Unlike in Docker Swarm, in UCP, there is no need to pause or stop any platform components to execute a backup. This feature is quite new. In older releases, components had to be paused on nodes while performing a backup. We will just execute this backup on a single node because UCP data will allow us to recover the entire cluster. But there are a few important notes about this backup:</p>
<ul>
<li>This backup does not include Docker Swarm deployed workloads, networks, configurations, or secrets.</li>
<li>We cannot recover an updated UCP using a backup from an older release.</li>
<li>Neither <kbd>ucp-metrics-data</kbd> nor <kbd>ucp-node-certs</kbd> volumes are included.</li>
<li>Kubernetes data will be covered in a UCP backup.</li>
<li>Neither Router Mesh's nor Interlock settings will be stored. Once the restored components have been redeployed, configurations will also be recovered.</li>
<li>Backup content will be stored in a <kbd>.tar</kbd> file in a user-defined location. It can be secured using a passphrase.</li>
</ul>
<p>We can create UCP backups using the web UI, command line, or its API (on the latest releases).</p>
<p>Using the command line, we will need to use the <kbd>ucp</kbd> release container. For the current version at the time of writing this book, we will use the <kbd>docker/ucp:3.2.4</kbd> image. To create a backup from the command line, we will execute <kbd>docker container run docker/ucp:&lt;RELEASE&gt; backup</kbd>:</p>
<pre><strong>$ docker container run \</strong><br/><strong> --rm \</strong><br/><strong> --interactive \</strong><br/><strong> --log-driver none \</strong><br/><strong> --name ucp \</strong><br/><strong> --volume /var/run/docker.sock:/var/run/docker.sock \</strong><br/><strong> --volume &lt;FULL_PATH_FOR_UCP_BACKUP_DIRECTORY&gt;:/backup \</strong><br/><strong> docker/ucp:3.2.4 backup \</strong><br/><strong> --file &lt;BACKUP_FILENAME&gt;.tar \</strong><br/><strong> --passphrase "&lt;PASSPHRASE&gt;" \</strong><br/><strong> --include-logs=false</strong></pre>
<p>In this example, we are not including UCP platform logs (they will be included by default). If SELinux is enabled, which is recommended, we will also add <kbd>--security-opt label=disable</kbd>.</p>
<p>Using the web UI, we will first navigate to Admin Settings. Then, we'll select Backup Admin, and finally, we'll click on Backup Now to immediately launch the backup execution.</p>
<p>We will not cover the API method in this book and how to verify backup content when the process has finished, but it is described on the Docker documentation website. It is also recommended to review the latest backup information provided at <a href="https://docs.docker.com/ee/admin/backup/back-up-ucp/">https://docs.docker.com/ee/admin/backup/back-up-ucp/</a>.</p>
<p>To restore a UCP backup, we can start from one of these situations:</p>
<ul>
<li>We can start from scratch, restoring a UCP backup on a new, recently installed Docker Enterprise Engine.</li>
<li>We can also recover a UCP backup on an initiated Docker Swarm, restoring UCP so that it has a new, fully functional cluster.</li>
<li>We can restore UCP on the Docker Swarm cluster where it was created. We will just choose one of its manager nodes and run the recovery process after the previous UCP deployment is completely uninstalled. This is the only case where the previously created user's bundle will continue to work.</li>
</ul>
<p>If recovery is started from scratch or using a new Docker Swarm cluster, the IP addresses and SAN that were used will not be valid. Therefore, we will need to regenerate server certificates after the UCP restore.<br/>
<br/>
After you have successfully restored UCP, you can add new managers and workers the same way you would after a fresh installation.</p>
<p>To restore a previously created backup, we will execute <kbd>docker container run docker/ucp:&lt;RELEASE&gt; restore</kbd>. We need to use the same image release that was used to create the backup. This is very important because we cannot restore from a different release:</p>
<pre><strong>$ docker container run \</strong><br/><strong> --rm \</strong><br/><strong> --interactive \</strong><br/><strong> --name ucp \</strong><br/><strong> --volume /var/run/docker.sock:/var/run/docker.sock \</strong><br/><strong> docker/ucp:3.2.4 restore \</strong><br/><strong>--passphrase "&lt;PASSPHRASE&gt;" &lt; &lt;FULL_PATH_FOR_UCP_BACKUP_DIRECTORY&gt;/&lt;BACKUP_FILENAME&gt;.tar</strong></pre>
<p>It is important to know that backup and restore are processes that you should execute when everything else is not working. We deploy UCP environments with high availability to avoid unexpected situations. You have to actively monitor your cluster environments and not leave unattended errors or alarms on monitoring systems. In the event of a manager node failure, a cluster will continue working, but we must reestablish a healthy status as soon as possible. From my experience, most of the issues found in production Docker Enterprise environments are related to filesystems growing without control, processes eating all the resources, or communication problems. Take care of these possible issues, monitor cluster health, and do periodic backups (and update their processes) to ensure you are able to recover your environment if everything fails.</p>
<p>In the next section, we will learn what to monitor and how to check different components' statuses to avoid unnoticed failures.</p>
<h1 id="uuid-dae3de8f-4fd2-48d2-ab83-aa20c8372d74">Upgrades, monitoring, and troubleshooting</h1>
<p>In this section, we will review how cluster upgrades must be deployed. We will work in a cluster environment. There are some steps to follow in order to execute platform updates without service interruption. Monitoring and troubleshooting are critical in production. We will learn about what important keys and values we should review to ensure a cluster's health and what steps we should follow to troubleshoot a degraded or faulty environment.</p>
<h2 id="uuid-68601aaf-cf30-401c-a39f-2067afa566b6">Upgrading your environment</h2>
<p>We must review the Docker UCP release notes and upgrade procedure for each version. At the time of writing this book, the current release documentation is available on Docker's website: <a href="https://docs.docker.com/reference/ucp/3.2/cli/upgrade">https://docs.docker.com/reference/ucp/3.2/cli/upgrade</a>.</p>
<p>We should always perform a backup before any procedure, and we usually start by upgrading Docker Engine. You should review the Docker documentation to ensure that these steps are not changed between releases. Node upgrades should be done one at time. We will begin with non-leader manager nodes. Once all the managers have been upgraded, we will move the running services between different worker nodes to ensure minimal service interruption between upgrades.</p>
<p>Once all the Docker Engine instances have been updated, we will start with the UCP upgrade. We can execute this process from the web UI and from the command line. We recommend following the command-line steps because the process will give you more information. We can execute this process offline if all the required images have been previously downloaded on all the nodes. We can check the required images at this link: <a href="https://docs.docker.com/ee/ucp/admin/install/upgrade-offline/">https://docs.docker.com/ee/ucp/admin/install/upgrade-offline/</a>. We will pre-load all the images using <kbd>docker image load -i &lt;PACKAGE_WITH_ALL_IMAGES&gt;.tar.gz</kbd>.</p>
<p>We will run <kbd>docker container run docker/ucp upgrade</kbd> with the appropriate arguments to upgrade our UCP environment. Docker Engine should be upgraded before executing this command:</p>
<pre><strong>docker container run --rm -it \</strong><br/><strong> --name ucp \</strong><br/><strong> -v /var/run/docker.sock:/var/run/docker.sock \</strong><br/><strong> docker/ucp:3.2.X \</strong><br/><strong> upgrade --interactive</strong></pre>
<p class="mce-root">If your nodes work with more than one interface, we will also add <kbd>--host-address</kbd> with an appropriate IP address.</p>
<p>We can run and upgrade the process with the debug option, <kbd>--debug</kbd>, which is very useful for identifying errors if something goes wrong.</p>
<p>There is an interesting option on the current release because we can upgrade workers manually using <kbd>--manual-worker-upgrade</kbd>. This helps us control the impact we have on services that are deployed in the environment.</p>
<p>All UCP processes will be upgraded in all nodes unless the <kbd>--manual-worker-upgrade</kbd> option is used. Once the upgrade process ends, the environment will be completely upgraded to the new release. At that moment, it is important to verify the health of the cluster.</p>
<h2 id="uuid-a7595746-99f6-4a97-b16d-ea397546fd10">Monitoring a cluster's health</h2>
<p>We can use either the command line or the web UI to review the environment's health. We will use common Docker Swarm commands because we are running the environment on top of this orchestrator. We can review the node status with <kbd>docker node ls</kbd>. If we use the Docker UCP bundle to connect to the environment, we might miss some components. Make sure you are using an administrator user in the environment to be able to retrieve its health. Using the bundle, we can list all the control plane processes and use <kbd>docker container ls</kbd> to verify their statuses.</p>
<p>We can retrieve a manager's status from the <kbd>https://&lt;ucp-manager-url&gt;/_ping</kbd> endpoint, on each manager node's IP address or FQDN name. Requests to this URL can give us a <kbd>200</kbd> code if a node is healthy and a <kbd>500</kbd> code if there are some faulty components.</p>
<p>It is important to understand that this endpoint must be verified on each node because we are accessing the cluster through a load balancer to each manager. This configuration helps us provide high availability to the environment.</p>
<p>The web UI also provides cluster status information. The Dashboard page shows us a clear status overview of the environment. This page includes counters for errors, warnings, or pending states for managers and workers. We will quickly notice errors on platform nodes. A performance summary for managers and worker nodes allows us to verify cluster sizing and its usage. We will also see information about deployed services on Docker Swarm and Kubernetes. This way, we can drill down to different UCP sections to deep dive into encountered errors. The following screenshot shows how the UCP Dashboard looks when showing the described platform overview:</p>
<div><img src="img/5ef9dd82-827f-469c-a2ec-279470806946.png" style=""/></div>
<p>In each UCP resource section, we will see the resources' statuses, along with their properties. To monitor the cluster node's health, we will review the Nodes section, which can be found under Shared Resources. In this section, we will review the CPU and memory usage per node. This view will also help us find possible service degradation when node resource usage is too high. </p>
<h2 id="uuid-97490f72-bae4-47d8-a22c-87b661284749">Troubleshooting UCP</h2>
<p>Throughout this chapter, we have been reviewing the main monitoring endpoints for UCP components. There are some critical endpoints in the cluster. We also described some database processes that manage important clusters' persistent data. These components run on managers and they replicate their data between them. It should be enough for the UCP environment, but sometimes, things can go wrong and they lose node synchronization. Network latency and performance problems can lead to these situations.</p>
<h3 id="uuid-9f659298-648d-421d-89fe-ad4210d7ed78">Troubleshooting UCP-KV</h3>
<p>If we lose some manager nodes, <kbd>ucp-kv</kbd> can show the incorrect number of nodes. We can check the number of configured nodes with <kbd>etcdctl</kbd>. We can execute <kbd>etcdctl</kbd> on the <kbd>ucp-kv</kbd> container directly:</p>
<pre><strong>$ docker exec -it ucp-kv etcdctl \</strong><br/><strong> --endpoint https://127.0.0.1:2379 \</strong><br/><strong> --ca-file /etc/docker/ssl/ca.pem \</strong><br/><strong> --cert-file /etc/docker/ssl/cert.pem \</strong><br/><strong> --key-file /etc/docker/ssl/key.pem \</strong><br/><strong> cluster-health</strong></pre>
<p>This will show us <kbd>cluster is healthy</kbd> messages if the configured number of managers are healthy. If <kbd>ucp-kv</kbd> is unhealthy, we should review whether all the manager nodes are fine. If we deleted one manager but this change was not correctly updated on the other nodes, we could end up with an unhealthy cluster. To recover from this situation, we would need to remove the deleted node from the <kbd>etcd</kbd> database using <kbd>etcdctl member remove</kbd> (check the <kbd>etcd</kbd> documentation at <a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/">https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/</a>).</p>
<p>Update the component configurations and their states (deleting nodes, for example) one by one. Wait until the changes are synced in the cluster before executing a new update command.</p>
<p>We can also have problems with the authentication database. In the next section, we will learn how to correct the number of nodes if we lose one manager node.</p>
<h3 id="uuid-be85d9ee-1b5f-42bb-98fd-d982081cf906">Troubleshooting UCP-Auth</h3>
<p>First, we will review the current number of healthy manager nodes. If some of them are still unhealthy, we should correct that situation first. Once the managers are healthy, if the authentication database continues to be in an inconsistent state, we will follow this procedure:</p>
<pre><strong>$ docker container run --rm -v ucp-auth-store-certs:/tls \</strong><br/><strong>docker/ucp-auth:&lt;RUNNING_VERSION&gt; \</strong><br/><strong>--db-addr=&lt;HEALTHY_MANAGER_IP_ADDRESS&gt;:12383 \</strong><br/><strong>--debug reconfigure-db --num-replicas &lt;NUMBER_OF_MANAGERS&gt;</strong></pre>
<p>This command will run a <kbd>docker/ucp-auth</kbd> container using the RethinkDB <kbd>reconfigure-db</kbd> command to fix the right number of managers.</p>
<h3 id="uuid-7324e966-9f8f-4b96-bcfd-557f79927387">Troubleshooting nodes</h3>
<p>As we mentioned previously, network latency and performance can lead you to problematic situations. Take care of node resources and filesystems. If manager nodes become exhausted, the cluster will end up in an unhealthy state.</p>
<p>We can observe heartbeat failures if nodes cannot be contacted in 10 seconds. Worker nodes will reach a pending state if they cannot contact a manager node. We check these nodes locally. We also take a look at possible network or performance issues.</p>
<p>Managers can also become unhealthy. If other managers cannot reach them, <kbd>ucp-controller</kbd> processes will be impacted. We can check container logs for network issues.</p>
<p>These are some of the most common issues found on the Docker UCP platform. We usually start by reviewing the web UI dashboard and the <kbd>ucp-controller</kbd> container logs. If any other component seems unhealthy, we review its logs.</p>
<p>In the next chapter, we will learn how to publish applications deployed within the Docker Enterprise platform using the Interlock feature.</p>
<h1 id="uuid-8bc359f1-13e9-4d13-bd74-d11b386aad19" class="mce-root">Summary</h1>
<p class="mce-root">This chapter covered the main Docker UCP features. We learned how to deploy a cluster with high availability and how to manage and deploy workloads using either UCP's web UI or the user bundle with the Docker and Kubernetes command line. We also introduced UCP's role-based access control, which helps us provide fine-grained access to cluster resources. We also took a look at the web UI and the main configurations available for managing Docker Enterprise's control plane.</p>
<p>We also learned about UCP's components and how to deploy and manage Docker Enterprise's control plane and user resources in production. Finally, we learned how to ensure platform availability by verifying a cluster's components' status and executing backups.</p>
<p>In the next chapter, we will learn how to publish a deployed application using Docker's integrated tools and features.</p>
<h1 id="uuid-cbd74d2c-7268-4cba-9a7a-859c0e4b3222" class="mce-root">Questions</h1>
<ol>
<li class="mce-root">Which of these sentences are true?</li>
</ol>
<p style="padding-left: 90px">a) The Docker UCP installation process will also install the Docker Enterprise Engine on our hosts.<br/>
b) UCP provides an integrated RBAC system that will help us to authenticate and authorize our users against its database.<br/>
c) Docker UCP provides two kinds of access: the web UI and the UCP bundle.<br/>
d) All of the above sentences are true.</p>
<ol start="2">
<li class="mce-root">Which of these sentences is not true about a <kbd>docker</kbd>/<kbd>ucp</kbd> image?</li>
</ol>
<p style="padding-left: 90px">a) This image will provide UCP's backup and restore.<br/>
b) We should always use the latest <kbd>docker</kbd>/<kbd>ucp</kbd> release version in our environment.<br/>
c) Docker UCP can be completely removed using a <kbd>docker</kbd>/<kbd>ucp</kbd> image.<br/>
d) The upgrade process must be executed manually on each cluster's node.</p>
<ol start="3">
<li class="mce-root">What have we learned about the UCP installation process (which of the following is true)?</li>
</ol>
<p style="padding-left: 90px" class="mce-root">a) We can change the UCP controller and Kubernetes ports using special arguments.<br/>
b) We can isolate the control plane using <kbd>--data-path-addr</kbd> to specify an interface or an IP address for the data plane.<br/>
c) We can only have one subject alias name for the UCP environment and, by default, this will be the manager's IP address.<br/>
d) We will install UCP on the manager using the <kbd>docker/ucp install</kbd> procedure and then we will join worker nodes.</p>
<ol start="4">
<li>Which of the following is true about high availability in UCP?</li>
</ol>
<p style="padding-left: 90px" class="mce-root">a) UCP is deployed on top of a Docker Swarm cluster, so we will need an odd number of nodes to provide high availability.<br/>
b) We will need to deploy Kubernetes with high availability once UCP is installed.<br/>
c) An external load balancer is required to distribute client requests between different nodes using a transparent-proxy (passthrough) to allow managers to provide end-to-end TLS tunnels.<br/>
d) We can check a manager's availability using the <kbd>https://&lt;ucp-manager-url&gt;/_ping</kbd> endpoint.</p>
<ol start="5">
<li>Which one of these roles is not included in UCP by default?</li>
</ol>
<p style="padding-left: 90px" class="mce-root">a) <kbd>Privileged</kbd><br/>
b) <kbd>Full Control</kbd><br/>
c) <kbd>Administrator</kbd><br/>
d) <kbd>Scheduler</kbd></p>
<h1 id="uuid-fa135c43-faa1-473a-a0cf-352eeaebc0b2">Further reading</h1>
<p>Refer to the following links for more information regarding the topics that were covered in this chapter:</p>
<ul>
<li>Universal Control Plane overview: <a href="https://docs.docker.com/ee/ucp/">https://docs.docker.com/ee/ucp/</a></li>
<li>Docker Enterprise architecture: <a href="https://docs.docker.com/ee/docker-ee-architecture/">https://docs.docker.com/ee/docker-ee-architecture/</a></li>
<li>Docker Enterprise products: <a href="https://docs.docker.com/ee/supported-platforms/">https://docs.docker.com/ee/supported-platforms/</a> <a href="https://docs.docker.com/ee/supported-platforms/"/></li>
<li>UCP's access control mode: <a href="https://docs.docker.com/ee/ucp/authorization/">https://docs.docker.com/ee/ucp/authorization/</a></li>
<li>Deploying applications on UCP's Kubernetes: <a href="https://docs.docker.com/ee/ucp/kubernetes/">https://docs.docker.com/ee/ucp/kubernetes/</a></li>
<li>UCP access using the command line: <a href="https://docs.docker.com/ee/ucp/user-access/cli/">https://docs.docker.com/ee/ucp/user-access/cli/</a></li>
<li>Troubleshooting UCP node states: <a href="https://docs.docker.com/ee/ucp/admin/monitor-and-troubleshoot/troubleshoot-node-messages/">https://docs.docker.com/ee/ucp/admin/monitor-and-troubleshoot/troubleshoot-node-messages/</a></li>
<li>Docker UCP's API: <a href="https://docs.docker.com/reference/ucp/3.2/api/">https://docs.docker.com/reference/ucp/3.2/api/</a></li>
<li>Docker Enterprise best practices and design considerations: <a href="https://success.docker.com/article/docker-enterprise-best-practices">https://success.docker.com/article/docker-enterprise-best-practices</a></li>
<li>Designing a disaster recovery strategy: <a href="https://success.docker.com/article/dr-failover-strategy">https://success.docker.com/article/dr-failover-strategy</a></li>
</ul>


            

            
        
    </body></html>