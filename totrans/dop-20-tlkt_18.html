<html><head></head><body><div class="appendix" title="Appendix&#xA0;A.&#xA0;Docker Flow"><div class="titlepage"><div><div><h1 class="title"><a id="appA"/>Appendix A. Docker Flow</h1></div></div></div><p>Docker Flow<a class="indexterm" id="id848"/> is a project aimed towards creating an easy to use continuous deployment flow. It depends on Docker Engine, Docker Compose, Consul, and Registrator. Each of those tools is proven to bring value and are recommended for any Docker deployment. If you read the whole book, you should be familiar those tools as well as the process we're about to explore.</p><p>The goal of the project is to add features and processes that are currently missing inside the Docker ecosystem. The project, at the moment, solves the problems of blue-green deployments, relative scaling, and proxy service discovery and reconfiguration. Many additional features will be added soon.</p><p>The current list of features is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Blue-green deployment</li><li class="listitem" style="list-style-type: disc">Relative scaling</li><li class="listitem" style="list-style-type: disc">Proxy reconfiguration</li></ul></div><p>The<a class="indexterm" id="id849"/> latest release can be found at <a class="ulink" href="https://github.com/vfarcic/docker-flow/releases/tag/v1.0.2">https://github.com/vfarcic/docker-flow/releases/tag/v1.0.2</a>.</p><div class="section" title="The Background"><div class="titlepage"><div><div><h1 class="title"><a id="ch17lvl1sec47"/>The Background</h1></div></div></div><p>While working <a class="indexterm" id="id850"/>with different clients as well as writing the examples for this book, I realized that I end up writing different flavours of the same scripts. Some written in <span class="emphasis"><em>Bash</em></span>, others as <span class="emphasis"><em>Jenkins Pipeline</em></span>, some in <span class="emphasis"><em>Go</em></span>, and so on. So, as soon as I finished writing the book, I decided to start a project that will envelop many of the practices we explored. The result is the Docker Flow project.</p><div class="section" title="The Standard Setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch17lvl2sec99"/>The Standard Setup</h2></div></div></div><p>We'll start by<a class="indexterm" id="id851"/> exploring a typical Swarm cluster setup and discuss some of the problems we might face when using it as the cluster orchestrator. If you are already familiar with Docker Swarm, feel free to skip this section and jump straight into The Problems.</p><p>As a minimum, each node inside a Swarm cluster has to have Docker Engine and the Swarm container running. The later container should act as a node. On top of the cluster, we need at least one Swarm container running as master, and all Swarm nodes should announce their existence to it.</p><p>A combination <a class="indexterm" id="id852"/>of Swarm master(s) and nodes are a minimal setup that, in most cases, is far from sufficient. Optimum utilization of a cluster means that we are not in control anymore. Swarm is. It will decide which node is the most appropriate place for a container to run. That choice can be as simple as a node with the least number of containers running, or can be based on a more complex calculation that involves the amount of available CPU and memory, type of hard disk, affinity, and so on. No matter the strategy we choose, the fact is that we will not know where a container will run. On top of that, we should not specify ports our services should expose. "Hard-coded" ports reduce our ability to scale services and can result in conflicts. After all, two separate processes cannot listen to the same port. Long story short, once we adopt Swarm, both IPs and ports of our services will become unknown. So, the next step in setting up a Swarm cluster is to create a mechanism that will detect deployed services and store their information in a distributed registry so that the information is easily available.</p><p>Registrator is <a class="indexterm" id="id853"/>one of the tools that we can use to monitor Docker Engine events and send the information about deployed or stopped containers to a service registry. While there are many different service registries we can use, Consul proved to<a class="indexterm" id="id854"/> be, currently, the best one. Please read the Service Discovery: The Key to Distributed Services chapter for more information.</p><p>With <code class="literal">Registrator</code> and <code class="literal">Consul</code>, we can obtain information about any of the services running inside the Swarm cluster. A diagram of the setup we discussed, is as follows:</p><div class="mediaobject"><img alt="The Standard Setup" src="graphics/B05848_App_01.jpg"/><div class="caption"><p>Swarm cluster with basic service discovery</p></div></div><p>Please note that anything but a small cluster would have multiple Swarm masters and Consul instances thus preventing any loss of information or downtime in case one of them fails.</p><p>The process <a class="indexterm" id="id855"/>of deploying containers, in such a setup, is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The operator sends a request to <code class="literal">Swarm master</code> to deploy a service consisting of one or multiple containers. This request can be sent through <code class="literal">Docker CLI</code> by defining the <code class="literal">DOCKER_HOST</code> environment variable with the IP and the port of the <code class="literal">Swarm master</code>.</li><li class="listitem" style="list-style-type: disc">Depending on criteria sent in the request (CPU, memory, affinity, and so on), <code class="literal">Swarm master</code> makes the decision where to run the containers and sends requests to chosen <code class="literal">Swarm nodes</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Swarm node</code>, upon receiving the request to run (or stop) a container, invokes local <span class="emphasis"><em>Docker Engine</em></span>, which, in turn, runs (or stops) the desired container and publishes the result as an event.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Registrator</em></span> monitors <span class="emphasis"><em>Docker Engine</em></span> and, upon detecting a new event, sends the information to <span class="emphasis"><em>Consul</em></span>.</li><li class="listitem" style="list-style-type: disc">Anyone interested in data about containers running inside the cluster can consult <span class="emphasis"><em>Consul</em></span>.</li></ul></div><p>While this process is a vast improvement when compared to the ways we were operating clusters in the<a class="indexterm" id="id856"/> past, it is far from complete and creates quite a few problems that should be solved.</p></div><div class="section" title="The Problems"><div class="titlepage"><div><div><h2 class="title"><a id="ch17lvl2sec100"/>The Problems</h2></div></div></div><p>In this chapter, I will focus on three major problems or, to be more precise, features missing in the previously described setup.</p><div class="section" title="Deploying Without Downtime"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec43"/>Deploying Without Downtime</h3></div></div></div><p>When a new release is pulled, running <code class="literal">docker-compose up</code> will stop the containers running the old <a class="indexterm" id="id857"/>release and run the new one in their place. The problem with that approach is downtime. Between stopping the old release and running the new in its place, there is downtime. No matter whether it is one millisecond or a full minute, a new container needs to start, and the service inside it needs to initialize.</p><p>We can solve this by setting up a proxy with health checks. However, that would still require running multiple instances of the service (as you definitely should). The process would be to stop one instance and bring the new release in its place. During the downtime of that instance, the proxy would redirect the requests to one of the other instances. Then, when the first instance is running the new release and the service inside it is initialized, we would continue repeating the process with the other instances. This process can become very complicated and would prevent you from using Docker Compose <code class="literal">scale</code> command.</p><p>The better solution is to deploy the new release using the <span class="emphasis"><em>blue-green</em></span> deployment process. If you are unfamiliar with it, please read the <a class="link" href="ch13.html" title="Chapter 13. Blue-Green Deployment">Chapter 13</a>, <span class="emphasis"><em>Blue-Green </em></span>
<span class="emphasis"><em>Deployment</em></span>. In a nutshell, the process deploys the new release in parallel with the old one. Throughout the process, the proxy should continue sending all requests to the old release. Once the deployment is finished and the service inside the container is initialized, the proxy should be reconfigured to send all the requests to the new release and the old one can be stopped. With a process like this, we can avoid downtime. The problem is that Swarm does not support <span class="emphasis"><em>blue-green</em></span> deployment.</p></div><div class="section" title="Scaling Containers using Relative Numbers"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec44"/>Scaling Containers using Relative Numbers</h3></div></div></div><p>
<span class="emphasis"><em>Docker Compose</em></span> makes it very easy to scale services to a fixed number. We can specify how many instances of a container we want to run and watch the magic unfold. When combined with<a class="indexterm" id="id858"/> Docker Swarm, the result is an easy way to manage containers inside a cluster. Depending on how many instances are already running, Docker Compose will increase (or decrease) the number of running containers so that the desired result is achieved.</p><p>The problem is that Docker Compose always expects a fixed number as the parameter. That can be very limiting when dealing with production deployments. In many cases, we do not want to know how many instances are already running but send a signal to increase (or decrease) the capacity by some factor. For example, we might have an increase in<a class="indexterm" id="id859"/> traffic and want to increase the capacity by three instances. Similarly, if the demand for some service decreases, we might want the number of running instances to decrease by some factor and, in that way, free resources for other services and processes. This necessity is even more evident when we move towards autonomous and automated <a class="link" href="ch13.html" title="Chapter 13. Blue-Green Deployment">Chapter 13</a>, <span class="emphasis"><em>Self-Healing Systems</em></span> where human interactions are reduced to a minimum.</p><p>On top of the lack of relative scaling, <span class="emphasis"><em>Docker Compose</em></span> does not know how to maintain the same number of running instances when a new container is deployed.</p></div><div class="section" title="Proxy Reconfiguration after the New Release Is Tested"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec45"/>Proxy Reconfiguration after the New Release Is Tested</h3></div></div></div><p>The need for dynamic reconfiguration of the proxy becomes evident soon after we adopt microservices architecture. Containers allow us to pack them as immutable entities and Swarm lets us deploy them inside a cluster. The adoption of immutability through containers and <a class="indexterm" id="id860"/>cluster orchestrators like Swarm resulted in a<a class="indexterm" id="id861"/> huge increase in interest and adoption of microservices and, with them, the increase in deployment frequency. Unlike monolithic applications that forced us to deploy infrequently, now we can deploy often. Even if you do not adopt continuous deployment (each commit goes to production), you are likely to start deploying your microservices more often. That might be once a week, once a day, or multiple times a day. No matter the frequency, there is a high need to reconfigure the proxy every time a new release is deployed. Swarm will run containers somewhere inside the cluster, and proxy needs to be reconfigured to redirect requests to all the instances of the new release. That reconfiguration needs to be dynamic. That means that there must be a process that retrieves information from the service registry, changes the configuration of the proxy and, finally, reloads it.</p><p>There are several commonly used approaches to this problem.</p><p>Manual proxy reconfiguration should be discarded for obvious reasons. Frequent deploys mean that there is no time for an operator to change the configuration manually. Even if time is not of the essence, manual reconfiguration adds "human factor" to the process, and we are known to make mistakes.</p><p>There are quite a few tools that monitor Docker events or entries to the registry and reconfigure proxy whenever a new container is run or an old one is stopped. The problem with those tools is that they do not give us enough time to test the new release. If there is a bug or a feature is not entirely complete, our users will suffer. Proxy reconfiguration should be performed only after a set of tests is run, and the new release is validated.</p><p>We can use tools like <code class="literal">Consul Template</code> or <code class="literal">ConfD</code> into our deployment scripts. Both are great and work <a class="indexterm" id="id862"/>well but require quite a lot of plumbing before <a class="indexterm" id="id863"/>they are truly incorporated into the deployment process.</p></div><div class="section" title="Solving The Problems"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec46"/>Solving The Problems</h3></div></div></div><p>Docker Flow is the <a class="indexterm" id="id864"/>project that solves the problems we discussed. Its goal is to provide features that are not currently available in the Docker's ecosystem. It does not replace any of the ecosystem's features but builds on top of them.</p></div></div><div class="section" title="Docker Flow Walkthrough"><div class="titlepage"><div><div><h2 class="title"><a id="ch17lvl2sec101"/>Docker Flow Walkthrough</h2></div></div></div><p>The examples<a class="indexterm" id="id865"/> that follow will use Vagrant to simulate a Docker Swarm cluster. That does not mean that the usage of Docker Flow is limited to Vagrant. You can use it with a single Docker Engine or a Swarm cluster set up in any other way.</p><p>For similar <a class="indexterm" id="id866"/>examples based on Docker Machine (tested on Linux and OS X), please read the project (<a class="ulink" href="https://github.com/vfarcic/docker-flow">https://github.com/vfarcic/docker-flow</a>).</p><div class="section" title="Setting it up"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec47"/>Setting it up</h3></div></div></div><p>Before jumping into examples, please make sure that Vagrant is installed. You will not need anything <a class="indexterm" id="id867"/>else since the Ansible playbooks we are about to run will make sure that all the tools are correctly provisioned.</p><p>Please clone the code from the <code class="literal">vfarcic/docker-flow</code> repository:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>git clone https://github.com/vfarcic/docker-flow.git</strong></span>

<span class="strong"><strong>cd docker-flow</strong></span></pre></div><p>With the code downloaded, we can run Vagrant and create the cluster we'll use throughout this chapter:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>vagrant plugin install vagrant-cachier</strong></span>

<span class="strong"><strong>vagrant up master node-1 node-2 proxy</strong></span></pre></div><p>Once VMs are created and provisioned, the setup will be the same as explained in <span class="emphasis"><em>The Standard Setup</em></span> section of this chapter. The <code class="literal">master</code> server will contain <code class="literal">Swarm master</code> while nodes <code class="literal">1</code> and <code class="literal">2</code> will form the cluster. Each of those nodes will have <code class="literal">Registrator</code> pointing to the <code class="literal">Consul</code> instance running in the <code class="literal">proxy</code> server:</p><div class="mediaobject"><img alt="Setting it up" src="graphics/B05848_App_02.jpg"/><div class="caption"><p>Swarm cluster setup through Vagrant</p></div></div><p>Please note that<a class="indexterm" id="id868"/> this setup is for demo purposes only. While the same principle should be applied in production, you should aim at having multiple Swarm masters and Consul instances to avoid potential downtime in case one of them fails.</p><p>Once the <code class="literal">vagrant up</code> command is finished, we can enter the <code class="literal">proxy</code> VM and see <span class="emphasis"><em>Docker Flow</em></span> in action:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>vagrant ssh proxy</strong></span></pre></div><p>We'll run all the examples from the <code class="literal">proxy</code> machine. However, in production, you should run deployment commands from a separate machine (even your laptop).</p><p>The latest release of <span class="emphasis"><em>docker-flow</em></span> binary has been downloaded and ready to use, and the <code class="literal">/books-ms</code> directory contains the <code class="literal">docker-compose.yml</code> file we'll use in the examples that follow.</p><p>Let's enter the directory:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>cd /books-ms</strong></span></pre></div></div><div class="section" title="Reconfiguring Proxy after Deployment"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec48"/>Reconfiguring Proxy after Deployment</h3></div></div></div><p>Docker Flow<a class="indexterm" id="id869"/> requires the address of the Consul instance as well as the information about the node the proxy is (or will be) running on. It allows three ways to provide the necessary information. We can define arguments inside the <code class="literal">docker-flow.yml</code> file, as environment variables, or as command line arguments. In this example, we'll use all three input methods so that you can get familiar with them and choose the combination that suits you needs.</p><p>Let's start by <a class="indexterm" id="id870"/>defining proxy and Consul data through environment variables:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>export FLOW_PROXY_HOST=proxy</strong></span>

<span class="strong"><strong>export FLOW_CONSUL_ADDRESS=http://10.100.198.200:8500</strong></span>

<span class="strong"><strong>export FLOW_PROXY_DOCKER_HOST=tcp://proxy:2375</strong></span>

<span class="strong"><strong>export DOCKER_HOST=tcp://master:2375</strong></span>

<span class="strong"><strong>export BOOKS_MS_VERSION=":latest"</strong></span></pre></div><p>The <code class="literal">FLOW_PROXY_HOST</code> variable is the IP of the host where the proxy is running while the <code class="literal">FLOW_CONSUL_ADDRESS</code> represents the full address of the Consul API. The <code class="literal">FLOW_PROXY_DOCKER_HOST</code> is the host of the Docker Engine running on the server where the proxy container is (or will be) running. The last variable (<code class="literal">DOCKER_HOST</code>) is the address of the <code class="literal">Swarm master</code>. Docker Flow is designed to run operations on multiple servers at the same time, so we need to provide all the information it needs to do its tasks. In the examples we are exploring, it will deploy containers on the Swarm cluster, use Consul instance to store and retrieve information, and reconfigure the proxy every time a new service is deployed. Finally, we set the environment variable <code class="literal">BOOKS_MS_VERSION</code> to <span class="emphasis"><em>latest</em></span>. The <code class="literal">docker-compose.yml</code> uses it do determine which version we want to run.</p><p>Now we are ready to deploy the first release of our sample service:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker-flow \</strong></span>
<span class="strong"><strong>    --blue-green \</strong></span>
<span class="strong"><strong>    --target=app \</strong></span>
<span class="strong"><strong>    --service-path="/api/v1/books" \</strong></span>
<span class="strong"><strong>    --side-target=db \</strong></span>
<span class="strong"><strong>    --flow=deploy --flow=proxy</strong></span></pre></div><p>We instructed <code class="literal">docker-flow</code> to use the <span class="emphasis"><em>blue-green deployment</em></span> process and that the target (defined in <code class="literal">docker-compose.yml</code>) is <code class="literal">app</code>. We also told it that the service exposes an API on the address <code class="literal">/api/v1/books</code> and that it requires a side (or secondary) target <code class="literal">db</code>. Finally, through the <code class="literal">--flow</code> arguments we specified that the we want it to <span class="emphasis"><em>deploy</em></span> the targets and reconfigure the <code class="literal">proxy</code>. A lot happened in that single command so we'll explore the result in more detail.</p><p>Let's take a look at our servers and see what happened. We'll start with the Swarm cluster:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker ps --format "table {{.Names}}\t{{.Image}}"</strong></span></pre></div><p>The output of the <code class="literal">ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>NAMES                                IMAGE</strong></span>
<span class="strong"><strong>node-2/dockerflow_app-blue_1   vfarcic/books-ms</strong></span>
<span class="strong"><strong>node-1/books-ms-db             mongo</strong></span>
<span class="strong"><strong>...</strong></span></pre></div><p>Docker Flow run<a class="indexterm" id="id871"/> our main target <code class="literal">app</code> together with the side target named books-ms-db. Both targets are defined in <code class="literal">docker-compose.yml</code>. Container names depend on many different factors, some of which are the Docker Compose project (defaults to the current directory as in the case of the <code class="literal">app</code> target) or can be specified inside the <code class="literal">docker-compose.yml</code> through the <code class="literal">container_name</code> argument (as in the case of the <code class="literal">db</code> target). The first difference you'll notice is that <span class="emphasis"><em>Docker Flow</em></span> added <span class="emphasis"><em>blue</em></span> to the container name. The reason behind that is in the <code class="literal">--blue-green</code> argument. If present, <code class="literal">Docker Flow</code> will use the <span class="emphasis"><em>blue-green</em></span> process to run the primary target. Since this was the first deployment, <span class="emphasis"><em>Docker Flow</em></span> decided that it will be called <span class="emphasis"><em>blue</em></span>. If you are unfamiliar with the process, please read the <a class="link" href="ch13.html" title="Chapter 13. Blue-Green Deployment">Chapter 13</a>, <span class="emphasis"><em>Blue-Green Deployment</em></span> for general information.</p><p>Let's take a look at the <code class="literal">proxy</code> node as well:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>export DOCKER_HOST=tcp://proxy:2375</strong></span>

<span class="strong"><strong>docker ps --format "table {{.Names}}\t{{.Image}}"</strong></span></pre></div><p>The output of the <code class="literal">ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>NAMES               IMAGE</strong></span>
<span class="strong"><strong>docker-flow-proxy   vfarcic/docker-flow-proxy</strong></span>
<span class="strong"><strong>consul              progrium/consul</strong></span></pre></div><p>Docker Flow detected that there was no <code class="literal">proxy</code> on that node and run it for us. The <code class="literal">docker-flow-proxy</code> container contains <span class="emphasis"><em>HAProxy</em></span> together with custom code that reconfigures it every time a new service is run. For more information about the <span class="emphasis"><em>Docker Flow: Proxy</em></span>, please<a class="indexterm" id="id872"/> read the project (<a class="ulink" href="https://github.com/vfarcic/docker-flow-proxy">https://github.com/vfarcic/docker-flow-proxy</a>).</p><p>Since we instructed Swarm to deploy the service somewhere inside the cluster, we could not know in advance which server will be chosen. In this particular case, our service ended up running inside the <code class="literal">node-2</code>. Moreover, to avoid potential conflicts and allow easier scaling, we did not specify which port the service should expose. In other words, both the IP and the port of the service were not defined in advance. Among other things, <span class="emphasis"><em>Docker Flow</em></span> solves this by running <code class="literal">Docker Flow: Proxy</code> and instructing it to reconfigure itself with the information gathered after the container is run. We can confirm that the proxy reconfiguration was indeed successful by sending an HTTP request to the newly deployed service:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -I proxy/api/v1/books</strong></span></pre></div><p>The output of the <code class="literal">curl</code> command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>HTTP/1.1 200 OK</strong></span>
<span class="strong"><strong>Server: spray-can/1.3.1</strong></span>
<span class="strong"><strong>Date: Thu, 07 Apr 2016 19:23:34 GMT</strong></span>
<span class="strong"><strong>Access-Control-Allow-Origin: *</strong></span>
<span class="strong"><strong>Content-Type: application/json; charset=UTF-8</strong></span>
<span class="strong"><strong>Content-Length: 2</strong></span></pre></div><p>The flow <a class="indexterm" id="id873"/>of the events was as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Docker Flow inspected Consul to find out which release (blue or green) should be deployed next. Since this is the first deployment and no release was running, it decided to deploy it as blue.</li><li class="listitem">Docker Flow sent the request to deploy the blue release to Swarm Master, which, in turn, decided to run the container in the node-2. Registrator detected the new event created by Docker Engine and registered the service information in Consul. Similarly, the request was sent to deploy the side target db.</li><li class="listitem">Docker Flow retrieved the service information from Consul.</li><li class="listitem">Docker Flow inspected the server that should host the proxy, realized that it is not running, and deployed it.</li><li class="listitem">Docker Flow updated HAProxy with service information.<div class="mediaobject"><img alt="Reconfiguring Proxy after Deployment" src="graphics/B05848_App_03.jpg"/><div class="caption"><p>The first deployment through Docker Flow</p></div></div></li></ol></div><p>Even though our service is running in one of the servers chosen by Swarm and is exposing a random port, the proxy was reconfigured, and our users can access it through the fixed IP<a class="indexterm" id="id874"/> and without a port (to be more precise through the standard HTTP port <code class="literal">80</code> or HTTPS port <code class="literal">443</code>).</p><div class="mediaobject"><img alt="Reconfiguring Proxy after Deployment" src="graphics/B05848_App_04.jpg"/><div class="caption"><p>Users can access the service through the proxy</p></div></div><p>Let's see what happens when the second release is deployed.</p></div></div><div class="section" title="Deploying a New Release without Downtime"><div class="titlepage"><div><div><h2 class="title"><a id="ch17lvl2sec102"/>Deploying a New Release without Downtime</h2></div></div></div><p>After some time, a developer will push a new commit, and we'll want to deploy a new release<a class="indexterm" id="id875"/> of the service. We do not want to have any downtime so we'll continue using the <span class="emphasis"><em>blue-green</em></span> process. Since the current release is <span class="emphasis"><em>blue</em></span>, the new one will be named <span class="emphasis"><em>green</em></span>. Downtime will be avoided by running the new release (<span class="emphasis"><em>green</em></span>) in parallel with the old one (<span class="emphasis"><em>blue</em></span>) and, after it is fully up and running, reconfigure the proxy so that all requests are sent to the new release. Only after the proxy is reconfigured, we want the old release to stop running and free the resources it was using. We can accomplish all that by running the same <code class="literal">docker-flow</code> command. However, this time, we'll leverage the <code class="literal">docker-flow.yml</code> file that already has some of the arguments we used before.</p><p>The content of the <code class="literal">docker-flow.yml</code> is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>target: app</strong></span>
<span class="strong"><strong>side_targets:</strong></span>
<span class="strong"><strong>  - db</strong></span>
<span class="strong"><strong>blue_green: true</strong></span>
<span class="strong"><strong>service_path:</strong></span>
<span class="strong"><strong>  - /api/v1/books</strong></span></pre></div><p>Let's run the new release:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>export DOCKER_HOST=tcp://master:2375</strong></span>

<span class="strong"><strong>docker-flow \</strong></span>
<span class="strong"><strong>    --flow=deploy --flow=proxy --flow=stop-old</strong></span></pre></div><p>Just like<a class="indexterm" id="id876"/> before, let's explore Docker processes and see the result:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker ps -a --format "table {{.Names}}\t{{.Image}}\t{{.Status}}"</strong></span></pre></div><p>The output of the <code class="literal">ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>NAMES                        IMAGE                    STATUS</strong></span>
<span class="strong"><strong>node-1/booksms_app-green_1   vfarcic/books-ms         Up 33 seconds</strong></span>
<span class="strong"><strong>node-2/booksms_app-blue_1    vfarcic/books-ms         Exited (137) 22 seconds ago</strong></span>
<span class="strong"><strong>node-1/books-ms-db           mongo                    Up 41 minutes</strong></span>
<span class="strong"><strong>...</strong></span></pre></div><p>From the output, we can observe that the new release (<span class="emphasis"><em>green</em></span>) is running and that the old (<span class="emphasis"><em>blue</em></span>) was stopped. The reason the old release was only stopped and not entirely removed lies in potential need to rollback quickly in case a problem is discovered at some later moment in time.</p><p>Let's confirm that the proxy was reconfigured as well:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -I proxy/api/v1/books</strong></span></pre></div><p>The output of the curl command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>HTTP/1.1 200 OK</strong></span>
<span class="strong"><strong>Server: spray-can/1.3.1</strong></span>
<span class="strong"><strong>Date: Thu, 07 Apr 2016 19:45:07 GMT</strong></span>
<span class="strong"><strong>Access-Control-Allow-Origin: *</strong></span>
<span class="strong"><strong>Content-Type: application/json; charset=UTF-8</strong></span>
<span class="strong"><strong>Content-Length: 2</strong></span></pre></div><p>The flow of the events was as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Docker Flow inspected Consul to find out which release (blue or green) should be deployed next. Since the previous release was blue, it decided to deploy it as green.</li><li class="listitem">Docker Flow sent the request to Swarm Master to deploy the green release, which, in turn, decided to run the container in the node-1. Registrator detected the new event created by Docker Engine and registered the service information in Consul.</li><li class="listitem">Docker Flow retrieved the service information from Consul.</li><li class="listitem">Docker Flow<a class="indexterm" id="id877"/> updated HAProxy with service information.</li><li class="listitem">Docker Flow stopped the old release.<div class="mediaobject"><img alt="Deploying a New Release without Downtime" src="graphics/B05848_App_05.jpg"/><div class="caption"><p>The second deployment through Docker Flow</p></div></div></li></ol></div><p>Throughout the first three steps of the flow, HAProxy continued sending all requests to the old release. As the result, users were oblivious that deployment is in progress:</p><div class="mediaobject"><img alt="Deploying a New Release without Downtime" src="graphics/B05848_App_06.jpg"/><div class="caption"><p>During the deployment, users continue interacting with the old release</p></div></div><p>Only<a class="indexterm" id="id878"/> after the deployment is finished, HAProxy was reconfigured, and users were redirected to the new release. As the result, there was no downtime caused by deployment:</p><div class="mediaobject"><img alt="Deploying a New Release without Downtime" src="graphics/B05848_App_07.jpg"/><div class="caption"><p>After the deployment, users are redirected to the new release</p></div></div><p>Now that <a class="indexterm" id="id879"/>we have a safe way to deploy new releases, let us turn our attention to relative scaling.</p><div class="section" title="Scaling the service"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec49"/>Scaling the service</h3></div></div></div><p>One of the <a class="indexterm" id="id880"/>great benefits <span class="emphasis"><em>Docker Compose</em></span> provides is scaling. We can use it to scale to any number of instances. However, it allows only absolute scaling. We cannot instruct <span class="emphasis"><em>Docker Compose</em></span> to apply relative scaling. That makes the automation of some of the processes difficult. For example, we might have an increase in traffic that requires us to increase the number of instances by two. In such a scenario, the automation script would need to obtain the number of instances that are currently running, do some simple math to get to the desired number, and pass the result to Docker Compose. On top of all that, proxy still needs to be reconfigured as well. <span class="emphasis"><em>Docker Flow</em></span> makes this process much easier.</p><p>Let's see it in action:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker-flow \</strong></span>
<span class="strong"><strong>    --scale="+2" \</strong></span>
<span class="strong"><strong>    --flow=scale --flow=proxy</strong></span></pre></div><p>The scaling result can be observed by listing the currently running Docker processes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}"</strong></span></pre></div><p>The output of the <code class="literal">ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>NAMES                        IMAGE                     STATUS</strong></span>
<span class="strong"><strong>node-2/booksms_app-green_2   vfarcic/books-ms:latest   Up 5 seconds</strong></span>
<span class="strong"><strong>node-1/booksms_app-green_3   vfarcic/books-ms:latest   Up 6 seconds</strong></span>
<span class="strong"><strong>node-1/booksms_app-green_1   vfarcic/books-ms:latest   Up 40 minutes</strong></span>
<span class="strong"><strong>node-1/books-ms-db           mongo                     Up 53 minutes</strong></span></pre></div><p>The number of instances was increased by two. While only one instance was running before, now we have three.</p><p>Similarly, the proxy was reconfigured as well and, from now on, it will load balance all requests between those three instances.</p><p>The flow of the events was as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Docker Flow inspected Consul to find out how many instances are currently running.</li><li class="listitem">Since only one instance was running and we specified that we want to increase that number by two, Docker Flow sent the request to Swarm Master to scale the green release to three, which, in turn, decided to run one container on node-1 and the other on node-2. Registrator detected the new events created by Docker Engine and registered two new instances in Consul.</li><li class="listitem">Docker Flow retrieved the service information from Consul.</li><li class="listitem">Docker Flow updated HAProxy with the service information and set it up to perform <a class="indexterm" id="id881"/>load balancing among all three instances.<div class="mediaobject"><img alt="Scaling the service" src="graphics/B05848_App_08.jpg"/><div class="caption"><p>Relative scaling through Docker Flow</p></div></div></li></ol></div><p>From the users perspective, they continue receiving responses from the current release but, this time, their requests are load balanced among all instances of the service. As a result, service performance is improved:</p><div class="mediaobject"><img alt="Scaling the service" src="graphics/B05848_App_09.jpg"/><div class="caption"><p>Users requests are load balanced across all instances of the service</p></div></div><p>We can use the<a class="indexterm" id="id882"/> same method to de-scale the number of instances by prefixing the value of the <code class="literal">--scale</code> argument with the minus sign (<code class="literal">-</code>). Following the same example, when the traffic returns to normal, we can de-scale the number of instances to the original amount by running the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker-flow \</strong></span>
<span class="strong"><strong>    --scale="-1" \</strong></span>
<span class="strong"><strong>    </strong></span>
<span class="strong"><strong>--flow=scale --flow=proxy</strong></span></pre></div></div><div class="section" title="Testing Deployments to Production"><div class="titlepage"><div><div><h3 class="title"><a id="ch17lvl3sec50"/>Testing Deployments to Production</h3></div></div></div><p>The major<a class="indexterm" id="id883"/> downside of the proxy examples we run by now is the inability to verify the release before reconfiguring the proxy. Ideally, we should use the <span class="emphasis"><em>blue-green</em></span> process to deploy the new release in parallel with the old one, run a set of tests that validate that everything is working as expected, and, finally, reconfigure the proxy only if all tests were successful. We can accomplish that easily by running <code class="literal">docker-flow</code> twice.</p><p>Many tools aim at providing zero-downtime deployments but only a few of them (if any), take into account that a set of tests should be run before the proxy is reconfigured.</p><p>First, we should deploy the new version:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker-flow \</strong></span>
<span class="strong"><strong>    --flow=deploy</strong></span></pre></div><p>Let's list the Docker processes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"</strong></span></pre></div><p>The output of the <code class="literal">ps</code> command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>node-1/booksms_app-blue_2    Up 8 minutes        10.100.192.201:32773-&gt;8080/tcp</strong></span>
<span class="strong"><strong>node-2/booksms_app-blue_1    Up 8 minutes        10.100.192.202:32771-&gt;8080/tcp</strong></span>
<span class="strong"><strong>node-2/booksms_app-green_2   Up About an hour    10.100.192.202:32770-&gt;8080/tcp</strong></span>
<span class="strong"><strong>node-1/booksms_app-green_1   Up 2 hours          10.100.192.201:32771-&gt;8080/tcp</strong></span>
<span class="strong"><strong>node-1/books-ms-db           Up 2 hours          27017/tcp</strong></span></pre></div><p>At this moment, the new release (<span class="emphasis"><em>blue</em></span>) is running in parallel with the old release (<span class="emphasis"><em>green</em></span>). Since we did <a class="indexterm" id="id884"/>not specify the <span class="emphasis"><em>--flow=proxy</em></span> argument, the proxy is left unchanged and still redirects to all the instances of the old release. What this means is that the users of our service still see the old release, while we have the opportunity to test it. We can run integration, functional, or any other type of tests and validate that the new release indeed meets the expectations we have. While testing in production does not exclude testing in other environments (e.g. staging), this approach gives us greater level of trust by being able to validate the software under the same circumstances our users will use it, while, at the same time, not affecting them during the process (they are still oblivious to the existence of the new release).</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>Please note that even though we did not specify the number of instances that should be deployed, <span class="emphasis"><em>Docker Flow</em></span> deployed the new release and scaled it to the same number of instances as we had before.</p></div></div><p>The flow of the events was as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Docker Flow inspected Consul to find out the color of the current release and how many instances are currently running.</li><li class="listitem">Since two instances of the old release (<span class="emphasis"><em>green</em></span>) were running and we didn't specify that we want to change that number, Docker Flow sent the request to <span class="emphasis"><em>Swarm Master</em></span> to deploy the new release (<span class="emphasis"><em>blue</em></span>) and scale it to two instances.<div class="mediaobject"><img alt="Testing Deployments to Production" src="graphics/B05848_App_10.jpg"/><div class="caption"><p>Deployment without reconfiguring proxy</p></div></div></li></ol></div><p>From the <a class="indexterm" id="id885"/>users perspective, they continue receiving responses from the old release since we did not specify that we want to reconfigure the proxy:</p><div class="mediaobject"><img alt="Testing Deployments to Production" src="graphics/B05848_App_11.jpg"/><div class="caption"><p>Users requests are still redirected to the old release</p></div></div><p>From this moment, you can run tests in production against the new release. Assuming that you do not overload the server (e.g. stress tests), tests can run for any period without affecting users.</p><p>After the tests <a class="indexterm" id="id886"/>execution is finished, there are two paths we can take. If one of the tests failed, we can just stop the new release and fix the problem. Since the proxy is still redirecting all requests to the old release, our users would not be affected by a failure, and we can dedicate our time towards fixing the problem. On the other hand, if all tests were successful, we can run the rest of the <code class="literal">flow</code> that will reconfigure the proxy and stop the old release:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>docker-flow \</strong></span>
<span class="strong"><strong>    --flow=proxy --flow=stop-old</strong></span></pre></div><p>The command reconfigured the proxy and stopped the old release.</p><p>The flow of the events was as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Docker Flow inspected Consul to find out the color of the current release and how many instances are running.</li><li class="listitem">Docker Flow updated the proxy with service information.</li><li class="listitem">Docker Flow stopped the old release.<div class="mediaobject"><img alt="Testing Deployments to Production" src="graphics/B05848_App_12.jpg"/><div class="caption"><p>Proxy reconfiguration without deployment</p></div></div></li></ol></div><p>From the user's perspective, all new requests are redirected to the new release:</p><div class="mediaobject"><img alt="Testing Deployments to Production" src="graphics/B05848_App_13.jpg"/><div class="caption"><p>Users requests are redirected to the new release</p></div></div><p>That <a class="indexterm" id="id887"/>concludes the quick tour through some of the features <span class="emphasis"><em>Docker Flow</em></span> provides. Please explore the <span class="emphasis"><em>Usage</em></span> section for more details.</p></div></div></div></div></body></html>