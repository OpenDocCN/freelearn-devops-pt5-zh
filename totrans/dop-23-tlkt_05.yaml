- en: Using Services to Enable Communication between Pods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用服务启用 Pods 之间的通信
- en: Applications that cannot communicate with each other or are not accessible to
    end-users are worthless. Only once the communication paths are established, can
    applications fulfill their role.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 不能相互通信或无法对最终用户访问的应用程序毫无价值。只有在建立了通信路径后，应用程序才能履行其角色。
- en: Pods are the smallest unit in Kubernetes and have a relatively short life-span.
    They are born, and they are destroyed. They are never healed. The system heals
    itself by creating new Pods (cells) and by terminating those that are unhealthy
    or those that are surplus. The system is long-living, Pods are not.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是 Kubernetes 中最小的单位，生命周期相对较短。它们被创建，然后被销毁。它们从不被修复。系统通过创建新的 Pods（单元）和销毁那些不健康或多余的
    Pods 来进行自我修复。系统是长寿的，而 Pods 不是。
- en: Controllers, together with other components like the scheduler, are making sure
    that the Pods are doing the right thing. They control the scheduler. We used only
    one of them so far. ReplicaSet is in charge of making sure that the desired number
    of Pods is always running. If there's too few of them, new ones will be created.
    If there's too many of them, some will be destroyed. Pods that become unhealthy
    are terminated as well. All that, and a bit more, is controlled by ReplicaSet.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器与调度器等其他组件一起，确保 Pods 执行正确的操作。它们控制调度器。到目前为止，我们只使用了其中一个。ReplicaSet 负责确保所需数量的
    Pods 始终在运行。如果数量不足，它会创建新的 Pods；如果数量过多，它会销毁一些 Pods。那些变得不健康的 Pods 也会被终止。所有这些，甚至更多，都由
    ReplicaSet 控制。
- en: The problem with our current setup is that there are no communication paths.
    Our Pods cannot speak with each other. So far, only containers inside a Pod can
    talk with each other through `localhost`. That led us to the design where both
    the API and the database needed to be inside the same Pod. That was a lousy solution
    for quite a few reasons. The main problem is that we cannot scale one without
    the other. We could not design the setup in a way that there are, for example,
    three replicas of the API and one replica of the database. The primary obstacle
    was communication.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前设置的问题在于没有通信路径。我们的 Pods 不能相互通信。到目前为止，只有同一 Pod 内的容器能够通过`localhost`相互通信。这导致我们设计时将
    API 和数据库都放入同一个 Pod 内。这是一个糟糕的解决方案，原因有很多。主要问题在于我们不能在没有彼此的情况下扩展其中一个服务。我们无法设计出这样的设置，例如有三个
    API 副本和一个数据库副本。主要的障碍就是通信。
- en: Truth be told, each Pod does get its own address. We could have split the API
    and the database into different Pods and configure the API Pods to communicate
    with the database through the address of the Pod it lives in. However, since Pods
    are unreliable, short-lived, and volatile, we cannot assume that the database
    would always be accessible through the IP of a Pod. When that Pod gets destroyed
    (or fails), the ReplicaSet would create a new one and assign it a new address.
    We need a stable, never-to-be-changed address that will forward requests to whichever
    Pod is currently running.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，每个 Pod 确实会有自己的地址。我们本可以将 API 和数据库拆分到不同的 Pods 中，并配置 API Pods 通过所在 Pod 的地址与数据库通信。然而，由于
    Pods 不可靠、生命周期短暂且具有不稳定性，我们不能假设数据库总是可以通过 Pod 的 IP 访问。当该 Pod 被销毁（或失败）时，ReplicaSet
    会创建一个新的 Pod，并为其分配一个新的地址。我们需要一个稳定的、永不改变的地址，用于将请求转发到当前正在运行的 Pod。
- en: Kubernetes Services provide addresses through which associated Pods can be accessed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 服务提供了通过地址访问关联 Pods 的方式。
- en: Let's see Services in action.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看服务的实际应用。
- en: Creating a Cluster
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: You know the drill. Every chapter starts by pulling the latest code from the
    `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository, and with the creation of a new Minikube cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道的，按照惯例，每一章开始时，我们都会从 `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    仓库拉取最新代码，并创建一个新的 Minikube 集群。
- en: All the commands from this chapter are available in the `05-svc.sh` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    Gist.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 `05-svc.sh` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    Gist 中找到。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we have the latest code pulled and Minikube cluster running (again).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拉取了最新代码，并且 Minikube 集群正在运行（再次启动）。
- en: We can proceed with the first example of a Service.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始第一个 Service 示例了。
- en: Creating Services by exposing ports
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过暴露端口创建服务
- en: Before we dive into services, we should create a ReplicaSet similar to the one
    we used in the previous chapter. It'll provide the Pods we can use to demonstrate
    how Services work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解服务之前，我们应该创建一个与上一章中使用的类似的ReplicaSet。它将提供我们可以用来演示服务工作原理的Pods。
- en: 'Let''s take a quick look at the ReplicaSet definition:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下ReplicaSet的定义：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The only significant difference is the `db` container definition. It is as follows.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一显著的区别是`db`容器的定义。具体如下。
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We customized the command and the arguments so that MongoDB exposes the REST
    interface. We also defined the `containerPort`. Those additions are needed so
    that we can test that the database is accessible through the Service.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自定义了命令和参数，以便MongoDB可以暴露REST接口。我们还定义了`containerPort`。这些附加配置是必要的，以便我们能够测试数据库是否可以通过服务访问。
- en: 'Let''s create the ReplicaSet:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建ReplicaSet：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We created the ReplicaSet and retrieved its state from Kubernetes. The output
    is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了ReplicaSet并从Kubernetes中获取了其状态。输出如下：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You might need to wait until both replicas are up-and-running. If, in your case,
    the `READY` column does not yet have the value `2`, please wait for a while and
    `get` the state again. We can proceed after both replicas are running.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要等待直到两个副本都启动并运行。如果在你的情况下，`READY`列还没有显示`2`，请稍等片刻并再次执行`get`命令查看状态。等到两个副本都运行时，我们就可以继续操作了。
- en: We can use the `kubectl expose` command to expose a resource as a new Kubernetes
    service. That resource can be a Deployment, another Service, a ReplicaSet, a ReplicationController,
    or a Pod. We'll expose the ReplicaSet since it is already running in the cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl expose`命令将资源暴露为新的Kubernetes服务。该资源可以是Deployment、另一个Service、ReplicaSet、ReplicationController或Pod。我们将暴露ReplicaSet，因为它已经在集群中运行。
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We specified that we want to expose a ReplicaSet (`rs`) and that the name of
    the new Service should be `go-demo-2-svc`. The port that should be exposed is
    `28017` (the port MongoDB interface is listening to). Finally, we specified that
    the type of the Service should be `NodePort`. As a result, the target port will
    be exposed on every node of the cluster to the outside world, and it will be routed
    to one of the Pods controlled by the ReplicaSet.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了要暴露一个ReplicaSet（`rs`），并且新服务的名称应为`go-demo-2-svc`。应暴露的端口是`28017`（MongoDB接口正在监听的端口）。最后，我们指定服务的类型应为`NodePort`。因此，目标端口将在集群的每个节点上暴露到外部，并会路由到ReplicaSet控制的某个Pod。
- en: There are other Service types we could have used.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用其他类型的服务。
- en: '`ClusterIP` (the default type) exposes the port only inside the cluster. Such
    a port would not be accessible from anywhere outside. `ClusterIP` is useful when
    we want to enable communication between Pods and still prevent any external access.
    If `NodePort` is used, `ClusterIP` will be created automatically. The `LoadBalancer`
    type is only useful when combined with cloud provider''s load balancer. `ExternalName`
    maps a service to an external address (for example, `kubernetes.io`).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP`（默认类型）仅在集群内部暴露端口。此端口无法从外部任何地方访问。当我们希望启用Pod之间的通信，同时又不希望任何外部访问时，`ClusterIP`非常有用。如果使用`NodePort`，则会自动创建`ClusterIP`。`LoadBalancer`类型只有与云提供商的负载均衡器结合使用时才有意义。`ExternalName`将服务映射到外部地址（例如，`kubernetes.io`）。'
- en: In this chapter, we'll focus on `NodePort` and `ClusterIP` types. `LoadBalancer`
    will have to wait until we move our cluster to one of the cloud providers and
    `ExternalName` has a very limited usage.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将重点介绍`NodePort`和`ClusterIP`类型。`LoadBalancer`将等到我们将集群迁移到云提供商后才会使用，而`ExternalName`的使用非常有限。
- en: 'The processes that were initiated with the creation of the Service are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务时启动的进程如下：
- en: Kubernetes client (`kubectl`) sent a request to the API server requesting the
    creation of the Service based on Pods created through the `go-demo-2` ReplicaSet.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes客户端（`kubectl`）向API服务器发送请求，请求基于通过`go-demo-2` ReplicaSet创建的Pods来创建服务。
- en: Endpoint controller is watching the API server for new service events. It detected
    that there is a new Service object.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Endpoint控制器正在监视API服务器上的新服务事件。它检测到有一个新的服务对象。
- en: Endpoint controller created endpoint objects with the same name as the Service,
    and it used Service selector to identify endpoints (in this case the IP and the
    port of `go-demo-2` Pods).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Endpoint控制器创建了与服务同名的端点对象，并使用服务选择器来识别端点（在此例中是`go-demo-2` Pod的IP和端口）。
- en: kube-proxy is watching for service and endpoint objects. It detected that there
    is a new Service and a new endpoint object.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kube-proxy 正在监视 Service 和 endpoint 对象。它检测到有一个新的 Service 和一个新的 endpoint 对象。
- en: kube-proxy added iptables rules which capture traffic to the Service port and
    redirect it to endpoints. For each endpoint object, it adds iptables rule which
    selects a Pod.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kube-proxy 添加了 iptables 规则，捕获流量到 Service 端口并将其重定向到端点。对于每个端点对象，它会添加 iptables
    规则来选择一个 Pod。
- en: The kube-dns add-on is watching for Service. It detected that there is a new
    service.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kube-dns 插件正在监视 Service。它检测到有一个新的 Service。
- en: The kube-dns added `db` container's record to the dns server (skydns).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kube-dns 将 `db` 容器的记录添加到 DNS 服务器（skydns）。
- en: '![](img/9fb3fdee-8495-4fa1-b025-c3165544ee1b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fb3fdee-8495-4fa1-b025-c3165544ee1b.png)'
- en: 'Figure 5-1: The sequence of events followed by request to create a Service'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-1：请求创建 Service 后的事件顺序
- en: The sequence we described is useful when we want to understand everything that
    happened in the cluster from the moment we requested the creation of a new Service.
    However, it might be too confusing so we'll try to explain the same process through
    a diagram that more closely represents the cluster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的这个过程对于我们想要理解从请求创建新 Service 开始，到集群中发生的每一件事是非常有用的。然而，这个过程可能会显得过于混乱，因此我们将尝试通过一个更能代表集群的图表来解释相同的过程。
- en: '![](img/6e2811d0-694d-461d-ac6e-ffa0b38d73b5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e2811d0-694d-461d-ac6e-ffa0b38d73b5.png)'
- en: 'Figure 5-2: The Kubernetes components view when requesting creation of a Service'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-2：请求创建 Service 时 Kubernetes 组件的视图
- en: Let's take a look at our new Service.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下我们的新 Service。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see the name and the namespace. We did not yet explore namespaces (coming
    up later) and, since we didn't specify any, it is set to `default`. Since the
    Service is associated with the Pods created through the ReplicaSet, it inherited
    all their labels. The selector matches the one from the ReplicaSet. The Service
    is not directly associated with the ReplicaSet (or any other controller) but with
    Pods through matching labels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到名称和命名空间。我们还没有探索命名空间（稍后会介绍），由于我们没有指定任何命名空间，它被设置为 `default`。由于 Service 与通过
    ReplicaSet 创建的 Pods 关联，因此它继承了所有这些 Pod 的标签。选择器与 ReplicaSet 中的选择器匹配。Service 并未直接与
    ReplicaSet（或任何其他控制器）关联，而是通过匹配标签与 Pods 关联。
- en: Next is the `NodePort` type which exposes ports to all the nodes. Since `NodePort`
    automatically created `ClusterIP` type as well, all the Pods in the cluster can
    access the `TargetPort`. The `Port` is set to `28017`. That is the port that the
    Pods can use to access the Service. Since we did not specify it explicitly when
    we executed the command, its value is the same as the value of the `TargetPort`,
    which is the port of the associated Pod that will receive all the requests. `NodePort`
    was generated automatically since we did not set it explicitly. It is the port
    which we can use to access the Service and, therefore, the Pods from outside the
    cluster. In most cases, it should be randomly generated, that way we avoid any
    clashes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `NodePort` 类型，它将端口暴露给所有节点。由于 `NodePort` 自动创建了 `ClusterIP` 类型，因此集群中的所有 Pods
    都可以访问 `TargetPort`。`Port` 设置为 `28017`。这是 Pods 用来访问 Service 的端口。由于我们在执行命令时没有显式指定该端口，其值与
    `TargetPort` 的值相同，即与 Pod 关联的端口，该端口将接收所有请求。`NodePort` 是自动生成的，因为我们没有显式设置它。它是我们可以用来从集群外部访问
    Service 以及 Pods 的端口。在大多数情况下，它应该是随机生成的，这样可以避免端口冲突。
- en: 'Let''s see whether the Service indeed works:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Service 是否真的有效：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A note to Windows users
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 给 Windows 用户的提示
- en: Git Bash might not be able to use the `open` command. If that's the case, replace
    the `open` command with `echo`. As a result, you'll get the full address that
    should be opened directly in your browser of choice.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 `open` 命令。如果是这种情况，将 `open` 命令替换为 `echo`。这样，你将获得一个完整的地址，应该在你选择的浏览器中直接打开。
- en: We used the filtered output of the `kubectl get` command to retrieve the `nodePort`
    and store it as the environment variable `PORT`. Next, we retrieved the IP of
    the minikube VM. Finally, we opened MongoDB UI in a browser through the service
    port.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 `kubectl get` 命令的过滤输出来检索 `nodePort` 并将其存储为环境变量 `PORT`。接下来，我们获取了 minikube
    VM 的 IP 地址。最后，我们通过 Service 端口在浏览器中打开了 MongoDB UI。
- en: '![](img/09249c2b-a21e-4adb-bc07-1ac48dcc05f5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09249c2b-a21e-4adb-bc07-1ac48dcc05f5.png)'
- en: 'Figure 5-3: The Service created by exposing the ReplicaSet'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-3：通过暴露 ReplicaSet 创建的 Service
- en: As I already mentioned in the previous chapters, creating Kubernetes objects
    using imperative commands is not a good idea unless we're trying some quick hack.
    The same applies to Services. Even though `kubectl expose` did the work, we should
    try to use a documented approach through YAML files. In that spirit, we'll destroy
    the service we created and start over.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在前面章节中提到的，除非我们正在尝试一些快速的黑客操作，否则使用命令式命令创建 Kubernetes 对象并不是一个好主意。服务也是如此。尽管 `kubectl
    expose` 完成了工作，但我们应该尝试通过 YAML 文件使用一种文档化的方法。从这个角度出发，我们将销毁已创建的服务并重新开始。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Creating Services through declarative syntax
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过声明式语法创建服务
- en: We can accomplish a similar result as the one using `kubectl expose` through
    the `svc/go-demo-2-svc.yml` specification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `svc/go-demo-2-svc.yml` 规范实现与使用 `kubectl expose` 相似的结果。
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You should be familiar with the meaning of `apiVersion`, `kind`, and `metadata`,
    so we'll jump straight into the `spec` section. Since we already explored some
    of the options through the `kubectl expose` command, the `spec` should be relatively
    easy to grasp.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该熟悉 `apiVersion`、`kind` 和 `metadata` 的含义，所以我们将直接跳到 `spec` 部分。既然我们已经通过 `kubectl
    expose` 命令探索过一些选项，`spec` 部分应该比较容易理解。
- en: The type of the Service is set to `NodePort` meaning that the ports will be
    available both within the cluster as well as from outside by sending requests
    to any of the nodes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的类型设置为 `NodePort`，这意味着这些端口将同时在集群内和外部通过向任何节点发送请求的方式可用。
- en: The `ports` section specifies that the requests should be forwarded to the Pods
    on port `28017`. The `nodePort` is new. Instead of letting the service expose
    a random port, we set it to the explicit value of `30001`. Even though, in most
    cases, that is not a good practice, I thought it might be a good idea to demonstrate
    that option as well. The protocol is set to `TCP`. The only other alternative
    would be to use `UDP`. We could have skipped the protocol altogether since `TCP`
    is the default value but, sometimes, it is a good idea to leave things as a reminder
    of an option.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`ports` 部分指定请求应该转发到端口为 `28017` 的 Pods。`nodePort` 是新的。我们没有让服务暴露一个随机端口，而是将其设置为明确的
    `30001` 值。尽管在大多数情况下，这不是一个好的实践，但我觉得演示这个选项也是个不错的主意。协议设置为 `TCP`。唯一的其他选择是使用 `UDP`。我们本可以完全跳过协议部分，因为
    `TCP` 是默认值，但有时候，留下一些选项作为提醒是个好主意。'
- en: The `selector` is used by the Service to know which Pods should receive requests.
    It works in the same way as ReplicaSet selectors. In this case, we defined that
    the service should forward requests to Pods with labels `type` set to `backend`
    and `service` set to `go-demo`. Those two labels are set in the Pods `spec` of
    the ReplicaSet.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`selector` 被服务用于确定哪些 Pods 应该接收请求。它的工作方式与 ReplicaSet 的选择器相同。在这种情况下，我们定义了服务应将请求转发到
    `type` 设置为 `backend` 且 `service` 设置为 `go-demo` 的 Pods。这两个标签在 ReplicaSet 的 Pods
    `spec` 中设置。'
- en: Now that there's no mystery in the definition, we can proceed and create the
    Service.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 既然定义中没有什么神秘的内容，我们可以继续并创建服务。
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We created the Service and retrieved its information from the API server. The
    output of the latter command is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了服务并从 API 服务器获取了其信息。后一条命令的输出如下：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that the Service is running (again), we can double-check that it is working
    as expected by trying to access MongoDB UI.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经运行（再次），我们可以通过尝试访问 MongoDB UI 来再次确认它是否按预期工作。
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since we fixed the `nodePort` to `30001`, we did not have to retrieve the Port
    from the API server. Instead, we used the IP of the Minikube node and the hard-coded
    port `30001` to open the UI.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将 `nodePort` 固定为 `30001`，因此无需从 API 服务器获取端口。相反，我们使用了 Minikube 节点的 IP 和硬编码的端口
    `30001` 来打开 UI。
- en: '![](img/82fe0877-d23c-4f0a-b8af-9031ef7093fa.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82fe0877-d23c-4f0a-b8af-9031ef7093fa.png)'
- en: 'Figure 5-4: The Service with the matching Pods and hard-coded port'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-4：具有匹配 Pods 和硬编码端口的服务
- en: Let's take a look at the endpoint. It holds the list of Pods that should receive
    requests.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看端点。它包含了应该接收请求的 Pod 列表。
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see that there are two subsets, corresponding to the two Pods that contain
    the same labels as the Service `selector`. Each has a unique IP that is included
    in the algorithm used when forwarding requests. Actually, it's not much of an
    algorithm. Requests will be sent to those Pods randomly. That randomness results
    in something similar to round-robin load balancing. If the number of Pods does
    not change, each will receive an approximately equal number of requests.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有两个子集，分别对应于包含与 Service `selector` 相同标签的两个 Pods。每个 Pod 都有一个唯一的 IP 地址，并会在转发请求时包含在算法中。实际上，这并不算复杂的算法。请求将随机发送到这些
    Pods。这种随机性产生了类似于轮询负载均衡的效果。如果 Pods 数量不变，每个 Pod 会接收大致相等的请求数量。
- en: Random requests forwarding should be enough for most use cases. If it's not,
    we'd need to resort to a third-party solution (for now). However soon, when Kubernetes
    1.9 gets released, we'll have an alternative to the *iptables* solution. We'll
    be able to apply different types of load balancing algorithms like last connection,
    destination hashing, newer queue, and so on. Still, the current solution is based
    on *iptables*, and we'll stick with it, for now.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数使用场景，随机请求转发应该足够。如果不够，我们将不得不借助第三方解决方案（暂时如此）。然而，当 Kubernetes 1.9 发布时，我们将有一个替代
    *iptables* 解决方案。我们将能够应用不同类型的负载均衡算法，如最后连接、目标哈希、新队列等。不过，目前的解决方案仍基于 *iptables*，我们暂时保持这个方案。
- en: Throughout the book, so far, I repeated a few times that our current Pod design
    is flawed. We have two containers (an API and a database) packaged together. Among
    other problems, that prevents us from scaling one without the other. Now that
    we learned how to use Services, we can redesign our Pod solution.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，到目前为止，我已经提到过几次我们当前的 Pod 设计有缺陷。我们有两个容器（一个 API 和一个数据库）打包在一起。除此之外，它还阻止了我们对其中一个进行扩展而不影响另一个。现在，我们已经学会了如何使用
    Services，我们可以重新设计我们的 Pod 解决方案。
- en: 'Before we move on, we''ll delete the Service and the ReplicaSet we created:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将删除我们创建的 Service 和 ReplicaSet：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Both the ReplicaSet and the Service are gone, and we can start a new.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 和 Service 都已删除，我们可以重新开始。
- en: Splitting the Pod and establishing communication through Services
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割 Pod 并通过 Services 建立通信
- en: 'Let''s take a look at a ReplicaSet definition for a Pod with only the database:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下一个仅包含数据库的 Pod 的 ReplicaSet 定义：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We'll comment only on the things that changed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对发生变化的部分进行评论。
- en: Since this ReplicaSet defines only the database, we reduced the number of replicas
    to `1`. Truth be told, MongoDB should be scaled as well, but that's out of the
    scope of this chapter (and probably the book as well). For now, we'll pretend
    that one replica of a database is enough.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个 ReplicaSet 仅定义了数据库，我们将副本数减少到 `1`。说实话，MongoDB 也应该进行扩展，但这超出了本章（甚至可能是本书）的范围。现在，我们假设一个数据库副本就足够了。
- en: Since `selector` labels need to be unique, we changed them slightly. The `service`
    is still `go-demo-2`, but the `type` was changed to `db`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `selector` 标签需要唯一，我们稍微更改了一下它们。`service` 仍然是 `go-demo-2`，但 `type` 已更改为 `db`。
- en: The rest of the definition is the same except that the `containers` now contain
    only `mongo`. We'll define the API in a separate ReplicaSet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的定义相同，只是 `containers` 现在只包含 `mongo`。我们将在一个单独的 ReplicaSet 中定义 API。
- en: Let's create the ReplicaSet before we move to the Service that will reference
    its Pod.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转到引用其 Pod 的 Service 之前，先创建 ReplicaSet。
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: One object was created, three are left to go.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 已创建一个对象，剩下三个。
- en: The next one is the Service for the Pod we just created through the ReplicaSet.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个是我们刚通过 ReplicaSet 创建的 Pod 的 Service。
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This Service definition does not contain anything new. There is no `type`, so
    it'll default to `ClusterIP`. Since there is no reason for anyone outside the
    cluster to communicate with the database, there's no need to expose it using the
    `NodePort` type. We also skipped specifying the `nodePort`, since only internal
    communication within the cluster is allowed. The same is true for the `protocol`.
    `TCP` is all we need, and it happens to be the default one. Finally, the `selector`
    labels are the same as the labels that define the Pod.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Service 定义没有包含任何新内容。没有 `type`，所以它将默认使用 `ClusterIP`。由于没有理由让集群外部的任何人访问数据库，因此无需使用
    `NodePort` 类型暴露它。我们还跳过了指定 `nodePort`，因为只允许集群内部的通信。`protocol` 也是如此。`TCP` 就足够了，它恰好是默认协议。最后，`selector`
    标签与定义 Pod 的标签相同。
- en: 'Let''s create the Service:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来创建 Service：
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are finished with the database. The ReplicaSet will make sure that the Pod
    is (almost) always up-and-running and the Service will allow other Pods to communicate
    with it through a fixed DNS.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库部分已经完成。副本集将确保Pod（几乎）始终运行，而服务将允许其他Pods通过固定的DNS与其通信。
- en: Moving to the backend API...
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 继续处理后端API...
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Just as with the database, this ReplicaSet should be familiar since it's very
    similar to the one we used before. We'll comment only on the differences.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 和数据库一样，这个副本集应该很熟悉，因为它与我们之前使用的副本集非常相似。我们只会评论其中的差异。
- en: The number of `replicas` is set to `3`. That solves one of the main problems
    we had with the previous ReplicaSets that defined Pods with both containers. Now
    the number of replicas can differ, and we have one Pod for the database, and three
    for the backend API.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`replicas`的数量设置为`3`。这解决了我们在之前的副本集中遇到的一个主要问题，即定义了包含两个容器的Pod。现在副本数量可以不同，我们有一个数据库Pod，三个后端API
    Pod。'
- en: The `type` label is set to `api` so that both the ReplicaSet and the (soon to
    come) Service can distinguish the Pods from those created for the database.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`type`标签设置为`api`，以便副本集和（即将到来的）服务可以区分这些Pod与为数据库创建的Pod。'
- en: We have the environment variable `DB` set to `go-demo-2-db`. The code behind
    the `vfarcic/go-demo-2` image is written in a way that the connection to the database
    is established by reading that variable. In this case, we can say that it will
    try to connect to the database running on the DNS `go-demo-2-db`. If you go back
    to the database Service definition, you'll notice that its name is `go-demo-2-db`
    as well. If everything works correctly, we should expect that the DNS was created
    with the Service and that it'll forward requests to the database.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了环境变量`DB`，其值为`go-demo-2-db`。`vfarcic/go-demo-2`镜像中的代码通过读取该变量来建立与数据库的连接。在这种情况下，我们可以说它会尝试连接到运行在DNS
    `go-demo-2-db`上的数据库。如果你返回查看数据库服务定义，你会注意到它的名称也是`go-demo-2-db`。如果一切正常，我们应该期待DNS已经与服务一起创建，并且会将请求转发到数据库。
- en: In earlier Kubernetes versions it used `userspace` proxy mode. Its advantage
    is that the proxy would retry failed requests to another Pod. With the shift to
    the `iptables` mode, that feature is lost. However, `iptables` are much faster
    and more reliable, so the loss of the retry mechanism is well compensated. That
    does not mean that the requests are sent to Pods "blindly". The lack of the retry
    mechanism is mitigated with `readinessProbe`, which we added to the ReplicaSet.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的Kubernetes版本中，使用的是`userspace`代理模式。它的优点是代理会将失败的请求重试到另一个Pod。随着转向`iptables`模式，这一特性丧失了。然而，`iptables`更快、更可靠，因此丧失重试机制的缺失得到了很好的弥补。这并不意味着请求会“盲目地”发送到Pods。缺少重试机制通过我们为副本集添加的`readinessProbe`得到了缓解。
- en: The `readinessProbe` has the same fields as the `livenessProbe`. We used the
    same values for both, except for the `periodSeconds`, where instead of relying
    on the default value of `10`, we set it to `1`. While `livenessProbe` is used
    to determine whether a Pod is alive or it should be replaced by a new one, the
    `readinessProbe` is used by the `iptables`. A Pod that does not pass the `readinessProbe`
    will be excluded and will not receive requests. In theory, Requests might be still
    sent to a faulty Pod, between two iterations. Still, such requests will be small
    in number since the `iptables` will change as soon as the next probe responds
    with HTTP code less than `200`, or equal or greater than `400`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`readinessProbe`与`livenessProbe`有相同的字段。我们对两者使用了相同的值，除了`periodSeconds`，我们将其从默认的`10`设置为`1`。`livenessProbe`用于确定一个Pod是否存活，或者是否应该被新Pod替代，而`readinessProbe`则由`iptables`使用。未通过`readinessProbe`的Pod将被排除，并且不会接收请求。从理论上讲，请求可能仍然会在两次探测之间发送到故障Pod，但由于`iptables`会在下一次探测响应HTTP代码小于`200`或大于等于`400`时立即更改，因此这种情况发生的请求数量会很少。'
- en: Ideally, an application would have different end-points for the `readinessProbe`
    and the `livenessProbe`. This one doesn't so the same one should do. You can blame
    it on me being too lazy to add them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，应用程序会为`readinessProbe`和`livenessProbe`设置不同的端点。但这个没有，因此相同的端点就足够了。你可以怪我太懒，没去添加它们。
- en: Let's create the ReplicaSet.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建副本集。
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Only one object is missing, that is service:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 只缺少一个对象，那就是服务：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There's nothing truly new in this definition. The `type` is set to `NodePort`
    since the API should be accessible from outside the cluster. The `selector` label
    `type` is set to `api` so that it matches the labels defined for the Pods.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义中没有什么新的内容。`type`设置为`NodePort`，因为API应该可以从集群外部访问。`selector`标签`type`设置为`api`，以便它与Pod中定义的标签匹配。
- en: 'That is the last object we''ll create (in this section), so let''s move on
    and do it:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将要创建的最后一个对象（在本节中），所以让我们继续并进行创建：
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We''ll take a look at what we have in the cluster:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看集群中有什么内容：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Both ReplicaSets for `db` and api are there, followed by the three replicas
    of the `go-demo-2-api` Pods and one replica of the `go-demo-2-db` Pod. Finally,
    the two Services are running as well, together with the one created by Kubernetes
    itself.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`db`和api的两个ReplicaSets都在其中，接着是`go-demo-2-api` Pod的三个副本和`go-demo-2-db` Pod的一个副本。最后，两个服务也在运行，包括Kubernetes自身创建的那个。'
- en: I'm not sure why are the ReplicaSets duplicated in this view. My best guess
    is that it is a bug that will be corrected soon. To be honest, I haven't spent
    time investigating that since it does not affect how the cluster and ReplicaSets
    work. If you execute `kubectl get rs`, you'll see that there are only two of them,
    not four.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定为什么在这个视图中会出现重复的ReplicaSets。我的最佳猜测是这可能是一个bug，应该很快就会被修复。说实话，我没有花时间去调查这个问题，因为它不会影响集群和ReplicaSets的工作。如果你执行`kubectl
    get rs`，你会看到只有两个ReplicaSets，而不是四个。
- en: Before we proceed, it might be worth mentioning that the code behind the `vfarcic/go-demo-2`
    image is designed to fail if it cannot connect to the database. The fact that
    the three replicas of the `go-demo-2-api` Pod are running means that the communication
    is established. The only verification left is to check whether we can access the
    API from outside the cluster. Let's try that out.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，可能值得一提的是，`vfarcic/go-demo-2`镜像背后的代码设计是，如果它无法连接到数据库，它将失败。`go-demo-2-api`
    Pod的三个副本能够运行，意味着通信已经建立。剩下的唯一验证就是检查我们是否能从集群外部访问API。让我们试试看。
- en: '[PRE32]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We retrieved the port of the service (we still have the Minikube node `IP`
    from before) and used it to send a request. The output of the last command is
    as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取了服务的端口（我们仍然保留之前的Minikube节点`IP`）并使用它发送了请求。最后一条命令的输出如下：
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We got the response `200` and a friendly `hello, world!` message indicating
    that the API is indeed accessible from outside the cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了响应`200`，以及一个友好的`hello, world!`消息，表明API确实可以从集群外部访问。
- en: At this point, you might be wondering whether it is overkill to have four YAML
    files for a single application. Can't we simplify the definitions? Not really.
    Can we define everything in a single file? Read on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会想，是否需要为一个应用程序准备四个YAML文件。我们能不能简化这些定义？实际上不能。我们能否将所有内容定义在一个文件中？继续往下看。
- en: 'Before we move further, we''ll delete the objects we created. By now, you probably
    noticed that I like destroying things and starting over. Bear with me. There is
    a good reason for the imminent destruction:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将删除我们创建的对象。到现在为止，你可能已经注意到，我喜欢摧毁现有的东西然后重新开始。请耐心一点，摧毁是有充分理由的：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Everything we created is gone, and we can start over.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的所有内容都已删除，我们可以重新开始。
- en: Defining multiple objects in the same YAML file
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在同一个YAML文件中定义多个对象
- en: The `vfarcic/go-demo-2` and `mongo` images form the same stack. They work together
    and having four YAML definitions is confusing. It would get even more confusing
    later on since we are going to add more objects to the stack. Things would be
    much simpler and easier if we would move all the objects we created thus far into
    a single YAML definition. Fortunately, that is very easy to accomplish.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`vfarcic/go-demo-2`和`mongo`镜像构成了同一个堆栈。它们一起工作，拥有四个YAML定义很让人困惑。以后会更复杂，因为我们将会向堆栈中添加更多对象。如果我们把迄今为止创建的所有对象移到一个单一的YAML定义中，事情会变得更简单，容易得多。幸运的是，这很容易实现。'
- en: 'Let''s take a look at yet another YAML file:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看另一个YAML文件：
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We won't display the output since it is the same as the contents of the previous
    four YAML files combined. The only difference is that each object definition is
    separated by three dashes (`---`).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不展示输出，因为它与前四个YAML文件的内容相同。唯一的不同是每个对象的定义之间用三个破折号（`---`）分隔。
- en: 'If you''re as paranoid as I am, you''d like to double check that everything
    works as expected, so let''s create the objects defined in that file:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样偏执，你会想再次确认一切是否按预期工作，所以让我们创建那个文件中定义的对象：
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output of the latter command is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 后者命令的输出如下：
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The two ReplicaSets and the two Services were created, and we can rejoice in
    replacing four files with one.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了两个ReplicaSet和两个服务，我们可以为将四个文件替换为一个而感到高兴。
- en: Finally, to be on the safe side, we'll also double check that the stack API
    is up-and-running and accessible.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了安全起见，我们还将再次检查堆栈API是否正在运行并且可访问。
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The response is `200` indicating that everything works as expected.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 响应为`200`，表明一切按预期工作。
- en: Before we finish the discussion about Services, we might want to go through
    the discovery process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束关于服务的讨论之前，我们可能希望了解一下发现过程。
- en: Discovering Services
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: Services can be discovered through two principal modes; environment variables
    and DNS.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 服务可以通过两种主要模式进行发现：环境变量和DNS。
- en: Every Pod gets environment variables for each of the active Services. They are
    provided in the same format as what Docker links expect, as well with the simpler
    Kubernetes-specific syntax.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Pod都获取与每个活动服务相关的环境变量。它们的格式与Docker链接期望的格式相同，也具有更简单的Kubernetes特定语法。
- en: Let's take a look at the environment variables available in one of the Pods
    we're running.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们正在运行的其中一个Pod中的环境变量。
- en: '[PRE39]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output, limited to the environment variables related to the `go-demo-2-db`
    service, is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于与`go-demo-2-db`服务相关的环境变量，如下所示：
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The first five variables are using the Docker format. If you already worked
    with Docker networking, you should be familiar with them. At least, if you're
    familiar with the way Swarm (standalone) and Docker Compose operate. Later version
    of Swarm (Mode) still generate the environment variables but they are mostly abandoned
    by the users in favour of DNSes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 前五个变量采用Docker格式。如果你曾经使用过Docker网络，你应该对它们很熟悉。至少，如果你熟悉Swarm（独立）和Docker Compose的工作方式的话。Swarm（模式）的后续版本仍然会生成这些环境变量，但用户大多已放弃使用它们，转而使用DNS。
- en: The last two environment variables are Kubernetes specific and follow the `[SERVICE_NAME]_SERVICE_HOST`
    and `[SERVICE_NAME]_SERIVCE_PORT` format (service name is upper-cased).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个环境变量是Kubernetes特定的，遵循`[SERVICE_NAME]_SERVICE_HOST`和`[SERVICE_NAME]_SERVICE_PORT`格式（服务名称为大写）。
- en: No matter which set of environment variables you choose to use (if any), they
    all serve the same purpose. They provide a reference we can use to connect to
    a Service and, therefore to the related Pods.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择使用哪一组环境变量（如果有的话），它们的目的都是一样的。它们提供了一个可以用来连接到服务，从而连接到相关Pod的参考。
- en: Things will become more evident when we describe the `go-demo-2-db` Service.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们描述`go-demo-2-db`服务时，事情会变得更加清晰。
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The key is in the `IP` field. That is the IP through which this service can
    be accessed and it matches the values of the environment variables `GO_DEMO_2_DB_*`
    and `GO_DEMO_2_DB_SERVICE_HOST`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于`IP`字段。那是可以访问此服务的IP，它与环境变量`GO_DEMO_2_DB_*`和`GO_DEMO_2_DB_SERVICE_HOST`的值匹配。
- en: The code inside the containers that form the `go-demo-2-api` Pods could use
    any of those environment variables to construct a connection string towards the
    `go-demo-2-db` Pods. For example, we could have used `GO_DEMO_2_DB_SERVICE_HOST`
    to connect to the database. And, yet, we didn't do that. The reason is simple.
    It is easier to use DNS instead.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 形成`go-demo-2-api` Pods的容器中的代码可以使用任何这些环境变量来构造与`go-demo-2-db` Pods的连接字符串。例如，我们本可以使用`GO_DEMO_2_DB_SERVICE_HOST`来连接到数据库。然而，我们并没有这样做。原因很简单，使用DNS更为便捷。
- en: 'Let''s take another look at the snippet from the `go-demo-2-api-rs.yml` ReplicaSet
    definition:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一眼来自`go-demo-2-api-rs.yml`的`ReplicaSet`定义片段：
- en: '[PRE43]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We declared an environment variable with the name of the Service (`go-demo-2-db`).
    That variable is used by the code as a connection string to the database. Kubernetes
    converts Service names into DNSes and adds them to the DNS server. It is a cluster
    add-on that is already set up by Minikube.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明了一个名为服务名称（`go-demo-2-db`）的环境变量。代码使用该变量作为连接数据库的连接字符串。Kubernetes将服务名称转换为DNS，并将其添加到DNS服务器中。它是一个由Minikube已设置的集群插件。
- en: 'Let''s go through the sequence of events related to service discovery and components
    involved:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下与服务发现相关的事件顺序和涉及的组件：
- en: When the `api` container `go-demo-2` tries to connect with the `go-demo-2-db`
    Service, it looks at the nameserver configured in `/etc/resolv.conf`. `kubelet`
    configured the nameserver with the kube-dns Service IP (`10.96.0.10`) during the
    Pod scheduling process.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 `api` 容器 `go-demo-2` 尝试连接到 `go-demo-2-db` 服务时，它会查看 `/etc/resolv.conf` 中配置的名称服务器。`kubelet`
    在 Pod 调度过程中将名称服务器配置为 kube-dns 服务的 IP（`10.96.0.10`）。
- en: The container queries the DNS server listening to port `53`. `go-demo-2-db`
    DNS gets resolved to the service IP `10.0.0.19`. This DNS record was added by
    kube-dns during the service creation process.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器查询监听在端口 `53` 的 DNS 服务器。`go-demo-2-db` 的 DNS 被解析为服务 IP `10.0.0.19`。这个 DNS 记录是在服务创建过程中由
    kube-dns 添加的。
- en: The container uses the service IP which forwards requests through the iptables
    rules. They were added by kube-proxy during Service and Endpoint creation process.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器使用服务 IP，它通过 iptables 规则转发请求。iptables 规则是在服务和端点创建过程中由 kube-proxy 添加的。
- en: Since we only have one replica of the `go-demo-2-db` Pod, iptables forwards
    requests to just one endpoint. If we had multiple replicas, iptables would act
    as a load balancer and forward requests randomly among Endpoints of the Service.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们只有一个 `go-demo-2-db` Pod 的副本，iptables 只会将请求转发到一个端点。如果我们有多个副本，iptables 会充当负载均衡器，将请求随机转发到服务的多个端点。
- en: '![](img/238e11b0-6ce1-47d2-927e-c28311551956.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/238e11b0-6ce1-47d2-927e-c28311551956.png)'
- en: 'Figure 5-5: Service discovery process and components involved'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-5：服务发现过程及涉及的组件
- en: What now?
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: That was it. We went through most important aspects of Services. There are a
    few other cases we did not yet explore, but the current knowledge should be more
    than enough to get you going.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。我们已经讲解了服务的最重要方面。还有一些我们尚未探索的情况，但目前的知识应该足够让你继续前进。
- en: Services are indispensable objects without which communication between Pods
    would be hard and volatile. They provide static addresses through which we can
    access them not only from other Pods but also from outside the cluster. This ability
    to have fixed entry points is crucial as it provides stability to otherwise dynamic
    elements of the cluster. Pods come and go, Services stay.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是不可或缺的对象，没有它们，Pod 之间的通信将变得困难且不稳定。它们提供静态地址，通过这些地址我们不仅可以从其他 Pod 访问它们，还可以从集群外部访问它们。拥有固定的入口点至关重要，因为它为本来动态的集群元素提供了稳定性。Pod
    会来来去去，而服务则一直存在。
- en: We are one crucial topic away from having a fully functional, yet still simple,
    strategy for deployment and management of our applications. We are yet to explore
    how to deploy and update our services without downtime.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只差一个关键的主题，就能拥有一个功能完整、但仍然简单的应用部署和管理策略。我们还没有探索如何在没有停机的情况下部署和更新服务。
- en: We have exhausted this topic and the time has come to destroy everything we
    did so far.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论完这个话题，接下来是时候销毁我们迄今为止所做的一切了。
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If you'd like to know more about Services, please explore Service v1 core ([https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#service-v1-core](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#service-v1-core))
    API documentation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于服务的信息，请查看 Service v1 核心 ([https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#service-v1-core](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#service-v1-core))
    API 文档。
- en: 'Figure 5-6: The components explored so far'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-6：迄今为止探索的组件
- en: Kubernetes Pods, ReplicaSets, and Services compared to Docker Swarm stacks
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes Pods、ReplicaSets 和 Services 与 Docker Swarm 堆栈的对比
- en: Starting from this chapter, we'll compare each Kubernetes feature with Docker
    Swarm equivalents. That way, Swarm users can have a smoother transition into Kubernetes
    or, depending on their goals, choose to stick with Swarm.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们将把每个 Kubernetes 功能与 Docker Swarm 对应功能进行对比。这样，Swarm 用户可以更平滑地过渡到 Kubernetes，或者根据他们的目标，选择继续使用
    Swarm。
- en: Please bear in mind that the comparisons will be made only for a specific set
    of features. You will not (yet) be able to conclude whether Kubernetes is better
    or worse than Docker Swarm. You'll need to grasp both products in their entirety
    to make an educated decision. The comparisons like those that follow are useful
    only as a base for more detailed examinations of the two products.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，比较仅会基于特定的功能集进行。你（目前）无法得出 Kubernetes 是否优于 Docker Swarm 的结论。你需要全面掌握这两款产品，才能做出明智的决定。像接下来的比较，只有在对两款产品进行更详细的审视时，才会有用。
- en: For now, we'll limit the comparison scope to Pods, ReplicaSets, and Services
    on the one hand, and Docker Service stacks, on the other.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将比较范围限制为Pod、ReplicaSet和Service一方面，以及Docker Service堆栈另一方面。
- en: Let's start with Kubernetes file `go-demo-2.yml` ([https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2.yml](https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2.yml))
    (the same one we used before).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Kubernetes文件`go-demo-2.yml`（[https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2.yml](https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2.yml)）开始（这是我们之前使用过的同一个文件）。
- en: 'The definition is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 定义如下：
- en: '[PRE45]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now, let's take a look at the Docker stack defined in `go-demo-2-swarm.yml`
    ([https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2-swarm.yml](https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2-swarm.yml)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下在`go-demo-2-swarm.yml`中定义的Docker堆栈（[https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2-swarm.yml](https://github.com/vfarcic/k8s-specs/blob/master/svc/go-demo-2-swarm.yml)）。
- en: 'The specification is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 规范如下：
- en: '[PRE46]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Both definitions accomplish the same result. There is no important difference
    from the functional point of view, except in Pods. Docker does not have the option
    to create something similar. When Swarm services are created, they are spread
    across the cluster, and there is no easy way to specify that multiple containers
    should run on the same node. Whether multi-container Pods are useful or not is
    something we'll explore later. For now, we'll ignore that feature.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 两种定义实现了相同的结果。从功能角度来看，没有重要区别，唯一的区别是Pods。Docker没有创建类似的选项。当Swarm服务被创建时，它们会分布在集群中，并且没有简单的方法来指定多个容器应运行在同一节点上。是否需要多容器Pod，这是我们稍后探讨的内容。现在，我们暂时忽略这个功能。
- en: If we execute something like `docker stack deploy -c svc/go-demo-2-swarm.yml
    go-demo-2`, the result would be equivalent to what we got when we run `kubectl
    create -f svc/go-demo-2.yml`. In both cases, we get three replicas of `vfarcic/go-demo-2`,
    and one replica of `mongo`. Respective schedulers are making sure that the desired
    state (almost) always matches the actual state. Networking communication through
    internal DNSes is also established with both solutions. Each node in a cluster
    would expose a randomly defined port that forwards requests to the `api`. All
    in all, there are no functional differences between the two solutions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们执行类似于`docker stack deploy -c svc/go-demo-2-swarm.yml go-demo-2`的命令，结果将与我们运行`kubectl
    create -f svc/go-demo-2.yml`时的结果相同。在这两种情况下，我们都会得到三个`vfarcic/go-demo-2`副本，以及一个`mongo`副本。各自的调度器确保期望状态（几乎）始终与实际状态匹配。通过内部DNS进行的网络通信在这两种解决方案中都已建立。集群中的每个节点都会暴露一个随机定义的端口，将请求转发到`api`。总的来说，这两种解决方案在功能上没有区别。
- en: When it comes to the way services are defined, there is indeed, a considerable
    difference. Docker's stack definition is much more compact and straight-forward.
    We defined, in twelve lines, what took around eighty lines in the Kubernetes format.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务定义的方式上，确实存在相当大的差异。Docker的堆栈定义更加紧凑、直接。我们在12行中定义了Kubernetes格式中约80行所需要的内容。
- en: One might argue that Kubernetes YAML file could have been smaller. Maybe it
    could. Still, it'll be bigger and more complex no matter how much we simplify
    it. One might also say that Docker's stack is missing `readinessProbe` and `livenessProbe`.
    Yes it is, and that is because I decided not to put it there, because the `vfarcic/go-demo-2`
    image already has `HEALTHCHECK` definition that Docker uses for similar purposes.
    In most cases, Dockerfile is a better place to define health checks than a stack
    definition. That does not mean that it cannot be set, or overwritten, in a YAML
    file. It can, when needed. But, that is not the case in this example.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会争辩说Kubernetes的YAML文件可以更小，也许可以。但无论如何简化，它仍然会更大、更复杂。也有人可能会说，Docker的堆栈缺少`readinessProbe`和`livenessProbe`。是的，确实如此，这也是因为我决定不把它们放在那里，因为`vfarcic/go-demo-2`镜像已经有了Docker用于类似目的的`HEALTHCHECK`定义。在大多数情况下，Dockerfile是定义健康检查的更好地方，而不是堆栈定义。这并不意味着它不能在YAML文件中设置或覆盖。如果需要，它是可以的。但在这个例子中，并不需要。
- en: All in all, if we limit ourselves only to Kubernetes Pods, ReplicaSets, and
    Services, and their equivalents in Docker Swarm, the latter wins due to a much
    simpler and more straightforward way to define specs. From the functional perspective,
    both are very similar.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，如果我们仅限于Kubernetes的Pods、ReplicaSets和Services，以及它们在Docker Swarm中的等效项，由于Docker
    Swarm定义规范的方式更加简单直接，后者更具优势。从功能角度来看，两者非常相似。
- en: Should you conclude that Swarm is a better option than Kubernetes? Not at all.
    At least, not until we compare other features. Swarm won the battle, but the war
    has just begun. As we progress, you'll see that there's much more to Kubernetes.
    We only scratched the surface.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得出 Swarm 比 Kubernetes 更好的结论吗？完全不应如此。至少，在我们比较其他特性之前，不应该这么认为。Swarm 赢得了这场战斗，但战争才刚刚开始。随着我们的深入，你会发现
    Kubernetes 还有更多值得探索的内容。我们只是触及了皮毛。
