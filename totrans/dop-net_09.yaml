- en: Chapter 9. Using Continuous Delivery Pipelines to Deploy Network Changes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on some of the different methods that can be used to
    deploy network changes using deployment pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: It will first look at Continuous Delivery and continuous deployment processes
    and what these methodologies entail in terms of workflow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at the different deployment tools, artifacts repositories,
    and packaging methods that can be used to set up deployment pipelines and ways
    in which network changes can be integrated into those pipelines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration package management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous Delivery and deployment overview
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment methodologies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging deployment artifacts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment pipeline tooling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying network changes with deployment pipelines
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration package management
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.html "Chapter 7. Using Continuous Integration Builds for
    Network Configuration"), *Using Continuous Integration Builds For Network Configuration*,
    we looked at the process of continuous integration and in [Chapter 8](ch08.html
    "Chapter 8. Testing Network Changes"), *Testing Network Changes*, we looked at
    adding testing to the continuous integration process to provide increased validation
    and feedback loops in case of failure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: When carrying out continuous integration, using a fail fast / fix fast philosophy
    is desirable. This involves putting in necessary validation checks to decipher
    whether a build is valid and provide feedback loops to users.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: This promotes the correct behavior within the teams that do frequent, small,
    incremental changes, which de-risks the changes. While each change is validated
    using the **Continuous Integration** (**CI**) engine with instant feedback on
    changes, a process of continuous improvement is adhered to as teams strive to
    make more robust solutions that will pass all quality checks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: As important as providing feedback loops is, producing successful builds is
    equally important to the process as this is how products are shipped to market.
    When a continuous integration build completes, it often needs to package build
    artifacts that are in a fit state so they can be deployed to target servers. This
    is often referred to as creating a shippable product or artifact.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Any continuous integration process should carry out the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Commit
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build (Compile/Version/Tag)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Package
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time a new commit takes place, a new continuous integration build will
    be triggered. This will result in a code being pulled down from the SCM system,
    which will trigger a build step, which can either be a compilation process, or
    if the build process is not using a compiled language, then versioning or tagging
    of the binaries. Finally, a set of validation steps will be carried out inclusive
    of any required testing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: If all validations prove successful, then a set of post-continuous integration
    process steps need to be carried out. Post-build steps will include the package
    and push process, this means packaging build binaries and pushing the newly versioned
    package to an Artifact Repository of choice.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'An example process that includes **Commit Change**, Build (**Compile Code**),
    Validate (**Unit Tests**), **Package** (Artifact), and Push to an **Artifact Repository**
    is shown in the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: An important principle to remember when setting up continuous integration builds
    and packaging continuous integration artifacts, is that artifacts should be packaged
    once only, not every single time they need to be deployed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: This is important from a repeatability perspective and also reduces the time
    taken to deploy as a build process can be lengthy and take many minutes. When
    a build has been packaged, all tests and necessary validation have been carried
    out on the artifact as part of the continuous integration process, so there is
    no need to repeat this process again if no changes have occurred.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: It is imperative that we ensure the exact same artifact is deployed to test
    environments before being promoted onto production; this means there will be no
    drift between environments. The same source code being packaged on a different
    build server may result in the version of Java being slightly different, or even
    something as simple as a different environment variable could mean the build binaries
    are compiled differently.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Maintain consistent deployment artifacts, always swearing by the principle of
    package once and deploy multiple times.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard of package once, deploy multiple times is illustrated following,
    where a single artifact is used to seed test and production environments:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Creating different build artifacts for each environment is a non-starter; release
    management best practices dictate that a build package and artifacts should include
    tokens so different snapshots of the same package are not required.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Build package tokens can then be transformed at deployment time. All environment
    specific information is held in a configuration file of some sort, normally called
    an **environment** file, which is used to populate the tokens at deployment time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The following best practices should be adhered to when packaging continuous
    integration build artifacts:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Artifacts should be packaged once and distributed many times
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact packages should be packaged with tokenized configuration files
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact package configuration files should be transformed at deployment time
    using an environment file
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common files can be used to supplement environment files if deployment configuration
    is common to all environments to avoid repetition
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following popular configuration management tools have the ability to transform
    tokenized templates by utilizing configuration files. Each of these configuration
    files take on the role of the environment file:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Puppet [https://puppet.com/](https://puppet.com/)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chef [https://www.chef.io/chef/](https://www.chef.io/chef/)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible [https://www.ansible.com/](https://www.ansible.com/)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salt [https://saltstack.com/](https://saltstack.com/)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking Ansible as an example, in [Chapter 4](ch04.html "Chapter 4. Configuring
    Network Devices Using Ansible"), *Configuring Network Devices Using Ansible*,
    we covered the concept of jinja2 templates. Jinja2 templates allow template files
    to be populated with tokens and these tokens are substituted with particular key
    value pairs at deployment time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Ansible allows users to populate jinja2 templates to be populated with variables
    (tokens). Each `var` file can be configured so that it is unique to each environment.
    Environment files can be imported into playbooks and roles by inputting it as
    a command line argument. This will in turn transform the jinja2 templates at deployment
    time with the environment-specific information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we see an Ansible playbook `configure_env.yml` being
    executed, and a unique environment variable called environment needing to be set:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: This will be imported into the playbook `configure_env.yml` so that a unique
    set of environment information is loaded for each environment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, taking the component, integration, system test, and production environments
    as an example the following files would be loaded:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/comp.yml`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/int.yml`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/sys.yml`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/prod.yml`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each unique environment, the deployment command differs only in the environment
    file that is loaded which will make the deployment environment specific:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Continuous Delivery and deployment overview
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous Delivery and deployment are a natural extension of the continuous
    integration process. Continuous Delivery and deployment create a consistent mechanism
    to deploy changes to production and create a conveyer belt delivering new features
    to customers or end users. So conceptually a conveyer belt is what continuous
    Delivery is all about, but in terms of actual process how is this achieved?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'A continuous integration process will carry out the following high level steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Commit
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build (Compile/Version/Tag)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Package
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous Delivery and deployment take over once the artifact has been pushed
    to the artifact repository. Each and every build artifact created by a continuous
    integration process should be considered a release candidate, meaning that it
    can potentially be deployed to production if it passes all validations in the
    Continuous Delivery pipeline.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Like continuous integration, Continuous Delivery and deployment create a series
    of feedback loops to indicate if validation tests have failed on an environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 像持续集成一样，持续交付和部署会创建一系列反馈循环，以指示验证测试是否在某个环境中失败。
- en: 'A Continuous Delivery pipeline process will encapsulate the following high
    level steps at each stage of a deployment pipeline:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付流程将涵盖每个部署阶段的以下高层步骤：
- en: Deploy (pull/tokenize/setup)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署（拉取/令牌化/设置）
- en: Validate (test)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证（测试）
- en: Promote
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推广
- en: A stage in a deployment pipeline will contain a series of tests which will be
    used to help validate whether the application is functioning as required prior
    to it being released to production.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管道中的一个阶段将包含一系列测试，用于帮助验证应用程序是否在发布到生产环境之前按要求正常工作。
- en: Each stage in the deployment pipeline will have a deployment step which will
    pull down the artifact from the **Artifact Repository** to the target server and
    execute the deployment steps. The deployment process will normally involve installing
    software or configuring a change to the state of the server. Configuration changes
    are typically governed by a configuration management tool such as Puppet, Chef,
    Ansible, or Salt.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管道中的每个阶段都会有一个部署步骤，它将从**Artifact Repository**拉取工件到目标服务器并执行部署步骤。部署过程通常涉及安装软件或配置服务器状态的更改。配置更改通常由配置管理工具管理，例如
    Puppet、Chef、Ansible 或 Salt。
- en: Once deployment is completed, a series of tests will be carried out in the environment
    to validate the deployment and also test the functionality of the application
    or change.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署完成，将在环境中进行一系列测试，以验证部署并测试应用程序或更改的功能。
- en: Continuous Delivery means that if validation tests pass on a test environment
    then the build artifact is automatically promoted to the next environment. The
    deployment, validation, and promotion steps are carried out again on the next
    environment in the same way as the previous environment. In the event of a failure,
    the release candidate will break and it will not be promoted to the next stage
    of the process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付意味着如果在测试环境中验证测试通过，那么构建工件会自动推广到下一个环境。在下一个环境中，部署、验证和推广步骤将以与前一个环境相同的方式进行。如果出现故障，发布候选版本将会失败，并且不会被推广到下一阶段。
- en: 'When using Continuous Delivery this automatic promotion happens all the way
    to the environment prior to production as shown in the following diagram:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用持续交付时，自动推广会一直持续到生产前的环境，如下图所示：
- en: '![Continuous Delivery and deployment overview](img/B05559_09_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![持续交付和部署概览](img/B05559_09_05.jpg)'
- en: 'Continuous deployment on the other hand has no paused state before production
    and differs from Continuous Delivery in that it will automatically deploy to production:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 持续部署与持续交付不同，它在生产前没有暂停状态，并且自动部署到生产环境：
- en: '![Continuous Delivery and deployment overview](img/B05559_09_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![持续交付和部署概览](img/B05559_09_06.jpg)'
- en: So the only difference between Continuous Delivery and continuous deployment
    is the manual pause from promoting the build artifact to production.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，持续交付和持续部署之间的唯一区别就是是否手动暂停将构建工件推广到生产环境。
- en: The reason for implementing Continuous Delivery over continuous deployment is
    normally down to either governance or the maturity of testing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 实施持续交付而非持续部署的原因通常与治理或测试的成熟度有关。
- en: When starting out, continuously deploying to production throughout the day can
    seem very daunting as it mandates that the deployment process is completely automated
    and that the validation and testing on each environment is mature enough to catch
    all known errors.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 刚开始时，全天候持续部署到生产环境可能显得非常令人生畏，因为它要求部署过程完全自动化，并且每个环境中的验证和测试必须足够成熟，以捕捉所有已知的错误。
- en: With continuous deployment, the trigger of a production deployment is a SCM
    commit, so it puts a lot of trust in the deployment system. This means it is desirable
    that the branching strategy is set up to pull all changes from the **trunk/mainline/master**
    branch and trigger the deployment pipeline. Having multiple different branches
    will complicate the deployment process so it is important to implement a branching
    strategy that minimizes repetition, and an explosion of the number of deployment
    pipelines that are required.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续部署中，生产部署的触发器是SCM提交，因此它把大量的信任放在了部署系统上。这意味着期望分支策略被设置为从**主干/主线/主分支**拉取所有变更并触发部署流水线。拥有多个不同的分支将使部署过程变得复杂，因此重要的是实施一个最小化重复并避免部署流水线数量爆炸的分支策略。
- en: If implemented badly, continuous deployment can result in continuous downtime,
    so normally after setting up continuous integration businesses, teams should start
    with Continuous Delivery and aim to eventually move to a continuous deployment
    once processes have matured sufficiently.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果实施不当，持续部署可能导致持续的停机时间，因此通常在建立持续集成后，企业和团队应从持续交付开始，目标是随着流程的成熟最终过渡到持续部署。
- en: As covered in [Chapter 3](ch03.html "Chapter 3. Bringing DevOps to Network Operations"),
    *Bringing DevOps to Network Operations*, cultural change is needed within the
    business to implement a Continuous Delivery model and it really is an all or nothing
    approach for it to work successfully.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第3章](ch03.html "第3章. 将DevOps带入网络运营")中所述，*将DevOps带入网络运营*，业务内部需要进行文化变革来实施持续交付模型，实际上这是一种全有或全无的方式，才能成功实现。
- en: As stated in [Chapter 8](ch08.html "Chapter 8. Testing Network Changes"), *Testing
    Network Changes*, manually updating environments can compromise the validity of
    tests so they should be avoided at all costs, every change should flow through
    SCM to downstream environments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第8章](ch08.html "第8章. 测试网络变更")中所述，*测试网络变更*，手动更新环境可能会影响测试的有效性，因此应该尽一切可能避免，所有变更都应通过SCM流向下游环境。
- en: Continuous Delivery promotes automation and creation of test packs at every
    stage in the deployment pipeline, but it also allows a business to cherry-pick
    the release candidate that is finally deployed to production.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付促进了自动化和在每个部署流水线阶段创建测试包，但它也允许企业精挑细选最终部署到生产环境的发布候选版本。
- en: This means additional validation could be carried out manually during the imposed
    stop before production, in the absence of the desired level or test coverage for
    a build artifact. It also plays well with companies that are subject to regulatory
    requirements that may mean they only have a specified deployment window and they
    cannot deploy to production continuously.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在生产之前的停顿期间，可以手动进行额外的验证，以弥补构建制品未达到所需的测试覆盖率或测试水平。这对于受监管要求的公司来说尤其适用，因为它们可能只在指定的部署窗口内进行部署，而无法持续部署到生产环境。
- en: Continuous Delivery means that regulated companies can still benefit from automated
    environments and tests, but the production deployment is just a button click to
    select the artifact, which has passed all aforementioned promotions and is deployed
    to production.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付意味着受监管的公司仍然可以受益于自动化环境和测试，但生产部署只是通过点击按钮来选择已经通过所有前述验证并部署到生产环境的制品。
- en: Deployment methodologies
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署方法论
- en: When carrying out Continuous Delivery and deployment, there is no one size fits
    all deployment strategy. Configuration management tools such as Puppet, Chef,
    Ansible, and Salt have different approaches to deployment and use different approaches
    when keeping servers up to date.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行持续交付和部署时，并没有适合所有场景的统一部署策略。像Puppet、Chef、Ansible和Salt这样的配置管理工具在部署方法和保持服务器最新的方式上有所不同。
- en: The tool that is selected is not important, only the ideal workflow and processes
    to support delivering changes that are consistent, quick, and accurate.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的工具并不重要，重要的是理想的工作流和流程，以支持交付一致、快速且准确的变更。
- en: Pull model
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拉取模型
- en: Tools such as Puppet and Chef adopt a centralized approach to configuration
    management, where they have a centralized server that acts as the brain for the
    deployment process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet和Chef等工具采用集中式配置管理方法，它们有一个集中的服务器，充当部署过程的“大脑”。
- en: In Puppet's case the centralized server are the Puppet Master and in Chef's
    case the centralized server is the Chef Server. This centralized server is a set
    of infrastructure provisioned to store server configuration according to the configuration
    management tool's reference architecture.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: All updates to server configuration is pushed to the centralized server first
    and then subsequently pushed out to the corresponding servers using agents. These
    agents can either poll the centralized server for updates and apply them straight
    away or alternately wait for the Puppet Agent, or in Chef's case, the Chef Client
    to be invoked to start the convergence of configuration from the centralized server
    to the server containing the agent.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The overriding principle in a pull model is that the centralized server governs
    the state of the system and every change goes via the Puppet Master or Chef Server.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: If any user logs onto a server and changes the state, then the next time that
    the state converges from the centralized server it could overwrite those manual
    changes when the agent runs (Puppet Agent or Chef Client) if that particular configuration
    is managed by the centralized server.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In this model, the centralized server will control all application versioning
    information and environment configuration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a pull model is shown in the following diagram. This shows Chef
    being used in a Continuous Delivery process:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The continuous integration process creates a new build artifact which is pushed
    to the **Artifact Repository**
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chef's command line client **knife** is invoked as a post-build action which
    updates the **Chef Server** with the new version of the application which is being
    deployed
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment process is then triggered on **Component Test Environment** by
    running **Chef Client** which will trigger the **Chef Client** to check the state
    against the **Chef Server**
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Chef Client** in this case, sees a new application version is available
    based on the last **knife** update and as a result updates the environment to
    the new version of the application
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, all validation and test steps are run prior to promoting it to the
    next stage of the deployment pipeline
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence on the subsequent **Integration Test Environment** is only triggered
    if the **Component Test Environment** promotion is successful![Pull model](img/B05559_09_07.jpg)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tools such as Ansible and Salt adopt a push model to configuration management,
    where they have a control host that is used to connect to servers using SSH and
    configure them.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a centralized server, Ansible and Salt use a control host,
    which has a command-line client installed on the server. The control host is then
    used to push changes to servers via logging on to them using SSH either via password
    or alternatively, SSH keys.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: As Ansible and Salt are Python-based, they are agentless and can run on any
    Linux distribution, as Python is a pre-requisite for these servers. Windows machines
    are connected to and configured using WinRM.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Ansible和Salt是基于Python的，它们不需要代理，可以在任何Linux发行版上运行，因为Python是这些服务器的前提条件。Windows机器通过WinRM连接并进行配置。
- en: All configuration management information is stored in SCM systems and pulled
    down to the control host, this configuration is then used to push updates out
    to servers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所有配置管理信息都存储在SCM系统中，并被拉取到控制主机，这些配置随后用于将更新推送到服务器。
- en: The overriding principle in a push model is that changes are committed into
    SCM. The SCM server, rather than a centralized server, is the source of truth
    for state, configuration, and versioning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 推送模型的主要原则是将更改提交到SCM中。SCM服务器，而不是集中式服务器，是状态、配置和版本控制的真实来源。
- en: 'An example of a push model follows. This shows Ansible being used in a Continuous
    Delivery process:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是推送模型的一个示例，展示了Ansible在持续交付过程中的使用：
- en: The continuous integration process creates a new build artifact which is pushed
    to the **Artifact Repository**
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续集成过程创建了一个新的构建构件，并将其推送到**构件库**。
- en: A new artifact being present in the **Artifact Repository** triggers the deployment
    process and the Ansible playbook/role is downloaded from the **SCM System** to
    the **Ansible Control Host**
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构件库**中出现新构件时，会触发部署过程，Ansible的剧本/角色会从**SCM系统**下载到**Ansible控制主机**。'
- en: The deployment process is then triggered on **Component Test Environment** and
    Ansible is executed against all servers that are present in the targeted inventory
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后在**组件测试环境**中触发部署过程，并对目标清单中所有服务器执行Ansible。
- en: Finally, all validation and test steps are run prior to promoting it to the
    next stage of the deployment pipeline
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在将其提升到部署管道的下一个阶段之前，所有验证和测试步骤都会运行。
- en: Ansible is only executed on the subsequent Integration Test Environment if the
    Component Test Environment promotion is successful![Push model](img/B05559_09_08.jpg)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在**组件测试环境**的推广成功后，Ansible才会在后续的集成测试环境中执行![推送模型](img/B05559_09_08.jpg)
- en: When to choose pull or push
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时选择拉取或推送
- en: When selecting a pull or push method of configuration management, it is down
    to preference and should be selected based on the approach to infrastructure.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择拉取或推送配置管理方法时，取决于个人偏好，应该根据基础设施的处理方式来选择。
- en: Pull models are popular when dealing with server estates that have long-lived
    infrastructure. It lends itself well to patching a whole estate of servers to
    keep on top of compliance. Pull models, as they have a centralized server with
    the current state, means that if configuration is removed from a server, then
    the centralized server will register that a delete is required. Push models only
    understand the new desired state and don't take into account the previous state
    due to the lack of convergence. So if some configuration is removed from a playbook
    for example, it won't be automatically cleaned up when the next deployment occurs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取模型在处理长期存在的服务器基础设施时非常受欢迎。它非常适合补丁管理，以保持合规性。由于拉取模型具有一个集中式服务器来维护当前状态，如果从某个服务器中移除配置，集中式服务器会注册需要删除。推送模型只理解当前的期望状态，并且由于缺乏收敛性，不会考虑以前的状态。因此，如果从剧本中删除了一些配置，例如，在下次部署时不会自动清理这些已删除的配置。
- en: The drawbacks of a pull approach are the requirement to maintain the infrastructure
    for the centralized server which can be somewhat large, and as it is agent-based,
    agent versions also need to be maintained.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取方法的缺点是需要维护集中式服务器的基础设施，而这可能会变得相对庞大；同时，由于它是基于代理的，还需要维护代理的版本。
- en: Push models align themselves well to orchestration and updating large amounts
    of servers. They are popular when using immutable infrastructure as the old state
    of the server is not important. This means that only the current desired state
    is relevant, so it is not necessary to clean-up deleted configuration as servers
    will be deployed at every deployment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 推送模型非常适合协调和更新大量服务器。当使用不可变基础设施时，它们很受欢迎，因为服务器的旧状态并不重要。这意味着只有当前期望的状态才是相关的，因此不需要清理已删除的配置，因为服务器将在每次部署时重新部署。
- en: A pull model with immutable infrastructure wouldn't really make sense as the
    boxes would only converge once and then be destroyed, so the overhead of running
    large centralized servers to take care of convergence is wasteful.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Packaging deployment artifacts
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using configuration management tooling just to deploy applications is not enough;
    Continuous Delivery and deployment are only as quick as its slowest component.
    So having to wait for manual network or infrastructure changes is not an option;
    all components need to be built, versioned, and have their deployment automated.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: When looking at building new environments from scratch, multiple deployment
    artifacts need to be used to build an environment; application code is just one
    piece of the jigsaw.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The following dependencies are required to build a redundant environment:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Application
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure (base operating systems and virtual or physical servers)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment scripts (configuration management)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not versioning all these components together means that true rollback is not
    available as components may break if an application is rolled back and the network
    has moved forward in terms of state.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Ideally application code, infrastructure, networking, load balancing, and deployment
    scripts should all be versioned and tested together as one entity. So if rollback
    is required then operators can simply roll-back to the last known package which
    has tried and tested versions of the application code, infrastructure, networking,
    load balancing, and deployment scripts that were known to work together.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: One option is to have a single repository that versions all dependencies in
    that one repository. This can be inflexible when dealing with large numbers of
    applications and can result in repetition of configuration.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Another way to version all components is via continuous integration builds,
    each of the components can have their own continuous integration build to version
    the individual components and a unique repository.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Applications will be a packaged entity which may be an RPM file on Red Hat Linux,
    APT file on Ubuntu, or a NuGet package on Windows.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure will be provisioned using cloud provider APIs such as OpenStack,
    Microsoft Azure, Google Cloud, or AWS, so the desired number of servers will need
    to be specified using a version controlled inventory file.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The base operating system images can be created using tooling such as Packer
    or OpenStack Disk Image Builder and uploaded to a cloud provider's image registry.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: As covered in [Chapter 4](ch04.html "Chapter 4. Configuring Network Devices
    Using Ansible"), *Configuring Network Devices Using Ansible*, [Chapter 5](ch05.html
    "Chapter 5. Orchestrating Load Balancers Using Ansible"), *Configuring Load Balancers
    Using Ansible*, and [Chapter 6](ch06.html "Chapter 6. Orchestrating SDN Controllers
    Using Ansible"), *Configuring SDN Controllers Using Ansible*, network configuration,
    when utilizing Ansible, normally takes the form of `var` files which describe
    the desired state of the system.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: When using an SDN controller, the subnet ranges and ACL firewall rules can be
    described in these `var` files and utilize modules scheduled in specific orders
    to apply them at deployment time. In a similar vein, the load balancing configuration
    object model can be stored in Ansible `var` files to set up load balancing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Each of these repositories should be tagged as part of the continuous integration
    build and a supplementary package build can then be created for each application.
    This package build is used to roll-up all the dependencies and version them together
    using a manifest file.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The continuous integration builds that contribute to the manifest file are
    shown in the following diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Packaging deployment artifacts](img/B05559_09_09.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: A manifest file can take the form of a simple key value pair file or a JSON
    file. The format of the file is not important, recording the latest tagged version
    of each continuous integration build is integral to the process.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: At deployment time, a new packaged manifest should be used as the trigger for
    the deployment pipeline. The first step of the deployment pipeline will pull down
    the manifest file from the artifact repository and it can then be read for version
    information.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: All versions of the repositories present in the manifest file can then be pulled
    down to the Ansible control server and used to deploy the desired application
    version along with the desired state to the infrastructure, network, and load
    balancer required for each environment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Roll-back would involve passing the previous version of the manifest file to
    the deployment process which would then revert to the last tried and tested versions
    of the application code, infrastructure, networking, load balancing, and deployment
    scripts that were known to work together.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Deployment pipeline tooling
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deployment pipelines involve chaining different tools together to create Continuous
    Delivery processes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Being able to track the process flow through the Continuous Delivery tooling
    is integral, as it is important to be able to visualize the pipeline process,
    so it is easy for operators to follow.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Having visibility of a process makes debugging the process easy if errors occur,
    which may happen as errors will occur in any process and are inevitable. The whole
    point of the Continuous Delivery pipeline, aside from automating delivery of changes
    to environments, is to provide feedback loops. So if a pipeline is not easy to
    follow and debug, it has failed one of its main objectives.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Building automatic clean-up into pipelines should be implemented if possible,
    so if a failure occurs mid-deployment then changes can be reverted back to the
    last known good state without the need for manual intervention.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the following tooling is required when creating a deployment
    pipeline for Continuous Delivery which includes a **SCM System**, **CI Build Server**,
    **Artifact Repository**, and **CD Pipeline Scheduler**:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Deployment pipeline tooling](img/B05559_09_10.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: In [Chapter 7](ch07.html "Chapter 7. Using Continuous Integration Builds for
    Network Configuration"), *Using Continuous Integration Builds for Network Configuration*,
    and [Chapter 8](ch08.html "Chapter 8. Testing Network Changes"), *Unit Testing
    Network Changes*, we covered the importance of the SCM System and CI Build Server
    in continuous integration and testing. In this chapter we will focus on the tooling
    required for the deployment process which includes the Artifact Repository and
    CD Pipeline Scheduler that is used to schedule configuration management tooling.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Artifact repositories
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artifact repositories are a key component in any deployment pipeline; they can
    be used to host a multitude of different repositories or even just hold generic
    artifacts.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Platform golden images in ISO, AMI, VMDK, and QCOW format can be stored and
    versioned in artifact repositories and used as the source for image registries
    for cloud providers such as AWS, Google Cloud, Microsoft Azure, and OpenStack.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Manifest files can also be held in a release repository to govern the roll-forward
    and roll-back of application, infrastructure, networking, and load balancing requirements.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Artifactory
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artifactory from JFrog is one of the most popular artifact repositories on the
    market today and provides access to repositories via an NFS-based shared storage
    solution. Artifactory is bundled with the Apache Tomcat web server as part of
    the installer bundle and can be hosted on Linux or Windows.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In terms of load balancing, Artifactory can be set up in a highly available,
    three tier cluster for redundancy. Artifactory can use a wide variety of load
    balancers such as Nginx or HAProxy as well as proprietary load balancers such
    as Citrix NetScaler, F5 Big-IP, or Avi Networks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Artifactory is backed by a MySQL or Postgres database and requires an NFS file-system
    or Amazon S3 storage to store artifacts that are made available to each of Artifactories
    three HA nodes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectural overview of **Artifactory** is shown in the following diagram:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Artifactory](img/B05559_09_11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'Artifactory supports numerous different repository types, some of which are
    shown here, so it can host multiple different repositories for delivery teams
    depending on the applications that they are developing:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Maven [https://www.jfrog.com/confluence/display/RTF/Maven+Repository](https://www.jfrog.com/confluence/display/RTF/Maven+Repository)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivy [https://www.jfrog.com/confluence/display/RTF/Working+with+Ivy](https://www.jfrog.com/confluence/display/RTF/Working+with+Ivy)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradle [https://www.jfrog.com/confluence/display/RTF/Gradle+Artifactory+Plugin](https://www.jfrog.com/confluence/display/RTF/Gradle+Artifactory+Plugin)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git LFS [https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories](https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NPM [https://www.jfrog.com/confluence/display/RTF/Npm+Registry](https://www.jfrog.com/confluence/display/RTF/Npm+Registry)
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NuGet [https://www.jfrog.com/confluence/display/RTF/NuGet+Repositories](https://www.jfrog.com/confluence/display/RTF/NuGet+Repositories)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyPi [https://www.jfrog.com/confluence/display/RTF/PyPI+Repositories](https://www.jfrog.com/confluence/display/RTF/PyPI+Repositories)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bower [https://www.jfrog.com/confluence/display/RTF/Bower+Repositories](https://www.jfrog.com/confluence/display/RTF/Bower+Repositories)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YUM [https://www.jfrog.com/confluence/display/RTF/YUM+Repositories](https://www.jfrog.com/confluence/display/RTF/YUM+Repositories)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vagrant [https://www.jfrog.com/confluence/display/RTF/Vagrant+Repositories](https://www.jfrog.com/confluence/display/RTF/Vagrant+Repositories)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker [https://www.jfrog.com/confluence/display/RTF/Docker+Registry](https://www.jfrog.com/confluence/display/RTF/Docker+Registry)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debian [https://www.jfrog.com/confluence/display/RTF/Debian+Repositories](https://www.jfrog.com/confluence/display/RTF/Debian+Repositories)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBT [https://www.jfrog.com/confluence/display/RTF/SBT+Repositories](https://www.jfrog.com/confluence/display/RTF/SBT+Repositories)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generic [https://www.jfrog.com/confluence/display/RTF/Configuring+Repositories](https://www.jfrog.com/confluence/display/RTF/Configuring+Repositories)
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means Artifactory can be used as the single repository end-point for Continuous
    Delivery pipelines. Artifactory has recently introduced support for Vagrant boxes
    and Docker registry so it can be used to store Vagrant test environments, which
    could be used to store network operating systems or containers. This illustrates
    some of the features available from market-leading artifact repositories.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: CD pipeline scheduler
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the job of the artifact repository is relatively straightforward, but
    no less important, choosing the correct Continuous Delivery pipeline tool is much
    more difficult.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a wide array of options available such as:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: IBM Urban Code Deploy [https://developer.ibm.com/urbancode/products/urbancode-deploy/](https://developer.ibm.com/urbancode/products/urbancode-deploy/)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Electric Flow Deploy [http://electric-cloud.com/products/electricflow/deploy-automation/](http://electric-cloud.com/products/electricflow/deploy-automation/)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins [https://jenkins.io/](https://jenkins.io/)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoughtworks Go [https://www.go.cd/](https://www.go.cd/)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XL Deploy [https://xebialabs.com/products/xl-deploy](https://xebialabs.com/products/xl-deploy)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But before picking a tool, the process being implemented needs to be considered.
    So what are the main aims of a Continuous Delivery pipeline?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'A good Continuous Delivery pipeline should meet the following goals:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Trigger deployments based on new artifacts being available in artifact repository
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule command lines
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Render a pipeline view
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Break tasks into stages
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide good log output
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedback pass or failures
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with testing
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these points need to be considered when selecting tooling so we will
    cherry-pick one of the most popular Continuous Delivery pipeline scheduling tools
    Jenkins, and look at ways in which it schedules pipelines.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jenkins was primarily a continuous integration build server when it was conceived.
    Jenkins has a pluggable framework that means that it is often customized to carry
    out deployments, with plugins such as the multi-job plugin built to allow it to
    schedule pipelines.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: However, as of the Jenkins 2.x release, Jenkins now makes pipelines a core feature
    component of its distribution rather than depending on plugins to cater for its
    deployment capabilities.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the Jenkins Pipeline job type can be seen:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_12.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Using the Jenkins Pipeline job type, users can specify pipelines using a `Pipeline
    Script`, declaring each stage of the pipeline. In this instance, the echo command
    has been used to spoof each pipeline stage to show how a `Pipeline script` may
    look:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_13.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'The visual display of the pipeline from the pipeline script is shown in the
    **Stage View**. The **Stage View** shows the eight stages the pipeline went through
    in order to deploy the networking, virtual machines, application, and load balancer
    configuration:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_14.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Logging for each stage is shown clearly in the Jenkins console logs which allow
    users of the tool to see feedback on successful and unsuccessful console logs:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_15.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: When putting valid commands into `Pipeline script`, as opposed to simulating
    echoes as we have in the above example, Jenkins allows users to use a groovy snippet
    generator to translate any steps to Pipeline Script format.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'In this instance, a shell command is required to execute the Ansible playbook
    to `create_vip.yml` on the component test environment, so the snipped generator
    is used to create it:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_16.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'This snippet command can then be pasted into the `create_vip.yml` stage that
    was created on the Pipeline script:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_17.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: The output of the job configuration is a Jenkins file that can be stored in
    SCM to version control the deployment pipeline changes.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Deploying network changes with deployment pipelines
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When carrying out Continuous Delivery or deployment, it is essential to incorporate
    network changes. Network teams need to contribute major pieces of the deployment
    pipeline.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: As the CD Pipeline scheduler allows different stages to be specified in the
    deployment pipeline, it gives great flexibility and allows all teams to contribute
    pieces, forming a true collaborative DevOps model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes a concern from network teams is that developers should not have the
    necessary access to all network devices as they are not experts. Truth be told,
    developers don't want access to network devices, they instead want a quick way
    of pushing out their changes where they are not impeded by having to wait on network
    changes being applied.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Network self-service
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Allowing developers the ability to self-service their own network changes is
    very important, otherwise the network team becomes the bottleneck for the Continuous
    Delivery process.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: So providing development teams with, say, a hardened Ansible playbook to create
    everyday network functions will undoubtedly help alleviate developer pain and
    make deployment of new network changes a self-service function.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Developers can use a playbook that incorporates all the best practices of the
    network team to apply any network changes. This is following the model where developers
    can utilize a playbook provided by the infrastructure team to spin up new virtual
    machines and register their DNS entries with the IPAM solution.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Steps in a deployment pipeline
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating deployment pipelines, it is important to break up each function
    into a granular set of steps. This means if any step fails it can be easily rolled
    back. Understanding the deployment pipeline visually is also important as breaking
    down complex operations into small steps makes debugging failures less daunting
    too.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'A modern application deployment pipeline will provision new environments by
    carrying out the following high level steps every single deployment:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Download manifest
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create network
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create VMs in network
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install application
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create VIP
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rolling update
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run test pack
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Promote to next phase
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first stage of the pipeline is the trigger for a new deployment to the first
    test environment. In this case, the detection of a new manifest file artifact.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The manifest artifact will be downloaded to the CD Pipeline scheduler and parsed.
    The Ansible `var` file structure will be assembled from SCM using the manifest
    versions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Once assembled, the network needs to be provisioned. An A or B network will
    be created depending on the release and the necessary Ingress and Egress ACL rules
    will be applied to the network.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines will then be booted into the newly-provisioned network and
    tagged with their metadata profile stating the software that needs to be installed
    on them.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Ansible dynamic inventory is run to pull back the new virtual machines that
    were just created, Ansible reads the profile metadata from the virtual machines.
    metadata tags and Ansible installs the required role on the new cluster of virtual
    machines depending on what profile is specified.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: A VIP is created on the load balancer if it doesn't already exist and its load
    balancing policies are applied. Boxes are then rolled into service on the new
    VIP and old boxes are rolled out of service. The new boxes are smoke tested to
    make sure they are operating as expected before the previous release is destroyed.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: A full quality assurance test pack is then executed and the manifest artifact
    is then promoted to the next stage if successful.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Each of these steps will be repeated all the way up to production. In a Continuous
    Delivery model, the production deployment will be a manual button press to trigger
    the pipeline, where in Continuous Delivery pipeline will automatically trigger
    if all quality gates pass.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating configuration management tooling
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When utilizing a CD scheduler such as Jenkins, its agents, known as slaves,
    can be used to install Ansible on them and they become the Ansible Control Host
    for the deployment.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Each stage in the deployment pipeline can be a small modular Ansible playbook
    that allows developers to self-serve their network needs. These playbooks can
    be created by the network team and continuously improved over time.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'So the Jenkins `Pipeline script` would resemble the following, with a unique
    playbook for each stage:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Incorporating configuration management tooling](img/B05559_09_18.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: The steps applied on each test environment should be consistent with production
    and all steps should be carried out by a service account for the pipeline.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Each and every environment should be built from source control by implementing
    immutable infrastructure and networking. This is so that the desired state is
    always what is specified in the manifest file's associated repositories.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The Ansible `var` files that feed each playbook can be filled in by the development
    teams in order to set firewall policies or load balancing policies.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: These `var` files are versioned by the associated continuous integration builds
    for the SDN or load balancing configuration. Each network-related CI build then
    rolls up into a new manifest file when an application continuous integration build
    is triggered. The generation of a new manifest file triggers the first step in
    the deployment pipeline.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Network teams' role in Continuous Delivery pipelines
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When analyzing the steps that are executed by a deployment pipeline, if we look
    at which teams would have the necessary permissions to carry out each pipeline
    stage manually, it becomes very apparent the importance of integrating networking
    into the Continuous Delivery processes.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of the eight high level stages to deploy an application, three of them
    are integrating with the network when executing **create network**, **create vip**,
    and **rolling update** as shown here:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![Network teams'' role in Continuous Delivery pipelines](img/B05559_09_19.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: This shows that if network operations were not part of the deployment pipeline
    then true Continuous Delivery would not be achievable.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Failing fast and feedback loops
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key objectives of creating Continuous Delivery pipelines is creating
    feedback loops which fail fast and create a radiator view for developers. However,
    with Continuous Delivery moving into continuous operations space, as it now incorporates
    infrastructure, networking, and quality assurance, all teams need to be mindful
    of failures and react accordingly.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: When pipeline stages fail, it is important to incorporate automated clean-up
    every time there is a failure, this leaves the pipeline in a good state so the
    next pipeline is not impeded. Any break in the process means that changes cannot
    reach production.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'So although it may be a test environment that is breaking, it is now blocking
    potential fixes being deployed to production. If a failure occurs, the pipeline
    should also halt the whole process and not proceed to the next stage as shown
    below:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Failing fast and feedback loops](img/B05559_09_20.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Ansible block rescue functionality is very useful when dealing with failed pipeline
    stages and clean-up, providing a try and catch-like feature for playbooks and
    roles.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Testing should also be incorporated into the deployment pipeline so if the run
    test stage of the pipeline fails, then there is a history of why the tests failed
    that can be audited. Pipelines also help provide a full history of changes that
    have been applied to the environment. Although triggered by a service account,
    the user that committed the change in source control should take ownership for
    each change.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at integrating network changes into deployment pipelines
    so that network teams can contribute to the Continuous Delivery process. We then
    discussed the difference between Continuous Delivery and deployment.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how package management is crucial for wrapping development,
    infrastructure, quality assurance, and network changes together as part of deployment
    pipelines. We also illustrated some of the market-leading artifact repositories
    and CD pipeline schedulers using Artifactory and Jenkins as examples.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at best practices that should be adopted when setting up
    deployment pipelines within the remits of Continuous Delivery and deployment.
    We then focused on ways network teams could contribute to deployment pipelines
    by providing self-service deployment scripts to developers, so they keep the overall
    process quick, lean, and automated.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should now understand why that applications
    should be compiled only once and stored in an artifact repository, and the same
    binaries should be deployed to multiple environments so the deployment process
    is consistent.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also focused on the differences between pull-based tools, such as
    Chef and Puppet, and tools such as Ansible and Salt that utilize a push model
    for configuration management.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Key takeaways should also include how to utilize Artifactory as an artifact
    repository to store numerous types of build artifacts, and ways in which manifest
    files can be generated using continuous integration to version code, infrastructure,
    networking, and load balancing.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Readers should learn all the necessary steps in a Continuous Delivery pipeline,
    how to set up a deployment pipeline using Jenkins 2.x, and the importance of integrating
    networking in the Continuous Delivery model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on containers and look at the impact they
    have had on networking and network operations. We will look at some of the different
    orchestration options that can be used such as Docker and Kubernetes.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
