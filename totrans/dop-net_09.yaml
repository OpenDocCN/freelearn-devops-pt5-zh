- en: Chapter 9. Using Continuous Delivery Pipelines to Deploy Network Changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on some of the different methods that can be used to
    deploy network changes using deployment pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: It will first look at Continuous Delivery and continuous deployment processes
    and what these methodologies entail in terms of workflow.
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at the different deployment tools, artifacts repositories,
    and packaging methods that can be used to set up deployment pipelines and ways
    in which network changes can be integrated into those pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration package management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous Delivery and deployment overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment methodologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging deployment artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment pipeline tooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying network changes with deployment pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration package management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.html "Chapter 7. Using Continuous Integration Builds for
    Network Configuration"), *Using Continuous Integration Builds For Network Configuration*,
    we looked at the process of continuous integration and in [Chapter 8](ch08.html
    "Chapter 8. Testing Network Changes"), *Testing Network Changes*, we looked at
    adding testing to the continuous integration process to provide increased validation
    and feedback loops in case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: When carrying out continuous integration, using a fail fast / fix fast philosophy
    is desirable. This involves putting in necessary validation checks to decipher
    whether a build is valid and provide feedback loops to users.
  prefs: []
  type: TYPE_NORMAL
- en: This promotes the correct behavior within the teams that do frequent, small,
    incremental changes, which de-risks the changes. While each change is validated
    using the **Continuous Integration** (**CI**) engine with instant feedback on
    changes, a process of continuous improvement is adhered to as teams strive to
    make more robust solutions that will pass all quality checks.
  prefs: []
  type: TYPE_NORMAL
- en: As important as providing feedback loops is, producing successful builds is
    equally important to the process as this is how products are shipped to market.
    When a continuous integration build completes, it often needs to package build
    artifacts that are in a fit state so they can be deployed to target servers. This
    is often referred to as creating a shippable product or artifact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any continuous integration process should carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Commit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build (Compile/Version/Tag)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time a new commit takes place, a new continuous integration build will
    be triggered. This will result in a code being pulled down from the SCM system,
    which will trigger a build step, which can either be a compilation process, or
    if the build process is not using a compiled language, then versioning or tagging
    of the binaries. Finally, a set of validation steps will be carried out inclusive
    of any required testing.
  prefs: []
  type: TYPE_NORMAL
- en: If all validations prove successful, then a set of post-continuous integration
    process steps need to be carried out. Post-build steps will include the package
    and push process, this means packaging build binaries and pushing the newly versioned
    package to an Artifact Repository of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example process that includes **Commit Change**, Build (**Compile Code**),
    Validate (**Unit Tests**), **Package** (Artifact), and Push to an **Artifact Repository**
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An important principle to remember when setting up continuous integration builds
    and packaging continuous integration artifacts, is that artifacts should be packaged
    once only, not every single time they need to be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: This is important from a repeatability perspective and also reduces the time
    taken to deploy as a build process can be lengthy and take many minutes. When
    a build has been packaged, all tests and necessary validation have been carried
    out on the artifact as part of the continuous integration process, so there is
    no need to repeat this process again if no changes have occurred.
  prefs: []
  type: TYPE_NORMAL
- en: It is imperative that we ensure the exact same artifact is deployed to test
    environments before being promoted onto production; this means there will be no
    drift between environments. The same source code being packaged on a different
    build server may result in the version of Java being slightly different, or even
    something as simple as a different environment variable could mean the build binaries
    are compiled differently.
  prefs: []
  type: TYPE_NORMAL
- en: Maintain consistent deployment artifacts, always swearing by the principle of
    package once and deploy multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard of package once, deploy multiple times is illustrated following,
    where a single artifact is used to seed test and production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Creating different build artifacts for each environment is a non-starter; release
    management best practices dictate that a build package and artifacts should include
    tokens so different snapshots of the same package are not required.
  prefs: []
  type: TYPE_NORMAL
- en: Build package tokens can then be transformed at deployment time. All environment
    specific information is held in a configuration file of some sort, normally called
    an **environment** file, which is used to populate the tokens at deployment time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following best practices should be adhered to when packaging continuous
    integration build artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: Artifacts should be packaged once and distributed many times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact packages should be packaged with tokenized configuration files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact package configuration files should be transformed at deployment time
    using an environment file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common files can be used to supplement environment files if deployment configuration
    is common to all environments to avoid repetition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following popular configuration management tools have the ability to transform
    tokenized templates by utilizing configuration files. Each of these configuration
    files take on the role of the environment file:'
  prefs: []
  type: TYPE_NORMAL
- en: Puppet [https://puppet.com/](https://puppet.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chef [https://www.chef.io/chef/](https://www.chef.io/chef/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible [https://www.ansible.com/](https://www.ansible.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salt [https://saltstack.com/](https://saltstack.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking Ansible as an example, in [Chapter 4](ch04.html "Chapter 4. Configuring
    Network Devices Using Ansible"), *Configuring Network Devices Using Ansible*,
    we covered the concept of jinja2 templates. Jinja2 templates allow template files
    to be populated with tokens and these tokens are substituted with particular key
    value pairs at deployment time.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible allows users to populate jinja2 templates to be populated with variables
    (tokens). Each `var` file can be configured so that it is unique to each environment.
    Environment files can be imported into playbooks and roles by inputting it as
    a command line argument. This will in turn transform the jinja2 templates at deployment
    time with the environment-specific information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we see an Ansible playbook `configure_env.yml` being
    executed, and a unique environment variable called environment needing to be set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This will be imported into the playbook `configure_env.yml` so that a unique
    set of environment information is loaded for each environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, taking the component, integration, system test, and production environments
    as an example the following files would be loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/comp.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/int.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/sys.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`../roles/networking/vars/prod.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each unique environment, the deployment command differs only in the environment
    file that is loaded which will make the deployment environment specific:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous integration package management](img/B05559_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Continuous Delivery and deployment overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous Delivery and deployment are a natural extension of the continuous
    integration process. Continuous Delivery and deployment create a consistent mechanism
    to deploy changes to production and create a conveyer belt delivering new features
    to customers or end users. So conceptually a conveyer belt is what continuous
    Delivery is all about, but in terms of actual process how is this achieved?
  prefs: []
  type: TYPE_NORMAL
- en: 'A continuous integration process will carry out the following high level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Commit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build (Compile/Version/Tag)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous Delivery and deployment take over once the artifact has been pushed
    to the artifact repository. Each and every build artifact created by a continuous
    integration process should be considered a release candidate, meaning that it
    can potentially be deployed to production if it passes all validations in the
    Continuous Delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Like continuous integration, Continuous Delivery and deployment create a series
    of feedback loops to indicate if validation tests have failed on an environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Continuous Delivery pipeline process will encapsulate the following high
    level steps at each stage of a deployment pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy (pull/tokenize/setup)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate (test)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promote
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A stage in a deployment pipeline will contain a series of tests which will be
    used to help validate whether the application is functioning as required prior
    to it being released to production.
  prefs: []
  type: TYPE_NORMAL
- en: Each stage in the deployment pipeline will have a deployment step which will
    pull down the artifact from the **Artifact Repository** to the target server and
    execute the deployment steps. The deployment process will normally involve installing
    software or configuring a change to the state of the server. Configuration changes
    are typically governed by a configuration management tool such as Puppet, Chef,
    Ansible, or Salt.
  prefs: []
  type: TYPE_NORMAL
- en: Once deployment is completed, a series of tests will be carried out in the environment
    to validate the deployment and also test the functionality of the application
    or change.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery means that if validation tests pass on a test environment
    then the build artifact is automatically promoted to the next environment. The
    deployment, validation, and promotion steps are carried out again on the next
    environment in the same way as the previous environment. In the event of a failure,
    the release candidate will break and it will not be promoted to the next stage
    of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Continuous Delivery this automatic promotion happens all the way
    to the environment prior to production as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous Delivery and deployment overview](img/B05559_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Continuous deployment on the other hand has no paused state before production
    and differs from Continuous Delivery in that it will automatically deploy to production:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous Delivery and deployment overview](img/B05559_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So the only difference between Continuous Delivery and continuous deployment
    is the manual pause from promoting the build artifact to production.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for implementing Continuous Delivery over continuous deployment is
    normally down to either governance or the maturity of testing.
  prefs: []
  type: TYPE_NORMAL
- en: When starting out, continuously deploying to production throughout the day can
    seem very daunting as it mandates that the deployment process is completely automated
    and that the validation and testing on each environment is mature enough to catch
    all known errors.
  prefs: []
  type: TYPE_NORMAL
- en: With continuous deployment, the trigger of a production deployment is a SCM
    commit, so it puts a lot of trust in the deployment system. This means it is desirable
    that the branching strategy is set up to pull all changes from the **trunk/mainline/master**
    branch and trigger the deployment pipeline. Having multiple different branches
    will complicate the deployment process so it is important to implement a branching
    strategy that minimizes repetition, and an explosion of the number of deployment
    pipelines that are required.
  prefs: []
  type: TYPE_NORMAL
- en: If implemented badly, continuous deployment can result in continuous downtime,
    so normally after setting up continuous integration businesses, teams should start
    with Continuous Delivery and aim to eventually move to a continuous deployment
    once processes have matured sufficiently.
  prefs: []
  type: TYPE_NORMAL
- en: As covered in [Chapter 3](ch03.html "Chapter 3. Bringing DevOps to Network Operations"),
    *Bringing DevOps to Network Operations*, cultural change is needed within the
    business to implement a Continuous Delivery model and it really is an all or nothing
    approach for it to work successfully.
  prefs: []
  type: TYPE_NORMAL
- en: As stated in [Chapter 8](ch08.html "Chapter 8. Testing Network Changes"), *Testing
    Network Changes*, manually updating environments can compromise the validity of
    tests so they should be avoided at all costs, every change should flow through
    SCM to downstream environments.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery promotes automation and creation of test packs at every
    stage in the deployment pipeline, but it also allows a business to cherry-pick
    the release candidate that is finally deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: This means additional validation could be carried out manually during the imposed
    stop before production, in the absence of the desired level or test coverage for
    a build artifact. It also plays well with companies that are subject to regulatory
    requirements that may mean they only have a specified deployment window and they
    cannot deploy to production continuously.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery means that regulated companies can still benefit from automated
    environments and tests, but the production deployment is just a button click to
    select the artifact, which has passed all aforementioned promotions and is deployed
    to production.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment methodologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When carrying out Continuous Delivery and deployment, there is no one size fits
    all deployment strategy. Configuration management tools such as Puppet, Chef,
    Ansible, and Salt have different approaches to deployment and use different approaches
    when keeping servers up to date.
  prefs: []
  type: TYPE_NORMAL
- en: The tool that is selected is not important, only the ideal workflow and processes
    to support delivering changes that are consistent, quick, and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Pull model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tools such as Puppet and Chef adopt a centralized approach to configuration
    management, where they have a centralized server that acts as the brain for the
    deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: In Puppet's case the centralized server are the Puppet Master and in Chef's
    case the centralized server is the Chef Server. This centralized server is a set
    of infrastructure provisioned to store server configuration according to the configuration
    management tool's reference architecture.
  prefs: []
  type: TYPE_NORMAL
- en: All updates to server configuration is pushed to the centralized server first
    and then subsequently pushed out to the corresponding servers using agents. These
    agents can either poll the centralized server for updates and apply them straight
    away or alternately wait for the Puppet Agent, or in Chef's case, the Chef Client
    to be invoked to start the convergence of configuration from the centralized server
    to the server containing the agent.
  prefs: []
  type: TYPE_NORMAL
- en: The overriding principle in a pull model is that the centralized server governs
    the state of the system and every change goes via the Puppet Master or Chef Server.
  prefs: []
  type: TYPE_NORMAL
- en: If any user logs onto a server and changes the state, then the next time that
    the state converges from the centralized server it could overwrite those manual
    changes when the agent runs (Puppet Agent or Chef Client) if that particular configuration
    is managed by the centralized server.
  prefs: []
  type: TYPE_NORMAL
- en: In this model, the centralized server will control all application versioning
    information and environment configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a pull model is shown in the following diagram. This shows Chef
    being used in a Continuous Delivery process:'
  prefs: []
  type: TYPE_NORMAL
- en: The continuous integration process creates a new build artifact which is pushed
    to the **Artifact Repository**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chef's command line client **knife** is invoked as a post-build action which
    updates the **Chef Server** with the new version of the application which is being
    deployed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment process is then triggered on **Component Test Environment** by
    running **Chef Client** which will trigger the **Chef Client** to check the state
    against the **Chef Server**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Chef Client** in this case, sees a new application version is available
    based on the last **knife** update and as a result updates the environment to
    the new version of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, all validation and test steps are run prior to promoting it to the
    next stage of the deployment pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence on the subsequent **Integration Test Environment** is only triggered
    if the **Component Test Environment** promotion is successful![Pull model](img/B05559_09_07.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tools such as Ansible and Salt adopt a push model to configuration management,
    where they have a control host that is used to connect to servers using SSH and
    configure them.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a centralized server, Ansible and Salt use a control host,
    which has a command-line client installed on the server. The control host is then
    used to push changes to servers via logging on to them using SSH either via password
    or alternatively, SSH keys.
  prefs: []
  type: TYPE_NORMAL
- en: As Ansible and Salt are Python-based, they are agentless and can run on any
    Linux distribution, as Python is a pre-requisite for these servers. Windows machines
    are connected to and configured using WinRM.
  prefs: []
  type: TYPE_NORMAL
- en: All configuration management information is stored in SCM systems and pulled
    down to the control host, this configuration is then used to push updates out
    to servers.
  prefs: []
  type: TYPE_NORMAL
- en: The overriding principle in a push model is that changes are committed into
    SCM. The SCM server, rather than a centralized server, is the source of truth
    for state, configuration, and versioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a push model follows. This shows Ansible being used in a Continuous
    Delivery process:'
  prefs: []
  type: TYPE_NORMAL
- en: The continuous integration process creates a new build artifact which is pushed
    to the **Artifact Repository**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new artifact being present in the **Artifact Repository** triggers the deployment
    process and the Ansible playbook/role is downloaded from the **SCM System** to
    the **Ansible Control Host**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment process is then triggered on **Component Test Environment** and
    Ansible is executed against all servers that are present in the targeted inventory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, all validation and test steps are run prior to promoting it to the
    next stage of the deployment pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible is only executed on the subsequent Integration Test Environment if the
    Component Test Environment promotion is successful![Push model](img/B05559_09_08.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to choose pull or push
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When selecting a pull or push method of configuration management, it is down
    to preference and should be selected based on the approach to infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Pull models are popular when dealing with server estates that have long-lived
    infrastructure. It lends itself well to patching a whole estate of servers to
    keep on top of compliance. Pull models, as they have a centralized server with
    the current state, means that if configuration is removed from a server, then
    the centralized server will register that a delete is required. Push models only
    understand the new desired state and don't take into account the previous state
    due to the lack of convergence. So if some configuration is removed from a playbook
    for example, it won't be automatically cleaned up when the next deployment occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The drawbacks of a pull approach are the requirement to maintain the infrastructure
    for the centralized server which can be somewhat large, and as it is agent-based,
    agent versions also need to be maintained.
  prefs: []
  type: TYPE_NORMAL
- en: Push models align themselves well to orchestration and updating large amounts
    of servers. They are popular when using immutable infrastructure as the old state
    of the server is not important. This means that only the current desired state
    is relevant, so it is not necessary to clean-up deleted configuration as servers
    will be deployed at every deployment.
  prefs: []
  type: TYPE_NORMAL
- en: A pull model with immutable infrastructure wouldn't really make sense as the
    boxes would only converge once and then be destroyed, so the overhead of running
    large centralized servers to take care of convergence is wasteful.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging deployment artifacts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using configuration management tooling just to deploy applications is not enough;
    Continuous Delivery and deployment are only as quick as its slowest component.
    So having to wait for manual network or infrastructure changes is not an option;
    all components need to be built, versioned, and have their deployment automated.
  prefs: []
  type: TYPE_NORMAL
- en: When looking at building new environments from scratch, multiple deployment
    artifacts need to be used to build an environment; application code is just one
    piece of the jigsaw.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following dependencies are required to build a redundant environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure (base operating systems and virtual or physical servers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment scripts (configuration management)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not versioning all these components together means that true rollback is not
    available as components may break if an application is rolled back and the network
    has moved forward in terms of state.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally application code, infrastructure, networking, load balancing, and deployment
    scripts should all be versioned and tested together as one entity. So if rollback
    is required then operators can simply roll-back to the last known package which
    has tried and tested versions of the application code, infrastructure, networking,
    load balancing, and deployment scripts that were known to work together.
  prefs: []
  type: TYPE_NORMAL
- en: One option is to have a single repository that versions all dependencies in
    that one repository. This can be inflexible when dealing with large numbers of
    applications and can result in repetition of configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to version all components is via continuous integration builds,
    each of the components can have their own continuous integration build to version
    the individual components and a unique repository.
  prefs: []
  type: TYPE_NORMAL
- en: Applications will be a packaged entity which may be an RPM file on Red Hat Linux,
    APT file on Ubuntu, or a NuGet package on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure will be provisioned using cloud provider APIs such as OpenStack,
    Microsoft Azure, Google Cloud, or AWS, so the desired number of servers will need
    to be specified using a version controlled inventory file.
  prefs: []
  type: TYPE_NORMAL
- en: The base operating system images can be created using tooling such as Packer
    or OpenStack Disk Image Builder and uploaded to a cloud provider's image registry.
  prefs: []
  type: TYPE_NORMAL
- en: As covered in [Chapter 4](ch04.html "Chapter 4. Configuring Network Devices
    Using Ansible"), *Configuring Network Devices Using Ansible*, [Chapter 5](ch05.html
    "Chapter 5. Orchestrating Load Balancers Using Ansible"), *Configuring Load Balancers
    Using Ansible*, and [Chapter 6](ch06.html "Chapter 6. Orchestrating SDN Controllers
    Using Ansible"), *Configuring SDN Controllers Using Ansible*, network configuration,
    when utilizing Ansible, normally takes the form of `var` files which describe
    the desired state of the system.
  prefs: []
  type: TYPE_NORMAL
- en: When using an SDN controller, the subnet ranges and ACL firewall rules can be
    described in these `var` files and utilize modules scheduled in specific orders
    to apply them at deployment time. In a similar vein, the load balancing configuration
    object model can be stored in Ansible `var` files to set up load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these repositories should be tagged as part of the continuous integration
    build and a supplementary package build can then be created for each application.
    This package build is used to roll-up all the dependencies and version them together
    using a manifest file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The continuous integration builds that contribute to the manifest file are
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Packaging deployment artifacts](img/B05559_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A manifest file can take the form of a simple key value pair file or a JSON
    file. The format of the file is not important, recording the latest tagged version
    of each continuous integration build is integral to the process.
  prefs: []
  type: TYPE_NORMAL
- en: At deployment time, a new packaged manifest should be used as the trigger for
    the deployment pipeline. The first step of the deployment pipeline will pull down
    the manifest file from the artifact repository and it can then be read for version
    information.
  prefs: []
  type: TYPE_NORMAL
- en: All versions of the repositories present in the manifest file can then be pulled
    down to the Ansible control server and used to deploy the desired application
    version along with the desired state to the infrastructure, network, and load
    balancer required for each environment.
  prefs: []
  type: TYPE_NORMAL
- en: Roll-back would involve passing the previous version of the manifest file to
    the deployment process which would then revert to the last tried and tested versions
    of the application code, infrastructure, networking, load balancing, and deployment
    scripts that were known to work together.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment pipeline tooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deployment pipelines involve chaining different tools together to create Continuous
    Delivery processes.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to track the process flow through the Continuous Delivery tooling
    is integral, as it is important to be able to visualize the pipeline process,
    so it is easy for operators to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Having visibility of a process makes debugging the process easy if errors occur,
    which may happen as errors will occur in any process and are inevitable. The whole
    point of the Continuous Delivery pipeline, aside from automating delivery of changes
    to environments, is to provide feedback loops. So if a pipeline is not easy to
    follow and debug, it has failed one of its main objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Building automatic clean-up into pipelines should be implemented if possible,
    so if a failure occurs mid-deployment then changes can be reverted back to the
    last known good state without the need for manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the following tooling is required when creating a deployment
    pipeline for Continuous Delivery which includes a **SCM System**, **CI Build Server**,
    **Artifact Repository**, and **CD Pipeline Scheduler**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deployment pipeline tooling](img/B05559_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In [Chapter 7](ch07.html "Chapter 7. Using Continuous Integration Builds for
    Network Configuration"), *Using Continuous Integration Builds for Network Configuration*,
    and [Chapter 8](ch08.html "Chapter 8. Testing Network Changes"), *Unit Testing
    Network Changes*, we covered the importance of the SCM System and CI Build Server
    in continuous integration and testing. In this chapter we will focus on the tooling
    required for the deployment process which includes the Artifact Repository and
    CD Pipeline Scheduler that is used to schedule configuration management tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact repositories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artifact repositories are a key component in any deployment pipeline; they can
    be used to host a multitude of different repositories or even just hold generic
    artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Platform golden images in ISO, AMI, VMDK, and QCOW format can be stored and
    versioned in artifact repositories and used as the source for image registries
    for cloud providers such as AWS, Google Cloud, Microsoft Azure, and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: Manifest files can also be held in a release repository to govern the roll-forward
    and roll-back of application, infrastructure, networking, and load balancing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Artifactory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artifactory from JFrog is one of the most popular artifact repositories on the
    market today and provides access to repositories via an NFS-based shared storage
    solution. Artifactory is bundled with the Apache Tomcat web server as part of
    the installer bundle and can be hosted on Linux or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of load balancing, Artifactory can be set up in a highly available,
    three tier cluster for redundancy. Artifactory can use a wide variety of load
    balancers such as Nginx or HAProxy as well as proprietary load balancers such
    as Citrix NetScaler, F5 Big-IP, or Avi Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Artifactory is backed by a MySQL or Postgres database and requires an NFS file-system
    or Amazon S3 storage to store artifacts that are made available to each of Artifactories
    three HA nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectural overview of **Artifactory** is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Artifactory](img/B05559_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Artifactory supports numerous different repository types, some of which are
    shown here, so it can host multiple different repositories for delivery teams
    depending on the applications that they are developing:'
  prefs: []
  type: TYPE_NORMAL
- en: Maven [https://www.jfrog.com/confluence/display/RTF/Maven+Repository](https://www.jfrog.com/confluence/display/RTF/Maven+Repository)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivy [https://www.jfrog.com/confluence/display/RTF/Working+with+Ivy](https://www.jfrog.com/confluence/display/RTF/Working+with+Ivy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradle [https://www.jfrog.com/confluence/display/RTF/Gradle+Artifactory+Plugin](https://www.jfrog.com/confluence/display/RTF/Gradle+Artifactory+Plugin)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git LFS [https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories](https://www.jfrog.com/confluence/display/RTF/Git+LFS+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NPM [https://www.jfrog.com/confluence/display/RTF/Npm+Registry](https://www.jfrog.com/confluence/display/RTF/Npm+Registry)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NuGet [https://www.jfrog.com/confluence/display/RTF/NuGet+Repositories](https://www.jfrog.com/confluence/display/RTF/NuGet+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyPi [https://www.jfrog.com/confluence/display/RTF/PyPI+Repositories](https://www.jfrog.com/confluence/display/RTF/PyPI+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bower [https://www.jfrog.com/confluence/display/RTF/Bower+Repositories](https://www.jfrog.com/confluence/display/RTF/Bower+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YUM [https://www.jfrog.com/confluence/display/RTF/YUM+Repositories](https://www.jfrog.com/confluence/display/RTF/YUM+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vagrant [https://www.jfrog.com/confluence/display/RTF/Vagrant+Repositories](https://www.jfrog.com/confluence/display/RTF/Vagrant+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker [https://www.jfrog.com/confluence/display/RTF/Docker+Registry](https://www.jfrog.com/confluence/display/RTF/Docker+Registry)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debian [https://www.jfrog.com/confluence/display/RTF/Debian+Repositories](https://www.jfrog.com/confluence/display/RTF/Debian+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBT [https://www.jfrog.com/confluence/display/RTF/SBT+Repositories](https://www.jfrog.com/confluence/display/RTF/SBT+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generic [https://www.jfrog.com/confluence/display/RTF/Configuring+Repositories](https://www.jfrog.com/confluence/display/RTF/Configuring+Repositories)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means Artifactory can be used as the single repository end-point for Continuous
    Delivery pipelines. Artifactory has recently introduced support for Vagrant boxes
    and Docker registry so it can be used to store Vagrant test environments, which
    could be used to store network operating systems or containers. This illustrates
    some of the features available from market-leading artifact repositories.
  prefs: []
  type: TYPE_NORMAL
- en: CD pipeline scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the job of the artifact repository is relatively straightforward, but
    no less important, choosing the correct Continuous Delivery pipeline tool is much
    more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a wide array of options available such as:'
  prefs: []
  type: TYPE_NORMAL
- en: IBM Urban Code Deploy [https://developer.ibm.com/urbancode/products/urbancode-deploy/](https://developer.ibm.com/urbancode/products/urbancode-deploy/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Electric Flow Deploy [http://electric-cloud.com/products/electricflow/deploy-automation/](http://electric-cloud.com/products/electricflow/deploy-automation/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins [https://jenkins.io/](https://jenkins.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoughtworks Go [https://www.go.cd/](https://www.go.cd/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XL Deploy [https://xebialabs.com/products/xl-deploy](https://xebialabs.com/products/xl-deploy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But before picking a tool, the process being implemented needs to be considered.
    So what are the main aims of a Continuous Delivery pipeline?
  prefs: []
  type: TYPE_NORMAL
- en: 'A good Continuous Delivery pipeline should meet the following goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Trigger deployments based on new artifacts being available in artifact repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule command lines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Render a pipeline view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Break tasks into stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide good log output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedback pass or failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these points need to be considered when selecting tooling so we will
    cherry-pick one of the most popular Continuous Delivery pipeline scheduling tools
    Jenkins, and look at ways in which it schedules pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jenkins was primarily a continuous integration build server when it was conceived.
    Jenkins has a pluggable framework that means that it is often customized to carry
    out deployments, with plugins such as the multi-job plugin built to allow it to
    schedule pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: However, as of the Jenkins 2.x release, Jenkins now makes pipelines a core feature
    component of its distribution rather than depending on plugins to cater for its
    deployment capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the Jenkins Pipeline job type can be seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the Jenkins Pipeline job type, users can specify pipelines using a `Pipeline
    Script`, declaring each stage of the pipeline. In this instance, the echo command
    has been used to spoof each pipeline stage to show how a `Pipeline script` may
    look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The visual display of the pipeline from the pipeline script is shown in the
    **Stage View**. The **Stage View** shows the eight stages the pipeline went through
    in order to deploy the networking, virtual machines, application, and load balancer
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Logging for each stage is shown clearly in the Jenkins console logs which allow
    users of the tool to see feedback on successful and unsuccessful console logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When putting valid commands into `Pipeline script`, as opposed to simulating
    echoes as we have in the above example, Jenkins allows users to use a groovy snippet
    generator to translate any steps to Pipeline Script format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this instance, a shell command is required to execute the Ansible playbook
    to `create_vip.yml` on the component test environment, so the snipped generator
    is used to create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This snippet command can then be pasted into the `create_vip.yml` stage that
    was created on the Pipeline script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](img/B05559_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output of the job configuration is a Jenkins file that can be stored in
    SCM to version control the deployment pipeline changes.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying network changes with deployment pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When carrying out Continuous Delivery or deployment, it is essential to incorporate
    network changes. Network teams need to contribute major pieces of the deployment
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: As the CD Pipeline scheduler allows different stages to be specified in the
    deployment pipeline, it gives great flexibility and allows all teams to contribute
    pieces, forming a true collaborative DevOps model.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes a concern from network teams is that developers should not have the
    necessary access to all network devices as they are not experts. Truth be told,
    developers don't want access to network devices, they instead want a quick way
    of pushing out their changes where they are not impeded by having to wait on network
    changes being applied.
  prefs: []
  type: TYPE_NORMAL
- en: Network self-service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Allowing developers the ability to self-service their own network changes is
    very important, otherwise the network team becomes the bottleneck for the Continuous
    Delivery process.
  prefs: []
  type: TYPE_NORMAL
- en: So providing development teams with, say, a hardened Ansible playbook to create
    everyday network functions will undoubtedly help alleviate developer pain and
    make deployment of new network changes a self-service function.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can use a playbook that incorporates all the best practices of the
    network team to apply any network changes. This is following the model where developers
    can utilize a playbook provided by the infrastructure team to spin up new virtual
    machines and register their DNS entries with the IPAM solution.
  prefs: []
  type: TYPE_NORMAL
- en: Steps in a deployment pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating deployment pipelines, it is important to break up each function
    into a granular set of steps. This means if any step fails it can be easily rolled
    back. Understanding the deployment pipeline visually is also important as breaking
    down complex operations into small steps makes debugging failures less daunting
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 'A modern application deployment pipeline will provision new environments by
    carrying out the following high level steps every single deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: Download manifest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create VMs in network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create VIP
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rolling update
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run test pack
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Promote to next phase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first stage of the pipeline is the trigger for a new deployment to the first
    test environment. In this case, the detection of a new manifest file artifact.
  prefs: []
  type: TYPE_NORMAL
- en: The manifest artifact will be downloaded to the CD Pipeline scheduler and parsed.
    The Ansible `var` file structure will be assembled from SCM using the manifest
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: Once assembled, the network needs to be provisioned. An A or B network will
    be created depending on the release and the necessary Ingress and Egress ACL rules
    will be applied to the network.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines will then be booted into the newly-provisioned network and
    tagged with their metadata profile stating the software that needs to be installed
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible dynamic inventory is run to pull back the new virtual machines that
    were just created, Ansible reads the profile metadata from the virtual machines.
    metadata tags and Ansible installs the required role on the new cluster of virtual
    machines depending on what profile is specified.
  prefs: []
  type: TYPE_NORMAL
- en: A VIP is created on the load balancer if it doesn't already exist and its load
    balancing policies are applied. Boxes are then rolled into service on the new
    VIP and old boxes are rolled out of service. The new boxes are smoke tested to
    make sure they are operating as expected before the previous release is destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: A full quality assurance test pack is then executed and the manifest artifact
    is then promoted to the next stage if successful.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these steps will be repeated all the way up to production. In a Continuous
    Delivery model, the production deployment will be a manual button press to trigger
    the pipeline, where in Continuous Delivery pipeline will automatically trigger
    if all quality gates pass.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating configuration management tooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When utilizing a CD scheduler such as Jenkins, its agents, known as slaves,
    can be used to install Ansible on them and they become the Ansible Control Host
    for the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Each stage in the deployment pipeline can be a small modular Ansible playbook
    that allows developers to self-serve their network needs. These playbooks can
    be created by the network team and continuously improved over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the Jenkins `Pipeline script` would resemble the following, with a unique
    playbook for each stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Incorporating configuration management tooling](img/B05559_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The steps applied on each test environment should be consistent with production
    and all steps should be carried out by a service account for the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Each and every environment should be built from source control by implementing
    immutable infrastructure and networking. This is so that the desired state is
    always what is specified in the manifest file's associated repositories.
  prefs: []
  type: TYPE_NORMAL
- en: The Ansible `var` files that feed each playbook can be filled in by the development
    teams in order to set firewall policies or load balancing policies.
  prefs: []
  type: TYPE_NORMAL
- en: These `var` files are versioned by the associated continuous integration builds
    for the SDN or load balancing configuration. Each network-related CI build then
    rolls up into a new manifest file when an application continuous integration build
    is triggered. The generation of a new manifest file triggers the first step in
    the deployment pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Network teams' role in Continuous Delivery pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When analyzing the steps that are executed by a deployment pipeline, if we look
    at which teams would have the necessary permissions to carry out each pipeline
    stage manually, it becomes very apparent the importance of integrating networking
    into the Continuous Delivery processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of the eight high level stages to deploy an application, three of them
    are integrating with the network when executing **create network**, **create vip**,
    and **rolling update** as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network teams'' role in Continuous Delivery pipelines](img/B05559_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This shows that if network operations were not part of the deployment pipeline
    then true Continuous Delivery would not be achievable.
  prefs: []
  type: TYPE_NORMAL
- en: Failing fast and feedback loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key objectives of creating Continuous Delivery pipelines is creating
    feedback loops which fail fast and create a radiator view for developers. However,
    with Continuous Delivery moving into continuous operations space, as it now incorporates
    infrastructure, networking, and quality assurance, all teams need to be mindful
    of failures and react accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: When pipeline stages fail, it is important to incorporate automated clean-up
    every time there is a failure, this leaves the pipeline in a good state so the
    next pipeline is not impeded. Any break in the process means that changes cannot
    reach production.
  prefs: []
  type: TYPE_NORMAL
- en: 'So although it may be a test environment that is breaking, it is now blocking
    potential fixes being deployed to production. If a failure occurs, the pipeline
    should also halt the whole process and not proceed to the next stage as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Failing fast and feedback loops](img/B05559_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ansible block rescue functionality is very useful when dealing with failed pipeline
    stages and clean-up, providing a try and catch-like feature for playbooks and
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: Testing should also be incorporated into the deployment pipeline so if the run
    test stage of the pipeline fails, then there is a history of why the tests failed
    that can be audited. Pipelines also help provide a full history of changes that
    have been applied to the environment. Although triggered by a service account,
    the user that committed the change in source control should take ownership for
    each change.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at integrating network changes into deployment pipelines
    so that network teams can contribute to the Continuous Delivery process. We then
    discussed the difference between Continuous Delivery and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at how package management is crucial for wrapping development,
    infrastructure, quality assurance, and network changes together as part of deployment
    pipelines. We also illustrated some of the market-leading artifact repositories
    and CD pipeline schedulers using Artifactory and Jenkins as examples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at best practices that should be adopted when setting up
    deployment pipelines within the remits of Continuous Delivery and deployment.
    We then focused on ways network teams could contribute to deployment pipelines
    by providing self-service deployment scripts to developers, so they keep the overall
    process quick, lean, and automated.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should now understand why that applications
    should be compiled only once and stored in an artifact repository, and the same
    binaries should be deployed to multiple environments so the deployment process
    is consistent.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also focused on the differences between pull-based tools, such as
    Chef and Puppet, and tools such as Ansible and Salt that utilize a push model
    for configuration management.
  prefs: []
  type: TYPE_NORMAL
- en: Key takeaways should also include how to utilize Artifactory as an artifact
    repository to store numerous types of build artifacts, and ways in which manifest
    files can be generated using continuous integration to version code, infrastructure,
    networking, and load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Readers should learn all the necessary steps in a Continuous Delivery pipeline,
    how to set up a deployment pipeline using Jenkins 2.x, and the importance of integrating
    networking in the Continuous Delivery model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on containers and look at the impact they
    have had on networking and network operations. We will look at some of the different
    orchestration options that can be used such as Docker and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
