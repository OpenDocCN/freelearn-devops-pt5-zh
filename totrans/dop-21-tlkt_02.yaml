- en: Setting Up and Operating a Swarm Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Organizations which design systems … are constrained to produce designs which
    are copies of the communication structures of these organizations'
  prefs: []
  type: TYPE_NORMAL
- en: –M.Conway*
  prefs: []
  type: TYPE_NORMAL
- en: Many will tell you that they have a *scalable system*. After all, scaling is
    easy. Buy a server, install WebLogic (or whichever other monster application server
    you're using) and deploy your applications. Then wait for a few weeks until you
    discover that everything is so *fast* that you can click a button, have some coffee,
    and, by the time you get back to your desk, the result will be waiting for you.
    What do you do? You scale. You buy a few more servers, install your monster application
    servers and deploy your monster applications on top of them. Which part of the
    system was the bottleneck? Nobody knows. Why did you duplicate everything? Because
    you must. And then some more time passes, and you continue scaling until you run
    out of money and, simultaneously, people working for you go crazy. Today we do
    not approach scaling like that. Today we understand that scaling is about many
    other things. It's about elasticity. It's about being able to quickly and easily
    scale and de-scale depending on variations in your traffic and growth of your
    business, and that you should not go bankrupt during the process. It's about the
    need of almost every company to scale their business without thinking that IT
    department is a liability. It's about getting rid of those monsters.
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to The DevOps 2.0 Toolkit readers**'
  prefs: []
  type: TYPE_NORMAL
- en: The text that follows is identical to the one published in *The DevOps 2.0 Toolkit*.
    If it is still fresh in your mind, feel free to jump to the section *Docker Swarm
    Mode* of this chapter. You'll see that a lot has changed. One of those changes
    is that the old Swarm running as a separate container is deprecated for *Swarm
    Mode*. There are many other new things we'll discover along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us, for a moment take a step back and discuss why we want to scale applications.
    The main reason is *high availability*. Why do we want high availability? We want
    it because we want our business to be available under any load. The bigger the
    load, the better (unless you are under DDoS). It means that our business is booming.
    With high availability our users are happy. We all want speed, and many of us
    simply leave the site if it takes too long to load. We want to avoid having outages
    because every minute our business is not operational can be translated into a
    money loss. What would you do if an online store is not available? Probably go
    to another. Maybe not the first time, maybe not the second, but, sooner or later,
    you would get fed up and switch it for another. We are used to everything being
    fast and responsive, and there are so many alternatives that we do not think twice
    before trying something else. And if that something else turns up to be better.
    One man's loss is another man's gain. Do we solve all our problems with scalability?
    Not even close. Many other factors decide the availability of our applications.
    However, scalability is an important part of it, and it happens to be the subject
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What is scalability? It is a property of a system that indicates its ability
    to handle increased load in a graceful manner or its potential to be enlarged
    as demand increases. It is the capacity to accept increased volume or traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that the way we design our applications dictates the scaling options
    available. Applications will not scale well if they are not designed to scale.
    That is not to say that an application not designed for scaling cannot scale.
    Everything can scale, but not everything can scale well.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly observed scenario is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a simple architecture, sometimes with load balancer sometimes
    without, setup a few application servers and one database. Everything is great,
    complexity is low, and we can develop new features very fast. The cost of operations
    is low, income is high (considering that we just started), and everyone is happy
    and motivated.
  prefs: []
  type: TYPE_NORMAL
- en: Business is growing, and the traffic is increasing. Things are beginning to
    fail, and performance is dropping. Firewalls are added, additional load balancers
    are set up, the database is scaled, more application servers are added and so
    on. Things are still relatively straightforward. We are faced with new challenges,
    but we can overcome obstacles in time. Even though the complexity is increasing,
    we can still handle it with relative ease. In other words, what we're doing is
    still, more or less, the same but bigger. Business is doing well, but it is still
    relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: And then it happens. The big thing you've been waiting for. Maybe one of the
    marketing campaigns hit the spot. Maybe there was an adverse change in your competition.
    Maybe that last feature was indeed a killer one. No matter the reasons, business
    got a big boost. After a short period of happiness due to this change, your pain
    increases tenfold. Adding more databases does not seem to be enough. Multiplying
    application servers does not appear to fulfill the needs. You start adding caching
    and what not. You start getting the feeling that every time you multiply something,
    benefits are not equally big. Costs increase, and you are still not able to meet
    the demand. Database replications are too slow. New application servers do not
    make such a big difference anymore. Operational costs are increasing faster than
    you expected. The situation hurts the business and the team. You are starting
    to realize that the architecture you were so proud of cannot fulfill this increase
    in load. You cannot split it. You cannot scale things that hurt the most. You
    cannot start over. All you can do is continue multiplying with ever decreasing
    benefits of such actions.
  prefs: []
  type: TYPE_NORMAL
- en: The situation described above is quite common. What was good at the beginning,
    is not necessarily right when the demand increases. We need to balance the need
    for **You ain't gonna need it** (**YAGNI**) principle and the longer term vision.
    We cannot start with the system optimized for large companies because it is too
    expensive and does not provide enough benefits when business is small. On the
    other hand, we cannot lose the focus from one of the primary objectives of any
    business. We cannot not think about scaling from the very first day. Designing
    scalable architecture does not mean that we need to start with a cluster of a
    hundred servers. It does not mean that we have to develop something big and complex
    from the start. It means that we should start small, but in the way that, when
    it becomes big, it is easy to scale. While microservices are not the only way
    to accomplish that goal, they are indeed a good way to approach this problem.
    The cost is not in development but operations. If operations are automated, that
    cost can be absorbed quickly and does not need to represent a massive investment.
    As you already saw (and will continue seeing throughout the rest of the book),
    there are excellent open source tools at our disposal. The best part of automation
    is that the investment tends to have lower maintenance cost than when things are
    done manually.
  prefs: []
  type: TYPE_NORMAL
- en: We already discussed microservices and automation of their deployments on a
    tiny scale. Now it's time to convert this small scale to something bigger. Before
    we jump into practical parts, let us explore what are some of the different ways
    one might approach scaling.
  prefs: []
  type: TYPE_NORMAL
- en: We are often limited by our design and choosing the way applications are constructed
    limits our choices severely. Although there are many different ways to scale,
    the most common one is called *Axis Scaling*.
  prefs: []
  type: TYPE_NORMAL
- en: Axis scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Axis scaling can be best represented through three dimensions of a cube; *X-Axis*,
    *Y-Axis*, and *Z-Axis*. Each of those dimensions describes a type of scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: 'X-Axis: Horizontal duplication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y-Axis: Functional decomposition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Z-Axis: Data partitioning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/scale-cube.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-1: Scale cube'
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through the Axes, one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: X-axis scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a nutshell, *x-axis* scaling is accomplished by running multiple instances
    of an application or a service. In most cases, there is a load balancer on top
    that makes sure that the traffic is shared among all those instances. The biggest
    advantage of *x-axis* scaling is simplicity. All we have to do is deploy the same
    application on multiple servers. For that reason, this is the most commonly used
    type of scaling. However, it comes with its set of disadvantages when applied
    to monolithic applications.
  prefs: []
  type: TYPE_NORMAL
- en: Having a large application usually requires a big cache that demands heavy usage
    of memory. When such an application is multiplied, everything is multiplied with
    it, including the cache. Another, often more important, problem is an inappropriate
    usage of resources. Performance issues are almost never related to the whole application.
    Not all modules are equally affected, and, yet, we multiply everything. That means
    that even though we could be better off by scaling only part of the application
    that requires such an action, we scale everything. Nevertheless, *x- axis* scaling
    is important no matter the architecture. The major difference is the effect that
    such a scaling has.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/monolith-scaling.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-2: Monolithic application scaled inside a cluster'
  prefs: []
  type: TYPE_NORMAL
- en: By using microservices, we are not removing the need for *x-axis* scaling but
    making sure that due to their architecture such scaling has more effect than with
    alternative and more traditional approaches to architecture. With microservices,
    we have the option to fine-tune scaling. We can have many instances of services
    that suffer a lot under heavy load and only a few instances of those that are
    used less often or require fewer resources. On top of that, since they are small,
    we might never reach a limit of a service. A small service in a big server would
    need to receive a truly massive amount of traffic before the need for scaling
    arises. Scaling microservices is more often related to fault tolerance than performance
    problems. We want to have multiple copies running so that, if one of them dies,
    the others can take over until recovery is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Y-axis scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Y-axis scaling is all about decomposition of an application into smaller services.
    Even though there are different ways to accomplish this decomposition, microservices
    are probably the best approach we can take. When they are combined with immutability
    and self-sufficiency, there is indeed no better alternative (at least from the
    prism of y-axis scaling). Unlike x-axis scaling, the y-axis is not accomplished
    by running multiple instances of the same application but by having multiple different
    services distributed across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Z-axis scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Z-axis scaling is rarely applied to applications or services. Its primary and
    most common usage is among databases. The idea behind this type of scaling is
    to distribute data among multiple servers thus reducing the amount of work that
    each of them needs to perform. Data is partitioned and distributed so that each
    server needs to deal only with a subset of the data. This type of separation is
    often called **sharding**, and there are many databases specially designed for
    this purpose. Benefits of z-axis scaling are most noticeable in I/O and cache
    and memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A server cluster consists of a set of connected servers that work together and
    can be seen as a single system. They are usually connected to a fast **Local Area
    Network **(**LAN**). The significant difference between a cluster and a group
    of servers is that the cluster acts as a single system trying to provide high
    availability, load balancing, and parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we deploy applications, or services, to individually managed servers and
    treat them as separate units, the utilization of resources is sub-optimum. We
    cannot know in advance which group of services should be deployed to a server
    and utilize resources to their maximum. Moreover, resource usage tends to fluctuate.
    While, in the morning, some service might require a lot of memory, during the
    afternoon that usage might be lower. Having predefined servers prevents us from
    having elasticity that would balance that usage in the best possible way. Even
    if such a high level of dynamism is not required, predefined servers tend to create
    problems when something goes wrong, resulting in manual actions to redeploy the
    affected services to a healthy node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/servers.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-3: Cluster with containers deployed to predefined servers'
  prefs: []
  type: TYPE_NORMAL
- en: Real clustering is accomplished when we stop thinking in terms of individual
    servers and start thinking of a cluster; of all servers as one big entity. That
    can be better explained if we drop to a bit lower level. When we deploy an application,
    we tend to specify how much memory or CPU it might need. However, we do not decide
    which memory slots our application will use nor which CPUs it should utilize.
    For example, we don’t specify that some application should use CPUs 4, 5 and 7\.
    That would be inefficient and potentially dangerous. We only decide that three
    CPUs are required. The same approach should be taken on a higher level. We should
    not care where an application or a service will be deployed but what it needs.
    We should be able to define that the service has certain requirements and tell
    some tool to deploy it to whichever server in our cluster, as long as it fulfills
    the needs we have. The best (if not the only) way to accomplish that is to consider
    the whole cluster as one entity.
  prefs: []
  type: TYPE_NORMAL
- en: We can increase or decrease the capacity of a cluster by adding or removing
    servers but, no matter what we do, it should still be a single entity. We define
    a strategy and let our services be deployed somewhere inside the cluster. Those
    using cloud providers like **Amazon Web Services **(**AWS**), Microsoft Azure
    and **Google Compute Engine **(**GCE**) are already accustomed to this approach,
    even though they might not be aware of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the rest of this chapter, we''ll explore ways to create our cluster
    and explore tools that can help us with that objective. The fact that we''ll be
    simulating the cluster locally does not mean that the same strategies cannot be
    applied to public or private clouds and data centers. Quite the opposite:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cluster.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-4: Cluster with containers deployed to servers based on a predefined
    strategy'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Docker Engine v1.12* was released in July 2016\. It is the most significant
    version since *v1.9*. Back then, we got Docker networking that, finally, made
    containers ready for use in clusters. With *v1.12*, Docker is reinventing itself
    with a whole new approach to cluster orchestration. Say goodbye to Swarm as a
    separate container that depends on an external data registry and welcome the *new
    Docker Swarm* or *Swarm Mode*. Everything you''ll need to manage your cluster
    is now incorporated into Docker Engine. Swarm is there. Service discovery is there.
    Improved networking is there. That does not mean that we do not need additional
    tools. We do. The major difference is that Docker Engine now incorporates all
    the "essential" (not to say minimal) tools we need.'
  prefs: []
  type: TYPE_NORMAL
- en: The old Swarm (before *Docker v1.12*) used *fire-and-forget principle*. We would
    send a command to Swarm master, and it would execute that command. For example,
    if we would send it something like `docker-compose scale go-demo=5`, the old Swarm
    would evaluate the current state of the cluster, discover that, for example, only
    one instance is currently running, and decide that it should run four more. Once
    such a decision is made, the old Swarm would send commands to Docker Engines.
    As a result, we would have five containers running inside the cluster. For all
    that to work, we were required to set up Swarm agents (as separate containers)
    on all the nodes that form the cluster and hook them into one of the supported
    data registries (Consul, etcd, and Zookeeper).
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem was that Swarm was executing commands we send it. It was not maintaining
    the desired state. We were, effectively, telling it what we want to happen (example:
    scale up), not the state we wanted (make sure that five instances are running).
    Later on, the old Swarm got the feature that would reschedule containers from
    failed nodes. However, that feature had a few problems that prevented it from
    being a reliable solution (example: failed containers were not removed from the
    overlay network).'
  prefs: []
  type: TYPE_NORMAL
- en: Now we got a brand new Swarm. It is part of Docker Engine (no need to run it
    as separate containers), it has incorporated service discovery (no need to set
    up Consul or whatever is your data registry of choice), it is designed from the
    ground up to accept and maintain the desired state, and so on. It is a truly major
    change in how we deal with cluster orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past, I was inclined towards the old Swarm more than Kubernetes. However,
    that inclination was only slight. There were pros and cons for using either solution.
    Kubernetes had a few features Swarm was missing (example: the concept of the desired
    state), the old Swarm shined with its simplicity and low usage of resources. With
    the new Swarm (the one that comes with *v1.12*), I have no more doubts which one
    to use. *The new Swarm is often a better choice than Kubernetes*. It is part of
    Docker Engine, so the whole setup is a single command that tells an engine to
    join the cluster. The new networking works like a charm. The bundle that can be
    used to define services can be created from Docker Compose files, so there is
    no need to maintain two sets of configurations (Docker Compose for development
    and a different one for orchestration). Most importantly, the new Docker Swarm
    continues being simple to use. From the very beginning, Docker community pledged
    that they are committed to simplicity and, with this release, they, once again,
    proved that to be true.'
  prefs: []
  type: TYPE_NORMAL
- en: And that's not all. The new release comes with a lot of other features that
    are not directly related with Swarm. However, this book is dedicated to cluster
    management. Therefore, I'll focus on Swarm and leave the rest for one of the next
    books or a blog article.
  prefs: []
  type: TYPE_NORMAL
- en: Since I believe that code explains things better than words, we'll start with
    a demo of some of the new features introduced in *version 1.12*.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll continue using Docker Machine since it provides a very convenient way
    to simulate a cluster on a laptop. Three servers should be enough to demonstrate
    some of the key features of a Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `02-docker-swarm.sh` ([https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896](https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896))
    Gist
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At this moment, we have three nodes. Please note that those servers are not
    running anything but Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the status of the nodes by executing the following `ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (ERROR column removed for brievity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/nodes.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-5: Machines running Docker Engines'
  prefs: []
  type: TYPE_NORMAL
- en: With the machines up and running we can proceed and set up the Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cluster setup consists of two types of commands. We need to start by initializing
    the first node which will be our manager. Refer to the following illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The first command set environment variables so that the local Docker Engine
    is pointing to the `node-1`. The second initialized Swarm on that machine.
  prefs: []
  type: TYPE_NORMAL
- en: We specified only one argument with the `swarm init` command. The `--advertise-addr`
    is the address that this node will expose to other nodes for internal communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `swarm init` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the node is now a manager and we've got the commands we can
    use to join other nodes to the cluster. As a way to increase security, a new node
    can be added to the cluster only if it contains the token generated when Swarm
    was initialized. The token was printed as a result of the `docker swarm init`
    command. You can copy and paste the code from the output or use the `join-token`
    command. We'll use the latter.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, our Swarm cluster consists of only one VM. We'll add the other two
    nodes to the cluster. But, before we do that, let us discuss the difference between
    a *manager* and a *worker*.
  prefs: []
  type: TYPE_NORMAL
- en: A Swarm manager continuously monitors the cluster state and reconciles any differences
    between the actual state and your expressed desired state. For example, if you
    set up a service to run ten replicas of a container, and a worker machine hosting
    two of those replicas crashes, the manager will create two new replicas to replace
    the ones that failed. Swarm manager assigns new replicas to workers that are running
    and available. A manager has all the capabilities of a worker.
  prefs: []
  type: TYPE_NORMAL
- en: We can get a token required for adding additional nodes to the cluster by executing
    the `swarm join-token` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to obtain a token for adding a manager is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, to get a token for adding a worker, we would run the command that
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, we'd get a long hashed string.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the worker token is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Please note that this token was generated on my machine and, in your case, it
    will be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put the token into an environment variable and add the other two nodes
    as workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that have the token inside a variable, we can issue the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The command we just ran iterates over nodes two and three and executes the
    `swarm join command`. We set the token, the advertise address, and the address
    of our manager. As a result, the two machines joined the cluster as workers. We
    can confirm that by sending the `node ls` command to the manager node `node-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `node ls` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The asterisk tells us which node we are currently using. The `MANAGER STATUS`
    indicates that the `node-1` is the *leader:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-6: Docker Swarm cluster with three nodes'
  prefs: []
  type: TYPE_NORMAL
- en: In a production environment, we would probably set more than one node to be
    a manager and, thus, avoid deployment downtime if one of them fails. For the purpose
    of this demo, having one manager should suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying services to the Swarm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we deploy a demo service, we should create a new network so that all
    containers that constitute the service can communicate with each other no matter
    on which nodes they are deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The next chapter will explore networking in more details. Right now, we'll discuss
    and do only the absolute minimum required for an efficient deployment of services
    inside a Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the status of all networks with the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `network ls` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have two networks that have the `swarm` scope. The one named
    `ingress` was created by default when we set up the cluster. The second `go-demo` was
    created with the `network create` command. We'll assign all containers that constitute
    the `go-demo` service to that network.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will go deep into the Swarm networking. For now, it is important
    to understand that all services that belong to the same network can speak with
    each other freely.
  prefs: []
  type: TYPE_NORMAL
- en: The *go-demo* application requires two containers. Data will be stored in MongoDB.
    The back-end that uses that `DB` is defined as `vfarcic/go-demo` container.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by deploying the `mongo` container somewhere within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, we''d use constraints to specify the requirements for the container
    (example: HD type, the amount of memory and CPU, and so on). We''ll skip that,
    for now, and tell Swarm to deploy it anywhere within the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Please note that we haven't specified the port `Mongo` listens to `27017`. That
    means that the database will not be accesible to anyone but other services that
    belong to the same `go-demo` network .
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the way we use service create is similar to the Docker `run`
    command you are, probably, already used to.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can list all the running services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Depending on how much time passed between `service create` and `service ls`
    commands, you'll see the value of the `REPLICAS` column being zero or one. Immediately
    after creating the service, the value should be `*0/1*`, meaning that zero replicas
    are running, and the objective is to have one. Once the `mongo` image is pulled,
    and the container is running, the value should change to `*1/1*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output of the `service ls` command should be as follows (IDs are
    removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we need more information about the `go-demo-db` service, we can run the
    service inspect command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the database is running, we can deploy the `go-demo` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: There's nothing new about that command. The service will be attached to the
    `go-demo` network. The environment variable `DB` is an internal requirement of
    the `go-demo` service that tells the code the address of the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have two containers (`mongo` and `go-demo`) running inside
    the cluster and communicating with each other through the `go-demo` network. Please
    note that none of them is *yet* accessible from outside the network. At this point,
    your users do not have access to the service API. We''ll discuss this in more
    details soon. Until then, I''ll give you only a hint: *you need a reverse proxy*
    capable of utilizing the new Swarm networking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `service ls` command one more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The result, after the `go-demo` service is pulled to the destination node,
    should be as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, both services are running as a single replica:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes-single-container.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-8: Docker Swarm cluster containers communicating through the go-demo
    SDN'
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we want to scale one of the containers? How do we scale our
    services?
  prefs: []
  type: TYPE_NORMAL
- en: Scaling services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should always run at least two instances of any given service. That way they
    can share the load and, if one of them fails, there will be no downtime. We'll
    explore Swarm's failover capability soon and leave load balancing for the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, for example, tell Swarm that we want to run five replicas of then `go-demo`
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With the `service scale` command, we scheduled five replicas. Swarm will make
    sure that five instances of `go-demo` are running somewhere inside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm that, indeed, five replicas are running through the, already
    familiar, `service ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, five out of five `REPLICAS` of the `go-demo` service are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `service ps` command provides more detailed information about a single
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs and ERROR PORTs columns are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `go-demo` service is running five instances distributed
    across the three nodes. Since they all belong to the same **go-demo SDN**, they
    can communicate with each other no matter where they run inside the cluster. At
    the same time, none of them is accessible from outside:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes-replicas-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-9: Docker Swarm cluster with go-demo service scaled to five replicas'
  prefs: []
  type: TYPE_NORMAL
- en: What happens if one of the containers is stopped or if the entire node fails?
    After all, processes and nodes do fail sooner or later. Nothing is perfect, and
    we need to be prepared for such situations.
  prefs: []
  type: TYPE_NORMAL
- en: Failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, failover strategies are part of Docker Swarm. Remember, when we
    execute a `service` command, we are not telling Swarm what to do but the state
    we desire. In turn, Swarm will do its best to maintain the specified state no
    matter what happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test a failure scenario, we''ll destroy one of the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Swarm needs a bit of time until it detects that the node is down. Once it does,
    it will reschedule containers. We can monitor the situation through `service ps
    command`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output (after rescheduling) is as follows (`ID` is removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Capture.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, after a short period, Swarm rescheduled containers among healthy
    nodes (`node-1` and `node-2`) and changed the state of those that were running
    on the failed node to `Shutdown`. If your output still shows that some instances
    are running on the `node-3`, please wait for a few moments and repeat the `service
    ps command`.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That concludes the exploration of basic concepts of the new Swarm features we
    got with *Docker v1.12+*.
  prefs: []
  type: TYPE_NORMAL
- en: Is this everything there is to know to run a Swarm cluster successfully? Not
    even close! What we explored by now is only the beginning. There are quite a few
    questions waiting to be answered. How do we expose our services to the public?
    How do we deploy new releases without downtime? I'll try to give answers to those
    and quite a few other questions in the chapters that follow. The next one will
    be dedicated to the exploration of the ways we can expose our services to the
    public. We'll try to integrate a proxy with a Swarm cluster. To do that, we need
    to dive deeper into Swarm networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
