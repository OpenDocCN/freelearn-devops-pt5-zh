- en: Chapter 4. Monitoring Docker Hosts and Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：监控 Docker 主机和容器
- en: We now know some ways to optimize our Docker deployments. We also know how to
    scale to improve performance. But how do we know that our tuning assumptions were
    correct? Being able to monitor our Docker infrastructure and application is important
    to figure out why and when we need to optimize. Measuring how our system is performing
    allows us to identify its limits to scale and tune accordingly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经知道了优化 Docker 部署的一些方法，也知道如何通过扩展来提高性能。但是我们如何知道我们的调优假设是否正确呢？能够监控我们的 Docker
    基础设施和应用程序对于确定我们何时以及为什么需要进行优化非常重要。衡量系统的性能可以帮助我们识别其扩展极限，并据此进行调优。
- en: In addition to monitoring low-level information about Docker, it is also important
    to measure the business-related performance of our application. By tracing the
    value stream of our application, we can correlate business-related metrics to
    system-level ones. With this, our Docker development and operations teams can
    show their business colleagues how Docker saves their organization's costs and
    increases business value.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监控 Docker 的低层次信息，衡量应用程序的业务相关性能同样重要。通过追踪应用程序的价值流，我们可以将业务相关的指标与系统层面的指标关联起来。这样，Docker
    开发和运维团队就能够向业务同事展示 Docker 如何帮助节省组织成本并提高业务价值。
- en: 'In this chapter, we will cover the following topics about being able to monitor
    our Docker infrastructure and applications at scale:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下关于能够大规模监控 Docker 基础设施和应用程序的主题：
- en: The importance of monitoring
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控的重要性
- en: Collecting monitored data in Graphite
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Graphite 中收集监控数据
- en: Monitoring Docker with collectd
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 collectd 监控 Docker
- en: Consolidating logs in an ELK stack
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 ELK 堆栈中合并日志
- en: Sending logs from Docker
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送来自 Docker 的日志
- en: The importance of monitoring
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控的重要性
- en: Monitoring is important as it provides a source of feedback on the Docker deployment
    that we built. It answers several questions about our application from low-level
    operating system performance to high-level business targets. Having proper instrumentation
    inserted in our Docker hosts allows us to identify our system's state. We can
    use this source of feedback to identify whether our application is behaving as
    originally planned.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监控非常重要，因为它提供了关于我们构建的 Docker 部署的反馈来源。它可以回答关于我们的应用程序的一些问题，从低层次的操作系统性能到高层次的业务目标。通过在
    Docker 主机中插入适当的监控工具，我们可以识别系统的状态。我们可以利用这个反馈源来判断我们的应用程序是否按最初计划的方式运行。
- en: If our initial hypothesis was incorrect, we can use the feedback data to revise
    our plan and change our system accordingly by tuning our Docker host and containers
    or updating our running Docker application. We can also use the same monitoring
    process to identify errors and bugs after our system is deployed to production.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的初始假设不正确，我们可以利用反馈数据修正我们的计划，并通过调优 Docker 主机和容器或更新正在运行的 Docker 应用程序来调整系统。我们还可以使用相同的监控过程来识别系统部署到生产环境后出现的错误和
    bug。
- en: Docker has built-in features to log and monitor. By default, a Docker host stores
    a Docker container's standard output and error streams to JSON files in `/var/lib/docker/<container_id>/<container_id>-json.log`.
    The `docker logs` command asks the Docker engine daemon to read the content of
    the files here.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 具有内置的日志记录和监控功能。默认情况下，Docker 主机会将 Docker 容器的标准输出和错误流存储为 JSON 文件，路径为 `/var/lib/docker/<container_id>/<container_id>-json.log`。`docker
    logs` 命令会请求 Docker 引擎守护进程读取此处文件的内容。
- en: 'Another monitoring facility is the docker stats command. This queries the Docker
    engine''s remote API''s `/containers/<container_id>/stats` endpoint to report
    runtime statistics about the running container''s control group regarding its
    CPU, memory, and network usage. The following is an example output of the `docker
    stats` command reporting the said metrics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个监控工具是 `docker stats` 命令。该命令查询 Docker 引擎远程 API 的 `/containers/<container_id>/stats`
    端点，报告关于运行容器的控制组的运行时统计信息，包括其 CPU、内存和网络使用情况。以下是 `docker stats` 命令报告上述指标的示例输出：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The built-in `docker logs` and `docker stats` commands work well to monitor
    our Docker applications for development and small-scale deployments. When we get
    to a point in our production-grade Docker deployment where we manage tens, hundreds,
    or even thousands of Docker hosts, this approach will no longer be scalable. It
    is not feasible to log in to each of our thousand Docker hosts and type `docker
    logs` and `docker stats`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的 `docker logs` 和 `docker stats` 命令非常适合用来监控我们用于开发和小规模部署的 Docker 应用程序。当我们进入生产级别的
    Docker 部署，管理数十个、数百个甚至数千个 Docker 主机时，这种方法就不再具备扩展性。因为我们无法登录到每一个 Docker 主机并输入 `docker
    logs` 和 `docker stats`。
- en: Doing this one by one also makes it difficult to create a more holistic picture
    of our entire Docker deployment. Also, not everyone interested in our Docker application's
    performance can log in to our Docker hosts. Our colleagues dealing with only the
    business aspect of our application may want to ask certain questions on how our
    application's deployment in Docker improves what our organization wants. However,
    they do not necessarily want to learn how to log in and start typing Docker commands
    in our infrastructure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一一操作还使得我们很难对整个 Docker 部署形成更全面的了解。而且，并不是每一个关心我们 Docker 应用性能的人都能登录到我们的 Docker
    主机。仅仅处理应用业务方面的同事可能希望了解我们的 Docker 部署如何帮助组织实现目标，但他们不一定希望学习如何登录并在我们的基础设施中输入 Docker
    命令。
- en: Hence, it is important to be able to consolidate all of the events and metrics
    from our Docker deployments into a centralized monitoring infrastructure. It allows
    our operations to scale by having a single point to ask what is happening to our
    system. A centralized dashboard also enables people outside our development and
    operations team, such as our business colleagues, to have access to the feedback
    provided by our monitoring system. The remaining sections will show you how to
    consolidate messages from `docker logs` and collect statistics from data sources
    such as `docker stats`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，能够将我们 Docker 部署中的所有事件和度量数据整合到一个集中的监控基础设施中是非常重要的。它通过提供一个单一的查询点来让我们的运维团队能够了解系统的运行情况，从而帮助我们的运维实现规模化。一个集中的仪表盘还使得开发和运维团队以外的人，比如业务同事，能够访问我们的监控系统提供的反馈信息。接下来的章节将向你展示如何整合来自
    `docker logs` 的信息，并收集来自数据源（如 `docker stats`）的统计数据。
- en: Collecting metrics to Graphite
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集度量数据到 Graphite
- en: To begin monitoring our Docker deployments, we must first set up an endpoint
    to send our monitored values to. Graphite is a popular stack for collecting various
    metrics. Its plaintext protocol is very popular because of its simplicity. Many
    third-party tools understand this simple protocol. Later, we will show you how
    easy it is to send data to Graphite after we finish setting it up.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始监控我们的 Docker 部署，首先我们必须设置一个端点，将我们的监控数据发送到那里。Graphite 是一个流行的堆栈，用于收集各种度量数据。它的纯文本协议因其简单性而广受欢迎，许多第三方工具都能理解这个简单的协议。稍后我们将展示，在设置完
    Graphite 后，如何轻松地向其发送数据。
- en: Another feature of graphite is that it can render the data it gathers into graphs.
    We can then consolidate these graphs to build a dashboard. The dashboard we crafted
    in the end will show the various kinds of information that we need to monitor
    our Docker application.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite 的另一个特点是它可以将收集到的数据呈现为图表。然后我们可以将这些图表整合到一起，构建一个仪表盘。最终我们构建的仪表盘将展示我们需要监控
    Docker 应用程序的各种信息。
- en: 'In this section, we will set up the following components of Graphite to create
    a minimal stack:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设置 Graphite 的以下组件，以创建一个最小化的堆栈：
- en: '**carbon-cache**: This is the Graphite component that receives metrics over
    the network. This implements the simple plaintext protocol described earlier.
    It can also listen to a binary-based protocol called the pickle protocol, which
    is a more advanced but smaller and optimized format to receive metrics.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**carbon-cache**：这是接收网络上传输的度量数据的 Graphite 组件。它实现了前面描述的简单纯文本协议。它还可以监听一种基于二进制的协议，叫做
    pickle 协议，这是一种更先进但更小巧且优化过的格式，用于接收度量数据。'
- en: '**whisper**: This is a file-based bounded time series database in which the
    carbon-cache persists the metrics it receives. Its bounded or fixed-size nature
    makes it an ideal solution for monitoring. Over time, the metrics that we monitor
    will accumulate. Hence, the size of our database will just keep increasing so
    that you will need to monitor it as well! However, in practice we are mostly interested
    in monitoring our application until a fixed point. Given this assumption, we can
    plan the resource requirements of our whisper database beforehand and not think
    about it this much as the operation of our Docker application continues.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**whisper**：这是一个基于文件的有界时间序列数据库，carbon-cache 在其中持久化它接收到的指标。它的有界或固定大小特性使其成为监控的理想解决方案。随着时间的推移，我们监控的指标将不断积累。因此，我们的数据库大小将不断增加，你需要对其进行监控！然而，在实践中，我们大多数情况下关心的是在固定时间点之前监控我们的应用程序。基于这一假设，我们可以提前规划
    whisper 数据库的资源需求，随着 Docker 应用程序的运行而不必过多考虑它。'
- en: '**graphite-web**: This reads the whisper database to render graphs and dashboards.
    It is also optimized to create such visualizations in real time by querying carbon-cache
    endpoints to display data that is yet to be persistent in the whisper database
    as well.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**graphite-web**：该组件读取 whisper 数据库以渲染图表和仪表板。它还经过优化，可以通过查询 carbon-cache 端点实时创建此类可视化，显示尚未在
    whisper 数据库中持久化的数据。'
- en: Note
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There are other components in carbon, such as carbon-aggregator and carbon-relay.
    These are the components that are needed in order to scale out Graphite effectively
    as the number of metrics you measure grows. More information about these components
    can be found at [https://github.com/graphite-project/carbon](https://github.com/graphite-project/carbon).
    For now, we will focus on deploying just the carbon-cache to create a simple Graphite
    cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: carbon 中还有其他组件，如 carbon-aggregator 和 carbon-relay。这些组件对于在你测量的指标数量增长时有效地扩展 Graphite
    至关重要。有关这些组件的更多信息，请访问 [https://github.com/graphite-project/carbon](https://github.com/graphite-project/carbon)。目前，我们将专注于仅部署
    carbon-cache 来创建一个简单的 Graphite 集群。
- en: 'The next few steps describe how to deploy the carbon-cache and a whisper database:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几个步骤描述了如何部署 carbon-cache 和 whisper 数据库：
- en: 'First, prepare a Docker image for carbon to dogfood our Docker host deployments.
    Create the following `Dockerfile` to prepare this image:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，为 carbon 准备一个 Docker 镜像，以便在我们的 Docker 主机部署中使用。创建以下 `Dockerfile` 来准备此镜像：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, build the `Dockerfile` we created earlier as the `hubuser/carbon` image,
    as follows:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，构建我们之前创建的 `Dockerfile` 作为 `hubuser/carbon` 镜像，如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Inside the `carbon.conf` configuration file, we will configure the carbon-cache
    to use the Docker volume `/whisper` as the whisper database. The following is
    the content describing this setting:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `carbon.conf` 配置文件中，我们将配置 carbon-cache 使用 Docker 卷 `/whisper` 作为 whisper 数据库。以下是描述此设置的内容：
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After building the `hubuser/carbon` image, we will prepare a whisper database
    by creating a data container. Type the following command to accomplish this:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建 `hubuser/carbon` 镜像后，我们将通过创建数据容器来准备一个 whisper 数据库。输入以下命令来完成此操作：
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, run the carbon-cache endpoint attached to the data container we created
    earlier. We will use custom container names and publicly exposed ports so that
    we can send and read metrics from it, as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行附加到我们之前创建的数据容器的 carbon-cache 端点。我们将使用自定义容器名称和公开端口，以便可以从中发送和读取指标，具体如下：
- en: '[PRE5]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now have a place to send all our Docker-related metrics that we will gather
    later. In order to make use of the metrics we stored, we need a way to read and
    visualize them. We will now deploy `graphite-web` to visualize what is going on
    with our Docker containers. The following are the steps to deploy graphite-web
    in our Docker hosts:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个可以发送我们稍后收集的所有 Docker 相关指标的地方。为了利用我们存储的指标，我们需要一种方法来读取和可视化它们。我们将部署 `graphite-web`
    来可视化我们 Docker 容器的运行情况。以下是在我们的 Docker 主机上部署 graphite-web 的步骤：
- en: 'Build `Dockerfile` as `hubuser/graphite-web` to prepare a Docker image to deploy
    graphite-web via the following code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 `Dockerfile` 为 `hubuser/graphite-web`，以准备一个 Docker 镜像来通过以下代码部署 graphite-web：
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding Docker image refers to `local_settings.py` to configure graphite-web.
    Place the following annotations to link the carbon-cache container and whisper
    volume:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述 Docker 镜像引用了 `local_settings.py` 来配置 graphite-web。请添加以下注释来链接 carbon-cache
    容器和 whisper 卷：
- en: '[PRE7]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After preparing the `Dockerfile` and `local_settings.py` configuration files,
    build the `hubuser/graphite-web` Docker image with the following command:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在准备好 `Dockerfile` 和 `local_settings.py` 配置文件后，使用以下命令构建 `hubuser/graphite-web`
    Docker 镜像：
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, run the `hubuser/graphite-web` Docker image linked with the carbon-cache
    container and whisper volume by typing the following command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行与 carbon-cache 容器和 whisper 卷链接的 `hubuser/graphite-web` Docker 镜像，执行以下命令：
- en: '[PRE9]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `SECRET_KEY` environment variable is a necessary component to group together
    multiple graphite-web instances when you decide to scale out. More graphite-web
    settings can be found at [http://graphite.readthedocs.org/en/latest/config-local-settings.html](http://graphite.readthedocs.org/en/latest/config-local-settings.html).
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`SECRET_KEY` 环境变量是将多个 graphite-web 实例组合在一起的必要组件，当你决定扩展时，它是必不可少的。更多 graphite-web
    设置的相关信息可以在 [http://graphite.readthedocs.org/en/latest/config-local-settings.html](http://graphite.readthedocs.org/en/latest/config-local-settings.html)
    找到。'
- en: 'Now that we have a complete Graphite deployment, we can run some preliminary
    tests to see it in action. We will test this by populating the whisper database
    with random data. Type the following command to send random metrics called `local.random`
    to the carbon-cache endpoint:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了 Graphite 的部署，可以进行一些初步的测试，查看其实际运行效果。我们将通过向 whisper 数据库填充随机数据来进行测试。输入以下命令，将名为
    `local.random` 的随机指标发送到 carbon-cache 端点：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, confirm that the data is persistent by visiting our `hubuser/graphite-web`''s
    composer URL [http://dockerhost/compose](http://dockerhost/compose). Go to the
    **Tree** tab and then expand the `Graphite/local` folder to get the `random` metric.
    The following is a graph that we will see on our graphite-web deployment:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过访问我们的 `hubuser/graphite-web` 的 composer URL [http://dockerhost/compose](http://dockerhost/compose)
    来确认数据是否持久化。进入 **Tree** 选项卡，展开 `Graphite/local` 文件夹，获取 `random` 指标。以下是我们在 graphite-web
    部署中看到的图表：
- en: '![Collecting metrics to Graphite](img/00015.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![将指标收集到 Graphite](img/00015.jpeg)'
- en: Graphite in production
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产环境中的 Graphite
- en: In production, this simple Graphite setup will reach its limits as we monitor
    more and more metrics in our Docker deployment. We need to scale this out in order
    to keep up with the increased number of metrics that we monitor. With this, you
    need to deploy Graphite with a cluster setup.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，随着我们在 Docker 部署中监控越来越多的指标，这种简单的 Graphite 配置将达到其极限。为了跟上监控指标数量的增加，我们需要对其进行扩展。为此，你需要以集群方式部署
    Graphite。
- en: To scale out the metric processing capacity of carbon-cache, we need to augment
    it with a carbon-relay and carbon-aggregator. For graphite-web to be more responsive,
    we need to scale it out horizontally along with other caching components, such
    as memcached. We also need to add another graphite-web instance that connects
    to other graphite-web instances to create a unified view of all the metrics. The
    whisper databases will be co-located with a carbon-cache and graphite-web, so
    it will scale-out naturally along with them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展 carbon-cache 的指标处理能力，我们需要用 carbon-relay 和 carbon-aggregator 进行增强。为了让 graphite-web
    更具响应性，我们需要将其与其他缓存组件（如 memcached）一起进行水平扩展。我们还需要添加另一个 graphite-web 实例，它连接到其他 graphite-web
    实例，从而创建一个统一的指标视图。whisper 数据库将与 carbon-cache 和 graphite-web 一起共同部署，因此它会随着它们的扩展而自然扩展。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information on how to scale out a Graphite cluster in production is found
    at [http://graphite.readthedocs.org](http://graphite.readthedocs.org).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何在生产环境中扩展 Graphite 集群的更多信息，请访问 [http://graphite.readthedocs.org](http://graphite.readthedocs.org)。
- en: Monitoring with collectd
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 collectd 进行监控
- en: We finished setting up a place to send all our Docker-related data to. Now,
    it is time to actually fetch all the data related to our Docker applications.
    In this section, we will use `collectd`, a popular system statistics collection
    daemon. It is a very lightweight and high-performance C program. This makes it
    a noninvasive monitoring software because it doesn't consume many resources from
    the system it monitors. Being lightweight, it is very simple to deploy as it requires
    minimum dependencies. It has a wide variety of plugins to monitor almost every
    component of our system.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了设置一个接收所有 Docker 相关数据的位置。现在，到了真正获取我们 Docker 应用程序相关数据的时候。在本节中，我们将使用 `collectd`，一个流行的系统统计收集守护进程。它是一个非常轻量且高性能的
    C 程序，这使得它成为一种非侵入式监控软件，因为它不会消耗被监控系统的太多资源。由于其轻量性，它非常易于部署，所需的依赖很少。它有各种各样的插件，几乎可以监控我们系统的每个组件。
- en: 'Let''s begin monitoring our Docker host. Follow the next few steps to install
    `collectd` and send the metrics to our Graphite deployment:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始监控我们的Docker主机。按照接下来的步骤安装`collectd`并将度量数据发送到我们的Graphite部署：
- en: 'First, install `collectd` in our Docker host by typing the following command:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过输入以下命令在我们的Docker主机上安装`collectd`：
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, create a minimum `collectd` configuration to send data to our Graphite
    deployment. You may recall from before that we exposed the carbon-cache''s default
    plaintext protocol port (`2003`). Write the following configuration entry in `/etc/collectd/collectd.conf`
    to set this up:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个最简化的`collectd`配置，将数据发送到我们的Graphite部署。你可能还记得之前我们暴露了carbon-cache的默认明文协议端口（`2003`）。在`/etc/collectd/collectd.conf`中写入以下配置项来设置：
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, it is time to measure a few things from our Docker host. Load the following
    `collectd` plugins by placing the next few lines in `/etc/collectd/collectd.conf`:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，轮到我们从Docker主机上收集一些数据了。通过将以下几行添加到`/etc/collectd/collectd.conf`中，加载相应的`collectd`插件：
- en: '[PRE13]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After finishing the configuration, restart `collectd` by typing the following
    command:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置完成后，使用以下命令重启`collectd`：
- en: '[PRE14]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, let''s create a visualization dashboard in our graphite-web deployment
    to look at the preceding metrics. Go to [http://dockerhost/dashboard](http://dockerhost/dashboard),
    click on **Dashboard**, and then on the **Edit Dashboard** link. It will prompt
    us with a text area to place a dashboard definition. Paste the following JSON
    text in this text area to create our preliminary dashboard:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们在graphite-web部署中创建一个可视化仪表盘，用来查看之前的度量数据。访问[http://dockerhost/dashboard](http://dockerhost/dashboard)，点击**Dashboard**，然后点击**编辑仪表盘**链接。系统会提示我们输入一个文本区域以放置仪表盘定义。将以下JSON文本粘贴到此文本区域，创建我们的初步仪表盘：
- en: '[PRE15]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We now have a basic monitoring stack for our Docker host. The last step in
    the previous section will show a dashboard that looks something similar to the
    following screenshot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经为Docker主机构建了一个基本的监控堆栈。上一节的最后一步将展示一个类似于以下屏幕截图的仪表盘：
- en: '![Monitoring with collectd](img/00016.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![使用collectd进行监控](img/00016.jpeg)'
- en: Collecting Docker-related data
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集与Docker相关的数据
- en: 'Now, we will measure some basic metrics that govern the performance of our
    application. But how do we drill down to further details on the containers running
    in our Docker hosts? Inside our Debian Jessie Docker host, our containers run
    under the `docker-[container_id].scope` control group. This information is found
    at `/sys/fs/cgroup/cpu,cpuacct/system.slice` in our Docker host''s sysfs. Fortunately,
    `collectd` has a `cgroups` plugin that interfaces with the sysfs information exposed
    earlier. The next few steps will show you how to use this plugin to measure the
    CPU performance of our running Docker containers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将测量一些基本的度量指标，这些指标决定了应用程序的性能。但是，如何深入了解在Docker主机中运行的容器呢？在我们的Debian Jessie
    Docker主机中，容器运行在`docker-[container_id].scope`控制组下。这些信息可以在Docker主机的sysfs路径`/sys/fs/cgroup/cpu,cpuacct/system.slice`中找到。幸运的是，`collectd`提供了一个`cgroups`插件，用于与先前暴露的sysfs信息接口。接下来的步骤将展示如何使用该插件来测量我们正在运行的Docker容器的CPU性能：
- en: 'First, insert the following lines in `/etc/collectd/collectd.conf`:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将以下行插入`/etc/collectd/collectd.conf`中：
- en: '[PRE16]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, restart collectd by typing the following command:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令重启collectd：
- en: '[PRE17]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Finally, wait for a few minutes for Graphite to receive enough metrics from
    `collectd` so that we can get an initial feel to visualize our Docker container's
    CPU metrics.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，等待几分钟，让Graphite从`collectd`接收到足够的度量数据，这样我们就可以初步了解如何可视化我们Docker容器的CPU度量。
- en: 'We can now check the CPU metrics of our Docker containers by querying for the
    `dockerhost.cgroups-docker*.*` metrics on our Graphite deployment''s render API.
    The following is the image produced by the render API URL `http://dockerhost/render/?target=dockerhost.cgroups-docker-*.*`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过查询Graphite部署的渲染API上的`dockerhost.cgroups-docker*.*`度量指标，来查看Docker容器的CPU度量数据。以下是通过渲染API
    URL `http://dockerhost/render/?target=dockerhost.cgroups-docker-*.*`生成的图像：
- en: '![Collecting Docker-related data](img/00017.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![收集与Docker相关的数据](img/00017.jpeg)'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information about the `cgroups` plugin can be found in the `collectd` documentation
    page at [https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups](https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于`cgroups`插件的信息，可以在`collectd`文档页面中找到，链接为[https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups](https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups)。
- en: 'Currently, the `cgroups` plugin only measures the CPU metrics of our running
    Docker containers. There is some work in progress, but it is not yet ready at
    the time of this book''s writing. Fortunately, there is a Python-based `collectd`
    plugin that interfaces itself to `docker stats`. The following are the steps needed
    to set up this plugin:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，`cgroups` 插件仅测量我们运行的 Docker 容器的 CPU 度量。虽然有一些工作正在进行中，但在本书撰写时尚未准备好。幸运的是，有一个基于
    Python 的 `collectd` 插件，它与 `docker stats` 进行接口。以下是设置该插件所需的步骤：
- en: 'First, download the following dependencies to be able to run the plugin:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，下载以下依赖项以便能够运行插件：
- en: '[PRE18]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, download and install the plugin from its GitHub page:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从其 GitHub 页面下载并安装插件：
- en: '[PRE19]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add the following lines to `/etc/collectd/collectd.conf` to configure the plugin:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下行添加到 `/etc/collectd/collectd.conf` 以配置插件：
- en: '[PRE20]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, restart `collectd` to reflect the preceding configuration changes
    via the following command:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过以下命令重启 `collectd` 以反映前述配置更改：
- en: '[PRE21]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There is a case in which we don't want to install a whole stack of Python to
    just query the Docker container stat's endpoint. In this case, we can use the
    lower-level `curl_json` plugin of `collectd` to gather statistics about our container.
    We can configure it to make a request against the container stat's endpoint and
    parse the resulting JSON into a set of `collectd` metrics. More on how the plugin
    works can be found at [https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json](https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种情况，我们不希望仅为了查询 Docker 容器的统计信息端点而安装完整的 Python 堆栈。在这种情况下，我们可以使用 `collectd` 的低级插件
    `curl_json` 来收集容器的统计数据。我们可以将其配置为向容器统计端点发起请求，并将返回的 JSON 解析为一组 `collectd` 度量。有关该插件如何工作的更多信息，可以参考
    [https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json](https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json)。
- en: 'In the following screenshot, we can explore the metrics given out by the `cgroups`
    plugin from our Graphite deployment at `http://docker/compose`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以查看 `cgroups` 插件从我们的 Graphite 部署 `http://docker/compose` 中提供的度量：
- en: '![Collecting Docker-related data](img/00018.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![收集与 Docker 相关的数据](img/00018.jpeg)'
- en: Running collectd inside Docker
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Docker 中运行 collectd
- en: 'If we want to deploy our `collectd` configuration similarly to our applications,
    we can run it inside Docker as well. The following is an initial `Dockerfile`
    that we can use to start with deploying `collectd` as a running Docker container:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望像部署应用程序一样部署 `collectd` 配置，我们也可以将其运行在 Docker 中。以下是一个初始的 `Dockerfile`，我们可以用它开始将
    `collectd` 部署为正在运行的 Docker 容器：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Most plugins look at the `/proc` and `/sys` filesystems. In order for `collectd`
    inside a Docker container to access these files, we need to mount them as Docker
    volumes, such as `--volume /proc:/host/proc`. However, most plugins currently
    read the hardwired `/proc` and `/sys` paths. There is ongoing discussion to make
    this configurable. Refer to this GitHub page to track its progress: [https://github.com/collectd/collectd/issues/1169](https://github.com/collectd/collectd/issues/1169).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数插件查看 `/proc` 和 `/sys` 文件系统。为了让 `collectd` 在 Docker 容器内访问这些文件，我们需要将它们挂载为 Docker
    卷，如 `--volume /proc:/host/proc`。然而，目前大多数插件仍然读取硬编码的 `/proc` 和 `/sys` 路径。关于使其可配置的讨论正在进行中。请参考此
    GitHub 页面以跟踪进展：[https://github.com/collectd/collectd/issues/1169](https://github.com/collectd/collectd/issues/1169)。
- en: Consolidating logs in an ELK stack
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 ELK 堆栈中合并日志
- en: Not all statuses of our Docker hosts and containers are readily available to
    be queried with our monitoring solution in collectd and Graphite. Some events
    and metrics are only available as raw lines of text in log files. We need to transform
    these raw and unstructured logs to meaningful metrics. Similar to raw metrics,
    we can later ask higher-level questions on what is happening in our Docker-based
    application through analytics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Docker 主机和容器的所有状态并不能立即通过我们的监控解决方案（collectd 和 Graphite）查询到。有些事件和度量仅以原始文本行的形式出现在日志文件中。我们需要将这些原始且无结构的日志转化为有意义的度量。类似于原始度量，我们以后可以通过分析提出更高层次的问题，了解在我们的基于
    Docker 的应用程序中发生了什么。
- en: 'The ELK stack is a popular combination suite from Elastic that addresses these
    problems. Each letter in the acronym represents each of its components. The following
    is a description of each of them:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ELK 堆栈是 Elastic 提供的一套流行的组合工具，解决了这些问题。该缩写中的每个字母代表其组件。以下是每个组件的描述：
- en: '**Logstash**: Logstash is the component that is used to collect and manage
    logs and events. It is the central point that we use to collect all the logs from
    different log sources, such as multiple Docker hosts and containers running in
    our deployment. We can also use Logstash to transform and annotate the logs we
    receive. This allows us to search and explore the richer features of our logs
    later.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Logstash**：Logstash 是用于收集和管理日志与事件的组件。它是我们用来收集来自不同日志源的所有日志的中心点，例如我们部署中的多个
    Docker 主机和容器。我们还可以使用 Logstash 来转换和注释接收到的日志。这使我们能够在后续探索日志的更丰富特性时进行搜索。'
- en: '**Elasticsearch**: Elasticsearch is a distributed search engine that is highly
    scalable. Its sharding capabilities allow us to grow and scale our log storage
    as we continue to receive more and more logs from our Docker containers. Its database
    engine is document-oriented. This allows us to store and annotate logs as we see
    fit as we continue to discover more insights about the events we are managing
    in our large Docker deployments.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**：Elasticsearch 是一个分布式搜索引擎，具有高度可扩展性。它的分片功能使我们能够随着 Docker 容器不断发送更多日志而扩展日志存储。它的数据库引擎是面向文档的，这使我们能够在继续发掘关于我们在大型
    Docker 部署中管理的事件的更多洞察时，灵活地存储和注释日志。'
- en: '**Kibana**: Kibana is an analytics and search dashboard for Elasticsearch.
    Its simplicity allows us to create dashboards for our Docker applications. However,
    Kibana is also very flexible to customize, so we can build dashboards that can
    provide valuable insights to people who want to understand our Docker-based applications,
    whether it is a low-level technical detail or higher-level business need.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kibana**：Kibana 是一个 Elasticsearch 的分析和搜索仪表盘。它的简便性使我们能够为 Docker 应用程序创建仪表盘。然而，Kibana
    也非常灵活，可以进行定制，因此我们可以构建出可以为需要了解我们基于 Docker 的应用程序的人提供有价值洞察的仪表盘，无论是低级的技术细节还是更高层次的业务需求。'
- en: 'In the remaining parts of this section, we will set up each of these components
    and send our Docker host and container logs to it. The next few steps describe
    how to build the ELK stack:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们将设置这些组件，并将我们的 Docker 主机和容器日志发送到它们。接下来的几个步骤将描述如何构建 ELK 堆栈：
- en: 'First, launch the official Elasticsearch image in our Docker host. We will
    put a container name so that we can link it easily in the later steps, as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在我们的 Docker 主机上启动官方的 Elasticsearch 镜像。我们将为其指定一个容器名称，以便后续步骤中可以轻松链接它，如下所示：
- en: '[PRE23]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we will run Kibana''s official Docker image by linking it against the
    Elasticsearch container we created in the previous step. Note that we publicly
    mapped the exposed port `5601` to port `80` in our Docker host so that the URL
    for Kibana is prettier, as follows:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过将其链接到前面创建的 Elasticsearch 容器来运行 Kibana 的官方 Docker 镜像。请注意，我们将公开暴露的端口
    `5601` 映射到 Docker 主机中的端口 `80`，以便 Kibana 的 URL 更加简洁，如下所示：
- en: '[PRE24]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, prepare our Logstash Docker image and configuration. Prepare the following
    `Dockerfile` to create the Docker image:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，准备我们的 Logstash Docker 镜像和配置。准备以下的 `Dockerfile` 来创建 Docker 镜像：
- en: '[PRE25]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this Docker image, configure Logstash as a Syslog server. This explains
    the exposed UDP port in the preceding `Dockerfile`. As for the `logstash.conf`
    file, the following is the basic configuration to make it listen as a Syslog server.
    The latter part of the configuration shows that it sends logs to an Elasticsearch
    called `elasticsearch`. We will use this as the hostname when we link the Elasticsearch
    container we ran earlier:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此 Docker 镜像中，将 Logstash 配置为 Syslog 服务器。这解释了在前面的 `Dockerfile` 中暴露的 UDP 端口。至于
    `logstash.conf` 文件，以下是使其作为 Syslog 服务器监听的基本配置。配置的后半部分表明它将日志发送到名为 `elasticsearch`
    的 Elasticsearch。我们将在链接先前运行的 Elasticsearch 容器时使用此作为主机名：
- en: '[PRE26]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Logstash has a wealth of plugins so that it can read a wide variety of log data
    sources. In particular, it has a `collectd` codec plugin. With this, we can use
    an ELK stack instead of Graphite to monitor our metrics.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Logstash 拥有大量插件，可以读取各种日志数据源。特别是，它有一个 `collectd` 编解码器插件。通过这个插件，我们可以使用 ELK 堆栈来代替
    Graphite 监控我们的指标。
- en: For more information on how to do this setup, visit [https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html).
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更多关于如何进行此设置的信息，请访问 [https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html)。
- en: 'Now that we have prepared all the files needed, type the following command
    to create it as the `hubuser/logstash` Docker image:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有需要的文件，输入以下命令将其创建为 `hubuser/logstash` Docker 镜像：
- en: '[PRE27]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run Logstash with the following command. Note that we are exposing port `1514`
    to the Docker host as the Syslog port. We also linked the Elasticsearch container
    named `elastic` that we created earlier. The target name is set to `elasticsearch`
    as it is the hostname of Elasticsearch that we configured earlier in `logstash.conf`
    to send the logs to:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 Logstash。请注意，我们正在将端口`1514`暴露给 Docker 主机，作为 Syslog 端口。我们还链接了之前创建的名为
    `elastic` 的 Elasticsearch 容器。目标名称设置为 `elasticsearch`，因为它是我们在 `logstash.conf` 中配置的
    Elasticsearch 主机名，用于将日志发送到该主机：
- en: '[PRE28]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, let''s configure our Docker host''s Syslog service to forward it to our
    Logstash container. As a basic configuration, we can set up Rsyslog to forward
    all the logs. This will include the logs coming from the Docker engine daemon
    as well. To do this, create the `/etc/rsyslog.d/100-logstash.conf` file with the
    following content:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们配置 Docker 主机的 Syslog 服务，将日志转发到 Logstash 容器。作为基本配置，我们可以设置 Rsyslog 将所有日志转发。这将包括来自
    Docker 引擎守护进程的日志。为此，创建一个 `/etc/rsyslog.d/100-logstash.conf` 文件，并包含以下内容：
- en: '[PRE29]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, restart Syslog to load the changes in the previous step by typing
    the following command:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过输入以下命令重启 Syslog，以加载前一步骤中的更改：
- en: '[PRE30]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We now have a basic functioning ELK stack. Let''s now test it by sending a
    message to Logstash and seeing it appear in our Kibana dashboard:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经有了一个基本运行的 ELK 堆栈。现在让我们通过发送一条消息到 Logstash 并查看它是否出现在 Kibana 仪表板中来进行测试：
- en: 'First, type the following command to send a test message:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，输入以下命令发送一条测试消息：
- en: '[PRE31]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Next, go to our Kibana dashboard by visiting `http://dockerhost`. Kibana will
    now ask us to set the default index. Use the following default values and click
    on **Create** to start indexing:![Consolidating logs in an ELK stack](img/00019.jpeg)
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，访问我们的 Kibana 仪表板，访问 `http://dockerhost`。Kibana 现在会要求我们设置默认索引。使用以下默认值并点击
    **Create** 开始索引：![Consolidating logs in an ELK stack](img/00019.jpeg)
- en: Go to `http://dockerhost/#discover` and type `elasticsearch` in the search.
    The following screenshot shows the Syslog message we generated earlier:![Consolidating
    logs in an ELK stack](img/00020.jpeg)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 `http://dockerhost/#discover` 并在搜索框中输入 `elasticsearch`。以下截图显示了我们之前生成的 Syslog
    消息：![Consolidating logs in an ELK stack](img/00020.jpeg)
- en: Note
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There are lot more things we can do on the ELK stack to optimize our logging
    infrastructure. We can add Logstash plugins and filters to annotate the logs we
    receive from our Docker hosts and containers. Elasticsearch can be scaled out
    and tuned to increase its capacity as our logging needs increase. We can create
    Kibana dashboards to share metrics. To find out more details on how to tune our
    ELK stack, visit Elastic's guides at [https://www.elastic.co/guide](https://www.elastic.co/guide).
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 ELK 堆栈上，我们可以做很多事情来优化日志基础设施。我们可以添加 Logstash 插件和过滤器来注释从 Docker 主机和容器接收到的日志。随着日志需求的增加，Elasticsearch
    可以进行扩展和调优，以提高其容量。我们可以创建 Kibana 仪表板来共享指标。欲了解更多如何调优 ELK 堆栈的细节，请访问 Elastic 的 [https://www.elastic.co/guide](https://www.elastic.co/guide)。
- en: Forwarding Docker container logs
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转发 Docker 容器日志
- en: 'Now that we have a basic functional ELK stack, we can start forwarding our
    Docker logs to it. From Docker 1.7 onwards, support for custom logging drivers
    has been available. In this section, we will configure our Docker host to use
    the syslog driver. By default, Syslog events from Docker will go to the Docker
    host''s Syslog service and since we configured Syslog to forward to our ELK stack,
    we will see the container logs there. Follow the next few steps to start receiving
    our container logs in the ELK stack:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个基本功能的 ELK 堆栈，我们可以开始将 Docker 日志转发到它。从 Docker 1.7 版本开始，Docker 支持自定义日志驱动程序。在这一部分，我们将配置
    Docker 主机使用 syslog 驱动程序。默认情况下，Docker 的 Syslog 事件会发送到 Docker 主机的 Syslog 服务，并且由于我们已将
    Syslog 配置为转发到我们的 ELK 堆栈，因此我们可以在那里看到容器日志。按照以下步骤开始在 ELK 堆栈中接收容器日志：
- en: 'The Docker engine service is configured via Systemd on our Debian Jessie host.
    To update how it runs in our Docker host, create a Systemd unit file called `/etc/systemd/system/docker.service.d/10-syslog.conf`
    with the following content:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker 引擎服务通过 Systemd 在我们的 Debian Jessie 主机上配置。为了更新它在 Docker 主机中的运行方式，创建一个名为
    `/etc/systemd/system/docker.service.d/10-syslog.conf` 的 Systemd 单元文件，并包含以下内容：
- en: '[PRE32]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Apply the changes on how we will run Docker in our host by reloading the Systemd
    configuration. The following command will do this:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重新加载 Systemd 配置，应用我们将如何在主机中运行 Docker 的更改。以下命令将完成此操作：
- en: '[PRE33]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, restart the Docker engine daemon by issuing the following command:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过执行以下命令重启 Docker 引擎守护进程：
- en: '[PRE34]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Optionally, apply any Logstash filtering if we want to do custom annotations
    on our Docker container's logs.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们希望对 Docker 容器的日志进行自定义注释，可选择应用任何 Logstash 过滤。
- en: 'Now, any standard output and error streams coming out from our Docker container
    should be captured to our ELK stack. We can do some preliminary tests to confirm
    that the setup works. Type the following command to create a test message from
    Docker:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，任何来自我们 Docker 容器的标准输出和错误流都应该被捕获到我们的 ELK 堆栈中。我们可以做一些初步测试，以确认设置是否有效。输入以下命令从
    Docker 创建一个测试消息：
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `docker run` command also supports the `--log-driver` and `--log-opt=[]`
    command-line options to set up the logging driver only for the container we want
    to run. We can use it to further tune our logging policies for each Docker container
    running in our Docker host.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run` 命令还支持 `--log-driver` 和 `--log-opt=[]` 命令行选项，仅为我们要运行的容器设置日志驱动程序。我们可以使用它进一步调整我们在
    Docker 主机中运行的每个 Docker 容器的日志策略。'
- en: 'After typing the preceding command, our message should now be stored in Elasticsearch.
    Let''s go to our Kibana endpoint in `http://dockerhost`, and search for the word
    `message to elk` in the textbox. It should give the Syslog entry for the message
    we sent earlier. The following screenshot is what the search result should look
    like in our Kibana results:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输入前面的命令后，我们的消息现在应该存储在 Elasticsearch 中。我们可以访问 `http://dockerhost` 上的 Kibana 端点，并在文本框中搜索
    `message to elk`。它应该会显示我们之前发送的消息的 Syslog 条目。以下截图是 Kibana 搜索结果应该显示的内容：
- en: '![Forwarding Docker container logs](img/00021.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![转发 Docker 容器日志](img/00021.jpeg)'
- en: In the preceding screenshot, we can see the message we sent. There is also other
    information about Syslog. Docker's Syslog driver sets the default syslog annotations
    on facility and severity as **system** and **informational**, respectively. In
    addition, the preceding program is set to **docker/c469a2dfdc9a**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到我们发送的消息。还有关于 Syslog 的其他信息。Docker 的 Syslog 驱动程序默认将设施和严重性注释设置为**系统**和**信息**，分别对应。除此之外，前面的程序被设置为**docker/c469a2dfdc9a**。
- en: The **c469a2dfdc9a** string is the container ID of the busybox image we ran
    earlier. The default program label for Docker containers is set in the `docker/<container-id>`
    format. All of the preceding default annotations can be configured by passing
    arguments to the `--log-opt=[]` option.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**c469a2dfdc9a** 字符串是我们之前运行的 busybox 镜像的容器 ID。Docker 容器的默认程序标签设置为 `docker/<container-id>`
    格式。所有前述的默认注释可以通过向 `--log-opt=[]` 选项传递参数来配置。'
- en: Note
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Aside from the Syslog and JSON file-logging drivers, Docker supports several
    other endpoints to send logs to. More information about all the logging drivers
    and their respective usage guides can be found in [https://docs.docker.com/reference/logging](https://docs.docker.com/reference/logging).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Syslog 和 JSON 文件日志驱动程序，Docker 还支持将日志发送到多个其他端点。有关所有日志驱动程序及其使用指南的更多信息，可以在 [https://docs.docker.com/reference/logging](https://docs.docker.com/reference/logging)
    中找到。
- en: Other monitoring and logging solutions
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他监控和日志解决方案
- en: There are several other solutions for us to deploy to monitor and log infrastructure
    to support our Docker-based application. Some of them already have built-in support
    for monitoring Docker containers. Others should be combined with other solutions,
    such as the ones we showed previously because they only focus on a specific part
    of monitoring or logging.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他解决方案可以部署来监控和记录基础设施，以支持基于 Docker 的应用程序。其中一些已经内建支持 Docker 容器的监控。其他的则需要与我们之前展示的其他解决方案结合使用，因为它们仅专注于监控或日志的特定部分。
- en: 'With others, we may have to do some workarounds. However, their benefits clearly
    outweigh the compromise we have to make. While the following list is not exhaustive,
    these are a few stacks we can explore to create our logging and monitoring solutions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他一些，我们可能需要做一些变通方法。然而，它们的好处显然超过了我们需要做出的妥协。虽然以下列表并不详尽，但这些是我们可以探索的一些堆栈，以创建我们的日志记录和监控解决方案：
- en: cAdvisor ([http://github.com/google/cadvisor](http://github.com/google/cadvisor))
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cAdvisor ([http://github.com/google/cadvisor](http://github.com/google/cadvisor))
- en: InfluxDB ([http://influxdb.com](http://influxdb.com))
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InfluxDB ([http://influxdb.com](http://influxdb.com))
- en: Sensu ([http://sensuapp.org](http://sensuapp.org))
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensu ([http://sensuapp.org](http://sensuapp.org))
- en: Fluentd ([http://www.fluentd.org/](http://www.fluentd.org/))
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd ([http://www.fluentd.org/](http://www.fluentd.org/))
- en: Graylog ([http://www.graylog.org](http://www.graylog.org))
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graylog ([http://www.graylog.org](http://www.graylog.org))
- en: Splunk ([http://www.splunk.com](http://www.splunk.com))
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Splunk ([http://www.splunk.com](http://www.splunk.com))
- en: Sometimes, our operations staff and developers running and developing our Docker
    applications are not yet mature enough or do not want to focus on maintaining
    such monitoring and logging infrastructures. There are several hosted monitoring
    and logging platforms that we can use so that we can focus on actually writing
    and improving the performance of our Docker application.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们的运营人员和开发者在运行和开发 Docker 应用时还不够成熟，或者他们不想集中精力维护这些监控和日志基础设施。有几个托管的监控和日志平台可以供我们使用，这样我们就可以专注于实际编写和提升
    Docker 应用的性能。
- en: 'Some of them work with existing monitoring and logging agents, such as Syslog
    and collectd. With others, we may have to download and deploy their agents to
    be able to forward the events and metrics to their hosted platform. The following
    is a nonexhaustive list of some solutions we may want to consider:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些与现有的监控和日志代理（如 Syslog 和 collectd）兼容。对于其他一些，我们可能需要下载并部署它们的代理，才能将事件和指标转发到它们的托管平台。以下是我们可能希望考虑的一些解决方案的非详尽列表：
- en: New Relic ([http://www.newrelic.com](http://www.newrelic.com))
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: New Relic ([http://www.newrelic.com](http://www.newrelic.com))
- en: Datadog ([http://www.datadoghq.com](http://www.datadoghq.com))
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Datadog ([http://www.datadoghq.com](http://www.datadoghq.com))
- en: Librato ([http://www.librato.com](http://www.librato.com))
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Librato ([http://www.librato.com](http://www.librato.com))
- en: Elastic's Found ([http://www.elastic.co/found](http://www.elastic.co/found))
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elastic's Found ([http://www.elastic.co/found](http://www.elastic.co/found))
- en: Treasure Data ([http://www.treasuredata.com](http://www.treasuredata.com))
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Treasure Data ([http://www.treasuredata.com](http://www.treasuredata.com))
- en: Splunk Cloud ([http://www.splunk.com](http://www.splunk.com))
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Splunk Cloud ([http://www.splunk.com](http://www.splunk.com))
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We now know why it is important to monitor our Docker deployments in a scalable
    and accessible manner. We deployed collectd and Graphite to monitor our Docker
    container's metrics. We rolled out an ELK stack to consolidate the logs coming
    from various Docker hosts and containers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，以可扩展和可访问的方式监控 Docker 部署是很重要的。我们部署了 collectd 和 Graphite 来监控 Docker 容器的指标。我们推出了
    ELK 堆栈，用于整合来自不同 Docker 主机和容器的日志。
- en: In addition to raw metrics and events, it is also important to know what it
    means for our application. Graphite-web and Kibana allow us to create custom dashboards
    and analysis to provide insight in to our Docker applications. With these monitoring
    tools and skills in our arsenal, we should be able to operate and run our Docker
    deployments well in production.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始的指标和事件，了解这些数据对我们的应用意味着什么也很重要。Graphite-web 和 Kibana 允许我们创建自定义的仪表板和分析，以提供关于
    Docker 应用的洞察。凭借这些监控工具和技能，我们应该能够在生产环境中良好地操作和运行我们的 Docker 部署。
- en: In the next chapter, we will start doing performance tests and benchmark how
    our Docker applications fare well with a high load. We should be able to use the
    monitoring systems we deployed to observe and validate our performance testing
    activities there.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始进行性能测试，并基准测试我们的 Docker 应用在高负载下的表现。我们应该能够利用我们部署的监控系统来观察和验证我们的性能测试活动。
