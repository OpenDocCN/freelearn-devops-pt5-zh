- en: Chapter 4. Monitoring Docker Hosts and Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know some ways to optimize our Docker deployments. We also know how to
    scale to improve performance. But how do we know that our tuning assumptions were
    correct? Being able to monitor our Docker infrastructure and application is important
    to figure out why and when we need to optimize. Measuring how our system is performing
    allows us to identify its limits to scale and tune accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to monitoring low-level information about Docker, it is also important
    to measure the business-related performance of our application. By tracing the
    value stream of our application, we can correlate business-related metrics to
    system-level ones. With this, our Docker development and operations teams can
    show their business colleagues how Docker saves their organization's costs and
    increases business value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics about being able to monitor
    our Docker infrastructure and applications at scale:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting monitored data in Graphite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring Docker with collectd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consolidating logs in an ELK stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending logs from Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring is important as it provides a source of feedback on the Docker deployment
    that we built. It answers several questions about our application from low-level
    operating system performance to high-level business targets. Having proper instrumentation
    inserted in our Docker hosts allows us to identify our system's state. We can
    use this source of feedback to identify whether our application is behaving as
    originally planned.
  prefs: []
  type: TYPE_NORMAL
- en: If our initial hypothesis was incorrect, we can use the feedback data to revise
    our plan and change our system accordingly by tuning our Docker host and containers
    or updating our running Docker application. We can also use the same monitoring
    process to identify errors and bugs after our system is deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: Docker has built-in features to log and monitor. By default, a Docker host stores
    a Docker container's standard output and error streams to JSON files in `/var/lib/docker/<container_id>/<container_id>-json.log`.
    The `docker logs` command asks the Docker engine daemon to read the content of
    the files here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another monitoring facility is the docker stats command. This queries the Docker
    engine''s remote API''s `/containers/<container_id>/stats` endpoint to report
    runtime statistics about the running container''s control group regarding its
    CPU, memory, and network usage. The following is an example output of the `docker
    stats` command reporting the said metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The built-in `docker logs` and `docker stats` commands work well to monitor
    our Docker applications for development and small-scale deployments. When we get
    to a point in our production-grade Docker deployment where we manage tens, hundreds,
    or even thousands of Docker hosts, this approach will no longer be scalable. It
    is not feasible to log in to each of our thousand Docker hosts and type `docker
    logs` and `docker stats`.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this one by one also makes it difficult to create a more holistic picture
    of our entire Docker deployment. Also, not everyone interested in our Docker application's
    performance can log in to our Docker hosts. Our colleagues dealing with only the
    business aspect of our application may want to ask certain questions on how our
    application's deployment in Docker improves what our organization wants. However,
    they do not necessarily want to learn how to log in and start typing Docker commands
    in our infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is important to be able to consolidate all of the events and metrics
    from our Docker deployments into a centralized monitoring infrastructure. It allows
    our operations to scale by having a single point to ask what is happening to our
    system. A centralized dashboard also enables people outside our development and
    operations team, such as our business colleagues, to have access to the feedback
    provided by our monitoring system. The remaining sections will show you how to
    consolidate messages from `docker logs` and collect statistics from data sources
    such as `docker stats`.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting metrics to Graphite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin monitoring our Docker deployments, we must first set up an endpoint
    to send our monitored values to. Graphite is a popular stack for collecting various
    metrics. Its plaintext protocol is very popular because of its simplicity. Many
    third-party tools understand this simple protocol. Later, we will show you how
    easy it is to send data to Graphite after we finish setting it up.
  prefs: []
  type: TYPE_NORMAL
- en: Another feature of graphite is that it can render the data it gathers into graphs.
    We can then consolidate these graphs to build a dashboard. The dashboard we crafted
    in the end will show the various kinds of information that we need to monitor
    our Docker application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will set up the following components of Graphite to create
    a minimal stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '**carbon-cache**: This is the Graphite component that receives metrics over
    the network. This implements the simple plaintext protocol described earlier.
    It can also listen to a binary-based protocol called the pickle protocol, which
    is a more advanced but smaller and optimized format to receive metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**whisper**: This is a file-based bounded time series database in which the
    carbon-cache persists the metrics it receives. Its bounded or fixed-size nature
    makes it an ideal solution for monitoring. Over time, the metrics that we monitor
    will accumulate. Hence, the size of our database will just keep increasing so
    that you will need to monitor it as well! However, in practice we are mostly interested
    in monitoring our application until a fixed point. Given this assumption, we can
    plan the resource requirements of our whisper database beforehand and not think
    about it this much as the operation of our Docker application continues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**graphite-web**: This reads the whisper database to render graphs and dashboards.
    It is also optimized to create such visualizations in real time by querying carbon-cache
    endpoints to display data that is yet to be persistent in the whisper database
    as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are other components in carbon, such as carbon-aggregator and carbon-relay.
    These are the components that are needed in order to scale out Graphite effectively
    as the number of metrics you measure grows. More information about these components
    can be found at [https://github.com/graphite-project/carbon](https://github.com/graphite-project/carbon).
    For now, we will focus on deploying just the carbon-cache to create a simple Graphite
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few steps describe how to deploy the carbon-cache and a whisper database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, prepare a Docker image for carbon to dogfood our Docker host deployments.
    Create the following `Dockerfile` to prepare this image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, build the `Dockerfile` we created earlier as the `hubuser/carbon` image,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the `carbon.conf` configuration file, we will configure the carbon-cache
    to use the Docker volume `/whisper` as the whisper database. The following is
    the content describing this setting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After building the `hubuser/carbon` image, we will prepare a whisper database
    by creating a data container. Type the following command to accomplish this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, run the carbon-cache endpoint attached to the data container we created
    earlier. We will use custom container names and publicly exposed ports so that
    we can send and read metrics from it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have a place to send all our Docker-related metrics that we will gather
    later. In order to make use of the metrics we stored, we need a way to read and
    visualize them. We will now deploy `graphite-web` to visualize what is going on
    with our Docker containers. The following are the steps to deploy graphite-web
    in our Docker hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build `Dockerfile` as `hubuser/graphite-web` to prepare a Docker image to deploy
    graphite-web via the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding Docker image refers to `local_settings.py` to configure graphite-web.
    Place the following annotations to link the carbon-cache container and whisper
    volume:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After preparing the `Dockerfile` and `local_settings.py` configuration files,
    build the `hubuser/graphite-web` Docker image with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, run the `hubuser/graphite-web` Docker image linked with the carbon-cache
    container and whisper volume by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The `SECRET_KEY` environment variable is a necessary component to group together
    multiple graphite-web instances when you decide to scale out. More graphite-web
    settings can be found at [http://graphite.readthedocs.org/en/latest/config-local-settings.html](http://graphite.readthedocs.org/en/latest/config-local-settings.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have a complete Graphite deployment, we can run some preliminary
    tests to see it in action. We will test this by populating the whisper database
    with random data. Type the following command to send random metrics called `local.random`
    to the carbon-cache endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, confirm that the data is persistent by visiting our `hubuser/graphite-web`''s
    composer URL [http://dockerhost/compose](http://dockerhost/compose). Go to the
    **Tree** tab and then expand the `Graphite/local` folder to get the `random` metric.
    The following is a graph that we will see on our graphite-web deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting metrics to Graphite](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Graphite in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In production, this simple Graphite setup will reach its limits as we monitor
    more and more metrics in our Docker deployment. We need to scale this out in order
    to keep up with the increased number of metrics that we monitor. With this, you
    need to deploy Graphite with a cluster setup.
  prefs: []
  type: TYPE_NORMAL
- en: To scale out the metric processing capacity of carbon-cache, we need to augment
    it with a carbon-relay and carbon-aggregator. For graphite-web to be more responsive,
    we need to scale it out horizontally along with other caching components, such
    as memcached. We also need to add another graphite-web instance that connects
    to other graphite-web instances to create a unified view of all the metrics. The
    whisper databases will be co-located with a carbon-cache and graphite-web, so
    it will scale-out naturally along with them.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information on how to scale out a Graphite cluster in production is found
    at [http://graphite.readthedocs.org](http://graphite.readthedocs.org).
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with collectd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We finished setting up a place to send all our Docker-related data to. Now,
    it is time to actually fetch all the data related to our Docker applications.
    In this section, we will use `collectd`, a popular system statistics collection
    daemon. It is a very lightweight and high-performance C program. This makes it
    a noninvasive monitoring software because it doesn't consume many resources from
    the system it monitors. Being lightweight, it is very simple to deploy as it requires
    minimum dependencies. It has a wide variety of plugins to monitor almost every
    component of our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin monitoring our Docker host. Follow the next few steps to install
    `collectd` and send the metrics to our Graphite deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install `collectd` in our Docker host by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a minimum `collectd` configuration to send data to our Graphite
    deployment. You may recall from before that we exposed the carbon-cache''s default
    plaintext protocol port (`2003`). Write the following configuration entry in `/etc/collectd/collectd.conf`
    to set this up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it is time to measure a few things from our Docker host. Load the following
    `collectd` plugins by placing the next few lines in `/etc/collectd/collectd.conf`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After finishing the configuration, restart `collectd` by typing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s create a visualization dashboard in our graphite-web deployment
    to look at the preceding metrics. Go to [http://dockerhost/dashboard](http://dockerhost/dashboard),
    click on **Dashboard**, and then on the **Edit Dashboard** link. It will prompt
    us with a text area to place a dashboard definition. Paste the following JSON
    text in this text area to create our preliminary dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have a basic monitoring stack for our Docker host. The last step in
    the previous section will show a dashboard that looks something similar to the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with collectd](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Collecting Docker-related data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will measure some basic metrics that govern the performance of our
    application. But how do we drill down to further details on the containers running
    in our Docker hosts? Inside our Debian Jessie Docker host, our containers run
    under the `docker-[container_id].scope` control group. This information is found
    at `/sys/fs/cgroup/cpu,cpuacct/system.slice` in our Docker host''s sysfs. Fortunately,
    `collectd` has a `cgroups` plugin that interfaces with the sysfs information exposed
    earlier. The next few steps will show you how to use this plugin to measure the
    CPU performance of our running Docker containers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, insert the following lines in `/etc/collectd/collectd.conf`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, restart collectd by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, wait for a few minutes for Graphite to receive enough metrics from
    `collectd` so that we can get an initial feel to visualize our Docker container's
    CPU metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can now check the CPU metrics of our Docker containers by querying for the
    `dockerhost.cgroups-docker*.*` metrics on our Graphite deployment''s render API.
    The following is the image produced by the render API URL `http://dockerhost/render/?target=dockerhost.cgroups-docker-*.*`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting Docker-related data](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about the `cgroups` plugin can be found in the `collectd` documentation
    page at [https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups](https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups).
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the `cgroups` plugin only measures the CPU metrics of our running
    Docker containers. There is some work in progress, but it is not yet ready at
    the time of this book''s writing. Fortunately, there is a Python-based `collectd`
    plugin that interfaces itself to `docker stats`. The following are the steps needed
    to set up this plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, download the following dependencies to be able to run the plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, download and install the plugin from its GitHub page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following lines to `/etc/collectd/collectd.conf` to configure the plugin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, restart `collectd` to reflect the preceding configuration changes
    via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a case in which we don't want to install a whole stack of Python to
    just query the Docker container stat's endpoint. In this case, we can use the
    lower-level `curl_json` plugin of `collectd` to gather statistics about our container.
    We can configure it to make a request against the container stat's endpoint and
    parse the resulting JSON into a set of `collectd` metrics. More on how the plugin
    works can be found at [https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json](https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can explore the metrics given out by the `cgroups`
    plugin from our Graphite deployment at `http://docker/compose`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting Docker-related data](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Running collectd inside Docker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we want to deploy our `collectd` configuration similarly to our applications,
    we can run it inside Docker as well. The following is an initial `Dockerfile`
    that we can use to start with deploying `collectd` as a running Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most plugins look at the `/proc` and `/sys` filesystems. In order for `collectd`
    inside a Docker container to access these files, we need to mount them as Docker
    volumes, such as `--volume /proc:/host/proc`. However, most plugins currently
    read the hardwired `/proc` and `/sys` paths. There is ongoing discussion to make
    this configurable. Refer to this GitHub page to track its progress: [https://github.com/collectd/collectd/issues/1169](https://github.com/collectd/collectd/issues/1169).'
  prefs: []
  type: TYPE_NORMAL
- en: Consolidating logs in an ELK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all statuses of our Docker hosts and containers are readily available to
    be queried with our monitoring solution in collectd and Graphite. Some events
    and metrics are only available as raw lines of text in log files. We need to transform
    these raw and unstructured logs to meaningful metrics. Similar to raw metrics,
    we can later ask higher-level questions on what is happening in our Docker-based
    application through analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ELK stack is a popular combination suite from Elastic that addresses these
    problems. Each letter in the acronym represents each of its components. The following
    is a description of each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logstash**: Logstash is the component that is used to collect and manage
    logs and events. It is the central point that we use to collect all the logs from
    different log sources, such as multiple Docker hosts and containers running in
    our deployment. We can also use Logstash to transform and annotate the logs we
    receive. This allows us to search and explore the richer features of our logs
    later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch**: Elasticsearch is a distributed search engine that is highly
    scalable. Its sharding capabilities allow us to grow and scale our log storage
    as we continue to receive more and more logs from our Docker containers. Its database
    engine is document-oriented. This allows us to store and annotate logs as we see
    fit as we continue to discover more insights about the events we are managing
    in our large Docker deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**: Kibana is an analytics and search dashboard for Elasticsearch.
    Its simplicity allows us to create dashboards for our Docker applications. However,
    Kibana is also very flexible to customize, so we can build dashboards that can
    provide valuable insights to people who want to understand our Docker-based applications,
    whether it is a low-level technical detail or higher-level business need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the remaining parts of this section, we will set up each of these components
    and send our Docker host and container logs to it. The next few steps describe
    how to build the ELK stack:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, launch the official Elasticsearch image in our Docker host. We will
    put a container name so that we can link it easily in the later steps, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will run Kibana''s official Docker image by linking it against the
    Elasticsearch container we created in the previous step. Note that we publicly
    mapped the exposed port `5601` to port `80` in our Docker host so that the URL
    for Kibana is prettier, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, prepare our Logstash Docker image and configuration. Prepare the following
    `Dockerfile` to create the Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this Docker image, configure Logstash as a Syslog server. This explains
    the exposed UDP port in the preceding `Dockerfile`. As for the `logstash.conf`
    file, the following is the basic configuration to make it listen as a Syslog server.
    The latter part of the configuration shows that it sends logs to an Elasticsearch
    called `elasticsearch`. We will use this as the hostname when we link the Elasticsearch
    container we ran earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Logstash has a wealth of plugins so that it can read a wide variety of log data
    sources. In particular, it has a `collectd` codec plugin. With this, we can use
    an ELK stack instead of Graphite to monitor our metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For more information on how to do this setup, visit [https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have prepared all the files needed, type the following command
    to create it as the `hubuser/logstash` Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Logstash with the following command. Note that we are exposing port `1514`
    to the Docker host as the Syslog port. We also linked the Elasticsearch container
    named `elastic` that we created earlier. The target name is set to `elasticsearch`
    as it is the hostname of Elasticsearch that we configured earlier in `logstash.conf`
    to send the logs to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s configure our Docker host''s Syslog service to forward it to our
    Logstash container. As a basic configuration, we can set up Rsyslog to forward
    all the logs. This will include the logs coming from the Docker engine daemon
    as well. To do this, create the `/etc/rsyslog.d/100-logstash.conf` file with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, restart Syslog to load the changes in the previous step by typing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have a basic functioning ELK stack. Let''s now test it by sending a
    message to Logstash and seeing it appear in our Kibana dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, type the following command to send a test message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, go to our Kibana dashboard by visiting `http://dockerhost`. Kibana will
    now ask us to set the default index. Use the following default values and click
    on **Create** to start indexing:![Consolidating logs in an ELK stack](img/00019.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to `http://dockerhost/#discover` and type `elasticsearch` in the search.
    The following screenshot shows the Syslog message we generated earlier:![Consolidating
    logs in an ELK stack](img/00020.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: There are lot more things we can do on the ELK stack to optimize our logging
    infrastructure. We can add Logstash plugins and filters to annotate the logs we
    receive from our Docker hosts and containers. Elasticsearch can be scaled out
    and tuned to increase its capacity as our logging needs increase. We can create
    Kibana dashboards to share metrics. To find out more details on how to tune our
    ELK stack, visit Elastic's guides at [https://www.elastic.co/guide](https://www.elastic.co/guide).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Forwarding Docker container logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a basic functional ELK stack, we can start forwarding our
    Docker logs to it. From Docker 1.7 onwards, support for custom logging drivers
    has been available. In this section, we will configure our Docker host to use
    the syslog driver. By default, Syslog events from Docker will go to the Docker
    host''s Syslog service and since we configured Syslog to forward to our ELK stack,
    we will see the container logs there. Follow the next few steps to start receiving
    our container logs in the ELK stack:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker engine service is configured via Systemd on our Debian Jessie host.
    To update how it runs in our Docker host, create a Systemd unit file called `/etc/systemd/system/docker.service.d/10-syslog.conf`
    with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the changes on how we will run Docker in our host by reloading the Systemd
    configuration. The following command will do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, restart the Docker engine daemon by issuing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Optionally, apply any Logstash filtering if we want to do custom annotations
    on our Docker container's logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, any standard output and error streams coming out from our Docker container
    should be captured to our ELK stack. We can do some preliminary tests to confirm
    that the setup works. Type the following command to create a test message from
    Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `docker run` command also supports the `--log-driver` and `--log-opt=[]`
    command-line options to set up the logging driver only for the container we want
    to run. We can use it to further tune our logging policies for each Docker container
    running in our Docker host.
  prefs: []
  type: TYPE_NORMAL
- en: 'After typing the preceding command, our message should now be stored in Elasticsearch.
    Let''s go to our Kibana endpoint in `http://dockerhost`, and search for the word
    `message to elk` in the textbox. It should give the Syslog entry for the message
    we sent earlier. The following screenshot is what the search result should look
    like in our Kibana results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Forwarding Docker container logs](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we can see the message we sent. There is also other
    information about Syslog. Docker's Syslog driver sets the default syslog annotations
    on facility and severity as **system** and **informational**, respectively. In
    addition, the preceding program is set to **docker/c469a2dfdc9a**.
  prefs: []
  type: TYPE_NORMAL
- en: The **c469a2dfdc9a** string is the container ID of the busybox image we ran
    earlier. The default program label for Docker containers is set in the `docker/<container-id>`
    format. All of the preceding default annotations can be configured by passing
    arguments to the `--log-opt=[]` option.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Aside from the Syslog and JSON file-logging drivers, Docker supports several
    other endpoints to send logs to. More information about all the logging drivers
    and their respective usage guides can be found in [https://docs.docker.com/reference/logging](https://docs.docker.com/reference/logging).
  prefs: []
  type: TYPE_NORMAL
- en: Other monitoring and logging solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several other solutions for us to deploy to monitor and log infrastructure
    to support our Docker-based application. Some of them already have built-in support
    for monitoring Docker containers. Others should be combined with other solutions,
    such as the ones we showed previously because they only focus on a specific part
    of monitoring or logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'With others, we may have to do some workarounds. However, their benefits clearly
    outweigh the compromise we have to make. While the following list is not exhaustive,
    these are a few stacks we can explore to create our logging and monitoring solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor ([http://github.com/google/cadvisor](http://github.com/google/cadvisor))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InfluxDB ([http://influxdb.com](http://influxdb.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensu ([http://sensuapp.org](http://sensuapp.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluentd ([http://www.fluentd.org/](http://www.fluentd.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graylog ([http://www.graylog.org](http://www.graylog.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splunk ([http://www.splunk.com](http://www.splunk.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, our operations staff and developers running and developing our Docker
    applications are not yet mature enough or do not want to focus on maintaining
    such monitoring and logging infrastructures. There are several hosted monitoring
    and logging platforms that we can use so that we can focus on actually writing
    and improving the performance of our Docker application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of them work with existing monitoring and logging agents, such as Syslog
    and collectd. With others, we may have to download and deploy their agents to
    be able to forward the events and metrics to their hosted platform. The following
    is a nonexhaustive list of some solutions we may want to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: New Relic ([http://www.newrelic.com](http://www.newrelic.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog ([http://www.datadoghq.com](http://www.datadoghq.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Librato ([http://www.librato.com](http://www.librato.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic's Found ([http://www.elastic.co/found](http://www.elastic.co/found))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Treasure Data ([http://www.treasuredata.com](http://www.treasuredata.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splunk Cloud ([http://www.splunk.com](http://www.splunk.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know why it is important to monitor our Docker deployments in a scalable
    and accessible manner. We deployed collectd and Graphite to monitor our Docker
    container's metrics. We rolled out an ELK stack to consolidate the logs coming
    from various Docker hosts and containers.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to raw metrics and events, it is also important to know what it
    means for our application. Graphite-web and Kibana allow us to create custom dashboards
    and analysis to provide insight in to our Docker applications. With these monitoring
    tools and skills in our arsenal, we should be able to operate and run our Docker
    deployments well in production.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start doing performance tests and benchmark how
    our Docker applications fare well with a high load. We should be able to use the
    monitoring systems we deployed to observe and validate our performance testing
    activities there.
  prefs: []
  type: TYPE_NORMAL
