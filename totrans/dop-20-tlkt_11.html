<html><head></head><body><div class="chapter" title="Chapter&#xA0;11.&#xA0;Automating Implementation of the Deployment Pipeline"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Automating Implementation of the Deployment Pipeline</h1></div></div></div><p>Now that we are in control of the process of manually executing the deployment pipeline, we can start working on the creation of a fully automated version. After all, our goal is not to employ an army of operators that will sit in front of their computers and continuously execute deployment commands. Before we proceed, let us quickly go through the process one more time.</p><div class="section" title="Deployment Pipeline Steps"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec28"/>Deployment Pipeline Steps</h1></div></div></div><p>The steps<a class="indexterm" id="id462"/> of the pipeline are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Check out the code</li><li class="listitem">Run pre-deployment tests, compile and package the code</li><li class="listitem">Build the container</li><li class="listitem">Push the container to the registry</li><li class="listitem">Deploy the container to the production server</li><li class="listitem">Integrate the container</li><li class="listitem">Run post-deployment tests</li><li class="listitem">Push the tests container to the registry<div class="mediaobject"><img alt="Deployment Pipeline Steps" src="graphics/B05848_11_01.jpg"/><div class="caption"><p>Figure 11-1 – Deployment pipeline</p></div></div></li></ol></div><p>To minimize the<a class="indexterm" id="id463"/> impact the pipeline has on our business, we tried our best to run as many tasks as possible outside the production server. The only two steps that we had to perform on the <code class="literal">prod</code> node is deployment itself and the integrations (at the moment only with the proxy service). All the rest of the steps were done inside the <code class="literal">cd</code> server:</p><div class="mediaobject"><img alt="Deployment Pipeline Steps" src="graphics/B05848_11_02.jpg"/><div class="caption"><p>Figure 11-2 – Tasks distribution between the CD and production nodes</p></div></div><p>We already <a class="indexterm" id="id464"/>chose Ansible as the tool we're using for servers provisioning. We used it in several occasions to install packages, setup configurations and so on. Up until now, all those usages were aimed at providing all the requirements necessary for the deployment of our containers. We'll extend the usage of Ansible playbooks and add the deployment pipeline to it:</p><div class="mediaobject"><img alt="Deployment Pipeline Steps" src="graphics/B05848_11_03.jpg"/><div class="caption"><p>Figure 11-3 – Automated deployment pipeline with Ansible</p></div></div><p>Of all the steps involved, we'll leave only one of them outside of the automation scope. We won't check out the code with Ansible. The reason behind this is not that Ansible is not capable of cloning a Git repository. It certainly is. The problem is that Ansible is not a tool designed to <a class="indexterm" id="id465"/>run continuously and monitor code repositories for changes. There are a few more problems that we did not yet tackle. For example, we do not have a set of actions that should be run in case of a failure of the process. Another hole in the current pipeline is that there is a short downtime related to each deployment. The process stops the running release and brings up the new one. Between those two actions, there is a (short) period the service we're deploying is not operational.</p><p>We'll leave those and other possible improvements for later on:</p><div class="mediaobject"><img alt="Deployment Pipeline Steps" src="graphics/B05848_11_04.jpg"/><div class="caption"><p>Figure 11-4 – Missing pieces in the deployment pipeline</p></div></div><p>To get a better <a class="indexterm" id="id466"/>grasp on the process, we'll go through each manual step we performed earlier and see how it can be done with Ansible.</p><p>We'll start by creating up the nodes and cloning the code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd prod</strong></span>
<span class="strong"><strong>vagrant ssh cd</strong></span>
<span class="strong"><strong>git clone https://github.com/vfarcic/books-ms.git</strong></span>
</pre></div><div class="section" title="The Playbook and the Role"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec57"/>The Playbook and the Role</h2></div></div></div><p>If you already tried automated deployment, the chances are that the scripts you created were mostly related to the deployment itself. With Ansible (and CM tools in general), we have the<a class="indexterm" id="id467"/> option to do the process from <a class="indexterm" id="id468"/>scratch every time. Not only that we'll automate the deployment, but we'll set up the whole server. We cannot be confident in which state the server is. For example, maybe it has nginx or maybe it doesn't. Maybe it did have the nginx container up and running but, for some reason, its process stopped. Even if the process is running, maybe some crucial configuration changed. The same logic can be applied to anything, directly or indirectly, related to the service we want to deploy. The approach we'll take is to have a playbook that will make sure that everything is set <a class="indexterm" id="id469"/>correctly. Ansible is intelligent enough to check the status of all <a class="indexterm" id="id470"/>those dependencies and applies changes only if something is wrong.</p><p>Let us take a look at the <code class="literal">se</code>
<code class="literal">rvice.yml</code> playbook:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: prod</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - docker</strong></span>
<span class="strong"><strong>    - docker-compose</strong></span>
<span class="strong"><strong>    - consul</strong></span>
<span class="strong"><strong>    - registrator</strong></span>
<span class="strong"><strong>    - consul-template</strong></span>
<span class="strong"><strong>    - nginx</strong></span>
<span class="strong"><strong>    - service</strong></span>
</pre></div><p>The <code class="literal">service</code> role will contain tasks directly related to the deployment and all the others before them are dependencies our service needs to work correctly. Since we already went through all but the last role from this playbook, stands to reason that we should jump directly to the definition of the list of tasks in the <code class="literal">service</code> role defined in the <code class="literal">roles/service/tasks</code>
<code class="literal">/main.yml</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- include: pre-deployment.yml</strong></span>
<span class="strong"><strong>- include: deployment.yml</strong></span>
<span class="strong"><strong>- include: post-deployment.yml</strong></span>
</pre></div><p>Since this role will be a bit bigger than those we used before, we made the decision to split them into logical groups (<span class="emphasis"><em>pre-deployment</em></span>, <span class="emphasis"><em>deployment</em></span> and <span class="emphasis"><em>post-deployment</em></span>) and include them into the <code class="literal">main.yml</code> file. That way we won't be dealing with too many tasks at a time, and we'll increase the readability of the role.</p></div><div class="section" title="Pre-Deployment tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec58"/>Pre-Deployment tasks</h2></div></div></div><p>The first <a class="indexterm" id="id471"/>thing we should do is build the tests container. We already used the following command (please don't run it):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker pull \</strong></span>
<span class="strong"><strong>    -t 10.100.198.200:5000/books-ms-tests</strong></span>
<span class="strong"><strong>docker build \</strong></span>
<span class="strong"><strong>    -t 10.100.198.200:5000/books-ms-tests \</strong></span>
<span class="strong"><strong>    -f Dockerfile.test \</strong></span>
<span class="strong"><strong>    .</strong></span>
</pre></div><p>Replicating the same command in Ansible is very easy with the <code class="literal">Sh</code>
<code class="literal">ell module</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Tests container is pulled</strong></span>
<span class="strong"><strong>  shell: docker pull \</strong></span>
<span class="strong"><strong>    {{ registry_url }}{{ service_name }}-tests</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  ignore_errors: yes</strong></span>
<span class="strong"><strong>  tags: [service, tests]</strong></span>
<span class="strong"><strong>- name: Tests container is built</strong></span>
<span class="strong"><strong>  shell: docker build \</strong></span>
<span class="strong"><strong>    -t {{ registry_url }}{{ service_name }}-tests \</strong></span>
<span class="strong"><strong>    -f Dockerfile.test \</strong></span>
<span class="strong"><strong>    .</strong></span>
<span class="strong"><strong>  args:</strong></span>
<span class="strong"><strong>    chdir: "{{ repo_dir }}"</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  tags: [service, tests]</strong></span>
</pre></div><p>We changed the command itself so that parts that might be prone to change are used as variables. The first one is the <code class="literal">registry_url</code> that should contain the IP and the port of the Docker registry. The default value is specified in the <code class="literal">grou</code>
<code class="literal">p_vars/all</code> file. The second one<a class="indexterm" id="id472"/> is more interesting. We are not creating this role to work with the service <code class="literal">books-ms</code> but as something that can be used with (almost) any service since all of them can follow the same pattern. We can do this sorts of things without sacrificing the freedom since the key instructions are stored in a few files located in the repository of each service. The most important ones are the <code class="literal">Dockerfile.test</code> and the <span class="emphasis"><em>Dockerfile</em></span> that define testing and service containers, Docker Compose configurations that define how should containers be run and, finally, the proxy configuration and template. All those files are separated from the process we're creating, and people in charge of the project have the full freedom to tailor them to their needs. That showcases a very important aspect I'm trying to promote. It is crucial not only to have the right process in place but also to have the scripts, configurations and the code properly located. Everything that is common to multiple projects should be <a class="indexterm" id="id473"/>centralized (as is the case with Ansible playbooks located in the <a class="ulink" href="https://github.com/vfarcic/ms-lifecycle">https://github.com/vfarcic/ms-lifecycle</a> repository). On the other hand, things that might be specific to a project should be stored in the repository that project resides in. Storing everything in one centralized place would introduce quite a lot of waiting time since a project team would need to request a change from the delivery team. The other extreme is just as wrong. If everything is stored in the project repositories, there would be quite a lot of duplication. Each project would need to come up with scripts to set up servers, deploy a service, and so on.</p><p>Next we specified a single argument <code class="literal">chdir</code>. It will make sure that the command is run from the directory that, in this case, contains the <code class="literal">Dockerfile.test</code> file. The <code class="literal">chdir</code> value is the variable <code class="literal">repo_dir</code> that, unlike <code class="literal">registry_url</code> does not have the default value. We'll specify it at runtime when we run the playbook. Then comes the <code class="literal">delegate_to</code> instruction. Since we are committed to disrupting the destination server as little as possible, tasks like this one will be run on the localhost (<code class="literal">127.0.0.1</code>). Finally, we set few tags that can be used to filter which tasks will or will not be run.</p><p>The reason behind pulling the tests container before building it is to save the time. The execution of the playbook might change from one server to another and, if such a thing happens, without first pulling the container from the Registry, Docker would build all the layers even though most of them are likely to be the same as before. Take a note that we introduced the <code class="literal">ignore_errors</code> instruction. Without it, the playbook would fail if this is the first build of the container and there is nothing to be pulled.</p><p>Please keep in<a class="indexterm" id="id474"/> mind that the <code class="literal">shell</code> module should be avoided in most cases. The idea behind Ansible is to specify the desired behavior and not the action that should be performed. Once that desire is run, Ansible will try to do the right thing. If, for example, we specify that some package should be installed, Ansible will check whether such a package already exists and do the installation only if it doesn't. The shell module that we used, in this case, will always run, no matter the state of the system. In this particular situation, that is OK, because Docker itself will make sure that only changed layers are built. It won't build the whole container every time. Please keep this in mind when designing your roles.</p><p>The rest of the commands we used in the pre-deployment phase are as follows (please don't run them):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose -f docker-compose-dev.yml \</strong></span>
<span class="strong"><strong>    run --rm tests</strong></span>
<span class="strong"><strong>docker pull 10.100.198.200:5000/books-ms</strong></span>
<span class="strong"><strong>docker build -t 10.100.198.200:5000/books-ms .</strong></span>
<span class="strong"><strong>docker push 10.100.198.200:5000/books-ms</strong></span>
</pre></div><p>When translated to the Ansible format, the result is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Pre-deployment tests are run</strong></span>
<span class="strong"><strong>  shell: docker-compose \</strong></span>
<span class="strong"><strong>    -f docker-compose-dev.yml \</strong></span>
<span class="strong"><strong>    run --rm tests</strong></span>
<span class="strong"><strong>  args:</strong></span>
<span class="strong"><strong>    chdir: "{{ repo_dir }}"</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  tags: [service, tests]</strong></span>
<span class="strong"><strong>- name: Container is built</strong></span>
<span class="strong"><strong>  shell: docker build \</strong></span>
<span class="strong"><strong>    -t {{ registry_url }}{{ service_name }} \</strong></span>
<span class="strong"><strong>    .</strong></span>
<span class="strong"><strong>  args:</strong></span>
<span class="strong"><strong>    chdir: "{{ repo_dir }}"</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
<span class="strong"><strong>- name: Container is pushed</strong></span>
<span class="strong"><strong>  shell: docker push \</strong></span>
<span class="strong"><strong>    {{ registry_url }}{{ service_name }}</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
</pre></div><p>There's not much to be said about those tasks. They all use the shell module and are all running on localhost. We run the tests container that, besides the obvious function of checking the quality of the code, compiles the service. The result of that compilation is used to <a class="indexterm" id="id475"/>build the service container that is later on pushed to the Docker registry.</p><p>The final result can be seen in the <code class="literal">roles/service/tasks/pre-</code>
<code class="literal">deployment.yml</code> file and we can proceed with the deployment tasks.</p></div><div class="section" title="Deployment tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec59"/>Deployment tasks</h2></div></div></div><p>The next<a class="indexterm" id="id476"/> set of commands we did when manually running the deployment pipeline had the goal of creating directories and files required for the process. They were as follows (please don't run them).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir -p /data/books-ms</strong></span>
<span class="strong"><strong>cd /data/books-ms</strong></span>
<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/docker-compose.yml</strong></span>
<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-includes.conf \</strong></span>
<span class="strong"><strong>    -O /data/nginx/includes/books-ms.conf</strong></span>
<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-upstreams.ctmpl \</strong></span>
<span class="strong"><strong>    -O /data/nginx/upstreams/books-ms.ctmpl</strong></span>
</pre></div><p>We created the service directory and downloaded the <code class="literal">docker-compose.yml</code>, <code class="literal">nginx-includes.conf</code> and <code class="literal">nginx-upstreams.ctmpl</code> files from the code repository. The latter two we'll download later when the time comes to change the proxy, but we can group them all together as a single Ansible task. With Ansible, we'll do it a bit differently. Since we already checked out the code, there is no reason to download those files. We can just copy them to the destination server. Ansible tasks that replicate this same set of commands are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Directory is created</strong></span>
<span class="strong"><strong>  file:</strong></span>
<span class="strong"><strong>    path: /data/{{ service_name }}</strong></span>
<span class="strong"><strong>    recurse: yes</strong></span>
<span class="strong"><strong>    state: directory</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
<span class="strong"><strong>- name: Files are copied</strong></span>
<span class="strong"><strong>  copy:</strong></span>
<span class="strong"><strong>    src: "{{ item.src }}"</strong></span>
<span class="strong"><strong>    dest: "{{ item.dest }}"</strong></span>
<span class="strong"><strong>  with_items: files</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
</pre></div><p>We created two tasks. The first one uses the Ansible module <code class="literal">file</code> to create the service directory. Since this role is supposed to be generic and apply to (almost) any service, the name of the service is a variable that we'll set at runtime when we run the playbook. The second task uses<a class="indexterm" id="id477"/> the <code class="literal">copy</code> module to copy all the files that we'll need on the destination server. We're using the <code class="literal">with_items</code> instruction that will repeat this task for each entry into the <code class="literal">*files_ variable</code>. The variable is defined in the <code class="literal">roles/service/defaults/main.yml</code> file and is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>files: [</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    src: "{{ repo_dir }}/docker-compose.yml",</strong></span>
<span class="strong"><strong>    dest: "/data/{{ service_name }}/docker-compose.yml"</strong></span>
<span class="strong"><strong>  }, {</strong></span>
<span class="strong"><strong>    src: "{{ repo_dir }}/nginx-includes.conf",</strong></span>
<span class="strong"><strong>    dest: "/data/nginx/includes/{{ service_name }}.conf"</strong></span>
<span class="strong"><strong>  }, {</strong></span>
<span class="strong"><strong>    src: "{{ repo_dir }}/nginx-upstreams.ctmpl",</strong></span>
<span class="strong"><strong>    dest: "/data/nginx/upstreams/{{ service_name }}.ctmpl"</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>]</strong></span>
</pre></div><p>The source of all of those files utilizes the <code class="literal">repo_dir</code> variable that we already used in the pre-deployment tasks. Similarly, file destinations are using the <code class="literal">service_name</code> variable.</p><p>Once we're sure that all the files we'll need are on the destination server, we can proceed with the actual deployment that consists of two steps (please don't run them).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose pull app</strong></span>
<span class="strong"><strong>docker-compose up -d app</strong></span>
<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul localhost:8500 \</strong></span>
<span class="strong"><strong>    -template "/data/nginx/upstreams/books-ms.ctmpl:\</strong></span>
<span class="strong"><strong>/data/nginx/upstreams/books-ms.conf:\</strong></span>
<span class="strong"><strong>docker kill -s HUP nginx" \</strong></span>
<span class="strong"><strong>    -once</strong></span>
</pre></div><p>First we pulled the latest image from the Docker registry and then we brought it up. When <code class="literal">docker-compose up</code> is run, it checks whether the container image or its configuration changed when compared with the running container. If it is indeed different, Docker Compose will stop the running containers and run the new ones while preserving mounted volumes. We already discussed that, during some time (between the stopping the current version and running the new one), our service will be unavailable. We'll deal with this problem later on. For now, a (very short) downtime will be something we'll have to live with. Finally, we run <code class="literal">consul-template</code> that updates configurations and reloads nginx.</p><p>As you <a class="indexterm" id="id478"/>probably guessed, we'll run those two commands through the Ansible <code class="literal">shell</code> module:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Containers are pulled</strong></span>
<span class="strong"><strong>  shell: docker-compose pull app</strong></span>
<span class="strong"><strong>  args:</strong></span>
<span class="strong"><strong>    chdir: /data/{{ service_name }}</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
<span class="strong"><strong>- name: Containers are running</strong></span>
<span class="strong"><strong>  shell: docker-compose up -d app</strong></span>
<span class="strong"><strong>  args:</strong></span>
<span class="strong"><strong>    chdir: /data/{{ service_name }}</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
<span class="strong"><strong>- name: Proxy is configured</strong></span>
<span class="strong"><strong>  shell: consul-template \</strong></span>
<span class="strong"><strong>    -consul localhost:8500 \</strong></span>
<span class="strong"><strong>    -template "{{ ct_src }}:{{ ct_dest }}:{{ ct_cmd }}" \</strong></span>
<span class="strong"><strong>    -once</strong></span>
<span class="strong"><strong>  tags: [service]</strong></span>
</pre></div><p>We're not doing anything new. It's the same pattern as the shell tasks we defined as pre-deployment tasks. The only thing worth noting is that we used variables as the <code class="literal">-template</code> value. The only reason behind this is that the length of the book has a maximum limit of characters per line, and all the parameters would not fit. Those variables are defined in the <code class="literal">roles/service/defaults/main.yml</code> file and are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ct_src: /data/nginx/upstreams/{{ service_name }}.ctmpl</strong></span>
<span class="strong"><strong>ct_dest: /data/nginx/upstreams/{{ service_name }}.conf</strong></span>
<span class="strong"><strong>ct_cmd: docker kill -s HUP nginx</strong></span>
</pre></div><p>The final result can be seen in the <code class="literal">roles/service/tas</code>
<code class="literal">ks/deployment.yml</code> file. Please note that, unlike the pre-deployment tasks, all those in this group are indeed going to run on the destination server. That can be seen by the lack of the <code class="literal">delegate_to: 127.0.0.1</code> instruction.</p><p>We're done with deployment and can turn our attention to the last group of tasks.</p></div><div class="section" title="Post-Deployment tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec60"/>Post-Deployment tasks</h2></div></div></div><p>All that is left<a class="indexterm" id="id479"/> is to run integration tests and push the tests container to the registry. As a reminder, the commands are as follows (please don't run them).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose \</strong></span>
<span class="strong"><strong>    -f docker-compose-dev.yml \</strong></span>
<span class="strong"><strong>    run --rm \</strong></span>
<span class="strong"><strong>    -e DOMAIN=http://10.100.198.201 \</strong></span>
<span class="strong"><strong>    integ</strong></span>
<span class="strong"><strong>docker push 10.100.198.200:5000/books-ms-tests</strong></span>
</pre></div><p>Ansible equivalent of those commands is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Post-deployment tests are run</strong></span>
<span class="strong"><strong>  shell: docker-compose \</strong></span>
<span class="strong"><strong>    -f docker-compose-dev.yml \</strong></span>
<span class="strong"><strong>    run --rm \</strong></span>
<span class="strong"><strong>    -e DOMAIN={{ proxy_url }} \</strong></span>
<span class="strong"><strong>    Integ</strong></span>
<span class="strong"><strong>  args:</strong></span>
<span class="strong"><strong>    chdir: "{{ repo_dir }}"</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  tags: [service, tests]</strong></span>
<span class="strong"><strong>- name: Tests container is pushed</strong></span>
<span class="strong"><strong>  shell: docker push \</strong></span>
<span class="strong"><strong>    {{ registry_url }}{{ service_name }}-tests</strong></span>
<span class="strong"><strong>  delegate_to: 127.0.0.1</strong></span>
<span class="strong"><strong>  tags: [service, tests]</strong></span>
</pre></div><p>There's nothing new here so we won't go into details. The complete version of post-deployment tasks can be found in the <code class="literal">roles/service/tasks/p</code>
<code class="literal">ost-deploym</code>
<code class="literal">ent.yml</code> file.</p></div></div></div>
<div class="section" title="Running the Automated Deployment Pipeline"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec29"/>Running the Automated Deployment Pipeline</h1></div></div></div><p>Let us <a class="indexterm" id="id480"/>see the <code class="literal">service</code> playbook in action:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd ~/books-ms</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/service.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod \</strong></span>
<span class="strong"><strong>    --extra-vars "repo_dir=$PWD service_name=books-ms"</strong></span>
</pre></div><p>We run the playbook <code class="literal">service.yml</code> with the inventory pointing to the <code class="literal">hosts/prod</code> file and few extra variables. The first one is the <code class="literal">repo_dir</code> with the value of the current directory (<code class="literal">$PWD</code>). The second represents the name of the service we want to deploy (<code class="literal">books-ms</code>). At the moment, we have only this service. If there would be more, they could all be deployed with this same playbook by changing the value of this variable.</p><p>We managed to have not only the fully automated deployment but also provisioning of the destination server. The first of the playbook was done against a virgin Ubuntu server, so Ansible made sure that everything needed for the deployment is properly configured. The result is not perfect, but it is a good start.</p><p>Feel free to <a class="indexterm" id="id481"/>repeat the execution of the playbook and observe the differences when compared to the first run. You'll notice that most of the Ansible tasks will be in the status <code class="literal">ok</code> since there was nothing to be done and that the playbook runs much faster.</p><p>What could be the things that we might be missing? There are quite a few. However, before we proceed and try to fix them, we should set up a proper <span class="emphasis"><em>Continuous Deployment</em></span> platform and see whether it can help with the current process. Until then, let us destroy the VMs and let your computer take a break:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div></div></body></html>