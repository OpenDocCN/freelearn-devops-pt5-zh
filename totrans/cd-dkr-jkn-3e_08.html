<html><head></head><body>
		<div><h1 id="_idParaDest-153"><em class="italic"><a id="_idTextAnchor152"/>Chapter 6</em>: Clustering with Kubernetes</h1>
			<p>So far, in this book, we have covered the fundamental aspects of the acceptance testing process. In this chapter, we will see how to change the Docker environment from a single Docker host into a cluster of machines and how to change an independent application into a system composed of multiple applications.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Server clustering</li>
				<li>Introducing Kubernetes</li>
				<li>Kubernetes installation</li>
				<li>Using Kubernetes</li>
				<li>Advanced Kubernetes</li>
				<li>Application dependencies</li>
				<li>Alternative cluster management systems</li>
			</ul>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor153"/>Technical requirements</h1>
			<p>To follow along with the instructions in this chapter, you'll need the following hardware/software requirements:</p>
			<ul>
				<li>At least 4 GB of RAM</li>
				<li>At least 1 GB of free disk space</li>
				<li>Java JDK 8+</li>
			</ul>
			<p>All the examples and solutions to the exercises in this chapter can be found in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Continuous-Delivery-With-Docker-and-Jenkins-3rd-Edition/tree/main/Chapter06">https://github.com/PacktPublishing/Continuous-Delivery-With-Docker-and-Jenkins-3rd-Edition/tree/main/Chapter06</a>.</p>
			<p>Code in Action videos for this chapter can be viewed at <a href="https://bit.ly/3rcffcz">https://bit.ly/3rcffcz</a>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor154"/>Server clustering</h1>
			<p>So far, we have interacted with each of the machines individually. What we did was connect to the <code>localhost</code> Docker daemon server. We could have used the <code>-H</code> option in the <code>docker run</code> command <a id="_idIndexMarker601"/>to specify the address of the remote Docker, but that would still mean deploying our application to a single Docker host machine. In real life, however, if servers share the same physical location, we are not interested in which particular machine the service is deployed in. All we need is to have it accessible and replicated in many instances to support high availability. <em class="italic">How can we configure a set of machines to work that way?</em> This is the role of clustering.</p>
			<p>In the following subsections, you will be introduced to the concept of server clustering and the Kubernetes environment, which is an example of cluster management software.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>Introducing server clustering</h2>
			<p>A server cluster is a set of connected computers that work together in such a way that they can be used similarly to a single system. Servers are usually connected through the local network by a connection <a id="_idIndexMarker602"/>that's fast enough to ensure that the services that are being run are distributed. A simple server cluster is presented in the following diagram:</p>
			<div><div><img src="img/B18223_06_01.jpg" alt="Figure 6.1 – Server clustering&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Server clustering</p>
			<p>A user accesses the cluster through a master host, which exposes the cluster API. There are multiple nodes that act as computing resources, which means that they are responsible for running <a id="_idIndexMarker603"/>applications. The master, on the other hand, is responsible for all other activities, such as the orchestration process, service discovery, load balancing, and node failure detection.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>Introducing Kubernetes</h1>
			<p>Kubernetes is an open source cluster management system that was originally designed by Google. Looking at the popularity charts, it is a clear winner among other competitors, such as <a id="_idIndexMarker604"/>Docker Swarm and Apache Mesos. Its popularity has grown so fast that most cloud platforms provide Kubernetes out of the box. It's not Docker-native, but there are a lot of additional tools and integrations to make it work smoothly with the whole Docker ecosystem; for example, <strong class="bold">kompose</strong> can translate Docker <a id="_idIndexMarker605"/>Compose files into Kubernetes configurations.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">In the first edition of this book, I recommended Docker Compose and Docker Swarm for application dependency resolution and server clustering. While they're both good tools, Kubernetes' popularity has grown so high recently that I decided to use Kubernetes as the recommended approach and keep Docker-native tooling as an alternative.</p>
			<p>Let's take a look at the simplified architecture of Kubernetes:</p>
			<div><div><img src="img/B18223_06_02.jpg" alt="Figure 6.2 – Simplified Kubernetes architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Simplified Kubernetes architecture</p>
			<p>The Kubernetes <code>8080</code>) and the control plane is responsible for making it happen. A Kubernetes Node, on the <a id="_idIndexMarker607"/>other hand, is a worker. You may see it just as a (Docker) container host with a special Kubernetes process (called <code>kubelet</code>) installed.</p>
			<p>From the user's perspective, you provide a declarative deployment configuration in the form of a YAML file and pass it to the Kubernetes control plane through its API. Then, the control plane reads <a id="_idIndexMarker608"/>the configuration and installs the deployment. Kubernetes introduces the concept of a <strong class="bold">Pod</strong>, which represents a single deployment unit. The Pod contains Docker <strong class="bold">containers</strong>, which are scheduled together. While you can put multiple containers into <a id="_idIndexMarker609"/>a single Pod, in real-life scenarios, you will see that most Pods contain just a single Docker container. Pods are dynamically built and removed depending on the requirement changes that are expressed in the YAML configuration updates.</p>
			<p>You will gain more practical knowledge about Kubernetes in later sections of this chapter, but first, let's <a id="_idIndexMarker610"/>name the features that make Kubernetes such a great environment.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Kubernetes features overview</h2>
			<p>Kubernetes provides a <a id="_idIndexMarker611"/>number of interesting features. Let's walk through the most important ones:</p>
			<ul>
				<li><strong class="bold">Container balancing</strong>: Kubernetes takes care of the load balancing of Pods on nodes; you specify the number of replicas of your application and Kubernetes takes care of the rest.</li>
				<li><strong class="bold">Traffic load balancing</strong>: When you have multiple replicas of your application, the Kubernetes service can load balance the traffic. In other words, you create a service with a single IP (or DNS) and Kubernetes takes care of load balancing the traffic to your application replicas.</li>
				<li><strong class="bold">Dynamic horizontal scaling</strong>: Each deployment can be dynamically scaled up or down; you specify the number of application instances (or the rules for autoscaling) and Kubernetes starts/stops Pod replicas.</li>
				<li><strong class="bold">Failure recovery</strong>: Pods (and nodes) are constantly monitored and if any of them fail, new Pods are started so that the declared number of replicas is constant.</li>
				<li><strong class="bold">Rolling updates</strong>: An update to the configuration can be applied incrementally; for example, if we have 10 replicas and we would like to make a change, we can define a delay between the deployment to each replica. In such a case, when anything goes wrong, we never end up with a scenario where a replica isn't working correctly.</li>
				<li><strong class="bold">Storage orchestration</strong>: Kubernetes can mount a storage system of your choice to your applications. Pods are stateless in nature and, therefore, Kubernetes <a id="_idIndexMarker612"/>integrates with a number of storage <a id="_idIndexMarker613"/>providers, such as Amazon <strong class="bold">Elastic Block Storage</strong> (<strong class="bold">EBS</strong>), <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>) Persistent Disk, and Azure Data Storage.</li>
				<li><strong class="bold">Service discovery</strong>: Kubernetes Pods are ephemeral in nature and their IPs are dynamically assigned, but Kubernetes provides DNS-based service discovery for this.</li>
				<li><strong class="bold">Run everywhere</strong>: Kubernetes is an open source tool, and you have a lot of options of how <a id="_idIndexMarker614"/>to run it: on-premises, cloud infrastructure, or hybrid.</li>
			</ul>
			<p>Now that we have some background about Kubernetes, let's see what it all looks like in practice, starting with the installation process.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor158"/>Kubernetes installation</h1>
			<p>Kubernetes, just like Docker, consists of two parts: the client and the server. The client is a command-line tool named <code>kubectl</code> and it connects to the server part using the Kubernetes API. The server <a id="_idIndexMarker615"/>is much more complex and is as we described in the previous section. Obviously, to do anything with Kubernetes, you need both parts, so let's describe them one by one, starting with the client.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>Kubernetes client</h2>
			<p>The Kubernetes client, <code>kubectl</code>, is a command-line application that allows you to perform operations <a id="_idIndexMarker616"/>on the Kubernetes cluster. The installation process depends <a id="_idIndexMarker617"/>on your operating system. You can check out the details on the official Kubernetes website: <a href="https://kubernetes.io/docs/tasks/tools/">https://kubernetes.io/docs/tasks/tools/</a>.</p>
			<p>After you have successfully installed <code>kubectl</code>, you should be able to execute the following command:</p>
			<pre>$ kubectl version --client
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.4", ...</pre>
			<p>Now that you have the Kubernetes client configured, we can move on to the server.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>Kubernetes server</h2>
			<p>There are multiple ways to set up a Kubernetes server. Which one you should use depends on your <a id="_idIndexMarker618"/>needs, but if you are completely new to Kubernetes, then I recommend starting from a local environment.</p>
			<h3>Local environment</h3>
			<p>Even though Kubernetes itself is a complex clustering system, there are a few tools that can simplify your local <a id="_idIndexMarker619"/>development setup. Let's walk through <a id="_idIndexMarker620"/>the options you have, which include Docker Desktop, kind, and minikube.</p>
			<h4>Docker Desktop</h4>
			<p>Docker Desktop is an application that is used to set up a local Docker environment on macOS or Windows. As you may remember from the previous chapters, the Docker daemon can only <a id="_idIndexMarker621"/>run natively on Linux, so for other operating <a id="_idIndexMarker622"/>systems, you need to get it running on a VM. Docker Desktop provides a super-intuitive way to do this, and luckily, it also supports the creation of Kubernetes clusters.</p>
			<p>If you have Docker Desktop installed, then all you need to do is check the <code>kubectl</code> will be configured:</p>
			<div><div><img src="img/B18223_06_03.jpg" alt="Figure 6.3 – Kubernetes in Docker Desktop&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Kubernetes in Docker Desktop</p>
			<p>Starting from this <a id="_idIndexMarker623"/>point, you are ready to use the <a id="_idIndexMarker624"/>Kubernetes cluster.</p>
			<h4>kind</h4>
			<p>If you use the Linux operating system and can't, or just don't want to, use Docker Desktop, then <a id="_idIndexMarker625"/>your second simplest option is <strong class="bold">kind</strong> (short for <strong class="bold">Kubernetes in Docker</strong>). It's a tool <a id="_idIndexMarker626"/>for which the only requirement is to have Docker installed and configured.</p>
			<p>After installing kind, you can start and configure your local Kubernetes cluster with this one command:</p>
			<pre>$ kind create cluster</pre>
			<p class="callout-heading">Information </p>
			<p class="callout">You can check <a id="_idIndexMarker627"/>the kind installation steps at <a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a>.</p>
			<h4>minikube</h4>
			<p>minikube is a command-line tool that starts a fully functional Kubernetes environment inside <a id="_idIndexMarker628"/>a VM. It is backed up by a VM hypervisor, so you need to have <a id="_idIndexMarker629"/>VirtualBox, Hyper-V, VMware, or a similar tool installed. The instructions to install minikube depend on your operating system, and you can find instructions for each at <a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a>.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">minikube is an open <a id="_idIndexMarker630"/>source tool that you can find on GitHub at <a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>.</p>
			<p>After you have successfully installed minikube, you can start your Kubernetes cluster with the following command:</p>
			<pre>$ minikube start</pre>
			<p>minikube starts a Kubernetes cluster and automatically configures your Kubernetes client with the cluster URL and credentials, so you can move directly to the <em class="italic">Verifying the Kubernetes setup</em> section.</p>
			<h3>Cloud platforms</h3>
			<p>Kubernetes has become <a id="_idIndexMarker631"/>so popular that most cloud <a id="_idIndexMarker632"/>computing platforms provide it as a service. The leader here is <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), which allows you to create a Kubernetes <a id="_idIndexMarker633"/>cluster within a few minutes. Other cloud platforms, such as Microsoft Azure, <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), and IBM <a id="_idIndexMarker634"/>Cloud, also have Kubernetes in their portfolios. Let's take a closer look at the three most popular solutions—<strong class="bold">GCP</strong>, <strong class="bold">Azure</strong>, and <strong class="bold">AWS</strong>.</p>
			<h4>Google Cloud Platform</h4>
			<p>You can access GCP at <a href="https://cloud.google.com/">https://cloud.google.com/</a>. After creating an account, you should be <a id="_idIndexMarker635"/>able to open their web console (<a href="https://console.cloud.google.com">https://console.cloud.google.com</a>). One of <a id="_idIndexMarker636"/>the <a id="_idIndexMarker637"/>services in <a id="_idIndexMarker638"/>their portfolio is called <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>).</p>
			<p>You can create a Kubernetes cluster by clicking in the user interface or by using the GCP command-line tool, called <code>gcloud</code>.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">You can read how <a id="_idIndexMarker639"/>to install <code>gcloud</code> on your operating system at the official GCP website: <a href="https://cloud.google.com/sdk/docs/install">https://cloud.google.com/sdk/docs/install</a>.</p>
			<p>To create a Kubernetes cluster using the command-line tool, it's enough to execute the following command:</p>
			<pre>$ gcloud container clusters create test-cluster</pre>
			<p>Apart from creating a Kubernetes cluster, it automatically configures <code>kubectl</code>.</p>
			<h4>Microsoft Azure</h4>
			<p>Microsoft Azure also <a id="_idIndexMarker640"/>offers <a id="_idIndexMarker641"/>a very quick Kubernetes setup thanks to <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>). Like GCP, you can use either a web interface <a id="_idIndexMarker642"/>or a command-line tool to create a cluster.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">You can access the Azure web console at <a href="https://portal.azure.com/">https://portal.azure.com/</a>. To install the Azure command-line tool, check <a id="_idIndexMarker643"/>the installation guide on their official page at <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">https://docs.microsoft.com/en-us/cli/azure/install-azure-cli</a>.</p>
			<p>To create a Kubernetes cluster using the Azure command-line tool, assuming you already have an Azure resource group created, it's enough to run the following command:</p>
			<pre>$ az aks create -n test-cluster -g test-resource-group</pre>
			<p>After a few <a id="_idIndexMarker644"/>seconds, your Kubernetes cluster should be ready. To <a id="_idIndexMarker645"/>configure <code>kubectl</code>, run the following command:</p>
			<pre>$ az aks get-credentials -n test-cluster -g test-resource-group</pre>
			<p>By doing this, you will have successfully set up a Kubernetes cluster and configured <code>kubectl</code>.</p>
			<h4>Amazon Web Services</h4>
			<p>AWS provides a <a id="_idIndexMarker646"/>managed Kubernetes service called Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>). You can <a id="_idIndexMarker647"/>start using it by accessing the AWS web <a id="_idIndexMarker648"/>console at <a href="https://console.aws.amazon.com/eks">https://console.aws.amazon.com/eks</a> or using the AWS command-line tool.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">You can check <a id="_idIndexMarker649"/>all the information (and the installation guide) for the AWS command-line tool at its official website: <a href="https://docs.aws.amazon.com/cli/">https://docs.aws.amazon.com/cli/</a>.</p>
			<p>As you can see, using Kubernetes in the cloud is a relatively simple option. Sometimes, however, you may need to install an on-premises Kubernetes environment from scratch on your own server machines. Let's discuss this in the next section.</p>
			<h3>On-premises</h3>
			<p>Installing Kubernetes <a id="_idIndexMarker650"/>from scratch on your own servers makes <a id="_idIndexMarker651"/>sense if you don't want to depend on cloud platforms or if your corporate security policies don't allow it. The installation process is relatively complex and out of the scope of this book, but you can find all the details in the official documentation at <a href="https://kubernetes.io/docs/setup/production-environment/">https://kubernetes.io/docs/setup/production-environment/</a>.</p>
			<p>Now that we have the <a id="_idIndexMarker652"/>Kubernetes environment configured, we can check that <code>kubectl</code> is connected <a id="_idIndexMarker653"/>to the cluster correctly and that we are ready to start deploying our applications.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/>Verifying the Kubernetes setup</h2>
			<p>No matter which Kubernetes server installation you choose, you should already have everything <a id="_idIndexMarker654"/>configured and the Kubernetes client should be filled with the cluster's URL and credentials. You can check this with the following command:</p>
			<pre>$ kubectl cluster-info
Kubernetes control plane is running at https://kubernetes.docker.internal:6443
CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</pre>
			<p>This is the output for the Docker Desktop scenario and is why you can see <code>localhost</code>. Your output may be slightly different and may include more entries. If you see no errors, then everything is correct, and we can start using Kubernetes to run applications.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor162"/>Using Kubernetes</h1>
			<p>We have the whole Kubernetes environment ready and <code>kubectl</code> configured. This means that it's high <a id="_idIndexMarker655"/>time to finally present the power of Kubernetes and deploy our first application. Let's use the <code>leszko/calculator</code> Docker image that we built in the previous chapters and start it in multiple replicas on Kubernetes.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>Deploying an application</h2>
			<p>In order to <a id="_idIndexMarker656"/>start a Docker container on Kubernetes, we need to prepare a deployment configuration as a YAML file. Let's name it <code>deployment.yaml</code>:</p>
			<pre>apiVersion: apps/v1
kind: Deployment                 (1)
metadata:
  name: calculator-deployment    (2)
  labels:
    app: calculator
spec:
  replicas: 3                    (3)
  selector:                      (4)
    matchLabels:
      app: calculator
  template:                      (5)
    metadata:
      labels:                    (6)
        app: calculator
    spec:
      containers:
      - name: calculator         (7)
        image: leszko/calculator (8)
        ports:                   (9)
        - containerPort: 8080</pre>
			<p>In this YAML configuration, we have to ensure the following:</p>
			<ol>
				<li>We have defined <a id="_idIndexMarker657"/>a Kubernetes resource of the <code>Deployment</code> type from the <code>apps/v1</code> Kubernetes API version. </li>
				<li>The unique deployment name is <code>calculator-deployment</code>.</li>
				<li>We have defined that there should be exactly <code>3</code> of the same Pods created.</li>
				<li><code>selector</code> defines how <code>Deployment</code> finds Pods to manage, in this case, just by the label.</li>
				<li><code>template</code> defines the specification for each created Pod.</li>
				<li>Each Pod is labeled with <code>app: calculator</code>.</li>
				<li>Each Pod contains a Docker container named <code>calculator</code>.</li>
				<li>A Docker container was created from the image called <code>leszko/calculator</code>.</li>
				<li>The Pod exposes container port <code>8080</code>.</li>
			</ol>
			<p>To install the deployment, run the following command:</p>
			<pre>$ kubectl apply -f deployment.yaml</pre>
			<p>You can check that the three Pods, each containing one Docker container, have been created:</p>
			<pre>$ kubectl get pods
NAME                                  READY STATUS  RESTARTS AGE
calculator-deployment-dccdf8756-h2l6c 1/1   Running 0        1m
calculator-deployment-dccdf8756-tgw48 1/1   Running 0        1m
calculator-deployment-dccdf8756-vtwjz 1/1   Running 0        1m</pre>
			<p>Each Pod runs a Docker container. We can check its logs by using the following command:</p>
			<pre>$ kubectl logs pods/calculator-deployment-dccdf8756-h2l6c</pre>
			<p>You should see the familiar Spring logo and the logs of our Calculator web service.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">To look at an <a id="_idIndexMarker658"/>overview of <code>kubectl</code> commands, please check out the official guide: <a href="https://kubernetes.io/docs/reference/kubectl/overview/">https://kubernetes.io/docs/reference/kubectl/overview/</a>.</p>
			<p>We have just performed our first deployment to Kubernetes, and with just a few lines of code, we have three <a id="_idIndexMarker659"/>replicas of our Calculator web service application. Now, let's see how we can use the application we deployed. For this, we'll need to understand the concept of a Kubernetes Service.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>Deploying a Kubernetes Service</h2>
			<p>Each Pod has an IP address in the internal Kubernetes network, which means that you can already access <a id="_idIndexMarker660"/>each Calculator instance from another Pod running in the same Kubernetes cluster. But <em class="italic">how do we access our application from the outside?</em> That is the role of a Kubernetes Service.</p>
			<p>The idea of Pods and Services is that Pods are mortal—they get terminated, and then they get restarted. The Kubernetes orchestrator only cares about the right number of Pod replicas, not about the Pod's identity. That's why, even though each Pod has an (internal) IP address, we should not stick to it or use it. Services, on the other hand, act as a frontend for Pods. They have IP addresses (and DNS names) that can be used. Let's look at the following diagram, which presents the idea of a Pod and Service:</p>
			<div><div><img src="img/B18223_06_04.jpg" alt="Figure 6.4 – Kubernetes Pod and Service&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Kubernetes Pod and Service</p>
			<p>Pods are physically placed on different nodes, but you don't have to worry about this since Kubernetes takes care of the right orchestration and introduces the abstraction of a Pod and Service. The user <a id="_idIndexMarker661"/>accesses the Service, which load balances the traffic between the Pod replicas. Let's look at an example of how to create a service for our Calculator application.</p>
			<p>Just like we did for the Deployment, we start from a YAML configuration file. Let's name it <code>service.yaml</code>:</p>
			<pre>apiVersion: v1
kind: Service
metadata:
  name: calculator-service
spec:
  type: NodePort
  selector:
    app: calculator
  ports:
  - port: 8080</pre>
			<p>This is a configuration for a simple service that load balances the traffic to all the Pods that meet the criteria <a id="_idIndexMarker662"/>we mentioned in <code>selector</code>. To install the service, run the following command:</p>
			<pre>$ kubectl apply -f service.yaml</pre>
			<p>You can then check that the service was correctly deployed by running the following command:</p>
			<pre>$ kubectl get service calculator-service
NAME               TYPE     CLUSTER-IP    EXTERNAL-IP PORT(S)        AGE
calculator-service NodePort 10.19.248.154 &lt;none&gt;     8080:32259/TCP 13m</pre>
			<p>To check that the service points to the three Pod replicas we created in the previous section, run the following command:</p>
			<pre>$ kubectl describe service calculator-service | grep Endpoints
Endpoints: 10.16.1.5:8080,10.16.2.6:8080,10.16.2.7:8080</pre>
			<p>From the last two commands we ran, we can see that the service is available under the IP address of <code>10.19.248.154</code> and that it load balances the traffic to three Pods with the IPs of <code>10.16.1.5</code>, <code>10.16.2.6</code>, and <code>10.16.2.7</code>. All of these IP addresses, for both the Service and Pod, are internal to the Kubernetes cluster network.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">To read more <a id="_idIndexMarker663"/>about Kubernetes Services, please visit the official Kubernetes website at <a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a>.</p>
			<p>In the next section, we'll take a look at how to access a service from outside the Kubernetes cluster.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/>Exposing an application</h2>
			<p>To understand how <a id="_idIndexMarker664"/>your application can be accessed from the outside, we need to start with the types of Kubernetes Services. You can use four different service types, as follows:</p>
			<ul>
				<li><strong class="bold">ClusterIP (default)</strong>: The service has an internal IP only.</li>
				<li><code>&lt;NODE-IP&gt;:&lt;NODE-PORT&gt;</code>.</li>
				<li><strong class="bold">LoadBalancer</strong>: Creates an external load balancer and assigns a separate external IP for the service. Your Kubernetes cluster must support external load balancers, which works fine in the case of cloud platforms, but may not work if you use minikube.</li>
				<li><code>externalName</code> in the spec).</li>
			</ul>
			<p>If you use a Kubernetes instance that's been deployed on a cloud platform (for example, GKE), then the <a id="_idIndexMarker665"/>simplest way to expose your service is to use <code>kubectl get service</code> command. If we had used it in our configuration, then you could have accessed the Calculator service at <code>http://&lt;EXTERNAL-IP&gt;:8080</code>.</p>
			<p>While LoadBalancer seems to be the simplest solution, it has two drawbacks:</p>
			<ul>
				<li>First, it's not always available, for example, if you deployed on-premises Kubernetes or used minikube.</li>
				<li>Second, external public IPs are usually expensive. A different solution is to use a <code>NodePort</code> service, as we did in the previous section. </li>
			</ul>
			<p>Now, let's see how we can access our service.</p>
			<p>We can repeat the same command we ran already:</p>
			<pre>$ kubectl get service calculator-service
NAME               TYPE     CLUSTER-IP    EXTERNAL-IP  PORT(S)        AGE
calculator-service NodePort 10.19.248.154 &lt;none&gt;       8080:32259/TCP 13m</pre>
			<p>You can see that port <code>32259</code> was selected as a node port. This means that we can access our Calculator service using that port and the IP of any of the Kubernetes nodes.</p>
			<p>The IP address of your Kubernetes node depends on your installation. If you used Docker Desktop, then your node IP is <code>localhost</code>. In the case of minikube, you can check it with the <code>minikube ip</code> command. In the case of cloud platforms or the on-premises installation, you can check the IP addresses with the following command:</p>
			<pre>$ kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type=="ExternalIP")].address }'
35.192.180.252 35.232.125.195 104.198.131.248</pre>
			<p>To check that you can access Calculator from the outside, run the following command:</p>
			<pre>$ curl &lt;NODE-IP&gt;:32047/sum?a=1\&amp;b=2
3</pre>
			<p>We made an HTTP request to one of our Calculator container instances and it returned the right response, which <a id="_idIndexMarker666"/>means that we successfully deployed the application on Kubernetes.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <code>kubectl</code> command offers a shortcut to create a service without using YAML. Instead of the configuration we used, you could just execute the following command:</p>
			<p class="callout"><code>$ kubectl expose deployment calculator-deployment --type=NodePort --name=calculator-service</code>.</p>
			<p>What we've just learned gives us the necessary basics about Kubernetes. We can now use it for the staging and production environments and, therefore, include it in the continuous delivery process. Before we do so, however, let's look at a few more Kubernetes features that make it a great and useful tool.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor166"/>Advanced Kubernetes</h1>
			<p>Kubernetes provides a way to dynamically modify your deployment during runtime. This is especially important if your application is already running in production and you need to support zero-downtime <a id="_idIndexMarker667"/>deployments. First, let's look at how to scale up an application and then present the general approach Kubernetes takes on any deployment changes.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor167"/>Scaling an application</h2>
			<p>Let's imagine that <a id="_idIndexMarker668"/>our Calculator application is getting popular. People have started using it and the traffic is so high that the three Pod replicas are overloaded. <em class="italic">What can we do now?</em></p>
			<p>Luckily, <code>kubectl</code> provides a simple way to scale up and down deployments using the <code>scale</code> keyword. Let's scale our Calculator deployment to <code>5</code> instances:</p>
			<pre>$ kubectl scale --replicas 5 deployment calculator-deployment</pre>
			<p>That's it, our application is now scaled up:</p>
			<pre>$ kubectl get pods
NAME                                  READY STATUS  RESTARTS AGE
calculator-deployment-dccdf8756-h2l6c 1/1   Running 0        19h
calculator-deployment-dccdf8756-j87kg 1/1   Running 0        36s
calculator-deployment-dccdf8756-tgw48 1/1   Running 0        19h
calculator-deployment-dccdf8756-vtwjz 1/1   Running 0        19h
calculator-deployment-dccdf8756-zw748 1/1   Running 0        36s</pre>
			<p>Note that, from now on, the service we created load balances the traffic to all <code>5</code> Calculator Pods. Also, note that you don't even need to wonder about which physical machine each Pod runs on, since this is covered by the Kubernetes orchestrator. All you have to think about is your desired number of application instances.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Kubernetes also <a id="_idIndexMarker669"/>provides a way to autoscale your Pods, depending on its metrics. This feature is called the <strong class="bold">HorizontalPodAutoscaler</strong>, and <a id="_idIndexMarker670"/>you can read more about it at <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a>.</p>
			<p>We have just <a id="_idIndexMarker671"/>seen how we can scale applications. Now, let's take a more generic look at how to update any part of a Kubernetes deployment.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>Updating an application</h2>
			<p>Kubernetes takes <a id="_idIndexMarker672"/>care of updating your deployments. Let's make a change to <code>deployment.yaml</code> and add a new label to the Pod template:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: calculator-deployment
  labels:
    app: calculator
spec:
  replicas: 5
  selector:
    matchLabels:
      app: calculator
  template:
    metadata:
      labels:
        app: calculator
        <strong class="bold">label: label</strong>
    spec:
      containers:
      - name: calculator
        image: leszko/calculator
        ports:
        - containerPort: 8080</pre>
			<p>Now, if we <a id="_idIndexMarker673"/>repeat this and apply the same deployment, we can observe what happens with the Pods:</p>
			<pre>$ kubectl apply -f deployment.yaml
$ kubectl get pods
NAME                                   READY STATUS      RESTARTS AGE
pod/calculator-deployment-7cc54cfc58-5rs9g 1/1   Running     0    7s
pod/calculator-deployment-7cc54cfc58-jcqlx 1/1   Running     0    4s
pod/calculator-deployment-7cc54cfc58-lsh7z 1/1   Running     0    4s
pod/calculator-deployment-7cc54cfc58-njbbc 1/1   Running     0    7s
pod/calculator-deployment-7cc54cfc58-pbthv 1/1   Running     0    7s
pod/calculator-deployment-dccdf8756-h2l6c  0/1   Terminating 0    20h
pod/calculator-deployment-dccdf8756-j87kg  0/1   Terminating 0    18m
pod/calculator-deployment-dccdf8756-tgw48  0/1   Terminating 0    20h
pod/calculator-deployment-dccdf8756-vtwjz  0/1   Terminating 0    20h
pod/calculator-deployment-dccdf8756-zw748  0/1   Terminating 0    18m</pre>
			<p>We can see that Kubernetes terminated all the old Pods and started the new ones.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">In our example, we modified the deployment of the YAML configuration, not the application itself. However, modifying the application is actually the same. If we make any change to the source code of the application, we need to build a new Docker image with the new version and then update this version in <code>deployment.yaml</code>.</p>
			<p>Every time you change something and run <code>kubectl apply</code>, Kubernetes checks whether there is any change between the existing state and the YAML configuration, and then, if needed, it performs the update operation we described previously.</p>
			<p>This is all well <a id="_idIndexMarker674"/>and good, but if Kubernetes suddenly terminates all Pods, we may end up in a situation where all the old Pods are already killed and none of the new Pods are ready yet. This would make our application unavailable for a moment. <em class="italic">How do we ensure zero-downtime deployments?</em> That's the role of rolling updates.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>Rolling updates</h2>
			<p>A rolling <a id="_idIndexMarker675"/>update entails incrementally terminating old instances and <a id="_idIndexMarker676"/>starting new ones. In other words, the workflow is as follows:</p>
			<ol>
				<li value="1">Terminate one of the old Pods.</li>
				<li>Start a new Pod.</li>
				<li>Wait until the new Pod is ready.</li>
				<li>Repeat <em class="italic">step 1</em> until all old instances are replaced.<p class="callout-heading">Information </p><p class="callout">The concept of a rolling update works correctly only if the new application version is backward compatible with the old application version. Otherwise, we risk having two different incompatible versions at the same time.</p></li>
			</ol>
			<p>To configure it, we need to add the <code>RollingUpdate</code> strategy to our deployment and specify <code>readinessProbe</code>, which makes Kubernetes aware when the Pod is ready. Let's modify <code>deployment.yaml</code>:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: calculator-deployment
  labels:
    app: calculator
spec:
  replicas: 5
  <strong class="bold">strategy:</strong>
    <strong class="bold">type: RollingUpdate</strong>
    <strong class="bold">rollingUpdate:</strong>
      <strong class="bold">maxUnavailable: 25%</strong>
      <strong class="bold">maxSurge: 0</strong>
  selector:
    matchLabels:
      app: calculator
  template:
    metadata:
      labels:
        app: calculator
    spec:
      containers:
      - name: calculator
        image: leszko/calculator
        ports:
        - containerPort: 8080
        <strong class="bold">readinessProbe:</strong>
          <strong class="bold">httpGet:</strong>
             <strong class="bold">path: /sum?a=1&amp;b=2</strong>
             <strong class="bold">port: 8080</strong></pre>
			<p>Let's explain the parameters we used in our configuration:</p>
			<ul>
				<li><code>maxUnavailable</code>: The maximum number of Pods that can be unavailable during the update <a id="_idIndexMarker677"/>process; in our case, Kubernetes won't <a id="_idIndexMarker678"/>terminate at the same time when there's more than one Pod (<em class="italic">75%</em> <em class="italic">* 5</em> desired replicas).</li>
				<li><code>maxSurge</code>: The maximum number of Pods that can be created over the desired number of Pods; in our case, Kubernetes won't create any new Pods before terminating an old one.</li>
				<li><code>path</code> and <code>port</code>: The <a id="_idIndexMarker679"/>endpoint of the container to check for <a id="_idIndexMarker680"/>readiness; an HTTP <code>GET</code> request is sent to <code>&lt;POD-IP&gt;:8080/sum?a=1&amp;b=2</code> and when it finally returns <code>200</code> as the HTTP status code, the Pod is marked as <em class="italic">ready</em>.<p class="callout-heading">Tip</p><p class="callout">By modifying the <code>maxUnavailable</code> and <code>maxSurge</code> parameters, we can decide whether Kubernetes first starts new Pods and later terminates old ones or, as we did in our case, first terminates old Pods and later starts new ones.</p></li>
			</ul>
			<p>We can now apply the deployment and observe that the Pods are updated one by one:</p>
			<pre>$ kubectl apply -f deployment.yaml
$ kubectl get pods
NAME                                   READY STATUS      RESTARTS AGE
calculator-deployment-78fd7b57b8-npphx 0/1   Running     0        4s
calculator-deployment-7cc54cfc58-5rs9g 1/1   Running     0        3h
calculator-deployment-7cc54cfc58-jcqlx 0/1   Terminating 0        3h
calculator-deployment-7cc54cfc58-lsh7z 1/1   Running     0        3h
calculator-deployment-7cc54cfc58-njbbc 1/1   Running     0        3h
calculator-deployment-7cc54cfc58-pbthv 1/1   Running     0        3h</pre>
			<p>That's it, we have just configured a rolling update for our Calculator deployment, which means that we can provide zero-downtime releases.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Kubernetes also provides a different way of running applications. You can use <code>StatefulSet</code> instead of <code>Deployment</code>, and then the rolling update is always enabled (even without specifying any additional strategy).</p>
			<p>Rolling updates are especially important in the context of continuous delivery, because if we deploy very often, then we definitely can't afford any downtime.</p>
			<p class="callout-heading">Tip </p>
			<p class="callout">After playing with Kubernetes, it's good to perform the cleanup to remove all the resources we created. In our case, we can execute the following commands to remove the service and deployment we created:</p>
			<p class="callout"><code>$ kubectl delete -f service.yaml</code></p>
			<p class="callout"><code>$ kubectl delete -f deployment.yaml</code></p>
			<p>We've already presented <a id="_idIndexMarker681"/>all the Kubernetes features that <a id="_idIndexMarker682"/>are needed for the continuous delivery process. Let's look at a short summary and add a few words about other useful features.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor170"/>Kubernetes objects and workloads</h2>
			<p>The execution unit in Kubernetes is always a Pod, which contains one or more (Docker) containers. There are <a id="_idIndexMarker683"/>multiple different resource types to orchestrate Pods:</p>
			<ul>
				<li><strong class="bold">Deployment</strong>: This is the <a id="_idIndexMarker684"/>most common workload, which manages the life cycle of the desired number of replicated Pods.</li>
				<li><strong class="bold">StatefulSet</strong>: This is a <a id="_idIndexMarker685"/>specialized Pod controller that guarantees the ordering and uniqueness of Pods. It is usually associated with data-oriented applications (in which it's not enough to say, <em class="italic">my desired number of replicas is 3</em>, as in the <a id="_idIndexMarker686"/>case of a Deployment, but rather, <em class="italic">I want exactly 3 replicas, with always the same predictable Pod names, and always started in the same order</em>).</li>
				<li><strong class="bold">DaemonSet</strong>: This is a specialized <a id="_idIndexMarker687"/>Pod controller that runs a copy of a Pod on each Kubernetes node.</li>
				<li><strong class="bold">Job/CronJob</strong>: This is a workflow that's dedicated to <a id="_idIndexMarker688"/>task-based operations in which containers are expected to exist successfully.<p class="callout-heading">Information </p><p class="callout">You may also find a Kubernetes resource called <strong class="bold">ReplicationController</strong>, which is deprecated and has <a id="_idIndexMarker689"/>been replaced by Deployment.</p></li>
			</ul>
			<p>Apart from Pod management, there are other Kubernetes objects. The most useful ones that you may often <a id="_idIndexMarker690"/>encounter are as follows:</p>
			<ul>
				<li><strong class="bold">Service</strong>: A component <a id="_idIndexMarker691"/>that acts as an internal load balancer for Pods.</li>
				<li><strong class="bold">ConfigMap</strong>: This decouples configuration from the image content; it can be any data that's defined <a id="_idIndexMarker692"/>separately from the image and then mounted onto the container's filesystem.</li>
				<li><strong class="bold">Secret</strong>: This allows you <a id="_idIndexMarker693"/>to store sensitive information, such as passwords.</li>
				<li><strong class="bold">PersistentVolume/PersistentVolumeClaim</strong>: These allow you to mount a persistent <a id="_idIndexMarker694"/>volume into <a id="_idIndexMarker695"/>a (stateless) container's filesystem.</li>
			</ul>
			<p>Actually, there are many more objects available, and you can even create your own resource definitions. However, the ones we've mentioned here are the most frequently used in practice.</p>
			<p>We already have a good understanding of clustering in Kubernetes, but Kubernetes isn't just about workloads <a id="_idIndexMarker696"/>and scaling. It can also help with resolving dependencies between applications. In the next section, we will approach this topic and describe application dependencies in the context of Kubernetes and the continuous delivery process.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/>Application dependencies</h1>
			<p>Life is easy without dependencies. In real life, however, almost every application links to a database, cache, messaging <a id="_idIndexMarker697"/>system, or another application. In the case of (micro) service architecture, each service needs a bunch of other services to do its work. The monolithic architecture does not eliminate the issue—an application usually has some dependencies, at least to the database.</p>
			<p>Imagine a newcomer joining your development team; <em class="italic">how much time does it take to set up the entire development environment and run the application with all its dependencies?</em></p>
			<p>When it comes to automated acceptance testing, the dependencies issue is no longer only a matter of convenience—it becomes a necessity. While, during unit testing, we could mock the <a id="_idIndexMarker698"/>dependencies, the acceptance testing suite requires a complete environment. <em class="italic">How do we set it up quickly and in a repeatable manner?</em> Luckily, Kubernetes can help thanks to its built-in DNS resolution for Services and Pods.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/>The Kubernetes DNS resolution</h2>
			<p>Let's present the Kubernetes DNS resolution with a real-life scenario. Let's say we would like to deploy a caching service as a separate application and make it available for other services. One of <a id="_idIndexMarker699"/>the best in-memory caching solutions is Hazelcast, so let's use it here. In the case of the Calculator application, we need <code>Deployment</code> and <code>Service</code>. Let's define them both in one file, <code>hazelcast.yaml</code>:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: hazelcast
  labels:
    app: hazelcast
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hazelcast
  template:
    metadata:
      labels:
        app: hazelcast
    spec:
      containers:
      - name: hazelcast
        image: hazelcast/hazelcast:5.0.2
        ports:
        - containerPort: 5701
---
apiVersion: v1
kind: Service
metadata:
  name: hazelcast
spec:
  selector:
    app: hazelcast
  ports:
  - port: 5701</pre>
			<p>Similar to what we did previously for the Calculator application, we will now define the Hazelcast configuration. Let's start it in the same way:</p>
			<pre>$ kubectl apply -f hazelcast.yaml</pre>
			<p>After a few seconds, the Hazelcast caching application should start. You can check its Pod logs with the <code>kubectl logs</code> command. We also created a service of a default type (<code>ClusterIP</code>, which is only exposed inside the same Kubernetes cluster).</p>
			<p>So far, so good—we did <a id="_idIndexMarker700"/>nothing different from what we've already seen in the case of the Calculator application. Now comes the most interesting part. Kubernetes provides a way of resolving a service IP using the service name. What's even more interesting is that we know the <code>Service</code> name upfront—in our case, it's always <code>hazelcast</code>. So, if we use this as the cache address in our application, the dependency will be automatically resolved.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Actually, Kubernetes DNS resolution is even more powerful, and it can resolve Services in a different <a id="_idIndexMarker701"/>Kubernetes namespace. Read more at <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</a>.</p>
			<p>Before we show you how to implement caching inside the Calculator application, let's take a moment to overview the system we will build.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>Multiapplication system overview</h2>
			<p>We already have the Hazelcast server deployed on Kubernetes. Before we modify our Calculator <a id="_idIndexMarker702"/>application so that we can use it as a caching provider, let's take a look at a diagram of the complete system we want to build:</p>
			<div><div><img src="img/B18223_06_05.jpg" alt="Figure 6.5 – Sample multiapplication deployment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Sample multiapplication deployment</p>
			<p>The user uses the <code>hazelcast</code>). The <strong class="bold">Hazelcast Service</strong> redirects to the <strong class="bold">Hazelcast Pod</strong>.</p>
			<p>If you look at the diagram, you can <a id="_idIndexMarker703"/>see that we have just deployed the Hazelcast part (<strong class="bold">Hazelcast Service</strong> and <strong class="bold">Hazelcast Pod</strong>). We also deployed the Calculator part (<strong class="bold">Calculator Service</strong> and <strong class="bold">Calculator Pod</strong>) in the previous section. The final missing part is the Calculator code to use Hazelcast. Let's implement it now.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/>Multiapplication system implementation</h2>
			<p>To implement <a id="_idIndexMarker704"/>caching with Hazelcast in our Calculator application, we need to do the following:</p>
			<ol>
				<li value="1">Add the Hazelcast client library to Gradle.</li>
				<li>Add the Hazelcast cache configuration.</li>
				<li>Add Spring Boot caching.</li>
				<li>Build a Docker image.</li>
			</ol>
			<p>Let's proceed step by step.</p>
			<h3>Adding the Hazelcast client library to Gradle</h3>
			<p>In <a id="_idIndexMarker705"/>the <code>build.gradle</code> file, add <a id="_idIndexMarker706"/>the <a id="_idIndexMarker707"/>following configuration to the <code>dependencies</code> section:</p>
			<pre>implementation 'com.hazelcast:hazelcast:5.0.2'</pre>
			<p>This adds the Java libraries that take care of communication with the Hazelcast server.</p>
			<h3>Adding the Hazelcast cache configuration</h3>
			<p>Add the <a id="_idIndexMarker708"/>following <a id="_idIndexMarker709"/>parts to the <code>src/main/java/com/leszko/calculator/CalculatorApplication.java</code> file:</p>
			<pre>package com.leszko.calculator;
import com.hazelcast.client.config.ClientConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.context.annotation.Bean;
@SpringBootApplication
<strong class="bold">@EnableCaching</strong>
public class CalculatorApplication {
   public static void main(String[] args) {
      SpringApplication.run(CalculatorApplication.class, args);
   }
   <strong class="bold">@Bean</strong>
   <strong class="bold">public ClientConfig hazelcastClientConfig() {</strong>
      <strong class="bold">ClientConfig clientConfig = new ClientConfig();</strong>
      <strong class="bold">clientConfig.getNetworkConfig().addAddress("hazelcast");</strong>
      <strong class="bold">return clientConfig;</strong>
   <strong class="bold">}</strong>
}</pre>
			<p>This is a <a id="_idIndexMarker710"/>standard Spring cache configuration. Note <a id="_idIndexMarker711"/>that for the Hazelcast server address, we use <code>hazelcast</code>, which is automatically available thanks to the Kubernetes DNS resolution.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In real life, if you use Hazelcast, you don't even need to specify the service name, since Hazelcast provides an autodiscovery plugin dedicated to the Kubernetes environment. Read more at <a href="https://docs.hazelcast.com/hazelcast/latest/deploy/deploying-in-kubernetes.html">https://docs.hazelcast.com/hazelcast/latest/deploy/deploying-in-kubernetes.html</a>.</p>
			<p>We also need to remove the Spring context test automatically created by Spring Initializr, <code>src/test/java/com/leszko/calculator/CalculatorApplicationTests.java</code>.</p>
			<p>Next, let's add caching to the Spring Boot service.</p>
			<h3>Adding Spring Boot caching</h3>
			<p>Now that <a id="_idIndexMarker712"/>the cache is configured, we <a id="_idIndexMarker713"/>can finally add caching to our web service. In order to do this, we need to change the <code>src/main/java/com/leszko/calculator/Calculator.java</code> file so that it looks as follows:</p>
			<pre>package com.leszko.calculator;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;
@Service
public class Calculator {
   <strong class="bold">@Cacheable("sum")</strong>
   public int sum(int a, int b) {
      <strong class="bold">try {</strong>
         <strong class="bold">Thread.sleep(3000);</strong>
      <strong class="bold">}</strong>
      <strong class="bold">catch (InterruptedException e) {</strong>
         <strong class="bold">e.printStackTrace();</strong>
      <strong class="bold">}</strong>
      return a + b;
   }
}</pre>
			<p>We added the <code>@Cacheable</code> annotation to make Spring automatically cache every call of the <code>sum()</code> method. We also added sleeping for 3 seconds, just for the purpose of testing, so that we could see that the cache works correctly.</p>
			<p>From now on, the <a id="_idIndexMarker714"/>sum calculations are <a id="_idIndexMarker715"/>cached in Hazelcast, and when we call the <code>/sum</code> endpoint of the Calculator web service, it will first try to retrieve the result from the cache. Now, let's build our application.</p>
			<h3>Building a Docker image</h3>
			<p>As the next <a id="_idIndexMarker716"/>step, we need to remove the <a id="_idIndexMarker717"/>Spring default context test, <code>src/test/java/com/leszko/calculator/CalculatorApplicationTests.java</code> (to avoid failing because of the missing Hazelcast dependency).</p>
			<p>Now, we can rebuild the Calculator application and the Docker image with a new tag. Then, we will push it to Docker Hub once more:</p>
			<pre>$ ./gradlew build
$ docker build -t leszko/calculator:caching .
$ docker push leszko/calculator:caching</pre>
			<p>Obviously, you should change <code>leszko</code> to your Docker Hub account.</p>
			<p>The application is ready, so let's test it all together on Kubernetes.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/>Multiapplication system testing</h2>
			<p>We should already have the Hazelcast caching server deployed on Kubernetes. Now, let's change the <a id="_idIndexMarker718"/>deployment for the Calculator application to use the <code>leszko/calculator:caching</code> Docker image. You need to modify <code>image</code> in the <code>deployment.yaml</code> file:</p>
			<pre>image: leszko/calculator:caching</pre>
			<p>Then, apply the Calculator deployment and service:</p>
			<pre>$ kubectl apply -f deployment.yaml
$ kubectl apply -f service.yaml</pre>
			<p>Let's repeat the <code>curl</code> operation we did before:</p>
			<pre>$ curl &lt;NODE-IP&gt;:&lt;NODE-PORT&gt;/sum?a=1\&amp;b=2</pre>
			<p>The first time you execute it, it should reply in 3 seconds, but all subsequent calls should be instant, which means that caching works correctly.</p>
			<p class="callout-heading">Tip </p>
			<p class="callout">If you're interested, you can also check the logs of the Calculator Pod. You should see some logs there that confirm that the application is connected to the Hazelcast server:</p>
			<p class="callout"><code>Members [1] {</code></p>
			<p class="callout"><code>    Member [10.16.2.15]:5701 - 3fca574b-bbdb-4c14-ac9d-73c45f56b300</code></p>
			<p class="callout"><code>}</code></p>
			<p>You can probably already see how we could perform acceptance testing on a multicontainer system. All we need is an acceptance test specification for the whole system. Then, we could deploy the complete system into the Kubernetes staging environment and run a suite of acceptance tests against it. We'll talk about this in more detail in <a href="B18223_08_ePub.xhtml#_idTextAnchor218"><em class="italic">Chapter 8</em></a>, <em class="italic">Continuous Delivery Pipeline</em>.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">In our example, the dependent service was related to caching, which doesn't really change the functional acceptance tests we created in <a href="B18223_05_ePub.xhtml#_idTextAnchor133"><em class="italic">Chapter 5</em></a>, <em class="italic">Automated Acceptance Testing</em>.</p>
			<p>That's all we need to know about how to approach dependent applications that are deployed on <a id="_idIndexMarker719"/>the Kubernetes cluster in the context of continuous delivery. Nevertheless, before we close this chapter, let's write a few words about Kubernetes' competitors, that is, other popular cluster management systems.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Alternative cluster management systems</h1>
			<p>Kubernetes is not the only system that can be used to cluster Docker containers. Even though it's currently the most popular one, there may be some valid reasons to use different software. Let's walk through the alternatives.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor177"/>Docker Swarm</h2>
			<p>Docker Swarm is a native clustering system for Docker that turns a set of Docker hosts into one consistent <a id="_idIndexMarker720"/>cluster, called a <strong class="bold">swarm</strong>. Each host connected to the swarm <a id="_idIndexMarker721"/>plays the role of a manager or a worker (there must be at least one manager in a cluster). Technically, the physical location of the machines does not matter; however, it's <a id="_idIndexMarker722"/>reasonable to have all Docker hosts inside one local network; otherwise, managing operations (or reaching a consensus between multiple managers) can take a significant amount of time.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">Since Docker 1.12, Docker Swarm is natively integrated into Docker Engine in swarm mode. In older versions, it was necessary to run the swarm container on each of the hosts to provide the clustering functionality.</p>
			<p>Let's look at the following diagram, which presents the terminology and the Docker Swarm clustering process:</p>
			<div><div><img src="img/B18223_06_06.jpg" alt="Figure 6.6 – Docker Swarm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Docker Swarm</p>
			<p>In Docker Swarm mode, a running image is called a <strong class="bold">Service</strong>, as opposed to a <strong class="bold">container</strong>, which is run <a id="_idIndexMarker723"/>on a single Docker host. One service runs a specified number of <strong class="bold">tasks</strong>. A task is an atomic scheduling unit of the swarm that holds the information about the container <a id="_idIndexMarker724"/>and the command that should be run inside the container. A <strong class="bold">replica</strong> is each container that is run on the node. The number of replicas is the expected <a id="_idIndexMarker725"/>number of all containers for the given service.</p>
			<p>We start by specifying a service, the Docker image, and the number of replicas. The manager <a id="_idIndexMarker726"/>automatically assigns tasks to worker nodes. Obviously, each replicated container <a id="_idIndexMarker727"/>is run from the same Docker image. In the context of the presented flow, Docker Swarm can be viewed as a layer on top of the Docker Engine mechanism that is responsible for container orchestration.</p>
			<p class="callout-heading">Information </p>
			<p class="callout">In the first edition of this book, Docker Swarm was used for all the examples that were provided. So, if Docker Swarm is your clustering system of choice, you may want to read the first edition.</p>
			<p>Another alternative to Kubernetes is Apache Mesos. Let's talk about it now.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/>Apache Mesos</h2>
			<p>Apache Mesos is an open source scheduling and clustering system that was started at the University of California, Berkeley, in 2009, long before Docker emerged. It provides an abstraction layer <a id="_idIndexMarker728"/>over CPU, disk space, and RAM. One of the great advantages <a id="_idIndexMarker729"/>of Mesos is that it supports any Linux application, but not necessarily (Docker) containers. This is why it's possible to create a cluster out of thousands of machines and use it for both Docker containers and other programs, for example, Hadoop-based calculations.</p>
			<p>Let's look at the following diagram, which presents the Mesos architecture:</p>
			<div><div><img src="img/B18223_06_07.jpg" alt="Figure 6.7 – Apache Mesos&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Apache Mesos</p>
			<p>Apache Mesos, similar to other clustering systems, has the master-slave architecture. It uses node agents that have been installed on every node for communication, and it provides two types of schedulers:</p>
			<ul>
				<li><strong class="bold">Chronos</strong>: For cron-style <a id="_idIndexMarker730"/>repeating tasks</li>
				<li><strong class="bold">Marathon</strong>: To provide <a id="_idIndexMarker731"/>a REST API to orchestrate services and containers</li>
			</ul>
			<p>Apache Mesos is very <a id="_idIndexMarker732"/>mature compared to other <a id="_idIndexMarker733"/>clustering systems, and it has been adopted in a large number of organizations, such as Twitter, Uber, and CERN.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor179"/>Comparing features</h2>
			<p>Kubernetes, Docker Swarm, and Mesos are all good choices for the cluster management system. All of them are free and open source, and all of them provide important cluster management <a id="_idIndexMarker734"/>features, such as load balancing, service discovery, distributed storage, failure recovery, monitoring, secret management, and rolling updates. All of them can also be used in the continuous delivery process without huge differences. This is because, in the Dockerized infrastructure, they all address the same issue—the clustering of Docker containers. Nevertheless, the systems are not exactly the same. Let's take a look at the following table, which presents the differences:</p>
			<div><div><img src="img/B18223_06_Table_1.jpg" alt=""/>
				</div>
			</div>
			<p>Obviously, apart from Kubernetes, Docker Swarm, and Apache Mesos, there are other clustering systems <a id="_idIndexMarker735"/>available on the market. Especially in the era of cloud platforms, there are very popular platform-specific systems, for example, Amazon <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>). The good news is that if you understand <a id="_idIndexMarker736"/>the idea of clustering Docker containers, then using another system won't be difficult for you.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/>Summary</h1>
			<p>In this chapter, we took a look at the clustering methods for Docker environments that allow you to set up complete staging and production environments. Let's go over some of the key takeaways from this chapter:</p>
			<ul>
				<li>Clustering is a method of configuring a set of machines in a way that, in many respects, can be viewed as a single system.</li>
				<li>Kubernetes is the most popular clustering system for Docker.</li>
				<li>Kubernetes consists of the Kubernetes server and the Kubernetes client (<code>kubectl</code>).</li>
				<li>The Kubernetes server can be installed locally (through minikube or Docker Desktop), on the cloud platform (AKS, GKE, or EKS), or manually on a group of servers. Kubernetes uses YAML configurations to deploy applications.</li>
				<li>Kubernetes provides features such as scaling and rolling updates out of the box.</li>
				<li>Kubernetes provides DNS resolution, which can help when you're deploying systems that consist of multiple dependent applications. </li>
				<li>The most popular clustering systems that support Docker are Kubernetes, Docker Swarm, and Apache Mesos.</li>
			</ul>
			<p>In the next chapter, we will describe the configuration management part of the continuous delivery pipeline.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor181"/>Exercises</h1>
			<p>In this chapter, we have covered Kubernetes and the clustering process in detail. In order to enhance this knowledge, we recommend the following exercises:</p>
			<ol>
				<li value="1">Run a <code>hello world</code> application on the Kubernetes cluster:<ol><li>The <code>hello world</code> application can look exactly the same as the one we described in the exercises for <a href="B18223_02_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Docker</em>.</li><li>Deploy the application with three replicas.</li><li>Expose the application with the <code>NodePort</code> service.</li><li>Make a request (using <code>curl</code>) to the application.</li></ol></li>
				<li>Implement a new feature, <em class="italic">Goodbye World!</em>, and deploy it using a rolling update:<ol><li>This feature can be added as a new endpoint, <code>/bye</code>, which always returns <em class="italic">Goodbye World!</em>.</li><li>Rebuild a Docker image with a new version tag.</li><li>Use the <code>RollingUpdate</code> strategy and <code>readinessProbe</code>.</li><li>Observe the rolling update procedure.</li><li>Make a request (using <code>curl</code>) to the application.</li></ol></li>
			</ol>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Questions</h1>
			<p>To verify your knowledge from this chapter, please answer the following questions:</p>
			<ol>
				<li value="1">What is a server cluster?</li>
				<li>What is the difference between a Kubernetes control plane and Kubernetes Node?</li>
				<li>Name at least three cloud platforms that provide a Kubernetes environment out of the box.</li>
				<li>What is the difference between a Kubernetes deployment and service?</li>
				<li>What is the Kubernetes command for scaling deployments?</li>
				<li>Name at least two cluster management systems other than Kubernetes.</li>
			</ol>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Further reading</h1>
			<p>To find out more about Kubernetes, please refer to the following resources:</p>
			<ul>
				<li><strong class="bold">Kubernetes official documentation</strong>: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
				<li><strong class="bold">Nigel Poulton: The Kubernetes Book</strong> (<a href="https://leanpub.com/thekubernetesbook">https://leanpub.com/thekubernetesbook</a>)</li>
			</ul>
		</div>
	</body></html>