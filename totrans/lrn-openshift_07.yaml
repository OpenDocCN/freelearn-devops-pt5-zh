- en: Managing Persistent Storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理持久存储
- en: 'In the previous chapter, we described how to install an OpenShift cluster using
    an advanced installation method. The next step of the installation process is
    to make persistent storage available for OpenShift users. In [Chapter 1](part0021.html#K0RQ0-78aafb146b304cdeb9b3261a70edabde),
    *Containers and Docker Overview*, we already how to use Docker persistent volumes. Usually,
    we do not need any for development or testing purposes, but it is not the case
    with production environments where we need to store persistent data in certain
    cases. In this chapter, we will describe the persistent storage concept regarding
    the OpenShift infrastructure. We will also explain the need for using persistent
    storage in a production environment. The focus of this chapter is all about configuring
    an infrastructure to support persistent storage. This includes the following storage
    types: NFS, GlusterFS, iSCSI, and more. Besides infrastructure preparations, this
    chapter covers how to leverage persistent storage in OpenShift using **Persistent
    Volumes** (**PVs**) and **Persistent Volume Claims** (**PVCs**). Lastly, we will
    show you how to use persistent storage in your pods/applications that are deployed
    on OpenShift.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们描述了如何使用高级安装方法安装 OpenShift 集群。安装过程的下一步是为 OpenShift 用户提供持久存储。在 [第 1 章](part0021.html#K0RQ0-78aafb146b304cdeb9b3261a70edabde)
    *容器和 Docker 概述* 中，我们已经介绍了如何使用 Docker 持久卷。通常，开发或测试时不需要持久存储，但在生产环境中，我们有时需要存储持久数据。本章将描述有关
    OpenShift 基础设施的持久存储概念。我们还将解释在生产环境中使用持久存储的必要性。本章的重点是如何配置基础设施以支持持久存储。这包括以下存储类型：NFS、GlusterFS、iSCSI
    等。除了基础设施准备之外，本章还介绍了如何在 OpenShift 中使用 **持久卷**（**PVs**）和 **持久卷声明**（**PVCs**）来利用持久存储。最后，我们将展示如何在部署在
    OpenShift 上的 Pods/应用程序中使用持久存储。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Persistent versus ephemeral storage
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久存储与临时存储
- en: OpenShift persistent storage concept
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 持久化存储概念
- en: Storage backends comparison
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储后端比较
- en: Storage infrastructure setup
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储基础设施设置
- en: Configuring PVs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 PVs
- en: Using persistent storage in Pods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Pods 中使用持久存储
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The learning environment for this chapter consists of two VMs with the following
    characteristics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习环境由两台虚拟机组成，具有以下特点：
- en: '| **Hostname** | **RAM** | **vCPU** | **OS** |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **主机名** | **内存** | **vCPU** | **操作系统** |'
- en: '| `openshift.example.com` | 4GB | 2 | CentOS 7 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| `openshift.example.com` | 4GB | 2 | CentOS 7 |'
- en: '| `storage.example.com` | 2GB | 1 | CentOS 7 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| `storage.example.com` | 2GB | 1 | CentOS 7 |'
- en: These machines can be deployed anywhere (bare metal, VMware, OpenStack, AWS,
    and so on). However, for educational purposes, we recommend using the Vagrant
    + VirtualBox/libvirt configuration to simplify the process of deployment and re-deployment
    of our virtual environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机器可以部署在任何地方（裸机、VMware、OpenStack、AWS 等）。然而，出于教育目的，我们建议使用 Vagrant + VirtualBox/libvirt
    配置，以简化虚拟环境的部署和重新部署过程。
- en: 'We also assume that all servers are accessible via both FQDNs and short names.
    This requires configuring `/etc/hosts` records, which is shown as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设所有服务器都可以通过 FQDN 和短名称访问。这需要配置 `/etc/hosts` 记录，具体配置如下：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The IPs must be the same as the ones that are specified in the following Vagrantfile.
    If you only want to use one machine for this lab, configure the `/etc/hosts` file
    to point both records to the same machine.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 IP 地址必须与以下 Vagrantfile 中指定的 IP 地址相同。如果你只想在此实验中使用一台机器，可以将 `/etc/hosts` 文件配置为将两个记录指向同一台机器。
- en: 'Lab environment deployment can be simplified by using the following `Vagrantfile`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用以下 `Vagrantfile` 来简化实验环境的部署：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It's not mandatory to use the same IPs that were used in the preceding code.
    What is important is that you point your `/etc/hosts` records to them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不必使用前面代码中使用的相同 IP 地址。重要的是将你的 `/etc/hosts` 记录指向这些 IP。
- en: Persistent versus ephemeral storage
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久存储与临时存储
- en: By default, OpenShift/Kubernetes containers don't store data persistently. We
    can start an application and OpenShift will start a new container from an immutable
    Docker image. It uses an ephemeral storage, which means that data is available
    until the container is deleted or rebuilt. If our application (and all related
    containers) has been rebuilt, all data will be lost. Still, this approach is fine
    for any stateless application. For example, it will work for a simple website
    that doesn't act as a portal and only provides information embedded into HTML/CSS.
    Another example would be a database used for development—usually, no one cares
    if data is lost.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，OpenShift/Kubernetes 容器不会持久化存储数据。我们可以启动一个应用程序，OpenShift 会从一个不可变的 Docker
    镜像启动一个新的容器。它使用的是短暂存储，这意味着数据在容器被删除或重建之前是可用的。如果我们的应用程序（及所有相关容器）被重建，所有数据都会丢失。然而，这种方法对于任何无状态应用程序来说是可行的。例如，它适用于一个简单的网站，这个网站不充当门户，只提供嵌入在
    HTML/CSS 中的信息。另一个例子是用于开发的数据库——通常没有人在乎数据是否丢失。
- en: Let's consider another example. Imagine that we need a database for a WordPress
    container. If we store database files on an ephemeral storage, we can lose all
    our data if the database container was rebuilt or deleted. We cannot allow our database
    files to be deleted or lost. OpenShift can rebuild our database container without
    any issues. It will give us a working instance of a database but without required
    databases/table structures and data in the tables. From the application's perspective,
    this means that all required information is lost. For these kinds of applications
    (stateful), we need a persistent storage that will be available even if the container
    crashed, was deleted, or was rebuilt.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个例子。假设我们需要为 WordPress 容器提供数据库。如果我们将数据库文件存储在短暂存储中，当数据库容器被重建或删除时，我们可能会丢失所有数据。我们不能允许数据库文件被删除或丢失。OpenShift
    可以无问题地重建我们的数据库容器。它将给我们一个工作实例的数据库，但没有所需的数据库/表结构和表中的数据。从应用程序的角度来看，这意味着所有必需的信息都丢失了。对于这些类型的应用程序（有状态的），我们需要一个持久存储，即使容器崩溃、被删除或重建，它依然可用。
- en: Storage requirements (ephemeral versus persistent) are dependent on your particular
    use case.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存储需求（短暂存储与持久存储）取决于您的具体使用案例。
- en: The OpenShift persistent storage concept
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 持久存储概念
- en: OpenShift uses the **Persistent Volume** (**PV**) concept to allow administrators
    to provide persistent storage for a cluster and then let developers request storage
    resources via **Persistent Volume Claims** (**PVC**). Thus, end users can request
    storage without having deep knowledge of the underlying storage infrastructure.
    At the same time, administrators can configure the underlying storage infrastructure
    and make it available to end users via the PV concept.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 使用**持久卷**（**PV**）概念，允许管理员为集群提供持久存储，然后让开发人员通过**持久卷声明**（**PVC**）请求存储资源。因此，最终用户可以在无需深入了解底层存储基础设施的情况下请求存储。同时，管理员可以配置底层存储基础设施，并通过
    PV 概念将其提供给最终用户。
- en: PV resources are shared across the OpenShift cluster since any of them can (if
    it is allowed) potentially be used by any users/projects. On the other hand, PVC
    resources are specific to a project (namespace) and they are usually created and
    used by end users, such as developers. Once PVC resources are created, OpenShift
    tries to find a suitable PV resource that matches specific criteria, like size
    requirements, access mode (RWO, ROX, RWX), and so on.  If PV has been found to
    satisfy the request from the PVC, OpenShift binds that PV to our PVC. Once this
    is complete, PV cannot be bound to additional PVCs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PV 资源在 OpenShift 集群中是共享的，因为其中任何一个都可以（如果允许的话）被任何用户/项目使用。另一方面，PVC 资源是特定于某个项目（命名空间）的，通常由最终用户（如开发人员）创建和使用。一旦
    PVC 资源被创建，OpenShift 会尝试找到一个合适的 PV 资源，满足特定的标准，比如大小要求、访问模式（RWO、ROX、RWX）等。如果找到了满足
    PVC 请求的 PV，OpenShift 会将该 PV 绑定到我们的 PVC。一旦绑定完成，PV 将不能再绑定到其他 PVC。
- en: 'This concept is shown in the following screenshot:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念在以下截图中有所展示：
- en: '![](img/00049.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpeg)'
- en: OpenShift Pod, PV, PVC, and the storage relationship
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift Pod、PV、PVC 和存储关系
- en: Persistent Volumes
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷
- en: PVs are represented by a PersistentVolume OpenShift API object, which describes
    an existing piece of storage infrastructure like NFS share, GlusterFS volume,
    iSCSI target, a Ceph RBD device, and so on. It is assumed that the underlying
    storage component already exists and is ready to be consumed by the OpenShift
    cluster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PV 由一个 PersistentVolume OpenShift API 对象表示，该对象描述了现有的存储基础设施，如 NFS 共享、GlusterFS
    卷、iSCSI 目标、Ceph RBD 设备等。假定底层存储组件已经存在并准备好被 OpenShift 集群使用。
- en: PVs have their own life cycle, which is independent of any pods that use PV.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PV（持久卷）有其自己的生命周期，这与任何使用 PV的 pod 是独立的。
- en: High availability of storage in the infrastructure is left to the underlying
    storage provider.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施中的存储高可用性由底层存储提供商负责。
- en: Persistent Volume Claims
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷声明（PVC）
- en: As I mentioned previously, OpenShift users can request storage resources for
    their applications by means of PVCs that are defined by a `PersistentVolumeClaim`
    OpenShift API object. PVC represents a request made by an end user (usually developers).
    PVC consumes PV resources.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，OpenShift 用户可以通过 PVC（由 `PersistentVolumeClaim` OpenShift API 对象定义）请求应用程序所需的存储资源。PVC
    代表终端用户（通常是开发人员）提出的请求。PVC 消耗 PV 资源。
- en: 'A PVC contains some important information regarding resources that are requested
    by applications/users:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PVC 包含一些关于应用程序/用户请求的资源的重要信息：
- en: Size needed
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需大小
- en: Access mode
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问模式
- en: 'There are several access modes that can be used in the OpenShift infrastructure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenShift 基础架构中，可以使用几种访问模式：
- en: '| **Mode** | **Description** | **Examples** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **模式** | **描述** | **示例** |'
- en: '| ReadOnlyMany | The volume can be mounted read-only by many nodes. | NFS in
    RO mode |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ReadOnlyMany | 卷可以被多个节点以只读模式挂载。 | NFS 只读模式 |'
- en: '| ReadWriteOnce | The volume can be mounted as read-write by a single node.
    | iSCSI-based xfs, and so on |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ReadWriteOnce | 卷可以被单个节点以读写模式挂载。 | 基于 iSCSI 的 xfs 等 |'
- en: '| ReadWriteMany | The volume can be mounted as read-write by many nodes. |
    GlusterFSNFS |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ReadWriteMany | 卷可以被多个节点以读写模式挂载。 | GlusterFS，NFS |'
- en: Once a PVC resource is created, OpenShift has to find a suitable PV resource
    and bind it to the PVC. If the binding is successful, the PVC resource can be
    consumed by an application.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 PVC 资源被创建，OpenShift 必须找到合适的 PV 资源并将其绑定到 PVC。如果绑定成功，PVC 资源可以被应用程序使用。
- en: The storage life cycle in OpenShift
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 中的存储生命周期
- en: 'The interaction between PV and PVC resources is comprised of several steps,
    which are shown in the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PV 和 PVC（持久卷声明）资源之间的交互包括多个步骤，如下图所示：
- en: '![](img/00050.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpeg)'
- en: 'OpenShift: storage lifecycle'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'OpenShift: 存储生命周期'
- en: OpenShift cluster administrators can configure dynamic PV provisioning or configure
    PV resources in advance. Once a user has requested a storage resource using the
    PVC with specific size and access mode requirements, OpenShift looks for an available
    PV resource. The user always gets what they ask for, at least. In order to keep
    storage usage to a minimum, OpenShift binds the smallest PV that matches all criteria.
    A PVC remains unbound until a suitable PV is found. If there is a volume matching
    all criteria, OpenShift software binds them together. Starting from this step,
    storage can be used by pods. A pod consumes PVC resources as volumes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 集群管理员可以配置动态 PV 配置，或提前配置 PV 资源。一旦用户使用 PVC 请求具有特定大小和访问模式要求的存储资源，OpenShift
    将查找可用的 PV 资源。至少，用户总是能获得他们要求的资源。为了将存储使用保持在最低限度，OpenShift 会绑定最小的符合所有标准的 PV。PVC 会一直处于未绑定状态，直到找到合适的
    PV。如果有符合所有标准的卷，OpenShift 软件会将它们绑定在一起。从这一步骤开始，存储可以被 pod 使用。一个 pod 消耗 PVC 资源作为卷。
- en: OpenShift inspects the claim to find the bound volume and mounts that volume
    to the pod. For those volumes that support multiple access modes, the user specifies
    which mode is desired when using their claim as a volume in a pod.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 检查声明，找到绑定的卷并将该卷挂载到 pod。对于那些支持多种访问模式的卷，用户在将其作为 pod 中的卷使用时，指定希望使用的模式。
- en: Users can delete PVC objects, which allows reclamation of storage resources.
    If PVC is deleted, the volume is considered as *released* but is not yet immediately
    available to be bound to other claims. This requires that data stored on the volumes
    are handled according to the reclaim policy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以删除 PVC 对象，这样就可以回收存储资源。如果 PVC 被删除，该卷被视为 *已释放*，但尚未立即可以绑定到其他声明。这要求根据回收策略处理存储在卷上的数据。
- en: 'The reclaim policy defines the way OpenShift understands what to do with the
    volume after it is released. The following reclaim policies are supported:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回收策略定义了 OpenShift 在释放卷后如何处理卷的方式。支持以下回收策略：
- en: '| **Policy** | **Description** |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **策略** | **描述** |'
- en: '| Retain | Allows manual reclamation of the resource for those volume plugins
    that support it. In this case, storage administrators should delete data manually.
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 保留 | 允许手动回收支持此功能的卷插件的资源。在这种情况下，存储管理员应手动删除数据。 |'
- en: '| Delete | Deletes both the PV object from the OpenShift Container Platform
    and the associated storage asset in external infrastructures, such as AWS EBS,
    GCE PD, or Cinder volume. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 删除 | 从 OpenShift 容器平台中删除 PV 对象以及在外部基础设施（如 AWS EBS、GCE PD 或 Cinder 卷）中关联的存储资产。
    |'
- en: Storage backends comparison
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储后端比较
- en: OpenShift supports a number of persistent storage backends that work differently.
    Some of them support reads/writes from many clients (like NFS), while others support
    only one mount.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 支持多种不同工作方式的持久存储后端。其中一些支持多客户端的读写（如 NFS），而其他的则仅支持单个挂载。
- en: 'The following table contains a comparison of supported storage backends/plugins:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下表包含了支持的存储后端/插件的比较：
- en: '| **Volume backend** | **ReadWriteOnce** | **ReadWriteMany** | **ReadOnlyMany**
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **卷后端** | **ReadWriteOnce** | **ReadWriteMany** | **ReadOnlyMany** |'
- en: '| AWS EBS | Yes |  |   |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| AWS EBS | 是 |  |   |'
- en: '| Azure Disk | Yes |  |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Azure Disk | 是 |  |  |'
- en: '| Ceph RBD | Yes |  | Yes |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Ceph RBD | 是 |  | 是 |'
- en: '| Fibre Channel | Yes |  | Yes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 光纤通道 | 是 |  | 是 |'
- en: '| GCE Persistent Disk | Yes |  |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GCE Persistent Disk | 是 |  |  |'
- en: '| GlusterFS | Yes | Yes | Yes |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GlusterFS | 是 | 是 | 是 |'
- en: '| HostPath | Yes |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| HostPath | 是 |  |  |'
- en: '| iSCSI | Yes |  | Yes |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| iSCSI | 是 |  | 是 |'
- en: '| NFS (Network File System) | Yes | Yes | Yes |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| NFS（网络文件系统） | 是 | 是 | 是 |'
- en: '| OpenStack Cinder | Yes |  |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| OpenStack Cinder | 是 |  |  |'
- en: '| VMware vSphere | Yes |  |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| VMware vSphere | 是 |  |  |'
- en: '| Local | Yes |  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 是 |  |  |'
- en: '`HostPath` allows you to mount persistent storage directly from the node your
    pod runs on and as such is not suitable for production usage. Please only use
    it for testing or development purposes.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`HostPath` 允许您直接从运行您的 Pod 的节点挂载持久存储，因此不适合生产使用。请仅在测试或开发目的下使用它。'
- en: 'There are two types of supported storage in an OpenShift cluster:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenShift 集群中支持两种类型的存储：
- en: Filesystem-based storage (like NFS, Gluster, and HostPath)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于文件系统的存储（如 NFS、Gluster 和 HostPath）
- en: Block-based storage (like iSCSI, OpenStack Cinder, and so on)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于块的存储（如 iSCSI、OpenStack Cinder 等）
- en: Docker containers need file system-based storage to use as a persistent volume.
    This means that OpenShift can use file system-based storage directly.  OpenShift
    needs to create a file system on block storage before using it as a persistent
    volume. For example, if an iSCSI block device is provided, the cluster administrator
    has to define what file system will be created on the block device during the
    PV creation process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 容器需要基于文件系统的存储作为持久卷使用。这意味着 OpenShift 可以直接使用基于文件系统的存储。在使用作为持久卷之前，OpenShift
    需要在块存储上创建文件系统。例如，如果提供了一个 iSCSI 块设备，集群管理员在 PV 创建过程中必须定义将在块设备上创建的文件系统。
- en: Storage infrastructure setup
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储基础设施设置
- en: Configuring the underlying storage infrastructure is usually a task for storage
    administrators. This requires a number of settings and design decisions so that
    you can achieve the expected level of durability, availability, and performance.
    This requires a significant knowledge of underlying resources, physical infrastructure,
    networking, and so on. Once the storage subsystem has been configured properly
    by storage administrators, OpenShift cluster administrators can leverage it to
    create PVs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 配置底层存储基础设施通常是存储管理员的任务。这需要一些设置和设计决策，以便达到预期的耐用性、可用性和性能水平。这需要对底层资源、物理基础设施、网络等有重要的了解。一旦存储子系统由存储管理员正确配置，OpenShift
    集群管理员可以利用它来创建 PV。
- en: This book is about OpenShift administration, and thus the configuration of an
    underlying storage technology is out of its scope. However, we want to demonstrate
    how to perform the basic setup of storage infrastructure on Linux systems for
    NFS, GlusterFS, and iSCSI.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讲述的是 OpenShift 管理，因此底层存储技术的配置超出了其范围。然而，我们想演示如何在 Linux 系统上为 NFS、GlusterFS 和
    iSCSI 执行存储基础设施的基本设置。
- en: If you still need to set up a different kind of storage, please refer to the
    relevant documentation. For example, you may find some Ceph storage documentation
    at [https://ceph.com](https://ceph.com); OpenStack Cinder documentation may be
    found on the project homepage at [openstack.org](http://openstack.org).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然需要设置其他类型的存储，请参考相关文档。例如，你可以在 [https://ceph.com](https://ceph.com) 查找到 Ceph
    存储文档；OpenStack Cinder 文档可以在项目主页 [openstack.org](http://openstack.org) 查找到。
- en: 'We have chosen NFS and GlusterFS-based storage for a number of reasons:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了 NFS 和 GlusterFS 基于存储的方式，原因有很多：
- en: Both are file system-based storage solutions
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都是基于文件系统的存储解决方案
- en: Both support the `ReadWriteMany` OpenShift access type
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都支持 `ReadWriteMany` OpenShift 访问类型
- en: Both can easily be configured on any OpenShift cluster nodes
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者都可以轻松地在任何 OpenShift 集群节点上配置
- en: NFS is known to any Linux system administrator
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS 是任何 Linux 系统管理员都熟知的
- en: We also want to demonstrate how to use block-based storage in the OpenShift
    cluster. We have chosen iSCSI-based storage as an example storage, as it is one
    of the easiest ways to go with block-based storage on Linux.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想展示如何在 OpenShift 集群中使用基于块的存储。我们选择了基于 iSCSI 的存储作为示例存储，因为它是 Linux 上使用块存储的最简单方式之一。
- en: Setting up NFS
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 NFS
- en: The **Network File System **(**NFS**) is a client/server filesystem protocol
    that was originally developed by Sun Microsystems in 1984\. NFS allows a user
    on a client computer (NFS client) to access files stored on the NFS server over
    a network, or even over the internet. The NFS server shares one or more NFS shares
    with a number of allowed NFS clients. NFS clients mount NFS shares as regular
    filesystems. No specific application settings are required since NFS is a POSIX
    compliant file system protocol. This is the main reason why NFS is very popular
    as a network storage solution. NFS is supported by Linux kernel by default and
    can be configured on any Linux-based server.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络文件系统** (**NFS**) 是一种客户端/服务器文件系统协议，最初由 Sun Microsystems 于 1984 年开发。NFS 允许客户端计算机（NFS
    客户端）通过网络甚至互联网访问存储在 NFS 服务器上的文件。NFS 服务器与多个允许的 NFS 客户端共享一个或多个 NFS 共享。NFS 客户端将 NFS
    共享挂载为常规文件系统。由于 NFS 是符合 POSIX 的文件系统协议，因此不需要特定的应用程序设置。这是 NFS 作为网络存储解决方案如此受欢迎的主要原因。NFS
    默认由 Linux 内核支持，可以在任何基于 Linux 的服务器上进行配置。'
- en: In this tutorial, we will use a standalone NFS server for providing persistent
    storage to applications that will be deployed on our OpenShift cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用一个独立的 NFS 服务器为我们将在 OpenShift 集群上部署的应用程序提供持久存储。
- en: The installation process that is described is for CentOS 7.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所描述的安装过程适用于 CentOS 7。
- en: 'The NFS installation and configuration process involves several steps:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: NFS 安装和配置过程涉及多个步骤：
- en: Installing NFS packages on the server and clients
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在服务器和客户端上安装 NFS 软件包
- en: Configuring NFS exports on the server
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置服务器上的 NFS 导出
- en: Starting and enabling the NFS service
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动并启用 NFS 服务
- en: Verification or mounting the NFS share(s) on clients
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端上验证或挂载 NFS 共享
- en: Before we begin, we need to deploy two machines, as described in the *Technical
    requirements* section. In this lab, we assume that machines were deployed as VMs
    using Vagrant.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，需要部署两台机器，具体描述请见 *技术要求* 部分。在本实验中，我们假设这些机器是通过 Vagrant 部署为虚拟机的。
- en: 'Bring your Vagrant environment up and log in to `storage` VM:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 启动你的 Vagrant 环境并登录到 `storage` 虚拟机：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Installing NFS packages on the server and clients
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在服务器和客户端上安装 NFS 软件包
- en: 'NFS packages need to be installed on the NFS server, as well as on all OpenShift
    nodes, as they will act as NFS clients. NFS libraries and binaries are provided
    by the `nfs-utils` package:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在 NFS 服务器以及所有 OpenShift 节点上安装 NFS 软件包，因为它们将作为 NFS 客户端。NFS 库和二进制文件由 `nfs-utils`
    包提供：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will configure NFS services on storage.example.com. All configuration is
    done under the `root` account. You can use `sudo -i` command to switch to root.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 storage.example.com 上配置 NFS 服务。所有配置均在 `root` 账户下完成。你可以使用 `sudo -i` 命令切换到
    root 用户。
- en: Configuring NFS exports on the server
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置服务器上的 NFS 导出
- en: This needs to be done on the server-side only. We are going to export several
    file systems under the `/exports` directory. The exports will only be accessible
    by OpenShift nodes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这只需要在服务器端完成。我们将会在 `/exports` 目录下导出多个文件系统。这些导出的文件系统将仅对 OpenShift 节点可访问。
- en: 'OpenShift cluster runs Docker containers using random **User IDs** (**UIDs**).
    It is difficult to predict a UID to give proper NFS permissions, so we have to
    configure the following NFS settings to allow OpenShift to use NFS shares properly:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 集群使用随机 **用户 ID** (**UIDs**) 运行 Docker 容器。由于难以预测 UID 以授予正确的 NFS 权限，因此我们必须配置以下
    NFS 设置，允许 OpenShift 正常使用 NFS 共享：
- en: A share should be owned by the `nfsnobody` user and group.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享应由 `nfsnobody` 用户和组拥有。
- en: A share should have `0700` access permissions.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享应具有 `0700` 访问权限。
- en: A share should be exported using the `all_squash` option. This will be described
    later in this topic.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享应使用 `all_squash` 选项进行导出。稍后将在本主题中描述这一点。
- en: 'Create the required directories and assign them to the proper permissions:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建所需的目录并赋予正确的权限：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Configure the firewall:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置防火墙：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is not required on Vagrant box centos/7 since `firewalld` is disabled by
    default.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Vagrant box centos/7 上不需要此步骤，因为默认情况下 `firewalld` 被禁用。
- en: 'Create an NFS export by adding the following lines to `/etc/exports`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将以下行添加到 `/etc/exports` 文件中创建 NFS 导出：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Instead of providing FQDNs, you may also specify IP addresses of the nodes.
    This will look like as shown in the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以指定节点的 IP 地址，而不是提供 FQDN。如下所示，代码中将显示：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `all_squash` NFS export option configures NFS to map all UIDs to the `nfsnobody`
    user ID.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`all_squash` NFS 导出选项将 NFS 配置为将所有 UID 映射到 `nfsnobody` 用户 ID。'
- en: Starting and enabling the NFS service
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动并启用 NFS 服务
- en: 'We also need to enable and start nfs-server using the `systemctl` command.
    The following snippet shows how to enable and start all services required for
    NFS service:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要使用 `systemctl` 命令启用并启动 nfs-server。以下代码片段展示了如何启用和启动 NFS 服务所需的所有服务：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Verification
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证
- en: 'You may want to check that NFS share was exported properly. The following command
    will show all available exports:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要检查 NFS 共享是否正确导出。以下命令将显示所有可用的导出：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Configuring GlusterFS shares
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 GlusterFS 共享
- en: 'GlusterFS is a free and scalable network file system that is suitable for data-intensive
    tasks, such as cloud storage and media streaming. GlusterFS creates a volume on
    top of one or more storage nodes using *bricks*. A brick represents a file system
    on a storage node. There are several types of GlusterFS volumes defined by the
    placement of data on bricks. More information can be found by following the links
    provided at the end of this chapter. In this chapter, we only need to have a basic
    knowledge of the following volume types:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS 是一个免费的可扩展网络文件系统，适用于数据密集型任务，如云存储和媒体流。GlusterFS 在一个或多个存储节点上创建一个卷，使用
    *砖块*。砖块代表存储节点上的文件系统。根据数据在砖块上的存放方式，定义了几种类型的 GlusterFS 卷。更多信息可以通过本章末尾提供的链接找到。在本章中，我们只需要了解以下几种卷类型的基本知识：
- en: '| **Type** | **Description** |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **类型** | **描述** |'
- en: '| Distributed | All files are distributed between bricks/storage nodes. No
    redundancy is provided by this volume type. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 分布式 | 所有文件在砖块/存储节点之间分布。此卷类型不提供冗余。 |'
- en: '| Replicated | All files are replicated between two or more bricks. Thus, each
    file is stored on at least two bricks, which provides redundancy. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 复制 | 所有文件在两个或多个砖块之间复制。因此，每个文件至少存储在两个砖块上，从而提供冗余。 |'
- en: '| Striped | Each file is striped across several bricks. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 条带化 | 每个文件被分布在多个砖块上。 |'
- en: For this demonstration, we will set up a basic GlusterFS volume on a single
    storage node.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本演示中，我们将在单个存储节点上设置一个基本的 GlusterFS 卷。
- en: Installing packages
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装软件包
- en: 'First, we need to install the GlusterFS packages, which are located in a special
    GlusterFS repository. The `centos-release-gluster312` package configures the GlusterFS
    3.12 repository. We need to install the `glusterfs-server` on the server side
    (`storage.example.com`) and the `glusterfs` package on the client side (`openshift.example.com`):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装 GlusterFS 软件包，这些软件包位于一个特殊的 GlusterFS 仓库中。`centos-release-gluster312`
    包配置了 GlusterFS 3.12 仓库。我们需要在服务器端（`storage.example.com`）安装 `glusterfs-server`，并在客户端（`openshift.example.com`）安装
    `glusterfs` 包：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the GlusterFS packages are installed, we need to start and enable the
    gluster management service—`glusterd`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 GlusterFS 软件包，我们需要启动并启用 Gluster 管理服务 `glusterd`：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Configuring a brick and volume
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置砖块和卷
- en: 'The following steps of GlusterFS volume configuration require that we create
    a brick file system and the volume itself:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 GlusterFS 卷配置步骤要求我们创建一个砖块文件系统和卷本身：
- en: For this lab, we will use the root file system to create a GlusterFS brick.
    This setup can only be used for test and development purposes and is not suitable
    for production usage. All GlusterFS production installations should use separate
    file systems for GlusterFS bricks, preferably located on separate physical block
    devices.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本实验，我们将使用根文件系统来创建GlusterFS砖块。此设置仅适用于测试和开发目的，不适合生产使用。所有GlusterFS生产安装应使用独立的文件系统来存储GlusterFS砖块，最好位于独立的物理块设备上。
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `force` option is required here since we are using the `/` file system
    to create glusterFS volume. If this isn''t provided, you may see the following
    output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用`/`文件系统来创建GlusterFS卷，因此需要使用`force`选项。如果未提供此选项，您可能会看到以下输出：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have created a volume, it is a good time to start it, making it
    available to clients:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个卷，正是启动它、使其对客户端可用的好时机：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Configuring iSCSI
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置iSCSI
- en: The **internet Small Computer Systems Interface** (**iSCSI**) is a client/server
    protocol that provides block-level access to storage devices by carrying SCSI commands
    over a TCP/IP network. Since iSCSI uses the TCP/IP network, it can be used to
    transmit data over **local area networks** (**LANs**), **wide area networks**(**WANs**),
    and the internet, making location-independent data storage and retrieval possible. This protocol allows
    clients (*initiators)* to send SCSI commands to storage devices (*targets*) on
    remote servers. It is a**storage area network** (**SAN**) protocol. iSCSI allows
    clients to work with remote block devices and treat them as locally attached disks. There
    are a number of iSCSI target implementations (like stgtd, LIO target, and so on).
    As a part of this chapter, we will configure LIO target-based iSCSI storage.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**互联网小型计算机系统接口**（**iSCSI**）是一种客户端/服务器协议，通过在TCP/IP网络上传输SCSI命令，为存储设备提供块级访问。由于iSCSI使用TCP/IP网络，它可以在**局域网**（**LAN**）、**广域网**（**WAN**）和互联网中传输数据，实现位置无关的数据存储和检索。该协议允许客户端（*发起者*）向远程服务器上的存储设备（*目标*）发送SCSI命令。它是**存储区域网络**（**SAN**）协议。iSCSI允许客户端与远程块设备一起工作，并将其视为本地连接的磁盘。存在多种iSCSI目标实现（如stgtd、LIO目标等）。在本章中，我们将配置基于LIO目标的iSCSI存储。'
- en: 'The necessary steps to configure an iSCSI target on `storage.example.com` are
    outlined as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 配置`storage.example.com`上的iSCSI目标所需的步骤如下：
- en: 'Install the CLI tool:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装CLI工具：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Enable the `target` service:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`target`服务：
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Configure the firewall:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置防火墙：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Configure the iSCSI export using `targetcli`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`targetcli`配置iSCSI导出：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: All basic configuration options can be found in man `targetcli` under the *QUICKSTART*
    section. For educational purposes, the preceding example exports the iSCSI volume
    to any host. Please be aware that it is not a production-ready configuration.
    In production, you may only want to grant access to the target to certain hosts.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所有基本配置选项都可以在man `targetcli`的*QUICKSTART*部分找到。出于教育目的，前面的示例将iSCSI卷导出到任何主机。请注意，这不是一个生产就绪的配置。在生产环境中，您可能只希望将目标的访问权限授予某些主机。
- en: Client-side verification
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端验证
- en: 'The followings topics will describe how to use NFS, Gluster, and iSCSI storage
    resources inside the OpenShift cluster. However, you can use previously configured
    resources manually as well. Before going to the next topic, we strongly recommend
    verifying that all your resources are configured properly by mounting them on
    the client side. In our case, the client is located on the `openshift.example.com`
    node. Let''s log in to openshift node and switch to root account before we begin:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的主题将描述如何在OpenShift集群内使用NFS、Gluster和iSCSI存储资源。不过，您也可以手动使用之前配置的资源。在进入下一个主题之前，我们强烈建议通过在客户端进行挂载来验证所有资源是否正确配置。在我们的例子中，客户端位于`openshift.example.com`节点。我们在开始之前，先登录到OpenShift节点并切换到root账户：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: NFS verification
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NFS验证
- en: 'To verify that the NFS exports work properly, we need to mount them on the
    `openshift.example.com` node, which is shown in the following code. If all shares
    can be mounted without any issues, you can assume that that share was exported
    properly:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证NFS导出是否正常工作，我们需要在`openshift.example.com`节点上挂载它们，下面的代码展示了这一点。如果所有共享都能顺利挂载，您可以认为该共享已正确导出：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It's assumed that all required packages are already installed by running `yum
    install -y nfs-utils`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设通过运行`yum install -y nfs-utils`，所有必需的包已安装。
- en: GlusterFS verification
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GlusterFS验证
- en: 'The GlusterFS volume can be mounted manually by using the FUSE client. The
    verification procedure looks like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用 FUSE 客户端手动挂载 GlusterFS 卷。验证过程如下：
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create a sample of persistent data to be used later:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个示例持久数据以供以后使用：
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This storage will be used as web server root data storage.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此存储将作为 Web 服务器根数据存储使用。
- en: 'Verify that mount point is available and then unmount the storage:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 验证挂载点是否可用，然后卸载存储：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: iSCSI verification
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iSCSI 验证
- en: 'iSCSI verification assumes that an OpenShift node can access block storage
    devices. If everything goes well, you should see an additional disk at `/proc/partitions`.
    The iSCSI client utilities are provided by the `iscsi-initiator-utils` package.
    Once the package is installed, the `iscsiadm` utility can be used to scan the
    target for iSCSI exports:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI 验证假设 OpenShift 节点可以访问块存储设备。如果一切顺利，你应该会在 `/proc/partitions` 看到一个额外的磁盘。iSCSI
    客户端工具由 `iscsi-initiator-utils` 包提供。安装该包后，可以使用 `iscsiadm` 工具扫描目标以查找 iSCSI 导出：
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can also use the `lsblk` utility to discover block devices that are available
    in the system.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 `lsblk` 工具发现系统中可用的块设备。
- en: Configuring Physical Volumes (PV)
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置物理卷（PV）
- en: As we mentioned previously, OpenShift cluster administrators can create PV resources
    for future usage by OpenShift users.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，OpenShift 集群管理员可以为 OpenShift 用户创建 PV 资源以供将来使用。
- en: This topic assumes that the OpenShift environment is up and running on the `openshift.example.com`
    node. You may use `oc cluster up` or do an advanced OpenShift installation by
    using Ansible.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本主题假设 OpenShift 环境已在 `openshift.example.com` 节点上运行。你可以使用 `oc cluster up` 或通过
    Ansible 进行高级 OpenShift 安装。
- en: 'As we mentioned previously, only cluster administrators can configure PVs.
    So, before you begin the following labs, you have to switch to the admin account:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，只有集群管理员才能配置 PV。因此，在开始以下实验之前，你必须切换到管理员账户：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We recommend creating a new project to perform this `persistent storage`-related
    lab:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议创建一个新项目来进行这个与 `持久存储` 相关的实验：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The client will automatically change the current project to the newly created
    one.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端将自动将当前项目切换到新创建的项目。
- en: 'In the upcoming examples, we will create the following PVs:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中，我们将创建以下 PV：
- en: '| **PV** | **Storage backend** | **Size** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **PV** | **存储后端** | **大小** |'
- en: '| `pv-nfsvol1` | NFS | 2 GiB |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `pv-nfsvol1` | NFS | 2 GiB |'
- en: '| `pv-gluster` | GlusterFS | 3 GiB |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `pv-gluster` | GlusterFS | 3 GiB |'
- en: '| `pv-iscsi` | iSCSI | 1 GiB |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `pv-iscsi` | iSCSI | 1 GiB |'
- en: Creating PVs for NFS shares
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 NFS 共享创建 PV
- en: The NFS-related `PersistentVolume` resource that was created by the OpenShift
    API can be defined using either a YAML or JSON notation and can be submitted to
    the API by using the `oc create` command. Previously, we set up several NFS exports
    on `storage.example.com`. Now, we need to create the appropriate PV resources
    for each of them.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由 OpenShift API 创建的与 NFS 相关的 `PersistentVolume` 资源可以使用 YAML 或 JSON 表示法定义，并可以通过
    `oc create` 命令提交给 API。之前，我们在 `storage.example.com` 上设置了多个 NFS 导出。现在，我们需要为它们创建适当的
    PV 资源。
- en: 'The following example provides a file that can make NFS resources available
    for OpenShift clusters:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例提供了一个文件，可以使 NFS 资源在 OpenShift 集群中可用：
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This file contains the following information:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件包含以下信息：
- en: Persistent Volume name (`pv_nfsvol1`) in the `metadata` section
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久卷名称（`pv_nfsvol1`）位于 `metadata` 部分
- en: Available capacity (2 GibiBytes)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用容量（2 GiB）
- en: Supported access modes (ReadWriteMany)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的访问模式（ReadWriteMany）
- en: Storage reclaim policy (Retain)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储回收策略（保留）
- en: NFS export information (server address and path)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS 导出信息（服务器地址和路径）
- en: 'Once the file is created, we can make the resource available for the cluster
    by using the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 文件创建后，我们可以通过以下命令使资源可用于集群：
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding command creates the appropriate OpenShift API resource. Please
    be aware that the resource is not mounted to pod yet, but is ready to be bound
    to a PVC.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令会创建相应的 OpenShift API 资源。请注意，资源尚未挂载到 Pod，但已准备好与 PVC 绑定。
- en: You may want to create two other definitions to abstract the rest of the NFS
    shares we created previously. The shares are located on `storage.example.com:/exports/nfsvol2`
    and `storage.example.com:/exports/nfsvol3`. Shares `/exports/nfsvol2` and `/exports/nfsvol3`
    will not be used.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望创建另外两个定义来抽象之前创建的其他 NFS 共享。这些共享位于 `storage.example.com:/exports/nfsvol2`
    和 `storage.example.com:/exports/nfsvol3`。共享 `/exports/nfsvol2` 和 `/exports/nfsvol3`
    将不会被使用。
- en: 'As with any other OpenShift API resource, we can see its configuration by running
    the `describe` command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他OpenShift API资源一样，我们可以通过运行`describe`命令查看其配置：
- en: '[PRE29]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can see our PV using the `oc get pv` command as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令使用`oc get pv`查看我们的PV：
- en: '[PRE30]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`oc cluster up` creates a number of pre-defined PVs, which are named `pv0001`
    and `pv0100`. They are not shown in the preceding output.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`oc cluster up`创建了一些预定义的PV，命名为`pv0001`和`pv0100`。它们未显示在前面的输出中。'
- en: Creating a PV for the GlusterFS volume
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为GlusterFS卷创建PV
- en: 'GlusterFS is distributed by nature and is quite different from the previous
    NFS-based storage. OpenShift cluster needs to be aware of the underlying Gluster
    storage infrastructure so that any schedulable OpenShift node can mount a GlusterFS
    volume. Configuring a GlusterFS persistent volume involves the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS天生是分布式的，且与之前基于NFS的存储有很大不同。OpenShift集群需要了解底层的Gluster存储基础设施，以便任何可调度的OpenShift节点都能挂载GlusterFS卷。配置GlusterFS持久卷涉及以下内容：
- en: The `glusterfs**-**fuse` package being installed on every schedulable OpenShift
    node
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个可调度的OpenShift节点上安装`glusterfs-fuse`包
- en: An existing GlusterFS storage in your underlying infrastructure
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你底层基础设施中的现有GlusterFS存储
- en: A distinct list of servers (IP addresses) in the GlusterFS cluster to be defined
    as endpoints
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GlusterFS集群中定义一组不同的服务器（IP地址）作为端点
- en: A service to persist the endpoints (optional)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于持久化端点的服务（可选）
- en: An existing Gluster volume to be referenced in the persistent volume object
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个现有的Gluster卷将作为持久卷对象中的引用。
- en: 'First, we need to install the `glusterfs-fuse` package on our OpenShift nodes:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在OpenShift节点上安装`glusterfs-fuse`包：
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: An endpoints' definition is intended to represent the GlusterFS cluster's servers
    as endpoints and as such includes the IP addresses of your Gluster servers. The
    port value can be any numeric value within the accepted range of ports (`0` – `65535`).
    Optionally, you can create a service that persists the endpoints.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 端点定义旨在表示GlusterFS集群的服务器作为端点，因此包括你的Gluster服务器的IP地址。端口值可以是端口范围内的任何数值（`0` - `65535`）。可选地，你可以创建一个持久化端点的服务。
- en: 'The GlusterFS service is represented by a `Service` OpenShift API object, which
    is shown as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS服务由`Service` OpenShift API对象表示，显示如下：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once this file is created, `glusterfs` endpoints can be created as regular
    API objects:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦该文件创建完成，可以像常规API对象一样创建`glusterfs`端点：
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The GlusterFS endpoint''s definition should contain information about all Gluster
    Storage nodes that are going to be used for data exchange. Our example only contains
    one node with an IP address of `172.24.0.12`. So, in order to create a Gluster
    endpoint definition file and create Gluster endpoints, run the following commands:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS端点的定义应包含所有用于数据交换的Gluster存储节点的信息。我们的示例只包含一个节点，IP地址为`172.24.0.12`。因此，为了创建Gluster端点定义文件并创建Gluster端点，请运行以下命令：
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we are ready to create a PV which points to the Gluster volume we created
    previously:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备创建一个指向之前创建的Gluster卷的PV：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We are using the `Retain` policy to demonstrate that the system administrator
    has to take care of data reclamation manually.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用`Retain`策略来演示系统管理员需要手动处理数据回收。
- en: As we can see, the PV definition file for GlusterFS contains endpoints information
    and the volume's name.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，GlusterFS的PV定义文件包含端点信息和卷的名称。
- en: 'Now, the following volumes should be available:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，以下卷应该可以使用：
- en: '[PRE36]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: PV for iSCSI
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iSCSI的PV
- en: 'Unlike NFS or GlusterFS persistent volumes, iSCSI volumes can only be accessed
    from one client/pod at a time. This is a block-based persistent storage and we
    should provide the file system type we are going to use. In the following example,
    the `ext4` file system will be used:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与NFS或GlusterFS持久卷不同，iSCSI卷一次只能从一个客户端/Pod访问。这是一个基于块的持久存储，我们应该提供我们将要使用的文件系统类型。在以下示例中，将使用`ext4`文件系统：
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s create the volume:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建卷：
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'At the end of the lab, you should have at least three PVs, like the ones shown
    in the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验结束时，你应该至少有三个PV，如以下代码所示：
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Using persistent storage in pods
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Pod中使用持久存储
- en: Previously, we created all required PV OpenShift API objects, which are provided
    by OpenStack cluster administrators. Now, we are going to show you how to use
    persistent storage in your applications. Any OpenShift users can request persistent
    volume through the PVC concept.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们创建了所有必需的PV OpenShift API对象，这些对象由OpenStack集群管理员提供。现在，我们将向你展示如何在应用程序中使用持久化存储。任何OpenShift用户都可以通过PVC概念请求持久化卷。
- en: Requesting persistent volume
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请求持久化卷
- en: Once the PV resource is available, any OpenShift user can create a PVC to request
    storage and later use that PVC to attach it as a volume to containers in pods.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦PV资源可用，任何OpenShift用户都可以创建PVC来请求存储，并稍后使用该PVC将其附加为Pod中容器的卷。
- en: Upcoming examples don't have to be run under the `system:admin` account. Any
    unprivileged OpenShift user can request persistent volumes using PVC.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的示例不需要在`system:admin`账户下运行。任何非特权的OpenShift用户都可以使用PVC请求持久化卷。
- en: 'Users should create PVC definitions using either YAML or JSON syntax. The following
    example shows a claim that requests 1 GiB of persistent storage with `ReadWriteOnce`
    capabilities:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应使用YAML或JSON语法创建PVC定义。以下示例展示了一个请求1 GiB持久化存储，并具有`ReadWriteOnce`权限的声明：
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, we are able to create the corresponding API entity—PVC:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建相应的API实体——PVC：
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can verify the PVC status by using the `oc get pv` and `oc get pvc` commands.
    Both should show the status of PV/PVC:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`oc get pv`和`oc get pvc`命令来验证PVC的状态。两者应显示PV/PVC的状态：
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In your particular case, PVC will be bound to the iSCSI-based physical volume,
    because it satisfies all requirements (`ReadWriteOnce` and `capacity`). `Bound` state
    means that OpenShift was able to find a proper physical volume to perform the
    binding process.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的特定案例中，PVC将绑定到基于iSCSI的物理卷，因为它满足所有要求（`ReadWriteOnce`和`capacity`）。`Bound`状态意味着OpenShift能够找到合适的物理卷来执行绑定过程。
- en: Binding a PVC to a particular PV
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将PVC绑定到特定的PV
- en: Usually, users don't have to worry about underlying storage infrastructure.
    A user just needs to order required storage using PVC with the size and access
    mode specified. In some cases, there is a need to bind a PVC to a specific PV.
    Imagine the following scenario, where your storage infrastructure is complex and
    you need the storage for your database server to be as fast as possible. It would
    be good to place it on an SSD storage. In this case, storage administrators can
    provide you with either an FC or iSCSI-based volume that is backed by SSD drives.
    The OpenShift administrator may create a specific PV for future usage. On the
    user side, we will need to bind the newly created PVC to that specific PV. Static
    binding PVC to PV can be achieved by specifying the `volumeName` parameter under
    the `spec` section (`spec.volumeName`).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，用户无需担心底层存储基础设施。用户只需使用PVC指定所需的存储大小和访问模式即可订购所需的存储。在某些情况下，可能需要将PVC绑定到特定的PV。假设以下场景：你的存储基础设施复杂，并且你需要为数据库服务器提供尽可能快速的存储。将其放在SSD存储上会是一个不错的选择。在这种情况下，存储管理员可以为你提供由SSD硬盘支持的FC或iSCSI基础卷。OpenShift管理员可能会为未来使用创建一个特定的PV。在用户端，我们需要将新创建的PVC绑定到该特定的PV。通过在`spec`部分下指定`volumeName`参数（`spec.volumeName`），可以实现PVC到PV的静态绑定。
- en: 'In our particular example, we have two remaining unbound volumes with the `ReadWriteMany`
    access type: `pv-gluster` and `pv-nfsvol1`. In the following example, we are going
    to perform their static binding.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特定示例中，我们剩下两个未绑定的卷，具有`ReadWriteMany`访问类型：`pv-gluster`和`pv-nfsvol1`。在以下示例中，我们将执行它们的静态绑定。
- en: 'Let''s create a PVS definition for web server data:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为Web服务器数据创建一个PVS定义：
- en: '[PRE43]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create the PVC from the previous definition and see if OpenShift found a matching
    PV for it:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的定义创建PVC，并查看OpenShift是否为其找到匹配的PV：
- en: '[PRE44]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And lastly, we will request 100 MiB of data by using the following PVC:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用以下PVC请求100 MiB的数据：
- en: '[PRE45]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Notice that all PVSs are in the `Bound` state now.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有PVS现在都处于`Bound`状态。
- en: Using claims as volumes in pod definition
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Pod定义中使用声明作为卷
- en: 'Previously, we requested a persistent storage by creating PVCs, and now we
    are going to create an application using corresponding PVCs, as they are now bound
    to PVs that are backed by real storage. OpenShift allows developers to create
    a `Pod` and use PVC as a volume. The following example shows how it can be used
    in order to create an Apache-based container:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们通过创建PVC请求了持久化存储，现在我们将创建一个应用程序，使用相应的PVC，因为它们现在已绑定到由真实存储支持的PVs。OpenShift允许开发人员创建`Pod`并将PVC用作卷。以下示例展示了如何在基于Apache的容器中使用它：
- en: '[PRE46]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In the preceding code, we defined an Apache pod and configured it to attach
    persistent volume that was provided as part of our previous claim to `pvc-web`
    to its container. OpenShift will automatically find the bound PV and mount it
    to the container.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个 Apache pod，并配置它附加上之前申请的 `pvc-web` 提供的持久化卷到容器。OpenShift 会自动找到绑定的
    PV，并将其挂载到容器上。
- en: 'The PVC named `pvc-web` is bound to the GlusterFS-based PV. This persistent
    storage implementation requires gluster endpoints and services to be defined in
    each namespace/project in OpenShift. So, before moving on to the next part of
    the lab, we will need to create these service and endpoints again by running the
    following commands:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 名为 `pvc-web` 的 PVC 已绑定到基于 GlusterFS 的 PV。这个持久化存储实现要求在 OpenShift 的每个命名空间/项目中定义
    Gluster 端点和服务。所以，在进入实验的下一部分之前，我们需要通过运行以下命令再次创建这些服务和端点：
- en: '`oc create -f gluster-service.yaml`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`oc create -f gluster-service.yaml`'
- en: '`oc create -f gluster-endpoint.yaml`:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`oc create -f gluster-endpoint.yaml`：'
- en: '[PRE47]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can display pod and volume-related information by using the following command:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令显示 pod 和卷相关信息：
- en: '[PRE48]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If we connect to the container and try to create the `/var/www/index.html`
    file, it will reside in GlusterFS. We can verify that the GlusterFS volume was
    mounted on the node:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们连接到容器并尝试创建 `/var/www/index.html` 文件，它将存储在 GlusterFS 中。我们可以验证 GlusterFS 卷是否已经挂载到节点上：
- en: '[PRE49]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: So, now the container has access to persistent data mounted at `/var/www/html`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在容器可以访问挂载在`/var/www/html`的持久化数据。
- en: Previously, we created an `index.html` file stored on GlusterFS storage. This
    means that our web server will automatically have access to all data on the GlusterFS
    volume `gvol1`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们创建了一个存储在 GlusterFS 上的 `index.html` 文件。这意味着我们的 Web 服务器将自动访问 GlusterFS 卷
    `gvol1` 上的所有数据。
- en: 'Now, we can verify that the persistent data written earlier is accessible.
    First, we will need to get the cluster IP address of our web server:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以验证之前写入的持久化数据是否可访问。首先，我们需要获取 Web 服务器的集群 IP 地址：
- en: '[PRE50]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'And secondly, try to reach it via `curl`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，尝试通过 `curl` 访问：
- en: '[PRE51]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As we can see, now, the web server displays data that's available on the GlusterFS.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当前，Web 服务器显示的是 GlusterFS 上可用的数据。
- en: 'We can now verify that data is stored persistently using two different ways:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过两种不同的方式验证数据是持久存储的：
- en: On the backend storage
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在后台存储上
- en: By recreating the container
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重新创建容器
- en: 'Let''s verify that the file indeed exists on our `storage.example.com` server:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证该文件确实存在于我们的 `storage.example.com` 服务器上：
- en: '[PRE52]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, let''s try to delete and create the container again:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试再次删除并创建容器：
- en: '[PRE53]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As we can see, the data persists and is available, even after the container
    has been deleted.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据得以持久存储并可访问，即使容器已被删除。
- en: Managing volumes through oc volume
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 `oc volume` 管理卷
- en: OpenShift users can attach a volume to any running application by using `oc
    volume`. In this example, we are going to create a pod with a basic application
    and attach a persistent volume to it.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 用户可以通过 `oc volume` 将卷附加到任何运行中的应用程序。在这个例子中，我们将创建一个包含基本应用程序的 pod，并将持久化卷附加到该
    pod 上。
- en: 'First, just deploy a basic Apache web server using `oc new-app`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用 `oc new-app` 部署一个基本的 Apache Web 服务器：
- en: '[PRE54]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'After a little while, all httpd service resources will be available:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一会儿后，所有 httpd 服务资源将会可用：
- en: '[PRE55]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`oc new-app` created a deployment configuration that controls the application
    deployment process.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`oc new-app` 创建了一个部署配置，控制应用程序部署过程。'
- en: 'In this example, we are going to attach a PVC named `pvc-data` as a volume
    to the running container:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将把名为 `pvc-data` 的 PVC 作为卷附加到正在运行的容器中：
- en: '[PRE56]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can verify that an NFS share was mounted to the container:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证是否将 NFS 共享挂载到容器：
- en: '[PRE57]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we can create an `index.html` file on our storage server directly in the
    export:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以直接在导出目录中，在我们的存储服务器上创建一个 `index.html` 文件：
- en: '[PRE58]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The previous command was run on the storage server, not on OpenShift!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令是在存储服务器上运行的，而不是在 OpenShift 上！
- en: 'Once persistent data is available, we can try to access the web service:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦持久化数据可用，我们可以尝试访问 Web 服务：
- en: '[PRE59]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'As we can see, attaching a volume to the pod worked fine. Now, we can detach
    it:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，将卷附加到 pod 上是成功的。现在，我们可以将其分离：
- en: '[PRE60]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Please be aware that OpenShift rolls new pods out each time you update the corresponding
    deployment config. This means that the container's IP addresses will be changed.
    To avoid that, we recommend testing your configuration using the service's IP
    address.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每次更新相应的部署配置时，OpenShift 会滚动部署新的 pod。这意味着容器的 IP 地址会发生变化。为了避免这种情况，我们建议使用服务的
    IP 地址来测试配置。
- en: 'Once the container is recreated, notice that the persistent data is not available.
    The `httpd` daemon shows the default page:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦容器被重新创建，注意到持久数据不可用。`httpd` 守护进程会显示默认页面：
- en: '[PRE61]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Persistent data for a database container
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库容器的持久数据
- en: 'Let''s attach an iSCSI-based persistent volume to a MariaDB instance. First,
    we will have to launch a `mariadb` application as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为 MariaDB 实例附加一个基于 iSCSI 的持久卷。首先，我们需要启动一个 `mariadb` 应用程序，如下所示：
- en: '[PRE62]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Wait a couple of minutes, and check the status of the `mariadb` instance:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几分钟，然后检查 `mariadb` 实例的状态：
- en: '[PRE63]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We need to know the default location of the database files. This can be gathered
    using the `oc describe dc` command, as shown in the following code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要知道数据库文件的默认位置。可以使用`oc describe dc`命令来获取此信息，示例如下所示：
- en: '[PRE64]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: As we can see, by default, that container stores all data at `/var/lib/mysql/data`
    in the `mariadb-volume-1` volume. This allows us to replace data on it by using
    the `oc volume` subcommand.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，默认情况下，该容器将所有数据存储在 `/var/lib/mysql/data` 目录下的 `mariadb-volume-1` 卷中。这允许我们使用
    `oc volume` 子命令替换其中的数据。
- en: 'Now, we are going to attach a volume to the `mariadb` container. Please be
    aware that previously created database structures will be lost, as they are not
    stored persistently:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为 `mariadb` 容器附加一个卷。请注意，之前创建的数据库结构将丢失，因为它们未被持久化存储：
- en: '[PRE65]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This will automatically redeploy `mariadb` and place database files on the persistent
    storage.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自动重新部署 `mariadb` 并将数据库文件放置在持久存储上。
- en: The `ext4` file system should be created on the iSCSI target in advance.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`ext4` 文件系统应该提前在 iSCSI 目标上创建。'
- en: Now you see that Openshift integrates easily with the most popular storage protocols
    and allows you to make containerized applications to be more resilient.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以看到，OpenShift 可以轻松与最流行的存储协议集成，并允许你使容器化应用程序更具弹性。
- en: Summary
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Persistent storage usage is a daily activity for OpenShift cluster administrators
    and OpenShift users in a production environment. In this chapter, we briefly discussed
    persistent storage OpenShift API objects such as PV and PVC. Both PV and PVC allow
    you to define and use persistent storage. We showed you how to configure basic
    underlying storage services such as NFS, GlusterFS, and iSCSI, and how to add
    them to OpenShift's infrastructure via `PV` objects. Additionally, we worked on
    requesting persistent storage via `PVC` objects. Lastly, we showed you a basic
    example of persistent storage usage from an application point of view.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，持久存储的使用是 OpenShift 集群管理员和 OpenShift 用户的日常活动。在本章中，我们简要讨论了 OpenShift API
    对象，如 PV 和 PVC，它们允许你定义并使用持久存储。我们展示了如何配置基础的存储服务，如 NFS、GlusterFS 和 iSCSI，并通过 `PV`
    对象将它们添加到 OpenShift 的基础设施中。此外，我们还介绍了如何通过 `PVC` 对象请求持久存储。最后，我们从应用程序的角度展示了持久存储的基本使用示例。
- en: Questions
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Which would be a good use case for persistent storage?
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种情况下使用持久存储比较合适？
- en: PostgreSQL database for development
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发环境中的 PostgreSQL 数据库
- en: MariaDB database for production
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产环境中的 MariaDB 数据库
- en: Memcached
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Memcached
- en: JDBC-connector
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: JDBC-连接器
- en: 'Which of the following OpenShift storage plugins supports the `ReadWriteMany`
    access mode? choose two:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些 OpenShift 存储插件支持 `ReadWriteMany` 访问模式？选择两个：
- en: NFS
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: NFS
- en: iSCSI
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: iSCSI
- en: Cinder Volume
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cinder 卷
- en: GlusterFS
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: GlusterFS
- en: Which project must PVs belong to?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PV 必须属于哪个项目？
- en: default
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认
- en: openshift
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: openshift
- en: Any project
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何项目
- en: openshift-infra
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: openshift-infra
- en: Suppose we created a PVC that requests 2 Gi storage. Which PV will be bound
    to it?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们创建了一个请求 2 Gi 存储的 PVC。哪个 PV 会与其绑定？
- en: 1950 Mi
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1950 Mi
- en: 1950 M
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1950 M
- en: 2 Gi
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2 Gi
- en: 3 Gi
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3 Gi
- en: 'Which OpenShift API objects must be created before using GlusterFS volumes?
    choose two:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用 GlusterFS 卷之前，必须创建哪些 OpenShift API 对象？选择两个：
- en: Pod
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod
- en: Service
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务
- en: Endpoint
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点
- en: Route
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 路由
- en: Further reading
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Here is a list of links for you to take a look at if you are interested in
    the topics we covered in this chapter:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对本章讨论的主题感兴趣，以下是一些链接供你参考：
- en: '[https://docs.openshift.org/latest/install_config/persistent_storage/index.html](https://docs.openshift.org/latest/install_config/persistent_storage/index.html)'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.openshift.org/latest/install_config/persistent_storage/index.html](https://docs.openshift.org/latest/install_config/persistent_storage/index.html)'
- en: '[http://linux-iscsi.org/wiki/LIO](http://linux-iscsi.org/wiki/LIO)'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://linux-iscsi.org/wiki/LIO](http://linux-iscsi.org/wiki/LIO)'
- en: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)'
