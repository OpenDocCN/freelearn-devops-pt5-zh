- en: Managing Persistent Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we described how to install an OpenShift cluster using
    an advanced installation method. The next step of the installation process is
    to make persistent storage available for OpenShift users. In [Chapter 1](part0021.html#K0RQ0-78aafb146b304cdeb9b3261a70edabde),
    *Containers and Docker Overview*, we already how to use Docker persistent volumes. Usually,
    we do not need any for development or testing purposes, but it is not the case
    with production environments where we need to store persistent data in certain
    cases. In this chapter, we will describe the persistent storage concept regarding
    the OpenShift infrastructure. We will also explain the need for using persistent
    storage in a production environment. The focus of this chapter is all about configuring
    an infrastructure to support persistent storage. This includes the following storage
    types: NFS, GlusterFS, iSCSI, and more. Besides infrastructure preparations, this
    chapter covers how to leverage persistent storage in OpenShift using **Persistent
    Volumes** (**PVs**) and **Persistent Volume Claims** (**PVCs**). Lastly, we will
    show you how to use persistent storage in your pods/applications that are deployed
    on OpenShift.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Persistent versus ephemeral storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift persistent storage concept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage backends comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage infrastructure setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring PVs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using persistent storage in Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The learning environment for this chapter consists of two VMs with the following
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hostname** | **RAM** | **vCPU** | **OS** |'
  prefs: []
  type: TYPE_TB
- en: '| `openshift.example.com` | 4GB | 2 | CentOS 7 |'
  prefs: []
  type: TYPE_TB
- en: '| `storage.example.com` | 2GB | 1 | CentOS 7 |'
  prefs: []
  type: TYPE_TB
- en: These machines can be deployed anywhere (bare metal, VMware, OpenStack, AWS,
    and so on). However, for educational purposes, we recommend using the Vagrant
    + VirtualBox/libvirt configuration to simplify the process of deployment and re-deployment
    of our virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also assume that all servers are accessible via both FQDNs and short names.
    This requires configuring `/etc/hosts` records, which is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The IPs must be the same as the ones that are specified in the following Vagrantfile.
    If you only want to use one machine for this lab, configure the `/etc/hosts` file
    to point both records to the same machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lab environment deployment can be simplified by using the following `Vagrantfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It's not mandatory to use the same IPs that were used in the preceding code.
    What is important is that you point your `/etc/hosts` records to them.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent versus ephemeral storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, OpenShift/Kubernetes containers don't store data persistently. We
    can start an application and OpenShift will start a new container from an immutable
    Docker image. It uses an ephemeral storage, which means that data is available
    until the container is deleted or rebuilt. If our application (and all related
    containers) has been rebuilt, all data will be lost. Still, this approach is fine
    for any stateless application. For example, it will work for a simple website
    that doesn't act as a portal and only provides information embedded into HTML/CSS.
    Another example would be a database used for development—usually, no one cares
    if data is lost.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider another example. Imagine that we need a database for a WordPress
    container. If we store database files on an ephemeral storage, we can lose all
    our data if the database container was rebuilt or deleted. We cannot allow our database
    files to be deleted or lost. OpenShift can rebuild our database container without
    any issues. It will give us a working instance of a database but without required
    databases/table structures and data in the tables. From the application's perspective,
    this means that all required information is lost. For these kinds of applications
    (stateful), we need a persistent storage that will be available even if the container
    crashed, was deleted, or was rebuilt.
  prefs: []
  type: TYPE_NORMAL
- en: Storage requirements (ephemeral versus persistent) are dependent on your particular
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenShift persistent storage concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift uses the **Persistent Volume** (**PV**) concept to allow administrators
    to provide persistent storage for a cluster and then let developers request storage
    resources via **Persistent Volume Claims** (**PVC**). Thus, end users can request
    storage without having deep knowledge of the underlying storage infrastructure.
    At the same time, administrators can configure the underlying storage infrastructure
    and make it available to end users via the PV concept.
  prefs: []
  type: TYPE_NORMAL
- en: PV resources are shared across the OpenShift cluster since any of them can (if
    it is allowed) potentially be used by any users/projects. On the other hand, PVC
    resources are specific to a project (namespace) and they are usually created and
    used by end users, such as developers. Once PVC resources are created, OpenShift
    tries to find a suitable PV resource that matches specific criteria, like size
    requirements, access mode (RWO, ROX, RWX), and so on.  If PV has been found to
    satisfy the request from the PVC, OpenShift binds that PV to our PVC. Once this
    is complete, PV cannot be bound to additional PVCs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: OpenShift Pod, PV, PVC, and the storage relationship
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PVs are represented by a PersistentVolume OpenShift API object, which describes
    an existing piece of storage infrastructure like NFS share, GlusterFS volume,
    iSCSI target, a Ceph RBD device, and so on. It is assumed that the underlying
    storage component already exists and is ready to be consumed by the OpenShift
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: PVs have their own life cycle, which is independent of any pods that use PV.
  prefs: []
  type: TYPE_NORMAL
- en: High availability of storage in the infrastructure is left to the underlying
    storage provider.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Volume Claims
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I mentioned previously, OpenShift users can request storage resources for
    their applications by means of PVCs that are defined by a `PersistentVolumeClaim`
    OpenShift API object. PVC represents a request made by an end user (usually developers).
    PVC consumes PV resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'A PVC contains some important information regarding resources that are requested
    by applications/users:'
  prefs: []
  type: TYPE_NORMAL
- en: Size needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several access modes that can be used in the OpenShift infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Mode** | **Description** | **Examples** |'
  prefs: []
  type: TYPE_TB
- en: '| ReadOnlyMany | The volume can be mounted read-only by many nodes. | NFS in
    RO mode |'
  prefs: []
  type: TYPE_TB
- en: '| ReadWriteOnce | The volume can be mounted as read-write by a single node.
    | iSCSI-based xfs, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| ReadWriteMany | The volume can be mounted as read-write by many nodes. |
    GlusterFSNFS |'
  prefs: []
  type: TYPE_TB
- en: Once a PVC resource is created, OpenShift has to find a suitable PV resource
    and bind it to the PVC. If the binding is successful, the PVC resource can be
    consumed by an application.
  prefs: []
  type: TYPE_NORMAL
- en: The storage life cycle in OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The interaction between PV and PVC resources is comprised of several steps,
    which are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenShift: storage lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift cluster administrators can configure dynamic PV provisioning or configure
    PV resources in advance. Once a user has requested a storage resource using the
    PVC with specific size and access mode requirements, OpenShift looks for an available
    PV resource. The user always gets what they ask for, at least. In order to keep
    storage usage to a minimum, OpenShift binds the smallest PV that matches all criteria.
    A PVC remains unbound until a suitable PV is found. If there is a volume matching
    all criteria, OpenShift software binds them together. Starting from this step,
    storage can be used by pods. A pod consumes PVC resources as volumes.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift inspects the claim to find the bound volume and mounts that volume
    to the pod. For those volumes that support multiple access modes, the user specifies
    which mode is desired when using their claim as a volume in a pod.
  prefs: []
  type: TYPE_NORMAL
- en: Users can delete PVC objects, which allows reclamation of storage resources.
    If PVC is deleted, the volume is considered as *released* but is not yet immediately
    available to be bound to other claims. This requires that data stored on the volumes
    are handled according to the reclaim policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reclaim policy defines the way OpenShift understands what to do with the
    volume after it is released. The following reclaim policies are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Policy** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Retain | Allows manual reclamation of the resource for those volume plugins
    that support it. In this case, storage administrators should delete data manually.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Delete | Deletes both the PV object from the OpenShift Container Platform
    and the associated storage asset in external infrastructures, such as AWS EBS,
    GCE PD, or Cinder volume. |'
  prefs: []
  type: TYPE_TB
- en: Storage backends comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift supports a number of persistent storage backends that work differently.
    Some of them support reads/writes from many clients (like NFS), while others support
    only one mount.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table contains a comparison of supported storage backends/plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Volume backend** | **ReadWriteOnce** | **ReadWriteMany** | **ReadOnlyMany**
    |'
  prefs: []
  type: TYPE_TB
- en: '| AWS EBS | Yes |  |   |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Disk | Yes |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ceph RBD | Yes |  | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Fibre Channel | Yes |  | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| GCE Persistent Disk | Yes |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GlusterFS | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| HostPath | Yes |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| iSCSI | Yes |  | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| NFS (Network File System) | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| OpenStack Cinder | Yes |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VMware vSphere | Yes |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Local | Yes |  |  |'
  prefs: []
  type: TYPE_TB
- en: '`HostPath` allows you to mount persistent storage directly from the node your
    pod runs on and as such is not suitable for production usage. Please only use
    it for testing or development purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of supported storage in an OpenShift cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Filesystem-based storage (like NFS, Gluster, and HostPath)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Block-based storage (like iSCSI, OpenStack Cinder, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker containers need file system-based storage to use as a persistent volume.
    This means that OpenShift can use file system-based storage directly.  OpenShift
    needs to create a file system on block storage before using it as a persistent
    volume. For example, if an iSCSI block device is provided, the cluster administrator
    has to define what file system will be created on the block device during the
    PV creation process.
  prefs: []
  type: TYPE_NORMAL
- en: Storage infrastructure setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Configuring the underlying storage infrastructure is usually a task for storage
    administrators. This requires a number of settings and design decisions so that
    you can achieve the expected level of durability, availability, and performance.
    This requires a significant knowledge of underlying resources, physical infrastructure,
    networking, and so on. Once the storage subsystem has been configured properly
    by storage administrators, OpenShift cluster administrators can leverage it to
    create PVs.
  prefs: []
  type: TYPE_NORMAL
- en: This book is about OpenShift administration, and thus the configuration of an
    underlying storage technology is out of its scope. However, we want to demonstrate
    how to perform the basic setup of storage infrastructure on Linux systems for
    NFS, GlusterFS, and iSCSI.
  prefs: []
  type: TYPE_NORMAL
- en: If you still need to set up a different kind of storage, please refer to the
    relevant documentation. For example, you may find some Ceph storage documentation
    at [https://ceph.com](https://ceph.com); OpenStack Cinder documentation may be
    found on the project homepage at [openstack.org](http://openstack.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen NFS and GlusterFS-based storage for a number of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Both are file system-based storage solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both support the `ReadWriteMany` OpenShift access type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both can easily be configured on any OpenShift cluster nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFS is known to any Linux system administrator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also want to demonstrate how to use block-based storage in the OpenShift
    cluster. We have chosen iSCSI-based storage as an example storage, as it is one
    of the easiest ways to go with block-based storage on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up NFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Network File System **(**NFS**) is a client/server filesystem protocol
    that was originally developed by Sun Microsystems in 1984\. NFS allows a user
    on a client computer (NFS client) to access files stored on the NFS server over
    a network, or even over the internet. The NFS server shares one or more NFS shares
    with a number of allowed NFS clients. NFS clients mount NFS shares as regular
    filesystems. No specific application settings are required since NFS is a POSIX
    compliant file system protocol. This is the main reason why NFS is very popular
    as a network storage solution. NFS is supported by Linux kernel by default and
    can be configured on any Linux-based server.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will use a standalone NFS server for providing persistent
    storage to applications that will be deployed on our OpenShift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The installation process that is described is for CentOS 7.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NFS installation and configuration process involves several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing NFS packages on the server and clients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring NFS exports on the server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starting and enabling the NFS service
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verification or mounting the NFS share(s) on clients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we begin, we need to deploy two machines, as described in the *Technical
    requirements* section. In this lab, we assume that machines were deployed as VMs
    using Vagrant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bring your Vagrant environment up and log in to `storage` VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Installing NFS packages on the server and clients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NFS packages need to be installed on the NFS server, as well as on all OpenShift
    nodes, as they will act as NFS clients. NFS libraries and binaries are provided
    by the `nfs-utils` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will configure NFS services on storage.example.com. All configuration is
    done under the `root` account. You can use `sudo -i` command to switch to root.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring NFS exports on the server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This needs to be done on the server-side only. We are going to export several
    file systems under the `/exports` directory. The exports will only be accessible
    by OpenShift nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenShift cluster runs Docker containers using random **User IDs** (**UIDs**).
    It is difficult to predict a UID to give proper NFS permissions, so we have to
    configure the following NFS settings to allow OpenShift to use NFS shares properly:'
  prefs: []
  type: TYPE_NORMAL
- en: A share should be owned by the `nfsnobody` user and group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A share should have `0700` access permissions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A share should be exported using the `all_squash` option. This will be described
    later in this topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create the required directories and assign them to the proper permissions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the firewall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is not required on Vagrant box centos/7 since `firewalld` is disabled by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an NFS export by adding the following lines to `/etc/exports`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of providing FQDNs, you may also specify IP addresses of the nodes.
    This will look like as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `all_squash` NFS export option configures NFS to map all UIDs to the `nfsnobody`
    user ID.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and enabling the NFS service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also need to enable and start nfs-server using the `systemctl` command.
    The following snippet shows how to enable and start all services required for
    NFS service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may want to check that NFS share was exported properly. The following command
    will show all available exports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Configuring GlusterFS shares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GlusterFS is a free and scalable network file system that is suitable for data-intensive
    tasks, such as cloud storage and media streaming. GlusterFS creates a volume on
    top of one or more storage nodes using *bricks*. A brick represents a file system
    on a storage node. There are several types of GlusterFS volumes defined by the
    placement of data on bricks. More information can be found by following the links
    provided at the end of this chapter. In this chapter, we only need to have a basic
    knowledge of the following volume types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Distributed | All files are distributed between bricks/storage nodes. No
    redundancy is provided by this volume type. |'
  prefs: []
  type: TYPE_TB
- en: '| Replicated | All files are replicated between two or more bricks. Thus, each
    file is stored on at least two bricks, which provides redundancy. |'
  prefs: []
  type: TYPE_TB
- en: '| Striped | Each file is striped across several bricks. |'
  prefs: []
  type: TYPE_TB
- en: For this demonstration, we will set up a basic GlusterFS volume on a single
    storage node.
  prefs: []
  type: TYPE_NORMAL
- en: Installing packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to install the GlusterFS packages, which are located in a special
    GlusterFS repository. The `centos-release-gluster312` package configures the GlusterFS
    3.12 repository. We need to install the `glusterfs-server` on the server side
    (`storage.example.com`) and the `glusterfs` package on the client side (`openshift.example.com`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the GlusterFS packages are installed, we need to start and enable the
    gluster management service—`glusterd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Configuring a brick and volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps of GlusterFS volume configuration require that we create
    a brick file system and the volume itself:'
  prefs: []
  type: TYPE_NORMAL
- en: For this lab, we will use the root file system to create a GlusterFS brick.
    This setup can only be used for test and development purposes and is not suitable
    for production usage. All GlusterFS production installations should use separate
    file systems for GlusterFS bricks, preferably located on separate physical block
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `force` option is required here since we are using the `/` file system
    to create glusterFS volume. If this isn''t provided, you may see the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have created a volume, it is a good time to start it, making it
    available to clients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Configuring iSCSI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **internet Small Computer Systems Interface** (**iSCSI**) is a client/server
    protocol that provides block-level access to storage devices by carrying SCSI commands
    over a TCP/IP network. Since iSCSI uses the TCP/IP network, it can be used to
    transmit data over **local area networks** (**LANs**), **wide area networks**(**WANs**),
    and the internet, making location-independent data storage and retrieval possible. This protocol allows
    clients (*initiators)* to send SCSI commands to storage devices (*targets*) on
    remote servers. It is a**storage area network** (**SAN**) protocol. iSCSI allows
    clients to work with remote block devices and treat them as locally attached disks. There
    are a number of iSCSI target implementations (like stgtd, LIO target, and so on).
    As a part of this chapter, we will configure LIO target-based iSCSI storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The necessary steps to configure an iSCSI target on `storage.example.com` are
    outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the CLI tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable the `target` service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the firewall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the iSCSI export using `targetcli`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: All basic configuration options can be found in man `targetcli` under the *QUICKSTART*
    section. For educational purposes, the preceding example exports the iSCSI volume
    to any host. Please be aware that it is not a production-ready configuration.
    In production, you may only want to grant access to the target to certain hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Client-side verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The followings topics will describe how to use NFS, Gluster, and iSCSI storage
    resources inside the OpenShift cluster. However, you can use previously configured
    resources manually as well. Before going to the next topic, we strongly recommend
    verifying that all your resources are configured properly by mounting them on
    the client side. In our case, the client is located on the `openshift.example.com`
    node. Let''s log in to openshift node and switch to root account before we begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: NFS verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To verify that the NFS exports work properly, we need to mount them on the
    `openshift.example.com` node, which is shown in the following code. If all shares
    can be mounted without any issues, you can assume that that share was exported
    properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It's assumed that all required packages are already installed by running `yum
    install -y nfs-utils`.
  prefs: []
  type: TYPE_NORMAL
- en: GlusterFS verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GlusterFS volume can be mounted manually by using the FUSE client. The
    verification procedure looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a sample of persistent data to be used later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This storage will be used as web server root data storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that mount point is available and then unmount the storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: iSCSI verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'iSCSI verification assumes that an OpenShift node can access block storage
    devices. If everything goes well, you should see an additional disk at `/proc/partitions`.
    The iSCSI client utilities are provided by the `iscsi-initiator-utils` package.
    Once the package is installed, the `iscsiadm` utility can be used to scan the
    target for iSCSI exports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You can also use the `lsblk` utility to discover block devices that are available
    in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Physical Volumes (PV)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned previously, OpenShift cluster administrators can create PV resources
    for future usage by OpenShift users.
  prefs: []
  type: TYPE_NORMAL
- en: This topic assumes that the OpenShift environment is up and running on the `openshift.example.com`
    node. You may use `oc cluster up` or do an advanced OpenShift installation by
    using Ansible.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned previously, only cluster administrators can configure PVs.
    So, before you begin the following labs, you have to switch to the admin account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We recommend creating a new project to perform this `persistent storage`-related
    lab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The client will automatically change the current project to the newly created
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upcoming examples, we will create the following PVs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PV** | **Storage backend** | **Size** |'
  prefs: []
  type: TYPE_TB
- en: '| `pv-nfsvol1` | NFS | 2 GiB |'
  prefs: []
  type: TYPE_TB
- en: '| `pv-gluster` | GlusterFS | 3 GiB |'
  prefs: []
  type: TYPE_TB
- en: '| `pv-iscsi` | iSCSI | 1 GiB |'
  prefs: []
  type: TYPE_TB
- en: Creating PVs for NFS shares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NFS-related `PersistentVolume` resource that was created by the OpenShift
    API can be defined using either a YAML or JSON notation and can be submitted to
    the API by using the `oc create` command. Previously, we set up several NFS exports
    on `storage.example.com`. Now, we need to create the appropriate PV resources
    for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example provides a file that can make NFS resources available
    for OpenShift clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This file contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Volume name (`pv_nfsvol1`) in the `metadata` section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available capacity (2 GibiBytes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported access modes (ReadWriteMany)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage reclaim policy (Retain)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFS export information (server address and path)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the file is created, we can make the resource available for the cluster
    by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command creates the appropriate OpenShift API resource. Please
    be aware that the resource is not mounted to pod yet, but is ready to be bound
    to a PVC.
  prefs: []
  type: TYPE_NORMAL
- en: You may want to create two other definitions to abstract the rest of the NFS
    shares we created previously. The shares are located on `storage.example.com:/exports/nfsvol2`
    and `storage.example.com:/exports/nfsvol3`. Shares `/exports/nfsvol2` and `/exports/nfsvol3`
    will not be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any other OpenShift API resource, we can see its configuration by running
    the `describe` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see our PV using the `oc get pv` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`oc cluster up` creates a number of pre-defined PVs, which are named `pv0001`
    and `pv0100`. They are not shown in the preceding output.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a PV for the GlusterFS volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GlusterFS is distributed by nature and is quite different from the previous
    NFS-based storage. OpenShift cluster needs to be aware of the underlying Gluster
    storage infrastructure so that any schedulable OpenShift node can mount a GlusterFS
    volume. Configuring a GlusterFS persistent volume involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `glusterfs**-**fuse` package being installed on every schedulable OpenShift
    node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An existing GlusterFS storage in your underlying infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A distinct list of servers (IP addresses) in the GlusterFS cluster to be defined
    as endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A service to persist the endpoints (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An existing Gluster volume to be referenced in the persistent volume object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we need to install the `glusterfs-fuse` package on our OpenShift nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: An endpoints' definition is intended to represent the GlusterFS cluster's servers
    as endpoints and as such includes the IP addresses of your Gluster servers. The
    port value can be any numeric value within the accepted range of ports (`0` – `65535`).
    Optionally, you can create a service that persists the endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GlusterFS service is represented by a `Service` OpenShift API object, which
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this file is created, `glusterfs` endpoints can be created as regular
    API objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The GlusterFS endpoint''s definition should contain information about all Gluster
    Storage nodes that are going to be used for data exchange. Our example only contains
    one node with an IP address of `172.24.0.12`. So, in order to create a Gluster
    endpoint definition file and create Gluster endpoints, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to create a PV which points to the Gluster volume we created
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `Retain` policy to demonstrate that the system administrator
    has to take care of data reclamation manually.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the PV definition file for GlusterFS contains endpoints information
    and the volume's name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the following volumes should be available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: PV for iSCSI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike NFS or GlusterFS persistent volumes, iSCSI volumes can only be accessed
    from one client/pod at a time. This is a block-based persistent storage and we
    should provide the file system type we are going to use. In the following example,
    the `ext4` file system will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the lab, you should have at least three PVs, like the ones shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Using persistent storage in pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we created all required PV OpenShift API objects, which are provided
    by OpenStack cluster administrators. Now, we are going to show you how to use
    persistent storage in your applications. Any OpenShift users can request persistent
    volume through the PVC concept.
  prefs: []
  type: TYPE_NORMAL
- en: Requesting persistent volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the PV resource is available, any OpenShift user can create a PVC to request
    storage and later use that PVC to attach it as a volume to containers in pods.
  prefs: []
  type: TYPE_NORMAL
- en: Upcoming examples don't have to be run under the `system:admin` account. Any
    unprivileged OpenShift user can request persistent volumes using PVC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Users should create PVC definitions using either YAML or JSON syntax. The following
    example shows a claim that requests 1 GiB of persistent storage with `ReadWriteOnce`
    capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are able to create the corresponding API entity—PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify the PVC status by using the `oc get pv` and `oc get pvc` commands.
    Both should show the status of PV/PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In your particular case, PVC will be bound to the iSCSI-based physical volume,
    because it satisfies all requirements (`ReadWriteOnce` and `capacity`). `Bound` state
    means that OpenShift was able to find a proper physical volume to perform the
    binding process.
  prefs: []
  type: TYPE_NORMAL
- en: Binding a PVC to a particular PV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, users don't have to worry about underlying storage infrastructure.
    A user just needs to order required storage using PVC with the size and access
    mode specified. In some cases, there is a need to bind a PVC to a specific PV.
    Imagine the following scenario, where your storage infrastructure is complex and
    you need the storage for your database server to be as fast as possible. It would
    be good to place it on an SSD storage. In this case, storage administrators can
    provide you with either an FC or iSCSI-based volume that is backed by SSD drives.
    The OpenShift administrator may create a specific PV for future usage. On the
    user side, we will need to bind the newly created PVC to that specific PV. Static
    binding PVC to PV can be achieved by specifying the `volumeName` parameter under
    the `spec` section (`spec.volumeName`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our particular example, we have two remaining unbound volumes with the `ReadWriteMany`
    access type: `pv-gluster` and `pv-nfsvol1`. In the following example, we are going
    to perform their static binding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a PVS definition for web server data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the PVC from the previous definition and see if OpenShift found a matching
    PV for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And lastly, we will request 100 MiB of data by using the following PVC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Notice that all PVSs are in the `Bound` state now.
  prefs: []
  type: TYPE_NORMAL
- en: Using claims as volumes in pod definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we requested a persistent storage by creating PVCs, and now we
    are going to create an application using corresponding PVCs, as they are now bound
    to PVs that are backed by real storage. OpenShift allows developers to create
    a `Pod` and use PVC as a volume. The following example shows how it can be used
    in order to create an Apache-based container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we defined an Apache pod and configured it to attach
    persistent volume that was provided as part of our previous claim to `pvc-web`
    to its container. OpenShift will automatically find the bound PV and mount it
    to the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PVC named `pvc-web` is bound to the GlusterFS-based PV. This persistent
    storage implementation requires gluster endpoints and services to be defined in
    each namespace/project in OpenShift. So, before moving on to the next part of
    the lab, we will need to create these service and endpoints again by running the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`oc create -f gluster-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '`oc create -f gluster-endpoint.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display pod and volume-related information by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If we connect to the container and try to create the `/var/www/index.html`
    file, it will reside in GlusterFS. We can verify that the GlusterFS volume was
    mounted on the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: So, now the container has access to persistent data mounted at `/var/www/html`.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we created an `index.html` file stored on GlusterFS storage. This
    means that our web server will automatically have access to all data on the GlusterFS
    volume `gvol1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can verify that the persistent data written earlier is accessible.
    First, we will need to get the cluster IP address of our web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'And secondly, try to reach it via `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, now, the web server displays data that's available on the GlusterFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now verify that data is stored persistently using two different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: On the backend storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By recreating the container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s verify that the file indeed exists on our `storage.example.com` server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s try to delete and create the container again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the data persists and is available, even after the container
    has been deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Managing volumes through oc volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift users can attach a volume to any running application by using `oc
    volume`. In this example, we are going to create a pod with a basic application
    and attach a persistent volume to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, just deploy a basic Apache web server using `oc new-app`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'After a little while, all httpd service resources will be available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '`oc new-app` created a deployment configuration that controls the application
    deployment process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are going to attach a PVC named `pvc-data` as a volume
    to the running container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify that an NFS share was mounted to the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create an `index.html` file on our storage server directly in the
    export:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The previous command was run on the storage server, not on OpenShift!
  prefs: []
  type: TYPE_NORMAL
- en: 'Once persistent data is available, we can try to access the web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, attaching a volume to the pod worked fine. Now, we can detach
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Please be aware that OpenShift rolls new pods out each time you update the corresponding
    deployment config. This means that the container's IP addresses will be changed.
    To avoid that, we recommend testing your configuration using the service's IP
    address.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the container is recreated, notice that the persistent data is not available.
    The `httpd` daemon shows the default page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Persistent data for a database container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s attach an iSCSI-based persistent volume to a MariaDB instance. First,
    we will have to launch a `mariadb` application as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait a couple of minutes, and check the status of the `mariadb` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to know the default location of the database files. This can be gathered
    using the `oc describe dc` command, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, by default, that container stores all data at `/var/lib/mysql/data`
    in the `mariadb-volume-1` volume. This allows us to replace data on it by using
    the `oc volume` subcommand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to attach a volume to the `mariadb` container. Please be
    aware that previously created database structures will be lost, as they are not
    stored persistently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This will automatically redeploy `mariadb` and place database files on the persistent
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: The `ext4` file system should be created on the iSCSI target in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Now you see that Openshift integrates easily with the most popular storage protocols
    and allows you to make containerized applications to be more resilient.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Persistent storage usage is a daily activity for OpenShift cluster administrators
    and OpenShift users in a production environment. In this chapter, we briefly discussed
    persistent storage OpenShift API objects such as PV and PVC. Both PV and PVC allow
    you to define and use persistent storage. We showed you how to configure basic
    underlying storage services such as NFS, GlusterFS, and iSCSI, and how to add
    them to OpenShift's infrastructure via `PV` objects. Additionally, we worked on
    requesting persistent storage via `PVC` objects. Lastly, we showed you a basic
    example of persistent storage usage from an application point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which would be a good use case for persistent storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PostgreSQL database for development
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: MariaDB database for production
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Memcached
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: JDBC-connector
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which of the following OpenShift storage plugins supports the `ReadWriteMany`
    access mode? choose two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NFS
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: iSCSI
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Cinder Volume
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GlusterFS
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which project must PVs belong to?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: default
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: openshift
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Any project
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: openshift-infra
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we created a PVC that requests 2 Gi storage. Which PV will be bound
    to it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1950 Mi
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1950 M
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 2 Gi
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 3 Gi
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which OpenShift API objects must be created before using GlusterFS volumes?
    choose two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pod
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Service
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Endpoint
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Route
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of links for you to take a look at if you are interested in
    the topics we covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.openshift.org/latest/install_config/persistent_storage/index.html](https://docs.openshift.org/latest/install_config/persistent_storage/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://linux-iscsi.org/wiki/LIO](http://linux-iscsi.org/wiki/LIO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
