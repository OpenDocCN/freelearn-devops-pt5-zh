- en: Instrumenting Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we used data from *cAdvisor* to scale services automatically.
    Specifically, Prometheus was firing alerts if memory limits were reached. When
    memory utilization was over the limit, we were scaling the service associated
    with the data. While that approach is a good start, it is far from enough for
    the type of the system we’re building. As a minimum, we need to measure response
    times of our services. Should we look for an exporter that would provide that
    information?
  prefs: []
  type: TYPE_NORMAL
- en: The chances are that your first thought would be to use [haproxy_exporter](https://github.com/prometheus/haproxy_exporter).
    If all public requests are going through it, it makes sense to scrape response
    times and set some alerts based on collected data. That model would be in line
    with the most of the other monitoring systems. The only problem with that approach
    is that it would be almost useless.
  prefs: []
  type: TYPE_NORMAL
- en: Not all requests are going through the proxy. Services that do not need to be
    accessed publicly are not hooked into the proxy. For example, *Docker Flow Swarm
    Listener* cannot be accessed. It does not publish any port, nor it has an API.
    It listens to Docker Socket and sends information to other services (e.g. *Docker
    Flow Proxy*, *Docker Flow Monitor*, and so on). It is entirely invisible to the
    proxy. We could overlook this lack of information if that were the only problem
    behind the idea of monitoring the proxy.
  prefs: []
  type: TYPE_NORMAL
- en: When a request enters the proxy, it is forwarded to a service based on request
    paths, domains, and a few other criteria. By scraping metrics from the proxy,
    we would know only response times of those requests. In many cases, a service
    that receives a request forwarded from the proxy is making other requests. For
    example, *go-demo* communicates with *MongoDB*. A service that receives a request
    from the proxy might make many requests to other services. The proxy does not
    know about any of those. It receives a request, forwards it, waits for a response,
    and re-sends it to the client that initiated the communication. It is oblivious
    of any other processes or requests happening in the middle. As a result, we would
    know the duration of a request that enters the proxy but would be oblivious what
    are response times of each service involved in serving those requests.
  prefs: []
  type: TYPE_NORMAL
- en: Without the knowledge about response times of each service, we cannot deduce
    which one needs to be scaled. If a response time of a backend is high, should
    we scale that same backend or the database it uses?
  prefs: []
  type: TYPE_NORMAL
- en: To make things more complicated, response times are not the only metrics we
    need. We might be interested in failure rates, paths, methods, and a few other
    pieces of additional data. And all that needs to be related to a particular service,
    or even a concrete replica of a service
  prefs: []
  type: TYPE_NORMAL
- en: If your memory serves you well, you might remember that I said that **my advice
    is to always start with exporters, and instrument your services only if you require
    metrics that are not provided by one of the existing exporters**. Well… We reached
    that point when exporters are not enough. We need to instrument our services and
    gather more detailed metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll limit the focus to only a few metrics. Specifically, we’ll explore ways
    to collect error counts, response times, status codes, and a few other metrics.
    Do not take that as a sign that other types are not needed. They are. However,
    we need to keep the scope limited and produce tangible results within a decent
    number of pages. Otherwise, we could just as well start competing with [Encyclopedia
    Britannica](https://en.wikipedia.org/wiki/Encyclop%C3%A6dia_Britannica). I will
    assume that you will take those examples for what they are and use them as a base
    for your own system. Error rates, response times, and status codes might be the
    most common types of metrics, but they are almost certainly not the only ones
    you need.
  prefs: []
  type: TYPE_NORMAL
- en: With the scope limited to only a few metrics, we should spend a bit of time
    discussing the data we would need.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Requirements Behind Service Specific Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We might need different types of metrics. Some of them could be simple counters.
    A good example is errors. We might want to count them and react when their numbers
    reach certain thresholds. That in itself might not be enough, and we should be
    able to differentiate errors depending on a function or part of the service that
    produced them.
  prefs: []
  type: TYPE_NORMAL
- en: How about more complex metrics? Response times are another good example.
  prefs: []
  type: TYPE_NORMAL
- en: We might need a metric that will provide request response times. That might
    lead us towards having something like `resp_time 0.043` as the metric. It has
    a name (`resp_time`) and value in seconds (`0.043`). If we’d implement a metric
    like that, we’d soon discover that we need more. Having the information that the
    system responses are slow does not give us a clue which part of it is misbehaving.
    We need to know the name of the service.
  prefs: []
  type: TYPE_NORMAL
- en: We might not be able to instrument all the services in our clusters. If we take
    `go-demo` stack as an example, it consists of two services. It has a backend and
    a MongoDB. The backend is in our control, and we can easily extend it by adding
    instrumentation. The database is a different story. While we can (and should)
    use [MongoDB Exporter](https://github.com/dcu/mongodb_exporter), it provides data
    related to the server status. What we need are metrics that we can relate to the
    backend service. We need to know whether a request sent to the `go-demo` stack
    is slow because of an issue in the backend or the database. Assuming that we are
    not going to “adapt” MongoDB to our own needs, we should try to answer that and
    few other questions by extending metrics inside services we’re controlling.
  prefs: []
  type: TYPE_NORMAL
- en: We can use request path and method. If we add it to our metric, it should give
    us fairly good granularity of information. Depending on the path and the method,
    we can know whether the metric is related to the database or is limited to the
    internal processes of the service. We could also add query, but that would go
    too far. It would record almost each request separately and might result in too
    much memory and CPU usage when stored in Prometheus. Our updated metric could
    be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
