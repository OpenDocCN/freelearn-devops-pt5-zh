- en: Creating and Managing Stateful Services in a Swarm Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any sufficiently advanced technology is indistinguishable from magic.
  prefs: []
  type: TYPE_NORMAL
- en: '- Arthur C. Clarke'
  prefs: []
  type: TYPE_NORMAL
- en: If you're attending conferences, listening to podcasts, reading forums, or you
    are involved in any other form of a debate related to containers and cloud-native
    applications, you must have heard the mantra stateless services. It's almost like
    a cult. Only stateless services are worthy. Everything else is heresy. The solution
    to any problem is to remove the state. How do we scale this application? Make
    it stateless. How do we put this into a container? Make it stateless. How do we
    make it fault tolerant? Make it stateless. No matter the problem, the solution
    is to be stateless.
  prefs: []
  type: TYPE_NORMAL
- en: Are all the services we used until now stateless? They're not. Therefore, the
    logic dictates, we did not yet solve all the problems.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start exploring stateless services, we should go back in time and
    discuss The twelve-factor app methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the twelve-factor app methodology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming that my memory still serves me well, *Heroku* ([https://www.heroku.com/](https://www.heroku.com/))
    became popular somewhere around 2010\. It showed us how to leverage *Software-as-a-Service*
    principles. It freed developers from thinking too much about underlying infrastructure.
    It allowed them to concentrate on development and leave the rest to others. All
    we had to do is push our code to Heroku. It would detect the programming language
    we use, create a VM and install all the dependencies, build, launch, and so on.
    The result would be our application running on a server.
  prefs: []
  type: TYPE_NORMAL
- en: Sure, in some cases Heroku would not manage to figure out everything by itself.
    When that happens, all we'd have to do is create a simple config that would give
    it a few extra pieces of information. Still very easy and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Startups loved it (some still do). It allowed them to concentrate on developing
    new features and leave everything else to Heroku. We write software, and someone
    else runs it. This is **Software-as-a-Service (SaaS)** at its best. The idea and
    the principles behind it become so popular that many decided to clone the idea
    and create their own Heroku-like services.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after Heroku received a broad adoption, its creators realized that many
    applications did not perform as expected. It's one thing to have a platform that
    frees developers from operations, but quite another thing to actually write code
    that fares well under SaaS providers. So, Heroku folks and a few others came up
    with *The Twelve-Factor App* ([https://12factor.net/](https://12factor.net/))
    principles. If your application fulfills all twelve factors, it will work well
    as SaaS. Most of those factors are valid for any modern application, no matter
    whether it will run inside on-premise servers or through a cloud computing provider,
    inside PaaS, SaaS, a container, or none of the above. Every modern application
    should be made using The twelve-factor app methodology. Or, at least, that's what
    many are saying.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore each of those factors and see how well we fare against it. Maybe,
    just maybe, what we learned so far will make us twelve-factor compliant. We'll
    go through all the factors and compare them with the services we used through
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Codebase**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One codebase tracked in revision control, many deploys. The `go-demo` service
    is in a single Git repository. Every commit is deployed to testing and production
    environments. All other services we created are released by someone else. – Passed
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependencies**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explicitly declare and isolate dependencies. All the dependencies are inside
    Dockers images. Excluding Docker Engine, there is no system-wide dependency. Docker
    images fulfill this principle by default.*– Passed*
  prefs: []
  type: TYPE_NORMAL
- en: '**Config**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store config in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The `go-demo` service does not have any configuration file. Everything is set
    through environment variables. The same can be said for all other services we
    created. Service discovery through networking was a huge help accomplishing this,
    by allowing us to find services without any configuration. Please note that this
    principle applies only to configurations that vary between deployments. Everything
    else can continue being a file as long as it stays the same no matter when and
    where the service is deployed. *-* Passed
  prefs: []
  type: TYPE_NORMAL
- en: '**Backing services**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Treat backing services as attached resources.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, MongoDB is a backing service. It is attached to the primary service
    `go-demo` through networking. *– Passed*
  prefs: []
  type: TYPE_NORMAL
- en: '**Build, release, run**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Strictly separate build and run stages.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, everything except running services is considered the build
    phase. In our case, the build phase is clearly separated from run stages. Jenkins
    is building our services while Swarm is running them. Building and running are
    performed in separate clusters. *–* Passed
  prefs: []
  type: TYPE_NORMAL
- en: '**Processes**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the app as one or more stateless processes.
  prefs: []
  type: TYPE_NORMAL
- en: We are failing this principle big time. Even though the `go-demo` service is
    stateless, almost everything else (`docker-flow-proxy`, `jenkins`, `prometheus`,
    and so on) is not. – Failed
  prefs: []
  type: TYPE_NORMAL
- en: '**Port binding**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Export services via port binding.
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking and the `docker-flow-proxy` are taking care of port bindings.
    In many cases, the only service that will bind any port is the `proxy`. Everything
    else should be inside one or more networks and made accessible through the `proxy`.
    – Passed
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrency**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale out via the process model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This factor is directly related to statelessness. Stateless services (example:
    `go-demo`) are easy to scale. Some non-stateless services (example: `docker-flow-proxy`)
    are designed to be scalable, so they fulfill this principle as well. Many other
    stateful services (example: Jenkins, Prometheus, and so on) cannot be scaled horizontally.
    Even when they can, the process is often too complicated and prone to errors.
    – Failed'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disposability**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maximize robustness with fast startup and graceful shutdown.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless services are disposable by default. They can be started and stopped
    at a moments notice, and they tend to be fault tolerant. In case an instance fails,
    Swarm will reschedule it in one of the healthy nodes. The same cannot be said
    for all the services we used. Jenkins and MongoDB, just to name a few, will lose
    their state in case of a failure. That makes them anything but disposable. – Failed
  prefs: []
  type: TYPE_NORMAL
- en: '**Dev/prod parity**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep development, staging, and production as similar as possible.
  prefs: []
  type: TYPE_NORMAL
- en: That is one of the main benefits Docker provides. Since containers are created
    from immutable images, a service will be the same no matter whether it runs on
    our laptop, testing environment, or production. – Passed
  prefs: []
  type: TYPE_NORMAL
- en: '**Logs**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Treat logs as event streams.
  prefs: []
  type: TYPE_NORMAL
- en: The *ELK* stack together with *LogSpout* fulfills this principle. All the logs
    from all the containers are streamed into *ElasticSearch* as long as applications
    inside the containers are outputting them to `stdout`. Jenkins, as we run it,
    is the exception since it writes some of the logs to files. However, that is configurable,
    so we won't fault it for that. – Passed
  prefs: []
  type: TYPE_NORMAL
- en: '**Admin processes**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `admin/management` tasks as one-off processes. In our case, all the processes
    are executed as Docker containers, apparently fulfilling this factor. – Passed
  prefs: []
  type: TYPE_NORMAL
- en: We passed nine out of twelve factors. Should we aim for all twelve? Actually,
    the question is wrong. A better-phrased question would be whether we can aim for
    all twelve factors. We often can't. The world was not built yesterday, and we
    cannot throw away all the legacy code and start fresh. Even if we could, twelve-factor
    app principles have one big fallacy. They assume that there is such a thing as
    a system comprised completely of stateless services.
  prefs: []
  type: TYPE_NORMAL
- en: No matter which architecture style we adopt (microservices included), applications
    have a state! In a microservices style architecture, each service can have multiple
    instances, and each service instance should be designed to be stateless. What
    that means, is that a service instance does not store any data across operations.
    Hence, being stateless means that any service instance can retrieve all application
    state required to execute a behavior, from somewhere else. That is a significant
    architectural constraint of microservices style applications, as it enables resiliency,
    elasticity, and allows any available service instance to execute any task. Even
    though the state is not inside a service we are developing, it still exists and
    needs to be managed somehow. The fact that we did not develop the database where
    the state is stored does not mean that it should not follow the same principles
    and be scalable, fault tolerant, resilient, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: So, all systems have a state, but a service can be stateless if it cleanly separates
    behaviors from data, and can fetch data required to perform any behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Could the authors of twelve-factor app principles be so shortsighted as to assume
    that state does not exist? They indeed aren't. They assume that everything but
    the code we write will be a service maintained by someone else. Take MongoDB as
    an example. Its primary purpose is to store state, so it is, of course, stateful.
    The twelve-factor app authors assume that we are willing to let someone else manage
    stateful services and focus only on those we are developing.
  prefs: []
  type: TYPE_NORMAL
- en: While, in some cases, we might choose to use Mongo as a service maintained by
    one of the cloud providers, in many others such a choice is not the most efficient.
    If anything else, such services tend to be very expensive. That cost is often
    worth paying when we do not have the knowledge or the capacity to maintain our
    backing services. However, when we do, we can expect better and cheaper results
    if we run a database ourselves. In such a case, it is one of our services, and
    it is obviously stateful. The fact that we did not write all the services does
    not mean we are not running them and are, therefore, responsible for them.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that all three principles we failed are related to statefulness.
    If we manage to create services in a way that their state is preserved on shutdown
    and shared between all instances, we'll manage to make the whole system *cloud
    native*. We'll be able to run it anywhere, scale its services as needed, and make
    the system fault tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and managing stateful services is the only major piece of the puzzle
    we are missing. After this chapter, you will be on your way to running any type
    of services inside your Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We will start the practical part of this chapter by creating a Swarm cluster.
    We'll use AWS only as a demonstration. The principles explored here can be applied
    to almost any cloud computing provider as well as to on-premise servers.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Swarm cluster and the proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use *Packer* ([https://www.packer.io/](https://www.packer.io/)) and *Terraform* ([https://www.terraform.io/](https://www.terraform.io/))
    to create a Swarm cluster in AWS. For now, the configuration we'll use will be
    (almost) the same as the one we explored in the Chapter 12, *Creating and Managing
    a Docker Swarm Cluster in Amazon Web Services (AWS)*. We'll extend it later on
    when we reach more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `13-volumes.sh` ([https://gist.github.com/vfarcic/338e8f2baf2f0c9aa1ebd70daac31899](https://gist.github.com/vfarcic/338e8f2baf2f0c9aa1ebd70daac31899))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll continue using the `vfarcic/cloud-provisioning` ([https://github.com/vfarcic/cloud-provisioning](https://github.com/vfarcic/cloud-provisioning))
    repository. It contains configurations and scripts that''ll help us out. You already
    have it cloned. To be on the safe side, we''ll `pull` the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Packer and Terraform configurations are in the `terraform/aws-full` ([https://github.com/vfarcic/cloud-provisioning/tree/master/terraform/aws-full](https://github.com/vfarcic/cloud-provisioning/tree/master/terraform/aws-full))
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define a few environment variables that will provide Packer the information
    it needs when working with AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual values. Consult the Chapter 12, *Creating
    And Managing A Docker Swarm Cluster in Amazon Web Services* if you lost the keys
    and forgot how to create them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are ready to create the first image we''ll use in this chapter. The Packer
    configuration we''ll use is in `terraform/aws-full/packer-ubuntu-docker-compose.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-compose.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-compose.json)).
    It is almost the same as the one we used before so we''ll comment only the relevant
    differences. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The file provisioner copies the docker.service file into the VM. The commands
    from the shell provisioner will move the uploaded file to the correct directory,
    give it correct permissions, and restart the `docker service`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `docker.service` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/docker.service](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/docker.service))
    file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Docker service configuration is almost identical to the default one. The
    only difference is `-H tcp://0.0.0.0:2375` in `ExecStart`.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Docker Engine does not allow remote connections. If its configuration
    is left unchanged, we cannot send commands from one server to another. By adding
    `-H tcp://0.0.0.0:2375`, we are telling Docker to accept requests from any address
    `0.0.0.0`. Normally, that would be a big security risk. However, all AWS ports
    are closed by default. Later on, we'll open `2375` only to servers that belong
    to the same security group. As a result, we will be able to control any Docker
    Engine as long as we are inside one of our servers. As you will see soon, this
    will come in handy in quite a few examples that follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build the AMI defined in `packer-ubuntu-docker-compose.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can turn our attention to Terraform that''ll create our cluster. We''ll
    copy the SSH key `devops21.pem` that we created earlier and declare a few environment
    variables that will allow Terraform to access our AWS account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Terraform expects environment variables to be prefixed with `TF_VAR`, so we
    had to create new ones, even though their values are the same as those we used
    for Packer. The value of the environment variable `KEY_PATH` is only an example.
    You might have it stored somewhere else. If that's the case, please change the
    value to the correct path.
  prefs: []
  type: TYPE_NORMAL
- en: The last command filters the `packer-ubuntu-docker.log` and stores the AMI ID
    as the environment variable `TF_VAR_swarm_ami_id`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create a Swarm cluster. Three VMs should suffice for the exercises
    that follow, so we''ll only create managers. Since the commands will be the same
    as those we executed in the previous chapters, we''ll just skip the explanation
    and run them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We created the first server and initialized the Swarm cluster. Later on, we
    retrieved the token and the IP of one of the managers and used that data to create
    and join two additional nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be on the safe side, we''ll enter one of the managers and list the nodes
    that form the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Where are the workers?**'
  prefs: []
  type: TYPE_NORMAL
- en: We did not create any worker nodes. The reason is simple. For the exercises
    in this chapter, three nodes are more than enough. That should not prevent you
    from adding worker nodes when you start using a similar cluster setup in your
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add worker nodes, please execute the commands that follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '`export TF_VAR_swarm_worker_token=$(ssh\ ''-i devops21.pem ''''ubuntu@$(terraform
    output ''''swarm_manager_1_public_ip)'' ''docker swarm join-token -q worker) terraform
    apply\''-target aws_instance.swarm-worker''`'
  prefs: []
  type: TYPE_NORMAL
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4/jenkins` in
    your browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are almost done. The only thing left, before we move into statefulness,
    is to run the `docker-flow-proxy` and `docker-flow-swarm-listener` services. Since
    we already created them quite a few times, there''s no need for an explanation
    so we can speed up the process by deploying the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Running stateful services without data persistence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start the exploration of stateful services in a Swarm cluster by taking
    a look at what would happen if we deploy them as any other service.
  prefs: []
  type: TYPE_NORMAL
- en: A good example is Jenkins. Every job we create is an XML file. Every plugin
    we install is an HPI file. Every configuration change is stored as XML. You get
    the picture. Everything we do in Jenkins ends up being a file. All those files
    form its state. Without it, Jenkins would not be able to operate. Jenkins is also
    a good example of the problems we have with legacy applications. If we were to
    design it today, it would probably use a database to store its state. That would
    allow us to scale it since all instances would share the same state by being connected
    to the same database. There are quite a few other design choices we would probably
    make if we were to design it today from scratch. Being legacy is not necessarily
    a bad thing. Sure, having the experience we have today would help us avoid some
    of the pitfalls of the past. On the other hand, being around for a long time means
    that it is battle tested, has a high rate of adoption, a huge number of contributors,
    a big user base, and so on. Everything is a trade-off, and we cannot have it all.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll put aside the pros and cons of having a well established and battle-tested
    versus young and modern, but often unproven application. Instead, let''s take
    a look at how Jenkins, as a representative of a stateful service, behaves when
    running inside the Swarm cluster we created with Terraform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We entered one of the managers and created the `jenkins` service.
  prefs: []
  type: TYPE_NORMAL
- en: Please wait a few moments until `jenkins` service is running. You can use docker
    `service ps jenkins` to check the current state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that Jenkins is running, we should open it in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users** Git Bash might not be able to use the `open` command.
    If that''s the case, execute `terraform output` `swarm_manager_1_public_ip` to
    find out the IP of the manager and open the URL directly in your browser of choice.
    For example, the command above should be replaced with the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`terraform output swarm_manager_1_public_ip`'
  prefs: []
  type: TYPE_NORMAL
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4/jenkins` in
    your browser.
  prefs: []
  type: TYPE_NORMAL
- en: As you remember from the [Chapter 6](600ac100-1492-4de3-9607-5ad11b628bbb.xhtml), *Automating
    Continuous Deployment Flow with Jenkins*, we need to retrieve the password from
    logs or its file system. However, this time, doing that is a bit more complicated.
    Docker Machine mounts local (laptop's) directory into every VM it creates so we
    could retrieve the `initialAdminPassword` without even entering VMs.
  prefs: []
  type: TYPE_NORMAL
- en: There is no such thing with AWS *at least not yet*, so we need to find out which
    EC2 instance hosts Jenkins, find the ID of the container, and enter into it to
    get the file. Such a thing would be easy to do manually but, since we are committed
    to automation, we'll do it the hard way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start the quest of finding the password by entering one of the managers
    and list service tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs and ERROR coloumn are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Luckily, AWS EC2 instances contain internal IP in their names. We can use that
    to our advantage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We listed the service tasks and piped it to tail so that only the last line
    is returned. Then we used `awk` to get the fourth column. The cut command printed
    the result from the fourth byte effectively removing `ip-`. All that was piped
    to tr that replaced - with Finally, the result was stored in the environment variable
    `JENKINS_IP`.
  prefs: []
  type: TYPE_NORMAL
- en: If this was too freaky for you, feel free to assign the value manually (in my
    case it was `172.31.16.159).`
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know which node is hosting Jenkins, we need to retrieve the ID of
    the container. Since we modified the `docker.service` config to allow us to send
    commands to a remote engine, we can use the `-H` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that retrieves the ID of the Jenkins container is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We used `-H` to tell the local client to connect to the remote engine running
    in `tcp://$JENKINS_IP:2375`. We listed all running containers `ps` in quiet mode
    `-q` so that only IDs are returned. We also applied a filter, so that only the
    service named Jenkins is retrieved. The result was stored in the environment variable
    `JENKINS_ID`.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can use the IP and the ID to enter the container and output the password
    stored in the file `/var/jenkins_home/secrets/initialAdminPassword`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is, in my case, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Please copy the password, return to the Jenkins UI, and paste it.
  prefs: []
  type: TYPE_NORMAL
- en: Complete the Jenkins setup before proceeding further. You already know the drill
    from the [Chapter 6](600ac100-1492-4de3-9607-5ad11b628bbb.xhtml), *Automating
    Continuous Deployment Flow with Jenkins*, so I'll let you do it in peace.
  prefs: []
  type: TYPE_NORMAL
- en: The result should be the screen similar to the one in the *figure 13-1:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/jenkins-home.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-1: Jenkins home screen after the initial setup'
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the easy question which I'm sure you'll know how to answer. What
    would happen if, for whatever reason, Jenkins instance fails?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s simulate the failure and observe the outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We used the environment variables `JENKINS_IP` and `JENKINS_ID` to send the
    forced remove `rm -f` command to the remote node that hosts Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: Nothing lasts forever. Sooner or later, the service would fail. If it doesn't,
    the node where it runs will. By removing the container, we simulated what would
    happen in a real-world situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a while, Swarm will detect that the jenkins replica failed and instantiate
    a new one. We can confirm that by listing jenkins tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: So far, so good. Swarm is doing what we want it to do. It is making sure that
    our services are (almost) always running.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing left is to go back to the UI and refresh the screen.
  prefs: []
  type: TYPE_NORMAL
- en: The screen should look similar to the one in *figure 13-2:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/jenkins-setup.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-2: Jenkins initial setup screen'
  prefs: []
  type: TYPE_NORMAL
- en: That's embarrassing. Everything we did is lost, and we are back to square one.
    Since Jenkins state was not persisted outside the container, when Swarm created
    a new one, it started with a blank slate.
  prefs: []
  type: TYPE_NORMAL
- en: How can we solve this problem? Which solutions can we employ to address the
    persistence issue?
  prefs: []
  type: TYPE_NORMAL
- en: 'Please remove the `jenkins` service before proceeding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Persisting stateful services on the host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Host-based persistence was very common in the early Docker days when people
    were running containers on predefined nodes without schedulers like Docker Swarm,
    Kubernetes, or Mesos. Back then, we would choose a node where we'll run a container
    and put it there. Upgrades were performed on the same server. In other words,
    we packaged applications as containers and, for the most part, treated them as
    any other traditional service. If a node fails... tough luck! It's a disaster
    with or without containers.
  prefs: []
  type: TYPE_NORMAL
- en: Since serves were prederfined, we could persist the state on the host and rely
    on backups when that host dies. Depending on the backup frequency, we could lose
    a minute, an hour, a day, or even a whole week worth of data. Life is hard.
  prefs: []
  type: TYPE_NORMAL
- en: The only positive thing about this approach is that persistence is easy. We
    would mount a host volume inside a container. Files are persisted outside the
    container so no data would be lost under "normal" circumstances. If the container
    is restarted as a result of a failure or an upgrade, data would still be there
    when we run a new container.
  prefs: []
  type: TYPE_NORMAL
- en: There are other single-host variations of the model. Data volumes, data only
    containers, and so on. All of them share the same drawback. They remove portability.
    Without portability, there is no fault tolerance, nor is there scaling. There
    is no Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Host-based persistence is unacceptable, so I won't waste any more of your time.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a sysadmin background, you are probably wondering why I haven’t
    mentioned N**etwork File System **(**NFS**). The reason is simple. I wanted you
    to feel the pain before diving into the obvious solution.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting stateful services on a Network File System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to find a way to retain state outside containers that run our services.
  prefs: []
  type: TYPE_NORMAL
- en: We could mount a volume on the host. That would allow us to preserve state if
    a container fails and is rescheduled on the same node. The problem is that such
    a solution is too limited. There is no guarantee that Swarm will reschedule the
    service to the same node unless we constrain it. If we would do something like
    that, we'd prevent Swarm from ensuring service availability. When that node would
    fail (every node fails sooner or later), Swarm could not reschedule the service.
    We would be fault tolerant only as long as our servers are running.
  prefs: []
  type: TYPE_NORMAL
- en: We can solve the problem of a node failure by mounting a NFS to each of the
    servers. That way, every server would have access to the same data, and we could
    mount a Docker volume to it.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use **Amazon Elastic File System** (**EFS**) ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)).
    Since this book is not dedicated to AWS, I’ll skip the comparison of different
    AWS file systems and only note that the choice of EFS is based on its ability
    to be used across multiple availability zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please open the *EFS home *([https://console.aws.amazon.com/efs/home](https://console.aws.amazon.com/efs/home))
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: Git Bash might not be able to use the `open` command. If that's the case, please
    replace `$AWS_DEFAULT_REGION` with the region where your cluster is running (for
    example, `us-east-1`) and open it in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: Click the Create file system button. For each of the availability zones, replace
    the default security group with *docker* (we created it earlier with Terraform).
    Click the button Next Step twice, followed by Create File System.
  prefs: []
  type: TYPE_NORMAL
- en: We should wait until Life cycle state is set to Available for each of the zones.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to mount the EFS in each of the nodes. The easiest way to do
    that is by clicking the Amazon EC2 mount instructions link. We are interested
    only in the command from the third point of the Mounting your file system section.
    Please copy it.
  prefs: []
  type: TYPE_NORMAL
- en: 'All that''s left is to enter each of the nodes and execute the command that
    will mount the EFS volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We entered the first manager and created `/mnt/efs` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Paste the command you copied from the EC2 mount instructions screen. We'll make
    a tiny modification before executing it. Please change the destination path from
    `efs` to `/mnt/efs` and execute it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, the command is as follows (yours will be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We should also create a sub-directory where we''ll store Jenkins state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We created the directory `/mnt/efs/jenkins,` gave full permissions to everyone,
    and exited the server. Since Swarm might decide to create the service on any of
    the nodes, we should repeat the same process on the rest of the servers. Please
    note that your mount will be different, so do not simply paste the `sudo mount`
    command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can, finally, try again to create the `jenkins` service. Hopefully,
    this time the state will be preserved in case of a failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between this command and the one we used before is in the
    `--mount` argument. It tells Docker to mount host directory `/mnt/efs/jenkins`
    as `/var/jenkins_home` inside the container. Since we mounted `/mnt/efs` as EFS
    volume on all nodes, the `jenkins` service will have access to the same files
    no matter which server it will run in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we should wait until the service is running. Please execute the `service
    ps` command to see the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s open Jenkins UI in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Git Bash might not be able to use the `open` command. If that''s the case,
    execute `terraform output swarm_manager_1_public_ip` to find out the IP of the
    manager and open the URL directly in your browser of choice. For example, the
    preceding command should be replaced with the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`terraform output swarm_manager_1_public_ip`If the output would be `1.2.3.4`,
    you should open `http://1.2.3.4/jenkins` in your browser.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, since Jenkins home directory is mounted as `/mnt/efs/jenkins,` finding
    the password will be much easier. All we have to to is output the contents of
    the file `/mnt/efs/jenkins/secrets/initialAdminPassword` from one of the servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Please copy the password and paste it to the Administrator password field in
    the Jenkins UI. Complete the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/jenkins-home.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-3: Jenkins home screen after the initial setup'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll simulate a failure one more time and observe the results. The commands
    that follow are the same as those we executed previously so there should be no
    reason to comment them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Please wait until Swarm instantiates a new replica and refresh the Jenkins
    UI screen in your browser. This time, we are presented with the login page instead
    of going back to the initial setup. The state was preserved, making our `jenkins`
    service fault tolerant. In the worst case scenario, when the service or the entire
    node fails, we''ll have a short period of downtime until Swarm recreates the failed
    replica. You might be wondering: why did I force you to go through manual steps
    to create an EFS and mount it? Shouldn''t that be done automatically through Terraform?
    The reason is simple. This solution is not worth automating. It has quite a few
    downsides.'
  prefs: []
  type: TYPE_NORMAL
- en: We would need to place states from all the services into the same EFS drive.
    A better solution would be to create an EFS volume for each service. The problem
    with such an approach is that we would need to alter the Terraform config every
    time someone adds a new stateful service to the cluster. In that case, Terraform
    would not be of much help since it is not meant to have service-specific configs.
    It should act as a method to setup a cluster that could host any service. Even
    if we accept a single EFS volume for all services, we would still need to create
    a new sub-directory for each service. Wouldn't it be much better if we leave Terraform
    as a tool for creating infrastructure and Docker for all tasks related to services?
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are better ways to create and mount EFS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we explore alternatives, please remove the `jenkins` service and `exit`
    the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: There is no reason to keep the EFS volume we created earlier, so please head
    back to *EFS console* ([https://console.aws.amazon.com/efs](https://console.aws.amazon.com/efs)),
    select the file system, and click Actions followed with the Delete file system
    button. For the rest of the steps, please follow the instructions on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Data volume orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are quite a few storage orchestration solutions that integrate with Docker
    through its volume plugins. We won’t compare them. Such an attempt would require
    a whole chapter, maybe even a book.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you choose a different solution, the principles that will be explained
    shortly apply to (almost) all others. For a complete list of currently supported
    plugins, please visit the *Volume plugins* ([https://docs.docker.com/engine/extend/legacy_plugins/#/volume-plugins](https://docs.docker.com/engine/extend/legacy_plugins/#/volume-plugins))
    section of the *Use Docker Engine Plugins* ([https://docs.docker.com/engine/extend/legacy_plugins/](https://docs.docker.com/engine/extend/legacy_plugins/))
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '*REX-Ray* ([https://github.com/codedellemc/rexray](https://github.com/codedellemc/rexray))
    is a vendor agnostic storage orchestration engine. It is built on top of the *libStorage* ([http://libstorage.readthedocs.io](http://libstorage.readthedocs.io))
    framework. It supports *EMC*, *Oracle VirtualBox*, and *Amazon EC2*. At the time
    of this writing, support for *GCE*, *Open Stack*, *Rackspace*, and *DigitalOcean*
    is under way.'
  prefs: []
  type: TYPE_NORMAL
- en: I find it easier to grasp something when I see it in action. In that spirit,
    instead of debating for a long time what REX-Ray does and how it works, we'll
    jump right into a practical demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting stateful services with REX-Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start by setting up REX-Ray manually. If it turns out to be a good solution
    for our stateful services, we'll move it to Packer and Terraform configurations.
    Another reason for starting with a manual setup is to give you a better understanding
    how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the AWS access keys and the region that we already used quite a few
    times, we''ll also need the ID of the security group we created previously with
    Terraform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the one that follows (yours will be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Please copy the value. We'll need it soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll enter one of the nodes where we''ll install and configure REX-Ray:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'REX-Ray is fairly simple to set up. That''s one of the reasons I prefer it
    over some other solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We installed *REX-Ray version 0.6.3*, as well as its dependency *libStorage
    version 0.3.5*. In your case, versions might be newer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll create environment variables with values required for REX-Ray
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Please replace `[...]` with the actual values. The value of the security group
    should be the same as the one we previously retrieved with the `terraform output
    security_group_id` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to configure REX-Ray through its YML configuration file stored
    in `/etc/rexray/config.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We set the driver to `efs` and provided it with the AWS data. The result was
    output to the `/etc/rexray/config.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can start the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'REX-Ray is running, and we can `exit` the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Since we do not know which node will host our stateful services, we need to
    set up REX-Ray on every node of the cluster. Please repeat the setup steps on
    Swarm manager nodes 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once REX-Ray is running on all the nodes, we can give it a spin. Please enter
    one of the managers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'REX-Ray can be used directly through the `rexray` binary we installed. For
    example, we can list all the volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: There's not much to see since we have not created any volumes yet. We can do
    that with the `rexray` volume create command. However, there is no need for such
    a thing. Thanks to its integration with Docker, there is not much need to use
    the binary directly for any operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try one more time to create the `jenkins` service. This time, we''ll
    use REX-Ray as the volume driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between the command we just executed and the previous attempt
    to create the `jenkins` service is in the `--mount` argument. The source is now
    simply a name `jenkins`. It represents the name of the volume. The target is still
    the same and represents Jenkins home inside a container. The important difference
    is the addition of the `volume-driver` argument. That was the instruction that
    Docker should use `rexray` to mount a volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'If integration between REX-Ray and Docker worked, we should see a `jenkins`
    volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the output of the `rexray` volume get command is not empty. We can
    see the `jenkins` volume. As I already mentioned, there''s not much need to use
    the `rexray` binary. We can accomplish many of its features directly through Docker.
    For example, we can execute `docker volume ls` command to list all the volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing volumes proves only that Docker and REX-Ray registered a new mount.
    Let''s take a look at what happened in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: Git Bash might not be able to use the `open` command. If that's the case, please
    replace `$AWS_DEFAULT_REGION` with the region where your cluster is running (for
    example, `us-east-1`) and open it in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a screen similar to the one presented in *Figure 13-4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efs-rexray.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-4: AWS EFS volume created and mounted with REX-Ray'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, REX-Ray created a new EFS volume called `rexray/jenkins` and
    mounted a target in the same availability zone as the node that hosts the `jenkins`
    service.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing missing to satisfy my paranoid nature is to kill Jenkins and
    confirm that REX-Ray mounted the EFS volume on a new container that will be re-scheduled
    by Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we’ll start by setting up Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Git Bash might not be able to use the `open` command. If that’s the case, execute
    `terraform output` `swarm_manager_1_public_ip` to find out the IP of the manager
    and open the URL directly in your browser of choice. For example, the preceding
    command should be replaced with the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`terraform output swarm_manager_1_public_ip`If the output would be `1.2.3.4`,
    you should open `http://1.2.3.4/jenkins` in your browser.'
  prefs: []
  type: TYPE_NORMAL
- en: We are faced with a recurring challenge. How to find the initial Jenkins administrator
    password. On the bright side, the challenge is useful as a demonstration of different
    ways to access content inside containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we''ll leverage REX-Ray to access data stored in the EFS volume
    instead of trying to find the node and the ID of the container that hosts the
    `jenkins` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the one that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We created a new alpine container that also used the rexray volume driver to
    attach to the jenkins EFS volume. The command output the contents of the `/var/jenkins_home/secrets/initialAdminPassword`
    file that contains the password. Since we specified the `--rm` argument, Docker
    removed the container after the process `cat` exited. The final result is the
    password output to the screen. Please copy it and paste it to the Administrator
    password field in the Jenkins UI. Complete the setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to go through the painful processes of finding the node that hosts
    Jenkins, getting the ID of the container, and executing the `docker rm` command
    on the remote engine. In other words, we''ll run the same set of commands we executed
    during previous murderous attempts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: A few moments later, Swarm will re-schedule the container, and Jenkins will
    be running again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please wait until the service current state is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Reload Jenkins UI and observe that you are redirected to the login screen instead
    to the initial setup. The state was preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re finished with this cluster. Now we need to remove the volume manually.
    Otherwise, since it was not created by Terraform, it would be preserved even after
    we destroy the cluster and AWS would continue charging us for it. The problem
    is that a volume cannot be removed as long as one or more services are using it,
    so we''ll need to destroy `jenkins` service as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Choosing the persistence method for stateful services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are quite a few other tools we could use to persist state. Most of them
    fall into one of the groups we explored. Among different approaches we can take,
    the three most commonly taken are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Do not persist the state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Persist the state on the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Persist the state somewhere outside the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There’s no reason to debate why persisting data from stateful services is critical,
    so the first option is automatically discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are operating a cluster, we cannot rely on any given host to be always
    available. It might fail at any given moment. Even if a node does not fail, sooner
    or later a service will, and Swarm will reschedule it. When that happens, there
    is no guarantee that Swarm will run a new replica on the same host. Even if, against
    all odds, the node never fails, and the service is unbreakable, the first time
    we execute an update of that service (example: a new release), Swarm will, potentially,
    create the new replica somewhere else. All in all, we don’t know where the service
    will run nor how long it will stay there. The only way to invalidate that statement
    is to use constraints that would tie a service to a particular host. However,
    if we do that, there would be no purpose in using Swarm nor reading this book.
    All in all, the state should not be persisted on a specific host.'
  prefs: []
  type: TYPE_NORMAL
- en: That leaves us with the third option. The state should be persisted somewhere
    outside the cluster, probably on a network drive. Traditionally, sysadmins would
    mount a network drive on all hosts, thus making the state available to services
    no matter where they’re running. There are quite a few problems with that approach,
    the main one being the need to mount a single drive and expect all stateful services
    to persist their state to it. We could, theoretically, mount a new drive for every
    single service. Such a requirement would quickly become a burden. If, for example,
    we used Terraform to manage our infrastructure, we’d need to update it every time
    there is a new service. Do you remember the first principle of twelve-factor apps?
    One service should have one codebase. Everything that a service needs should be
    in a single repository. Therefore, Terraform or any other infrastructure configuration
    tool should not contain any details specific to a service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the problem is to manage volumes using similar principles as
    those we''re using to manage services. Just as we adopted schedulers (example:
    Swarm) that are managing our services, we should adopt volume schedulers that
    should handle our mounts.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we adopted Docker containers as a way to run our services, a volume scheduler
    should be able to integrate with it and provide a seamless experience. In other
    words, managing volumes should be an integral part of managing services. Docker
    volume plugins allow us precisely that. Their purpose is to integrate third party
    solutions into the Docker ecosystem and make volume management transparent.
  prefs: []
  type: TYPE_NORMAL
- en: REX-Ray is one of the solutions we explored. There are many others, and I'll
    leave it to you to compare them and make your decision which volume scheduler
    works the best in your use case.
  prefs: []
  type: TYPE_NORMAL
- en: If we are presented only with the choices we explored in this chapter, REX-Ray
    is a clear winner. It allows us to persist data across the cluster in a transparent
    way. The only extra requirement is to make sure that REX-Ray is installed. After
    that, we mount volumes with its driver as if they are regular host volumes. Behind
    the scenes, REX-Ray does the heavy lifting. It creates a network drive, mounts
    it, and manages it.
  prefs: []
  type: TYPE_NORMAL
- en: Long story short, we'll use REX-Rey for all our stateful services. That is not
    entirely accurate so let me rephrase. We'll use REX-Ray for all our stateful services
    that do not use replication and synchronization between instances. If you are
    wondering what that means, all I can say is that patience is a virtue. We’ll get
    there soon.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we decided that REX-Ray will be part of our stack, it is worthwhile
    adding it to our Packer and Terraform configs.
  prefs: []
  type: TYPE_NORMAL
- en: Adding REX-Ray to packer and terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already went through REX-Ray manual setup so it should be relatively easy
    to add it to Packer and Terraform configurations. We'll add to Packer the static
    parts that do not depend on runtime resources and the rest to Terraform. What
    that means is that Packer will create AMIs with REX-Ray installed and Terraform
    will create its configuration and start the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `terraform/aws-full/packer-ubuntu-docker-rexray.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-rexray.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker-rexray.json))
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference when compared with the `terraform/aws-full/packer-ubuntu-docker.json` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker.json](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/packer-ubuntu-docker.json))
    config we used before is an additional command in the shell `provisioner`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: When creating a VM which will later become an AMI, Packer will execute the same
    command that we ran when we installed REX-Ray manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build the AMI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We built the AMI and stored the ID in the environment variable `TF_VAR_swarm_ami_id.`
    It'll be used by Terraform soon.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Terraform part of the REX-Ray setup is a bit more complex since
    its configuration needs to be dynamic and decided at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration is defined in the `terraform/aws-full/rexray.tpl` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl))
    template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the AWS keys, the region, and the security groups are defined
    as variables. The magic happens in the `terraform/aws-full/common.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/common.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/common.tf))
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant part of the output is the `template_file` datasource. It is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Contents of the template are in the `rexray.tpl` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/rexray.tpl))
    file we explored earlier. The variables from the template are defined in the vars
    section. The value of the last variable `aws_security_group` will be decided at
    runtime once the `aws_security_group` called docker is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last piece of the puzzle is in the `terraform/aws-full/swarm.tf` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf))
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Both `swarm-manager` and `swarm-worker` AWS instances have two extra lines
    in the `remote-exec provisioners`. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Commands are inside an `if` statement. That allows us to decide at runtime whether
    REX-Ray should be configured and started or not. Normally, you would not need
    the if statement. You'll either choose to use REX-Ray, or not. However, at the
    beginning of this chapter we needed a cluster with REX-Ray and I did not want
    to maintain two almost identical copies of the configuration (one with, and the
    other without REX-Ray).
  prefs: []
  type: TYPE_NORMAL
- en: The important part is inside the if statement. The first line puts the content
    of the template into the `/etc/rexray/config.yml` ([https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf](https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws-full/swarm.tf))
    file. The second starts the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that it is evident how we defined REX-Ray inside Terraform configs, it
    is time to create a cluster with it automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The first node that initializes the Swarm cluster was created and we can proceed
    by adding two more manager nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We retrieved the token and the IP of the first manager and used that info to
    create the rest of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s enter one of the servers and confirm that REX-Ray is installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `rexray version` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Both REX-Ray and libStorage are installed. Finally, before we check whether
    it is working, let's have a quick look at the config.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: I obscured my AWS account details for obvious reasons. Nevertheless, the config
    looks *OK* and we can give REX-Ray a spin.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll deploy the `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    stack and create the `jenkins` service in the same way we did while we were experimenting
    with REX-Ray installed manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'It should take only a few moments until the script is finished. Now we can
    check the Docker volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the `jenkins` service created the `rexray` volume with the same
    name. We should wait until Jenkins is running and open it in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '**A note to Windows users**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Git Bash might not be able to use the open command. If that’s the case, execute
     `terraform output swarm_manager_1_public_ip` to find out the IP of the manager
    and open the URL directly in your browser of choice. For example, the command
    above should be replaced with the command that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`terraform output swarm_manager_1_public_ip`'
  prefs: []
  type: TYPE_NORMAL
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4/jenkins` in
    your browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing left is to recuperate the initial administrative password and
    use it to setup Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: I'll leave the rest to you. Finish the setup, destroy the container, wait until
    Swarm reschedules it, confirm that the state is preserved, and so on and so forth.
    Spend some time playing with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unless you developed an emotional attachment with Jenkins and REX-Ray, please
    remove the service and the volume. We won''t need it anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Prometheus, ElasticSearch, and Mongo are only a few examples of services that
    store the state. Should we add REX-Ray mounts for all of them? Not necessarily.
    Some stateful services already have a mechanism to preserve their state. Before
    we start attaching REX-Ray mount like there is no tomorrow, we should first check
    whether a service already has a data replication mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting stateful services without replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jenkins is a good example of a stateful service that forces us to preserve its
    state. Moreover, it is incapable of sharing or synchronizing state between multiple
    instances. As a result, it cannot scale. There cannot be two Jenkins masters with
    the same or replicated state. Sure, you can create as many masters as you want
    but each will be an entirely separate service without any relation to other instances.
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious negative side-effect of Jenkins inability to scale horizontally
    is performance. If a master is under heavy load, we cannot create a new instance
    and thus reduce the burden from the original.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are only three types of services that can be scaled. They need to be
    stateless, stateful and capable of using shared state, or stateful and capable
    of synchronizing state. Jenkins is none of those and, therefore, it cannot be
    scaled horizontally. The only thing we can do to increase Jenkins capacity is
    to add more resources (for example: CPU and memory). Such an action would improve
    its performance but would not provide high-availability. When Jenkins fails, Swarm
    will re-schedule it. Still, there is a period between a failure and a new replica
    being fully operational.'
  prefs: []
  type: TYPE_NORMAL
- en: During that time, Jenkins would not be functional. Without the ability to scale,
    there is no high-availability.
  prefs: []
  type: TYPE_NORMAL
- en: A big part of Jenkins workload is performed by its agents, so many organizations
    will not need to deal with the fact that it is not scalable.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this transgression into Jenkins statefulness is to demonstrate
    one of the ways stateful services can be designed. When running stateful services
    that do not have a synchronization mechanism, there is no better option we can
    employ than to mount a volume from an external drive. That does not mean that
    mounting a volume is the only option we can use to deploy stateful services, but
    that is a preferable way to treat those that cannot share or synchronize their
    state across multiple replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore stateful services that do implement state synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting stateful services with synchronization and replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating stateful services, the natural reaction is to think of a way to
    preserve their state. While in many cases that is the correct thing to do, in
    some others it isn't. It depends on service’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we explored at least two stateful services that can synchronize
    their state across all instances. Those are *Docker Flow Proxy* and *MongoDB*.
    In a nutshell, the ability to synchronize state means that when data inside one
    instance changes, it is capable of propagating that change to all other instances.
    The biggest problem with that process is how to guarantee that everyone has the
    same data without sacrificing availability. We'll leave that discussion for some
    other time and place. Instead, let us go through the `docker-flow-proxy` and `mongo`
    services and decide which changes (if any) we need to apply to accomplish high
    availability and performance. We'll use them as examples how to treat stateful
    services capable of data replication and synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Not everyone uses Mongo for storing data, nor everyone thinks that *Docker Flow
    Proxy* is the best choice for routing requests. The chances are that your choice
    of a database and the proxy is different. Even in that case, I strongly suggest
    you read the text that follows since it uses those two services only as examples
    of how you could design your replication and how to set up third-party stateful
    service that already has it incorporated. Most DBs use the same principles for
    replication and synchronization, and you should have no problem taking MongoDB
    examples as a blueprint for creating your database services.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting docker flow proxy state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Docker Flow Proxy* is a stateful service. However, that did not prevent us
    from scaling it. Its architecture is made in a way that, even though it is stateful,
    all instances have the same data. The mechanism to accomplish that has quite a
    few names. I prefer calling it state replication and synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: When one of the instances receives a new instruction that changes its state,
    it should find all the other replicas and propagate the change.
  prefs: []
  type: TYPE_NORMAL
- en: 'The replication flow is usually as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An instance receives a request that changes its state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It finds the addresses of all the other instances of the same service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It re-sends the request to all other instances of the same service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ability to propagate changes is not enough. When a new instance is created,
    a stateful service with data replication needs to be capable of requesting a complete
    state from one of the other instances. The first action it needs to perform when
    initialized is to reach the same state as other replicas. That can be accomplished
    through a pull mechanism. While propagation of a change of state of one instance
    often entails a push to all other instances, initialization of a new instance
    is often followed by a data pull.
  prefs: []
  type: TYPE_NORMAL
- en: 'The synchronization flow is often as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A new instance of a service is created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It finds the address of one of the other instances of the same service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It pulls data from the other instance of the same service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You already saw *Docker Flow Proxy* in action quite a few times. We scaled it,
    and we simulated a failure which resulted in re-scheduling. In both cases, all
    replicas always had the same state or, to be more precise, the same configuration.
    You saw it before, so there's no need to go into another round of a practical
    demonstration of proxy's capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how replication and synchronization works does not mean that we
    should write our services as stateful and employ those mechanisms ourselves. Quite
    the contrary. When appropriate, design your services to be stateless and store
    their state in a database. Otherwise, you might quickly run into problems and
    realize that you'll have to reinvent the wheel. For example, you might be faced
    with consensus problems that are already solved in protocols like Raft and Paxos.
    You might need to implement a variation of a Gossip protocol. And so on, and so
    forth. Concentrate on what brings value to your project and use proven solutions
    for everything else.
  prefs: []
  type: TYPE_NORMAL
- en: Recommending the usage of an external database instead of storing the state
    inside our services might sound conflicting knowing that *Docker Flow Proxy* did
    the opposite. It is a stateful application without any external data store (at
    least when running in Swarm mode). The reason is simple. The proxy was not written
    from scratch. It uses HAProxy in the background which, in turn, does not have
    the ability to store its configs (state) externally. If I were to write a proxy
    from scratch, it would save its state externally. I might do that one day. Until
    then, HAProxy is stateful and so is *Docker Flow Proxy*. From a user's perspective,
    that should not be an issue since it employs data replication and synchronization
    between all instances. The problem is for developers working on the project.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at another example of a stateful service with data replication.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting MongoDB state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used `go-demo` service throughout the book. It helped us understand better
    how Swarm works. Among other things, we scaled the service quite a few times.
    That was easy to do since it is stateless. We can create as many replicas as we
    want without having to worry about data. It is stored somewhere else.
  prefs: []
  type: TYPE_NORMAL
- en: The `go-demo` service externalizes its state to MongoDB. If you paid attention,
    we never scaled the database. The reason is simple. MongoDB cannot be scaled with
    a simple `docker service scale` command.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike *Docker Flow Proxy* that was designed from the ground up to leverage
    Swarm's networking to find other instances before replicating data, MongoDB is
    network agnostic. It cannot auto-discover its replicas. To make things more complicated,
    only one instance can be primary meaning that only one instance can receive write
    requests. All that means that we cannot scale Mongo using Swarm. We need a different
    approach. Let's try to set up three MongoDBs with data replication by creating
    a replica set. We'll start with a manual process that will provide us with an
    understanding of the problems we might face and solutions we might employ. Later
    on, once we reach a satisfactory outcome, we'll try to automate the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by entering one of the manager nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'All members of a Mongo replica set need to be able to communicate with each
    other, so we''ll create the same old `go-demo` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: If we were to create a single service with three replicas, Swarm would create
    a single network endpoint for the service and load balance requests among all
    instances. The problem with that approach is in MongoDB configuration. It needs
    a fixed address of every DB that will belong to the replica set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of creating three replicas of a service, we''ll create three services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The command we executed created services `go-demo-db-rs1`, `go-demo-db-rs2`,
    and `go-demo-db-rs3`. They all belong to the `go-demo` network so that they can
    communicate with each other freely. The command specified for all services `mongod
    --replSet "rs0"`, making them all belong to the same Mongo replica set called
    `rs0.` Please don't confuse Swarm replicas with Mongo replica sets. While they
    have a similar objective, the logic behind them is quite different.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should wait until all the services are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant part of the output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we should configure Mongo''s replica set. We''ll do that by creating one
    more `mongo` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We made the service global so that we ensure it will run on the same node we're
    in. That makes the process easier than trying to figure out the IP of a node it
    runs in. It belongs to the same `go-demo` network so that it can access the other
    DB services.
  prefs: []
  type: TYPE_NORMAL
- en: We do not want to run Mongo server inside this service. The purpose of `go-demo-db-util`
    is to give us a Mongo client that we can use to connect to other DBs and configure
    them. Therefore, we replaced the default command `mongod` with a very long sleep.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enter into one of the containers of the `go-demo-db-util` service, we need
    to find its ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the ID of the `go-demo-db-util` replica running on the same
    server, we can enter inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to execute a command that will initiate Mongo''s replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: We used the local `mongo` client to issue the command on the server running
    inside `go-demo-db-rs1`. It initiated the replica set with the ID `rs0` and specified
    that the three services we created previously should be its members. Thanks to
    Docker Swarm networking, we do not need to know the IPs. It was enough to specify
    the names of the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'We should not trust the acknowledgment alone. Let''s take a look at the config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We issued another command to the remote server running in `go-demo-db-rs1`.
    It retrieved the replica set configuration. Part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the replica set has three members (two were removed for brevity).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s send one more command to the remote Mongo running in `go-demo-db-rs1`.
    This time, we''ll check the status of the replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Info about two replicas is removed for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all the Mongo replicas are running. The `go-demo-db-rs1` service
    is acting as the primary, while the other two are secondary nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Mongo replica set means that data will be replicated to all its
    members. One is always a primary, while the rest are secondary servers. With the
    current configuration, we can read and write data only to the primary server.
    Replica set can be configured to allow read access to all the servers. Writing
    is always restricted to the primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us generate some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: We entered into the remote Mongo running on `go-demo-db-rs1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the prompt, we are inside the primary database server.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create a few records in the database test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: The previous command retrieved all the records from the database test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the replica set configured and a few sample records, we can
    simulate failure of one of the servers and observe the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: We exited MongoDB and the `go-demo-db-util` service replica. Then we found the
    IP of `go-demo-db-rs1` (primary member of the Mongo replica set) and listed all
    the containers running on the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows (IDs and STATUS columns are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can find the ID of the `go-demo-db-rs1` service replica and simulate
    failure by removing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the `go-demo-db-rs1` tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Swarm discovered that one of the replicas failed and re-scheduled it. A new
    instance will be running a few moments later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `service ps` command is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll enter the `go-demo-db-util` service replica one more time and output
    the status of the Mongo replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `go-demo-db-rs2` become the primary Mongo replica. A simplified
    flow of what happened is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mongo replica `go-demo-db-rs1` failed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining members noticed its absence and promoted `go-demo-db-rs2` to the
    PRIMARY status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the meantime, Swarm rescheduled the failed service replica
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primary Mongo replica noticed that the `go-demo-db-rs1` server came back
    online and joined the Mongo replica set as secondary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The newly created `go-demo-db-rs1` synchronized its data from one of the other
    members of the Mongo replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the key elements for all this to work is Docker networking. When the
    rescheduled service replica came back online, it maintained the same address `go-demo-db-rs1`,
    and we did not need to change the configuration of the Mongo replica set.
  prefs: []
  type: TYPE_NORMAL
- en: If we used VMs and, in the case of AWS, *Auto Scaling Groups* to host Mongo,
    when a node fails, a new one would be created in its place. However, the new node
    would receive a new IP and would not be able to join the Mongo replica set without
    modifications to the configuration. The are ways we could accomplish the same
    in AWS without containers, but none would be so simple and elegant as with Docker
    Swarm and networking.
  prefs: []
  type: TYPE_NORMAL
- en: What happened to the sample data we created? Remember, we wrote data to the
    primary Mongo replica `go-demo-db-rs1` and, later on, removed it. We did not use
    REX-Ray or any other solution to persist data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s enter the new primary Mongo replica:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: In your cluster, the new primary might be `go-demo-db-rs3`. If that's the case,
    please change the above command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll specify that we want to use the test database and retrieve all
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Even though we did not setup up data persistence, all data is there.
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of Mongo replica sets is to provide fault tolerance. If a DB
    fails, other members will take over. Any change to data (state) is replicated
    among all members of a replica set.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean that we do not need to preserve the state to an external drive?
    That depends on the use case. If data we are operating with is massive, we might
    employ some form of disk persistence to speed up the synchronization process.
    In any other circumstances, using volumes is a waste since most databases are
    designed to provide data replication and synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: The current solution worked well, and we should seek a way to set it up in a
    more automated (and simpler) way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll exit the MongoDB and the `go-demo-db-util` service replica, remove all
    the DB services, and start over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Initializing MongoDB replica set through a swarm service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try to define a better and easier way to set up a MongoDB replica set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by creating three `mongo` services. Later on, each will become
    a member of a Mongo replica set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The only difference, when compared with the previous command we used to create
    `mongo` services, is the addition of the environment variable `MEMBERS.` It holds
    service names of all MongoDBs. We'll use that as the argument for the next service.
  prefs: []
  type: TYPE_NORMAL
- en: Since the official `mongo` image does not have a mechanism to configure Mongo
    replica sets, we'll use a custom one. Its purpose will be only to configure Mongo
    replica sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of the image is in the `conf/Dockerfile.mongo` ([https://github.com/vfarcic/cloud-provisioning/blob/master/conf/Dockerfile.mongo](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/Dockerfile.mongo))
    file. Its content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '`Dockerfile.mongo` extends the official `mongo` image, adds a custom `init-mongo-rs.sh`
    script, gives it execute permissions, and sets it as the entry point.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ENTRYPOINT` defines the executable that will run whenever a container is run.
    Any command arguments we specify will be appended to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `conf/init-mongo-rs.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/conf/init-mongo-rs.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/init-mongo-rs.sh))
    script is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: The first section loops over all `DB` addresses (defined as script arguments)
    and checks whether they are available. If they're not, it waits for three seconds
    before repeating the loop.
  prefs: []
  type: TYPE_NORMAL
- en: The second section formats a JSON string that defines the list of all members
    (id and host). Finally, we initiate the replica set, wait for three seconds, and
    output its status.
  prefs: []
  type: TYPE_NORMAL
- en: 'This script is a slightly more elaborate version of the commands we executed
    previously when we set up the replica set manually. Instead of hard-coding values
    (for example: `service names`), it is written in a way that it can be reused for
    multiple Mongo replica sets with varying number of members.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All that''s left is to run the container as a Swarm service. I already built
    the image as `vfarcic/mongo-devops21` and pushed it to Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: When the script is finished, the container will stop. Typically, Swarm would
    interpret a stopped container as a failure and re-schedule it. That's not the
    behavior we need. We want this service to perform some tasks (configure replica
    set) and stop when finished. We accomplished that with the `--restart-condition
    none` argument. Otherwise, Swarm would enter into an endless loop of continuously
    re-scheduling a service replica that keeps failing a few moments later.
  prefs: []
  type: TYPE_NORMAL
- en: The command of the service is `$MEMBERS.` When appended to the `ENTRYPOINT`,
    the full command was `init-mongo-rs.sh` `go-demo-db-rs1 go-demo-db-rs2 go-demo-db-rs3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s confirm that all services (except `go-demo-db-init`) are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: The only service that is not running is `go-demo-db-init`. By this time, it
    finished executing and, since we used the `--restart-condition none` argument,
    Swarm did not re-schedule it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already developed a level of trust, and you probably believe me that the
    `go-demo-db-init` did its job correctly. Nevertheless, it doesn''t hurt to double-check
    it. Since the last command of the script output the replica set status, we can
    check its logs to see whether everything is configured correctly. That means we''ll
    need to go one more time into trouble of finding the IP and the ID of the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant parts of the output of the `logs` command are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Mongo replica set is indeed configured with all three members. We have a working
    group of fault tolerant set of MongoDBs that provide high availability. We can
    use them with our `go-demo` (or any other) service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: There is only one difference between this command and those we used in the previous
    chapters. If we continued using a single address of the primary MongoDB, we would
    not have high availability. When that DB fails, the service would not be able
    to serve requests. Even though Swarm would re-schedule it, the address of the
    primary would become different since the replica set would elect a new one.
  prefs: []
  type: TYPE_NORMAL
- en: This time we specified all three MongoDBs as the value of the environment variable
    `DB`. The code of the service will pass that string to the MongoDB driver. In
    turn, the driver will use those addresses to deduce which DB is primary and use
    it to send requests. All Mongo drivers have the same mechanism to specify members
    of a replica set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s confirm that all three replicas of the `go-demo` service are
    indeed running. Remember, the service is coded in a way that it would fail if
    the connection to the database could not be established. If all service replicas
    are running, it is the proof that we set up everything correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs and ERROR columns are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all stateful services should be treated in the same way. Some might need
    an external drive mounted, while others already have some kind of a replication
    and synchronization incorporated. In some cases, you might want to combine both
    mounting and replication, while in others replication itself is enough.
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that there are many other combinations we did not explore.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing is to understand how a service works and how it was designed
    to persist its state. In many cases, the logic of the solution is the same no
    matter whether we use containers or not. Containers often do not make things different,
    only easier.
  prefs: []
  type: TYPE_NORMAL
- en: With the right approach, there is no reason why stateful services would not
    be cloud-friendly, fault tolerant, with high availability, scalable, and so on.
    The major question is whether you want to manage them yourself or you'd prefer
    leaving it to your cloud computing provider (if you use one). The important thing
    is that you got a glimpse how to manage stateful services yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s destroy the cluster before we move on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
