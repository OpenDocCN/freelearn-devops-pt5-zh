- en: Collecting Metrics and Monitoring the Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集指标并监控集群
- en: Let us change our traditional attitude to the construction of programs. Instead
    of imagining that our main task is to instruct a computer what to do, let us concentrate
    rather on explaining to human beings what we want a computer to do.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变传统的程序构建思维。与其想象我们的主要任务是指示计算机做什么，不如集中精力向人类解释我们希望计算机做什么。
- en: –Donald Knuth
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ——唐纳德·克努斯
- en: We managed to add centralized logging to our cluster. Logs from any container
    running inside any of the nodes are shipped to a central location. They are stored
    in Elasticsearch and available through Kibana. However, the fact that we have
    easy access to all the logs does not mean that we have all the information we
    would need to debug a problem or prevent it from happening in the first place.
    We need to complement our logs with the rest of the information about the system.
    We need much more than what logs alone can provide.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地将集中式日志记录添加到集群中。来自任何节点中运行的任何容器的日志都会被发送到一个中央位置。它们存储在 Elasticsearch 中，并可以通过
    Kibana 访问。然而，虽然我们可以轻松访问所有日志，但这并不意味着我们拥有调试问题或防止问题发生所需的所有信息。我们需要通过系统的其他信息来补充我们的日志。我们需要的远不止日志所能提供的内容。
- en: The requirements of a cluster monitoring system
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群监控系统的要求
- en: With everything we've done until now, not to mention the tasks we'll do throughout
    the rest of the book, we are simultaneously decreasing and increasing the complexity
    of our system. Scaling a service is easier and less complex with Docker Swarm
    than it would be with containers alone. The fact is that Docker already simplified
    a lot the process we had before. Add to that the new networking with service discovery
    baked in, and the result is almost too simple to be true. On the other hand, there
    is complexity hidden below the surface. One of the ways such complexity manifests
    itself can be easily observed if we try to combine dynamic tools we have used
    so far, with those created in (and for) a different era.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止我们所做的一切，更不用说接下来在全书中要做的任务，我们正在同时增加和减少系统的复杂性。使用 Docker Swarm 比仅使用容器来扩展服务更简单且不那么复杂。事实上，Docker
    已经简化了我们之前的很多过程。再加上新的网络功能和内置的服务发现，结果几乎简单到难以置信。但另一方面，复杂性也隐藏在表面之下。如果我们尝试将到目前为止使用的动态工具与为其他时代设计（并为其设计）的工具结合起来，这种复杂性就会显现出来。
- en: Take *Nagios* ([https://www.nagios.org/](https://www.nagios.org/)) as an example.
    I won't say that we could not use it to monitor our system (we certainly can).
    What I will state is that it would clash with the new system architecture we've
    designed so far. Our system has gotten much more complex than it was. The number
    of replicas is fluctuating. While today we have four instances of a service, tomorrow
    morning there could be six, only to fall to three in the afternoon. They are distributed
    across multiple nodes of the cluster, and being moved around. Servers are being
    created and destroyed. Our cluster and everything inside it is truly dynamic and
    elastic.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以 *Nagios* ([https://www.nagios.org/](https://www.nagios.org/)) 为例。我并不是说我们不能使用它来监控我们的系统（我们当然可以）。我要说的是，它会与我们迄今为止设计的新系统架构发生冲突。我们的系统变得比以前更复杂了。副本的数量在波动。今天我们有四个实例的服务，但明天早上可能会有六个，下午可能会降到三个。它们分布在集群的多个节点上，并且在不断移动。服务器正在创建和销毁。我们的集群及其内部的一切都是真正动态和弹性的。
- en: The dynamic nature of the system we are building would not fit into Nagios,
    which expects services and servers to be relatively static. It expects us to define
    things in advance. The problem with such an approach is that we do not have the
    information in advance. Swarm does. Even if we get the information we need, it
    will change soon.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的系统的动态特性不适合 Nagios，因为它期望服务和服务器相对静态。它要求我们提前定义事物。采用这种方法的问题在于我们并没有提前获得信息，而
    Swarm 则做到了。即便我们获取了所需的信息，它很快就会发生变化。
- en: The system we're building is highly dynamic, and the tools we should use to
    monitor such a system need to be able to cope with this dynamism.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的系统高度动态，监控这样的系统所使用的工具需要能够应对这种动态变化。
- en: It's more than that. Most of the "traditional" tools tend to treat the whole
    system as a black box. That, on the one hand, has a certain number of advantages.
    The main one is that it allows us to decouple our services from the rest of the
    system. In many (but not all) cases, white box monitoring means that we need to
    add to our services monitoring libraries and write some code around them so that
    they can expose the internals of our services.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 事情不止于此。大多数“传统”工具往往将整个系统视为黑盒。一方面，这样做有一定的优点。最主要的优点是，它允许我们将服务与系统的其他部分解耦。在许多（但不是所有）情况下，白盒监控意味着我们需要向服务中添加监控库，并围绕它们编写代码，以便它们能够暴露服务的内部信息。
- en: Think twice before choosing to add something to your service that is not strictly
    its job. When we adopt a microservices approach, we should strive towards services
    being functionally limited to their primary goal. If it's a shopping cart, it
    should be an API that will allow us to add and remove items. Adding libraries
    and code that will extend such a service so that it can register itself in a service
    discovery store, or expose its metrics to the monitoring tool, produces too much
    coupling. Once we do that, our future options will be very limited, and making
    a change in the system might require considerable time and effort.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定为服务添加不属于其本职工作的内容时，请三思。当我们采用微服务架构时，我们应该尽量让服务的功能限制在其主要目标上。如果它是一个购物车，它应该是一个
    API，允许我们添加和移除商品。添加库和代码以扩展该服务，使其能够在服务发现存储中注册自己，或者将其度量数据暴露给监控工具，这会产生过多的耦合。一旦我们这样做了，未来的选择将变得非常有限，且系统的变更可能需要相当多的时间和精力。
- en: We already managed to avoid coupling service discovery with our services. The
    `go-demo` service does not have any knowledge of service discovery and yet, our
    system has all the information it needs. There are many other examples where organizations
    fall into a trap and start coupling their services with the system around them.
    In this case, our main preoccupation is whether we can accomplish the same with
    monitoring. Can we avoid coupling creation of metrics with the code we write for
    our services?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功避免了将服务发现与服务本身耦合。`go-demo` 服务并不具备任何服务发现的知识，但我们的系统却拥有所需的所有信息。许多组织在此过程中落入陷阱，开始将他们的服务与周围的系统耦合。在这种情况下，我们主要关心的是，是否能够在监控中做到同样的事情。我们能否避免将度量数据的创建与为服务编写的代码耦合在一起？
- en: Then again, being able to do white box monitoring provides a lot of benefits
    black box does not have. For one, understanding the internals of a service allows
    us to operate with a much finer level of detail. It gives us knowledge that we
    could not obtain if we were to treat the system as a black box.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，能够进行白盒监控比黑盒监控带来更多的好处。首先，了解服务的内部结构使我们能够更细致地操作。这为我们提供了在将系统视为黑盒时无法获得的知识。
- en: In a world of distributed systems designed for high availability and fast response
    time, it is not enough to be limited to health checks and CPU, memory, and disk
    usage monitoring. We already have Swarm that makes sure the services are healthy
    and we could easily make scripts that check essential resource usage. We need
    much more than that. We need white box monitoring that does not introduce unnecessary
    coupling. We need intelligent alerting that will notify us when something is wrong,
    or even automatically fix the problem. Ideally, we would have alerts and automated
    corrections executed before the problems even happen.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个为高可用性和快速响应时间设计的分布式系统中，仅仅依赖健康检查以及 CPU、内存和磁盘使用情况的监控是不够的。我们已经有了 Swarm 来确保服务的健康状态，而且我们可以轻松地编写脚本来检查基本的资源使用情况。但我们需要的远不止这些。我们需要的是白盒监控，它不会引入不必要的耦合。我们需要智能告警，当出现问题时能够及时通知我们，甚至自动修复问题。理想情况下，我们希望能够在问题发生之前就触发告警并执行自动修复。
- en: 'Some of the requirements we''d need from a monitoring system would be as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 监控系统需要满足的一些要求如下：
- en: '*A decentralized way of generating metrics* that will be able to cope with
    the highly dynamic nature of our cluster'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一种去中心化的度量生成方式*，能够应对我们集群的高度动态特性'
- en: '*A multi-dimensional data model* that can be queried across as many dimensions
    as needed'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一种多维数据模型*，可以跨多个维度进行查询'
- en: '*An efficient query language* that will allow us to exploit our monitoring
    data model and create effective alerting and visualization'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一种高效的查询语言*，使我们能够利用监控数据模型，创建有效的告警和可视化'
- en: '*Simplicity* that will allow (almost) anyone to utilize the system without
    extensive training'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简便性*，让（几乎）任何人都能在没有广泛培训的情况下使用该系统。'
- en: In this chapter, we'll continue the work we started in the previous. We'll explore
    ways to export a different set of metrics, a way to collect them, query them,
    and expose them through dashboards.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将继续前一章的工作。我们将探索导出另一组指标的方法，收集它们、查询它们并通过仪表板展示它们的方法。
- en: Before we do all that, we should make some choices. Which tools shall we use
    for our monitoring solution?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做这些之前，我们需要做一些选择。我们应该使用哪些工具来进行监控解决方案？
- en: Choosing the right database to store system metrics
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的数据库来存储系统指标
- en: In *The DevOps 2.0 Toolkit*, I argued against "traditional" monitoring tools
    like *Nagios* ([https://www.nagios.org/](https://www.nagios.org/)) and *Icinga* ([https://www.icinga.org/](https://www.icinga.org/)).
    Instead, we chose to use Elasticsearch for both the logs and the system metrics.
    In the previous chapter, I reiterated the choice for using Elasticsearch as the
    logging solution. Can we extend its usage by storing metrics? Yes, we can. Should
    we do that? Should we use it as a place to store system metrics? Are there better
    solutions?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《DevOps 2.0工具包》*中，我反对使用像*Nagios*（[https://www.nagios.org/](https://www.nagios.org/)）和*Icinga*（[https://www.icinga.org/](https://www.icinga.org/)）这样的“传统”监控工具。相反，我们选择使用Elasticsearch来处理日志和系统指标。在前一章中，我重申了选择Elasticsearch作为日志解决方案的理由。那么，我们可以通过存储指标来扩展它的使用吗？是的，我们可以。我们应该这么做吗？我们应该用它来存储系统指标吗？是否有更好的解决方案？
- en: The biggest problem with Elasticsearch, if used as a database to store system
    metrics, is that it is not a time series type of database. Logs benefit greatly
    from Elasticsearch ability to perform free text search and store data in an unstructured
    way. However, for system metrics, we might take advantage of a different type
    of data storage. We need a time series database.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将Elasticsearch作为存储系统指标的数据库，最大的难题是它并不是一个时间序列类型的数据库。日志从Elasticsearch能够进行自由文本搜索并以非结构化方式存储数据的能力中受益匪浅。然而，对于系统指标，我们可能需要利用另一种类型的数据存储。我们需要一个时间序列数据库。
- en: Time series databases are designed around optimized ways to store and retrieve
    time series data. One of their greatest benefits is that they store information
    in a very compact format allowing them to host a vast amount of data. If you compare
    storage needs for time-based data in other types of databases (Elasticsearch included),
    you'll discover that time series databases are much more efficient. In other words,
    if your data are time-based metrics, use a database designed for such data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据库是围绕优化存储和检索时间序列数据的方式设计的。它们的一个主要优势是将信息存储在非常紧凑的格式中，使得它们能够承载大量数据。如果将基于时间的数据存储需求与其他类型的数据库（包括Elasticsearch）进行比较，你会发现时间序列数据库更加高效。换句话说，如果你的数据是基于时间的指标，使用专门为此类数据设计的数据库。
- en: The biggest problem with most (if not all) time series databases is distributed
    storage. Running them with replication is not possible, or a challenge at best.
    To put it bluntly, such databases are designed to run a single instance. Luckily
    we often do not need to store long term data in such databases and can clean them
    up periodically. If long term storage is a must, the solution would be to export
    aggregated data into some other type of database like Elasticsearch which, by
    the way, shines when it comes to replication and sharding. However, before you
    go "crazy" and start exporting data, make sure that you truly need to do something
    like that. Time series databases can easily store a vast amount of information
    in a single instance. The chances are that you won't need to scale them for capacity
    reasons. On the other hand, if a database fails, Swarm will reschedule it, and
    you'll lose only a few seconds of information. Such a scenario should not be a
    disaster since we are dealing with aggregated data, not individual transactions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数（如果不是所有的话）时间序列数据库的最大问题是分布式存储。以复制方式运行它们几乎是不可能的，或者说，至少是一个挑战。直白地说，这种数据库是为了运行单个实例而设计的。幸运的是，我们通常不需要在这些数据库中存储长期数据，可以定期清理它们。如果必须进行长期存储，解决方案是将汇总数据导出到其他类型的数据库中，如Elasticsearch，而Elasticsearch在复制和分片方面表现出色。然而，在你“疯狂”地开始导出数据之前，确保你真的需要这么做。时间序列数据库可以轻松在单个实例中存储大量信息。很可能你不会因为容量问题而需要扩展它们。另一方面，如果数据库出现故障，Swarm会重新调度它，你只会丢失几秒钟的信息。这种情况不应成为灾难，因为我们处理的是汇总数据，而不是单个事务。
- en: One of the most prominent time series databases is *InfluxDB* ([https://www.influxdata.com/](https://www.influxdata.com/)).
    *Prometheus* ([https://prometheus.io/](https://prometheus.io/)) is a commonly
    used alternative. We'll skip the comparison of these two products except to note
    that we'll use the latter. Both are worthy candidates for your monitoring solution
    with Prometheus having a potential advantage we should not ignore. The community
    plan is to expose Docker metrics natively in Prometheus format. At the time of
    this writing, there is no fixed date when that'll happen, but we'll do our best
    to design the system around that plan. If you'd like to monitor the progress yourself,
    please watch *Docker issue 27307* ([https://github.com/docker/docker/issues/27307](https://github.com/docker/docker/issues/27307)).
    We'll use Prometheus in such a way that we'll be able to switch to Docker native
    metrics once they are available.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的时序数据库之一是*InfluxDB* ([https://www.influxdata.com/](https://www.influxdata.com/))。*Prometheus* ([https://prometheus.io/](https://prometheus.io/))是一个常用的替代品。我们将跳过这两者的比较，唯一要提及的是我们将使用后者。两者都是值得考虑的监控解决方案，其中Prometheus具有我们不能忽视的潜在优势。社区计划是将Docker指标以原生Prometheus格式暴露。目前没有确切的日期表明何时会实现这一点，但我们会尽力围绕这个计划设计系统。如果你想亲自跟踪进展，请关注*Docker
    issue 27307* ([https://github.com/docker/docker/issues/27307](https://github.com/docker/docker/issues/27307))。我们将以一种方式使用Prometheus，使得一旦Docker原生指标可用时，我们可以切换过去。
- en: Let's convert words into actions and create the cluster that we'll use throughout
    this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将文字转化为行动，创建我们将在本章中使用的集群。
- en: Creating the cluster
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'This time we''ll create more services than before so we''ll need a bit bigger
    cluster. It''s not that the services will be very demanding but that our VMs have
    only one CPU and 1GB memory each. Such machines are not something to brag about.
    This time, we''ll create a cluster that consists of five machines. Apart from
    increasing the capacity of the cluster, everything else will be the same as before,
    so there''s no good reason to go through the process again. We''ll simply execute
    `scripts/dm-swarm-5.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-5.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-5.sh)):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将创建比之前更多的服务，因此需要一个稍大的集群。并不是因为服务本身要求很高，而是因为我们的虚拟机每台只有一个CPU和1GB内存。这类机器并不值得炫耀。此次，我们将创建一个包含五台机器的集群。除了增加集群的容量外，其他一切将与之前相同，因此没有必要再次经历整个过程。我们只需执行`scripts/dm-swarm-5.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-5.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-5.sh))：
- en: All the commands from this chapter are available in the `09-monitoring.sh` ([https://gist.github.com/vfarcic/271fe5ab7eb6a3307b9f062eadcc3127](https://gist.github.com/vfarcic/271fe5ab7eb6a3307b9f062eadcc3127))
    Gist.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中的所有命令都可以在`09-monitoring.sh` ([https://gist.github.com/vfarcic/271fe5ab7eb6a3307b9f062eadcc3127](https://gist.github.com/vfarcic/271fe5ab7eb6a3307b9f062eadcc3127))
    Gist中找到。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the `docker node ls` command is as follows (IDs are removed for
    brevity):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker node ls`命令的输出如下（为了简洁，已移除ID）：'
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We created a Swarm cluster with five nodes, three of them acting as managers
    and the rest as workers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含五个节点的Swarm集群，其中三个作为管理节点，剩下的作为工作节点。
- en: 'Now we can create the services we used before. Since this is also something
    we practiced quite a few times, we''ll create stacks from Compose files `vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))
    and `vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建之前使用过的服务了。由于这也是我们已经多次练习过的内容，我们将从Compose文件`vfarcic/docker-flow-proxy/docker-compose-stack.yml` ([https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml))和`vfarcic/go-demo/docker-compose-stack.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml))创建堆栈：
- en: '**A note to Windows users**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意事项**'
- en: 'You might experience a problem with volumes not being mapped correctly with
    Docker Compose. If you see an *Invalid volume specification* error, please export
    the environment variable `COMPOSE_CONVERT_WINDOWS_PATHS` set to `0`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到Docker Compose中卷没有正确映射的问题。如果看到*Invalid volume specification*错误，请将环境变量`COMPOSE_CONVERT_WINDOWS_PATHS`设置为`0`并导出：
- en: '`export COMPOSE_CONVERT_WINDOWS_PATHS=0`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`export COMPOSE_CONVERT_WINDOWS_PATHS=0`'
- en: Please make sure that the variable is exported every time you run `docker-compose`
    or `docker stack deploy`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保每次运行 `docker-compose` 或 `docker stack deploy` 时都导出该变量。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After a while, the output of the `docker service ls` command is as follows
    (IDs are removed for brevity):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，`docker service ls` 命令的输出如下（为简洁起见，删除了 ID）：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We used stacks downloaded from GitHub repositories to create all the services
    except util. Right now, our cluster is hosting the demo services `go-demo` and
    `go-demo-db`, the `proxy`, the `swarm-listener`, and the globally scheduled util
    service that we'll use to experiment with monitoring metrics.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从 GitHub 仓库下载的栈来创建除 util 以外的所有服务。目前，我们的集群托管着演示服务 `go-demo` 和 `go-demo-db`、`proxy`、`swarm-listener`，以及我们将用来实验监控度量的全球调度
    util 服务。
- en: We're ready to start generating some metrics.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备开始生成一些度量数据了。
- en: Prometheus metrics
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prometheus 度量
- en: Prometheus stores all data as time series. It is a stream of timestamped values
    that belong to the same metric and the same labels. The labels provide multiple
    dimensions to the metrics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 将所有数据存储为时间序列。它是一串带有时间戳的值，这些值属于相同的度量和相同的标签。标签为度量提供了多个维度。
- en: 'For example, if we''d like to export data based on HTTP requests from the `proxy`,
    we might create a metric called `proxy_http_requests_total`. Such a metric could
    have labels with the `request` method, `status`, and `path`. These three could
    be specified as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想基于 `proxy` 的 HTTP 请求导出数据，我们可以创建一个名为 `proxy_http_requests_total` 的度量。这样的度量可能会包含
    `request` 方法、`status` 和 `path` 的标签。这三者可以如下指定：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we need a value of the metric, which, in our example, would be the
    total number of requests.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个度量值，在我们的示例中，它将是请求的总数。
- en: 'When we combine metric names with the labels and values, the example result
    could be as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将度量名称与标签和值结合时，示例结果可能如下所示：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Through these three metrics, we can see that there were `654` successful `GET`
    requests, `143` successful `PUT` requests, and `13` failed `GET` requests `HTTP
    403`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这三个度量，我们可以看到有 `654` 次成功的 `GET` 请求，`143` 次成功的 `PUT` 请求，以及 `13` 次失败的 `GET` 请求
    `HTTP 403`。
- en: Now that the format is more or less clear, we can discuss different ways to
    generate metrics and feed them to Prometheus.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在格式基本清晰，我们可以讨论生成度量数据并将其提供给 Prometheus 的不同方式。
- en: Prometheus is based on a *pull* mechanism that scrapes metrics from the configured
    targets. There are two ways we can generate Prometheus-friendly data. One is to
    instrument our own services. Prometheus offers client libraries for *Go *([https://github.com/prometheus/client_golang](https://github.com/prometheus/client_golang)),
    *Python *([https://github.com/prometheus/client_python](https://github.com/prometheus/client_python)),
    *Ruby* ([https://github.com/prometheus/client_ruby](https://github.com/prometheus/client_ruby)),
    and *Java* ([https://github.com/prometheus/client_java](https://github.com/prometheus/client_java)).
    On top of those, there are quite a few unofficial libraries available for other
    languages. Exposing metrics of our services is called instrumentation. Instrumenting
    your code is, in a way, similar to logging.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 基于一种 *拉取* 机制，从配置的目标中抓取度量数据。我们可以通过两种方式生成 Prometheus 友好的数据。一种是对我们自己的服务进行监控。Prometheus
    提供了适用于 *Go*（[https://github.com/prometheus/client_golang](https://github.com/prometheus/client_golang)）、*Python*（[https://github.com/prometheus/client_python](https://github.com/prometheus/client_python)）、*Ruby*（[https://github.com/prometheus/client_ruby](https://github.com/prometheus/client_ruby)）和
    *Java*（[https://github.com/prometheus/client_java](https://github.com/prometheus/client_java)）的客户端库。在这些库之上，还有许多非官方库可用于其他语言。暴露我们服务的度量数据被称为监控。对代码进行监控在某种程度上类似于日志记录。
- en: Even though instrumentation is the preferred way of providing data that will
    be stored in Prometheus, I advise against it. That is, unless the same data cannot
    be obtained by different means. The reasons for such a suggestion lie in my preference
    for keeping microservices decoupled from the rest of the system. If we managed
    to keep service discovery outside our services, maybe we can do the same with
    metrics.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监控是提供将存储在 Prometheus 中的数据的首选方式，但我建议避免这样做。也就是说，除非无法通过其他方式获取相同的数据。这样建议的原因在于我倾向于将微服务与系统的其他部分解耦。如果我们能将服务发现保持在我们的服务之外，也许我们可以对度量数据做同样的处理。
- en: When our service cannot be instrumented or, even better, when we do not want
    to instrument it, we can utilize Prometheus exporters. Their purpose is to collect
    already existing metrics and convert them to Prometheus format. As you'll see,
    our system already exposes quite a lot of metrics. Since it would be unrealistic
    to expect all our solutions to provide metrics in Prometheus format, we'll use
    exporters to do the transformation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的服务无法被仪表化，或者更好的是，当我们不想对其进行仪表化时，我们可以使用Prometheus导出器。它们的作用是收集已经存在的度量数据并将其转换为Prometheus格式。正如你将看到的，我们的系统已经暴露了很多度量数据。由于不现实地期望我们的所有解决方案都提供Prometheus格式的度量数据，我们将使用导出器来进行转换。
- en: When scraping (pulling) data is not enough, we can change direction and push
    them. Even though scraping is the preferred way for Prometheus to get metrics,
    there are cases when such an approach is not appropriate. An example would be
    short-lived batch jobs. They might be so short lived that Prometheus might not
    be able to pull the data before the job is finished and destroyed. In such cases,
    the batch job can push data to the *Push Gateway *([https://github.com/prometheus/pushgateway](https://github.com/prometheus/pushgateway))
    from which Prometheus can scrape metrics.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当抓取（拉取）数据不足以满足需求时，我们可以改变方向并推送数据。尽管抓取是Prometheus获取度量数据的首选方式，但有些情况下这种方法并不适用。一个例子是短生命周期的批处理作业。它们可能存活的时间非常短，以至于Prometheus可能在作业结束并被销毁之前无法拉取数据。在这种情况下，批处理作业可以将数据推送到*Push
    Gateway*（[https://github.com/prometheus/pushgateway](https://github.com/prometheus/pushgateway)），然后Prometheus可以从Push
    Gateway抓取度量数据。
- en: For the list of currently supported exporters, please consult the *Exporters
    and Integrations* ([https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/))
    section of the Prometheus documentation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有关当前支持的导出器列表，请参考Prometheus文档中的*Exporters and Integrations*（[https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/)）部分。
- en: Now, after a short introduction to metrics, we're ready to create services that
    will host the exporters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在简要介绍了度量标准之后，我们准备创建将托管导出器的服务。
- en: Exporting system wide metrics
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出系统级别的度量数据
- en: 'We''ll start with the *Node Exporter* ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter))
    service. It''ll export different types of metrics related to our servers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从*Node Exporter*（[https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)）服务开始。它将导出与我们的服务器相关的不同类型的度量数据：
- en: '**A note to Windows users**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意事项**'
- en: 'For mounts used in the next command to work, you have to stop Git Bash from
    altering file system paths. Set this environment variable:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使下一个命令中使用的挂载有效，您需要阻止Git Bash更改文件系统路径。请设置以下环境变量：
- en: '`export MSYS_NO_PATHCONV=1`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`export MSYS_NO_PATHCONV=1`'
- en: 'This chapter contains many `docker service create` commands that use mounts.
    Before you execute those commands, please ensure that the environment variable
    `MSYS_NO_PATHCONV` exists and is set to `1`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含许多使用挂载的`docker service create`命令。在执行这些命令之前，请确保环境变量`MSYS_NO_PATHCONV`存在并设置为`1`：
- en: '`echo $MSYS_NO_PATHCONV`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`echo $MSYS_NO_PATHCONV`'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Since we need the `node-exporter` to be available on each server, we specified
    that the service should be global. Normally, we'd attach it to a separate network
    dedicated to monitoring tools (example:monitoring). However, Docker machines running
    locally might have problems with more than two networks. Since we already created
    the `go-demo` and `proxy` networks through `scripts/dm-swarm-services-3.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services-3.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services-3.sh))
    we've reached the safe limit. For that reason, we'll use the existing `proxy`
    network for monitoring services as well. When operating the "real" cluster, you
    should create a separate network for monitoring services.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要`node-exporter`在每台服务器上可用，因此我们指定该服务应为全局服务。通常，我们会将它连接到一个专门用于监控工具的网络（例如：monitoring）。然而，本地运行的Docker机器可能会遇到超过两个网络时的问题。由于我们已经通过`scripts/dm-swarm-services-3.sh`（[https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services-3.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services-3.sh)）创建了`go-demo`和`proxy`网络，因此已经达到了安全限制。因此，我们将使用现有的`proxy`网络来为监控服务提供网络支持。在操作“真实”集群时，您应该为监控服务创建一个单独的网络。
- en: We mounted a few volumes as well.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还挂载了一些卷。
- en: 'The `/proc` directory is very special in that it is also a virtual filesystem.
    It''s sometimes referred to as a process information pseudo-file system. It doesn''t
    contain "real" files but runtime system information (example: system memory, devices
    mounted, hardware configuration, and so on).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`/proc` 目录非常特殊，它也是一个虚拟文件系统。它有时被称为进程信息伪文件系统。它不包含“真实”的文件，而是包含运行时系统信息（例如：系统内存、挂载的设备、硬件配置等）。'
- en: For this reason, it can be regarded as a control and information center for
    the kernel. In fact, quite a lot of system utilities are simply calls to files
    in this directory. For example, `lsmod` is the same as `cat /proc/modules` while
    `lspci` is a synonym for `cat /proc/pci`. By altering files located in that directory,
    you can even `read/change` kernel parameters `sysctl` while the system is running.
    The `node-exporter` service will use it to find all the processes running inside
    our system.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它可以被视为内核的控制和信息中心。实际上，很多系统工具本质上只是对该目录中文件的调用。例如，`lsmod` 实际上就是 `cat /proc/modules`，而
    `lspci` 是 `cat /proc/pci` 的同义词。通过更改该目录中的文件，甚至可以在系统运行时 `读取/更改` 内核参数 `sysctl`。`node-exporter`
    服务将使用它来查找系统中运行的所有进程。
- en: Modern Linux distributions include a `/sys` directory as a virtual filesystem
    (`sysfs`, comparable to `/proc`, which is a `procfs`), which stores and allows
    modification of the devices connected to the system, whereas many traditional
    UNIX and Unix-like operating systems use `/sys` as a symbolic link to the kernel
    source tree.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 Linux 发行版包括一个 `/sys` 目录，作为一个虚拟文件系统（`sysfs`，类似于 `/proc`，后者是 `procfs`），它存储并允许修改连接到系统的设备，而许多传统的
    UNIX 和类 UNIX 操作系统则将 `/sys` 用作指向内核源代码树的符号链接。
- en: The `sys` directory is a virtual file system provided by Linux. It provides
    a set of virtual files by exporting information about various kernel subsystems,
    hardware devices and associated device drivers from the kernel's device model
    to user space. By exposing it as a volume, the service will be able to gather
    information about the kernel.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`sys` 目录是 Linux 提供的虚拟文件系统。它通过从内核的设备模型将关于各种内核子系统、硬件设备及其相关设备驱动程序的信息导出到用户空间，提供一组虚拟文件。通过将其暴露为卷，服务将能够收集关于内核的信息。'
- en: Finally, we defined the image `prom/node-exporter` and passed a few command
    arguments. We specified the target volumes for `/proc` and `/sys` followed with
    the instruction to ignore mount points inside the container.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了镜像 `prom/node-exporter` 并传递了一些命令参数。我们指定了 `/proc` 和 `/sys` 的目标卷，并指示忽略容器内的挂载点。
- en: Please visit the *Node Exporter project* ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter))
    for more information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问 *Node Exporter 项目* ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter))
    获取更多信息。
- en: 'By this time, the service should be running inside the cluster. Let''s confirm
    that:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到这时，服务应该已经在集群内运行。让我们确认一下：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the `service ps` command is as follows (IDs are removed for brevity):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps` 命令的输出如下（为了简洁，已移除 ID）：'
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s have a quick look at the metrics provided by the `node-exporter` service.
    We''ll use the `util` service to retrieve the metrics:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看 `node-exporter` 服务提供的指标。我们将使用 `util` 服务来检索这些指标：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A sample of the `curl` output is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 输出示例如下：'
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the metrics are in the Prometheus-friendly format. Please explore
    the *Node Exporter collectors* ([https://github.com/prometheus/node_exporter#collectors](https://github.com/prometheus/node_exporter#collectors))
    for more information about the meaning of each metric. For now, you should know
    that most of the node information you would need is available and will be, later
    on, scraped by Prometheus.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，指标以 Prometheus 友好的格式展示。请访问 *Node Exporter 收集器* ([https://github.com/prometheus/node_exporter#collectors](https://github.com/prometheus/node_exporter#collectors))
    了解更多关于每个指标的含义。目前，你应该知道大多数节点信息已经可以获取，并且稍后会被 Prometheus 抓取。
- en: Since we sent a request through Docker networking, we got a load-balanced response
    and cannot be sure which node produced the output. When we reach the Prometheus
    configuration, we'll have to be more specific and skip networks load balancing.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通过 Docker 网络发送了请求，获得了一个负载均衡的响应，因此无法确定是哪一个节点产生了输出。当我们配置 Prometheus 时，我们需要更具体一点，并跳过网络负载均衡。
- en: Now that we have the information about servers, we should add metrics specific
    to containers. We'll use `cAdvisor` also known as **container Advisor**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了关于服务器的信息，我们应该添加特定于容器的指标。我们将使用 `cAdvisor`，也叫做 **容器顾问**。
- en: The `cAdvisor` provides container users an understanding of the resource usage
    and performance characteristics of their running containers. It is a running daemon
    that collects, aggregates, processes, and exports information about running containers.
    Specifically, for each container it keeps resource isolation parameters, historical
    resource usage, histograms of complete historical resource usage and network statistics.
    This data is exported container and machine-wide. It has native support for Docker
    containers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`cAdvisor` 提供了容器用户对其运行容器的资源使用和性能特征的了解。它是一个运行中的守护进程，收集、聚合、处理并导出有关运行容器的信息。具体来说，它为每个容器保持资源隔离参数、历史资源使用情况、完整的历史资源使用直方图和网络统计数据。这些数据按容器和机器范围导出。它原生支持
    Docker 容器。'
- en: 'Let''s create the service:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建服务：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Just as with the `node-exporter`, the `cadvisor` service is global and attached
    to the `proxy` network. It mounts a few directories that allows it to monitor
    Docker stats and events on the host. Since `cAdvisor` comes with a web UI, we
    opened port `8080` that will allow us to open it in a browser.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `node-exporter` 一样，`cadvisor` 服务是全局的，并连接到 `proxy` 网络。它挂载了一些目录，使其能够监控主机上的
    Docker 状态和事件。由于 `cAdvisor` 自带一个 Web UI，我们开放了端口 `8080`，这样我们就可以在浏览器中打开它。
- en: 'Before we proceed, we should confirm that the service is indeed running:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们应该确认服务确实在运行：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the `service ps` is as follows (IDs are removed for brevity):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps` 的输出如下（为了简洁，省略了 ID）：'
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can open the UI:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以打开 UI：
- en: '**A note to Windows users**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**给 Windows 用户的提示**'
- en: 'Git Bash might not be able to use the open command. If that''s the case, execute `docker-machine
    ip <SERVER_NAME>` to find out the IP of the machine and open the URL directly
    in your browser of choice. For example, the command below should be replaced with
    the command that follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 open 命令。如果是这种情况，请执行 `docker-machine ip <SERVER_NAME>` 以查找机器的
    IP，并在你选择的浏览器中直接打开该 URL。例如，下面的命令应该替换为随后的命令：
- en: '`docker-machine ip swarm-1`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine ip swarm-1`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4:8080` in your
    browser.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是 `1.2.3.4`，你应该在浏览器中打开 `http://1.2.3.4:8080`。
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/cadvisor.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cadvisor.png)'
- en: 'Figure 9-1: cAdvisor UI'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1：cAdvisor UI
- en: Feel free to scroll down and explore different graphs and metrics provided by
    `cAdvisor`. If they are not enough, information about running containers can be
    obtained by clicking the Docker Containers link at the top of the screen.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随意向下滚动并探索 `cAdvisor` 提供的各种图表和指标。如果这些信息还不够，可以通过点击屏幕顶部的 Docker 容器链接获取有关运行中容器的更多信息。
- en: Even though it might seem impressive on the first look, the UI is, more or less,
    useless for anything but a single server. Since it is designed as a tool to monitor
    a single node, it does not have much usage inside a Swarm cluster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然乍一看界面可能让人印象深刻，但对于除单个服务器外的任何其他用途，UI 几乎是无用的。因为它是作为一个监控单个节点的工具设计的，所以在 Swarm 集群中并没有太大作用。
- en: For one, the page and all the requests it makes are load-balanced by the ingress
    network. That means not only that we do not know which server returned the UI,
    but requests that return data used by metrics and graphs are load balanced as
    well. In other words, different data from all the servers is mixed, giving us
    a very inaccurate picture. We could skip using the service and run the image with
     `docker run` command (repeated for each server). However, even though that would
    allow us to see a particular server, the solution would still be insufficient
    since we would be forced to go from one server to another. Our goal is different.
    We need to gather and visualize data from the whole cluster, not individual servers.
    Therefore, the UI must go.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，页面及其所有请求都是由入口网络进行负载均衡的。这不仅意味着我们无法知道是哪个服务器返回了 UI，还意味着返回指标和图表所使用的数据请求也进行了负载均衡。换句话说，来自所有服务器的不同数据被混合在一起，给我们一个非常不准确的视图。我们可以跳过使用该服务，直接使用
    `docker run` 命令（对每个服务器重复）。然而，即使那样能让我们看到特定的服务器，解决方案仍然是不充分的，因为我们将被迫从一台服务器切换到另一台。我们的目标不同。我们需要收集并可视化整个集群的数据，而不是单个服务器。因此，UI
    必须被去除。
- en: As a side note, certain types of metrics overlap between the `node-exporter`
    and `cadvisor` services. You might be tempted to choose only one of those. However,
    their focus is different, and the full picture can be accomplished only with the
    combination of the two.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，某些类型的指标在 `node-exporter` 和 `cadvisor` 服务之间有重叠。你可能会倾向于只选择其中一个。然而，它们的关注点不同，只有将两者结合起来，才能得到完整的视图。
- en: 'Since we established that the UI is useless when hosted inside a Swarm cluster,
    there is no good reason to expose port `8080`. Therefore, we should remove it
    from the service. You might be tempted to remove the service and create it again
    without exposing the port. There is no need for such an action. Instead, we can
    eliminate the port by updating the service:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定当 UI 托管在 Swarm 集群中时是无用的，因此没有充分理由暴露端口 `8080`。因此，我们应将其从服务中移除。你可能会想删除该服务并重新创建它，而不暴露端口。其实不需要采取这种操作。相反，我们可以通过更新服务来消除该端口：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: By examining the output of the `service inspect` command, you'll notice that
    the port is not opened (it does not exist).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查 `service inspect` 命令的输出，你会注意到端口没有打开（它不存在）。
- en: 'Now that the `cadvisor` service is running, and we do not generate noise from
    the useless UI, we can take a quick look at the metrics `cAdvisor` exports:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `cadvisor` 服务已运行，并且我们没有从无用的 UI 生成噪声，我们可以快速查看 `cAdvisor` 导出的指标：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A sample of the `curl` output is as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 输出的示例如下：'
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We're making excellent progress. We are exporting server and container metrics.
    We might continue adding metrics indefinitely and extend this chapter to an unbearable
    size. I'll leave the creation of services that will provide additional info as
    an exercise you should perform later on. Right now we'll move onto Prometheus.
    After all, having metrics is not of much use without being able to query and visualize
    them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进展顺利。我们正在导出服务器和容器指标。我们可能会继续无限地添加指标，并将本章扩展到无法承受的大小。我将把创建提供额外信息的服务作为你后续需要完成的练习。现在我们将进入
    Prometheus。毕竟，没有能够查询和可视化指标，它们的存在没有太大意义。
- en: Scraping, querying, and visualizing Prometheus metrics
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取、查询和可视化 Prometheus 指标
- en: Prometheus server is designed to pull the metrics from instrumented services.
    However, since we wanted to avoid unnecessary coupling, we used exporters that
    provide the metrics we need. Those exporters are already running as Swarm services,
    and now we are ready to exploit them through Prometheus.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 服务器旨在从已仪表化的服务中拉取指标。然而，由于我们希望避免不必要的耦合，因此我们使用了提供所需指标的导出器。这些导出器已经作为
    Swarm 服务在运行，现在我们可以通过 Prometheus 来利用它们。
- en: To instantiate the Prometheus service, we should create a configuration file
    with the exporters running in our cluster. Before we do that, we need to retrieve
    the IPs of all the instances of an exporter service. If you recall the [Chapter
    4](014d8ab7-c047-47ff-a5af-ef6325ae9519.xhtml), *Service Discovery inside a Swarm
    Cluster*, we can retrieve all the IPs by appending the tasks. prefix to the service
    name.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要实例化 Prometheus 服务，我们应创建一个配置文件，其中包含在集群中运行的导出器。 在此之前，我们需要获取所有导出器服务实例的 IP 地址。如果你记得[第
    4 章](014d8ab7-c047-47ff-a5af-ef6325ae9519.xhtml)，*Swarm 集群中的服务发现*，我们可以通过在服务名称前添加
    tasks. 前缀来获取所有的 IP 地址。
- en: 'To retrieve the list of all the replicas of the `node-exporter` service, we
    could, for example, drill it from one of the instances of the `util` service:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索 `node-exporter` 服务的所有副本列表，我们可以例如从 `util` 服务的一个实例中提取：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The relevant part of the output is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的相关部分如下：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We retrieved the IPs of all currently running replicas of the service.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已获取所有当前运行的服务副本的 IP 地址。
- en: The list of the IPs themselves is not enough. We need to tell Prometheus that
    it should use them in a dynamic fashion. It should consult tasks.`<SERVICE_NAME>`
    every time it wants to pull new data. Fortunately, Prometheus can be configured
    through `dns_sd_configs` to use an address as service discovery. For more information
    about the available options, please consult the *Configuration *([https://prometheus.io/docs/operating/configuration/](https://prometheus.io/docs/operating/configuration/))
    section of the documentation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 单独列出 IP 地址是不够的。我们需要告诉 Prometheus 它应该动态地使用这些地址。每次想要拉取新数据时，它应该查询 tasks.`<SERVICE_NAME>`。幸运的是，Prometheus
    可以通过 `dns_sd_configs` 配置，使用地址作为服务发现。有关可用选项的更多信息，请参阅文档中的 *配置*（[https://prometheus.io/docs/operating/configuration/](https://prometheus.io/docs/operating/configuration/)）部分。
- en: Equipped with the knowledge of the existence of the `dns_sd_configs` option,
    we can move forward and define a Prometheus configuration. We'll use the one I
    prepared for this chapter. It is located in `conf/prometheus.yml` ([https://github.com/vfarcic/cloud-provisioning/blob/master/conf/prometheus.yml](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/prometheus.yml))
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了`dns_sd_configs`选项的存在后，我们可以继续定义Prometheus配置。我们将使用我为本章准备的配置文件。它位于`conf/prometheus.yml`中（[https://github.com/vfarcic/cloud-provisioning/blob/master/conf/prometheus.yml](https://github.com/vfarcic/cloud-provisioning/blob/master/conf/prometheus.yml)）
- en: 'Let us quickly go through it:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We defined three jobs. The first two `node` and `cadvisor` are using the `dns_sd_configs`
    (DNS service discovery configs) option. Both have the tasks.`<SERVICE_NAME>` defined
    as the name, are of type A (you'll notice the type from the `drill` output), and
    have the internal ports defined. The last one `prometheus` will provide the internal
    metrics.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了三个任务。前两个`node`和`cadvisor`使用了`dns_sd_configs`（DNS服务发现配置）选项。它们都定义了任务`<SERVICE_NAME>`，类型为A（您可以从`drill`的输出中看到类型），并且定义了内部端口。最后一个`prometheus`将提供内部度量。
- en: Please note that we set `scrape_interval` to five seconds. In production, you
    might want more granular data and change it to, for example, one-second interval.
    Beware! The shorter the interval, the higher the cost. The more often we scrape
    metrics, the more resources will be required to do that, as well as to query those
    results, and even store the data. Try to find a balance between data granularity
    and resource usage. Creating the Prometheus service is easy (as is almost any
    other Swarm service).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将`scrape_interval`设置为五秒。在生产环境中，您可能希望获取更精细的数据并将其更改为例如一秒的间隔。小心！间隔越短，成本越高。我们抓取度量的频率越高，所需的资源就越多，包括查询这些结果，甚至存储数据。尽量在数据粒度和资源使用之间找到平衡。创建Prometheus服务很容易（几乎和创建其他任何Swarm服务一样简单）。
- en: 'We''ll start by creating a directory where we''ll persist Prometheus data:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个目录来持久化Prometheus数据：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we can create the service:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建服务了：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We created the `docker/prometheus` directory where we'll persist Prometheus
    state.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了`docker/prometheus`目录，用于持久化Prometheus状态。
- en: The service is quite ordinary. It is attached to the `proxy` network, exposes
    the port `9090`, and mounts the configuration file and the state directory.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务非常普通。它附加到`proxy`网络，暴露端口`9090`，并挂载配置文件和状态目录。
- en: 'The output of the `service ps` command is as follows (IDs and ERROR columns
    are removed for brevity):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps`命令的输出如下（为了简洁，省略了ID和ERROR列）：'
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Please note that it would be pointless to scale this service. Prometheus is
    designed to work as a single instance. In most cases, that's not a problem since
    it can easily store and process a vast amount of data. If it fails, Swarm will
    reschedule it somewhere else and, in that case, we would lose only a few seconds
    of data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，扩展此服务没有意义。Prometheus被设计为单实例工作。在大多数情况下，这不是问题，因为它可以轻松存储和处理大量数据。如果它失败，Swarm将重新调度它到其他地方，届时我们只会丢失几秒钟的数据。
- en: 'Let''s open its UI and explore what can be done with it:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开它的UI，看看可以做什么：
- en: '**A note to Windows users**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意**'
- en: 'Git Bash might not be able to use the open command. If that''s the case, execute `docker-machine
    ip <SERVER_NAME>` to find out the IP of the machine and open the URL directly
    in your browser of choice. For example, the command below should be replaced with
    the command that follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash可能无法使用open命令。如果是这种情况，请执行`docker-machine ip <SERVER_NAME>`以找出机器的IP，并直接在您选择的浏览器中打开该URL。例如，下面的命令应替换为如下命令：
- en: '`docker-machine ip swarm-1`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine ip swarm-1`'
- en: If the output would be `1.2.3.4`, you should open  `http://1.2.3.4:9090` in
    your browser.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是`1.2.3.4`，您应该在浏览器中打开`http://1.2.3.4:9090`。
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first thing we should do is check whether it registered all the exported
    targets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该检查它是否注册了所有导出的目标。
- en: 'Please click the Status button in the top menu and select Targets. You should
    see that five *cadvisor* targets match the five servers that form the cluster.
    Similarly, there are five node targets. Finally, one prometheus target is registered
    as well:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击顶部菜单中的“Status”按钮并选择“Targets”。您应该能看到五个*cadvisor*目标与形成集群的五台服务器匹配。同样，也有五个节点目标。最后，一个prometheus目标也已注册：
- en: '![](img/prometheus-status-targets.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/prometheus-status-targets.png)'
- en: 'Figure 9-2: Targets registered in Prometheus'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-2：在Prometheus中注册的目标
- en: Now that we confirmed that all the targets are indeed registered and that Prometheus
    started scraping metrics they provide, we can explore ways to retrieve data and
    visualize them through `ad-hoc` queries.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确认所有目标都已注册，并且Prometheus已经开始抓取它们提供的指标，我们可以探索通过`ad-hoc`查询获取数据并将其可视化的方法。
- en: Please click the *Graph* button from the top menu, select `node_memory_MemAvailable`
    from the `- insert metric at cursor *-*` list, and click the Execute button.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击顶部菜单中的*图表*按钮，从`- 插入光标处的指标 *-*`列表中选择`node_memory_MemAvailable`，然后点击执行按钮。
- en: You should see a table with the list of metrics and a value associated with
    each. Many prefer a visual representation of the data which can be obtained by
    clicking the Graph tab located above the list. Please click it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一个包含指标列表以及与每个指标相关的数值的表格。许多人更喜欢通过点击列表上方的“图表”选项卡来获取数据的可视化表示。请点击它。
- en: You should see the available memory for each of the five servers. It is displayed
    as evolution over the specified period which can be adjusted with the fields and
    buttons located above the graph. Not much time passed since we created the `prometheus`
    service so you should probably reduce the period to five or fifteen minutes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到五台服务器的可用内存。它以指定时间段内的变化趋势显示，可以通过位于图表上方的字段和按钮进行调整。自从我们创建了`prometheus`服务以来，时间并不长，所以你可能需要将时间段缩短到五分钟或十五分钟。
- en: 'The same result can be accomplished by typing the query (or in this case the
    name of the metric) in the Expression field. Later on, we''ll do a bit more complicated
    queries that cannot be defined by selecting a single metric from the `*-*insert
    metric at cursor *-*` list:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的结果也可以通过在“表达式”字段中输入查询（或在这种情况下输入指标的名称）来实现。稍后，我们将做一些更复杂的查询，这些查询无法通过从`*-*插入光标处的指标
    *-*`列表中选择单个指标来定义：
- en: '![](img/prometheus-memory-graph.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/prometheus-memory-graph.png)'
- en: 'Figure 9-3: Prometheus graph with available memory'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-3：Prometheus图表与可用内存
- en: 'Now might be a good time to discuss one of the main shortcomings of the system
    we set up so far. We do not have the information that would allow us to relate
    data with a particular server easily. Since the list of addresses is retrieved
    through Docker networking which creates a virtual IP for each replica, the addresses
    are not those of the servers. There is no easy way around this (as far as I know)
    so we are left with two options. One would be to run the exporters as "normal"
    containers (example: `docker run`) instead of as services. The advantage of such
    an approach is that we could set network type as `host` and get the IP of the
    server. The problem with such an approach is that we''d need to run exporters
    separately for each server.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能是讨论我们目前设置的系统主要缺点的好时机。我们没有能够轻松将数据与特定服务器关联的信息。由于地址列表是通过Docker网络获取的，而Docker网络为每个副本创建了一个虚拟IP，因此这些地址并不是服务器的真实地址。对此没有简单的解决方法（据我所知），所以我们只有两个选择。一种方法是将导出程序作为“正常”容器运行（例如：`docker
    run`），而不是作为服务运行。这样做的好处是我们可以将网络类型设置为`host`，并获取服务器的IP。这样做的问题是，我们需要为每个服务器单独运行导出程序。
- en: That wouldn't be so bad except for the fact that each time we add a new server
    to the cluster, we'd need to run all the exporters again. To make things more
    complicated, it would also mean that we'd need to change the Prometheus configuration
    as well, or add a separate service registry only for that purpose. The alternative
    is to wait. The inability to retrieve a host IP from a service replica is a known
    limitation. It is recorded in several places, one of them being *issue 25526* ([https:](https://github.com/docker/docker/issues/25526)[//github.com/docker/docker/issues/25526](https://github.com/docker/docker/issues/25526)).
    At the same time, the community has already decided to expose Prometheus metrics
    natively from Docker Engine. That would remove the need for some, if not all,
    of the exporters we created as services. I'm confident that one of those two will
    happen soon. Until then, you'll have to make a decision to ignore the fact that
    IPs are virtual or replace services with containers run separately on each server
    in the cluster. No matter the choice you make, I'll show you, later on, how to
    find the relation between virtual IPs and hosts.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这本来不会太糟糕，除了每次我们向集群添加新服务器时，都需要重新运行所有的导出器。更复杂的是，这也意味着我们需要更改 Prometheus 配置，或者仅为此目的添加一个单独的服务注册表。另一种选择是等待。无法从服务副本中检索主机
    IP 是已知的限制。这个问题已经在多个地方记录过，其中之一是 *issue 25526* ([https:](https://github.com/docker/docker/issues/25526)[//github.com/docker/docker/issues/25526](https://github.com/docker/docker/issues/25526))。同时，社区已经决定从
    Docker 引擎原生地暴露 Prometheus 指标。这将消除我们作为服务创建的一些，甚至是所有导出器的需求。我相信这两者中的一个很快会实现。在那之前，你必须做出决定，要么忽略
    IP 是虚拟的事实，要么将服务替换为在集群中每台服务器上单独运行的容器。无论你做出什么选择，稍后我会向你展示如何找到虚拟 IP 和主机之间的关系。
- en: Let's go back to querying Prometheus metrics.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到查询 Prometheus 指标。
- en: The example with `node_memory_MemAvailable` used only the metric and, as a result,
    we got all its time series.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`node_memory_MemAvailable`的示例只使用了该指标，因此我们得到了它的所有时间序列。'
- en: Let's spice it up a bit and create a graph that will return idle CPU. The query
    would be `node_cpu{mode="idle"}`. Using `mode="idle"` will limit the `node_cpu`
    metric only to data labeled as idle. Try it out and you'll discover that the graph
    should consist of five almost straight lines going upwards. That does not look
    correct.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微增加点趣味，创建一个将返回空闲 CPU 的图表。查询将是`node_cpu{mode="idle"}`。使用`mode="idle"`将`node_cpu`指标限制为仅显示标记为空闲的数据。试试看，你会发现图表应该由五条几乎直线的上升曲线组成。这看起来不太对。
- en: 'Let''s create a bit more accurate picture by introducing the `irate` function.
    It calculates the per-second instant rate of increase of the time series. That
    is based on the last two data points. To use the `irate` function, we also need
    to specify the measurement duration. The modified query is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过引入`irate`函数来创建一个更精确的图像。它计算时间序列的每秒瞬时增长率，基于最后两个数据点。要使用`irate`函数，我们还需要指定测量的持续时间。修改后的查询如下：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Since we are scraping metrics from the `cadvisor` service, we can query different
    container metrics as well. For example, we can see the memory usage of each container.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在从`cadvisor`服务抓取指标，我们也可以查询不同容器的指标。例如，我们可以查看每个容器的内存使用情况。
- en: 'Please execute the query that follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行下面的查询：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Please execute the query and see the result for yourself. You should see the
    idle CPU rate per node measured over 5 minute intervals:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行查询并亲自查看结果。你应该看到每个节点在5分钟间隔内测量的空闲 CPU 使用率：
- en: '![](img/prometheus-cpu-graph.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/prometheus-cpu-graph.png)'
- en: 'Figure 9-4: Prometheus graph with CPU idle rate'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-4：Prometheus 图表展示 CPU 空闲率
- en: If you explore the results through the graph, you'll discover that `cAdvisor`
    uses the most memory (around `800M` on my machine). That does not look correct.
    The service should have a much smaller footprint. If you look at its labels, you'll
    notice that the ID is `/`. That's the cumulative result of the total memory of
    all containers passing through `cAdvisor`. We should exclude it from the results
    with the `!=` operator.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过图表查看结果，你会发现`cAdvisor`使用了最多的内存（在我的机器上大约是`800M`）。这看起来不太对。该服务的内存占用应该要小得多。如果你查看它的标签，你会注意到ID是`/`。这代表的是所有通过`cAdvisor`的容器的总内存使用情况。我们应该用`!=`操作符将其从结果中排除。
- en: 'Please execute the query that follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行下面的查询：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This time, the result makes much more sense. The service that uses the most
    memory is Prometheus itself.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，结果更有意义了。使用最多内存的服务是 Prometheus 本身。
- en: The previous query used label id to filter data. When combined with the `!=`
    operator, it excluded all metrics that have the id set to `/`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的查询使用标签ID来过滤数据。当与`!=`操作符结合使用时，它排除了所有ID设置为`/`的度量。
- en: Even with such a small cluster, the number of containers might be too big for
    a single graph so we might want to see the results limited to a single service.
    That can be accomplished by filtering the data with the `container_label_com_docker_swarm_service_name`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是如此小的集群，容器数量也可能太多，无法在一个图表中显示，因此我们可能希望将结果限制为单个服务。可以通过使用`container_label_com_docker_swarm_service_name`来过滤数据，完成这一操作。
- en: 'Let''s see the memory usage of all `cadvisor` replicas:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下所有`cadvisor`副本的内存使用情况：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: All this looks great but is not very useful as a monitoring system. Prometheus
    is geared more towards `ad-hoc` queries than as a tool we can use to create dashboards
    that would give us a view of the whole system. For that, we need to add one more
    service to the mix.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切看起来不错，但作为监控系统并不十分有用。Prometheus更多的是用于`ad-hoc`查询，而不是我们可以用来创建能够展示整个系统的仪表盘的工具。为此，我们需要在其中再添加一个服务。
- en: Using Grafana to create dashboards
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Grafana创建仪表盘
- en: Prometheus offers a dashboard builder called *PromDash* ([https://github.com/prometheus/promdash](https://github.com/prometheus/promdash)).
    However, it is deprecated for Grafana, so we won't consider it as worthy of running
    inside our cluster.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus提供了一个名为*PromDash*的仪表盘构建工具([https://github.com/prometheus/promdash](https://github.com/prometheus/promdash))。然而，它已经不再推荐使用，并且对于Grafana而言，已经不再值得在我们的集群中运行，因此我们不再考虑它。
- en: Grafana ([http://grafana.org/](http://grafana.org/)) is one of the leading tools
    for querying and visualization of time series metrics. It features interactive
    and editable graphs and supports multiple data sources. Graphite, Elasticsearch,
    InfluxDB, OpenTSDB, KairosDB, and, most importantly, Prometheus are supported
    out of the box. If that's not enough, additional data sources can be added through
    plugins. Grafana is truly a rich UI that has established itself as a market leader.
    Best of all, it's free.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana ([http://grafana.org/](http://grafana.org/)) 是一个领先的时间序列度量查询和可视化工具。它具有交互式和可编辑的图表，并支持多个数据源。Graphite、Elasticsearch、InfluxDB、OpenTSDB、KairosDB，以及最重要的Prometheus都能开箱即用地支持。如果这些还不够，还可以通过插件添加额外的数据源。Grafana确实是一个功能丰富的UI，已经在市场上确立了领导地位。最棒的是，它是免费的。
- en: 'Let''s create a `grafana` service:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个`grafana`服务：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'A few moments later, the status of the replica should be running:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，副本的状态应该显示为正在运行：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of the `service ps` command is as follows (IDs are removed for brevity):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps`命令的输出如下（为简洁起见，已移除ID）：'
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that the service is running, we can open the UI:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经运行，我们可以打开UI：
- en: '**A note to Windows users**'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意事项**'
- en: 'Git Bash might not be able to use the `open` command. If that''s the case,
    execute `docker-machine ip <SERVER_NAME>` to find out the IP of the machine and
    open the URL directly in your browser of choice. For example, the command below
    should be replaced with the command that follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash可能无法使用`open`命令。如果是这种情况，执行`docker-machine ip <SERVER_NAME>`来查找机器的IP地址，并在你选择的浏览器中直接打开URL。例如，下面的命令应该替换为以下命令：
- en: '`docker-machine ip swarm-1`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine ip swarm-1`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4:3000` in your
    browser.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是`1.2.3.4`，你应该在浏览器中打开`http://1.2.3.4:3000`。
- en: '[PRE33]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You will be presented with the login screen. The default username and password
    are admin. Go ahead and log in.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到登录界面。默认的用户名和密码是admin。请继续登录。
- en: The username and password, as well as many other settings, can be adjusted through
    configuration files and environment variables. Since we are running Grafana inside
    a Docker container, environment variables are a better option. For more info,
    please visit the *Configuration* ([http://docs.grafana.org/installation/configuration/](http://docs.grafana.org/installation/configuration/))
    section of the official documentation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 用户名和密码以及其他许多设置可以通过配置文件和环境变量进行调整。由于我们在Docker容器内运行Grafana，环境变量是更好的选择。更多信息，请访问官方文档的*配置*部分([http://docs.grafana.org/installation/configuration/](http://docs.grafana.org/installation/configuration/))。
- en: The first thing we should do is add Prometheus as a data source.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做的第一件事是将Prometheus添加为数据源。
- en: Please click the *Grafana* logo located in the top-left part of the screen,
    select Data Sources, and click the + Add data source button.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击位于屏幕左上角的*Grafana*徽标，选择数据源，并点击+添加数据源按钮。
- en: We'll name it `Prometheus` and choose the same for the Type. Enter `http://prometheus:9090`
    as the `Url` and click the Add button. That's it. From now on, we can visualize
    and query any metric stored in Prometheus.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Let's create the first dashboard.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Please click the *Grafana* logo, select Dashboards, and click + New. In the
    top-left part of the screen, there is a green vertical button. Click it, select
    Add Panel, and choose Graph. You'll see the default graph with test metrics. It's
    not very useful unless you'd like to admire pretty lines going up and down. We'll
    change the Panel data source from default to Prometheus. Enter `irate(node_cpu{mode="idle"}[5m])`
    as Query. A few moments later you should see a graph with CPU usage.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: By default, graphs display six hours of data. In this case, that might be *OK*
    if you are a slow reader and it took you that much time to create the prometheus
    service and read the text that followed. I will assume that you have only half
    an hour worth of data and want to change the graph's timeline.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Please click the Last 6 hours button located in the top-right corner of the
    screen, followed by the Last 30 minutes link. The graph should be similar to *Figure
    9-5:*
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/grafana-cpu-graph.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-5: Grafana graph with CPU rate fetched from Prometheus'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few things you can customize to make a graph fit your needs.
    I'll leave that to you. Go ahead and play with the new toy. Explore different
    options it offers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: If you are lazy as I am, you might want to skip creating all the graphs and
    dashboards you might need and just leverage someone else's effort. Fortunately,
    the Grafana community is very active and has quite a few dashboards made by its
    members.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Please open the *dashboards* ([https://grafana.net/dashboards](https://grafana.net/dashboards))
    section in *grafana.net* ([https://grafana.net](https://grafana.net)). You'll
    see a few filters on the left-hand side as well as the general search field. We
    can, for example, search for     `node exporter`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to explore all the offered node exporter dashboards at some
    later time. For now, we'll select the *Node Exporter Server Metrics* ([https://grafana.net/dashboards/405](https://grafana.net/dashboards/405)).
    Inside the page, you'll see the Download Dashboard button. Use it to download
    the JSON file with dashboard definition.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get back to our `grafana` service:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to Windows users**'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Git Bash might not be able to use the `open` command. If that''s the case,
    execute `docker-machine ip <SERVER_NAME>` to find out the IP of the machine and
    open the URL directly in your browser of choice. For example, the command below
    should be replaced with the command that follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-machine ip swarm-1`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4:3000` in your
    browser.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Open, one more time, the Dashboards option is hidden behind the Grafana logo
    and select Import. Click the Upload .json file button and open the file you just
    downloaded. We'll leave the Name intact and choose Prometheus as datasource. Finally,
    click the Save & Open button to finish.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 再次打开仪表板选项，点击隐藏在Grafana徽标下的选项并选择导入。点击上传 .json 文件按钮，打开你刚刚下载的文件。我们将保持名称不变，并选择Prometheus作为数据源。最后，点击保存并打开按钮完成操作。
- en: The magic happened, and we got quite a few graphs belonging to one of the nodes.
    However, the graphs are mostly empty since the default duration is seven days
    and we have only an hour or so worth of data. Change the time range to, let's
    say, one hour. The graphs should start making sense.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 奇迹发生了，我们得到了属于某个节点的几个图表。然而，由于默认的持续时间是七天，而我们只有大约一个小时的数据，这些图表大部分是空的。将时间范围改为一个小时。图表应该开始变得有意义。
- en: 'Let''s spice it up a bit and add more servers to the mix. Please click the
    `IP/port` of the selected node and choose a few more. You should see metrics from
    each of the nodes:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们增加一些变化，加入更多的服务器。请点击选定节点的`IP/port`，选择更多节点。你应该能够看到每个节点的度量数据：
- en: '![](img/grafana-nodes-dashboard.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/grafana-nodes-dashboard.png)'
- en: 'Figure 9-6: Grafana dashboard with metrics from selected nodes from Prometheus'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-6：Grafana仪表板，显示来自选定节点的Prometheus度量
- en: While this dashboard is useful when we want to compare metrics between the selected
    nodes, I think it is not so useful if we'd like to focus on a single node. In
    that case, the *Node Exporter Server Stats* ([https://grafana.net/dashboards/704](https://grafana.net/dashboards/704))
    dashboard might be a better option. Please follow the same steps to import it
    into the `grafana` service.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个仪表板在我们想比较选定节点之间的度量时非常有用，但我认为如果我们想专注于单一节点，它的用处就不大了。在这种情况下，*Node Exporter
    Server Stats* ([https://grafana.net/dashboards/704](https://grafana.net/dashboards/704))仪表板可能是更好的选择。请按照相同的步骤将其导入到`grafana`服务中。
- en: You can still change the node presented in the dashboard (IP in the top-left
    corner of the screen). However, unlike the other dashboard, this one displays
    only one node at the time.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然可以更改仪表板中显示的节点（屏幕左上角的IP）。然而，与其他仪表板不同，这个仪表板每次只显示一个节点。
- en: Both dashboards are very useful depending on the case. If we need to compare
    multiple nodes then the *Node Exporter Server Metrics* ([https://grafana.net/dashboards/405](https://grafana.net/dashboards/405))
    might be a better option. On the other hand, when we want to concentrate on a
    specific server, the *Node Exporter Server Stats* ([https://grafana.net/dashboards/704](https://grafana.net/dashboards/704))
    dashboard is probably a better option. You should go back and import the rest
    of the *Node Exporter* dashboards and try them as well. You might find them more
    useful than those I suggested.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 根据具体情况，这两个仪表板都非常有用。如果我们需要比较多个节点，那么*Node Exporter Server Metrics* ([https://grafana.net/dashboards/405](https://grafana.net/dashboards/405))可能是更好的选择。另一方面，当我们想集中在一个特定的服务器时，*Node
    Exporter Server Stats* ([https://grafana.net/dashboards/704](https://grafana.net/dashboards/704))仪表板可能是更好的选择。你应该返回并导入其余的*Node
    Exporter*仪表板并尝试它们。你可能会发现它们比我建议的更有用。
- en: 'Sooner or later, you''ll want to create your own dashboards that fit your needs
    better. Even in that case, I still advise you to start by importing one of those
    made by the community and modifying it instead of starting from scratch. That
    is, until you get more familiar with Prometheus and Grafana, refer to the following
    image:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 迟早，你会想创建适合自己需求的仪表板。即使是这样，我仍然建议你从导入一个社区制作的仪表板开始，并对其进行修改，而不是从头开始。也就是说，在你更熟悉Prometheus和Grafana之前，请参考以下图像：
- en: '![](img/grafana-node-dashboard.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/grafana-node-dashboard.png)'
- en: 'Figure 9-7: Grafana dashboard with a single node metrics from Prometheus'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-7：Grafana仪表板，显示来自Prometheus的单一节点度量
- en: The next dashboard we'll create will need logs from Elasticsearch so let's set
    up logging as well.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要创建的仪表板需要来自Elasticsearch的日志，因此我们也需要设置日志记录。
- en: 'We won''t go into details of the logging services since we already explored
    them in the [Chapter 9](3be31bb8-e0b6-4395-ab76-624ba5d30d26.xhtml), *Defining
    Logging Strategy*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讨论日志服务，因为我们在[第9章](3be31bb8-e0b6-4395-ab76-624ba5d30d26.xhtml)中已经探讨过它们，*定义日志策略*：
- en: '[PRE35]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Before we proceed with a `LogStash` service, we should confirm that `elasticsearch`
    is running:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行`LogStash`服务之前，我们应该确认`elasticsearch`正在运行：
- en: '[PRE36]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output of the `service ps` command should be similar to the one that follows
    (IDs & Error Ports column are removed for brevity):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps`命令的输出应该类似于以下内容（为了简洁，已移除ID和错误端口列）：'
- en: '[PRE37]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we can create a `logstash` service:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个 `logstash` 服务：
- en: '[PRE38]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s confirm it''s running before moving onto the last logging service:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续到最后的日志服务之前，让我们确认它正在运行：
- en: '[PRE39]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output of the `service ps` command should be similar to the one that follows
    (IDs  and ERROR PORTS columns are removed for brevity):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps` 命令的输出应该类似于下面所示（为了简洁，ID 和 ERROR PORTS 列已被移除）：'
- en: '[PRE40]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we''ll create a `logspout` service as well:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将创建一个 `logspout` 服务：
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '… and confirm it''s running:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: …并确认它正在运行：
- en: '[PRE42]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output of the `service ps` command should be similar to the one that follows
    (IDs and ERROR PORTS columns are removed for brevity):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps` 命令的输出应该类似于下面所示（为了简洁，ID 和 ERROR PORTS 列已被移除）：'
- en: '[PRE43]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that logging is operational, we should add Elasticsearch as one more Grafana
    data source:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在日志功能已经正常运行，我们应该添加 Elasticsearch 作为另一个 Grafana 数据源：
- en: '**A note to Windows users**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows 用户注意事项**'
- en: 'Git Bash might not be able to use the open command. If that''s the case, execute
                            `docker-machine ip <SERVER_NAME>` to find out the IP of
    the machine and open the URL directly in your browser of choice. For example,
    the command below should be replaced with the command that follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Git Bash 可能无法使用 open 命令。如果是这种情况，请执行 `docker-machine ip <SERVER_NAME>` 来查找机器的
    IP 地址，并在您选择的浏览器中直接打开该 URL。例如，下面的命令应替换为以下命令：
- en: '`docker-machine ip swarm-1`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine ip swarm-1`'
- en: If the output would be `1.2.3.4`, you should open `http://1.2.3.4:3000` in your
    browser.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是 `1.2.3.4`，您应该在浏览器中打开 `http://1.2.3.4:3000`。
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Please click on the Grafana logo, and select Data Sources. A new screen will
    open with the currently defined sources (at the moment only Prometheus). Click
    the + Add data source button.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 请点击 Grafana 徽标，选择数据源。一个新屏幕将打开，显示当前定义的源（目前只有 Prometheus）。点击 + 添加数据源 按钮。
- en: We'll use `Elasticsearch` as both the name and the type. The Url should be [http://e](http://elasticsearch:9200)[lasticsearch:9200](http://elasticsearch:9200)
    and the value of the Index name should be set to `"logstash-*"`. Click the Add
    button when finished.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `Elasticsearch` 作为名称和类型。Url 应设置为 [http://e](http://elasticsearch:9200)[lasticsearch:9200](http://elasticsearch:9200)，索引名称的值应设置为
    `"logstash-*"`。完成后点击添加按钮。
- en: Now we can create or, to be more precise, import our third dashboard. This time,
    we'll import a dashboard that will be primarily focused on Swarm services.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建或更准确地说是导入我们的第三个仪表板。这次，我们将导入一个主要聚焦于 Swarm 服务的仪表板。
- en: Please open the Docker Swarm & Container Overview ([https://grafana.net/dashboards/609](https://grafana.net/dashboards/609))
    dashboard page, download it, and import it into Grafana. In the Import Dashboard
    screen for Grafana , you will be asked to set one *Prometheus* and two Elasticsearch
    data sources. After you click the Save & Open button, you will be presented with
    a dashboard full of metrics related to Docker Swarm and containers in general.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开 Docker Swarm 和容器概览的仪表板页面 ([https://grafana.net/dashboards/609](https://grafana.net/dashboards/609))，下载并导入到
    Grafana 中。在 Grafana 的导入仪表板屏幕中，您将被要求设置一个 *Prometheus* 数据源和两个 Elasticsearch 数据源。点击保存并打开按钮后，您将看到一个包含与
    Docker Swarm 和容器相关的各种指标的仪表板。
- en: You will notice that some of the graphs from the dashboard are empty. That's
    not an error but an indication that our services are not prepared to be monitored.
    Let's update them with some additional information that the dashboard expects.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到仪表板中的一些图表为空。这不是错误，而是表示我们的服务尚未准备好被监控。让我们通过添加一些仪表板期望的信息来更新它们。
- en: Exploring Docker Swarm and container overview dashboard in Grafana
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Grafana 中查看 Docker Swarm 和容器概览仪表板
- en: One of the things missing from the dashboard are host names. If you select the
    Hostnames list, you'll notice that it is empty. The reason behind that lies in
    the `node-exporter` service. Since it is running inside containers, it is oblivious
    of the name of the underlying host.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板缺少的一项内容是主机名。如果您选择 Hostnames 列表，您会发现它是空的。原因在于 `node-exporter` 服务。由于它在容器内运行，因此它无法识别底层主机的名称。
- en: We already commented that IPs from the `node-exporter` are not very valuable
    since they represent addresses of network endpoints. What we truly need are either
    "real" host IPs or host names. Since we cannot get the real IPs from Docker services,
    the alternative is to use host names instead. However, the official `Node Exporter`
    container does not provide that so we'll need to resort to an alternative.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll change our `node-exporter` service with the image created by GitHub
    user `bvis`. The project can be found in the `bvis/docker-node-exporter` ([https://github.com/bvis/docker-node-exporter](https://github.com/bvis/docker-node-exporter))
    GitHub repository. Therefore, we''ll remove the `node-exporter` service and create
    a new one based on the `basi/node-exporter` ([https://hub.docker.com/r/basi/node-exporter/](https://hub.docker.com/r/basi/node-exporter/))
    image:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Apart from using a different image `basi/node-exporter`, we mounted the `/etc/hostname`
    directory from which the container can retrieve the name of the underlying host.
    We also added the environment variable `HOST_HOSTNAME` as well as a few additional
    collectors.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into details of the command since it is similar to the one we used
    previously. The meaning of the additional arguments can be found in the project's
    `README` ([https://github.com/bvis/docker-node-exporter](https://github.com/bvis/docker-node-exporter))
    file.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to note is that the new `node-exporter` service will include
    the `hostname` together with the virtual IP created by Docker networking. We'll
    be able to use that to establish the relation between the two.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Instead of creating the new service, we could have updated the one that was
    running before. I decided against that so that you can see the complete command
    in case you choose to use node metrics in your production cluster.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Please go back to the Grafana dashboard that is already opened in your browser
    and refresh the screen *Ctrl*+*R* or *Cmd*+*R*. You'll notice that some of the
    graphs that were empty are now colorful with metrics coming from the new `node-exporter`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hostnames list holds all the nodes with their IPs on the left side and
    their host names on the right. We can now select any combination of the hosts
    and the CPU Usage by Node, Free Disk by Node, Available Memory by Node, and Disk
    I/O by Node graphs will be updated accordingly, as shown in the following image:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/grafana-swarm-nodes-dashboard.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-8: Docker Swarm Grafana dashboard with node metrics'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Not only have we obtained part of the data required for the dashboard, but we
    also established the relation between virtual IPs and host names. Now you will
    be able to find out the relation between virtual IPs used in other dashboards
    and `hostnames`. In particular, if you monitor Node Exporter dashboards and detect
    a problem that should be fixed, you can go back to the Swarm dashboard and find
    out the host that requires your attention.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The solution with host names is still not the best one but should be a decent
    workaround *until issue 27307* ([https://github.com/docker/docker/issues/27307](https://github.com/docker/docker/issues/27307))
    is fixed. The choice is yours. With the ability to relate virtual IPs with host
    names, I chose to stick with Docker services instead resorting to non-Swarm solutions.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The next thing that needs fixing are service groups.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: If you open the Service Group list, you'll notice that it is empty. The reason
    behind that lies in the way the dashboard is configured. It expects that we distinguish
    services through the container label `com.docker.stack.namespace`. Since we did
    not specify any, the list contains only the All option.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Which groups should we have? The answer to that question varies from one use
    case to another. With time, you'll define the groups that best fit your organization.
    For now, we'll split our services into databases, backend, and infrastructure.
    We'll put `go-demo-db` into the `db group`, `go-demo` into `backend`, and all
    the rest into infra. Even though `elasticsearch` is a database, it is part of
    our infrastructure services, so we'll treat it as such.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: We can add labels to existing services. There is no need to remove them and
    create new ones. Instead, we'll execute `docker service update` commands to add
    `com.docker.stack.namespace` labels by leveraging the `--container-label-add`
    argument.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The first service we''ll put into a group is `go-demo_db`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s confirm that the label was indeed added:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `--format` argument allowed us to avoid lengthy output and display only
    what interests us.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `service inspect` command is as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As you can see, the `com.docker.stack.namespace` label was added and holds the
    value `db`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'We should do the same with the `go-demo` service and put it to the `backend`
    group:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The last group is `infra`. Since quite a few services should belong to it,
    we''ll update them all with a single command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We iterated over the names of all the services and executed the `service update`
    command for each.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the `service update` command reschedules replicas. That means
    that the containers will be stopped and run again with new parameters. It might
    take a while until all services are fully operational. Please list the services
    with `docker service ls` and confirm that they are all running before proceeding.
    Once all the replicas are up, we should go back to the Grafana dashboard and refresh
    the screen (*Ctrl*+*R* or *cmd*+*R*).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: This time, when you open the Service Group list, you'll notice that the three
    groups we created are now available. Go ahead, and select a group or two. You'll
    see that the graphs related to services are changing accordingly.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: We can also filter the result by `Service Name` and limit the metrics displayed
    in some of the graphs to a selected set of services.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: If you scroll down towards the middle of the dashboard, you'll notice that network
    graphs related to the `proxy` have too many services while those that exclude
    `proxy` are empty. We can correct that through the Proxy selector. It allows us
    to define which services should be treated as a `proxy`. Please open the list
    and select `proxy`.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/grafana-swarm-cpu-graph.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-10: Grafana dashboard with network traffic graphs'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The two network graphs related to a `proxy` are now limited to the `proxy` service
    or, to be more concrete, the service we identified as such. The bottom now contains
    metrics from all other services. Separating monitoring of the external and internal
    traffic is useful. Through the proxy graphs, you can see the traffic coming from
    and going to external sources while the other two are reserved for internal communication
    between services.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s generate a bit of traffic and confirm that the change is reflected in
    proxy graphs. We''ll generate a hundred requests:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If you go back to `proxy` network graphs, you should see a spike in traffic.
    Please note that the dashboard refreshes data every minute. If the spike is still
    not visible, you might need to wait, click the Refresh button located in the top-right
    corner of the screen, or change the refresh frequency, refer the following image:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/grafana-swarm-network-dashboard.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-10: Grafana dashboard with network traffic graphs'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: We'll move on towards the next option in the dashboard menu and click the Errors
    checkbox. This checkbox is connected to Elasticsearch. Since there are no logged
    errors, the graphs stayed the same.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Let's generate a few errors and see how they visualize in the dashboard.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'The `go-demo` service has an API that will allow us to create random errors.
    On average, one out of ten requests will produce an error. We''ll need them to
    demonstrate one of the integrations between Prometheus metrics and data from Elasticsearch:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'A sample of the output should be as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If you go back to the dashboard, you''ll notice red lines representing the
    point of time when the errors occurred. When such a thing happens, you can investigate
    system metrics and try to deduce whether the errors were caused by some hardware
    failure, saturated network, or some other reason. If all that fails, you should
    go to your Kibana UI, browse through logs and try to deduce the cause from them.Refer
    to the following image:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/grafana-swarm-errors-graphs.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-11: Grafana dashboard with network traffic graphs'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: It is important that your system does not report false positives as errors.
    If you notice that there is an error reported through logs, but there's nothing
    to do, it would be better to change the code, so that particular case is not treated
    as an error. Otherwise, with false positives, you'll start seeing too many errors
    and start ignoring them. As a result, when a real error occurs, the chances are
    that you will not notice it.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: We'll skip the Alerts Fired and Alerts Resolved options since they are related
    to *X-Pack* ([https://www.elastic.co/products/x-pack](https://www.elastic.co/products/x-pack)),
    which is a commercial product. Since the book is aimed at open source solutions,
    we'll skip it. That does not mean that you should not consider purchasing it.
    Quite the contrary. Under certain circumstances, *X-Pack* is a valuable addition
    to the tool set.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our quick exploration of the Docker Swarm & Container Overview
    dashboard options. The graphs themselves should be self-explanatory. Take a few
    moments to explore them yourself.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting services through dashboard metrics
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our services are not static. Swarm will reschedule them with each release, when
    a replica fails, when a node becomes unhealthy, or as a result of a myriad of
    other causes. We should do our best to provide Swarm as much information as we
    can. The better we describe our desired service state, the better will Swarm do
    its job.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into all the information we can provide through `docker service
    create` and `docker service update` commands. Instead, we'll concentrate on the
    `--reserve-memory` argument. Later on, you can apply similar logic to `--reserve-cpu`,
    `--limit-cpu`, `--limit-memory`, and other arguments.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: We'll observe the memory metrics in Grafana and update our services accordingly.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Please click on the Memory Usage per Container (Stacked) graph in Grafana and
    choose View. You'll see a screen with a zoomed graph that displays the memory
    consumption of the top twenty containers. Let's filter the metrics by selecting
    prometheus from the Service Name list.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus uses approximately 175 MB of memory. Let''s add that information
    to the service:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/grafana-swarm-memory-graph.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-12: Grafana graph with containers memory consumption filtered with
    Prometheus service'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We updated the `prometheus` service by reserving `200m` of memory. We can assume
    that its memory usage will increase with time, so we reserved a bit more than
    what it currently needs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Please note that `--reserve-memory` does not truly reserve memory, but gives
    a hint to Swarm how much we expect the service should use. With that information,
    Swarm will make a better distribution of services inside the cluster.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s confirm that Swarm rescheduled the service:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output of the `service ps` command is as follows (IDs and Error columns
    are removed for brevity):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can also confirm that the `--reserve-memory` argument is indeed applied:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As you can observe from the `Resources` section, the service now has `200 MiB`
    reserved. We should repeat a similar set of actions for `logstash`, `go-demo`,
    `go-demo-db`, `elasticsearch`, and `proxy` services.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The results might be different on your laptop. In my case, the commands that
    reserve memory based on Grafana metrics are as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: After each update, Swarm will reschedule containers that belong to the service.
    As a result, it'll place them inside a cluster in such a way that none is saturated
    with memory consumption. You should extend the process with CPU and other metrics
    and repeat it periodically.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Please note that increasing memory and CPU limits and reservations is not always
    the right thing to do. In quite a few cases, you might want to scale services
    so that resource utilization is distributed among multiple replicas.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: We used ready-to-go dashboards throughout this chapter. I think that they are
    an excellent starting point and provide a good learning experience. With time,
    you will discover what works best for your organization and probably start modifying
    those dashboards or create new ones specifically tailored to your needs. Hopefully,
    you will contribute them back to the community.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Please let me know if you create a dashboard that complements those we used
    in this chapter or even replaces them. I'd be happy to feature them in the book.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next chapter, let us discuss some of the monitoring
    best practices.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring best practices
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be tempted to put as much information in a dashboard as you can. There
    are so many metrics, so why not visualize them? Right? Wrong! Too much data makes
    important information hard to find. It makes us ignore what we see since too much
    of it is noise.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: What you really need is to have a quick glance of the central dashboard and
    deduce in an instant whether there is anything that might require your attention.
    If there is something to be fixed or adjusted, you can use more specialized Grafana
    dashboards or ad-hoc queries in Prometheus to drill into details.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Create the central dashboard with just enough information to fit a screen and
    provide a good overview of the system. Further on, create additional dashboards
    with more details. They should be organized similar to how we organize code. There
    is usually a main function that is an entry point towards more specific classes.
    When we start coding, we tend to open the main function and drill down from it
    until we reach a piece of code that will occupy our attention. Dashboards should
    be similar. We start with a dashboard that provides critical and very generic
    information. Such a dashboard should be our home and provide just enough metrics
    to deduce whether there is a reason to go deeper into more specific dashboards.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: A single dashboard should have no more than six graphs. That's usually just
    the size that fits a single screen. You are not supposed to scroll up and down
    to see all the graphs while in the central dashboard. Everything essential or
    critical should be visible.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Each graph should be limited to no more than six plots. In many cases, more
    than that only produces noise that is hard to decipher.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Do allow different teams to have different dashboards, especially those that
    are considered as primary or main. Trying to create a dashboard that fits everyone's
    needs is a bad practice. Each team has different priorities that should be fulfilled
    with different metrics visualizations.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the dashboards we used in this chapter fulfill those rules? They don''t.
    They have too many graphs with too many plots. That begs the question: Why did
    we use them? The answer is simple. I wanted to show you a quick and dirty way
    to get a monitoring system up-and-running in no time. I also wanted to show you
    as many different graphs as I could without overloading your brain. See for yourself
    which graphs do not provide value and remove them. Keep those that are truly useful
    and modify those that provide partial value. Create your own dashboards. See what
    works best for you.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Put the monitoring system into practice. Don't try to make it perfect from the
    first attempt. You'll fail if you do. Iterate over dashboards. Start small, grow
    with time. If you are a bigger organization, let each team create their own set
    of dashboards and share what worked well, as well as what failed to provide enough
    value.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring is not a simple thing to do unless you want to spend all your time
    in front of a dashboard. The solution should be designed in such a way that you
    need only a glance to discover whether a part of the system requires your attention.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us destroy everything we did. The next chapter will be a new subject
    with a new set of challenges and solutions:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
