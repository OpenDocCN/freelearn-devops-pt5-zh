["```\ncd k8s-specs\n\ngit pull \n```", "```\nexport AWS_ACCESS_KEY_ID=[...] \n\nexport AWS_SECRET_ACCESS_KEY=[...] \n```", "```\naws --version\n```", "```\naws-cli/1.11.15 Python/2.7.10 Darwin/16.0.0 botocore/1.4.72\n```", "```\nexport AWS_DEFAULT_REGION=us-east-2 \n```", "```\naws iam create-group \\\n --group-name kops\n\n```", "```\n{\n \"Group\": {\n \"Path\": \"/\",\n \"CreateDate\": \"2018-02-21T12:58:47.853Z\",\n \"GroupId\": \"AGPAIF2Y6HJF7YFYQBQK2\",\n \"Arn\": \"arn:aws:iam::036548781187:group/kops\",\n \"GroupName\": \"kops\"\n }\n}\n```", "```\naws iam attach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess \\\n --group-name kops\n\naws iam attach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \\\n --group-name kops\n\naws iam attach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess \\\n --group-name kops\n\naws iam attach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/IAMFullAccess \\\n --group-name kops  \n```", "```\naws iam create-user \\\n --user-name kops\n```", "```\n{\n \"User\": {\n \"UserName\": \"kops\",\n \"Path\": \"/\",\n \"CreateDate\": \"2018-02-21T12:59:28.836Z\",\n \"UserId\": \"AIDAJ22UOS7JVYQIAVMWA\",\n \"Arn\": \"arn:aws:iam::036548781187:user/kops\"\n }\n}  \n```", "```\naws iam add-user-to-group \\\n --user-name kops \\\n --group-name kops  \n```", "```\naws iam create-access-key \\\n --user-name kops >kops-creds  \n```", "```\ncat kops-creds  \n```", "```\n{\n \"AccessKey\": {\n \"UserName\": \"kops\",\n \"Status\": \"Active\",\n \"CreateDate\": \"2018-02-21T13:00:24.733Z\",\n \"SecretAccessKey\": \"...\",\n \"AccessKeyId\": \"...\"\n }\n}  \n```", "```\nexport AWS_ACCESS_KEY_ID=$(\\\n cat kops-creds | jq -r \\\n '.AccessKey.AccessKeyId')\n\nexport AWS_SECRET_ACCESS_KEY=$(\n cat kops-creds | jq -r \\\n '.AccessKey.SecretAccessKey')  \n```", "```\naws ec2 describe-availability-zones \\\n    --region $AWS_DEFAULT_REGION\n```", "```\n{\n \"AvailabilityZones\": [\n {\n \"State\": \"available\", \n \"RegionName\": \"us-east-2\", \n \"Messages\": [], \n \"ZoneName\": \"us-east-2a\"\n }, \n {\n \"State\": \"available\", \n \"RegionName\": \"us-east-2\", \n \"Messages\": [], \n \"ZoneName\": \"us-east-2b\"\n }, \n {\n \"State\": \"available\", \n \"RegionName\": \"us-east-2\", \n \"Messages\": [], \n \"ZoneName\": \"us-east-2c\"\n }\n ]\n}\n```", "```\nexport ZONES=$(aws ec2 \\\n describe-availability-zones \\\n --region $AWS_DEFAULT_REGION \\\n | jq -r \\\n '.AvailabilityZones[].ZoneName' \\\n | tr '\\n' ',' | tr -d ' ')\n\nZONES=${ZONES%?}\n\necho $ZONES  \n```", "```\nus-east-2a,us-east-2b,us-east-2c  \n```", "```\nmkdir -p cluster\n\ncd cluster  \n```", "```\naws ec2 create-key-pair \\\n --key-name devops23 \\\n | jq -r '.KeyMaterial' \\\n >devops23.pem\n```", "```\nchmod 400 devops23.pem \\ \n```", "```\nssh-keygen -y -f devops23.pem \n >devops23.pub  \n```", "```\nexport NAME=devops23.k8s.local  \n```", "```\nexport BUCKET_NAME=devops23-$(date +%s)\n\naws s3api create-bucket \\\n --bucket $BUCKET_NAME \\\n --create-bucket-configuration \\ \n LocationConstraint=$AWS_DEFAULT_REGION  \n```", "```\n{\n \"Location\": http://devops23-1519993212.s3.amazonaws.com/\n}  \n```", "```\nexport KOPS_STATE_STORE=s3://$BUCKET_NAME  \n```", "```\nbrew update && brew install kops  \n```", "```\ncurl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s \nhttps://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-darwin-amd64\n\nchmod +x ./kops\n\nsudo mv ./kops /usr/local/bin/  \n```", "```\nwget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s \nhttps://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | \ncut -d '\"' -f 4)/kops-linux-amd64\n\nchmod +x ./kops\n\nsudo mv ./kops /usr/local/bin/  \n```", "```\nmkdir config\n\nalias kops=\"docker run -it --rm \\\n -v $PWD/devops23.pub:/devops23.pub \\ \n -v $PWD/config:/config \\\n -e KUBECONFIG=/config/kubecfg.yaml \\ \n -e NAME=$NAME -e ZONES=$ZONES \\\n -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\ \n -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\ \n -e KOPS_STATE_STORE=$KOPS_STATE_STORE \\\n vfarcic/kops\"  \n```", "```\nkops create cluster \\\n --name $NAME \\\n --master-count 3 \\\n --node-count 1 \\\n --node-size t2.small \\\n --master-size t2.small \\\n --zones $ZONES \\\n --master-zones $ZONES \\\n --ssh-public-key devops23.pub \\\n --networking kubenet \\\n --kubernetes-version v1.8.4 \\\n --yes  \n```", "```\n...\nkops has set your kubectl context to devops23.k8s.local\n\nCluster is starting.  It should be ready in a few minutes.\n\nSuggestions:\n * validate cluster: kops validate cluster\n * list nodes: kubectl get nodes --show-labels\n * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.devops23.k8s.local\nThe admin user is specific to Debian. If not using Debian please use the appropriate user based on your OS.\n * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs/addons.md  \n```", "```\nkops get cluster  \n```", "```\nNAME               CLOUD ZONES\ndevops23.k8s.local aws   us-east-2a,us-east-2b,us-east-2c  \n```", "```\nkubectl cluster-info  \n```", "```\nKubernetes master is running at https://api-devops23-k8s-local-ivnbim-6094461\n90.us-east-2.elb.amazonaws.com\nKubeDNS is running at https://api-devops23-k8s-local-ivnbim-609446190.us-east\n-2.elb.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  \n```", "```\nkops validate cluster  \n```", "```\nUsing cluster from kubectl context: devops23.k8s.local\n\nValidating cluster devops23.k8s.local\n\nINSTANCE GROUPS\nNAME              ROLE   MACHINETYPE MIN MAX SUBNETS\nmaster-us-east-2a Master t2.small    1   1   us-east-2a\nmaster-us-east-2b Master t2.small    1   1   us-east-2b\nmaster-us-east-2c Master t2.small    1   1   us-east-2c\nnodes             Node   t2.small    1   1   us-east-2a,us-east-2b,us-east-2c\n\nNODE STATUS\nNAME                 ROLE   READY\nip-172-20-120-133... master True\nip-172-20-34-249...  master True\nip-172-20-65-28...   master True\nip-172-20-95-101...  node   True\n\nYour cluster devops23.k8s.local is ready  \n```", "```\nkubectl --namespace kube-system get pods \n```", "```\nNAME                                         READY STATUS  RESTARTS AGE\ndns-controller-...                           1/1   Running 0        5m\netcd-server-events-ip-172-20-120-133...      1/1   Running 0        5m\netcd-server-events-ip-172-20-34-249...       1/1   Running 1        4m\netcd-server-events-ip-172-20-65-28...        1/1   Running 0        4m\netcd-server-ip-172-20-120-133...             1/1   Running 0        4m\netcd-server-ip-172-20-34-249...              1/1   Running 1        3m\netcd-server-ip-172-20-65-28...               1/1   Running 0        4m\nkube-apiserver-ip-172-20-120-133...          1/1   Running 0        4m\nkube-apiserver-ip-172-20-34-249...           1/1   Running 3        3m\nkube-apiserver-ip-172-20-65-28...            1/1   Running 1        4m\nkube-controller-manager-ip-172-20-120-133... 1/1   Running 0        4m\nkube-controller-manager-ip-172-20-34-249...  1/1   Running 0        4m\nkube-controller-manager-ip-172-20-65-28...   1/1   Running 0        4m\nkube-dns-7f56f9f8c7-...                      3/3   Running 0        5m\nkube-dns-7f56f9f8c7-...                      3/3   Running 0        2m\nkube-dns-autoscaler-f4c47db64-...            1/1   Running 0        5m\nkube-proxy-ip-172-20-120-133...              1/1   Running 0        4m\nkube-proxy-ip-172-20-34-249...               1/1   Running 0        4m\nkube-proxy-ip-172-20-65-28...                1/1   Running 0        4m\nkube-proxy-ip-172-20-95-101...               1/1   Running 0        3m\nkube-scheduler-ip-172-20-120-133...          1/1   Running 0        4m\nkube-scheduler-ip-172-20-34-249...           1/1   Running 0        4m\nkube-scheduler-ip-172-20-65-28...            1/1   Running 0        4m  \n```", "```\nkops edit --help  \n```", "```\n...\nAvailable Commands:\n cluster       Edit cluster.\n federation    Edit federation.\n instancegroup Edit instancegroup.\n...  \n```", "```\nkops edit ig --name $NAME nodes  \n```", "```\napiVersion: kops/v1alpha2\nkind: InstanceGroup\nmetadata:\n creationTimestamp: 2018-02-23T00:04:50Z\n labels:\n kops.k8s.io/cluster: devops23.k8s.local\n name: nodes\nspec:\n image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-01-14\n machineType: t2.small\n maxSize: 1\n minSize: 1\n nodeLabels:\n kops.k8s.io/instancegroup: nodes\n role: Node\n subnets:\n - us-east-2a\n - us-east-2b\n - us-east-2c  \n```", "```\nkops update cluster --name $NAME --yes  \n```", "```\n...\nkops has set your kubectl context to devops23.k8s.local\n\nCluster changes have been applied to the cloud.\n\nChanges may require instances to restart: kops rolling-update cluster  \n```", "```\nkops validate cluster  \n```", "```\nValidating cluster devops23.k8s.local\n\nINSTANCE GROUPS\nNAME              ROLE   MACHINETYPE MIN MAX SUBNETS\nmaster-us-east-2a Master t2.small    1   1   us-east-2a\nmaster-us-east-2b Master t2.small    1   1   us-east-2b\nmaster-us-east-2c Master t2.small    1   1   us-east-2c\nnodes             Node   t2.small    2   2   us-east-2a,us-east-2b,us-east-2c\n\nNODE STATUS\nNAME                 ROLE   READY\nip-172-20-120-133... master True\nip-172-20-33-237...  node   True\nip-172-20-34-249...  master True\nip-172-20-65-28...   master True\nip-172-20-95-101...  node   True\n\nYour cluster devops23.k8s.local is ready  \n```", "```\nkubectl get nodes  \n```", "```\nNAME                 STATUS ROLES  AGE VERSION\nip-172-20-120-133... Ready  master 13m v1.8.4\nip-172-20-33-237...  Ready  node   1m  v1.8.4\nip-172-20-34-249...  Ready  master 13m v1.8.4\nip-172-20-65-28...   Ready  master 13m v1.8.4\nip-172-20-95-101...  Ready  node   12m v1.8.4  \n```", "```\nkops edit cluster $NAME  \n```", "```\nkops update cluster $NAME  \n```", "```\nkops update cluster $NAME --yes  \n```", "```\nkops rolling-update cluster $NAME  \n```", "```\nNAME              STATUS      NEEDUPDATE READY MIN MAX NODES\nmaster-us-east-2a NeedsUpdate 1          0     1   1   1\nmaster-us-east-2b NeedsUpdate 1          0     1   1   1\nmaster-us-east-2c NeedsUpdate 1          0     1   1   1\nnodes             NeedsUpdate 2          0     2   2   2\n\nMust specify --yes to rolling-update.  \n```", "```\nkops rolling-update cluster $NAME --yes  \n```", "```\nNAME              STATUS      NEEDUPDATE READY MIN MAX NODES\nmaster-us-east-2a NeedsUpdate 1          0     1   1   1\nmaster-us-east-2b NeedsUpdate 1          0     1   1   1\nmaster-us-east-2c NeedsUpdate 1          0     1   1   1\nnodes             NeedsUpdate 2          0     2   2   2  \n```", "```\nI0225 23:03:03.993068       1 instancegroups.go:130] Draining the node: \"ip-1\n 72-20-40-167...\".\nnode \"ip-172-20-40-167...\" cordoned\nnode \"ip-172-20-40-167...\" cordoned\nWARNING: Deleting pods not managed by ReplicationController, \n ReplicaSet, Job, DaemonSet or StatefulSet: etcd-server-events-\n ip-172-20-40-167..., etcd-server-ip-172-20-40-167..., kube-apiserver-ip-172-20-40-167..., kube-controller-manager-ip-172-20-40-167..., \n kube-proxy-ip-172-20-40-167..., kube-scheduler-ip-172-20-40-167...\nnode \"ip-172-20-40-167...\" drained\n```", "```\nI0225 23:04:37.479407 1 instancegroups.go:237] Stopping instance \"i-\n 06d40d6ff583fe10b\", node \"ip-172-20-40-167...\", in group \"master-us-east-\n 2a.masters.devops23.k8s.local\".  \n```", "```\nI0225 23:09:38.218945 1 instancegroups.go:161] Validating the cluster.\nI0225 23:09:39.437456 1 instancegroups.go:212] Cluster validated.  \n```", "```\nI0225 23:34:01.148318 1 rollingupdate.go:191] Rolling update \n completed for cluster \"devops23.k8s.local\"!\n\n```", "```\nkubectl get nodes  \n```", "```\nNAME                 STATUS ROLES  AGE VERSION\nip-172-20-107-172... Ready  node   4m  v1.8.5\nip-172-20-124-177... Ready  master 16m v1.8.5\nip-172-20-44-126...  Ready  master 28m v1.8.5\nip-172-20-56-244...  Ready  node   10m v1.8.5\nip-172-20-67-40...   Ready  master 22m v1.8.5  \n```", "```\nkops upgrade cluster $NAME --yes  \n```", "```\nITEM    PROPERTY          OLD    NEW\nCluster KubernetesVersion v1.8.5 1.8.6\n\nUpdates applied to configuration.\nYou can now apply these changes, using 'kops update cluster \ndevops23.k8s.local'\n```", "```\nkops update cluster $NAME --yes  \n```", "```\nkops rolling-update cluster $NAME --yes  \n```", "```\naws elb describe-load-balancers  \n```", "```\n{\n \"LoadBalancerDescriptions\": [\n {\n ...\n \"ListenerDescriptions\": [\n {\n \"Listener\": {\n \"InstancePort\": 443, \n \"LoadBalancerPort\": 443, \n \"Protocol\": \"TCP\", \n \"InstanceProtocol\": \"TCP\"\n }, \n ...\n \"Instances\": [\n {\n \"InstanceId\": \"i-01f5c2ca47168b248\"\n }, \n {\n \"InstanceId\": \"i-0305e3b2d3da6e1ce\"\n }, \n {\n \"InstanceId\": \"i-04291ef2432b462f2\"\n }\n ], \n\n \"DNSName\": \"api-devops23-k8s-local-ivnbim-1190013982.us-east-2.elb.amazonaws.com\", \n ...\n \"LoadBalancerName\": \"api-devops23-k8s-local-ivnbim\", \n ...  \n```", "```\nkubectl config view  \n```", "```\napiVersion: v1\nclusters:\n- cluster:\n certificate-authority-data: REDACTED\n server: https://api-devops23-k8s-local-ivnbim-1190013982.us-east-2.elb.am\n azonaws.com\n name: devops23.k8s.local\n...\ncurrent-context: devops23.k8s.local\n...  \n```", "```\nkubectl create \\\n -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/ingress-nginx/v1.6.0.yaml\n```", "```\nnamespace \"kube-ingress\" created\nserviceaccount \"nginx-ingress-controller\" created\nclusterrole \"nginx-ingress-controller\" created\nrole \"nginx-ingress-controller\" created\nclusterrolebinding \"nginx-ingress-controller\" created\nrolebinding \"nginx-ingress-controller\" created\nservice \"nginx-default-backend\" created\ndeployment \"nginx-default-backend\" created\nconfigmap \"ingress-nginx\" created\nservice \"ingress-nginx\" created\ndeployment \"ingress-nginx\" created  \n```", "```\nkubectl --namespace kube-ingress \\\n get all\n\n```", "```\nNAME                         DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\ndeploy/ingress-nginx         3       3       3          3         1m\ndeploy/nginx-default-backend 1       1       1          1         1m\nNAME                                DESIRED CURRENT READY AGE\nrs/ingress-nginx-768fc7997b         3       3       3     1m\nrs/nginx-default-backend-74f9cd546d 1       1       1     1m\nNAME                                      READY STATUS  RESTARTS AGE\npo/ingress-nginx-768fc7997b-4xfq8         1/1   Running 0        1m\npo/ingress-nginx-768fc7997b-c7zvx         1/1   Running 0        1m\npo/ingress-nginx-768fc7997b-clr5m         1/1   Running 0        1m\npo/nginx-default-backend-74f9cd546d-mtct8 1/1   Running 0        1m\nNAME                      TYPE         CLUSTER-IP     EXTERNAL-IP      PORT(S)                    AGE\nsvc/ingress-nginx         LoadBalancer 100.66.190.165 abb5117871831... 80:301\n    07/TCP,443:30430/TCP 1m\nsvc/nginx-default-backend ClusterIP    100.70.227.240 <none>           80/TCP\n                         1m\n```", "```\naws elb describe-load-balancers  \n```", "```\n{\n \"LoadBalancerDescriptions\": [\n {\n ...\n \"LoadBalancerName\": \"api-devops23-k8s-local-ivnbim\",\n ...\n }, \n {\n ...\n \"ListenerDescriptions\": [\n {\n \"Listener\": {\n \"InstancePort\": 30107, \n \"LoadBalancerPort\": 80, \n \"Protocol\": \"TCP\", \n \"InstanceProtocol\": \"TCP\"\n }, \n \"PolicyNames\": []\n }, \n {\n \"Listener\": {\n \"InstancePort\": 30430, \n \"LoadBalancerPort\": 443, \n \"Protocol\": \"TCP\", \n \"InstanceProtocol\": \"TCP\"\n }, \n          \"PolicyNames\": []\n        }\n    ], \n      ...\n      \"Instances\": [\n        {\n          \"InstanceId\": \"i-063fabc7ad5935db5\"\n        },\n        {\n          \"InstanceId\": \"i-04d32c91cfc084369\"\n        }\n    ], \n    \"DNSName\": \"a1c431cef1bfa11e88b600650be36f73-2136831960.us-east-2.elb.amazonaws.com\", \n      ...\n   \"LoadBalancerName\": \"a1c431cef1bfa11e88b600650be36f73\", \n      ...\n```", "```\nCLUSTER_DNS=$(aws elb \\\n describe-load-balancers | jq -r \\\n \".LoadBalancerDescriptions[] \\\n | select(.DNSName \\\n | contains (\\\"api-devops23\\\") \\\n | not).DNSName\")  \n```", "```\ncd ..\n\nkubectl create \\\n -f aws/go-demo-2.yml \\\n --record --save-config\n```", "```\ningress \"go-demo-2\" created\ndeployment \"go-demo-2-db\" created\nservice \"go-demo-2-db\" created\ndeployment \"go-demo-2-api\" created\nservice \"go-demo-2-api\" created  \n```", "```\nkubectl rollout status \\\n deployment go-demo-2-api  \n```", "```\ndeployment \"go-demo-2-api\" successfully rolled out  \n```", "```\ncurl -i \"http://$CLUSTER_DNS/demo/hello\"  \n```", "```\naws ec2 \\\n describe-instances | jq -r \\\n \".Reservations[].Instances[] \\\n | select(.SecurityGroups[]\\\n .GroupName==\\\"nodes.$NAME\\\")\\\n .InstanceId\"  \n```", "```\ni-063fabc7ad5935db5\ni-04d32c91cfc084369  \n```", "```\nINSTANCE_ID=$(aws ec2 \\\n describe-instances | jq -r \\\n \".Reservations[].Instances[] \\ \n | select(.SecurityGroups[]\\\n .GroupName==\\\"nodes.$NAME\\\")\\\n .InstanceId\" | tail -n 1)  \n```", "```\naws ec2 terminate-instances \\\n --instance-ids $INSTANCE_ID  \n```", "```\n{\n \"TerminatingInstances\": [\n {\n \"InstanceId\": \"i-063fabc7ad5935db5\",\n \"CurrentState\": {\n \"Code\": 32,\n \"Name\": \"shutting-down\"\n },\n \"PreviousState\": {\n \"Code\": 16,\n \"Name\": \"running\"\n }\n }\n ]\n}  \n```", "```\naws ec2 describe-instances | jq -r \\\n    \".Reservations[].Instances[] \\\n    | select(\\\n    .SecurityGroups[].GroupName \\\n    ==\\\"nodes.$NAME\\\").InstanceId\"\n\n```", "```\ni-04d32c91cfc084369  \n```", "```\naws ec2 \\ \n describe-instances | jq -r \\\n \".Reservations[].Instances[] \\ \n | select(.SecurityGroups[]\\\n .GroupName==\\\"nodes.$NAME\\\")\\\n .InstanceId\"  \n```", "```\ni-003b4b1934d85641a\ni-04d32c91cfc084369  \n```", "```\nkubectl get nodes  \n```", "```\nNAME                                        STATUS ROLES  AGE VERSION\nip-172-20-55-183.us-east-2.compute.internal Ready  master 30m v1.\n 8.6\nip-172-20-61-82.us-east-2.compute.internal  Ready  node   13m v1.\n 8.6\nip-172-20-71-53.us-east-2.compute.internal  Ready  master 30m v1.\n 8.6\nip-172-20-97-39.us-east-2.compute.internal  Ready  master 30m v1.\n 8.6 \n```", "```\nkubectl get nodes  \n```", "```\nNAME                                        STATUS ROLES  AGE VERSION\nip-172-20-55-183.us-east-2.compute.internal Ready  master 32m v1.\n 8.6\nip-172-20-61-82.us-east-2.compute.internal  Ready  node   15m v1.\n 8.6\nip-172-20-71-53.us-east-2.compute.internal  Ready  master 32m v1.\n 8.6\nip-172-20-79-161.us-east-2.compute.internal Ready  node   2m  v1.\n 8.6\nip-172-20-97-39.us-east-2.compute.internal  Ready  master 32m v1.\n 8.6  \n```", "```\ncd cluster\n\nmkdir -p config\n\nexport KUBECONFIG=$PWD/config/kubecfg.yaml  \n```", "```\nkops export kubecfg --name ${NAME}\n\ncat $KUBECONFIG  \n```", "```\napiVersion: v1\nclusters:\n- cluster:\n certificate-authority-data: ...\n server: https://api-devops23-k8s-local-ivnbim-609446190.us-east-2.elb.amazonaws.com\n  name: devops23.k8s.local\ncontexts:\n- context:\n cluster: devops23.k8s.local\n user: devops23.k8s.local\n name: devops23.k8s.local\ncurrent-context: devops23.k8s.local\nkind: Config\npreferences: {}\nusers:\n- name: devops23.k8s.local\n user:\n as-user-extra: {}\n client-certificate-data: ...\n client-key-data: ...\n password: oeezRbhG4yz3oBUO5kf7DSWcOwvjKZ6l\n username: admin\n- name: devops23.k8s.local-basic-auth\n user:\n as-user-extra: {}\n    password: oeezRbhG4yz3oBUO5kf7DSWcOwvjKZ6l\n    username: admin\n```", "```\necho \"export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \nexport AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \nexport AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \nexport ZONES=$ZONES \nexport NAME=$NAME \nexport KOPS_STATE_STORE=$KOPS_STATE_STORE\" \\ \n    >kops \n```", "```\nkops delete cluster \\\n --name $NAME \\\n --yes  \n```", "```\n...\nDeleted kubectl config for devops23.k8s.local\n\nDeleted cluster: \"devops23.k8s.local\"  \n```", "```\naws s3api delete-bucket \\\n --bucket $BUCKET_NAME  \n```", "```\n# Replace `[...]` with the administrative access key ID.\nexport AWS_ACCESS_KEY_ID=[...]\n\n# Replace `[...]` with the administrative secret access key.\nexport AWS_SECRET_ACCESS_KEY=[...]\n\naws iam remove-user-from-group \\\n --user-name kops \\\n --group-name kops\n\naws iam delete-access-key \\ \n --user-name kops \\\n --access-key-id $(\\\n cat kops-creds | jq -r\\ \n '.AccessKey.AccessKeyId')\n\naws iam delete-user \\\n --user-name kops\n\naws iam detach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess \\\n --group-name kops\n\naws iam detach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \\\n --group-name kops\n\naws iam detach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess \\\n --group-name kops\n\naws iam detach-group-policy \\\n --policy-arn arn:aws:iam::aws:policy/IAMFullAccess \\\n --group-name kops\n\naws iam delete-group \\\n --group-name kops  \n```", "```\naws cloudformation create-stack \\\n --template-url https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl \\\n --capabilities CAPABILITY_IAM \\\n --stack-name devops22 \\\n    --parameters \\\n ParameterKey=ManagerSize,ParameterValue=3 \\\n ParameterKey=ClusterSize,ParameterValue=2 \\\n ParameterKey=KeyName,ParameterValue=workshop \\ \n ParameterKey=EnableSystemPrune,ParameterValue=yes \\\n ParameterKey=EnableCloudWatchLogs,ParameterValue=no \\ \n ParameterKey=EnableCloudStorEfs,ParameterValue=yes \\\n ParameterKey=ManagerInstanceType,ParameterValue=t2.small \\\n    ParameterKey=InstanceType,ParameterValue=t2.small  \n```", "```\nkops create cluster \\\n --name $NAME \\\n --master-count 3 \\\n --node-count 1 \\\n --node-size t2.small \\\n --master-size t2.small \\ \n --zones $ZONES \\\n --master-zones $ZONES \\\n --ssh-public-key devops23.pub \\\n --networking kubenet \\\n --kubernetes-version v1.8.4 \\\n --yes \n```"]