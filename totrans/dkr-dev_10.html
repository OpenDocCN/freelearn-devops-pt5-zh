<html><head></head><body>
		<div><h1 id="_idParaDest-148"><em class="italic"><a id="_idTextAnchor157"/>Chapter 8</em>: Deploying Docker Apps to Kubernetes</h1>
			<p>Recently, lots of container orchestrators have sprung up like mushrooms after a rainstorm, but one orchestrator is poised to dominate the market: Kubernetes, from the Cloud Native Computing Foundation. Google originally released Kubernetes with the intention of bringing the same level of sophistication to the world of open source container runtimes as it has been doing for years internally with the Borg clustering system.</p>
			<p>We will begin by learning more about different Kubernetes distributions and why you might want to use each one. We will start with using Kubernetes on a local development workstation, and then install a sample application locally.</p>
			<p>As we progress through the chapter, you will learn how to create a Kubernetes cluster on <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) through <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), and deploy your application to a cluster running on multiple <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) nodes. We will use AWS CloudFormation, an infrastructure-as-code system, to deploy the EKS cluster. Once we have deployed the cluster to AWS, we will learn about using labels and namespaces to organize our applications.</p>
			<p>Running a Kubernetes cluster is more complex than the alternatives presented so far, but it opens up a huge universe of tools and techniques for running clustered applications with a vendor-neutral, cloud-native approach. Kubernetes is useful not only for cloud deployments, but also for on-premises deployments and local development.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Options for Kubernetes local installation</li>
				<li>Deploying a sample application – ShipIt Clicker v4</li>
				<li>Choosing a Kubernetes distribution</li>
				<li>Getting familiar with Kubernetes concepts</li>
				<li>Spinning up AWS EKS with CloudFormation </li>
				<li>Deploying an application with resource limits to Kubernetes on AWS EKS</li>
				<li>Using AWS Elastic Container Registry with AWS EKS</li>
				<li>Using labels and namespaces to segregate environments </li>
			</ul>
			<p>Let's get started by getting Kubernetes running on our local workstation. Then, we will look at the various Kubernetes distributions available.  </p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor158"/>Technical requirements</h1>
			<p>For this chapter, you will need to set up Kubernetes on your local workstation, either through Docker Desktop or by installing a Kubernetes distribution, such as Minikube. In addition, to deploy your containers to AWS, you will need an account set up in advance. </p>
			<p>You can sign up for an AWS account at the following URL if you haven't already done so: </p>
			<p><a href="https://aws.amazon.com/">https://aws.amazon.com/</a></p>
			<p>The code files for this chapter can be downloaded from the <code>chapter8</code> directory at <a href="https://github.com/PacktPublishing/Docker-for-Developers/">https://github.com/PacktPublishing/Docker-for-Developers/</a>.</p>
			<p>Check out the following video to see the Code in Action:</p>
			<p><a href="https://bit.ly/3fXO5xy">https://bit.ly/3fXO5xy</a></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor159"/>Options for Kubernetes local installation</h1>
			<p>You need to set up<a id="_idIndexMarker433"/> a local Kubernetes installation in order to build, package, and test your Docker application in preparation for deploying it to a production installation in the cloud. Please review the Kubernetes <em class="italic">Getting Started</em> documentation (<a href="https://kubernetes.io/docs/setup/">https://kubernetes.io/docs/setup/</a>). This documentation calls this local environment a <strong class="bold">learning environment</strong>. Think of the local environment as a<a id="_idIndexMarker434"/> way to learn about and test your application before you take the application to production with Kubernetes in the cloud. Let's continue by weighing up the options, starting with Docker Desktop's Kubernetes support.</p>
			<p>Docker Desktop with Kubernetes</p>
			<p>For most people, this<a id="_idIndexMarker435"/> is the easiest way to start experimenting<a id="_idIndexMarker436"/> with Kubernetes. You don't have to set up cloud accounts or do a complicated installation to get started if you choose to do this. To install Docker Desktop, follow the download links at <a href="https://www.docker.com/products/docker-desktop">https://www.docker.com/products/docker-desktop</a>.</p>
			<p>With recent versions of Docker Desktop, you can enable Kubernetes support and run and develop Kubernetes applications on your workstation. Open the Docker Desktop application on your workstation and go to the <strong class="bold">Preferences</strong> menu to open the <strong class="bold">Settings</strong> dialog. Tick the <strong class="bold">Enable Kubernetes</strong> box and hit the <strong class="bold">Apply &amp; Restart</strong> button:</p>
			<div><div><img src="img/B11641_08_001.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Example of enabling Kubernete</p>
			<p>This will activate<a id="_idIndexMarker437"/> a single-node Kubernetes cluster on your local <a id="_idIndexMarker438"/>workstation. Once you have enabled Kubernetes, you are ready to verify that your local installation works. See the following section to find out how to do this.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor160"/>Minikube</h2>
			<p>If you don't want<a id="_idIndexMarker439"/> to run Kubernetes through Docker Desktop, you should <a id="_idIndexMarker440"/>probably use Minikube to set up a local Kubernetes single-node cluster environment. This is available on Windows, Macintosh, and a wide variety of Linux operating system distributions.</p>
			<p>To install<a id="_idIndexMarker441"/> Minikube, follow the directions for your <a id="_idIndexMarker442"/>operating system found at <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">https://kubernetes.io/docs/tasks/tools/install-minikube/</a>, and then follow the instructions in the<a id="_idIndexMarker443"/> following section to verify that your Minikube installation works.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor161"/>Verifying that your Kubernetes installation works</h2>
			<p>Interacting with <a id="_idIndexMarker444"/>Kubernetes is done mostly through the <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>). You can issue the following command to <a id="_idIndexMarker445"/>see whether your environment is functional; it will show all the running pods, including the system pods:</p>
			<pre>kubectl get pods -A</pre>
			<p>The output will look something like this:</p>
			<div><div><img src="img/B11641_08_002.jpg" alt="Figure 8.2 – Output of kubectl get pods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Output of kubectl get pods</p>
			<p>Now that you have Kubernetes running on your local workstation, you can develop and deploy applications using Kubernetes. Applications you develop and package with Kubernetes can be deployed with the same tools that you use locally – but at a much larger scale in the cloud. Before we deploy an application to the cloud, though, we should show that we can deploy a packaged application locally.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor162"/>Deploying a sample application – ShipIt Clicker v4</h1>
			<p>Let's imagine that the ShipIt Clicker application introduced in previous chapters has been shipped to<a id="_idIndexMarker446"/> production and the team responsible for operations is nervous about the limits of scaling this application since it is only deployed on one server. In order to scale out this Docker application to multiple servers, the team has decided to migrate to Kubernetes and package the software for Kubernetes using the Helm package manager. To proceed, let's install Helm and test it out.</p>
			<p>Installing Helm</p>
			<p>Helm is to Kubernetes what a package manager is to a modern operating system. It allows<a id="_idIndexMarker447"/> developers to specify <a id="_idIndexMarker448"/>how their application is packaged and deployed in a Kubernetes cluster. Helm is not only a package manager, but also a templating system for generating Kubernetes configurations and applying those configurations in a controlled way. Helm allows developers to define the entire set of containers and their interrelated Kubernetes configurations. Once you have defined an application in Helm, it becomes simple to install and update that application.</p>
			<p>You can install this on macOS easily with Homebrew using the following command:</p>
			<pre>brew install helm</pre>
			<p>For other operating<a id="_idIndexMarker449"/> systems, follow the Helm installation instructions at <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a>.</p>
			<p>Once you have installed Helm, use it to install the stable Helm repository (so that we can install other software packages that Helm supports, such as the NGINX Ingress Controller) with the following command:</p>
			<pre>helm repo add stable https://kubernetes-charts.storage.googleapis.com/</pre>
			<p>Once you have installed this, you can use Helm to install applications from the catalog to your local Kubernetes instance. You can also use Helm to install applications defined in local Helm charts. We will use Helm to deploy ShipIt Clicker to Kubernetes, in conjunction with another Helm package, the NGINX Ingress Controller. In this chapter, we will first deploy the ShipIt Clicker application to the local learning environment Kubernetes <a id="_idIndexMarker450"/>cluster, and later, we will deploy <a id="_idIndexMarker451"/>ShipIt Clicker to the cloud on Amazon EKS.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor163"/>Deploying the NGINX Ingress Controller and ShipIt Clicker locally</h2>
			<p>Let's use<a id="_idIndexMarker452"/> Helm to install a packaged application, the NGINX <a id="_idIndexMarker453"/>Ingress Controller, and then use it to install ShipIt Clicker. An Ingress Controller is a Kubernetes networking proxy that allows requests from the outside to reach applications deployed to Kubernetes, with well-defined interfaces to help wire together the applications. The stable Helm repository contains the NGINX Ingress Controller. Install it as follows:</p>
			<pre>helm install nginx-ingress stable/nginx-ingress</pre>
			<p>Later in the chapter, we will explore Ingress Controller in more detail. Know for now that this simple installation is sufficient to expose services inside the Kubernetes cluster with the right configurations to <code>localhost</code> so that you can test them.</p>
			<p>Next, we will build the ShipIt Clicker Docker container, tag it, and push it to Docker Hub. Kubernetes relies on pulling Docker images from a Docker image registry, so it is insufficient to only have the container on your local system. Issue these commands, replacing <code>dockerfordevelopers</code> with your Docker Hub username:</p>
			<pre>$ cd chapter8
$ docker build . -t dockerfordevelopers/shipitclicker:0.4.0
$ docker push dockerfordevelopers/shipitclicker:0.4.0</pre>
			<p>Edit the <code>shipitclicker/values.yaml</code> file and replace <code>dockerfordevelopers</code> with your Docker Hub username in this stanza:</p>
			<pre># Default values for shipitclicker.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: dockerfordevelopers/shipitclicker
  pullPolicy: IfNotPresent</pre>
			<p>Then, deploy ShipIt Clicker to the Kubernetes local environment. In this case, we will use a local Helm Chart instead of one from a network Helm Chart repository. The Helm Chart for ShipIt Clicker is in the GitHub repository, in the <code>chapter8/shipitclicker</code> directory. Install it with Helm, as follows:</p>
			<pre>$ helm install shipitclicker shipitclicker NAME: shipitclicker
LAST DEPLOYED: Fri Apr 24 23:21:22 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  http://localhost</pre>
			<p>Visit <code>http://localhost/</code> to view<a id="_idIndexMarker454"/> the ShipIt Clicker <a id="_idIndexMarker455"/>application. You should see the running application splash screen.</p>
			<h3>Troubleshooting local installation</h3>
			<p>If you can't reach<a id="_idIndexMarker456"/> the application at <code>http://localhost/</code>, you might have another web server running on port <code>80</code>, such as Apache 2.</p>
			<p>Now that we are running this on Kubernetes, you need to use Kubernetes commands to connect to services that are on the inside of the cluster and not exposed through the Ingress Controller.</p>
			<p>To expose the Redis port from the Kubernetes cluster for testing, use the following commands:</p>
			<pre>$ brew install redis
$ kubectl port-forward deployment/shipitclicker 6379 &amp;
$ redis-cli
&gt; keys *
&gt; quit</pre>
			<p>Now that you have deployed the ShipIt Clicker application to a local Kubernetes installation, you can proceed with deploying it to a larger cloud environment and configuring it for <a id="_idIndexMarker457"/>production readiness.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor164"/>Choosing a Kubernetes distribution</h1>
			<p>So, how do we host<a id="_idIndexMarker458"/> Kubernetes beyond installing it on our workstations? When it comes to choosing a Kubernetes distribution, you are presented with a plethora of options, as we saw in <a href="B11641_05_Final_NM_ePub.xhtml#_idTextAnchor080"><em class="italic">Chapter 5</em></a>, <em class="italic">Alternatives for Deploying and Running Containers in Production</em>. We are now going to revisit some of the most popular options to help you gain an understanding of the choices available based on your cloud provider or bare-metal data center setup, as well as see why we are choosing to use EKS to demonstrate the migration of the ShipIt Clicker sample application to Kubernetes.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor165"/>Google Kubernetes Engine</h2>
			<p><strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) is Google's key <a id="_idIndexMarker459"/>service for hosting containers in a Kubernetes-based <a id="_idIndexMarker460"/>environment. GKE (formerly known as Google Container Engine) was released in an Alpha state in November 2014 and went live in August 2015 for general usage.</p>
			<p>It currently offers one of the most mature Kubernetes services offered by cloud providers, including the following features:</p>
			<ul>
				<li>A single cluster quick start option for trialing the service</li>
				<li>Container vulnerability scanning</li>
				<li>Built-in data encryption </li>
				<li>Multiple channels for upgrading, repairing, and releasing </li>
				<li>Integration with Google monitoring services</li>
				<li>Automatic scaling and load balancing</li>
				<li>Google-managed underlying hardware</li>
			</ul>
			<p>Further documentation for<a id="_idIndexMarker461"/> interested readers can be found at the GKE website at <a href="https://cloud.google.com/kubernetes-engine/docs">https://cloud.google.com/kubernetes-engine/docs</a>.</p>
			<p>Let's now compare this with Amazon's offerings.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor166"/>AWS EKS</h2>
			<p>Amazon's answer<a id="_idIndexMarker462"/> to serving and managing<a id="_idIndexMarker463"/> containers in the cloud is its EKS service. As with GKE, Amazon's Kubernetes services, EKS, offers a managed service. Unlike Google's offering, it came to the market later, not being available until early 2018. However, what EKS loses in maturity, it makes up for in features.</p>
			<p>These<a id="_idIndexMarker464"/> features include the following:</p>
			<ul>
				<li>Serverless hosting via AWS Fargate (<a href="https://aws.amazon.com/fargate/">https://aws.amazon.com/fargate/</a>)</li>
				<li>Server deployment options on EC2</li>
				<li>Zero-downtime upgrades and patching</li>
				<li>Auto-detection of unhealthy nodes</li>
				<li>Hybrid hosting<a id="_idIndexMarker465"/> solutions with AWS Outposts (<a href="https://aws.amazon.com/outposts/">https://aws.amazon.com/outposts/</a>)</li>
				<li>Kubernetes Jobs for batch processing </li>
			</ul>
			<p>You can read more about EKS on the official website at <a href="https://aws.amazon.com/eks/features/">https://aws.amazon.com/eks/features/</a>.</p>
			<p>We'll be exploring EKS in more detail throughout this chapter and in subsequent chapters, mostly since it is the managed Kubernetes offering from the dominant cloud vendor. Other distributions have their merits, however, so we will also examine some of the other options out there. Next is Red Hat OpenShift.</p>
			<p>Red Hat OpenShift</p>
			<p>OpenShift is a <a id="_idIndexMarker466"/>collection of software<a id="_idIndexMarker467"/> developed by Red Hat geared toward containerized application architectures. Like GKE and EKS, OpenShift is Kubernetes-focused; however, where it diverges is with its focus on build-related artifacts and a native image repository.</p>
			<p>Having used Jenkins in the projects presented in this book, you will now be familiar with <code>kubectl</code><strong class="bold">  </strong>commands to include mechanisms that replicate the sort of CI/CD functionality that you might otherwise have to use software such as Jenkins or Spinnaker to get. This includes the ability to create builds, test runs, and deployments. </p>
			<p>There are some other key features that also make OpenShift a desirable option:</p>
			<ul>
				<li>Automated upgrades and life cycle management</li>
				<li>Open <a id="_idIndexMarker469"/>source code base available on GitHub (<a href="https://github.com/openshift">https://github.com/openshift</a>)</li>
				<li>Deploy in any cloud, in a data center, or on-premises</li>
				<li>An image registry </li>
				<li>Monitoring and log aggregation</li>
			</ul>
			<p>For further<a id="_idIndexMarker470"/> information on Red Hat OpenShift, make sure to check out the documentation on GitHub (<a href="https://github.com/openshift/openshift-docs">https://github.com/openshift/openshift-docs</a>) or on the official website (<a href="https://www.openshift.com/">https://www.openshift.com/</a>).</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor167"/>Microsoft Azure Kubernetes Service</h2>
			<p>We've looked<a id="_idIndexMarker471"/> at the major players so far, but of course, couldn't go any further without mentioning Microsoft's contribution to the Kubernetes ecosystem. For users of Microsoft cloud <a id="_idIndexMarker472"/>products, <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) provides a mechanism to serve Docker containers in a Kubernetes-based environment. </p>
			<p>Let's take a brief tour of what AKS offers:</p>
			<ul>
				<li>The elastic provisioning of services </li>
				<li>Integration with the Azure DevOps and Monitor services</li>
				<li>Identity and access management with Active Directory</li>
				<li>Failure detection and container health monitoring</li>
				<li>Canary deployments</li>
				<li>Log aggregation</li>
			</ul>
			<p>As you can see, for Azure users, it has a comparable set of features to those available in EKS and GKE. If<a id="_idIndexMarker473"/> you would like to learn more, please refer to the AKS documentation (<a href="https://docs.microsoft.com/en-us/azure/aks/">https://docs.microsoft.com/en-us/azure/aks/</a>). Here, you will also find a quick start guide for getting a taste of what the service has to offer.</p>
			<p>Before<a id="_idIndexMarker474"/> running <a id="_idIndexMarker475"/>through the components that form the basis of Kubernetes, let's briefly review the other options available.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor168"/>Reviewing other relevant options</h2>
			<p>EKS, OpenShift, GKE, and <a id="_idIndexMarker476"/>AKS represent the most popular Kubernetes services on the market. However, they are not alone. Digital Ocean offers an option for those wishing to get a taste of a managed service outside of deploying your own RedShift infrastructure or signing up to the big cloud providers. You can read more about it at <a href="https://www.digitalocean.com/products/kubernetes/">https://www.digitalocean.com/products/kubernetes/</a>.</p>
			<p>Many readers will be familiar with IBM, and they too offer cloud-hosting services. If you want to try out Kubernetes in their cloud environment, you can find details on their website, including how to set up a free cluster (<a href="https://www.ibm.com/cloud/container-service/">https://www.ibm.com/cloud/container-service/</a>).</p>
			<p>Anyone familiar with VMware might wish to explore their Kubernetes offering as well –VMware Tanzu <a id="_idIndexMarker477"/>Kubernetes Grid – which has strengths in building hybrid clouds (<a href="https://tanzu.vmware.com/kubernetes-grid">https://tanzu.vmware.com/kubernetes-grid</a>).</p>
			<p>Finally, those<a id="_idIndexMarker478"/> looking for a fully managed Kubernetes service or those who are already customers of Rackspace have the option of checking out their <strong class="bold">Kubernetes as a Service</strong> (<strong class="bold">KaaS</strong>) offerings (<a href="https://www.rackspace.com/managed-kubernetes">https://www.rackspace.com/managed-kubernetes</a>).</p>
			<p>That wraps up our whistle-stop tour of the hosting platforms available for deploying your containers.</p>
			<p>For the remainder of this chapter, we will be using Amazon's EKS service. If you haven't created an account, we recommend you sign up for one here now:</p>
			<p><a href="https://aws.amazon.com/">https://aws.amazon.com/</a></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Users of other cloud providers may find that they can adapt the following sections to their own services if they wish.</p>
			<p>Let's now dig <a id="_idIndexMarker479"/>into the core concepts of Kubernetes, including pods, nodes, and namespaces.</p>
			<p>Getting familiar with Kubernetes concepts</p>
			<p>Now that you know <a id="_idIndexMarker480"/>where you can deploy Kubernetes, let's dive into some of the key concepts (including objects, ConfigMaps, pods, nodes, services, Ingress Controllers, secrets, and namespaces) and how they work. Let's start by examining an architecture diagram that shows the relationship between the various components of the system:</p>
			<div><div><img src="img/B11641_08_003.jpg" alt="Figure 8.3 – Kubernetes architecture diagram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure">Figure 8.3 – Kubernetes architecture diagram</p>
			<p class="figure-caption">Figure 8.3 – Kubernetes architecture diagram</p>
			<p>With Kubernetes, the cluster consists of a control plane that manages all aspects of the Kubernetes cluster (including the interface with the cloud provider) and a set of workers for the cluster, known as nodes, where the applications hosted by the cluster live. Developers and<a id="_idIndexMarker481"/> cluster operators interact with Kubernetes via the control plane through an API. The processes in the control plane communicate with the processes running on the individual worker nodes via the <code>kubelet</code> process, and the processes on the worker nodes are organized as pods that communicate with one another via the <code>kube-proxy</code> process that runs on each node.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor169"/>Objects</h2>
			<p>The most fundamental <a id="_idIndexMarker482"/>concept in Kubernetes is an <code>kubectl</code> utility to create, query, and modify all the different types of Kubernetes objects, as well as to configure the cluster.</p>
			<p>The <code>kubectl</code> command-line utility can take YAML format files that describe the objects and use them to create and update the state of the system. This is the most basic way of defining, installing, and upgrading Kubernetes applications. The Helm tool we used to install applications takes this a step further by providing templating and life cycle capabilities.</p>
			<p>We recommend configuring your application through Helm Charts. You briefly saw how to use Helm at the beginning of this chapter. A Helm Chart is simply a set of YAML configuration files that<a id="_idIndexMarker483"/> contain information about your containerized application.</p>
			<p>You can create a new Helm Chart using the following command:</p>
			<pre>helm create my-chart</pre>
			<p>This sets up a Helm Chart structure with template files that are ready for customization.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor170"/>ConfigMaps</h2>
			<p>Kubernetes handles <a id="_idIndexMarker484"/>application configuration with a concept known as a ConfigMap. Then, we need to define the configuration for the container itself. This is handled through a ConfigMap.</p>
			<p>The key idea behind ConfigMaps is that you can separate the important configuration from the content of the images themselves. This is done in order to provide better portability of your microservices and applications.</p>
			<p>ConfigMaps can be created directly through <code>kubectl</code> using the following command:</p>
			<pre>kubectl create configmap sample-configmap-name</pre>
			<p>A ConfigMap will contain information used by your application, and other key-value pairs, such as the namespace. The following example illustrates how an application's ConfigMap might look:</p>
			<pre>apiVersion: v1 
kind: ConfigMap 
metadata: 
    name: shipitclicker-configmap 
data: 
    language: "JavaScript"
    node.version: "13.x"</pre>
			<p>A ConfigMap such as the one we just demonstrated would then be stored inside your Helm Chart directory in the templates folder – for example, <code>shipitclicker/templates/configmap.yaml</code>.</p>
			<p>With this basic <a id="_idIndexMarker485"/>setup in place, you can then install your configuration through the <code>helm install</code> command. We will be exploring configuration in both its ConfigMap and Helm Chart formats in further detail throughout this chapter. </p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor171"/>Pods</h2>
			<p>Pods in Kubernetes <a id="_idIndexMarker486"/>serve the purpose of grouping together <em class="italic">1</em> to <em class="italic">n</em> containerized components, which are then run in a shared context. They also include shared resources, such as IP addresses, storage, and definitions on how containers should be run. Multiple containers running together in a pod can communicate with each other on fixed ports on <code>localhost</code>, simplifying application configuration significantly.</p>
			<p>When defining what should be run in a pod, the best approach is to think of it as holding all the necessary containers for a system or application. Multiple pods can then be added to Kubernetes to scale your application out horizontally. This allows you to create redundancy and helps cope with increases in traffic and load.</p>
			<p>The shared context that the pods use is implemented through Linux concepts such as cgroups and namespaces. In <a href="B11641_12_Final_NM_ePub.xhtml#_idTextAnchor278"><em class="italic">Chapter 12</em></a>, <em class="italic">Introduction to Container Security</em>, we will explore some of these concepts in depth in relation to container security.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor172"/>Nodes</h2>
			<p>Machines that host <a id="_idIndexMarker487"/>Docker containers in Kubernetes' ecosystem are known as <strong class="bold">nodes</strong>, though you may also encounter the terms <em class="italic">minions</em> or <em class="italic">workers</em> – they all mean the same thing, but node is the official term. Kubernetes supports nodes that are either physical or virtual machines. Services such as Amazon's EKS provide the mechanisms for deploying node infrastructure. You deploy Kubernetes pods on nodes; the pods include both containers and shared resources.  </p>
			<p>In the learning environment that we are using, our local development workstation is the sole node in the cluster. Later in this chapter, we will be creating a Kubernetes cluster with nodes managed by EKS on AWS EC2. Kubernetes nodes run containers through pods and othe<a id="_idIndexMarker488"/>r Kubernetes objects, such as DaemonSets.</p>
			<p class="callout-heading">Alternative container runtimes</p>
			<p class="callout">Kubernetes nodes could potentially run different container runtimes. Kubernetes not only supports Docker containers, but also other container technologies, including containerd, CRI-O, and Frakti. Since this book is about Docker, we will exclusively use the Docker runtime in our examples. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor173"/>Services</h2>
			<p>A Kubernetes<a id="_idIndexMarker489"/> service is a way of declaring how your application exposes its interfaces to the world. It typically defines a network port that other Kubernetes pods can use to communicate with your application.</p>
			<p>The Helm Chart for ShipIt Clicker emits a service template that defines a <code>ClusterIP</code> service definition:</p>
			<pre>$ helm template shipitclicker ./shipitclicker | less
…
# Source: shipitclicker/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: shipitclicker
  labels:
    helm.sh/chart: shipitclicker-0.1.10
    app.kubernetes.io/name: shipitclicker
    app.kubernetes.io/instance: shipitclicker
    app.kubernetes.io/version: "0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8008
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: shipitclicker
    app.kubernetes.io/instance: shipitclicker</pre>
			<p>This declaration<a id="_idIndexMarker490"/> describes the fact that ShipIt Clicker exposes HTTP on port <code>8008</code> as a service on each pod. This lets other Kubernetes services discover and make connections to it.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor174"/>Ingress Controllers</h2>
			<p>Kubernetes<a id="_idIndexMarker491"/> manages an internal network where the applications in a cluster can communicate with one another via a private network. By default, there is no way to reach applications running on the inside of a Kubernetes cluster from the outside. The Ingress Controller plays the role of a proxy and connection broker. Depending on whether you are deploying on-premises or in the cloud, different types of Ingress Controller have different uses. For example, earlier in this chapter, we installed the <code>nginx-ingress</code> Ingress Controller to allow us to reach applications running on our local Kubernetes installation. That controller is also useful when you want a vendor-neutral way of granting access to Kubernetes applications.</p>
			<p>Other Ingress Controllers allow Kubernetes to work smoothly with different types of external load balancers, such as <code>aws-alb-ingress-controller</code>, which enables the use of an <code>k8s-bigip-ctlr</code>, which <a id="_idIndexMarker492"/>enables the use of F5 BIG-IP load balancers, which are found in many data centers.</p>
			<p>You can use Ingress Controllers to map domain names and HTTP paths to Kubernetes services. This makes it really easy to expose different services at different URLs. If you had a fleet of microservices, you could expose them at different API endpoints using this pattern. You can take advantage of Ingress Controllers by declaring an ingress object for your application that advertises how to connect your service to the outside world. For the ShipIt Clicker<a id="_idIndexMarker493"/> example, we use the following to map the service to <code>localhost</code> in the default namespace:</p>
			<pre>$ helm template shipitclicker ./shipitclicker | less
…
# Source: shipitclicker/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: shipitclicker
  labels:
    helm.sh/chart: shipitclicker-0.1.10
    app.kubernetes.io/name: shipitclicker
    app.kubernetes.io/instance: shipitclicker
    app.kubernetes.io/version: "0.4.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
spec:
  rules:
    - host: "localhost"
      http:
        paths:
          - path: /
            backend:
              serviceName: shipitclicker
              servicePort: 8008
…</pre>
			<p>The Kubernetes system handles connections to applications hosted inside the cluster from the outside using this Ingress Controllers definition. This means that when you are first developing <a id="_idIndexMarker494"/>your application, you do not need to worry about how it is connected to the outside world. The Kubernetes configurations that enable Ingress Controllers can all be managed with Helm Charts, too.</p>
			<p>Next, we will examine how Kubernetes deals with sensitive information – using secrets.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor175"/>Secrets</h2>
			<p>Every application has values that need to be protected, from database passwords to API keys, so having a<a id="_idIndexMarker495"/> mechanism to store and retrieve them securely is an important function. In Kubernetes, this is handled with a mechanism called secrets. You can use a combination of configuration files and <code>kubectl</code> commands for sharing and modifying information that needs to be protected with your pods and their running containers. Once you have created a secret, you can use it in your application through a variety of mechanisms, including exposing a secret as an environment variable or creating a file that containers running in a pod can retrieve.</p>
			<p>The key operations in Kubernetes related to secrets are as follows:</p>
			<ul>
				<li>Creating a secret</li>
				<li>Describing a secret</li>
				<li>Retrieving a secret</li>
				<li>Editing a secret</li>
			</ul>
			<p>Let's explore these four concepts, starting with creating a secret.</p>
			<h3>Creating a secret</h3>
			<p>We can use several <a id="_idIndexMarker496"/>procedures to create a secret. This could be done by adding it manually on the command line or storing it in a YAML template file and using it from there.</p>
			<p>To add a secret stored in a text document via the command line, we can use the following commands:</p>
			<pre>$ echo "new-secret" &gt; secret.txt
$ kubectl create secret generic secex --from-file=./secret.txt</pre>
			<p>If we do this, <code>kubectl</code> will take care of encoding the secret for us using Base64 encoding. </p>
			<p>Let's prepare a secret another way, with a configuration file. In order to prepare a text secret for this file, it must be Base64-encoded. You can do that from the command line in macOS or Linux with the following command:</p>
			<pre>$ echo -n "changed-api-key" | base64
Y2hhbmdlZC1hcGkta2V5</pre>
			<p>If we wanted to instead store the secret in a configuration file, and use <code>kubectl</code> to add it to Kubernetes, we could create the following <code>secret-api-token.yaml</code> file:</p>
			<pre>---
apiVersion: v1
kind: Secret
metadata:
  name: api-token
  namespace: default
type: Opaque
data:
  token: "Y2hhbmdlZC1hcGkta2V5"</pre>
			<p>Then, using the <code>kubectl apply</code> command-line option, we can create the secret:</p>
			<pre>kubectl apply –f ./secret-api-token.yaml</pre>
			<p>You will notice that the configuration file format for the secret is very similar to the example ConfigMap we examined.</p>
			<p>Because <code>shipitclicker</code> uses Helm to manage its Kubernetes objects, it has support for secrets built into its templates. The one secret it references in the code in this chapter is related to a Node.js server-side framework setting for the Express framework used by the sample<a id="_idIndexMarker497"/> application that deals with server sessions. This secret is called <code>SESSION_SECRET</code>, and it is stored in the <code>chapter8/shipitclicker/templates/secrets.yaml</code> file:</p>
			<pre>---
apiVersion: v1
kind: Secret
metadata:
  name: {{ .Release.Name}}-secrets
  namespace: {{ .Release.Namespace }}
type: Opaque
data:
  SESSION_SECRET: "bXlTZWNyZXQtdjQK"</pre>
			<p>Notice that this uses template expressions for <code>name</code> and <code>namespace</code> in order to align with the other templates that Helm transforms.</p>
			<p>We created this secret when we installed the <code>shipitclicker</code> Helm template earlier in the chapter when we used the <code>helm install</code> command. That is how you create secrets when you use a Helm template.</p>
			<p>Now that we have seen several ways of creating secrets, we will show how we ask Kubernetes<a id="_idIndexMarker498"/> what secrets it knows about.</p>
			<h3>Describing a secret</h3>
			<p>Once a secret has <a id="_idIndexMarker499"/>been created, you can list it using the <code>kubectl get secrets</code><strong class="bold"> </strong>command. This will list the secrets in a similar way to this:</p>
			<div><div><img src="img/B11641_08_004.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 8.4 – List of secrets</p>
			<p>To learn more about the secret, use the <code>kubectl describe</code> command:</p>
			<pre>kubectl describe secrets/shipitclicker-secrets</pre>
			<p>The output of the preceding command is shown in the following screenshot:</p>
			<div><div><img src="img/B11641_08_005.jpg" alt="Figure 8.5 – Output of the kubectl describe command showing the secret's metadata&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Output of the kubectl describe command showing the secret's metadata</p>
			<p>You will see metadata about your secret displayed, including the key of the secret – in this case, <code>SESSION_SECRET</code>. It will <a id="_idIndexMarker500"/>not show the value of the secret, though.</p>
			<h3>Retrieving a secret</h3>
			<p>A typical way for a <a id="_idIndexMarker501"/>Kubernetes application to retrieve a simple secret is to define it as an environment variable passed to the container referencing the secret. See this excerpt from the rendered Helm chart templates:</p>
			<pre># Source: shipitclicker/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shipitclicker …      containers:
        - name: shipitclicker
…
          env:…            - name: REDIS_PORT
              valueFrom:
                configMapKeyRef:
                  name: shipitclicker-configmap
                  key: REDIS_PORT
            - name: SESSION_SECRET
              valueFrom:
                secretKeyRef:
                  name: shipitclicker-secrets
                  key: SESSION_SECRET</pre>
			<p>You can see that the environment variables mapped to the deployment for the <code>shipitclicker</code> container reference both the <code>configMapKeyRef</code> and <code>secretKeyRef</code> entries.</p>
			<p>To deal with more complex secrets that are complete files, such as SSH private keys, the mechanism is similar. See the Kubernetes secrets documentation for more scenarios at <a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a>.</p>
			<p>For troubleshooting purposes, we can retrieve a secret from Kubernetes from the command line:</p>
			<pre>$ template='go-template={{index .data "SESSION_SECRET"}}'
$ kubectl get secrets shipitclicker-secrets -o "$template" | base64 -D
mySecret-v4</pre>
			<p>Now that we have <a id="_idIndexMarker502"/>seen how to retrieve a secret, we will examine how to edit secrets.</p>
			<h3>Editing secrets</h3>
			<p>If you wish to edit<a id="_idIndexMarker503"/> the secret after creating it, use the <code>kubectl edit</code> command:</p>
			<pre>kubectl edit secrets secex</pre>
			<p>This will open your default editor (by default, vi) and you can edit the secret. You will have to have the Base64-encoded replacement value ready. It will look something like this:</p>
			<pre>apiVersion: v1
data:
  secret.txt: Y2hhbmdlZC1hcGkta2V5LTI=
kind: Secret
metadata:
  creationTimestamp: "2020-04-25T20:54:31Z"
  name: secex
  namespace: default
  resourceVersion: "826562"
  selfLink: /api/v1/namespaces/default/secrets/sample-secret
  uid: ce8fbf27-33ba-461e-9bb8-1ca31fa3e888
type: Opaque</pre>
			<p>You can edit secrets directly this way. You might need to redeploy your application after updating a secret, depending on how it uses that secret. Having to manage this by hand can get<a id="_idIndexMarker504"/> complicated, which is one of the reasons why we use Helm to package applications. </p>
			<h3>Updating the ShipIt Clicker session secret</h3>
			<p>For applications<a id="_idIndexMarker505"/> deployed with Helm, it is usual <a id="_idIndexMarker506"/>practice to make changes through the Helm templates instead of using raw <code>kubectl</code> commands. Now, we will change the ShipIt Clicker <code>SESSION_SECRET</code> key using Helm by following this procedure:</p>
			<ol>
				<li value="1">Generate a Base64-encoded secret with the following command: <pre><strong class="bold">echo -n "new-session-secret" | base64</strong></pre></li>
				<li>Edit the template <code>chapter8/shipitclicker/templates/secrets.yaml</code> file.</li>
				<li>Use the value outputted by the <code>openssl</code> command for the new <code>SESSION_SECRET</code> value.</li>
				<li>Edit the <code>chapter8/shipitclicker/Chart.yaml</code> file and increment the chart's <code>version</code> number.</li>
				<li>You have to do this every time you update a Helm Chart. Then, update the template with the following command:<pre><strong class="bold">helm upgrade shipitclicker ./shipitclicker</strong></pre></li>
			</ol>
			<p>As you can see, the basic commands to add and edit secrets are very simple. Using them in our application is slightly more complex. This should give you a taste of how to create a secret value and retrieve information on it to explore the feature.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For further information on secrets, you can check out the latest Kubernetes documentation at <a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a>.</p>
			<p>In <a href="B11641_14_Final_NM_ePub.xhtml#_idTextAnchor316"><em class="italic">Chapter 14</em></a>, <em class="italic">Advanced Docker Security – Secrets, Secret Commands, Tagging, and Labels</em>, we look into secret storage and usage in relation to Docker Swarm. While Docker Swarm is falling out of favor, with many teams switching to Kubernetes, it is important to<a id="_idIndexMarker507"/> understand these concepts<a id="_idIndexMarker508"/> when maintaining legacy systems. Additionally, you may find yourself in a position where you have to migrate systems from Docker Swarm to Kubernetes. The information provided in this chapter and <a href="B11641_14_Final_NM_ePub.xhtml#_idTextAnchor316"><em class="italic">Chapter 14</em></a>, <em class="italic">Advanced Docker Security – Secrets, Secret Commands, Tagging, and Labels</em>, should help you map concepts from one technology to the other.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor176"/>Namespaces</h2>
			<p>In order to partition<a id="_idIndexMarker509"/> resources within Kubernetes, we can use a concept called namespaces. Namespaces provide a mechanism to group container resources into non-overlapping sets, which then allows you to subdivide your Kubernetes resources, based on your business needs, within the same cluster. This could include everything from environments (development, staging, and production) to groups of microservices. One important factor you should consider is that applications in the same namespace can read any secret in that namespace, so it represents a security boundary as well.</p>
			<p>It is tempting, once you learn of this feature, to want to use it everywhere, but the Kubernetes documentation cautions against this. The main namespaces content page (<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/</a>) states the following: </p>
			<p>"<em class="italic">For clusters with a few to tens of users, you should not need to create or think about namespaces at all</em>."</p>
			<p>Keep in mind, though, that different teams might want to segregate applications from one another, and namespaces are a good way to do that as they provide a security boundary. Later in this chapter, in the <em class="italic">Using labels and namespaces to segregate environments</em> section, we will explore using this concept to deploy our application to both a staging <a id="_idIndexMarker510"/>and production environment in AWS.</p>
			<p>Next, let's set up AWS EKS with CloudFormation in order to deploy our application to the public cloud using Kubernetes.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor177"/>Spinning up AWS EKS with CloudFormation</h1>
			<p>Now that we<a id="_idIndexMarker511"/> have walked through a local<a id="_idIndexMarker512"/> installation of Kubernetes and explored some of the cloud vendor options, we are going to try deploying containers to an AWS-hosted Kubernetes environment. This will be the EKS service we briefly introduced in the previous section of this chapter.</p>
			<p>In order to achieve this, we will describe how to create and manage an EKS cluster using AWS CloudFormation, their infrastructure-as-code service. For more information on CloudFormation, be sure to check out the AWS guides and documentation at <a href="https://docs.aws.amazon.com/cloudformation/">https://docs.aws.amazon.com/cloudformation/</a>.</p>
			<p>Assuming you have previously created an AWS account or followed the instructions under the <em class="italic">Technical requirements</em> section of this chapter, load up the AWS cloud console.</p>
			<p>To proceed, we need to set up EKS. There are many ways to get a working EKS cluster that require varying amounts of work:</p>
			<ul>
				<li>Set up everything by hand, step by step through the AWS console. We <em class="italic">do not recommend</em> this approach as it requires deep AWS knowledge to carry out correctly, and will lead to a hard-to-replicate environment with poor controls.</li>
				<li>Write infrastructure-as-code templates from scratch in either AWS CloudFormation or Terraform to control all the resources needed. This is an approach that might work for you if you are an expert in either CloudFormation or Terraform and have an existing investment in CloudFormation or Terraform tooling, but we <em class="italic">do not recommend this for beginners</em>.</li>
				<li>Use the <code>eksctl</code> tool (see <a href="https://eksctl.io">https://eksctl.io</a>) to <a id="_idIndexMarker513"/>create a cluster with a simple CLI tool. This could work well if you are already familiar with AWS and want to put your cluster in a specific region and tweak more of the parameters of your cluster. We <em class="italic">only recommend this if you are familiar with AWS and EKS already</em>.</li>
				<li>Research and adopt infrastructure-as-code templates that someone else has already written. Both AWS and many other people have created CloudFormation <a id="_idIndexMarker514"/>and Terraform<a id="_idIndexMarker515"/> templates.</li>
			</ul>
			<p>We are going to follow this last approach and use the AWS Quick Start CloudFormation templates for EKS to create our first cloud Kubernetes cluster.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor178"/>Introducing the AWS EKS Quick Start CloudFormation templates</h2>
			<p>Amazon provides a <a id="_idIndexMarker516"/>handy set of CloudFormation templates called Quick Starts, built by their expert cloud architects to quickly get you up and running for a wide selection of AWS services and scenarios (<a href="https://aws.amazon.com/quickstart/">https://aws.amazon.com/quickstart/</a>).</p>
			<p>We will be using an AWS EKS Quick Start template for the next section of this chapter.</p>
			<p>However, before you deploy the EKS Quick Start CloudFormation templates, please take a moment to prepare your AWS account for deployment.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor179"/>Preparing an AWS account </h2>
			<p>If you are just<a id="_idIndexMarker517"/> starting to use AWS, there are a few critical things to take care of before you proceed in order to protect your account. These precautions and preparations also apply if you choose a method other than using the AWS Quick Start CloudFormation templates to create your EKS cluster.</p>
			<p>If you are already an<a id="_idIndexMarker518"/> experienced AWS user and have an AWS <code>us-east-2</code> region, and you know your public IPv4 address, you can skip ahead to the <em class="italic">Launching the AWS EKS Quick Start CloudFormation templates</em> section. Avoid using an assumed IAM role with administrative privileges to create the CloudFormation template, though – that can cause some of the child templates to enter an <code>UPDATE_ROLLBACK_FAILED</code> state, which is difficult<a id="_idIndexMarker519"/> to recover from. </p>
			<h3>Using an IAM administrator user and not the root account user</h3>
			<p>First of all, ensure<a id="_idIndexMarker520"/> that you are not using the AWS <a id="_idIndexMarker521"/>console as the root account user. This is a major security risk. You will need an AWS IAM user account with administrative privileges. If you have just created your AWS root account, you can set one up by following the AWS instructions at <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html</a>.</p>
			<p>Once you have set up this user and enabled billing access for the IAM user as per instructions, go to the <a href="https://console.aws.amazon.com/iam/home#/home">https://console.aws.amazon.com/iam/home#/home</a> page and copy the IAM user's sign-in link to the clipboard. Edit your web browser bookmarks and use this URL to create an <strong class="bold">AWS IAM Login</strong> item. You will want to use this to sign in to your AWS account with your administrator account instead of using the root account.</p>
			<p>On your local system, create an <code>eks-notes.txt</code> file and record the sign-in link there. Also, record the <strong class="bold">User ARN</strong> value of the administrator user from the <a href="https://console.aws.amazon.com/iam/home?region=us-east-2#/users/Administrator">https://console.aws.amazon.com/iam/home?region=us-east-2#/users/Administrator</a> URL:</p>
			<div><div><img src="img/B11641_08_006.jpg" alt="Figure 8.6 – AWS IAM user summary for the administrative user&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – AWS IAM user summary for the administrative user</p>
			<p>This <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>) user is <a id="_idIndexMarker522"/>a string, much<a id="_idIndexMarker523"/> like a web <strong class="bold">Uniform Resource Identifier</strong> (<strong class="bold">URI</strong>), but it is Amazon-specific. Now that we have set up an administrative user, let's set up <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>) to<a id="_idIndexMarker524"/> protect both the root<a id="_idIndexMarker525"/> account and the administrator user.</p>
			<h3>Setting up MFA</h3>
			<p>We recommend <a id="_idIndexMarker526"/>that you protect both the root account and every IAM user account with administrative privileges using MFA. If someone compromises your root account, they could create huge bills by launching expensive cloud resources, steal your information, or even delete all your data. When you are getting started, we recommend that you use MFA with a virtual MFA device and supporting software such as Google Authenticator, Authy, or 1Password.</p>
			<p>For added security, you have the option of using one of the supported hardware token solutions, but virtual MFA works fine. Please see the AWS MFA documentation for more details on setting up MFA:</p>
			<p><a href="https://aws.amazon.com/iam/features/mfa/">https://aws.amazon.com/iam/features/mfa/</a></p>
			<h3>Signing in to the AWS console with the IAM user account</h3>
			<p>Ensure you<a id="_idIndexMarker527"/> have signed out of the root <a id="_idIndexMarker528"/>account. Then, use the sign-in URL from your <code>eks-notes.txt</code> document to sign in to the AWS console with your administrator IAM user account before proceeding.</p>
			<h3>Creating access keys for the IAM administrator user</h3>
			<p>In order to use<a id="_idIndexMarker529"/> the AWS command-line tools, you will need to generate AWS access keys. You can read more about access keys and other types of AWS credentials at <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html">https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html</a>.</p>
			<p>In the AWS console, go to the IAM service and look in the <strong class="bold">Users</strong> section for the administrator user you just created. Then, navigate to the <strong class="bold">Security credentials</strong> tab and create new access keys by pressing the <strong class="bold">Create access key</strong> button:</p>
			<div><div><img src="img/B11641_08_007.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7– AWS IAM user summary for an administrative user</p>
			<p>Download these <a id="_idIndexMarker530"/>access keys as a CSV file to your local system. You will need to open that file and examine the keys in order to configure the AWS CLI, which we will do next.</p>
			<h3>Configuring the AWS CLI on your local workstation</h3>
			<p>You are going to<a id="_idIndexMarker531"/> need a working AWS CLI installation on your local workstation to complete the configuration of the EKS cluster. If you don't already have this<a id="_idIndexMarker532"/> installed, follow the instructions to install it at <a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>.</p>
			<p>Once it is installed, issue the <code>aws configure</code> command and use the access ID and secret key from the access key's CSV file you saved in the previous section to configure the CLI to use the administrator user. Verify that it works with the <code>aws sts get-caller-identity</code> command. Inspect the output to make sure that it does not show an error message, and then verify that the ARN that this command emits for the active user is the same one as for the administrator user shown in the IAM web console. The output should look something like this:</p>
			<div><div><img src="img/B11641_08_008.jpg" alt="Figure 8.8 – Output of aws sts get-caller-identity&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Output of aws sts get-caller-identity</p>
			<p>You will need <a id="_idIndexMarker533"/>this set up when you configure the cluster for the ALB Ingress Controller later in the chapter.</p>
			<h3>Creating an EC2 key pair for the EKS cluster</h3>
			<p>In order to<a id="_idIndexMarker534"/> perform the initial configuration of the EKS cluster, you <a id="_idIndexMarker535"/>will need to SSH to an EC2 virtual server that the CloudFormation template sets up, known as the bastion host. A <code>us-east-2</code> region. Signed in as your IAM administrator user, go to <a href="https://console.aws.amazon.com/ec2">https://console.aws.amazon.com/ec2</a>, and then make sure you switch your region to <strong class="bold">us-east-2</strong> from the region picker:</p>
			<div><div><img src="img/B11641_08_009.jpg" alt="Figure 8.9 – Switching your AWS region&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Switching your AWS region</p>
			<p>Then, find and click on the key pairs link in the menu on the left, create a new key pair called <code>ec2-eks</code>, and download it. You will need this key pair when you configure the EKS cluster. To prepare for that, copy this key pair to the <code>.ssh</code> directory under your local user home directory and set its permissions so that SSH will allow its use:</p>
			<pre>$ mkdir -p ~/.ssh
$ chmod 0700 ~/.ssh
$ cp ~/Downloads/ec2-eks.pem ~/.ssh/
$ chmod 0600 ~/.ssh/ec2-eks.pem</pre>
			<p>You will need this key to connect to the bastion host for your EKS cluster later. Next, make sure<a id="_idIndexMarker537"/> you <a id="_idIndexMarker538"/>know your public IP address.</p>
			<h3>Recording your public IP address in CIDR notation</h3>
			<p>We are going<a id="_idIndexMarker539"/> to restrict access from the internet to the Kubernetes<a id="_idIndexMarker540"/> cluster by restricting it to just the pubic IPv4 address you are currently using. This will keep malicious hackers and people who attack internet hosts from scanning your system. To do this, go to <a href="https://whatismyip.com/">https://whatismyip.com/</a> and copy your public IPv4 address in CIDR format, which is the raw numerical address with <code>/32</code> appended. For example, if it was <code>192.2.0.15</code>, the CIDR form of your IPv4 address would be <code>192.2.0.15/32</code>. On your local system, open your <code>eks-notes.txt</code> file and record the CIDR address there.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor180"/>Launching the AWS EKS Quick Start CloudFormation templates</h2>
			<p>You can find the<a id="_idIndexMarker541"/> documentation<a id="_idIndexMarker542"/> on the AWS EKS Quick Start CloudFormation templates at <a href="https://aws.amazon.com/quickstart/architecture/amazon-eks/">https://aws.amazon.com/quickstart/architecture/amazon-eks/</a>.</p>
			<p>To get a complete picture of what this offers, read the deployment guide that AWS offers related to this quick start: </p>
			<p><a href="https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/welcome.html">https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/welcome.html</a></p>
			<p>At a minimum, review the outline on that page. When you want to proceed with deployment, click on the <strong class="bold">How to Deploy</strong> section. You will see that you have two options when deploying the CloudFormation templates, as follows:</p>
			<ul>
				<li><strong class="bold">Deploy to a new VPC</strong> (<a href="https://fwd.aws/6dEQ7">https://fwd.aws/6dEQ7</a>)</li>
				<li><strong class="bold">Deploy to an existing VPC</strong> (<a href="https://fwd.aws/e37MA">https://fwd.aws/e37MA</a>)</li>
			</ul>
			<p>Before you begin, sign out of the AWS console if you are still signed in with the root account user, and sign in as a administrator user using the IAM sign-in URL you recorded in the <code>eks-notes.txt</code> file.</p>
			<p>We recommend that<a id="_idIndexMarker543"/> you start by deploying this infrastructure to a new <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>). Click on that link or use the preceding URL to go to the CloudFormation stack creation forms. Most of the items in these forms can be left at their defaults, but some must be filled out both to complete initial cluster configuration and to ensure that you <a id="_idIndexMarker544"/>do not accidentally create an unsecure configuration.</p>
			<h3>Guidance for EKS Quick Start CloudFormation creation</h3>
			<p>Creating the<a id="_idIndexMarker545"/> CloudFormation stack will require you to fill out a four-page CloudFormation parameters form by following the <strong class="bold">Deploy into a new VPC</strong> link in the previous section. This is the first page of that form:</p>
			<div><div><img src="img/B11641_08_010.jpg" alt="Figure 8.10 – CloudFormation form, page 1 of 4: Prepare template&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – CloudFormation form, page 1 of 4: Prepare template</p>
			<p>This guidance will <a id="_idIndexMarker546"/>allow you to complete the items to get a working EKS cluster in about 30 minutes.</p>
			<h4>Create Stack – Prerequisite – Prepare Template</h4>
			<p>Leave all the<a id="_idIndexMarker547"/> items on this form at their defaults and hit the <strong class="bold">Next</strong> button. This will take you to the <strong class="bold">Specify Stack Details</strong> screen.</p>
			<h4>Specify Stack Details</h4>
			<p>You can leave<a id="_idIndexMarker548"/> almost all of these items at their defaults, but specify items for the following parameters:</p>
			<ul>
				<li><code>us-east-2a</code>, <code>us-east-2b</code>, and <code>us-east-2c</code>.</li>
				<li><code>192.2.0.15/32</code>.</li>
				<li><strong class="bold">EKS cluster name</strong>: Choose a short cluster name.</li>
				<li><code>8</code>.</li>
				<li><code>eks-ec2</code>.</li>
				<li><strong class="bold">Additional EKS admin ARN (IAM Role)</strong>: Leave this blank, unless you have another AWS IAM role in your account that you want to give access to.</li>
				<li><strong class="bold">Additional EKS admin ARN (IAM User)</strong>: Leave this blank, unless you have another AWS IAM user in your account that you want to give access to.</li>
				<li><strong class="bold">Kubernetes Version</strong>: 1.15.<p class="callout-heading">Note</p><p class="callout">Do not use 1.16 or higher if you want to experiment with Spinnaker as described in <a href="B11641_09_Final_NM_ePub.xhtml#_idTextAnchor191"><em class="italic">Chapter 9</em></a>, <em class="italic">Cloud-Native Continous Deployment Using Spinnaker</em>, as Spinnaker is not compatible with higher versions</p></li>
				<li><strong class="bold">EKS Public Access Endpoint</strong>: Enabled.</li>
				<li><code>192.2.0.15/32</code>.</li>
				<li><strong class="bold">ALB Ingress Controller</strong>: Enabled.</li>
				<li><strong class="bold">Cluster Autoscaler</strong>: Enabled.</li>
				<li><strong class="bold">EFS Storage Class</strong>: Enabled.</li>
				<li><strong class="bold">Monitoring Stack</strong>: Prometheus and Grafana.</li>
			</ul>
			<p>Selecting these options will ultimately allow you to manage the EKS cluster from your local workstation<a id="_idIndexMarker549"/> using the <code>kubectl</code>, <code>helm</code>, and <code>eksctl</code> tools. Once these are specified, press the <strong class="bold">Next</strong> button at the bottom of the form. This will take you to the <strong class="bold">Configure Stack Options</strong> screen.</p>
			<h4>Configure Stack Options</h4>
			<p>Leave all of <a id="_idIndexMarker550"/>these at their defaults. Press the <strong class="bold">Next</strong> button at the bottom of the form. This will take you to the <strong class="bold">Review</strong> screen.</p>
			<h4>Review</h4>
			<p>Scroll to the bottom<a id="_idIndexMarker551"/> of the form and check both of the checkboxes acknowledging that this might create IAM resources with custom names and that it might require the <code>CAPABILITY_AUTO_EXPAND</code> capability. Press the <strong class="bold">Next</strong> button at the bottom of the form to create the CloudFormation template. Wait about 30 minutes and review the creation status of the template in the CloudFormation console—it should complete without issue. Check that all the CloudFormation templates reach the completed state before proceeding. It should look something like this:</p>
			<div><div><img src="img/B11641_08_011.jpg" alt="Figure 8.11 – The CloudFormation console with the CREATE_COMPLETE status&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – The CloudFormation console with the CREATE_COMPLETE status</p>
			<p>Now, your EKS <a id="_idIndexMarker552"/>cluster is ready for its initial configuration.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor181"/>Configuring the EKS cluster</h2>
			<p>Having deployed the<a id="_idIndexMarker553"/> CloudFormation template, you will have an environment that contains the following AWS services:</p>
			<ul>
				<li>A VPC that serves as networking infrastructure for the cluster</li>
				<li>An EKS Kubernetes control plane managed by AWS</li>
				<li>An EC2 bastion host used to configure the cluster</li>
				<li>Kubernetes infrastructure, including three EC2 instances serving as nodes deployed across three AWS availability zones</li>
				<li>An ALB Ingress Controller that will allow outside access to cluster services</li>
			</ul>
			<p>To gain initial access to the cluster, view the CloudFormation outputs for the stack and note the IPv4 address marked <code>BastionIP</code>. Then, SSH to the host with that address, replacing <code>192.2.10</code> with that IP address:</p>
			<pre>ssh -i ~/.ssh/eks-ec2.pem ec2-user@192.2.0.10</pre>
			<p>Once the deployment is complete, follow the AWS deployment guide to validate the cluster state:</p>
			<p><a href="https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/step-3.html">https://docs.aws.amazon.com/quickstart/latest/amazon-eks-architecture/step-3.html</a>.</p>
			<p>Use some of the commands you have learned about, such as <code>kubectl get all -A</code>, <code>kubectl get nodes</code>, and <code>kubectl describe service/kubernetes</code>, to explore the cluster configuration from the bastion host.</p>
			<p>The bastion node already has <code>kubectl</code>, <code>helm</code>, and <code>git</code> installed, so you have the option of using it to perform some cluster maintenance chores. The Helm installation even has the stable<a id="_idIndexMarker554"/> charts repository already installed, which you can verify with the <code>helm repo list</code> command.</p>
			<p class="callout-heading">Keep an eye on AWS costs</p>
			<p class="callout">Once you have deployed the EKS infrastructure, AWS will start charging you by the hour while it is running. You will be responsible for all charges incurred while the EKS cluster and EC2 servers are running. Keeping this EKS cluster running might cost up to <strong class="bold">$10-20 per day</strong>. Please visit the <strong class="bold">Billing &amp; Cost Management</strong> dashboard at <a href="https://console.aws.amazon.com/billing/home?#/">https://console.aws.amazon.com/billing/home?#/</a> in order to see your month-to-date and projected costs. We recommend that you have AWS generate cost and usage reports on a regular basis to help you track your spending. Information on enabling this can be found at <a href="https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html">https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html</a>.</p>
			<h3>Verifying that the ALB Ingress Controller is working</h3>
			<p>Because we enabled<a id="_idIndexMarker555"/> the ALB Ingress Controller optional add-in when we created the EKS cluster, we can skip the detailed directions in the ALB user guide (<a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html</a>) to set up an ALB Ingress Controller for EKS. Since the ALB Ingress Controller is already set up, the cluster will automatically be able to create new Ingress Controllers and application load balancers when it finds a correctly annotated ingress object.</p>
			<p>As an exercise, you can<a id="_idIndexMarker556"/> deploy the 2048 game described in the last section of the user guide to validate that the ALB works as expected.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor182"/>Deploying an application with resource limits to Kubernetes on AWS EKS</h1>
			<p>In Kubernetes, we<a id="_idIndexMarker557"/> can set resource limits on an application in order to prevent it from consuming all the available CPU and memory resources in the cluster. This is desirable to protect the system from resource exhaustion, and to ensure that an application that has a memory leak or a bug that causes it to consume more CPU than expected does not bring down the entire cluster.</p>
			<p>To demonstrate setting resource limits, we are going to deploy the ShipIt Clicker Docker container and Helm charts we deployed to our local Kubernetes installation in the<em class="italic"> Deploying a sample application</em> section earlier in this chapter to the EKS cluster. </p>
			<p>To demonstrate setting resource limits, we will now look at deploying the ShipIt Clicker application to Kubernetes, managed by the AWS EKS service, with CPU and memory limits enabled. We will also expose this application to the world using an Ingress Controller.</p>
			<p>Configuring resource limits to guard against memory leaks and runaway CPU usage</p>
			<p>Now that we <a id="_idIndexMarker558"/>are <a id="_idIndexMarker559"/>deploying to EKS, we want to be sure that our pod's containers are good citizens in the cluster. To do this, we will specify both resource requests and limits. Requests give Kubernetes guidance about how much of each resource it will initially allocate to the application, and will guide the orchestrator when it places the containers and pods on the nodes. Kubernetes will only schedule a pod on a node if it has adequate headroom to support a request. Limits give the orchestrator hard-maximum limits on<a id="_idIndexMarker560"/> how much CPU or memory to allocate. If a container exceeds its memory limit, its process will be killed with an <strong class="bold">out-of-memory</strong> (<strong class="bold">OOM</strong>) error. </p>
			<p>We are going to use the Helm templates at <code>chapter8/shipitclicker-eks/</code> in order to make<a id="_idIndexMarker561"/> the<a id="_idIndexMarker562"/> first set of changes versus the basic Helm template we installed on our local system.</p>
			<p>In <code>chapter8/shipitclicker-eks/values.yaml</code>, we are now specifying the CPU and memory requests and limits for the containers:</p>
			<pre>resources:
   limits:
     cpu: 500m
     memory: 512Mi
   requests:
     cpu: 500m
     memory: 512Mi </pre>
			<p>These apply both to the Redis and the ShipIt Clicker containers.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor183"/>Annotating ShipIt Clicker to use the ALB Ingress Controller</h2>
			<p>Some changes<a id="_idIndexMarker563"/> are required <a id="_idIndexMarker564"/>for the <code>chapter8/shipitclicker-eks/values.yaml</code> file to make sure that the Ingress Controller annotations are compatible with the EKS setup. We need to switch up the annotations so that they are targeted toward EKS. Also, we will remove the host restriction and make sure that the configuration for paths has a wildcard in it. Since we use a <code>ClusterIP</code> service point, we also need to use the <code>ip</code> target type for the ALB Ingress Controller:</p>
			<pre>ingress:
  enabled: true
  annotations:
<strong class="bold">    kubernetes.io/ingress.class: alb</strong>
<strong class="bold">    alb.ingress.kubernetes.io/scheme: internet-facing</strong>
<strong class="bold">    alb.ingress.kubernetes.io/target-type: ip</strong>
  hosts:
<strong class="bold">  #  - host: "*"</strong>
<strong class="bold">    - paths: ['/*']</strong></pre>
			<p>Without these<a id="_idIndexMarker565"/> annotations, the <a id="_idIndexMarker566"/>ALB Ingress Controller would have trouble connecting to the services.</p>
			<p>Deploying an EKS-ready ShipIt Clicker to EKS</p>
			<p>SSH to the<a id="_idIndexMarker567"/> bastion host, clone the repository, and <a id="_idIndexMarker568"/>deploy the software with Helm:</p>
			<pre>$ git clone https://github.com/PacktPublishing/Docker-for-Developers.git
$ cd Docker-for-Developers helm install shipitclicker chapter8/shipitclicker-eks/</pre>
			<p>Check in the AWS EC2 console for evidence that an elastic load balancer is getting created. It may take a few minutes to become available. When it does, enter its DNS name in a browser and you should see the ShipIt Clicker game.</p>
			<p>If you don't see it, troubleshoot by looking at the Ingress Controller logs:</p>
			<pre>kubectl logs -n kube-system   deployment.apps/alb-ingress-controller</pre>
			<p>Now that we have the ShipIt Clicker application deployed to EKS and exposed to the world with an ALB Ingress Controller, let's examine how we can segregate environments so that different <a id="_idIndexMarker569"/>Docker containers can run without<a id="_idIndexMarker570"/> interfering with each other.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor184"/>Using AWS Elastic Container Registry with AWS EKS</h1>
			<p>Using pu<a id="_idIndexMarker571"/>blic images stored in Docker Hub is<a id="_idIndexMarker572"/> fine for some applications, but for more sensitive applications, you might want to store your Docker containers in a private Docker registry. AWS provides just such a registry: <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>). You can read more about the basics of ECR on the main <a id="_idIndexMarker573"/>product website at <a href="https://aws.amazon.com/ecr/">https://aws.amazon.com/ecr/</a>.</p>
			<p>In order to get a Kubernetes cluster to use images from a private repository, you must configure the cluster with the right credentials so that it can pull images from the repository. The process for most repositories is in the Kubernetes documentation at <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</a>.</p>
			<p>However, AWS ECR uses an enhanced security system that relies on AWS IAM to grant temporary access tokens that are used to authenticate with ECR. Kubernetes has built-in support for this authentication process, as described in the documentation on images regarding using a private registry (<a href="https://kubernetes.io/docs/concepts/containers/images/#using-aws-ec2-container-registry">https://kubernetes.io/docs/concepts/containers/images/#using-aws-ec2-container-registry</a>).</p>
			<p>When using ECR with Kubernetes, you use an ECR identifier in the specification for the images used in pod configurations or their Helm templates. Instead of using the default Docker Hub image specifications, you can specify images using the following syntax:</p>
			<p><code>ACCOUNT.dkr.ecr.REGION.amazonaws.com/imagename:tag</code></p>
			<p>The AWS documentation on EKS explains that the worker nodes that run the pods must have the correct IAM policies applied via IAM roles in order to get authentication tokens and retrieve the images:</p>
			<p><a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html</a></p>
			<p>Fortunately, the AWS CloudFormation templates we used to set up the EKS cluster produce worker nodes that already have the correct permissions applied, as do all clusters set up using the <code>eksctl</code> tool, if you set up your cluster with that alternative path. The access control rules described in ECR on the preceding EKS web page will grant EKS nodes permission to read any images stored in any ECR repository on the account.</p>
			<p>So, to use ECR with EKS, all we should have to do is make sure our containers are pushed to an ECR repository in the same account with the EKS cluster, and that we use the ECR-style <a id="_idIndexMarker574"/>repository URIs as the identifiers<a id="_idIndexMarker575"/> for the containers that run in our Kubernetes pods.</p>
			<p>Next up, let's create an ECR repository so that we can prepare for integrating ECR and EKS.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor185"/>Creating an ECR repository</h2>
			<p>In a web browser, log in<a id="_idIndexMarker576"/> to the AWS console. Make sure you switch to the <code>us-east-2</code> region (the same region where your EKS cluster lives), and then click on the <strong class="bold">Services</strong> link and choose <strong class="bold">Elastic Container Registry</strong>. If you don't have any registries created yet, click on the <strong class="bold">Get Started</strong> button. The AWS console will prompt you for a namespace and repository.</p>
			<p>Alternatively, visit the following URL to start the creation process:</p>
			<p><a href="https://console.aws.amazon.com/ecr/create-repository?region=us-east-2">https://console.aws.amazon.com/ecr/create-repository?region=us-east-2</a></p>
			<p>Either way, you will see something like this:</p>
			<div><div><img src="img/B11641_08_012.jpg" alt="Figure 8.12 – The ECR Create repository form&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – The ECR Create repository form</p>
			<p>Leave the other <a id="_idIndexMarker577"/>settings at their defaults. After you create the repository, note the URI for your<a id="_idTextAnchor186"/><a id="_idTextAnchor187"/> repository; you will need it in order to push containers to the registry. You will see the URI on a screen that looks like this:</p>
			<div><div><img src="img/B11641_08_013.jpg" alt="Figure 8.13 – The ECR Repositories page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – The ECR Repositories page</p>
			<p>Then, click on the <strong class="bold">View push commands</strong> button. This will give you detailed instructions on how to use the AWS CLI to get temporary credentials that you can use to accomplish a Docker<a id="_idIndexMarker578"/> push to the ECR repository.</p>
			<h3>Exercise – pushing ShipIt Clicker to the ECR repository</h3>
			<p>Follow the<a id="_idIndexMarker579"/> instructions shown after clicking <a id="_idIndexMarker580"/>on the <code>REPO</code> value with the hostname of your ECR registry from the URI generated in the <strong class="bold">Create</strong> form):</p>
			<pre>$ cd Docker-for-Developers/chapter8	
$ REPO=143970405955.dkr.ecr.us-east-2.amazonaws.com
$ IMAGE=dockerfordevelopers/shipitclicker
$ aws ecr get-login-password --region us-east-2 | \
  docker login --username AWS --password-stdin $REPO
$ docker build -t $IMAGE:latest .
$ docker tag $IMAGE:latest $REPO/$IMAGE:latest
$ docker push $REPO/$IMAGE:latest </pre>
			<p>If this succeeds, you will see an output similar to the following:</p>
			<div><div><img src="img/B11641_08_014.jpg" alt="Figure 8.14 – A Docker push to ECR&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – A Docker push to ECR</p>
			<p>In the next chapter, we are going to use ECR to store Docker images that we build through Jenkins and deploy using Spinnaker and Helm. </p>
			<p>Now that we<a id="_idIndexMarker581"/> have seen how we might store Docker<a id="_idIndexMarker582"/> container images in an ECR repository, we will examine how we can segregate environments using labels and namespaces.</p>
			<p>Using labels and namespaces to segregate environments</p>
			<p>We learned earlier<a id="_idIndexMarker583"/> in this chapter what a namespace is. Now, we will explore how we can use both namespaces and labels to create separate environments in both a local environment and in an EKS cluster.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor188"/>Local example – labeled environments in the default namespace</h2>
			<p>Let's imagine you are developing the ShipIt Clicker application and want to keep a working <a id="_idIndexMarker584"/>stable environment deployed so that you can demonstrate it to others and compare new behaviors in code that you are changing to stable behavior. While you could use namespaces to segregate the application, it would be simpler to just deploy the Helm Chart again with deployments that have different labels. You can use multiple deployments with distinct labels, along with some template overrides, to accomplish this with Helm, without having to deal with the complexity of multiple namespaces.</p>
			<p>To do this, we need to do the following:</p>
			<ol>
				<li value="1">Define a hostname to use to reach the service.</li>
				<li>Configure the Ingress Controller for ShipIt Clicker to use that hostname.</li>
				<li>Configure and bump the chart version in <code>chapter8/shipitclicker/Chart.yaml</code>.</li>
				<li>Deploy the Helm Chart with a different name from the one already deployed, for example <code>shipit-stable</code>.</li>
				<li>Test that we can reach the alternative environment.</li>
			</ol>
			<p>Let's go through each of these steps in order to set up this stable environment using namespaces.</p>
			<p>Adding multiple hostnames to the local environment</p>
			<p>The time-tested way <a id="_idIndexMarker585"/>to add alternative names for your local environment is to edit your operating system hosts file – this is <code>/etc/hosts</code> on UNIX-inspired systems, such as Linux and macOS, or <code>C:\Windows\System32\Drivers\etc\hosts</code> on Windows systems. You must do so as a user with administrative privileges, though. You might add an entry such as <code>127.0.0.1 shipit-stable.internal.</code> to your <code>hosts</code> file, following some of the guidance at <a href="https://tools.ietf.org/html/rfc6762#appendix-G">https://tools.ietf.org/html/rfc6762#appendix-G</a> to pick a TLD that is unlikely to cause operational problems.</p>
			<p>However, there is an easier way to do this now. You can use a hostname of the <code>name.A.B.C.D.nip.io</code> form and it will map to whatever IP address you give, thanks to the free <a href="https://nip.io/">https://nip.io/</a> service. This enables the easy creation of <code>localhost</code> aliases as we can use <code>shipit-stable.127.0.0.1.nip.io</code> and similar names for local development.</p>
			<h3>Temporarily configuring the Helm Chart for the shipit-stable environment</h3>
			<p>Edit the <code>chapter8/shipitclicker/values.yaml</code> file to switch up the host so that it <a id="_idIndexMarker586"/>matches <code>shipit-stable.127.0.0.1.nip.io</code>, and bump the chart version. Then, use Helm to deploy the app using the command <code>helm install shipit-stable shipitclicker/</code>. You should then be able to see the application in your web browser by going to http://shipit-stable.127.0.0.1.<a href="http://nip.io/">nip.io/</a>.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor189"/>Staged environments – Dev, QA, staging, and production</h2>
			<p>In the EKS environment, you could also get a pretty good separation of environments just by deploying labeled stacks. You could label the stacks with a prefix or suffix name that indicates what environment they are. With ALB support, each separate service that is exposed to the world will get its own distinct load balancer, whether they are in different namespaces or not.</p>
			<p>But there are some cases where you would want to use namespaces. For example, if you host both production and non-production resources in the cluster, you could make it so that the namespaces for the non-production resources use quotes. Refer to <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">https://kubernetes.io/docs/concepts/policy/resource-quotas/</a> for more information on quotas.</p>
			<p class="callout-heading">Exercise</p>
			<p class="callout">Create a <code>qa</code> namespace with <code>kubectl</code> and use Helm to deploy ShipIt Clicker to that namespace. Then, set a memory quota on that namespace so that it never uses more than 1 GB of RAM.</p>
			<p>For even more advanced practices regarding namespaces, you should consult the best practices documentation at <a href="https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-organizing-with-namespaces">https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-organizing-with-namespaces</a>.</p>
			<p>Now that we have set up a separate environment that is segregated using namespaces, we have more flexibility in how we might deploy and manage our applications. Next, let's review what we have learned in this chapter.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor190"/>Summary</h1>
			<p>In this chapter, we learned all about Kubernetes and options for hosting it in the cloud. We walked through some of the cloud-hosting platforms on the market and then completed a quick overview of the key components of Kubernetes.</p>
			<p>Following this, we developed a process for deploying our Docker containers to AWS EKS, using AWS ECR as a Docker container registry. Here, you also got the chance to experiment with Amazon's CloudFormation technology, a platform for developing infrastructure as code.  </p>
			<p>Next, we studied Helm and Helm Charts and built on the ShipIt Clicker application. This was stood up in AWS with resource limits. </p>
			<p>You should now feel comfortable with repeating this process for another project if you wish!</p>
			<p>Now that our basic Kubernetes setup is ready to go, what other concerns do we need to address before we can use it for a scalable production project? We have seen how we can use Jenkins for continuous deployment, but it would be tedious to write all the scripts required to get the basic Jenkins system to manage a complex Kubernetes cluster and deploy applications to it reliably. </p>
			<p>This chapter has presented a simplified set of Helm Charts that generate Kubernetes configurations that result in a running application, but there are some refinements we must make in order to make the application production-ready, just as we did in previous chapters with Docker Compose.</p>
			<p>In the next chapter, we are going to introduce Spinnaker as a cloud-native CI/CD platform that will help us facilitate CI/CD for a Kubernetes for this exact task.</p>
			<p>Further reading</p>
			<p>These articles may help you get a better handle on some of the essential Kubernetes concepts:</p>
			<ul>
				<li>A gentle illustrated introduction to Kubernetes concepts through this tongue-in-cheek guide: <a href="https://www.cncf.io/the-childrens-illustrated-guide-to-kubernetes/">https://www.cncf.io/the-childrens-illustrated-guide-to-kubernetes/</a></li>
				<li>Another Cloud Native Computing Foundation illustrated guide to Kubernetes concepts featuring Phippy: <a href="https://www.cncf.io/phippy-goes-to-the-zoo-book/">https://www.cncf.io/phippy-goes-to-the-zoo-book/</a></li>
				<li>Why is Kubernetes getting so popular? See this blog article: <a href="https://stackoverflow.blog/2020/05/29/why-kubernetes-getting-so-popular/">https://stackoverflow.blog/2020/05/29/why-kubernetes-getting-so-popular/</a></li>
				<li>Many applications require you to use private Docker image registries, whether that is Docker Hub, AWS ECR, or something else. Read this to find out how to integrate registry secrets into your Kubernetes configuration files: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</a></li>
				<li>While this is targeted at customers of Digital Ocean using their Kubernetes service, it does an excellent job of explaining NGINX Ingress Controllers: <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-on-digitalocean-kubernetes-using-helm">https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-on-digitalocean-kubernetes-using-helm</a></li>
				<li>The user guide for EKS. This is chock full of super-detailed information about running EKS: <a href="https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html">https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html</a></li>
				<li>Deploy the Kubernetes dashboard. This is optional but will give you a nice web user interface to see more information about the cluster: <a href="https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html">https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html</a></li>
				<li>An example of an advanced configuration using Kubernetes namespaces might involve using the Kubernetes <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) system to further restrict how applications in different namespaces interact: <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></li>
				<li>Learn more about the options for EKS installations, including Terraform, using a hybrid strategy that mixes NGINX and ALB Ingress Controller, and more: <a href="https://medium.com/">https://medium.com/</a>@dmaas/setting-up-amazon-eks-what-you-must-know-9b9c39627fbc</li>
			</ul>
		</div>
	</body></html>