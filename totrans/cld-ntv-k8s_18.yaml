- en: '*Chapter 14*: Service Meshes and Serverless'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discusses advanced Kubernetes patterns. First, it details the in-vogue
    service mesh pattern, where observability and service-to-service discovery are
    handled by a sidecar proxy, as well as a guide to setting up Istio, a popular
    service mesh. Lastly, it describes the serverless pattern and how it can be applied
    in Kubernetes. The major case study in this chapter will include setting up Istio
    for an example application and service discovery, along with Istio ingress gateways.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a discussion of the sidecar proxy, which builds the foundation
    of service-to-service connectivity for service meshes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using sidecar proxies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a service mesh to Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing serverless on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool, along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  prefs: []
  type: TYPE_NORMAL
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14).
  prefs: []
  type: TYPE_NORMAL
- en: Using sidecar proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier in this book, a sidecar is a pattern where a Pod contains
    another container in addition to the actual application container to be run. This
    additional "extra" container is the sidecar. Sidecars can be used for a number
    of different reasons. Some of the most popular uses for sidecars are monitoring,
    logging, and proxying.
  prefs: []
  type: TYPE_NORMAL
- en: For logging, a sidecar container can fetch application logs from the application
    container (since they can share volumes and communicate on localhost), before
    sending the logs to a centralized logging stack, or parsing them for the purpose
    of alerting. It's a similar story for monitoring, where the sidecar Pod can track
    and send metrics about the application Pod.
  prefs: []
  type: TYPE_NORMAL
- en: With a sidecar proxy, when requests come into the Pod, they first go to the
    proxy container, which then routes requests (after logging or performing other
    filtering) to the application container. Similarly, when requests leave the application
    container, they first go to the proxy, which can provide routing out of the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, proxy sidecars such as NGINX only provide proxying for requests coming
    into a Pod. However, in the service mesh pattern, both requests coming into and
    leaving the Pod go through the proxy, which provides the foundation for the service
    mesh pattern itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram to see how a sidecar proxy can interact with
    an application container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Proxy sidecar](img/B14790_14_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Proxy sidecar
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the sidecar proxy is in charge of routing requests to and from
    the application container in the Pod, allowing for functionality such as service
    routing, logging, and filtering.
  prefs: []
  type: TYPE_NORMAL
- en: The sidecar proxy pattern is an alternative to a DaemonSet-based proxy, where
    a proxy Pod on each node handles proxying to other Pods on that node. The Kubernetes
    proxy itself is similar to a DaemonSet pattern. Using a sidecar proxy can provide
    more flexibility than using a DaemonSet proxy, at the expense of performance efficiency,
    since many extra containers need to be run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some popular proxy options for Kubernetes include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*NGINX*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HAProxy*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Envoy*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While NGINX and HAProxy are more traditional proxies, Envoy was built specifically
    for a distributed, cloud-native environment. For this reason, Envoy forms the
    core of popular service meshes and API gateways built for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to Envoy, let's discuss the installation of other proxies as sidecars.
  prefs: []
  type: TYPE_NORMAL
- en: Using NGINX as a sidecar reverse proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we specify how NGINX can be used as a sidecar proxy, it is relevant to
    note that in an upcoming Kubernetes release, the sidecar will be a Kubernetes
    resource type that will allow easy injection of sidecar containers to large numbers
    of Pods. Currently however, sidecar containers must be specified at the Pod or
    controller (ReplicaSet, Deployment, and others) level.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how we can configure NGINX as a sidecar, with the following
    Deployment YAML, which we will not create just yet. This process is a bit more
    manual than using the NGINX Ingress Controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve split the YAML into two parts for space reasons and trimmed some of
    the fat, but you can see it in its entirety in the code repository. Let''s start
    with the containers spec for our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nginx-sidecar.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we specify two containers, both our main app container, `myapp`,
    and the `nginx` sidecar, where we inject some configuration via volume mounts,
    as well as some TLS certificates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the `volumes` spec in the same file, where we inject some
    certs (from a secret) and `config` (from a `ConfigMap`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we need both a cert and a secret key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create the NGINX configuration using `ConfigMap`. The NGINX
    configuration looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'nginx.conf:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have some basic NGINX configuration. Importantly, we have
    the `proxy_pass` field, which proxies requests to a port on `127.0.0.1`, or localhost.
    Since containers in a Pod can share localhost ports, this acts as our sidecar
    proxy. We won't review all the other lines for the purposes of this book, but
    check the NGINX docs for more information about what each line means ([https://nginx.org/en/docs/](https://nginx.org/en/docs/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create the `ConfigMap` from this file. Use the following command
    to imperatively create the `ConfigMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's make our certificates for TLS in NGINX, and embed them in a Kubernetes
    secret. You will need the CFSSL (CloudFlare's PKI/TLS open source toolkit) library
    installed to follow these instructions, but you can use any other method to create
    your cert.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create the **Certificate Authority** (**CA**). Start with
    the JSON configuration for the CA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'nginxca.json:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use CFSSL to create the CA certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will require the CA config:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nginxca-config.json:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And we''ll also need a cert request config:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nginxcarequest.json:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can actually make our certs! Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As the final step for our cert secrets, create the Kubernetes secret from the
    certificate files'' output by means of the last `cfssl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can finally create our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to check the NGINX proxy functionality, let''s create a service to
    direct to our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nginx-sidecar-service.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, accessing any node of the cluster using `https` should result in a working
    HTTPS connection! However, since our cert is self-signed, browsers will display
    an *insecure* message.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've seen how NGINX can be used as a sidecar proxy with Kubernetes,
    let's move on to a more modern, cloud-native proxy sidecar – Envoy.
  prefs: []
  type: TYPE_NORMAL
- en: Using Envoy as a sidecar proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Envoy is a modern proxy built for cloud-native environments. In the Istio service
    mesh, which we'll review later in this chapter, Envoy acts as both a reverse and
    forward proxy. Before we get to Istio, however, let's try our hand at deploying
    Envoy as a proxy.
  prefs: []
  type: TYPE_NORMAL
- en: We will tell Envoy where to route various requests using routes, listeners,
    clusters, and endpoints. This functionality is what forms the core of Istio, which
    we will review later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through each of the Envoy configuration pieces to see how it all works.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy listeners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Envoy allows the configuration of one or more listeners. With each listener,
    we specify a port for Envoy to listen on, as well as any filters we want to apply
    to the listener.
  prefs: []
  type: TYPE_NORMAL
- en: Filters can provide complex functionality, including caching, authorization,
    **Cross-Origin Resource Sharing** (**CORS**) configuration, and more. Envoy supports
    the chaining of multiple filters together.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy routes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Certain filters have route configuration, which specifies domains from which
    requests should be accepted, route matching, and forwarding rules.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Cluster in Envoy represents a logical service where requests can be routed
    to based-on routes in listeners. A cluster likely contains more than one possible
    IP address in a cloud-native setting, so it supports load balancing configurations
    such as *round robin*.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy endpoints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, endpoints are specified within a cluster as one logical instance of
    a service. Envoy supports fetching a list of endpoints from an API (this is essentially
    what happens in the Istio service mesh) and load balancing between them.
  prefs: []
  type: TYPE_NORMAL
- en: In a production Envoy deployment on Kubernetes, it is likely that some form
    of dynamic, API-driven Envoy configuration is going to be used. This feature of
    Envoy is called xDS, and is used by Istio. Additionally, there are other open
    source products and solutions that use Envoy along with xDS, including the Ambassador
    API gateway.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this book, we will look at some static (non-dynamic) Envoy
    configuration; that way, we can pick apart each piece of the config, and you'll
    have a good idea of how everything works when we review Istio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now dive into an Envoy configuration for a setup where a single Pod
    needs to be able to route requests to two services, *Service 1* and *Service 2*.
    The setup looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Outbound envoy proxy](img/B14790_14_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Outbound envoy proxy
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Envoy sidecar in our application Pod will have configurations
    to route to two upstream services, *Service 1* and *Service 2*. Both services
    have two possible endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: In a dynamic setting with Envoy xDS, the Pod IPs for the endpoints would be
    loaded from the API, but for the purposes of our review, we will show the static
    Pod IPs in the endpoints. We will completely ignore Kubernetes Services and instead
    directly access Pod IPs in a round robin configuration. In a service mesh scenario,
    Envoy would also be deployed on all of the destination Pods, but we'll keep it
    simple for now.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how this network map is configured in an envoy configuration
    YAML (which you can find in its entirety in the code repository). This is, of
    course, very different from a Kubernetes resource YAML – we will get to that part
    later. The entire configuration has a lot of YAML involved, so let's take it piece
    by piece.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Envoy configuration files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First off, let''s look at the first few lines of our config—some basic information
    about our Envoy setup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Envoy-configuration.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we specify a port and address for Envoy''s `admin`. As with
    the following configuration, we are running Envoy as a sidecar so the address
    will always be local – `0.0.0.0`. Next, we start our list of listeners with an
    HTTPS listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, for each Envoy listener, we have a local address and port for
    the listener (this listener is an HTTPS listener). Then, we have a list of filters
    – though in this case, we only have one. Each envoy filter type has slightly different
    configuration, and we won''t review it line by line (check the Envoy docs for
    more information at [https://www.envoyproxy.io/docs](https://www.envoyproxy.io/docs)),
    but this particular filter matches two routes, `/service/1` and `/service/2`,
    and routes them to two envoy clusters. Still under our first HTTPS listener section
    of the YAML, we have the TLS configuration, including certs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this configuration passes in a `private_key` and a `certificate_chain`.
    Next, we have our second and final listener, an HTTP listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this configuration is quite similar to that of our HTTPS listener,
    except that it listens on a different port, and does not include certificate information.
    Next, we move into our cluster configuration. In our case, we have two clusters,
    one for `service1` and one for `service2`. First off, `service1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And next, `Service 2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of these clusters, we specify where requests should be routed, and
    to which port. For instance, for our first cluster, requests are routed to `http://service1:5000`.
    We also specify a load balancing policy (in this case, round robin) and a timeout
    for the connections. Now that we have our Envoy configuration, we can go ahead
    and create our Kubernetes Pod and inject our sidecar along with the envoy configuration.
    We''ll also split this file into two since it is a bit too big to understand as
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Envoy-sidecar-deployment.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this is a typical deployment YAML. In this case, we actually
    have two containers. First off is the Envoy proxy container (or sidecar). It listens
    on two ports. Next up, moving further down the YAML, we have a volume mount for
    that first container (to hold the Envoy config) as well as a start command and
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have our second container in the Pod, which is an application container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this application responds on port `5000`. Lastly, we also have
    our Pod-level volume definition to match the Envoy config volume mounted in the
    Envoy container. Before we create our deployment, we need to create a `ConfigMap`
    with our Envoy configuration. We can do this using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create our deployment with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need our downstream services, `service1` and `service2`. For this
    purpose, we will continue to use the `http-responder` open source container image,
    which will respond on port `5000`. The deployment and service specs can be found
    in the code repository, and we can create them using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can test our Envoy configuration! From our `my-service` container,
    we can make a request to localhost on port `8080`, with the `/service1` path.
    This should direct to one of our `service1` Pod IPs. To make this request we use
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve set up out services to echo their names on a `curl` request. Look at
    the following output of our `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've looked at how Envoy works with a static configuration, let's
    move on to a dynamic service mesh based on Envoy – Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a service mesh to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *service mesh* pattern is a logical extension of the sidecar proxy. By attaching
    sidecar proxies to every Pod, a service mesh can control functionality for service-to-service
    requests, such as advanced routing rules, retries, and timeouts. In addition,
    by having every request pass through a proxy, service meshes can implement mutual
    TLS encryption between services for added security and can give administrators
    incredible observability into requests in their cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several service mesh projects that support Kubernetes. The most popular
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Istio*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linkerd*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kuma*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Consul*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these service meshes has different takes on the service mesh pattern.
    *Istio* is likely the single most popular and comprehensive solution, but is also
    quite complex. *Linkerd* is also a mature project, but is easier to configure
    (though it uses its own proxy instead of Envoy). *Consul* is an option that supports
    Envoy in addition to other providers, and not just on Kubernetes. Finally, *Kuma*
    is an Envoy-based option that is also growing in popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring all the options is beyond the scope of this book, so we will stick
    with Istio, as it is often considered the default solution. That said, all of
    these meshes have strengths and weaknesses, and it is worth looking at each one
    when planning to adopt the service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Istio on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although Istio can be installed with Helm, the Helm installation option is no
    longer the officially supported installation method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we use the `Istioctl` CLI tool to install Istio with configuration
    onto our clusters. This configuration can be completely customized, but for the
    purposes of this book, we will just use the "demo" configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to installing Istio on a cluster is to install the Istio CLI
    tool. We can do this with the following command, which installs the newest version
    of the CLI tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll want to add the CLI tool to our path for ease of use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's install Istio! Istio configurations are called *profiles* and, as
    mentioned previously, they can be completely customized using a YAML file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this demonstration, we''ll use the inbuilt `demo` profile with Istio, which
    provides some basic setup. Install profile using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Istioctl profile installation output](img/B14790_14_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 14.3 – Istioctl profile installation output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since the sidecar resource has not been released yet as of Kubernetes 1.19,
    Istio will itself inject Envoy proxies into any namespace that is labeled with
    `istio-injection=enabled`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To label any namespace with this, run the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To test easily, label the `default` namespace with the preceding `label` command.
    Once the Istio components come up, any Pods in that namespace will automatically
    be injected with the Envoy sidecar, just like we created manually in the previous
    section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to remove Istio from the cluster, run the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should result in a confirmation message telling you that Istio has been
    removed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s deploy a little something to test our new mesh with! We will deploy
    three different application services, each with a deployment and a service resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Service Frontend
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Service Backend A
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Service Backend B
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here''s the Deployment for *Service Frontend*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s the Service for *Service Frontend*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The YAML for Service Backends A and B will be the same as *Service Frontend*,
    apart from swapping the names, image names, and selector labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have a couple of services to route to (and between), let's start
    setting up some Istio resources!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First thing''s first, we need a `Gateway` resource. In this case, we are not
    using the NGINX Ingress Controller, but that''s fine because Istio provides a
    `Gateway` resource that can be used for ingress and egress. Here''s what an Istio
    `Gateway` definition looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These `Gateway` definitions look pretty similar to ingress records. We have
    `name`, and `selector`, which Istio uses to decide which Istio Ingress Controller
    to use. Next, we have one or more servers, which are essentially ingress points
    on our gateway. In this case, we do not restrict the host, and we accept requests
    on port `80`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have a gateway for getting requests into our cluster, we can start
    setting up some routes. We do this in Istio using `VirtualService`. `VirtualService`
    in Istio is a set of routes that should be followed when requests to a particular
    hostname are made. In addition, we can use a wildcard host to make global rules
    for requests from anywhere in the mesh. Let''s take a look at an example `VirtualService`
    configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this `VirtualService`, we route requests to any host to our entry point at
    *Service Frontend* if it matches one of our `uri` prefixes. In this case, we are
    matching on the prefix, but you can use exact matching as well by swapping out
    `prefix` with `exact` in the URI matcher.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, now we have a setup fairly similar to what we would expect with an NGINX
    Ingress, with entry into the cluster dictated by a route match.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'However, what''s that `v1` in our route? This actually represents a version
    of our *Frontend Service*. Let''s go ahead and specify this version using a new
    resource type – the Istio `DestinationRule`. Here''s what a `DestinationRule`
    config looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, we specify two different versions of our frontend service in
    Istio, each looking at a label selector. From our previous Deployment and Service,
    you see that our current frontend service version is `v2`, but we could be running
    both in parallel! By specifying our `v2` version in the ingress virtual service,
    we tell Istio to route all requests to `v2` of the service. In addition, we have
    our `v1` version also configured, which is referenced in the previous `VirtualService`.
    This hard rule is only one possible way to route requests to different subsets
    in Istio.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we've managed to route traffic into our cluster via a gateway, and to a
    virtual service subset based on a destination rule. At this point, we are effectively
    "inside" our service mesh!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, from our *Service Frontend*, we want to be able to route to *Service Backend
    A* and *Service Backend B*. How do we do this? More virtual services is the answer!
    Let''s take a look at a virtual service for *Backend Service A*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, this `VirtualService` routes to a `v1` subset for our service,
    `service-backend-a`. We'll also need another `VirtualService` for `service-backend-b`,
    which we won't include in full (but looks nearly identical). To see the full YAML,
    check the code repository for `istio-virtual-service-3.yaml`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once our virtual services are ready, we require some destination rules! The
    `DestinationRule` for *Backend Service A* is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Istio-destination-rule-2.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: And the `DestinationRule` for *Backend Service B* is similar, just with different
    subsets. We won't include the code, but check `istio-destination-rule-3.yaml`
    in the code repository for the exact specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'These destination rules and virtual services add up to make the following routing
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Istio routing diagram](img/B14790_14_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – Istio routing diagram
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, requests from *Frontend Service* Pods can route to *Backend
    Service A version 1* or *Backend Service B version 3*, and each backend service
    can route to the other as well. These requests to Backend Service A or B additionally
    engage one of the most valuable features of Istio – mutual (two-way) TLS. In this
    setup, TLS security is maintained between any two points in the mesh, and this
    all happens automatically!
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at using serverless patterns with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing serverless on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Serverless patterns on cloud providers have quickly been gaining in popularity.
    Serverless architectures consist of compute that can automatically scale up and
    down, even scaling all the way to zero (where zero compute capacity is being used
    to serve a function or other application). **Function-as-a-Service** (**FaaS**)
    is an extension of the serverless pattern, where function code is the only input,
    and the serverless system takes care of routing requests to compute and scale
    as necessary. AWS Lambda, Azure Functions, and Google Cloud Run are some of the
    more popular FaaS/serverless options officially supported by cloud providers.
    Kubernetes also has many different serverless frameworks and libraries that can
    be used to run serverless, scale-to-zero workloads as well as FaaS on Kubernetes.
    Some of the most popular ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Knative*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kubeless*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenFaaS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fission*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A full discussion of all serverless options on Kubernetes is beyond the scope
    of this book, so we''ll focus on two different ones, which aim to serve two vastly
    different use cases: *OpenFaaS* and *Knative*.'
  prefs: []
  type: TYPE_NORMAL
- en: While Knative is highly extensible and customizable, it uses multiple coupled
    components that add complexity. This means that some added configuration is necessary
    to get started with an FaaS solution, since functions are just one of many other
    patterns that Knative supports. OpenFaaS, on the other hand, makes getting up
    and running with serverless and FaaS on Kubernetes extremely easy. Both technologies
    are valuable for different reasons.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter's tutorial, we will look at Knative, one of the most popular
    serverless frameworks, and one that also supports FaaS via its eventing feature.
  prefs: []
  type: TYPE_NORMAL
- en: Using Knative for FaaS on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, Knative is a modular set of building blocks for serverless
    patterns on Kubernetes. For this reason, it requires a bit of configuration before
    we can get to the actual functions. Knative can also be installed with Istio,
    which it uses as a substrate for routing and scaling serverless applications.
    Other non-Istio routing options are also available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Knative for FaaS, we will need to install both *Knative Serving* and
    *Knative Eventing*. While Knative Serving will allow us to run our serverless
    workloads, Knative Eventing will provide the pathway to make FaaS requests to
    these scale-to-zero workloads. Let''s accomplish this by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s install the Knative Serving components. We will begin by installing
    the CRDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can install the serving components themselves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we''ll need to install a networking/routing layer for Knative
    to use. Let''s use Istio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll need the gateway IP address from Istio. Depending on where you''re running
    this (in other words, AWS or locally), this value may differ. Pull it using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Knative requires a specific DNS setup for enabling the serving component. The
    easiest way to do this in a cloud setting is to use `xip.io` "Magic DNS," though
    this will not work for Minikube-based clusters. If you're running one of these
    (or just want to see all the options available), check out the Knative docs at
    [https://knative.dev/docs/install/any-kubernetes-cluster/](https://knative.dev/docs/install/any-kubernetes-cluster/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To set up Magic DNS, use the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we''ve installed Knative Serving, let''s install Knative Eventing
    to deliver our FaaS requests. First, we''ll need more CRDs. Install them using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, install the eventing components just like we did with serving:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this point, we need to add a queue/messaging layer for our eventing system
    to use. Did we mention that Knative supports lots of modular components?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To make things easy, let's just use the basic in-memory messaging layer, but
    it's good to know all the options available to you. As regards modular options
    for messaging channels, check the docs at [https://knative.dev/docs/eventing/channels/channels-crds/](https://knative.dev/docs/eventing/channels/channels-crds/).
    For event source options, you can look at [https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To install the `in-memory` messaging layer, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Thought we were done? Nope! One last thing. We need to install a broker, which
    will take events from the messaging layer and get them processed in the right
    place. Let''s use the default broker layer, the MT-Channel broker layer. You can
    install it using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, we are finally done. We have installed an end-to-end FaaS implementation
    via Knative. As you can tell, this was not an easy task. What makes Knative amazing
    is the same thing that makes it a pain – it offers so many different modular options
    and configurations that even when selecting the most basic options for each step,
    we've still taken a lot of time to explain the install. There are other options
    available, such as OpenFaaS, which are a bit easier to get up and running with,
    and we'll look into that in the next section! On the Knative side, however, now
    that we have our setup finally ready, we can add in our FaaS.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an FaaS pattern in Knative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have Knative set up, we can use it to implement an FaaS pattern
    where events will trigger some code running in Knative through a trigger. To set
    up a simple FaaS, we will require three things:'
  prefs: []
  type: TYPE_NORMAL
- en: A broker to route our events from an entry point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A consumer service to actually process our events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trigger definition that specifies when to route events to the consumer for
    processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First thing''s first, our broker needs to be created. This is simple and similar
    to creating an ingress record or gateway. Our `broker` YAML looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knative-broker.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can create a consumer service. This component is really just our application
    that is going to process events – our function itself! Rather than showing you
    even more YAML than you've already seen, let's assume our consumer service is
    just a regular old Kubernetes Service called `service-consumer`, which routes
    to a four-replica deployment of Pods running our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''re going to need a trigger. This determines how and which events
    will be routed from the broker. The YAML for a trigger looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knative-trigger.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In this YAML, we create a `Trigger` rule that any event that comes through our
    broker, `my-broker`, and has a type of `myeventtype`, will automatically be routed
    to our consumer, `service-consumer`. For full documentation on trigger filters
    in Knative, check out the docs at [https://knative.dev/development/eventing/triggers/](https://knative.dev/development/eventing/triggers/).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do we create some events? First, check the broker URL using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now finally test our FaaS solution. Let''s spin up a quick Pod from
    which we can make requests to our trigger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Now, from inside this Pod, we can go ahead and test our trigger, using `curl`.
    The request we need to make needs to have a `Ce-Type` header that equals `myeventtype`,
    since this is what our trigger requires. Knative uses headers in the form `Ce-Id`,
    `Ce-Type`, as shown in the following code block, to do the routing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `curl` request will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are sending a `curl` `http` request to the broker URL. Additionally,
    we are passing some special headers along with the HTTP request. Importantly,
    we are passing `type=myeventtype`, which our filter on our trigger requires in
    order to send the request for processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, our consumer service echoes back the payload key of the body
    JSON, along with a `200` HTTP response, so running this `curl` request gives us
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Success! We have tested our FaaS and it returns what we are expecting. From
    here, our solution will scale up and down to zero along with the number of events,
    and, as with everything Knative, there are many more customizations and configuration
    options to tailor our solution precisely to what we need.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we'll look at the same pattern with OpenFaaS instead of Knative in
    order to highlight the differences between the two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenFaaS for FaaS on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've discussed getting started with Knative, let's do the same with
    OpenFaaS. First, to install OpenFaaS itself, we are going to use the Helm charts
    from the `faas-netes` repository, found at [https://github.com/openfaas/faas-netes](https://github.com/openfaas/faas-netes).
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenFaaS components with Helm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will create two namespaces to hold our OpenFaaS components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openfaas` to hold the actual service components of OpenFaas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openfaas-fn` to hold our deployed functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can add these two namespaces using a nifty YAML file from the `faas-netes`
    repository using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to add the `faas-netes` `Helm` `repository` with the following
    Helm command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we actually deploy OpenFaaS!
  prefs: []
  type: TYPE_NORMAL
- en: 'The Helm chart for OpenFaaS at the preceding `faas-netes` repository has several
    possible variables, but we will use the following configuration to ensure that
    an initial set of authentication credentials are created, and that ingress records
    are deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, that our OpenFaaS infrastructure has been deployed to our cluster, we''ll
    want to fetch the credentials that were generated as part of the Helm install.
    The Helm chart will create these as part of a hook and store them in a secret,
    so we can get them by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: That is all the Kubernetes setup we require!
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, let's install the OpenFaas CLI, which will make it extremely easy
    to manage our OpenFaas functions.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the OpenFaaS CLI and deploying functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install the OpenFaaS CLI, we can use the following command (for Windows,
    check the preceding OpenFaaS documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can get started with building and deploying some functions. This is
    easiest to do via the CLI.
  prefs: []
  type: TYPE_NORMAL
- en: When building and deploying functions for OpenFaaS, the OpenFaaS CLI provides
    an easy way to generate boilerplates, and build and deploy functions for specific
    languages. It does this via "templates," and supports various flavors of Node,
    Python, and more. For a full list of the template types, check the templates repository
    at [https://github.com/openfaas/templates](https://github.com/openfaas/templates).
  prefs: []
  type: TYPE_NORMAL
- en: 'The templates created using the OpenFaaS CLI are similar to what you would
    expect from a hosted serverless platform such as AWS Lambda. Let''s create a brand-new
    Node.js function using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `new` command generates a folder, and within it some boilerplate
    for the function code itself, and an OpenFaaS YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenFaaS YAML file will appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'My-function.yml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual function code (inside the `my-function` folder) consists of a function
    file – `handler.js` – and a dependencies manifest, `package.json`. For other languages,
    these files will be different, and we won''t delve into the specifics of dependencies
    in Node. However, we will edit the `handler.js` file to return some text. This
    is what the edited file looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Handler.js:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: This JavaScript code will return a JSON response with our text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our function and handler, we can move on to building and deploying
    our function. The OpenFaaS CLI makes it simple to build the function, which we
    can do with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The output of this command is long, but when it is complete, we will have a
    new container image built locally with our function handler and dependencies embedded!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we push our container image to a container repository as we would for
    any other container. The OpenFaaS CLI has a neat wrapper command for this, which
    will push the image to Docker Hub or an alternate container image repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can deploy our function to OpenFaaS. Once again, this is made easy
    by the CLI. Deploy it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Everything is now set up for us to test our function, deployed on OpenFaaS!
    We used an ingress setting when deploying OpenFaaS so requests can go through
    that ingress. However, our generated YAML file from our new function is set to
    make requests on `localhost:8080` for development purposes. We could edit that
    file to the correct `URL` for our ingress gateway (refer to the docs at [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/)
    for how to do that), but instead, let's just do a shortcut to get OpenFaaS open
    on our localhost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use a `kubectl port-forward` command to open our OpenFaaS service on
    localhost port `8080`. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add our previously generated auth credentials to the OpenFaaS CLI,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, all we need to do in order to test our function is to run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we've successfully received our intended response!
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if we want to delete this specific function, we can do so with the
    following command, similar to how we would use `kubectl delete -f`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: And that's it! Our function has been removed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about service mesh and serverless patterns on Kubernetes.
    In order to set the stage for these, we first discussed running sidecar proxies
    on Kubernetes, specifically with the Envoy proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we moved on to service mesh, and learned how to install and configure
    the Istio service mesh for service-to-service routing with mutual TLS.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we moved on to serverless patterns on Kubernetes, where you learned
    how to configure and install Knative, and an alternative, OpenFaaS, for serverless
    eventing, and FaaS on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The skills you used in this chapter will help you to build service mesh and
    serverless patterns on Kubernetes, setting you up for fully automated service-to-service
    discovery and FaaS eventing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next (and final) chapter, we'll discuss running stateful applications
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between static and dynamic Envoy configurations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the four major pieces of Envoy configuration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the downsides to Knative, and how does OpenFaaS compare?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNCF landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Official Kubernetes forums: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
