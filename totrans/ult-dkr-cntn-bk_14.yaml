- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we introduced orchestrators. Like a conductor in an orchestra,
    an orchestrator makes sure that all our containerized application services play
    together nicely and contribute harmoniously to a common goal. Such orchestrators
    have quite a few responsibilities, which we discussed in detail. Finally, we provided
    a short overview of the most important container orchestrators on the market.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces Docker’s native orchestrator, **SwarmKit**. It elaborates
    on all of the concepts and objects SwarmKit uses to deploy and run distributed,
    resilient, robust, and highly available applications in a cluster on-premises
    or in the cloud. This chapter also introduces how SwarmKit ensures secure applications
    by using a **Software-Defined Network** (**SDN**) to isolate containers. We will
    learn how to create a Docker Swarm locally, in a special environment called **Play
    with Docker** (**PWD**), and in the cloud. Lastly, we will deploy an application
    that consists of multiple services related to Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the topics we are going to discuss in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Swarm architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacks, services, and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-host networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a first application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After completing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sketch the essential parts of a highly available Docker Swarm on a whiteboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain what a (Swarm) service is in two or three simple sentences to an interested
    layman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a highly available Docker Swarm in AWS, Azure, or GCP consisting of three
    manager and two worker nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Successfully deploy a replicated service such as Nginx on a Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale a running Docker Swarm service up and down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the aggregated log of a replicated Docker Swarm service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a simple stack file for a sample application consisting of at least two
    interacting services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a stack into a Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Swarm architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of a Docker Swarm from a 30,000-foot view consists of two
    main parts—a raft consensus group of an odd number of manager nodes, and a group
    of worker nodes that communicate with each other over a gossip network, also called
    the **control plane**. The following diagram illustrates this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – High-level architecture of a Docker Swarm](img/Figure_14.01_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – High-level architecture of a Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: The manager nodes manage the swarm while the worker nodes execute the applications
    deployed into the swarm. Each manager has a complete copy of the full state of
    the Swarm in its local raft store. Managers synchronously communicate with each
    other, and their raft stores are always in sync.
  prefs: []
  type: TYPE_NORMAL
- en: The workers, on the other hand, communicate with each other asynchronously for
    scalability reasons. There can be hundreds if not thousands of worker nodes in
    a Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a high-level overview of what a Docker Swarm is, let’s describe
    all of the individual elements of a Docker Swarm in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Swarm is a collection of nodes. We can classify a node as a physical computer
    or **Virtual Machine** (**VM**). Physical computers these days are often referred
    to as bare metal. People say we’re running on bare metal to distinguish from running
    on a VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we install Docker on such a node, we call this node a Docker host. The
    following diagram illustrates a bit better what a node and a Docker host are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Bare-metal and VM types of Docker Swarm nodes](img/Figure_14.02_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Bare-metal and VM types of Docker Swarm nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'To become a member of a Docker Swarm, a node must be a Docker host. A node
    in a Docker Swarm can have one of two roles: it can be a manager or it can be
    a worker. Manager nodes do what their name implies; they manage the Swarm. The
    worker nodes, in turn, execute the application workload.'
  prefs: []
  type: TYPE_NORMAL
- en: Technically, a manager node can also be a worker node and hence run the application
    workload—although that is not recommended, especially if the Swarm is a production
    system running mission-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm managers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each Docker Swarm needs to include at least one manager node. For high-availability
    reasons, we should have more than one manager node in a Swarm. This is especially
    true for production or production-like environments. If we have more than one
    manager node, then these nodes work together using the Raft consensus protocol.
    The Raft consensus protocol is a standard protocol that is often used when multiple
    entities need to work together and always need to agree with each other as to
    which activity to execute next.
  prefs: []
  type: TYPE_NORMAL
- en: To work well, the Raft consensus protocol asks for an odd number of members
    in what is called the **consensus group**. Hence, we should always have 1, 3,
    5, 7, and so on manager nodes. In such a consensus group, there is always a leader.
    In the case of Docker Swarm, the first node that starts the Swarm initially becomes
    the leader. If the leader goes away, then the remaining manager nodes elect a
    new leader. The other nodes in the consensus group are called followers.
  prefs: []
  type: TYPE_NORMAL
- en: Raft leader election
  prefs: []
  type: TYPE_NORMAL
- en: Raft uses a heartbeat mechanism to trigger leader election. When servers start
    up, they begin as followers. A server remains in the follower state as long as
    it receives valid **Remote Procedure Calls** (**RPCs**) from a leader or candidate.
    Leaders send periodic heartbeats to all followers in order to maintain their authority.
    If a follower receives no communication over a period of time called the election
    timeout, then it assumes there is no viable leader and begins an election to choose
    a new leader. During the election, each server will start a timer with a random
    time chosen. When this timer fires, the server turns itself from a follower into
    a candidate. At the same time, it increments the term and sends messages to all
    its peers asking for a vote and waits for the responses back.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the Raft consensus algorithm, a “term” corresponds to a round
    of election and serves as a logical clock for the system, allowing Raft to detect
    obsolete information such as stale leaders. Every time an election is initiated,
    the term value is incremented.
  prefs: []
  type: TYPE_NORMAL
- en: When a server receives a vote request, it casts its vote only if the candidate
    has a higher term or the candidate has the same term. Otherwise, the vote request
    will be rejected. One peer can only vote for one candidate for one term, but when
    it receives another vote request with a higher term than the candidate it voted
    for, it will discard its previous vote.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of Raft and many other distributed systems, “logs” refer to the
    state machine logs or operation logs, not to be confused with traditional application
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: If the candidate doesn’t receive enough votes before the next timer fires, the
    current vote will be void and the candidate will start a new election with a higher
    term. Once the candidate receives votes from the majority of their peers, it turns
    itself from candidate to leader and immediately broadcasts the authorities to
    prevent other servers from starting the leader election. The leader will periodically
    broadcast this information. Now, let’s assume that we shut down the current leader
    node for maintenance reasons. The remaining manager nodes will elect a new leader.
    When the previous leader node comes back online, it will now become a follower.
    The new leader remains the leader.
  prefs: []
  type: TYPE_NORMAL
- en: All of the members of the consensus group communicate synchronously with each
    other. Whenever the consensus group needs to make a decision, the leader asks
    all followers for agreement. If the majority of the manager nodes gives a positive
    answer, then the leader executes the task. That means if we have three manager
    nodes, then at least one of the followers has to agree with the leader. If we
    have five manager nodes, then at least two followers have to agree with the leader.
  prefs: []
  type: TYPE_NORMAL
- en: Since all manager follower nodes have to communicate synchronously with the
    leader node to make a decision in the cluster, the decision-making process gets
    slower and slower the more manager nodes we have forming the consensus group.
    The recommendation of Docker is to use one manager for development, demo, or test
    environments. Use three managers nodes in small to medium-sized Swarms and use
    five managers in large to extra-large Swarms. Using more than five managers in
    a Swarm is hardly ever justified.
  prefs: []
  type: TYPE_NORMAL
- en: 'The manager nodes are not only responsible for managing the Swarm but also
    for maintaining the state of the Swarm. What do we mean by that? When we talk
    about the state of the Swarm, we mean all the information about it—for example,
    how many nodes are in the Swarm and what the properties of each node are, such
    as the name or IP address. We also mean what containers are running on which node
    in the Swarm and more. What, on the other hand, is not included in the state of
    the Swarm is data produced by the application services running in containers on
    the Swarm. This is called **application data** and is definitely not part of the
    state that is managed by the manager nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – A Swarm manager consensus group](img/Figure_14.03_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – A Swarm manager consensus group
  prefs: []
  type: TYPE_NORMAL
- en: All of the Swarm states are stored in a high-performance **key-value store**
    (**kv-store**) on each manager node. That’s right, each manager node stores a
    complete replica of the whole Swarm state. This redundancy makes the Swarm highly
    available. If a manager node goes down, the remaining managers all have the complete
    state at hand.
  prefs: []
  type: TYPE_NORMAL
- en: If a new manager joins the consensus group, then it synchronizes the Swarm state
    with the existing members of the group until it has a complete replica. This replication
    is usually pretty fast in typical Swarms but can take a while if the Swarm is
    big and many applications are running on it.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm workers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned earlier, a Swarm worker node is meant to host and run containers
    that contain the actual application services we’re interested in running on our
    cluster. They are the workhorses of the Swarm. In theory, a manager node can also
    be a worker. But, as we already said, this is not recommended on a production
    system. On a production system, we should let managers be managers.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes communicate with each other over the so-called control plane. They
    use the gossip protocol for their communication. This communication is asynchronous,
    which means that, at any given time, it is likely that not all worker nodes are
    in perfect sync.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might ask—what information do worker nodes exchange? It is mostly
    information that is needed for service discovery and routing, that is, information
    about which containers are running on with nodes and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Worker nodes communicating with each other](img/Figure_14.04_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – Worker nodes communicating with each other
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you can see how workers communicate with each other.
    To make sure the gossiping scales well in a large Swarm, each worker node only
    synchronizes its own state with three random neighbors. For those who are familiar
    with Big O notation, that means that the synchronization of the worker nodes using
    the gossip protocol scales with O(0).
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation explained
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation is a way to describe the speed or complexity of a given algorithm.
    It tells you the number of operations an algorithm will make. It’s used to communicate
    how fast an algorithm is, which can be important when evaluating other people’s
    algorithms, and when evaluating your own.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you have a list of numbers and you want to find a specific
    number in the list. There are different algorithms you can use to do this, such
    as simple search or binary search. Simple search checks each number in the list
    one by one until it finds the number you’re looking for. Binary search, on the
    other hand, repeatedly divides the list in half until it finds the number you’re
    looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s say you have a list of 100 numbers. With simple search, in the worst
    case, you’ll have to check all 100 numbers, so it takes 100 operations. With binary
    search, in the worst case, you’ll only have to check about 7 numbers (because
    log2(100) is roughly 7), so it takes 7 operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, binary search is faster than simple search. But what if you
    have a list of 1 billion numbers? Simple search would take 1 billion operations,
    while binary search would take only about 30 operations (because log2(1 billion)
    is roughly 30). So, as the list gets bigger, binary search becomes much faster
    than simple search.
  prefs: []
  type: TYPE_NORMAL
- en: Big O notation is used to describe this difference in speed between algorithms.
    In Big O notation, simple search is described as O(n), which means that the number
    of operations grows linearly with the size of the list (n). Binary search is described
    as O(log n), which means that the number of operations grows logarithmically with
    the size of the list.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes are kind of passive. They never actively do anything other than
    run the workloads they get assigned by the manager nodes. The worker makes sure,
    though, that it runs these workloads to the best of its capabilities. Later on
    in this chapter, we will get to know more about exactly what workloads the worker
    nodes are assigned by the manager nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what master and worker nodes in a Docker Swarm are, we are
    going to introduce stacks, services, and tasks next.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks, services, and tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using a Docker Swarm versus a single Docker host, there is a paradigm change.
    Instead of talking about individual containers that run processes, we are abstracting
    away to services that represent a set of replicas of each process, and, in this
    way, become highly available. We also do not speak anymore of individual Docker
    hosts with well-known names and IP addresses to which we deploy containers; we’ll
    now be referring to clusters of hosts to which we deploy services. We don’t care
    about an individual host or node anymore. We don’t give it a meaningful name;
    each node rather becomes a number to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also don’t care about individual containers and where they are deployed
    any longer—we just care about having a desired state defined through a service.
    We can try to depict that as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – Containers are deployed to well-known servers](img/Figure_14.05_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – Containers are deployed to well-known servers
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of deploying individual containers to well-known servers like in the
    preceding diagram, where we deploy the web container to the alpha server with
    the IP address `52.120.12.1`, and the payments container to the beta server with
    the IP `52.121.24.33`, we switch to this new paradigm of services and Swarms (or,
    more generally, clusters):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – Services are deployed to Swarms](img/Figure_14.06_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 – Services are deployed to Swarms
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we see that a web service and an inventory service
    are both deployed to a Swarm that consists of many nodes. Each of the services
    has a certain number of replicas: five for web and seven for inventory. We don’t
    really care which node the replicas will run on; we only care that the requested
    number of replicas is always running on whatever nodes the Swarm scheduler decides
    to put them on.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, let’s now introduce the concept of a service in the context of a
    Docker swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Swarm service is an abstract thing. It is a description of the desired state
    of an application or application service that we want to run in a Swarm. The Swarm
    service is like a manifest describing things such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image from which to create the containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of replicas to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network(s) that the containers of the service are attached to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ports that should be mapped
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having this service manifest, the Swarm manager then makes sure that the described
    desired state is always reconciled if the actual state should ever deviate from
    it. So, if, for example, one instance of the service crashes, then the scheduler
    on the Swarm manager schedules a new instance of this particular service on a
    node with free resources so that the desired state is re-established.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what is a task? This is what we’re going to learn next.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have learned that a service corresponds to a description of the desired state
    in which an application service should be at all times. Part of that description
    was the number of replicas the service should be running. Each replica is represented
    by a task. In this regard, a Swarm service contains a collection of tasks. On
    Docker Swarm, a task is an atomic unit of deployment. Each task of a service is
    deployed by the Swarm scheduler to a worker node. The task contains all of the
    necessary information that the worker node needs to run a container based on the
    image, which is part of the service description. Between a task and a container,
    there is a one-to-one relation. The container is the instance that runs on the
    worker node, while the task is the description of this container as a part of
    a Swarm service.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s talk about a stack in the context of a Docker swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a good idea about what a Swarm service is and what tasks are,
    we can introduce the stack. A stack is used to describe a collection of Swarm
    services that are related, most probably because they are part of the same application.
    In that sense, we could also say that a stack describes an application that consists
    of one-to-many services that we want to run on the Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we describe a stack declaratively in a text file that is formatted
    using the YAML format and that uses the same syntax as the already known Docker
    Compose file. This leads to a situation where people sometimes say that a stack
    is described by a Docker Compose file. A better wording would be that a stack
    is described in a stack file that uses similar syntax to a Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to illustrate the relationship between the stack, services, and tasks
    in the following diagram and connect it with the typical content of a stack file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Diagram showing the relationship between stack, services, and
    tasks](img/Figure_14.07_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 – Diagram showing the relationship between stack, services, and
    tasks
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we see on the right-hand side a declarative description
    of a sample Stack. The Stack consists of three services, called `web`, `payments`,
    and `inventory`. We also see that the `web` service uses the `example/web:1.0`
    image and has four replicas. On the left-hand side of the diagram, we see that
    the Stack embraces the three services mentioned. Each service, in turn, contains
    a collection of Tasks, as many as there are replicas. In the case of the `web`
    service, we have a collection of four Tasks. Each Task contains the name of the
    Image from which it will instantiate a container once the Task is scheduled on
    a Swarm node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a good understanding of the main concepts of a Docker swarm,
    such as nodes, stack, services, and tasks, let’s look a bit more closely into
    the networking used in a swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-host networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 10*](B19199_10.xhtml#_idTextAnchor218), *Using Single-Host Networking*,
    we discussed how containers communicate on a single Docker host. Now, we have
    a Swarm that consists of a cluster of nodes or Docker hosts. Containers that are
    located on different nodes need to be able to communicate with each other. Many
    techniques can help us to achieve this goal. Docker has chosen to implement an
    overlay network driver for Docker Swarm. This overlay network allows containers
    attached to the same overlay network to discover each other and freely communicate
    with each other. The following is a schema for how an overlay network works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – The overlay network](img/Figure_14.08_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 – The overlay network
  prefs: []
  type: TYPE_NORMAL
- en: We have two nodes or Docker hosts with the IP addresses `172.10.0.15` and `172.10.0.16`.
    The values we have chosen for the IP addresses are not important; what is important
    is that both hosts have a distinct IP address and are connected by a physical
    network (a network cable), which is called the **underlay network**.
  prefs: []
  type: TYPE_NORMAL
- en: On the node on the left-hand side, we have a container running with the IP address
    `10.3.0.2`, and on the node on the right-hand side, we have another container
    with the IP address `10.3.0.5`. Now, the former container wants to communicate
    with the latter. How can this happen? In [*Chapter 10*](B19199_10.xhtml#_idTextAnchor218),
    *Using* *Single-Host Networking*, we saw how this works when both containers are
    located on the same node—by using a Linux bridge. But Linux bridges only operate
    locally and cannot span across nodes. So, we need another mechanism. Linux VXLAN
    comes to the rescue. VXLAN has been available on Linux since way before containers
    were a thing.
  prefs: []
  type: TYPE_NORMAL
- en: VXLAN explained
  prefs: []
  type: TYPE_NORMAL
- en: '**VXLAN**, or **Virtual eXtensible Local Area Network**, is a networking protocol
    that allows for the creation of virtual layer 2 domains over an IP network using
    the UDP protocol. It was designed to solve the problem of limited VLAN IDs (4,096)
    in IEEE 802.1q by expanding the size of the identifier to 24 bits (16,777,216).'
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, VXLAN allows for the creation of virtual networks that can
    span across different physical locations. For example, certain VMs that are running
    on different hosts can communicate over a VXLAN tunnel. The hosts can be in different
    subnets or even in different data centers around the world. From the perspective
    of the VMs, other VMs in the same VXLAN are within the same layer 2 domain.
  prefs: []
  type: TYPE_NORMAL
- en: When the left-hand container in *Figure 14**.8* sends a data packet, the bridge
    realizes that the target of the packet is not on this host. Now, each node participating
    in an overlay network gets a so-called **VXLAN Tunnel Endpoint** (**VTEP**) object,
    which intercepts the packet (the packet at that moment is an OSI layer 2 data
    packet), wraps it with a header containing the target IP address of the host that
    runs the destination container (this now makes it an OSI layer 3 data packet),
    and sends it over the VXLAN tunnel. The VTEP on the other side of the tunnel unpacks
    the data packet and forwards it to the local bridge, which in turn forwards it
    to the destination container.
  prefs: []
  type: TYPE_NORMAL
- en: The overlay driver is included in SwarmKit and is in most cases the recommended
    network driver for Docker Swarm. There are other multi-node-capable network drivers
    available from third parties that can be installed as plugins in each participating
    Docker host. Certified network plugins are available from the Docker store.
  prefs: []
  type: TYPE_NORMAL
- en: Great, we have all the basic knowledge about a Docker swarm. So, let’s create
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a Docker Swarm is almost trivial. It is so easy that if you know how
    orchestrators work, it might even seem unbelievable. But it is true, Docker has
    done a fantastic job in making Swarms simple and elegant to use. At the same time,
    Docker Swarm has been proven to be very robust and scalable when used by large
    enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a local single-node swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, enough imagining—let’s demonstrate how we can create a Swarm. In its most
    simple form, a fully functioning Docker Swarm consists only of a single node.
    If you’re using Docker Desktop, or even if you’re using Docker Toolbox, then your
    personal computer or laptop is such a node. Hence, we can start right there and
    demonstrate some of the most important features of a Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize a Swarm. On the command line, just enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After an incredibly short time, you should see an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our computer is now a Swarm node. Its role is that of a manager and it is the
    leader (of the managers, which makes sense since there is only one manager at
    this time). Although it took only a very short time to finish `docker swarm init`,
    the command did a lot of things during that time. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It created a root **Certificate** **Authority** (**CA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It created a kv-store that is used to store the state of the whole Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, in the preceding output, we can see a command that can be used to join
    other nodes to the Swarm that we just created. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<join-token>` is a token generated by the Swarm leader at the time the Swarm
    was initialized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<IP address>` is the IP address of the leader'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although our cluster remains simple, as it consists of only one member, we
    can still ask the Docker CLI to list all of the nodes of the Swarm using the `docker
    node ls` command. This will look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – Listing the nodes of the Docker Swarm](img/Figure_14.09_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.9 – Listing the nodes of the Docker Swarm
  prefs: []
  type: TYPE_NORMAL
- en: In this output, we first see the ID that was given to the node. The star (`*`)
    that follows the ID indicates that this is the node on which `docker node ls`
    was executed—basically saying that this is the active node. Then, we have the
    (human-readable) name of the node and its status, availability, and manager status.
    As mentioned earlier, this very first node of the Swarm automatically became the
    leader, which is indicated in the preceding screenshot. Lastly, we see which version
    of Docker Engine we’re using.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get even more information about a node, we can use the `docker node inspect`
    command, as shown in the following truncated output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot of information generated by this command, so we have only presented
    a shortened version of the output. This output can be useful, for example, when
    you need to troubleshoot a misbehaving cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you continue, don’t forget to shut down or dissolve the swarm by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will use the PWD environment to generate and use a Docker
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Using PWD to generate a Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To experiment with Docker Swarm without having to install or configure anything
    locally on our computer, we can use PWD. PWD is a website that can be accessed
    with a browser and that offers us the ability to create a Docker Swarm consisting
    of up to five nodes. It is definitely a playground, as the name implies, and the
    time for which we can use it is limited to four hours per session. We can open
    as many sessions as we want, but each session automatically ends after four hours.
    Other than that, it is a fully functional Docker environment that is ideal for
    tinkering with Docker or demonstrating some features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s access the site now. In your browser, navigate to the website [https://labs.play-with-docker.com](https://labs.play-with-docker.com).
    You will be presented with a welcome and login screen. Use your Docker ID to log
    in. After successfully doing so, you will be presented with a screen that looks
    like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10 – PWD window](img/Figure_14.10_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.10 – PWD window
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see immediately, there is a big timer counting down from four hours.
    That’s how much time we have left to play in this session. Furthermore, we see
    an **+ ADD NEW INSTANCE** link. Click it to create a new Docker host. When you
    do that, your screen should look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.11 – PWD with one new node](img/Figure_14.11_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.11 – PWD with one new node
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side, we see the newly created node with its IP address (`192.168.0.13`)
    and its name (`node1`). On the right-hand side, we have some additional information
    about this new node in the upper half of the screen and a terminal in the lower
    half. Yes, this terminal is used to execute commands on this node that we just
    created. This node has the Docker CLI installed, and hence we can execute all
    of the familiar Docker commands on it, such as the Docker version. Try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now we want to create a Docker Swarm. Execute the following command in
    the terminal in your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output generated by the preceding command is similar to the one we saw when
    creating a local Docker Swarm. The important thing to note is the `join` command,
    which is what we want to use to join additional nodes to the cluster we just created.
  prefs: []
  type: TYPE_NORMAL
- en: You might have noted that we specified the `--advertise-addr` parameter in the
    Swarm `init` command. Why is that necessary here? The reason is that the nodes
    generated by PWD have more than one IP address associated with them. We can easily
    verify that by executing the `ip` command on the node. This command will show
    us that there are indeed two endpoints, `eth0` and `eth1`, present. We hence have
    to specify explicitly to the new to-be swarm manager which one we want to use.
    In our case, it is `eth0`.
  prefs: []
  type: TYPE_NORMAL
- en: Create four additional nodes in PWD by clicking four times on the `node2`, `node3`,
    `node4`, and `node5` and will all be listed on the left-hand side. If you click
    on one of the nodes on the left-hand side, then the right-hand side shows the
    details of the respective node and a terminal window for that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select each node (2 to 5) and execute the `docker swarm join` command that
    you have copied from the leader node (`node1`) in the respective terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This node joined the swarm as a worker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have joined all four nodes to the Swarm, switch back to `node1` and
    list all nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This, unsurprisingly, results in this (slightly reformatted for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Still on `node1`, we can now promote, say, `node2` and `node3`, to make the
    Swarm highly available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With this, our Swarm on PWD is ready to accept a workload. We have created a
    highly available Docker Swarm with three manager nodes that form a Raft consensus
    group and two worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Docker Swarm in the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the Docker Swarms we have created so far are wonderful to use in development,
    to experiment with, or to use for demonstration purposes. If we want to create
    a Swarm that can be used as a production environment where we run our mission-critical
    applications, though, then we need to create a—I’m tempted to say—real Swarm in
    the cloud or on-premises. In this book, we are going to demonstrate how to create
    a Docker Swarm in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can manually create a Swarm through the AWS console:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your AWS account. If you do not have one yet, create a free one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we create an AWS `aws-docker-demo-sg`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to your default VPC.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left-hand side, select `aws-docker-demo-sg`, as mentioned, and add a
    description such as `A SG for our` `Docker demo`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, click the `sg-030d0...`)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom UDP, **Protocol**: UDP, **Port range**: 7946, **Source**:
    Custom'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the value, select the SG just created
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom TCP, **Protocol**: TCP, **Port range**: 7946, **Source**:
    Custom'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the value select the SG just created
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom TCP, **Protocol**: TCP, **Port range**: 4789, **Source**:
    Custom'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the value, select the SG just created
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom TCP, **Protocol**: TCP, **Port range**: 22, **Source**: My
    IP'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This last rule is to be able to access the instances from your host via SSH
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker Swarm ports
  prefs: []
  type: TYPE_NORMAL
- en: '**TCP port 2377**: This is the main communication port for swarm mode. The
    Swarm management and orchestration commands are communicated over this port. It’s
    used for communication between nodes and plays a crucial role in the Raft consensus
    algorithm, which ensures that all the nodes in a swarm act as a single system.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TCP and UDP port 7946**: This port is used for communication among nodes
    (container network discovery). It helps the nodes in the swarm to exchange information
    about the services and tasks running on each of them.'
  prefs: []
  type: TYPE_NORMAL
- en: '**UDP port 4789**: This port is used for overlay network traffic. When you
    create an overlay network for your services, Docker Swarm uses this port for the
    data traffic between the containers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.12 – Inbound rules for the AWS SG](img/Figure_14.12_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.12 – Inbound rules for the AWS SG
  prefs: []
  type: TYPE_NORMAL
- en: When done, click **Save rules**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the EC2 dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we create a key pair for all EC2 instances we are going to create next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate and click the `aws-docker-demo`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure the private key file format is `.pem`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the `.pem` file in a safe location.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Back on the EC2 dashboard, launch a new EC2 instance with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the instance `manager1`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `t2.micro` as the instance type.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the key pair that we created before, called `aws-docker-demo`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the existing SG, `aws-docker-demo-sg`, that we created previously.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click the **Launch** button.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the previous step to create two worker nodes and call them `worker1`
    and `worker2`, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the list of EC2 instances. You may have to wait a few minutes until they
    are all ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with the `manager1` instance by selecting it and then clicking the `ssh`.
    Follow those instructions carefully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once connected to the `manager1` instance, let’s install Docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This may take a couple of minutes to finish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, make sure you can use Docker without having to use the `sudo` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To apply the preceding command, you have to quickly exit from the AWS instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, immediately connect again using the `ssh` command from *step 6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back on the EC2 instance, make sure you can access Docker with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If everything is installed and configured correctly, you should see the version
    information of the Docker client and engine.
  prefs: []
  type: TYPE_NORMAL
- en: Now repeat *steps 6* to *10* for the other two EC2 instances, `worker1` and
    `worker2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now go back to your `manager1` instance and initialize a Docker swarm on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output should be the same as you saw for the case when you created a swarm
    locally or on PWD.
  prefs: []
  type: TYPE_NORMAL
- en: Copy the `docker swarm join` command from the preceding output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to each worker node and run the command. The node should respond with the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Go back to the `manager1` node and run the following command to list all nodes
    of the swarm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'What you should see is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.13 – List of swarm nodes on AWS](img/Figure_14.13_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.13 – List of swarm nodes on AWS
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a Docker swarm in the (AWS) cloud, let’s deploy a simple application
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a first application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have created a few Docker Swarms on various platforms. Once created, a Swarm
    behaves the same way on any platform. The way we deploy and update applications
    on a Swarm is not platform-dependent. It has been one of Docker’s main goals to
    avoid vendor lock-in when using a Swarm. Swarm-ready applications can be effortlessly
    migrated from, say, a Swarm running on-premises to a cloud-based Swarm. It is
    even technically possible to run part of a Swarm on-premises and another part
    in the cloud. It works, yet we have, of course, to consider possible side effects
    due to the higher latency between nodes in geographically distant areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a highly available Docker Swarm up and running, it is time
    to run some workloads on it. I’m using the swarm just created on AWS. We’ll start
    by first creating a single service. For this, we need to SSH into one of the manager
    nodes. I selected the swarm node on the `manager1` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We start the deployment of our first application to the swarm by creating a
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A service can be created either as part of a stack or directly using the Docker
    CLI. Let’s first look at a sample stack file that defines a single service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the Vi editor to create a new file called `stack.yml` and add this content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Exit the Vi editor by first pressing the *Esc* key, then typing `:wq`, and then
    pressing *Enter*. This will save the code snippet and exit vi.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar with Vi, you can also use nano instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, we see what the desired state of a service called
    `whoami` is:'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the `training/whoami:latest` image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers of the service are attached to the `test-net` network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container port `8000` is published to port `81`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is running with six replicas (or tasks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During a rolling update, the individual tasks are updated in batches of two,
    with a delay of 10 seconds between each successful batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service (and its tasks and containers) is assigned the two labels, `app`
    and `environment`, with the values `sample-app` and `prod-south`, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more settings that we could define for a service, but the preceding
    ones are some of the more important ones. Most settings have meaningful default
    values. If, for example, we do not specify the number of replicas, then Docker
    defaults it to `1`. The name and image of a service are, of course, mandatory.
    Note that the name of the service must be unique in the Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the preceding service, we use the `docker stack deploy` command.
    Assuming that the file in which the preceding content is stored is called `stack.yaml`,
    we have the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have created a stack called `sample-stack` that consists of one service,
    `whoami`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can list all stacks on our Swarm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon doing so, we should get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list the services defined in our Swarm, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.14 – List of all services running in the Swarm](img/Figure_14.14_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.14 – List of all services running in the Swarm
  prefs: []
  type: TYPE_NORMAL
- en: In the output, we can see that currently, we have only one service running,
    which was to be expected. The service has an ID. The format of the ID, contrary
    to what you have used so far for containers, networks, or volumes, is alphanumeric
    (in the latter cases, it was always SHA-256). We can also see that the name of
    the service is a combination of the service name we defined in the stack file
    and the name of the stack, which is used as a prefix. This makes sense since we
    want to be able to deploy multiple stacks (with different names) using the same
    stack file into our Swarm. To make sure that service names are unique, Docker
    decided to combine the service name and stack name.
  prefs: []
  type: TYPE_NORMAL
- en: In the third column, we see the mode, which is replicated. The number of replicas
    is shown as `6/6`. This tells us that six out of the six requested replicas are
    running. This corresponds to the desired state. In the output, we also see the
    image that the service uses and the port mappings of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the service and its tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding output, we cannot see the details of the six replicas that
    have been created.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get some deeper insight into that, we can use the `docker service ps <service-id>`
    command. If we execute this command for our service, we will get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.15 – Details of the whoami service](img/Figure_14.15_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.15 – Details of the whoami service
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see the list of six tasks that corresponds to
    the requested six replicas of our `whoami` service. In the **NODE** column, we
    can also see the node to which each task has been deployed. The name of each task
    is a combination of the service name plus an increasing index. Also note that,
    similar to the service itself, each task gets an alphanumeric ID assigned.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, apparently tasks 3 and 6, with the names `sample-stack_whoami.3`
    and `sample-stack_whoami.6`, have been deployed to `ip-172-31-32-21`, which is
    the leader of our Swarm. Hence, I should find a container running on this node.
    Let’s see what we get if we list all containers running on `ip-172-31-32-21`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.16 – List of containers on node ip-172-31-32-21](img/Figure_14.16_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.16 – List of containers on node ip-172-31-32-21
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, we find a container running from the `training/whoami:latest`
    image with a name that is a combination of its parent task name and ID. We can
    try to visualize the whole hierarchy of objects that we generated when deploying
    our sample stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.17 – Object hierarchy of a Docker Swarm stack](img/Figure_14.17_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.17 – Object hierarchy of a Docker Swarm stack
  prefs: []
  type: TYPE_NORMAL
- en: 'A stack can consist of one-to-many services. Each service has a collection
    of tasks. Each task has a one-to-one association with a container. Stacks and
    services are created and stored on the Swarm manager nodes. Tasks are then scheduled
    to Swarm worker nodes, where the worker node creates the corresponding container.
    We can also get some more information about our service by inspecting it. Execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This provides a wealth of information about all of the relevant settings of
    the service. This includes those we have explicitly defined in our `stack.yaml`
    file, but also those that we didn’t specify and that, therefore, got their default
    values assigned. We’re not going to list the whole output here, as it is too long,
    but I encourage you to inspect it on your own machine. We will discuss part of
    the information in more detail in the *The swarm routing mesh* section in [*Chapter
    15*](B19199_15.xhtml#_idTextAnchor328).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To see that the swarm load balances incoming requests to our sample `whoami`
    application, we can use the `curl` tool. Execute the following command a few times
    and observe how the answer changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that after the sixth item, the sequence is repeating. This is due to the
    fact that the Docker swarm is load balancing calls using a round-robin algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Logs of a service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In an earlier chapter, we worked with the logs produced by a container. Here,
    we’re concentrating on a service. Remember that, ultimately, a service with many
    replicas has many containers running. Hence, we would expect that, if we asked
    the service for its logs, Docker would return an aggregate of all logs of those
    containers belonging to the service. And indeed, we’ll see this when we use the
    `docker service` `logs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.18 – Logs of the whoami service](img/Figure_14.18_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.18 – Logs of the whoami service
  prefs: []
  type: TYPE_NORMAL
- en: There is not much information in the logs at this point, but it is enough to
    discuss what we get. The first part of each line in the log always contains the
    name of the container combined with the node name from which the log entry originates.
    Then, separated by the vertical bar (`Listening` `on :8000`.
  prefs: []
  type: TYPE_NORMAL
- en: The aggregated logs that we get with the `docker service logs` command are not
    sorted in any particular way. So, if the correlation of events is happening in
    different containers, you should add information to your log output that makes
    this correlation possible.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this is a timestamp for each log entry. But this has to be done at
    the source; for example, the application that produces a log entry needs to also
    make sure a timestamp is added.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also query the logs of an individual task of the service by providing
    the task ID instead of the service ID or name. So, say we queried the logs from
    task 6 with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we are investigating how the swarm reconciles the desired
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Reconciling the desired state
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have learned that a Swarm service is a description or manifest of the desired
    state that we want an application or application service to run in. Now, let’s
    see how Docker Swarm reconciles this desired state if we do something that causes
    the actual state of the service to be different from the desired state. The easiest
    way to do this is to forcibly kill one of the tasks or containers of the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this with the container that has been scheduled on `node-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do that and then run `docker service ps` right afterward, we will see
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.19 – Docker Swarm reconciling the desired state after one task
    failed](img/Figure_14.19_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.19 – Docker Swarm reconciling the desired state after one task failed
  prefs: []
  type: TYPE_NORMAL
- en: We see that task 2 failed with exit code `137` and that the Swarm immediately
    reconciled the desired state by rescheduling the failed task on a node with free
    resources. In this case, the scheduler selected the same node as the failed tasks,
    but this is not always the case. So, without us intervening, the Swarm completely
    fixed the problem, and since the service is running in multiple replicas, at no
    time was the service down.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try another failure scenario. This time, we’re going to shut down an entire
    node and are going to see how the Swarm reacts. Let’s take node `ip-172-31-47-124`
    for this, as it has two tasks (tasks 1 and 4) running on it. For this, we can
    head over to the AWS console and in the EC2 dashboard, stop the instance called
    `ip-172-31-47-124`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that I had to go into the details of each worker instance to find out which
    one has the hostname `ip-172-31-47-124`; in my case, it was `worker2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back on the master node, we can now again run `docker service ps` to see what
    happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.20 – Swarm reschedules all tasks of a failed node](img/Figure_14.20_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.20 – Swarm reschedules all tasks of a failed node
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see that immediately, task 1 was rescheduled
    on node `ip-172-31-32-189`, while task 4 was rescheduled on node `ip-172-31-32-21`.
    Even this more radical failure is handled gracefully by Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note though that if node `ip-172-31-47-124` ever comes back
    online in the Swarm, the tasks that had previously been running on it will not
    automatically be transferred back to it.
  prefs: []
  type: TYPE_NORMAL
- en: But the node is now ready for a new workload.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a service or a stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we want to remove a particular service from the Swarm, we can use the `docker
    service rm` command. If, on the other hand, we want to remove a stack from the
    Swarm, we analogously use the `docker stack rm` command. This command removes
    all services that are part of the stack definition. In the case of the `whoami`
    service, it was created by using a stack file and hence we’re going to use the
    latter command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will make sure that all tasks of each service of the stack
    are terminated, and the corresponding containers are stopped by first sending
    `SIGTERM`, and then, if not successful, `SIGKILL` after 10 seconds of timeout.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the stopped containers are not removed from the
    Docker host.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is advised to purge containers from time to time on worker nodes to
    reclaim unused resources. Use `docker container purge -f` for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Why does it make sense to leave stopped or crashed containers
    on the worker node and not automatically remove them?'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a multi-service stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 11*](B19199_11.xhtml#_idTextAnchor237), *Managing Containers with
    Docker Compose*, we used an application consisting of two services that were declaratively
    described in a Docker Compose file. We can use this Compose file as a template
    to create a stack file that allows us to deploy the same application into a Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called `pets-stack.yml`, and add this content to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We request that the web service has three replicas, and both services are attached
    to the overlay network, `pets-net`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can deploy this application using the `docker stack` `deploy` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Docker creates the `pets_pets-net` overlay network and then the two services,
    `pets_web` and `pets_db`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then list all of the tasks in the `pets` stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.21 – List of all of the tasks in the pets stack](img/Figure_14.21_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.21 – List of all of the tasks in the pets stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s test the application using `curl` to retrieve an HTML page with
    a pet. And, indeed, the application works as expected, as the expected page is
    returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.22 – Testing the pets application using curl](img/Figure_14.22_B19199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.22 – Testing the pets application using curl
  prefs: []
  type: TYPE_NORMAL
- en: The container ID is in the output, where it says `Delivered to you by container
    d01e2f1f87df`. If you run the `curl` command multiple times, the ID should cycle
    between three different values. These are the IDs of the three containers (or
    replicas) that we have requested for the web service.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’re done, we can remove the stack with `docker stack` `rm pets`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we’re done with the swarm in AWS, we can remove it.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the swarm in AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To clean up the Swarm in the AWS cloud and avoid incurring unnecessary costs,
    we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s summarize what we have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced Docker Swarm, which, next to Kubernetes, is the
    second most popular orchestrator for containers. We looked into the architecture
    of a Swarm, discussed all of the types of resources running in a Swarm, such as
    services, tasks, and more, and we created services in the Swarm. We learned how
    to create a Docker Swarm locally, in a special environment called PWD, as well
    as in the cloud. Lastly, we deployed an application that consists of multiple
    services related to Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to introduce the routing mesh, which provides
    layer 4 routing and load balancing in a Docker Swarm. After that, we will demonstrate
    how to deploy a first application consisting of multiple services onto the Swarm.
    We will also learn how to achieve zero downtime when updating an application in
    the swarm and finally how to store configuration data in the swarm and how to
    protect sensitive data using Docker secrets. Stay tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your learning progress, please try to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main components of a Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you initialize a Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you add nodes to a Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Docker Service in the context of Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you create and update services in Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Docker Stack and how does it relate to Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you deploy a Docker Stack in Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the networking options in Docker Swarm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Docker Swarm handle container scaling and fault tolerance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are sample answers to the preceding questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm is a native container orchestration tool built into Docker Engine
    that allows you to create, manage, and scale a cluster of Docker nodes, orchestrating
    the deployment, scaling, and management of containers across multiple hosts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A Docker Swarm consists of two primary components: the manager nodes, which
    are responsible for managing the cluster’s state, orchestrating tasks, and maintaining
    the desired state of services, and the worker nodes, which execute the tasks and
    run the container instances.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can initialize a Docker Swarm by running the `docker swarm init` command
    on a Docker host, which will become the first manager node of the Swarm. The command
    will provide a token that can be used to join other nodes to the Swarm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To add nodes to a Docker Swarm, use the `docker swarm join` command on the new
    node, along with the token and the IP address of the existing manager node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Docker Service is a high-level abstraction that represents a containerized
    application or microservice in a Docker Swarm. It defines the desired state of
    the application, including the container image, number of replicas, network, and
    other configuration options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can create a new service using the `docker service create` command, and
    update an existing service using the `docker service update` command, followed
    by the desired configuration options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Docker Stack is a collection of services that is deployed together and shares
    dependencies, defined in a Docker Compose file. Docker Stacks can be deployed
    in a Docker Swarm to manage and orchestrate multi-service applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To deploy a Docker Stack in Docker Swarm, use the `docker stack deploy` command,
    followed by the stack name and the path to the Docker Compose file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker Swarm supports various networking options, including the default ingress
    network for load balancing and routing, overlay networks for container-to-container
    communication across nodes, and custom networks for specific use cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker Swarm automatically manages container scaling by adjusting the number
    of replicas based on the desired state specified in the service definition. It
    also monitors the health of containers and replaces any failed instances to maintain
    fault tolerance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
