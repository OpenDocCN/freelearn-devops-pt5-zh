- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Introducing Docker Swarm
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Docker Swarm
- en: In the last chapter, we introduced orchestrators. Like a conductor in an orchestra,
    an orchestrator makes sure that all our containerized application services play
    together nicely and contribute harmoniously to a common goal. Such orchestrators
    have quite a few responsibilities, which we discussed in detail. Finally, we provided
    a short overview of the most important container orchestrators on the market.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了编排工具。就像乐团中的指挥，编排工具确保我们所有的容器化应用服务能够和谐地一起工作，并为共同的目标做出贡献。这些编排工具有很多责任，我们已经详细讨论过。最后，我们简要概述了市场上最重要的容器编排工具。
- en: This chapter introduces Docker’s native orchestrator, **SwarmKit**. It elaborates
    on all of the concepts and objects SwarmKit uses to deploy and run distributed,
    resilient, robust, and highly available applications in a cluster on-premises
    or in the cloud. This chapter also introduces how SwarmKit ensures secure applications
    by using a **Software-Defined Network** (**SDN**) to isolate containers. We will
    learn how to create a Docker Swarm locally, in a special environment called **Play
    with Docker** (**PWD**), and in the cloud. Lastly, we will deploy an application
    that consists of multiple services related to Docker Swarm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Docker 的原生编排工具**SwarmKit**。详细阐述了 SwarmKit 用来在集群中部署和运行分布式、弹性、稳健和高可用应用程序的所有概念和对象，不论是在本地环境还是云环境中。本章还介绍了
    SwarmKit 如何通过使用**软件定义网络**（**SDN**）来隔离容器，从而确保应用程序的安全。我们将学习如何在本地、一个叫做**Play with
    Docker**（**PWD**）的特殊环境中，及在云中创建 Docker Swarm。最后，我们将部署一个由多个与 Docker Swarm 相关的服务组成的应用程序。
- en: 'These are the topics we are going to discuss in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主题：
- en: The Docker Swarm architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm 架构
- en: Stacks, services, and tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆栈、服务和任务
- en: Multi-host networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多主机网络
- en: Creating a Docker Swarm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 Docker Swarm
- en: Deploying a first application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署第一个应用程序
- en: 'After completing this chapter, you will be able to do the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将能够做以下事情：
- en: Sketch the essential parts of a highly available Docker Swarm on a whiteboard
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在白板上勾画出高可用 Docker Swarm 的关键部分
- en: Explain what a (Swarm) service is in two or three simple sentences to an interested
    layman
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用两三句简单的语言向感兴趣的外行解释什么是（Swarm）服务
- en: Create a highly available Docker Swarm in AWS, Azure, or GCP consisting of three
    manager and two worker nodes
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS、Azure 或 GCP 上创建一个高可用性的 Docker Swarm，其中包括三个管理节点和两个工作节点
- en: Successfully deploy a replicated service such as Nginx on a Docker Swarm
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功部署一个像 Nginx 这样的复制服务到 Docker Swarm 中
- en: Scale a running Docker Swarm service up and down
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展和缩减运行中的 Docker Swarm 服务
- en: Retrieve the aggregated log of a replicated Docker Swarm service
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索复制的 Docker Swarm 服务的聚合日志
- en: Write a simple stack file for a sample application consisting of at least two
    interacting services
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为一个由至少两个交互服务组成的示例应用程序编写一个简单的堆栈文件
- en: Deploy a stack into a Docker Swarm
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将堆栈部署到 Docker Swarm
- en: Let’s get started!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: The Docker Swarm architecture
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm 架构
- en: 'The architecture of a Docker Swarm from a 30,000-foot view consists of two
    main parts—a raft consensus group of an odd number of manager nodes, and a group
    of worker nodes that communicate with each other over a gossip network, also called
    the **control plane**. The following diagram illustrates this architecture:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从 30,000 英尺的高度来看，Docker Swarm 的架构由两部分组成：一个奇数个管理节点的 Raft 共识组，以及一个通过 Gossip 网络相互通信的工作节点组，这个网络也被称为**控制平面**。下图展示了这一架构：
- en: '![Figure 14.1 – High-level architecture of a Docker Swarm](img/Figure_14.01_B19199.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.1 – Docker Swarm 的高级架构](img/Figure_14.01_B19199.jpg)'
- en: Figure 14.1 – High-level architecture of a Docker Swarm
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – Docker Swarm 的高级架构
- en: The manager nodes manage the swarm while the worker nodes execute the applications
    deployed into the swarm. Each manager has a complete copy of the full state of
    the Swarm in its local raft store. Managers synchronously communicate with each
    other, and their raft stores are always in sync.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 管理节点负责管理 Swarm，而工作节点则执行部署到 Swarm 中的应用程序。每个管理节点都有 Swarm 完整状态的副本，存储在本地的 Raft 存储中。管理节点同步地相互通信，它们的
    Raft 存储始终保持同步。
- en: The workers, on the other hand, communicate with each other asynchronously for
    scalability reasons. There can be hundreds if not thousands of worker nodes in
    a Swarm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，工作节点为了可扩展性原因是异步地相互通信的。在一个 Swarm 中，工作节点的数量可以达到数百，甚至数千个。
- en: Now that we have a high-level overview of what a Docker Swarm is, let’s describe
    all of the individual elements of a Docker Swarm in more detail.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Docker Swarm 有了一个高层次的概览，接下来让我们更详细地描述 Docker Swarm 的所有组成部分。
- en: Swarm nodes
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swarm 节点
- en: A Swarm is a collection of nodes. We can classify a node as a physical computer
    or **Virtual Machine** (**VM**). Physical computers these days are often referred
    to as bare metal. People say we’re running on bare metal to distinguish from running
    on a VM.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 是一个节点集合。我们可以将节点分类为物理计算机或**虚拟机**（**VM**）。如今，物理计算机通常被称为裸金属。人们用“裸金属”来区分与虚拟机上的运行。
- en: 'When we install Docker on such a node, we call this node a Docker host. The
    following diagram illustrates a bit better what a node and a Docker host are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这样的节点上安装 Docker 时，我们称这个节点为 Docker 主机。以下图示更清楚地展示了节点和 Docker 主机的概念：
- en: '![Figure 14.2 – Bare-metal and VM types of Docker Swarm nodes](img/Figure_14.02_B19199.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2 – Docker Swarm 节点的裸金属和虚拟机类型](img/Figure_14.02_B19199.jpg)'
- en: Figure 14.2 – Bare-metal and VM types of Docker Swarm nodes
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 – Docker Swarm 节点的裸金属和虚拟机类型
- en: 'To become a member of a Docker Swarm, a node must be a Docker host. A node
    in a Docker Swarm can have one of two roles: it can be a manager or it can be
    a worker. Manager nodes do what their name implies; they manage the Swarm. The
    worker nodes, in turn, execute the application workload.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为 Docker Swarm 的成员，节点必须是 Docker 主机。Docker Swarm 中的节点可以有两种角色：它可以是管理节点，也可以是工作节点。管理节点做它名字所暗示的工作；它们管理
    Swarm。而工作节点则执行应用程序负载。
- en: Technically, a manager node can also be a worker node and hence run the application
    workload—although that is not recommended, especially if the Swarm is a production
    system running mission-critical applications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，管理节点也可以是工作节点，因此可以运行应用程序负载——尽管不推荐这样做，特别是当 Swarm 是一个运行关键任务应用程序的生产系统时。
- en: Swarm managers
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Swarm 管理节点
- en: Each Docker Swarm needs to include at least one manager node. For high-availability
    reasons, we should have more than one manager node in a Swarm. This is especially
    true for production or production-like environments. If we have more than one
    manager node, then these nodes work together using the Raft consensus protocol.
    The Raft consensus protocol is a standard protocol that is often used when multiple
    entities need to work together and always need to agree with each other as to
    which activity to execute next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Docker Swarm 至少需要包含一个管理节点。出于高可用性的考虑，我们应该在 Swarm 中有多个管理节点。这对于生产环境或类似生产的环境尤为重要。如果我们有多个管理节点，那么这些节点将使用
    Raft 共识协议一起工作。Raft 共识协议是一种标准协议，通常用于多个实体需要共同工作，并始终需要就接下来执行哪个操作达成一致的场景。
- en: To work well, the Raft consensus protocol asks for an odd number of members
    in what is called the **consensus group**. Hence, we should always have 1, 3,
    5, 7, and so on manager nodes. In such a consensus group, there is always a leader.
    In the case of Docker Swarm, the first node that starts the Swarm initially becomes
    the leader. If the leader goes away, then the remaining manager nodes elect a
    new leader. The other nodes in the consensus group are called followers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了良好运作，Raft 共识协议要求在所谓的**共识组**中有一个奇数数量的成员。因此，我们应该始终有 1、3、5、7 等个管理节点。在这样的共识组中，总是会有一个领导者。在
    Docker Swarm 中，第一个启动 Swarm 的节点最初会成为领导者。如果领导者离开，剩余的管理节点会选举出一个新的领导者。共识组中的其他节点被称为跟随者。
- en: Raft leader election
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Raft 领导者选举
- en: Raft uses a heartbeat mechanism to trigger leader election. When servers start
    up, they begin as followers. A server remains in the follower state as long as
    it receives valid **Remote Procedure Calls** (**RPCs**) from a leader or candidate.
    Leaders send periodic heartbeats to all followers in order to maintain their authority.
    If a follower receives no communication over a period of time called the election
    timeout, then it assumes there is no viable leader and begins an election to choose
    a new leader. During the election, each server will start a timer with a random
    time chosen. When this timer fires, the server turns itself from a follower into
    a candidate. At the same time, it increments the term and sends messages to all
    its peers asking for a vote and waits for the responses back.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Raft 使用心跳机制来触发领导者选举。当服务器启动时，它们会首先作为跟随者存在。只要服务器接收到来自领导者或候选者的有效**远程过程调用**（**RPCs**），它就保持在跟随者状态。领导者会定期向所有跟随者发送心跳，以维持其权威。如果跟随者在一段时间内没有收到任何通信（该时间段称为选举超时），它就假设没有可行的领导者，并开始选举一个新的领导者。在选举过程中，每台服务器都会启动一个随机选择的计时器。当计时器触发时，服务器将自己从跟随者变为候选者。同时，它会增加
    term 值，并向所有对等节点发送投票请求，等待回应。
- en: In the context of the Raft consensus algorithm, a “term” corresponds to a round
    of election and serves as a logical clock for the system, allowing Raft to detect
    obsolete information such as stale leaders. Every time an election is initiated,
    the term value is incremented.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Raft 共识算法的上下文中，“term”对应于一次选举轮次，并作为系统的逻辑时钟，使 Raft 能够检测到过时的信息，如过期的领导者。每次发起选举时，term
    值都会增加。
- en: When a server receives a vote request, it casts its vote only if the candidate
    has a higher term or the candidate has the same term. Otherwise, the vote request
    will be rejected. One peer can only vote for one candidate for one term, but when
    it receives another vote request with a higher term than the candidate it voted
    for, it will discard its previous vote.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器收到投票请求时，只有在候选者的 term 值较高或候选者的 term 与自身相同时，服务器才会投票。否则，投票请求将被拒绝。每个对等节点每个 term
    只能投给一个候选者，但如果它收到的投票请求的 term 值比之前投票的候选者更高，它将放弃之前的投票。
- en: In the context of Raft and many other distributed systems, “logs” refer to the
    state machine logs or operation logs, not to be confused with traditional application
    logs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Raft 和许多其他分布式系统的上下文中，“日志”指的是状态机日志或操作日志，而不是传统的应用程序日志。
- en: If the candidate doesn’t receive enough votes before the next timer fires, the
    current vote will be void and the candidate will start a new election with a higher
    term. Once the candidate receives votes from the majority of their peers, it turns
    itself from candidate to leader and immediately broadcasts the authorities to
    prevent other servers from starting the leader election. The leader will periodically
    broadcast this information. Now, let’s assume that we shut down the current leader
    node for maintenance reasons. The remaining manager nodes will elect a new leader.
    When the previous leader node comes back online, it will now become a follower.
    The new leader remains the leader.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果候选者在下一个计时器触发前没有获得足够的票数，当前投票将作废，候选者将以更高的 term 值开始新一轮选举。一旦候选者获得大多数对等节点的票数，它就会将自己从候选者转为领导者，并立即广播其权威，以防止其他服务器开始领导者选举。领导者会定期广播这一信息。现在，假设我们因为维护原因关闭了当前的领导者节点，剩余的管理节点将选举新的领导者。当之前的领导者节点重新上线时，它将变为跟随者，而新的领导者将继续担任领导者职务。
- en: All of the members of the consensus group communicate synchronously with each
    other. Whenever the consensus group needs to make a decision, the leader asks
    all followers for agreement. If the majority of the manager nodes gives a positive
    answer, then the leader executes the task. That means if we have three manager
    nodes, then at least one of the followers has to agree with the leader. If we
    have five manager nodes, then at least two followers have to agree with the leader.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有共识组成员彼此之间同步通信。每当共识组需要做出决策时，领导者会向所有跟随者请求同意。如果大多数管理节点给予肯定回答，领导者就会执行任务。这意味着，如果我们有三个管理节点，那么至少一个跟随者必须同意领导者。如果我们有五个管理节点，那么至少两个跟随者必须同意领导者。
- en: Since all manager follower nodes have to communicate synchronously with the
    leader node to make a decision in the cluster, the decision-making process gets
    slower and slower the more manager nodes we have forming the consensus group.
    The recommendation of Docker is to use one manager for development, demo, or test
    environments. Use three managers nodes in small to medium-sized Swarms and use
    five managers in large to extra-large Swarms. Using more than five managers in
    a Swarm is hardly ever justified.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有管理节点必须与领导节点同步通信以在集群中做出决策，随着我们形成共识组的管理节点数量增加，决策过程变得越来越慢。Docker的推荐做法是在开发、演示或测试环境中使用一个管理节点。在小型到中型Swarm中使用三个管理节点，在大型到超大型Swarm中使用五个管理节点。在Swarm中使用超过五个管理节点几乎是没有必要的。
- en: 'The manager nodes are not only responsible for managing the Swarm but also
    for maintaining the state of the Swarm. What do we mean by that? When we talk
    about the state of the Swarm, we mean all the information about it—for example,
    how many nodes are in the Swarm and what the properties of each node are, such
    as the name or IP address. We also mean what containers are running on which node
    in the Swarm and more. What, on the other hand, is not included in the state of
    the Swarm is data produced by the application services running in containers on
    the Swarm. This is called **application data** and is definitely not part of the
    state that is managed by the manager nodes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 管理节点不仅负责管理Swarm，还负责维护Swarm的状态。我们说的“状态”是什么意思？当我们谈论Swarm的状态时，我们指的是关于它的所有信息——例如，Swarm中有多少个节点，每个节点的属性是什么，如名称或IP地址。我们还指的是哪些容器在Swarm中的哪个节点上运行等等。而Swarm的状态中不包含的是由在Swarm上运行的容器中的应用服务所产生的数据。这些被称为**应用数据**，绝对不属于由管理节点管理的状态：
- en: '![Figure 14.3 – A Swarm manager consensus group](img/Figure_14.03_B19199.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3 – 一个Swarm管理节点共识组](img/Figure_14.03_B19199.jpg)'
- en: Figure 14.3 – A Swarm manager consensus group
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – 一个Swarm管理节点共识组
- en: All of the Swarm states are stored in a high-performance **key-value store**
    (**kv-store**) on each manager node. That’s right, each manager node stores a
    complete replica of the whole Swarm state. This redundancy makes the Swarm highly
    available. If a manager node goes down, the remaining managers all have the complete
    state at hand.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的Swarm状态都存储在每个管理节点上的高性能**键值存储**（**kv-store**）中。没错，每个管理节点都存储整个Swarm状态的完整副本。这种冗余使Swarm具有高度可用性。如果一个管理节点发生故障，其余的管理节点都可以快速访问完整的状态。
- en: If a new manager joins the consensus group, then it synchronizes the Swarm state
    with the existing members of the group until it has a complete replica. This replication
    is usually pretty fast in typical Swarms but can take a while if the Swarm is
    big and many applications are running on it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个新的管理节点加入共识组，那么它会与该组的现有成员同步Swarm状态，直到它拥有完整的副本。在典型的Swarm中，这种复制通常非常快速，但如果Swarm很大并且上面运行着许多应用程序，它可能需要一些时间。
- en: Swarm workers
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Swarm工作节点
- en: As we mentioned earlier, a Swarm worker node is meant to host and run containers
    that contain the actual application services we’re interested in running on our
    cluster. They are the workhorses of the Swarm. In theory, a manager node can also
    be a worker. But, as we already said, this is not recommended on a production
    system. On a production system, we should let managers be managers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Swarm工作节点的任务是托管和运行包含实际应用服务的容器，这些是我们希望在集群中运行的服务。它们是Swarm的“工作马”。理论上，管理节点也可以是工作节点。但正如我们所说的，这在生产系统中并不推荐。在生产系统中，我们应该让管理节点专职管理。
- en: Worker nodes communicate with each other over the so-called control plane. They
    use the gossip protocol for their communication. This communication is asynchronous,
    which means that, at any given time, it is likely that not all worker nodes are
    in perfect sync.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点通过所谓的控制平面相互通信。它们使用gossip协议进行通信。这种通信是异步的，这意味着在任何给定的时刻，不是所有工作节点都处于完美同步状态。
- en: 'Now, you might ask—what information do worker nodes exchange? It is mostly
    information that is needed for service discovery and routing, that is, information
    about which containers are running on with nodes and more:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能会问——工作节点交换哪些信息？这些信息主要是服务发现和路由所需的信息，也就是关于哪些容器正在运行在什么节点上的信息，等等：
- en: '![Figure 14.4 – Worker nodes communicating with each other](img/Figure_14.04_B19199.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4 – 工作节点之间的通信](img/Figure_14.04_B19199.jpg)'
- en: Figure 14.4 – Worker nodes communicating with each other
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – 工作节点之间的通信
- en: In the preceding diagram, you can see how workers communicate with each other.
    To make sure the gossiping scales well in a large Swarm, each worker node only
    synchronizes its own state with three random neighbors. For those who are familiar
    with Big O notation, that means that the synchronization of the worker nodes using
    the gossip protocol scales with O(0).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，你可以看到工作节点之间是如何相互通信的。为了确保在大规模 Swarm 中传播（gossip）能够良好扩展，每个工作节点仅与三个随机邻居同步自己的状态。对于熟悉大
    O 符号的人来说，这意味着使用传播协议的工作节点同步的扩展是 O(0)。
- en: Big O notation explained
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 大 O 符号解释
- en: Big O notation is a way to describe the speed or complexity of a given algorithm.
    It tells you the number of operations an algorithm will make. It’s used to communicate
    how fast an algorithm is, which can be important when evaluating other people’s
    algorithms, and when evaluating your own.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大 O 符号是一种描述给定算法的速度或复杂度的方式。它告诉你一个算法将执行多少次操作。它用于传达一个算法的速度，这在评估他人的算法和自己算法时都非常重要。
- en: For example, let’s say you have a list of numbers and you want to find a specific
    number in the list. There are different algorithms you can use to do this, such
    as simple search or binary search. Simple search checks each number in the list
    one by one until it finds the number you’re looking for. Binary search, on the
    other hand, repeatedly divides the list in half until it finds the number you’re
    looking for.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个数字列表，并且你想在列表中查找一个特定的数字。你可以使用不同的算法来完成这个任务，比如简单查找或二分查找。简单查找会逐个检查列表中的数字，直到找到你要找的数字。另一方面，二分查找则会反复将列表分成两半，直到找到你要找的数字。
- en: Now, let’s say you have a list of 100 numbers. With simple search, in the worst
    case, you’ll have to check all 100 numbers, so it takes 100 operations. With binary
    search, in the worst case, you’ll only have to check about 7 numbers (because
    log2(100) is roughly 7), so it takes 7 operations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你有一个包含 100 个数字的列表。对于简单查找，最坏情况下，你需要检查所有 100 个数字，所以需要 100 次操作。而对于二分查找，最坏情况下，你只需检查大约
    7 个数字（因为 log2(100) 大约是 7），所以只需要 7 次操作。
- en: In this example, binary search is faster than simple search. But what if you
    have a list of 1 billion numbers? Simple search would take 1 billion operations,
    while binary search would take only about 30 operations (because log2(1 billion)
    is roughly 30). So, as the list gets bigger, binary search becomes much faster
    than simple search.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，二分查找比简单查找要快。但如果你有一个包含 10 亿个数字的列表呢？简单查找需要进行 10 亿次操作，而二分查找只需要大约 30 次操作（因为
    log2(10 亿) 大约是 30）。因此，随着列表的增大，二分查找比简单查找要快得多。
- en: Big O notation is used to describe this difference in speed between algorithms.
    In Big O notation, simple search is described as O(n), which means that the number
    of operations grows linearly with the size of the list (n). Binary search is described
    as O(log n), which means that the number of operations grows logarithmically with
    the size of the list.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 大 O 符号用于描述算法之间速度的差异。在大 O 符号中，简单查找被描述为 O(n)，这意味着操作的数量随着列表大小（n）的增长呈线性增长。二分查找被描述为
    O(log n)，这意味着操作的数量随着列表大小的增长呈对数增长。
- en: Worker nodes are kind of passive. They never actively do anything other than
    run the workloads they get assigned by the manager nodes. The worker makes sure,
    though, that it runs these workloads to the best of its capabilities. Later on
    in this chapter, we will get to know more about exactly what workloads the worker
    nodes are assigned by the manager nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点是被动的。它们除了运行由管理节点分配的工作负载外，通常不会主动做其他任何事情。不过，工作节点会确保以其最大能力运行这些工作负载。在本章稍后部分，我们将详细了解管理节点分配给工作节点的具体工作负载。
- en: Now that we know what master and worker nodes in a Docker Swarm are, we are
    going to introduce stacks, services, and tasks next.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了 Docker Swarm 中的主节点和工作节点，我们将介绍堆栈、服务和任务。
- en: Stacks, services, and tasks
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆栈、服务和任务
- en: When using a Docker Swarm versus a single Docker host, there is a paradigm change.
    Instead of talking about individual containers that run processes, we are abstracting
    away to services that represent a set of replicas of each process, and, in this
    way, become highly available. We also do not speak anymore of individual Docker
    hosts with well-known names and IP addresses to which we deploy containers; we’ll
    now be referring to clusters of hosts to which we deploy services. We don’t care
    about an individual host or node anymore. We don’t give it a meaningful name;
    each node rather becomes a number to us.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker Swarm 而非单个 Docker 主机时，出现了范式的变化。我们不再谈论运行进程的单个容器，而是将其抽象为表示每个进程副本集合的服务，通过这种方式实现高可用性。我们也不再谈论拥有固定名称和
    IP 地址的单个 Docker 主机来部署容器；现在我们将谈论部署服务的主机集群。我们不再关心单个主机或节点，我们不再给它赋予有意义的名称；每个节点对我们来说只是一个数字。
- en: 'We also don’t care about individual containers and where they are deployed
    any longer—we just care about having a desired state defined through a service.
    We can try to depict that as shown in the following diagram:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在不再关心单个容器以及它们的部署位置——我们只关心通过服务定义的期望状态。我们可以尝试通过下面的图示来表示这一点：
- en: '![Figure 14.5 – Containers are deployed to well-known servers](img/Figure_14.05_B19199.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5 – 容器被部署到已知的服务器上](img/Figure_14.05_B19199.jpg)'
- en: Figure 14.5 – Containers are deployed to well-known servers
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – 容器被部署到已知的服务器上
- en: 'Instead of deploying individual containers to well-known servers like in the
    preceding diagram, where we deploy the web container to the alpha server with
    the IP address `52.120.12.1`, and the payments container to the beta server with
    the IP `52.121.24.33`, we switch to this new paradigm of services and Swarms (or,
    more generally, clusters):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面图示中的做法不同，之前我们将 web 容器部署到 IP 地址为 `52.120.12.1` 的 alpha 服务器，将支付容器部署到 IP 为 `52.121.24.33`
    的 beta 服务器，现在我们切换到这个新的服务和 Swarm（或者更广义的集群）范式：
- en: '![Figure 14.6 – Services are deployed to Swarms](img/Figure_14.06_B19199.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.6 – 服务被部署到 Swarm 集群](img/Figure_14.06_B19199.jpg)'
- en: Figure 14.6 – Services are deployed to Swarms
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 – 服务被部署到 Swarm 集群
- en: 'In the preceding diagram, we see that a web service and an inventory service
    are both deployed to a Swarm that consists of many nodes. Each of the services
    has a certain number of replicas: five for web and seven for inventory. We don’t
    really care which node the replicas will run on; we only care that the requested
    number of replicas is always running on whatever nodes the Swarm scheduler decides
    to put them on.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们看到一个 web 服务和一个库存服务都被部署到由多个节点组成的 Swarm 集群中。每个服务都有一定数量的副本：web 服务有五个副本，库存服务有七个副本。我们并不关心这些副本会在哪个节点上运行；我们只关心所请求的副本数量始终在
    Swarm 调度器决定将它们放在哪些节点上时保持运行。
- en: That said, let’s now introduce the concept of a service in the context of a
    Docker swarm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，现在让我们介绍一下在 Docker Swarm 中服务的概念。
- en: Services
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务
- en: 'A Swarm service is an abstract thing. It is a description of the desired state
    of an application or application service that we want to run in a Swarm. The Swarm
    service is like a manifest describing things such as the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 服务是一个抽象概念。它是我们希望在 Swarm 中运行的应用程序或应用服务的期望状态描述。Swarm 服务就像一个清单，描述以下内容：
- en: The name of the service
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的名称
- en: The image from which to create the containers
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于创建容器的镜像
- en: The number of replicas to run
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行的副本数量
- en: The network(s) that the containers of the service are attached to
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务容器所连接的网络
- en: The ports that should be mapped
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应映射的端口
- en: Having this service manifest, the Swarm manager then makes sure that the described
    desired state is always reconciled if the actual state should ever deviate from
    it. So, if, for example, one instance of the service crashes, then the scheduler
    on the Swarm manager schedules a new instance of this particular service on a
    node with free resources so that the desired state is re-established.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个服务清单后，Swarm 管理器会确保如果实际状态与期望状态发生偏差时，始终会将它们调整回期望状态。所以，例如，如果某个服务的实例崩溃了，Swarm
    管理器上的调度器就会在有空闲资源的节点上调度该服务的一个新实例，以便重新建立期望的状态。
- en: Now, what is a task? This is what we’re going to learn next.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，任务是什么呢？这就是我们接下来要学习的内容。
- en: Tasks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务
- en: We have learned that a service corresponds to a description of the desired state
    in which an application service should be at all times. Part of that description
    was the number of replicas the service should be running. Each replica is represented
    by a task. In this regard, a Swarm service contains a collection of tasks. On
    Docker Swarm, a task is an atomic unit of deployment. Each task of a service is
    deployed by the Swarm scheduler to a worker node. The task contains all of the
    necessary information that the worker node needs to run a container based on the
    image, which is part of the service description. Between a task and a container,
    there is a one-to-one relation. The container is the instance that runs on the
    worker node, while the task is the description of this container as a part of
    a Swarm service.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，服务对应的是应用服务应始终处于的期望状态的描述。该描述的一部分是服务应运行的副本数量。每个副本由一个任务表示。在这方面，Swarm 服务包含一个任务集合。在
    Docker Swarm 中，任务是一个原子部署单元。服务的每个任务都由 Swarm 调度器部署到一个工作节点。任务包含工作节点运行基于镜像的容器所需的所有信息，而镜像是服务描述的一部分。在任务和容器之间，存在一对一的关系。容器是运行在工作节点上的实例，而任务是容器作为
    Swarm 服务一部分的描述。
- en: Finally, let’s talk about a stack in the context of a Docker swarm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在 Docker Swarm 的背景下讨论一下栈。
- en: Stacks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 栈
- en: Now that we have a good idea about what a Swarm service is and what tasks are,
    we can introduce the stack. A stack is used to describe a collection of Swarm
    services that are related, most probably because they are part of the same application.
    In that sense, we could also say that a stack describes an application that consists
    of one-to-many services that we want to run on the Swarm.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Swarm 服务和任务有了很好的了解，接下来我们可以介绍栈。栈用于描述一组相关的 Swarm 服务，它们很可能是因为属于同一个应用程序而关联的。从这个意义上讲，我们也可以说栈描述的是一个由一个或多个服务组成的应用程序，我们希望在
    Swarm 上运行这些服务。
- en: Typically, we describe a stack declaratively in a text file that is formatted
    using the YAML format and that uses the same syntax as the already known Docker
    Compose file. This leads to a situation where people sometimes say that a stack
    is described by a Docker Compose file. A better wording would be that a stack
    is described in a stack file that uses similar syntax to a Docker Compose file.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在一个使用 YAML 格式的文本文件中声明一个栈，并且该文件使用与已知的 Docker Compose 文件相同的语法。这导致了一种情况，人们有时会说栈是由
    Docker Compose 文件描述的。更好的说法是，栈是在一个使用与 Docker Compose 文件相似语法的栈文件中描述的。
- en: 'Let’s try to illustrate the relationship between the stack, services, and tasks
    in the following diagram and connect it with the typical content of a stack file:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过以下图示来说明栈、服务和任务之间的关系，并将其与栈文件的典型内容联系起来：
- en: '![Figure 14.7 – Diagram showing the relationship between stack, services, and
    tasks](img/Figure_14.07_B19199.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7 – 显示栈、服务和任务之间关系的图示](img/Figure_14.07_B19199.jpg)'
- en: Figure 14.7 – Diagram showing the relationship between stack, services, and
    tasks
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 – 显示栈、服务和任务之间关系的图示
- en: In the preceding diagram, we see on the right-hand side a declarative description
    of a sample Stack. The Stack consists of three services, called `web`, `payments`,
    and `inventory`. We also see that the `web` service uses the `example/web:1.0`
    image and has four replicas. On the left-hand side of the diagram, we see that
    the Stack embraces the three services mentioned. Each service, in turn, contains
    a collection of Tasks, as many as there are replicas. In the case of the `web`
    service, we have a collection of four Tasks. Each Task contains the name of the
    Image from which it will instantiate a container once the Task is scheduled on
    a Swarm node.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到右侧是一个示例栈的声明式描述。该栈包含三个服务，分别是 `web`、`payments` 和 `inventory`。我们还看到，`web`
    服务使用的是 `example/web:1.0` 镜像，并且有四个副本。在图示的左侧，我们看到栈包含了前面提到的三个服务。每个服务又包含了若干任务，副本数目就是任务的数量。在
    `web` 服务的情况下，我们有一个包含四个任务的集合。每个任务包含将从其启动容器的镜像名称，一旦任务被调度到 Swarm 节点上，容器便会启动。
- en: Now that you have a good understanding of the main concepts of a Docker swarm,
    such as nodes, stack, services, and tasks, let’s look a bit more closely into
    the networking used in a swarm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你已经对 Docker Swarm 的主要概念有了很好的理解，比如节点、栈、服务和任务，让我们更仔细地看看在 Swarm 中使用的网络。
- en: Multi-host networking
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多主机网络
- en: 'In [*Chapter 10*](B19199_10.xhtml#_idTextAnchor218), *Using Single-Host Networking*,
    we discussed how containers communicate on a single Docker host. Now, we have
    a Swarm that consists of a cluster of nodes or Docker hosts. Containers that are
    located on different nodes need to be able to communicate with each other. Many
    techniques can help us to achieve this goal. Docker has chosen to implement an
    overlay network driver for Docker Swarm. This overlay network allows containers
    attached to the same overlay network to discover each other and freely communicate
    with each other. The following is a schema for how an overlay network works:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 10 章*](B19199_10.xhtml#_idTextAnchor218)，*使用单主机网络*中，我们讨论了容器如何在单一 Docker
    主机上进行通信。现在，我们有一个由多个节点或 Docker 主机构成的 Swarm 集群。位于不同节点上的容器需要能够相互通信。许多技术可以帮助我们实现这个目标。Docker
    选择为 Docker Swarm 实现一个覆盖网络驱动程序。这个覆盖网络允许连接到同一覆盖网络的容器相互发现并自由通信。以下是覆盖网络工作原理的示意图：
- en: '![Figure 14.8 – The overlay network](img/Figure_14.08_B19199.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.8 – 覆盖网络](img/Figure_14.08_B19199.jpg)'
- en: Figure 14.8 – The overlay network
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8 – 覆盖网络
- en: We have two nodes or Docker hosts with the IP addresses `172.10.0.15` and `172.10.0.16`.
    The values we have chosen for the IP addresses are not important; what is important
    is that both hosts have a distinct IP address and are connected by a physical
    network (a network cable), which is called the **underlay network**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个节点或 Docker 主机，IP 地址分别为`172.10.0.15`和`172.10.0.16`。我们选择的 IP 地址值并不重要；重要的是这两个主机有不同的
    IP 地址，并且通过物理网络（网络电缆）连接，这个物理网络被称为**底层网络**。
- en: On the node on the left-hand side, we have a container running with the IP address
    `10.3.0.2`, and on the node on the right-hand side, we have another container
    with the IP address `10.3.0.5`. Now, the former container wants to communicate
    with the latter. How can this happen? In [*Chapter 10*](B19199_10.xhtml#_idTextAnchor218),
    *Using* *Single-Host Networking*, we saw how this works when both containers are
    located on the same node—by using a Linux bridge. But Linux bridges only operate
    locally and cannot span across nodes. So, we need another mechanism. Linux VXLAN
    comes to the rescue. VXLAN has been available on Linux since way before containers
    were a thing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的节点上，我们有一个运行中的容器，IP 地址为`10.3.0.2`；在右侧的节点上，我们有另一个容器，IP 地址为`10.3.0.5`。现在，前者容器想要与后者容器通信。这个过程怎么实现呢？在[*第
    10 章*](B19199_10.xhtml#_idTextAnchor218)，*使用单主机网络*中，我们已经看到当两个容器位于同一节点时，如何通过使用
    Linux 桥接来实现这种通信。但 Linux 桥接仅在本地运行，无法跨节点工作。所以，我们需要其他机制。此时，Linux VXLAN 来到救援。VXLAN
    从容器技术出现之前就已在 Linux 中可用。
- en: VXLAN explained
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN 解释
- en: '**VXLAN**, or **Virtual eXtensible Local Area Network**, is a networking protocol
    that allows for the creation of virtual layer 2 domains over an IP network using
    the UDP protocol. It was designed to solve the problem of limited VLAN IDs (4,096)
    in IEEE 802.1q by expanding the size of the identifier to 24 bits (16,777,216).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**VXLAN**，即**虚拟扩展局域网**，是一种网络协议，它通过使用 UDP 协议在 IP 网络上创建虚拟的二层域。它的设计目的是解决 IEEE
    802.1q 中 VLAN ID 数量有限（4,096）的难题，通过将标识符的大小扩展到 24 位（16,777,216）。'
- en: In simpler terms, VXLAN allows for the creation of virtual networks that can
    span across different physical locations. For example, certain VMs that are running
    on different hosts can communicate over a VXLAN tunnel. The hosts can be in different
    subnets or even in different data centers around the world. From the perspective
    of the VMs, other VMs in the same VXLAN are within the same layer 2 domain.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，VXLAN 允许创建可以跨越不同物理位置的虚拟网络。例如，某些运行在不同主机上的虚拟机可以通过 VXLAN 隧道进行通信。这些主机可以位于不同的子网，甚至在全球不同的数据中心。从虚拟机的角度来看，同一
    VXLAN 中的其他虚拟机在同一个二层域内。
- en: When the left-hand container in *Figure 14**.8* sends a data packet, the bridge
    realizes that the target of the packet is not on this host. Now, each node participating
    in an overlay network gets a so-called **VXLAN Tunnel Endpoint** (**VTEP**) object,
    which intercepts the packet (the packet at that moment is an OSI layer 2 data
    packet), wraps it with a header containing the target IP address of the host that
    runs the destination container (this now makes it an OSI layer 3 data packet),
    and sends it over the VXLAN tunnel. The VTEP on the other side of the tunnel unpacks
    the data packet and forwards it to the local bridge, which in turn forwards it
    to the destination container.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *图 14.8* 中左侧的容器发送数据包时，桥接器意识到该数据包的目标不在此主机上。现在，每个参与 Overlay 网络的节点都会获得一个所谓的 **VXLAN
    隧道端点**（**VTEP**）对象，它会拦截数据包（此时的数据包是 OSI 第 2 层的数据包），并用一个包含目标主机的 IP 地址的标头将其包装起来（这将其转变为
    OSI 第 3 层的数据包），然后通过 VXLAN 隧道发送。隧道另一端的 VTEP 会解包数据包并将其转发给本地桥接器，本地桥接器再将其转发给目标容器。
- en: The overlay driver is included in SwarmKit and is in most cases the recommended
    network driver for Docker Swarm. There are other multi-node-capable network drivers
    available from third parties that can be installed as plugins in each participating
    Docker host. Certified network plugins are available from the Docker store.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Overlay 驱动程序包含在 SwarmKit 中，并且在大多数情况下是 Docker Swarm 推荐的网络驱动程序。还有其他可以支持多节点的第三方网络驱动程序，可以作为插件安装在每个参与的
    Docker 主机中。经过认证的网络插件可以从 Docker 商店获得。
- en: Great, we have all the basic knowledge about a Docker swarm. So, let’s create
    one.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们已经掌握了关于 Docker Swarm 的所有基础知识。那么，接下来我们就来创建一个。
- en: Creating a Docker Swarm
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Docker Swarm
- en: Creating a Docker Swarm is almost trivial. It is so easy that if you know how
    orchestrators work, it might even seem unbelievable. But it is true, Docker has
    done a fantastic job in making Swarms simple and elegant to use. At the same time,
    Docker Swarm has been proven to be very robust and scalable when used by large
    enterprises.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Docker Swarm 几乎是微不足道的。它非常简单，以至于如果你了解编排器的工作原理，可能会觉得这几乎难以置信。但这是真的，Docker 在使
    Swarm 变得简单而优雅方面做得非常出色。同时，Docker Swarm 已被证明在大企业使用时非常稳健且可扩展。
- en: Creating a local single-node swarm
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个本地单节点 Swarm
- en: So, enough imagining—let’s demonstrate how we can create a Swarm. In its most
    simple form, a fully functioning Docker Swarm consists only of a single node.
    If you’re using Docker Desktop, or even if you’re using Docker Toolbox, then your
    personal computer or laptop is such a node. Hence, we can start right there and
    demonstrate some of the most important features of a Swarm.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，够了，别再想象了——让我们演示一下如何创建一个 Swarm。在最简单的形式下，一个完全运行的 Docker Swarm 只包含一个节点。如果你使用
    Docker Desktop，甚至是 Docker Toolbox，那么你的个人电脑或笔记本电脑就是这样的一个节点。因此，我们可以从这里开始，并演示一些 Swarm
    的最重要功能。
- en: 'Let’s initialize a Swarm. On the command line, just enter the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一个 Swarm。在命令行中，只需输入以下命令：
- en: '[PRE0]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After an incredibly short time, you should see an output like the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 经过极短的时间后，你应该看到如下输出：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our computer is now a Swarm node. Its role is that of a manager and it is the
    leader (of the managers, which makes sense since there is only one manager at
    this time). Although it took only a very short time to finish `docker swarm init`,
    the command did a lot of things during that time. Some of them are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算机现在是一个 Swarm 节点。它的角色是管理者，并且是领导者（在管理者中是领导者，因为目前只有一个管理者）。虽然 `docker swarm
    init` 命令仅用了非常短的时间就完成，但在此期间，该命令做了很多事情。以下是其中的一些：
- en: It created a root **Certificate** **Authority** (**CA**)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个根 **证书** **授权中心**（**CA**）
- en: It created a kv-store that is used to store the state of the whole Swarm
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个 kv-store，用来存储整个 Swarm 的状态
- en: 'Now, in the preceding output, we can see a command that can be used to join
    other nodes to the Swarm that we just created. The command is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在前面的输出中，我们可以看到一个命令，可以用来将其他节点加入我们刚刚创建的 Swarm。该命令如下：
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, we have the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '`<join-token>` is a token generated by the Swarm leader at the time the Swarm
    was initialized'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<join-token>` 是 Swarm 领导者在初始化 Swarm 时生成的令牌'
- en: '`<IP address>` is the IP address of the leader'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<IP 地址>` 是领导者的 IP 地址'
- en: 'Although our cluster remains simple, as it consists of only one member, we
    can still ask the Docker CLI to list all of the nodes of the Swarm using the `docker
    node ls` command. This will look similar to the following screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的集群保持简单，因为它只包含一个成员，但我们仍然可以要求 Docker CLI 列出 Swarm 的所有节点，使用 `docker node ls`
    命令。这将类似于以下截屏：
- en: '![Figure 14.9 – Listing the nodes of the Docker Swarm](img/Figure_14.09_B19199.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.9 – 显示 Docker Swarm 的节点](img/Figure_14.09_B19199.jpg)'
- en: Figure 14.9 – Listing the nodes of the Docker Swarm
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9 – 列出 Docker Swarm 的节点
- en: In this output, we first see the ID that was given to the node. The star (`*`)
    that follows the ID indicates that this is the node on which `docker node ls`
    was executed—basically saying that this is the active node. Then, we have the
    (human-readable) name of the node and its status, availability, and manager status.
    As mentioned earlier, this very first node of the Swarm automatically became the
    leader, which is indicated in the preceding screenshot. Lastly, we see which version
    of Docker Engine we’re using.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，我们首先看到赋予节点的ID。跟随ID的星号（`*`）表示这是执行了 `docker node ls` 命令的节点—基本上表明这是活动节点。然后，我们有节点的（人类可读的）名称及其状态、可用性和管理状态。正如前面提到的，这个
    Swarm 的第一个节点自动成为了领导者，这在前面的截屏中已经显示出来了。最后，我们看到我们正在使用的 Docker Engine 版本。
- en: 'To get even more information about a node, we can use the `docker node inspect`
    command, as shown in the following truncated output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取关于节点的更多信息，我们可以使用 `docker node inspect` 命令，如下截断输出所示：
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There is a lot of information generated by this command, so we have only presented
    a shortened version of the output. This output can be useful, for example, when
    you need to troubleshoot a misbehaving cluster node.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令生成了大量信息，因此我们只呈现了输出的缩短版本。例如，在需要排除集群节点行为不端时，此输出非常有用。
- en: 'Before you continue, don’t forget to shut down or dissolve the swarm by using
    the following command:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请不要忘记使用以下命令关闭或解散该群集：
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the next section, we will use the PWD environment to generate and use a Docker
    Swarm.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用 PWD 环境来生成和使用 Docker Swarm。
- en: Using PWD to generate a Swarm
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PWD 生成 Swarm
- en: To experiment with Docker Swarm without having to install or configure anything
    locally on our computer, we can use PWD. PWD is a website that can be accessed
    with a browser and that offers us the ability to create a Docker Swarm consisting
    of up to five nodes. It is definitely a playground, as the name implies, and the
    time for which we can use it is limited to four hours per session. We can open
    as many sessions as we want, but each session automatically ends after four hours.
    Other than that, it is a fully functional Docker environment that is ideal for
    tinkering with Docker or demonstrating some features.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要试验 Docker Swarm 而不必在本地计算机上安装或配置任何东西，我们可以使用 PWD。PWD 是一个可以通过浏览器访问的网站，提供了创建最多五个节点的
    Docker Swarm 的能力。正如其名称所示，它绝对是一个游乐场，并且我们可以使用它的时间限制为每个会话四个小时。我们可以打开任意多个会话，但每个会话在四小时后会自动结束。除此之外，它是一个完全功能的
    Docker 环境，非常适合玩弄 Docker 或展示一些功能。
- en: 'Let’s access the site now. In your browser, navigate to the website [https://labs.play-with-docker.com](https://labs.play-with-docker.com).
    You will be presented with a welcome and login screen. Use your Docker ID to log
    in. After successfully doing so, you will be presented with a screen that looks
    like the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在访问该网站。在浏览器中导航至网站 [https://labs.play-with-docker.com](https://labs.play-with-docker.com)。您将看到一个欢迎和登录界面。使用您的
    Docker ID 登录。成功登录后，您将看到一个类似以下截屏的界面：
- en: '![Figure 14.10 – PWD window](img/Figure_14.10_B19199.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.10 – PWD 窗口](img/Figure_14.10_B19199.jpg)'
- en: Figure 14.10 – PWD window
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10 – PWD 窗口
- en: 'As we can see immediately, there is a big timer counting down from four hours.
    That’s how much time we have left to play in this session. Furthermore, we see
    an **+ ADD NEW INSTANCE** link. Click it to create a new Docker host. When you
    do that, your screen should look as in the following screenshot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即可以看到一个大计时器，从四小时开始倒计时。这就是我们在此会话中可以使用的时间。此外，我们看到一个**+ 添加新实例**链接。点击它以创建一个新的
    Docker 主机。当你这样做时，你的屏幕应该如下截屏所示：
- en: '![Figure 14.11 – PWD with one new node](img/Figure_14.11_B19199.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.11 – PWD 带有一个新节点](img/Figure_14.11_B19199.jpg)'
- en: Figure 14.11 – PWD with one new node
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.11 – PWD 带有一个新节点
- en: On the left-hand side, we see the newly created node with its IP address (`192.168.0.13`)
    and its name (`node1`). On the right-hand side, we have some additional information
    about this new node in the upper half of the screen and a terminal in the lower
    half. Yes, this terminal is used to execute commands on this node that we just
    created. This node has the Docker CLI installed, and hence we can execute all
    of the familiar Docker commands on it, such as the Docker version. Try it out.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们可以看到新创建的节点及其 IP 地址（`192.168.0.13`）和名称（`node1`）。在右侧，上半部分显示了该新节点的一些附加信息，底部是一个终端窗口。是的，这个终端窗口用于在我们刚创建的节点上执行命令。该节点已安装
    Docker CLI，因此我们可以在其上执行所有熟悉的 Docker 命令，比如 Docker 版本命令。试试看。
- en: 'But now we want to create a Docker Swarm. Execute the following command in
    the terminal in your browser:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们想创建一个 Docker Swarm。在浏览器的终端中执行以下命令：
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output generated by the preceding command is similar to the one we saw when
    creating a local Docker Swarm. The important thing to note is the `join` command,
    which is what we want to use to join additional nodes to the cluster we just created.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令生成的输出与我们在创建本地 Docker Swarm 时看到的类似。需要注意的是 `join` 命令，它是我们希望用来将其他节点加入到刚创建的集群中的命令。
- en: You might have noted that we specified the `--advertise-addr` parameter in the
    Swarm `init` command. Why is that necessary here? The reason is that the nodes
    generated by PWD have more than one IP address associated with them. We can easily
    verify that by executing the `ip` command on the node. This command will show
    us that there are indeed two endpoints, `eth0` and `eth1`, present. We hence have
    to specify explicitly to the new to-be swarm manager which one we want to use.
    In our case, it is `eth0`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到我们在 Swarm `init` 命令中指定了 `--advertise-addr` 参数。为什么在这里需要这个呢？原因是 PWD 生成的节点关联了多个
    IP 地址。我们可以通过在节点上执行 `ip` 命令轻松验证这一点。这个命令会显示出确实存在两个端点，`eth0` 和 `eth1`。因此，我们必须明确指定给新的
    Swarm 管理节点使用哪个 IP 地址。在我们的例子中，是 `eth0`。
- en: Create four additional nodes in PWD by clicking four times on the `node2`, `node3`,
    `node4`, and `node5` and will all be listed on the left-hand side. If you click
    on one of the nodes on the left-hand side, then the right-hand side shows the
    details of the respective node and a terminal window for that node.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PWD 中创建四个额外的节点，通过点击 `node2`、`node3`、`node4` 和 `node5` 四次，它们将会在左侧列出。如果你点击左侧的某个节点，右侧将显示该节点的详细信息以及一个终端窗口。
- en: 'Select each node (2 to 5) and execute the `docker swarm join` command that
    you have copied from the leader node (`node1`) in the respective terminal:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 选择每个节点（2 到 5），并在相应的终端中执行你从主节点（`node1`）复制过来的 `docker swarm join` 命令：
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This node joined the swarm as a worker.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点作为工作节点加入了 Swarm。
- en: 'Once you have joined all four nodes to the Swarm, switch back to `node1` and
    list all nodes:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将所有四个节点加入到 Swarm，切换回 `node1` 并列出所有节点：
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This, unsurprisingly, results in this (slightly reformatted for readability):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这，毫不意外地，生成了如下输出（为了可读性稍作重新格式化）：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Still on `node1`, we can now promote, say, `node2` and `node3`, to make the
    Swarm highly available:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在 `node1` 上，我们现在可以提升，比如将 `node2` 和 `node3` 提升为 Swarm 管理节点，以实现高度可用：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in this output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With this, our Swarm on PWD is ready to accept a workload. We have created a
    highly available Docker Swarm with three manager nodes that form a Raft consensus
    group and two worker nodes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们在 PWD 上的 Swarm 就可以接受工作负载了。我们创建了一个高度可用的 Docker Swarm，包含三个管理节点，它们组成一个 Raft
    共识组，以及两个工作节点。
- en: Creating a Docker Swarm in the cloud
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在云中创建 Docker Swarm
- en: All of the Docker Swarms we have created so far are wonderful to use in development,
    to experiment with, or to use for demonstration purposes. If we want to create
    a Swarm that can be used as a production environment where we run our mission-critical
    applications, though, then we need to create a—I’m tempted to say—real Swarm in
    the cloud or on-premises. In this book, we are going to demonstrate how to create
    a Docker Swarm in AWS.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们创建的所有 Docker Swarm 都非常适合用于开发、实验或演示目的。不过，如果我们想创建一个可以作为生产环境的 Swarm，用来运行我们至关重要的应用程序，那么我们需要在云端或本地创建一个——我敢说——真正的
    Swarm。在本书中，我们将演示如何在 AWS 中创建一个 Docker Swarm。
- en: 'We can manually create a Swarm through the AWS console:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 AWS 控制台手动创建一个 Swarm：
- en: Log in to your AWS account. If you do not have one yet, create a free one.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到你的 AWS 账户。如果你还没有账户，可以创建一个免费的。
- en: 'First, we create an AWS `aws-docker-demo-sg`:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个 AWS `aws-docker-demo-sg`：
- en: Navigate to your default VPC.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left-hand side, select `aws-docker-demo-sg`, as mentioned, and add a
    description such as `A SG for our` `Docker demo`.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, click the `sg-030d0...`)
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom UDP, **Protocol**: UDP, **Port range**: 7946, **Source**:
    Custom'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the value, select the SG just created
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom TCP, **Protocol**: TCP, **Port range**: 7946, **Source**:
    Custom'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the value select the SG just created
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom TCP, **Protocol**: TCP, **Port range**: 4789, **Source**:
    Custom'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the value, select the SG just created
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Type**: Custom TCP, **Protocol**: TCP, **Port range**: 22, **Source**: My
    IP'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This last rule is to be able to access the instances from your host via SSH
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker Swarm ports
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**TCP port 2377**: This is the main communication port for swarm mode. The
    Swarm management and orchestration commands are communicated over this port. It’s
    used for communication between nodes and plays a crucial role in the Raft consensus
    algorithm, which ensures that all the nodes in a swarm act as a single system.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**TCP and UDP port 7946**: This port is used for communication among nodes
    (container network discovery). It helps the nodes in the swarm to exchange information
    about the services and tasks running on each of them.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**UDP port 4789**: This port is used for overlay network traffic. When you
    create an overlay network for your services, Docker Swarm uses this port for the
    data traffic between the containers.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.12 – Inbound rules for the AWS SG](img/Figure_14.12_B19199.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 14.12 – Inbound rules for the AWS SG
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: When done, click **Save rules**.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the EC2 dashboard.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we create a key pair for all EC2 instances we are going to create next:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate and click the `aws-docker-demo`.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure the private key file format is `.pem`.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the `.pem` file in a safe location.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Back on the EC2 dashboard, launch a new EC2 instance with the following settings:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the instance `manager1`.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `t2.micro` as the instance type.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the key pair that we created before, called `aws-docker-demo`.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the existing SG, `aws-docker-demo-sg`, that we created previously.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click the **Launch** button.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the previous step to create two worker nodes and call them `worker1`
    and `worker2`, respectively.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the list of EC2 instances. You may have to wait a few minutes until they
    are all ready.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with the `manager1` instance by selecting it and then clicking the `ssh`.
    Follow those instructions carefully.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once connected to the `manager1` instance, let’s install Docker:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This may take a couple of minutes to finish.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, make sure you can use Docker without having to use the `sudo` command:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To apply the preceding command, you have to quickly exit from the AWS instance:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, immediately connect again using the `ssh` command from *step 6*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Back on the EC2 instance, make sure you can access Docker with the following:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If everything is installed and configured correctly, you should see the version
    information of the Docker client and engine.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Now repeat *steps 6* to *10* for the other two EC2 instances, `worker1` and
    `worker2`.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now go back to your `manager1` instance and initialize a Docker swarm on it:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The output should be the same as you saw for the case when you created a swarm
    locally or on PWD.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Copy the `docker swarm join` command from the preceding output.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to each worker node and run the command. The node should respond with the
    following:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Go back to the `manager1` node and run the following command to list all nodes
    of the swarm:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'What you should see is similar to this:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.13 – List of swarm nodes on AWS](img/Figure_14.13_B19199.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 14.13 – List of swarm nodes on AWS
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a Docker swarm in the (AWS) cloud, let’s deploy a simple application
    to it.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a first application
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have created a few Docker Swarms on various platforms. Once created, a Swarm
    behaves the same way on any platform. The way we deploy and update applications
    on a Swarm is not platform-dependent. It has been one of Docker’s main goals to
    avoid vendor lock-in when using a Swarm. Swarm-ready applications can be effortlessly
    migrated from, say, a Swarm running on-premises to a cloud-based Swarm. It is
    even technically possible to run part of a Swarm on-premises and another part
    in the cloud. It works, yet we have, of course, to consider possible side effects
    due to the higher latency between nodes in geographically distant areas.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a highly available Docker Swarm up and running, it is time
    to run some workloads on it. I’m using the swarm just created on AWS. We’ll start
    by first creating a single service. For this, we need to SSH into one of the manager
    nodes. I selected the swarm node on the `manager1` instance:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We start the deployment of our first application to the swarm by creating a
    service.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A service can be created either as part of a stack or directly using the Docker
    CLI. Let’s first look at a sample stack file that defines a single service:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the Vi editor to create a new file called `stack.yml` and add this content:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Exit the Vi editor by first pressing the *Esc* key, then typing `:wq`, and then
    pressing *Enter*. This will save the code snippet and exit vi.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar with Vi, you can also use nano instead.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, we see what the desired state of a service called
    `whoami` is:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the `training/whoami:latest` image
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers of the service are attached to the `test-net` network
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container port `8000` is published to port `81`
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is running with six replicas (or tasks)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During a rolling update, the individual tasks are updated in batches of two,
    with a delay of 10 seconds between each successful batch
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service (and its tasks and containers) is assigned the two labels, `app`
    and `environment`, with the values `sample-app` and `prod-south`, respectively
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more settings that we could define for a service, but the preceding
    ones are some of the more important ones. Most settings have meaningful default
    values. If, for example, we do not specify the number of replicas, then Docker
    defaults it to `1`. The name and image of a service are, of course, mandatory.
    Note that the name of the service must be unique in the Swarm.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the preceding service, we use the `docker stack deploy` command.
    Assuming that the file in which the preceding content is stored is called `stack.yaml`,
    we have the following:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we have created a stack called `sample-stack` that consists of one service,
    `whoami`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'We can list all stacks on our Swarm:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Upon doing so, we should get this:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can list the services defined in our Swarm, as follows:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.14 – List of all services running in the Swarm](img/Figure_14.14_B19199.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: Figure 14.14 – List of all services running in the Swarm
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: In the output, we can see that currently, we have only one service running,
    which was to be expected. The service has an ID. The format of the ID, contrary
    to what you have used so far for containers, networks, or volumes, is alphanumeric
    (in the latter cases, it was always SHA-256). We can also see that the name of
    the service is a combination of the service name we defined in the stack file
    and the name of the stack, which is used as a prefix. This makes sense since we
    want to be able to deploy multiple stacks (with different names) using the same
    stack file into our Swarm. To make sure that service names are unique, Docker
    decided to combine the service name and stack name.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In the third column, we see the mode, which is replicated. The number of replicas
    is shown as `6/6`. This tells us that six out of the six requested replicas are
    running. This corresponds to the desired state. In the output, we also see the
    image that the service uses and the port mappings of the service.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the service and its tasks
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding output, we cannot see the details of the six replicas that
    have been created.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'To get some deeper insight into that, we can use the `docker service ps <service-id>`
    command. If we execute this command for our service, we will get the following
    output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.15 – Details of the whoami service](img/Figure_14.15_B19199.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: Figure 14.15 – Details of the whoami service
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see the list of six tasks that corresponds to
    the requested six replicas of our `whoami` service. In the **NODE** column, we
    can also see the node to which each task has been deployed. The name of each task
    is a combination of the service name plus an increasing index. Also note that,
    similar to the service itself, each task gets an alphanumeric ID assigned.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, apparently tasks 3 and 6, with the names `sample-stack_whoami.3`
    and `sample-stack_whoami.6`, have been deployed to `ip-172-31-32-21`, which is
    the leader of our Swarm. Hence, I should find a container running on this node.
    Let’s see what we get if we list all containers running on `ip-172-31-32-21`:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 就我而言，显然任务 3 和 6，名称分别是 `sample-stack_whoami.3` 和 `sample-stack_whoami.6`，已经部署到
    `ip-172-31-32-21`，这是我们 Swarm 的领导节点。因此，我应该会在此节点上找到一个正在运行的容器。让我们看看如果列出 `ip-172-31-32-21`
    上的所有容器会得到什么：
- en: '![Figure 14.16 – List of containers on node ip-172-31-32-21](img/Figure_14.16_B19199.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.16 – 节点 ip-172-31-32-21 上的容器列表](img/Figure_14.16_B19199.jpg)'
- en: Figure 14.16 – List of containers on node ip-172-31-32-21
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.16 – 节点 ip-172-31-32-21 上的容器列表
- en: 'As expected, we find a container running from the `training/whoami:latest`
    image with a name that is a combination of its parent task name and ID. We can
    try to visualize the whole hierarchy of objects that we generated when deploying
    our sample stack:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们发现一个容器正在运行，来自 `training/whoami:latest` 镜像，容器名称是其父任务名称和 ID 的组合。我们可以尝试可视化在部署我们的示例堆栈时生成的所有对象层次结构：
- en: '![Figure 14.17 – Object hierarchy of a Docker Swarm stack](img/Figure_14.17_B19199.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.17 – Docker Swarm 堆栈的对象层次结构](img/Figure_14.17_B19199.jpg)'
- en: Figure 14.17 – Object hierarchy of a Docker Swarm stack
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.17 – Docker Swarm 堆栈的对象层次结构
- en: 'A stack can consist of one-to-many services. Each service has a collection
    of tasks. Each task has a one-to-one association with a container. Stacks and
    services are created and stored on the Swarm manager nodes. Tasks are then scheduled
    to Swarm worker nodes, where the worker node creates the corresponding container.
    We can also get some more information about our service by inspecting it. Execute
    the following command:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一个堆栈可以由一个或多个服务组成。每个服务都有一组任务。每个任务与一个容器一一对应。堆栈和服务是在 Swarm 管理节点上创建和存储的。任务随后被调度到
    Swarm 工作节点，在工作节点上创建相应的容器。我们还可以通过检查服务来获取更多关于服务的信息。执行以下命令：
- en: '[PRE24]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This provides a wealth of information about all of the relevant settings of
    the service. This includes those we have explicitly defined in our `stack.yaml`
    file, but also those that we didn’t specify and that, therefore, got their default
    values assigned. We’re not going to list the whole output here, as it is too long,
    but I encourage you to inspect it on your own machine. We will discuss part of
    the information in more detail in the *The swarm routing mesh* section in [*Chapter
    15*](B19199_15.xhtml#_idTextAnchor328).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了关于服务所有相关设置的丰富信息。这些包括我们在 `stack.yaml` 文件中明确定义的设置，但也包括我们没有指定的设置，因此它们被分配了默认值。我们不会在此列出完整的输出，因为它太长，但我鼓励你在自己的机器上检查。我们将在
    [*第 15 章*](B19199_15.xhtml#_idTextAnchor328) 的 *Swarm 路由网格* 部分中详细讨论部分信息。
- en: Testing the load balancing
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试负载均衡
- en: 'To see that the swarm load balances incoming requests to our sample `whoami`
    application, we can use the `curl` tool. Execute the following command a few times
    and observe how the answer changes:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看 Swarm 如何将传入请求负载均衡到我们的示例 `whoami` 应用程序，我们可以使用 `curl` 工具。多次执行以下命令并观察答案的变化：
- en: '[PRE25]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This results in an output like this:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生如下输出：
- en: '[PRE26]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that after the sixth item, the sequence is repeating. This is due to the
    fact that the Docker swarm is load balancing calls using a round-robin algorithm.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第六项之后，序列开始重复。这是因为 Docker Swarm 使用轮询算法来进行负载均衡。
- en: Logs of a service
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务日志
- en: 'In an earlier chapter, we worked with the logs produced by a container. Here,
    we’re concentrating on a service. Remember that, ultimately, a service with many
    replicas has many containers running. Hence, we would expect that, if we asked
    the service for its logs, Docker would return an aggregate of all logs of those
    containers belonging to the service. And indeed, we’ll see this when we use the
    `docker service` `logs` command:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们处理过容器生成的日志。在这里，我们专注于服务。记住，最终，一个具有多个副本的服务会运行多个容器。因此，我们可以预计，如果我们请求该服务的日志，Docker
    会返回该服务所有容器日志的汇总。事实上，当我们使用 `docker service` `logs` 命令时，我们会看到这一点：
- en: '[PRE27]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is what we get:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的结果：
- en: '![Figure 14.18 – Logs of the whoami service](img/Figure_14.18_B19199.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.18 – whoami 服务的日志](img/Figure_14.18_B19199.jpg)'
- en: Figure 14.18 – Logs of the whoami service
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.18 – whoami 服务的日志
- en: There is not much information in the logs at this point, but it is enough to
    discuss what we get. The first part of each line in the log always contains the
    name of the container combined with the node name from which the log entry originates.
    Then, separated by the vertical bar (`Listening` `on :8000`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The aggregated logs that we get with the `docker service logs` command are not
    sorted in any particular way. So, if the correlation of events is happening in
    different containers, you should add information to your log output that makes
    this correlation possible.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this is a timestamp for each log entry. But this has to be done at
    the source; for example, the application that produces a log entry needs to also
    make sure a timestamp is added.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also query the logs of an individual task of the service by providing
    the task ID instead of the service ID or name. So, say we queried the logs from
    task 6 with the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This gives us the following output:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the next section, we are investigating how the swarm reconciles the desired
    state.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Reconciling the desired state
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have learned that a Swarm service is a description or manifest of the desired
    state that we want an application or application service to run in. Now, let’s
    see how Docker Swarm reconciles this desired state if we do something that causes
    the actual state of the service to be different from the desired state. The easiest
    way to do this is to forcibly kill one of the tasks or containers of the service.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this with the container that has been scheduled on `node-1`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we do that and then run `docker service ps` right afterward, we will see
    the following output:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.19 – Docker Swarm reconciling the desired state after one task
    failed](img/Figure_14.19_B19199.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 14.19 – Docker Swarm reconciling the desired state after one task failed
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: We see that task 2 failed with exit code `137` and that the Swarm immediately
    reconciled the desired state by rescheduling the failed task on a node with free
    resources. In this case, the scheduler selected the same node as the failed tasks,
    but this is not always the case. So, without us intervening, the Swarm completely
    fixed the problem, and since the service is running in multiple replicas, at no
    time was the service down.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try another failure scenario. This time, we’re going to shut down an entire
    node and are going to see how the Swarm reacts. Let’s take node `ip-172-31-47-124`
    for this, as it has two tasks (tasks 1 and 4) running on it. For this, we can
    head over to the AWS console and in the EC2 dashboard, stop the instance called
    `ip-172-31-47-124`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Note that I had to go into the details of each worker instance to find out which
    one has the hostname `ip-172-31-47-124`; in my case, it was `worker2`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Back on the master node, we can now again run `docker service ps` to see what
    happened:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.20 – Swarm reschedules all tasks of a failed node](img/Figure_14.20_B19199.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: Figure 14.20 – Swarm reschedules all tasks of a failed node
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see that immediately, task 1 was rescheduled
    on node `ip-172-31-32-189`, while task 4 was rescheduled on node `ip-172-31-32-21`.
    Even this more radical failure is handled gracefully by Docker Swarm.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note though that if node `ip-172-31-47-124` ever comes back
    online in the Swarm, the tasks that had previously been running on it will not
    automatically be transferred back to it.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: But the node is now ready for a new workload.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a service or a stack
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we want to remove a particular service from the Swarm, we can use the `docker
    service rm` command. If, on the other hand, we want to remove a stack from the
    Swarm, we analogously use the `docker stack rm` command. This command removes
    all services that are part of the stack definition. In the case of the `whoami`
    service, it was created by using a stack file and hence we’re going to use the
    latter command:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This gives us this output:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The preceding command will make sure that all tasks of each service of the stack
    are terminated, and the corresponding containers are stopped by first sending
    `SIGTERM`, and then, if not successful, `SIGKILL` after 10 seconds of timeout.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the stopped containers are not removed from the
    Docker host.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is advised to purge containers from time to time on worker nodes to
    reclaim unused resources. Use `docker container purge -f` for this purpose.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Why does it make sense to leave stopped or crashed containers
    on the worker node and not automatically remove them?'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a multi-service stack
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 11*](B19199_11.xhtml#_idTextAnchor237), *Managing Containers with
    Docker Compose*, we used an application consisting of two services that were declaratively
    described in a Docker Compose file. We can use this Compose file as a template
    to create a stack file that allows us to deploy the same application into a Swarm:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called `pets-stack.yml`, and add this content to it:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We request that the web service has three replicas, and both services are attached
    to the overlay network, `pets-net`.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'We can deploy this application using the `docker stack` `deploy` command:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This results in this output:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Docker creates the `pets_pets-net` overlay network and then the two services,
    `pets_web` and `pets_db`.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then list all of the tasks in the `pets` stack:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.21 – List of all of the tasks in the pets stack](img/Figure_14.21_B19199.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: Figure 14.21 – List of all of the tasks in the pets stack
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s test the application using `curl` to retrieve an HTML page with
    a pet. And, indeed, the application works as expected, as the expected page is
    returned:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.22 – Testing the pets application using curl](img/Figure_14.22_B19199.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: Figure 14.22 – Testing the pets application using curl
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The container ID is in the output, where it says `Delivered to you by container
    d01e2f1f87df`. If you run the `curl` command multiple times, the ID should cycle
    between three different values. These are the IDs of the three containers (or
    replicas) that we have requested for the web service.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Once we’re done, we can remove the stack with `docker stack` `rm pets`.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we’re done with the swarm in AWS, we can remove it.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Removing the swarm in AWS
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To clean up the Swarm in the AWS cloud and avoid incurring unnecessary costs,
    we can use the following command:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Next, let’s summarize what we have learned in this chapter.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced Docker Swarm, which, next to Kubernetes, is the
    second most popular orchestrator for containers. We looked into the architecture
    of a Swarm, discussed all of the types of resources running in a Swarm, such as
    services, tasks, and more, and we created services in the Swarm. We learned how
    to create a Docker Swarm locally, in a special environment called PWD, as well
    as in the cloud. Lastly, we deployed an application that consists of multiple
    services related to Docker Swarm.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to introduce the routing mesh, which provides
    layer 4 routing and load balancing in a Docker Swarm. After that, we will demonstrate
    how to deploy a first application consisting of multiple services onto the Swarm.
    We will also learn how to achieve zero downtime when updating an application in
    the swarm and finally how to store configuration data in the swarm and how to
    protect sensitive data using Docker secrets. Stay tuned.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess your learning progress, please try to answer the following questions:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: What is Docker Swarm?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main components of a Docker Swarm?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you initialize a Docker Swarm?
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you add nodes to a Docker Swarm?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Docker Service in the context of Docker Swarm?
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you create and update services in Docker Swarm?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Docker Stack and how does it relate to Docker Swarm?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you deploy a Docker Stack in Docker Swarm?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the networking options in Docker Swarm?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Docker Swarm handle container scaling and fault tolerance?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are sample answers to the preceding questions:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm is a native container orchestration tool built into Docker Engine
    that allows you to create, manage, and scale a cluster of Docker nodes, orchestrating
    the deployment, scaling, and management of containers across multiple hosts.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A Docker Swarm consists of two primary components: the manager nodes, which
    are responsible for managing the cluster’s state, orchestrating tasks, and maintaining
    the desired state of services, and the worker nodes, which execute the tasks and
    run the container instances.'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can initialize a Docker Swarm by running the `docker swarm init` command
    on a Docker host, which will become the first manager node of the Swarm. The command
    will provide a token that can be used to join other nodes to the Swarm.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To add nodes to a Docker Swarm, use the `docker swarm join` command on the new
    node, along with the token and the IP address of the existing manager node.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Docker Service is a high-level abstraction that represents a containerized
    application or microservice in a Docker Swarm. It defines the desired state of
    the application, including the container image, number of replicas, network, and
    other configuration options.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can create a new service using the `docker service create` command, and
    update an existing service using the `docker service update` command, followed
    by the desired configuration options.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Docker Stack is a collection of services that is deployed together and shares
    dependencies, defined in a Docker Compose file. Docker Stacks can be deployed
    in a Docker Swarm to manage and orchestrate multi-service applications.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To deploy a Docker Stack in Docker Swarm, use the `docker stack deploy` command,
    followed by the stack name and the path to the Docker Compose file.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker Swarm supports various networking options, including the default ingress
    network for load balancing and routing, overlay networks for container-to-container
    communication across nodes, and custom networks for specific use cases.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker Swarm automatically manages container scaling by adjusting the number
    of replicas based on the desired state specified in the service definition. It
    also monitors the health of containers and replaces any failed instances to maintain
    fault tolerance.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
