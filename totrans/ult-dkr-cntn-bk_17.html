<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-374"><a id="_idTextAnchor374"/>17</h1>
<h1 id="_idParaDest-375"><a id="_idTextAnchor375"/>Deploying, Updating, and Securing an Application with Kubernetes</h1>
<p>In the previous chapter, we learned about the basics of the container orchestrator known <a id="_idIndexMarker1440"/>as Kubernetes. We got a high-level overview of the architecture of Kubernetes and learned a lot about the important objects used by Kubernetes to define and manage a containerized application.</p>
<p>In this chapter, we will learn how to deploy, update, and scale applications into a Kubernetes cluster. We will also explain how zero-downtime deployments are achieved to enable disruption-free updates and rollbacks of mission-critical applications. Finally, we will introduce Kubernetes secrets as a means to configure services and protect sensitive data.</p>
<p>This chapter covers the following topics:</p>
<ul>
<li>Deploying our first application</li>
<li>Defining liveness and readiness</li>
<li>Zero-downtime deployments</li>
<li>Kubernetes secrets</li>
</ul>
<p>After working through this chapter, you will be able to do the following:</p>
<ul>
<li>Deploy a multi-service application into a Kubernetes cluster</li>
<li>Define a liveness and readiness probe for your Kubernetes application service</li>
<li>Update an application service running in Kubernetes without causing downtime</li>
<li>Define secrets in a Kubernetes cluster</li>
<li>Configure an application service to use Kubernetes secrets</li>
</ul>
<h1 id="_idParaDest-376"><a id="_idTextAnchor376"/>Technical requirements</h1>
<p>In this chapter, we’re going to use Docker Desktop on our local computer. Please refer to <a href="B19199_02.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Setting Up a Working Environment</em>, for more information on how to install and use Docker Desktop.</p>
<p>The code for this chapter can be found here: <code>main/sample-solutions/ch17</code>.</p>
<p>Please make sure you have cloned this book’s GitHub repository, as described in <a href="B19199_02.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>.</p>
<p>In your Terminal, navigate to the <code>~/The-Ultimate-Docker-Container-Book</code> folder and create a subfolder called <code>ch17</code> and navigate to it:</p>
<pre class="source-code">
$ mkdir ch17 &amp; cd ch17</pre> <h1 id="_idParaDest-377"><a id="_idTextAnchor377"/>Deploying our first application</h1>
<p>We will take our pets <a id="_idIndexMarker1441"/>application, which we first introduced in <a href="B19199_11.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Managing Containers with </em><em class="italic">Docker Compose</em>, and deploy it into a Kubernetes cluster. Our cluster will be Docker Desktop, which, as you know, is offering us a single node Kubernetes cluster. However, from the perspective of deployment, it doesn’t matter how big the cluster is and whether the cluster is located in the cloud, in your company’s data center, or on your workstation.</p>
<h2 id="_idParaDest-378"><a id="_idTextAnchor378"/>Deploying the web component</h2>
<p>Just as a <a id="_idIndexMarker1442"/>reminder, our application<a id="_idIndexMarker1443"/> consists of two application services: the Node-based web component and the backing PostgreSQL database. In the previous chapter, we learned that we need to define a Kubernetes Deployment object for each application service we want to deploy. We’ll do this for the web component first. As always in this book, we will choose the declarative way of defining our objects:</p>
<ol>
<li>We will use our local Kubernetes single-node cluster provided by Docker Desktop. Make sure Kubernetes is turned on for your Docker Desktop installation:</li>
</ol>
<div><div><img alt="Figure 17.1 – Kubernetes on Docker Desktop" height="600" src="img/Figure_17.01_B19199.jpg" width="862"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.1 – Kubernetes on Docker Desktop</p>
<ol>
<li value="2">To your <a id="_idIndexMarker1444"/>code <a id="_idIndexMarker1445"/>subfolder (<code>ch17</code>), add a file called <code>web-deployment.yaml</code> with the following content:</li>
</ol>
<div><div><img alt="Figure 17.2 – Kubernetes deployment definition for the web component" height="915" src="img/Figure_17.02_B19199.jpg" width="785"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.2 – Kubernetes deployment definition for the web component</p>
<p>The preceding<a id="_idIndexMarker1446"/> deployment<a id="_idIndexMarker1447"/> definition can be found in the <code>web-deployment.yaml</code> file in the <code>sample-solutions/ch17</code> subfolder. It contains the instructions necessary to deploy the <code>web</code> component. The lines of code are as follows:</p>
<ul>
<li>Line 7: We define the name for our <code>Deployment</code> object as <code>web</code>.</li>
<li>Line 9: We declare that we want to have one instance of the <code>web</code> component running.</li>
<li>Lines 11 to 13: Through <code>Selector</code>, we define which pods will be part of our deployment, namely those that have the <code>app</code> and <code>service</code> labels with values of <code>pets</code> and <code>web</code>, respectively.</li>
<li>Line 14: In the template for the pods starting at line 11, we define that each pod will have the <code>app</code> and <code>service</code> labels applied to them.</li>
<li>Lines 20 onward: We define the single container that will be running in the pod. The image for the container is our well-known <code>fundamentalsofdocker/ch11-web:2.0</code> image and the name of the container will be <code>web</code>.</li>
<li>Lines 23 and 24: It is worth noting that we declare that the container exposes port <code>3000</code> to incoming traffic.</li>
</ul>
<ol>
<li value="3">Please<a id="_idIndexMarker1448"/> make<a id="_idIndexMarker1449"/> sure that you have set the context of <code>kubectl</code> to Docker Desktop. See <a href="B19199_02.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Setting Up a Working Environment</em>, for details on how to do that. Use the following command:<pre class="source-code">
$ kubectl config use-context docker-desktop</pre></li> </ol>
<p>You will receive the following output:</p>
<pre class="source-code">
Switched to context "docker-desktop".</pre> <ol>
<li value="4">We can deploy this <code>Deployment</code> object using the following command:<pre class="source-code">
$ kubectl create -f web-deployment.yaml</pre></li> </ol>
<p>The preceding command outputs the following message:</p>
<pre class="source-code">
deployment.apps/web created</pre> <ol>
<li value="5">We can double-check that the deployment has been created again using our Kubernetes CLI:<pre class="source-code">
$ kubectl get all</pre></li> </ol>
<p>We should see the following output:</p>
<div><div><img alt="Figure 17.3 – Listing all the resources running in Kind" height="280" src="img/Figure_17.03_B19199.jpg" width="845"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.3 – Listing all the resources running in Kind</p>
<p>In the <a id="_idIndexMarker1450"/>preceding <a id="_idIndexMarker1451"/>output, we can see that Kubernetes created three objects – the deployment, a pertaining <code>ReplicaSet</code>, and a single pod (remember that we specified that we want one replica only). The current state corresponds to the desired state for all three objects, so we are fine so far.</p>
<ol>
<li value="6">Now, the web service needs to be exposed to the public. For this, we need to define a Kubernetes <code>Service</code> object of the <code>NodePort</code> type. Create a new file called <code>web-service.yaml</code> and add the following code to it:</li>
</ol>
<div><div><img alt="Figure 17.4 – Definition of the Service object for our web component" height="808" src="img/Figure_17.04_B19199.jpg" width="535"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.4 – Definition of the Service object for our web component</p>
<p>Once <a id="_idIndexMarker1452"/>again, the <a id="_idIndexMarker1453"/>same file can be found in the <code>web-service.yaml</code> file in the <code>sample-solutions/ch17</code> subfolder.</p>
<p>The preceding lines of code are as follows:</p>
<ul>
<li>Line 7: We set the name of this <code>Service</code> object to <code>web</code>.</li>
<li>Line 9: We define the type of <code>Service</code> object we’re using. Since the <code>web</code> component has to be accessible from outside of the cluster, this cannot be a <code>Service</code> object of the <code>ClusterIP</code> type and must be of the <code>NodePort</code> or <code>LoadBalancer</code> type. We discussed the various types of Kubernetes services in the previous chapter, so will not go into further detail about this. In our example, we’re using a <code>NodePort</code> type of service.</li>
<li>Lines 10 to 13: We specify that we want to expose port <code>3000</code> for access through the TCP protocol. Kubernetes will map container port <code>3000</code> automatically to a free host port in the range of 30,000 to 32,768. Which port Kubernetes effectively chooses can be determined using the <code>kubectl get service</code> or <code>kubectl describe</code> command for the service after it has been created.</li>
<li>Lines 14 to 16: We define the filter criteria for the pods that this service will be a stable endpoint for. In this case, it is all the pods that have the <code>app</code> and <code>service</code><a id="_idIndexMarker1454"/> labels <a id="_idIndexMarker1455"/>with the <code>pets</code> and <code>web</code> values, respectively.</li>
</ul>
<ol>
<li value="7">Now that we have this specification for a <code>Service</code> object, we can create it using <code>kubectl</code>:<pre class="source-code">
$ kubectl apply -f web-service.yaml</pre></li> <li>We can list all the services to see the result of the preceding command:<pre class="source-code">
$ kubectl get services</pre></li> </ol>
<p>The preceding command produces the following output:</p>
<div><div><img alt="Figure 17.5 – The Service object that was created for the web component" height="109" src="img/Figure_17.05_B19199.jpg" width="863"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.5 – The Service object that was created for the web component</p>
<p>In the preceding output, we can see that a service called <code>web</code> has been created. A unique <code>ClusterIP</code> value of <code>10.96.195.255</code> has been assigned to this service, and container port <code>3000</code> has been published on port <code>30319</code> on all cluster nodes.</p>
<ol>
<li value="9">If we want to test this deployment, we can use <code>curl</code>:<pre class="source-code">
$ curl localhost:30319/</pre></li> </ol>
<p>This will result in the following output:</p>
<pre class="source-code">
Pets Demo Application</pre> <p>As we can see, the<a id="_idIndexMarker1456"/> response <a id="_idIndexMarker1457"/>is <code>Pets Demo Application</code>, which is what we expected. The web service is up and running in the Kubernetes cluster. Next, we want to deploy the database.</p>
<h2 id="_idParaDest-379"><a id="_idTextAnchor379"/>Deploying the database</h2>
<p>A database is a<a id="_idIndexMarker1458"/> stateful component <a id="_idIndexMarker1459"/>and has to be treated differently from stateless components, such as our web component. We discussed the difference between stateful and stateless components in a distributed application architecture in detail in <a href="B19199_09.xhtml#_idTextAnchor194"><em class="italic">Chapter 9</em></a>,<em class="italic"> </em><em class="italic">Learning about </em><em class="italic">Distributed Application Architecture</em>, and <a href="B19199_03.xhtml#_idTextAnchor057"><em class="italic">Chapter 3</em></a>, <em class="italic">Introducing </em><em class="italic">Container Orchestration</em>.</p>
<p>Kubernetes has defined a special type of <code>ReplicaSet</code> object for stateful components. This object is called <code>StatefulSet</code>. Let’s use this kind of object to deploy our database.</p>
<ol>
<li>Create a new file called <code>db-stateful-set.yaml</code> and add the following content to it:</li>
</ol>
<div><div><img alt="Figure 17.6 – A StatefulSet object for the DB component" height="1044" src="img/Figure_17.06_B19199.jpg" width="685"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.6 – A StatefulSet object for the DB component</p>
<p>The definition<a id="_idIndexMarker1460"/> can<a id="_idIndexMarker1461"/> also be found in the <code>sample-solutions/ch17</code> subfolder.</p>
<p>OK; this looks a bit scary, but it isn’t. It is a bit longer than the definition of the deployment for the web component since we also need to define a volume where the PostgreSQL database can store the data. The volume claim definition is on lines 25 to 33.</p>
<p>We want to create a volume called <code>pets-data</code> that has a maximum size equal to 100 MB. On lines 22 to 24, we use this volume and mount it into the container at <code>/var/lib/postgresql/data</code>, where PostgreSQL expects it. On line 21, we also declare that PostgreSQL is listening at port <code>5432</code>.</p>
<ol>
<li value="2">As always, we use <code>kubectl</code> to deploy our <code>StatefulSet</code>:<pre class="source-code">
$ kubectl apply -f db-stateful-set.yaml</pre></li> <li>Now, if we list all the resources in the cluster, we will be able to see the additional objects that were created:</li>
</ol>
<div><div><img alt="Figure 17.7 – The StatefulSet and its pod" height="381" src="img/Figure_17.07_B19199.jpg" width="941"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.7 – The StatefulSet and its pod</p>
<p>Here, we <a id="_idIndexMarker1462"/>can <a id="_idIndexMarker1463"/>see that <code>StatefulSet</code> and a pod have been created. For both, the current state corresponds to the desired state and thus the system is healthy, but that doesn’t mean that the <code>web</code> component can access the database at this time. Service discovery won’t work. Remember that the <code>web</code> component wants to access the <code>db</code> service under the name <code>db</code>. We hardcoded the <code>db</code> hostname in the <code>server.js</code> file.</p>
<ol>
<li value="4">To make service discovery work inside the cluster, we have to define a Kubernetes <code>Service</code> object for the database component too. Since the database should only ever be accessible from within the cluster, the type of <code>Service</code> object we need is <code>ClusterIP</code>.</li>
</ol>
<p>Create a new file called <code>db-service.yaml</code> and add the following specification to it. It can be found in the <code>sample-solutions/ch17</code> subfolder:</p>
<div><div><img alt="Figure 17.8 – Definition of the Kubernetes Service object for the database" height="601" src="img/Figure_17.08_B19199.jpg" width="437"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.8 – Definition of the Kubernetes Service object for the database</p>
<p>The <a id="_idIndexMarker1464"/>database <a id="_idIndexMarker1465"/>component will be represented by this <code>Service</code> object. It can be reached by the name <code>db</code>, which is the name of the service, as defined on line 4. The database component does not have to be publicly accessible, so we decided to use a <code>Service</code> object of the <code>ClusterIP</code> type. The selector on lines 10 to 12 defines that this service represents a stable endpoint for all the pods that have the necessary labels defined – that is, <code>app: pets</code> and <code>service: db</code>.</p>
<ol>
<li value="5">Let’s deploy this service with the following command:<pre class="source-code">
$ kubectl apply -f db-service.yaml</pre></li> <li>Now, we should be ready to test the application. We can use the browser this time to enjoy<a id="_idIndexMarker1466"/> the beautiful animal <a id="_idIndexMarker1467"/>images from the Maasai Mara national park in Kenya:</li>
</ol>
<div><div><img alt="Figure 17.9 – Testing the pets application running in Kubernetes" height="671" src="img/Figure_17.09_B19199.jpg" width="1065"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.9 – Testing the pets application running in Kubernetes</p>
<p>In this case, port number <code>30317</code> is the number that Kubernetes automatically selected for my <code>web</code> <code>Service</code> object. Replace this number with the port that Kubernetes assigned to your service. You can get the number by using the <code>kubectl get </code><code>services</code> command.</p>
<p>With that, we have successfully deployed the pets application to a single-node Kubernetes cluster provided by Docker Desktop. We had to define four artifacts to do so, which are as follows:</p>
<ul>
<li><code>Deployment</code> and <code>Service</code> objects for the <code>web</code> component</li>
<li><code>StatefulSet</code> and <code>Service</code> objects for the <code>database</code> component</li>
</ul>
<p>To remove the application from the cluster, we can use the following small script:</p>
<pre class="source-code">
kubectl delete svc/webkubectl delete deploy/web
kubectl delete svc/db
kubectl delete statefulset/db
kubectl delete pvc/pets-data-db-0</pre>
<p>Please note the last line in this script. We are deleting the persistent volume claim that Kubernetes automatically created as part of the <code>db</code> deployment. When we delete the <code>db</code> deployment, this claim is not automatically deleted! Persistent volume claims are a bit similar (but not<a id="_idIndexMarker1468"/> the same, mind you) as<a id="_idIndexMarker1469"/> Docker volumes.</p>
<p>Use the <code>kubectl get pvc</code> command to get a list of all claims on your machine.</p>
<p>Next, we will optimize the deployment.</p>
<h3>Streamlining the deployment</h3>
<p>So far, we have <a id="_idIndexMarker1470"/>created four artifacts that needed to be deployed to the cluster. This is only a very simple application, consisting of two components. Imagine having a much more complex application. It would quickly become a maintenance nightmare. Luckily, we have several options as to how we can simplify the deployment. The method that we are going to discuss here is the possibility of defining all the components that make up an application in Kubernetes in a single file.</p>
<p>Other solutions that lie outside the scope of this book include using a package manager, such as Helm (<a href="https://helm.sh/">https://helm.sh/</a>), or Kustomize (<a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/</a>), the native Kubernetes solution.</p>
<p>If we have an application consisting of many Kubernetes objects, such as <code>Deployment</code> and <code>Service</code> objects, then we can keep them all in a single file and separate the individual object definitions by three dashes. For example, if we wanted to have the <code>Deployment</code> and <code>Service</code> definitions for the <code>web</code> component in a single file, this would look as <a id="_idIndexMarker1471"/>follows:</p>
<div><div><img alt="Figure 17.10 – Deployment and Service for web in a single file" height="1049" src="img/Figure_17.10_B19199.jpg" width="580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.10 – Deployment and Service for web in a single file</p>
<p>You can find this file in <code>sample-solutions/ch17/install-web.yaml</code>.</p>
<p>Next, we collected all four object definitions for the pets application in the <code>sample-solutions/ch17/install-pets.yaml</code> file, and we can deploy the application in one go:</p>
<pre class="source-code">
$ kubectl apply -f install-pets.yaml</pre> <p>This will give us this output:</p>
<pre class="source-code">
deployment "web" createdservice "web" created
deployment "db" created
service "db" created</pre>
<p>Similarly, we created a script called <code>sample-solutions/ch17/remove-pets.sh</code> to remove all the artifacts of the pets application from the Kubernetes cluster. Note that the file was made executable with <code>chmod +x ./remove-pets.sh</code> first. Now, we can use the following command:</p>
<pre class="source-code">
$ ./remove-pets.sh</pre> <p>This will result <a id="_idIndexMarker1472"/>in an output like is:</p>
<pre class="source-code">
deployment.apps "web" deletedservice "web" deleted
statefulset.apps "db" deleted
service "db" deleted
persistentvolumeclaim "pets-data-db-0" deleted</pre>
<p>Alternatively, you can use the following command:</p>
<pre class="source-code">
$ kubectl delete -f install-pets.yaml</pre> <p>This will delete all the resources except the persistent volume claim, which you still need to delete by hand:</p>
<pre class="source-code">
$ kubectl delete pvc/pets-data-db-0</pre> <p>With this, we have taken the pets application we introduced in <a href="B19199_11.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a><em class="italic">, </em><em class="italic">Managing Containers with </em><em class="italic">Docker Compose</em>, and defined all the Kubernetes objects that are necessary to deploy this application into a Kubernetes cluster. In each step, we made sure that we got the expected result, and<a id="_idIndexMarker1473"/> once all the artifacts existed in the cluster, we showed the running application.</p>
<h1 id="_idParaDest-380"><a id="_idTextAnchor380"/>Defining liveness and readiness</h1>
<p>Container<a id="_idIndexMarker1474"/> orchestration systems such as Kubernetes and Docker Swarm make it significantly easier to deploy, run, and update highly distributed, mission-critical applications. The orchestration engine automates many cumbersome tasks, such as scaling up or down, asserting that the desired state is maintained at all times, and more.</p>
<p>However, the orchestration engine cannot just do everything automatically. Sometimes, we developers need to support the engine with some information that only we can know about. So, what do I mean by that?</p>
<p>Let’s look at a single application service. Let’s assume it is a microservice and let’s call it service A. If we run service A containerized on a Kubernetes cluster, then Kubernetes can make sure that we have the five instances that we require in the service definition running at all times. If one instance crashes, Kubernetes can quickly launch a new instance and thus maintain the desired state. But what if an instance of the service does not crash, but is unhealthy or just not ready yet to serve requests? Kubernetes should know about both situations. But it can’t, since good or bad health from an application service perspective is outside of the knowledge of the orchestration engine. Only we application developers can know when our service is healthy and when it is not.</p>
<p>The application service could, for example, be running, but its internal state could have been corrupted due to some bug, it could be in an endless loop, or it could be in a deadlock situation.</p>
<p>Similarly, only we application developers know whether our service is ready to work, or whether it is still initializing. Although it is highly recommended to keep the initialization phase of a microservice as short as possible, it often cannot be avoided if a significant time span is needed by a particular service so that it’s ready to operate. Being in this state of initialization is not the same thing as being unhealthy, though. The initialization phase is an expected part of the life cycle of a microservice or any other application service.</p>
<p>Thus, Kubernetes should not try to kill our microservice if it is in the initialization phase. If our microservice is unhealthy, though, Kubernetes should kill it as quickly as possible and replace it with a fresh instance.</p>
<p>Kubernetes has the concept of probes to provide the seam between the orchestration engine and the application developer. Kubernetes uses these probes to find out more about the inner state of the application service at hand. Probes are executed locally, inside each container. There<a id="_idIndexMarker1475"/> is a probe for the health – also called liveness – of the service, a startup probe, and a probe for the readiness of the service. Let’s look at them in turn.</p>
<h2 id="_idParaDest-381"><a id="_idTextAnchor381"/>Kubernetes liveness probes</h2>
<p>Kubernetes uses the<a id="_idIndexMarker1476"/> liveness probe to decide when a container needs to be killed and when another instance should be launched instead. Since Kubernetes operates at a pod level, the respective pod is killed if at least one of its containers reports as being unhealthy.</p>
<p>Alternatively, we can say it the other way around: only if all the containers of a pod report to be healthy is the pod considered to be healthy.</p>
<p>We can define the liveness probe in the specification for a pod as follows:</p>
<pre class="source-code">
apiVersion: v1kind: Pod
metadata:
…
spec:
  containers:
  - name: liveness-demo
    image: postgres:12.10
…
    <strong class="bold">livenessProbe:</strong>
<strong class="bold">      exec:</strong>
<strong class="bold">        command: nc localhost 5432 || exit –1</strong>
<strong class="bold">      initialDelaySeconds: 10</strong>
<strong class="bold">      periodSeconds: 5</strong></pre>
<p>The relevant part is in the <code>livenessProbe</code> section. First, we define a command that Kubernetes will execute as a probe inside the container. In our case, we have a <code>PostreSQL</code> container and use the <code>netcat</code> Linux tool to probe port <code>5432</code> over TCP. The <code>nc localhost 5432</code> command is successful once Postgres listens to it.</p>
<p>The other two settings, <code>initialDelaySeconds</code> and <code>periodSeconds</code>, define how long Kubernetes should wait after starting the container until it first executes the probe and how frequently the probe should be executed thereafter. In our case, Kubernetes waits for 10 seconds before executing the first probe and then executes a probe every 5 seconds.</p>
<p>It is also possible to probe an HTTP endpoint instead of using a command. Let’s assume we’re running a microservice from an image, <code>acme.com/my-api:1.0</code>, with an API that has an endpoint <a id="_idIndexMarker1477"/>called <code>/api/health</code> that returns status <code>200 (OK)</code> if the microservice is healthy, and <code>50x (Error)</code> if it is unhealthy. Here, we can define the liveness probe as follows:</p>
<pre class="source-code">
apiVersion: v1kind: Pod
metadata:
…
spec:
  containers:
  - name: liveness
    image: acme.com/my-api:1.0
…
    <strong class="bold">livenessProbe:</strong>
<strong class="bold">      httpGet:</strong>
<strong class="bold">        path: /api/health</strong>
<strong class="bold">        port: 3000</strong>
<strong class="bold">      initialDelaySeconds: 5</strong>
<strong class="bold">      periodSeconds: 3</strong></pre>
<p>In the preceding snippet, I defined the liveness probe so that it uses the HTTP protocol and executed a <code>GET</code> request to the <code>/api/health</code> endpoint on port <code>5000</code> of <code>localhost</code>. Remember, the probe is executed inside the container, which means I can use localhost.</p>
<p>We can also directly use the TCP protocol to probe a port on the container. But wait a second – didn’t we just do that in our first example, where we used the generic liveness probe based on an arbitrary command? Yes, you’re right, we did, but we had to rely on the presence of the <code>netcat</code> tool in the container to do so. We cannot assume that this tool is always there. Thus, it is favorable to rely on Kubernetes to do the TCP-based probing for us out of the box. The<a id="_idIndexMarker1478"/> modified pod spec looks like this:</p>
<pre class="source-code">
apiVersion: v1kind: Pod
metadata:
…
spec:
  containers:
  - name: liveness-demo
    image: postgres:12.10
…
    <strong class="bold">livenessProbe:</strong>
<strong class="bold">      tcpSocket:</strong>
<strong class="bold">        port: 5432</strong>
<strong class="bold">      initialDelaySeconds: 10</strong>
<strong class="bold">      periodSeconds: 5</strong></pre>
<p>This looks very similar. The only change is that the type of probe has been changed from <code>exec</code> to <code>tcpSocket</code> and that, instead of providing a command, we provide the port to probe.</p>
<p>Note that we could also use <code>failureThreshold</code> here with Kubernetes’ <code>livenessProbe</code>. The <code>livenessProbe</code> failure threshold in Kubernetes is the minimum number of consecutive failures that must occur before the container is restarted. The default value is <code>3</code>. The minimum value is <code>1</code>. If the handler returns a failure code, <code>kubelet</code> kills the container and restarts it. Any code greater than or equal to <code>200</code> and less than <code>400</code> indicates success. Any other code indicates failure.</p>
<p>Let’s try this out:</p>
<ol>
<li>Copy the <code>probes</code> subfolder from the <code>sample-solutions/ch17</code> folder to your <code>ch17</code> folder.</li>
<li>Build the Docker image with the following command:<pre class="source-code">
$ docker image build -t demo/probes-demo:2.0 probes</pre></li> <li>Use <code>kubectl</code> to deploy the sample pod that’s defined in <code>probes-demo.yaml</code>:<pre class="source-code">
$ kubectl apply -f probes/probes-demo.yaml</pre></li> <li>Describe the pod and <a id="_idIndexMarker1479"/>specifically analyze the log part of the output:<pre class="source-code">
$ kubectl describe pods/probes-demo</pre></li> </ol>
<p>During the first half minute or so, you should get the following output:</p>
<div><div><img alt="Figure 17.11 – Log output of the healthy pod" height="159" src="img/Figure_17.11_B19199.jpg" width="1239"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.11 – Log output of the healthy pod</p>
<ol>
<li value="5">Wait at least 30 seconds and then describe the pod again. This time, you should see the following output:</li>
</ol>
<div><div><img alt="Figure 17.12 – Log output of the pod after it has changed its state to Unhealthy" height="202" src="img/Figure_17.12_B19199.jpg" width="1423"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.12 – Log output of the pod after it has changed its state to Unhealthy</p>
<p>The marked lines indicate the failure of the probe and the fact that the pod is going to be restarted.</p>
<ol>
<li value="6">If you get the list of pods, you will see that the pod has been restarted several times:<pre class="source-code">
$ kubectl get pods</pre></li> </ol>
<p>This results in this output:</p>
<pre class="source-code">
NAME                   READY  STATUS     RESTARTS       AGEprobes-demo  1/1        Running  5 (49s ago)   7m22s</pre>
<ol>
<li value="7">When you’re <a id="_idIndexMarker1480"/>done with the sample, delete the pod with the following command:<pre class="source-code">
$ kubectl delete pods/probes-demo</pre></li> </ol>
<p>Next, we will have a look at the Kubernetes readiness probe.</p>
<h2 id="_idParaDest-382"><a id="_idTextAnchor382"/>Kubernetes readiness probes</h2>
<p>Kubernetes uses <a id="_idIndexMarker1481"/>a readiness probe to decide when a service instance – that is, a container – is ready to accept traffic. Now, we all know that Kubernetes deploys and runs pods and not containers, so it only makes sense to talk about the readiness of a pod. Only if all containers in a pod report as ready is the pod considered to be ready itself. If a pod reports as not ready, then Kubernetes removes it from the service load balancers.</p>
<p>Readiness probes are defined the same way as liveness probes: just switch the <code>livenessProbe</code> key in the pod spec to <code>readinessProbe</code>. Here is an example using our prior pod spec:</p>
<pre class="source-code">
…spec:
  containers:
  - name: liveness-demo
    image: postgres:12.10
…
    livenessProbe:
       tcpSocket:
         port: 5432
      failureThreshold: 2
      periodSeconds: 5
    readinessProbe:
      tcpSocket:
        port: 5432
      initialDelaySeconds: 10
      periodSeconds: 5</pre>
<p>Note that, in this example, we don’t need an initial delay for the liveness probe anymore since we now have a readiness probe. Thus, I have replaced the initial delay entry for the liveness probe<a id="_idIndexMarker1482"/> with an entry called <code>failureThreshold</code>, which indicates how many times Kubernetes should repeat probing in case of a failure until it assumes that the container is unhealthy.</p>
<h2 id="_idParaDest-383"><a id="_idTextAnchor383"/>Kubernetes startup probes</h2>
<p>It is often helpful for<a id="_idIndexMarker1483"/> Kubernetes to know when a service instance has started. If we define a startup probe for a container, then Kubernetes does not execute the liveness or readiness probes, so long as the container’s startup probe does not succeed. Once again, Kubernetes looks at pods and starts executing liveness and readiness probes on its containers if the startup probes of all the pod’s containers succeed.</p>
<p>When would we use a startup probe, given the fact that we already have the liveness and readiness probes? There might be situations where we have to account for exceptionally long startup and initialization times, such as when containerizing a legacy application. We could technically configure the readiness or liveness probes to account for this fact, but that would defeat the purpose of these probes. The latter probes are meant to provide quick feedback to Kubernetes on the health and availability of the container. If we configure for long initial delays or periods, then this would counter the desired outcome.</p>
<p>Unsurprisingly, the startup probe is defined the same way as the readiness and liveness probes. Here is an example:</p>
<pre class="source-code">
spec:  containers:
...
    startupProbe:
      tcpSocket:
        port: 3000
      failureThreshold: 30
      periodSeconds: 5
...</pre>
<p>Make sure that you define the <code>failureThreshold * periodSeconds</code> product so that it’s big enough <a id="_idIndexMarker1484"/>to account for the worst startup time.</p>
<p>In our example, the max startup time should not exceed 150 seconds.</p>
<h1 id="_idParaDest-384"><a id="_idTextAnchor384"/>Zero-downtime deployments</h1>
<p>In a mission-critical <a id="_idIndexMarker1485"/>environment, the application must be always up and running. These days, we cannot afford downtime anymore. Kubernetes gives us various means of achieving this. Performing an update on an application in the cluster that causes no downtime is called a <strong class="bold">zero-downtime deployment</strong>. In this section, we will present two ways of achieving this. These are as follows:</p>
<ul>
<li>Rolling updates</li>
<li>Blue-green deployments</li>
</ul>
<p>Let’s start by discussing rolling updates.</p>
<h2 id="_idParaDest-385"><a id="_idTextAnchor385"/>Rolling updates</h2>
<p>In the previous<a id="_idIndexMarker1486"/> chapter, we learned that the Kubernetes <code>Deployment</code> object distinguishes itself from the <code>ReplicaSet</code> object in that it adds rolling updates and rollbacks on top of the latter’s functionality. Let’s use our web component to demonstrate this. We will have to modify the manifest or description of the deployment for the web component.</p>
<p>We will use the same deployment definition as in the previous section, with one important difference – we will have <code>web</code> component running. The following definition can also be found in the <code>sample-solutions/ch17/web-deployment-rolling-v1.yaml</code> file:</p>
<div><div><img alt="Figure 17.13 – Deployment for the web component with five replicas" height="1015" src="img/Figure_17.13_B19199.jpg" width="864"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.13 – Deployment for the web component with five replicas</p>
<p>Now, we can create this deployment as usual and also, at the same time, the service that makes our component accessible:</p>
<pre class="source-code">
$ kubectl apply -f web-deployment-rolling-v1.yaml$ kubectl apply -f web-service.yaml</pre>
<p>Once we have deployed the pods and the service, we can test our web component. First, we can get the assigned node port with this command:</p>
<pre class="source-code">
$ PORT=$(kubectl get svc/web -o jsonpath='{.spec.ports[0].nodePort}')</pre> <p>Next, we can use the <code>$PORT</code> environment variable in our <code>curl</code> statement:</p>
<pre class="source-code">
$ curl localhost:${PORT}/</pre> <p>This provides the expected output:</p>
<pre class="source-code">
Pets Demo Application</pre> <p>As we can see, the<a id="_idIndexMarker1487"/> application is up and running and returns the expected message, <code>Pets </code><code>Demo Application</code>.</p>
<p>Our developers have created a new version, 2.1, of the web component. The code of the new version of the web component can be found in the <code>sample-solutions/ch17/web</code> folder, and the only change is located on line 12 of the <code>server.js</code> file:</p>
<div><div><img alt="Figure 17.14 – Code change for version 2.0 of the web component" height="86" src="img/Figure_17.14_B19199.jpg" width="647"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.14 – Code change for version 2.0 of the web component</p>
<p>We can now build the new image as follows (replace <code>demo</code> with your GitHub username):</p>
<pre class="source-code">
$ docker image build -t demo/ch17-web:2.1 web</pre> <p>Subsequently, we can push the image to Docker Hub, as follows (replace <code>demo</code> with your GitHub username):</p>
<pre class="source-code">
$ docker image push demo/ch17-web:2.1</pre> <p>Now, we want to update the image that’s used by our pods that are part of the <code>web</code> <code>Deployment</code> object. We can do this by using the <code>set image</code> command of <code>kubectl</code>:</p>
<pre class="source-code">
$ kubectl set image deployment/web \    web=demo/ch17-web:2.1</pre>
<p>If we test the application again, we’ll get a confirmation that the update has indeed happened:</p>
<pre class="source-code">
$ curl localhost:${PORT}/</pre> <p>The output indicates that we now have version 2 installed:</p>
<pre class="source-code">
Pets Demo Application v2</pre> <p>Now, how do we know that there hasn’t been any downtime during this update? Did the update happen in a rolling fashion? What does rolling update mean at all? Let’s investigate. First, we can get a confirmation from Kubernetes that the deployment has indeed happened and<a id="_idIndexMarker1488"/> was successful by using the <code>rollout </code><code>status</code> command:</p>
<pre class="source-code">
$ kubectl rollout status deploy/web</pre> <p>The command will respond as follows:</p>
<pre class="source-code">
deployment "web" successfully rolled out</pre> <p>If we describe the <code>web</code> deployment object with <code>kubectl describe deploy/web</code>, we will get the following list of events at the end of the output:</p>
<div><div><img alt="Figure 17.15 – List of events found in the output of the deployment description of the web component" height="199" src="img/Figure_17.15_B19199.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.15 – List of events found in the output of the deployment description of the web component</p>
<p>The first event tells us that, when we created the deployment, a <code>ReplicaSet</code> object called <code>web-769b88f67</code> with five replicas was created. Then, we executed the <code>update</code> command. The second event in the list tells us that this meant creating a new <code>ReplicaSet</code> object called <code>web-55cdf67cd</code> with, initially, one replica only. Thus, at that particular moment, six pods existed on the system: the five initial pods and one pod with the new version. But, since the desired state of the <code>Deployment</code> object states that we want five replicas only, Kubernetes now scales down the old <code>ReplicaSet</code> object to four instances, which we can see in the third event.</p>
<p>Then, again, the new <code>ReplicaSet</code> object was scaled up to two instances, and, subsequently, the old <code>ReplicaSet</code> object was scaled down to three instances, and so on, until we had five new instances and all the old instances were decommissioned. Although we cannot see any precise time (other than 3 minutes) when that happened, the order of the events tells us that the whole update happened in a rolling fashion.</p>
<p>During a short period, some of the calls to the web service would have had an answer from the old version of the component, and some calls would have received an answer from the new version<a id="_idIndexMarker1489"/> of the component, but at no time would the service have been down.</p>
<p>We can also list the <code>ReplicaSet</code> objects in the cluster and get confirmation of what I said in the preceding section:</p>
<div><div><img alt="Figure 17.16 – Listing all the ReplicaSet objects in the cluster" height="104" src="img/Figure_17.16_B19199.jpg" width="558"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.16 – Listing all the ReplicaSet objects in the cluster</p>
<p>Here, we can see that the new <code>ReplicaSet</code> object has five instances running and that the old one has been scaled down to zero instances. The reason that the old <code>ReplicaSet</code> object is still lingering is that Kubernetes provides us with the possibility of rolling back the update and, in that case, will reuse that <code>ReplicaSet</code>.</p>
<p>To roll back the update of the image in case some undetected bug sneaked into the new code, we can use the <code>rollout </code><code>undo</code> command:</p>
<pre class="source-code">
$ kubectl rollout undo deploy/web</pre> <p>This outputs the following:</p>
<pre class="source-code">
deployment.apps/web rolled back</pre> <p>We can test whether the rollback was successful like so:</p>
<pre class="source-code">
$ curl localhost:${PORT}/</pre> <p>As we can see, the output shows us that this is the case:</p>
<pre class="source-code">
Pets Demo Application</pre> <p>If we list the <code>ReplicaSet</code> objects, we will see the following output:</p>
<div><div><img alt="Figure 17.17 – Listing the ReplicaSet objects after rolling back" height="104" src="img/Figure_17.17_B19199.jpg" width="558"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.17 – Listing the ReplicaSet objects after rolling back</p>
<p>This confirms that the old <code>ReplicaSet</code> (<code>web-9d66cd994</code>) object has been reused and that the new one has been scaled down to zero instances.</p>
<p>Before continuing, please delete the deployment and the service:</p>
<pre class="source-code">
$ kubectl delete deploy/web$ kubectl delete service/web</pre>
<p>Sometimes, though, we cannot, or do not want to, tolerate the mixed state of an old version coexisting<a id="_idIndexMarker1490"/> with the new version. We want an all-or-nothing strategy. This is where blue-green deployments come into play, which we will discuss next.</p>
<h2 id="_idParaDest-386"><a id="_idTextAnchor386"/>Blue-green deployment</h2>
<p>If we want to do a<a id="_idIndexMarker1491"/> blue-green style <a id="_idIndexMarker1492"/>deployment for our <code>web</code> component of the pets application, then we can do so by using labels creatively. First, let’s remind ourselves how blue-green deployments work. Here is a rough step-by-step guide:</p>
<ol>
<li>Deploy the first version of the <code>web</code> component as <code>blue</code>. We will label the pods with a label of <code>color: blue</code> to do so.</li>
<li>Deploy the Kubernetes service for these pods with the <code>color: blue</code> label in the <code>selector</code> section.</li>
<li>Now, we can deploy version 2 of the web component, but, this time, the pods have a label of <code>color: green</code>.</li>
<li>We can test the green version of the service to check that it works as expected.</li>
<li>Now, we can flip traffic from <code>blue</code> to <code>green</code> by updating the Kubernetes service for the web component. We will modify the selector so that it uses the <code>color: </code><code>green</code> label.</li>
</ol>
<p>Let’s<a id="_idIndexMarker1493"/> define a <code>Deployment</code> object <a id="_idIndexMarker1494"/>for version 1, <code>blue</code>:</p>
<div><div><img alt="Figure 17.18 – Specification of the blue deployment for the web component" height="1024" src="img/Figure_17.18_B19199.jpg" width="801"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.18 – Specification of the blue deployment for the web component</p>
<p>The preceding definition can be found in the <code>sample-solutions/ch17/web-deployment-blue.yaml</code> file.</p>
<p>Please take note of line 8, where we define the name of the deployment as <code>web-blue</code> to distinguish it from the upcoming deployment, <code>web-green</code>. Also, note that we have added the <code>color: blue</code> label on lines 7, 15, and 21. Everything else remains the same as before.</p>
<p>Now, we can define the <code>Service</code> object for the web component. It will be the same as the one we <a id="_idIndexMarker1495"/>used before but with a <a id="_idIndexMarker1496"/>minor change, as shown in the following screenshot:</p>
<div><div><img alt="Figure 17.19 – Kubernetes service for the web component supporting blue-green deployments" height="844" src="img/Figure_17.19_B19199.jpg" width="538"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.19 – Kubernetes service for the web component supporting blue-green deployments</p>
<p>The only difference<a id="_idIndexMarker1497"/> regarding the definition of the service we used earlier in this chapter is line 17, which adds the <code>color: blue</code> label to the selector. We can find the preceding definition in the <code>sample-solutions/ch17/web-service-blue-green.yaml</code> file.</p>
<p>Then, we can deploy the blue version of the <code>web</code> component with the following command:</p>
<pre class="source-code">
$ kubectl apply -f web-deploy-blue.yaml</pre> <p>We can deploy its service with this command:</p>
<pre class="source-code">
$ kubectl apply -f web-service-blue-green.yaml</pre> <p>Once the service is <a id="_idIndexMarker1498"/>up and running, we can <a id="_idIndexMarker1499"/>determine its IP address and port number and test it:</p>
<pre class="source-code">
$ PORT=$(kubectl get svc/web -o jsonpath='{.spec.ports[0].nodePort}')</pre> <p>Then, we can access it with the <code>curl</code> command:</p>
<pre class="source-code">
$ curl localhost:${PORT}/</pre> <p>This gives us what we expect:</p>
<pre class="source-code">
Pets Demo Application</pre> <p>Now, we can deploy the green version of the <code>web</code> component. The definition of its <code>Deployment</code> object can be found in the <code>sample-solutions/ch17/web-deployment-green.yaml</code> file and looks as follows:</p>
<div><div><img alt="Figure 17.20 – Specification of the green deployment for the web component" height="1029" src="img/Figure_17.20_B19199.jpg" width="606"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.20 – Specification of the green deployment for the web component</p>
<p>The interesting<a id="_idIndexMarker1500"/> lines are as <a id="_idIndexMarker1501"/>follows:</p>
<ul>
<li>Line 8: Named <code>web-green</code> to distinguish it from <code>web-blue</code> and allow for parallel installation</li>
<li>Lines 7, 15, and 21: Have the color green</li>
<li>Line 24: Now using version 2.1 of the web image we built earlier in this chapter</li>
</ul>
<p>Do not forget to change ‘‘<code>demo</code>’’ to your own GitHub username on line 24.</p>
<p>Now, we’re ready to deploy this green version of the service. It should run separately from the blue service:</p>
<pre class="source-code">
$ kubectl apply -f web-deployment-green.yaml</pre> <p>We can make sure that both deployments coexist like so:</p>
<div><div><img alt="Figure 17.21 – Displaying the list of Deployment objects running in the cluster" height="104" src="img/Figure_17.21_B19199.jpg" width="856"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.21 – Displaying the list of Deployment objects running in the cluster</p>
<p>As expected, we have both blue and green running. We can verify that blue is still the active service:</p>
<pre class="source-code">
$ curl localhost:${PORT}/</pre> <p>We should still receive the following output:</p>
<pre class="source-code">
Pets Demo Application</pre> <p>Now comes<a id="_idIndexMarker1502"/> the <a id="_idIndexMarker1503"/>interesting part: we can flip traffic from <code>blue</code> to <code>green</code> by editing the existing service for the web component. To do so, execute the following command:</p>
<pre class="source-code">
$ kubectl edit svc/web</pre> <p>Change the value of the label color from <code>blue</code> to <code>green</code>. Then, save and quit the editor. The Kubernetes CLI will automatically update the service. Now, when we query the web service again, we’ll get this:</p>
<pre class="source-code">
$ curl localhost:${PORT}/</pre> <p>This time, we should get the following output:</p>
<pre class="source-code">
Pets Demo Application v2</pre> <p>This confirms that the traffic has indeed switched to the green version of the web component (note <code>v2</code> at the end of the response to the <code>curl</code> command).</p>
<p class="callout-heading">Note</p>
<p class="callout">If we wanted to stick to the declarative form, it would be better to update the <code>web-service-blue-green.yaml</code> file and apply the new version so that the desired state is still present in a file, avoiding potential mismatch in reality and the file. However, for illustration, the presented way is fine.</p>
<p>If we realize that something went wrong with our green deployment and the new version has a defect, we can easily switch back to the blue version by editing the web service again and replacing the value of the <code>color</code> label with blue. This rollback is instantaneous and should always work. Then, we can remove the buggy green deployment and fix the component. Once we have corrected the problem, we can deploy the green version once again.</p>
<p>Once the green version of the component is running as expected and performing well, we can decommission the blue version:</p>
<pre class="source-code">
$ kubectl delete deploy/web-blue</pre> <p>When we’re ready to deploy a new version, 3.0, this one becomes the blue version. We must update the <code>ch17/web-deployment-blue.yaml</code> file accordingly and deploy it. Then, we must flip the web service from <code>green</code> to <code>blue</code>, and so on.</p>
<p>With that, we have<a id="_idIndexMarker1504"/> successfully<a id="_idIndexMarker1505"/> demonstrated, with our <code>web</code> component of the pets application, how blue-green deployment can be achieved in a Kubernetes cluster.</p>
<p>Next, we are going to learn how to deal with secrets used by applications running on Kubernetes.</p>
<h1 id="_idParaDest-387"><a id="_idTextAnchor387"/>Kubernetes secrets</h1>
<p>Sometimes, services <a id="_idIndexMarker1506"/>that we want to run in the Kubernetes cluster have to use confidential data such as passwords, secret API keys, or certificates, to name just a few. We want to make sure that this sensitive information can only ever be seen by the authorized or dedicated service. All other services running in the cluster should not have any access to this data.</p>
<p>For this reason, Kubernetes secrets were introduced. A secret is a key-value pair where the key is the unique name of the secret, and the value is the actual sensitive data. Secrets are stored in <code>etcd</code>. Kubernetes can be configured so that secrets are encrypted at rest – that is, in <code>etcd</code> – and in transit – that is, when the secrets are going over the wire from a master <a id="_idIndexMarker1507"/>node to the worker nodes that the pods of the service using this secret are running on.</p>
<h2 id="_idParaDest-388"><a id="_idTextAnchor388"/>Manually defining secrets</h2>
<p>We can create a<a id="_idIndexMarker1508"/> secret declaratively in the same way as we can create any other object in Kubernetes. Here is the YAML for such a secret:</p>
<pre class="source-code">
apiVersion: v1kind: Secret
metadata:
  name: pets-secret
type: Opaque
data:
  username: am9obi5kb2UK
  password: c0VjcmV0LXBhc1N3MHJECg==</pre>
<p>The preceding definition can be found in the <code>sample-solutions/ch17/pets-secret.yaml</code> file. Now, you might be wondering what the values are. Are these the real (unencrypted) values? No, they are not. And they are also not encrypted values, but just <code>base64</code>-encoded values.</p>
<p>Thus, they are not really secure, since base64-encoded values can easily be reverted to cleartext values. How did I get these values? That’s easy – follow these steps:</p>
<ol>
<li>Use the <code>base64</code> tool as follows to encode the values:<pre class="source-code">
$ echo "john.doe" | base64</pre></li> </ol>
<p>This will result in the following output:</p>
<pre class="source-code">
am9obi5kb2UK</pre> <p>Also, try the following:</p>
<pre class="source-code">
$ echo "sEcret-pasSw0rD" | base64</pre> <p>This will give us the following output:</p>
<pre class="source-code">
c0VjcmV0LXBhc1N3MHJECg==</pre> <ol>
<li value="2">Using the preceding values, we can create the secret:<pre class="source-code">
$ kubectl create -f pets-secret.yaml</pre></li> </ol>
<p>Here, the command outputs this:</p>
<pre class="source-code">
secret/pets-secret created</pre> <ol>
<li value="3">We can describe the secret with the following command:<pre class="source-code">
$ kubectl describe secrets/pets-secret</pre></li> </ol>
<p>The output of<a id="_idIndexMarker1509"/> the preceding command looks like this:</p>
<div><div><img alt="Figure 17.22 – Creating and describing the Kubernetes secret" height="274" src="img/Figure_17.22_B19199.jpg" width="418"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.22 – Creating and describing the Kubernetes secret</p>
<ol>
<li value="4">In the description of the secret, the values are hidden and only their length is given. So, maybe the secrets are safe now. No, not really. We can easily decode this secret using the <code>kubectl </code><code>get</code> command:<pre class="source-code">
$ kubectl get secrets/pets-secret -o yaml</pre></li> </ol>
<p>The output looks like this:</p>
<div><div><img alt="Figure 17.23 – Kubernetes secret decoded" height="294" src="img/Figure_17.23_B19199.jpg" width="487"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.23 – Kubernetes secret decoded</p>
<p>As we can see in the preceding screenshot, we have our original secret values back.</p>
<ol>
<li value="5">Decode the values you got previously:<pre class="source-code">
$ echo "c0VjcmV0LXBhc1N3MHJECg==" | base64 –decode</pre></li> </ol>
<p>This will result in the following output:</p>
<pre class="source-code">
sEcret-pasSw0rD</pre> <p>Thus, the consequence is that this method of creating a Kubernetes secret is not to be used in any<a id="_idIndexMarker1510"/> environment other than development, where we deal with non-sensitive data. In all other environments, we need a better way to deal with secrets.</p>
<h2 id="_idParaDest-389"><a id="_idTextAnchor389"/>Creating secrets with kubectl</h2>
<p>A much safer <a id="_idIndexMarker1511"/>way to define secrets is to use <code>kubectl</code>. First, we<a id="_idIndexMarker1512"/> must create files containing the base64-encoded secret values, similar to what we did in the preceding section, but, this time, we must store the values in temporary files:</p>
<pre class="source-code">
$ echo "sue-hunter" | base64 &gt; username.txt$ echo "123abc456def" | base64 &gt; password.txt</pre>
<p>Now, we can use <code>kubectl</code> to create a secret from those files, as follows:</p>
<pre class="source-code">
$ kubectl create secret generic pets-secret-prod \    --from-file=./username.txt \
    --from-file=./password.txt</pre>
<p>This will result in this output:</p>
<pre class="source-code">
secret "pets-secret-prod" created</pre> <p>The secret can then be used the same way as the manually created secret.</p>
<p>Why is this method more secure than the other one, you might ask? Well, first of all, no YAML defines a secret, and it is stored in some source code version control system, such as GitHub, which many people have access to, so they can see and decode the secrets.</p>
<p>Only the admin that is authorized to know the secrets ever sees their values and uses them to directly create the secrets in the (production) cluster. The cluster itself is protected by role-based access control so that no unauthorized persons have access to it, nor can<a id="_idIndexMarker1513"/> they<a id="_idIndexMarker1514"/> possibly decode the secrets defined in the cluster.</p>
<p>Now, let’s see how we can use the secrets that we have defined.</p>
<h2 id="_idParaDest-390"><a id="_idTextAnchor390"/>Using secrets in a pod</h2>
<p>Let’s say we want<a id="_idIndexMarker1515"/> to create a <code>Deployment</code> object <a id="_idIndexMarker1516"/>where the <code>web</code> component uses our secret, <code>pets-secret</code>, which we introduced in the preceding section. We can use the following command to create the secret in the cluster:</p>
<pre class="source-code">
$ kubectl apply -f pets-secret.yaml</pre> <p>In the <code>sample-solutions/ch17/web-deployment-secret.yaml</code> file, we can find the definition of the <code>Deployment</code> object. We had to add the part starting from line 23 to the original definition of the <code>Deployment</code> object:</p>
<div><div><img alt="Figure 17.24 – The Deployment object for the web component with a secret" height="1040" src="img/Figure_17.24_B19199.jpg" width="687"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.24 – The Deployment object for the web component with a secret</p>
<p>On lines 29 through 32, we define a volume called <code>secrets</code> from our secret, <code>pets-secret</code>. Then, we use this volume in the container, as described on lines 25 through 28.</p>
<p>We mount<a id="_idIndexMarker1517"/> the <a id="_idIndexMarker1518"/>secrets in the container filesystem at <code>/etc/secrets</code> and mount the volume in read-only mode. Thus, the secret values will be available to the container as files in said folder. The names of the files will correspond to the key names, and the content of the files will be the values of the corresponding keys. The values will be provided in unencrypted form to the application running inside the container.</p>
<p>Apply the deployment with the following command:</p>
<pre class="source-code">
$ kubectl apply -f web-deployment-secret.yaml</pre> <p>In our case, since we have the username and password keys in the secret, we will find two files, named <code>username</code> and <code>password</code>, in the <code>/etc/secrets</code> folder in the container filesystem. The <code>username</code> file should contain the <code>john.doe</code> value and the <code>password</code> file should contain the <code>sEcret-pasSw0rD</code> value. Let’s confirm this:</p>
<ul>
<li>First, we will get the name of the pod:<pre class="source-code">
$ kubectl get pods</pre></li> </ul>
<p>This will give us the following output:</p>
<div><div><img alt="Figure 17.25 – Looking for the name of the pod" height="88" src="img/Figure_17.25_B19199.jpg" width="632"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.25 – Looking for the name of the pod</p>
<ul>
<li>Using<a id="_idIndexMarker1519"/> the<a id="_idIndexMarker1520"/> pod’s name, we can execute the commands shown in the following screenshot to retrieve the secrets:</li>
</ul>
<div><div><img alt="Figure 17.26 – Confirming that secrets are available inside the container" height="289" src="img/Figure_17.26_B19199.jpg" width="1103"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.26 – Confirming that secrets are available inside the container</p>
<p>On line 1 of the preceding output, we <code>exec</code> into the container where the <code>web</code> component runs. Then, on lines 2 to 5, we list the files in the <code>/etc/secrets</code> folder, and, finally, on the last 3 lines, we show the content of the two files, which, unsurprisingly, shows the secret values in clear text.</p>
<p>Since any application written in any language can read simple files, this mechanism of using secrets is very backward-compatible. Even an old Cobol application can read clear text files from the filesystem.</p>
<p>Before leaving, please delete the Kubernetes deployment:</p>
<pre class="source-code">
$ kubectl delete deploy/web</pre> <p>Sometimes, though, applications<a id="_idIndexMarker1521"/> expect secrets to be available<a id="_idIndexMarker1522"/> in environment variables.</p>
<p>Let’s look at what Kubernetes offers us in this case.</p>
<h2 id="_idParaDest-391"><a id="_idTextAnchor391"/>Secret values in environment variables</h2>
<p>Let’s say our<a id="_idIndexMarker1523"/> web component <a id="_idIndexMarker1524"/>expects the username in the <code>PETS_USERNAME</code> environment variable and the password in the <code>PETS_PASSWORD</code> environment variable. If this is the case, we can modify our deployment YAML file so that it looks as follows:</p>
<div><div><img alt="Figure 17.27 – Deployment mapping secret values to environment variables" height="1045" src="img/Figure_17.27_B19199.jpg" width="644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.27 – Deployment mapping secret values to environment variables</p>
<p>On lines 25 through 35, we<a id="_idIndexMarker1525"/> define the two <a id="_idIndexMarker1526"/>environment variables, <code>PETS_USERNAME</code> and <code>PETS_PASSWORD</code>, and map the corresponding key-value pair of <code>pets-secret</code> to them.</p>
<p>Apply the updated deployment:</p>
<pre class="source-code">
$ kubectl apply -f web-deployment-secret.yaml</pre> <p>Note that we don’t need a volume anymore; instead, we directly map the individual keys of <code>pets-secret</code> to the corresponding environment variables that are valid inside the container. The following sequence of commands shows that the secret values are indeed available inside the container in the respective environment variables:</p>
<div><div><img alt="Figure 17.28 – The secret values have been mapped to environment variables" height="102" src="img/Figure_17.28_B19199.jpg" width="548"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.28 – The secret values have been mapped to environment variables</p>
<p>In this section, we have shown you how to define secrets in a Kubernetes cluster and how to use those<a id="_idIndexMarker1527"/> secrets in containers<a id="_idIndexMarker1528"/> running as part of the pods of a deployment. We have shown two variants of how secrets can be mapped inside a container – using files and using environment variables.</p>
<h1 id="_idParaDest-392"><a id="_idTextAnchor392"/>Summary</h1>
<p>In this chapter, we learned how to deploy an application into a Kubernetes cluster and how to set up application-level routing for this application. Furthermore, we learned how to update application services running in a Kubernetes cluster without causing any downtime. Finally, we used secrets to provide sensitive information to application services running in the cluster.</p>
<p>In the next chapter, we are going to learn about different techniques that are used to monitor an individual service or a whole distributed application running on a Kubernetes cluster. We will also learn how we can troubleshoot an application service that is running in production without altering the cluster or the cluster nodes that the service is running on. Stay tuned.</p>
<h1 id="_idParaDest-393"><a id="_idTextAnchor393"/>Further reading</h1>
<p>Here are a few links that provide additional information on the topics that were discussed in this chapter:</p>
<ul>
<li><em class="italic">Performing a rolling </em><em class="italic">update</em>: <a href="https://bit.ly/2o2okEQ">https://bit.ly/2o2okEQ</a></li>
<li><em class="italic">Blue-green </em><em class="italic">deployment</em>: <a href="https://bit.Ly/2r2IxNJ">https://bit.Ly/2r2IxNJ</a></li>
<li><em class="italic">Secrets in </em><em class="italic">Kubernetes</em>: <a href="https://bit.ly/2C6hMZF">https://bit.ly/2C6hMZF</a></li>
</ul>
<h1 id="_idParaDest-394"><a id="_idTextAnchor394"/>Questions</h1>
<p>To assess your learning progress, please answer the following questions:</p>
<ol>
<li>You have an application consisting of two services, the first one being a web API and the second one being a database, such as MongoDB. You want to deploy this application into a Kubernetes cluster. In a few short sentences, explain how you would proceed.</li>
<li>What are liveness and readiness probes in the context of a Kubernetes application service?</li>
<li>Describe in your own words what components you need to establish layer 7 (or application-level) routing for your application.</li>
<li>List the main steps needed to implement a blue-green deployment for a simple application service. Avoid going into too much detail.</li>
<li>Name three or four types of information that you would provide to an application service through Kubernetes secrets.</li>
<li>Name the sources that Kubernetes accepts when creating a secret.</li>
<li>How do you configure an application service to use Kubernetes secrets?</li>
</ol>
<h1 id="_idParaDest-395"><a id="_idTextAnchor395"/>Answers</h1>
<p>Here are the answers to this chapter’s questions:</p>
<ol>
<li>Assuming we have a Docker image in a registry for the two application services – the web API and MongoDB – we need to do the following:<ol><li>Define a deployment for MongoDB using a <code>StatefulSet</code> object; let’s call this deployment <code>db-deployment</code>. The <code>StatefulSet</code> object should have one replica (replicating MongoDB is a bit more involved and is outside the scope of this book).</li><li>Define a Kubernetes service called <code>db</code> of the <code>ClusterIP</code> type for <code>db-deployment</code>.</li><li>Define a deployment for the web API; let’s call it <code>web-deployment</code>.</li><li>Let’s scale this service to three instances.</li><li>Define a Kubernetes service called <code>api</code> of the <code>NodePort</code> type for <code>web-deployment</code>.</li><li>If we are using secrets, then define those secrets directly in the cluster using <code>kubectl</code>.</li><li>Deploy the application using <code>kubectl</code>.</li></ol></li>
<li>Liveness and readiness probes are health checks provided by Kubernetes for containers. A liveness probe checks whether a container is still running, and if not, Kubernetes automatically restarts it. A readiness probe checks whether a container is ready to serve requests. If a container fails the readiness check, it is not removed, but it does not receive incoming requests until it passes the readiness probe.</li>
<li>To implement layer 7 routing for an application, we ideally use <code>IngressController</code>. This is a reverse proxy such as Nginx that has a sidecar listening on the Kubernetes Server API for relevant changes and updating the reverse proxy’s configuration and restarting it if such a change has been detected. Then, we need to define ingress resources in the cluster that define the routing, for example, from a context-based route such as <code>https://example.com/pets</code> to <code>&lt;a service name&gt;/&lt;port&gt;</code> or a pair such as <code>api/32001</code>. The moment Kubernetes creates or changes this <code>Ingress</code> object, the sidecar of <code>IngressController</code> picks it up and updates the proxy’s routing configuration.</li>
<li>Assuming this is a cluster internal inventory service, then we do the following:<ol><li>When deploying version 1.0, we define a deployment called <code>inventory-deployment-blue</code> and label the pods with a label of <code>color:blue</code>.</li><li>We deploy the Kubernetes service of the <code>ClusterIP</code> type called <code>inventory</code> for the preceding deployment with the selector containing <code>color:blue</code>.</li><li>When we’re ready to deploy the new version of the <code>payments</code> service, we define a deployment for version 2.0 of the service and call it <code>inventory-deployment-green</code>. We add a label of <code>color:green</code> to the pods.</li><li>We can now smoke-test the “green” service and when everything is OK, we can update the inventory service so that the selector contains <code>color:green</code>.</li></ol></li>
<li>Some forms of information that are confidential and thus should be provided to services through Kubernetes secrets include passwords, certificates, API key IDs, API key secrets, and tokens.</li>
<li>Sources for secret values can be files or base64-encoded values.</li>
<li>To configure an application to use a Kubernetes secret, you must create a <code>Secret</code> object with the sensitive data. Then, you must modify your <code>Pod</code> specification so that it includes a reference to the <code>Secret</code> object. This reference can be made as an environment variable in the container specification or as a volume mount, allowing the secret data to be used by your application.</li>
</ol>
</div>
</div></body></html>