- en: Orchestration Using Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is dedicated to the most widely used container orchestrator today—Kubernetes.
    In 2018, Kubernetes was adopted by 51% of container users as their main orchestrator.
    Kubernetes adoption has increased in recent years, and it is now at the core of
    most **Container-as-a-S****ervice** (**CaaS**) platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud providers have followed the expansion of Kubernetes, and most of them
    (including Amazon, Google, and Azure) now provide their own **Kubernetes-as-a-Service**
    (**KaaS**) platforms where users do not have to take care of Kubernetes' administrative
    tasks. These services are designed for simplicity and availability on cloud platforms.
    Users just run their workloads on them and the cloud providers manage complicated
    maintenance tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how Kubernetes works and what features it provides.
    We'll review what is required to deploy a Kubernetes cluster with high availability.
    We will then learn about Kubernetes objects, such as pods and services, among
    others. Networking is key to distributing workloads within a cluster; we will
    learn how Kubernetes networking works and how it provides service discovery and
    load balancing. Finally, we will review some of the special security features
    provided by Kubernetes to manage cluster authentication and authorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods, services, and other Kubernetes resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying orchestrated resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes is not part of the Docker Certified Associate exam yet, but it probably
    will be in the next release as Docker Enterprise comes with a fully compatible
    Kubernetes platform deployed on top of the Docker Swarm orchestrator. Docker Enterprise
    is the only container platform that provides both orchestrators at the same time.
    We will learn about Docker Enterprise's components and features in the third section
    of this book, with a chapter dedicated to each component.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the features of the Docker Swarm orchestrator.
    We also provide some labs at the end of the chapter to help you to understand
    and learn about the concepts that we will cover. These labs can be run on your
    laptop or PC using the provided Vagrant *Kubernetes environment* or any already-deployed
    Docker Swarm cluster by yourself. You can view additional information in this
    book's GitHub code repository, which is available at [https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '"[https://bit.ly/3gzAnS3](https://bit.ly/3gzAnS3)"'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kubernetes using Docker Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has many features and is more complex than Docker Swarm. It provides
    additional features not available on Docker Swarm without having to modify our
    application code. Docker Swarm is more aligned with microservices logic, while
    Kubernetes is closer to the virtual machine application's **lift and shift** approach
    (move application as is to a new infrastructure). This is because the Kubernetes
    pod object can be compared to virtual machines (with application processes running
    as containers inside a pod).
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin discussing Kubernetes architecture, let's review some of the
    concepts that we've learned about orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration should provide all that's required for deploying a solution to
    execute, manage, and publish applications based on the containers distributed
    on a pool of nodes. Therefore, it should provide a control plane to ensure cluster
    availability, a scheduler for deploying applications, and a network plane to interconnect
    distributed applications. It should also provide features for publishing cluster-distributed
    applications. Application health will also be managed by the orchestrator. As
    a result, if one application component dies, a new one will be deployed to ensure
    the application's health.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides all of these features, and so does Docker Swarm too. However,
    Kubernetes has many more features, is extensible, and has a bigger community behind
    the project. Docker also adopted Kubernetes in its Docker Enterprise 2.0 release.
    It is the only platform that supports Docker Swarm and Kubernetes on the same
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides more container density because it is able to run more than
    one container at once for each application component. It also provides autoscale
    features and other advanced scheduling features.
  prefs: []
  type: TYPE_NORMAL
- en: Because Kubernetes is a big community project, some of its components have also
    been decoupled on different projects to provide faster deployment. The main open
    source project is hosted by the **Cloud Native Computing Foundation** (**CNCF**).
    Kubernetes releases a new version every 6 months—imagine updating old legacy applications
    in production every 6 months. As previously mentioned, it is not easy to follow
    this application life cycle for many other products, but Kubernetes provides a
    methodology to upgrade to new software releases easily.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes' architectural model is based on the usual orchestration components.
    We deploy master nodes to execute management tasks and worker nodes (also known
    as minions) to run application workloads. We also deploy an `etcd` key-value database
    to store all of the cluster object data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce the Kubernetes components. Masters and workers run different
    processes, and their number may vary depending on the functionalities provided
    by each role. Most of these components could be installed as either system services
    or containers. Here is a list of Kubernetes cluster components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `kubelet` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-proxy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this list is very different from what we learned about Docker Swarm,
    where everything was built-in. Let's review each component's features and properties.
    Remember, this is not a Kubernetes book—we will only learn the basics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will run dedicated master nodes to provide an isolated cluster control plane.
    The following components will run on these nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver`: This is the Kubernetes core, and it exposes the Kubernetes
    API via HTTP (HTTPS if we use TLS certificates). We will connect to this component
    in order to deploy and manage applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`: When we deploy an application''s components, the scheduler
    will decide where to run each one if no node-specific location has been defined.
    To decide where to run deployed workloads, it will review workload properties,
    such as specific resources, limits, architecture requirements, affinities, or
    constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`: This component will manage controllers, which are
    processes that are always watching for a cluster object''s state changes. This,
    for example, will manage the node''s and workload''s states to ensure the desired
    number of instances are running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`: This is the key-value store for all Kubernetes objects'' information
    and states. Some production environments will run `etcd` out of the master nodes''
    infrastructure to avoid performance issues and to improve components'' high availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worker processes, on the other hand, can run on any node. As we learned with
    Docker Swarm, we can decide to run application workloads on worker and master
    nodes. These are the required components for compute nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubelet`: This is the core Kubernetes agent component. It will run on any
    cluster node that is able to execute application workloads. This process will
    also ensure that node-assigned Kubernetes workloads are running and are healthy
    (it will only manage pods created within Kubernetes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are talking about scheduling containers or workloads on a Kubernetes cluster.
    The fact is that we will schedule pods, which are Kubernetes-specific objects.
    Kubernetes will run pods; it will never run standalone containers.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy`: This component will manage the workload''s network interactions
    using operating system packet filtering and routing features. `kube-proxy` should
    run on any worker node (that is, nodes that run workloads).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earlier, we mentioned the container runtime as one of the Kubernetes cluster's
    components. In fact, it is a requirement because Kubernetes itself does not provide
    one. We will use Docker Engine as it is the most widely used engine, and we have
    already discussed it in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following workflow represents all Kubernetes components distributed on
    five nodes (notice that the master has worker components too and that `etcd` is
    also deployed out of it):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/698fa4f7-056a-4506-a8aa-de77fe895290.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As discussed in [Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml), *Orchestration
    Using Docker Swarm*, external load balancers will provide L4 and L7 routing on
    replicated services. In this case, cluster management components do not use router
    mesh-like services. We will provide high availability for core components using
    replicated processes on different nodes. A virtual IP address will be required
    and we will also use **Fully Qualified Domain Name** (**FQDN**) names for **Transport
    Layer Security** (**TLS**) certificates. This will ensure secure communications
    and access to and from Kubernetes components.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the TLS certificates that will be created to ensure
    secure communication between components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86153b0f-f81f-4726-a0d3-52bf4519857b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will use the `kubectl` command line to interact with the Kubernetes cluster,
    and we will always connect to the `kube-apiserver` processes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to implement high-availability Kubernetes
    cluster environments.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Kubernetes cluster with high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm was easy to implement. To provide high availability, we simply
    changed the node roles to accomplish the required odd number of managers. In Kubernetes,
    this is not so easy; roles cannot be changed, and, usually, administrators do
    not change the initial number of master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, installing a Kubernetes cluster with high-availability components
    requires some planning. The good thing here is that Docker Enterprise will deploy
    the cluster for you (since the 2.0 release). We will review this method in [Chapter
    11](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml), *Universal Control Plane*, as
    **Universal Control Plane** (**UCP**) will deploy Kubernetes on top of Docker
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide high availability, we will deploy an odd number of control plane
    components. It is usual to deploy `etcd` on three additional nodes. In this scenario,
    nodes would be neither masters nor workers because `etcd` will be deployed out
    of the Kubernetes nodes. We will require access to this external `etcd` from the
    master nodes only. Therefore, in this situation, we will run a cluster of eight
    nodes: three nodes will run `etcd`, three masters nodes will run all of the other
    control plane components (cluster management), and there will be at least two
    workers to provide redundancy if one of them dies. This is appropriate for many
    Kubernetes environments. We isolate `etcd` from the control plane components to
    provide better management performance.'
  prefs: []
  type: TYPE_NORMAL
- en: We can deploy `etcd` on master nodes. This is similar to what we learned about
    Docker Swarm. We can have *pure masters—*running only management components—and
    worker nodes for workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kubernetes is not easy, and there are many software vendors that
    have developed their own KaaS platforms to provide different methods of installation.
  prefs: []
  type: TYPE_NORMAL
- en: For high availability we will run distributed copies of `etcd`. In this scenario,
    `kube-apiserver` will connect to a list of nodes instead of just one `etcd` node.
    The `kube-apiserver`, `kube-scheduler`, and `kube-controller-manager` processes
    will run duplicated on different master nodes (one instance on each master node).
  prefs: []
  type: TYPE_NORMAL
- en: We will use `kube-apiserver` to manage the cluster. The Kubernetes client will
    connect to this server process using the HTTP/HTTPS protocol. We will use an external
    load balancer to distribute traffic between different replicas running on the
    master nodes. Kubernetes works with the Raft algorithm because `etcd` uses it.
  prefs: []
  type: TYPE_NORMAL
- en: Applications deployed in the cluster will have high availability based on resilience
    by default (just like in Docker Swarm clusters). Once an application is deployed
    with all of its components, if one of them fails, `kube-controller-manager` will
    run a new one. There are different controllers processes, for different deployments
    that are responsible for executing applications based on replicas, on all nodes
    at the same time, and other specific execution situations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce the pod concept, which is key to understanding
    the differences between Kubernetes and Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Pods, services, and other Kubernetes resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pod concept is key to understanding Kubernetes. A pod is a group of containers
    that run together. It is very simple. All of these containers share a network
    namespace and storage. It is like a small logical host because we run many processes
    together, sharing the same IP addresses and volumes. The isolation methods that
    we learned about in [Chapter 1](c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml), *Modern
    Infrastructures and Applications with Docker*, are applicable here.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pods are the smallest scheduling unit in Kubernetes environments. Containers
    within a pod will share the same IP address and can find each other using `localhost`.
    Therefore, assigned ports must be unique within pods. We cannot reuse ports for
    other containers and inter-process communication because processes will run as
    if they were executed on the same logical host. A pod's life relies on the healthiness
    of a container.
  prefs: []
  type: TYPE_NORMAL
- en: Pods can be used to integrate full application stacks, but it is true that they
    are usually used with a few containers. In fact, microservices rely on small functionalities;
    therefore, we will run just one container per node. As pods are the smallest Kubernetes
    scheduling unit, we scale pods up and down, not containers. Therefore, complete
    stacks will be replicated if many grouped application components are executed
    together within a pod.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, pods allow us, for example, to execute a container in order
    to initialize some special features or properties for another container. Remember
    the *Deploying using Docker Stacks* section from [Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml),
    *Orchestration Using Docker Swarm*? In that lab, we launched a PostgreSQL database
    and we added an initialization script to create a specific database. We can do
    this on Kubernetes using the initial containers within a pod.
  prefs: []
  type: TYPE_NORMAL
- en: Terminating and removing pods will depend on how much time it will take to stop
    or delete all of the containers running within a pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents a pod with some containers inside, sharing
    the same IP address and volume, among other features (we will be able to apply
    a special security context to all containers within a pod):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f23d1b15-275f-4766-8466-05b44fb71f75.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's now review the service resources on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Services have a different meaning in Kubernetes. Services are abstract objects
    for the cluster; we do not schedule services in Kubernetes. They define a logical
    set of pods that work together to serve an application component. We can also
    associate a service with an external resource (endpoint). This service will be
    used inside a cluster like any other, but with external IP addresses and ports,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also use services to publish applications inside and outside a Kubernetes
    cluster. For these purposes, there are different types of services. All of them,
    except headless services, provide internal load balancing between all pod replicas
    for a common service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Headless**: We use headless services to interface with non-Kubernetes service
    discovery solutions. No virtual IP will be allocated. There will be no load balancing
    or proxy to reach the service''s pods. This behavior is similar to Docker Swarm''s
    DNSRR mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ClusterIP**: This is the default service type. Kubernetes will provide an
    internal virtual IP address chosen from a configurable pool. This will allow only
    internal cluster objects to reach the defined service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodePort**: NodePort services also receive a virtual IP (ClusterIP), but
    exposed services'' ports will be available on all cluster nodes. Kubernetes will
    route requests to the service''s ClusterIP address, no matter which node received
    them. Therefore, the service''s defined port will be available on `<ANY_CLUSTER_NODE>:<NODEPORT_PORT>`.
    This effectively reminds us of the routing mesh''s behavior on Docker Swarm. In
    this case, we need to add some cluster nodes to external load balancers to reach
    the defined and exposed service''s ports.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LoadBalancer**: This service type is available only in a cloud provider''s
    Kubernetes deployment. We expose a service externally using automatically created
    (using the cloud provider''s API integration) load balancers. It uses both a ClusterIP
    virtual IP for internal routing and a NodePort concept for reaching service-defined
    ports from load balancers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ExternalName**: This is not very common nowadays because it relies on DNS
    CNAME records and is a new implementation. It is used to add external services,
    out of the Kubernetes cluster. External services will be reachable by their names
    as if they were running inside'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following schema represents the NodePort service type''s usual configuration.
    In this example, the service is reachable on port `7000` from an external load
    balancer, while pods are reachable internally on port `5000`. All traffic will
    be internally load balanced between all of the service''s pod endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d68704cd-fb33-4e9a-95f6-0d1afacbed24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are many other resources in Kubernetes. We will take a quick look at some
    of them before going into how we deploy applications on Kubernetes clusters in
    depth.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps and secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned how to distribute the required application information cluster-wide
    with Docker Swarm. Kubernetes also provides solutions for this. We will use ConfigMaps,
    instead of Docker Swarm config objects, and secrets.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, we can use either files or standard input (using the `--from-literal`
    option) to create these resources. The literal option will allow us to create
    these objects using the command line instead of a YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes `kubectl` command line provides two different approaches to create
    cluster resources/objects (imperative and declarative). We will use either command-line
    generators or resource files, usually in YAML format. The first method is usually
    known as imperative, but is not available for all kinds of resources, and using
    files is known as declarative. This will apply to all Kubernetes resources; therefore,
    we will be able to use either `kubectl create pod` with arguments or `kubectl
    create -f <POD_DEFINITION_FILE_IN_YAML_FORMAT>`. We can export a previously generated
    command-line object into YAML format easily to allow resource reproducibility,
    to save its definition somewhere safe.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps and secrets allow us to decouple configurations from image content
    without using unsecured runtime-visible variables or local files shared on some
    nodes. We will use secrets for sensitive data, while ConfigMaps will be used for
    common configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Namespaces can be understood as scopes based on names. They allow us to isolate
    resources between them. The names of resources are unique within each namespace.
    Resources can only be within one namespace; therefore, we can divide access to
    them using namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest uses for namespaces is to limit user access and the usage
    of Kubernetes' objects and resources' quotas. Based on namespaces, we will allow
    a specific set of host resources for users. For example, different groups of users
    or teams will have their own resources and a quota that will limit their environment's
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent volumes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned about volumes in [Chapter 4](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml),
    *Container Persistency and Networking*. In Kubernetes, volumes are attached to
    pods, not containers; therefore, volumes will follow a pod's life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many volume types in Kubernetes and we can mix them inside pods.
    Volumes are available to any container running within a pod. There are volumes
    specially designed for cloud providers and storage solutions that are available
    in most data centers. Let''s review a couple of interesting, commonly used volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`emptyDir`: This volume is created when a pod is assigned to a node and is
    removed with the pod. It starts off empty and is usually used to share information
    between containers running within a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hostPath`: We have already used this type of volume on Docker. These volumes
    allow us to mount a file or directory from the host into pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each volume type has its own special options to enable its unique features.
  prefs: []
  type: TYPE_NORMAL
- en: These volumes are designed to be used within pods, but they are not prepared
    for Kubernetes clustering and storing permanent data. For these situations, we
    use **Persistent Volumes** (**PVs**).
  prefs: []
  type: TYPE_NORMAL
- en: PVs allow us to abstract how storage is provided. It doesn't matter how storage
    hosts arrive in the cluster; we only care about how to use them. A PV is provisioned
    by an administrator, for example, and users are allowed to use it. PVs are Kubernetes
    resources; hence, we can associate them with namespaces and they have their own
    life cycle. They are pod-independent.
  prefs: []
  type: TYPE_NORMAL
- en: PVs are requested by **Persistent Volume Claims** (**PVCs**). Therefore, PVCs
    consume defined PVs. This is the way to associate a pod with a PV.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, PVCs allow users to consume storage. We can designate storage according
    to internal properties, such as speed, how it is provided on the hosts, and more,
    and allow dynamic provisioning using **storage classes**. With these objects,
    we describe all of the storage solutions available in the cluster with their properties
    as profiles and Kubernetes prepares the persistent storage to be used.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to know that we can decide the behavior of the PV data once
    pods die. The **retail reclaim** policy describes what to do with volumes and
    their content once pods no longer use them. Therefore, we will choose between
    deleting the volume, retaining the volume and its content, and recycling it.
  prefs: []
  type: TYPE_NORMAL
- en: We can say that PVs are Kubernetes cluster resources designated for application
    persistent storage and PVCs are the requests to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Storage classes are a new feature that allow administrators to integrate dynamic
    provisions into our cluster. This helps us to provide storage without having to
    manually configure each volume. We will just define profiles and features for
    storage and the provisioners will give the best solution for the required volume.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to deploy workloads on Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying orchestrated resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying workloads in Kubernetes is easy. We will use `kubectl` to specify
    the resources to be created and interact with `kube-apiserver`.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, we can use the command line to either use built-in generators
    or YAML files. Depending on the Kubernetes API version, some options may not be
    available, but we will assume Kubernetes 1.11 or higher.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, all examples use Kubernetes 1.14 because it is the version
    available on the current Docker Enterprise release, 3.0, at the time of writing
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by creating a simple pod. We will review both options—imperative,
    using the command-line, and declarative, using YAML manifests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the pod generator, we will run the `kubectl run --generator=run-pod/v1`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a YAML definition file, we will describe all of the required properties
    of the pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To deploy this `.yaml` definition file, we will just run `kubectl create -f
    <YAML_DEFINITION_FILE>`. This will create all of the defined resources in the
    file on the specified namespace. Because we are not using an argument to specify
    a namespace, they will be created on the user-defined one. In our case, we are
    using the `default` namespace by default.
  prefs: []
  type: TYPE_NORMAL
- en: We can define the namespace either on each YAML file or by using a command-line
    argument. The latter will overwrite the YAML definition.
  prefs: []
  type: TYPE_NORMAL
- en: Both examples will create the same pod, with one container inside, running an
    `nginx:alpine` image.
  prefs: []
  type: TYPE_NORMAL
- en: Take care when using the `args` and `command` definitions on Kubernetes. These
    keys differ from the definitions we used for Docker containers or images. Kubernetes'
    `command` will represent `ENTRYPOINT`, while `args` will represent the container/image
    `CMD` definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can kill this pod by simply removing it using `kubectl delete`. To get a
    list of pods running within a namespace, we will use `kubectl get pods`. If the
    namespace is omitted on the `kubectl` execution, the user-assigned namespace will
    be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But this just created a simple pod; we cannot create more NGINX replicas with
    this kind of resource. To use replicas, we will use ReplicaSets instead of single
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: We will set up a pod template section and pod selectors to identify which deployed
    pods belong to this `ReplicaSet` resource within a new YAML file. This will help
    the controller to watch the pods' health.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, to the previous pod definition, we add a `template` section and a `selector`
    key with labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we created three replicas using the same pod definition as we did
    earlier. This pod''s definition was used as a template for all of the replicas.
    We can review all of the resources deployed using `kubectl get all`. In the following
    command, we filter the results to retrieve only resources with the `example` label
    and the `myfirstrs` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each replica will have the same prefix name, but its own ID will be part of
    the name. This uniquely identifies the resource in the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We are using `kubectl get all -l <KEY=VALUE>` to filter all of the resources
    we labeled with the `example` key and the `myfirstrs` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `DaemonSet` to deploy a replica on each node in the cluster, just
    as we did with Docker Swarm''s global services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now review the pod distribution again using `kubectl get all`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we added the container''s resource limits and resource requests.
    The `limits` key allows us to specify resource limits for each container. On the
    other hand, `requests` informs the scheduler about the minimal resources required
    to run this component. A pod will not be able to run on a node if there are not
    enough resources to achieve the requested CPU, memory, and more. If any containers
    exceed their limits, they will be terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Deployment` resource is a higher-level concept, as it manages `ReplicaSet`
    and allows us to issue application component updates. It is recommended that you
    use `Deployment` instead of `ReplicaSet`. We will again use the `template` and
    `select` sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, the deployment will run three replicas of `nginx:alpine`, distributed
    again on cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Notice that replicas are only running on some nodes. This is because we have
    some taintson the other nodes (some Kubernetes deployments avoid workloads on
    master nodes by default). Taints and tolerations help us to allow the scheduling
    of pods on only specific nodes. In this example, the master node will not run
    a workload, although it also has a worker role (it runs the Kubernetes worker
    processes that we learned about, `kubelet` and `kube-proxy`). These features remind
    us of Docker Swarm's node availability concepts. In fact, we can also execute
    `kubectl cordon <NODE>` to set a node as non-schedulable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is a brief introduction to the main concepts of Kubernetes. We
    highly recommend that you view the Kubernetes documentation for further information:
    [https://kubernetes.io](https://kubernetes.io).'
  prefs: []
  type: TYPE_NORMAL
- en: We can set replication based on a pod's performance and limits. This is known
    as **autoscaling**, and it is an interesting feature that is not available in
    Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: When an application's replicated components require persistence, we use another
    kind of resource. StatefulSets guarantee the order and uniqueness of pods.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to deploy applications, let's review how Kubernetes manages
    and deploys a network locally with components distributed on different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes, like any other orchestrator, provides local and distributed networking.
    There are a few important communication assumptions that Kubernetes has to accomplish:'
  prefs: []
  type: TYPE_NORMAL
- en: Container-to-container communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-to-pod communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-to-service communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User access and communication between external or internal applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container-to-container communication is easy because we learned that containers
    within a pod share the same IP and network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: We know that each pod gets its own IP address. Therefore, Kubernetes needs to
    provide routing and accessibility to and from pods running on different hosts.
    Following the Docker concepts that we learned about in [Chapter 4](e7804d8c-ed8c-4013-8449-b746ee654210.xhtml),
    *Container Persistency and Networking*, Kubernetes also uses bridge networking
    for pods running on the same host. Therefore, all pods running on a host will
    be able to talk with each other using bridge networking.
  prefs: []
  type: TYPE_NORMAL
- en: Remember how Docker allowed us to deploy different bridge networks on a single
    host? This way, we were able to isolate applications on a host using different
    networks. Using this local concept, overlaying networks on a Docker Swarm cluster
    also deployed bridged interfaces. And these interfaces will be connected using
    tunnels created between hosts using VXLAN. Isolation was something simple on Docker
    standalone hosts and Docker Swarm. Docker Engine had to manage all of the backstage
    magic to make this work with firewall rules and routing, but overlay networking
    is available out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides a simpler approach. All pods run on the same network; hence,
    every pod will see other pods within the same host. In fact, we can go further—pods
    are locally accessible from hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider this concept with a couple of pods. We will run `example-webserver`
    and `example-nettools` at the same time, executing simple `nginx:alpine` and `frjaraur/nettools:minimal`
    (this is a small alpine image with some helpful network tools) pods. First, we
    will create a deployment for `example-webserver` using `kubectl create deployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We review the pod''s IP address using `kubectl get pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we said, `localhost` communications to the pod will work. Let''s try a simple
    `ping` command from the host to the pod''s IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can also have access to its running `nginx` process. Let''s
    try `curl` using the pod''s IP again, but this time, we will use port `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the host can communicate with all of the pods running on top of Docker
    Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get a pod''s IP address using `jsonpath`, to format the pod''s information
    output, which is very interesting when we have hundreds of pods: `kubectl get
    pod example-webserver -o jsonpath=''{.status.podIP}''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute an interactive pod with the aforementioned `frjaraur/nettools:minimal`image.
    We will use `kubectl run --generator=run-pod/v1` to execute this new pod. Notice
    that we added `-ti -- sh` to run an interactive shell within this pod. From this
    pod, we will run `curl` again, connecting to the `example-webserver` pod''s IP
    address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully accessed the deployed `example-webserver` pod using `ping`
    and `curl`, sending some requests to its `nginx` running process. It is clear
    that both containers can see each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is something even more interesting in this example: we have not reviewed
    where these pods are running. In fact, they are running on different hosts, as
    we can read from the `kubectl get pods -o wide` command''s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Networking between hosts is controlled by another component that will allow
    these distributed communications. In this case, this component is Calico, which
    is a **container network interface** (**CNI**) applied to this Kubernetes cluster.
    The Kubernetes network model provides a flat network (all pods are distributed
    on the same network), and data plane networking is based on interchangeable plugins.
    We will use the plugin that best affords all of the required features in our environment.
  prefs: []
  type: TYPE_NORMAL
- en: There are other CNI implementations apart from Calico, such as Flannel, Weave,
    Romana, Cillium, and more. Each one provides its own features and host-to-host
    implementations. For example, Calico uses **Border Gateway Protocol** (**BGP**)
    to route real container IP addresses inside the cluster. Once a CNI is deployed,
    all of the container IP addresses will be managed by its implementation. They
    are usually deployed at the beginning of a Kubernetes cluster implementation.
    Calico allows us to implement network policies, which are very important to ensure
    security in this flat network where every pod sees other pods.
  prefs: []
  type: TYPE_NORMAL
- en: We have not looked at any service networking yet, which is also important here.
    If a pod dies, a new IP will be allocated, hence access will be lost on the previous
    IP address; that is why we use services. Remember, services are logical groupings
    of pods, usually with a virtual IP address. This IP address will be assigned from
    another pool of IP addresses (the service IP addresses pool). Pods and services
    do not share the same IP address pool. A service's IP will not change when new
    pods are recreated.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s create a service associated with the currently deployed `example-webserver`
    deployment. We''ll use `kubectl expose`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We could have done this using either `kubectl create service` (imperative format)
    or a YAML definition file (declarative format). We used `kubectl expose` because
    it''s simpler to quickly publish any kind or resource. We can review a service''s
    IP addresses using `kubectl get services`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that we define the services associated with pods using selectors.
    In this case, the service will group all pods with the `app` label and the `example-webserver`
    value. This label was automatically created because we created `Deployment`. As
    a result, all pods grouped for this service will be accessible on the `10.98.107.31`
    IP address and the internal TCP port `80`. We defined which pod''s port will be
    associated with this service—in both cases, we set port `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It is accessible, as expected. Kubernetes' internal network has published this
    service on the defined ClusterIP address.
  prefs: []
  type: TYPE_NORMAL
- en: Because we created this service as `NodePort`, a random port has been associated
    with the service. In this case, it is port `30951`. As a result, requests will
    be routed to the application's pods within the cluster when we reach the cluster
    nodes' IP addresses in the randomly chosen port.
  prefs: []
  type: TYPE_NORMAL
- en: '`NodePort` ports are assigned randomly by default, but we can set them manually
    in the range between `30000` and `32767`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s verify this feature. We will send some requests to the port that is
    listening on cluster nodes. In this example, we''ll use the `curl` command on
    the local `0.0.0.0` IP address and port `30951` on various nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Communication between pods happens even if they are not running on the same
    node. The following output shows that pods are not running in either `node1` or
    `node3`. The application''s pod is running on `node2`. The internal routing works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'There is something more interesting, though—services create a DNS entry with
    their names following this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we have not used a namespace or a domain. The service resolution
    will be simple: `example-webserver.default.svc.cluster.local`. This resolution
    is only available in the Kubernetes cluster by default. Therefore, we can test
    this resolution by executing a pod with the `host` or `nslookup` tools. We will
    attach our terminal interactively to the running `example-nettools`pod using `kubectl
    attach` and run `host` and `curl` to test the DNS resolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We have confirmed that the service has a DNS entry that is reachable by any
    other Kubernetes cluster resource. We have also published the service using `NodePort`,
    so it is accessible on any node IP address. We could have an external load balancer
    routing requests to this deployed service on any cluster node's IP address and
    a chosen (or manually set) port. This port will be fixed for this service until
    it is removed.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we used `kubectl attach example-nettools -c example-nettools -i
    -t` to reconnect to a running pod left in the background.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how scaling will change the described behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we now scale up to three replicas, without changing anything on the deployed
    service, we will add load balancing features. Let''s scale up using `kubectl scale`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now we will have three running instances or pods for the `example-webserver`
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we have scaled from the command line using the resource''s type
    and its name: `kubectl scale --replicas=<NUMBER_OF_REPLICAS> <RESOURCE_TYPE>/<NAME>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can review deployment pods using `kubectl get pods` with the associated
    label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now test the service''s access again, we will reach each one of the three
    replicas. We execute the next simple loop to reach the service''s backend pods
    five times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we review one of the deployed pod''s logs using `kubectl logs`, we will
    notice that not all requests were logged. Although we made more than two requests
    using the service''s IP address, we just logged a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Only one-third of the requests are logged on each pod; therefore, the internal
    load balancer is distributing the traffic between all available applications'
    pods. Internal load balancing is deployed by default between all pods associated
    with a service.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, Kubernetes provides flat networks for pods and services, simplifying
    networking and internal application accessibility. On the other hand, it is insecure
    because any pod can reach any other pods or services. In the next section, we
    will learn how to avoid this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Network policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network policies define rules to allow communication between groups of pods
    and other components. Using labels, we apply specific rules to matching pods for
    ingress and egress traffic on defined ports. These rules can be set using IP ranges,
    namespaces, or even other labels to include or exclude resources.
  prefs: []
  type: TYPE_NORMAL
- en: Network policies are applied using network plugins; therefore, the CNI deployed
    on our cluster must support them. For example, Calico supports `NetworkPolicy`
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: We will be able to define default rules to all pods in the cluster, isolating
    all internet traffic, for example, or a defined group of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This YAML file represents an example of a `NetworkPolicy` resource applying
    ingress and egress traffic rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we will apply defined ingress and egress rules to all pods
    including the `tier` label with the `database` value.
  prefs: []
  type: TYPE_NORMAL
- en: The ingress rule allows traffic from any pod on the same namespace with the
    `tier` label and the `frontend`value. All IP addresses in subnet `172.17.10.0/24`
    will also be allowed to access defined `database` pods.
  prefs: []
  type: TYPE_NORMAL
- en: The egress rule allows traffic from defined `database` pods to port `5978` on
    all IP addresses on subnet `10.0.0.0/24`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we do not apply a `NetworkPolicy` resource to a namespace, all traffic is
    allowed. We can change this behavior using `podSelector: {}`. This will match
    all pods in the namespace. For example, to disallow all egress traffic, we can
    use the following `NetworkPolicy` YAML definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: So, we have learned that we can ensure security even on a Kubernetes flat network
    with `NetworkPolicy` resources. Let's review the ingress resources.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingress resources help us to publish applications deployed on Kubernetes clusters.
    They work very well with HTTP and HTTPS services, providing many features for
    distributing and managing traffic between services. This traffic will be located
    on the OSI model's transport and application layers; they are also known as layers
    4 and 7, respectively. It also works with raw TCP and UDP services; however, in
    these cases, traffic will be load balanced at layer 4 only.
  prefs: []
  type: TYPE_NORMAL
- en: 'These resources route traffic from outside the cluster to services running
    within the cluster. Ingress resources require the existence of a special service
    called an **ingress controller**. These services will load balance or route traffic
    using rules created by ingress resources. Therefore, publishing an application
    using this feature requires two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingress resource**: The rules to apply to incoming traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress controller**: The load balancer that will automatically convert or
    translate ingress rules to load balance configurations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A combination of both objects provides the dynamic publishing of applications.
    If one application''s pod dies, a new one will be created and the service and
    ingress controller will automatically route all traffic to the new one. This will
    also isolate services from external networks. We will publish one single endpoint
    instead of the `NodePort` or `LoadBalancer` service types for all services, which
    will consume many nodes'' ports or cloud IP addresses. This endpoint is the load
    balancer that will use the ingress controller and ingress resource rules to route
    traffic internally to deployed services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64584d31-c831-4590-a961-cc5877468a3a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This chapter''s labs show us an interesting load balancing example using **NGINX
    Ingress Controller**. Let''s review a quick example YAML configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This example outlines the rules that should be applied to route requests to
    a specific `example.local`-specific host header. Any request containing `/example1`
    in its URL will be guided to `example-webserver`, while `another-service` will
    receive requests containing the `/example2` path in its URL. Notice that we have
    used the internal service's ports; therefore, no additional service exposure is
    required. One ingress controller endpoint will redirect traffic to the `example-webserver`
    and `another-service` services. This saves up the host's ports (and/or IP addresses
    on the cloud providers because the `LoadBalancer` service type uses one published
    public IP address per service).
  prefs: []
  type: TYPE_NORMAL
- en: We can provide as many ingress controllers as needed. In fact, in multi-tenant
    environments, we usually deploy more than just one to isolate publishing planes
    between different tenants.
  prefs: []
  type: TYPE_NORMAL
- en: This brief look at publishing applications on Kubernetes has finished this review
    of the main Kubernetes networking features. Let's now move on to Kubernetes security
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes security components and features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides mechanisms to authenticate and authorize access to its API.
    This allows us to apply different levels of privileges for users or roles within
    a cluster. This prevents unauthorized access to some core resources, such as scheduling
    or nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once users are allowed to use cluster resources, we use namespaces to isolate
    their own resources from other users. This works even in multi-tenant environments
    where a higher level of security is required.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes works with the very elaborate **Role-Based Access Control** (**RBAC**)
    environment, which provides a great level of granularity to allow specific actions
    on some resources while other actions are denied.
  prefs: []
  type: TYPE_NORMAL
- en: We manage the `Role` and `ClusterRole` resources to describe permissions for
    different resources. We use `Role` to define permissions within namespaces and
    `ClusterRole` for permissions on cluster-wide resources. Rules are supplied using
    some defined verbs, such as `list`, `get`, `update`, and more, and these verbs
    are applied to resources (or even specific resource names). The `RoleBinding`
    and `ClusterRoleBinding` resources grant permissions defined in roles to users
    or sets of users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes also provides the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Service accounts to identify processes within pods to other resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod security policies to control the special behaviors of pods, such as privileged
    containers, host namespaces, restrictions on running containers with root users,
    or enabling read-only root filesystems on containers, among other features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admission controllers to intercept API requests, allowing us to validate or
    modify them to ensure image freshness and security, forcing the creation of pods
    to always pull from registries, to set the default storage, to deny the execution
    of processes within privileged containers, or to specify the default host resource
    limit ranges if none are declared, among other security features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very important in production environments to limit a host's resource usage
    because non-limited pods can consume all of their resources by default.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides many features to ensure cluster security at all levels.
    It is up to you to use them because most of them are not applied by default. We
    will learn more about the roles and grants applied to resources in [Chapter 11](1879ea92-ae47-4230-ac84-784d4bc73185.xhtml),
    *Universal Control Plane*, because many of these configurations are integrated
    into Docker Enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to go deeper into this topic because Kubernetes is not part
    of the current Docker Certified Associate curriculum, and this is just a quick
    introduction.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended that you take a closer look at Kubernetes' security features
    because it has many more compared to Docker Swarm. On the other hand, it is true
    that Docker Enterprise provides many of these features to Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Docker Swarm and Kubernetes side by side
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will compare the Docker Swarm and Kubernetes features side
    by side to get a good idea of how they solve common problems. We have discussed
    these concepts in both this chapter and in [Chapter 8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml),
    *Orchestration Using Docker Swarm.* They have common approaches to many problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameters** | **Docker Swarm** | **Kubernetes** |'
  prefs: []
  type: TYPE_TB
- en: '| High-availability solution | Provides high availability for core components.
    | Provides high availability for core components. |'
  prefs: []
  type: TYPE_TB
- en: '| Resilience | All services run with resilience based on the state definition.
    | All resources based on replication controllers will provide resilience (`ReplicaSet`,
    `DaemonSet`, `Deployment`, and `StatefulSet`) based on the state definition. |'
  prefs: []
  type: TYPE_TB
- en: '| Infrastructure as code | The Docker Compose file format will allow us to
    deploy stacks. | We will use YAML to format resource files. These will allow us
    to deploy workloads using a declarative format. |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic distribution | Application components and their replicas will be
    automatically distributed cluster-wide, although we can provide some constraints.
    | Kubernetes also distributes components, but we can provide advanced constraints
    using labels and other features. |'
  prefs: []
  type: TYPE_TB
- en: '| Automatic updates | Application components can be upgraded using rolling
    updates and rollbacks in the case of a failure. | Kubernetes also provides rolling
    updates and rollbacks. |'
  prefs: []
  type: TYPE_TB
- en: '| Publishing applications | Docker Swarm provides internal load balancing between
    service replicas and **router mesh** to publish an application''s service''s ports
    on all of the cluster nodes at the same time. | Kubernetes also provides internal
    load balancing, and `NodePort` type services will also publish the application''s
    components on all of the nodes at the same time. But Kubernetes also provides
    load balancing services (among other types) to auto-configure external load balancers
    to route requests to deployed services. |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster-internal networking | Containers that are deployed as tasks for each
    service can communicate with other containers deployed in the same network. Internal
    IP management will provide their IP addresses, and services can be consumed by
    their names so that there is internal DNS resolution. | Pod-to-pod communication
    works and IP addresses are provided by internal **Internet Protocol Address Management**
    (IPAM). We will also have service-to-service communication and resolution. |'
  prefs: []
  type: TYPE_TB
- en: '| Key-value store | Docker Swarm provides an internal store to manage all objects
    and their statuses. This store will have high availability with an odd number
    of master nodes. | Kubernetes also requires a key-value store to manage its resources.
    This component is provided using `etcd` and we can deploy it externally out of
    Kubernetes cluster nodes. We should provide an odd number of `etcd` nodes to provide
    high availability. |'
  prefs: []
  type: TYPE_TB
- en: 'The preceding table showed us the main similarities regarding solving common
    problems. The next table will show the main differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameters** | **Docker Swarm** | **Kubernetes** |'
  prefs: []
  type: TYPE_TB
- en: '| Pods versus tasks | Docker Swarm deploys tasks for services. Each task will
    run one container at a time. If the container dies, a new one will be created
    to ensure the required number of replicas (tasks).Services are the smallest unit
    of deployment. We will deploy applications running their components as services.
    | Kubernetes has the concept of a pod. Each pod can run more than one container
    inside. All of them share the same IP address (networking namespace). Containers
    inside a pod share volumes and will always run on the same host. A pod''s life
    relies on containers. If one of them dies, a pod is unhealthy. Pods are the smallest
    unit of deployment in Kubernetes; therefore, we scale pods up and down, with all
    of their containers. |'
  prefs: []
  type: TYPE_TB
- en: '| Services | Services in Docker Swarm are objects with an IP address for internal
    load balancing between replicas (by default, we can avoid this using the `dnsrr`
    endpoint mode). We create services to execute our application components, and
    we scale up or down the number of replicas required to be healthy. | In Kubernetes,
    services are different. They are logical resources. This means that they are deployed
    only to publish a group of pod resources. Kubernetes services are logical groupings
    of pods that work together. Kubernetes services also get an IP address for internal
    load balancing (`clusterIP`), and we can also avoid this situation by using the
    "headless" feature. |'
  prefs: []
  type: TYPE_TB
- en: '| Networking | Docker Swarm deploys overlay networking by default. This ensures
    communications between an application''s components are deployed on different
    hosts.Stacks in Docker Swarm will be deployed on different networks. This means
    that we can provide a subnet for each application. Multiple networks for deployments
    will provide a good level of security because they are isolated from each other.
    This can be improved using available network encryption (disabled by default).
    However, on the other hand, they are difficult to manage and things can get complicated
    when we need to provide isolation on services integrated into multiple stacks.
    | Kubernetes provides a flat network using a common interface called a CNI. Networking
    has been decoupled from Kubernetes'' core to allow us to use multiple and different
    networking solutions. Each solution has its own features and implementation for
    routing on a cluster environment. A flat network makes things easier. All pods
    and services will see each other by default. On the other hand, security is not
    provided. We will deploy `NetworkPolicy` resources to ensure secure communications
    between resources in the cluster. These policies will manage who can talk to who
    in the Kubernetes world. |'
  prefs: []
  type: TYPE_TB
- en: '| Authentication and authorization | Docker Swarm, by default, does not provide
    any mechanism to authenticate or authorize specific requests. Once a Docker Swarm
    node has published its daemon access (in a `daemon.json` configuration file),
    anyone can connect to it and manage the cluster if we use a manager node. This
    is a security risk that should always be avoided. We can create a secure client
    configuration with SSL/TLS certificates. But certificates in Docker Swarm will
    ensure secure communication only. There is no authorization validation. Docker
    Enterprise will provide the required features to provide RBAC to Docker Swarm''s
    clusters. | Kubernetes does provide authentication and authorization. In fact,
    it includes a full-featured RBAC system to manage users'' and applications'' accesses
    to the resources deployed within the Kubernetes cluster. This RBAC system allows
    us to set specific permissions for a user''s or team''s access. Using Kubernetes
    namespaces will also improve security in multi-tenant or team scenarios. |'
  prefs: []
  type: TYPE_TB
- en: '| Secrets | Docker encrypts secrets by default. They will only be readable
    inside containers while they are running. | Kubernetes will encode secrets using
    the Base64 algorithm by default. We will need to use external secret providers
    or additional encryption configuration (`EncryptionConfig`) to ensure a secret''s
    integrity. |'
  prefs: []
  type: TYPE_TB
- en: '| Publishing applications | Docker Swarm just provides a router mesh for publishing
    applications. This will publish application ports on all cluster nodes. This can
    be insecure because all nodes will have all the applications published and we
    will use a lot of ports (at least one for each published application). Docker
    Enterprise will provide Interlock, which has many features in common with ingress
    controllers. | Kubernetes provides ingress controller resources. Ingress controllers
    publish a few endpoints (using `NodePort` or any other cloud service definition),
    and this internal ingress will talk to services'' backends (pods). This will require
    fewer ports for applications (only those required to publish ingress controllers).
    Requests will be routed by these resources to real backend services. Security
    is improved because we add a smart piece of software in the middle of the requests
    to help us to decide which backends will process requests. The ingress controller
    acts as a reverse-proxy and it will verify whether a valid host header is used
    on every request. If none is used, requests will be forwarded to a default backend.
    If requests contain a valid header, they will be forwarded to the defined service''s
    virtual IP and the internal load balancer will choose which pod will finally receive
    them. The orchestrator will manage defined rules and clusters, and the internal
    or external load balancer will interpret them to ensure the right backend receives
    the user''s request.  |'
  prefs: []
  type: TYPE_TB
- en: 'So far, we have learned that there are several similarities and differences
    between Docker Swarm and Kubernetes. We can note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides more container density.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm provides cluster-wide networking with subnets for isolation by
    default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes provides role-based access to cluster resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing applications in Kubernetes is better using ingress controllers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now review some of the topics we have learned by applying them to some
    easy labs.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter labs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now work through a long lab that will help us to review the concepts
    we've learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy `environments/kubernetes` from this book's GitHub repository ([https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git](https://github.com/PacktPublishing/Docker-Certified-Associate-DCA-Exam-Guide.git))
    if you have not done so yet. You can use your own Linux server. Use `vagrant up`
    from the `environments/kubernetes` folder to start your virtual environment. All
    files used during these labs can be found inside the `chapter9` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait until all of the nodes are running. We can check the status of the nodes
    using `vagrant status`. Connect to your lab node using `vagrant ssh kubernetes-node1`.
    Vagrant deploys three nodes for you, and you will be using the `vagrant` user
    with root privileges using `sudo`. You should have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Nodes will have three interfaces (IP addresses and virtual hardware resources
    can be modified by changing the `config.yml` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '`eth0 [10.0.2.15]`: This is an internal interface, required for Vagrant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eth1 [10.10.10.X/24]`: This is prepared for Docker Kubernetes'' internal communication.
    The first node will get the `10.10.10.11` IP address and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eth2 [192.168.56.X/24]`: This is a host-only interface for communication between
    your host and the virtual nodes. The first node will get the `192.168.56.11` IP
    address and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the `eth1` interface for Kubernetes, and we will be able to connect
    to published applications using the `192.168.56.X/24` IP address' range. All nodes
    have Docker Engine Community Edition installed and a Vagrant user is allowed to
    execute `docker`. A small Kubernetes cluster with one master (`kubernetes-node1`)
    and two worker nodes (`kubernetes-node2` and `kubernetes-node3`) will be deployed
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now connect to the first deployed virtual node using `vagrant ssh kubernetes-node1`.
    The process may vary if you have already deployed a Kubernetes virtual environment
    and have just started it using `vagrant up`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now you are ready to start the labs. We will start these labs by deploying a
    simple application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once Vagrant (or your own environment) is deployed, we will have three nodes
    (named `kubernetes-node<index>` from `1` to `3`) with Ubuntu Xenial and Docker
    Engine installed. Kubernetes will also be up and running for you, with one master
    node and two workers. The Calico CNI will also be deployed for you automatically.
  prefs: []
  type: TYPE_NORMAL
- en: First, review your node IP addresses (`10.10.10.11` to `10.10.10.13` if you
    used Vagrant, because the first interface will be Vagrant-internal).
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for deploying our application are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to `kubernetes-node1` and review the deployed Kubernetes cluster using
    `kubectl get nodes`. A file, named `config`, including the required credentials
    and the Kubernetes API endpoint will be copied under the `~/.kube`directory automatically.
    We''ll also refer to this file as **Kubeconfig**. This file configures the `kubectl`
    command line for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes cluster version 1.14.00 has been deployed and is running. Notice
    that `kubernetes-node1` is the only master node in this cluster; therefore, we
    are not providing high availability.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, we are using the `admin` user, and, by default, all deployments will
    run on the `default` namespace, unless another is specified. This configuration
    is also done in the `~/.kube/config` file.
  prefs: []
  type: TYPE_NORMAL
- en: The Calico CNI was also deployed; hence, host-to-container networking should
    work cluster-wide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a deployment file, named `blue-deployment-simple.yaml`, using your favorite
    editor with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This will deploy two replicas of the`codegazers/colors:1.12` image. We will
    expect two running pods after it is deployed. We set the `COLOR` environment variable
    to `blue` and, as a result, all of the application components will be `blue`.
    Containers will expose port `3000` internally within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy this `blue-app` application using `kubectl create -f <KUBERNETES_RESOURCES_FILE>.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This command line has created a deployment, named `blue-app`, with two replicas.
    Let''s review the deployment created using `kubectl get deployments`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, two pods will be running, associated with the `blue-app`deployment.
    Let''s now review the deployed pods using `kubectl get pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, one pod runs on `kubernetes-node2` and another one runs on `kubernetes-node3`.
    Let''s try to connect to their virtual assigned IP addresses on the exposed port.
    Remember that IP addresses will be assigned randomly, hence they may vary on your
    environment. We will just use `curl` against the IP address of `kubernetes-node1`
    and the pod''s internal port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can connect from `kubernetes-node1` to pods running on other hosts correctly.
    So, Calico is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We should be able to connect to any pods' deployed IP addresses. These IP addresses
    will change whenever a container dies and a new pod is deployed. We will never
    connect to pods to consume their application processes. We will use services instead
    of pods to publish applications, as we have already discussed in this chapter.
    They will not change their IP addresses when application components, running as
    pods, have to be recreated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a service to load balance requests between deployed pods with
    a fixed virtual IP address. Create the `blue-service-simple.yaml` file with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'A random IP address will be associated with this service. This IP address will
    be fixed, and it will be valid even if pods die. Notice that we have exposed a
    new port for the service. This will be the service''s port, and requests reaching
    the defined port `80`will be routed to port `3000` on each pod. We will use `kubectl
    get svc` to retrieve the service''s port and IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s verify the internal load balance by sending some requests to the `blue-svc`
    service using `curl` against its IP address, accessing port `80`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try again using `curl`. We will test the internal load balancing by
    executing some requests to the service''s IP address and port:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The service has load-balanced our requests between both pods. Let's now try
    to expose this service to be accessible to the application's users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will remove the previous service''s definition and deploy a new one
    with the service''s `NodePort` type. We will use `kubectl delete -f <KUBERNETES_RESOURCES_FILE>.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new definition, `blue-service-nodeport.yaml`, with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We now just create a service definition and notice a random port associated
    with it. We will also use `kubectl create` and `kubectl get svc` after it is deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that the `NodePort` service will act as Docker Swarm''s router mesh.
    Therefore, the service''s port will be fixed on every node. Let''s verify this
    feature using `curl` against any node''s IP address and assigned port. In this
    example, it is `32648`. This port may vary on your environment because it will
    be assigned dynamically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Locally, on `node1` port `32648`, the service is accessible. It should be accessible
    on any of the nodes on the same port. Let''s try on `node3`, for example, using
    `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We learned that even if a node does not run a related workload, the service
    will be accessible on the defined (or, in this case, random) port using `NodePort`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will finish this lab by upgrading the deployment images to a newer version.
    We will use `kubectl set image deployment`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s review the deployment again to verify that the update was done. We will
    use `kubectl get all -o wide` to retrieve all of the created resources and their
    locations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that new pods were created with a newer image. We can verify the update
    using `kubectl rollout status`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can go back to the previous image version just by executing `kubectl rollout
    undo`. Let''s go back to the previous image version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, we can verify that the current `blue-app` deployment runs the `codegazers/colors:1.12`
    images again. We will again review deployment locations using `kubectl get all`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Going back to the previous state was very easy.
  prefs: []
  type: TYPE_NORMAL
- en: We can set comments for each change using the `--record` option on the `update`
    commands.
  prefs: []
  type: TYPE_NORMAL
- en: Using volumes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lab, we will deploy a simple web server using different volumes. We
    will use `webserver.deployment.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have prepared the following volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`congigMap`: Config volume with `/etc/nginx/conf.d/default.conf`—the configuration
    file)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emptyDir`: Empty volume for NGINX logs, `/var/log/nginx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`secret`: Secret volume to specify some variables to compose the `index.html`
    page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persistentVolumeClaim`: Data volume bound to the `hostPath` defined as `persistentVolume`
    using the host''s `/mnt` content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have declared one specific node for our web server to ensure the `index.html`
    file location under the `/mnt` directory. We have used `nodeName: kubernetes-node2`
    in our deployment file, `webserver.deployment.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we verify that there is no file under the `/mnt` directory in the `kubernetes-node2`
    node. We connect to `kubernetes-node2` and then we review the `/mnt` content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we change to `kubernetes-node1` to clone our repository and launch the
    web server deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We move to `chapter9/nginx-lab/yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `ConfigMap`, `Secret`, `Service`, `PersistentVolume`, and `PersistentVolumeClaim`
    resources in this lab using YAML files. We will deploy all of the resource files
    in the `yaml` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will review all of the resources created. We have not defined a namespace;
    therefore, the `default` namespace will be used (we omitted it in our commands
    because it is our default namespace). We will use `kubectl get all` to list all
    of the resources available in the default namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'However, not all of the resources are listed. The `PersistentVolume` and `PersistentVolumeClaim`
    resources are not shown. Therefore, we will ask the Kubernetes API about these
    resources using `kubectl get pv` (`PersisteVolumes`) and `kubectl get pvs` (`PersistenVolumeClaims`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s send some requests to our web server. You can see, in `kubectl get all`
    output, that `webserver-svc` is published using `NodePort` on port `30080`, associating
    the host''s port `30080` with the service''s port `80`. As mentioned earlier,
    all hosts will publish port `30080`; therefore, we can use `curl` on the current
    host (`kubernetes-node1`) and port `30080` to try to reach our web server''s pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We have used ;a `ConfigMap` resource to specify an NGINX configuration file,
    `webserver.configmap.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This configuration is included inside our deployment file, `webserver.deployment.yaml`
    . Here is the piece of code where it is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The first piece declares where this configuration file will be mounted, while
    the second part links the defined resource: `webserver-test-config`. Therefore,
    the data defined inside the `ConfigMap` resource will be integrated inside the
    web server''s pod as `/etc/nginx/conf.d/default.conf` (take a look at the data
    block).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we also have a `Secret` resource (`webserver.secret.yaml`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can verify, here, that keys are visible while values are not (encoded using
    the Base64 algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create this secret using the imperative format with the `kubectl`
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl create secret generic webserver-secret \ --from-literal=PAGETITLE="Docker_Certified_DCA_Exam_Guide"
    \ --from-literal=PAGEBODY="Hello_World_from_Secret"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also used this secret resource in our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the `PAGETITLE` and `PAGEBODY` keys will be integrated as environment
    variables inside the web server's pod. These values will be used in our lab as
    values for the `index.html` page. `DEFAULT_BODY` and `DEFAULT_TITLE` will be changed
    from the pod's container process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This lab has another volume definition. In fact, we have `PersistentVolumeclaim`
    included as a volume in our deployment''s definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The volume claim is used here and is mounted in `/wwwroot` inside the web server's
    pod. `PersistentVolume` and `PersistentVolumeClaim` are defined in `webserver.persistevolume.yaml`
    and `webserver.persistevolumeclaim.yaml`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have an `emptyDir` volume definition. This will be used to bypass
    the container''s filesystem and save the NGINX logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The first pod execution will create a default `/wwwroot/index.html` file inside
    it. This is mounted inside the `kubernetes-node2` node's filesystem, inside the
    `/mount` directory. Therefore, after this first execution, we find that `/mnt/index.html`
    was created (you can verify this by following *step 1* again). The file was published,
    and we get it when we execute `curl 0.0.0.0:30080` in *step 5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our application is quite simple, but it is prepared to modify the content of
    the `index.html` file. As mentioned earlier, the default title and body will be
    changed with the values defined in the secret resource. This will happen after
    the creation of the container if the `index.html` file already exists. Now that
    it has been created, as verified in *step 10*, we can delete the web server''s
    pod. Kubernetes will create a new one, and, therefore, the application will change
    its content. We use `kubectl delete pod`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, a new pod is created (we are using a deployment and Kubernetes
    takes care of the application''s component resilience):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s again verify the content of our web server using `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Now the content has changed inside the defined `PersistentVolume` resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also verify the `/mnt/index.html` content in `kubernetes-node2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In this lab, we have used four different volume resources, with different definitions
    and features. These labs were very simple, showing you how to deploy a small application
    on Kubernetes. All of the labs can be easily removed by destroying all the Vagrant
    nodes using `vagrant destroy` from the `environments/kubernetes` directory.
  prefs: []
  type: TYPE_NORMAL
- en: We highly recommend going further with Kubernetes because it will become a part
    of the exam in the near future. However, right now, Kubernetes is outside the
    scope of the Docker Certified Associate exam.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we quickly reviewed some of Kubernetes' main features. We compared
    most of the must-have orchestration features with those discussed in [Chapter
    8](78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml), *Orchestration Using Docker Swarm*.
    Both provide workload deployment and the management of a distributed pool of nodes.
    They monitor an application's health and allow us to upgrade components without
    service interruption. They also provide networking and publishing solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Pods provide higher container density, allowing us to run more than one container
    at once. This concept is closer to applications running on virtual machines and
    makes container adoption easier. Services are logical groups of pods and we can
    use them to expose applications. Service discovery and load balancing work out
    of the box dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster-wide networking requires additional plugins in Kubernetes, and we also
    learned that a flat network can facilitate routing on different hosts and make
    some things easier; however, it does not provide security by default. Kubernetes
    provides enough mechanisms to ensure network security using network policies and
    single endpoints for multiple services with ingress. Publishing applications is
    even easier with ingress. It adds internal load balancing features dynamically
    with rules managed using ingress resources. This allows us to save up node ports
    and public IP addresses within the environment.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we reviewed a number of points about Kubernetes security.
    We discussed how RBAC provides different environments to users running their workloads
    on the same cluster. We also talked about some features provided by Kubernetes
    to ensure default security on resources.
  prefs: []
  type: TYPE_NORMAL
- en: There is much more to learn about Kubernetes, but we will have to end this chapter
    here. We highly recommend that you follow the Kubernetes documentation and the
    release notes on the project's website ([https://kubernetes.io/](https://kubernetes.io/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at the differences and similarities between
    Swarm and Kubernetes, side by side.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of these features is not included in Kubernetes by default?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) An internal key-value store.
  prefs: []
  type: TYPE_NORMAL
- en: b) Network communication between containers distributed on different Docker
    hosts.
  prefs: []
  type: TYPE_NORMAL
- en: c) Controllers for deploying workload updates without service interruptions.
  prefs: []
  type: TYPE_NORMAL
- en: d) None of these features are included.
  prefs: []
  type: TYPE_NORMAL
- en: Which of these statements is true about pods?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Pods always run in pairs to provide an application with high availability.
  prefs: []
  type: TYPE_NORMAL
- en: b) Pods are the minimum unit of deployment on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: c) We can deploy more than one container per pod.
  prefs: []
  type: TYPE_NORMAL
- en: d) We need to choose which containers in a pod should be replicated when pods
    are scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Which of these statements is true about pods?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) All pod containers run using a unique network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: b) All containers within a pod can share volumes.
  prefs: []
  type: TYPE_NORMAL
- en: c) All pods running on Docker Engine are accessible from the host using their
    IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of these statements are true.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides different controllers to deploy application workloads. Which
    of these statements is true?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) `DaemonSet` will run one replica on each cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: b) `ReplicaSet` will allow us to scale application pods up or down.
  prefs: []
  type: TYPE_NORMAL
- en: c) Deployments are higher-level resources. They manage `ReplicaSet`.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of these statements are true.
  prefs: []
  type: TYPE_NORMAL
- en: How can we expose services to users in Kubernetes? (Which of these statements
    is false?)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) ClusterIP services provide a virtual IP accessible to users.
  prefs: []
  type: TYPE_NORMAL
- en: b) NodePort services listen on all nodes and route traffic using the provided
    ClusterIP to reach all service backends.
  prefs: []
  type: TYPE_NORMAL
- en: c) LoadBalancer creates simple load balancers on cloud providers to load balance
    requests to service backends.
  prefs: []
  type: TYPE_NORMAL
- en: d) Ingress controllers help us to use single endpoints (one per ingress controller)
    to load balance requests to non-published services.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links for more information on topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes documentation: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes concepts: [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes learning tasks: [https://kubernetes.io/docs/tasks/](https://kubernetes.io/docs/tasks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes on Docker Enterprise: [https://docs.docker.com/ee/ucp/kubernetes/kube-resources](https://docs.docker.com/ee/ucp/kubernetes/kube-resources)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting Started with Kubernetes*: [https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
