<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-175"><a id="_idTextAnchor174"/>8</h1>
<h1 id="_idParaDest-176"><a id="_idTextAnchor175"/>Increasing Productivity with Docker Tips and Tricks</h1>
<p>This chapter introduces miscellaneous tips, tricks, and concepts that are useful when containerizing complex distributed applications or when using Docker to automate sophisticated tasks. You will also learn how to leverage containers to run your whole development environment in them. Here is the list of topics we are going to discuss:</p>
<ul>
<li>Keeping your Docker environment clean</li>
<li>Using a <code>.</code><code>dockerignore</code> file</li>
<li>Executing simple admin tasks in a container</li>
<li>Limiting the resource usage of a container</li>
<li>Avoiding running a container as <code>root</code></li>
<li>Running Docker from within Docker</li>
<li>Optimizing your build process</li>
<li>Scanning for vulnerabilities and secrets</li>
<li>Running your development environment in a container</li>
</ul>
<p>After reading this chapter, you will have learned how to do the following:</p>
<ul>
<li>Successfully restore your Docker environment after it has been messed up completely</li>
<li>Use a <code>.dockerignore</code> file to speed up builds, reduce image size, and enhance security</li>
<li>Run various tools to perform tasks on your computer without installing them</li>
<li>Limit the number of resources a containerized application uses during runtime</li>
<li>Harden your system by not running containers as root</li>
<li>Enable advanced scenarios by running Docker inside of a Docker container</li>
<li>Accelerate and improve the build process of your custom Docker images</li>
<li>Scan your Docker images for common vulnerabilities and exposures and the accidental inclusion of secrets</li>
<li>Run a whole development environment inside a container running locally or remotely</li>
</ul>
<p>Let’s get started!</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Technical requirements</h1>
<p>In this chapter, if you want to follow along with the code, you need Docker Desktop installed on your local machine as well as the Visual Studio Code editor.</p>
<p>Before we start, let’s create a folder for the samples that we will be using during this part of the book. Open a new terminal window and navigate to the folder you clone the sample code to. Usually, this is <code>~/The-Ultimate-Docker-Container-Book</code>:</p>
<pre class="source-code">
$ cd ~/The-Ultimate-Docker-Container-Book</pre> <p>Create a new subfolder for <a href="B19199_08.xhtml#_idTextAnchor174"><em class="italic">Chapter 8</em></a> called <code>ch08</code> and navigate to it:</p>
<pre class="source-code">
$ mkdir ch08 &amp;&amp; cd ch08</pre> <p>Now that you are ready, let’s start with the tips and tricks on how to keep our Docker environment clean.</p>
<p>You can find the sample code here: <a href="https://github.com/PacktPublishing/The-Ultimate-Docker-Container-Book/tree/main/sample-solutions/ch08">https://github.com/PacktPublishing/The-Ultimate-Docker-Container-Book/tree/main/sample-solutions/ch08</a>.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Keeping your Docker environment clean</h1>
<p>First, we want to learn how we can delete dangling images. A <a id="_idIndexMarker685"/>dangling Docker image is an unused image that has no association with any tagged images or <a id="_idIndexMarker686"/>containers. It usually occurs when a new image is built using the same tag as an existing image. Instead of removing the old image, Docker preserves it but removes the tag reference, leaving the image without a proper tag.</p>
<p>Dangling images<a id="_idIndexMarker687"/> are not referenced by any container or tagged image, and they consume disk space without providing any benefit. They can accumulate over time, especially in environments with frequent image builds and updates. Thus, it is better to remove them from time to time. Here is the command to do so:</p>
<pre class="source-code">
$ docker image prune -f</pre> <p>Please note that we have added the <code>-f</code> (or <code>--force</code>) parameter to the <code>prune</code> command. This is to prevent the CLI from asking you to confirm that you really want to delete those superfluous layers.</p>
<p>Stopped containers can waste precious resources too. If you’re sure that you don’t need these containers anymore, then you should remove them. You can remove them individually with the following command:</p>
<pre class="source-code">
$ docker container rm &lt;container-id|container-name&gt;</pre> <p>You can also remove them as a batch by using this command:</p>
<pre class="source-code">
$ docker container prune --force</pre> <p>In the command for removing them individually, <code>&lt;container-id|container-name&gt;</code> means that we can either use the container ID or its name to identify the container.</p>
<p>Unused Docker volumes can also quickly fill up disk space. It is good practice to tender your volumes, specifically in a development<a id="_idIndexMarker688"/> or <strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>) environment where you create a lot of mostly temporary volumes. But I have to warn you, Docker volumes are meant to store data. Often, this data must live longer than the life cycle of a container. This is specifically true in a production or production-like environment where the data is often mission-critical. Hence, be 100% sure of what you’re doing when using the following command to prune volumes on your Docker host:</p>
<pre class="source-code">
$ docker volume prune   WARNING! This will remove all local volumes not used by at least one container.
   Are you sure you want to continue? [y/N]</pre>
<p>We <a id="_idIndexMarker689"/>recommend using this command without the <code>-f</code> (or <code>--force</code>) flag. It is a dangerous and terminal operation and it’s better to give yourself a second chance to reconsider your action. Without the flag, the CLI outputs the warning you see in the preceding command. You have to explicitly confirm by typing <code>y</code> and pressing the <em class="italic">Enter</em> key.</p>
<p>On production or production-like systems, you should abstain from the preceding command and rather delete unwanted volumes one at a time by using this command:</p>
<pre class="source-code">
$ docker volume rm &lt;volume-name&gt;</pre> <p>I should also mention that there is a command to prune Docker networks. But since we have not yet officially introduced networks, I will defer this to <a href="B19199_10.xhtml#_idTextAnchor218"><em class="italic">Chapter 10</em></a>,<em class="italic"> </em><em class="italic">Using </em><em class="italic">Single-Host Networking</em>.</p>
<p>In the next section, we are going to show how we can exclude some folders and files from being included in the build context for a Docker image.</p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor178"/>Using a .dockerignore file</h1>
<p>The <code>.dockerignore</code> file is a text <a id="_idIndexMarker690"/>file that tells Docker to ignore certain files and directories when building a Docker image from a Dockerfile. This is similar to how the <code>.gitignore</code> file works in Git.</p>
<p>The primary <a id="_idIndexMarker691"/>benefit of using a <code>.dockerignore</code> file is that it can<a id="_idIndexMarker692"/> significantly speed up the Docker build process. When Docker builds an image, it first sends all of the files in the current directory (known as the “build context”) to the Docker daemon. If this directory contains large files or directories that aren’t necessary for building the Docker image (such as log files, local environment variables, cache files, etc.), these can be ignored to speed up the build process.</p>
<p>Moreover, using a <code>.dockerignore</code> file can help to improve security and maintain clean code practices. For instance, it helps prevent potentially sensitive information (such as <code>.env</code> files containing private keys) from being included in the Docker image. It can also help to keep <a id="_idIndexMarker693"/>the Docker image size minimal by avoiding unnecessary files, which is particularly beneficial when deploying the image or transferring it across networks.</p>
<p>Here’s<a id="_idIndexMarker694"/> an example of a .<code>dockerignore</code> file:</p>
<pre class="source-code">
# Ignore everything**
# Allow specific directories
!my-app/
!scripts/
# Ignore specific files within allowed directories
my-app/*.log
scripts/temp/</pre>
<p>In this example, all files are ignored except those in the <code>my-app/</code> and <code>scripts/</code> directories. However, log files within <code>my-app/</code> and all files in the <code>scripts/temp/</code> subdirectory are ignored. This level of granularity provides developers with fine control over what is included in the Docker build context.</p>
<p>In conclusion, the use of a <code>.dockerignore</code> file is a best practice for Docker builds, helping to speed up builds, reduce image size, and enhance security by excluding unnecessary or sensitive files from the build context. In the next section, we are going to show how to execute simple administrative tasks within a Docker container.</p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Executing simple admin tasks in a container</h1>
<p>In this section, we<a id="_idIndexMarker695"/> want to provide a few examples of tasks you may want to run in a container instead of natively on your computer.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>Running a Perl script</h2>
<p>Let’s assume <a id="_idIndexMarker696"/>you need to strip all leading whitespaces from a file and you found the following handy Perl script to do exactly that:</p>
<pre class="source-code">
$ cat sample.txt | perl -lpe 's/^\s*//'</pre> <p>As it turns out, you<a id="_idIndexMarker697"/> don’t have Perl installed on your working machine. What can you do? Install Perl on the machine? Well, that would certainly be an option, and it’s exactly what most developers or system admins do. But wait a second, you already have Docker installed on your machine. Can’t we use Docker to circumvent the need to install Perl? And can’t we do this on any operating system supporting Docker? Yes, we can. This is how we’re going to do it:</p>
<ol>
<li>Navigate to the chapter’s code folder:<pre class="source-code">
$ cd ~/The-Ultimate-Docker-Container-Book/ch08</pre></li> <li>Create a new subfolder called <code>simple-task</code>, and navigate to it:<pre class="source-code">
$ mkdir simple-task &amp;&amp; cd simple-task</pre></li> <li>Open VS Code from within this folder:<pre class="source-code">
$ code .</pre></li> <li>In this folder, create a <code>sample.txt</code> file with the following content:<pre class="source-code">
           1234567890              This is some text               another line of text         more textfinal line</pre></li> </ol>
<p>Please note the whitespaces at the beginning of each line. Save the file.</p>
<ol>
<li value="5">Now, we can run a container with Perl installed in it. Thankfully, there is an official Perl image on Docker Hub. We are going to use the slim version of the image. The primary difference between the normal Perl Docker image and the slim version lies in their size and the components included in the images. Both images provide the Perl runtime environment, but they are optimized for different use cases:<pre class="source-code">
$ docker container run --rm -it \    -v $(pwd):/usr/src/app \    -w /usr/src/app \    perl:slim sh -c "cat sample.txt | perl -lpe 's/^\s*//'"</pre></li> </ol>
<p>The <a id="_idIndexMarker698"/>preceding command runs a Perl container (<code>perl:slim</code>) interactively, maps the content of the current folder into the <code>/usr/src/app</code> folder of the container, and sets the working folder inside the container to <code>/usr/src/app</code>. The command that is run inside the container is as follows:</p>
<pre class="source-code">
sh -c "cat sample.txt | perl -lpe 's/^\s*//'"</pre> <p>It basically spawns a Bourne shell and executes our desired Perl command.</p>
<ol>
<li value="6">Analyze <a id="_idIndexMarker699"/>the output generated by the preceding command. It should look like this:<pre class="source-code">
1234567890This is some textanother line of textmore textfinal line</pre></li> </ol>
<p>That is, all trailing blanks have been removed.</p>
<p>Without needing to install Perl on our machine, we were able to achieve our goal. The nice thing is that, after the script has run, the container is removed from your system without leaving any traces because we used the <code>--rm</code> flag in the <code>docker container run</code> command, which automatically removes a stopped container.</p>
<p class="callout-heading">Tip</p>
<p class="callout">If that doesn’t convince you yet because, if you’re on macOS, you already have Perl installed, then consider you’re looking into running a Perl script named <code>your-old-perl-script.pl</code> that is old and not compatible with the newest release of Perl that you happen to have installed on your system. Do you try to install multiple versions of Perl on your machine and potentially break something? No, you just run a container with the (old) version of Perl that is compatible with your script, as in this example:</p>
<p class="callout"><code>$ docker container run -it --</code><code>rm \</code></p>
<p class="callout"><code>    -v $(</code><code>pwd):/usr/src/app \</code></p>
<p class="callout"><code>    -w /</code><code>usr/src/app \</code></p>
<p class="callout"><code>    perl:&lt;old-version&gt; </code><code>perl your-old-perl-script.p</code>l</p>
<p class="callout">Here, <code>&lt;old-version&gt;</code> corresponds to the tag of the version of Perl that you need to run your script.</p>
<p>In the <a id="_idIndexMarker700"/>next<a id="_idIndexMarker701"/> section, we are going to demonstrate how to run a Python script.</p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>Running a Python script</h2>
<p>A lot of <a id="_idIndexMarker702"/>people<a id="_idIndexMarker703"/> use quick and dirty Python scripts or mini apps to automate tasks that are not easily coded with, say, Bash. Now, if the Python script has been written in Python 3.x and you only happen to have Python 2.7 installed or no version at all on your machine, then the easiest solution is to execute the script inside a container. Let’s assume a simple example where the Python script counts lines, words, and letters in a given file and outputs the result to the console:</p>
<ol>
<li>Still in the <code>simple-task</code> folder, add a <code>stats.py</code> file and add the following content:</li>
</ol>
<div><div><img alt="Figure 8.1 – Python script to calculate statistics of a sample text" height="790" src="img/B19199_08_01.jpg" width="691"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Python script to calculate statistics of a sample text</p>
<ol>
<li value="2">After<a id="_idIndexMarker704"/> saving the file, you can run it with the<a id="_idIndexMarker705"/> following command:<pre class="source-code">
$ docker container run --rm -it \    -v $(pwd):/usr/src/app \    -w /usr/src/app \     python:3-alpine python stats.py sample.txt</pre></li> <li>Note that, in this example, we are reusing the <code>sample.txt</code> file from the previous <em class="italic">Running a Perl script</em> section. The output in my case is as follows:<pre class="source-code">
Lines: 5Words: 13Letters: 121</pre></li> </ol>
<p>The <a id="_idIndexMarker706"/>beauty of this approach is that the Perl script before<a id="_idIndexMarker707"/> and this last Python script will now run on any computer with any OS installed, as long as the machine is a Docker host and hence, can run containers.</p>
<p>Next, we are going to learn how to limit the number of resources a container running on the system can consume.</p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Limiting the resource usage of a container</h1>
<p>One of the <a id="_idIndexMarker708"/>great features of a container, apart from encapsulating application processes, is the possibility of limiting the resources a single container can consume at most. This includes CPU and memory consumption. Let’s have a look at how limiting the amount of memory (RAM) works:</p>
<pre class="source-code">
$ docker container run --rm -it \       --name stress-test \
       --memory 512M \
       ubuntu:22.04 /bin/bash</pre>
<p>Once inside the container, install the <code>stress</code> tool, which we will use to simulate memory pressure:</p>
<pre class="source-code">
/# apt-get update &amp;&amp; apt-get install -y stress</pre> <p>Open another terminal window and execute the <code>docker stats</code> command to observe the resource consumption of all running Docker containers. You should see something like this:</p>
<div><div><img alt="Figure 8.2 – The Docker stats showing a resource-limited container" height="73" src="img/B19199_08_02.jpg" width="1081"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The Docker stats showing a resource-limited container</p>
<p>Look at <code>MEM USAGE</code> and <code>LIMIT</code>. Currently, the container uses only <code>36.57MiB</code> memory and has a limit of <code>512MiB</code>. The latter corresponds to what we have configured for this container. Now, let’s use the <code>stress</code> tool to simulate three workers, which will allocate memory using the <code>malloc()</code> function in blocks of <code>256MiB</code>. Run this command inside the container to do so:</p>
<pre class="source-code">
/# stress -m 3</pre> <p>The <a id="_idIndexMarker709"/>preceding command puts stress on the system’s memory by creating three child processes that will <code>malloc()</code> and touch memory until the system runs out of memory. In the terminal running Docker stats, observe how the value for <code>MEM USAGE</code> approaches but never exceeds <code>LIMIT</code>. This is exactly the behavior we expected from Docker. Docker<a id="_idIndexMarker710"/> uses Linux <strong class="bold">cgroups</strong> to enforce those limits.</p>
<p class="callout-heading">What are cgroups?</p>
<p class="callout">Linux <strong class="bold">cgroups</strong>, short for <strong class="bold">control groups</strong>, is a<a id="_idIndexMarker711"/> kernel-level feature that allows you to organize processes into hierarchical groups, and to allocate, restrict, and monitor system resources such as CPU, memory, disk I/O, and network among these groups. Cgroups provide a way to manage and limit the resource usage of processes, ensuring fair distribution and preventing individual processes from monopolizing system resources.</p>
<p>We could similarly limit the amount of CPU a container can consume with the <code>--</code><code>cpu</code> switch.</p>
<p>With this operation, engineers can avoid the noisy neighbor problem on a busy Docker host, where a single container starves all of the others by consuming an excessive amount of resources.</p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Avoiding running a container as root</h1>
<p>Most <a id="_idIndexMarker712"/>applications or application services that run inside a container do not need <code>root</code> access. To tighten security, it is helpful in those scenarios to run these processes with minimal necessary privileges. These applications should not be run as <code>root</code> nor assume that they have <code>root</code>-level privileges.</p>
<p>Once again, let’s illustrate what we mean with an example. Assume we have a file with top-secret content. We want to secure this file on our Unix-based system using the <code>chmod</code> tool so that only users with <code>root</code> permissions can access it. Let’s assume I am logged in as <code>demo</code> on the dev host and hence my prompt is <code>demo@dev $</code>. I can use <code>sudo su</code> to impersonate a superuser. I have to enter the superuser password though:</p>
<pre class="source-code">
demo@dev $ sudo su   Password: &lt;root password&gt;
   root@dev $</pre>
<p>Now, as the <code>root</code> user, I can create this file called <code>top-secret.txt</code> and secure it:</p>
<pre class="source-code">
root@dev $ echo "You should not see this." &gt; top-secret.txtroot@dev $ chmod 600 ./top-secret.txt
root@dev $ exit
demo@dev $</pre>
<p>If I try to access the file as user <code>demo</code>, the following happens:</p>
<pre class="source-code">
cat: ./top-secret.txt: Permission denied</pre> <p>I get a <code>Permission denied</code> message, which is what we wanted. No other user except <code>root</code> can access this file. Now, let’s build a Docker image that contains this secured file and when a container is created from it, tries to output the content of the <code>secrets</code> file. The Dockerfile could look like this:</p>
<pre class="source-code">
FROM ubuntu:22.04COPY ./top-secret.txt /secrets/
# simulate use of restricted file
CMD cat /secrets/top-secret.txt</pre>
<p>We can build an image from that Dockerfile (as <code>root</code>!) with the following:</p>
<pre class="source-code">
demo@dev $ sudo suPassword: &lt;root password&gt;
root@dev $ docker image build -t demo-image .
root@dev $ exit
demo@dev $</pre>
<p>Then, by<a id="_idIndexMarker713"/> running a container with the image built in the previous step, we get the following:</p>
<pre class="source-code">
demo@dev $ docker container run demo-image</pre> <p>The preceding command will generate this output:</p>
<pre class="source-code">
You should not see this.</pre> <p>OK, so although I am impersonating the <code>demo</code> user on the host and running the container under this user account, the application running inside the container automatically runs as <code>root</code>, and hence has full access to protected resources. That’s bad, so let’s fix it! Instead of running with the default, we define an explicit user inside the container. The modified Dockerfile looks like this:</p>
<pre class="source-code">
FROM ubuntu:22.04RUN groupadd -g 3000 demo-group |
       &amp;&amp; useradd -r -u 4000 -g demo-group demo-user
USER demo-user
COPY ./top-secret.txt /secrets/
# simulate use of restricted file
CMD cat /secrets/top-secret.txt</pre>
<p>We use the <code>groupadd</code> tool to define a new group, <code>demo-group</code>, with the ID <code>3000</code>. Then, we use the <code>useradd</code> tool to add a new user, <code>demo-user</code>, to this group. The user has the ID <code>4000</code> inside the container. Finally, with the <code>USER demo-user</code> statement, we declare that all subsequent operations should be executed as <code>demo-user</code>.</p>
<p>Rebuild the image—again, as <code>root</code>—and then try to run a container from it:</p>
<pre class="source-code">
demo@dev $ sudo suPassword: &lt;root password&gt;
root@dev $ docker image build -t demo-image .
root@dev $ exit
demo@dev $ docker container run demo-image \
     cat: /secrets/top-secret.txt:
Permission denied</pre>
<p>And as you <a id="_idIndexMarker714"/>can see on the last line, the application running inside the container runs with restricted permissions and cannot access resources that need root-level access. By the way, what do you think would happen if I ran the container as <code>root</code>? Try it out!</p>
<p>In the next section, we are going to show how we can automate Docker from within a container.</p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor184"/>Running Docker from within Docker</h1>
<p>At times, we<a id="_idIndexMarker715"/> may want to run a container hosting an application that automates certain Docker tasks. How can we do that? Docker Engine and the Docker CLI are installed on the host, yet the application runs inside the container. Well, from early on, Docker has provided a means to bind-mount Linux sockets from the host into the container. On Linux, sockets are used as very efficient data communications endpoints between processes that run on the same host. The <a id="_idIndexMarker716"/>Docker CLI uses a<a id="_idIndexMarker717"/> socket to communicate with Docker Engine; it is often called the <strong class="bold">Docker socket</strong>. If we can give access to the Docker socket to an application running inside a container, then we can just install the Docker CLI inside this container, and we will then be able to run an application in the same container that uses this locally installed Docker CLI to automate container-specific tasks.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Here, we are not talking about running Docker Engine inside the container but rather only the Docker CLI and bind-mounting the Docker socket from the host into the container so that the CLI can communicate with Docker Engine running on the host computer. This is an important distinction.</p>
<p class="callout">Running <a id="_idIndexMarker718"/>Docker Engine inside a container is generally not recommended due to several reasons, including security, stability, and potential performance issues. This practice is often <a id="_idIndexMarker719"/>referred to as <strong class="bold">Docker-in-Docker</strong> or <strong class="bold">DinD</strong>. The main concerns are as follows:</p>
<ul>
<li class="callout"><strong class="bold">Security</strong>: Running <a id="_idIndexMarker720"/>Docker Engine inside a container requires elevated privileges, such as running the container in privileged mode or mounting the Docker socket. This can expose the host system to potential security risks, as a compromised container could gain control over the host’s Docker daemon and escalate privileges, affecting other containers and the host itself.</li>
<li class="callout"><strong class="bold">Stability</strong>: Containers <a id="_idIndexMarker721"/>are designed to be isolated, lightweight, and ephemeral. Running Docker Engine inside a container can create complex dependencies and increase the chances of conflicts or failures, particularly when managing storage, networking, and process namespaces between the host and the nested container environment.</li>
<li class="callout"><strong class="bold">Performance</strong>: Running <a id="_idIndexMarker722"/>Docker Engine inside a container can introduce performance overhead, as it adds another layer of virtualization, particularly in terms of storage and networking. This can lead to increased latency and reduced throughput, particularly when managing large numbers of containers or when working with high-performance applications.</li>
<li class="callout"><strong class="bold">Resource management</strong>: Docker-in-Docker can make it challenging to manage<a id="_idIndexMarker723"/> and allocate resources effectively, as nested containers may not inherit resource limits and restrictions from their parent container, leading to potential resource contention or over-commitment on the host.</li>
</ul>
<p>To illustrate the concept, let’s look at an example using the preceding technique. We are going to use a copy of the <code>library</code> component we built in the previous chapter (<a href="B19199_07.xhtml#_idTextAnchor150"><em class="italic">Chapter 7</em></a>) for this:</p>
<ol>
<li>Navigate to the chapter folder:<pre class="source-code">
$ cd ~/The-Ultimate-Docker-Container-Book/ch08</pre></li> <li>Copy the <code>library</code> component from the <code>ch07</code> directory to this folder:<pre class="source-code">
$ cp -r ../ch07/library .</pre></li> <li>Open the component in VS Code:<pre class="source-code">
$ code library</pre></li> <li>Add a<a id="_idIndexMarker724"/> new file called <code>pipeline.sh</code> to the root of the project and add the following code to it, which automates the building, testing, and pushing of a Docker image:</li>
</ol>
<div><div><img alt="Figure 8.3 – Script to build, test, and push a Java application" height="507" src="img/B19199_08_03.jpg" width="878"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Script to build, test, and push a Java application</p>
<p>Note that we’re using four environment variables: <code>$HUB_USER</code> and <code>$HUB_PWD</code> being the credentials for Docker Hub, and <code>$REPOSITORY</code> and <code>$TAG</code> being the name and tag of the Docker image we want to build. Eventually, we will have to pass values for those environment variables in the <code>docker container run</code> command, so that they are available for any process running inside the container.</p>
<ol>
<li value="5">Save the file and make it an executable:<pre class="source-code">
$ chmod +x ./pipeline.sh</pre></li> </ol>
<p>We want<a id="_idIndexMarker725"/> to run the <code>pipeline.sh</code> script inside a builder container. Since the script uses the Docker CLI, our builder container must have the Docker CLI installed, and to access Docker Engine, the builder container must have the Docker socket bind-mounted.</p>
<p>Let’s start creating a Docker image for such a builder container:</p>
<ol>
<li>Add a file called <code>Dockerfile.builder</code> to the root of the project and add the following content to it:</li>
</ol>
<div><div><img alt="Figure 8.4 – Dockerfile for the builder" height="601" src="img/B19199_08_04.jpg" width="945"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Dockerfile for the builder</p>
<p>Note the long <code>RUN</code> command on line 3 onward. This is needed to install Docker in the container. For more details about this command, you may want to consult the Docker online documentation here: <a href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a>.</p>
<ol>
<li value="2">Building a Docker image with this Dockerfile is straightforward:<pre class="source-code">
$ docker image build -f Dockerfile.builder -t builder .</pre></li> <li>We are <a id="_idIndexMarker726"/>now ready to try the <code>builder</code> command with a real Java application; for example, let’s take the sample app we defined in the <code>ch08/library</code> folder. Make sure you replace <code>&lt;user&gt;</code> and <code>&lt;password&gt;</code> with your own credentials for Docker Hub:</li>
</ol>
<div><div><img alt="Figure 8.5 – Docker run command for the builder" height="395" src="img/B19199_08_05.jpg" width="793"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Docker run command for the builder</p>
<p>Notice how, in the preceding command, we mounted the Docker socket into the container with <code>-v /var/run/docker.sock:/var/run/docker.sock</code>. If everything goes well, you should have a container image built for the sample application, the test should have been run, and the image should have been pushed to Docker Hub. This is only one of the many use cases where it is very useful to be able to bind-mount the Docker socket.</p>
<p>A special <a id="_idIndexMarker727"/>notice to those of you who want to try Windows containers on a Windows computer: on Docker Desktop for Windows, you can create a similar environment by bind-mounting <a id="_idIndexMarker728"/>Docker’s <strong class="bold">named pipe</strong> instead of a socket. A named pipe on Windows is roughly the same as a socket on a Unix- based system. Assuming you’re using a PowerShell terminal, the command to bind-mount a named pipe when running a Windows container hosting Jenkins looks like this:</p>
<pre class="source-code">
PS&gt; docker container run `       --name jenkins `
       -p 8080:8080 `
       -v \\.\pipe\docker_engine:\\.\pipe\docker_engine `
       friism/jenkins</pre>
<p>Note the special syntax, <code>\\.\pipe\docker_engine</code>, to access Docker’s named pipe.</p>
<p>In this section, we <a id="_idIndexMarker729"/>have shown how to run Docker from within Docker by mounting the Docker socket into the respective container.</p>
<p>Next, we are going to revisit the topic of how to make your Docker build as fast as possible to reduce friction in the development cycle.</p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor185"/>Optimizing your build process</h1>
<p>The <a id="_idIndexMarker730"/>Docker build process can and should be optimized. This will remove a lot of friction in the software development life cycle.</p>
<p>Many Docker beginners make the following mistake when crafting their first Dockerfile:</p>
<div><div><img alt="Figure 8.6 – Unoptimized Dockerfile for a Node.js application" height="251" src="img/B19199_08_06.jpg" width="360"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Unoptimized Dockerfile for a Node.js application</p>
<p>Can you spot the weak point in this typical Dockerfile for a Node.js application? In<em class="italic"> </em><a href="B19199_04.xhtml#_idTextAnchor083"><em class="italic">Chapter 4</em></a>, <em class="italic">Creating and Managing Container Images</em>, we learned that an image consists of a series of layers. Each (logical) line in a Dockerfile creates a layer, except the lines with the <code>CMD</code> and/or <code>ENTRYPOINT</code> keywords. We also learned that the Docker builder tries to do its best by caching layers and reusing them if they have not changed between subsequent builds. But the caching only uses cached layers that occur before the first changed layer. All subsequent layers need to be rebuilt. That said, the preceding structure of the Dockerfile invalidates – or as we often hear said – <em class="italic">busts</em> the image layer cache!</p>
<p>Why? Well, from<a id="_idIndexMarker731"/> experience, you certainly know that the <code>npm install</code> command can be a pretty expensive operation in a typical Node.js application with many external dependencies. The execution of this command can take from seconds to many minutes. That said, each time one of the source files changes, and we know that happens frequently during development, line 3 in the Dockerfile causes the corresponding image layer to change. Hence, the Docker builder cannot reuse this layer from the cache, nor can it reuse the subsequent layer created by <code>RUN npm install</code>. Any minor change in code causes a complete rerun of <code>npm install</code>. That can be avoided. The <code>package.json</code> file containing the list of external dependencies rarely changes. With all of that information, let’s fix the Dockerfile:</p>
<div><div><img alt="Figure 8.7 – Optimized Dockerfile for a Node.js application" height="285" src="img/B19199_08_07.jpg" width="365"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Optimized Dockerfile for a Node.js application</p>
<p>This time, on line 3, we only copy the <code>package.json</code> file into the container, which rarely changes. Hence, the subsequent <code>npm install</code> command has to be executed equally rarely. The <code>COPY</code> command on line 5 is then a very fast operation and hence rebuilding an <a id="_idIndexMarker732"/>image after some code has changed only needs to rebuild this last layer. Build times reduce to merely a fraction of a second.</p>
<p>The very same principle applies to most languages or frameworks, such as Python, .NET, or Java. Avoid busting your image layer cache!</p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor186"/>Scanning for vulnerabilities and secrets</h1>
<p>What exactly <a id="_idIndexMarker733"/>are vulnerabilities, or to be more accurate, <strong class="bold">Common Vulnerabilities and </strong><strong class="bold">Exposures</strong> (<strong class="bold">CVE</strong>)?</p>
<p>A <a id="_idIndexMarker734"/>database of information security problems that have been made publicly known is called <strong class="bold">Common Vulnerabilities and Exposures</strong>. A number uniquely identifies each vulnerability from the list of all other entries in the database. This list is continuously reviewed and updated by experts who include any new vulnerabilities or exposures as soon as they are found.</p>
<p>Now, we can scan the various layers of our Docker images using specialist software, such as Snyk, to find software libraries that are known to have such CVE. If we find that our image is flawed, we should and can repair the issue by switching to a more recent version of the flawed library. The image will then need to be rebuilt.</p>
<p>But our work is not yet done. Security experts frequently find new CVE, as was already mentioned previously. As a result, a software library that was previously secure may suddenly be vulnerable as a result of newly revealed CVE.</p>
<p>This means that we must ensure that all of our active Docker images are routinely inspected, notify our developers and security experts about the issue, and take other steps to ensure a speedy resolution of the issue.</p>
<p>There are a few ways <a id="_idIndexMarker735"/>to scan a Docker image for vulnerabilities and secrets:</p>
<ul>
<li>Use a vulnerability scanner such as Clair, Anchore, or Trivy. These tools can scan a Docker image and check it against a database of known vulnerabilities in order to identify any potential security risks.</li>
<li>Use a tool such as Aquasec or Sysdig to scan the image for secrets. These tools can detect and alert on sensitive information such as private keys, passwords, and other sensitive data that may have been accidentally committed to the image.</li>
<li>Use a <a id="_idIndexMarker736"/>combination of both tools, for example, Docker Bench for Security, which checks for dozens of common best practices around deploying Docker containers in production.</li>
<li>Use a tool such as OpenSCAP, which can perform vulnerability scans, security configuration assessments, and compliance checks on a Docker image.</li>
</ul>
<p>It’s important to<a id="_idIndexMarker737"/> note that it’s always good practice to keep your images updated and only use official and trusted images.</p>
<p>In the next section, we will investigate how we can discover vulnerabilities inside our Docker images.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/>Using Snyk to scan a Docker image</h2>
<p>Snyk is a <a id="_idIndexMarker738"/>security <a id="_idIndexMarker739"/>platform that can be used to scan Docker images for vulnerabilities. Here is an example <a id="_idIndexMarker740"/>of how to use Snyk to scan a Docker image for vulnerabilities:</p>
<ol>
<li>First, we have to install the Snyk CLI on our machine. We can do this by running the following command:<pre class="source-code">
$ npm install -g snyk</pre></li> <li>Once Snyk is installed, we can authenticate with our Snyk account by running the following command and following the prompts:<pre class="source-code">
$ snyk auth</pre></li> <li>Next, we can run the following command to scan a specific Docker image for vulnerabilities:<pre class="source-code">
$ snyk test --docker &lt;image-name&gt;</pre></li> </ol>
<p>The preceding command will perform a vulnerability scan on the specified Docker image and print the results in the console. The results will show the number of vulnerabilities found, the severity of each vulnerability, and the package and version that is affected.</p>
<ol>
<li value="4">We<a id="_idIndexMarker741"/> can also use the <code>--file</code> flag to scan a Dockerfile instead of a built image:<pre class="source-code">
$ snyk test --file=path/to/Dockerfile</pre></li> <li>Additionally, we <a id="_idIndexMarker742"/>can also use the <code>--org</code> flag to specify an organization, if we’re a member of multiple organizations:<pre class="source-code">
$ snyk test --docker &lt;image-name&gt; --org=my-org</pre></li> <li>Finally, we can use the <code>--fix</code> flag to automatically fix the vulnerabilities found by running the following command:<pre class="source-code">
$ snyk protect --docker &lt;image-name&gt;</pre></li> </ol>
<p>Please note that this feature is only available for images that are built using a Dockerfile and it will update the Dockerfile with the new package versions, and you will need to rebuild the image to take advantage of the fix.</p>
<p class="callout-heading">Note</p>
<p class="callout">The Snyk free plan is limited to a certain number of scans, and it does not include the <em class="italic">Protect</em> feature. You will have to upgrade to a paid plan to have access to this feature.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor188"/>Using docker scan to scan a Docker image for vulnerabilities</h2>
<p>In this <a id="_idIndexMarker743"/>section, we are <a id="_idIndexMarker744"/>once again going to use Snyk to scan a Docker image for vulnerabilities. Snyk should be included with your Docker Desktop installation:</p>
<ol>
<li>Check by using this command:<pre class="source-code">
$ docker scan --version</pre></li> </ol>
<p>The output should look similar to this:</p>
<pre class="source-code">
Version:    v0.22.0Git commit: af9ca12
Provider:   Snyk (1.1054.0)</pre>
<ol>
<li value="2">Let’s<a id="_idIndexMarker745"/> try to scan <a id="_idIndexMarker746"/>a sample <code>whoami</code> application from the author’s Docker Hub account. First, make sure you have the <code>whoami</code> image in your local cache:<pre class="source-code">
$ docker image pull gnschenker/whoami:1.0</pre></li> <li>Scan the image for vulnerabilities:<pre class="source-code">
$ docker scan gnschenker/whoami:1.0</pre></li> <li>You will be asked the following:<pre class="source-code">
Docker Scan relies upon access to Snyk, a third party provider, do you consent to proceed using Snyk? (y/N)</pre></li> </ol>
<p>Please answer this with <code>y</code>.</p>
<p>The result of the preceding scan looks like this on my computer:</p>
<div><div><img alt="Figure 8.8 – Scanning the gnschenker/whoami:1.0 Docker image" height="835" src="img/B19199_08_08.jpg" width="1149"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Scanning the gnschenker/whoami:1.0 Docker image</p>
<p>As you <a id="_idIndexMarker747"/>can see, there <a id="_idIndexMarker748"/>were three vulnerabilities found in this version of the image: one of <em class="italic">medium</em>, one of <em class="italic">high</em>, and one of <em class="italic">critical</em> severity. It is clear that we should address critical vulnerabilities as soon as possible. Let’s do this now:</p>
<ol>
<li>First, we copy over the original <code>whoami</code> project including the Dockerfile we used to build this image. You can find the copy in your <code>~/</code><code>The-Ultimate-Docker-Container-Book/sample-solutions/</code><code>ch14</code> folder.</li>
<li>Open the Dockerfile and inspect it. We used version <code>6.0-alpine</code> for both the .NET SDK and the runtime. Let’s see whether Microsoft has updated the vulnerabilities in this version already.</li>
<li>Navigate to your <code>…/</code><code>ch08/whoami</code> folder.</li>
<li>Build a new version of the Docker image with this command:<pre class="source-code">
$ docker image build -t gnschenker/whoami:1.0.1 .</pre></li> </ol>
<p>Note, you may want to replace <code>gnschenker</code> with your own Docker account name.</p>
<ol>
<li value="5">Scan<a id="_idIndexMarker749"/> the new image:<pre class="source-code">
$ docker scan gnschenker/whoami:1.0.1</pre></li> </ol>
<p>This<a id="_idIndexMarker750"/> time, the output should look like this:</p>
<div><div><img alt="Figure 8.9 – Scanning the rebuilt whoami Docker image" height="262" src="img/B19199_08_09.jpg" width="955"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Scanning the rebuilt whoami Docker image</p>
<p>As you can see, this time, the image is free from any vulnerabilities. We should now instruct our DevOps to use this new version of the image. We can use a rolling update in production and should be just fine, as the application itself did not change.</p>
<p>In the next section, we are going to learn how to run a complete development environment inside a container.</p>
<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/>Running your development environment in a container</h1>
<p>Imagine <a id="_idIndexMarker751"/>that you only have access to a <a id="_idIndexMarker752"/>workstation with Docker Desktop installed, but no possibility to add or change anything else on this workstation. Now you want to do some proofs of concept and code some sample applications using Java. Unfortunately, Java and SpringBoot are not installed on your computer. What can you do? What if you could run a whole development environment inside a container, including a code editor and debugger? What if, at the same time, you could still have your code files on your host machine?</p>
<p>Containers<a id="_idIndexMarker753"/> are awesome, and genius engineers <a id="_idIndexMarker754"/>have come up with solutions for exactly this kind of problem.</p>
<p class="callout-heading">Note</p>
<p class="callout">Microsoft and the community are continuously updating VS Code and the plugins. Thus your version of VS Code may be newer than the one used during the writing of this book. As such, expect a slightly different experience. Refer to the official documentation for more details on how to work <a id="_idIndexMarker755"/>with Dev containers: <a href="https://code.visualstudio.com/docs/devcontainers/containers">https://code.visualstudio.com/docs/devcontainers/containers</a>.</p>
<p>We will be using Visual Studio Code, our favorite code editor, to show how to run a complete Java development environment inside a container:</p>
<ol>
<li>But first, we need to install the necessary VS Code extension. Open VS Code and install the extension called <strong class="bold">Remote Development</strong>:</li>
</ol>
<div><div><img alt="Figure 8.10 – Adding the Remote Development extension to VS Code" height="287" src="img/B19199_08_10.jpg" width="550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Adding the Remote Development extension to VS Code</p>
<ol>
<li value="2">Then, click<a id="_idIndexMarker756"/> the green quick <a id="_idIndexMarker757"/>actions status bar item in the lower-left of the Visual Studio Code window. In the popup, select <strong class="bold">Remote-Containers</strong> | <strong class="bold">Open Folder </strong><strong class="bold">in Container...</strong>:</li>
</ol>
<div><div><img alt="Figure 8.11 – Open Folder in Container" height="656" src="img/B19199_08_11.jpg" width="811"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Open Folder in Container</p>
<ol>
<li value="3">Select <a id="_idIndexMarker758"/>the project folder you<a id="_idIndexMarker759"/> want to work with in the container. In our case, we selected the <code>~/</code><code>The-Ultimate-Docker-Container-Book/ch08/library</code> folder.</li>
<li>A popup will appear asking you to define how you want to create the development container. From the list, select <strong class="bold">From ‘Dockerfile’</strong>:</li>
</ol>
<div><div><img alt="Figure 8.12 – Selecting the method to create the development container" height="344" src="img/B19199_08_12.jpg" width="852"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Selecting the method to create the development container</p>
<ol>
<li value="5">When<a id="_idIndexMarker760"/> asked to add additional features to install, just click <strong class="bold">OK</strong> to continue. At this time, we do not need anything special.</li>
</ol>
<p>VS Code will now start preparing the environment, which, the very first time, can take a couple of minutes or so.</p>
<ol>
<li value="6">Once the <a id="_idIndexMarker761"/>environment is ready, you should notice that in the lower-left corner, the prompt has changed to the following:<pre class="source-code">
Dev Container: Existing Dockerfile @ &lt;folder-path&gt;</pre></li> </ol>
<p>This indicates that VS Code has indeed run a container based on the Dockerfile found in the library folder and is allowing you to work within it.</p>
<ol>
<li value="7">You will be asked to install the extension pack for Java since VS Code has recognized that this is a Java project. Click <code>dev</code> container and only the UI is still running on your laptop. Thus, the extension pack will be installed for the engine inside the container. You will notice this when you open the <strong class="bold">EXTENSIONS</strong> panel and find a list of remote extensions under <strong class="bold">DEV CONTAINER</strong>. In our case, by installing the Java extensions pack, we now have the following eight remote extensions installed:</li>
</ol>
<div><div><img alt="Figure 8.13 – Remote extensions installed on the dev container" height="892" src="img/B19199_08_13.jpg" width="566"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Remote extensions installed on the dev container</p>
<ol>
<li value="8">Open a <a id="_idIndexMarker762"/>Terminal inside VS Code with <em class="italic">Shift</em> + <em class="italic">Ctrl</em> + <em class="italic">‘</em> and notice the prompt revealing that the terminal session<a id="_idIndexMarker763"/> is ins<a href="mailto:root@c96b82891be7:/workspaces/.../ch08/library">ide the <code>dev</code> container and that we are <em class="italic">not</em> runni</a>ng directly on our Docker host:<pre class="source-code">
root@c96b82891be7:/workspaces/.../ch08/library#</pre></li> </ol>
<p>Note that for readability, we have shortened the preceding prompt.</p>
<ol>
<li value="9">Now, try to run the Java application by locating the <code>main</code> method in the <code>LibraryApplication</code> class and clicking the <strong class="bold">Run</strong> link just above the method. The application should start as normal, but notice that our context is inside the dev container and not directly on our working machine.</li>
</ol>
<p>Alternatively, we <a id="_idIndexMarker764"/>could have started<a id="_idIndexMarker765"/> the application from the command line with this command:</p>
<pre class="source-code">
$ ./mvnw spring-boot:run</pre> <ol>
<li value="10">Now, add a file called <code>DefaultController.java</code> to the <code>controllers</code> folder and give it this content:</li>
</ol>
<div><div><img alt="Figure 8.14 – Adding a default controller while working inside the dev container" height="490" src="img/B19199_08_14.jpg" width="893"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Adding a default controller whil<a href="http://localhost:8080">e working inside the </a>dev container</p>
<ol>
<li value="11">Restart the application and open a browser as <code>http://localhost:8080</code>. The message <code>Library component</code> should be displayed as expected.</li>
<li>When done experimenting, click on the green area in the lower-left corner of VS Code and select <strong class="bold">Open folder locally</strong> from the pop-up menu to quit the dev container and open the project locally.</li>
<li>Observe <a id="_idIndexMarker766"/>that a new folder, <code>.devcontainer</code>, has been added to the project containing a <code>devcontainer.json</code> file. This file contains the configuration needed <a id="_idIndexMarker767"/>to run a dev container from this project. Please read the documentation of VS Code to familiarize yourself with the possibilities this file offers to you.</li>
</ol>
<p>These have been a few tips and tricks for pros that are useful in the day-to-day usage of containers. There are many more. Google them. It is worth it.</p>
<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/>Summary</h1>
<p>In this chapter, we presented miscellaneous tips, tricks, and concepts that are useful when containerizing complex distributed applications or when using Docker to automate sophisticated tasks. We also learned how to leverage containers to run a whole development environment inside of them.</p>
<p>In the next chapter, we will introduce the concept of a distributed application architecture and discuss the various patterns and best practices that are required to run a distributed application successfully.</p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor191"/>Questions</h1>
<p>Here are a few questions you should try to answer to assess your progress:</p>
<ol>
<li>Name the reasons why you would want to run a complete development environment inside a container.</li>
<li>Why should you avoid running applications inside a container as <code>root</code>?</li>
<li>Why would you ever bind-mount the Docker socket into a container?</li>
<li>When pruning your Docker resources to make space, why do you need to handle volumes with special care?</li>
<li>Why would you want to run certain admin tasks inside a Docker container and not natively on the host machine?</li>
</ol>
<h1 id="_idParaDest-193"><a id="_idTextAnchor192"/>Answers</h1>
<p>Here are sample answers for the questions in this chapter:</p>
<ol>
<li>You could be working on a workstation with limited resources or capabilities, or your workstation could be locked down by your company so that you are not allowed to install any software that is not officially approved. Sometimes, you might need to do proofs of concept or experiments using languages or frameworks that are not yet approved by your company (but might be in the future if the proof of concept is successful).</li>
<li>Bind-mounting a Docker socket into a container is the recommended method when a containerized application needs to automate some container-related tasks. This can be an application such as an automation server (such as Jenkins) that you are using to build, test, and deploy Docker images.</li>
<li>Most business applications do not need <code>root</code>-level authorizations to do their job. From a security perspective, it is therefore strongly recommended to run such applications with the least necessary access rights to their job. Any unnecessary elevated privileges could possibly be exploited by hackers in a malicious attack. By running the application as a non-<code>root</code> user, you make it more difficult for potential hackers to compromise your system.</li>
<li>Volumes contain data and the lifespan of data most often needs to go far beyond the life cycle of a container, or an application, for that matter. Data is often mission-critical and needs to be stored safely for days, months, or even years. When you delete a volume, you irreversibly delete the data associated with it. Hence, make sure you know what you’re doing when deleting a volume.</li>
<li>There are several reasons why you might want to run certain admin tasks inside a Docker container, rather than natively on the host machine:<ul><li><strong class="bold">Isolation</strong>: Containers provide a level of isolation from the host machine, so running admin tasks inside a container can help to prevent conflicts with other processes or dependencies on the host machine.</li><li><strong class="bold">Portability</strong>: Containers are designed to be lightweight and portable, which allows for easy deployment of admin tasks across different environments. This can be particularly useful for tasks that need to be run in multiple environments or on multiple machines.</li><li><strong class="bold">Consistency</strong>: Containers provide a consistent environment for running admin tasks, regardless of the underlying host machine’s configuration. This can be useful for ensuring that tasks are run in a predictable and repeatable manner, which can help to minimize errors and improve efficiency.</li><li><strong class="bold">Versioning</strong>: Containers allow for easy versioning of admin tasks, which allows for rollbacks and roll forward of the tasks. This can be useful for testing, troubleshooting, and production environments.</li><li><strong class="bold">Security</strong>: Running admin tasks inside a container can help to improve security by isolating the task from the host machine, and by making it easier to limit the permissions and access that the task has.</li><li><strong class="bold">Scalability</strong>: Containers can be easily scaled up and down, allowing you to increase or decrease the resources that the admin task needs.</li></ul></li>
</ol>
<p>Please note that this is not a comprehensive list and different use cases may require different approaches. It’s important to weigh the pros and cons of running admin tasks inside a container versus natively on the host machine and to choose the approach that best fits your particular use case.</p>
</div>
</div>

<div><div><h1 id="_idParaDest-194"><a id="_idTextAnchor193"/>Part 3:Orchestration Fundamentals</h1>
<p>By the end of <em class="italic">Part 3</em>, you will be familiar with the concepts of a Dockerized distributed application and container orchestrators, and be able to use Docker Swarm to deploy and run your applications.</p>
<ul>
<li><a href="B19199_09.xhtml#_idTextAnchor194"><em class="italic">Chapter 9</em></a>, <em class="italic">Learning about </em><em class="italic">Distributed Application Architecture</em></li>
<li><a href="B19199_10.xhtml#_idTextAnchor218"><em class="italic">Chapter 10</em></a>, <em class="italic">Using Singl</em><em class="italic">e-H</em><em class="italic">ost Networking</em></li>
<li><a href="B19199_11.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Managing Containers with Docker Compose</em></li>
<li><a href="B19199_12.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">Shipping Logs and Monitoring Containers</em></li>
<li><a href="B19199_13.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>, <em class="italic">Introducing Container Orchestration</em></li>
<li><a href="B19199_14.xhtml#_idTextAnchor303"><em class="italic">Chapter 14</em></a>,<em class="italic"> </em><em class="italic">Introducing Introducing</em><em class="italic"> Docker Swarm</em></li>
<li><a href="B19199_15.xhtml#_idTextAnchor328"><em class="italic">Chapter 15</em></a>, <em class="italic">Deploying and Running a Distributed Application on Docker Swarm</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</div></body></html>