- en: Building Big Data Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn to build big data applications, analyze a traditional
    end-to end data workflow life cycle, and on similar lines build a big data application
    step by step. We will cover the big data process--discovery, ingestion, visualization,
    and governance. The emphasis will be on the Spark platform and data science prediction
    models. DevOps applications to various phases of big data will be explored in
    the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional data platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data platform core principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Big data life cycle:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data discovery
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analytics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark platform
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data governance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building enterprise applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data science--prediction models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional enterprise architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, an **enterprise data warehouse **(**EDW**) system is considered
    as a core component of the business intelligence environment. Data warehouse systems
    are central repositories built by integrating data from multiple disparate source
    systems, used for data analysis and reporting the needs of the enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review the end-to end data life cycle components of the traditional
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: The **data discovery** phase is where the source systems are explored and analyzed
    for relevant data and data structures. If the analyzed data is valid, correct,
    and usable, it is ingested into the data warehouse system. For example, if we
    need customer ID information, we should be connecting and extracting data from
    the correct columns and tables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality **ensures that the ingested data is acceptable and usable. A
    simple example is name formats of the first name and last name convention, which
    should be adhered to and, as appropriate, corrected for a few records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation** is the phase where data manipulation rules as per business
    logic are applied in tune with business needs. For example, every employee''s
    yearly salary is computed from multiple systems and saved in the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extract**, **Transform**, and **Load** (**ETL**) is the common term for consolidated
    reference for all the preceding phases together (data discovery, data quality,
    and data transformation) as a cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data staging **is the landing area on your systems for data collected from
    source systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data lineage **traces the origin and credibility of the data ingested into
    the system to ensure only authentic, trusted, and authorized data is inducted
    into the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata **is data about data. For example, a sales receipt is the origin
    of the record with transaction details, from where the requisite data from our
    computations is extracted. Details such as store ID, sales amount, item ID, date
    of transaction, and so on, are extracted from a sales receipt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data warehouse** is the storage layer, into which the transformed data is
    loaded as consolidated copy. It is time-variant, consistent, and read-intensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data marts** are data serving repositories specializing in some category,
    such as customer data, product data, employee data, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analytics **is the common term for an analysis to address all the business
    needs. Its building queries address business demands, such as how many customers
    were added last month, or what products are selling above targets this week.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic layer**--its business interface builds queries on the database with
    business intelligence tools, hiding complex data tables from business users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reporting**--reports are for business use, such as all products sold last
    month in a state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **dashboard **provides a consolidated quick view of important key performance
    indicators. An analogy is a car dashboard with speed, battery, petrol reserve,
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data visualization**--finding key performance business trends solely based
    on Excel reports can be a daunting task. Presenting them in a visual form is quite
    appealing, such as representing them as charts, histograms, and pie diagrams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principles to build big data enterprise applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data platforms and applications manage, integrate, analyze, and secure analytics
    on many data types both within the enterprise as well as in external data. They
    integrate multiple data sources in real time, taking into account volume, velocity,
    and variety. The platform can be built as a repository of an enterprise knowledge
    base with the organization's collective data assets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the salient features for building these platforms are discussed and
    as we see DevOps is very appropriate and instruments to enhance value at every
    stage, like versioning systems for building algorithms, data models, scalable
    reproducible platforms with virtual machines as seen in previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexible data modeling**: Big data systems integrate many different forms
    of data from multiple data sources. Rather than a pre-defined schema of rigid
    rows and columns, the schema is to be defined on the fly and data modeled to reflect
    how information is to be assimilated. To reflect real-world entities, it is also
    flexibly specified as a graph of objects with relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge management**: Version controlled knowledge base with accumulated
    insights of an organization can be leveraged as an enterprise asset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy and security controls**: The platform is designed for data lineage,
    multi-level security, and audit compliance. Every object integrated into the platform
    is traced to its original data source, where access restrictions are in place
    including authorization and authentication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithms for data processing**: These are massive datasets to be compiled
    and analyzed with built-in machine learning algorithms to augment the human user''s
    ability to make sense of large-scale data by identifying patterns in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalable platforms**: These platforms handle petabyte-scale data through
    a combination of a scalable architecture with federated data storage to hold large
    types of unstructured data, such as documents, emails, audio, video, images, and
    so on. These platforms are designed as open platforms extendable at every layer
    of the stack. The provision for efficient data discovery, data lineage, and elastic
    search tools should be considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration**: This platform enables multiple users, within and across
    organizations, to seamlessly, securely collaborate to analyze the same data concurrently,
    from low-level data integration, importing pipeline customizations, to building
    custom user interfaces. Data that has been integrated can be accessed as objects
    via APIs or can be exported for other frameworks and tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building models on models**: Simple models can serve as building blocks for
    more complex models, building out sophisticated analyses to be streamlined as
    a modular process. Models can be built using various in-built rich reusable libraries
    of statistical and mathematical operators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data visualization**: This is an interactive user interface to provide a
    seamless holistic view of all the integrated data of interest in the form of rich
    visualizations, such as tables, scatter plots, and charts. These visualizations
    in real-time are up to date with the source data so that users always see the
    most accurate and current information at any given time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data systems life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Big data systems are built in accordance with the data life cycle model, which
    can be broadly categorized in the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Data discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting data into the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting the data in storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytics on the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data governance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will study them in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Data discovery into the system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data discovery, like in the traditional process, ingests raw data from multiple
    source systems; however, the data will be divergent in volume, variety, and velocity
    when it comes to transforming it into business insights. Leveraging the power
    of big data, the data discovery process enables data wrangling and data enrichment
    facilitates combining datasets to recreate new perspectives and interactive visual
    analytics. An interactive data catalog facilitates guided search capabilities
    and enables us to thoroughly analyze and understand the data quality. A matured
    and robust data discovery process ensures possible data correlations; it lets
    users define attribute-based rules, relationships between data sources, harmonize
    data sources, and create enriched data based on a data mart.
  prefs: []
  type: TYPE_NORMAL
- en: To expand the boundaries of traditional business intelligence systems, harnessing
    the potential of big data is the key for business success. It helps to unlock
    the insights from new sources of information effectively. Organizations have the
    potential to access a wealth of knowledge to be tapped appropriately, with the
    proliferation of new sources of digital information.
  prefs: []
  type: TYPE_NORMAL
- en: Data discovery with big data tools and technology facilitates deep exploration
    for visibility into business performance with a big variety of data across the
    organization and even beyond. The business can explore new dimensions, transforming
    the way that business analytic systems are built and used more efficiently. Data
    discovery with any combination of data sources allows rapid, intuitive exploration
    and analysis of information; it enables deeper insight into the business, and
    the opportunity for greater efficiency. It builds new relations redefining roles
    between business and IT, adding new roles, new leadership, and revised means of
    data governance as well.
  prefs: []
  type: TYPE_NORMAL
- en: Many organizations have updated business intelligence decision systems to improve
    business performance based on existing data and systems to understand and monitor.
    These days, the vast majority of data growth is from systems beyond the reach
    of traditional BI environments, such as websites, social media, content management
    systems, emails, documents, sensor data, external databases, and so on. Hence
    the need to adopt to new age data discovery tools, also to consider that this
    diverse and changing data is growing exponentially. Data types to be dealt by
    the discover phase are varied, ranging from structured database tables to semi-structured
    forms containing a mix of numbers and free-form text to wholly unstructured documents.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenge along with the volume of data is the variety rather and
    its uncertain value. An internet-savvy business culture and the impact of consumer
    interaction with enterprise business software with mobile and web applications
    have created an urgent need to quickly explore relevant information to discover
    new insight for business prospects and decisions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Datafication** is the process of quantifying data from all types of sources,
    in all types of formats. Datafication allows information to be collected, tabulated,
    and analyzed, so that the potential uses of the information are limited only by
    the ingenuity of the skilled business user. The data''s true value is like an
    iceberg floating in the ocean. Only a tiny part of it is visible at first sight,
    while much of it is hidden beneath the surface. Innovative companies that understand
    this can extract that hidden value and reap potentially huge benefits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enterprise-class data discovery systems enable rapid, intuitive exploration
    and analysis of data from a wide combination of structured and unstructured sources.
    They enable organizations to channel their existing investments to extend business
    analytics capabilities to new combinations of a greater variety of sources, including
    social media, websites, content systems, e-mail, and database text, providing
    a new level of visibility into data and business processes, saving time and cost
    and leading to better business decisions. Some of the advantages of this approach
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: You can gain deeper insight into the business by enabling users with increased
    insight and visibility to find the data they want to analyze.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Near real-time data and content can be delivered by access to fresher information,
    helping people make decisions based on the most current information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased reuse of assets. You are able to reuse information assets and eliminate
    the costs of re-creating these assets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of use for BI professionals to develop and deliver analytic consumer-style
    applications for business professionals, leading to higher adoption rates, lower
    training costs, and faster time to value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data discovery stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data discovery process involves stages such as prototyping, visualization,
    bridging, replication, and transformation. These concepts applied in tandem in
    the context of data discovery provide value out of big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototyping**: Generally, in any business context, projects are always under
    pressure and constrained to deliver under-limited resources of time and budgets.
    For a complex project, prototyping is an effective way of making progress when
    challenges are pressing, demanding, seem difficult, and are under time constraints.
    Prototyping is about acting before you''ve got the answers, about taking chances
    in the absence of a proven formula or a known way of solving a problem. Prototyping
    lets us test a hypothesis and explore alternative approaches, building in small
    blocks to get the big picture. DevOps expedites the process of prototyping to
    make it a continuous cycle of development'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With data discovery, people are empowered to wander around in the data, try
    new combinations of sources, filter and refine the analysis, find previously hidden
    patterns, or jump to another experimental thread if the first one yields no insight.
    One of the key challenges is synchronizing data schema changes at the source systems
    with staging and development systems in real time; the DevOps methodology and
    process can be adopted to address this.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation through data discovery comprises three main tasks--asking new
    questions, seeing new patterns, and adding new data. These steps comprise a continuous
    process which is iterative and which flows in any direction, depending upon what
    the user sees or theorizes at any given moment. DevOps integrated with the data
    discovery phase makes it an automated process from source systems identification
    to ingestion of data to staging systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualization**: Data visualization helps data analysts gain immediate feedback
    on their hypotheses, as graphics are more convenient instruments for realizing
    patterns in quantitative information. Viewing the results of empirical business
    analysis in real time enables data analysts to determine what refined or further
    search could lead to a deeper understanding of a performance gap or market opportunity.
    Effective data discovery is augmented by quick visualization of analysis and results
    to aid both experienced data analysts and non-technical business users. Data visualization
    techniques such as drag and drop dashboards, quickly produced charts, easy to
    use wizards, and consumer-style navigation, accelerate understanding of patterns
    of data, and its behavior attributes quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bridging**: Business and IT can have a more effective working relationship
    through the data discovery process. In a traditional setup, business and IT perform
    separate activities as a provider and user in the data discovery phase. This enables
    them to work together as a team to combine efforts to jointly explore and learn.
    IT analysts help their business partners, showing them how to add data sources,
    refine searches, and explore new questions, stepping through the capabilities
    of the software. They can work with the business directly on how to build discovery
    applications in real time, and how to become self-sufficient in analysis of the
    possibilities. Conventional extracting requirements, writing complex specs, and
    going through a lengthy development and deployment process are tedious in terms
    of time and effort. IT gains huge efficiencies bypassing much of the SDLC process
    and by empowering business users via self-service data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Systematizing**: As data exploration becomes more versed with deep insight
    skills, in some cases a certain path of investigation could be repeatable a few
    times. It could also yield valuable insights beyond a limited one-time scenario
    to more effectively grasp a part of the business question to continually repeat
    and track the outcome. Once the business queries and responses are well established
    by the metrics and dimensions that are used to describe key business processes,
    they can be considered to roll the analysis and metrics into the organization''s
    enterprise BI system. Business intelligence platforms excel in allowing users
    to perform standardized queries for well-known questions on standard datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data discovery and business intelligence systems complement each other, with
    data discovery excelling in unknown questions and BI focusing on systematized
    analysis, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e727252e-fd20-4acf-a1fb-3f486db0a40d.png)'
  prefs: []
  type: TYPE_IMG
- en: DevOps will standardize the process of data models, dashboards, and visualization
    reports can be maintained into the repository and automated testing and deployment
    from development systems to QA and production as continuous integration and deployment
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformation**: The data discovery process can help explore an unlimited
    number of new avenues to address business problems and find new opportunities
    previously hidden in the data. Uncovering these new dimensions through a process
    of experimentation uncovers valuable new insights, paving the way for transformation.
    However, for standard known areas, they will continue using existing BI systems,
    and use data discovery to explore ways to give insights for new questions and
    problems. Thus data discovery is proving its value in deploying an easy exploration
    of diverse data to uncover insights that drive dramatic increases in revenue and
    productivity while improving the alignment and relationship of business and IT.
    It enables a transformation in the world of business analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several tools in big data discovery, such as Apache PIG, Oracle Big
    Data Discovery, Zoomdata, Exasol, Revolution, GridGain, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional BI tools such as Tableau, QlikView, Microstrategy, Informatica,
    and so on, are also offering extensive data discovery features for big data.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the biggest challenges is ensuring data quality and accuracy, which
    is easier said than done. DevOps will augment the data quality process to ensure
    the data quality cycle from scripts to automation of the entire validation process.
    Let''s look at some of the challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data variety**: The data coming from diverse data sources such as mobile
    devices, web technologies, sensor data, social media, and so on brings with it
    multiple complex data types and data structures, increasing the difficulty of
    data integration. These data sources produce data types such as unstructured data
    in the form of documents, video, audio, and so on, and semi-structured data like
    software package, modules, spreadsheets, financial reports, and structured data.
    The valuable insights gained depend on the data collected, stored, and verified.
    They come from so many divergent sources and rely on the effectiveness of the
    integration process. When working with data-intensive and sensitive industries
    such as life sciences, this process has to be foolproof.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity of data**: The data becomes complex, accounting for multiple attributes
    such as raw data from a variety of different sources directly from consumers,
    salespeople, operations, and other sources within the organization. The timeliness
    of the data is crucial; you need to gather the required data in real time or deal
    with the data needs in real time, otherwise the data can become stale, outdated,
    and invalid. Processing analysis based on the data will produce useless or misleading
    information and conclusions, misleading the decision-making systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensuring data security**: There are so many technologies that contribute
    to multiple channels of communications across applications of mobile, web, and
    ERP, which adds to the complexity of maintaining data security. New age technologies
    such as cloud, big data, and mobile are expanding their reach very quickly and
    becoming popular. However, data security needs and challenges are more complex
    than before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'General guidelines to ensure data quality are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data availability**: The data intended for consumption to the big data systems
    should be appropriately available for the intended use. It should be timely data
    with either real-time streaming or batch mode. The data should be accessible with
    API interfaces available and also the data should be authorized for use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data appropriateness**: The data should be credible; otherwise, the purpose
    is defeated. The data lineage process traces the origin of the data. The data
    definitions should be appropriate and acceptable on freshness and regular updates
    for the data. The data documentation and metadata should be audited for correctness
    within an acceptable range of values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data accuracy**: The data should be reliable; the data value should represent
    the true state of the source information and the data should not be ambiguous
    and should be a single version of a fact. Data should be consistent and verifiable
    during the intended time as per the time stamp. The data should be complete and
    auditable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integrity**: The data format should be clear and meet the set criteria
    of consistency with the structure and content. Its integrity should be intact.
    The data should be relevant and match the required purpose, and be complete in
    all aspects and fit to use for its intended purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presentation quality**: The data content and format for the data should be
    clear and understandable. The data description, classification, and coding content
    should be easy to understand and meet the stated objectives and specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many big data open tools providing multiple features for data quality,
    such as Talend, AB Initio, Data Manager, Datamartist, and iManage Data.
  prefs: []
  type: TYPE_NORMAL
- en: In the data ingestion process, raw data is added to the system from multiple
    data sources. The process can be complex, depending on the format and quality
    of the data from the source systems and the state of the data to be processed
    from the desired target state for consumption. There are multiple ways and means
    of ingesting data into big data systems, depending on the type of big data ingested.
    To prepare raw data for the system's use, some level of analysis, sorting, and
    labeling usually takes place during the ingestion process. Extending conventionally
    from the legacy data warehousing processes consists of extracting, transforming,
    and loading, referred to as the ETL process. Some of the same concepts apply to
    data entering the big data system as well.
  prefs: []
  type: TYPE_NORMAL
- en: The process of data ingestion involves massaging the input data for proper formatting,
    categorizing, and labeling. The data structure should adhere to predefined standards
    by eliminating unwanted data. The data thus captured from multiple source systems
    in large volumes is used for further processing. It is stored in a data lake in
    raw format
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch jobs, as the name indicates, are dataset jobs which are non-time sensitive
    and processed in batches in large datasets. The processing of batch jobs can happen
    in multiple modes, as described next.
  prefs: []
  type: TYPE_NORMAL
- en: RDBMS to NoSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the legacy data stored in RDBMS can be imported into NoSQL databases
    on HDFS using Sqoop, DevOps can aid for baseline of the Sqoop scripts, automating
    the process of importing and exporting the data, scalability of the storage and
    computing systems, test automation of the data integrity, and automated deployment..
    The data migration process is described here:'
  prefs: []
  type: TYPE_NORMAL
- en: Sqoop is a command-line interface application for transferring data between
    relational databases and Hadoop. It supports incremental loads of a single table
    or a free form SQL query as well as saved jobs which can be run multiple times
    to import updates made to a database since the last import. Imports can also be
    used to populate tables in Hive or HBase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oozie can be used to schedule and create flows for importing/exporting data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full as well as incremental imports can be configured in Sqoop/Oozie
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can directly import and create Hive tables, but if we build a layered architecture
    it is suggested to import to the staging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example--`sqoop import -connect jdbc:mysql://:/ -username -password --table
    --target-dir`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a9a8a246-8299-4065-84ef-8292381e98a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Flume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For other batch sources, we can deploy Flume. It will watch for new source files
    and push the data to HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Flume is effective and reliable for moving log data in large volumes to a distributed
    system, collecting and aggregating it in accordance with business demand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture is simple and flexible, matching the incoming streaming data
    flow needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flume is robust, fault-tolerant, customizable, with advanced features such as
    failover and a recovery mechanism, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple extensible data model supports online analytic applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0ed49622-6fcc-4849-ae46-48f9851634ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Stream processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stream processing, as the name indicates, is the computing of real-time data
    which is time-sensitive, usually with high-velocity metrics. Analytics are usually
    performed on the streaming data while it is being ingested for quality checks,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If data is available in streams such as application logs, program output (like
    web scrapper), sensors, geo-location, or social media, this can be collected using
    Kafka on a real-time basis.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka is an open-source message broker project that aims to provide a unified,
    high-throughput, low-latency platform for handling real-time data feeds. It is,
    in essence, *a massively scalable pub/sub message queue architected as a distributed
    transaction log, making it highly valuable for enterprise infrastructures to process
    streaming data*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka can be easily scaled to support more data sources and growing data volumes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka also supports direct connectivity to Spark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is one topic per incoming data source and one topic per consumer group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of partitions per topic will depend on the data size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b08b30a6-0cd8-4657-89e0-09effbfb6389.png)'
  prefs: []
  type: TYPE_IMG
- en: Apart from Sqoop and Kafka, Some other specialized data ingestion tools for
    importing and aggregating both server and application logs are Apache Flume and
    Apache Chukwa. **Gobblin** also offers a data ingestion framework to aggregate
    and normalize date.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The lambda architecture is effective when both batch and streaming data are
    ingested to systems at the same time, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddfe9eef-df2d-4862-b1c2-479da1a168fe.png)'
  prefs: []
  type: TYPE_IMG
- en: The data storage layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Persisting big data has multiple storage options, such as Data Lakes and data
    warehouse; cloud technologies for data storage greatly augment the needs of big
    data storage systems, scalable to terabytes and petabytes through simple storage
    devices and virtual machines. DevOps is an effective solution for managing scalable
    data storage with infrastructure as code discussed in detail in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Lake**: Data Lake is synonymous with a water lake where water is stored
    and consumed by many people. A Data Lake is the repository for the collection
    of raw data. The data collected could be unstructured and frequently changing,
    so, initially the data is pooled into the large repository to be consumed by users
    as per their scheduled needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data warehouse**: A data warehouse is an ordered repository for large volumes
    of data to be used for analysis and reporting. The data in a data warehouse is
    typically being cleaned, is well-ordered, and is integrated with other sources.
    It is generally more prominent with conventional systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ingestion process ensures the incoming data is processed as per business
    needs to be persisted reliably in the storage disk. The ingestion process can
    be complex, depending on the volume and variety of the data from the source systems.
    The availability of the distributed storage system is accomplished by Apache Hadoop's
    HDFS filesystem. With HDFS, large quantities of raw data are written to multiple
    nodes simultaneously with redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: Ceph and GlusterFS are other filesystems offering all these capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed databases such as NoSQL are well placed to import data for more
    structured access. They have features such as fault tolerance, and they have the
    capability to ingest heterogeneous data formats. Based on the organization's business
    needs, appropriate databases can be selected from a variety of available choices.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage - best practices for better organization and effectiveness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/fdae2dc1-b9d9-4e7d-bcea-62398e3a1785.png)'
  prefs: []
  type: TYPE_IMG
- en: Landing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A landing zone is where in the initial data lands from different source systems
    in the as it is state to the storage system.
  prefs: []
  type: TYPE_NORMAL
- en: The landing zone is where incoming data is stored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All input validation should be done here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The folder structure can be `<source>/<type of data>/<yyyymmddhhisss>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The archive mechanism should be applied as well (day/week/month)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access should be restricted to only processing users and not end users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the data lands in the landing zone, it undergoes sanity checks as to its
    appropriateness, format, and quality; upon satisfactory compliance, it is stored
    as raw data.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the raw data is stored in its original format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validated input from the landing layer is stored here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directory structure is managed by the ingestion framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only selected super-users and system users will have access to this data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snappy/LZO compression should be applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data classifier should be applied (hot, cold) and set to the archive policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The folder structure can be `<base dir>/<system type>/<dataset Source Name>/<Source
    Type>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The work area stores identified clean data to be used for business purposes.
  prefs: []
  type: TYPE_NORMAL
- en: This is the temporary working area and cleanup up should take place after related
    jobs unless the jobs require data for debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The checkpoint location for Spark streaming will be located here as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gold data is the valuable data that is the transformed, partitioned, and classified
    as master data.
  prefs: []
  type: TYPE_NORMAL
- en: This is the location that stores transformed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning is important in this layer and should be done based on the most
    frequently accessed columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification should be done for very hot, hot, cold, and very cold data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very cold data should be archived to blob storage (or any other cheap storage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The folder structure could be `<base dir>/<system type>/<dataset Source Name>/<Source
    Type>/<Job ID>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quarantine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the place where unwanted and stale data, which is of no active immediate
    use, is saved.
  prefs: []
  type: TYPE_NORMAL
- en: All rejected files from the various steps will be stored here, such as ingestion,
    transformation, as well as validation exception records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification should be done for very hot, hot, cold, and very cold data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very cold data should be archived to blob storage (or any other cheap storage)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Business
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Business data is the master data for a client's particular details, such as
    address, product preference, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This layer will have application/client-specific data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All data must be store in parquet format since all data at this stage will be
    structured
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outgoing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outgoing data is what is to be shared with external entities, such as suppliers,
    vendors, or third-party APIs.
  prefs: []
  type: TYPE_NORMAL
- en: This is the location from where data can be shared with the external application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files need to be archived once they are copied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients can have temporary or permanent access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good easy-to-use backup and disaster recovery solution provides integrated
    data sync between Hadoop clusters. It enables data protection by replicating data
    stored in HDFS platforms and across data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Computing and analyzing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data made available from the preceding processes can be analyzed to unearth
    the actual information value. The computational layer performs diverse functions
    where data is often processed iteratively with a combination of tools. The different
    types of insight needed for business needs are extracted by tailored practices
    that vary from organization to organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch processing is one method of computing over a large dataset where data
    is ingested in batch mode either daily or hourly. Apache Hadoop''s MapReduce is
    the most prominent and powerful batch processing engine and it is known as a distributed
    Map Reduce algorithm; it adopts the following strategy and is most useful when
    dealing with very large datasets that require quite a bit of computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Splitting**: In this process, the work is divided into smaller pieces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mapping**: This is the process of scheduling each piece on an individual
    machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shuffling**: Reshuffling data based on the intermediate results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing**: Processing each group of output data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assembling**: The final result is assembled together'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MapReduce framework forms the compute node while the HDFS filesystem forms
    the data node. Typically, in the Hadoop ecosystem architecture, both the data
    node and compute node perform similar roles. The delegation tasks of the MapReduce
    component are performed by two daemons, the **Job Tracker** and **Task Tracker**,
    Their activities are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a3405a6-ad22-4820-b3f5-fba4b8975a6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Data processing in batch, real-time, and stream processing is discussed next.
    DevOps is integral to big data systems for high volume data processing from source
    system discovery to the scalability of infrastructure to support storage needs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch processing**: It is very efficient in processing high volume data.The
    data is ingested to the system, processed, and then results are produced in batches.
    The computational power of the system is designed based on the size of the data
    being processed. The systems are configured to run automatically without manual
    intervention. The system can scale very quickly to accommodate the entire dataset
    for computational analyses of the huge volume of data files. Based on the volume
    of data processed and the computational power of the system defined, the output
    timelines can vary significantly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time/stream processing**: Though batch processing is a good choice for
    certain types of data and computation, other workloads require a more low-latency
    turnaround. There are real-time systems which are required to respond in real-time
    as they process information and make the analytics or visualizations readily available
    to the business while assimilating the new information continuously. These systems
    are called stream processing, which operates on a continuous stream of data composed
    of individual items. These systems, real-time or stream processing systems, utilize
    the real-time processing capability of in-memory engines; computational analytics
    are performed in the cluster''s memory to avoid having to write back to disk as
    in traditional disk-based persistent systems. Real-time processing is best suited
    for analyzing smaller chunks of data that are changing or being added to the system
    rapidly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many platforms and tools to achieve real-time processing, such as
    Apache Storm, Apache Flink, and Apache Spark. Each of them is designed as different
    ways of achieving real-time or near real-time processing. Apart from these listed
    computational frameworks, there are many other means of analyzing data or performing
    computations within a big data ecosystem. These tools frequently plug into the
    aforementioned frameworks and provide additional interfaces for interacting with
    the underlying layers. We have already discussed in the previous chapter their
    applicability to different scenarios and the best application for any individual
    problem, but we will recap a few of the tools here again:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hive provides a data warehouse interface for Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Pig provides a high-level querying interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Drill, Apache Impala, Apache Spark SQL, Presto, and so on, provide SQL-like
    interactions with data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R and Python are popular choices for simple analytics programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache SystemML, Apache Mahout, and Apache Spark's MLlib, provide building prediction
    models for machine learning libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark analytic platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a next-generation in-memory open-source platform, combining
    batch, streaming, and interactive analytics under one umbrella. Spark facilitates
    ease of use, providing the ability to quickly write applications with built-in
    operators and APIs along with faster performance and implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a faster and more general data processing platform and runs programs
    up to 100x faster in memory, or 10x faster on disk, than Hadoop, with lightning-fast
    cluster computing. The Spark framework is built on top of Hadoop clusters to process
    data from structured system such as Hive and stream data from Flume and Kafka.
    It has many advanced features and supports a variety of languages, including Java,
    Python, and Scala. It has extensive features for analytics, out-of-the-box algorithms,
    machine learning, interactive queries, and complex function analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark''s popularity is due to many advantages, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a subset of the Hadoop ecosystem. It integrates well with the reliable
    and secure storage platform, HDFS of the ecosystem. It is compatible with other
    data sources, such as Amazon S3, Hive, HBase, and Cassandra. It can run on clusters
    managed by Hadoop YARN or Apache Mesos, and can also run as a standalone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is a fast technology to enable large-scale data processing. This framework
    provides Java-, Scala-, and Python-based high-level APIs with a rich set of data
    stores for stream processing and machine learning. Though primary APIs are for
    Scala, Java, and Python, languages such as R are also supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark ensures parallel processing for data, very well integrated with Hadoop/HDFS
    for data storage, and to support variety of filesystems and databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark's machine learning capabilities are proved to be excellent solution for
    stream processing. With Spark REPL writing code is easier and quicker with inbuilt
    high-level (80) operators . Spark's **Read Evaluate Print Loop** (**REPL**) is
    interactive (out-of-the box) shell is a modified version of the interactive scala
    REPL. With REPL, no need to compile and execute the code. User expressions are
    evaluated and REPL will display the results of the expression. The *Read* takes
    expression as an input and parses and stores in memory as an internal data structure.
    *Eval* traverses the data structure, evaluates the called functions. *Print* displays
    the results with print ability. *Loop* iterates going back to read state to terminate
    the loop on exit. REPL expedites the turnaround time also supports ad hoc data
    query analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is more nimble and well suited for big data analytics. Continuous micro-batch
    processing with integrated advanced analytics is based on its own streaming API,
    which is developer-friendly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is more efficient compared to MapReduce. It is 100 times faster than MapReduce
    for the same process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The popular batch job processing engine for Hadoop, MapReduce poses a significant
    challenge due to its high-latency to the batch-mode response and it is difficult
    to maintain due to inherent inefficiencies associated with its architecture design
    and code. Spark''s main component is the Spark Core Engine. It is complemented
    by a set of powerful, higher-level libraries that can be seamlessly used in the
    same application:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL is a powerful query language with inbuilt DataFrames
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming engine is for data streaming
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark MLlib for machine learning along with machine learning pipelines models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GraphX with GraphFrames stores relationship between entities as a graphical
    representation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a6de56f0-97e8-40bf-adde-a96b9aa8c862.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark Core Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The base engine for Spark Core to perform large-scale parallel and distributed
    data processing is the Spark Core Engine. It performs the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Fault-tolerant and recovery-based memory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster job scheduling, distributing, and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage device systems interfacing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is built and based on an immutable, fault-tolerant, distributed collection
    of objects called **Resilient Distributed Dataset** (**RDD**) that can be operated
    on in parallel. Objects are created through loading an external dataset or distributing
    from the internal driver program. The operations performed on objects through
    RDD are transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation operations include map, filter, join, and union, and are performed
    on RDD and yield a new result.
  prefs: []
  type: TYPE_NORMAL
- en: Action operations include reduce, count, first and they return a value after
    running a computation on RDD.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark Engine is designed efficiently. The transformations are actually computed
    when an action is called and the result is returned to the driver program. Transformations
    are lazy and do not compute their results right away; however, they remember the
    task to be performed and the dataset (for example, a file) to perform the operation.
    The transformed RDD can be recomputed for the next transformation. The advantage
    is avoiding unnecessary changes and computations in the process. All this is achieved
    by the in-memory engine where the data is persisted and cached for the RDD objects.
    Spark will keep the elements around on the cluster-cached memory for much faster
    access the next time you query it.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark component, Spark SQL, supports querying data either through SQL or
    through the Hive query language. Spark SQL is integrated with the Spark stack
    providing support for various data sources. It allows you to weave SQL queries
    with code transformations, making it a very powerful tool.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming is a powerful functionality built on in-memory technology. The
    Spark Streaming API is compatible with the Spark Core Engine. It facilitates both
    batch and streaming data to process real-time streaming data from systems such
    as web server log files, Twitter-based social media data, and so on. Spark interfaces
    with other tools, such as Kafka, for various messaging queues.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming receives the input data from upstream systems such as Apache
    Flume, Kafka, and so on, and divides the data into batches to process them with
    the Spark Engine and generate a final stream of results in batches to store them
    on HDFS/S3, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f28d00a5-e3c5-4e7b-b5b2-a3b0c1d8b977.png)'
  prefs: []
  type: TYPE_IMG
- en: MLlib is a set of library functions that provides various algorithms of machine
    learning, such as classification, regression, clustering, and collaborative filtering.
    Apache Mahout (a machine learning library for Hadoop) is integrated into Spark
    MLlib. A few algorithms such as linear regression or k-means clustering also work
    with streaming data designed to scale out on a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: GraphX provides ETL functionality, exploratory analysis, and iterative graph
    computations. It provides a library for manipulating graphs and performing graph-parallel
    operations on common graph algorithms such as **page rank**.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is ideal to simplify challenging and compute-intensive task for real-time
    data processing of high volumes of streaming or archived data, both structured
    and unstructured, seamlessly integrating relevant complex capabilities, such as
    machine learning and graph algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Some challenges include operational complexity and the high skills required
    to develop and manage applications. Spark performs well with Hadoop to take advantage
    of Hadoop's HDFS. Performance tuning of both systems is imperative; otherwise,
    Spark's nuances can lead to out-of-memory error issues and memory lag, if jobs
    are not tuned well.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization with big data systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are well versed with the saying: *garbage in, garbage out*. Identifying
    and recognizing trends, variations, and changes in data over time is often more
    important and data visualization is an inevitable step. Due to the complexity
    of information being processed in big data systems, visualizing data is one of
    the most important and useful ways to spot trends and create meaningful insights
    from a large number of data points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss some real-time processing tools here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus**: To visualize application and server metrics in real-time processing,
    data streams as a time-series database and visualizes that information. The health
    of the systems is gauged by the frequent data changes and large variations in
    the metrics typically indicate significant KPIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic Stack**: It is popular for visualizing big data systems to visually
    interface with the results of calculations or raw metrics. It is also known as
    the ELK stack, composed of Logstash for data collection, Elasticsearch for indexing
    data, and Kibana for visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SILK**: It is a similar stack achieved by using Apache Solr for indexing
    and a Kibana fork called **Banana for visualization**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jupyter Notebook** and **Apache Zeppelin** offer visualization interfaces
    for interactive exploration and visualization of data in a format conducive to
    sharing, presenting, or collaborating. This technology is typically used for interactive
    data science work and is termed a data *notebook*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data governance is the process of classifying enterprise data and providing
    the right access and privileges for appropriate roles and personnel. It refers
    to the overall process of managing the availability, usability, integrity, and
    security of the data assets in an organization. A matured data governance model
    is a defined set of strategies, and includes a governing group and a well-orchestrated
    plan to execute those procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adopting an open source software development approach for platform/product
    development will ensure many advantages, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Seamless collaboration across different diverse technology teams spread across
    the organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage and re-use of existing software and knowledge assets across the organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ensures region, client, and country-specific needs are addressed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach for software development helps a governance mechanism in place
    to ensuring avoid duplication of work and enable transparency following a common
    standard framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data governance should define the minimum necessary rules instead of being
    an overhead. It should also balance across:'
  prefs: []
  type: TYPE_NORMAL
- en: Rules versus public (free for all)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People versus process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Empowerment versus directing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open source governance operates on three pillars:'
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set governance parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster delivery of every team and every member
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open source communities that practice transparency, encourage active participation,
    and recognize the contributions of all constituents are more likely to thrive,
    iterate, and strengthen their prospects. The guiding principles of governance
    in the open source community development model can be better demonstrated by describing
    the seven pillars illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/530f5272-4684-40bf-8606-8a2ca5c1d53f.png)'
  prefs: []
  type: TYPE_IMG
- en: People and collaboration in accordance with DevOps core concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Technology is driven by people; therefore, the success of technology depends
    on a few attributes--it should be quite easily adoptable for people, it should
    be flexible, easy to learn, and convenient to collaborate with. We will discuss
    them here:'
  prefs: []
  type: TYPE_NORMAL
- en: Establishing ownership for the process example code, submission, and review
    process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common dashboard to check the status of any component, changes made, review
    status, testing, impacted components, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopt forums for discussions for resolution of queries than e-mail, Yammer Groups,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular (weekly, bi-weekly) all-hands deployment to discuss changes in components
    and roadmaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Management of environments in information technology is a complex and critical
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel environments for development to be maintained, for example, current
    programs and other production fixes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define access restriction for components to interact with the database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define allocation of resources (disk space, threads/mappers, and so on) for
    each component/program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation of the ongoing work is important for the shell life of the project,
    features upgrades, and also to support and maintain as an operations manual.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensive documentation of the core components, business/assembly line usage,
    governance, and so on. It is important for collaboration across teams, new members,
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The artifacts documents to help each team member could be revised regularly
    to add more granular details:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master architecture reference document--created in TOGAF suggested format to
    speak in a common language
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Developer guides
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Governance guide
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment guide
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture board
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architecture board has the responsibility for the long-term enterprise and
    for ensuring the architecture meets the business goals such as service-oriented
    architecture, usage of components meeting the security guidelines, open source
    tools usage percentage as a roadmap, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The **Architecture Review Board** (**ARB**) is the authority for defining and
    participating in Tollgate (or milestone) planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Tollgate, the milestone for every program's high-level design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budget the re-factoring effort in every program and approve this in Tollgate
    meetings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralize decision making for design approvals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review checklists used for Tollgate and reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development and build best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Development best practices are the adoption of coding standards, adequate documentation,
    peer reviews, quality of the code, and so on, to ensure high quality of the code
    and performance. Build is a complex task with many interfaces. Adherence to proper
    guidelines will make it robust and well functioning as per the organization's
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Automated builds with automated testing processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peer review of source code based on sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common IDE, code review tools (PMD, check style, and so on), build tools (Hudson
    and Maven)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard build promotion procedures and schedules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish environment stability on a weekly basis through continuous integration
    and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a test bed and regression suite to execute impacted modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Version control will ensure code changes are well tracked for traceability and
    accountability.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the organization teams using an enterprise standard tool for version
    control is the ideal. Governance in versioning has the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated email for every check-in to a controlled group of supervisors mailing
    list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define contributors for every program component and restrict access to the components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Periodic audit of check-ins and approval from a component owner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A mature release management process is a strong asset for the organization to
    deliver timely dependable products to its customers with quality.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized release management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common priority defined across programs and features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employ microservices/incremental deployment--architecture for independent deployments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building enterprise applications with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For enterprise applications to be successful, it is very important that you
    carefully define the data access, processing, and governance framework.
  prefs: []
  type: TYPE_NORMAL
- en: Client-services presentation tier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This graphical user interface will be backed by a set of APIs to help on-board
    new users. Some features that can be supported are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Manage client data sources, file formats, delivery frequency, validation rules,
    join conditions (if multiple datasets are present), and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate and transform datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage access to datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional data delivery requirements from Eureka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data catalog services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This graphical user interface will be backed by a set of APIs to provide data-related
    services. Some of the features that can be supported are:'
  prefs: []
  type: TYPE_NORMAL
- en: Search for any dataset/data in the data lake in a fashion similar to Google
    Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Browse (preview with pagination) search results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the lineage and data profile of the selected data set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow catalog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This graphical user interface will let you define workflows for an application
    and schedule runs. The main functionalities include:'
  prefs: []
  type: TYPE_NORMAL
- en: Create workflows for an application and schedule them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the execution status of workflows, the time taken, as well as the datasets
    involved along with the lineage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to restart the process in case of a failure or from any given point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure status updates, notifications, and alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage and tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This graphical user interface will be in use for Sentry and Navigator to track
    the usage of datasets across the cluster. The main functionalities include
  prefs: []
  type: TYPE_NORMAL
- en: '**Valid usage tracking of datasets**: How many times a dataset was accessed
    and by who'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invalid usage tracking**: Who tried to access a dataset they did not have
    access to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process tracking in terms of which user is running what process and what resources
    are being consumed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need to define dashboards based on requirements from the operations team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security catalog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This graphical user interface will be backed by REST APIs to configure access
    control for user groups. The features included are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Manage users and groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage user access to various datasets across the cluster and applications that
    can run in the data lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is where all transformations and processing on the data will take place.
    Processing can be batch as well as real-time with support for various frameworks
    such as Spark and MapReduce, along with querying engines such as Hive and Impala.
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be ingested from various sources, ranging from batch to real-time streams
    using tools such as Sqoop, Flume, Kafka, and SFTP.
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion will be metadata-driven
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to ingest new data sources, new code is not required as long as it
    makes use of the supported ingestion methods, such as Kafka, Flume, Sqoop, and
    so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should be able to register datasets and start cataloging them into the data
    catalog as soon as they are ingested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ingested data can be used in multiple forms: stored, persisted into a device,
    published to external vendors. It can be accessed by other third-party programs
    through APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storage layers**: In order to maintain data integrity and isolation, we can
    spread our data in HDFS across multiple layers so that each layer defines a certain
    stage between ingesting raw data and generating insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Publishing**: This service will be used to publish data to external users
    and subscribers, as well as applications, as an outward push from the data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bulk API**: This service will be used to download data from the system using
    an asynchronous API. Users will make requests for data retrieval and they will
    be notified when the data set is ready. Delivery of the data can be provided in
    multiple ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download link.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Push to SFTP location.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: HDFS location for internal users (same cluster or another cluster).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: API contracts need to defined.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Access API**: This API is similar to the Bulk API, but will only support
    small datasets such as a credit score to be synchronized frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notebooks**: These can include interfaces such as Apache Zeppelin or the
    Hue data science workbench, which will give a GUI interface for users to access
    datasets in the cluster and query them using Impala, Spark, R, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science as a field has many dimensions and applications. As we all are
    familiar with science by formulating reusable and established formulas, we understand
    the features, behavior patterns, and meaningful sights. In a similar way from
    relevant data too through engineering and statistical methods, we understand the
    behavior patterns and meaningful sights. Thus, it's also viewed as data plus science,
    the science of data or data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data science has been in use for decades across industries. Many algorithms
    have been developed and are in use across the industries, including:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayesian classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual work flows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apriori
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combinations of the preceding algorithms are used to solve popular business
    problems such as the following, and new business opportunities are surfacing continuously:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cross-selling | Find relationship among customer characteristicsMatch campaigns
    to potential customers |'
  prefs: []
  type: TYPE_TB
- en: '| Product yield analysis | Classify product defects |'
  prefs: []
  type: TYPE_TB
- en: '| Direct marketing | Classify customersMatch campaigns to potential customers
    |'
  prefs: []
  type: TYPE_TB
- en: '| Churn analysis | Predict the tendency of churningWin back customers |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-selling | Find relationship of products in transactions |'
  prefs: []
  type: TYPE_TB
- en: '| Segmentation analysis | Classify customersMatch campaigns to potential customers
    |'
  prefs: []
  type: TYPE_TB
- en: '| Inventory analysis | Find relationship of products in transactionsMake replenishment
    decision |'
  prefs: []
  type: TYPE_TB
- en: '| Product-mix-analysis | Classify customersMatch campaigns to potential customersEstimate
    the revenue of product mix |'
  prefs: []
  type: TYPE_TB
- en: '| Fraud detection | Classify customersDetect unusual activities |'
  prefs: []
  type: TYPE_TB
- en: '| Credit-rating | Classify customersCredit rating customers |'
  prefs: []
  type: TYPE_TB
- en: 'For example, in data classification itself we can implement the following three
    models to get a refined and accurate model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forests**: Random forests or random decision forests are an ensemble
    learning method for classification, regression, and other tasks. They operate
    by constructing a multitude of decision trees at training time and outputting
    the class that is the mode of the classes (classification) or mean prediction
    (regression) of the individual trees. Random decision forests correct for decision
    trees'' habit of overfitting to their training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please check the following link: [https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Bayes**: Naive Bayes classifiers are a family of simple probabilistic
    classifiers based on applying Bayes'' theorem with strong independence assumptions
    between the features. Naive Bayes classifiers are highly scalable, requiring a
    number of parameters to be linear in the number of variables (features/predictors)
    in a learning problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector machine**: Support vector machines are supervised learning
    models with associated learning algorithms that analyze data used for classification
    and regression analysis. Given a set of training examples, each marked as belonging
    to one of two categories. An SVM training algorithm builds a model that assigns
    new examples into one category or the other, making it a non-probabilistic binary
    linear classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the steps towards building a prediction model to solve a
    business problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the tangible business goal is the most important criteria. The business
    value and purpose of the data science problem agreement by different stakeholders
    is the most important step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sponsorship and buy-in from key stakeholders is crucial to the success of the
    project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaboration among data engineers and data scientists is critical; otherwise,
    working in silos will not lead to project success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data lake is a repository that gathers useful data from different source systems
    of appropriate, valid, meaningful, useful, and historical data required for the
    business problem. For building a data lake, capacity planning for the initial
    data and growth considerations for the future should be taken into account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleanse data as per quality norms to ensure only valid and appropriate data
    is used and no dirty or stale data enters the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering is core engineering of the data science project to extract
    meaningful insights (features) from the raw data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection is to eliminate irrelevant, redundant, or highly correlated
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the prediction models by following the proper validation methods, such
    as K-Fold Cross Validation or 70:30 models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish the model results. As with any project, the repetition of results
    accuracy validates the model's effectiveness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the model by continuous improvement with new data iteratively, and
    by fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many popular statistical modeling tools are on the market, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: SPSS modeler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNIME
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Revolution Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RapidMiner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAP Predictive Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAS Enterprise Miner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle Advanced Analytics (Oracle Data Miner, Oracle R Advanced Analytics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few popular open source tools offer all the functionality on a par with commercial
    established statistical packages:'
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MatLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recent surge in low-cost technology availability such as Hadoop Eco systems,
    cloud computing, big data and open source tools has led to large-scale adoption
    by every industry from small enterprises to large giants.
  prefs: []
  type: TYPE_NORMAL
- en: Approach to data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The approach to data science solutions involves the following staged approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/064bb679-aa95-48ab-b565-0e068db1a8f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Knowledge mining (discovery) in datasets is an interactive and iterative process
    involving several steps for identifying valid, useful, and understandable patterns
    in data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data processing**: Data cleansing pre-transforms the raw data into an easy
    and convenient format for usage; a few related tasks are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling, that is, selecting representative subsets from a large population
    of data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove noise
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing data handling for incomplete rows
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature extraction: data useful in a particular context is extracted'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation**: Ensure usability of data by missing data treatment
    methods like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use only rows with relevant data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Substitution of values:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean value of particular attribute is used
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression substitution with historical value from similar case
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching imputation, similar attribute correlation case is used
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood, EM, and so on
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data mining**: It is automating the searching patterns in the data by methods
    like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence and path analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clustering analysis:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning a data set into clusters or subsets based on some common attributes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification methods:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Division of samples into classes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Trained set is used previous labelled data
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction of new values based on past data by inference
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute new values for dependent variable values based on few other measured
    attributes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization patterns
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification is similar to clustering but requires classes to be defined
    ahead of time:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification with classifier based on input label is returned
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic classification is where classifier returns probable values to
    assign them to a class
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific criteria like data more than 90% to avoid costly mistakes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign objects to class based on probability limits (greater than 40%, and so
    on)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression and forecasting
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data table statistical correlation:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data distribution in functional forms with prior assumptions
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning-based algorithms
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Curve fitting:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A well defined and known function underlying the data is explored
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Theory- and expertise based
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervised:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both input and desired results are part of training data
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training process inputs are based on correct known target and results
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper training, validation, and test set construction are crucial
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast and accurate results
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to generalize, new data should produce correct results without prior
    knowledge of target
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalization means the ability to produce valid outputs for inputs not available
    during training
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct results are not provided to the model during training
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Input data clustered to classes based on their statistical properties alone
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Significance of cluster and label
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Even small number of objects which are representative of desired classes and
    labels can be applied
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Curve fitting
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is by using proper subsets and early stopping
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on data learning and not just underlying function
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data used in training should perform well with new data
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4c3da999-84f6-4ed8-8796-31c7f7e62f69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data sets:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training set**: Used for learning where target value is known.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: Used to tune classifier architecture to estimate error.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test set**: Performance of classifier is assessed only; it''s never used
    in the training process. Test set error should provide unbiased generalization
    error estimate.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/312fc9c9-cc15-44a2-92a9-c97b076e211f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Data selection**: Garbage In , Garbage Out, underlying model representation
    should be training, validation, and test data.'
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unbalanced datasets**:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Network minimizes overall error so the proportion of types of data in the set
    is critical
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss matrix inclusion
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An even representation of different cases is the best approach to interpret
    networks decision
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning process:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Back propagation:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Output values compared with target value to compute the predefined error function
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Error is fed back into the network
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these inputs the algorithm adjusts the weights of each connection to reduce
    the value of error function.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The network will converge after repeating the process for longer training cycles
    for sufficient numbers
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9cf39fb0-b1cc-4dec-9ef3-fedfac318dd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Results:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confusion matrix: The prediction results on X axis are compared to target values
    on Y. Rows represent the true classes and columns predicted classes.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2695048c-bfd3-44fe-8db7-1698019e41c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Completeness and contamination
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performances are rated as the following criteria for the classifiers, for example
    between two classes:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness**: The percentage of objects of class A correctly classified'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contamination**: The percentage of objects of class A incorrectly classified
    as objects belonging to class B'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification rate**: The overall percentage of objects correctly classified'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi layer perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a structured flow consisting of the input layer of neurons and the output
    layer of neurons with one or more hidden layers in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons are well connected with adjacent layers though being in different topologies,
    they are connection by a choice of activation function, the **weights** are assigned
    as function values associated with the connections of various types and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eefb53d0-7d3f-46a3-ac3a-09156bcc0cd0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Artificial neural networks** this is inspired by the biological nervous system
    process. An artificial neural network is an information-processing mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: Highly interconnected large number of simple processing neuron elements working
    together to address specific problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple artificial neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9c38d79-06f3-41bd-934f-098d854172b8.png)'
  prefs: []
  type: TYPE_IMG
- en: A node or unit is a basic computational element that receives input from other
    units or external source.
  prefs: []
  type: TYPE_NORMAL
- en: To model synaptic learning, each input is considered with associated weight
    *w*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weighted sum of its inputs is computed as a function by the unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e295094-0714-43c9-9891-a83e78188e96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The different types of neural network are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedforward**: **Adaptive Linear Neuron** (**ADALINE**), RBF, single layer
    perception'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-organised**: SOM (Kohonen Maps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent**: Simple recurrent network, Hopfield network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic**: Boltzmann machines, RBM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modular**: **Associative Neural Networks** (**ASNN**), committee of machines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Others**: NeuroFuzzy, Cascades, PPS, GTM, Spiking (SNN), Instantaneously
    Trained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0daf9047-0a3b-41d3-8981-1e5e6f53247c.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi layer perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a most popular supervised model consisting of multiple layers of computational
    units inter connected in a feed-forward way usually. Each neuron in one layer
    is connected to subsequent layer neurons through direct connections.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a classification method with a set of simple rules; they are non-parametric
    without the need for any assumptions on distribution of the variables in each
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'As example decision tree is depicted in following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/395ad59d-c432-4a07-a086-1788017f9062.png)'
  prefs: []
  type: TYPE_IMG
- en: Unsupervised models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-organizing maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clusters are hierarchical, it finds successive clusters using previously assigned
    clusters with bottom up (agglomerative) or top-down (divisive) and partitional
    type cluster depicted below as right and left side respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Distances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To determine the similarity between two clusters and the shape of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: "**VAR**: For a transformed set of data points for each attribute, the mean\
    \ is \Preduced to zero; this is by subtracting the mean of each attribute from\
    \ the \Pvalues of the attributes and dividing the result by the standard deviation\
    \ of the attribute."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "**RANGE (Min‐Max\P Normalization)**: It subtracts the minimum value of an attribute\
    \ from each value of the attribute and then divides the difference by \Pthe range\
    \ of the attribute. The advantage is preserving all relationship in the\Pdata\
    \ precisely, without adding any bias."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SOFTMAX**: It is a way of reducing the influence  of extreme values or outliers
    in the data without removing them from the dataset. It is useful when you have
    outlier data that you wish to include in the dataset while still preserving the
    significance of data within a standard deviation of the mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means is popular model as it's fast and simple; however it does not yield
    the same result with every run.
  prefs: []
  type: TYPE_NORMAL
- en: Partitions data into K clusters based on their features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each cluster is represented by its centroid, the center of the cluster points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each point is assigned to the nearest cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to minimize intra-cluster variance or the sum of squares of distances
    between data and the corresponding cluster centroid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computes the mean point--centroid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new partition is built by associating each point with the nearest centroid.
    Computes the mean point or centroid of each set.
  prefs: []
  type: TYPE_NORMAL
- en: The recent surge of low-cost technology availability such as Hadoop eco systems,
    cloud computing, big data, and open source tools has led to large-scale adoption
    by every industry from small to large giants. Data science penetration is also
    witnessed across every industry with eco system being available.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the key concepts of building big data applications,
    covering the tools for data discovery, quality, and ingestion. We discussed the
    Spark Stream in-memory engine and its versatility; data science models for various
    industry solutions were also discussed.
  prefs: []
  type: TYPE_NORMAL
