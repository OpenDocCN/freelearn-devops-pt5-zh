<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Multinode Applications</h1></div></div></div><p>In this chapter, we are going to start playing with the really cool stuff. We are going to use all the skills we have learned in the book so far. We are really going to step it up by a notch. In this chapter, we are going to deploy four servers. We will look at how to cluster Consul, which will give us a perfect opportunity to further our modules' functionality. In this chapter, we are going to look at two ways to network our containers. First, by using the standard host IP network, that<a id="id171" class="indexterm"/> our Consul cluster will communicate on. We will also install the <strong>ELK</strong> (<strong>Elasticsearch</strong>, <strong>Logstash</strong>, <strong>and Kibana</strong>) stack (<a class="ulink" href="https://www.elastic.co/">https://www.elastic.co/</a>). To do this, we will be writing a module for each of the products. Because Elasticsearch is our data store in this solution, we want to hide it so that only Logstash and Kibana can access the application. We will accomplish this using the native Docker networking stack and isolate Elasticsearch using VXLAN. As you can see, we are going to get through a lot in this chapter. We will cover the following topics in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The design of our solution</li><li class="listitem" style="list-style-type: disc">Putting it all together</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec27"/>The design of our solution</h1></div></div></div><p>As there are a lot of<a id="id172" class="indexterm"/> moving parts in the solution, it would be best to visualize what we are going to be coding for. As this will be a big step up from our last chapter, we will break down the solution. In the first topic, we will look at the design for our Consul cluster.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec36"/>The Consul cluster</h2></div></div></div><p>In the design, we are going to <a id="id173" class="indexterm"/>use four servers: <strong>node-01</strong>, <strong>node-02</strong>, <strong>node-03</strong>, and <strong>node-04</strong>. We will use <strong>node-01</strong> to bootstrap our Consul cluster. We will add the other three nodes to the cluster as servers. They will be able to join the conciseness, vote, and replicate the key/value store. We will set up an IP network that is on the <code class="literal">172.17.8.0/24</code> network. We will map our container ports to the host ports that sit on the <code class="literal">172.17.8.0/24</code> network. The following image will show the network flow:</p><div><img src="img/B05201_06_1.jpg" alt="The Consul cluster"/></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec37"/>The ELK stack</h2></div></div></div><p>Now that we have our Consul cluster, we can look at what our ELK stack is going to look like. First, we will walk<a id="id174" class="indexterm"/> through the design of the network. The stack will be connected to the native Docker network. Note that I have not listed the IP addresses of our Docker network. The reason for this is that we will let the Docker daemon select the networking address range. For this solution, we are not going to route any traffic out of this network, so letting the daemon choose the IP range is fine. So you will also note that Elasticsearch is only connected to our Docker network. This is because we only want Logstash and Kibana. This is to keep other applications from being able to send requests or queries to Elasticsearch. You will note that both Logstash and Kibana are connected to both the Docker network and the host network. The reason for this is that we want applications to send their logs to Logstash, and we will want to access the Kibana web application:</p><div><img src="img/B05201_06_2.jpg" alt="The ELK stack"/></div><p>To get the full picture of the architecture, we just need to overlay both the diagrams. So, let's start coding!!!</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec28"/>Putting it all together</h1></div></div></div><p>Now that we have looked at the design, let's put it all together. We will look at the changes to the plumbing of our Vagrant repo. Then, we will code the extra functionality into our <code class="literal">consul</code> module. We <a id="id175" class="indexterm"/>will then run our Vagrant repo to make sure that our Consul cluster is up and running. After we have completed that task, we will build the ELK stack, and we will build a module for each of the products. We will also set up Logstash to forward logs to node-03, just so we can test to make sure that our ELK stack is correct. Let's get some light on this.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec38"/>The server setup</h2></div></div></div><p>We will now look at the changes we are going to make to our new Vagrant repo. The first file that we are going<a id="id176" class="indexterm"/> to look at is our <code class="literal">servers.yaml</code> file. The first thing we need to do is change our base box. As we are going to be connecting containers to the native Docker network, our host machines must run a kernel version above 3.19. I have created a prebuilt vagrant box with just this configuration. It is the Puppetlabs box that we have been using in all the other chapters with the kernel updated to version 4.4:</p><div><img src="img/B05201_06_3.jpg" alt="The server setup"/></div><p>As you can note in the preceding screenshot, the other change that we have made to the <code class="literal">servers.yaml</code> file is that we have added entries to the <code class="literal">/etc/hosts</code> directory. We have done this to simulate a traditional DNS infrastructure. If this had been in your production environment, we wouldn't have needed to add that configuration.</p><p>Now, we have to add the other three servers. The following screenshot will show exactly how it should look:</p><div><img src="img/B05201_06_4.jpg" alt="The server setup"/></div><p>So, the ports that we will hit <a id="id177" class="indexterm"/>once all our servers are built are <code class="literal">8500</code> on <code class="literal">node-01</code> (the Consul web UI <code class="literal">127.0.0.1:8500</code>) and <code class="literal">8081</code> (the Kibana web UI <code class="literal">127.0.0.1:8081</code>).</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec39"/>The Consul cluster</h2></div></div></div><p>We are well aquatinted<a id="id178" class="indexterm"/> with the <code class="literal">consul</code> module now, but we are going to take it to the next level. For this chapter, we are just going to use the compose version. The reason why is because when you start getting into more complex applications or need to use an <code class="literal">if</code> statement to add logic, an <code class="literal">.erb</code> file gives us that freedom. There are a fair few changes to this module. So, let's start again at our <code class="literal">params.pp</code> file:</p><div><img src="img/B05201_06_5.jpg" alt="The Consul cluster"/></div><p>As you can see, we<a id="id179" class="indexterm"/> have added two new parameters. The first one is <code class="literal">$consul_master_ip</code> and the other parameter is <code class="literal">$consul_is_master</code>. We will use this to define which server will bootstrap our Consul cluster and which server will join the cluster. We have hardcoded the hostname of <strong>node-01</strong>. If this was a production module, I would not hardcode a hostname that should be a parameter that is<a id="id180" class="indexterm"/> looked up in Hiera (<a class="ulink" href="https://docs.puppetlabs.com/hiera/3.0/">https://docs.puppetlabs.com/hiera/3.0/</a>). We will pick up on this again when we look at our <code class="literal">docker-compose.yml.erb</code> file. The other parameters should look very familiar to you.</p><p>Next, let's look at our <code class="literal">init.pp</code> file:</p><div><img src="img/B05201_06_6.jpg" alt="The Consul cluster"/></div><p>As you can see here, we have<a id="id181" class="indexterm"/> not changed this file much, as we have added a Boolean (<code class="literal">$consul_is_master</code>). However, we will want to validate the input. We do this by calling the stdlib function, <code class="literal">validate_bool</code>.</p><p>Let's quickly browse through the <code class="literal">install.pp</code> file:</p><div><img src="img/B05201_06_7.jpg" alt="The Consul cluster"/></div><p>Now, let's look<a id="id182" class="indexterm"/> at the <code class="literal">network.pp</code> file:</p><div><img src="img/B05201_06_8.jpg" alt="The Consul cluster"/></div><p>Finally, we will look at the <code class="literal">package.pp</code> file:</p><div><img src="img/B05201_06_9.jpg" alt="The Consul cluster"/></div><p>As you can see, we<a id="id183" class="indexterm"/> have not made any changes to these files. Now, we can look at the file that will really have the logic for deploying our container. We will then move to our <code class="literal">templates</code> folder and look at our <code class="literal">docker-compose.yml.erb</code> file. This is where most of the changes in the module have been made.</p><p>So, let's look at the contents of the file, which are shown in the following screenshot:</p><div><img src="img/B05201_06_10.jpg" alt="The Consul cluster"/></div><p>So as you can<a id="id184" class="indexterm"/> see, the code in this file has doubled. Let's break it down into three pieces, as shown in the following screenshot:</p><div><img src="img/B05201_06_11.jpg" alt="The Consul cluster"/></div><p>In the first block of code, the first change that you will note is the <code class="literal">if</code> statement. This is a choice to determine whether the node will be a master for bootstrapping Consul or a server in the cluster. If you remember from our <code class="literal">params.pp</code> file, we set <code class="literal">node-01</code> as our master. When we <a id="id185" class="indexterm"/>apply this class to our node, if its <code class="literal">node-01</code>, it will bootstrap the cluster. The next line that we want to pay attention to is as follows:</p><div><pre class="programlisting">
<strong>command: -server --client 0.0.0.0 --advertise &lt;%= @consul_advertise %&gt;  -bootstrap-expect &lt;%= @consul_bootstrap_expect %&gt;</strong>
</pre></div><p>We should just take note to compare the same line in the next block of code:</p><div><img src="img/B05201_06_12.jpg" alt="The Consul cluster"/></div><p>First, we can see that this is <code class="literal">elsif</code>, the second half of our <code class="literal">if</code> statement. So, this will be the block of code that will install Consul on the other three nodes. They will still be servers in the cluster. They will just not have the job of bootstrapping the cluster. We can tell this from the following line:</p><div><pre class="programlisting">
<strong>command: -server -bind 0.0.0.0 --client 0.0.0.0  --advertise &lt;%= @consul_advertise %&gt; -join &lt;%= @consul_master_ip %&gt;</strong>
</pre></div><p>Remember earlier that we looked at the same line from the first line of code. You see the difference? In block one, we declare <code class="literal">-bootstrap-expect &lt;%= @consul_bootstrap_expect %&gt;</code>, and in the second block, we declare <code class="literal">-join &lt;%= @consul_master_ip %&gt;</code>. By looking at the code, this is how we can tell the bootstrap order. Lastly, we can see that we are declaring <code class="literal">&lt;% end -%&gt;</code> to close the <code class="literal">if</code> statement.</p><p>Now, let's look at the last block of code:</p><div><img src="img/B05201_06_13.jpg" alt="The Consul cluster"/></div><p>As you can<a id="id186" class="indexterm"/> see, it's going to deploy the <code class="literal">registrator</code> container. As this sits outside the <code class="literal">if</code> statement, this container will be deployed on any node that the <code class="literal">consul</code> class is applied to. We have made a lot of progress till now. We should check the module changes before moving on to creating our new elastic modules. The one last thing we need to change is our <code class="literal">default.pp</code> manifest file, which is as follows:</p><div><img src="img/B05201_06_14.jpg" alt="The Consul cluster"/></div><p>As you can see, we have a node declaration for each node and have applied the <code class="literal">consul</code> class. Now, let's open our terminal change directory to the root of our Vagrant repo and issue the <code class="literal">vagrant up</code> <a id="id187" class="indexterm"/>command. This time, it will download a new base box from Hashicloud. So, depending on your Internet connection, this could take some time. Remember that the reason we need this new box is that it has an updated kernel to take advantage of the native Docker networking. In the last chapter, we were able to create a network, but we weren't able to connect containers to it. In this chapter, we will. Also, we are going to build four servers, so running Vagrant should take about 5 minutes. As soon as our first machine is up, we can log on to the consul web UI. There we can watch the progress as each node is joined.</p><p>As you can see in the following screenshot, our cluster is bootstrapped:</p><div><img src="img/B05201_06_15.jpg" alt="The Consul cluster"/></div><p>We can also <a id="id188" class="indexterm"/>check whether all our services are up and stable on the <strong>SERVICES</strong> tabs, as shown in this screenshot:</p><div><img src="img/B05201_06_16.jpg" alt="The Consul cluster"/></div><p>As you can see in the following screenshot, our second node has checked in:</p><div><img src="img/B05201_06_17.jpg" alt="The Consul cluster"/></div><p>The following screenshot shows what the screen looks like when we go to our <strong>SERVICES</strong> tab:</p><div><img src="img/B05201_06_18.jpg" alt="The Consul cluster"/></div><p>As you can see, our services have doubled. So things are looking good.</p><p>Now, the <code class="literal">vagrant up</code> <a id="id189" class="indexterm"/>command is complete, and our terminal output should look like the following screenshot:</p><div><img src="img/B05201_06_19.jpg" alt="The Consul cluster"/></div><p>Let's log back into our web browser to our Consul UI (<code class="literal">127.0.0.1:8500</code>). Under our <strong>NODES</strong> tab, we should now see all four nodes:</p><div><img src="img/B05201_06_20.jpg" alt="The Consul cluster"/></div><p>We can see that our cluster is in a good state as all four nodes have the same amount of services, which is <strong>10</strong>, and that all services are green. The last thing that we need to check is our DNS service<a id="id190" class="indexterm"/> discovery. So lets login to one of our boxes. We will choose <strong>node-03</strong>. So in our terminal, let's issue the <code class="literal">vagrant ssh node-03</code> command. We need to specify the node now as we have more than one vagrant box. We are going to ping the Consul service 8500. So, we just issue the <code class="literal">ping consul-8500.service.consul</code> command. The terminal output should look like the following screenshot:</p><div><img src="img/B05201_06_21.jpg" alt="The Consul cluster"/></div><p>This is now working perfectly. So, let's check one more thing now. We need to make sure that our Docker network is configured. For that, we will need to change the directory to root (<code class="literal">sudo -i</code>) and then issue the <code class="literal">docker network ls</code> command, as follows:</p><div><img src="img/B05201_06_22.jpg" alt="The Consul cluster"/></div><p>Now that everything<a id="id191" class="indexterm"/> is up and running, let's move on to our ELK stack.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec40"/>The ELK stack</h2></div></div></div><p>One of the focuses I had when<a id="id192" class="indexterm"/> planning this book was to use examples that could be ported to the real world so that readers get some real value. The ELK stack is no exception. The ELK stack is a very powerful stack of applications that lets you collate all your application logs to see the health of your application. For <a id="id193" class="indexterm"/>more reading on the ELK stack, visit <a class="ulink" href="https://www.elastic.co/">https://www.elastic.co/</a>. It has great documentation on all the products. Now, let's start our first new module.</p><p>As per our design, there is an order in which we need to install the ELK stack. Seeing as both Logstash and Kibana depend on Elasticsearch, we will build it first. All the images that we are going to use in our modules are built, maintained, and released by Elasticsearch. So we can be sure that the quality is good. The first thing that we need to do is create a new module called <code class="literal">&lt;AUTHOR&gt;-elasticsearch</code>. We covered how to create a module in the previous chapter, so if you are unsure how to do this, go back and read that chapter again. Now that we have our module, let's move it into the modules directory in the root of our Vagrant repo.</p><p>Seeing as the containers are already built by elastic, these modules are going to be short and sweet. We are just going to add code to the <code class="literal">init.pp</code> file:</p><div><img src="img/B05201_06_23.jpg" alt="The ELK stack"/></div><p>As you can see, we are calling the <code class="literal">docker::image</code> class to download <code class="literal">ealsticsearch</code>. In the <code class="literal">docker::run</code> class, we are calling our <code class="literal">elasticearch</code> container, and we are going to bind our container only to the <code class="literal">docker-internal</code> Docker network. You will note that we are not binding any ports. This is because, by default, this container will expose port 9200. We <a id="id194" class="indexterm"/>only want to expose port 9200 on the Docker network. Docker is smart enough to allow the exposed ports automatically on a Docker native network. In the next resource we are declaring just the host network for <code class="literal">elasticsearch</code>. We are specifying <code class="literal">0.0.0.0</code> as we don't know the IP that the container is going to get from the Docker network. As this service will be hidden from the outside world, this configuration will be fine. We will then map a persistent drive to keep our data.</p><p>The next thing that we need to do now is to add <code class="literal">elasticsearch</code> to a node. As per our design, we will add <code class="literal">elasticsearch</code> to <code class="literal">node-02</code>. We do this in our <code class="literal">default.pp</code> file in our <code class="literal">manifests</code> directory, as shown in the following screenshot:</p><div><img src="img/B05201_06_24.jpg" alt="The ELK stack"/></div><p>You will note that I have used <code class="literal">contain</code> instead of <code class="literal">include</code>. This is because I want to make sure that the <code class="literal">consul</code> class is applied before the <code class="literal">elasticsearch</code> class, as we need our Docker network to be there before <code class="literal">elasticsearch</code> comes up. If the network is not there, our catalogue will fail as the container will not build.</p><p>The next module we are going to write is <code class="literal">logstash</code>. Our <code class="literal">logstash</code> module is going to be a little more complicated, as it will be on both the Docker network and the host network. The reason we<a id="id195" class="indexterm"/> want it on both is because we want applications to forward their logs to <code class="literal">logstash</code>. We also need <code class="literal">logstash</code> to talk to <code class="literal">elasticsearch</code>. Thus, we add <code class="literal">logstash</code> to the Docker network as well. We will create the module in the same way we did for <code class="literal">elasticsearch</code>. We will call our <code class="literal">&lt;AUTHOR&gt;-logstash</code> module. So, let's look at our code in our <code class="literal">init.pp</code> file, which is as follows:</p><div><img src="img/B05201_06_25.jpg" alt="The ELK stack"/></div><p>Here, the first thing you will note is that we are creating a directory. This is to map to the container and will contain our <code class="literal">logstash.conf</code> file. The next declaration is a file type. This is our <code class="literal">logstah.conf</code> file, and as we can see, it's a template from the code. So, let's come back to it after looking at the rest of the code in our <code class="literal">init.pp</code> file. The next line of code will pull our <code class="literal">logstash</code> image from Docker Hub. In the <code class="literal">docker::run</code> class we will call our <code class="literal">logstash</code> container, use the <code class="literal">logstash</code> image, and attach the container to our <code class="literal">docker-internal</code> Docker network.</p><p>The next line of code will tell <code class="literal">logstash</code> to start using our <code class="literal">logstash.conf</code> file; we will then mount the directory we created earlier in our <code class="literal">init.pp</code> file to the container. Now, you can see in this module that we've exposed ports to the host network. In the last line, we tell <code class="literal">logstash</code> about our Elasticsearch host and Elasticsearch port. How does Logstash know where Elasticsearch is? We are not linking the containers like we did in previous chapters. This works in the same way to when we named our Elasticsearch container <code class="literal">elasticsearch</code>, and our Docker network has an inbuilt DNS server that lives at the address <code class="literal">127.0.0.11</code>. Any container that joins that network will register itself as its container name. This is how services on the <code class="literal">docker-internal</code> network find each other.</p><p>The last thing we need to <a id="id196" class="indexterm"/>look at is our template file for our <code class="literal">logstash.conf</code> file that we declared in our <code class="literal">init.pp</code> file. So, create a new folder called <code class="literal">templates</code> in the root of our module and then a file called <code class="literal">logstash.conf.erb</code>. We will add the following configuration to accept logs from syslog and Docker.</p><p>Lastly, at the bottom, we put our Elasticsearch configuration, as shown in this screenshot:</p><div><img src="img/B05201_06_26.jpg" alt="The ELK stack"/></div><p>Now, let's add our <code class="literal">logstash</code> module to <code class="literal">node-03</code> in the same way that we did with our <code class="literal">elastcsearch</code> module.</p><div><img src="img/B05201_06_27.jpg" alt="The ELK stack"/></div><p>Again, we will use <code class="literal">contain</code> instead of <code class="literal">include</code>. Now it's time to move on to our last module. We will create this in the same way as we have done for the last two modules. We will call this<a id="id197" class="indexterm"/> module <code class="literal">&lt;AUTHOR&gt;-kibana</code>.</p><p>In Kibana, we will only be adding code to the <code class="literal">init.pp</code> file, as shown in the following screenshot:</p><div><img src="img/B05201_06_28.jpg" alt="The ELK stack"/></div><p>As you can see, we are downloading the <code class="literal">kibana</code> image. In the <code class="literal">docker::run</code> class, we are calling our <code class="literal">kibana</code> container using the <code class="literal">kibana</code> image, attaching the container to our local Docker network. In the next line, we are mapping the container port <code class="literal">5601</code> (Kibana's default port) to port <code class="literal">80</code> on the host. This is just for ease of use for our lab. In the last line, we are telling <code class="literal">kibana</code> how to connect to <code class="literal">elasticsearch</code>.</p><p>Let's add <code class="literal">kibana</code> to <code class="literal">node-04</code> again using <code class="literal">contain</code> instead of <code class="literal">include</code>:</p><div><img src="img/B05201_06_29.jpg" alt="The ELK stack"/></div><p>We are now ready to run our Vagrant environment. Let's open our terminal and change the directory to the <a id="id198" class="indexterm"/>root of our Vagrant repo. We will build this completely from scratch, so let's issue the <code class="literal">vagrant destroy -f &amp;&amp; vagrant up</code> command.</p><p>This will take about 5 minutes or so to build, depending on your Internet connection, so please be patient. Once the build is complete, our terminal should have no errors and look like the following screenshot:</p><div><img src="img/B05201_06_30.jpg" alt="The ELK stack"/></div><p>The next thing we<a id="id199" class="indexterm"/> will check is our Consul web UI (<code class="literal">127.0.0.1:8500</code>):</p><div><img src="img/B05201_06_31.jpg" alt="The ELK stack"/></div><p>In the preceding screenshot, you can see that our Logstash and Kibana services are there, but where is Elasticsearch ? Don't worry, Elasticsearch is there, but we can't see it in Consul as we have<a id="id200" class="indexterm"/> not forwarded any ports to the host network. Registrator will only register services with exposed ports. We can make sure that our ELK stack is configured by logging in to our Kibana web UI (<code class="literal">127.0.0.1:8080</code>):</p><div><img src="img/B05201_06_32.jpg" alt="The ELK stack"/></div><p>The next thing we need to do is click on the <strong>Create</strong> button. Then, if we go to the <strong>Discover</strong> tab, we can see the logs from Logstash:</p><div><img src="img/B05201_06_33.jpg" alt="The ELK stack"/><div><p>Logstash's logs</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec29"/>Summary</h1></div></div></div><p>In this chapter, we looked at how to use Puppet to deploy containers across multiple nodes. We took advantage of the native Docker networking to hide services. This is a good security practice when working with production environments. The only issue with this chapter is that we don't have any failover or resiliency in our applications. This is why container schedulers are so important.</p><p>In the next chapter, we will drive into three different schedulers to arm you with the knowledge that you will need to make sound design choices in the future.</p></div></body></html>