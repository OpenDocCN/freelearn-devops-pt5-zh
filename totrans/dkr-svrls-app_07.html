<html><head></head><body>
        

                            
                    <h1 class="header-title">Operating FaaS Clusters</h1>
                
            
            
                
<p>One of the hardest things about having a system up and running is administering and maintaining our own clusters. Although serverless is a paradigm aimed at solving this problem entirely, in reality, there are some situations where we still need to provision and take care of servers by ourselves.</p>
<p>The idea behind serverless and Docker is to have a balance between reducing cluster maintenance and administration, and having full control of the cluster. Using Docker is a great way to help balance this.</p>
<p>Along with this balance, the most attractive driving factor for serverless is the <em>price model</em>. However, we have found that using Docker on EC2 Spot instances, given the competitive price, is sometimes even cheaper than AWS Lambda or other cloud functions. So with Spot instances, we will get the cheaper price, while our functions will not hit any limitation found in AWS Lambda or others.</p>
<p>Operating Docker-based FaaS clusters uses the same techniques as operating Docker clusters. We need to mix the techniques of running standalone Docker together with the techniques to utilize the Docker Swarm mode. This chapter focuses on <em>configuration stabilization</em>, how to prepare the new ingress layer, how to use a network plugin, how to set up the logging system, and how to operate the cluster using Golang scripting.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Stabilizing the configuration</h1>
                
            
            
                
<p>Let's start by carefully stabilizing the cluster configuration. At the time of writing, a Docker cluster works best with the following configuration. <em>Figure: 7.1</em> illustrated in this section depicts it well:</p>
<ul>
<li><strong>Ubuntu Server 16.04.3 LTS</strong>: Although Red Hat Linux or CentOS may work best for you, Ubuntu Server is easy to handle. We are constantly informed that Docker has been really well tested with Ubuntu Server. If you choose to use Red Hat or CentOS, please go with version 7.4.</li>
<li><strong>Linux Kernel 4.4 LTS</strong>: The 4.4 kernel is an LTS and it's great for Docker. You can also use kernel 4.9 but the kernel, like 4.13, is still too new for Docker.</li>
<li><strong>Overlay2</strong> <strong>as the Docker storage driver</strong>: Although the <strong>advanced multi-layered unification filesystem</strong> (<strong>AUFS</strong>) has worked well for Docker for quite a long time, overlay2 should be the new default storage driver for Docker running on the 4.4+ kernel. If you get a chance to run a production cluster on CentOS or RHEL 7.4, overlay2 is also a good option on these distributions.</li>
<li><strong>Docker CE 17.06.2</strong> <strong>or 17.09.1</strong>: Docker EE 17.06 is also a great option, if you can afford the enterprise edition:</li>
</ul>
<div><img src="img/43707075-e64a-4bdb-93a6-63f8b53205c4.png"/></div>
<p>Figure 7.1: A stabilized Docker Swarm stack with Træfik and WeaveWorks network plugin</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Choosing the right network plugin</h1>
                
            
            
                
<p>For a long time, people have said that the default Docker overlay network is not great for production. Although the quality of the overlay network driver is getting better and better, we may look at some other network plugins for optimum results. We can replace the default overlay driver with other plugins, for example, WeaveWorks or Contiv. We use WeaveWorks network plugin version 2 in this chapter.</p>
<p><em>Why WeaveWorks?</em></p>
<p>The WeaveWorks network plugin for Docker uses the same underlying network implementation as those of Kubernetes CNI. It has also been battle tested by its development team, WeaveWorks Inc. Additionally, it has been working really great so far, on my production clusters.</p>
<p>WeaveWorks network plugin version 2.1.3, in order to avoid disconnection bugs found in the current version of the overlay network driver, it is recommended entirely removing the default ingress network, which is based on the default overlay network driver, in production. A question may be raised here. If the ingress network is removed, we will lose the whole routing mesh, so then how can we route traffic into the cluster? The answer is in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">New ingress and routing</h1>
                
            
            
                
<p>As previously mentioned, we will not use the default Docker <em>ingress network</em> for <em>routing requests</em> to the running container:</p>
<div><img src="img/98d729e9-1aa1-4d8b-8e19-a99df09ecaa6.png" style="width:38.42em;height:28.92em;"/></div>
<p>Figure 7.2: The new ingress layer built on top of Træfik, connected to underlying Swarm tasks to form a routing mesh</p>
<p>Yes, we will lose the routing mesh, but we will build our own instead. As shown in the previous figure, we will replace the default routing mesh with a new ingress layer built on top of an L7 load balancer, <strong>Træfik</strong>. You can choose one from the following list of stable versions:</p>
<ul>
<li>Træfik v1.4.5 (<kbd>traefik@sha256:9c299d9613</kbd>)</li>
<li>Træfik v1.4.6 (<kbd>traefik@sha256:89cb51b507</kbd>)</li>
</ul>
<p>The advantage of using Træfik is that the newly built ingress layer is better stabilized. Each service is automatically resolved to be a list of IP addresses by Træfik. So you can choose to use either an IPVS-based load balancer offered by Docker Swarm, or the built-in mechanism offered by Træfik itself.</p>
<p>As Træfik works with the L7 layer, we are additionally allowed to match services with the hostname, and forward the request to a certain task of the matched service. Also, with this new implementation, we could flexibly restart or re-configure the ingress layer on-the-fly without touching the running services. This has been a weak point of the Docker's ingress layer for a very long time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Tracing component</h1>
                
            
            
                
<p>In the architecture proposed in this book, we use Envoy as a sidecar proxy for every deployed function. With Envoy, it allows distributed trace calling between functions, as in illustrated in the following figure, even if they are prepared by or deployed to different FaaS platforms. This is really an important step for avoiding vendor lock-in. Envoy is compiled and pushed to Docker hub incrementally. We have picked a certain version of Envoy for this book: <strong>E</strong><strong>nvoyProxy</strong>, <kbd>envoyproxy/envoy:29989a38c017d3be5aa3c735a797fcf58b754fe5</kbd>:</p>
<div><img src="img/b96914b6-55b6-460b-bed4-f0241670efac.png" style="width:38.92em;height:29.25em;"/></div>
<p>Figure 7.3: A block diagram showing the distributed tracing mechanism with Envoy</p>
<p>The following figure shows two levels of implementation for the sidecar proxy pattern. First, we directly tweak the <kbd>Dockerfile</kbd> of a function or a service by embedding the <strong>EnvoyProxy</strong> binary into the Docker image. This technique yields the best performance because <strong>EnvoyProxy</strong> talks to the function program through the <strong>loopback</strong> interface inside the container. But when we need to change the configuration of Envoy, such as <em>retry</em> or <em>circuit breaker</em>, we need to restart the <strong>EnvoyProxy</strong> together with the function instance, shown as the first (<strong>1</strong>) configuration in the following figure:</p>
<div><img src="img/9ed59fa0-2acc-46b3-8d8a-1cc2742e5095.png" style="width:46.58em;height:31.00em;"/></div>
<p>Figure 7.4: Two configurations to implement Envoy as (1) sidecar proxy and (2) edge proxy</p>
<p>So the better configuration when it comes to flexibility and management is the second (<strong>2</strong>) configuration, where we separate <strong>EnvoyProxy</strong>, as an edge proxy, out of the function container. The trade-off here is the network overheads between them.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Retry and circuit breaker</h1>
                
            
            
                
<p>In this section, we discuss one of the most interesting topics to date: the retry and circuit breaker pattern. It would be great to get familiar with this concept before proceeding to implementing a production cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Retry</h1>
                
            
            
                
<p>The problem solved by retry and circuit breaker stems from cascade failures caused by a service or a function inside a chain of calling becoming unavailable. In the following figure, we assume that five different functions or services have 99% availability, so they will fail once every 100 calls. The client observing this service's chain will experience the availability of <strong>A</strong> at only <strong>95.09%</strong>:</p>
<div><img src="img/bf31eea3-05e5-4a42-8990-4654a64e0bc7.png" style="width:40.92em;height:30.67em;"/></div>
<p>Figure 7.5: A chain of functions or microservices would make their overall availability lower</p>
<p>What does this imply? It means that when this chain becomes eight functions long, the availability will become 92.27%, and if it's 20 functions long, this figure will decrease to 81.79%. To reduce the failure rate, we should retry calling to another instance of function or service when an error, such as HTTP 500, occurs.</p>
<p>But a simple or constant-rate retry is not enough. If we use a simple strategy, our retry calls would increase unnecessary loads to the already broken service. This would cause more problems than it would solve.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Circuit breaker</h1>
                
            
            
                
<p>To resolve this problem, many retry pattern implementations usually come with <strong>Exponential Back-off Retry</strong>. With the exponential back-off strategy, we gradually increase the delay between each retry. For example, we may retry the second call to the service 3 seconds after the fault occurs. If the service still returns an error, we increase the delay to 9 seconds and 27 seconds, for the third and fourth calls respectively. This strategy leaves some room for the service to recover from transient faults. The difference between two kinds of retry strategies is shown in the following figure:</p>
<div><img src="img/0abeef59-8c82-41bf-9643-5f04d99e7d9a.png" style="width:37.25em;height:23.92em;"/></div>
<p>Figure 7.6: The difference between the constant-rate retry and exponential back-off retry strategies</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing a production cluster</h1>
                
            
            
                
<p>In this section, we will discuss how to prepare a production Docker Swarm cluster to run FaaS platforms at the cheapest rate possible on AWS Spot instances. The cost of deploying a Docker cluster would be as cheap as running codes on AWS Lambda, but it allows us to control almost everything in our cluster. If the deployment policy is cost-driven, this is the best way to go.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cost savings with Spot instances</h1>
                
            
            
                
<p>When we are talking about the cloud, its on-demand instances are actually cheap already. However, in the long run, the price of using cloud instances will be similar to buying real machines. To solve this pricing problem, major cloud providers, such as Amazon EC2, and Google Cloud Platform, provide a new instance type, collectively called a <strong>Spot instance</strong> in this book:</p>
<div><img src="img/18255aa6-7399-4cf8-83df-42ed2065c696.png" style="font-size: 1em;width:36.83em;height:19.50em;"/></div>
<p>Figure 7.7: Comparison of shutdown signals of a Spot instance on AWS versus Google Cloud</p>
<p>Spot instances are far cheaper than on-demand instances. However, their weak point is the short life cycle and unexpected termination. That is, a Spot instance could be terminated at any time. When it is gone, you have a choice as to whether to preserve or completely discard the volumes. On AWS, the instance will get the notification around 120 seconds before termination via remote metadata, while on Google Cloud, the notification will be sent via an ACPI signal 30 seconds before the machine stops. The rough comparison is shown in the previous figure.</p>
<p>We could put stateless computing to run on these kinds of instances. Both microservices and functions are naturally stateless, so Spot instances fit with the deployment of microservices and functions nicely.</p>
<p>With this kind of infrastructure on cheap instances, its cost will be comparable to AWS Lambda or Google Cloud Functions, but we are more in control of the overall system, meaning no invocation timeout for functions on this kind of infrastructure.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using EC2 Spot instances</h1>
                
            
            
                
<p>On Amazon EC2, go to <a href="https://aws.amazon.com/ec2/spot/">https://aws.amazon.com/ec2/spot/</a> and we will find the page as shown in the following screenshot. Log onto the AWS Console for Spot instances to set up some of them:</p>
<div><img src="img/0db6a15c-462f-4395-8ee5-28d1bf12fe96.png"/></div>
<p>Figure 7.8: The landing page of AWS Spot instances</p>
<p>On the navigation bar, we see Spot Requests. Click it to go to the Spot Requests screen as shown in the following screenshot. On this screen, clicking Request Spot Instances starts the request process:</p>
<div><img src="img/e688906e-192c-4731-a8c2-881348b7bc76.png"/></div>
<p>Figure 7.9: The Spot Requests screen on AWS displaying a request with its associated instances</p>
<p>There are three models for requesting Spot instances:</p>
<ul>
<li>One-time request. This is one time only, so when the instance is gone, we need to do another request.</li>
<li>Request a fleet of instances and let AWS maintain the number of target instances. When some instances are terminated, AWS will try its best, depending on our maximum bidding price, to allocate instances to meet the target numbers of each fleet. We have opted for this request model in this chapter.</li>
<li>Request instances for a fixed period of time. A fixed period is called a <strong>Spot block</strong>, which is between 1 and 6 hours. We will pay more if we set the longer period.</li>
</ul>
<p>The following diagram shows what the cluster in preparation will look like:</p>
<div><img src="img/e5d916bd-1965-4eec-b6cb-fe704ca74c96.png" style="width:45.92em;height:35.00em;"/></div>
<p>Figure 7.10: A Docker cluster forming on Spot instances using an automatic operator to take care of it</p>
<p>Assume that we already have three boxes provisioned to be managers. To get the cheapest rate possible, it is recommended using three on-demand EC2 nodes as Docker managers, and N-3 Spot instances as Docker workers. We start small with three Spot workers.</p>
<p>If possible, choose a cloud provider that allows you to create a private network and floating IPs. We will form a Docker cluster on the private network. Most cloud providers allow this, so do not worry.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Let's start</h1>
                
            
            
                
<p>First, SSH into a node we would like to be the first manager, install Docker, and run the <kbd>docker swarm init</kbd> command on it. The <kbd>eth0</kbd> is the private network interface provided by the cloud provider. Check yours using the <kbd>ip addr</kbd> command before proceeding. If you know which interface is the private one, initialize the cluster using the following command:</p>
<pre><strong>$ docker swarm init --advertise-addr=eth0</strong></pre>
<p>Next, SSH into the other two nodes. Install Docker and join the cluster using the <kbd>docker swarm join</kbd> command. Do not forget that we need to use the join token for the <em>manager</em>, not for the worker. The token in the following example is the manager token. Please note that my first manager's IP is <kbd>172.31.4.52</kbd> during this setup. Replace it with your IP address:</p>
<pre><strong>$ docker swarm join --token SWMTKN-1-5rvucdwofoam27qownciovd0sngpm31825r2wbdz1jdneiyfyt-b5bdh4i2jzev4aq4oid1pubi6 172.31.4.52:2377</strong></pre>
<p>For these first three nodes, do not forget to label them as managers to help you remember.</p>
<p>Here, please make sure that <kbd>docker info</kbd> shows the list of managers, containing all their private IP addresses. We use <kbd>grep -A3</kbd> to see the next three lines after the target:</p>
<pre><strong>$ docker info | grep -A3 "Manager Addresses:"</strong><br/><strong>Manager Addresses:</strong><br/><strong> 172.31.0.153:2377</strong><br/><strong> 172.31.1.223:2377</strong><br/><strong> 172.31.4.52:2377</strong></pre>
<p>Or, if you are familiar with the <kbd>jq</kbd> command, try the following:</p>
<pre><strong>$ docker info --format="{{json .Swarm.RemoteManagers}}" | jq -r .[].Addr</strong><br/><strong>172.31.4.52:2377</strong><br/><strong>172.31.1.223:2377</strong><br/><strong>172.31.0.153:2377</strong></pre>
<p>The <kbd>docker info</kbd> command also accepts <kbd>--format</kbd> to let us customize the output. In the previous example, we used the JSON method provided by the template to generate JSON output. Then we used <kbd>jq</kbd> to query the IP addresses of all the Swarm managers. The combination of JSON templating and <kbd>jq</kbd> will be a great tool to build our own set of Docker-based scripts for operating clusters in the long term.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Workers on Spot instances</h1>
                
            
            
                
<p>Then, we will provision another three nodes as a fleet of Spot instances. Here, in the following screenshot, it shows the setup to request a fleet of three Spot instances. Choose the Request and Maintain option, then set the Target capacity to <kbd>3</kbd> instances:</p>
<div><img src="img/69a23956-d79a-4188-964d-65b163e10155.png"/></div>
<p>Figure 7.11: Requesting and maintaining a fleet of 3 instances</p>
<p>We configure the setup script to install Docker, join the node to the cluster, and set up the network driver upon the instance creation. The setup must be put into the User data section of the fleet setup, as shown in the following screenshot:</p>
<div><img src="img/3c4f9f38-655a-441b-bbdd-8119fd64fc6d.png"/></div>
<p>Figure 7.12: Putting join instructions into the request's user data</p>
<p>Here's the script used in the User data section. Please replace <kbd>$TOKEN</kbd> with your worker's token, and <kbd>$MANAGER_IP</kbd> with one of your manager's private IP addresses:</p>
<pre><strong>#!/bin/bash</strong><br/><strong>curl -sSL https://get.docker.com | sh</strong><br/><strong>service docker start</strong><br/><strong>usermod -aG docker ubuntu</strong><br/><strong>docker swarm join --token $TOKEN $MANAGER_IP:2377</strong><br/><strong>docker plugin install --grant-all-permissions weaveworks/net-plugin:2.1.3</strong></pre>
<p>Now, we wait until the fleet request is fulfilled.</p>
<p>If we get into the first manager, we could check the current nodes in the cluster with the <kbd>docker node ls</kbd> command. If everything is OK, we should have six nodes in the cluster:</p>
<pre><strong>$ docker node ls</strong><br/><strong>ID          HOSTNAME          STATUS  AVAILABILITY   MANAGER STATUS</strong><br/><strong>btul0hbd    ip-172-31-11-209  Ready   Active </strong><br/><strong>etm8veip    ip-172-31-8-157   Ready   Active </strong><br/><strong>iwl4pxnf *  ip-172-31-4-52    Ready   Active         Leader</strong><br/><strong>rsqsflmv    ip-172-31-1-223   Ready   Active         Reachable</strong><br/><strong>uxd36bok    ip-172-31-15-229  Ready   Active </strong><br/><strong>xn7fz2q1    ip-172-31-0-153   Ready   Active         Reachable</strong></pre>
<p>With this technique, we can easily scale the cluster by simply adjusting the number of Spot instances.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Working with the network plugin</h1>
                
            
            
                
<p>As we can see in the User data section in the fleet setup, there will be a line of the script that installs the network plugin for us. It is the WeaveWorks network plugin. The WeaveWorks network plugin uses the information from the <kbd>docker info</kbd> command to list the IP addresses of all the Swarm managers. The plugin then uses these IP addresses to bootstrap the network mesh.</p>
<p>The WeaveWorks network plugin must be installed only after you successfully form the set of managers in the cluster.</p>
<p>We use WeaveWorks network plugin 2.1.3. This is the most stable version of it at the time of writing. It is also recommended upgrading to the next minor versions of this plugin, if available.</p>
<p>To install the network plugin, we use the <kbd>docker plugin install</kbd> command:</p>
<pre><strong>$ docker plugin install --grant-all-permissions weaveworks/net-plugin:2.1.3</strong><br/><strong>2.1.3: Pulling from weaveworks/net-plugin</strong><br/><strong>82e7025f1f50: Download complete </strong><br/><strong>Digest: sha256:84e5ff14b54bfb9798a995ddd38956d5c34ddaa4e48f6c0089f6c0e86f1ecfea</strong><br/><strong>Status: Downloaded newer image for weaveworks/net-plugin:2.1.3</strong><br/><strong>Installed plugin weaveworks/net-plugin:2.1.3</strong></pre>
<p>We use <kbd>--grant-all-permissions</kbd> just to automate the installation step. Without this parameter, we must manually grant the permissions required by each plugin.</p>
<p>We need to install a plugin for every single node in the cluster, which means we need to do this six times for our six boxes.</p>
<p>We could check to see whether the network plugin is installed correctly using the following command:</p>
<pre><strong>$ docker plugin ls</strong><br/><strong>ID            NAME                         DESCRIPTION                  ENABLED</strong><br/><strong>f85f0fca2af9  weaveworks/net-plugin:2.1.3  Weave Net plugin for Docker  true</strong></pre>
<p>The <kbd>ENABLED</kbd> status of the plugin will be <kbd>true</kbd>, meaning that it is currently active. To check the status of the WeaveWork plugin and its network mesh, the plain text status could be CURLed from <kbd>localhost:6782/status</kbd>. The following status information was obtained from a worker node. We can check the number of connections between peers, or a number of peers, for example, from that URL:</p>
<pre><strong>$ curl localhost:6782/status</strong><br/><strong>        Version: 2.1.3</strong><br/><br/><strong>        Service: router</strong><br/><strong>       Protocol: weave 1..2</strong><br/><strong>           Name: e6:cc:59:df:57:72(ip-172-31-11-209)</strong><br/><strong>     Encryption: disabled</strong><br/><strong>  PeerDiscovery: enabled</strong><br/><strong>        Targets: 3</strong><br/><strong>    Connections: 5 (5 established)</strong><br/><strong>          Peers: 6 (with 30 established connections)</strong><br/><strong> TrustedSubnets: none</strong><br/><br/><strong>        Service: ipam</strong><br/><strong>         Status: idle</strong><br/><strong>          Range: 10.32.0.0/12</strong><br/><strong>  DefaultSubnet: 10.32.0.0/12</strong><br/><br/><strong>        Service: plugin (v2)</strong></pre>
<p>The previous example shows us having six peers with five connections each. The IP range and the default subnet are important information for us to use when we create Docker networks. The IP range is <kbd>10.32.0.0/12</kbd>, so if we create a network with subnet <kbd>10.32.0.0/24</kbd>, it will be valid, while <kbd>10.0.0.0/24</kbd> will be invalid, for example.</p>
<p>The following figure illustrates our WeaveWorks network topology. Each node has five connections to another five nodes, as shown by solid lines from an <strong>mg</strong> node pointing to others. To make the diagram comprehensible, it shows only an <strong>mg</strong> node and another <strong>wk</strong> node connecting their five lines to the rest of the peers in the cluster:</p>
<div><img src="img/7d482584-1a08-4c7e-959a-f6fc0d0c7337.png" style="width:25.42em;height:23.92em;"/></div>
<p>Figure 7.13: Swarm nodes connecting together via a WeaveWorks full-mesh network</p>
<p>For advanced troubleshooting, we could check the plugin's running process, <kbd>weaver</kbd>:</p>
<pre><strong>$ ps aux | grep weaver</strong><br/><strong>root   4097   0.0 3.4 418660 34968 ? Ssl 06:15 0:06 /home/weave/weaver --port=6783 --datapath=datapath --host-root=/host --proc-path=/host/proc --http-addr=127.0.0.1:6784 --status-addr=0.0.0.0:6782 --no-dns --ipalloc-range=10.32.0.0/12 --nickname ip-172-31-11-209 --log-level=debug --db-prefix=/host/var/lib/weave/weave --plugin-v2 --plugin-mesh-socket= --docker-api= 172.31.4.52 172.31.1.223 172.31.0.153</strong></pre>
<p>As you can see from grepping the output of <kbd>ps</kbd>, the final parts of the command are the list of Swarm manager IP addresses. If it looks like this, our networking layer is good to go. But if you do not see the list of manager IP addresses here, remove the plugin and start over again.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a network</h1>
                
            
            
                
<p>When we prepare a network with the WeaveWorks driver, please keep in mind that we always need to specify the <kbd>--subnet</kbd> and <kbd>--gateway</kbd> parameters as we do not use the default subnet value provided by the Docker's libnetwork. We need to make a network attachable, with <kbd>--attachable</kbd>, to allow containers started using <kbd>docker run</kbd> command attach to the network. Without this option, only Swarm services, started by <kbd>docker service create</kbd>, are allowed to join the network.</p>
<p>For example, we can create a <em>class C</em> network using the following command:</p>
<pre><strong>$ docker network create -d weaveworks/net-plugin:2.1.3 \</strong><br/><strong>  --subnet=10.32.0.0/24 \</strong><br/><strong>  --gateway=10.32.0.1 \</strong><br/><strong>  --attachable my_net</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating an operational control plane</h1>
                
            
            
                
<p>An operational control plane is where we deploy operator containers to help operate the cluster. It is a concept that stems from the CoreOS's operator pattern, <a href="https://coreos.com/blog/operators">https://coreos.com/blog/operators</a>.</p>
<p>Firstly, we create the control network to allow operator agents connecting to the manager nodes. Just name it <kbd>control</kbd>. We create this network to be a size of <em>class C</em>. So please be careful that the number of operator containers does not go beyond <kbd>255</kbd>:</p>
<pre><strong>$ docker network create \</strong><br/><strong>    --driver weaveworks/net-plugin:2.1.3 \</strong><br/><strong>    --subnet 10.32.100.0/24 \</strong><br/><strong>    --attachable \</strong><br/><strong>    control</strong></pre>
<p>Operators in the <kbd>control</kbd> plane usually require access to Docker APIs to observe the cluster's state, to decide what to do, and to make changes back to the cluster.</p>
<p>To make the Docker API accessible via every operator inside the same control network, we deploy the <kbd>docker-api</kbd> service in the control plane.</p>
<p>We use <kbd>rancher/socat-docker</kbd> as the image of the <kbd>docker-api </kbd>service for the control plane because it is widely used and has proven stable for production. The <kbd>docker-api</kbd> will be deployed globally on every manager, using <kbd>node.role==manager</kbd>. The endpoint's mode will be set to <kbd>dnsrr</kbd> as each <kbd>docker-api</kbd> instance is stateless and the Docker managers are already taking care of the whole cluster state. So the <kbd>vip</kbd> endpoint mode is not necessary here.</p>
<p>Each <kbd>docker-api</kbd> instance binds to <kbd>/var/run/docker.sock</kbd> on their Docker host to connect to their local manager:</p>
<pre><strong>$ docker service create \</strong><br/><strong>  --name=docker-api \</strong><br/><strong>  --mode=global \</strong><br/><strong>  --endpoint-mode=dnsrr \</strong><br/><strong>  --network control \</strong><br/><strong>  --constraint "node.role==manager" \</strong><br/><strong>  --mount "type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock" \</strong><br/><strong>  rancher/socat-docker</strong></pre>
<p>We will run an operator container called <strong>service balancer</strong> as an example of using the operator pattern in production.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Service balancer operator</h1>
                
            
            
                
<p>Service rebalancing has been one of the requested features for Docker. However, it is better to have this feature running outside the orchestrator and to run it as an operator container.</p>
<p>The problem is that after a new node joins the cluster, we usually rebalance the running services to spread loads across the cluster. The main reason this feature is not built into the orchestrator is because it is application-specific. Also, if the cluster keeps rebalancing everything when nodes dynamically come and go, running services may be broken all the time, and not in a good enough condition to serve requests.</p>
<p>However, if we implement this kind of feature as an operator container, we can optionally disable it when necessary as it is running outside the orchestrator. Also, we can selectively pick only particular services to be rebalanced.</p>
<p>The service balancer is currently available as <kbd>chanwit/service-balancer</kbd> on Docker's hub. We will be running only one instance of service balancer on any manager:</p>
<pre><strong>$ docker service create \</strong><br/><strong> --name service-balancer \</strong><br/><strong> --network control \</strong><br/><strong> --constraint node.role==manager \</strong><br/><strong> --replicas 1 \</strong><br/><strong> chanwit/service-balancer</strong></pre>
<p>Something to consider when using the auto-rebalancer is that <kbd>--update-delay</kbd> must be set to greater than the startup time of each task. This is really important, especially for Java-based services. This delay should be large enough, at least larger than the interval used by the health checking mechanism.</p>
<p>Also, for the safest result, the value of <kbd>--update-parallelism</kbd> should start at <kbd>1</kbd>, and gradually increase when the system can stably serve the requests.</p>
<p>To allow a service to automatically rebalance, the service balancer operator checks the service's label <kbd>rebalance.on.node.create=true</kbd>. If this label is present on the service, it will be rebalanced every time a new node is added to the cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logging</h1>
                
            
            
                
<p>When it comes to logging, one popular solution is to set up an Elasticsearch stack. The natural combination could be <strong>Elasticsearch</strong>-<strong>Logstash</strong>-<strong>Kibana</strong> (<strong>ELK</strong>).</p>
<p>We use an ELK stack from <a href="https://github.com/deviantony/docker-elk">https://github.com/deviantony/docker-elk</a> with modification to improve it by adding Docker Swarm configs, and to deploy each of them independently. The original Docker Compose file, <kbd>docker-compose.yml</kbd>, are split into three YML files, each for <strong>Elasticsearch</strong>, <strong>Kibana</strong>, and <strong>Logstash</strong>, respectively<strong>. </strong>Services must be deployed this way because we do not want to bring the whole logging system down when we change each service's configs. The fork used in this chapter is available at <a href="https://github.com/chanwit/docker-elk">https://github.com/chanwit/docker-elk</a>.</p>
<p>The following figure shows what the stack will look like. All ELK components will be in <strong>elk_net</strong>. The <strong>Logstash</strong> instance will be exposed on port <strong>5000</strong>. On each Docker host, its local <strong>Logspout</strong> agent will forward log messages from the Docker host to the <strong>Logstash</strong> instance. <strong>Logstash</strong> will then transform each message and store them in <strong>ElasticSearch</strong>. Finally, a user can access <strong>Kibana</strong> via port <strong>5601</strong> to visualize all the logs:</p>
<div><img src="img/9d8fb58e-8213-4753-80eb-d8ca97450c92.png"/></div>
<p>Figure 7.14: An ELK stack block diagram for cluster-wide logging</p>
<p>We start with the preparation of a dedicated network for our ELK stack. We name this network <kbd>elk_net</kbd> and use it for all ELK components:</p>
<pre><strong>docker network create \</strong><br/><strong>  --driver weaveworks/net-plugin:2.1.3 \</strong><br/><strong>  --subnet 10.32.200.0/24 \</strong><br/><strong>  --attachable \</strong><br/><strong>  elk_net</strong></pre>
<p>The following is the source of <kbd>elasticsearch.yml</kbd>. We use Docker compose YAML specification version 3.3 throughout the chapter. This is the minimum requirement, as we will use Docker Swarm configs to manage all configuration files for us:</p>
<pre>version: '3.3'<br/><br/>configs:<br/>  elasticsearch_config:<br/>    file: ./elasticsearch/config/elasticsearch.yml<br/><br/>services:<br/>  elasticsearch:<br/>    build:<br/>      context: elasticsearch/<br/>    image: chanwit/elasticsearch:6.1<br/>    configs:<br/>      - source: elasticsearch_config<br/>        target: /usr/share/elasticsearch/config/elasticsearch.yml<br/>    environment:<br/>      ES_JAVA_OPTS: "-Xmx512m -Xms512m"<br/><br/>networks:<br/>  default:<br/>    external:<br/>      name: elk_net</pre>
<p>It is the requirement that <kbd>docker stack</kbd> needs the image name to be specified before it can be deployed. So, we need to build the container image using <kbd>docker-compose</kbd> first.</p>
<p>We use <kbd>docker-compose</kbd> only for building images.</p>
<p>Let's do it! We use <kbd>docker-compose</kbd> build to prepare images defined in the YML file. The <kbd>docker-compose</kbd> command also tags images for us too. As we have a separate YML file each service, we use <kbd>-f</kbd> to tell <kbd>docker-compose</kbd> to build the correct file:</p>
<pre><strong>$ docker-compose -f elasticsearch.yml build</strong></pre>
<p>When the image is ready, we can simply deploy the stack, <kbd>es</kbd>, using the following command:</p>
<pre><strong>$ docker stack deploy -c elasticsearch.yml es</strong></pre>
<p>Next, we move to the preparation and deployment of Kibana.</p>
<p>Here's the stack YML file for Kibana. We have <kbd>kibana_config</kbd> pointing to our Kibana configuration. The Kibana port <kbd>5601</kbd> is published using Swarm's host mode to bypass the ingress layer. Please remember that we do not really have the default ingress layer in our cluster. As previously mentioned, we use Træfik as our new ingress:</p>
<pre>version: '3.3'<br/><br/>configs:<br/>  kibana_config:<br/>    file: ./kibana/config/kibana.yml<br/><br/>services:<br/>  kibana:<br/>    build:<br/>      context: kibana/<br/>    image: chanwit/kibana:6.1<br/>    configs:<br/>      - source: kibana_config<br/>        target: /usr/share/kibana/config/kibana.yml<br/>    ports:<br/>      - published: 5601<br/>        target: 5601<br/>        mode: host<br/><br/>networks:<br/>  default:<br/>    external:<br/>      name: elk_net</pre>
<p>Similar to Elasticsearch, now the Kibana image can be prepared using the <kbd>docker-compose build</kbd> command:</p>
<pre><strong>$ docker-compose -f kibana.yml build</strong></pre>
<p>After that, we deploy Kibana with the stack name <kbd>kb</kbd>:</p>
<pre><strong>$ docker stack deploy -c kibana.yml kb</strong></pre>
<p>With Logstash, there are two configuration files to consider. The most important one is the pipeline config, <kbd>logstash_pipeline_config</kbd>. We need to add custom rules to this file for log message transformation. It keeps changing, unlike the first two components of ELK. Logstash listens to port <kbd>5000</kbd>, both for TCP and UDP, inside <kbd>elk_net</kbd>. We will later plug Logspout into this network to convey log messages from Docker daemons to this Logstash service:</p>
<pre>version: '3.3'<br/><br/>configs:<br/>  logstash_config:<br/>    file: ./logstash/config/logstash.yml<br/>  logstash_pipeline_config:<br/>    file: ./logstash/pipeline/logstash.conf<br/><br/>services:<br/>  logstash:<br/>    build:<br/>      context: logstash/<br/>    image: chanwit/logstash:6.1<br/>    configs:<br/>      - source: logstash_config<br/>        target: /usr/share/logstash/config/logstash.yml<br/>      - source: logstash_pipeline_config<br/>        target: /usr/share/logstash/pipeline/logstash.conf<br/>    environment:<br/>      LS_JAVA_OPTS: "-Xmx256m -Xms256m"<br/><br/>networks:<br/>  default:<br/>    external:<br/>      name: elk_net</pre>
<p>The next steps are to build and deploy, similar to the first two components:</p>
<pre><strong>$ docker-compose -f logstash.yml build</strong><br/><strong>$ docker stack deploy -c logstash.yml log</strong></pre>
<p>We started these three components as separate stacks linked together via <kbd>elk_net</kbd>. To check if all components are running, simply check this using <kbd>docker stack ls</kbd>:</p>
<pre><strong>$ docker stack ls</strong><br/><strong>NAME      SERVICES</strong><br/><strong>es        1</strong><br/><strong>kb        1</strong><br/><strong>log       1</strong></pre>
<p>Finally, we can redirect all logs from each Docker daemon to the ELK stack, the central service, using Logspout. This can be done by attaching each local <kbd>logspout</kbd> container to the <kbd>elk_net</kbd> so that they will all be able to connect to a Logstash instance inside the network. We start each Logspout using the following command:</p>
<pre><strong>$ docker run -d \</strong><br/><strong>  --name=logspout \</strong><br/><strong>  --network=elk_net \</strong><br/><strong>  --volume=/var/run/docker.sock:/var/run/docker.sock \</strong><br/><strong>  gliderlabs/logspout \</strong><br/><strong>  syslog+tcp+udp://logstash:5000</strong></pre>
<p>We are now able to log all messages via Logspout to Logstash, storing them in Elasticsearch, and visualizing them with Kibana.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scripting Docker with Golang</h1>
                
            
            
                
<p>When it comes to operating and administrating Docker, we could do everything by controlling the cluster via the <kbd>docker</kbd> CLI using the <kbd>jq</kbd> command. Another powerful and very flexible way is to control the cluster via scripting. The most suitable programming language for scripting Docker cluster is, of course, Golang.</p>
<p>Why not Python? How could Golang, a statically compiled language, come to fit scripting?</p>
<ul>
<li>First, Go is the language that Docker is written in. The Docker library written in the Go language is the same piece of codes used by Docker itself. So, the scripts written using this library will be naturally in high quality and greatly reliable.</li>
<li>Second, the language constructs and the idioms fit the way Docker works. For example, the Go programming language has the channel construct and it fits nicely for processing event messages emitted by the Docker cluster.</li>
<li>Third, the Go compiler is incredibly fast. Also, once all related libraries get compiled, the compilation time is greatly reduced. We can normally use it to run scripts just like other scripting language interpreters.</li>
</ul>
<p>In this section, we will discuss how to use scripts written in Golang to control Docker directly via its API. This will become a powerful tool for taking care of running the cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the tool</h1>
                
            
            
                
<p>Installing the Go compiler and making it ready to use is sometimes tricky. However, <strong>Golang Version Manager</strong> (<strong>GVM</strong>), is a tool that helps with installing and uninstalling different Go versions on the same machine. It also helps manage <kbd>GOPATH</kbd> effectively.</p>
<p>What is GOPATH? It is defined as follows in Wikipedia:</p>
<p>"The GOPATH environment variable is used to specify directories outside of $GOROOT that contain the source for Go projects and their binaries."</p>
<p>To start using GVM, we first install the <kbd>gvm</kbd> command using the snippet provided on <a href="https://github.com/moovweb/gvm">https://github.com/moovweb/gvm</a>. It can be installed with a single command:</p>
<pre><strong>$ bash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)</strong></pre>
<p>Now we have GVM installed already, and we continue by installing Go.</p>
<p>It is great to use Go's most recent version-1.9.3. The command to install is, of course, <kbd>gvm install</kbd>. We pass the <kbd>-B</kbd> parameter to the <kbd>install</kbd> command, so that it will download and use only the binary of the Go distribution:</p>
<pre><strong>$ gvm install go1.9.3 -B</strong><br/><strong>Installing go1.9.3 from binary source</strong></pre>
<p>Next, if we choose to go with Go v1.9.3 when taking care of our cluster, we should make it the default version. Issue the <kbd>gvm use</kbd> command with the <kbd>--default</kbd> parameter to do so:</p>
<pre><strong>$ gvm use go1.9.3 --default</strong><br/><strong>Now using version go1.9.3</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Making Go scriptable</h1>
                
            
            
                
<p>Next, prepare the next tool, <kbd>gorun</kbd>, to make a Go program scriptable. With <kbd>gorun</kbd>, you can add a shebang to the first line of the script, as shown in the following command:</p>
<pre><strong>#!/usr/bin/env gorun</strong></pre>
<p>The normal Go program will then be allowed to execute directly from the shell.</p>
<p>To install <kbd>gorun</kbd>, just do <kbd>go get</kbd>. The <kbd>gorun</kbd> binary will now be available under the path provided by the current <kbd>go1.9.3</kbd> managed by GVM. Please note that if you switch Go version with GVM, you need to do <kbd>go get</kbd> again:</p>
<pre><strong>$ go get github.com/erning/gorun</strong></pre>
<p>We could install all necessary libraries for controlling Docker programmatically by installing the Docker client library itself:</p>
<pre><strong>$ go get github.com/docker/docker/client</strong></pre>
<p>If nothing goes wrong, we will be ready to start writing a Golang script.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simple Docker script</h1>
                
            
            
                
<p>Let's write a simple script that interacts with Docker:</p>
<pre>#!/usr/bin/env gorun<br/>package main<br/><br/>import (<br/>  "fmt"<br/>  "context"<br/><br/>  "github.com/docker/docker/client"<br/>)<br/><br/>func main() {<br/>  ctx := context.Background()<br/><br/>  cli, err := client.NewClient(client.DefaultDockerHost, "1.30", nil, nil)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/><br/>  info, err := cli.Info(ctx)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/><br/>  fmt.Println(info.ServerVersion)<br/>}</pre>
<p>First, the script must have the first line with shebang and <kbd>gorun</kbd>. Second, import a line with Docker's client library, <kbd>github.com/docker/docker/client</kbd>. Although, Docker has been moved to <kbd>github.com/moby/moby</kbd>, but we still need to import all related library using the <kbd>docker/docker</kbd> repository name. Just <kbd>go get github.com/docker/docker/client</kbd> and everything is still working fine for us.</p>
<p>Then we start programming our cluster by creating a client while also setting the API version to 1.30. This script then calls <kbd>cli.Info(ctx)</kbd> to obtain the engine's information from the Docker daemon, as the <kbd>info</kbd> variable. It simply prints out the version of the Docker daemon we're talking to. The version information is stored in <kbd>info.ServerVersion</kbd>.</p>
<p>Save the script to a file named <kbd>server-version</kbd>. We can now run it as a normal shell script:</p>
<pre><strong>$ chmod +x ./server-version</strong><br/><strong>$ ./server-version</strong><br/><strong>17.06.2-ce</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Script reacting to Docker events</h1>
                
            
            
                
<p class="mce-root">Next, we will write a script to monitor changes in the Docker cluster and then do a print out when a node is updated:</p>
<pre>#!/usr/bin/env gorun<br/>package main<br/><br/>import (<br/>  "context"<br/>  "fmt"<br/><br/>  "github.com/docker/docker/api/types"<br/>  "github.com/docker/docker/api/types/filters"<br/>  "github.com/docker/docker/client"<br/>)<br/><br/>func main() {<br/>  ctx := context.Background()<br/><br/>  cli, err := client.NewClient(client.DefaultDockerHost, "1.30", nil, nil)<br/>  if err != nil {<br/>    panic(err)<br/>  }<br/><br/>  filter := filters.NewArgs(filters.Arg("type", "node"))<br/>  ch, _ := cli.Events(ctx, types.EventsOptions{<br/>    Filters: filter,<br/>  })<br/><br/>  for {<br/><br/>    fmt.Println("Waiting for event ...")<br/>    message := &lt;-ch<br/>    action := message.Action<br/><br/>    switch action {<br/>    case "create":<br/>      fmt.Println(" - New node added.")<br/>    case "update":<br/>      fmt.Println(" - Node updated.")<br/>    case "remove":<br/>      fmt.Println(" - Node removed.")<br/>    }<br/><br/>  }<br/><br/>}</pre>
<p>This is also a script executed by <kbd>gorun</kbd>. The script starts by creating a Docker client CLI pointing to the local socket, <kbd>/var/run/docker.sock</kbd>.</p>
<p>Then it creates a filter, the <kbd>filter</kbd> variable. This filter makes the event emitter select only the type of events we are interested in, in this case, when the <kbd>type</kbd> of events is <kbd>node</kbd>. This is equivalent to passing <kbd>--filter type=node</kbd> to the command line. The <kbd>cli.Events</kbd> method will return a Go channel for retrieving messages. A message is then retrieved inside the <kbd>for</kbd> loop. The program will be automatically blocked if the message is not available in the channel. So the script just becomes a single-thread style and easy to program.</p>
<p>Inside the loop, we can manipulate information inside the message, for example, checking the action of a certain event. Normally, most types of event contain three possible actions, <kbd>create</kbd>, <kbd>update</kbd>, and <kbd>remove</kbd>. For a node, <kbd>create</kbd> means there is a new node added to the cluster. The <kbd>update</kbd> action means something has changed on a certain node. The <kbd>remove</kbd> action means the node is removed from the cluster.</p>
<p>Just save this script to <kbd>./node-event</kbd>, then <kbd>chmod +x</kbd> it.</p>
<pre>$ chmod +x ./node-event</pre>
<p>The <kbd>chmod</kbd> command will change executable bits of the script. With these bits, the Linux system will be able to detect that the file should be executed. Then, it will tell <kbd>gorun</kbd> to take care of that execution.</p>
<p>Try changing some properties of the current working node. We may observe that the text <kbd>- Node updated.</kbd> will be printed out.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>Please try to answer the following questions without going back to read the chapter's content:</p>
<ol>
<li>List at least three components described in the stable cluster configuration.</li>
<li>Why are retry and circuit breaker important?</li>
<li>How do we replace the default ingress layer with the new one?</li>
<li>How can we install the network plugin?</li>
<li>What is the most frontal part of the ELK stack?</li>
<li>Why is the Go language suitable for scripting the Docker system?</li>
<li>How do we listen to Docker events of a certain type?</li>
<li>How do we set up a control plane?</li>
<li>What is the operator pattern? Why is it important?</li>
<li>What is the characteristic of Spot instances that makes them cheaper than normal instances?</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter discussed various topics on how to prepare and operate a Docker cluster with a stable configuration. We introduced a low-cost alternative to Lambda by deploying a Docker cluster on Spot instances. This chapter also introduced the concept of CoreOS's operator pattern, and how to use it practically to auto-balance the tasks of our cluster.</p>
<p>When it comes to logging, the ELK stack is usually the first choice. This chapter also discussed how to efficiently prepare ELK on Docker Swarm and it ended with how to operate a cluster with Golang scripts, the scripting technique that can fully leverage Docker and its ecosystem.</p>
<p>In the next chapter, we will put all FaaS platforms into the same cluster and make them work together to demonstrate a use case of event-driven FaaS systems over a Docker cluster.</p>
<p> </p>


            

            
        
    </body></html>