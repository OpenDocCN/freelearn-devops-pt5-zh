- en: Chapter 16. Centralized Logging and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *I have so much chaos in my life, it''s become normal. You become used
    to it. You have just to relax, calm down, take a deep breath and try to see how
    you can make things work rather than complain about how they''re wrong.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Tom Welling* |'
  prefs: []
  type: TYPE_TB
- en: Our exploration of DevOps practices and tools led us towards clustering and
    scaling. As a result, we developed a system that allows us to deploy services
    to a cluster, in an easy and efficient way. The result is an ever increasing number
    of containers running on a cluster consisting of, potentially, many servers. Monitoring
    one server is easy. Monitoring many services on a single server poses some difficulties.
    Monitoring many services on many servers requires a whole new way of thinking
    and a new set of tools. As you start embracing microservices, containers, and
    clusters, the number of deployed containers will begin increasing rapidly. The
    same holds true for servers that form the cluster. We cannot, anymore, log into
    a node and look at logs. There are too many logs to look at. On top of that, they
    are distributed among many servers. While yesterday we had two instances of a
    service deployed on a single server, tomorrow we might have eight instances deployed
    to six servers. The same holds true for monitoring. Old tools, like Nagios, are
    not designed to handle constant changes in running servers and services. We already
    used Consul that provides a different, not to say new, approach to managing near
    real-time monitoring and reaction when thresholds are reached. However, that is
    not enough. Real-time information is valuable to detect that something is wrong,
    but it does not give us information why the failure happened. We can know that
    a service is not responding, but we cannot know why.
  prefs: []
  type: TYPE_NORMAL
- en: We need historical information about our system. That information can be in
    the form of logs, hardware utilization, health checking, and many other things.
    The need to store historical data is not new and has been in use for a long time.
    However, the direction that information travels changed over time. While, in the
    past, most solutions were based on a centralized data collectors, today, due to
    very dynamic nature of services and servers, we tend to have data collectors decentralized.
  prefs: []
  type: TYPE_NORMAL
- en: What we need for cluster logging and monitoring is a combination of decentralized
    data collectors that are sending information to a centralized parsing service
    and data storage. There are plenty of products specially designed to fulfill this
    requirement, ranging from on-premise to cloud solutions, and everything in between.
    FluentD, Loggly, GrayLog, Splunk, and DataDog are only a few of the solutions
    we can employ. I chose to show you the concepts through the ELK stack (ElasticSearch,
    LogStash, and Kibana). The stack has the advantage of being free, well documented,
    efficient, and widely used. ElasticSearch established itself as one of the best
    databases for real-time search and analytics. It is distributed, scalable, highly
    available, and provides a sophisticated API. LogStash allows us to centralize
    data processing. It can be easily extended to custom data formats and offers a
    lot of plugins that can suit almost any need. Finally, Kibana is an analytics
    and visualization platform with intuitive interface sitting on top of ElasticSearch.
    The fact that we'll use the ELK stack does not mean that it is better than the
    other solutions. It all depends on specific use cases and particular needs. I'll
    walk you through the principles of centralized logging and monitoring using the
    ELK stack. Once those principles are understood, you should have no problem applying
    them to a different stack if you choose to do so.
  prefs: []
  type: TYPE_NORMAL
- en: We switched the order of things and chose the tools before discussing the need
    for centralized logging. Let's remedy that.
  prefs: []
  type: TYPE_NORMAL
- en: The Need for Centralized Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, log messages are written to files. That is not to say that files
    are the only, nor the most efficient way of storing logs. However, since most
    teams are using file-based logs in one form or another, for the time being, I'll
    assume that is your case as well.
  prefs: []
  type: TYPE_NORMAL
- en: If we are lucky, there is one log file per a service or application. However,
    more often than not, there are multiple files into which our services are outputting
    information. Most of the time, we do not care much what is written in logs. When
    things are working well, there is not much need to spend valuable time browsing
    through logs. A log is not a novel we read to pass the time, nor it is a technical
    book we spend time with as a way to improve our knowledge. Logs are there to provide
    valuable info when something, somewhere, went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The situation seems to be simple. We write information to logs that we ignore
    most of the time, and when something goes wrong, we consult them and find the
    cause of the problem in no time. At least, that's what many are hoping for. The
    reality is far more complicated than that. In all but most trivial systems, the
    debugging process is much more complex. Applications and services are, almost
    always, interconnected, and it is often not easy to know which one caused the
    problem. While it might manifest in one application, investigation often shows
    that the cause is in another. For example, a service might have failed to instantiate.
    After some time spent browsing its logs, we might discover that the cause is in
    the database. The service could not connect to it and failed to launch. We got
    the symptom, but not the cause. We need to switch to the database log to find
    it out. With this simple example, we already got to the point where looking at
    one log is not enough.
  prefs: []
  type: TYPE_NORMAL
- en: With distributed services running on a cluster, the situation complicates exponentially.
    Which instance of the service is failing? Which server is it running on? What
    are the upstream services that initiated the request? What is the memory and hard
    disk usage in the node where the culprit resides? As you might have guessed, finding,
    gathering, and filtering the information needed for the successful discovery of
    the cause is often very complicated. The bigger the system, the harder it gets.
    Even with monolithic applications, things can easily get out of hand. If (micro)services
    approach is adopted, those problems are multiplied. Centralized logging is a must
    for all but simplest and smallest systems. Instead, many of us, when things go
    wrong, start running from one server to another, jumping from one file to the
    other. Like a chicken with its head cut off - running around with no direction.
    We tend to accept the chaos logging creates, and consider it part of our profession.
  prefs: []
  type: TYPE_NORMAL
- en: 'What do we look for in centralized logging? As it happens, many things, but
    the most important are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A way to parse data and send them to a central database in near real-time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The capacity of the database to handle near real-time data querying and analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A visual representation of the data through filtered tables, dashboards, and
    so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We already choose the tools that will be able to fulfill all those requirements
    (and more). The ELK stack (LogStash, ElasticSearch, and Kibana) can do all that.
    As in the case of all other tools we explored, this stack can easily be extended
    to satisfy the particular needs we'll set in front of us.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a vague idea what we want to accomplish, and have the tools
    to do that, let us explore a few of the logging strategies we can use. We'll start
    with the most commonly used scenario and, slowly, move towards more complicated
    and more efficient ways to define our logging strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without further ado, let''s create the environments we''ll use to experiment
    with centralized logging and, later on, monitoring. We''ll create three nodes.
    You should already be familiar with the `cd` and `prod` VMs. The first one will
    be used mainly for provisioning while the second will act as a production server.
    We''ll introduce a new one called `logging`. It will be an imitation of a production
    server aimed at running all the logging and monitoring tools. Ideally, instead
    of a single production server (`prod`), we would run examples against the, let''s
    say, Swarm cluster. That would allow us to see the benefits in a more production-like
    setting. However, since the previous few chapters already stretched limits of
    what could be run on a single laptop, I did not want to risk it and opted for
    a single VM. That being said, all the examples are equally applicable to one,
    ten, hundred, or thousand servers. You should have no problem extending them to
    you entire cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Sending Log Entries to ElasticSearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start by provisioning the `logging` server with the ELK stack (ElasticSearch,
    LogStash, and Kibana). We'll continue using Ansible for provisioning since it
    converted itself into our favorite configuration management tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the elk.yml playbook and explore it while it''s executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The definition of the playbook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `common` and the `docker` roles many times before, so we''ll skip
    them, and jump straight into `elasticsearch` tasks defined in the `roles/elasticsearch/tasks`
    `/main.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to Docker, all we have to do is run the official `elasticsearch` image.
    It exposes its API through the port `9200` and defines a single volume we'll use
    to persist data in the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next in line is the `logstash` role. The tasks set in the `roles/logstash/tas`
    `ks/main.yml` file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'While a big more numerous than the `elasticsearch` tasks, these are still pretty
    straightforward. The tasks create a directory, copy few configuration files we''ll
    use throughout this chapter, and run the official logstash image. Since we''ll
    experiment with quite a few scenarios, different ports need to be exposed and
    defined. The role exposes two volumes. The first one will hold configuration files
    while we''ll use the second as a directory to place some logs. Finally, the task
    creates the link to the `elasticsearch` container and specifies that the `command`
    should start `logstash` with the configuration file defined as the variable. The
    command we used to run the playbook contained the `logstash_config` variable set
    to `file.conf`. Let us take a quick look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'LogStash configurations consist of three main sections: `input`, `output`,
    and `filters`. We''ll skip `filter`, for now, and focus on the other two.'
  prefs: []
  type: TYPE_NORMAL
- en: The `input` section defines one or more log sources. In this case, we defined
    that input should be handled through the file plugin, with the `path` set to `/logs/**/*`.
    One asterisk means any file or directory while two consecutive ones mean any file
    in any directory or subdirectory. The `/logs/**/*` value can be described as any
    file in the `/logs/` directory or any of its subdirectories. Bear in mind that,
    even though we specified only one input, there can, and often are, multiple inputs.
    For more information on all the supported input plugins, please consult the official
    input plugins page.
  prefs: []
  type: TYPE_NORMAL
- en: The `output` section defines the destination of log entries collected through
    the input. In this case, we set two. The first one is using the stdout output
    plugin that will print everything to standard output using `rubydebug` codec.
    Please note that we are using `stdout` only for demonstration purposes so that
    we can quickly see the result. In a production setting, you should probably remove
    it for performance reasons. The second output is more interesting. It uses the
    ElasticSearch output plugin to send all the log entries to the database. Please
    note that the `hosts` variable is set to `db`. Since we linked the `logstash`
    and `elasticsearch` containers, Docker created the `db` entry in the `/etc/hosts`
    file. For more information on all supported output plugins, please consult the
    [https://www.elastic.co/guide/en/logstash/current/output-plugins.html](https://www.elastic.co/guide/en/logstash/current/output-plugins.html)
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration file is probably one of the simplest we could start with.
    Before we see it in action, let us go through the last element in the stack. Kibana
    will provide user interface we can use to interact with ElasticSearch. The tasks
    of the kibana role are defined in the `roles/kibana/tasks/main.yml` file. It contains
    backup restoration tasks that we''ll skip, for now, and concentrate only on the
    part that runs the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Just like the rest of the ELK stack, Kibana has the official Docker image. All
    we have to do is link the container to `elasticsearch`, and expose the port `6501`
    that we'll use to access the UI. We'll see Kibana in action soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we simulate some log entries, we''ll need to enter the `logging` node
    where the ELK stack is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the `/data/logstash/logs` volume is shared with the container, and LogStash
    is monitoring any file inside it, we can create a log with a single entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us take a look at LogStash output and see what happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that it might take a few seconds until the first log entry is processed,
    so, if the `docker logs` command did not return anything, please re-execute it.
    All new entries to the same file will be processed much faster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, LogStash processed our `my first log entry` and added a few
    additional pieces of information. We got the timestamp, host name, and the path
    of the log file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a few more entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `docker logs` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, all three log entries were processed by LogStash, and the time
    has come to visualize them through Kibana. Please open `http://` `10.100.198.202:5601/`
    from a browser. Since this is the first time we run Kibana, it will ask us to
    configure an index pattern. Luckily, it already figured out what the index format
    is (`logstash-*`), as well as which field contains timestamps (`@timestamp`).
    Please click the **Create** button, followed with **Discover** located in the
    top menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sending Log Entries to ElasticSearch](img/B05848_16_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-01 – Kibana Discover screen with a few log entries
  prefs: []
  type: TYPE_NORMAL
- en: By default, the **Discover** screen displays all the entries generated in ElasticSearch
    during the last fifteen minutes. We'll explore functions this screen offers later
    on when we produce more logs. For now, please click the arrow on the left-most
    column of one of the log entries. You'll see all the fields LogStash generated
    and sent to ElasticSearch. At the moment, since we are not using any filters,
    those fields are limited to the *message* representing the whole log entry, and
    a few generic fields LogStash generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we used was trivial, and it did not even look like a log entry.
    Let us increase the complexity of our logs. We''ll use a few entries I prepared.
    The sample log is located in the `/tmp/apache.log` file, and it contains a few
    log entries following the Apache format. Its content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Since LogStash is expecting log files in the `/data/logstash/logs/` directory,
    let us copy the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us take a look the output LogStash generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'LogStash might need a few seconds to detect that there is a new file to monitor.
    If the `docker logs` output does not display anything new, please repeat the command.
    The output should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The same data can be observed from Kibana running on `http://10.100.198.202:5601/`.
  prefs: []
  type: TYPE_NORMAL
- en: We just started, and we already accomplished a vast improvement. When something
    fails on a server, we do not need to know which service failed, nor where its
    logs are. We can get all the log entries from that server from a single place.
    Anyone, be it a developer, tester, operator, or any other role, can open Kibana
    running on that node, and inspect all the logs from all services and applications.
  prefs: []
  type: TYPE_NORMAL
- en: The last examples of the Apache log were more production-like than the first
    one we used. However, the entries are still stored as one big message. While ElasticSearch
    is capable of searching almost anything, in almost any format, we should help
    it a bit and try to split this log into multiple fields.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing Log Entries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We mentioned earlier that LogStash configurations consist of three main sections:
    `input`, `output`, and `filters`. The previous examples used only `input` and
    `output`, and the time has come to get introduced to the third section. I already
    prepared an example configuration that can be found in the `roles/logstash/files/file-with-filters.conf`
    file. Its content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `input` and `output` sections are the same as before. The difference is
    the addition of the `filter`. Just like the other two, we can use one or more
    of the plugins. In this case, we specified that the grok filter plugin should
    be used. If for no other reason, the official description of the plugin should
    compel you to at least try it out.
  prefs: []
  type: TYPE_NORMAL
- en: Grok is currently the best way in logstash to parse **crappy unstructured log
    data** into something structured and queryable.
  prefs: []
  type: TYPE_NORMAL
- en: Grok sits on top of regular expressions, and LogStash already comes with quite
    a few patterns. They can be found in the [https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns](https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns)
    repository. In our case, since the log we used matches Apache format that is already
    included, all the had to do is tell LogStash to parse the `message` using the
    `COMBINEDAPACHELOG` pattern. Later on, we'll see how we can combine different
    patterns but, for now, `COMBINEDAPACHELOG` should do.
  prefs: []
  type: TYPE_NORMAL
- en: The second filter we'll be using is defined through the date plugin. It will
    transform the timestamp from log entries into LogStash format.
  prefs: []
  type: TYPE_NORMAL
- en: Please explore filter plugins in more details. Chances are you'll find one,
    or more, that suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s replace the `file.conf` with the `file-with-filters.conf` file, restart
    LogStash, and see how it behaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With the new LogStash configuration, we can add a few more Apache log entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `docker logs` output of the last entry is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the message is still there in its entirety. In addition, this
    time we got quite a few additional fields. The `clientip`, `verb`, `referrer`,
    `agent`, and other data, are all properly separated. This will allow us to filter
    logs much more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Let's open Kibana running on the address `http://10.100.198.202:5601/`. One
    of the things you'll notice is that Kibana claims that no results are found even
    though we just parsed three log entries. The reason behind that is in the second
    filter that transformed the log timestamp to the LogStash format. Since, by default,
    Kibana displays last 15 minutes of logs, and log entries were made during December
    2015, they are indeed older than 15 minutes. Click on the **Last 15 minutes**
    button located in the top-right corner of the screen, select **Absolute** and
    pick the range starting from December 1st to December 31th, 2015\. That should
    give us all logs made during December 2015.
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Go** button and observe that the three logs we just sent to ElasticSearch,
    through LogStash, are displayed on the screen. You'll notice that many new fields
    are available in the right-hand menu. We'll use them later when we explore Kibana
    filters. For now, the important thing to note is that this time we parsed the
    log entries before sending them to ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'By employing LogStash filters, we improved the data that is stored in ElasticSearch.
    The solution relies on the whole ELK stack being installed on the same server
    where logs are, and we can see all the logs we decided to tail from a single interface
    (Kibana). The problem is that the solution is limited to a single server. If,
    for example, we''d have ten servers, we''d need to install ten ELK stacks. That
    would introduce quite a significant overhead on resources. ElasticSearch is memory
    hungry, and LogStash can grab more CPU than what we would be willing to part from.
    Of equal importance is that, while what we have by now is an improvement, it is
    far from ideal. We would still need to know which server produced a problem and,
    potentially, go from one Kibana to another, when trying to cross-reference different
    services and applications involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parsing Log Entries](img/B05848_16_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-02 – ELK stack running on a single server
  prefs: []
  type: TYPE_NORMAL
- en: 'Before I introduce you to the concept of decentralized logs and centralized
    logs parsing, let us remove the LogStash instance and go back to the `cd` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Sending Log Entries to a Central LogStash Instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we did by now is helpful, but it still does not solve the problem of having
    all logs in one place. At the moment, we have all logs from a single server in
    a single location. How can we change that?
  prefs: []
  type: TYPE_NORMAL
- en: One simple solution would be to install LogStash on each server, and configure
    it to send entries to a remote ElasticSearch. At least, that's how most companies
    I worked with solved it. Should we do the same? The answer is no; we shouldn't.
    The problem lies in LogStash itself. While it is an excellent solution for collecting,
    parsing, and outputting logs, it uses too many resources. Having LogStash installed
    on each and every server would result in a huge waste. Instead, we'll use Filebeat.
  prefs: []
  type: TYPE_NORMAL
- en: Filebeat is a lightweight shipper for log files and represents the next-generation
    of LogStash Forwarder. Just like LogStash, it tails log files. The difference
    is that it is optimized for just tailing and sending logs. It will not do any
    parsing. Another difference is that it is written in Go. Those two things alone
    make it much more resource efficient with such a small footprint that we can safely
    run it on all servers without noticing a significant increase in memory and CPU
    consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we see Filebeat in action, we need to change the `input` section of
    our LogStash configuration. The new configuration is located in the `roles/logstash/files/beats.conf`
    file and its content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the only difference is in the input section. It uses the beats
    plugin that is set to listen to the port `5044`. With this configuration, we can
    run a single LogStash instance, and have all the other servers send their logs
    to this port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy LogStash with these settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'LogStash is now running inside the `logging` server and listening for beats
    packets on port `5044`. Before we proceed and deploy Filebeat on, let''s say,
    the `prod` node, let us take a quick look at the `prod3.yml` playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'new addition is the `roles/filebe` `at` role. Its tasks, defined in the `roles/filebeat/tasks/main.yml`
    file, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The tasks will download the package, install it, copy the configuration, and,
    finally, run the service. The only thing worth looking at is the `r` `oles/filebeat/templates/filebeat.yml`
    configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The *filebeat* section specifies a list of prospectors which are used to locate
    and process log files. Each prospector item begins with a dash (`-`) and specifies
    prospector-specific configuration options, including the list of paths that are
    crawled to locate log files. In our case, we're having only one path set to `/var/log/**/*.log`.
    When started, Filebeat will look for all files ending in `.log located in the
    /var/log/*` directory, or any of its subdirectories. Since that happens to be
    the location where most of Ubuntu logs are located, we'll have quite a lot of
    log entries to process.
  prefs: []
  type: TYPE_NORMAL
- en: The *output* section is used to send log entries to various destinations. In
    our case, we specified LogStash as the only output. Since the current LogStash
    configuration does not have any filtering, we could have set ElasticSearch as
    output, and the result would be the same, but with less overhead. However, since
    it is very likely that we'll add some filters in the future, the output is set
    to logstash.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that filters are a blessing and a curse at the same time. They allow
    us to split log entries into easier-to-manage fields. On the other hand, if log
    formats differ too much, you might spend an eternity writing parsers. Whether
    you should use filters, or depend on ElasticSearch filtering capabilities without
    specialized fields, is entirely up to you. I tend to go both ways. If log contains
    an important piece of information (as you will see in one of the following examples),
    filtering logs is a must. If log entries are generic messages without analytical
    value, I skip filtering altogether. With a bit of practice, you'll establish your
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about configuration options, please consult the [https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-configuration-details.html](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-configuration-details.html)
    page.
  prefs: []
  type: TYPE_NORMAL
- en: Let's run the playbook and see Filebeat in action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now that Filebeat is running in the `prod` node, we can take a look at logs
    generated by LogStash running on the `logging` server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The last few lines of the `docker logs` command are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: FileBeats sent all the log entries from the `/var/log/` directory in the `prod`
    node to LogStash running in the `logging` server. It did that without breaking
    a sweat and, as a result, we got over 350 log entries stored in ElasticSearch.
    OK, 350 log entries is not something to brag about, but, it there were 350000,
    it would still do it effortlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Let's confirm that logs reached Kibana. Please open `http://10.100.198.202:5601/`.
    If you see no entries, it means that more than fifteen minutes passed, and you
    should increase the time by clicking the **time selector** button in the top-right
    corner of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that every time a new field type is added to ElasticSearch index,
    we should recreate the pattern. We can do that by navigating to the **Settings**
    screen and clicking the **Create** button.
  prefs: []
  type: TYPE_NORMAL
- en: 'We, again, improved the solution quite a bit. There is a central place where
    logs are parsed (LogStash), stored (ElasticSearch), and explored (Kibana). We
    can plug in any number of servers with Filebeat running on each of them. It will
    tail logs and send them to LogStash:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sending Log Entries to a Central LogStash Instance](img/B05848_16_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-03 – ELK stack running on a single server with Filebeat distributed
    to the whole cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s up the ante a bit and apply what we learned to Docker containers. Since
    we''ll change the LogStash configuration, let us end this section by removing
    the running instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Sending Docker Log Entries to a Central LogStash Instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we are using containers, we can run them with volume sharing the directory
    where the service is writing its logs. Shall we do that? The answer is no and,
    at this time, you probably think that I am continuously leading you from one wrong
    solution to another. What I'm really trying to do is to build the solution step
    by step and, at the same time, show you different paths that you might choose
    to take. My preferred solution does not necessarily have to be adopted by you.
    The more choices you have, the more informed decisions you'll make.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to the subject of writing logs to a file and shipping them to
    LogStash. My, strongly subjective, opinion is that all logs, no matter the way
    we package our services, should be sent to standard output or error (`stdout`
    or `stderr`). There are many practical reasons for this opinion which, be it as
    it may, I won't elaborate. I already received quite a few emails from people stating
    that my views and practices are too radical (most of them got a response saying
    that the century changed more than fifteen years ago). I'll just try to avoid
    another war on the logging subject in general terms, and skip to reasons for not
    writing logs to files when services are deployed inside containers. Two of them
    stick from the crowd. First of all, the less we use volumes, the less are containers
    dependent on the host they're running on, and easier it is to move them around
    (either in the case of a failure or for scaling purposes). The second reason is
    that Docker's logging drivers expect logs to be sent to `stdout` and `stderr`.
    By not writing logs to files, we avoid coupling with a server or particular logging
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: If you are about to send me a hate email stating that log files are a grace
    from heaven, please note that I am referring to their output destination when
    generated inside containers (even though I was applying the rule before I started
    using them).
  prefs: []
  type: TYPE_NORMAL
- en: What is the alternative to exposing container directory with logs as a volume?
    Docker introduced logging driver feature in its version 1.6\. While it passed
    mostly unnoticed, it is a very cool capability and was a huge step toward creating
    a comprehensive approach to logging in Docker environments. Since then, besides
    the default `json-file` driver, we got `syslog`, `journald`, `gelf`, `fluentd`,
    `splunk`, and `awslogs`. By the time you read this book, new ones might have arrived
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we decided to use Docker's logging drivers, the question arises which
    one to choose. The GELF driver writes messages in *Greylog Extended Log Format*
    supported by LogStash. If all we need is to store logs generated by our containers,
    this is a good option. On the other hand, if we want not only logs generated by
    services running inside containers but also from the rest of the system, we might
    opt for `JournalD` or `syslog`. In such a case, we'd get truly (almost) complete
    information about everything that happens, not only inside containers but on the
    whole OS level. The latter option (`JournalD` or `syslog`) is preferable when
    there is a substantial available memory for ElasticSearch (more logs equals more
    memory consumption), and that is the one we'll explore deeper. Do not get scared
    by ElasticSearch's need for a lot of memory. With clever cleanups of old data,
    this can be easily mitigated. We'll skip the debate whether `JournalD` is a better
    or worse solution than `syslog`, and use the latter. It does not matter which
    one is your preference since the same set of principles applies to both.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we''ll use the `roles/logstash/files/syslog.conf` file as LogStash
    configuration. Let''s go through its sections one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `input` section should be self-explanatory. We're using the `syslog plugin`
    with two settings. The first one adds a type field to all events handled by this
    input. It will help us distinguish logs coming from `syslog`, from those we're
    generating through other methods. The `port` setting states that LogStash should
    listen on `25826` for syslog events.
  prefs: []
  type: TYPE_NORMAL
- en: 'The filter section of the config file is a bit more complicated. I decided
    to use it mostly as a way to showcase a fraction of what can be done through filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: It starts with an `if` statement. Docker will send logs to syslog with a value
    of the `program` field set in the `docker/[CONTAINER_ID]` format. We are leveraging
    that fact to distinguish log entries coming from Docker, from those generated
    through some other means. Inside the `if` statement, we are performing a few mutations.
    The first one is the addition of a new field called `container_id` that, for now,
    has the same value as the `program` field. The second mutation is the removal
    of the `docker/` part of that value so that we are left with only container ID.
    Finally, we change the value of the `program` field to `docker`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variables, and their values, before and after mutations, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable name | Value before | Value after |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `program` | `docker/[CONTAINER_ID]` | `docker` |'
  prefs: []
  type: TYPE_TB
- en: '| `container_id` | `/` | `[CONTAINER_ID]` |'
  prefs: []
  type: TYPE_TB
- en: The second conditional starts by checking whether the `container_id` is set
    to `nginx`. If it is, it parses the message using the `COMBINEDAPACHELOG` pattern
    that we already saw in action and adds to it two new fields called `upstream_address`
    and `upstream_response_time`. Both of those fields also use predefined grok patterns
    `HOSTPORT` and `NOTSPACE`. If you'd like to dive deeper, and take a closer look
    at those patterns, please consult the [https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns](https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns)
    repository. If you are familiar with regular expressions, this should be easy
    to understand (if there is such a thing as easy with RegEx).
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, you might want to rely on declared names to find the expression
    you need (at least until you learn regular expressions). The truth is that RegEx
    is a very powerful language for parsing text but, at the same time, very hard
    to master:'
  prefs: []
  type: TYPE_NORMAL
- en: My wife claimed that my hair went gray at approximately the same time I worked
    on a project that required quite a lot of regular expressions. That is one of
    the few things we agreed on.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the mutation inside `nginx` conditional transforms `upstream_response_time`
    field from `string` (default) to `float`. We'll use this information later on,
    and will need it to be a number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third and the last section of the configuration file is `output`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: It is the same as the previous ones. We're sending filtered log entries to standard
    output and ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the configuration file, or, at least, pretend that we
    do, we can deploy LogStash one more time through the Ansible playbook `elk.` `yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have LogStash up and running, and configured to use `syslog` as input.
    Let''s remove the currently running `nginx` instance and run it again with Docker
    log driver set to `syslog`. While at it, we''ll also provision the `prod` node
    with `syslog`. The `prod4.yml` playbook that we''ll use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this playbook is similar to the others we used for provisioning
    the `prod` server. The difference is in the `log_to_syslog` variable, and the
    addition of the `rsyslog` role.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relevant part of the `nginx` tasks defined in the `roles/nginx/tasks/main.`
    `yml` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The difference is in the addition of `log_driver` and `log_opt` declarations.
    The first one sets Docker log driver to `syslog`. The `log_opt` can be used to
    specify additional logging options, which depend on a driver. In this case, we
    are specifying the `tag`. Without it, Docker would use container ID to identify
    logs sent to syslog. That was, when we query ElasticSearch, it will be much easier
    to find `nginx` entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rsyslog` tasks defined in the `roles/rsyslog/tasks/mai` `n.yml` file are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'It will make sure that `rsyslog` and `logrotate` packages are installed, copy
    the `10-logstash.conf` configuration file, and restart the service. The `roles/rsyslog/templates/10-logstash`
    `.conf` template is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the file is an Ansible's template and that `{{ elk_ip }}` will
    be replaced with the IP. The configuration is dead simple. Everything sent to
    syslog will be re-sent to the LogStash running on the specified IP and port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we''re ready to remove the currently running `nginx` container and run
    the playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what was sent to LogStash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the syslog entries generated by the system. One of them might
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We can also explore the same data through Kibana running on `http://10.100.198.20`
    `2:5601/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happens when we deploy our services packed into containers.
    First we''ll enter the `prod` node from which we''ll run the `books-ms` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we deploy the `books-ms` service, let us take a quick look at the `docker-compose-logg`
    `ing.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it follows the same logic as the one we used to provision `nginx`
    with Ansible. The only difference is that, in this case, it is Docker Compose
    configuration. It contains the same `log_driver` and `log_opt` keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the changes we had to add to the Docker Compose configuration,
    we can deploy the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s double check that it is indeed running by listing and filtering Docker
    processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the service is up and running, with the `syslog` logging driver, we
    should verify that log entries were indeed sent to LogStash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Service logs are indeed sent to LogStash. Please notice that LogStash filters
    did what we told them to do. The `program` field was transformed from `docker/books-ms`
    to `docker`, and a new field called `container_id` was created. Since we defined
    `message` parsing only when `container_id` is `nginx`, it stayed intact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us confirm that `message` parsing is indeed working correctly for log entries
    coming from `nginx`. We''ll need to make a few requests to the proxy, so we''ll
    start by configuring it properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: You already used `nginx` configurations and Consul Template, so there is no
    need for an explanation of those commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the service is running, is integrated, and is sending logs to LogStash,
    let us generate a few `nginx` log entries by making a few requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what did LogStash receives this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Part of the output of the `docker logs` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This time, not only that we stored logs coming from containers, but we also
    parsed them. The main reason for parsing `nginx` logs lies in the `upstream_response_time`
    field. Can you guess why? While you think about possible usages of that field,
    let us take a closer look at a few of the features of the *Discover* screen in
    Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: We generated quite enough logs, so we might, just as well, want to start using
    Kibana filters. Please open `http://10.10` `0.198.202:5601/`. Please change the
    time to, let's say, 24 hours, by clicking the top-right button. That will give
    us plenty of time to play with the few logs we created. Before we jump into filtering,
    please go to the **Settings** screen, and click **Create**. That will refresh
    our index pattern with new fields. When finished, please return to the **Discover**
    screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us begin with the left-hand menu. It contains all the available fields,
    found in all logs that match given period. Clicking on any of those fields provides
    us with the list of values it holds. For example, `container_id` contains `books-ms`
    and `nginx`. Next to those values are icons with the magnifying glass. The one
    with the plus sign can be used to filter only entries that contain that value.
    Similarly, the icon with the minus sign can be used to exclude records. Click
    the icon with the plus sign next to `nginx`. As you can see, only log entries
    coming from `nginx` are displayed. The result of applied filters is located in
    the horizontal bar above. Hovering over one of the filters (in this case `container_id:
    "nginx"`), allows us to use additional options to enable, disable, pin, unpin,
    invert, toggle, and remove that filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sending Docker Log Entries to a Central LogStash Instance](img/B05848_16_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-04 – Kibana Discover screen with log entries filtered by container_id
    nginx
  prefs: []
  type: TYPE_NORMAL
- en: At the top of the main frame is a graph with the number of logs distributed
    over the specified period. Below it is a table with log entries. By default, it
    shows the `Time` and the `*_source*` columns. Please click the arrow icon on the
    left side of one of the rows. It expands the row to display all the fields available
    in that log entry. They are a combination of data generated by LogStash and those
    we parsed through its configuration. Each field has the same icons as those we
    found in the left-hand menu.
  prefs: []
  type: TYPE_NORMAL
- en: Through them, we can *filter for value or filter out value*. The third button,
    represented by an icon that looks like a single row table with two columns, can
    be used to *toggle that column in table*. Since default columns are not very useful,
    not to say boring, please toggle `logsource`, `request`, `verb`, `upstream_address`,
    and `upstream_response_time`. Click, again, the arrow, to hide the fields. We
    just got ourselves a nice table that shows some of the most important pieces of
    information coming from `nginx`. We can see that the server where requests are
    made (`logsource`), the address of requests (`request`), the type of requests
    (`verb`), how much it took to receive responses (`upstream_response_time`), and
    where were the requests proxied to (`upstream_address`). If you think the *search*
    you created is useful, you can save it by clicking the **Save Search** button
    located in the top-right part of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next to it is the **Load Saved Search** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sending Docker Log Entries to a Central LogStash Instance](img/B05848_16_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-05 – Kibana Discover screen with log entries filtered by container_id
    nginx and custom columns
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore **Visualize** and **Dashboard** screens a bit later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s us summarize the flow we have at this moment:'
  prefs: []
  type: TYPE_NORMAL
- en: Containers are deployed with Docker's logging driver set to `syslog`. With such
    a configuration, Docker redirects everything that is sent to standard output,
    or error (`stdout`/`stderr`), to `syslog`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the log entries, be it from containers or processes deployed through other
    methods, are redirected from syslog to LogStash.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LogStash receives syslog events, applies filters and transformations, and re-sends
    them to ElasticSearch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everybody is happy, because finding specific log entries is a breeze, and life,
    during office hours, is a bit easier to cope with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Sending Docker Log Entries to a Central LogStash Instance](img/B05848_16_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-06 – ELK stack running on a single server with containers logging
    to syslog
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing Based on Software Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us put the response time we are logging through `nginx` to a good use.
    Since data is stored in ElasticSearch, we might do a few quick examples of using
    its API. We can, for instance, retrieve all entries stored inside the `logstash`
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Elastic search returned the first ten entries (default page size), together
    with some additional information, like the total number of records. There''s not
    much use in retrieving all the entries, so let us try to narrow it down. We can,
    for example, request all records that have `nginx` as `container_id` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The results are the same three entries we observed from LogStash logs. Again,
    there's not much use of them. If this were a production system, we would get thousands
    upon thousands of results (distributed among multiple pages).
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, let us try something truly useful. We''ll analyze data and, for
    example, retrieve the average response time from `nginx` logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the last command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: With something like that request, we can extend our self-healing system and,
    for example, retrieve average response time of a service during last hour. If
    responses were, on average, slow, we could scale the service. Similarly, if responses
    were fast, we can descale it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's filter the results so that only those made by `nginx`, with a request
    to `/api/v1/books` (the address of our service), and created during the last hour,
    are retrieved. Once data is filtered, we'll aggregate all the results and get
    the average value of the `upstream_response_time` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chances are that more than one hour passed since you sent a request to
    the service through `nginx`. If that''s the case, the resulting value would be
    `null` since there are no records that would match the filter we are about to
    make. We can easily fix that, by making, let''s say, a hundred new requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have recent data, we can ask ElasticSearch to give us the average
    response time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The ElasticSearch API and the Lucene engine used in the background are so vast
    that it would require a whole book to describe it, so the explanation is out of
    the scope of this book. You can find detailed information in the [https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html)
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the request will vary from one case to another. My result was
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We can now take this response time and, depending on the rules we set, scale,
    descale, or do nothing. Right now we have all the elements to extend our self-healing
    system. We have the process that stores response times in ElasticSearch and the
    API to analyze data. We can create one more Consul watch that will, periodically,
    query the API and, if an action is needed, send a request to Jenkins to prevent
    the disease from spreading. I'll leave that to you, as a few exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exercise: Scaling the service if response time is too long**'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Consul watch that will use the ElasticSearch request we created,
    and invoke a Jenkins job that will scale the service if the average response time
    is too long. Similarly, descale the service if the response time is too short,
    and more than two instances are running (less than two poses a downtime risk).
  prefs: []
  type: TYPE_NORMAL
- en: Without introducing more complexity, we can try other types of future predictions.
    We can, for example, predict the future by observing the previous day.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exercise: Predict the future by observing the past**'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the process from the previous exercise with the different analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variables:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'T: The current time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AVG1: Average traffic between T and T+1h of the previous day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AVG2: Average traffic between T+1h and T+2h of the previous day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The task:**'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the increase (or the decrease) of the traffic between `AVG1` and `AVG2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide whether to scale, de-scale, or do nothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not need to base our analysis only on the previous day. We can also evaluate
    the same day of the preceding week, of the past month, or even of the last year.
    Do we have an increase in traffic every first day of the month? What happened
    on Christmas day last year? Do people visit our store less after summer vacations?
    The beauty is not only that we have the data to answer those questions, but we
    can incorporate the analysis into the system and run it periodically.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that some of the analysis are better of running as Consul watches,
    while the others belong to Jenkins. Tasks that should be run periodically with
    the same frequency are good use cases for Consul. While they can run as easily
    from Jenkins, Consul is more lightweight, and will use fewer resources. Examples
    would be every hour or every 5 minutes. On the other hand, Consul does not have
    a proper scheduler. If you'd like to run analysis at specific moments in time,
    Jenkins with its cron-like scheduler is a better fit. Examples would be each day
    at midnight, each first day of a month, two weeks before Christmas, and so on.
    You should evaluate both tools for each given case, and choose the one that fits
    better. An alternative would be to run all such analysis from Jenkins and benefit
    from having everything in one place. Then again, you might opt for an entirely
    different set of tools. I'll leave the choice to you. The importance lies in understanding
    the process and the goals we want to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that I provided one example that can be used as a self-healing process.
    Response times analysis does not have to be the only thing we do. Look at the
    data you can collect, decide what is useful, and what isn't, and make other types
    of data crunching. Collect everything you need, but not more. Do not fall into
    the trap of storing all you can think of, without using it. That is a waste of
    memory, CPU, and hard disk space. Do not forget to set up a process that periodically
    cleans data. You won't need all the logs from a year ago. Heck, you probably won't
    need most of the logs older than a month. If a problem is not found within thirty
    days, the chances are that there is no problem and, even if there is, it relates
    to an old release not running anymore. If, after reading this book, your release
    cycle lasts for months, and you are not planning to shorten it, I failed miserably.
    Please do not send me an email confirming this. It would only make me feel depressed.
  prefs: []
  type: TYPE_NORMAL
- en: That was a short detour from the main subject of the chapter (logging and monitoring).
    Since the book is mostly based on hands-on examples, I could not explain *self-healing
    based on historical response times* without having data to work with. Therefore,
    this discussion was added here. Throughout the rest of this chapter, there will
    be at one more excursion into a subject that might just as well belong to the
    [Chapter 15](ch15.html "Chapter 15. Self-Healing Systems"), *Self-Healing Systems*
    chapter. Now, let's get back to logging and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have all the information representing the past and the present status
    of the cluster, we can... This is the moment I imagine you, dear reader, rolling
    your eyes and mumbling to yourself that software logs do not constitute the full
    information about the cluster. Only software (logs), together with hardware data
    (metrics), can be close to a complete information about the cluster. Then again,
    my imagination might not (and often doesn't) represent reality. You might not
    have rolled your eyes, or even noticed that hardware is missing.
  prefs: []
  type: TYPE_NORMAL
- en: If that's the case, you are not paying close attention to what I wrote, and
    should have a good night sleep, or, at least, grab a coffee. Truth be told, we
    do have hardware information in Consul, but that is only the current status. We
    cannot analyze that data, see tendencies, find out why something happened, nor
    predict the future. If you are still awake, let's look at how we can log hardware
    status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, we''ll remove the currently running LogStash instance, and
    exit the prod node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Logging Hardware Status
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first things they teach you when starting to learn to work on computers
    is that software runs on hardware. A software cannot run without hardware and
    hardware is useless without software. Since they are dependent on each other,
    any attempt to collects the information about the system needs to include both.
    We explored some of the ways to gather software data, so the next step is to try
    to accomplish a similar result with hardware.
  prefs: []
  type: TYPE_NORMAL
- en: We need a tool that will collect statistics about the system it is running on
    and has the flexibility to send that information to LogStash. Once we find and
    deploy such a tool, we can start using statistics it provides to find past and
    current performance bottlenecks and predict future system requirements. Since
    LogStash will send the information received from that tool to ElasticSearch, we
    can create formulas that will allow us to perform performance analysis and capacity
    planning.
  prefs: []
  type: TYPE_NORMAL
- en: One such tool is CollectD. It is free open source project written in C, making
    it high performant and very portable. It can easily handle hundreds of thousands
    of data sets, and it comes with over ninety plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily for us, LogStash has the CollectD input plugin that we can use to receive
    its events through a UDP port. We''ll use (`roles/logstash/files/syslog-collectd.conf`)[[https://github.com/vfarcic/ms-lifecycle/blob/master/ansible/roles/logstash/files/syslog-collectd.conf](https://github.com/vfarcic/ms-lifecycle/blob/master/ansible/roles/logstash/files/syslog-collectd.conf)]
    file to configure LogStash to accept *CollectD* input. It is a copy of the (`roles/logstash/files/syslog.conf`)[`https://github.com/vfarcic/ms-lifecycle/blob/master/ansible/roles/logstash/files/syslog.conf`]
    with an additional input definition. Let''s take a look at its `input` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, all we did was add a new input that listens on the UDP port
    `25827`, set buffer size, define that `collectd` codec should be used, and added
    a new field called type. With the value from the type field, we can distinguish
    `syslog` logs from those coming from `collectd`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the playbook that will provision the `logging` server with LogStash
    and configure it to accept both `syslog` and `collectd` input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: You might have noticed the usage of the `restore_backup` variable. One of *kibana*
    tasks is to restore an ElasticSearch backup with the definitions of Kibana Dashboards
    that will be discussed soon. Backup is restored through the `vfarcic/elastic-dump`
    container containing a nifty tool called `elasticsearch-dump` by *taskrabbit*.
    It can be used to create and restore ElasticSearch backups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that LogStash is configured to accept `CollectD` input, let''s turn our
    attention to the `prod` server, and install `Co` `llectD`. We''ll use the `prod5.yml`
    playbook that, in addition to the tools we used before, contains the `collectd`
    role. The tasks are defined in the (`roles/collectd/tasks/main.yml`)[[https://github.com/vfarcic/ms-lifecycle/tree/master/ansible/roles/collectd/tasks/main.yml](https://github.com/vfarcic/ms-lifecycle/tree/master/ansible/roles/collectd/tasks/main.yml)]
    file. Its content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'By this time, you should probably consider yourself an expert in Ansible, and
    do not need an explanation of the role. The only thing worth commenting is the
    `roles/collectd/files/collectd.conf` template that represents the `CollectD` configuration.
    Let''s take a quick look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: It starts by defining the hostname through the Ansible variable `ansible_hostname`,
    followed by the load of the plugins we'll use. Their names should be self-explanatory.
    Finally, few of the plugins have additional configurations. Please consult [https://collectd.org/documentation.shtml](https://collectd.org/documentation.shtml)
    documentation for more information about configuration format, all the plugins
    you can use, and their settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the playbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that `CollectD` is running, we can give it a few seconds to kick in and
    take a look at LogStash logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'A few of the entries are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: From that output, we can see that CollectD sent information about memory. The
    first entry contains `used`, the second `buffered`, the third `cached`, and, finally,
    the fourth represents `free` memory. Similar entries can be seen from the other
    plugins. CollectD will periodically repeat the process, thus allowing us to analyze
    both historical and near real-time tendencies and problems.
  prefs: []
  type: TYPE_NORMAL
- en: Since CollectD generated the new fields, let us recreate index pattern by opening
    `http://10.100.198.202:5601/`, navigating to the **Settings** screen, and clicking
    the **Create** button.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are many reasons to visit Kibana''s **Discover** screen for software
    logs, there are only a few, if any, to use it for CollectD metrics, so we''ll
    concentrate on Dashboards. That being said, even if we are not going to look at
    hardware data from this screen, we still need to create searches required for
    visualization. An example search that would retrieve all records from `collectd`,
    made in the `prod` host, through the `memory` plugin, would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: That line can be written (or pasted) to the *search* field in the **Discover**
    screen, and it will return all data matching that filter and the time set in the
    top-right corner of the screen. The backup we restored already contained a few
    saved searches that can be opened through the **Open Saved Search** button in
    the top-right corner of the screen. With those searches, we can proceed to visualizations.
    As an example, please open the `prod-df` saved search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana Dashboards consist of one or more visualizations. They can be accessed
    by clicking the **Visualize** button. When you open the **Visualize** screen,
    you''ll see different types of graphs you can choose to create a new visualization.
    Since we restored a backup with a few visualizations I prepared, you can load
    one by clicking it from the **open a saved visualization** section located at
    the bottom of the screen. Please note that this screen appears only the first
    time and, from there on, the same action can be accomplished by the **Load Saved
    Visualization** button located on the top-right side of the screen. Go ahead and
    play a bit with Kibana visualizations. Once you''re done, we''ll move to dashboards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logging Hardware Status](img/B05848_16_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-07 – Kibana visualization of hard disk usage
  prefs: []
  type: TYPE_NORMAL
- en: Dashboard can be opened from the top menu. The backup we restored contains one
    so let's use it to see CollectD in action. Please click the **Dashboard** button,
    followed by the **Load Saved Dashboard** icon, and select the `prod` dashboard.
    It will display visualizations with one (and the only) *CPU* (`prod-cpu-0`), *hard
    disk* (`prod-df`), and *memory* (`prod-memory`) usage inside the `prod` VM. CollectD
    offers many more plugins than those we used. With more information coming in,
    this dashboard can be made much more colorful, not to say useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, even though the dashboard we created does not have much activity,
    you can probably imagine how it could be transformed into an indispensable tool
    for monitoring the cluster status. There could be a separate dashboard for each
    server, one for the whole cluster, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logging Hardware Status](img/B05848_16_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-08 – Kibana dashboard with CPU, hard disk, and memory usage over time
  prefs: []
  type: TYPE_NORMAL
- en: That was the basis of your future hardware monitoring dashboard. What else can
    with do with hardware information (besides looking at dashboards)?
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing Based on Hardware Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using hardware data for self-healing is as important as software information.
    Now that we have both, we can extend our system. Since we already went through
    all the tools and practices required for such a system, there is no real need
    for us to go through them in the hardware context. Instead, I'll just give you
    a few ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Consul is already monitoring hardware utilization. With historical data in ElasticSearch,
    we can predict not only that the warning threshold is reached (for example 80%),
    but when it will get critical (for example 90%). We can analyze the data and see
    that, for instance, during last 30 days, disk utilization was increasing by an
    average rate of 0.5%, meaning that we have twenty days until it reaches the critical
    state. We could also draw a conclusion that even through the warning threshold
    is reached, it was a one time deal, and the available space is not shrinking anymore.
  prefs: []
  type: TYPE_NORMAL
- en: We could combine software and hardware metrics. With only software data, we
    might conclude that at peak hours, when traffic increases, we need to scale our
    services, by adding hardware we might change that opinion after realizing that
    the problem was actually in the network that cannot support such a load.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis combinations we can create are limitless, and the number of formulas
    we'll create will grow with time and experience. Every time we pass through one
    door, another one opens.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is my favorite chapter. It combines most of the practices we learned throughout
    the book into a grand finale. Almost everything happening on a server, be it software
    or hardware, system programs or those we deployed, is sent to LogStash and, from
    there, to ElasticSearch. And it's not only one server. With a simple `rsyslog`
    and `collectd` configurations applied to all your nodes, the whole cluster will
    be sending (almost) all the logs and events. You'll know who did what, which processes
    started, and which were stopped. You'll be aware what was added, and what was
    removed. You be alerted when a server is low on CPU, which one is about to get
    its hard disk full, and so on. You'll have the information about every service
    you deploy or remove. You'll know when were containers scaled, and when descaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'We created a logging and monitoring system that can be described through the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Final Thoughts](img/B05848_16_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-09 – Kibana dashboard with CPU, hard disk, and memory usage over time
  prefs: []
  type: TYPE_NORMAL
- en: Knowing everything is a worthy goal and, with a system we designed, you are
    one step closer to fulfilling it. On top of knowing everything about the past
    and the present, you made the first step towards knowing the future. If you combine
    the practices from this chapter with those we learned in the [Chapter 15](ch15.html
    "Chapter 15. Self-Healing Systems"), *Self-Healing Systems*, your systems will
    be able to recuperate from failures and, in many cases, prevent a disease from
    happening in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us finish with some cleaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
