<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-self-healing-applied-to-services" class="calibre15">Self-Healing Applied To Services</h2>

<p class="calibre3">The job of a system that self-heals services is to make sure that they are (almost) always running according to the design. Such a system needs to monitor the state of the cluster and continuously ensure that all the services are running the specified number of replicas. If one of them stops, the system should start a new one. If a whole node goes does, all the replicas that were running on that node should be scheduled to run across the healthy nodes. As long as the capacity of the cluster can host all the replicas, such a system should be able to maintain the defined specifications.</p>

<p class="calibre3">Having a system that self-heals services does not mean that it provides high-availability. If a replica stops being operational, the system will bring it back into the running state. However, there will be a (very) short period between a failure and until the system is restored to the desired state. If we’re running only one replica of a service, during that time there will be downtime. The best way to remedy this problem is to run at least a couple of replicas of each service. That way, when one of them goes down, the others will handle the requests until the failed one is restored to its desired state.</p>

<p class="calibre3">Assuming that the conditions of the cluster do not change (nodes do not go down) and that the load on the cluster is constant, the system capable of self-healing services should provide near 100% up-time. Unfortunately, nodes do go down, and the load on the cluster is (almost) never constant. We’ll explore how to remedy those problems later. For now, we’ll focus on how to build the part of the system that will make sure that the services are healing automatically.</p>

<h3 id="leanpub-auto-creating-the-cluster-and-deploying-services-3" class="calibre20">Creating The Cluster And Deploying Services</h3>

<p class="calibre3">We’ll start by setting up a Swarm cluster and deploying the stacks that we’ll use in this chapter.</p>

<aside class="tip">
    <h3 id="leanpub-auto-a-note-to-the-devops-21-toolkit-readers-1" class="calibre22">A note to <em class="calibre23">The DevOps 2.1 Toolkit</em> readers</h3>

  <p class="calibre3">You should be very familiar with Docker Swarm, and the rest of this chapter will not provide any information that you do not already know. You are free to skip to the <a href="part0014.html#self-healing-applied-is-not-enough">Is It Enough To Have Self-Healing Applied To Services?</a> sub-chapter. If, on the other hand, you feel a bit forgetful and would like to refresh your memory, I’ll do my best to be as brief as possible and provide only the bare minimum.</p>

</aside>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/99325930813d7e25375b982c7e2498d2">08-self-healing-services.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod +x scripts/dm-swarm-08.sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>./scripts/dm-swarm-08.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env swarm-1<code class="k">)</code>
<code class="lineno">6 </code>
<code class="lineno">7 </code>docker stack ls
</pre></div>

</figure>

<p class="calibre3">We executed the <code class="calibre19">dm-swarm-08.sh</code> script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks and deployed the stacks. The last command listed all the stacks in the cluster. We are running only <code class="calibre19">go-demo</code> and <code class="calibre19">proxy</code> stacks. Where are <code class="calibre19">prometheus</code> and  <code class="calibre19">exporter</code> stacks we deployed in the previous chapters? Why are we missing them? The reason is quite simple. We don’t need them to demonstrate self-healing applied to services. We have everything we need.</p>

<p class="calibre3">Before we proceed, please confirm that all the replicas that compose the <code class="calibre19">go-demo</code> stack are running. You can check their statuses by executing <code class="calibre19">docker stack ps go-demo</code> command. You might see a few failed replicas of the <code class="calibre19">go-demo_main</code> service. The reason is in its design. It fails if it cannot connect to the database running inside the <code class="calibre19">go-demo_db</code> service. Since the database is a bigger image, it takes more time to pull it. Ignore the failed replicas and confirm that there are three instances of <code class="calibre19">go-demo_main</code> running.</p>


<figure class="image">
  <img src="../images/00040.jpeg" alt="Figure 8-1: Replicas spread across the cluster" class="calibre17"/>
  <figcaption class="calibre18">Figure 8-1: Replicas spread across the cluster</figcaption>
</figure>


<h3 id="leanpub-auto-using-docker-swarm-for-self-healing-services" class="calibre20">Using Docker Swarm For Self-Healing Services</h3>

<p class="calibre3">Docker Swarm already provides almost everything we need from a system that self-heals services.</p>

<p class="calibre3">What follows is a short demonstration of some of the scenarios the system might encounter when facing failed service replicas. I already warned you that at least basic knowledge of operating Swarm is the pre-requirement for this book so I chose to skip a lengthy discussion about the features behind the scheduler. I won’t go into details but only prove that Swarm guarantees that the services will (almost) always be healthy.</p>

<p class="calibre3">Let’s see what happens when one of the three replicas of the <code class="calibre19">go-demo_main</code> service fails. We’ll simulate it by stopping the primary process inside one of the replicas.</p>

<p class="calibre3">The first thing we need to do is find out the node where one of the replicas are running.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">NODE</code><code class="o">=</code><code class="k">$(</code>docker service ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>Running <code class="se">\</code>
<code class="lineno">3 </code>    go-demo_main <code class="calibre19">|</code> tail -n <code class="o">1</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> awk <code class="s">'{print $4}'</code><code class="k">)</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env <code class="nv">$NODE</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">We listed all the processes of the <code class="calibre19">go-demo_main</code> service and used a filter to limit the output only to those that are running. The output was sent to <code class="calibre19">tail</code> so that only one result is returned. Further on, we used <code class="calibre19">awk</code> to print only the fourth column which contains the name of the node. The result was assigned to the environment variable <code class="calibre19">NODE</code>.</p>

<p class="calibre3">The second command changed our local Docker client to point to the node with one of the replicas.</p>

<p class="calibre3">Next, we need to find the ID of one of the replicas running on the node we selected.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">CONTAINER_ID</code><code class="o">=</code><code class="k">$(</code>docker container ls -q <code class="se">\</code>
<code class="lineno">2 </code>    -f <code class="s">"label=com.docker.swarm.service.name=go-demo_main"</code> <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> tail -n <code class="o">1</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">We listed all the containers in quiet mode so that only IDs are returned. We used filtering so that only containers labeled as the service <code class="calibre19">go-demo_main</code> are retrieved. Since we need only one container (there might be more on that node), we sent the output to <code class="calibre19">tail</code> that returned only the last row.</p>

<p class="calibre3">Now we can stop the main process inside the container and observe what happens.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker container <code class="nb">exec</code> -it <code class="se">\</code>
<code class="lineno">2 </code>    <code class="nv">$CONTAINER_ID</code> pkill go-demo
</pre></div>

</figure>

<p class="calibre3">We killed the <code class="calibre19">go-demo</code> process inside the container. That was the main and the only process inside that container. As soon as it stopped, container stopped as well.</p>

<p class="calibre3">Let’s list the processes of the <code class="calibre19">go-demo</code> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps go-demo
</pre></div>

</figure>

<p class="calibre3">The output, limited to the replica we killed, is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME               IMAGE                  NODE    DESIRED STATE CURRENT STATE   \
<code class="lineno">2 </code>      ERROR                     PORTS
<code class="lineno">3 </code>go-demo_main.3     vfarcic/go-demo:latest swarm-2 Running       Running 1 second\
<code class="lineno">4 </code> ago
<code class="lineno">5 </code> \_ go-demo_main.3 vfarcic/go-demo:latest swarm-2 Shutdown      Failed 11 second\
<code class="lineno">6 </code>s ago "task: non-zero exit (2)"
</pre></div>

</figure>

<p class="calibre3">As you can see, Swarm detected that one of the replicas failed, and scheduled a new one. It made sure that the specification (the design) is followed. When we deployed the <code class="calibre19">go-demo</code> stack, we told Swarm that we want to have three replicas of the <code class="calibre19">go-demo_main</code> service and Swarm is continuously monitoring the cluster making sure that our desire is always fulfilled. There were a few seconds between the failure and until the new replica was running. If we’d run only one replica, that would mean a short downtime. However, since we are running three, the other two took over the requests, and there was no downtime. High availability is preserved.</p>


<figure class="image">
  <img src="../images/00041.jpeg" alt="Figure 8-2: The failed replica was re-scheduled" class="calibre17"/>
  <figcaption class="calibre18">Figure 8-2: The failed replica was re-scheduled</figcaption>
</figure>


<p class="calibre3">What happens when a whole node is destroyed? I’m sure you already know the answer, but I’ll go through a small demonstration never the less.</p>

<p class="calibre3">We’ll repeat the command that we executed earlier and find a node with at least one of the <code class="calibre19">go-demo_main</code> replicas.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">NODE</code><code class="o">=</code><code class="k">$(</code>docker service ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>Running <code class="se">\</code>
<code class="lineno">3 </code>    go-demo_main <code class="calibre19">|</code> tail -n <code class="o">1</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> awk <code class="s">'{print $4}'</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">Let’s be destructive and delete the node.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker-machine rm -f <code class="nv">$NODE</code>
</pre></div>

</figure>

<p class="calibre3">To be on the safe side, we’ll list all the machines and confirm that one was indeed removed.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker-machine ls
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DO\
<code class="lineno">2 </code>CKER        ERRORS
<code class="lineno">3 </code>swarm-1   -        virtualbox   Running   tcp://192.168.99.100:2376           v1\
<code class="lineno">4 </code>7.03.1-ce
<code class="lineno">5 </code>swarm-3   *        virtualbox   Running   tcp://192.168.99.102:2376           v1\
<code class="lineno">6 </code>7.03.1-ce
</pre></div>

</figure>

<p class="calibre3">Next, we’ll have to change our environment variables to ensure that our local Docker client is not pointing to the node we just removed.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">NODE</code><code class="o">=</code><code class="k">$(</code>docker-machine ls -q <code class="calibre19">|</code> tail -n <code class="o">1</code><code class="k">)</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env <code class="nv">$NODE</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">Now we can, finally, list the processes of the <code class="calibre19">go-demo</code> stack and see the result.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps go-demo
</pre></div>

</figure>

<p class="calibre3">The output, limited to the relevant parts, is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME               IMAGE                  NODE    DESIRED STATE CURRENT STATE   \
<code class="lineno">2 </code>        ERROR                        PORTS
<code class="lineno">3 </code>...
<code class="lineno">4 </code>go-demo_main.3     vfarcic/go-demo:latest swarm-1 Running       Running 2 minute\
<code class="lineno">5 </code>s ago
<code class="lineno">6 </code> \_ go-demo_main.3 vfarcic/go-demo:latest swarm-2 Shutdown      Running 7 minute\
<code class="lineno">7 </code>s ago
<code class="lineno">8 </code>...
</pre></div>

</figure>

<p class="calibre3">Docker Swarm detected that <code class="calibre19">swarm-2</code> node is not available and changed the desired state of the replicas that were running there to <code class="calibre19">Shutdown</code>. Unlike the case when a container fails, the current state stayed unchanged. Swarm still assumes that the replicas are running inside <code class="calibre19">swarm-2</code>. We know that the node is destroyed and that no replicas are running inside it. Swarm, on the other hand, is not aware of that. It just knows what the last known state of that replica is. The node, from Swarm’s point of view, might still be operational and only lost the connection with the cluster. Theoretically, the connection could be reestablished later. There could be many other explanations besides the destruction of the node, so Swarm keeps the last known state. Never the less, if the node rejoins the cluster, that replica is scheduled for shutdown and will be destroyed immediately as a way to preserve the desired state.</p>


<figure class="image1">
  <img src="../images/00042.jpeg" alt="Figure 8-3: Replicas from a failed node are spread across the cluster" class="calibre17"/>
  <figcaption class="calibre18">Figure 8-3: Replicas from a failed node are spread across the cluster</figcaption>
</figure>


<h3 id="self-healing-applied-is-not-enough" class="calibre20">Is It Enough To Have Self-Healing Applied To Services?</h3>

<p class="calibre3">Self-healing applied to services is only the beginning. It is by no means enough. The system, as it is now, is far from being autonomous. At best, it can recuperate from a few types failures. If one replica of a service goes down, Swarm will do the right thing. Even a simultaneous failure of a few replicas should not be a cause for alarm. However, self-healing applied to services by itself does not contemplate many of the common circumstances.</p>

<p class="calibre3">Let us imagine that the sizing of a cluster is done in a way that around 80 percent of CPU and memory is utilized. Such a number, more or less, provides a good balance between having too many unused resources and under-provisioning our cluster. With greater resource utilization we are running a risk that even a failure of a single node would mean that there are no available resources to reschedule the replicas that were running inside it. On the other hand, if we have more than twenty percent of available resources, we are paying for more hardware than we need.</p>

<p class="calibre3">Assuming that we do aim for eighty percent of resource utilization, without self-healing applied to infrastructure, a failure of more than one node could have a devastating effect. Swarm would not have enough available resources to reschedule replicas from the failed servers. While it is not common, an availability zone (to use AWS terms) can go down. Assuming that our infrastructure is spread over three availability zones, such a failure would mean that our capacity is reduced by thirty-three percent. When we do the math, that would mean that we would be missing sixteen percent of resources. It is even worse than that since Swarm cannot schedule services so that hundred percent is used. Somewhere around ninety to ninety-five percent is more likely. So, a failure of an AZ would mean that we would be missing quite a lot of resources and some replicas could not be rescheduled. At best, we would have reduced performance. Self-healing applied to infrastructure is a must, and we will explore it soon.</p>

<p class="calibre3">Even if nothing failed, our system would not function autonomously for long. We should expect that the load will increase with time. After all, we want our business to expand and that, in most cases, results in increased load. We need to build a system that will adapt to those changes. We need it to expand when the load increases thus providing high availability and low response times. At the same time, we need it to contract when the load decreases and save money from paying for unused resources. We need the system not only to self-heal but also self-adapt to the changed conditions. We need it to redesign itself.</p>

<p class="calibre3">There are many other things that we are missing, and we won’t discuss them just yet. Patience is a virtue, and you’ll have to wait for a while longer.</p>

<h3 id="leanpub-auto-what-now-6" class="calibre20">What Now?</h3>

<p class="calibre3">We’re done with a brief exploration of self-healing capabilities provided with Docker Swarm. We have a system that will reschedule failed services as long as there is enough capacity inside our cluster. The next step is to apply self-adaptation to our services.</p>

<p class="calibre3">Please remove the machines we created. We’ll recreate the cluster in the next chapter.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker-machine rm -f <code class="se">\</code>
<code class="lineno">2 </code>    swarm-1 swarm-2 swarm-3
</pre></div>

</figure>



</div>
</body></html>