<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;System Architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. System Architecture</h1></div></div></div><p>From here on, the whole book will be one big project. We'll go through all the stages starting from development all the way until production deployment and monitoring. Each phase will begin with a discussion about different paths we can take to accomplish the goal. We'll choose the best given our needs and implement it. The objective is to learn techniques that you can apply to your projects so please feel free to adapt instructions to fit your needs.</p><p>As most other projects, this one will start with high-level requirements. Our goal is to create an online shop. The complete plan is still not available, but we do know that selling books has priority. We should design services and a Web application in a way that it can easily be extended. We do not have the whole set of requirements in front of us, so we need to be prepared for the unknown. Besides books, we'll be selling other types of goods, and there will be other kinds of functionality like a shopping cart, registration and login, and so on. Our job is to develop bookstore and be able to respond to the future requirements in a fast manner. Since it is a new endeavor, not much traffic is expected at the beginning, but we should be prepared to scale easily and quickly if the service becomes successful. We want to ship new features as fast as possible without any downtime and to be able to recuperate from failures.</p><p>Let us start working on the architecture. It is clear that requirements are very general and do not provide many details. That means that we should be prepared for very likely changes in the future as well as well as requests for new features. At the same time, business requires us to build something small but be prepared to grow. How should we solve the problems given to us?</p><p>The first thing we should decide is how to define the architecture of the application we're about to build. Which approach will allow us possible changes of the direction, additional (but at this moment unknown) requirements and the need to be ready to scale? We should start by examining two most common approaches to applications architecture; monoliths and microservices.</p><div class="section" title="Monolithic Applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec09"/>Monolithic Applications</h1></div></div></div><p>Monolithic applications are developed and deployed as a single unit. In the case of Java, the result is often<a class="indexterm" id="id80"/> a single WAR or JAR file. Similar statement is true for C++, .Net, Scala and many other programming languages.</p><p>Most of the short history of software development is marked by a continuous increment in size of the applications we are developing. As time passes, we're adding more and more to our applications continuously increasing their complexity and size and decreasing our development, testing and deployment speed.</p><p>We started dividing our applications into layers: presentation layer, business layer, data access layer, and so on. This separation is more logical than physical, and each of those layers tends to be in charge of one particular type of operations. This kind of architecture often provided immediate benefits since it made clear the responsibility of each layer. We got separation of concerns on a high level. Life was good. Productivity increased, time-to-market decreased and overall clarity of the code base was better. Everybody seemed to be happy, for a while:</p><div class="mediaobject"><img alt="Monolithic Applications" src="graphics/B05848_03_01.jpg"/><div class="caption"><p>Figure 3-1 – Monolithic application</p></div></div><p>With time, the number of features our application was required to support was increasing and with that comes<a class="indexterm" id="id81"/> increased complexity. One feature on UI level would need to speak with multiple business rules that in turn require multiple DAO classes that access many different database tables. No matter how hard we try, the sub-division within each layer and communication between them gets ever more complicated and, given enough time, developers start straying from the initial path. After all, a design made initially often does not pass the test of time. As a result, modifications to any given sub-section of a layer tends to be more complicated, time demanding and risky since they might affect many different parts of the system with often unforeseen effects:</p><div class="mediaobject"><img alt="Monolithic Applications" src="graphics/B05848_03_02.jpg"/><div class="caption"><p>Figure 3-2 – Monolithic application with increased number of features</p></div></div><p>As time passes, things start getting worse. In many cases, the number of layers increases. We might decide<a class="indexterm" id="id82"/> to add a layer with a rules engine, API layer, and so on. As things usually go, the flow between layers is in many cases mandatory. That results in situations where we might need to develop a simple feature that under different circumstances would require only a few lines of code but, due to the architecture we have, those few lines turn up to be hundreds or even thousands because all layers need to be passed through.</p><p>The development was not the only area that suffered from monolithic architecture. We still needed to test and deploy everything every time there was a change or a release. It is not uncommon in enterprise environments to have applications that take hours to test, build and deploy. Testing, especially regression, tends to be a nightmare that in some cases last for months. As time passes, our ability to make changes that affect only one module is decreasing. The primary objective of layers is to make them in a way that they can be easily replaced or upgraded. That promise is almost never actually fulfilled. Replacing something in big monolithic applications is hardly ever easy and without risks.</p><p>Scaling monoliths often mean scaling the entire application thus producing very unbalanced utilization of resources. If we need more resources, we are forced to duplicate everything on<a class="indexterm" id="id83"/> a new server even if a bottleneck is one module. In that case, we often end up with a monolith replicated across multiple nodes with a load balancer on top. This setup is sub-optimum at best:</p><div class="mediaobject"><img alt="Monolithic Applications" src="graphics/B05848_03_03.jpg"/><div class="caption"><p>Figure 3-3 – Scaling monolithic application</p></div></div><div class="section" title="Services Split Horizontally"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec12"/>Services Split Horizontally</h2></div></div></div><p>
<span class="strong"><strong>Service-oriented architecture</strong></span> (<span class="strong"><strong>SOA</strong></span>) was created as a way to solve problems created by, often tightly <a class="indexterm" id="id84"/>coupled, monolithic applications. The<a class="indexterm" id="id85"/> approach is based on four main concepts we should implement:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Boundaries are explicit</li><li class="listitem" style="list-style-type: disc">Services are autonomous</li><li class="listitem" style="list-style-type: disc">Services share schema and contract but not class</li><li class="listitem" style="list-style-type: disc">Services compatibility is based on policy</li></ul></div><p>SOA was such a big hit that many software provides jumped right in and created products that should<a class="indexterm" id="id86"/> help us in the transition. The<a class="indexterm" id="id87"/> most used type born out of SOA movement is <span class="strong"><strong>Enterprise Service Bus</strong></span> (<span class="strong"><strong>ESB</strong></span>). At the same time, companies that experienced problems with monolithic applications and big systems jumped into the train and started the SOA transition with ESB as the locomotive. However, the common problem with this move is the way we are used working that often resulted in an intention to artificially apply SOA architecture into the existing model.</p><p>We continued having the same layers as we had before, but this time physically separated from each other. There is an apparent benefit from this approach in that we can, at least, develop and deploy each layer independently from others. Another improvement is scaling. With the physical separation between what used to be layers, we are allowed to scale better. That approach was often combined with acquisitions of one of the <span class="strong"><strong>enterprise service bus</strong></span> (<span class="strong"><strong>ESB</strong></span>) products. In between services we would put ESB that would be in<a class="indexterm" id="id88"/> charge of transformation and redirection of requests from one service to another. ESB and similar products are beasts of their own and we often end up with another monolithic application that is as big or even bigger than the one we tried to split. What we needed was to break services by bounded contexts and separate them physically with each running in their own process and with clearly defined communication between them. Thus, microservices were born.</p></div><div class="section" title="Microservices"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec13"/>Microservices</h2></div></div></div><p>Microservices are an approach to architecture and development of a single application composed of small<a class="indexterm" id="id89"/> services. The key to understanding microservices<a class="indexterm" id="id90"/> is their independence. Each is developed, tested and deployed separately from each other. Each service runs as a separate process. The only relation between different microservices is data exchange accomplished through APIs they are exposing. They inherit, in a way, the idea of small programs and pipes used in Unix/Linux. Most Linux programs are small and produce some output. That output can be passed as input to other programs. When chained, those programs can perform very complex operations. It is complexity born from a combination of many simple units.</p><p>In a way, microservices use the concepts defined by SOA. Then why are the called differently? SOA implementations went astray. That is especially true with the emergence of ESB products that themselves become big and complex enterprise applications. In many cases, after adopting one of the ESB products, the business went as usual with one more layer sitting on top of what we had before. Microservices movement is, in a way, reaction to misinterpretation of SOA and the intention to go back to where it all started. The main difference between SOA and microservices is that the latter should be self-sufficient and deployable independently of each other while SOA tends to be implemented as a monolith.</p><p>Let's see what Gartner has to say about microservices. While I'm not a big fan of their predictions, they<a class="indexterm" id="id91"/> do strike the important aspect of the market by <a class="indexterm" id="id92"/>appealing to big enterprise environments. Their evaluations of market tendencies usually mean that we passed the adoption by greenfield projects, and the technology is ready for the big enterprises. Here's what Gary Olliffe said about microservices at the beginning of 2015.</p><p>Microservice architectures promise to deliver flexibility and scalability to the development and deployment of service-based applications. But how is that promise delivered? In short, by adopting an architecture that allows individual services to be built and deployed independently and dynamically; an architecture that embraces DevOps practices.</p><p>Microservices are simpler, developers get more productive and systems can be scaled quickly and precisely, rather than in large monolithic globs. And I haven't even mentioned the potential for polyglot coding and data persistence.</p><p>Key aspects of microservices are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">They do one thing or are responsible for one functionality.</li><li class="listitem" style="list-style-type: disc">Each microservice can be built by any set of tools or languages since each is independent of others.</li><li class="listitem" style="list-style-type: disc">They are truly loosely coupled since each microservice is physically separated from others.</li><li class="listitem" style="list-style-type: disc">Relative independence between different teams developing different microservices (assuming that APIs they expose are defined in advance).</li><li class="listitem" style="list-style-type: disc">Easier testing and continuous delivery or deployment.</li></ul></div><p>One of the problems with microservices is the decision when to use them. In the beginning, while the application is still small, problems that microservices are trying to solve do not exist. However, once the application grows and the case for microservices can be made, the cost of switching to a different architecture style might be too big. Experienced teams tend to use microservices from the very start knowing that technical debt they might have to pay later will be more expensive than working with microservices from the very beginning. Often, as it was the case with Netflix, eBay, and Amazon, monolithic applications start evolving towards microservices gradually. New modules are developed as microservices and integrated with the rest of the system. Once they prove their worth, parts of the existing monolithic application gets refactored into microservices as well.</p><p>One of the things that often gets most critique from developers of enterprise applications is decentralization of data storage. While microservices can work (with few adjustments) using centralized data storage, the option to decentralize that part as well should, at least, be explored. The option <a class="indexterm" id="id93"/>to store data related to some service in a separate (decentralized) storage and pack it together into the same container or as a separate one and link<a class="indexterm" id="id94"/> them together is something that in many cases could be a better option than storing that data in a centralized database. I am not proposing always to use decentralized storage but to have that option in account when designing microservices.</p><p>Finally, we often employ some kind of a lightweight proxy server that is in charge of the orchestration of all requests no matter whether they come from outside or from one microservice to another:</p><div class="mediaobject"><img alt="Microservices" src="graphics/B05848_03_04.jpg"/><div class="caption"><p>Figure 3-4 – Microservices with a proxy service</p></div></div><p>Armed with a basic knowledge about monoliths and microservices, let us compare the two and evaluate their strengths and weaknesses.</p></div></div></div>
<div class="section" title="Monolithic Applications and Microservices Compared"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec10"/>Monolithic Applications and Microservices Compared</h1></div></div></div><p>From what we learned by now, seems that microservices are a better option than monoliths. Indeed, in many (but far from all) cases they are. However, there is no such thing as a free lunch. Microservices have their set of disadvantages with increased operational and deployment complexity, and remote process calls being the most common.</p><div class="section" title="Operational and Deployment Complexity"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec14"/>Operational and Deployment Complexity</h2></div></div></div><p>The primary argument against microservices is increased operational and deployment complexity. This argument is correct, but thanks to relatively new tools it can be mitigated. <span class="strong"><strong>Configuration Management</strong></span> (<span class="strong"><strong>CM</strong></span>) tools can handle environment setups and deployments<a class="indexterm" id="id95"/> with relative ease. Utilization<a class="indexterm" id="id96"/> of <a class="indexterm" id="id97"/>containers with Docker significantly reduces deployment pains that microservices can cause. CM tools together with containers allow us to deploy and scale microservices quickly.</p><p>In my opinion, increased deployment complexity argument usually does not take into account advances we saw during last years and is greatly exaggerated. That does not mean that part of the work is not shifted from development to DevOps. It is. However, benefits are in many cases bigger than the inconvenience that change produces.</p></div><div class="section" title="Remote Process Calls"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec15"/>Remote Process Calls</h2></div></div></div><p>Another argument for monolithic applications is reduced performance produced by microservices remote process calls. Internal calls through classes and methods are faster and this<a class="indexterm" id="id98"/> problem cannot<a class="indexterm" id="id99"/> be removed. How much that loss of performance affects a system depends on case to case basis. The important factor is how we split our system. If we take it towards the extreme with very small microservices (some propose that they should not have more than 10-100 lines of code), this impact might be considerable. I like to create microservices organized around bounded contexts or functionality like users, shopping cart, products, and so on. That reduces the number of remote process calls but still keep services organization within healthy boundaries. Also, it's important to note that if calls from one microservice to another are going through a fast internal LAN, the negative impact is relatively small.</p><p>So, what are the advantages microservices have over monoliths? The following list is by no means final nor it represents advantages only available with microservices. While many of them are valid for other types of architecture, they are more prominent with microservices.</p></div><div class="section" title="Scaling"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec16"/>Scaling</h2></div></div></div><p>Scaling microservices is much easier than monolithic applications. With monoliths, we duplicate <a class="indexterm" id="id100"/>the whole application into a new machine. On the other hand, with microservices, we duplicate only those that<a class="indexterm" id="id101"/> need scaling. Not only that we can scale what needs to be scaled but we can distribute things better. We can, for example, put a service that has heavy utilization of CPU together with another one that uses a lot of RAM while moving the other CPU demanding service to a different hardware.</p></div><div class="section" title="Innovation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec17"/>Innovation</h2></div></div></div><p>Monolithic applications, once the initial architecture is made, do not leave much space for innovation. I'd go even further and claim that monoliths are innovation killers. Due to their nature, changing<a class="indexterm" id="id102"/> things takes time, and experimentation is perilous since it potentially affects everything. One cannot, for example, change Apache Tomcat for NodeJS just because it would better suit one particular module.</p><p>I'm not suggesting that we should change programming language, server, persistence, and other architecture aspects for each module. However, monolithic servers tend to go to an opposite extreme where changes are risky if not unwelcome. With microservices, we can choose what we think is the best solution for each service separately. One might use Apache Tomcat while the other could use NodeJS. One can be written in Java and the other in Scala. I'm not advocating that each service is different from the rest but that each can be made in a way we think is best suited for the goal at hand. On top of that, changes and experiments are much easier to do. After all, whatever we do affects only one out of many microservices and not the system as a whole as long as the API is respected.</p></div><div class="section" title="Size"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec18"/>Size</h2></div></div></div><p>Since microservices are small, they are much easier to understand. There is much less code to go<a class="indexterm" id="id103"/> through to see what one microservice is doing. That in itself greatly simplifies development, especially when newcomers join the project. On top of that, everything else tends to be much faster. IDEs work faster with a small project when compared to big ones used in monolithic applications. They start faster since there are no huge servers nor an enormous number of libraries to load.</p></div><div class="section" title="Deployment, Rollback, and Fault Isolation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Deployment, Rollback, and Fault Isolation</h2></div></div></div><p>Deployment is much faster and easier with microservices. Deploying something small is always <a class="indexterm" id="id104"/>quicker (if not easier) than deploying<a class="indexterm" id="id105"/> something big. In case<a class="indexterm" id="id106"/> we realized that there is a problem, that problem has potentially limited effect and can be rolled back much easier. Until we roll back, the fault is isolated to a small part of the<a class="indexterm" id="id107"/> system. Continuous <a class="indexterm" id="id108"/>delivery or deployment<a class="indexterm" id="id109"/> can be done with speed and frequencies that would not be possible with big applications.</p></div><div class="section" title="Commitment Term"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec20"/>Commitment Term</h2></div></div></div><p>One of the common problems with monolithic applications is commitment. We are often forced to choose<a class="indexterm" id="id110"/> from the start the architecture and the technologies that will last for a long time. After all, we're building something big that should last for a long time. With microservices that need for a long-term commitment is much smaller. Change the programming language in one microservice and if it turns out to be a good choice, apply it to others. If the experiment failed or is not the optimum, there's only one small part of the system that needs to be redone. Same applies to frameworks, libraries, servers, and so on. We can even use different databases. If some lightweight NoSQL seems like the best fit for a particular microservice, why not use it and pack it into the container?</p><p>Let us go one step back and look at this subject from the prism of deployment. How do those two architectural approaches differ when the time comes to deploy our applications.</p></div><div class="section" title="Deployment Strategies"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"/>Deployment Strategies</h2></div></div></div><p>We already discussed that continuous delivery and deployment strategies require us to rethink all aspects of the application lifecycle. That is nowhere more noticeable than at the very<a class="indexterm" id="id111"/> beginning when we are faced with architectural choices. We won't go into details of every possible deployment strategy we could face but limit the scope to two major decisions that we should make. First one is architecturally related to the choice between monolithic applications and microservices. The second one is related to how we package the artifacts that should be deployed. More precisely, whether we should perform mutable or immutable deployments.</p></div><div class="section" title="Mutable Monster Server"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>Mutable Monster Server</h2></div></div></div><p>Today, the most common way to build and deploy applications is as a mutable monster server. We create a web server that has the whole application and update it every time there is<a class="indexterm" id="id112"/> a new release. Changes <a class="indexterm" id="id113"/>can be in configuration (properties file, XMLs, DB tables, and so on), code artifacts (JARs, WARs, DLLs, static files, and so on) and <span class="emphasis"><em>database schemas and data</em></span>. Since we are changing it on every release, it is mutable.</p><p>With mutable servers, we cannot know for sure that development, test, and production environments<a class="indexterm" id="id114"/> are the<a class="indexterm" id="id115"/> same. Even different nodes in the production might have undesirable differences. Code, configuration or static files might not have been updated in all instances.</p><p>It is a monster server since it contains everything we need as a single instance. Back-end, front-end, APIs, and so on. Moreover, it grows over time. It is not uncommon that after some time no one is sure what is the exact configuration of all pieces in production and the only way to accurately reproduce it somewhere else (new production node, test environment, and so on) is to copy the VM where it resides and start fiddling with configurations (IPs, host file, DB connections, and so on). We just keep adding to it until we lose the track of what it has. Given enough time, your "perfect" design and impressive architecture will become something different. New layers will be added, the code will be coupled, patches on top of patches will be created and people will start losing themselves in the maze the code start looking like. Your beautiful little project will become a big monster.</p><p>The pride you have will become a subject people talk about on coffee breaks. People will start saying that the best thing they could do is to throw it to trash and start over. But, the monster is already too big to start over. Too much is invested. Too much time would be needed to rewrite it. Too much is at stake. Our monolith might continue existing for a long time:</p><div class="mediaobject"><img alt="Mutable Monster Server" src="graphics/B05848_03_05.jpg"/><div class="caption"><p>Figure 3-5 – Mutable application server as initially designed</p></div></div><p>Mutable deployments might look simple, but they are usually not. By coupling everything into one place, we are hiding complexity thus increasing the chance of discrepancies between different instances.</p><p>Time to restart such a server when it receives a new release can be considerable. During that time <a class="indexterm" id="id116"/>server is usually <a class="indexterm" id="id117"/>not operational. Downtime that the new release provokes is a loss of money and trust. Today's business expects us to operate 24/7 without any downtime, and it is not uncommon that a release to production means night work of the team during which our services are not available. Given such a situation, applying continuous deployment is a dream out of the reach. It is a dream that can not become a reality.</p><p>Testing is also a problem. No matter how much we tested the release on development and test environments, the first time it will be tried in production is when we deploy it and make it available not only to our testers but also to all our users.</p><p>Moreover, fast rollback of such a server is close to impossible. Since it is mutable, there is no "photo" of the previous version unless we create a snapshot of a whole virtual machine that brings up a whole new set of problems.</p><p>By having architecture like this, we cannot fulfill all, if any, of the requirements described earlier. We cannot deploy often, due to inability to produce zero-downtime and easily rollback. Full automation is risky due to mutable nature of its architecture thus preventing us to be fast.</p><p>By not deploying often we are accumulating changes that will be released and, in that way, we are increasing the probability of a failure.</p><p>To solve those problems, deployments should be immutable and composed of small, independent, and self-sufficient applications. Remember, our goals are to deploy often, have zero-downtime, be able to rollback any release, be automated and be fast. Moreover, we should be able to test the release on production environment before users see it.</p><div class="section" title="Immutable Server and Reverse Proxy"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec01"/>Immutable Server and Reverse Proxy</h3></div></div></div><p>Each traditional deployment introduces a risk tied with changes that need to be performed on the server. If we<a class="indexterm" id="id118"/> change our architecture to<a class="indexterm" id="id119"/> immutable deployments, we gain immediate<a class="indexterm" id="id120"/> benefits. Provisioning of environments becomes <a class="indexterm" id="id121"/>much simpler since there is no need to think about applications (they are unchangeable). Whenever we deploy an image or a container to the production server, we know that it is precisely the same as the one we built and tested. Immutable deployments reduce the risk tied to unknown. We know that each deployed instance is exactly the same as the other. Unlike mutable deployment, when a package is immutable and contains everything (application server, configurations, and artifacts) we stop caring about all those things. They were packaged for us throughout the deployment pipeline and all we have to do is make sure that the immutable package<a class="indexterm" id="id122"/> is sent to the destination server. It is the same package as the one we already<a class="indexterm" id="id123"/> tested in other environments and<a class="indexterm" id="id124"/> inconsistencies<a class="indexterm" id="id125"/> that could be introduced by mutable deployments are gone.</p><p>A reverse proxy can be used to accomplish zero-downtime. Immutable servers together with a reverse proxy in a simplified form can be as follows.</p><p>First we start with a reverse proxy that points to our fully self-sufficient immutable application package. This package could be a virtual machine or a container. We'll refer to this application as application image to establish a clear distinction from mutable applications. On top of the application is a proxy service that routes all the traffic towards the final destination instead of exposing the server directly:</p><div class="mediaobject"><img alt="Immutable Server and Reverse Proxy" src="graphics/B05848_03_06.jpg"/><div class="caption"><p>Figure 3-6: Immutable application server deployed as an image (a virtual machine or a container)</p></div></div><p>Once we decide to deploy a new version, we do it by deploying a separate image to a separate server. While in some cases we could deploy this image to the same server, more often than not, monolithic applications are very resource demanding and we cannot have both on the same node without affecting the performance. At this moment, we have two instances. One old (previous release) and one new (latest release). All traffic still goes to the old server through the reverse proxy so users of our application still do not notice any change. For them, we're still running the old and proven software. This is the right moment to execute the final set of tests. Preferably those tests are automatic and part of the deployment process but manual verification is not excluded. For example, if changes were done to front-end, we might want to do the final round of user experience tests. No matter what types <a class="indexterm" id="id126"/>of tests are performed, they should<a class="indexterm" id="id127"/> all attack the new release bypassing <a class="indexterm" id="id128"/>the reverse proxy. The good thing about those tests is<a class="indexterm" id="id129"/> that we are working with the future production version of the software that resides on production hardware. We are testing production software and hardware without affecting our users (they are still being redirected to the old version). We could even enable our new release only to a limited number of users in the form of A/B testing.</p><p>To summarize, at this stage we have two instances of the server, one (the previous release) used by our users and the other (the latest release) used for testing:</p><div class="mediaobject"><img alt="Immutable Server and Reverse Proxy" src="graphics/B05848_03_07.jpg"/><div class="caption"><p>Figure 3-7 – New release of the immutable application deployed to a separate node</p></div></div><p>Once we are finished with tests and are confident that the new release works as expected, all we have to do is change the reverse proxy to point to the new release. The old one can stay for a while in case we need to rollback the changes. However, for our users, it does not exist. All traffic is routed to the new release. Since the latest release was up-and-running before we changed the routing, the switch itself will not interrupt our service (unlike, for example, if we would need to restart the server in case of mutable deployments). When the route is changed we need to reload our reverse proxy. As an example, <span class="emphasis"><em>nginx</em></span> maintains old connections until all of them are switched to the new route:</p><div class="mediaobject"><img alt="Immutable Server and Reverse Proxy" src="graphics/B05848_03_08.jpg"/><div class="caption"><p>Figure 3-8: Poxy is rerouted to point to the new release</p></div></div><p>Finally, when we do not need the old version, we can remove it. Even better, we can let the next release<a class="indexterm" id="id130"/> remove it for us. In the latter case, when<a class="indexterm" id="id131"/> the time comes, release process will <a class="indexterm" id="id132"/>remove the older release and start the process all over<a class="indexterm" id="id133"/> again:</p><div class="mediaobject"><img alt="Immutable Server and Reverse Proxy" src="graphics/B05848_03_09.jpg"/><div class="caption"><p>Figure 3-9 – The old release is removed</p></div></div><p>The technique described <a class="indexterm" id="id134"/>above is called blue-green<a class="indexterm" id="id135"/> deployment and has been in use<a class="indexterm" id="id136"/> for a long time. We'll be practicing it later on when we reach<a class="indexterm" id="id137"/> the Docker packaging and deployment examples.</p></div><div class="section" title="Immutable Microservices"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec02"/>Immutable Microservices</h3></div></div></div><p>We can do even better than this. With immutable deployments, we can easily accomplish automatism of the process. Reverse proxy gives us zero-downtime and, having two releases up<a class="indexterm" id="id138"/> and running allows us to rollback <a class="indexterm" id="id139"/>easily. However, since we're still dealing with one big application, deployment and tests might take a long time to run. That in itself might prevent us from being fast and thus from deploying as often as needed. Moreover, having everything as one big server increases development, testing and deployment complexity. If things could be split into smaller pieces, we might divide complexity into easily manageable chunks. As a bonus, having small independent services would allow us to scale more easily. They can be deployed to the same machine, scaled out across the network or multiplied if the performance of one of them becomes the bottleneck. Microservices to the rescue!</p><p>With monster applications we tend to have decoupled layers. Front-end code should be separated from the back-end, business layer from data access layer, and so on. With microservices, we should start thinking in a different direction. Instead of having the business layer separated from the data access layer, we would separate services. For example, user's management could be split from the sales service. Another difference is physical. While<a class="indexterm" id="id140"/> traditional architecture separates <a class="indexterm" id="id141"/>on a level of packages and classes but still deploys everything together, microservices are split physically; they might not even reside on the same physical machine.</p><p>Deployment of microservices follows the same pattern as previously described. We deploy our microservice immutable image as any other software:</p><div class="mediaobject"><img alt="Immutable Microservices" src="graphics/B05848_03_10.jpg"/><div class="caption"><p>Figure 3-10 – Immutable microservice deployed as an image (a virtual machine or a container)</p></div></div><p>When the time comes to release a new version of some microservice we deploy it alongside the older version:</p><div class="mediaobject"><img alt="Immutable Microservices" src="graphics/B05848_03_11.jpg"/><div class="caption"><p>Figure 3-11 – New release of the immutable microservice deployed alongside the old release</p></div></div><p>When that microservice<a class="indexterm" id="id142"/> release is properly tested <a class="indexterm" id="id143"/>we change the proxy route.</p><div class="mediaobject"><img alt="Immutable Microservices" src="graphics/B05848_03_12.jpg"/><div class="caption"><p>Figure 3-12 – Poxy is re-configured to point to the new release</p></div></div><p>Finally, we remove the older version of the microservice.</p><div class="mediaobject"><img alt="Immutable Microservices" src="graphics/B05848_03_13.jpg"/><div class="caption"><p>Figure 3-13 – The old release is removed</p></div></div><p>The only significant difference is that due to the size of microservices, we often do not need a separate<a class="indexterm" id="id144"/> server to deploy the new release<a class="indexterm" id="id145"/> in parallel with the old one. Now we can truly deploy often automatically, be fast with zero-downtime and rollback in case something goes wrong.</p><p>Technologically, this architecture might pose particular problems that will be the subject of the next chapters. For now, let's just say that those problems are easy to solve with the tools and processes we have at our disposal.</p><p>Given our requirements that are poor at best and the advantages microservices bring over monoliths, the choice is clear. We will be building our application using immutable microservices<a class="indexterm" id="id146"/> approach. That decision calls for a <a class="indexterm" id="id147"/>discussion about the best practices we should follow.</p></div></div></div>
<div class="section" title="Microservices Best Practices"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec11"/>Microservices Best Practices</h1></div></div></div><p>Most of the following best practices can be applied to services oriented architecture in general. However, with<a class="indexterm" id="id148"/> microservices, they become even more significant or beneficial. Following is a very brief description that will be extended later on throughout the book when the time comes to apply them.</p><div class="section" title="Containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Containers</h2></div></div></div><p>Dealing with many microservices can quickly become a very complex endeavor. Each can be written in a different programming language, can require a different (hopefully light) application server<a class="indexterm" id="id149"/> or can use a different set of libraries. If each service is packed as a container, most of those problems will go away. All we have to do is run the container with, for example, Docker and trust that everything needed is inside it.</p><p>Containers are self-sufficient bundles that contain everything we need (with the exception of the kernel), run<a class="indexterm" id="id150"/> in an isolated process and are immutable. Being self-sufficient means that a container commonly has the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Runtime libraries (JDK, Python, or any other library required for the application to run)</li><li class="listitem" style="list-style-type: disc">Application server (Tomcat, nginx, and so on)</li><li class="listitem" style="list-style-type: disc">Database (preferably lightweight)</li><li class="listitem" style="list-style-type: disc">Artifact (JAR, WAR, static files, and so on)<div class="mediaobject"><img alt="Containers" src="graphics/B05848_03_14.jpg"/><div class="caption"><p>Figure 3-14 – Self-sufficient microservice inside a container</p></div></div></li></ul></div><p>Fully self-sufficient containers are the easiest way to deploy services but pose a few problems with scaling. If we'd like to scale such a container on multiple nodes in a cluster, we'd need<a class="indexterm" id="id151"/> to make sure that databases embedded into<a class="indexterm" id="id152"/> those containers are synchronized or that their data volumes are located on a shared drive. The first option often introduces unnecessary complexity while shared volumes might have a negative impact on performance. Alternative is to make containers almost self-sufficient by externalizing database into a separate container. In such a setting there would be two different containers per each service. One for the application and the other for the database. They would be linked (preferably through a proxy service). While such a combination slightly increases deployment complexity, it provides greater freedom when scaling. We can deploy multiple instances of the application container or several instances of the database depending performance testing results or increase in traffic. Finally, nothing prevents us to scale both if such a need arises.</p><div class="mediaobject"><img alt="Containers" src="graphics/B05848_03_15.jpg"/><div class="caption"><p>Figure 3-15 – Microservice inside a container with the separate database</p></div></div><p>Being self-sufficient and immutable allows us to move containers across different environments (development, testing, production, and so on) and always expect the same results. Those<a class="indexterm" id="id153"/> same characteristics combined with<a class="indexterm" id="id154"/> microservices approach of building small applications allows us to deploy and scale containers with very little effort and much lower risk than other methods would allow us.</p><p>However, there is a third commonly used combination when dealing with legacy systems. Even though we might decide to gradually move from monolithic applications towards microservices, databases tend to be the last parts of the system to be approved for refactoring. While this is far from the optimal way to perform the transition, the reality, especially in big enterprises is that data is the most valuable asset. Rewriting an application poses much lower risk than the one we'd be facing if we decide to restructure data. It's often understandable that management is very skeptical of such proposals. In such a case we might opt for a shared database (probably without containers). While such a decision would be partly against what we're trying to accomplish with microservices, the pattern that works best is to share the database but make sure that each schema or a group of tables is<a class="indexterm" id="id155"/> exclusively accessed by a single service. The<a class="indexterm" id="id156"/> other services that would require that data would need to go through the API of the service assigned to it. While in such a combination we do not accomplish clear separation (after all, there is no clearer more apparent than physical), we can at least control who accesses the data subset and have a clear relation between them and the data.</p><p>Actually, that is very similar to what is commonly the idea behind horizontal layers. In practice, as the monolithic application grows (and with it the number of layers) this approach tends to get abused and ignored. Vertical separation (even if a database is shared), helps us keep much clearer bounded context each service is in charge of:</p><div class="mediaobject"><img alt="Containers" src="graphics/B05848_03_16.jpg"/><div class="caption"><p>Figure 3-16 – Microservices inside containers accessing the shared database</p></div></div></div></div>
<div class="section" title="Proxy Microservices or API Gateway"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec12"/>Proxy Microservices or API Gateway</h1></div></div></div><p>Big enterprise <a class="indexterm" id="id157"/>front-ends might need to invoke tens or even hundreds<a class="indexterm" id="id158"/> of HTTP requests (as is the case with Amazon.com). Requests often take more time to be invoked than to receive response data. Proxy microservices might help in that case. Their goal is to invoke<a class="indexterm" id="id159"/> different microservices and return an aggregated service. They <a class="indexterm" id="id160"/>should not contain any logic but only group several responses together and respond with aggregated data to the consumer.</p><div class="section" title="Reverse Proxy"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Reverse Proxy</h2></div></div></div><p>Never expose microservice API directly. If there isn't some orchestration, the dependency between the<a class="indexterm" id="id161"/> consumer and the microservices becomes so big<a class="indexterm" id="id162"/> that it might remove freedom that microservices are supposed to give us. Lightweight servers like <span class="emphasis"><em>nginx</em></span>, <span class="emphasis"><em>Apache Tomcat</em></span>, and <span class="emphasis"><em>HAProxy</em></span> are excellent at performing reverse proxy tasks and can easily be employed with very little overhead.</p></div><div class="section" title="Minimalist Approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/>Minimalist Approach</h2></div></div></div><p>Microservices should contain only packages, libraries, and frameworks that they truly need. The smaller they are, the better. That is quite in contrast to the approach used with monolithic applications. While previously we might have used JEE servers like JBoss that packed <a class="indexterm" id="id163"/>all the tools that we might or might not need, microservices work best with much more minimalistic solutions. Having hundreds of microservices with each of them having a full JBoss server becomes overkill. <span class="emphasis"><em>Apache Tomcat</em></span>, for example, is a much better option. I tend to go for even smaller solutions with, for instance, <span class="emphasis"><em>Spray</em></span> as a very lightweight RESTful API server. Don't pack what you don't need.</p><p>The same approach should be applied to OS level as well. If we're deploying microservices as <span class="emphasis"><em>Docker</em></span> containers, <span class="emphasis"><em>CoreOS</em></span> might be a better solution than, for example, <span class="emphasis"><em>Red Hat</em></span> or <span class="emphasis"><em>Ubuntu</em></span>. It's free from things we do not need allowing us to obtain better utilization of resources. However, as we'll see later, choosing OS is not always that simple.</p></div><div class="section" title="Configuration Management"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Configuration Management</h2></div></div></div><p>As the number of microservices grows, the need for <span class="strong"><strong>Configuration Management</strong></span> (<span class="strong"><strong>CM</strong></span>) increases. Deploying<a class="indexterm" id="id164"/> many microservices without tools like <span class="emphasis"><em>Puppet</em></span>, <span class="emphasis"><em>Chef or Ansible</em></span> (just to name few) quickly becomes a<a class="indexterm" id="id165"/> nightmare. Actually, not using CM tools for any but simplest solutions is a waste, with or without microservices.</p></div><div class="section" title="Cross-Functional Teams"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"/>Cross-Functional Teams</h2></div></div></div><p>While no rule<a class="indexterm" id="id166"/> dictates what kinds of teams are utilized, microservices are done best when the team working on one is multifunctional. A single team should be responsible for it from the start (design) until the finish (deployment and maintenance). They are too small to be handled from one team to another (architecture/design, development, testing, deployment and maintenance teams). Preference is to have a team that is in charge of the full lifecycle of a microservice. In many cases, one team might be in charge of multiple microservices, but multiple teams should not be in charge of one.</p></div><div class="section" title="API Versioning"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/>API Versioning</h2></div></div></div><p>Versioning should be applied to any API, and this holds true for microservices as well. If some change<a class="indexterm" id="id167"/> breaks the API format, it should be released as a separate version. In the case of public APIs as well as those used by other internal services, we cannot be sure who is using them and, therefore, must maintain backward compatibility or, at least, give consumers enough time to adapt.</p></div><div class="section" title="Final Thoughts"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"/>Final Thoughts</h2></div></div></div><p>Microservices as a concept existed for a long time. Take a look at the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ps aux | grep jav[a] | awk '{print $2}' | xargs kill</strong></span>
</pre></div><p>The command listed above is an example of the usage of <span class="emphasis"><em>pipes</em></span> in Unix/Linux. It consists of four programs. Each<a class="indexterm" id="id168"/> of them is expecting an input (<code class="literal">stdin</code>) and/or an output (<code class="literal">stdout</code>). Each of them is highly specialized and performs one or very few functions. While simple by themselves, when combined those programs are capable performing some very complex operations. Same holds true for most of the programs found in today's Unix/Linux distributions. In this particular case, we're running <code class="literal">ps aux</code> that retrieves the list of all running processes and passing the output to the next in line. That output is used by <code class="literal">grep jav[a]</code> to limit the results to only Java processes. Again, the output is passed to whoever needs it. In this particular example, next in line is <code class="literal">awk '{print $2}'</code> that does, even more, filtering and returns only the second column that happens to be the process ID. Finally, <code class="literal">xargs kill</code> takes the output of <span class="emphasis"><em>awk</em></span> as input and kills all processes that match IDs we retrieved previously.</p><p>Those not used to Unix/Linux might think that the command we just examined is an overkill. However, after a bit of practice, those working with Linux commands find this approach very flexible and useful. Instead of having "big" programs that need to contemplate all possible use cases, we have a lot of small programs that can be combined to fulfill almost any task we might need. It is a power born out of utmost simplicity. Each program is small and created to achieve a very specific objective. More importantly, they all accept clearly defined input and produce well-documented output.</p><p>Unix is, as far as I know, the oldest example of microservices still in use. A lot of small, specific, easy to reason with services with well-defined interfaces.</p><p>Even though microservices exist for a long time, it is not a chance that they become popular only recently. Many things needed to mature and be available for microservices to be useful to all<a class="indexterm" id="id169"/> but selected few. Some of the concepts that made microservices widely used are domain-driven design, continuous delivery, containers, small autonomous teams, scalable systems, and so on. Only when all those are combined into a single framework microservices start to shine truly.</p><p>Microservices are used to create complex systems composed of small and autonomous services that exchange data through their APIs and limit their scope to a very specific bounded context. From a certain point of view, microservices are what object-oriented programming was initially designed to be. When you read thoughts of some of the leaders of our industry and, especially, object-oriented programming, their descriptions of best practices when absorbed for their logic and not the way authors implemented them initially, are the reminiscence of what microservices are today. The following quotes correctly describe some of the aspects of microservices.</p><div class="blockquote"><table border="0" cellpadding="0" cellspacing="0" class="blockquote" summary="Block quote" width="100%"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>The big idea is 'messaging'. The key in making great and growable systems is much more to design how its modules communicate rather than what their internal properties and behaviors should be.</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td align="right" colspan="2" style="text-align: center" valign="top">--<span class="attribution"><span class="emphasis"><em>Alan Kay</em></span></span></td></tr></table></div><div class="blockquote"><table border="0" cellpadding="0" cellspacing="0" class="blockquote" summary="Block quote" width="100%"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>Gather together those things that change for the same reason, and separate those things that change for different reasons.</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td align="right" colspan="2" style="text-align: center" valign="top">--<span class="attribution"><span class="emphasis"><em>Robert C. Martin</em></span></span></td></tr></table></div><p>When implementing microservices, we tend to organize them to do only one thing or perform only one function. This allows us to pick the best tools for each of the jobs. For example, we can code<a class="indexterm" id="id170"/> them in a language that best suits the objective. Microservices are truly loosely coupled due to their physical separation and provide a great level of independence between different teams as long as APIs are clearly defined in advance. On top of that, with microservices, we have much faster and easier testing and continuous delivery or deployment due to their decentralized nature. When concepts we discussed are combined with the emergence of new tools, especially <span class="emphasis"><em>Docker</em></span>, we can see microservices in a new light and remove part of the problems their development and deployment was creating earlier.</p><p>Still, do not take bits of advice from this book as something that should be applied to all cases. Microservices are not an answer to all our problems. Nothing is. They are not the way all applications should be created and no single solution fits all cases. With microservices, we are trying to solve very specific problems and not to change the way all applications are designed.</p><p>Armed with the decision to develop our application around microservices, it is time to do something practical. There is no<a class="indexterm" id="id171"/> coding without development environment so that will be our first goal. We'll create a development environment for our fancy books store service.</p><p>We had enough theory and the time is ripe to put this book in front of a computer. From now on, most of the book will be a hands-on experience.</p></div></div></body></html>