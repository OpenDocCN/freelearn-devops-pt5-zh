- en: Chapter 5. Benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In optimizing our Docker applications, it is important to validate the parameters
    that we tuned. Benchmarking is an experimental way of identifying if the elements
    we modified in our Docker containers performed as expected. Our application will
    have a wide area of options to be optimized. The Docker hosts running them have
    their own set of parameters such as memory, networking, CPU, and storage as well.
    Depending on the nature of our application, one or more of these parameters can
    become a bottleneck. Having a series of tests to validate each component with
    benchmarks is important for guiding our optimization strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, by creating proper performance tests, we can also identify the
    limits of the current configuration of our Docker-based application. With this
    information, we can start exploring infrastructure parameters such as scaling
    out our application by deploying them on more Docker hosts. We can also use this
    information to scale up the same application by moving our workload to a Docker
    host with higher storage, memory, or CPU. And when we have hybrid cloud deployments,
    we can use these measurements to identify which cloud provider gives our application
    its optimum performance.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring how our application responds to these benchmarks is important when
    planning the capacity needed for our Docker infrastructure. By creating a test
    workload simulating peak and normal conditions, we can predict how our application
    will perform once it is released to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics to benchmark a simple web
    application deployed in our Docker infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Apache JMeter for benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and designing a benchmark workload
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing application performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Apache JMeter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache JMeter is a popular application used to test the performance of web servers.
    Besides load testing web servers, the open source project grew to support testing
    other network protocols such as LDAP, FTP, and even raw TCP packets. It is highly
    configurable, and powerful enough to design complex workloads of different usage
    patterns. This feature can be used to simulate thousands of users suddenly visiting
    our web application thus inducing a spike in the load.
  prefs: []
  type: TYPE_NORMAL
- en: Another feature expected in any load-testing software is its data capture and
    analysis functions. JMeter has such a wide variety of data recording, plotting,
    and analysis features that we can explore the results of our benchmarks right
    away. Finally, it has a wide variety of plugins that may already have the load
    pattern, analysis, or network connection that we plan to use.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about the features and how to use Apache JMeter can be found
    on its website at [http://jmeter.apache.org](http://jmeter.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will deploy an example application to benchmark, and prepare
    our workstation to run our first JMeter-based benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a sample application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also bring our own web application we want to benchmark if we please.
    But for the rest of this chapter, we will benchmark the following application
    described in this section. The application is a simple Ruby web application deployed
    using Unicorn, a popular Ruby application server. It receives traffic via a Unix
    socket from Nginx. This setup is very typical for most Ruby applications found
    in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will deploy this Ruby application in a Docker host called
    `webapp`. We will use separate Docker hosts for the application, benchmark tools,
    and monitoring. This separation is important so that the benchmark and monitoring
    instrumentation we run doesn't affect the benchmark results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few steps show us how to build and deploy our simple Ruby web application
    stack:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create the Ruby application by creating the following Rack `config.ru`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we package the application as a Docker container with the following `Dockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will create the Nginx configuration file `nginx.conf`. It will forward
    requests to our Unicorn application server through the Unix socket that we created
    in the previous step. In logging the request, we will record `$remote_addr` and
    `$response_time`. We will pay particular attention to these metrics later when
    we analyze our benchmark results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding Nginx configuration will then be packaged as a Docker container
    with the following `Dockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last component will be a `docker-compose.yml` file to tie the two Docker
    containers together for deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the end, we will have the files shown in the following screenshot in our
    code base:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying a sample application](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After preparing our Dockerized web application, let us now deploy it to our
    Docker host by typing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker Compose is a tool for creating multi-container applications. It has a
    schema defined in YML to describe how we want our Docker containers to run and
    link to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Compose supports a curl | bash type of installation. To quickly install
    it on our Docker host, type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We only covered Docker Compose in passing in this chapter. However, we can get
    more information about Docker Compose on the documentation website found at [http://docs.docker.com/compose](http://docs.docker.com/compose).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let us conduct a preliminary test to determine if our application
    works properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now we are done preparing the application that we want to benchmark. In the
    next section, we will prepare our workstation to perform the benchmarks by installing
    Apache JMeter.
  prefs: []
  type: TYPE_NORMAL
- en: Installing JMeter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the rest of this chapter, we will use Apache JMeter version 2.13 to perform
    our benchmarks. In this section, we will download and install it in our workstation.
    Follow the next few steps to set up JMeter properly:'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, go to JMeter's download web page at [http://jmeter.apache.org/download_jmeter.cgi](http://jmeter.apache.org/download_jmeter.cgi).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the link for **apache-jmeter-2.13.tgz** to begin downloading the binary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When the download finishes, extract the tarball by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add the `bin/` directory to our `$PATH` so that JMeter can be
    easily launched from the command line. To do this, we will type the following
    command in our terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, launch JMeter by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now see the JMeter UI just like the following screenshot. Now we are
    finally ready to write the benchmark for our application!:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing JMeter](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that Apache JMeter is a Java application. According to the JMeter website,
    it requires at least Java 1.6 to work. Make sure you have a **Java Runtime Environment**
    (**JRE**) properly set up before installing JMeter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were in a Mac OSX environment, we could use Homebrew and just type the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For other platforms, the instructions described earlier should be sufficient
    to get started. More information on how to install JMeter can be found at [http://jmeter.apache.org/usermanual/get-started.html](http://jmeter.apache.org/usermanual/get-started.html).
  prefs: []
  type: TYPE_NORMAL
- en: Building a benchmark workload
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Writing benchmarks for an application is an open-ended area to explore. Apache
    JMeter can be overwhelming at first. It has several options to tune in order to
    write our benchmarks. To begin, we can use the "story" of our application as a
    start. The following are some of the questions we can ask ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: What does our application do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the persona of our users?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do they interact with our application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with these questions, we can then translate them into actual requests
    to our application.
  prefs: []
  type: TYPE_NORMAL
- en: In the sample application that we wrote in the earlier section, we have a web
    application that displays `Hello World` to our users. In web applications, we
    are typically interested with the throughput and response time. Throughput refers
    to the number of users that can receive `Hello World` at a time. Response time
    describes the time lag before the user receives the `Hello World` message from
    the moment they requested it.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will create a preliminary benchmark in Apache JMeter. Then
    we will begin analyzing our initial results with JMeter's analysis tools and the
    monitoring stack that we deployed in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*. After that, we will iterate on the benchmarks we developed, and
    tune it. This way, we know that we are benchmarking our application properly.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a test plan in JMeter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A series of benchmarks in Apache JMeter is described in a test plan. A test
    plan describes a series of steps that JMeter will execute like performing requests
    to a web application. Each step in a test plan is called an element. These elements
    themselves can have one or more elements as well. In the end, our test plan will
    look like a tree—an hierarchy of elements to describe the benchmark we want for
    our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add an element into our test plan, we simply right-click on the parent element
    that we want, and then select **Add**. This opens a context menu of elements that
    can be added to the selected parent element. In the following screenshot, we add
    a **Thread Group** element to the main element, **Test Plan**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a test plan in JMeter](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next few steps show the way to create a test plan conducting the benchmark
    that we want:'
  prefs: []
  type: TYPE_NORMAL
- en: First, let us rename the **Test Plan** to something more appropriate. Click
    on the **Test Plan** element. This will update the main JMeter window on the right.
    In the form field labeled **Name:**, set the value to **Unicorn Capacity**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under the **Unicorn Capacity** test plan, create a thread group. Name this
    **Application Users**. We will configure this thread group to send 10,000 requests
    to our application from a single thread in the beginning. Use the following parameters
    for filling out the form to achieve this setting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of Threads**: 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ramp-up Period**: 0 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop Count**: 120,000 times'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: When we start developing our test plans, having a low loop count is useful.
    Instead of 120,000 loop counts, we can begin with 10,000 or even just 10 instead.
    Our benchmarks are shorter, but we get immediate feedback when developing it such
    as when we proceed to the next step. When we finish the whole test plan, we can
    always revert and tune it later to generate more requests.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, under the **Application Users** thread group, we create the actual request
    by adding **Sampler, HTTP Request**. This is the configuration where we set the
    details of how we make a request to our web application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Name**: Go to `http://webapp/`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server Name**: `webapp`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we configure how to save the test results by adding a listener under
    the **Unicorn Capacity** test plan. For this, we will add a **Simple Data Writer,**
    and name it **Save Result**. We set the **Filename** field to `result.jtl` to
    save our benchmark results in the said file. We will refer to this file later
    when we analyze the result of the benchmark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we have a basic benchmark workload that generates 120,000 HTTP requests
    to `http://webapp/`. Then the test plan saves the result of each request in a
    file called `result.jtl`. The following is a screenshot of JMeter after the last
    step in creating the test plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a test plan in JMeter](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, it is time to run our benchmark. Go to the **Run** menu, then select
    **Start** to begin executing the test plan. While the benchmark is running, the
    **Start** button is grayed-out and disabled. When the execution finishes, it will
    be enabled again.
  prefs: []
  type: TYPE_NORMAL
- en: After running the benchmark, we will analyze the results by looking at the `result.jtl`
    file using JMeter's analysis tools in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various types of elements that can be placed in a JMeter test plan.
    Besides the three elements we used previously to create a basic benchmark for
    our application, there are several others that regulate requests, perform other
    network requests, and analyze data.
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive list of test plan elements and their description can be found
    on the JMeter page at [http://jmeter.apache.org/usermanual/component_reference.html](http://jmeter.apache.org/usermanual/component_reference.html).
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing benchmark results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will analyze the benchmark results, and identify how the
    120,000 requests affected our application. In creating web application benchmarks,
    there are typically two things we are usually interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: How many requests can our application handle at a time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For how long is each request being processed by our application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two low-level web performance metrics can easily translate to the business
    implications of our application. For example, how many customers are using our
    application? Another one is, how are they perceiving the responsiveness of our
    application from a user experience perspective? We can correlate secondary metrics
    in our application such as CPU, memory, and network to determine our system capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the results of JMeter runs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several listener elements of JMeter have features that render graphs. Enabling
    this when running the benchmark is useful when developing the test plan. But the
    time taken by the UI to render the results in real time, in addition to the actual
    benchmark requests, affects the performance of the test. Hence, it is better for
    us to separate the execution and analysis components of our benchmark. In this
    section, we will create a new test plan, and look at a few JMeter listener elements
    to analyze the data we acquired in `result.jtl`.
  prefs: []
  type: TYPE_NORMAL
- en: To begin our analysis, we first create a new test plan, and name this **Analyze
    Results**. We will add various listener elements under this test plan parent element.
    After this, follow the next few steps to add various JMeter listeners that can
    be used to analyze our benchmark result.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating throughput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our first analysis, we will use the **Summary Report** listener. This listener
    will show the throughput of our application. A measurement of throughput will
    show the number of transactions our application can handle per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'To display the throughput, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the listener, fill out the **Filename** field by selecting the
    `result.jtl` file that we generated when we ran our benchmark. For the run we
    did earlier, the following screenshot shows that the 120,000 HTTP requests were
    sent to `http://webapp/` at a throughput of 746.7 requests per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating throughput](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also look at how throughput evolved over the course of our benchmark
    with the **Graph Results** listener. Create this listener under the **Analyze
    Results** test plan element and name it **Throughput over time**. Make sure that
    only the **Throughput** checkbox is marked (feel free to look at the other data
    points later though). After creating the listener, load our `result.jtl` test
    result again. The following screenshot shows how the throughput evolved over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating throughput](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding screenshot, the throughput started slow while
    JMeter tries to warm up its single-thread pool of requests. But after our benchmark
    continues to run, the throughput level settles at a stable level. By having a
    large number of loop counts earlier in our thread group, we were able to minimize
    the effect of the earlier ramp-up period.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the throughput displayed in the **Summary Report** earlier is more
    or less a consistent result. Take note that the **Graph Results** listener wraps
    around its data points after several samples.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that in benchmarking, the more samples we get, the more precise our
    observations can be!
  prefs: []
  type: TYPE_NORMAL
- en: Plotting response time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another metric we are interested in when we benchmark our application is the
    **response time**. The response time shows the duration for which JMeter has to
    wait before receiving the web page response from our application. In terms of
    real users, we can look at this as the time our users typed our web application's
    URL to the time everything got displayed in their web browser (it may not represent
    the real whole picture if our application renders some slow JavaScript, but for
    the application we made earlier, this analogy should suffice).
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the response time of our application, we will use the **Response Time
    Graph** listener. As an initial setting, we can set the interval to 500 milliseconds.
    This will average some of the response times along 500 milliseconds in `result.jtl`.
    In the following image, you can see that our application''s response time is mostly
    at around 1 millisecond:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plotting response time](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If we want to display the response time in finer detail, we can decrease the
    interval to as low as 1 millisecond. Take note that this will take more time to
    display as the JMeter UI tries to plot more points in the application. Sometimes,
    when there are too many samples, JMeter may crash, because our workstation doesn't
    have enough memory to display the entire graph. In case of large benchmarks, we
    would be better off observing the results with our monitoring system. We will
    look at this data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Observing performance in Graphite and Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There might be a case when our workstation is so old that Java is not able to
    handle displaying 120,000 data points in its JMeter UI. To solve this, we can
    reduce the amount of data we have by either generating less requests in our benchmark
    or averaging out some of the data like we did earlier, when graphing response
    time. However, sometimes we want to see the full resolution of our data. This
    full view is useful when we want to inspect the finer details of how our application
    behaves. Fortunately, we already have a monitoring system in place for our Docker
    infrastructure that we built in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, our monitoring and logging systems are deployed in a Docker
    host called `monitoring`. Our Docker host `webapp` that runs our application containers
    will have Collected and Rsyslog send events to the Docker host `monitoring`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the Nginx configuration mentioned when describing our benchmarks?
    The access log generated from the standard of the Nginx container is captured
    by Docker. If we use the same setup of our Docker daemon in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*, these log events are captured by the local Rsyslog service. These
    Syslog entries will then be forwarded to the Logstash Syslog collector, and stored
    to Elasticsearch. We can then use the visualize feature of Kibana to look at the
    throughput of our application. The following analysis was made by counting the
    number of access log entries that Elasticsearch received per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Observing performance in Graphite and Kibana](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also plot our application''s response time during the course of the
    benchmark in Kibana. To do this, we first need to reconfigure our Logstash configuration
    to parse the data being received from the access log, and extract out the response
    time as a metric using filters. To do this, update `logstash.conf` from [Chapter
    4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e "Chapter 4. Monitoring
    Docker Hosts and Containers"), *Monitoring Docker Hosts and Containers*, to add
    the `grok {}` filter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logstash's Filter plugins are used to intermediately process events before they
    reach our target storage endpoint such as Elasticsearch. It transforms raw data
    such as lines of text to a richer data schema in JSON that we can then use later
    for further analysis. More information about Logstash Filter plugins can be found
    at [https://www.elastic.co/guide/en/logstash/current/filter-plugins.html](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `NGINXACCESS` pattern being referred to in the preceding code is defined
    externally in what the `grok {}` filter calls a `patterns` file. Write the following
    as its content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, rebuild our `hubuser/logstash` Docker container from [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*. Don''t forget to update the `Dockerfile` as follows to add the
    patterns file to our Docker context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we extracted the response times from the Nginx access logs, we can
    plot these data points in a Kibana visualization. The following is a screenshot
    of Kibana showing the average response time per second of the benchmark we ran
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Observing performance in Graphite and Kibana](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another result that we can explore is the way our Docker host `webapp` responds
    with the load received from our benchmark. First we can check how our web application
    consumes the CPU of our Docker host. Let''s log in to our monitoring system''s
    graphite-web dashboard and plot the metrics `webapp.cpu-0.cpu-*` except `cpu-idle`.
    As we can see in the following image, the CPU of our Docker host goes to 100 percent
    usage the moment we start sending our application a lot of requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Observing performance in Graphite and Kibana](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can explore other system measurements of our Docker host to see how it is
    affected by the load of HTTP requests that it gets. The important point is that
    we use this data and correlate it to see how our web application behaved.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache JMeter version 2.13 and later include a backend listener that we can
    use to send JMeter data measurements in real time to external endpoints. By default,
    it ships with support for the Graphite wire protocol. We can use this feature
    to send benchmark results to the Graphite monitoring infrastructure that we built
    in [Chapter 4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e
    "Chapter 4. Monitoring Docker Hosts and Containers"), *Monitoring Docker Hosts
    and Containers*. More information on how to use this feature is available at [http://jmeter.apache.org/usermanual/realtime-results.html](http://jmeter.apache.org/usermanual/realtime-results.html).
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we already have a basic workflow of creating a test plan in Apache
    JMeter and analyzing the preliminary results. From here, there are several parameters
    we can adjust to achieve our benchmark objectives. In this section, we will iterate
    on our test plan to identify the limits of our Docker application.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first parameter that we may want to tune is increasing the **Loop Count**
    of our test plan. Driving our test plan to generate more requests will allow us
    to see the effects of the load we induced to our application. This increases the
    precision of our benchmark experiments, because outlier events such as a slow
    network connection or hardware failure (unless we are testing that specifically!)
    affect our tests.
  prefs: []
  type: TYPE_NORMAL
- en: After having enough data points for our benchmarks, we may realize that the
    load being generated is not enough against our Docker application. For example,
    the current throughput we received from our first analysis may not simulate the
    behavior of real users. Let us say that we want to have 2000 requests per second.
    To increase the rate at which JMeter generates the requests, we can increase the
    number of threads in the thread group that we created earlier. This increases
    the number of concurrent requests that JMeter is creating at a time. If we want
    to simulate a gradual increase in the number of users, we can adjust the ramp-up
    period to be longer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For workloads where we want to simulate a sudden increase of users, we can stick
    with a ramp-up period of 0 to start all the threads right away. In cases where
    we want to tune other behaviors such as a constant load and then a sudden spike,
    we can use the **Stepping Thread Group** plugin.
  prefs: []
  type: TYPE_NORMAL
- en: We may also want to limit it to precisely just 100 requests per second. Here,
    we can use `Timer` elements to control how our threads generate the request. To
    start limiting throughput, we can use the **Constant Throughput Timer**. This
    will make JMeter automatically slow down threads when it perceives that the throughput
    it is receiving from our web application is increasing too much.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the benchmark techniques here are difficult to apply with the built-in
    Apache JMeter components. There are a variety of plugins that make it simpler
    to generate the load to drive our application. They are available as plugins.
    The Apache JMeter list of popularly used community plugins is found at [http://jmeter-plugins.org](http://jmeter-plugins.org).
  prefs: []
  type: TYPE_NORMAL
- en: Running distributed tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After tuning the concurrency parameters for a while, we realize that our result
    does not change. We may set JMeter to generate 10,000 requests at a time, but
    that will most likely crash our UI! In this case, we are already reaching the
    performance limits of our workstation while building the benchmarks. From this
    point, we can start exploring using a pool of servers that run JMeter to create
    distributed tests. Distributed tests are useful, because we can grab several servers
    from the cloud with higher performance to simulate spikes. It is also useful for
    creating load coming from several sources. This distributed setup is useful for
    simulating high-latency scenarios, where our users are accessing our Docker application
    from halfway across the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following steps for deploying Apache JMeter on several Docker hosts
    to perform a distributed benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create the following `Dockerfile` to create a Docker image called `hubuser/jmeter`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, provision the number of Docker hosts we want according to our cloud or
    server provider. Take note of the hostname or IP address of each Docker host.
    For our case, we created two Docker hosts called `dockerhost1` and `dockerhost2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will run the JMeter server on our Docker hosts. Log in to each of them,
    and type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To finalize our JMeter cluster, we will type the following command to launch
    the JMeter UI client connected to the JMeter servers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With an Apache JMeter cluster at our disposal, we are now ready to run distributed
    tests. Note that the number of threads in the test plan specifies the thread count
    on each JMeter server. In the case of the test plan we made in the earlier section,
    our JMeter benchmark will generate 240,000 requests. We should adjust these counts
    according to the test workload we have in mind. Some of the guidelines we mentioned
    in the previous section can be used to tune our remote tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to start the remote tests, select **Remote Start All** from the **Run**
    menu. This will spawn the thread groups we created in our test plan to our JMeter
    servers in `dockerhost1` and `dockerhost2`. When we look at our access logs of
    Nginx, we can now see that the IP sources are coming from two different sources.
    The following IP addresses come from each of our Docker hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information on distributed and remote testing can be found at [http://jmeter.apache.org/usermanual/remote-test.html](http://jmeter.apache.org/usermanual/remote-test.html).
  prefs: []
  type: TYPE_NORMAL
- en: Other benchmarking tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few other benchmarking tools specifically for benchmarking web-based
    applications. The following is a short list of such tools with their links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache** **Bench**: [http://httpd.apache.org/docs/2.4/en/programs/ab.html](http://httpd.apache.org/docs/2.4/en/programs/ab.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HP Lab''s** **Httperf**: [http://www.hpl.hp.com/research/linux/httperf](http://www.hpl.hp.com/research/linux/httperf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Siege**: [https://www.joedog.org/siege-home](https://www.joedog.org/siege-home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we created benchmarks for gauging the performance of our Docker
    application. By using Apache JMeter and the monitoring system we set up in [Chapter
    4](part0028_split_000.html#QMFO1-afc4585f6623427885a0b0c8e5b2e22e "Chapter 4. Monitoring
    Docker Hosts and Containers"), *Monitoring Docker Hosts and Containers,* we analyzed
    how our application behaved under various conditions. We now have an idea about
    the limitations of our application, and will use it to further optimize it or
    to scale it out.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will talk about load balancers for scaling-out our application
    to increase its capacity.
  prefs: []
  type: TYPE_NORMAL
