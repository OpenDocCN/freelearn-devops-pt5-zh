- en: Chapter 5. Orchestrating Load Balancers Using Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on some of the popular load balancing solutions that
    are available today and the approaches that they take to load balancing applications.
  prefs: []
  type: TYPE_NORMAL
- en: With the emergence of cloud solutions, such as AWS, Microsoft Azure, Google
    Cloud, and OpenStack, we will look at the impact this has had on load balancing
    with distributed load and centralized load balancing strategies. This chapter
    will show practical configuration management processes that can be used to orchestrate
    load balancers using Ansible to help automate the load balancing needs for applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Centralized and distributed load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular load balancing solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing immutable and static servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Ansible to orchestrate load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized and distributed load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the introduction of microservice architectures allowing development teams
    to make changes to production applications more frequently, developers no longer
    just need to release software on a quarterly basis.
  prefs: []
  type: TYPE_NORMAL
- en: With the move towards Continuous Delivery and DevOps, applications are now released
    weekly, daily, or even hourly with only one or a subset of those microservices
    being updated and released.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations have found microservice architectures to be easier to manage and
    have moved away from building monolith applications. Microservice applications
    break a larger application into smaller manageable chunks. This allows application
    features to be released to customers on a more frequent basis, as the business
    does not have to redeploy the whole product each time they release. This means
    only a small microservice needs to be redeployed to deploy a feature. As the release
    process is more frequent and continuous, then it is better understood, normally
    completely automated, and ultimately load balanced.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures can also be beneficial for large businesses, which
    are distributed across many offices or countries as different teams can own different
    microservices and release them independently of one another.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, means that development teams need a way of testing dependency
    management, and the onus is put on adequate testing to make sure that a microservice
    doesn't break other microservices when it is released.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, developers need to create mocking and stubbing services, so microservice
    applications can be effectively tested against multiple software versions without
    having to deploy the full production estate.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a microservice architecture is a huge mindset shift for a business
    but a necessary one to remain competitive. Releasing monolithic applications is
    often difficult and time-consuming for an organization, and businesses that have
    quarterly release cycles will eventually lose out to competitors that can release
    their features in a quicker, more granular way.
  prefs: []
  type: TYPE_NORMAL
- en: The use of microservice architectures has meant that being able to utilize the
    same load balancing in test environments as production has become even more important
    due to how dynamic environments need to be.
  prefs: []
  type: TYPE_NORMAL
- en: So having test environments load balancing configuration as close to production
    environments as possible is a must. Configuration management tooling can be used
    to control the desired state of the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: The delegation of responsibilities also needs to be reviewed to support microservice
    architectures, so control of some of the load balancing provisioning should move
    to development teams as opposed to being a request to the network team to make
    it manageable and not to impede development teams. This, of course, is a change
    in culture that needs sponsorship from senior management to make the required
    changes to the operational model.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing requirements when using microservice applications will evolve
    as an application is developed or scaled up and down in size, so it is important
    that these aspects are made available to developers to self-service requests rather
    than wait on a centralized network team to make load balancing changes.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of the shift towards microservices architectures, the networking
    and load balancing landscape has needed to evolve too to support those needs with
    PaaS solutions being created by many vendors to handle application deployment
    across hybrid cloud and load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Off-the-shelf PaaS solutions are a great option for companies that maybe aren't
    tech-savvy and are unable to create their own deployment pipelines using configuration
    management tooling, such as Chef, Puppet, Ansible, and Salt, to deploy their applications
    into cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the approach to deployment, roll your own or off-the-shelf PaaS.
    Both microservice and monolith applications still need to be supported when considering
    public, private, and hybrid clouds.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, networking and load balancing need to be adaptable to support varied
    workloads. Although the end goal for an organization is ultimately a microservice
    architecture, the reality for most companies is having to adopt a hybrid approach
    catering to centralized and distributed load balancing methods to support both
    monolithic and cloud native microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, load balancers were installed as external physical appliances
    with very complex designs and used very expensive equipment. Load balancers would
    be configured to serve web content with SSL requests terminated on the expensive
    physical appliances.
  prefs: []
  type: TYPE_NORMAL
- en: The load balancer would have complex configuration to route requests to applications
    using context switching, and requests would be served directly to the static backend
    servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was optimal for monolith configurations as applications typically were
    self-contained and followed a three-tier model:'
  prefs: []
  type: TYPE_NORMAL
- en: A frontend webserver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A business logic layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A database layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This didn't require a lot of east to west traffic within the network as the
    traffic was north to south, traversing the frontend, business logic, and database.
    Networks were designed to minimize the amount of time taken to process the request
    and serve it back to the end user, and it was always served by the core network
    each time.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the evolution towards microservice architectures, the way that applications
    operate has changed somewhat. Applications are less self-contained and need to
    talk to dependent microservices applications that exist within the same tenant
    network, or even across multiple tenants.
  prefs: []
  type: TYPE_NORMAL
- en: This means that east-west traffic within the data center is much higher, and
    that traffic in the data center doesn't always go through the core network like
    it once did.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters of microservices applications are instead instantiated and then load
    balanced within the tenant network using x86 software load balancing solutions
    with the endpoint of the microservices clusters **Virtual IP** (**VIP**) exposed
    to adjacent microservices that need to utilize it.
  prefs: []
  type: TYPE_NORMAL
- en: With the growing popularity of virtual machines, containers, and software-defined
    overlay networks, this means that software load balancing solutions are now used
    to load balance applications within the tenant network, as opposed to having to
    pin back to a centralized load balancing solution.
  prefs: []
  type: TYPE_NORMAL
- en: As a result load balancing vendors have had to adapt and produce virtualized
    or containerized versions of their physical appliances to stay competitive with
    open source software load balancing solutions, which are routinely used with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Popular load balancing solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As applications have moved from monoliths to microservices, load balancing requirements
    have undoubtedly changed. Today, we have seen a move towards open source load
    balancing solutions, which are tightly integrated with virtual machines and containers
    to serve east to west traffic between VPC in AWS or a tenant network in OpenStack
    as opposed to pinning out to centralized physical appliances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open source load balancing solutions are now available from **Nginx** and **HAProxy**
    to help developers load balance their applications or AWS elastic load balancing
    feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/)'
  prefs: []
  type: TYPE_NORMAL
- en: Just a few years ago, Citrix NetScalers ([https://www.citrix.com/products/netscaler-adc/](https://www.citrix.com/products/netscaler-adc/))
    and F5 Big-IP ([https://f5.com/products/big-ip](https://f5.com/products/big-ip))
    solutions had the monopoly in the enterprise load balancing space, but the load
    balancing landscape has changed significantly with a multitude of new solutions
    available.
  prefs: []
  type: TYPE_NORMAL
- en: New load balancing start-ups such as Avi networks ([https://avinetworks.com/](https://avinetworks.com/))
    focus on x86 compute and software solutions to deliver load balancing solutions,
    which have been created to assist with both modern micros-service applications
    and monolith applications to support both distributed and centralized load balancing
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this book is not about which load balancing vendor solution is the
    best; there is no *one size fits all* solution, and the load balancing solution
    chosen will depend on traffic patterns, performance, and portability that is required
    by an organization.
  prefs: []
  type: TYPE_NORMAL
- en: This book will not delve into performance metrics; its goal is to look at the
    different load balancing strategies that are available today from each vendor
    and the configuration management methods that could be utilized to fully automate
    and orchestrate load balancers which will in turn help network teams automate
    load balancing network operations.
  prefs: []
  type: TYPE_NORMAL
- en: Citrix NetScaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Citrix NetScaler** provides a portfolio of products to service an organization''s
    load balancing requirements. Citrix provide various different products to end
    users, such as the **MPX**, **SDX**, **VPX**, and more recently the **CPX** appliances,
    with flexible license costs available for each product based on the throughput
    they support.'
  prefs: []
  type: TYPE_NORMAL
- en: MPX and SDX are the NetScaler hardware appliances, whereas the VPX is a virtualized
    NetScaler and the CPX is a containerized NetScaler.
  prefs: []
  type: TYPE_NORMAL
- en: All of these products support differing amounts of throughput based on the license
    that is purchased ([https://www.citrix.com/products/netscaler-adc/platforms.html](https://www.citrix.com/products/netscaler-adc/platforms.html)).
  prefs: []
  type: TYPE_NORMAL
- en: All of the Citrix NetScaler family of products share the same common set of
    APIs and code, so the software is completely consistent. NetScaler has a REST
    API and a Python, Java, and C# Nitro SDK, which exposes all the NetScaler operations
    that are available in the GUI to the end user. All the NetScaler products allow
    programmatic control of NetScaler objects and entities that need to be set up
    to control load balancing or routing on MPX, SDX, VPX, or CPX.
  prefs: []
  type: TYPE_NORMAL
- en: The NetScaler MPX appliance is a centralized physical load balancing appliance
    that is used to deal with a high number of **Transactions Per Second** (**TPS**);
    MPX has numerous security features and complies with **Restriction of Hazardous
    Substances** (**RoHS**) and **Federal Information Processing Standard** (**FIPS**),
    so the solution can be used by heavily regulated industries that require businesses
    to comply with certain regulatory standards.
  prefs: []
  type: TYPE_NORMAL
- en: MPX is typically used to do SSL offloading; it supports a massive amount of
    SSL throughput, which can be very useful for very highly performant applications,
    so the SSL offloading can be done on the hardware appliance.
  prefs: []
  type: TYPE_NORMAL
- en: MPX can be used to direct traffic to different tenant networks using layer 4
    load balancing and layer 7 context switching or alternately direct traffic to
    a second load balancing tier.
  prefs: []
  type: TYPE_NORMAL
- en: The NetScaler SDX appliance is also a centralized physical appliance that is
    used to deal with a high number of TPS. SDX allows multiple VPX appliances to
    be set up as HA pairs and deployed on SDX to allow increased throughput and resiliency.
  prefs: []
  type: TYPE_NORMAL
- en: 'NetScaler also supports **Global Server Load Balancing** (**GSLB**), which
    allows load to be distributed across multiple VPX HA pairs in a scale out model
    utilizing **CNAME,** which directs traffic across multiple HA pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Citrix NetScaler](img/B05559_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The VPX can be installed on any x86 hypervisor and be utilized as a VM appliance,
    and a new CPX is now available that puts the NetScaler inside a Docker container,
    so they can be deployed within a tenant network as opposed to being set up in
    a centralized model. All appliances allow SSL certificates to be assigned and
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every NetScaler appliance, be it MPX, SDX, VPX, or CPX, utilize IP the same
    object model and code that has the following prominent entities defined in software
    to carry out application load balancing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Server**: A server entity on NetScaler binds a virtual machine or bare metal
    server''s IP address to the server entity. This means the IP address is a candidate
    for load balancing once it is bound to other NetScaler entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor**: The monitor entity on NetScaler are attached to services or service
    groups and provide health checks that are used to monitor the health of attached
    server entities. If the health checks, which could be as simple as a web-ping,
    are not positive, the service or service group will be marked as down, and NetScaler
    will not direct traffic to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service group**: A service group is a NetScaler entity used to bind a group
    of one or more servers to an `lbvserver` entity; a service group can have one
    or more monitors associated with it to health check the associated servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service**: The service entity is used to bind one server entity and one or
    more monitor health checks to an `lbvserver` entity, which specifies the protocol
    and port to check the server on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lbvserver**: An `lbvserver` entity determines the load balancing policy such
    as round robin or least connection and is connected to a service group entity
    or multiple service entities and will expose a virtual IP address that can be
    served to end users to access web applications or a web service endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gslbvserver**: When DNS load balancing between NetScaler appliances is required,
    a `gslbvserver` entity is used to specify the `gslb` domain name and TTL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**csvserver**: The `csvserver` entity is used to provide layer 7 context switching
    from a gslbvserver domain or lbvserver IP address to other lbvservers. This is
    used to route traffic using the NetScaler appliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gslbservice**: The `gslbvservice` entity binds the `gslbvserver` domain to
    one or more `gslbservers` entities to distribute traffic across NetScaler appliances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gslbserver**: The `gslbserver` entities are is the gslb-enabled IP addresses
    of the NetScaler appliances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple load balancing can be done utilizing the server, monitor, service group/service,
    and lbvserver combination. With `gslbvserver` and `csvserver`, context switching
    allows more complex requirements for complex routing and resiliency.
  prefs: []
  type: TYPE_NORMAL
- en: F5 Big-IP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **F5 Big-IP** suite is based on F5's very own custom TMOS real-time operating
    system, which is self-contained and runs on Linux. TMOS has a collection of operating
    systems and firmware, which all run on BIG-IP hardware appliances or within the
    BIG-IP virtual instances. BIG-IP and TMOS (and even TMM) can be used interchangeably
    depending on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: TMOS is at the heart of every F5 appliance and allows inspection of traffic.
    It makes forwarding decisions based on the type of traffic acting much in the
    same way as a firewall would, only allowing predefined protocols to flow through
    the F5 system.
  prefs: []
  type: TYPE_NORMAL
- en: TMOS also features iRules, which are programmatic scripts written using F5's
    very own **Tool Command Language** (**TCL**) that enables users to create unique
    functions triggered by specific events. This could be used to content switch traffic
    or red-order HTTP cookies; TCL is fully extensible and programmable and can carry
    out numerous operations.
  prefs: []
  type: TYPE_NORMAL
- en: The F5 Big-IP solution is primarily a hardware load balancing solution, that
    provides multiple sets of physical hardware boxes that customers can purchase
    based on their throughput requirements, and the hardware can be clustered together
    for redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: The F5 Big-IP suite provides a multitude of products that provide services catering
    for load balancing, traffic management, and even firewalling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main load balancing services provided by the F5 Big-IP Suite are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Big-IP DNS:** F5''s global load balancing solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local traffic manager:** The main load balancing product of the F5 Big-IP
    suite'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F5 Big-IP solution, like the Citrix NetScaler, implements an object model
    to allow load balancing to be programmatically defined and virtualized. F5 allows
    SSL certificates to be associated with entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following local traffic manager object entities allow F5 Big-IP to load
    balance applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pool members**: The pool member entity is mapped to a virtual or physical
    server''s IP address and can be bound to one or more pools. A pool member can
    have health monitors associated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor**: The monitor entity returns the status on specific pool members
    and acts as a health check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool**: The pool entity is a logical grouping of a cluster of pool members
    that are associated; a pool can have health monitors associated with it as well
    as **Quality of Service** (**QoS**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual servers**: The virtual server entity is associated with a pool or
    multiple pools, and the virtual server determines the load balancing policy, such
    as round robin or least connections. The F5 solution also will offer load balancing
    solutions based on capacity or fastest connection. Layer 7 profiles utilizing
    iRules can be configured against a virtual server and is used to expose an IP
    address to access pool members.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**iRules**: iRules utilize the programmatic TCL, so users can author particular
    load balancing rules based on events such as context switching to different pools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rate classes**: Rate classes implement rate shaping, and they are used to
    control bandwidth consumption on particular load balancing operations to cap throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic classes**: Traffic class entities are used to regulate traffic flow
    based on particular events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avi Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Avi Networks** are a relatively new start-up but have a very interesting
    load balancing product, which truly embraces the software-defined mandate. It
    is an enterprise software load balancing solution that comprises the **Avi Controller**
    that can be deployed on x86 compute. Avi is a pure software solution that deploys
    distributed Avi service engines into tenant networks and integrates with an AWS
    VPC and an OpenStack tenant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Avi Networks](img/B05559_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Avi Networks solution offers automated provisioning of load balancing services
    on x86 hypervisors, and it can automatically scale out to meet load balancing
    needs elastically based on utilization rules that users can configure.
  prefs: []
  type: TYPE_NORMAL
- en: The Avi Networks solution supports multiple or isolated tenants and has a real-time
    application monitoring and analytics engine that can work out where latency is
    occurring on the network and the location's packets are being routed from.
  prefs: []
  type: TYPE_NORMAL
- en: Avi also supports a rich graphical interface that shows load balancing entities
    so users have a visual view of load balancing, and it additionally supports anti-DDoS
    support.
  prefs: []
  type: TYPE_NORMAL
- en: 'All commands that are issued via GUI or API utilize the same REST API calls.
    The Avi Networks solution supports a Python and REST API. The Net Networks object
    model has numerous entities that are used to define load balancing in much the
    same way as NetScalers and F5:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Health monitor profile**: The health monitor pool profile entity specifies
    health checks for a pool of servers using health attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool**: The pool entity specifies the IP addresses of virtual or physical
    servers in the form of a server list and has associated health monitor profiles;
    it also allows an event to be specified using a data script if a pool goes down.
    One or more pools are bound to the virtual service entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom policy**: The custom policy allows users to programmatically specify
    policies against a virtual service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**App profile**: The app profile entity allows each application to be modeled
    with associated http attributes, security, DDoS, caching, compression, and PKI
    attributes specified as part of the app profile associated with a virtual service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytics profile**: The analytics profile makes use of the Avi analytics
    engine and captures threat, metrics, health score as well as latency thresholds
    and failure codes that are mapped to the virtual service entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP/UDP profile**: The TCP/UDP profile governs if TCP or UDP is used and
    any DDoS L3/L4 profiles are set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL profile**: The SSL entity governs SSL ciphers that will be used by a
    virtual service entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKI profile**: The PKI profile entity is bound to the virtual service entity
    and specifies the certificate authority for the virtual service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy set**: The policy set entity allows users to set security teams to
    set policies against each virtual service governing request and response polices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual service**: The virtual service entity is the entry point IP address
    to the load balanced pool of servers and is associated with all profiles to define
    the application pools load balancing and is bound to the TCP/UDP, app, SSL, SSL
    cert, policy, and analytics profiles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nginx
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nginx** ([https://www.nginx.com/](https://www.nginx.com/)) supports both
    commercial and open source versions. It is an x86 software load balancing solution.
    Nginx can be used as both an HTTP and TCP load balancer supporting HTTP, TCP,
    and even UDP, and can also support SSL/TLS termination.'
  prefs: []
  type: TYPE_NORMAL
- en: Nginx can be set up for redundancy in a highly available fashion using *keepalived*,so
    if there is an outage on one Nginx load balancer, it will seamlessly fail over
    to a backup with zero downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx Plus is the commercial offering and is more fully featured than the open
    source version, supporting features such as active health checks, session persistence,
    and caching.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing on Nginx is set up by declaring syntax in the `nginx.conf` file.
    It works on the principle of wanting to simplify load balancing configuration.
    Unlike NetScalers, F5s, and Avi Networks, it does not utilize an object model
    to define load balancing rules, instead Nginx describes load balanced virtual
    or physical machines as backend servers using declarative syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following simple example, we see three servers, `10.20.1.2`, `10.20.1.3`,
    and `10.20.1.4`, all load balanced on port `80` using Nginx declarative syntax,
    and it is served on `http://www.devopsfornetworking.com/devops_for_networking`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Nginx](img/B05559_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By default, Nginx will load balance servers using round-robin load balancing
    method, but it also supports other load balancing methods.
  prefs: []
  type: TYPE_NORMAL
- en: The Nginx `least_conn` load balancing method forwards to backend servers with
    the least connections at any particular time, whereas the Nginx `ip_hash` method
    of load balancing means that users can tie the same source address to the same
    target backend server for the entirety of a request.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful as some applications require that all requests are tied to the
    same server using sticky sessions while transactions are processed.
  prefs: []
  type: TYPE_NORMAL
- en: However, the proprietary Nginx Plus version supports an additional load balancing
    method named `least_time`, which calculates the lowest latency of backend servers
    based on the number of active connections and subsequently forwards requests appropriately
    based on those calculations.
  prefs: []
  type: TYPE_NORMAL
- en: The Nginx load balancer uses a weighting system at all times when load balancing;
    all servers by default have a weight of `1`. If a weight other than `1` is placed
    on a server, it will not receive requests unless the other servers on a backend
    are not available to process requests. This can be useful when throttling specific
    amounts of traffic to backend servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see that the backend servers have load balancing
    method least connection configured. Server `10.20.1.3` has a weight of `5`, meaning
    only when `10.20.1.2` and `10.20.1.4` are maxed out will requests is sent to the
    `10.20.1.3` backend server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Nginx](img/B05559_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By default, using round-robin load balancing in Nginx won't stop forwarding
    requests to servers that are not responding, so it utilizes `max_fails` and `fail_timeouts`
    for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see server `10.20.1.2` and `10.20.1.4` have
    the `max_fail` count of `2` and a `fail_timeout` of `1` second; if this is exceeded
    then Nginx will stop directing traffic to these servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Nginx](img/B05559_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: HAProxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**HAProxy** ([http://www.haproxy.org/](http://www.haproxy.org/)) is an open
    source x86 software load balancer that is session aware and can provide layer
    4 load balancing. The HAproxy load balancer can also carry out layer 7 context
    switching based on the content of the request as well as SSL/TLS termination.'
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy is primarily used for HTTP load balancing and can be set up in a redundant
    fashion using `keepalived` configuration using two apache configurations, so if
    the master fails, the slave will become the master to make sure there is no interruption
    in service for end users.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy uses declarative configuration files to support load balancing as opposed
    to an object model that proprietary load balancing solutions, such as NetScaler,
    F5 and Avi Networks, have adopted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HAProxy configuration file has the following declarative configuration
    sections to allow load balancing to be set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backend**: A backend declaration can contain one or more servers in it; backend
    servers are added in the format of a DNS record or an IP address. Multiple backend
    declarations can be set up on a HAProxy server. The load balancing algorithm can
    also be selected, such as round robin or least connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we see two backend servers, `10.11.0.1` and `10.11.0.2`,
    load balanced using the round-robin algorithm on port `80`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![HAProxy](img/B05559_05_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Check**: Checks avoid users having to manually remove a server from the backend
    if for any reason, it becomes unavailable and this mitigates outages. HAProxy''s
    default health always attempts to establish a TCP connection to the server using
    the default port and IP. HAProxy will automatically disable servers that are unable
    to serve requests to avoid outages. Servers will only be re-enabled when it passes
    its check. HAProxy will report whole backends as unavailable if all servers on
    a backend have failed their health checks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A number of different health checks can be put against backend servers by utilizing
    the option `{health-check}` line item; for instance, `tcp-check` in the following
    example can check on the health of port `8080` even though port `443` is being
    balanced:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![HAProxy](img/B05559_05_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Access Control List (ACL)**: ACL declarations are used to inspect headers
    and forward to specific backend servers based on the headers. An ACL in HAProxy
    will try to find conditions and trigger actions based on this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frontend**: The frontend declaration allows different kinds of traffic to
    be supported by the HAProxy load balancer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, HAProxy will accept http traffic on port `80`, with
    an ACL matching requests only if the request starts with `/network` and it is
    then forwarded to the `high-perf-backend` if the ACL `/web-network` is matched:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![HAProxy](img/B05559_05_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Load balancing immutable and static infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the introduction of public and private cloud solutions such as AWS and
    OpenStack, there has been a shift towards utilizing immutable infrastructure instead
    of traditional static servers.
  prefs: []
  type: TYPE_NORMAL
- en: This has raised a point of contention with *pets versus cattle* or, as Gartner
    defines it *bi-modal* ([http://www.gartner.com/it-glossary/bimodal/](http://www.gartner.com/it-glossary/bimodal/)).
  prefs: []
  type: TYPE_NORMAL
- en: Gartner has said that two different strategies need to be adopted, one for new
    microservices, *cattle,* and one for legacy infrastructure, *pets*. *Cattle* are
    servers that are killed off once they have served their purpose or have an issue,
    typically lasting one release iteration. Alternately, *pets* are servers that
    will have months or years of uptime and will be patched and cared for by operations
    staff.
  prefs: []
  type: TYPE_NORMAL
- en: Gartner defines *pets* as Mode 1 and *cattle* as Mode 2\. It is said that a
    *cattle* approach favors the stateless microservice cloud-native applications,
    whereas a *pet*, on the other hand, is any application that is a monolith, or
    potentially a single appliance or something that contains data, such as a database.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable infrastructure and solutions such as OpenStack and AWS are said by
    many to favor only the *cattle*, with monoliths and databases remaining pets still
    need a platform that caters for long-lived servers.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I find the *pets* versus *cattle* debate to be a very lazy argument
    and somewhat tiresome. Instead of dumping applications into two buckets, applications
    should be treated as a software delivery problem, which becomes a question of
    stateless read applications and stateful applications with caching and data. Cloud-native
    microservice applications still need data and state, so I am puzzled by the distinction.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is undisputed that the load balancer is key to immutable infrastructure,
    as at least one version of the application always needs to be exposed to a customer
    or other microservices to maintain that applications incur zero downtime and remain
    operational at all times.
  prefs: []
  type: TYPE_NORMAL
- en: Static and immutable servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Historically, an operations team was used by companies to perform the following
    operations on servers:'
  prefs: []
  type: TYPE_NORMAL
- en: Rack and cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing firmware updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the RAID configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing an operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patching the operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was all before making the servers available to developers. Static infrastructure
    can still exist within a cloud environment; for example, databases are still typically
    deployed as static, physical servers, given the volume of data that needs to be
    persisted on their local disk.
  prefs: []
  type: TYPE_NORMAL
- en: Static servers mean a set of long-lived servers that typically will contain
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable servers, on the other hand, mean that every time a virtual machine
    is changed, a new virtual machine is deployed, complete with a new operating system
    and new software released on them, delete please. Immutable infrastructure means
    no in-place changes to a server's state.
  prefs: []
  type: TYPE_NORMAL
- en: This moves away from the pain of doing in-place upgrades and makes sure that
    snowflake server configurations are a thing of the past, where every server, despite
    the best intentions, has drifted slightly from its desired state over a period
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: How many times when releasing software has a release worked on four out of five
    machines, and hours or days were wasted debugging why a particular software upgrade
    wasn't working on a particular server.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable infrastructure builds servers from a known state promoting the same
    configuration to quality assurance, integration, performance testing, and production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Parts of cloud infrastructure can be made completely immutable to reap these
    benefits. The operating system is one such candidate; rather than doing in-place
    patching, a single golden image can be created and patched using automation tooling
    such as Packer in a fully automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that require a caching layer are more stateful by nature so that
    cache needs to be available at all times to serve other applications. These caching
    applications should be deployed as clusters, which are load balanced, and rolling
    updates will be done to make sure one version of the cache data is always available.
    A new software release of that caching layer should synchronize the cache to the
    new release before the pervious release is destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: Data, on the other hand, is always persistent, so can be stored on persistent
    storage and then mounted by the operating system. When doing an immutable rolling
    update, the operating system layer can mount the data on either persistent or
    shared storage as part of the release process.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to separate the operating system and the data to make all virtual
    machines stateless, for instance, OpenStack Cinder ([https://wiki.openstack.org/wiki/Cinder](https://wiki.openstack.org/wiki/Cinder))
    can be utilized to store persistent data on volumes that can be attached to virtual
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: With all these use cases considered, most applications can be designed to be
    deployed immutably through proper configuration management, even monoliths, as
    long as they are not a single point of failure. If any applications are single
    points of failure, they should be rearchitected as releasing software should never
    result in downtime. Although applications are stateful, each state can be updated
    in stages so that an overall immutable infrastructure model can be maintained.
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **blue green deployment** process is not a new concept. Before cloud solutions
    came to prominence, production servers would typically have a set of servers consisting
    of blue (no live traffic) and green (serving customer traffic) that would be utilized.
    These are typically known as blue and green servers, which alternated per release.
  prefs: []
  type: TYPE_NORMAL
- en: The blue green model in simple terms means that when a software upgrade needed
    to be carried out, the blue servers would be upgraded to the latest software version.
    Once the upgrade had been completed, the blue servers would become the new green
    servers with live traffic switched to serve from the newly upgraded servers.
  prefs: []
  type: TYPE_NORMAL
- en: The switching of live traffic was typically done by switching DNS entries to
    point at the newly upgraded servers. So once the DNS **Time To Live** (**TTL**)
    had propagated, end user requests would be served by the newly upgraded servers.
  prefs: []
  type: TYPE_NORMAL
- en: This means that if there was an issue with a software release, rollback could
    be achieved by switching back the DNS entries to point at the previous software
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical blue green deployment process is described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Release 1.1** would be deployed on servers 1, 2, and 3 and served on a load
    balancer to customers and made Green (live):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Release 1.2** would be deployed on servers 4, 5, and 6 and then be patched
    to the latest patch version, upgraded to the latest release and tested. When ready,
    the operations team would toggle the load balancer to serve boxes 4, 5, and 6
    as the new production release, as shown later, and the previously green (live)
    deployment would become blue, and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When the operations team came to do the next release, servers 1, 2, and 3 would
    be patched to the latest version, upgraded to **Release 1.3** from **Release 1.1**,
    tested, and when ready, the operations team would direct traffic to the new release
    using the load balancer, making **Release 1.2** blue and **Release 1.3** green,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This was traditionally the procedure of running a blue green deployment using
    static servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when using an immutable model, instead of using long-lived static
    servers, such as Servers 1, 2, 3, 4, 5, and 6, after a release was successful,
    the servers would be destroyed, as shown here, as they have served their purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next time servers 4, 5, and 6 were required, instead of doing an in-place
    upgrade, three new virtual machines would be created from the golden base image
    in a cloud environment. These golden images would already be patched up to the
    latest version, so brand new servers 7, 8, and 9 with the old servers destroyed
    and the new **Release 1.4** would be deployed on them, as shown later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once server 7, 8, and 9 were live, servers 1, 2, and 3 would be destroyed as
    they have served their purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using Ansible to Orchestrate load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html "Chapter 4. Configuring Network Devices Using Ansible"),
    *Configuring Network Devices Using Ansible*, we covered the basics of Ansible
    and how to use an Ansible Control Host, playbooks, and roles for configuration
    management of network devices. Ansible, though, has multiple different core operations
    that can help with orchestrating load balancers, which we will look at in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Delegation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ansible delegation is a powerful mechanism that means from a playbook or role,
    Ansible can carry out actions on the target servers specified in the inventory
    file by connecting to them using SSH or WinRM, or alternately execute commands
    from the Ansible Control Host. WinRM is the Microsoft remote management standard
    and the equivalent of SSH for Windows that allows administrators to connect to
    Windows guests and execute programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows these two alternative connection methods with the
    Ansible Control Host either logging in to boxes using SSH or WinRM to configure
    them or running an API call from the Ansible Control Host directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Delegation](img/B05559_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Both of these options can be carried out from the same role or playbook using
    `delegate_to`, which makes playbooks and roles extremely flexible as they can
    combine API calls and server-side configuration management tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of delegation can be found later where the Ansible extras HAProxy
    modules are used, with `delegate_to` used to trigger an orchestration action that
    disables all backend services in the inventory file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Delegation](img/B05559_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Utilizing serial to control roll percentages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to release software without interruptions to service, a zero downtime
    approach is preferable, as it doesn't require a maintenance window to schedule
    a change or release. Ansible supports a *serial* option, which passes a percentage
    value to a playbook.
  prefs: []
  type: TYPE_NORMAL
- en: The *serial* option allows Ansible to iterate over the inventory and only carry
    out the action against a percentage of the boxes, completing the necessary playbook,
    before moving onto the next portion of the inventory. It is important to note
    that Ansible passes inventory as an unordered dictionary, so the percentage of
    the inventory that is processed will not be in a specific order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the *serial* option that a blue/green strategy could be employed in Ansible,
    so boxes will need to be taken out of the load balancer and upgraded before being
    put back into service. Rather than doubling up on the number of boxes, three boxes
    are required, as shown in the following image, which all serve **Release 1.4**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utilizing serial to control roll percentages](img/B05559_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Utilizing the following Ansible playbook, using a combination of `delegate_to`
    and `serial`, each of the servers can be upgraded using a rolling update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utilizing serial to control roll percentages](img/B05559_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The playbook will execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The `serial 30%` will mean that only one server at a time is upgraded. So, **Server
    7** will be taken out of the HAProxy `backend_nodes` pool by disabling the service
    calling the HAProxy using a local `delegate_to` action on the Ansible Control
    Host. A `yum` update will then be executed to upgrade the server version new `application1`
    release **version 1.5** on **server 7**, as follows:![Utilizing serial to control
    roll percentages](img/B05559_05_18.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Server 7** will then be enabled again and put into service on the load balancer
    using a local `delegate_to` action. The serial command will iterate onto `server
    8` and disable it on HAProxy, before doing a `yum` update to upgrade the server
    version new `application1` release **version 1.5**, as follows:![Utilizing serial
    to control roll percentages](img/B05559_05_19.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rolling update will then enable **Server 8** on the load balancer, and the
    serial command will iterate onto **Server 9**, disabling it on HAProxy before
    doing a yum update, which will upgrade the server with the new `application1`
    release **version 1.5** alternating when necessary between execution on the local
    server and the server, as shown here:![Utilizing serial to control roll percentages](img/B05559_05_20.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the playbook will finish by enabling **server 9** on the load balancer,
    and all servers will be upgraded to **Release 1.5** using Ansible as follows:![Utilizing
    serial to control roll percentages](img/B05559_05_21.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dynamic inventories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with cloud platforms, using just static inventories is sometimes
    not enough. It is useful to understand the inventory of servers that are already
    deployed within the estate and target subsets of them based on characteristics
    or profiles.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible has an enhanced feature named the dynamic inventory. It allows users
    to query a cloud platform of their choosing with a Python script; this will act
    as an autodiscovery tool that can be connected to AWS or OpenStack, returning
    the server inventory in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: This allows Ansible to load this JSON file into a playbook or role so that it
    can be iterated over. In the same way, a static inventory file can be via variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamic inventory fits into the same command-line constructs instead of
    passing the following static inventory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a dynamic inventory script, `openstack.py`, for the OpenStack cloud provider
    could be passed instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The dynamic inventory script can be set up to allow specific limits. In the
    preceding case, the only server inventory that has been returned is the quality
    assurance servers, which is controlled using the `–l qa` limit.
  prefs: []
  type: TYPE_NORMAL
- en: When using Ansible with immutable servers, the static inventory file can be
    utilized to spin up new virtual machines, whereas the static inventory can be
    used to query the estate and do supplementary actions when they have already been
    created.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using dynamic inventory in Ansible, metadata becomes a very important component,
    as servers deployed in a cloud environment can be sorted and filtered using metadata
    that is tagged against virtual or physical machines.
  prefs: []
  type: TYPE_NORMAL
- en: When provisioning AWS, Microsoft Azure, or OpenStack instances in a public or
    private cloud, metadata can be tagged against servers to group them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see a playbook creating new OpenStack servers
    using the `os_server` OpenStack module. It will iterate over the static inventory,
    tagging each newly created group, and release metadata on the machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tagging metadata](img/B05559_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The dynamic inventory can then be filtered using the `–l` argument to specify
    boxes with `group: qa`. This will return a consolidated list of servers.'
  prefs: []
  type: TYPE_NORMAL
- en: Jinja2 filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Jinja2 filters** allow Ansible to filter a playbook or role, allowing it
    to control which conditions need to be satisfied before executing a particular
    command or module. There are a wide variety of different jinja2 filters available
    out of the box with Ansible or custom filters can be written.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a playbook using a jinja2 filter would only add the server to
    the NetScaler if its metadata `openstack.metadata.build` value is equal to the
    current build version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jinja2 filters](img/B05559_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Executing the ansible-playbook `add_hosts_to_netscaler.yml command` with a
    limit `–l` on `qa` would only return boxes in the `qa` metadata group as the inventory.
    Then, the boxes can be further filtered at playbook or role using the when jinja2
    filter to only execute the `add into load balancer pool` command if the `openstack.metadata.build`
    number of the box matches the `current_build` variable of `9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The result of this would be that only the new boxes would be added to the NetScaler
    `lbvserver` VIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The boxes could be removed in a similar way in the same playbook with a *not
    equal to* condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jinja2 filters](img/B05559_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This could all be combined along with the serial percentage to roll percentages
    of the new release into service on the load balancer and decommission the old
    release utilizing dynamic inventory, delegation, jinja2 filters, and the serial
    rolling update features of Ansible together for simple orchestration of load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Ansible networking modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As Ansible can be used to schedule API commands against a load balancer, it
    can be easily utilized to build out a load balancer object model that popular
    networking solutions, such as Citrix NetScaler, F5 Big-IP, or Avi Networks, utilize.
  prefs: []
  type: TYPE_NORMAL
- en: With the move to microservice architectures, load balancing configuration needs
    to be broken out to remain manageable, so it is application-centric , as opposed
    to living in a centralized monolith configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: This means that there are operational concerns when doing load balancing changes,
    so Ansible can be utilized by network operators to build out the complex load
    balancing rules, apply SSL certificates, and set up more complex layer 7 context
    switching or public IP addresses and provide this as a service to developers.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Python APIs provided by load balancing vendors, each operation
    could then be created as a module with a set of YAML `var` files describing the
    intended state of the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example mentioned later, we look at how Ansible var files could be utilized
    by developers to create a service and health monitor for every new virtual server
    on a NetScaler. These services are then bound to the `lbvserver` entity, which
    was created by the network team, with a roll percentage of 10%, which can be loaded
    into the playbook''s `serial` command. The playbook or role is utilized to create
    services, lbmonitors, 34 servers and bind services to lbvservers, whereas the
    var file describes the desired state of those NetScaler objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating Ansible networking modules](img/B05559_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that the varied load balancing solutions are available
    from proprietary vendors to open source solutions, and discussed the impact that
    microservices have had on load balancing, moving it from a centralized to distributed
    model to help serve east-west traffic.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at blue/green deployment models, the merits of immutable and
    static servers, and how software releases can be orchestrated using Ansible in
    either model. In the process, we illustrated how useful Ansible is at orchestrating
    load balancers by utilizing dynamic inventory, rolling updates, delegation, and
    jinja2 filters can all be used to help fulfill load balancing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaways from this chapter are that microservice applications have
    changed the way applications need to be load balanced, and distributed load balancing
    is better suited when deploying microservice applications, which have more east-west
    traffic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The reasons that immutable infrastructure is well-suited to microservice applications
    should now be clear.. The chapter also defined ways that state and data can be
    separated from the operating system and that different rolling update models are
    required to support stateless and stateful applications. In the next chapter,
    we will look at applying these same automation principles to SDN Controllers,
    primarily focusing on the Nuage solution. It will cover configuring firewall rules
    and other SDN commands, so the whole network can be programmatically controlled
    and automated.
  prefs: []
  type: TYPE_NORMAL
