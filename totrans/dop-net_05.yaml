- en: Chapter 5. Orchestrating Load Balancers Using Ansible
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：使用Ansible编排负载均衡器
- en: This chapter will focus on some of the popular load balancing solutions that
    are available today and the approaches that they take to load balancing applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍一些当今流行的负载均衡解决方案，以及它们在负载均衡应用程序方面的策略。
- en: With the emergence of cloud solutions, such as AWS, Microsoft Azure, Google
    Cloud, and OpenStack, we will look at the impact this has had on load balancing
    with distributed load and centralized load balancing strategies. This chapter
    will show practical configuration management processes that can be used to orchestrate
    load balancers using Ansible to help automate the load balancing needs for applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云解决方案的出现，如AWS、Microsoft Azure、Google Cloud和OpenStack，我们将探讨这些技术对负载均衡的影响，特别是分布式负载和集中式负载均衡策略。本章将展示可用于通过Ansible编排负载均衡器的实际配置管理过程，以帮助自动化应用程序的负载均衡需求。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Centralized and distributed load balancers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中式和分布式负载均衡器
- en: Popular load balancing solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的负载均衡解决方案
- en: Load balancing immutable and static servers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡不可变和静态服务器
- en: Using Ansible to orchestrate load balancers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ansible编排负载均衡器
- en: Centralized and distributed load balancers
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集中式和分布式负载均衡器
- en: With the introduction of microservice architectures allowing development teams
    to make changes to production applications more frequently, developers no longer
    just need to release software on a quarterly basis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务架构的引入，允许开发团队更频繁地对生产环境中的应用程序进行更改，开发者不再仅仅需要按季度发布软件。
- en: With the move towards Continuous Delivery and DevOps, applications are now released
    weekly, daily, or even hourly with only one or a subset of those microservices
    being updated and released.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着持续交付（Continuous Delivery）和DevOps的推进，应用程序现在每周、每天，甚至每小时都会发布，且通常只更新和发布其中一个或部分微服务。
- en: Organizations have found microservice architectures to be easier to manage and
    have moved away from building monolith applications. Microservice applications
    break a larger application into smaller manageable chunks. This allows application
    features to be released to customers on a more frequent basis, as the business
    does not have to redeploy the whole product each time they release. This means
    only a small microservice needs to be redeployed to deploy a feature. As the release
    process is more frequent and continuous, then it is better understood, normally
    completely automated, and ultimately load balanced.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 组织发现微服务架构更易于管理，因此逐渐摒弃了构建单体应用程序的方式。微服务应用程序将较大的应用程序拆分成更小的、可管理的模块。这使得应用程序功能可以更频繁地发布给客户，因为业务在每次发布时不需要重新部署整个产品。这意味着只需重新部署一个小的微服务即可发布一个新功能。由于发布过程更为频繁且持续进行，因此它更容易被理解，通常是完全自动化的，并最终得到负载均衡。
- en: Microservice architectures can also be beneficial for large businesses, which
    are distributed across many offices or countries as different teams can own different
    microservices and release them independently of one another.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构对于跨多个办公室或国家的庞大企业也有益处，因为不同的团队可以拥有不同的微服务，并独立发布它们。
- en: This, of course, means that development teams need a way of testing dependency
    management, and the onus is put on adequate testing to make sure that a microservice
    doesn't break other microservices when it is released.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然意味着开发团队需要一种方式来测试依赖管理，并且要确保充分测试，以保证微服务在发布时不会破坏其他微服务。
- en: As a result, developers need to create mocking and stubbing services, so microservice
    applications can be effectively tested against multiple software versions without
    having to deploy the full production estate.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，开发者需要创建模拟和存根服务，以便在不部署完整生产环境的情况下有效测试微服务应用程序对多个软件版本的兼容性。
- en: Creating a microservice architecture is a huge mindset shift for a business
    but a necessary one to remain competitive. Releasing monolithic applications is
    often difficult and time-consuming for an organization, and businesses that have
    quarterly release cycles will eventually lose out to competitors that can release
    their features in a quicker, more granular way.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建微服务架构是企业的一大思维转变，但这是保持竞争力的必要措施。发布单体应用程序通常对企业来说既困难又耗时，而采用季度发布周期的企业最终会被能够以更快、更精细的方式发布功能的竞争对手超越。
- en: The use of microservice architectures has meant that being able to utilize the
    same load balancing in test environments as production has become even more important
    due to how dynamic environments need to be.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构的使用意味着，由于环境的动态性，能够在测试环境中使用与生产环境相同的负载均衡变得更加重要。
- en: So having test environments load balancing configuration as close to production
    environments as possible is a must. Configuration management tooling can be used
    to control the desired state of the load balancer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，测试环境的负载均衡配置必须尽可能接近生产环境。可以使用配置管理工具来控制负载均衡器的期望状态。
- en: The delegation of responsibilities also needs to be reviewed to support microservice
    architectures, so control of some of the load balancing provisioning should move
    to development teams as opposed to being a request to the network team to make
    it manageable and not to impede development teams. This, of course, is a change
    in culture that needs sponsorship from senior management to make the required
    changes to the operational model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 责任的委托也需要审查，以支持微服务架构，因此，部分负载均衡配置的控制应移交给开发团队，而不是向网络团队提出请求，这样既能使其易于管理，又不妨碍开发团队。这当然是一种文化变革，需要高级管理层的支持，以推动操作模型的必要变革。
- en: Load balancing requirements when using microservice applications will evolve
    as an application is developed or scaled up and down in size, so it is important
    that these aspects are made available to developers to self-service requests rather
    than wait on a centralized network team to make load balancing changes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微服务应用时，负载均衡的要求将随着应用程序的开发或规模的扩大而演变，因此，重要的是这些方面要提供给开发人员进行自助请求，而不是等待集中式网络团队来进行负载均衡的更改。
- en: As a result of the shift towards microservices architectures, the networking
    and load balancing landscape has needed to evolve too to support those needs with
    PaaS solutions being created by many vendors to handle application deployment
    across hybrid cloud and load balancing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于向微服务架构的转变，网络和负载均衡的领域也需要不断发展，以支持这些需求，许多供应商创建了PaaS解决方案来处理跨混合云的应用程序部署和负载均衡。
- en: Off-the-shelf PaaS solutions are a great option for companies that maybe aren't
    tech-savvy and are unable to create their own deployment pipelines using configuration
    management tooling, such as Chef, Puppet, Ansible, and Salt, to deploy their applications
    into cloud environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现成的PaaS解决方案是对于那些可能不太懂技术且无法使用配置管理工具（如Chef、Puppet、Ansible、Salt等）创建自己部署管道的公司来说的一个不错选择，帮助他们将应用程序部署到云环境中。
- en: Regardless of the approach to deployment, roll your own or off-the-shelf PaaS.
    Both microservice and monolith applications still need to be supported when considering
    public, private, and hybrid clouds.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是采用哪种部署方式，自建还是现成的PaaS解决方案。在考虑公有云、私有云和混合云时，微服务和单体应用仍然需要得到支持。
- en: As a result, networking and load balancing need to be adaptable to support varied
    workloads. Although the end goal for an organization is ultimately a microservice
    architecture, the reality for most companies is having to adopt a hybrid approach
    catering to centralized and distributed load balancing methods to support both
    monolithic and cloud native microservices.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络和负载均衡需要具备适应性，以支持不同的工作负载。虽然组织的最终目标是实现微服务架构，但大多数公司现实中必须采用混合方式，兼顾集中式和分布式负载均衡方法，以支持单体应用和云原生微服务。
- en: Centralized load balancing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式负载均衡
- en: Traditionally, load balancers were installed as external physical appliances
    with very complex designs and used very expensive equipment. Load balancers would
    be configured to serve web content with SSL requests terminated on the expensive
    physical appliances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，负载均衡器作为外部物理设备安装，设计非常复杂，并且使用非常昂贵的设备。负载均衡器会配置为为Web内容提供服务，并在昂贵的物理设备上终止SSL请求。
- en: The load balancer would have complex configuration to route requests to applications
    using context switching, and requests would be served directly to the static backend
    servers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器将有复杂的配置来通过上下文切换将请求路由到应用程序，且请求会直接传递给静态后端服务器。
- en: 'This was optimal for monolith configurations as applications typically were
    self-contained and followed a three-tier model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这对单体配置来说是最优的，因为应用程序通常是自包含的，并且遵循三层模型：
- en: A frontend webserver
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端Web服务器
- en: A business logic layer
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务逻辑层
- en: A database layer
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库层
- en: This didn't require a lot of east to west traffic within the network as the
    traffic was north to south, traversing the frontend, business logic, and database.
    Networks were designed to minimize the amount of time taken to process the request
    and serve it back to the end user, and it was always served by the core network
    each time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不需要大量的东西向流量，因为流量是南北向的，穿越了前端、业务逻辑和数据库。网络设计的目的是最小化处理请求并将其返回给最终用户所需的时间，并且每次都由核心网络提供服务。
- en: Distributed load balancing
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式负载均衡
- en: With the evolution towards microservice architectures, the way that applications
    operate has changed somewhat. Applications are less self-contained and need to
    talk to dependent microservices applications that exist within the same tenant
    network, or even across multiple tenants.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务架构的发展，应用程序的运行方式发生了一定变化。应用程序不再是自包含的，而是需要与同一租户网络内或跨多个租户的依赖微服务应用程序进行通信。
- en: This means that east-west traffic within the data center is much higher, and
    that traffic in the data center doesn't always go through the core network like
    it once did.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着数据中心内的东西向流量大大增加，并且数据中心中的流量不再像以前那样始终通过核心网络。
- en: Clusters of microservices applications are instead instantiated and then load
    balanced within the tenant network using x86 software load balancing solutions
    with the endpoint of the microservices clusters **Virtual IP** (**VIP**) exposed
    to adjacent microservices that need to utilize it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务应用程序的集群被实例化并在租户网络内进行负载均衡，使用x86软件负载均衡解决方案，微服务集群的**虚拟IP**（**VIP**）暴露给需要使用它的相邻微服务。
- en: With the growing popularity of virtual machines, containers, and software-defined
    overlay networks, this means that software load balancing solutions are now used
    to load balance applications within the tenant network, as opposed to having to
    pin back to a centralized load balancing solution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随着虚拟机、容器和软件定义覆盖网络的日益普及，这意味着软件负载均衡解决方案现在被用来在租户网络内部对应用程序进行负载均衡，而不再需要回到集中式的负载均衡解决方案。
- en: As a result load balancing vendors have had to adapt and produce virtualized
    or containerized versions of their physical appliances to stay competitive with
    open source software load balancing solutions, which are routinely used with microservices.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，负载均衡供应商不得不适应并生产虚拟化或容器化版本的物理设备，以与开源软件负载均衡解决方案竞争，而这些解决方案通常与微服务一起使用。
- en: Popular load balancing solutions
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的负载均衡解决方案
- en: As applications have moved from monoliths to microservices, load balancing requirements
    have undoubtedly changed. Today, we have seen a move towards open source load
    balancing solutions, which are tightly integrated with virtual machines and containers
    to serve east to west traffic between VPC in AWS or a tenant network in OpenStack
    as opposed to pinning out to centralized physical appliances.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序从单体架构转向微服务架构，负载均衡的需求无疑发生了变化。今天，我们看到有向开源负载均衡解决方案的转变，这些解决方案与虚拟机和容器紧密集成，用于在AWS中的VPC之间或OpenStack中的租户网络之间提供东西向流量，而不是依赖于集中式物理设备。
- en: 'Open source load balancing solutions are now available from **Nginx** and **HAProxy**
    to help developers load balance their applications or AWS elastic load balancing
    feature:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开源负载均衡解决方案已经由**Nginx**和**HAProxy**提供，帮助开发人员对他们的应用程序进行负载均衡，或使用AWS的弹性负载均衡功能：
- en: '[https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/)'
- en: Just a few years ago, Citrix NetScalers ([https://www.citrix.com/products/netscaler-adc/](https://www.citrix.com/products/netscaler-adc/))
    and F5 Big-IP ([https://f5.com/products/big-ip](https://f5.com/products/big-ip))
    solutions had the monopoly in the enterprise load balancing space, but the load
    balancing landscape has changed significantly with a multitude of new solutions
    available.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，Citrix NetScalers（[https://www.citrix.com/products/netscaler-adc/](https://www.citrix.com/products/netscaler-adc/)）和F5
    Big-IP（[https://f5.com/products/big-ip](https://f5.com/products/big-ip)）解决方案在企业负载均衡领域拥有垄断地位，但随着众多新解决方案的出现，负载均衡领域发生了显著变化。
- en: New load balancing start-ups such as Avi networks ([https://avinetworks.com/](https://avinetworks.com/))
    focus on x86 compute and software solutions to deliver load balancing solutions,
    which have been created to assist with both modern micros-service applications
    and monolith applications to support both distributed and centralized load balancing
    strategies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的负载均衡初创公司，如 Avi Networks （[https://avinetworks.com/](https://avinetworks.com/)），专注于
    x86 计算和软件解决方案，以提供负载均衡解决方案，这些解决方案旨在支持现代微服务应用程序和单体应用程序，支持分布式和集中式负载均衡策略。
- en: The aim of this book is not about which load balancing vendor solution is the
    best; there is no *one size fits all* solution, and the load balancing solution
    chosen will depend on traffic patterns, performance, and portability that is required
    by an organization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目的是讨论哪种负载均衡供应商解决方案最好；没有*一刀切*的解决方案，选择的负载均衡解决方案将取决于组织所需的流量模式、性能和可移植性。
- en: This book will not delve into performance metrics; its goal is to look at the
    different load balancing strategies that are available today from each vendor
    and the configuration management methods that could be utilized to fully automate
    and orchestrate load balancers which will in turn help network teams automate
    load balancing network operations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不会深入探讨性能指标；它的目标是探讨当前各供应商提供的不同负载均衡策略以及可以用来完全自动化和编排负载均衡器的配置管理方法，这将帮助网络团队实现负载均衡网络操作的自动化。
- en: Citrix NetScaler
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Citrix NetScaler
- en: '**Citrix NetScaler** provides a portfolio of products to service an organization''s
    load balancing requirements. Citrix provide various different products to end
    users, such as the **MPX**, **SDX**, **VPX**, and more recently the **CPX** appliances,
    with flexible license costs available for each product based on the throughput
    they support.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Citrix NetScaler** 提供一系列产品来满足组织的负载均衡需求。Citrix 提供多种不同的产品供最终用户使用，如 **MPX**、**SDX**、**VPX**，以及最近推出的
    **CPX** 设备，每种产品的许可证费用根据其支持的吞吐量提供灵活的定价。'
- en: MPX and SDX are the NetScaler hardware appliances, whereas the VPX is a virtualized
    NetScaler and the CPX is a containerized NetScaler.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MPX 和 SDX 是 NetScaler 的硬件设备，而 VPX 是虚拟化的 NetScaler，CPX 是容器化的 NetScaler。
- en: All of these products support differing amounts of throughput based on the license
    that is purchased ([https://www.citrix.com/products/netscaler-adc/platforms.html](https://www.citrix.com/products/netscaler-adc/platforms.html)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些产品根据所购买的许可证支持不同的吞吐量（[https://www.citrix.com/products/netscaler-adc/platforms.html](https://www.citrix.com/products/netscaler-adc/platforms.html)）。
- en: All of the Citrix NetScaler family of products share the same common set of
    APIs and code, so the software is completely consistent. NetScaler has a REST
    API and a Python, Java, and C# Nitro SDK, which exposes all the NetScaler operations
    that are available in the GUI to the end user. All the NetScaler products allow
    programmatic control of NetScaler objects and entities that need to be set up
    to control load balancing or routing on MPX, SDX, VPX, or CPX.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Citrix NetScaler 系列产品共享相同的 API 和代码集，因此软件完全一致。NetScaler 提供 REST API 以及 Python、Java
    和 C# Nitro SDK，暴露所有可以在 GUI 中进行操作的 NetScaler 功能。所有 NetScaler 产品允许以编程方式控制需要设置的 NetScaler
    对象和实体，以控制 MPX、SDX、VPX 或 CPX 上的负载均衡或路由。
- en: The NetScaler MPX appliance is a centralized physical load balancing appliance
    that is used to deal with a high number of **Transactions Per Second** (**TPS**);
    MPX has numerous security features and complies with **Restriction of Hazardous
    Substances** (**RoHS**) and **Federal Information Processing Standard** (**FIPS**),
    so the solution can be used by heavily regulated industries that require businesses
    to comply with certain regulatory standards.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: NetScaler MPX 设备是一个集中式的物理负载均衡设备，用于处理大量的**每秒事务数**（**TPS**）；MPX 具有众多安全特性，并且符合**有害物质限制**（**RoHS**）和**联邦信息处理标准**（**FIPS**），因此该解决方案可以被需要遵守特定监管标准的高监管行业所使用。
- en: MPX is typically used to do SSL offloading; it supports a massive amount of
    SSL throughput, which can be very useful for very highly performant applications,
    so the SSL offloading can be done on the hardware appliance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: MPX 通常用于进行 SSL 卸载；它支持大量的 SSL 吞吐量，这对于需要高性能的应用程序非常有用，因此 SSL 卸载可以在硬件设备上完成。
- en: MPX can be used to direct traffic to different tenant networks using layer 4
    load balancing and layer 7 context switching or alternately direct traffic to
    a second load balancing tier.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The NetScaler SDX appliance is also a centralized physical appliance that is
    used to deal with a high number of TPS. SDX allows multiple VPX appliances to
    be set up as HA pairs and deployed on SDX to allow increased throughput and resiliency.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'NetScaler also supports **Global Server Load Balancing** (**GSLB**), which
    allows load to be distributed across multiple VPX HA pairs in a scale out model
    utilizing **CNAME,** which directs traffic across multiple HA pairs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Citrix NetScaler](img/B05559_05_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: The VPX can be installed on any x86 hypervisor and be utilized as a VM appliance,
    and a new CPX is now available that puts the NetScaler inside a Docker container,
    so they can be deployed within a tenant network as opposed to being set up in
    a centralized model. All appliances allow SSL certificates to be assigned and
    used.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Every NetScaler appliance, be it MPX, SDX, VPX, or CPX, utilize IP the same
    object model and code that has the following prominent entities defined in software
    to carry out application load balancing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**Server**: A server entity on NetScaler binds a virtual machine or bare metal
    server''s IP address to the server entity. This means the IP address is a candidate
    for load balancing once it is bound to other NetScaler entities.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor**: The monitor entity on NetScaler are attached to services or service
    groups and provide health checks that are used to monitor the health of attached
    server entities. If the health checks, which could be as simple as a web-ping,
    are not positive, the service or service group will be marked as down, and NetScaler
    will not direct traffic to it.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service group**: A service group is a NetScaler entity used to bind a group
    of one or more servers to an `lbvserver` entity; a service group can have one
    or more monitors associated with it to health check the associated servers.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service**: The service entity is used to bind one server entity and one or
    more monitor health checks to an `lbvserver` entity, which specifies the protocol
    and port to check the server on.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lbvserver**: An `lbvserver` entity determines the load balancing policy such
    as round robin or least connection and is connected to a service group entity
    or multiple service entities and will expose a virtual IP address that can be
    served to end users to access web applications or a web service endpoints.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gslbvserver**: When DNS load balancing between NetScaler appliances is required,
    a `gslbvserver` entity is used to specify the `gslb` domain name and TTL.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**csvserver**: The `csvserver` entity is used to provide layer 7 context switching
    from a gslbvserver domain or lbvserver IP address to other lbvservers. This is
    used to route traffic using the NetScaler appliance.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gslbservice**: The `gslbvservice` entity binds the `gslbvserver` domain to
    one or more `gslbservers` entities to distribute traffic across NetScaler appliances.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gslbserver**: The `gslbserver` entities are is the gslb-enabled IP addresses
    of the NetScaler appliances.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple load balancing can be done utilizing the server, monitor, service group/service,
    and lbvserver combination. With `gslbvserver` and `csvserver`, context switching
    allows more complex requirements for complex routing and resiliency.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: F5 Big-IP
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **F5 Big-IP** suite is based on F5's very own custom TMOS real-time operating
    system, which is self-contained and runs on Linux. TMOS has a collection of operating
    systems and firmware, which all run on BIG-IP hardware appliances or within the
    BIG-IP virtual instances. BIG-IP and TMOS (and even TMM) can be used interchangeably
    depending on the use case.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: TMOS is at the heart of every F5 appliance and allows inspection of traffic.
    It makes forwarding decisions based on the type of traffic acting much in the
    same way as a firewall would, only allowing predefined protocols to flow through
    the F5 system.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: TMOS also features iRules, which are programmatic scripts written using F5's
    very own **Tool Command Language** (**TCL**) that enables users to create unique
    functions triggered by specific events. This could be used to content switch traffic
    or red-order HTTP cookies; TCL is fully extensible and programmable and can carry
    out numerous operations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The F5 Big-IP solution is primarily a hardware load balancing solution, that
    provides multiple sets of physical hardware boxes that customers can purchase
    based on their throughput requirements, and the hardware can be clustered together
    for redundancy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The F5 Big-IP suite provides a multitude of products that provide services catering
    for load balancing, traffic management, and even firewalling.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The main load balancing services provided by the F5 Big-IP Suite are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Big-IP DNS:** F5''s global load balancing solution'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local traffic manager:** The main load balancing product of the F5 Big-IP
    suite'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F5 Big-IP solution, like the Citrix NetScaler, implements an object model
    to allow load balancing to be programmatically defined and virtualized. F5 allows
    SSL certificates to be associated with entities.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'The following local traffic manager object entities allow F5 Big-IP to load
    balance applications:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Pool members**: The pool member entity is mapped to a virtual or physical
    server''s IP address and can be bound to one or more pools. A pool member can
    have health monitors associated.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor**: The monitor entity returns the status on specific pool members
    and acts as a health check.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool**: The pool entity is a logical grouping of a cluster of pool members
    that are associated; a pool can have health monitors associated with it as well
    as **Quality of Service** (**QoS**).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual servers**: The virtual server entity is associated with a pool or
    multiple pools, and the virtual server determines the load balancing policy, such
    as round robin or least connections. The F5 solution also will offer load balancing
    solutions based on capacity or fastest connection. Layer 7 profiles utilizing
    iRules can be configured against a virtual server and is used to expose an IP
    address to access pool members.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟服务器**：虚拟服务器实体与一个或多个池关联，虚拟服务器决定负载均衡策略，例如轮询或最少连接。F5 解决方案也提供基于容量或最快连接的负载均衡解决方案。利用
    iRules 的第七层配置文件可以针对虚拟服务器进行配置，并用于暴露 IP 地址以访问池成员。'
- en: '**iRules**: iRules utilize the programmatic TCL, so users can author particular
    load balancing rules based on events such as context switching to different pools.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**iRules**：iRules 使用编程语言 TCL，用户可以基于事件（如切换到不同池）编写特定的负载均衡规则。'
- en: '**Rate classes**: Rate classes implement rate shaping, and they are used to
    control bandwidth consumption on particular load balancing operations to cap throughput.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率类别**：速率类别实现了速率整形，用于控制特定负载均衡操作的带宽消耗，以限制吞吐量。'
- en: '**Traffic classes**: Traffic class entities are used to regulate traffic flow
    based on particular events.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流量类别**：流量类别实体用于根据特定事件调节流量流向。'
- en: Avi Networks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Avi Networks
- en: '**Avi Networks** are a relatively new start-up but have a very interesting
    load balancing product, which truly embraces the software-defined mandate. It
    is an enterprise software load balancing solution that comprises the **Avi Controller**
    that can be deployed on x86 compute. Avi is a pure software solution that deploys
    distributed Avi service engines into tenant networks and integrates with an AWS
    VPC and an OpenStack tenant:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**Avi Networks** 是一家相对较新的初创公司，但他们拥有一款非常有趣的负载均衡产品，真正体现了软件定义的理念。这是一款企业级软件负载均衡解决方案，包含可以部署在
    x86 计算平台上的 **Avi Controller**。Avi 是一种纯软件解决方案，将分布式 Avi 服务引擎部署到租户网络中，并与 AWS VPC
    及 OpenStack 租户进行集成：'
- en: '![Avi Networks](img/B05559_05_02.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![Avi Networks](img/B05559_05_02.jpg)'
- en: The Avi Networks solution offers automated provisioning of load balancing services
    on x86 hypervisors, and it can automatically scale out to meet load balancing
    needs elastically based on utilization rules that users can configure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Avi Networks 解决方案提供自动化的负载均衡服务配置功能，支持在 x86 虚拟化平台上进行自动扩展，根据用户配置的利用率规则，灵活地根据负载均衡需求进行扩展。
- en: The Avi Networks solution supports multiple or isolated tenants and has a real-time
    application monitoring and analytics engine that can work out where latency is
    occurring on the network and the location's packets are being routed from.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Avi Networks 解决方案支持多个或隔离的租户，并且拥有一个实时应用监控和分析引擎，可以定位网络中延迟的发生位置及数据包的路由来源地。
- en: Avi also supports a rich graphical interface that shows load balancing entities
    so users have a visual view of load balancing, and it additionally supports anti-DDoS
    support.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Avi 还支持丰富的图形界面，显示负载均衡实体，用户可以直观地查看负载均衡情况，同时还支持抗 DDoS 防护。
- en: 'All commands that are issued via GUI or API utilize the same REST API calls.
    The Avi Networks solution supports a Python and REST API. The Net Networks object
    model has numerous entities that are used to define load balancing in much the
    same way as NetScalers and F5:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通过 GUI 或 API 发出的命令都使用相同的 REST API 调用。Avi Networks 解决方案支持 Python 和 REST API。Net
    Networks 对象模型有许多实体，类似于 NetScalers 和 F5，能够定义负载均衡。
- en: '**Health monitor profile**: The health monitor pool profile entity specifies
    health checks for a pool of servers using health attributes.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康监测配置文件**：健康监测池配置文件实体指定池服务器的健康检查，使用健康属性进行监控。'
- en: '**Pool**: The pool entity specifies the IP addresses of virtual or physical
    servers in the form of a server list and has associated health monitor profiles;
    it also allows an event to be specified using a data script if a pool goes down.
    One or more pools are bound to the virtual service entity.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池**：池实体指定虚拟或物理服务器的 IP 地址，以服务器列表的形式呈现，并具有关联的健康监测配置文件；如果池出现故障，还可以通过数据脚本指定事件。一个或多个池与虚拟服务实体绑定。'
- en: '**Custom policy**: The custom policy allows users to programmatically specify
    policies against a virtual service.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义策略**：自定义策略允许用户以编程方式指定虚拟服务的策略。'
- en: '**App profile**: The app profile entity allows each application to be modeled
    with associated http attributes, security, DDoS, caching, compression, and PKI
    attributes specified as part of the app profile associated with a virtual service.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytics profile**: The analytics profile makes use of the Avi analytics
    engine and captures threat, metrics, health score as well as latency thresholds
    and failure codes that are mapped to the virtual service entity.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP/UDP profile**: The TCP/UDP profile governs if TCP or UDP is used and
    any DDoS L3/L4 profiles are set.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL profile**: The SSL entity governs SSL ciphers that will be used by a
    virtual service entity.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKI profile**: The PKI profile entity is bound to the virtual service entity
    and specifies the certificate authority for the virtual service.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy set**: The policy set entity allows users to set security teams to
    set policies against each virtual service governing request and response polices.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual service**: The virtual service entity is the entry point IP address
    to the load balanced pool of servers and is associated with all profiles to define
    the application pools load balancing and is bound to the TCP/UDP, app, SSL, SSL
    cert, policy, and analytics profiles.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nginx
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Nginx** ([https://www.nginx.com/](https://www.nginx.com/)) supports both
    commercial and open source versions. It is an x86 software load balancing solution.
    Nginx can be used as both an HTTP and TCP load balancer supporting HTTP, TCP,
    and even UDP, and can also support SSL/TLS termination.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Nginx can be set up for redundancy in a highly available fashion using *keepalived*,so
    if there is an outage on one Nginx load balancer, it will seamlessly fail over
    to a backup with zero downtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Nginx Plus is the commercial offering and is more fully featured than the open
    source version, supporting features such as active health checks, session persistence,
    and caching.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing on Nginx is set up by declaring syntax in the `nginx.conf` file.
    It works on the principle of wanting to simplify load balancing configuration.
    Unlike NetScalers, F5s, and Avi Networks, it does not utilize an object model
    to define load balancing rules, instead Nginx describes load balanced virtual
    or physical machines as backend servers using declarative syntax.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following simple example, we see three servers, `10.20.1.2`, `10.20.1.3`,
    and `10.20.1.4`, all load balanced on port `80` using Nginx declarative syntax,
    and it is served on `http://www.devopsfornetworking.com/devops_for_networking`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Nginx](img/B05559_05_03.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: By default, Nginx will load balance servers using round-robin load balancing
    method, but it also supports other load balancing methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The Nginx `least_conn` load balancing method forwards to backend servers with
    the least connections at any particular time, whereas the Nginx `ip_hash` method
    of load balancing means that users can tie the same source address to the same
    target backend server for the entirety of a request.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: This is useful as some applications require that all requests are tied to the
    same server using sticky sessions while transactions are processed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: However, the proprietary Nginx Plus version supports an additional load balancing
    method named `least_time`, which calculates the lowest latency of backend servers
    based on the number of active connections and subsequently forwards requests appropriately
    based on those calculations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The Nginx load balancer uses a weighting system at all times when load balancing;
    all servers by default have a weight of `1`. If a weight other than `1` is placed
    on a server, it will not receive requests unless the other servers on a backend
    are not available to process requests. This can be useful when throttling specific
    amounts of traffic to backend servers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see that the backend servers have load balancing
    method least connection configured. Server `10.20.1.3` has a weight of `5`, meaning
    only when `10.20.1.2` and `10.20.1.4` are maxed out will requests is sent to the
    `10.20.1.3` backend server:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![Nginx](img/B05559_05_04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: By default, using round-robin load balancing in Nginx won't stop forwarding
    requests to servers that are not responding, so it utilizes `max_fails` and `fail_timeouts`
    for this.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see server `10.20.1.2` and `10.20.1.4` have
    the `max_fail` count of `2` and a `fail_timeout` of `1` second; if this is exceeded
    then Nginx will stop directing traffic to these servers:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Nginx](img/B05559_05_05.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: HAProxy
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**HAProxy** ([http://www.haproxy.org/](http://www.haproxy.org/)) is an open
    source x86 software load balancer that is session aware and can provide layer
    4 load balancing. The HAproxy load balancer can also carry out layer 7 context
    switching based on the content of the request as well as SSL/TLS termination.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy is primarily used for HTTP load balancing and can be set up in a redundant
    fashion using `keepalived` configuration using two apache configurations, so if
    the master fails, the slave will become the master to make sure there is no interruption
    in service for end users.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy uses declarative configuration files to support load balancing as opposed
    to an object model that proprietary load balancing solutions, such as NetScaler,
    F5 and Avi Networks, have adopted.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The HAProxy configuration file has the following declarative configuration
    sections to allow load balancing to be set up:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**Backend**: A backend declaration can contain one or more servers in it; backend
    servers are added in the format of a DNS record or an IP address. Multiple backend
    declarations can be set up on a HAProxy server. The load balancing algorithm can
    also be selected, such as round robin or least connection.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, we see two backend servers, `10.11.0.1` and `10.11.0.2`,
    load balanced using the round-robin algorithm on port `80`:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![HAProxy](img/B05559_05_06.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Check**: Checks avoid users having to manually remove a server from the backend
    if for any reason, it becomes unavailable and this mitigates outages. HAProxy''s
    default health always attempts to establish a TCP connection to the server using
    the default port and IP. HAProxy will automatically disable servers that are unable
    to serve requests to avoid outages. Servers will only be re-enabled when it passes
    its check. HAProxy will report whole backends as unavailable if all servers on
    a backend have failed their health checks.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A number of different health checks can be put against backend servers by utilizing
    the option `{health-check}` line item; for instance, `tcp-check` in the following
    example can check on the health of port `8080` even though port `443` is being
    balanced:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![HAProxy](img/B05559_05_07.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Access Control List (ACL)**: ACL declarations are used to inspect headers
    and forward to specific backend servers based on the headers. An ACL in HAProxy
    will try to find conditions and trigger actions based on this.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frontend**: The frontend declaration allows different kinds of traffic to
    be supported by the HAProxy load balancer.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, HAProxy will accept http traffic on port `80`, with
    an ACL matching requests only if the request starts with `/network` and it is
    then forwarded to the `high-perf-backend` if the ACL `/web-network` is matched:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![HAProxy](img/B05559_05_08.jpg)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Load balancing immutable and static infrastructure
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the introduction of public and private cloud solutions such as AWS and
    OpenStack, there has been a shift towards utilizing immutable infrastructure instead
    of traditional static servers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: This has raised a point of contention with *pets versus cattle* or, as Gartner
    defines it *bi-modal* ([http://www.gartner.com/it-glossary/bimodal/](http://www.gartner.com/it-glossary/bimodal/)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Gartner has said that two different strategies need to be adopted, one for new
    microservices, *cattle,* and one for legacy infrastructure, *pets*. *Cattle* are
    servers that are killed off once they have served their purpose or have an issue,
    typically lasting one release iteration. Alternately, *pets* are servers that
    will have months or years of uptime and will be patched and cared for by operations
    staff.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Gartner defines *pets* as Mode 1 and *cattle* as Mode 2\. It is said that a
    *cattle* approach favors the stateless microservice cloud-native applications,
    whereas a *pet*, on the other hand, is any application that is a monolith, or
    potentially a single appliance or something that contains data, such as a database.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Immutable infrastructure and solutions such as OpenStack and AWS are said by
    many to favor only the *cattle*, with monoliths and databases remaining pets still
    need a platform that caters for long-lived servers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I find the *pets* versus *cattle* debate to be a very lazy argument
    and somewhat tiresome. Instead of dumping applications into two buckets, applications
    should be treated as a software delivery problem, which becomes a question of
    stateless read applications and stateful applications with caching and data. Cloud-native
    microservice applications still need data and state, so I am puzzled by the distinction.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: However, it is undisputed that the load balancer is key to immutable infrastructure,
    as at least one version of the application always needs to be exposed to a customer
    or other microservices to maintain that applications incur zero downtime and remain
    operational at all times.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Static and immutable servers
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Historically, an operations team was used by companies to perform the following
    operations on servers:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Rack and cable
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing firmware updates
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the RAID configuration
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing an operating system
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patching the operating system
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was all before making the servers available to developers. Static infrastructure
    can still exist within a cloud environment; for example, databases are still typically
    deployed as static, physical servers, given the volume of data that needs to be
    persisted on their local disk.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Static servers mean a set of long-lived servers that typically will contain
    state.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Immutable servers, on the other hand, mean that every time a virtual machine
    is changed, a new virtual machine is deployed, complete with a new operating system
    and new software released on them, delete please. Immutable infrastructure means
    no in-place changes to a server's state.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: This moves away from the pain of doing in-place upgrades and makes sure that
    snowflake server configurations are a thing of the past, where every server, despite
    the best intentions, has drifted slightly from its desired state over a period
    of time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: How many times when releasing software has a release worked on four out of five
    machines, and hours or days were wasted debugging why a particular software upgrade
    wasn't working on a particular server.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Immutable infrastructure builds servers from a known state promoting the same
    configuration to quality assurance, integration, performance testing, and production
    environments.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Parts of cloud infrastructure can be made completely immutable to reap these
    benefits. The operating system is one such candidate; rather than doing in-place
    patching, a single golden image can be created and patched using automation tooling
    such as Packer in a fully automated fashion.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Applications that require a caching layer are more stateful by nature so that
    cache needs to be available at all times to serve other applications. These caching
    applications should be deployed as clusters, which are load balanced, and rolling
    updates will be done to make sure one version of the cache data is always available.
    A new software release of that caching layer should synchronize the cache to the
    new release before the pervious release is destroyed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Data, on the other hand, is always persistent, so can be stored on persistent
    storage and then mounted by the operating system. When doing an immutable rolling
    update, the operating system layer can mount the data on either persistent or
    shared storage as part of the release process.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to separate the operating system and the data to make all virtual
    machines stateless, for instance, OpenStack Cinder ([https://wiki.openstack.org/wiki/Cinder](https://wiki.openstack.org/wiki/Cinder))
    can be utilized to store persistent data on volumes that can be attached to virtual
    machines.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: With all these use cases considered, most applications can be designed to be
    deployed immutably through proper configuration management, even monoliths, as
    long as they are not a single point of failure. If any applications are single
    points of failure, they should be rearchitected as releasing software should never
    result in downtime. Although applications are stateful, each state can be updated
    in stages so that an overall immutable infrastructure model can be maintained.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green deployments
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **blue green deployment** process is not a new concept. Before cloud solutions
    came to prominence, production servers would typically have a set of servers consisting
    of blue (no live traffic) and green (serving customer traffic) that would be utilized.
    These are typically known as blue and green servers, which alternated per release.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The blue green model in simple terms means that when a software upgrade needed
    to be carried out, the blue servers would be upgraded to the latest software version.
    Once the upgrade had been completed, the blue servers would become the new green
    servers with live traffic switched to serve from the newly upgraded servers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The switching of live traffic was typically done by switching DNS entries to
    point at the newly upgraded servers. So once the DNS **Time To Live** (**TTL**)
    had propagated, end user requests would be served by the newly upgraded servers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: This means that if there was an issue with a software release, rollback could
    be achieved by switching back the DNS entries to point at the previous software
    version.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical blue green deployment process is described here:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '**Release 1.1** would be deployed on servers 1, 2, and 3 and served on a load
    balancer to customers and made Green (live):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_09.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: '**Release 1.2** would be deployed on servers 4, 5, and 6 and then be patched
    to the latest patch version, upgraded to the latest release and tested. When ready,
    the operations team would toggle the load balancer to serve boxes 4, 5, and 6
    as the new production release, as shown later, and the previously green (live)
    deployment would become blue, and vice versa:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_10.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'When the operations team came to do the next release, servers 1, 2, and 3 would
    be patched to the latest version, upgraded to **Release 1.3** from **Release 1.1**,
    tested, and when ready, the operations team would direct traffic to the new release
    using the load balancer, making **Release 1.2** blue and **Release 1.3** green,
    as shown in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_11.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: This was traditionally the procedure of running a blue green deployment using
    static servers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when using an immutable model, instead of using long-lived static
    servers, such as Servers 1, 2, 3, 4, 5, and 6, after a release was successful,
    the servers would be destroyed, as shown here, as they have served their purpose:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_12.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: The next time servers 4, 5, and 6 were required, instead of doing an in-place
    upgrade, three new virtual machines would be created from the golden base image
    in a cloud environment. These golden images would already be patched up to the
    latest version, so brand new servers 7, 8, and 9 with the old servers destroyed
    and the new **Release 1.4** would be deployed on them, as shown later.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Once server 7, 8, and 9 were live, servers 1, 2, and 3 would be destroyed as
    they have served their purpose:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Blue/green deployments](img/B05559_05_13.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Using Ansible to Orchestrate load balancers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html "Chapter 4. Configuring Network Devices Using Ansible"),
    *Configuring Network Devices Using Ansible*, we covered the basics of Ansible
    and how to use an Ansible Control Host, playbooks, and roles for configuration
    management of network devices. Ansible, though, has multiple different core operations
    that can help with orchestrating load balancers, which we will look at in this
    chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Delegation
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ansible delegation is a powerful mechanism that means from a playbook or role,
    Ansible can carry out actions on the target servers specified in the inventory
    file by connecting to them using SSH or WinRM, or alternately execute commands
    from the Ansible Control Host. WinRM is the Microsoft remote management standard
    and the equivalent of SSH for Windows that allows administrators to connect to
    Windows guests and execute programs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows these two alternative connection methods with the
    Ansible Control Host either logging in to boxes using SSH or WinRM to configure
    them or running an API call from the Ansible Control Host directly:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Delegation](img/B05559_05_26.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Both of these options can be carried out from the same role or playbook using
    `delegate_to`, which makes playbooks and roles extremely flexible as they can
    combine API calls and server-side configuration management tasks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of delegation can be found later where the Ansible extras HAProxy
    modules are used, with `delegate_to` used to trigger an orchestration action that
    disables all backend services in the inventory file:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Delegation](img/B05559_05_15.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Utilizing serial to control roll percentages
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to release software without interruptions to service, a zero downtime
    approach is preferable, as it doesn't require a maintenance window to schedule
    a change or release. Ansible supports a *serial* option, which passes a percentage
    value to a playbook.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The *serial* option allows Ansible to iterate over the inventory and only carry
    out the action against a percentage of the boxes, completing the necessary playbook,
    before moving onto the next portion of the inventory. It is important to note
    that Ansible passes inventory as an unordered dictionary, so the percentage of
    the inventory that is processed will not be in a specific order.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the *serial* option that a blue/green strategy could be employed in Ansible,
    so boxes will need to be taken out of the load balancer and upgraded before being
    put back into service. Rather than doubling up on the number of boxes, three boxes
    are required, as shown in the following image, which all serve **Release 1.4**:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Utilizing serial to control roll percentages](img/B05559_05_16.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Utilizing the following Ansible playbook, using a combination of `delegate_to`
    and `serial`, each of the servers can be upgraded using a rolling update:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![Utilizing serial to control roll percentages](img/B05559_05_17.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'The playbook will execute the following steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The `serial 30%` will mean that only one server at a time is upgraded. So, **Server
    7** will be taken out of the HAProxy `backend_nodes` pool by disabling the service
    calling the HAProxy using a local `delegate_to` action on the Ansible Control
    Host. A `yum` update will then be executed to upgrade the server version new `application1`
    release **version 1.5** on **server 7**, as follows:![Utilizing serial to control
    roll percentages](img/B05559_05_18.jpg)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Server 7** will then be enabled again and put into service on the load balancer
    using a local `delegate_to` action. The serial command will iterate onto `server
    8` and disable it on HAProxy, before doing a `yum` update to upgrade the server
    version new `application1` release **version 1.5**, as follows:![Utilizing serial
    to control roll percentages](img/B05559_05_19.jpg)'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rolling update will then enable **Server 8** on the load balancer, and the
    serial command will iterate onto **Server 9**, disabling it on HAProxy before
    doing a yum update, which will upgrade the server with the new `application1`
    release **version 1.5** alternating when necessary between execution on the local
    server and the server, as shown here:![Utilizing serial to control roll percentages](img/B05559_05_20.jpg)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the playbook will finish by enabling **server 9** on the load balancer,
    and all servers will be upgraded to **Release 1.5** using Ansible as follows:![Utilizing
    serial to control roll percentages](img/B05559_05_21.jpg)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dynamic inventories
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with cloud platforms, using just static inventories is sometimes
    not enough. It is useful to understand the inventory of servers that are already
    deployed within the estate and target subsets of them based on characteristics
    or profiles.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Ansible has an enhanced feature named the dynamic inventory. It allows users
    to query a cloud platform of their choosing with a Python script; this will act
    as an autodiscovery tool that can be connected to AWS or OpenStack, returning
    the server inventory in JSON format.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: This allows Ansible to load this JSON file into a playbook or role so that it
    can be iterated over. In the same way, a static inventory file can be via variables.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamic inventory fits into the same command-line constructs instead of
    passing the following static inventory:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, a dynamic inventory script, `openstack.py`, for the OpenStack cloud provider
    could be passed instead:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dynamic inventory script can be set up to allow specific limits. In the
    preceding case, the only server inventory that has been returned is the quality
    assurance servers, which is controlled using the `–l qa` limit.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: When using Ansible with immutable servers, the static inventory file can be
    utilized to spin up new virtual machines, whereas the static inventory can be
    used to query the estate and do supplementary actions when they have already been
    created.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Tagging metadata
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using dynamic inventory in Ansible, metadata becomes a very important component,
    as servers deployed in a cloud environment can be sorted and filtered using metadata
    that is tagged against virtual or physical machines.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: When provisioning AWS, Microsoft Azure, or OpenStack instances in a public or
    private cloud, metadata can be tagged against servers to group them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see a playbook creating new OpenStack servers
    using the `os_server` OpenStack module. It will iterate over the static inventory,
    tagging each newly created group, and release metadata on the machine:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Tagging metadata](img/B05559_05_22.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: 'The dynamic inventory can then be filtered using the `–l` argument to specify
    boxes with `group: qa`. This will return a consolidated list of servers.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Jinja2 filters
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Jinja2 filters** allow Ansible to filter a playbook or role, allowing it
    to control which conditions need to be satisfied before executing a particular
    command or module. There are a wide variety of different jinja2 filters available
    out of the box with Ansible or custom filters can be written.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a playbook using a jinja2 filter would only add the server to
    the NetScaler if its metadata `openstack.metadata.build` value is equal to the
    current build version:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Jinja2 filters](img/B05559_05_23.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Executing the ansible-playbook `add_hosts_to_netscaler.yml command` with a
    limit `–l` on `qa` would only return boxes in the `qa` metadata group as the inventory.
    Then, the boxes can be further filtered at playbook or role using the when jinja2
    filter to only execute the `add into load balancer pool` command if the `openstack.metadata.build`
    number of the box matches the `current_build` variable of `9`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result of this would be that only the new boxes would be added to the NetScaler
    `lbvserver` VIP.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The boxes could be removed in a similar way in the same playbook with a *not
    equal to* condition:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Jinja2 filters](img/B05559_05_24.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: This could all be combined along with the serial percentage to roll percentages
    of the new release into service on the load balancer and decommission the old
    release utilizing dynamic inventory, delegation, jinja2 filters, and the serial
    rolling update features of Ansible together for simple orchestration of load balancers.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Creating Ansible networking modules
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As Ansible can be used to schedule API commands against a load balancer, it
    can be easily utilized to build out a load balancer object model that popular
    networking solutions, such as Citrix NetScaler, F5 Big-IP, or Avi Networks, utilize.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: With the move to microservice architectures, load balancing configuration needs
    to be broken out to remain manageable, so it is application-centric , as opposed
    to living in a centralized monolith configuration file.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This means that there are operational concerns when doing load balancing changes,
    so Ansible can be utilized by network operators to build out the complex load
    balancing rules, apply SSL certificates, and set up more complex layer 7 context
    switching or public IP addresses and provide this as a service to developers.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Python APIs provided by load balancing vendors, each operation
    could then be created as a module with a set of YAML `var` files describing the
    intended state of the load balancer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example mentioned later, we look at how Ansible var files could be utilized
    by developers to create a service and health monitor for every new virtual server
    on a NetScaler. These services are then bound to the `lbvserver` entity, which
    was created by the network team, with a roll percentage of 10%, which can be loaded
    into the playbook''s `serial` command. The playbook or role is utilized to create
    services, lbmonitors, 34 servers and bind services to lbvservers, whereas the
    var file describes the desired state of those NetScaler objects:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating Ansible networking modules](img/B05559_05_25.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that the varied load balancing solutions are available
    from proprietary vendors to open source solutions, and discussed the impact that
    microservices have had on load balancing, moving it from a centralized to distributed
    model to help serve east-west traffic.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at blue/green deployment models, the merits of immutable and
    static servers, and how software releases can be orchestrated using Ansible in
    either model. In the process, we illustrated how useful Ansible is at orchestrating
    load balancers by utilizing dynamic inventory, rolling updates, delegation, and
    jinja2 filters can all be used to help fulfill load balancing requirements.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaways from this chapter are that microservice applications have
    changed the way applications need to be load balanced, and distributed load balancing
    is better suited when deploying microservice applications, which have more east-west
    traffic patterns.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The reasons that immutable infrastructure is well-suited to microservice applications
    should now be clear.. The chapter also defined ways that state and data can be
    separated from the operating system and that different rolling update models are
    required to support stateless and stateful applications. In the next chapter,
    we will look at applying these same automation principles to SDN Controllers,
    primarily focusing on the Nuage solution. It will cover configuring firewall rules
    and other SDN commands, so the whole network can be programmatically controlled
    and automated.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
