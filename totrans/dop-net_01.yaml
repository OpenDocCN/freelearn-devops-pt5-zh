- en: Chapter 1. The Impact of Cloud on Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 云计算对网络的影响
- en: This chapter will look at ways that networking has changed in the private data
    centers and evolved in the past few years. It will focus on the emergence of **Amazon
    Web Services** (**AWS**) for public cloud and **OpenStack** for private cloud
    and ways in which this has changed the way developers want to consume networking.
    It will look at some of the networking services that AWS and OpenStack provide
    out of the box and look at some of the features they provide. It will show examples
    of how these cloud platforms have made networking a commodity much like infrastructure.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨过去几年内私有数据中心中网络的变化以及如何演变。重点将放在公有云中的**亚马逊网络服务**（**AWS**）和私有云中的**OpenStack**的出现，以及它们如何改变开发人员使用网络的方式。它还将探讨AWS和OpenStack提供的现成网络服务，并展示它们提供的一些功能。最后，它将展示这些云平台如何将网络变成了类似基础设施的商品。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: An overview of cloud approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云计算方法概述
- en: The difference between Spanning Tree networks and Leaf-Spine networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨越树（Spanning Tree）网络与叶脊（Leaf-Spine）网络的区别
- en: Changes that have occurred in networking with the introduction of public cloud
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公有云引入后，网络发生的变化
- en: The Amazon Web Services approach to networking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊网络服务（AWS）网络方法
- en: The OpenStack approach to networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack网络方法
- en: An overview of cloud approaches
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云计算方法概述
- en: The cloud provider market is currently saturated with a multitude of different
    private, public, and hybrid cloud solutions, so choice is not a problem for companies
    looking to implement public, private, or hybrid cloud solutions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商市场目前已经饱和，存在多种不同的私有云、公有云和混合云解决方案，因此对于希望实施公有云、私有云或混合云解决方案的公司来说，选择不再是问题。
- en: Consequently, choosing a cloud solution can sometimes be quite a daunting task,
    given the array of different options that are available.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择云解决方案有时会是一个令人生畏的任务，因为市场上有众多不同的选择。
- en: The battle between public and private cloud is still in its infancy, with only
    around 25 percent of the industry using public cloud, despite its perceived popularity,
    with solutions such as Amazon Web Services, Microsoft Azure, and Google Cloud
    taking a large majority of that market share. However, this still means that 75
    percent of the cloud market share is available to be captured, so the cloud computing
    market will likely go through many iterations in the coming years.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云与私有云之间的竞争仍处于起步阶段，尽管公有云被认为非常流行，但只有大约25%的行业在使用公有云，解决方案如亚马逊网络服务、微软Azure和谷歌云占据了市场的大部分份额。然而，这仍意味着云市场有75%的市场份额尚未被占领，因此云计算市场在未来几年可能会经历多次迭代。
- en: So why are many companies considering public cloud in the first place and why
    does it differ from private and hybrid clouds?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么许多公司最初考虑使用公有云，公有云与私有云和混合云又有什么不同呢？
- en: Public clouds
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公有云
- en: Public clouds are essentially a set of data centers and infrastructure that
    are made publicly available over the Internet to consumers. Despite its name,
    it is not magical or fluffy in any way. Amazon Web Services launched their public
    cloud based on the idea that they could rent out their servers to other companies
    when they were not using them during busy periods of the year.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云本质上是一组数据中心和基础设施，通过互联网公开提供给消费者。尽管名字中带有“云”，但它并非魔法或虚无缥缈的东西。亚马逊网络服务（Amazon Web
    Services）推出公有云的初衷是，在一年中忙碌的时期，他们可以将空闲的服务器租借给其他公司使用。
- en: Public cloud resources can be accessed via a **Graphical User Interface** (**GUI**)
    or, programmatically, via a set of API endpoints. This allows end users of the
    public cloud to create infrastructure and networking to host their applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云资源可以通过**图形用户界面**（**GUI**）或通过一组API端点以编程方式访问。这使得公有云的最终用户能够创建基础设施和网络，以托管他们的应用程序。
- en: Public clouds are used by businesses for various reasons, such as the speed
    it takes to configure and using public cloud resources is relatively low. Once
    credit card details have been provided on a public cloud portal, end users have
    the freedom to create their own infrastructure and networking, which they can
    run their applications on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 企业使用公有云有多种原因，例如配置速度快，并且使用公有云资源的成本相对较低。一旦提供了信用卡信息，最终用户就可以自由创建自己的基础设施和网络，并在其上运行应用程序。
- en: This infrastructure can be elastically scaled up and down as required, all at
    a cost of course to the credit card.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基础设施可以根据需要弹性地扩展和缩减，当然，所有这些都需要一定的费用，通过信用卡支付。
- en: Public cloud has become very popular as it removes a set of historical impediments
    associated with **shadow IT**. Developers are no longer hampered by the restrictions
    enforced upon them by bureaucratic and slow internal IT processes. Therefore,
    many businesses are seeing public cloud as a way to skip over these impediments
    and work in a more agile fashion allowing them to deliver new products to market
    at a greater frequency.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云已经变得非常流行，因为它解决了一些与**影子IT**相关的历史性障碍。开发者不再受到官僚化和缓慢的内部IT流程强加的限制。因此，许多企业将公有云视为跳过这些障碍、以更灵活的方式工作的途径，使他们能够更频繁地将新产品推向市场。
- en: When a business moves its operations to a public cloud, they are taking the
    bold step to stop hosting their own data centers and instead use a publicly available
    public cloud provider, such as Amazon Web Services, Microsoft Azure, IBM BlueMix,
    Rackspace, or Google Cloud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当企业将其运营迁移到公有云时，他们是在迈出大胆的一步，停止托管自己的数据中心，而是使用一个公开的公有云服务提供商，如Amazon Web Services、Microsoft
    Azure、IBM BlueMix、Rackspace或Google Cloud。
- en: The reliance is then put upon the public cloud for uptime and **Service Level
    Agreements** (**SLA**), which can be a huge cultural shift for an established
    business.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，依赖于公有云的正常运行时间和**服务等级协议**（**SLA**），这对一个成熟的企业来说可能是一个巨大的文化转变。
- en: Businesses that have moved to public cloud may find they no longer have a need
    for a large internal infrastructure team or network team, instead all infrastructure
    and networking is provided by the third-party public cloud, so it can in some
    quarters be viewed as giving up on internal IT.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 已经迁移到公有云的企业可能会发现，他们不再需要庞大的内部基础设施团队或网络团队，所有基础设施和网络都由第三方公有云提供，因此在某些情况下，这可能被视为放弃内部IT。
- en: Public cloud has proved a very successful model for many start-ups, given the
    agility it provides, where start-ups can put out products quickly using software-defined
    constructs without having to set up their own data center and remain product focused.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云证明了对于许多初创公司来说是一个非常成功的模式，因为它提供的灵活性，初创公司可以迅速推出产品，利用软件定义的构件，而无需建立自己的数据中心，保持产品专注。
- en: However, the **Total Cost of Ownership** (**TCO**) to run all of a business's
    infrastructure in a public cloud is a hotly debated topic, which can be an expensive
    model if it isn't managed and maintained correctly. The debate over public versus
    private cloud TCO rages on as some argue that public cloud is a great short-term
    fix but growing costs over a long period of time mean that it may not be a viable
    long-term solution compared with private cloud.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，运行所有企业基础设施在公有云中的**拥有总成本**（**TCO**）是一个备受争议的话题，如果没有正确的管理和维护，这可能是一个昂贵的模式。关于公有云与私有云TCO的辩论仍在继续，一些人认为公有云是一个很好的短期解决方案，但随着时间的推移，成本的增长意味着它可能不是与私有云相比的可行长期解决方案。
- en: Private cloud
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 私有云
- en: Private cloud is really just an extension of the initial benefits introduced
    by virtualization solutions, such as VMware, Hyper-V, and Citrix Xen, which were
    the cornerstone of the virtualization market. The private cloud world has moved
    on from just providing virtual machines, to providing software-defined networking
    and storage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 私有云实际上只是虚拟化解决方案（如VMware、Hyper-V和Citrix Xen）最初带来的好处的延伸，这些虚拟化解决方案曾是虚拟化市场的基石。私有云的世界已经不再仅仅提供虚拟机，而是提供软件定义的网络和存储。
- en: With the launch of public clouds, such as Amazon Web Services, private cloud
    solutions have sought to provide like-for-like capability by putting a software-defined
    layer on top of their current infrastructure. This infrastructure can be controlled
    in the same way as the public cloud via a GUI or programmatically using APIs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公有云的推出，如Amazon Web Services，私有云解决方案已试图通过在当前基础设施上加一个软件定义层来提供类似的功能。这个基础设施可以像公有云一样通过图形界面（GUI）或通过API编程方式进行控制。
- en: Private cloud solutions such as Apache CloudStack and open source solutions
    such as OpenStack have been created to bridge the gap between the private cloud
    and the public cloud.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 私有云解决方案，如Apache CloudStack和开源解决方案如OpenStack，已被创建来弥合私有云和公有云之间的差距。
- en: This has allowed vendors the agility of private cloud operations in their own
    data center by overlaying software-defined constructs on top of their existing
    hardware and networks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得厂商可以通过在现有硬件和网络之上叠加软件定义的构件，获得在自有数据中心内运行私有云操作的灵活性。
- en: However, the major benefit of private cloud is that this can be done within
    the security of a company's own data centers. Not all businesses can use public
    cloud for compliance, regularity, or performance reasons, so private cloud is
    still required for some businesses for particular workloads.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，私有云的主要好处在于这可以在公司自有数据中心的安全性范围内完成。并非所有企业都能出于合规性、常规性或性能原因使用公共云，因此一些企业在特定工作负载上仍然需要私有云。
- en: Hybrid cloud
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合云
- en: Hybrid cloud can often be seen as an amalgamation of multiple clouds. This allows
    a business to seamlessly run workloads across multiple clouds linked together
    by a network fabric. The business could select the placement of workloads based
    on cost or performance metrics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云通常可以被视为多个云的结合体。这使得企业能够通过网络结构无缝地在多个云之间运行工作负载。企业可以根据成本或性能指标来选择工作负载的部署位置。
- en: A hybrid cloud can often be made up of private and public clouds. So, as an
    example, a business may have a set of web applications that it wishes to scale
    up for particular busy periods and are better suited to run on public cloud so
    they are placed there. However, the business also needs a highly regulated, PCI-compliant
    database, which would be better-suited to being deployed in a private on-premises
    cloud. So a true hybrid cloud gives a business these kinds of options and flexibility.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云通常可以由私有云和公共云组成。因此，举个例子，一个企业可能有一套需要在特定繁忙时期进行扩展的Web应用，这些应用更适合运行在公共云中，因此它们被部署在那里。然而，该企业还需要一个高度受监管、符合PCI标准的数据库，后者更适合部署在私有的本地云中。因此，真正的混合云为企业提供了这类选择和灵活性。
- en: Hybrid cloud really works on the premise of using different clouds for different
    use cases, where each horse (application workload) needs to run a particular course
    (cloud). So, sometimes, a vendor-provided **Platform as a Service** (**PaaS**)
    layer can be used to place workloads across multiple clouds or alternately different
    configuration management tools, or container orchestration technologies can be
    used to orchestrate application workload placement across clouds.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云的真正运作前提是根据不同的用例使用不同的云，其中每匹马（应用工作负载）需要跑特定的赛道（云）。因此，有时可以使用厂商提供的**平台即服务**（**PaaS**）层来跨多个云部署工作负载，或者使用不同的配置管理工具或容器编排技术来协调跨云的应用工作负载部署。
- en: Software-defined
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件定义
- en: The choice between public, private, or hybrid cloud really depends on the business,
    so there is no real right or wrong answer. Companies will likely use hybrid cloud
    models as their culture and processes evolve over the next few years.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 公共云、私有云或混合云的选择实际上取决于企业，因此没有绝对的对错答案。随着公司文化和流程在未来几年内的发展，它们可能会使用混合云模型。
- en: If a business is using a public, private, or hybrid cloud, the common theme
    with all implementations is that they are moving towards a software-defined operational
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个企业正在使用公共云、私有云或混合云，那么所有实现方式的共同主题就是它们都朝着软件定义的运营模式发展。
- en: So what does the term *software-defined* really mean? In simple terms, *software-defined*
    means running a software abstraction layer over hardware. This software abstraction
    layer allows graphical or programmatic control of the hardware. So, constructs,
    such as infrastructure, storage, and networking, can be software defined to help
    simplify operations, manageability as infrastructure and networks scale out.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，*软件定义*到底是什么意思呢？简而言之，*软件定义*意味着在硬件上运行一个软件抽象层。这个软件抽象层允许对硬件进行图形化或编程化控制。因此，像基础设施、存储和网络这样的构件可以被软件定义，从而帮助简化操作、提高可管理性，尤其是在基础设施和网络扩展时。
- en: When running private clouds, modifications need to be made to incumbent data
    centers to make them private cloud ready; sometimes, this is important, so the
    private data center needs to evolve to meet those needs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行私有云时，需要对现有的数据中心进行修改，以使其准备好成为私有云；有时，这一点非常重要，因此私有数据中心需要进化以满足这些需求。
- en: The difference between Spanning Tree and Leaf-Spine networking
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨树和叶脊网络的区别
- en: When considering the private cloud, traditionally, company's private datacenters
    have implemented 3-tier layer 2 networks based on the **Spanning Tree Protocol**
    (**STP**), which doesn't lend itself well to modern software-defined networks.
    So, we will look at what a STP is in more depth as well as modern Leaf-Spine network
    architectures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑私有云时，传统上，公司的私有数据中心已实现基于**生成树协议**（**STP**）的3层二层网络，这对于现代的软件定义网络并不十分适用。因此，我们将深入了解什么是STP以及现代的叶脊网络架构。
- en: Spanning Tree Protocol
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成树协议**'
- en: The implementation of STP provides a number of options for network architects
    in terms of implementation, but it also adds a layer of complexity to the network.
    Implementation of the STP gives network architects the certainty that it will
    prevent layer 2 loops from occurring in the network.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: STP的实现为网络架构师提供了多种实现选项，但也为网络增加了一层复杂性。实施STP可以让网络架构师确信它能够防止网络中发生二层环路。
- en: 'A typical representation of a 3-tier layer 2 STP-based network can be shown
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的3层基于STP的二层网络表示如下：
- en: '![Spanning Tree Protocol](img/B05559_01_01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![生成树协议](img/B05559_01_01.jpg)'
- en: The **Core** layer provides routing services to other parts of the data center
    and contains the core switches
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心**层为数据中心的其他部分提供路由服务，并包含核心交换机'
- en: The **Aggregation** layer provides connectivity to adjacent **Access** layer
    switches and the top of the Spanning Tree core
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合层**提供与相邻**接入**层交换机的连接，并位于生成树核心的顶部'
- en: The bottom of the tree is the **Access** layer; this is where bare metal (physical)
    or virtual machines connect to the network and are segmented using different VLANs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 树的底部是**接入**层；这是裸金属（物理）或虚拟机连接到网络并使用不同VLAN进行分段的地方。
- en: The use of layer 2 networking and STP mean that at the access layer of the network
    will use VLANs spread throughout the network. The VLANs sit at the access layer,
    which is where virtual machines or bare metal servers are connected. Typically,
    these VLANs are grouped by type of application, and firewalls are used to further
    isolate and secure them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二层网络和STP意味着网络的接入层将使用遍布整个网络的VLAN。这些VLAN位于接入层，虚拟机或裸金属服务器连接在此。通常，这些VLAN按应用类型分组，防火墙用于进一步隔离和保护它们。
- en: 'Traditional networks are normally segregated into some combination of the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 传统网络通常被划分为以下几种组合：
- en: '**Frontend**: It typically has web servers that require external access'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前端**：通常包含需要外部访问的Web服务器'
- en: '**Business Logic**: This often contains stateful services'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务逻辑**：通常包含有状态服务'
- en: '**Backend**: This typically contains database servers'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端**：通常包含数据库服务器'
- en: Applications communicate with each other by tunneling between these firewalls,
    with specific **Access Control List** (**ACL**) rules that are serviced by network
    teams and governed by security teams.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序通过这些防火墙之间的隧道进行通信，使用特定的**访问控制列表**（**ACL**）规则，这些规则由网络团队提供服务，并由安全团队进行管理。
- en: When using STP in a layer 2 network, all switches go through an election process
    to determine the root switch, which is granted to the switch with the lowest bridge
    id, with a bridge id encompassing the bridge priority and MAC address of the switch.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在二层网络中使用STP时，所有交换机都通过选举过程来确定根交换机，根交换机会授予具有最低桥接ID的交换机，桥接ID包括交换机的桥接优先级和MAC地址。
- en: 'Once elected, the root switch becomes the base of the spanning tree; all other
    switches in the Spanning Tree are deemed non-root will calculate their shortest
    path to the root and then block any redundant links, so there is one clear path.
    The calculation process to work out the shortest path is referred to as network
    convergence. (For more information refer to the following link: [http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/](http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选举完成，根交换机成为生成树的基础；生成树中的所有其他交换机被视为非根交换机，它们将计算到根交换机的最短路径，然后阻止任何冗余链路，从而形成一条清晰的路径。计算最短路径的过程称为网络收敛。（更多信息请参阅以下链接：[http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/](http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/))
- en: Network architects designing the layer 2 Spanning Tree network need to be careful
    about the placement of the root switch, as all network traffic will need to flow
    through it, so it should be selected with care and given an appropriate bridge
    priority as part of the network reference architecture design. If at any point,
    switches have been given the same bridge priority then the bridge with the lowest
    MAC address wins.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Network architects designing the layer 2 Spanning Tree network need to be careful
    about the placement of the root switch, as all network traffic will need to flow
    through it, so it should be selected with care and given an appropriate bridge
    priority as part of the network reference architecture design. If at any point,
    switches have been given the same bridge priority then the bridge with the lowest
    MAC address wins.
- en: Network architects should also design the network for redundancy so that if
    a root switch fails, there is a nominated backup root switch with a priority of
    one value less than the nominated root switch, which will take over when a root
    switch fails. In the scenario, the root switch fails the election process will
    begin again and the network will converge, which can take some time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Network architects should also design the network for redundancy so that if
    a root switch fails, there is a nominated backup root switch with a priority of
    one value less than the nominated root switch, which will take over when a root
    switch fails. In the scenario, the root switch fails the election process will
    begin again and the network will converge, which can take some time.
- en: The use of STP is not without its risks, if it does fail due to user configuration
    error, data center equipment failure or software failure on a switch or bad design,
    then the consequences to a network can be huge. The result can be that loops might
    form within the bridged network, which can result in a flood of broadcast, multicast
    or unknown-unicast storms that can potentially take down the entire network leading
    to long network outages. The complexity associated with network architects or
    engineers troubleshooting STP issues is important, so it is paramount that the
    network design is sound.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: The use of STP is not without its risks, if it does fail due to user configuration
    error, data center equipment failure or software failure on a switch or bad design,
    then the consequences to a network can be huge. The result can be that loops might
    form within the bridged network, which can result in a flood of broadcast, multicast
    or unknown-unicast storms that can potentially take down the entire network leading
    to long network outages. The complexity associated with network architects or
    engineers troubleshooting STP issues is important, so it is paramount that the
    network design is sound.
- en: Leaf-Spine architecture
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Leaf-Spine architecture
- en: 'In recent years with the emergence of cloud computing, we have seen data centers
    move away from a STP in favor of a Leaf-Spine networking architecture. The Leaf-Spine
    architecture is shown in the following diagram:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'In recent years with the emergence of cloud computing, we have seen data centers
    move away from a STP in favor of a Leaf-Spine networking architecture. The Leaf-Spine
    architecture is shown in the following diagram:'
- en: '![Leaf-Spine architecture](img/B05559_01_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Leaf-Spine architecture](img/B05559_01_02.jpg)'
- en: 'In a Leaf-Spine architecture:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'In a Leaf-Spine architecture:'
- en: Spine switches are connected into a set of core switches
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spine switches are connected into a set of core switches
- en: Spine switches are then connected with Leaf switches with each Leaf switch deployed
    at the top of rack, which means that any Leaf switch can connect to any Spine
    switch in one hop
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spine switches are then connected with Leaf switches with each Leaf switch deployed
    at the top of rack, which means that any Leaf switch can connect to any Spine
    switch in one hop
- en: Leaf-Spine architectures are promoted by companies such as Arista, Juniper,
    and Cisco. A Leaf-Spine architecture is built on layer 3 routing principle to
    optimize throughput and reduce latency.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine architectures are promoted by companies such as Arista, Juniper,
    and Cisco. A Leaf-Spine architecture is built on layer 3 routing principle to
    optimize throughput and reduce latency.
- en: Both Leaf and Spine switches communicate with each other via **external Border
    Gate Protocol** (**eBGP**) as the routing protocol for the IP fabric. eBGP establishes
    a **Transmission Control Protocol** (**TCP**) connection to each of its BGP peers
    before BGP updates can be exchanged between the switches. Leaf switches in the
    implementation will sit at top of rack and can be configured in **Multichassis
    Link Aggregation** (**MLAG**) mode using **Network Interface Controller** (**NIC**)
    bonding.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Both Leaf and Spine switches communicate with each other via **external Border
    Gate Protocol** (**eBGP**) as the routing protocol for the IP fabric. eBGP establishes
    a **Transmission Control Protocol** (**TCP**) connection to each of its BGP peers
    before BGP updates can be exchanged between the switches. Leaf switches in the
    implementation will sit at top of rack and can be configured in **Multichassis
    Link Aggregation** (**MLAG**) mode using **Network Interface Controller** (**NIC**)
    bonding.
- en: MLAG was originally used with **STP** so that two or more switches are bonded
    to emulate like a single switch and used for redundancy so they appeared as one
    switch to STP. In the event of a failure this provided multiple uplinks for redundancy
    in the event of a failure as the switches are peered, and it worked around the
    need to disable redundant paths. Leaf switches can often have **internal Border
    Gate Protocol** (**iBGP**) configured between the pairs of switches for resiliency.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MLAG最初与**STP**一起使用，使得两个或更多交换机绑定在一起，模拟成一个交换机，作为冗余使用，从而在STP中显示为一个交换机。在发生故障时，这提供了多个上行链路作为冗余，因为交换机是对等的，并且可以绕过禁用冗余路径的需求。Leaf交换机之间通常会配置**内部边界网关协议**（**iBGP**）以实现容错。
- en: In a Leaf-Spine architecture, Spine switches do not connect to other Spine switches,
    and Leaf switches do not connect directly to other Leaf switches unless bonded
    top of rack using MLAG NIC bonding. All links in a Leaf-Spine architecture are
    set up to forward with no looping. Leaf-Spine architectures are typically configured
    to implement **Equal Cost Multipathing** (**ECMP**), which allows all routes to
    be configured on the switches so that they can access any Spine switch in the
    layer 3 routing fabric.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在Leaf-Spine架构中，Spine交换机不会与其他Spine交换机连接，Leaf交换机也不会直接与其他Leaf交换机连接，除非使用MLAG网卡绑定在机架顶部进行绑定。Leaf-Spine架构中的所有链接都被设置为无环转发。Leaf-Spine架构通常配置为实现**等成本多路径**（**ECMP**），允许在交换机上配置所有路由，从而可以访问层3路由结构中的任何Spine交换机。
- en: ECMP means that Leaf switches routing table has the next-hop configured to forward
    to each Spine switch. In an ECMP setup, each leaf node has multiple paths of equal
    distance to each Spine switch, so if a Spine or Leaf switch fails, there is no
    impact as long as there are other active paths to another adjacent Spine switches.
    ECMP is used to load balance flows and supports the routing of traffic across
    multiple paths. This is in contrast to the STP, which switches off all but one
    path to the root when the network converges.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ECMP意味着Leaf交换机的路由表已配置了指向每个Spine交换机的下一跳。在ECMP配置中，每个Leaf节点都有多条与每个Spine交换机等距离的路径，因此如果某个Spine或Leaf交换机故障，只要还有其他活跃路径连接到相邻的Spine交换机，就不会受到影响。ECMP用于负载均衡流量，并支持通过多条路径路由流量。与STP相比，STP会在网络收敛时关闭除一条路径外的所有路径。
- en: Normally, Leaf-Spine architectures designed for high performance use 10G access
    ports at Leaf switches mapping to 40G Spine ports. When device port capacity becomes
    an issue, new Leaf switches can be added by connecting it to every Spine on the
    network while pushing the new configuration to every switch. This means that network
    teams can easily scale out the network horizontally without managing or disrupting
    the switching protocols or impacting the network performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了实现高性能，Leaf-Spine架构在Leaf交换机上使用10G接入端口，映射到40G的Spine端口。当设备端口容量成为问题时，可以通过将新Leaf交换机连接到网络中的每个Spine交换机，并将新配置推送到每个交换机，来添加新的Leaf交换机。这意味着网络团队可以轻松地横向扩展网络，而无需管理或中断交换协议，也不会影响网络性能。
- en: 'An illustration of the protocols used in a Leaf-Spine architecture are shown
    later, with Spine switches connected to Leaf switches using BGP and ECMP and Leaf
    switches sitting top of rack and configured for redundancy using MLAG and iBGP:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 后续会展示Leaf-Spine架构中使用的协议图示，Spine交换机通过BGP和ECMP与Leaf交换机连接，Leaf交换机位于机架顶部，并使用MLAG和iBGP配置以实现冗余：
- en: '![Leaf-Spine architecture](img/B05559_01_02A.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Leaf-Spine架构](img/B05559_01_02A.jpg)'
- en: 'The benefits of a Leaf-Spine architecture are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine架构的优势如下：
- en: Consistent latency and throughput in the network
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中的一致性延迟和吞吐量
- en: Consistent performance for all racks
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有机架的一致性性能
- en: Network once configured becomes less complex
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络一旦配置完成，复杂性会大大降低
- en: Simple scaling of new racks by adding new Leaf switches at top of rack
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在机架顶部添加新的Leaf交换机，可以简单地扩展新机架。
- en: Consistent performance, subscription, and latency between all racks
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有机架之间的一致性性能、订阅和延迟
- en: East-west traffic performance is optimized (virtual machine to virtual machine
    communication) to support microservice applications
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 东西向流量性能优化（虚拟机到虚拟机的通信），以支持微服务应用
- en: Removes VLAN scaling issues, controls broadcast and fault domains
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决VLAN扩展问题，控制广播和故障域
- en: The one drawback of a Leaf-Spine topology is the amount of cables it consumes
    in the data center.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine拓扑的一个缺点是它在数据中心中消耗的电缆数量。
- en: OVSDB
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OVSDB
- en: Modern switches have now moved towards open source standards, so they can use
    the same pluggable framework. The open standard for virtual switches is **Open
    vSwitch**, which was born out of the necessity to come up with an open standard
    that allowed a virtual switch to forward traffic to different virtual machines
    on the same physical host and physical network. Open vSwitch uses **Open vSwitch
    database** (**OVSDB**) that has a standard extensible schema.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现代交换机已经开始采用开源标准，这样它们就可以使用相同的可插拔框架。虚拟交换机的开源标准是**Open vSwitch**，它源于需要提出一个开放标准，使虚拟交换机能够将流量转发到同一物理主机和物理网络上的不同虚拟机。Open
    vSwitch 使用**Open vSwitch 数据库**（**OVSDB**），该数据库具有标准的可扩展架构。
- en: Open vSwitch was initially deployed at the hypervisor level but is now being
    used in container technology too, which has Open vSwitch implementations for networking.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch 最初在虚拟化管理程序层面部署，但现在也被用于容器技术，这些技术为网络提供了 Open vSwitch 实现。
- en: 'The following hypervisors currently implement Open vSwitch as their virtual
    switching technology:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下超管程序目前将 Open vSwitch 作为其虚拟交换技术：
- en: KVM
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KVM
- en: Xen
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xen
- en: Hyper-V
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyper-V
- en: '**Hyper-V** has recently moved to support Open vSwitch using the implementation
    created by Cloudbase ([https://cloudbase.it/](https://cloudbase.it/)), which is
    doing some fantastic work in the open source space and is testament to how Microsoft''s
    business model has evolved and embraced open source technologies and standards
    in recent years. Who would have thought it? Microsoft technologies now run natively
    on Linux.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hyper-V** 最近已开始支持使用 Cloudbase ([https://cloudbase.it/](https://cloudbase.it/))
    创建的 Open vSwitch 实现，Cloudbase 在开源领域做出了出色的工作，这也证明了微软的商业模式在近年来如何发展并拥抱开源技术和标准。谁能想到呢？现在，微软的技术可以原生运行在
    Linux 上。'
- en: 'The Open vSwitch exchanges **OpenFlow** between virtual switch and physical
    switches in order to communicate and can be programmatically extended to fit the
    needs of vendors. In the following diagram, you can see the Open vSwitch architecture.
    Open vSwitch can run on a server using the KVM, Xen, or Hyper-V virtualization
    layer:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch 交换**OpenFlow**数据，在虚拟交换机和物理交换机之间进行通信，并且可以通过编程扩展以适应供应商的需求。在下面的图示中，你可以看到
    Open vSwitch 的架构。Open vSwitch 可以在使用 KVM、Xen 或 Hyper-V 虚拟化层的服务器上运行：
- en: '![OVSDB](img/B05559_01_03.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![OVSDB](img/B05559_01_03.jpg)'
- en: The **ovsdb-server** contains the OVSDB schema that holds all switching information
    for the virtual switch. The **ovs-vswitchd** daemon talks **OpenFlow** to any
    **Control & Management Cluster,** which could be any SDN controller that can communicate
    using the **OpenFlow** protocol.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**ovsdb-server** 包含 OVSDB 架构，保存虚拟交换机的所有交换信息。**ovs-vswitchd** 守护进程与任何**控制与管理集群**通信，控制与管理集群可以是任何可以使用
    **OpenFlow** 协议通信的 SDN 控制器。'
- en: Controllers use OpenFlow to install flow state on the virtual switch, and OpenFlow
    dictates what actions to take when packets are received by the virtual switch.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器使用 OpenFlow 在虚拟交换机上安装流状态，OpenFlow 决定当虚拟交换机接收到数据包时应采取哪些操作。
- en: When Open vSwitch receives a packet it has never seen before and has no matching
    flow entries, it sends this packet to the controller. The controller then makes
    a decision on how to handle this packet based on the flow rules to either block
    or forward. The ability to configure **Quality of Service** (**QoS**) and other
    statistics is possible on Open vSwitch.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Open vSwitch 接收到一个它从未见过并且没有匹配流条目的数据包时，它会将该数据包发送给控制器。控制器随后根据流规则决定如何处理此数据包，是阻止它还是转发它。在
    Open vSwitch 上，可以配置**服务质量**（**QoS**）和其他统计信息。
- en: Open vSwitch is used to configure security rules and provision ACL rules at
    the switch level on a hypervisor.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch 用于在虚拟化管理程序的交换机级别配置安全规则并提供 ACL 规则。
- en: A Leaf-Spine architecture allows overlay networks to be easily built, meaning
    that cloud and tenant environments are easily connected to the layer 3 routing
    fabric. Hardware **Vxlan Tunnel Endpoints** (**VTEPs**) IPs are associated with
    each Leaf switch or a pair of Leaf switches in MLAG mode and are connected to
    each physical compute host via **Virtual Extensible LAN** (**VXLAN**) to each
    Open vSwitch that is installed on a hypervisor.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine 架构允许轻松构建覆盖网络，这意味着云环境和租户环境可以轻松地连接到第 3 层路由结构。硬件**Vxlan 隧道端点**（**VTEPs**）的
    IP 地址与每个 Leaf 交换机或 MLAG 模式下的一对 Leaf 交换机关联，并通过**虚拟可扩展局域网**（**VXLAN**）与安装在虚拟化管理程序上的每个
    Open vSwitch 连接到每个物理计算主机。
- en: This allows an SDN controller, which is provided by vendors, such as Cisco,
    Nokia, and Juniper to build an overlay network that creates VXLAN tunnels to the
    physical hypervisors using Open vSwitch. New VXLAN tunnels are created automatically
    if a new compute is scaled out, then SDN controllers can create new VXLAN tunnels
    on the Leaf switch as they are peered with the Leaf switch's hardware **VXLAN
    Tunnel End Point** (**VTEP**).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得由厂商提供的SDN控制器，例如思科（Cisco）、诺基亚（Nokia）和瞻博网络（Juniper），能够构建一个覆盖网络，通过Open vSwitch创建到物理虚拟化主机的VXLAN隧道。如果有新的计算节点扩展，新的VXLAN隧道会自动创建，之后SDN控制器可以在Leaf交换机上创建新的VXLAN隧道，因为它们与Leaf交换机的硬件**VXLAN隧道终端**（**VTEP**）建立了对等连接。
- en: Modern switch vendors, such as Arista, Cisco, Cumulus, and many others, use
    OVSDB, and this allows SDN controllers to integrate at the **Control & Management
    Cluster** level. As long as an SDN controller uses OVSDB and OpenFlow protocol,
    they can seamlessly integrate with the switches and are not tied into specific
    vendors. This gives end users a greater depth of choice when choosing switch vendors
    and SDN controllers, which can be matched up as they communicate using the same
    open standard protocol.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现代交换机厂商，如Arista、思科、Cumulus等，使用OVSDB，这使得SDN控制器能够在**控制与管理集群**层级进行集成。只要SDN控制器使用OVSDB和OpenFlow协议，它们就可以与交换机无缝集成，并且不受特定厂商的限制。这为最终用户在选择交换机厂商和SDN控制器时提供了更大的选择空间，因为它们使用相同的开放标准协议进行通信。
- en: Changes that have occurred in networking with the introduction of public cloud
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公有云引入后网络领域发生的变化
- en: It is unquestionable that the emergence of the AWS, which was launched in 2006,
    changed and shaped the networking landscape forever. AWS has allowed companies
    to rapidly develop their products on the AWS platform. AWS has created an innovative
    set of services for end users, so they can manage infrastructure, load balancing,
    and even databases. These services have led the way in making the DevOps ideology
    a reality, by allowing users to elastically scale up and down infrastructure.
    They need to develop products on demand, so infrastructure wait times are no longer
    an inhibitor to development teams. AWS rich feature set of technology allows users
    to create infrastructure by clicking on a portal or more advanced users that want
    to programmatically create infrastructure using configuration management tooling,
    such as **Ansible**, **Chef**, **Puppet**, **Salt** or **Platform as a Service**
    (**PaaS**) solutions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，2006年推出的AWS的出现彻底改变并塑造了网络格局。AWS使公司能够在AWS平台上快速开发其产品。AWS为最终用户创造了一套创新的服务，使他们能够管理基础设施、负载均衡，甚至数据库。这些服务引领了DevOps理念的实现，允许用户按需弹性地扩展和收缩基础设施。开发人员不再受到基础设施等待时间的限制，可以更高效地进行产品开发。AWS丰富的技术功能集允许用户通过点击门户来创建基础设施，或者更多的高级用户通过配置管理工具（如**Ansible**、**Chef**、**Puppet**、**Salt**或**平台即服务**（**PaaS**）解决方案）来编程创建基础设施。
- en: An overview of AWS
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS概述
- en: In 2016, the AWS **Virtual Private Cloud** (**VPC**) secures a set of Amazon
    EC2 instances (virtual machines) that can be connected to any existing network
    using a VPN connection. This simple construct has changed the way that developers
    want and expect to consume networking.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，AWS的**虚拟私有云**（**VPC**）保护一组Amazon EC2实例（虚拟机），这些实例可以通过VPN连接连接到任何现有网络。这个简单的构造改变了开发人员希望和期望的网络消费方式。
- en: In 2016, we live in a consumer-based society with mobile phones allowing us
    instant access to the Internet, films, games, or an array of different applications
    to meet our every need, instant gratification if you will, so it is easy to see
    the appeal of AWS has to end users.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，我们生活在一个以消费为主导的社会，手机让我们能即时访问互联网、电影、游戏，或是各种不同的应用程序来满足我们的一切需求，换句话说，我们生活在即时满足的时代，因此不难理解AWS对最终用户的吸引力。
- en: AWS allows developers to provision instances (virtual machines) in their own
    personal network, to their desired specification by selecting different flavors
    (CPU, RAM, and disk) using a few button clicks on the AWS portal's graphical user
    interface, alternately using a simple call to an API or scripting against the
    AWS-provided SDKs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: AWS允许开发人员通过AWS门户的图形用户界面，通过几次点击，选择不同的规格（CPU、RAM和磁盘）来在他们自己的个人网络中配置实例（虚拟机），或者通过简单的API调用或使用AWS提供的SDK进行脚本编写。
- en: 'So now a valid question, why should developers be expected to wait long periods
    of time for either infrastructure or networking tickets to be serviced in on-premises
    data centers when AWS is available? It really shouldn''t be a hard question to
    answer. The solution surely has to either be moved to AWS or create a private
    cloud solution that enables the same agility. However, the answer isn''t always
    that straightforward, there are following arguments against using AWS and public
    cloud:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在有一个合理的问题，为什么开发人员在 AWS 可用的情况下，仍然需要在本地数据中心等待长时间才能处理基础设施或网络工单？这个问题其实不难回答。解决方案肯定是将其迁移到
    AWS，或者创建一个能够提供相同敏捷性的私有云解决方案。然而，答案并非总是如此简单，下面是反对使用 AWS 和公有云的一些理由：
- en: Not knowing where the data is actually stored and in which data center
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不知道数据实际上存储在哪里以及在哪个数据中心
- en: Not being able to hold sensitive data offsite
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法将敏感数据存储在远程地点
- en: Not being able to assure the necessary performance
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法保证所需的性能
- en: High running costs
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高昂的运行成本
- en: All of these points are genuine blockers for some businesses that may be highly
    regulated or need to be PCI compliant or are required to meet specific regularity
    standards. These points may inhibit some businesses from using public cloud so
    as with most solutions it isn't the case of one size fits all.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题对于一些受高度监管、需要符合 PCI 合规性或必须满足特定监管标准的企业来说，都是实际的障碍。这些问题可能会阻碍一些企业使用公有云，因此，像大多数解决方案一样，并不是“一个解决方案适用于所有”。
- en: In private data centers, there is a cultural issue that teams have been set
    up to work in silos and are not set up to succeed in an agile business model,
    so a lot of the time using AWS, Microsoft Azure, or Google Cloud is a quick fix
    for broken operational models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在私有数据中心，有一个文化问题，即团队被设置为在孤岛中工作，并没有为成功的敏捷业务模型做准备，因此，很多时候，使用 AWS、微软 Azure 或谷歌云只是对破碎的运营模型的一个快速修复。
- en: Ticketing systems, a staple of broken internal operational models, are not a
    concept that aligns itself to speed. An IT ticket raised to an adjacent team can
    take days or weeks to complete, so requests are queued before virtual or physical
    servers can be provided to developers. Also, this is prominent for network changes
    too, with changes such as a simple modification to ACL rules taking an age to
    be implemented due to ticketing backlogs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 工单系统，作为破碎的内部运营模型的典型代表，并不是一个与速度相匹配的概念。提交给相邻团队的 IT 工单可能需要几天或几周才能完成，因此在虚拟或物理服务器提供给开发人员之前，请求会被排队等待。此外，对于网络变更也同样如此，例如对
    ACL 规则的简单修改，由于工单积压，可能需要很长时间才能实施。
- en: Developers need to have the ability to scale up servers or prototype new features
    at will, so long wait times for IT tickets to be processed hinder delivery of
    new products to market or bug fixes to existing products. It has become common
    in internal IT that some **Information Technology Infrastructure Library** (**ITIL**)
    practitioners put a sense of value on how many tickets that processed over a week
    as the main metric for success. This shows complete disregard for customer experience
    of their developers. There are some operations that need to shift to the developers,
    which have traditionally lived with internal or shadow IT, but there needs to
    be a change in operational processes at a business level to invoke these changes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员需要能够随时扩展服务器或原型化新功能，因此，长时间等待 IT 工单处理会妨碍新产品推向市场或现有产品的 bug 修复。在内部 IT 中，**信息技术基础架构库**（**ITIL**）的某些从业者将每周处理的工单数量作为成功的主要指标，这表明他们完全忽视了开发人员的客户体验。一些操作需要转交给开发人员，这些操作传统上是由内部
    IT 或影子 IT 负责的，但在业务层面需要对操作流程进行调整，才能推动这些变化。
- en: Put simply, AWS has changed the expectations of developers and the expectations
    placed on infrastructure and networking teams. Developers should be able to service
    their needs as quickly as making an alteration to an application on their mobile
    phone, free from slow internal IT operational models associated with companies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，AWS 改变了开发人员的期望以及对基础设施和网络团队的期望。开发人员应该能够像在手机上修改应用程序一样迅速满足自己的需求，摆脱公司内部 IT
    操作模型带来的拖延。
- en: But for start-ups and businesses that can use AWS, which aren't constrained
    by regulatory requirements, it skips the need to hire teams to rack servers, configure
    network devices, and pay for the running costs of data centers. It means they
    can start viable businesses and run them on AWS by putting in credit card details
    the same way as you would purchase a new book on Amazon or eBay.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 但是对于可以使用AWS的初创公司和企业来说，若不受监管要求的限制，它就省去了雇佣团队去安装服务器、配置网络设备和支付数据中心运营成本的需求。这意味着他们可以通过输入信用卡信息来启动可行的业务并在AWS上运行，就像在亚马逊或eBay上购买新书一样简单。
- en: OpenStack overview
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack概述
- en: The reaction to AWS was met with trepidation from competitors, as it disrupted
    the cloud computing industry and has led to PaaS solutions such as **Cloud Foundry**
    and **Pivotal** coming to fruition to provide an abstraction layer on top of hybrid
    clouds.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对AWS的反应在竞争者中引起了恐慌，因为它打乱了云计算行业，并促使了**Cloud Foundry**和**Pivotal**等PaaS解决方案的出现，为混合云提供了一个抽象层。
- en: When a market is disrupted, it promotes a reaction, from it spawned the idea
    for a new private cloud. In 2010, a joint venture by Rackspace and NASA, launched
    an open source cloud-software initiative known as OpenStack, which came about
    as NASA couldn't put their data in a public cloud.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当市场遭遇颠覆时，它会引发反应，由此催生了新的私有云的构思。2010年，由Rackspace和NASA共同发起了一项名为OpenStack的开源云软件计划，因为NASA无法将其数据存储在公共云中。
- en: The OpenStack project intended to help organizations offer cloud computing services
    running on standard hardware and directly set out to mimic the model provided
    by AWS. The main difference with OpenStack is that it is an open source project
    that can be used by leading vendors to bring AWS-like ability and agility to the
    private cloud.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack项目旨在帮助组织提供基于标准硬件运行的云计算服务，并直接模仿AWS提供的模式。与AWS的主要区别在于，OpenStack是一个开源项目，领先的供应商可以使用它将AWS类似的能力和敏捷性带到私有云中。
- en: Since its inception in 2010, OpenStack has grown to have over 500 member companies
    as part of the OpenStack Foundation, with platinum members and gold members that
    comprise the biggest IT vendors in the world that are actively driving the community.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 自2010年成立以来，OpenStack已发展成为超过500家会员公司的一部分，这些公司通过OpenStack基金会积极推动社区发展，其中包括全球最大的IT供应商的铂金会员和黄金会员。
- en: 'The platinum members of the OpenStack foundation are:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack基金会的铂金会员包括：
- en: '![OpenStack overview](img/B05559_01_04.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![OpenStack概述](img/B05559_01_04.jpg)'
- en: OpenStack is an open source project, which means its source code is publicly
    available and its underlying architecture is available for analysis, unlike AWS,
    which acts like a magic box of tricks but it is not really known for how it works
    underneath its shiny exterior.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack是一个开源项目，这意味着它的源代码是公开的，其底层架构可以供分析使用，不像AWS那样像一个魔法盒子，外表光鲜亮丽，但我们并不了解它是如何在内部运作的。
- en: OpenStack is primarily used to provide an **Infrastructure as a Service** (**IaaS**)
    function within the private cloud, where it makes commodity x86 compute, centralized
    storage, and networking features available to end users to self-service their
    needs, be it via the horizon dashboard or through a set of common API's.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack主要用于在私有云中提供**基础设施即服务**（**IaaS**）功能，它使得普通的x86计算、集中存储和网络功能可以被终端用户自助服务，无论是通过Horizon仪表板，还是通过一套常用的API。
- en: Many companies are now implementing OpenStack to build their own data centers.
    Rather than doing it on their own, some companies are using different vendor hardened
    distributions of the community upstream project. It has been proven that using
    a vendor hardened distributions of OpenStack, when starting out, mean that OpenStack
    implementation is far likelier to be successful. Initially, for some companies,
    implementing OpenStack can be seen as complex as it is a completely new set of
    technology that a company may not be familiar with yet. OpenStack implementations
    are less likely to fail when using professional service support from known vendors,
    and it can create a viable alternative to enterprise solutions, such as AWS or
    Microsoft Azure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司现在正在实施OpenStack来建立自己的数据中心。与自己独立完成不同，一些公司正在使用不同供应商提供的社区上游项目的加固发行版。已经证明，使用供应商加固的OpenStack发行版，可以大大提高OpenStack实施成功的可能性。对于一些公司来说，实施OpenStack最初可能会觉得很复杂，因为它是一个公司可能尚不熟悉的新技术。使用知名供应商的专业服务支持时，OpenStack实施不太可能失败，并且它可以为企业解决方案（如AWS或Microsoft
    Azure）提供一个可行的替代方案。
- en: Vendors, such as Red Hat, HP, Suse, Canonical, Mirantis, and many more, provide
    different distributions of OpenStack to customers, complete with different methods
    of installing the platform. Although the source code and features are the same,
    the business model for these OpenStack vendors is that they harden OpenStack for
    enterprise use and their differentiator to customers is their professional services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如 Red Hat、HP、Suse、Canonical、Mirantis 等供应商为客户提供不同的 OpenStack 发行版，并附带不同的安装方法。尽管源代码和功能相同，这些
    OpenStack 供应商的商业模式是将 OpenStack 加固以便企业使用，而他们对客户的差异化服务是专业服务。
- en: 'There are many different OpenStack distributions available to customers with
    the following vendors providing OpenStack distributions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的 OpenStack 发行版可供客户选择，以下供应商提供 OpenStack 发行版：
- en: Bright Computing
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bright Computing
- en: Canonical
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Canonical
- en: HPE
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPE
- en: IBM
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM
- en: Mirantis
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirantis
- en: Oracle OpenStack for Oracle Linux, or O3L
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle OpenStack for Oracle Linux，或称 O3L
- en: Oracle OpenStack for Oracle Solaris
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle OpenStack for Oracle Solaris
- en: Red Hat
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Red Hat
- en: SUSE
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SUSE
- en: VMware Integrated OpenStack (VIO)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware 集成 OpenStack（VIO）
- en: OpenStack vendors will support build out, on-going maintenance, upgrades, or
    any customizations a client needs, all of which are fed back to the community.
    The beauty of OpenStack being an open source project is that if vendors customize
    OpenStack for clients and create a real differentiator or competitive advantage,
    they cannot fork OpenStack or uniquely sell this feature. Instead, they have to
    contribute the source code back to the upstream open source OpenStack project.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 供应商将支持构建、持续维护、升级或客户所需的任何定制服务，所有这些都将反馈给社区。OpenStack 作为开源项目的优势在于，如果供应商为客户定制了
    OpenStack，并创造了真正的差异化或竞争优势，他们不能将 OpenStack 分支或单独销售此功能。相反，他们必须将源代码贡献回上游开源 OpenStack
    项目。
- en: This means that all competing vendors contribute to its success of OpenStack
    and benefit from each other's innovative work. The OpenStack project is not just
    for vendors though, and everyone can contribute code and features to push the
    project forward.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着所有竞争供应商都在推动 OpenStack 的成功，并从彼此的创新工作中受益。然而，OpenStack 项目不仅仅是为了供应商，任何人都可以贡献代码和功能，推动项目向前发展。
- en: OpenStack maintains a release cycle where an upstream release is created every
    six months and is governed by the OpenStack Foundation. It is important to note
    that many public clouds, such as at&t, RackSpace, and GoDaddy, are based on OpenStack
    too, so it is not exclusive to private clouds, but it has undeniably become increasingly
    popular as a private cloud alternative to AWS public cloud and now widely used
    for **Network Function Virtualization** (**NFV**).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 维持一个发布周期，每六个月会创建一个上游发布版本，并由 OpenStack 基金会进行管理。需要注意的是，许多公共云服务，如 AT&T、RackSpace
    和 GoDaddy，也基于 OpenStack，因此它不仅限于私有云，它无可否认地已成为 AWS 公有云的私有云替代方案，并且现在广泛用于 **网络功能虚拟化**（**NFV**）。
- en: So how does AWS and OpenStack work in terms of networking? Both AWS and OpenStack
    are made up of some mandatory and optional projects that are all integrated to
    make up its reference architecture. Mandatory projects include compute and networking,
    which are the staple of any cloud solution, whereas others are optional bolt-ons
    to enhance or extend capability. This means that end users can cherry-pick the
    projects they are interested in to make up their own personal portfolio.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 AWS 和 OpenStack 在网络方面是如何工作的呢？AWS 和 OpenStack 都由一些必需的和可选的项目组成，所有这些项目集成在一起，形成其参考架构。必需的项目包括计算和网络，这是任何云解决方案的基础，而其他项目则是可选的附加组件，用于增强或扩展功能。这意味着最终用户可以根据自己的兴趣挑选项目，形成个人化的组合。
- en: The AWS approach to networking
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 的网络方法
- en: Having discussed both AWS and OpenStack, first, we will explore the AWS approach
    to networking, before looking at an alternative method using OpenStack and compare
    the two approaches. When first setting up networking in AWS, a tenant network
    in AWS is instantiated using VPC, which post 2013 deprecated AWS classic mode;
    but what is VPC?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论完 AWS 和 OpenStack 后，首先我们将探讨 AWS 的网络方法，然后再看看使用 OpenStack 的替代方法，并对这两种方法进行比较。在
    AWS 中首次设置网络时，AWS 中的租户网络是通过 VPC 实例化的，VPC 在 2013 年之后废弃了 AWS 经典模式；但 VPC 到底是什么？
- en: Amazon VPC
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊 VPC
- en: A VPC is the new default setting for new customers wishing to access AWS. VPCs
    can also be connected to customer networks (private data centers) by allowing
    AWS cloud to extend a private data center for agility. The concept of connecting
    a private data center to an AWS VPC is using something AWS refers to as a customer
    gateway and virtual private gateway. A virtual private gateway in simple terms
    is just two redundant VPN tunnels, which are instantiated from the customer's
    private network.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 是新客户访问 AWS 时的默认设置。VPC 也可以通过允许 AWS 云扩展私有数据中心来连接到客户的网络（私有数据中心），以提高灵活性。将私有数据中心连接到
    AWS VPC 的概念使用了 AWS 所称的客户网关和虚拟私有网关。简单来说，虚拟私有网关只是两个冗余的 VPN 隧道，它们从客户的私有网络实例化。
- en: Customer gateways expose a set of external static addresses from a customer
    site, which are typically **Network Address Translation-Traversal** (**NAT-T**)
    to hide the source address. UDP port `4500` should be accessible in the external
    firewall in the private data center. Multiple VPCs can be supported from one customer
    gateway device.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 客户网关暴露来自客户站点的一组外部静态地址，这些地址通常是**网络地址转换穿越**（**NAT-T**），以隐藏源地址。UDP 端口`4500`应该在私有数据中心的外部防火墙中可访问。一个客户网关设备可以支持多个
    VPC。
- en: '![Amazon VPC](img/B05559_01_05.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon VPC](img/B05559_01_05.jpg)'
- en: A VPC gives an isolated view of everything an AWS customer has provisioned in
    AWS public cloud. Different user accounts can then be set up against VPC using
    the AWS **Identity and Access Management (IAM)** service, which has customizable
    permissions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 提供了一个隔离的视图，展示了 AWS 客户在 AWS 公有云中配置的所有内容。不同的用户账户可以使用 AWS **身份与访问管理（IAM）**
    服务设置到 VPC 上，并可以设置自定义权限。
- en: 'The following example of a VPC shows instances (virtual machines) mapped with
    one or more security groups and connected to different subnets connected to the
    VPC router:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 VPC 示例，展示了与一个或多个安全组映射并连接到不同子网的实例（虚拟机），这些子网通过 VPC 路由器连接：
- en: '![Amazon VPC](img/B05559_01_06.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon VPC](img/B05559_01_06.jpg)'
- en: 'A **VPC** simplifies networking greatly by putting the constructs into software
    and allows users to perform the following network functions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**VPC**通过将构件集成到软件中，大大简化了网络操作，并允许用户执行以下网络功能：'
- en: Creating instances (virtual machines) mapped to subnets
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建映射到子网的实例（虚拟机）
- en: Creating **Domain Name System** (**DNS**) entries that are applied to instances
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建应用于实例的**域名系统**（**DNS**）条目
- en: Assigning public and private IP addresses
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配公共和私有 IP 地址
- en: Creating or associating subnets
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建或关联子网
- en: Creating custom routing
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义路由
- en: Applying security groups with associated ACL rules
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用与关联的 ACL 规则的安全组
- en: By default, when an instance (virtual machine) is instantiated in a VPC, it
    will either be placed on a default subnet or custom subnet if specified.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当一个实例（虚拟机）在 VPC 中实例化时，它将被放置在默认子网中，或者如果指定了自定义子网，则会放置在自定义子网中。
- en: All VPCs come with a default router when the VPC is created, the router can
    have additional custom routes added and routing priority can also be set to forward
    traffic to particular subnets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 VPC 在创建时都会有一个默认路由器，路由器可以添加额外的自定义路由，路由优先级也可以设置为将流量转发到特定的子网。
- en: Amazon IP addressing
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊 IP 地址分配
- en: When an instance is spun up in AWS, it will automatically be assigned a mandatory
    private IP address by **Dynamic Host Configuration Protocol** (**DHCP**) as well
    as a public IP and DNS entry too unless dictated otherwise. Private IPs are used
    in AWS to route east-west traffic between instances when virtual machine needs
    to communicate with adjacent virtual machines on the same subnet, whereas public
    IPs are available through the Internet.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个实例在 AWS 中启动时，它将自动由**动态主机配置协议**（**DHCP**）分配一个必需的私有 IP 地址，并且还会分配一个公共 IP 和 DNS
    记录，除非另有规定。私有 IP 地址用于 AWS 中的虚拟机之间的东西-东西流量路由，当虚拟机需要与同一子网上的相邻虚拟机进行通信时，而公共 IP 地址则通过互联网可用。
- en: If a persistent public IP address is required for an instance, AWS offers the
    elastic IP addresses feature, which is limited to five per VPC account, which
    any failed instances IP address can be quickly mapped to another instance. It
    is important to note that it can take up to 24 hours for a public IP address's
    DNS **Time To Live** (**TTL**) to propagate when using AWS.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要为实例分配一个持久的公共 IP 地址，AWS 提供了弹性 IP 地址功能，每个 VPC 账户最多可使用五个弹性 IP 地址，任何失败的实例的 IP
    地址可以快速映射到另一个实例。需要注意的是，使用 AWS 时，公共 IP 地址的 DNS **生存时间**（**TTL**）传播可能需要长达 24 小时。
- en: In terms of throughput, AWS instances can support a **Maximum Transmission Unit**
    (**MTU**) of 1,500 that can be passed to an instance in AWS, so this needs to
    be considered when considering application performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在吞吐量方面，AWS 实例支持 **最大传输单元** (**MTU**) 为 1,500，这可以传递给 AWS 中的实例，因此在考虑应用性能时需要考虑这一点。
- en: Amazon security groups
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊安全组
- en: Security groups in AWS are a way of grouping permissive ACL rules, so don't
    allow explicit denies. AWS security groups act as a virtual firewall for instances,
    and they can be associated with one or more instances' network interfaces. In
    a VPC, you can associate a network interface with up to five security groups,
    adding up to 50 rules to a security group, with a maximum of 500 security groups
    per VPC. A VPC in an AWS account automatically has a default security group, which
    will be automatically applied if no other security groups are specified.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 中的安全组是一种将许可访问控制列表（ACL）规则分组的方式，因此不允许显式拒绝。AWS 安全组充当实例的虚拟防火墙，可以与一个或多个实例的网络接口相关联。在
    VPC 中，您可以将网络接口与最多五个安全组相关联，并为每个安全组添加最多 50 条规则，每个 VPC 的最大安全组数量为 500 个。AWS 账户中的 VPC
    默认具有一个默认安全组，如果没有指定其他安全组，将自动应用此默认安全组。
- en: Default security groups allow all outbound traffic and all inbound traffic only
    from other instances in a VPC that also use the default security group. The default
    security group cannot be deleted. Custom security groups when first created allow
    no inbound traffic, but all outbound traffic is allowed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 默认安全组允许所有出站流量和来自同一 VPC 中其他使用默认安全组的实例的所有入站流量。默认安全组无法删除。自定义安全组在首次创建时不允许任何入站流量，但允许所有出站流量。
- en: 'Permissive ACL rules associated with security groups govern inbound traffic
    and are added using the AWS console (GUI) as shown later in the text, or they
    can be programmatically added using APIs. Inbound ACL rules associated with security
    groups can be added by specifying type, protocol, port range, and the source address.
    Refer to the following screenshot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与安全组相关联的许可访问控制列表（ACL）规则控制入站流量，并可以通过 AWS 控制台（GUI）添加，如后文所示，或者可以通过 API 进行编程添加。与安全组相关联的入站
    ACL 规则可以通过指定类型、协议、端口范围和源地址来添加。请参阅以下截图：
- en: '![Amazon security groups](img/B05559_01_07.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![亚马逊安全组](img/B05559_01_07.jpg)'
- en: Amazon regions and availability zones
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊区域和可用区
- en: A VPC has access to different regions and availability zones of shared compute,
    which dictate the data center that the AWS instances (virtual machines) will be
    deployed in. Regions in AWS are geographic areas that are completely isolated
    by design, where availability zones are isolated locations in that specific region,
    so an availability zone is a subset of a region.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 可以访问共享计算的不同区域和可用区，这决定了 AWS 实例（虚拟机）将部署在哪个数据中心。AWS 中的区域是完全隔离的地理区域，其中可用区是该特定区域中的独立位置，因此，可用区是区域的一个子集。
- en: AWS gives users the ability to place their resources in different locations
    for redundancy as sometimes the health of a specific region or availability zone
    can suffer issues. Therefore, AWS users are encouraged to use more than one availability
    zones when deploying production workloads on AWS. Users can choose to replicate
    their instances and data across regions if they choose to.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 为用户提供了将资源放置在不同位置以实现冗余的能力，因为有时特定区域或可用区的健康状况可能会出现问题。因此，AWS 鼓励用户在部署生产工作负载时使用多个可用区。用户还可以选择在多个区域之间复制他们的实例和数据。
- en: Within each isolated AWS region, there are child availability zones. Each availability
    zone is connected to sibling availability zones using low latency links. All communication
    from one region to another is across the public Internet, so using geographically
    distant regions will acquire latency and delay. Encryption of data should also
    be considered when hosting applications that send data across regions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个独立的 AWS 区域内，存在子可用区。每个可用区通过低延迟链接与同区域的其他可用区相连。不同区域之间的所有通信都是通过公共互联网进行的，因此使用地理上距离较远的区域时会产生延迟。托管跨区域发送数据的应用时，还应考虑数据加密。
- en: Amazon Elastic Load Balancing
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊弹性负载均衡
- en: AWS also allows **Elastic Load Balancing** (**ELB**) to be configured within
    a VPC as a bolt-on service. ELB can either be internal or external. When ELB is
    external, it allows the creation of an Internet-facing entry point into your VPC
    using an associated DNS entry and balances load between different instances. Security
    groups are assigned to ELBs to control the access ports that need to be used.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 还允许在 VPC 中配置 **Elastic Load Balancing** (**ELB**) 作为附加服务。ELB 可以是内部的也可以是外部的。当
    ELB 为外部时，它允许通过关联的 DNS 条目创建一个面向 Internet 的入口点，并在不同实例之间进行负载均衡。安全组会被分配给 ELB 以控制需要使用的访问端口。
- en: 'The following image shows an elastic load balancer, load balancing 3 instances:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个弹性负载均衡器，正在对 3 个实例进行负载均衡：
- en: '![Amazon Elastic Load Balancing](img/B05559_01_08.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon Elastic Load Balancing](img/B05559_01_08.jpg)'
- en: The OpenStack approach to networking
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 的网络架构
- en: Having considered AWS networking, we will now explore OpenStack's approach to
    networking and look at how its services are configured.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了 AWS 网络之后，我们将探索 OpenStack 的网络架构，并查看其服务是如何配置的。
- en: OpenStack is deployed in a data center on multiple controllers. These controllers
    contain all the OpenStack services, and they can be installed on either virtual
    machines, bare metal (physical) servers, or containers. The OpenStack controllers
    should host all the OpenStack services in a highly available and redundant fashion
    when they are deployed in production.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 部署在数据中心的多个控制节点上，这些控制节点包含所有 OpenStack 服务，可以安装在虚拟机、裸机（物理）服务器或容器上。在生产环境中部署时，OpenStack
    控制节点应以高度可用和冗余的方式托管所有 OpenStack 服务。
- en: Different OpenStack vendors provide different installers to install OpenStack.
    Some examples of installers from the most prominent OpenStack distributions are
    RedHat Director (based on OpenStack TripleO), Mirantis Fuel, HPs HPE installer
    (based on Ansible), and Juju for Canonical, which all install OpenStack controllers
    and are used to scale out compute nodes on the OpenStack cloud acting as an OpenStack
    workflow management tool.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 OpenStack 供应商提供不同的安装程序来安装 OpenStack。一些来自最著名的 OpenStack 发行版的安装程序示例包括 RedHat
    Director（基于 OpenStack TripleO）、Mirantis Fuel、HP 的 HPE 安装程序（基于 Ansible）和 Canonical
    的 Juju，它们都可以安装 OpenStack 控制节点，并用作 OpenStack 工作流管理工具，帮助扩展 OpenStack 云上的计算节点。
- en: OpenStack services
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack 服务
- en: 'A breakdown of the core OpenStack services that are installed on an OpenStack
    controller are as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 安装在 OpenStack 控制节点上的核心 OpenStack 服务的详细说明如下：
- en: '**Keystone** is the identity service for OpenStack that allows user access,
    which issues tokens, and can be integrated with LDAP or Active directory.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keystone** 是 OpenStack 的身份服务，允许用户访问，发放令牌，并可以与 LDAP 或 Active Directory 集成。'
- en: '**Heat** is the orchestration provisioning tool for OpenStack infrastructure.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Heat** 是 OpenStack 基础设施的编排配置工具。'
- en: '**Glance** is the image service for OpenStack that stores all image templates
    for virtual machines or bare metal servers.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glance** 是 OpenStack 的镜像服务，用于存储虚拟机或裸机服务器的所有镜像模板。'
- en: '**Cinder** is the block storage service for OpenStack that allows centralized
    storage volumes to be provisioned and attached to vms or bare metal servers that
    can then be mounted.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cinder** 是 OpenStack 的块存储服务，允许集中式存储卷的配置，并可以将其附加到虚拟机或裸机服务器上，之后可以进行挂载。'
- en: '**Nova** is the compute service for OpenStack used to provision vms and uses
    different scheduling algorithms to work out where to place virtual machines on
    available compute.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nova** 是 OpenStack 的计算服务，用于配置虚拟机，并使用不同的调度算法来决定将虚拟机放置在哪些可用的计算节点上。'
- en: '**Horizon** is the OpenStack dashboard that users connect to view the status
    of vms or bare metal servers that are running in a tenant network.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Horizon** 是 OpenStack 的仪表板，用户可以通过它查看运行在租户网络中的虚拟机或裸机服务器的状态。'
- en: '**Rabbitmq** is the message queue system for OpenStack.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rabbitmq** 是 OpenStack 的消息队列系统。'
- en: '**Galera** is the database used to store all OpenStack data in the Nova (compute)
    and neutron (networking) databases holding vm, port, and subnet information.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Galera** 是用于存储 OpenStack 数据的数据库，它存储所有 Nova（计算）和 Neutron（网络）数据库中的虚拟机、端口和子网信息。'
- en: '**Swift** is the object storage service for OpenStack and can be used as a
    redundant storage backend that stores replicated copies of objects on multiple
    servers. Swift is not like traditional block or file-based storage; objects can
    be any unstructured data.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift** 是 OpenStack 的对象存储服务，可以作为一个冗余的存储后端，在多个服务器上存储对象的复制副本。Swift 与传统的块存储或文件存储不同；对象可以是任何非结构化数据。'
- en: '**Ironic** is the bare metal provisioning service for OpenStack. Originally,
    a fork of part of the Nova codebase, it allows provisioning of images on to bare
    metal servers and uses IPMI and ILO or DRAC interfaces to manage physical hardware.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ironic** 是 OpenStack 的裸金属配置服务。最初是 Nova 代码库的一个分支，它允许将镜像部署到裸金属服务器，并使用 IPMI
    和 ILO 或 DRAC 接口来管理物理硬件。'
- en: '**Neutron** is the networking service for OpenStack and contains ML2 and L3
    agents and allows configuration of network subnets and routers.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron** 是 OpenStack 的网络服务，包含 ML2 和 L3 代理，并允许配置网络子网和路由器。'
- en: In terms of neutron networking services, neutron architecture is very similar
    in constructs to AWS.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 就 Neutron 网络服务而言，Neutron 架构在结构上与 AWS 非常相似。
- en: Note
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Useful links covering OpenStack services can be found at:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 OpenStack 服务的有用链接可以在以下位置找到：
- en: '[http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html](http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html](http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html)'
- en: '[https://www.youtube.com/watch?v=N90ufYN0B6U](https://www.youtube.com/watch?v=N90ufYN0B6U)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=N90ufYN0B6U](https://www.youtube.com/watch?v=N90ufYN0B6U)'
- en: OpenStack tenants
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack 租户
- en: A Project, often referred to in OpenStack as a tenant, gives an isolated view
    of everything that a team has provisioned in an OpenStack cloud. Different user
    accounts can then be set up against a Project (tenant) using the keystone identity
    service, which can be integrated with **Lightweight Directory Access Protocol**
    (**LDAP**) or Active Directory to support customizable permission models.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一个项目，在 OpenStack 中通常被称为租户，提供一个团队在 OpenStack 云中所部署的所有资源的隔离视图。然后，可以通过 Keystone
    身份服务为一个项目（租户）设置不同的用户帐户，Keystone 可以与 **轻量目录访问协议**（**LDAP**）或 Active Directory 集成，支持可定制的权限模型。
- en: OpenStack neutron
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack Neutron
- en: OpenStack neutron performs all the networking functions in OpenStack.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Neutron 执行 OpenStack 中的所有网络功能。
- en: 'The following network functions are provided by the neutron project in an OpenStack
    cloud:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下网络功能由 OpenStack 云中的 Neutron 项目提供：
- en: Creating instances (virtual machines) mapped to networks
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建与网络映射的实例（虚拟机）
- en: Assigning IP addresses using its in-built DHCP service
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内建的 DHCP 服务分配 IP 地址
- en: DNS entries are applied to instances from named servers
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS 条目从命名服务器应用到实例
- en: The assignment of private and Floating IP addresses
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配私有 IP 地址和浮动 IP 地址
- en: Creating or associating network subnets
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建或关联网络子网
- en: Creating routers
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建路由器
- en: Applying security groups
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用安全组
- en: OpenStack is set up into its **Modular Layer 2** (**ML2**) and **Layer 3** (**L3**)
    agents that are configured on the OpenStack controllers. OpenStack's ML2 plugin
    allows OpenStack to integrate with switch vendors that use either Open vSwitch
    or Linux Bridge and acts as an agnostic plugin to switch vendors, so vendors can
    create plugins, to make their switches OpenStack compatible. The ML2 agent runs
    on the hypervisor communicating over **Remote Procedure Call** (**RPC**) to the
    compute host server.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 被设置为其 **模块化第二层**（**ML2**）和 **第三层**（**L3**）代理，这些代理配置在 OpenStack 控制节点上。OpenStack
    的 ML2 插件允许 OpenStack 与使用 Open vSwitch 或 Linux Bridge 的交换机厂商集成，并充当一个与交换机厂商无关的插件，因此厂商可以创建插件，使其交换机与
    OpenStack 兼容。ML2 代理在 hypervisor 上运行，通过 **远程过程调用**（**RPC**）与计算主机服务器进行通信。
- en: OpenStack compute hosts are typically deployed using a hypervisor that uses
    Open vSwitch. Most OpenStack vendor distributions use the KVM hypervisor by default
    in their reference architectures, so this is deployed and configured on each compute
    host by the chosen OpenStack installer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 计算主机通常使用一个 hypervisor 部署，该 hypervisor 使用 Open vSwitch。大多数 OpenStack
    厂商发行版在其参考架构中默认使用 KVM hypervisor，因此这一组件由所选的 OpenStack 安装程序在每个计算主机上进行部署和配置。
- en: Compute hosts in OpenStack are connected to the access layer of the STP 3-tier
    model, or in modern networks connected to the Leaf switches, with VLANs connected
    to each individual OpenStack compute host. Tenant networks are then used to provide
    isolation between tenants and use VXLAN and GRE tunneling to connect the layer
    2 network.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 中的计算主机连接到 STP 三层模型的接入层，或者在现代网络中连接到 Leaf 交换机，VLAN 与每个单独的 OpenStack
    计算主机相连接。租户网络随后用于提供租户之间的隔离，并使用 VXLAN 和 GRE 隧道连接第二层网络。
- en: Open vSwitch runs in kernel space on the KVM hypervisor and looks after firewall
    rules by using OpenStack security groups that pushes down flow data via OVSDB
    from the switches. The neutron L3 agent allows OpenStack to route between tenant
    networks and uses neutron routers, which are deployed within the tenant network
    to accomplish this, without a neutron router networks are isolated from each other
    and everything else.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch 在 KVM hypervisor 上的内核空间运行，并通过使用从交换机推送流数据的 OpenStack 安全组管理防火墙规则。
    neutron L3 代理允许 OpenStack 在租户网络之间进行路由，并使用在租户网络内部部署的 neutron 路由器来完成此操作，没有 neutron
    路由器，网络将彼此隔离以及其他所有内容。
- en: Provisioning OpenStack networks
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 OpenStack 网络
- en: 'When setting up simple networking using neutron in a Project (tenant) network,
    two different networks, an internal network, and an external network will be configured.
    The internal network will be used for east-west traffic between instances. This
    is created as shown in the following horizon dashboard with an appropriate **Network
    Name**:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Project（租户）网络中使用 neutron 设置简单网络时，将配置两个不同的网络，即内部网络和外部网络。内部网络用于实例之间的东西流量。如下所示在适当的**网络名称**下使用地平线仪表板创建：
- en: '![Provisioning OpenStack networks](img/B05559_01_09.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_09.jpg)'
- en: 'The **Subnet Name** and subnet range are then specified in the **Subnet** section,
    as shown in the following screenshot:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**子网名称**和子网范围随后在**子网**部分指定，如下截图所示：'
- en: '![Provisioning OpenStack networks](img/B05559_01_10.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_10.jpg)'
- en: 'Finally, DHCP is enabled on the network, and any named **Allocation Pools**
    (specifies only a range of addresses that can be used in a subnet) are optionally
    configured alongside any named **DNS Name Servers**, as shown below:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在网络上启用 DHCP，并选择性配置任何名为**分配池**（指定子网中可用地址范围的名称）以及任何名为**DNS 名称服务器**，如下所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_11.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_11.jpg)'
- en: 'An external network will also need to be created to make the internal network
    accessible from outside of OpenStack, when external networks are created by an
    administrative user, the set **External Network** checkbox needs to be selected,
    as shown in the next screenshot:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要创建外部网络，以便从 OpenStack 外部访问内部网络，当由管理员用户创建外部网络时，需要选择设置**外部网络**复选框，如下一截图所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_12.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_12.jpg)'
- en: 'A router is then created in OpenStack to route packets to the network, as shown
    below:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在 OpenStack 中创建路由器以路由数据包到网络，如下所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_13.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_13.jpg)'
- en: 'The created router will then need to be associated with the networks; this
    is achieved by adding an interface on the router for the private network, as illustrated
    in the following screenshot:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的路由器随后需要与网络关联；通过为私有网络上的路由器添加接口来实现，如下截图所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_14.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_14.jpg)'
- en: 'The **External Network** that was created then needs to be set as the router''s
    gateway, as per the following screenshot:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的**外部网络**随后需要设置为路由器的网关，如下截图所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_15.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_15.jpg)'
- en: 'This then completes the network setup; the final configuration for the internal
    and external network is displayed below, which shows one router connected to an
    internal and external network:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就完成了网络设置；内部和外部网络的最终配置如下所示，显示了一个连接到内部和外部网络的路由器：
- en: '![Provisioning OpenStack networks](img/B05559_01_16.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_16.jpg)'
- en: In OpenStack, instances are provisioned onto the internal private network by
    selecting the private network NIC when deploying instances. OpenStack has the
    convention of assigning pools of public IPs (floating IP) addresses from an external
    network for instances that need to be externally routable outside of OpenStack.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 中，通过选择部署实例时选择私有网络 NIC 将实例配置到内部私有网络。 OpenStack 有一个公共 IP 地址池（浮动 IP）的分配约定，用于需要在
    OpenStack 外部可路由的实例。
- en: 'To set up a set of floating IP addresses, an OpenStack administrator will set
    up an allocation pool using the external network from an external network, as
    shown in the following screenshot:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为设置一组浮动 IP 地址，OpenStack 管理员将使用来自外部网络的分配池来设置分配池，如下截图所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_17.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_17.jpg)'
- en: OpenStack like AWS, uses security groups to set up firewall rules between instances.
    Unlike AWS, OpenStack supports both ingress and egress ACL rules, whereas AWS
    allows all outbound communication, OpenStack can deal with both ingress and egress
    rules. Bespoke security groups are created to group ACL rules as shown below
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS 类似，OpenStack 使用安全组来设置实例之间的防火墙规则。与 AWS 不同，OpenStack 支持传入和传出的 ACL 规则，而 AWS
    允许所有的出站通信，OpenStack 可以同时处理传入和传出的规则。根据需要创建定制的安全组，以便对 ACL 规则进行分组，如下所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_18.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_18.jpg)'
- en: 'Ingress and Rules can then be created against a security group. **SSH** access
    is configured as an ACL rule against the parent security group, which is pushed
    down to Open VSwitch into kernel space on each hypervisor, as seen in the next
    screenshot:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以针对安全组创建传入和规则。**SSH** 访问被配置为针对父安全组的 ACL 规则，该规则被推送到 Open VSwitch 内核空间中的每个虚拟化主机，如下图所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_19.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_19.jpg)'
- en: Once the Project (tenant) has two networks, one internal and one external, and
    an appropriate security group has been configured, instances are ready to be launched
    on the private network.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦项目（租户）拥有两个网络，一个内部网络和一个外部网络，并且配置了适当的安全组，实例就可以在私有网络上启动。
- en: 'An instance is launched by selecting **Launch Instance** in horizon and setting
    the following parameters:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 Horizon 中选择**启动实例**并设置以下参数来启动实例：
- en: '**Availability Zone**'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用区**'
- en: '**Instance Name**'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例名称**'
- en: '**Flavor** (CPU, RAM, and disk space)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规格**（CPU、RAM 和磁盘空间）'
- en: '**Image Name** (base operating system)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**镜像名称**（基础操作系统）'
- en: '![Provisioning OpenStack networks](img/B05559_01_20.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_20.jpg)'
- en: 'The private network is then selected as the **NIC** for the instance under
    the **Networking** tab:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后选择私有网络作为实例的**NIC**，并在**网络**选项卡下进行配置：
- en: '![Provisioning OpenStack networks](img/B05559_01_21.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_21.jpg)'
- en: This will mean that when the instance is launched, it will use OpenStack's internal
    DHCP service to pick an available IP address from the allocated subnet range.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当实例启动时，它将使用 OpenStack 的内部 DHCP 服务从分配的子网范围中选择一个可用的 IP 地址。
- en: 'A security group should also be selected to govern the ACL rules for the instance;
    in this instance, the `testsg1` security group is selected as shown in the following
    screenshot:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该选择一个安全组来管理实例的 ACL 规则；在这个例子中，选择了 `testsg1` 安全组，如下图所示：
- en: '![Provisioning OpenStack networks](img/B05559_01_22.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_22.jpg)'
- en: 'Once the instance has been provisioned, a floating IP address can be associated
    from the external network:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例已配置完成，可以从外部网络关联一个浮动 IP 地址：
- en: '![Provisioning OpenStack networks](img/B05559_01_23.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_23.jpg)'
- en: 'A floating IP address from the external network floating IP address pool is
    then selected and associated with the instance:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从外部网络浮动 IP 地址池中选择一个浮动 IP 地址，并将其与实例关联：
- en: '![Provisioning OpenStack networks](img/B05559_01_24.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![配置 OpenStack 网络](img/B05559_01_24.jpg)'
- en: The floating IP addresses NATs OpenStack instances that are deployed on the
    internal public IP address to the external network's floating IP address, which
    will allow the instance to be accessible from outside of OpenStack.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 浮动 IP 地址将 OpenStack 部署在内部公网 IP 地址上的实例通过 NAT 映射到外部网络的浮动 IP 地址，从而允许实例从 OpenStack
    外部访问。
- en: OpenStack regions and availability zones
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack 区域和可用区
- en: OpenStack like AWS, as seen on instance creation, also utilizes regions and
    availability zones. Compute hosts in OpenStack (hypervisors) can be assigned to
    different availability zones.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS 一样，OpenStack 在实例创建时也使用区域和可用区。OpenStack 中的计算主机（虚拟化主机）可以被分配到不同的可用区。
- en: An availability zone in OpenStack is just a virtual separation of compute resources.
    In OpenStack, an availability zone can be further segmented into host aggregates.
    It is important to note that a compute host can be assigned to only one availability
    zone, but can be a part of multiple host aggregates in that same availability
    zone.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 中，可用区只是计算资源的虚拟划分。可用区可以进一步细分为主机聚合。需要注意的是，一个计算主机只能分配给一个可用区，但可以是同一可用区中多个主机聚合的一部分。
- en: Nova uses a concept named **nova scheduler rules**, which dictates the placement
    of instances on compute hosts at provisioning time. A simple example of a nova
    scheduler rule is the `AvailabiltyZoneFilter` filter, which means that if a user
    selects an availability zone at provisioning time, then the instance will land
    only on any of the compute instances grouped under that availability zone.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 使用一个名为**Nova 调度器规则**的概念，规定了实例在资源配置时如何在计算主机上进行部署。Nova 调度器规则的一个简单例子是 `AvailabilityZoneFilter`
    过滤器，意味着如果用户在配置时选择了一个可用性区，那么实例将只会被部署到该可用性区下的任意计算实例上。
- en: Another example of the `AggregateInstanceExtraSpecsFilter` filter that means
    that if a custom flavor (CPU, RAM, and disk) is tagged with a key value pair and
    a host aggregate is tagged with the same key value pair, then if a user deploys
    with that flavor the `AggregateInstanceExtraSpecsFilter` filter will place all
    instances on compute hosts under that host aggregate.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是 `AggregateInstanceExtraSpecsFilter` 过滤器，意味着如果一个自定义的 flavor（CPU、RAM 和磁盘）被标记为某个键值对，且一个主机聚合被标记为相同的键值对，那么如果用户使用该
    flavor 部署，`AggregateInstanceExtraSpecsFilter` 过滤器将会把所有实例放置在该主机聚合下的计算主机上。
- en: These host aggregates can be assigned to specific teams, which means that teams
    can be selective about which applications they share their compute with and can
    be used to prevent noisy neighbor syndrome. There is a wide array of filters that
    can be applied in OpenStack in all sorts of orders to dictate instance scheduling.
    OpenStack allows cloud operators to create a traditional cloud model with large
    groups of contended compute to more bespoke use cases where the isolation of compute
    resources is required for particular application workloads.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主机聚合可以分配给特定团队，这意味着团队可以选择性地决定与哪些应用共享其计算资源，同时也可以用来防止“噪声邻居”问题。OpenStack 提供了多种过滤器，允许以不同的顺序应用来决定实例调度。OpenStack
    使得云操作员可以创建一个传统的云模型（有大量竞争资源的计算），也可以创建更为定制化的使用场景，在这些场景中需要为特定应用工作负载隔离计算资源。
- en: 'The following example shows host aggregates with groups and shows a host aggregate
    named **1-Host-Aggregate**, grouped under an **Availability Zone** named **DC1**
    containing two compute hosts (hypervisors), which could be allocated to a particular
    team:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了带有组的主机聚合，并展示了一个名为**1-Host-Aggregate**的主机聚合，归类在名为**DC1**的**可用性区**下，包含两台计算主机（hypervisors），它们可以分配给特定团队：
- en: '![OpenStack regions and availability zones](img/B05559_01_25.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![OpenStack 区域和可用性区](img/B05559_01_25.jpg)'
- en: OpenStack instance provisioning workflow
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack 实例资源配置工作流。
- en: 'When an instance (virtual machine) is provisioned in OpenStack, the following
    high-level steps are carried out:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 OpenStack 中配置实例（虚拟机）时，将执行以下高级步骤：
- en: The Nova compute service will issue a request for a new instance (virtual machine)
    using the image selected from the glance images service
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 计算服务将使用从 glance 镜像服务中选择的镜像发出请求以创建新的实例（虚拟机）。
- en: The nova request may then be queued by **RabbitMQ** before being processed (RabbitMQ
    allows OpenStack to deal with multiple simultaneous provisioning requests)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 请求可能会先通过**RabbitMQ**排队，然后才会被处理（RabbitMQ 使得 OpenStack 能够处理多个同时的资源配置请求）。
- en: Once the request for a new instance is processed, the request will write a new
    row into the nova Galera database in the nova database
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦新实例的请求被处理，该请求将在 Nova Galera 数据库中写入一个新行。
- en: Nova will look at the nova scheduler rules defined on the OpenStack controllers
    and will use those rules to place the instance on an available compute node (KVM
    hypervisor)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 将查看在 OpenStack 控制节点上定义的 Nova 调度器规则，并使用这些规则将实例部署到一个可用的计算节点（KVM hypervisor）。
- en: If an available hypervisor is found that meets the nova scheduler rules, then
    the provisioning process will begin
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果找到一个符合 Nova 调度器规则的可用 hypervisor，则资源配置过程将开始。
- en: Nova will check whether the image already exists on the matched hypervisor.
    If it doesn't, the image will be transferred from the hypervisor and booted from
    local disk
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 将检查所选 hypervisor 上是否已经存在镜像。如果不存在，镜像将从 hypervisor 转移并从本地磁盘启动。
- en: Nova will issue a neutron request, which will create a new VPort in OpenStack
    and map it to the neutron network
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 将发出一个 Neutron 请求，这将创建一个新的 VPort 并将其映射到 Neutron 网络。
- en: The VPort information will then be written to both the nova and neutron databases
    in Galera to correlate the instance with the network
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，VPort 信息将被写入 Nova 和 Neutron 数据库中的 Galera，以将实例与网络关联。
- en: Neutron will issue a DHCP request to assign the instance a private IP address
    from an unallocated IP address from the subnet it has been associated with
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neutron将发出DHCP请求，从它所关联的子网中分配一个私有IP地址给实例。
- en: A private IP address will then be assigned, and the instance will start to start
    up on the private network
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随后，将分配一个私有IP地址，并且实例将开始在私有网络上启动。
- en: The neutron metadata service will then be contacted to retrieve cloud-init information
    on boot, which will assign a DNS entry to the instance from the named server,
    if specified
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随后，将联系neutron元数据服务以检索云初始化信息，并在启动时从指定的命名服务器为实例分配DNS条目（如果已指定）。
- en: Once cloud-init has run, the instance will be ready to use
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦cloud-init运行完毕，实例将准备就绪。
- en: Floating IPs can then be assigned to the instance to NAT to external networks
    to make the instances publicly accessible
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，可以为实例分配浮动IP，以通过网络地址转换（NAT）访问外部网络，使实例能够公开访问。
- en: OpenStack LBaaS
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack LBaaS
- en: Like AWS OpenStack also offers a **Load-Balancer-as-a-Service** (**LBaaS**)
    option that allows incoming requests to be distributed evenly among designated
    instances using a **Virtual IP** (**VIP**). The features and functionality supported
    by LBaaS are dependent on the vendor plugin that is used.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 像AWS一样，OpenStack也提供**负载均衡即服务**（**LBaaS**）选项，允许将传入请求均匀地分配到指定的实例，使用**虚拟IP**（**VIP**）。LBaaS所支持的功能和特性取决于所使用的厂商插件。
- en: 'Popular LBaaS plugins in OpenStack are:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack中流行的LBaaS插件包括：
- en: Citrix NetScaler
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Citrix NetScaler
- en: F5
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F5
- en: HaProxy
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HaProxy
- en: Avi networks
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Avi networks
- en: These load balancers all expose varying degrees of features to the OpenStack
    LBaaS agent. The main driver for utilizing LBaaS on OpenStack is that it allows
    users to use LBaaS as a broker to the load balancing solution, allowing users
    to use the OpenStack API or configure the load balancer via the horizon GUI.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些负载均衡器向OpenStack LBaaS代理暴露不同程度的特性。利用OpenStack的LBaaS的主要驱动因素是，它允许用户将LBaaS作为负载均衡解决方案的中介，用户可以通过OpenStack
    API或通过Horizon GUI配置负载均衡器。
- en: LBaaS allows load balancing to be set up within a tenant network in OpenStack.
    Using LBaaS means that if for any reason a user wishes to use a new load balancer
    vendor as opposed to their incumbent one; as long as they are using OpenStack
    LBaaS, it is made much easier. As all calls or administration are being done via
    the LBaaS APIs or Horizon, no changes would be required to the orchestration scripting
    required to provision and administrate the load balancer, and they wouldn't be
    tied into each vendor's custom APIs and the load balancing solution becomes a
    commodity.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: LBaaS允许在OpenStack的租户网络中设置负载均衡。使用LBaaS意味着，如果用户出于某种原因希望使用新的负载均衡厂商而不是现有厂商，只要他们使用OpenStack
    LBaaS，这一过程就变得更加简单。由于所有调用或管理都是通过LBaaS的API或Horizon进行的，因此不需要修改用于配置和管理负载均衡器的编排脚本，也不会绑定到每个厂商的自定义API中，负载均衡解决方案就成为了一种商品化的服务。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered some of the basic networking principles that
    are used in today's modern data centers, with special focus on the AWS and OpenStack
    cloud technologies which are two of the most popular solutions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些现代数据中心中使用的基本网络原理，特别关注了AWS和OpenStack云技术，这两者是当前最流行的解决方案之一。
- en: Having read this chapter, you should now be familiar with the difference between
    Leaf-Spine and Spanning Tree network architectures, it should have demystified
    AWS networking, and you should now have a basic understanding of how private and
    public networks can be configured in OpenStack.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你应该已经了解了Leaf-Spine与Spanning Tree网络架构的区别，理解了AWS网络架构，并对如何在OpenStack中配置私有网络和公共网络有了基本的了解。
- en: In the forthcoming chapters, we will build on these basic networking constructs
    and look at how they can be programmatically controlled using configuration management
    tools and used to automate network functions. But first, we will focus on some
    of the software-defined networking controllers that can be used to extend the
    capability of OpenStack even further than neutron in the private clouds and some
    of the feature sets and benefits they bring to ease the pain of managing network
    operations.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将基于这些基础网络结构，探讨如何使用配置管理工具以编程方式控制它们，并利用这些工具自动化网络功能。但首先，我们将关注一些软件定义网络控制器，这些控制器可用于进一步扩展OpenStack在私有云中相较于neutron的功能，以及它们带来的一些特性和优势，帮助简化网络运维管理。
- en: Note
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Useful links for Amazon content are:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊内容的有用链接包括：
- en: '[https://aws.amazon.com/](https://aws.amazon.com/)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/](https://aws.amazon.com/)'
- en: '[https://www.youtube.com/watch?v=VgzzHCukwpc](https://www.youtube.com/watch?v=VgzzHCukwpc)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=VgzzHCukwpc](https://www.youtube.com/watch?v=VgzzHCukwpc)'
- en: '[https://www.youtube.com/watch?v=jLVPqoV4YjU](https://www.youtube.com/watch?v=jLVPqoV4YjU)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=jLVPqoV4YjU](https://www.youtube.com/watch?v=jLVPqoV4YjU)'
- en: 'Useful links for OpenStack content are:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 内容的有用链接包括：
- en: '[https://wiki.openstack.org/wiki/Main_Page](https://wiki.openstack.org/wiki/Main_Page)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://wiki.openstack.org/wiki/Main_Page](https://wiki.openstack.org/wiki/Main_Page)'
- en: '[https://www.youtube.com/watch?v=Qz5gyDenqTI](https://www.youtube.com/watch?v=Qz5gyDenqTI)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=Qz5gyDenqTI](https://www.youtube.com/watch?v=Qz5gyDenqTI)'
- en: '[https://www.youtube.com/watch?v=Li0Ed1VEziQ](https://www.youtube.com/watch?v=Li0Ed1VEziQ)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=Li0Ed1VEziQ](https://www.youtube.com/watch?v=Li0Ed1VEziQ)'
