- en: Chapter 1. The Impact of Cloud on Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 云计算对网络的影响
- en: This chapter will look at ways that networking has changed in the private data
    centers and evolved in the past few years. It will focus on the emergence of **Amazon
    Web Services** (**AWS**) for public cloud and **OpenStack** for private cloud
    and ways in which this has changed the way developers want to consume networking.
    It will look at some of the networking services that AWS and OpenStack provide
    out of the box and look at some of the features they provide. It will show examples
    of how these cloud platforms have made networking a commodity much like infrastructure.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨过去几年内私有数据中心中网络的变化以及如何演变。重点将放在公有云中的**亚马逊网络服务**（**AWS**）和私有云中的**OpenStack**的出现，以及它们如何改变开发人员使用网络的方式。它还将探讨AWS和OpenStack提供的现成网络服务，并展示它们提供的一些功能。最后，它将展示这些云平台如何将网络变成了类似基础设施的商品。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: An overview of cloud approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云计算方法概述
- en: The difference between Spanning Tree networks and Leaf-Spine networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨越树（Spanning Tree）网络与叶脊（Leaf-Spine）网络的区别
- en: Changes that have occurred in networking with the introduction of public cloud
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公有云引入后，网络发生的变化
- en: The Amazon Web Services approach to networking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊网络服务（AWS）网络方法
- en: The OpenStack approach to networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack网络方法
- en: An overview of cloud approaches
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云计算方法概述
- en: The cloud provider market is currently saturated with a multitude of different
    private, public, and hybrid cloud solutions, so choice is not a problem for companies
    looking to implement public, private, or hybrid cloud solutions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商市场目前已经饱和，存在多种不同的私有云、公有云和混合云解决方案，因此对于希望实施公有云、私有云或混合云解决方案的公司来说，选择不再是问题。
- en: Consequently, choosing a cloud solution can sometimes be quite a daunting task,
    given the array of different options that are available.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择云解决方案有时会是一个令人生畏的任务，因为市场上有众多不同的选择。
- en: The battle between public and private cloud is still in its infancy, with only
    around 25 percent of the industry using public cloud, despite its perceived popularity,
    with solutions such as Amazon Web Services, Microsoft Azure, and Google Cloud
    taking a large majority of that market share. However, this still means that 75
    percent of the cloud market share is available to be captured, so the cloud computing
    market will likely go through many iterations in the coming years.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云与私有云之间的竞争仍处于起步阶段，尽管公有云被认为非常流行，但只有大约25%的行业在使用公有云，解决方案如亚马逊网络服务、微软Azure和谷歌云占据了市场的大部分份额。然而，这仍意味着云市场有75%的市场份额尚未被占领，因此云计算市场在未来几年可能会经历多次迭代。
- en: So why are many companies considering public cloud in the first place and why
    does it differ from private and hybrid clouds?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么许多公司最初考虑使用公有云，公有云与私有云和混合云又有什么不同呢？
- en: Public clouds
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公有云
- en: Public clouds are essentially a set of data centers and infrastructure that
    are made publicly available over the Internet to consumers. Despite its name,
    it is not magical or fluffy in any way. Amazon Web Services launched their public
    cloud based on the idea that they could rent out their servers to other companies
    when they were not using them during busy periods of the year.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云本质上是一组数据中心和基础设施，通过互联网公开提供给消费者。尽管名字中带有“云”，但它并非魔法或虚无缥缈的东西。亚马逊网络服务（Amazon Web
    Services）推出公有云的初衷是，在一年中忙碌的时期，他们可以将空闲的服务器租借给其他公司使用。
- en: Public cloud resources can be accessed via a **Graphical User Interface** (**GUI**)
    or, programmatically, via a set of API endpoints. This allows end users of the
    public cloud to create infrastructure and networking to host their applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云资源可以通过**图形用户界面**（**GUI**）或通过一组API端点以编程方式访问。这使得公有云的最终用户能够创建基础设施和网络，以托管他们的应用程序。
- en: Public clouds are used by businesses for various reasons, such as the speed
    it takes to configure and using public cloud resources is relatively low. Once
    credit card details have been provided on a public cloud portal, end users have
    the freedom to create their own infrastructure and networking, which they can
    run their applications on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 企业使用公有云有多种原因，例如配置速度快，并且使用公有云资源的成本相对较低。一旦提供了信用卡信息，最终用户就可以自由创建自己的基础设施和网络，并在其上运行应用程序。
- en: This infrastructure can be elastically scaled up and down as required, all at
    a cost of course to the credit card.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基础设施可以根据需要弹性地扩展和缩减，当然，所有这些都需要一定的费用，通过信用卡支付。
- en: Public cloud has become very popular as it removes a set of historical impediments
    associated with **shadow IT**. Developers are no longer hampered by the restrictions
    enforced upon them by bureaucratic and slow internal IT processes. Therefore,
    many businesses are seeing public cloud as a way to skip over these impediments
    and work in a more agile fashion allowing them to deliver new products to market
    at a greater frequency.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云已经变得非常流行，因为它解决了一些与**影子IT**相关的历史性障碍。开发者不再受到官僚化和缓慢的内部IT流程强加的限制。因此，许多企业将公有云视为跳过这些障碍、以更灵活的方式工作的途径，使他们能够更频繁地将新产品推向市场。
- en: When a business moves its operations to a public cloud, they are taking the
    bold step to stop hosting their own data centers and instead use a publicly available
    public cloud provider, such as Amazon Web Services, Microsoft Azure, IBM BlueMix,
    Rackspace, or Google Cloud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当企业将其运营迁移到公有云时，他们是在迈出大胆的一步，停止托管自己的数据中心，而是使用一个公开的公有云服务提供商，如Amazon Web Services、Microsoft
    Azure、IBM BlueMix、Rackspace或Google Cloud。
- en: The reliance is then put upon the public cloud for uptime and **Service Level
    Agreements** (**SLA**), which can be a huge cultural shift for an established
    business.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，依赖于公有云的正常运行时间和**服务等级协议**（**SLA**），这对一个成熟的企业来说可能是一个巨大的文化转变。
- en: Businesses that have moved to public cloud may find they no longer have a need
    for a large internal infrastructure team or network team, instead all infrastructure
    and networking is provided by the third-party public cloud, so it can in some
    quarters be viewed as giving up on internal IT.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 已经迁移到公有云的企业可能会发现，他们不再需要庞大的内部基础设施团队或网络团队，所有基础设施和网络都由第三方公有云提供，因此在某些情况下，这可能被视为放弃内部IT。
- en: Public cloud has proved a very successful model for many start-ups, given the
    agility it provides, where start-ups can put out products quickly using software-defined
    constructs without having to set up their own data center and remain product focused.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 公有云证明了对于许多初创公司来说是一个非常成功的模式，因为它提供的灵活性，初创公司可以迅速推出产品，利用软件定义的构件，而无需建立自己的数据中心，保持产品专注。
- en: However, the **Total Cost of Ownership** (**TCO**) to run all of a business's
    infrastructure in a public cloud is a hotly debated topic, which can be an expensive
    model if it isn't managed and maintained correctly. The debate over public versus
    private cloud TCO rages on as some argue that public cloud is a great short-term
    fix but growing costs over a long period of time mean that it may not be a viable
    long-term solution compared with private cloud.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，运行所有企业基础设施在公有云中的**拥有总成本**（**TCO**）是一个备受争议的话题，如果没有正确的管理和维护，这可能是一个昂贵的模式。关于公有云与私有云TCO的辩论仍在继续，一些人认为公有云是一个很好的短期解决方案，但随着时间的推移，成本的增长意味着它可能不是与私有云相比的可行长期解决方案。
- en: Private cloud
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 私有云
- en: Private cloud is really just an extension of the initial benefits introduced
    by virtualization solutions, such as VMware, Hyper-V, and Citrix Xen, which were
    the cornerstone of the virtualization market. The private cloud world has moved
    on from just providing virtual machines, to providing software-defined networking
    and storage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 私有云实际上只是虚拟化解决方案（如VMware、Hyper-V和Citrix Xen）最初带来的好处的延伸，这些虚拟化解决方案曾是虚拟化市场的基石。私有云的世界已经不再仅仅提供虚拟机，而是提供软件定义的网络和存储。
- en: With the launch of public clouds, such as Amazon Web Services, private cloud
    solutions have sought to provide like-for-like capability by putting a software-defined
    layer on top of their current infrastructure. This infrastructure can be controlled
    in the same way as the public cloud via a GUI or programmatically using APIs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公有云的推出，如Amazon Web Services，私有云解决方案已试图通过在当前基础设施上加一个软件定义层来提供类似的功能。这个基础设施可以像公有云一样通过图形界面（GUI）或通过API编程方式进行控制。
- en: Private cloud solutions such as Apache CloudStack and open source solutions
    such as OpenStack have been created to bridge the gap between the private cloud
    and the public cloud.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 私有云解决方案，如Apache CloudStack和开源解决方案如OpenStack，已被创建来弥合私有云和公有云之间的差距。
- en: This has allowed vendors the agility of private cloud operations in their own
    data center by overlaying software-defined constructs on top of their existing
    hardware and networks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得厂商可以通过在现有硬件和网络之上叠加软件定义的构件，获得在自有数据中心内运行私有云操作的灵活性。
- en: However, the major benefit of private cloud is that this can be done within
    the security of a company's own data centers. Not all businesses can use public
    cloud for compliance, regularity, or performance reasons, so private cloud is
    still required for some businesses for particular workloads.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，私有云的主要好处在于这可以在公司自有数据中心的安全性范围内完成。并非所有企业都能出于合规性、常规性或性能原因使用公共云，因此一些企业在特定工作负载上仍然需要私有云。
- en: Hybrid cloud
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合云
- en: Hybrid cloud can often be seen as an amalgamation of multiple clouds. This allows
    a business to seamlessly run workloads across multiple clouds linked together
    by a network fabric. The business could select the placement of workloads based
    on cost or performance metrics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云通常可以被视为多个云的结合体。这使得企业能够通过网络结构无缝地在多个云之间运行工作负载。企业可以根据成本或性能指标来选择工作负载的部署位置。
- en: A hybrid cloud can often be made up of private and public clouds. So, as an
    example, a business may have a set of web applications that it wishes to scale
    up for particular busy periods and are better suited to run on public cloud so
    they are placed there. However, the business also needs a highly regulated, PCI-compliant
    database, which would be better-suited to being deployed in a private on-premises
    cloud. So a true hybrid cloud gives a business these kinds of options and flexibility.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云通常可以由私有云和公共云组成。因此，举个例子，一个企业可能有一套需要在特定繁忙时期进行扩展的Web应用，这些应用更适合运行在公共云中，因此它们被部署在那里。然而，该企业还需要一个高度受监管、符合PCI标准的数据库，后者更适合部署在私有的本地云中。因此，真正的混合云为企业提供了这类选择和灵活性。
- en: Hybrid cloud really works on the premise of using different clouds for different
    use cases, where each horse (application workload) needs to run a particular course
    (cloud). So, sometimes, a vendor-provided **Platform as a Service** (**PaaS**)
    layer can be used to place workloads across multiple clouds or alternately different
    configuration management tools, or container orchestration technologies can be
    used to orchestrate application workload placement across clouds.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 混合云的真正运作前提是根据不同的用例使用不同的云，其中每匹马（应用工作负载）需要跑特定的赛道（云）。因此，有时可以使用厂商提供的**平台即服务**（**PaaS**）层来跨多个云部署工作负载，或者使用不同的配置管理工具或容器编排技术来协调跨云的应用工作负载部署。
- en: Software-defined
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件定义
- en: The choice between public, private, or hybrid cloud really depends on the business,
    so there is no real right or wrong answer. Companies will likely use hybrid cloud
    models as their culture and processes evolve over the next few years.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 公共云、私有云或混合云的选择实际上取决于企业，因此没有绝对的对错答案。随着公司文化和流程在未来几年内的发展，它们可能会使用混合云模型。
- en: If a business is using a public, private, or hybrid cloud, the common theme
    with all implementations is that they are moving towards a software-defined operational
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个企业正在使用公共云、私有云或混合云，那么所有实现方式的共同主题就是它们都朝着软件定义的运营模式发展。
- en: So what does the term *software-defined* really mean? In simple terms, *software-defined*
    means running a software abstraction layer over hardware. This software abstraction
    layer allows graphical or programmatic control of the hardware. So, constructs,
    such as infrastructure, storage, and networking, can be software defined to help
    simplify operations, manageability as infrastructure and networks scale out.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，*软件定义*到底是什么意思呢？简而言之，*软件定义*意味着在硬件上运行一个软件抽象层。这个软件抽象层允许对硬件进行图形化或编程化控制。因此，像基础设施、存储和网络这样的构件可以被软件定义，从而帮助简化操作、提高可管理性，尤其是在基础设施和网络扩展时。
- en: When running private clouds, modifications need to be made to incumbent data
    centers to make them private cloud ready; sometimes, this is important, so the
    private data center needs to evolve to meet those needs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行私有云时，需要对现有的数据中心进行修改，以使其准备好成为私有云；有时，这一点非常重要，因此私有数据中心需要进化以满足这些需求。
- en: The difference between Spanning Tree and Leaf-Spine networking
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨树和叶脊网络的区别
- en: When considering the private cloud, traditionally, company's private datacenters
    have implemented 3-tier layer 2 networks based on the **Spanning Tree Protocol**
    (**STP**), which doesn't lend itself well to modern software-defined networks.
    So, we will look at what a STP is in more depth as well as modern Leaf-Spine network
    architectures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑私有云时，传统上，公司的私有数据中心已实现基于**生成树协议**（**STP**）的3层二层网络，这对于现代的软件定义网络并不十分适用。因此，我们将深入了解什么是STP以及现代的叶脊网络架构。
- en: Spanning Tree Protocol
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成树协议**'
- en: The implementation of STP provides a number of options for network architects
    in terms of implementation, but it also adds a layer of complexity to the network.
    Implementation of the STP gives network architects the certainty that it will
    prevent layer 2 loops from occurring in the network.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: STP的实现为网络架构师提供了多种实现选项，但也为网络增加了一层复杂性。实施STP可以让网络架构师确信它能够防止网络中发生二层环路。
- en: 'A typical representation of a 3-tier layer 2 STP-based network can be shown
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的3层基于STP的二层网络表示如下：
- en: '![Spanning Tree Protocol](img/B05559_01_01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![生成树协议](img/B05559_01_01.jpg)'
- en: The **Core** layer provides routing services to other parts of the data center
    and contains the core switches
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心**层为数据中心的其他部分提供路由服务，并包含核心交换机'
- en: The **Aggregation** layer provides connectivity to adjacent **Access** layer
    switches and the top of the Spanning Tree core
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合层**提供与相邻**接入**层交换机的连接，并位于生成树核心的顶部'
- en: The bottom of the tree is the **Access** layer; this is where bare metal (physical)
    or virtual machines connect to the network and are segmented using different VLANs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 树的底部是**接入**层；这是裸金属（物理）或虚拟机连接到网络并使用不同VLAN进行分段的地方。
- en: The use of layer 2 networking and STP mean that at the access layer of the network
    will use VLANs spread throughout the network. The VLANs sit at the access layer,
    which is where virtual machines or bare metal servers are connected. Typically,
    these VLANs are grouped by type of application, and firewalls are used to further
    isolate and secure them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二层网络和STP意味着网络的接入层将使用遍布整个网络的VLAN。这些VLAN位于接入层，虚拟机或裸金属服务器连接在此。通常，这些VLAN按应用类型分组，防火墙用于进一步隔离和保护它们。
- en: 'Traditional networks are normally segregated into some combination of the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 传统网络通常被划分为以下几种组合：
- en: '**Frontend**: It typically has web servers that require external access'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前端**：通常包含需要外部访问的Web服务器'
- en: '**Business Logic**: This often contains stateful services'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务逻辑**：通常包含有状态服务'
- en: '**Backend**: This typically contains database servers'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端**：通常包含数据库服务器'
- en: Applications communicate with each other by tunneling between these firewalls,
    with specific **Access Control List** (**ACL**) rules that are serviced by network
    teams and governed by security teams.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序通过这些防火墙之间的隧道进行通信，使用特定的**访问控制列表**（**ACL**）规则，这些规则由网络团队提供服务，并由安全团队进行管理。
- en: When using STP in a layer 2 network, all switches go through an election process
    to determine the root switch, which is granted to the switch with the lowest bridge
    id, with a bridge id encompassing the bridge priority and MAC address of the switch.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在二层网络中使用STP时，所有交换机都通过选举过程来确定根交换机，根交换机会授予具有最低桥接ID的交换机，桥接ID包括交换机的桥接优先级和MAC地址。
- en: 'Once elected, the root switch becomes the base of the spanning tree; all other
    switches in the Spanning Tree are deemed non-root will calculate their shortest
    path to the root and then block any redundant links, so there is one clear path.
    The calculation process to work out the shortest path is referred to as network
    convergence. (For more information refer to the following link: [http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/](http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选举完成，根交换机成为生成树的基础；生成树中的所有其他交换机被视为非根交换机，它们将计算到根交换机的最短路径，然后阻止任何冗余链路，从而形成一条清晰的路径。计算最短路径的过程称为网络收敛。（更多信息请参阅以下链接：[http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/](http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/))
- en: Network architects designing the layer 2 Spanning Tree network need to be careful
    about the placement of the root switch, as all network traffic will need to flow
    through it, so it should be selected with care and given an appropriate bridge
    priority as part of the network reference architecture design. If at any point,
    switches have been given the same bridge priority then the bridge with the lowest
    MAC address wins.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Network architects designing the layer 2 Spanning Tree network need to be careful
    about the placement of the root switch, as all network traffic will need to flow
    through it, so it should be selected with care and given an appropriate bridge
    priority as part of the network reference architecture design. If at any point,
    switches have been given the same bridge priority then the bridge with the lowest
    MAC address wins.
- en: Network architects should also design the network for redundancy so that if
    a root switch fails, there is a nominated backup root switch with a priority of
    one value less than the nominated root switch, which will take over when a root
    switch fails. In the scenario, the root switch fails the election process will
    begin again and the network will converge, which can take some time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Network architects should also design the network for redundancy so that if
    a root switch fails, there is a nominated backup root switch with a priority of
    one value less than the nominated root switch, which will take over when a root
    switch fails. In the scenario, the root switch fails the election process will
    begin again and the network will converge, which can take some time.
- en: The use of STP is not without its risks, if it does fail due to user configuration
    error, data center equipment failure or software failure on a switch or bad design,
    then the consequences to a network can be huge. The result can be that loops might
    form within the bridged network, which can result in a flood of broadcast, multicast
    or unknown-unicast storms that can potentially take down the entire network leading
    to long network outages. The complexity associated with network architects or
    engineers troubleshooting STP issues is important, so it is paramount that the
    network design is sound.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: The use of STP is not without its risks, if it does fail due to user configuration
    error, data center equipment failure or software failure on a switch or bad design,
    then the consequences to a network can be huge. The result can be that loops might
    form within the bridged network, which can result in a flood of broadcast, multicast
    or unknown-unicast storms that can potentially take down the entire network leading
    to long network outages. The complexity associated with network architects or
    engineers troubleshooting STP issues is important, so it is paramount that the
    network design is sound.
- en: Leaf-Spine architecture
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Leaf-Spine architecture
- en: 'In recent years with the emergence of cloud computing, we have seen data centers
    move away from a STP in favor of a Leaf-Spine networking architecture. The Leaf-Spine
    architecture is shown in the following diagram:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'In recent years with the emergence of cloud computing, we have seen data centers
    move away from a STP in favor of a Leaf-Spine networking architecture. The Leaf-Spine
    architecture is shown in the following diagram:'
- en: '![Leaf-Spine architecture](img/B05559_01_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Leaf-Spine architecture](img/B05559_01_02.jpg)'
- en: 'In a Leaf-Spine architecture:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'In a Leaf-Spine architecture:'
- en: Spine switches are connected into a set of core switches
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spine switches are connected into a set of core switches
- en: Spine switches are then connected with Leaf switches with each Leaf switch deployed
    at the top of rack, which means that any Leaf switch can connect to any Spine
    switch in one hop
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spine switches are then connected with Leaf switches with each Leaf switch deployed
    at the top of rack, which means that any Leaf switch can connect to any Spine
    switch in one hop
- en: Leaf-Spine architectures are promoted by companies such as Arista, Juniper,
    and Cisco. A Leaf-Spine architecture is built on layer 3 routing principle to
    optimize throughput and reduce latency.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine architectures are promoted by companies such as Arista, Juniper,
    and Cisco. A Leaf-Spine architecture is built on layer 3 routing principle to
    optimize throughput and reduce latency.
- en: Both Leaf and Spine switches communicate with each other via **external Border
    Gate Protocol** (**eBGP**) as the routing protocol for the IP fabric. eBGP establishes
    a **Transmission Control Protocol** (**TCP**) connection to each of its BGP peers
    before BGP updates can be exchanged between the switches. Leaf switches in the
    implementation will sit at top of rack and can be configured in **Multichassis
    Link Aggregation** (**MLAG**) mode using **Network Interface Controller** (**NIC**)
    bonding.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Both Leaf and Spine switches communicate with each other via **external Border
    Gate Protocol** (**eBGP**) as the routing protocol for the IP fabric. eBGP establishes
    a **Transmission Control Protocol** (**TCP**) connection to each of its BGP peers
    before BGP updates can be exchanged between the switches. Leaf switches in the
    implementation will sit at top of rack and can be configured in **Multichassis
    Link Aggregation** (**MLAG**) mode using **Network Interface Controller** (**NIC**)
    bonding.
- en: MLAG was originally used with **STP** so that two or more switches are bonded
    to emulate like a single switch and used for redundancy so they appeared as one
    switch to STP. In the event of a failure this provided multiple uplinks for redundancy
    in the event of a failure as the switches are peered, and it worked around the
    need to disable redundant paths. Leaf switches can often have **internal Border
    Gate Protocol** (**iBGP**) configured between the pairs of switches for resiliency.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MLAG最初与**STP**一起使用，使得两个或更多交换机绑定在一起，模拟成一个交换机，作为冗余使用，从而在STP中显示为一个交换机。在发生故障时，这提供了多个上行链路作为冗余，因为交换机是对等的，并且可以绕过禁用冗余路径的需求。Leaf交换机之间通常会配置**内部边界网关协议**（**iBGP**）以实现容错。
- en: In a Leaf-Spine architecture, Spine switches do not connect to other Spine switches,
    and Leaf switches do not connect directly to other Leaf switches unless bonded
    top of rack using MLAG NIC bonding. All links in a Leaf-Spine architecture are
    set up to forward with no looping. Leaf-Spine architectures are typically configured
    to implement **Equal Cost Multipathing** (**ECMP**), which allows all routes to
    be configured on the switches so that they can access any Spine switch in the
    layer 3 routing fabric.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在Leaf-Spine架构中，Spine交换机不会与其他Spine交换机连接，Leaf交换机也不会直接与其他Leaf交换机连接，除非使用MLAG网卡绑定在机架顶部进行绑定。Leaf-Spine架构中的所有链接都被设置为无环转发。Leaf-Spine架构通常配置为实现**等成本多路径**（**ECMP**），允许在交换机上配置所有路由，从而可以访问层3路由结构中的任何Spine交换机。
- en: ECMP means that Leaf switches routing table has the next-hop configured to forward
    to each Spine switch. In an ECMP setup, each leaf node has multiple paths of equal
    distance to each Spine switch, so if a Spine or Leaf switch fails, there is no
    impact as long as there are other active paths to another adjacent Spine switches.
    ECMP is used to load balance flows and supports the routing of traffic across
    multiple paths. This is in contrast to the STP, which switches off all but one
    path to the root when the network converges.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ECMP意味着Leaf交换机的路由表已配置了指向每个Spine交换机的下一跳。在ECMP配置中，每个Leaf节点都有多条与每个Spine交换机等距离的路径，因此如果某个Spine或Leaf交换机故障，只要还有其他活跃路径连接到相邻的Spine交换机，就不会受到影响。ECMP用于负载均衡流量，并支持通过多条路径路由流量。与STP相比，STP会在网络收敛时关闭除一条路径外的所有路径。
- en: Normally, Leaf-Spine architectures designed for high performance use 10G access
    ports at Leaf switches mapping to 40G Spine ports. When device port capacity becomes
    an issue, new Leaf switches can be added by connecting it to every Spine on the
    network while pushing the new configuration to every switch. This means that network
    teams can easily scale out the network horizontally without managing or disrupting
    the switching protocols or impacting the network performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了实现高性能，Leaf-Spine架构在Leaf交换机上使用10G接入端口，映射到40G的Spine端口。当设备端口容量成为问题时，可以通过将新Leaf交换机连接到网络中的每个Spine交换机，并将新配置推送到每个交换机，来添加新的Leaf交换机。这意味着网络团队可以轻松地横向扩展网络，而无需管理或中断交换协议，也不会影响网络性能。
- en: 'An illustration of the protocols used in a Leaf-Spine architecture are shown
    later, with Spine switches connected to Leaf switches using BGP and ECMP and Leaf
    switches sitting top of rack and configured for redundancy using MLAG and iBGP:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 后续会展示Leaf-Spine架构中使用的协议图示，Spine交换机通过BGP和ECMP与Leaf交换机连接，Leaf交换机位于机架顶部，并使用MLAG和iBGP配置以实现冗余：
- en: '![Leaf-Spine architecture](img/B05559_01_02A.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Leaf-Spine架构](img/B05559_01_02A.jpg)'
- en: 'The benefits of a Leaf-Spine architecture are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine架构的优势如下：
- en: Consistent latency and throughput in the network
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中的一致性延迟和吞吐量
- en: Consistent performance for all racks
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有机架的一致性性能
- en: Network once configured becomes less complex
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络一旦配置完成，复杂性会大大降低
- en: Simple scaling of new racks by adding new Leaf switches at top of rack
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在机架顶部添加新的Leaf交换机，可以简单地扩展新机架。
- en: Consistent performance, subscription, and latency between all racks
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有机架之间的一致性性能、订阅和延迟
- en: East-west traffic performance is optimized (virtual machine to virtual machine
    communication) to support microservice applications
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 东西向流量性能优化（虚拟机到虚拟机的通信），以支持微服务应用
- en: Removes VLAN scaling issues, controls broadcast and fault domains
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决VLAN扩展问题，控制广播和故障域
- en: The one drawback of a Leaf-Spine topology is the amount of cables it consumes
    in the data center.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Leaf-Spine拓扑的一个缺点是它在数据中心中消耗的电缆数量。
- en: OVSDB
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OVSDB
- en: Modern switches have now moved towards open source standards, so they can use
    the same pluggable framework. The open standard for virtual switches is **Open
    vSwitch**, which was born out of the necessity to come up with an open standard
    that allowed a virtual switch to forward traffic to different virtual machines
    on the same physical host and physical network. Open vSwitch uses **Open vSwitch
    database** (**OVSDB**) that has a standard extensible schema.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch was initially deployed at the hypervisor level but is now being
    used in container technology too, which has Open vSwitch implementations for networking.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The following hypervisors currently implement Open vSwitch as their virtual
    switching technology:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: KVM
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xen
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyper-V
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyper-V** has recently moved to support Open vSwitch using the implementation
    created by Cloudbase ([https://cloudbase.it/](https://cloudbase.it/)), which is
    doing some fantastic work in the open source space and is testament to how Microsoft''s
    business model has evolved and embraced open source technologies and standards
    in recent years. Who would have thought it? Microsoft technologies now run natively
    on Linux.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The Open vSwitch exchanges **OpenFlow** between virtual switch and physical
    switches in order to communicate and can be programmatically extended to fit the
    needs of vendors. In the following diagram, you can see the Open vSwitch architecture.
    Open vSwitch can run on a server using the KVM, Xen, or Hyper-V virtualization
    layer:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![OVSDB](img/B05559_01_03.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: The **ovsdb-server** contains the OVSDB schema that holds all switching information
    for the virtual switch. The **ovs-vswitchd** daemon talks **OpenFlow** to any
    **Control & Management Cluster,** which could be any SDN controller that can communicate
    using the **OpenFlow** protocol.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Controllers use OpenFlow to install flow state on the virtual switch, and OpenFlow
    dictates what actions to take when packets are received by the virtual switch.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: When Open vSwitch receives a packet it has never seen before and has no matching
    flow entries, it sends this packet to the controller. The controller then makes
    a decision on how to handle this packet based on the flow rules to either block
    or forward. The ability to configure **Quality of Service** (**QoS**) and other
    statistics is possible on Open vSwitch.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch is used to configure security rules and provision ACL rules at
    the switch level on a hypervisor.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: A Leaf-Spine architecture allows overlay networks to be easily built, meaning
    that cloud and tenant environments are easily connected to the layer 3 routing
    fabric. Hardware **Vxlan Tunnel Endpoints** (**VTEPs**) IPs are associated with
    each Leaf switch or a pair of Leaf switches in MLAG mode and are connected to
    each physical compute host via **Virtual Extensible LAN** (**VXLAN**) to each
    Open vSwitch that is installed on a hypervisor.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: This allows an SDN controller, which is provided by vendors, such as Cisco,
    Nokia, and Juniper to build an overlay network that creates VXLAN tunnels to the
    physical hypervisors using Open vSwitch. New VXLAN tunnels are created automatically
    if a new compute is scaled out, then SDN controllers can create new VXLAN tunnels
    on the Leaf switch as they are peered with the Leaf switch's hardware **VXLAN
    Tunnel End Point** (**VTEP**).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得由厂商提供的SDN控制器，例如思科（Cisco）、诺基亚（Nokia）和瞻博网络（Juniper），能够构建一个覆盖网络，通过Open vSwitch创建到物理虚拟化主机的VXLAN隧道。如果有新的计算节点扩展，新的VXLAN隧道会自动创建，之后SDN控制器可以在Leaf交换机上创建新的VXLAN隧道，因为它们与Leaf交换机的硬件**VXLAN隧道终端**（**VTEP**）建立了对等连接。
- en: Modern switch vendors, such as Arista, Cisco, Cumulus, and many others, use
    OVSDB, and this allows SDN controllers to integrate at the **Control & Management
    Cluster** level. As long as an SDN controller uses OVSDB and OpenFlow protocol,
    they can seamlessly integrate with the switches and are not tied into specific
    vendors. This gives end users a greater depth of choice when choosing switch vendors
    and SDN controllers, which can be matched up as they communicate using the same
    open standard protocol.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现代交换机厂商，如Arista、思科、Cumulus等，使用OVSDB，这使得SDN控制器能够在**控制与管理集群**层级进行集成。只要SDN控制器使用OVSDB和OpenFlow协议，它们就可以与交换机无缝集成，并且不受特定厂商的限制。这为最终用户在选择交换机厂商和SDN控制器时提供了更大的选择空间，因为它们使用相同的开放标准协议进行通信。
- en: Changes that have occurred in networking with the introduction of public cloud
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公有云引入后网络领域发生的变化
- en: It is unquestionable that the emergence of the AWS, which was launched in 2006,
    changed and shaped the networking landscape forever. AWS has allowed companies
    to rapidly develop their products on the AWS platform. AWS has created an innovative
    set of services for end users, so they can manage infrastructure, load balancing,
    and even databases. These services have led the way in making the DevOps ideology
    a reality, by allowing users to elastically scale up and down infrastructure.
    They need to develop products on demand, so infrastructure wait times are no longer
    an inhibitor to development teams. AWS rich feature set of technology allows users
    to create infrastructure by clicking on a portal or more advanced users that want
    to programmatically create infrastructure using configuration management tooling,
    such as **Ansible**, **Chef**, **Puppet**, **Salt** or **Platform as a Service**
    (**PaaS**) solutions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，2006年推出的AWS的出现彻底改变并塑造了网络格局。AWS使公司能够在AWS平台上快速开发其产品。AWS为最终用户创造了一套创新的服务，使他们能够管理基础设施、负载均衡，甚至数据库。这些服务引领了DevOps理念的实现，允许用户按需弹性地扩展和收缩基础设施。开发人员不再受到基础设施等待时间的限制，可以更高效地进行产品开发。AWS丰富的技术功能集允许用户通过点击门户来创建基础设施，或者更多的高级用户通过配置管理工具（如**Ansible**、**Chef**、**Puppet**、**Salt**或**平台即服务**（**PaaS**）解决方案）来编程创建基础设施。
- en: An overview of AWS
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS概述
- en: In 2016, the AWS **Virtual Private Cloud** (**VPC**) secures a set of Amazon
    EC2 instances (virtual machines) that can be connected to any existing network
    using a VPN connection. This simple construct has changed the way that developers
    want and expect to consume networking.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，AWS的**虚拟私有云**（**VPC**）保护一组Amazon EC2实例（虚拟机），这些实例可以通过VPN连接连接到任何现有网络。这个简单的构造改变了开发人员希望和期望的网络消费方式。
- en: In 2016, we live in a consumer-based society with mobile phones allowing us
    instant access to the Internet, films, games, or an array of different applications
    to meet our every need, instant gratification if you will, so it is easy to see
    the appeal of AWS has to end users.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，我们生活在一个以消费为主导的社会，手机让我们能即时访问互联网、电影、游戏，或是各种不同的应用程序来满足我们的一切需求，换句话说，我们生活在即时满足的时代，因此不难理解AWS对最终用户的吸引力。
- en: AWS allows developers to provision instances (virtual machines) in their own
    personal network, to their desired specification by selecting different flavors
    (CPU, RAM, and disk) using a few button clicks on the AWS portal's graphical user
    interface, alternately using a simple call to an API or scripting against the
    AWS-provided SDKs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: AWS允许开发人员通过AWS门户的图形用户界面，通过几次点击，选择不同的规格（CPU、RAM和磁盘）来在他们自己的个人网络中配置实例（虚拟机），或者通过简单的API调用或使用AWS提供的SDK进行脚本编写。
- en: 'So now a valid question, why should developers be expected to wait long periods
    of time for either infrastructure or networking tickets to be serviced in on-premises
    data centers when AWS is available? It really shouldn''t be a hard question to
    answer. The solution surely has to either be moved to AWS or create a private
    cloud solution that enables the same agility. However, the answer isn''t always
    that straightforward, there are following arguments against using AWS and public
    cloud:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在有一个合理的问题，为什么开发人员在 AWS 可用的情况下，仍然需要在本地数据中心等待长时间才能处理基础设施或网络工单？这个问题其实不难回答。解决方案肯定是将其迁移到
    AWS，或者创建一个能够提供相同敏捷性的私有云解决方案。然而，答案并非总是如此简单，下面是反对使用 AWS 和公有云的一些理由：
- en: Not knowing where the data is actually stored and in which data center
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不知道数据实际上存储在哪里以及在哪个数据中心
- en: Not being able to hold sensitive data offsite
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法将敏感数据存储在远程地点
- en: Not being able to assure the necessary performance
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法保证所需的性能
- en: High running costs
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高昂的运行成本
- en: All of these points are genuine blockers for some businesses that may be highly
    regulated or need to be PCI compliant or are required to meet specific regularity
    standards. These points may inhibit some businesses from using public cloud so
    as with most solutions it isn't the case of one size fits all.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题对于一些受高度监管、需要符合 PCI 合规性或必须满足特定监管标准的企业来说，都是实际的障碍。这些问题可能会阻碍一些企业使用公有云，因此，像大多数解决方案一样，并不是“一个解决方案适用于所有”。
- en: In private data centers, there is a cultural issue that teams have been set
    up to work in silos and are not set up to succeed in an agile business model,
    so a lot of the time using AWS, Microsoft Azure, or Google Cloud is a quick fix
    for broken operational models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在私有数据中心，有一个文化问题，即团队被设置为在孤岛中工作，并没有为成功的敏捷业务模型做准备，因此，很多时候，使用 AWS、微软 Azure 或谷歌云只是对破碎的运营模型的一个快速修复。
- en: Ticketing systems, a staple of broken internal operational models, are not a
    concept that aligns itself to speed. An IT ticket raised to an adjacent team can
    take days or weeks to complete, so requests are queued before virtual or physical
    servers can be provided to developers. Also, this is prominent for network changes
    too, with changes such as a simple modification to ACL rules taking an age to
    be implemented due to ticketing backlogs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 工单系统，作为破碎的内部运营模型的典型代表，并不是一个与速度相匹配的概念。提交给相邻团队的 IT 工单可能需要几天或几周才能完成，因此在虚拟或物理服务器提供给开发人员之前，请求会被排队等待。此外，对于网络变更也同样如此，例如对
    ACL 规则的简单修改，由于工单积压，可能需要很长时间才能实施。
- en: Developers need to have the ability to scale up servers or prototype new features
    at will, so long wait times for IT tickets to be processed hinder delivery of
    new products to market or bug fixes to existing products. It has become common
    in internal IT that some **Information Technology Infrastructure Library** (**ITIL**)
    practitioners put a sense of value on how many tickets that processed over a week
    as the main metric for success. This shows complete disregard for customer experience
    of their developers. There are some operations that need to shift to the developers,
    which have traditionally lived with internal or shadow IT, but there needs to
    be a change in operational processes at a business level to invoke these changes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员需要能够随时扩展服务器或原型化新功能，因此，长时间等待 IT 工单处理会妨碍新产品推向市场或现有产品的 bug 修复。在内部 IT 中，**信息技术基础架构库**（**ITIL**）的某些从业者将每周处理的工单数量作为成功的主要指标，这表明他们完全忽视了开发人员的客户体验。一些操作需要转交给开发人员，这些操作传统上是由内部
    IT 或影子 IT 负责的，但在业务层面需要对操作流程进行调整，才能推动这些变化。
- en: Put simply, AWS has changed the expectations of developers and the expectations
    placed on infrastructure and networking teams. Developers should be able to service
    their needs as quickly as making an alteration to an application on their mobile
    phone, free from slow internal IT operational models associated with companies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，AWS 改变了开发人员的期望以及对基础设施和网络团队的期望。开发人员应该能够像在手机上修改应用程序一样迅速满足自己的需求，摆脱公司内部 IT
    操作模型带来的拖延。
- en: But for start-ups and businesses that can use AWS, which aren't constrained
    by regulatory requirements, it skips the need to hire teams to rack servers, configure
    network devices, and pay for the running costs of data centers. It means they
    can start viable businesses and run them on AWS by putting in credit card details
    the same way as you would purchase a new book on Amazon or eBay.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack overview
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reaction to AWS was met with trepidation from competitors, as it disrupted
    the cloud computing industry and has led to PaaS solutions such as **Cloud Foundry**
    and **Pivotal** coming to fruition to provide an abstraction layer on top of hybrid
    clouds.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: When a market is disrupted, it promotes a reaction, from it spawned the idea
    for a new private cloud. In 2010, a joint venture by Rackspace and NASA, launched
    an open source cloud-software initiative known as OpenStack, which came about
    as NASA couldn't put their data in a public cloud.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The OpenStack project intended to help organizations offer cloud computing services
    running on standard hardware and directly set out to mimic the model provided
    by AWS. The main difference with OpenStack is that it is an open source project
    that can be used by leading vendors to bring AWS-like ability and agility to the
    private cloud.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Since its inception in 2010, OpenStack has grown to have over 500 member companies
    as part of the OpenStack Foundation, with platinum members and gold members that
    comprise the biggest IT vendors in the world that are actively driving the community.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The platinum members of the OpenStack foundation are:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenStack overview](img/B05559_01_04.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: OpenStack is an open source project, which means its source code is publicly
    available and its underlying architecture is available for analysis, unlike AWS,
    which acts like a magic box of tricks but it is not really known for how it works
    underneath its shiny exterior.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack is primarily used to provide an **Infrastructure as a Service** (**IaaS**)
    function within the private cloud, where it makes commodity x86 compute, centralized
    storage, and networking features available to end users to self-service their
    needs, be it via the horizon dashboard or through a set of common API's.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Many companies are now implementing OpenStack to build their own data centers.
    Rather than doing it on their own, some companies are using different vendor hardened
    distributions of the community upstream project. It has been proven that using
    a vendor hardened distributions of OpenStack, when starting out, mean that OpenStack
    implementation is far likelier to be successful. Initially, for some companies,
    implementing OpenStack can be seen as complex as it is a completely new set of
    technology that a company may not be familiar with yet. OpenStack implementations
    are less likely to fail when using professional service support from known vendors,
    and it can create a viable alternative to enterprise solutions, such as AWS or
    Microsoft Azure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Vendors, such as Red Hat, HP, Suse, Canonical, Mirantis, and many more, provide
    different distributions of OpenStack to customers, complete with different methods
    of installing the platform. Although the source code and features are the same,
    the business model for these OpenStack vendors is that they harden OpenStack for
    enterprise use and their differentiator to customers is their professional services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different OpenStack distributions available to customers with
    the following vendors providing OpenStack distributions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Bright Computing
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canonical
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HPE
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirantis
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle OpenStack for Oracle Linux, or O3L
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle OpenStack for Oracle Solaris
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Red Hat
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SUSE
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware Integrated OpenStack (VIO)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack vendors will support build out, on-going maintenance, upgrades, or
    any customizations a client needs, all of which are fed back to the community.
    The beauty of OpenStack being an open source project is that if vendors customize
    OpenStack for clients and create a real differentiator or competitive advantage,
    they cannot fork OpenStack or uniquely sell this feature. Instead, they have to
    contribute the source code back to the upstream open source OpenStack project.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: This means that all competing vendors contribute to its success of OpenStack
    and benefit from each other's innovative work. The OpenStack project is not just
    for vendors though, and everyone can contribute code and features to push the
    project forward.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack maintains a release cycle where an upstream release is created every
    six months and is governed by the OpenStack Foundation. It is important to note
    that many public clouds, such as at&t, RackSpace, and GoDaddy, are based on OpenStack
    too, so it is not exclusive to private clouds, but it has undeniably become increasingly
    popular as a private cloud alternative to AWS public cloud and now widely used
    for **Network Function Virtualization** (**NFV**).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: So how does AWS and OpenStack work in terms of networking? Both AWS and OpenStack
    are made up of some mandatory and optional projects that are all integrated to
    make up its reference architecture. Mandatory projects include compute and networking,
    which are the staple of any cloud solution, whereas others are optional bolt-ons
    to enhance or extend capability. This means that end users can cherry-pick the
    projects they are interested in to make up their own personal portfolio.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The AWS approach to networking
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having discussed both AWS and OpenStack, first, we will explore the AWS approach
    to networking, before looking at an alternative method using OpenStack and compare
    the two approaches. When first setting up networking in AWS, a tenant network
    in AWS is instantiated using VPC, which post 2013 deprecated AWS classic mode;
    but what is VPC?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Amazon VPC
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A VPC is the new default setting for new customers wishing to access AWS. VPCs
    can also be connected to customer networks (private data centers) by allowing
    AWS cloud to extend a private data center for agility. The concept of connecting
    a private data center to an AWS VPC is using something AWS refers to as a customer
    gateway and virtual private gateway. A virtual private gateway in simple terms
    is just two redundant VPN tunnels, which are instantiated from the customer's
    private network.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Customer gateways expose a set of external static addresses from a customer
    site, which are typically **Network Address Translation-Traversal** (**NAT-T**)
    to hide the source address. UDP port `4500` should be accessible in the external
    firewall in the private data center. Multiple VPCs can be supported from one customer
    gateway device.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![Amazon VPC](img/B05559_01_05.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: A VPC gives an isolated view of everything an AWS customer has provisioned in
    AWS public cloud. Different user accounts can then be set up against VPC using
    the AWS **Identity and Access Management (IAM)** service, which has customizable
    permissions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example of a VPC shows instances (virtual machines) mapped with
    one or more security groups and connected to different subnets connected to the
    VPC router:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Amazon VPC](img/B05559_01_06.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'A **VPC** simplifies networking greatly by putting the constructs into software
    and allows users to perform the following network functions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Creating instances (virtual machines) mapped to subnets
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating **Domain Name System** (**DNS**) entries that are applied to instances
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning public and private IP addresses
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating or associating subnets
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating custom routing
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying security groups with associated ACL rules
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, when an instance (virtual machine) is instantiated in a VPC, it
    will either be placed on a default subnet or custom subnet if specified.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: All VPCs come with a default router when the VPC is created, the router can
    have additional custom routes added and routing priority can also be set to forward
    traffic to particular subnets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Amazon IP addressing
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an instance is spun up in AWS, it will automatically be assigned a mandatory
    private IP address by **Dynamic Host Configuration Protocol** (**DHCP**) as well
    as a public IP and DNS entry too unless dictated otherwise. Private IPs are used
    in AWS to route east-west traffic between instances when virtual machine needs
    to communicate with adjacent virtual machines on the same subnet, whereas public
    IPs are available through the Internet.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: If a persistent public IP address is required for an instance, AWS offers the
    elastic IP addresses feature, which is limited to five per VPC account, which
    any failed instances IP address can be quickly mapped to another instance. It
    is important to note that it can take up to 24 hours for a public IP address's
    DNS **Time To Live** (**TTL**) to propagate when using AWS.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In terms of throughput, AWS instances can support a **Maximum Transmission Unit**
    (**MTU**) of 1,500 that can be passed to an instance in AWS, so this needs to
    be considered when considering application performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Amazon security groups
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security groups in AWS are a way of grouping permissive ACL rules, so don't
    allow explicit denies. AWS security groups act as a virtual firewall for instances,
    and they can be associated with one or more instances' network interfaces. In
    a VPC, you can associate a network interface with up to five security groups,
    adding up to 50 rules to a security group, with a maximum of 500 security groups
    per VPC. A VPC in an AWS account automatically has a default security group, which
    will be automatically applied if no other security groups are specified.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Default security groups allow all outbound traffic and all inbound traffic only
    from other instances in a VPC that also use the default security group. The default
    security group cannot be deleted. Custom security groups when first created allow
    no inbound traffic, but all outbound traffic is allowed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Permissive ACL rules associated with security groups govern inbound traffic
    and are added using the AWS console (GUI) as shown later in the text, or they
    can be programmatically added using APIs. Inbound ACL rules associated with security
    groups can be added by specifying type, protocol, port range, and the source address.
    Refer to the following screenshot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Amazon security groups](img/B05559_01_07.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Amazon regions and availability zones
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A VPC has access to different regions and availability zones of shared compute,
    which dictate the data center that the AWS instances (virtual machines) will be
    deployed in. Regions in AWS are geographic areas that are completely isolated
    by design, where availability zones are isolated locations in that specific region,
    so an availability zone is a subset of a region.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: AWS gives users the ability to place their resources in different locations
    for redundancy as sometimes the health of a specific region or availability zone
    can suffer issues. Therefore, AWS users are encouraged to use more than one availability
    zones when deploying production workloads on AWS. Users can choose to replicate
    their instances and data across regions if they choose to.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Within each isolated AWS region, there are child availability zones. Each availability
    zone is connected to sibling availability zones using low latency links. All communication
    from one region to another is across the public Internet, so using geographically
    distant regions will acquire latency and delay. Encryption of data should also
    be considered when hosting applications that send data across regions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Elastic Load Balancing
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS also allows **Elastic Load Balancing** (**ELB**) to be configured within
    a VPC as a bolt-on service. ELB can either be internal or external. When ELB is
    external, it allows the creation of an Internet-facing entry point into your VPC
    using an associated DNS entry and balances load between different instances. Security
    groups are assigned to ELBs to control the access ports that need to be used.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows an elastic load balancer, load balancing 3 instances:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Amazon Elastic Load Balancing](img/B05559_01_08.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: The OpenStack approach to networking
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having considered AWS networking, we will now explore OpenStack's approach to
    networking and look at how its services are configured.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack is deployed in a data center on multiple controllers. These controllers
    contain all the OpenStack services, and they can be installed on either virtual
    machines, bare metal (physical) servers, or containers. The OpenStack controllers
    should host all the OpenStack services in a highly available and redundant fashion
    when they are deployed in production.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Different OpenStack vendors provide different installers to install OpenStack.
    Some examples of installers from the most prominent OpenStack distributions are
    RedHat Director (based on OpenStack TripleO), Mirantis Fuel, HPs HPE installer
    (based on Ansible), and Juju for Canonical, which all install OpenStack controllers
    and are used to scale out compute nodes on the OpenStack cloud acting as an OpenStack
    workflow management tool.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack services
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A breakdown of the core OpenStack services that are installed on an OpenStack
    controller are as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '**Keystone** is the identity service for OpenStack that allows user access,
    which issues tokens, and can be integrated with LDAP or Active directory.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heat** is the orchestration provisioning tool for OpenStack infrastructure.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glance** is the image service for OpenStack that stores all image templates
    for virtual machines or bare metal servers.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cinder** is the block storage service for OpenStack that allows centralized
    storage volumes to be provisioned and attached to vms or bare metal servers that
    can then be mounted.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nova** is the compute service for OpenStack used to provision vms and uses
    different scheduling algorithms to work out where to place virtual machines on
    available compute.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizon** is the OpenStack dashboard that users connect to view the status
    of vms or bare metal servers that are running in a tenant network.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rabbitmq** is the message queue system for OpenStack.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Galera** is the database used to store all OpenStack data in the Nova (compute)
    and neutron (networking) databases holding vm, port, and subnet information.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swift** is the object storage service for OpenStack and can be used as a
    redundant storage backend that stores replicated copies of objects on multiple
    servers. Swift is not like traditional block or file-based storage; objects can
    be any unstructured data.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ironic** is the bare metal provisioning service for OpenStack. Originally,
    a fork of part of the Nova codebase, it allows provisioning of images on to bare
    metal servers and uses IPMI and ILO or DRAC interfaces to manage physical hardware.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neutron** is the networking service for OpenStack and contains ML2 and L3
    agents and allows configuration of network subnets and routers.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of neutron networking services, neutron architecture is very similar
    in constructs to AWS.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Useful links covering OpenStack services can be found at:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html](http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=N90ufYN0B6U](https://www.youtube.com/watch?v=N90ufYN0B6U)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack tenants
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Project, often referred to in OpenStack as a tenant, gives an isolated view
    of everything that a team has provisioned in an OpenStack cloud. Different user
    accounts can then be set up against a Project (tenant) using the keystone identity
    service, which can be integrated with **Lightweight Directory Access Protocol**
    (**LDAP**) or Active Directory to support customizable permission models.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack neutron
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenStack neutron performs all the networking functions in OpenStack.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'The following network functions are provided by the neutron project in an OpenStack
    cloud:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Creating instances (virtual machines) mapped to networks
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning IP addresses using its in-built DHCP service
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS entries are applied to instances from named servers
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assignment of private and Floating IP addresses
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating or associating network subnets
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating routers
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying security groups
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack is set up into its **Modular Layer 2** (**ML2**) and **Layer 3** (**L3**)
    agents that are configured on the OpenStack controllers. OpenStack's ML2 plugin
    allows OpenStack to integrate with switch vendors that use either Open vSwitch
    or Linux Bridge and acts as an agnostic plugin to switch vendors, so vendors can
    create plugins, to make their switches OpenStack compatible. The ML2 agent runs
    on the hypervisor communicating over **Remote Procedure Call** (**RPC**) to the
    compute host server.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack compute hosts are typically deployed using a hypervisor that uses
    Open vSwitch. Most OpenStack vendor distributions use the KVM hypervisor by default
    in their reference architectures, so this is deployed and configured on each compute
    host by the chosen OpenStack installer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Compute hosts in OpenStack are connected to the access layer of the STP 3-tier
    model, or in modern networks connected to the Leaf switches, with VLANs connected
    to each individual OpenStack compute host. Tenant networks are then used to provide
    isolation between tenants and use VXLAN and GRE tunneling to connect the layer
    2 network.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch runs in kernel space on the KVM hypervisor and looks after firewall
    rules by using OpenStack security groups that pushes down flow data via OVSDB
    from the switches. The neutron L3 agent allows OpenStack to route between tenant
    networks and uses neutron routers, which are deployed within the tenant network
    to accomplish this, without a neutron router networks are isolated from each other
    and everything else.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning OpenStack networks
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When setting up simple networking using neutron in a Project (tenant) network,
    two different networks, an internal network, and an external network will be configured.
    The internal network will be used for east-west traffic between instances. This
    is created as shown in the following horizon dashboard with an appropriate **Network
    Name**:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_09.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'The **Subnet Name** and subnet range are then specified in the **Subnet** section,
    as shown in the following screenshot:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_10.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Finally, DHCP is enabled on the network, and any named **Allocation Pools**
    (specifies only a range of addresses that can be used in a subnet) are optionally
    configured alongside any named **DNS Name Servers**, as shown below:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_11.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'An external network will also need to be created to make the internal network
    accessible from outside of OpenStack, when external networks are created by an
    administrative user, the set **External Network** checkbox needs to be selected,
    as shown in the next screenshot:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_12.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'A router is then created in OpenStack to route packets to the network, as shown
    below:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_13.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'The created router will then need to be associated with the networks; this
    is achieved by adding an interface on the router for the private network, as illustrated
    in the following screenshot:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_14.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'The **External Network** that was created then needs to be set as the router''s
    gateway, as per the following screenshot:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_15.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'This then completes the network setup; the final configuration for the internal
    and external network is displayed below, which shows one router connected to an
    internal and external network:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_16.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: In OpenStack, instances are provisioned onto the internal private network by
    selecting the private network NIC when deploying instances. OpenStack has the
    convention of assigning pools of public IPs (floating IP) addresses from an external
    network for instances that need to be externally routable outside of OpenStack.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a set of floating IP addresses, an OpenStack administrator will set
    up an allocation pool using the external network from an external network, as
    shown in the following screenshot:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_17.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: OpenStack like AWS, uses security groups to set up firewall rules between instances.
    Unlike AWS, OpenStack supports both ingress and egress ACL rules, whereas AWS
    allows all outbound communication, OpenStack can deal with both ingress and egress
    rules. Bespoke security groups are created to group ACL rules as shown below
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_18.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'Ingress and Rules can then be created against a security group. **SSH** access
    is configured as an ACL rule against the parent security group, which is pushed
    down to Open VSwitch into kernel space on each hypervisor, as seen in the next
    screenshot:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_19.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Once the Project (tenant) has two networks, one internal and one external, and
    an appropriate security group has been configured, instances are ready to be launched
    on the private network.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'An instance is launched by selecting **Launch Instance** in horizon and setting
    the following parameters:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '**Availability Zone**'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instance Name**'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flavor** (CPU, RAM, and disk space)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image Name** (base operating system)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_20.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: 'The private network is then selected as the **NIC** for the instance under
    the **Networking** tab:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_21.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: This will mean that when the instance is launched, it will use OpenStack's internal
    DHCP service to pick an available IP address from the allocated subnet range.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'A security group should also be selected to govern the ACL rules for the instance;
    in this instance, the `testsg1` security group is selected as shown in the following
    screenshot:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_22.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: 'Once the instance has been provisioned, a floating IP address can be associated
    from the external network:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_23.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'A floating IP address from the external network floating IP address pool is
    then selected and associated with the instance:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning OpenStack networks](img/B05559_01_24.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: The floating IP addresses NATs OpenStack instances that are deployed on the
    internal public IP address to the external network's floating IP address, which
    will allow the instance to be accessible from outside of OpenStack.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack regions and availability zones
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenStack like AWS, as seen on instance creation, also utilizes regions and
    availability zones. Compute hosts in OpenStack (hypervisors) can be assigned to
    different availability zones.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: An availability zone in OpenStack is just a virtual separation of compute resources.
    In OpenStack, an availability zone can be further segmented into host aggregates.
    It is important to note that a compute host can be assigned to only one availability
    zone, but can be a part of multiple host aggregates in that same availability
    zone.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Nova uses a concept named **nova scheduler rules**, which dictates the placement
    of instances on compute hosts at provisioning time. A simple example of a nova
    scheduler rule is the `AvailabiltyZoneFilter` filter, which means that if a user
    selects an availability zone at provisioning time, then the instance will land
    only on any of the compute instances grouped under that availability zone.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Another example of the `AggregateInstanceExtraSpecsFilter` filter that means
    that if a custom flavor (CPU, RAM, and disk) is tagged with a key value pair and
    a host aggregate is tagged with the same key value pair, then if a user deploys
    with that flavor the `AggregateInstanceExtraSpecsFilter` filter will place all
    instances on compute hosts under that host aggregate.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: These host aggregates can be assigned to specific teams, which means that teams
    can be selective about which applications they share their compute with and can
    be used to prevent noisy neighbor syndrome. There is a wide array of filters that
    can be applied in OpenStack in all sorts of orders to dictate instance scheduling.
    OpenStack allows cloud operators to create a traditional cloud model with large
    groups of contended compute to more bespoke use cases where the isolation of compute
    resources is required for particular application workloads.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows host aggregates with groups and shows a host aggregate
    named **1-Host-Aggregate**, grouped under an **Availability Zone** named **DC1**
    containing two compute hosts (hypervisors), which could be allocated to a particular
    team:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenStack regions and availability zones](img/B05559_01_25.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: OpenStack instance provisioning workflow
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When an instance (virtual machine) is provisioned in OpenStack, the following
    high-level steps are carried out:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The Nova compute service will issue a request for a new instance (virtual machine)
    using the image selected from the glance images service
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nova request may then be queued by **RabbitMQ** before being processed (RabbitMQ
    allows OpenStack to deal with multiple simultaneous provisioning requests)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the request for a new instance is processed, the request will write a new
    row into the nova Galera database in the nova database
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nova will look at the nova scheduler rules defined on the OpenStack controllers
    and will use those rules to place the instance on an available compute node (KVM
    hypervisor)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an available hypervisor is found that meets the nova scheduler rules, then
    the provisioning process will begin
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nova will check whether the image already exists on the matched hypervisor.
    If it doesn't, the image will be transferred from the hypervisor and booted from
    local disk
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nova will issue a neutron request, which will create a new VPort in OpenStack
    and map it to the neutron network
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VPort information will then be written to both the nova and neutron databases
    in Galera to correlate the instance with the network
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutron will issue a DHCP request to assign the instance a private IP address
    from an unallocated IP address from the subnet it has been associated with
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A private IP address will then be assigned, and the instance will start to start
    up on the private network
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neutron metadata service will then be contacted to retrieve cloud-init information
    on boot, which will assign a DNS entry to the instance from the named server,
    if specified
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once cloud-init has run, the instance will be ready to use
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Floating IPs can then be assigned to the instance to NAT to external networks
    to make the instances publicly accessible
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack LBaaS
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like AWS OpenStack also offers a **Load-Balancer-as-a-Service** (**LBaaS**)
    option that allows incoming requests to be distributed evenly among designated
    instances using a **Virtual IP** (**VIP**). The features and functionality supported
    by LBaaS are dependent on the vendor plugin that is used.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Popular LBaaS plugins in OpenStack are:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Citrix NetScaler
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F5
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HaProxy
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avi networks
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These load balancers all expose varying degrees of features to the OpenStack
    LBaaS agent. The main driver for utilizing LBaaS on OpenStack is that it allows
    users to use LBaaS as a broker to the load balancing solution, allowing users
    to use the OpenStack API or configure the load balancer via the horizon GUI.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: LBaaS allows load balancing to be set up within a tenant network in OpenStack.
    Using LBaaS means that if for any reason a user wishes to use a new load balancer
    vendor as opposed to their incumbent one; as long as they are using OpenStack
    LBaaS, it is made much easier. As all calls or administration are being done via
    the LBaaS APIs or Horizon, no changes would be required to the orchestration scripting
    required to provision and administrate the load balancer, and they wouldn't be
    tied into each vendor's custom APIs and the load balancing solution becomes a
    commodity.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered some of the basic networking principles that
    are used in today's modern data centers, with special focus on the AWS and OpenStack
    cloud technologies which are two of the most popular solutions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Having read this chapter, you should now be familiar with the difference between
    Leaf-Spine and Spanning Tree network architectures, it should have demystified
    AWS networking, and you should now have a basic understanding of how private and
    public networks can be configured in OpenStack.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: In the forthcoming chapters, we will build on these basic networking constructs
    and look at how they can be programmatically controlled using configuration management
    tools and used to automate network functions. But first, we will focus on some
    of the software-defined networking controllers that can be used to extend the
    capability of OpenStack even further than neutron in the private clouds and some
    of the feature sets and benefits they bring to ease the pain of managing network
    operations.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Useful links for Amazon content are:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/](https://aws.amazon.com/)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=VgzzHCukwpc](https://www.youtube.com/watch?v=VgzzHCukwpc)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=jLVPqoV4YjU](https://www.youtube.com/watch?v=jLVPqoV4YjU)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Useful links for OpenStack content are:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[https://wiki.openstack.org/wiki/Main_Page](https://wiki.openstack.org/wiki/Main_Page)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=Qz5gyDenqTI](https://www.youtube.com/watch?v=Qz5gyDenqTI)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=Li0Ed1VEziQ](https://www.youtube.com/watch?v=Li0Ed1VEziQ)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
