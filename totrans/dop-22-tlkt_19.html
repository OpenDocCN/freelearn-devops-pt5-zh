<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-self-adaptation-applied-to-infrastructure" class="calibre15">Self-Adaptation Applied To Infrastructure</h2>

<p class="calibre3">Our goal is within reach. We adopted schedulers (Docker Swarm in this case) that provide self-healing applied to services. We saw how <em class="calibre21">Docker For AWS</em> accomplishes a similar goal but on the infrastructure level. We used Prometheus, Alertmanager, and Jenkins to build a system that automatically adapts services to ever-changing conditions. The metrics we’re storing in Prometheus are a combination of those gathered through exporters and those we added to our services through instrumentation. The only thing we’re missing is self-adaptation applied to infrastructure. If we manage to build it, we’ll close the circle and witness a self-sufficient system capable of running without (almost) any human intervention.</p>

<p class="calibre3">The logic behind self-adaptation applied to infrastructure is not much different from the one we used with services. We need metrics, alerts, and scripts that will adapt cluster capacity whenever conditions change.</p>

<p class="calibre3">We already have all the tools we need. Prometheus will continue gathering metrics and firing alerts. Alertmanager is still an excellent choice to receive those alerts and resend them to different system components. We’ll keep using Jenkins as a tool that allows us to quickly write scripts that can interact with the system. Since we’re using AWS to host our cluster, Jenkins will have to interact with its API.</p>

<p class="calibre3">We are so close to the final objective that I feel we should skip the theory and jump straight into practical hands-on parts of the chapter. So, without further ado, we’ll create our cluster one more time.</p>

<h3 id="leanpub-auto-creating-a-cluster" class="calibre20">Creating A Cluster</h3>

<p class="calibre3">In the previous chapter, we already explored how to create a cluster without UI. The commands that follow should be familiar and, hopefully, should not require much explanation.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/7f49e5d1565b2234b84d8fe01e5c2356">15-self-adaptation-infra.sh</a> Gist.</p>

</aside>

<p class="calibre3">Please replace <code class="calibre19">[...]</code> with your keys before executing the commands that follow.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">export</code> <code class="nv">AWS_ACCESS_KEY_ID</code><code class="o">=[</code>...<code class="o">]</code>
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="nb">export</code> <code class="nv">AWS_SECRET_ACCESS_KEY</code><code class="o">=[</code>...<code class="o">]</code>
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code><code class="nb">export</code> <code class="nv">AWS_DEFAULT_REGION</code><code class="o">=</code>us-east-1
<code class="lineno"> 6 </code>
<code class="lineno"> 7 </code><code class="nb">export</code> <code class="nv">STACK_NAME</code><code class="o">=</code>devops22
<code class="lineno"> 8 </code>
<code class="lineno"> 9 </code><code class="nb">export</code> <code class="nv">KEY_NAME</code><code class="o">=</code>devops22
<code class="lineno">10 </code>
<code class="lineno">11 </code>aws cloudformation create-stack <code class="se">\</code>
<code class="lineno">12 </code>    --template-url https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker<code class="se">\</code>
<code class="lineno">13 </code>.tmpl <code class="se">\</code>
<code class="lineno">14 </code>    --capabilities CAPABILITY_IAM <code class="se">\</code>
<code class="lineno">15 </code>    --stack-name <code class="nv">$STACK_NAME</code> <code class="se">\</code>
<code class="lineno">16 </code>    --parameters <code class="se">\</code>
<code class="lineno">17 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>ManagerSize,ParameterValue<code class="o">=</code><code class="o">3</code> <code class="se">\</code>
<code class="lineno">18 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>ClusterSize,ParameterValue<code class="o">=</code><code class="o">0</code> <code class="se">\</code>
<code class="lineno">19 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>KeyName,ParameterValue<code class="o">=</code><code class="nv">$KEY_NAME</code> <code class="se">\</code>
<code class="lineno">20 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>EnableSystemPrune,ParameterValue<code class="o">=</code>yes <code class="se">\</code>
<code class="lineno">21 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>EnableCloudWatchLogs,ParameterValue<code class="o">=</code>no <code class="se">\</code>
<code class="lineno">22 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>EnableCloudStorEfs,ParameterValue<code class="o">=</code>yes <code class="se">\</code>
<code class="lineno">23 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>ManagerInstanceType,ParameterValue<code class="o">=</code>t2.small <code class="se">\</code>
<code class="lineno">24 </code>    <code class="nv">ParameterKey</code><code class="o">=</code>InstanceType,ParameterValue<code class="o">=</code>t2.small
</pre></div>

</figure>

<p class="calibre3">We defined a few environment variables and executed the <code class="calibre19">aws cloudformation create-stack</code> command that initiated creation of a cluster. It should take around five to ten minutes until it is finished.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws cloudformation describe-stacks <code class="se">\</code>
<code class="lineno">2 </code>    --stack-name <code class="nv">$STACK_NAME</code> <code class="calibre19">|</code> <code class="se">\</code>
<code class="lineno">3 </code>    jq -r <code class="s">".Stacks[0].StackStatus"</code>
</pre></div>

</figure>

<p class="calibre3">If the output of the <code class="calibre19">describe-stacks</code> command is <code class="calibre19">CREATE_COMPLETE</code>, our cluster is fully operational, and we can continue. Otherwise, please wait for a while longer and recheck the stack status.</p>

<p class="calibre3">Next, we’ll retrieve cluster DNS and public IP of one of the manager nodes and store those values as environment variables <code class="calibre19">CLUSTER_DNS</code> and <code class="calibre19">CLUSTER_IP</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nv">CLUSTER_DNS</code><code class="o">=</code><code class="k">$(</code>aws cloudformation <code class="se">\</code>
<code class="lineno"> 2 </code>    describe-stacks <code class="se">\</code>
<code class="lineno"> 3 </code>    --stack-name <code class="nv">$STACK_NAME</code> <code class="calibre19">|</code> <code class="se">\</code>
<code class="lineno"> 4 </code>    jq -r <code class="s">".Stacks[0].Outputs[] | \</code>
<code class="lineno"> 5 </code><code class="s">    select(.OutputKey==\"DefaultDNSTarget\")\</code>
<code class="lineno"> 6 </code><code class="s">    .OutputValue"</code><code class="k">)</code>
<code class="lineno"> 7 </code>
<code class="lineno"> 8 </code><code class="nv">CLUSTER_IP</code><code class="o">=</code><code class="k">$(</code>aws ec2 describe-instances <code class="se">\</code>
<code class="lineno"> 9 </code>    <code class="calibre19">|</code> jq -r <code class="s">".Reservations[] \</code>
<code class="lineno">10 </code><code class="s">    .Instances[] \</code>
<code class="lineno">11 </code><code class="s">    | select(.SecurityGroups[].GroupName \</code>
<code class="lineno">12 </code><code class="s">    | contains(\"</code><code class="nv">$STACK_NAME</code><code class="s">-ManagerVpcSG\"))\</code>
<code class="lineno">13 </code><code class="s">    .PublicIpAddress"</code> <code class="se">\</code>
<code class="lineno">14 </code>    <code class="calibre19">|</code> tail -n <code class="o">1</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">Once we enter the cluster, we’ll create a file that will hold the environment variables we’ll need inside the cluster. Those are the same variables we already defined on our host. We’ll output them so that we can easily copy and paste them when we enter one of the nodes.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"</code>
<code class="lineno">2 </code><code class="s">export CLUSTER_DNS=</code><code class="nv">$CLUSTER_DNS</code><code class="s"/>
<code class="lineno">3 </code><code class="s">export AWS_ACCESS_KEY_ID=</code><code class="nv">$AWS_ACCESS_KEY_ID</code><code class="s"/>
<code class="lineno">4 </code><code class="s">export AWS_SECRET_ACCESS_KEY=</code><code class="nv">$AWS_SECRET_ACCESS_KEY</code><code class="s"/>
<code class="lineno">5 </code><code class="s">export AWS_DEFAULT_REGION=</code><code class="nv">$AWS_DEFAULT_REGION</code><code class="s"/>
<code class="lineno">6 </code><code class="s">"</code>
</pre></div>

</figure>

<p class="calibre3">Please copy the output of the <code class="calibre19">echo</code> command. We’ll use it soon.</p>

<p class="calibre3">Now that we got all the cluster information we’ll need, we can <code class="calibre19">ssh</code> into one of the manager nodes.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
</pre></div>

</figure>

<p class="calibre3">Next, we’ll create a file that will hold all the information we’ll need. That way we’ll be able to get in and out of the cluster without losing the ability to retrieve that data quickly.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">echo</code> <code class="s">"</code>
<code class="lineno">2 </code><code class="s">export CLUSTER_DNS=[...]</code>
<code class="lineno">3 </code><code class="s">export AWS_ACCESS_KEY_ID=[...]</code>
<code class="lineno">4 </code><code class="s">export AWS_SECRET_ACCESS_KEY=[...]</code>
<code class="lineno">5 </code><code class="s">export AWS_DEFAULT_REGION=[...]</code>
<code class="lineno">6 </code><code class="s">"</code>&gt;creds
</pre></div>

</figure>

<p class="calibre3">Instead of typing the command from above, please type <code class="calibre19">echo "</code>, paste the output you copied a moment ago, and close it with <code class="calibre19">"&gt;creds</code>. The result should be four <code class="calibre19">export</code> commands inside the <code class="calibre19">creds</code> file.</p>

<p class="calibre3">Let’s download a script that will deploy (almost) all the services we used in the previous chapter.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl -o aws-services-15.sh <code class="se">\</code>
<code class="lineno">2 </code>    https://raw.githubusercontent.com/vfarcic/docker-flow-monitor/master/scripts<code class="se">\</code>
<code class="lineno">3 </code>/aws-services-15.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code>chmod +x aws-services-15.sh
</pre></div>

</figure>

<p class="calibre3">We download the script and gave it execute permissions.</p>

<p class="calibre3">Now we are ready to deploy the services.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">source</code> creds
<code class="lineno">2 </code>
<code class="lineno">3 </code>./aws-services-15.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code>docker stack ls
</pre></div>

</figure>

<p class="calibre3">Since <code class="calibre19">aws-services-15.sh</code> needs environment variable <code class="calibre19">CLUSTER_DNS</code>, we exported it by executing <code class="calibre19">source</code>. Further on, we executed the script and listed all the stacks deployed to the cluster. The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME                SERVICES
<code class="lineno">2 </code>exporter            3
<code class="lineno">3 </code>go-demo             2
<code class="lineno">4 </code>jenkins             2
<code class="lineno">5 </code>monitor             3
<code class="lineno">6 </code>proxy               2
</pre></div>

</figure>

<p class="calibre3">You’ll notice that the <code class="calibre19">logging</code> stack is missing. We did not deploy it since it is not relevant to the goals we’re trying to accomplish in this chapter and, at the same time, it requires extra nodes. Since I am committed towards not making you spend more money than needed, it seemed like a sensible thing not to deploy that stack.</p>

<p class="calibre3">Finally, let’s get out of the cluster and explore how we could scale it manually. That will give us an insight into the processes we’ll want to automate.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
</pre></div>

</figure>

<h3 id="leanpub-auto-scaling-nodes-manually" class="calibre20">Scaling Nodes Manually</h3>

<p class="calibre3">Let’s explore how we can scale nodes manually and, later on, try to apply the same logic to our automated processes.</p>

<p class="calibre3">We’re running the cluster in AWS which already has auto-scaling groups defined for both managers and workers. In such a setting, the most sensible way to scale the nodes is to change the desired capacity of those groups.</p>

<p class="calibre3">When new nodes are created by auto-scaling groups in <em class="calibre21">Docker For AWS</em> or <em class="calibre21">Azure</em>, they will join the cluster as managers or workers. If you choose not to use <em class="calibre21">Docker For AWS</em> or <em class="calibre21">Azure</em>, you’ll have to do some additional work to replicate the same functionality as the one we’re about to explore. You’ll have to create init scripts that will find IP of one of the managers, retrieve join token, and, finally, execute <code class="calibre19">docker swarm join</code> command.</p>

<aside class="tip">
    <p class="calibre3">If your hosting vendor does not provide functionality similar to auto-scaling groups, you might need to create new nodes using tools like <a href="https://www.terraform.io/">Terraform</a>.</p>

</aside>

<p class="calibre3">No matter which hosting vendor you’re using, the logic should, more or less, be always the same. We need to change the number of running managers or workers and, in case that number increased, join new nodes to the cluster. I am confident that you’ll be able to modify the logic that follows to your cluster setup.</p>

<p class="calibre3">The first thing we need to do is find out the name of the auto-scaling group created for our cluster. A good start is to list all the groups by executing <code class="calibre19">aws autoscaling describe-auto-scaling-groups</code> command.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    describe-auto-scaling-groups <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> jq <code class="s">"."</code>
</pre></div>

</figure>

<p class="calibre3">The output is too big to be presented in a book format, and we do not need it in its entirety. Therefore, we’ll limit the output. Luckily, we know that the name of the auto-scaling group starts with <code class="calibre19">[STACK_NAME]-Node</code>. We can use that to filter the output.</p>

<p class="calibre3">A command that will retrieve only the auto-scaling group assigned to worker nodes and retrieve just the name of the group is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    describe-auto-scaling-groups <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> jq -r <code class="s">".AutoScalingGroups[] \</code>
<code class="lineno">4 </code><code class="s">    | select(.AutoScalingGroupName \</code>
<code class="lineno">5 </code><code class="s">    | startswith(\"</code><code class="nv">$STACK_NAME</code><code class="s">-NodeAsg-\"))\</code>
<code class="lineno">6 </code><code class="s">    .AutoScalingGroupName"</code>
</pre></div>

</figure>

<p class="calibre3">We used <code class="calibre19">jq</code> to retrieve all data within the root node <code class="calibre19">AutoScalingGroups</code>. Further on, we used <code class="calibre19">select</code> command to retrieve only records with <code class="calibre19">AutoScalingGroupName</code> that starts with <code class="calibre19">[STACK_NAME]-Node</code>. Finally, we limited the output further so that only the name of the name of the group is retrieved.</p>

<p class="calibre3">The output will vary from one case to another. It should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>devops22-NodeAsg-1J93DRR7VYUHU
</pre></div>

</figure>

<p class="calibre3">We cannot change the auto-scaling group desired capacity without knowing what the current number of nodes is. Therefore, we need to construct another query that will provide that information. Fortunately, the command is very similar since all we need is to retrieve a different value based on the same filter.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    describe-auto-scaling-groups <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> jq -r <code class="s">".AutoScalingGroups[] \</code>
<code class="lineno">4 </code><code class="s">    | select(.AutoScalingGroupName \</code>
<code class="lineno">5 </code><code class="s">    | startswith(\"</code><code class="nv">$STACK_NAME</code><code class="s">-NodeAsg-\"))\</code>
<code class="lineno">6 </code><code class="s">    .DesiredCapacity"</code>
</pre></div>

</figure>

<p class="calibre3">When compared with the previous command, the only change is that, this time, we retrieved <code class="calibre19">DesiredCapacity</code> instead <code class="calibre19">AutoScalingGroupName</code>. The output is <code class="calibre19">0</code>. That should come as no surprise since we specified that we did not want any workers when we created the cluster.</p>

<p class="calibre3">We’ll repeat the command we used to retrieve the name of the auto-scaling group and, this time, we’ll put the result as a value of an environment variable. That way we’ll be able to reuse it across the commands we’ll execute later on.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">ASG_NAME</code><code class="o">=</code><code class="k">$(</code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    describe-auto-scaling-groups <code class="se">\</code>
<code class="lineno">3 </code>    <code class="calibre19">|</code> jq -r <code class="s">".AutoScalingGroups[] \</code>
<code class="lineno">4 </code><code class="s">    | select(.AutoScalingGroupName \</code>
<code class="lineno">5 </code><code class="s">    | startswith(\"</code><code class="nv">$STACK_NAME</code><code class="s">-NodeAsg-\"))\</code>
<code class="lineno">6 </code><code class="s">    .AutoScalingGroupName"</code><code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">Now that we have the name of the auto-scaling group, we can increase the desired capacity from <code class="calibre19">0</code> to <code class="calibre19">1</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    update-auto-scaling-group <code class="se">\</code>
<code class="lineno">3 </code>    --auto-scaling-group-name <code class="nv">$ASG_NAME</code> <code class="se">\</code>
<code class="lineno">4 </code>    --desired-capacity <code class="o">1</code>
</pre></div>

</figure>

<p class="calibre3">Let’s confirm that the capacity is indeed increased.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    describe-auto-scaling-groups <code class="se">\</code>
<code class="lineno">3 </code>    --auto-scaling-group-names <code class="nv">$ASG_NAME</code> <code class="se">\</code>
<code class="lineno">4 </code>    <code class="calibre19">|</code> jq <code class="s">".AutoScalingGroups[0]\</code>
<code class="lineno">5 </code><code class="s">    .DesiredCapacity"</code>
</pre></div>

</figure>

<p class="calibre3">We executed <code class="calibre19">describe-auto-scaling-groups</code> one more time. However, since now we know the name of the group, there was no need for <code class="calibre19">jq</code> filters.</p>

<p class="calibre3">As expected, the output is <code class="calibre19">1</code> confirming that the update indeed worked.</p>

<p class="calibre3">The fact that the desired capacity of the group was updated does not necessarily mean that a new node was created. We can check that easily by executing <code class="calibre19">ec2 describe-instances</code> combined with a bit of <code class="calibre19">jq</code> magic.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws ec2 describe-instances <code class="calibre19">|</code> jq -r <code class="se">\</code>
<code class="lineno">2 </code>    <code class="s">".Reservations[].Instances[] \</code>
<code class="lineno">3 </code><code class="s">    | select(.SecurityGroups[].GroupName \</code>
<code class="lineno">4 </code><code class="s">    | startswith(\"</code><code class="nv">$STACK_NAME</code><code class="s">-NodeVpcSG\"))\</code>
<code class="lineno">5 </code><code class="s">    .InstanceId"</code>
</pre></div>

</figure>

<p class="calibre3">We executed <code class="calibre19">ec2 describe-instances</code> and used <code class="calibre19">jq</code> to retrieve all instances, filter them by the security group which has a name that starts with a predictable string, and retrieved the ID of the only worker instance. The output should be similar to the one that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>i-06f7e78c063fedeb3
</pre></div>

</figure>

<p class="calibre3">Creation of an EC2 instance is fast. What takes a bit of time is its initialization. We should check its status and confirm that it finished initializing.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nv">INSTANCE_ID</code><code class="o">=</code><code class="k">$(</code>aws ec2 <code class="se">\</code>
<code class="lineno"> 2 </code>    describe-instances <code class="calibre19">|</code> jq -r <code class="se">\</code>
<code class="lineno"> 3 </code>    <code class="s">".Reservations[].Instances[] \</code>
<code class="lineno"> 4 </code><code class="s">    | select(.SecurityGroups[].GroupName \</code>
<code class="lineno"> 5 </code><code class="s">    | startswith(\"</code><code class="nv">$STACK_NAME</code><code class="s">-NodeVpcSG\"))\</code>
<code class="lineno"> 6 </code><code class="s">    .InstanceId"</code><code class="k">)</code>
<code class="lineno"> 7 </code>
<code class="lineno"> 8 </code>aws ec2 describe-instance-status <code class="se">\</code>
<code class="lineno"> 9 </code>    --instance-ids <code class="nv">$INSTANCE_ID</code> <code class="se">\</code>
<code class="lineno">10 </code>    <code class="calibre19">|</code> jq -r <code class="s">".InstanceStatuses[0]\</code>
<code class="lineno">11 </code><code class="s">    .InstanceStatus.Status"</code>
</pre></div>

</figure>

<p class="calibre3">We repeated the previous command but, this time, stored the instance ID as the environment variable <code class="calibre19">INSTANCE_ID</code>. Later on, we used it with the <code class="calibre19">ec2 describe-instance-status</code> command to retrieve the status.</p>

<p class="calibre3">If the output is <code class="calibre19">ok</code>, the new node is created, is initialized, and (probably) joined the cluster. Otherwise, please wait for a minute or two and recheck the status.</p>

<p class="calibre3">Finally, let’s confirm that the new node indeed joined the Swarm cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">exit</code>
</pre></div>

</figure>

<p class="calibre3">We entered one of the manager servers, listed all the nodes of the cluster, and returned to the host.</p>

<p class="calibre3">The output of the <code class="calibre19">node ls</code> command is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS
<code class="lineno">2 </code>ip-172-31-40-169.us-east-2.compute.internal Ready  Active
<code class="lineno">3 </code>ip-172-31-24-32.us-east-2.compute.internal  Ready  Active       Reachable
<code class="lineno">4 </code>ip-172-31-2-29.us-east-2.compute.internal   Ready  Active       Leader
<code class="lineno">5 </code>ip-172-31-42-64.us-east-2.compute.internal  Ready  Active       Reachable
</pre></div>

</figure>

<p class="calibre3">That’s brilliant! The new worker joined the cluster, and our capacity increased. If, in your case, the new node did not yet join the cluster, please wait for a few moments and list the nodes again.</p>


<figure class="image1">
  <img src="../images/00080.jpeg" alt="Figure 15-1: Manual updates of Auto-Scaling Groups" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-1: Manual updates of Auto-Scaling Groups</figcaption>
</figure>


<p class="calibre3">There are a few other manual actions we should explore before we move towards automation. But, before we proceed, we’ll change the auto-scaling group one more time. We’ll set the desired capacity back to <code class="calibre19">0</code>. That way we’ll not only confirm that the process works in both directions, but also save a bit of money by not running more nodes than we need.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>aws autoscaling <code class="se">\</code>
<code class="lineno">2 </code>    update-auto-scaling-group <code class="se">\</code>
<code class="lineno">3 </code>    --auto-scaling-group-name <code class="nv">$ASG_NAME</code> <code class="se">\</code>
<code class="lineno">4 </code>    --desired-capacity <code class="o">0</code>
</pre></div>

</figure>

<p class="calibre3">We updated the auto-scaling group back to the desired capacity of <code class="calibre19">0</code>.</p>

<p class="calibre3">After a while, we can return to the cluster and confirm that the worker is removed from the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">The output of the <code class="calibre19">node ls</code> command is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS
<code class="lineno">2 </code>ip-172-31-40-169.us-east-2.compute.internal Down   Active
<code class="lineno">3 </code>ip-172-31-24-32.us-east-2.compute.internal  Ready  Active       Reachable
<code class="lineno">4 </code>ip-172-31-2-29.us-east-2.compute.internal   Ready  Active       Leader
<code class="lineno">5 </code>ip-172-31-42-64.us-east-2.compute.internal  Ready  Active       Reachable
</pre></div>

</figure>

<p class="calibre3">As you can see, the status of the worker node is set to <code class="calibre19">Down</code>. Swarm lost communication with the node once the auto-scaling group shut it down and changed its status. Soon it will remove it completely from its records.</p>

<p class="calibre3">We still have one more problem to solve. We cannot run <code class="calibre19">aws</code> commands from the cluster. <em class="calibre21">Docker For AWS</em> does not let us install any additional software. Even if we would find the way to install the CLI, we should not pollute our production servers. Instead, we should run any tool we need thorough a container.</p>

<p class="calibre3">Since there is no official AWS CLI Docker image, I created one for the exercises in this chapter. Let’s take a look at the Dockerfile.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl <code class="s">"https://raw.githubusercontent.com/vfarcic/docker-aws-cli/master/Dockerfile"</code>
</pre></div>

</figure>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>FROM alpine
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>MAINTAINER Viktor Farcic &lt;viktor@farcic.com&gt;
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>RUN apk --update add python py-pip jq &amp;&amp; \
<code class="lineno"> 6 </code>    pip install awscli &amp;&amp; \
<code class="lineno"> 7 </code>    apk del py-pip &amp;&amp; \
<code class="lineno"> 8 </code>    rm -rf /var/cache/apk/*
<code class="lineno"> 9 </code>
<code class="lineno">10 </code>ENV AWS_ACCESS_KEY_ID ""
<code class="lineno">11 </code>ENV AWS_SECRET_ACCESS_KEY ""
<code class="lineno">12 </code>ENV AWS_DEFAULT_REGION "us-east-1"
</pre></div>

</figure>

<p class="calibre3">As you can see, it’s pretty straightforward. The image is based on <code class="calibre19">alpine</code> and installs <code class="calibre19">python</code>, <code class="calibre19">py-pip</code>, and <code class="calibre19">jq</code>. We’re installing Python since <code class="calibre19">pip</code> is the easiest way to install <code class="calibre19">awscli</code>. The rest of the image specification defines a few environment variables required by the AWS CLI. The image was built and pushed as <code class="calibre19">vfarcic/aws-cli</code>.</p>

<p class="calibre3">Let’s do a test run of a container based on the image.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">source</code> creds
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker container run --rm <code class="se">\</code>
<code class="lineno">4 </code>    -e <code class="nv">AWS_ACCESS_KEY_ID</code><code class="o">=</code><code class="nv">$AWS_ACCESS_KEY_ID</code> <code class="se">\</code>
<code class="lineno">5 </code>    -e <code class="nv">AWS_SECRET_ACCESS_KEY</code><code class="o">=</code><code class="nv">$AWS_SECRET_ACCESS_KEY</code> <code class="se">\</code>
<code class="lineno">6 </code>    -e <code class="nv">AWS_DEFAULT_REGION</code><code class="o">=</code><code class="nv">$AWS_DEFAULT_REGION</code> <code class="se">\</code>
<code class="lineno">7 </code>    vfarcic/aws-cli <code class="se">\</code>
<code class="lineno">8 </code>    aws ec2 describe-instances
</pre></div>

</figure>

<p class="calibre3">We sourced the <code class="calibre19">creds</code> file that contains the environment variables we need. Further on we run a container based on the <code class="calibre19">vfarcic/aws-cli</code> image. We used <code class="calibre19">aws ec2 describe-instances</code> as the command only to demonstrate that any <code class="calibre19">aws</code> command could be executed through a container. The result should be information about all the EC2 nodes we have in that region.</p>

<p class="calibre3">We’re using the <code class="calibre19">creds</code> file only as a convenience and for demo purposes since we cannot inject a secret into a container. It must be a Swarm service.</p>

<aside class="tip">
    <p class="calibre3">Do not keep credentials stored in files on servers. That is a huge security risk. Instead, store them as Docker secrets.</p>

</aside>

<p class="calibre3">The <code class="calibre19">docker container run</code> command we executed is too long to remember. We can mitigate that by creating a Docker Compose YAML file with all the <code class="calibre19">aws</code> commands we need. An example of such a file can be found in the <a href="https://github.com/vfarcic/docker-aws-cli">vfarcic/docker-aws-cli</a> repository. It contains all the commands we’ll use in this chapter.</p>

<p class="calibre3">Let’s take a brief look at it.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl <code class="s">"https://raw.githubusercontent.com/vfarcic/docker-aws-cli/master/docker-com\</code>
<code class="lineno">2 </code><code class="s">pose.yml"</code>
</pre></div>

</figure>

<p class="calibre3">The output is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>version: '3.2'
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>services:
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>  asg-name:
<code class="lineno"> 6 </code>    image: vfarcic/aws-cli
<code class="lineno"> 7 </code>    environment:
<code class="lineno"> 8 </code>      - AWS_ACCESS_KEY_ID=<code class="cp">${</code><code class="calibre19">AWS_ACCESS_KEY_ID</code><code class="cp">}</code>
<code class="lineno"> 9 </code>      - AWS_SECRET_ACCESS_KEY=<code class="cp">${</code><code class="calibre19">AWS_SECRET_ACCESS_KEY</code><code class="cp">}</code>
<code class="lineno">10 </code>      - AWS_DEFAULT_REGION=<code class="cp">${</code><code class="calibre19">AWS_DEFAULT_REGION</code><code class="cp">}</code>
<code class="lineno">11 </code>    command: sh -c "aws autoscaling describe-auto-scaling-groups | jq -r '.AutoS\
<code class="lineno">12 </code>calingGroups[] | select(.AutoScalingGroupName | startswith(\"<code class="cp">${</code><code class="calibre19">STACK_NAME</code><code class="cp">}</code>--Node\
<code class="lineno">13 </code>Asg\")).AutoScalingGroupName'"
<code class="lineno">14 </code>
<code class="lineno">15 </code>  asg-desired-capacity:
<code class="lineno">16 </code>    image: vfarcic/aws-cli
<code class="lineno">17 </code>    environment:
<code class="lineno">18 </code>      - AWS_ACCESS_KEY_ID=<code class="cp">${</code><code class="calibre19">AWS_ACCESS_KEY_ID</code><code class="cp">}</code>
<code class="lineno">19 </code>      - AWS_SECRET_ACCESS_KEY=<code class="cp">${</code><code class="calibre19">AWS_SECRET_ACCESS_KEY</code><code class="cp">}</code>
<code class="lineno">20 </code>      - AWS_DEFAULT_REGION=<code class="cp">${</code><code class="calibre19">AWS_DEFAULT_REGION</code><code class="cp">}</code>
<code class="lineno">21 </code>    command: sh -c "aws autoscaling describe-auto-scaling-groups --auto-scaling-\
<code class="lineno">22 </code>group-names <code class="nv">$ASG_NAME</code> | jq '.AutoScalingGroups[0].DesiredCapacity'"
<code class="lineno">23 </code>
<code class="lineno">24 </code>  asg-update-desired-capacity:
<code class="lineno">25 </code>    image: vfarcic/aws-cli
<code class="lineno">26 </code>    environment:
<code class="lineno">27 </code>      - AWS_ACCESS_KEY_ID=<code class="cp">${</code><code class="calibre19">AWS_ACCESS_KEY_ID</code><code class="cp">}</code>
<code class="lineno">28 </code>      - AWS_SECRET_ACCESS_KEY=<code class="cp">${</code><code class="calibre19">AWS_SECRET_ACCESS_KEY</code><code class="cp">}</code>
<code class="lineno">29 </code>      - AWS_DEFAULT_REGION=<code class="cp">${</code><code class="calibre19">AWS_DEFAULT_REGION</code><code class="cp">}</code>
<code class="lineno">30 </code>    command: sh -c "aws autoscaling update-auto-scaling-group --auto-scaling-gro\
<code class="lineno">31 </code>up-name <code class="nv">$ASG_NAME</code> --desired-capacity <code class="nv">$ASG_DESIRED_CAPACITY</code>"
</pre></div>

</figure>

<p class="calibre3">We won’t go into details of the services defined in that YAML file. It should be self explanatory what each of them does.</p>

<p class="calibre3">Please note that <code class="calibre19">docker-compose</code> is not installed on the nodes of the cluster. We will not need it since the plan is to use those Compose services through Jenkins agents which will have Docker Compose.</p>

<p class="calibre3">Let’s move on and explore how to transform the commands we used so far into automated scaling solution.</p>

<h3 id="leanpub-auto-creating-scaling-job" class="calibre20">Creating Scaling Job</h3>

<p class="calibre3">Let’s try to translate the commands we executed manually into a Jenkins job. If we manage to do that, we can go further and let Alertmanager trigger that job whenever certain thresholds are reached in Prometheus.</p>

<p class="calibre3">We’ll start by downloading Jenkins stack from the <a href="http://github.com/vfarcic/docker-flow-monitor">vfarcic/docker-flow-monitor</a> repository.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl -o jenkins.yml <code class="se">\</code>
<code class="lineno">2 </code>    https://raw.githubusercontent.com/vfarcic/docker-flow-monitor/master/stacks/<code class="se">\</code>
<code class="lineno">3 </code>jenkins-aws-secret.yml
<code class="lineno">4 </code>
<code class="lineno">5 </code>cat jenkins.yml
</pre></div>

</figure>

<p class="calibre3">The stack definition we just downloaded is almost identical to the one we used before so we’ll comment only the differences.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>version: "3.2"
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>services:
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>  ...
<code class="lineno"> 6 </code>
<code class="lineno"> 7 </code>  agent:
<code class="lineno"> 8 </code>    image: vfarcic/jenkins-swarm-agent
<code class="lineno"> 9 </code>    ...
<code class="lineno">10 </code>    secrets:
<code class="lineno">11 </code>      - aws
<code class="lineno">12 </code>      ...
<code class="lineno">13 </code>
<code class="lineno">14 </code>secrets:
<code class="lineno">15 </code>  aws:
<code class="lineno">16 </code>    external: true
<code class="lineno">17 </code>  ...
</pre></div>

</figure>

<p class="calibre3">The only new addition to the Jenkins stack is the <code class="calibre19">aws</code> secret. It should contain AWS keys and the region we’ll need for AWS CLI. So, let’s start by creating the secret.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">source</code> creds
<code class="lineno">2 </code>
<code class="lineno">3 </code><code class="nb">echo</code> <code class="s">"</code>
<code class="lineno">4 </code><code class="s">export AWS_ACCESS_KEY_ID=</code><code class="nv">$AWS_ACCESS_KEY_ID</code><code class="s"/>
<code class="lineno">5 </code><code class="s">export AWS_SECRET_ACCESS_KEY=</code><code class="nv">$AWS_SECRET_ACCESS_KEY</code><code class="s"/>
<code class="lineno">6 </code><code class="s">export AWS_DEFAULT_REGION=</code><code class="nv">$AWS_DEFAULT_REGION</code><code class="s"/>
<code class="lineno">7 </code><code class="s">export STACK_NAME=devops22</code>
<code class="lineno">8 </code><code class="s">"</code> <code class="calibre19">|</code> docker secret create aws -
</pre></div>

</figure>

<p class="calibre3">We sourced the <code class="calibre19">creds</code> file and used the environment variables to construct the <code class="calibre19">aws</code> secret.</p>

<p class="calibre3">Now we can deploy the <code class="calibre19">jenkins</code> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c jenkins.yml jenkins
<code class="lineno">3 </code>
<code class="lineno">4 </code><code class="nb">exit</code>
</pre></div>

</figure>

<p class="calibre3">We deployed the stack and exited the cluster.</p>

<p class="calibre3">Jenkins has a small nuance with its URL. If we do not change anything, it will not know what its address is and, when we construct notification messages, it’ll resolve itself to <code class="calibre19">null</code>. Fortunately, the fix is reasonably easy. All we have to do is open the configuration page and click the <em class="calibre21">Save</em> button.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Please login using <em class="calibre21">admin</em> as both the <em class="calibre21">User</em> and the <em class="calibre21">Password</em>. Once you’re authenticated, you’ll see the configuration screen which, among other fields, contains <em class="calibre21">Jenkins URL</em>. Please confirm that it is correct and click the <em class="calibre21">Save</em> button.</p>


<figure class="image">
  <img src="../images/00081.jpeg" alt="Figure 15-2: Jenkins URL configuration" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-2: Jenkins URL configuration</figcaption>
</figure>


<p class="calibre3">Now that we resolved Jenkins’ identity crisis, we can create a new job capable of scaling nodes of the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/view/all/newJob"</code>
</pre></div>

</figure>

<p class="calibre3">Please type <em class="calibre21">aws-scale</em> as the job name, select <em class="calibre21">Pipeline</em> as the job type, and click the <em class="calibre21">OK</em> button. You’ll see the job configuration screen.</p>

<p class="calibre3">Since we’re planning to trigger builds remotely, we should create an authentication token. Please click the <em class="calibre21">Build Triggers</em> tab, select the <em class="calibre21">Trigger builds remotely</em> checkbox, and type <em class="calibre21">DevOps22</em> as the <em class="calibre21">Authentication Token</em>.</p>


<figure class="image">
  <img src="../images/00082.jpeg" alt="Figure 15-3: Jenkins job build triggers" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-3: Jenkins job build triggers</figcaption>
</figure>


<p class="calibre3">Now we’re ready to define a pipeline script.</p>

<p class="calibre3">Please click the <em class="calibre21">Pipeline</em> tab and type the script that follows in the <em class="calibre21">Pipeline Script</em> field.</p>

<aside class="tip">
    <p class="calibre3">If you’re not thrilled with the prospect of typing, feel free to copy and paste the script from the <a href="https://gist.github.com/vfarcic/03b302f7ce31cf3bafb0529da7601126">15-self-adaptation-infra-jenkins-pipeline-01.groovy</a> gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">pipeline</code> <code class="o">{</code>
<code class="lineno"> 2 </code>  <code class="calibre19">agent</code> <code class="o">{</code>
<code class="lineno"> 3 </code>    <code class="calibre19">label</code> <code class="s">"prod"</code>
<code class="lineno"> 4 </code>  <code class="o">}</code>
<code class="lineno"> 5 </code>  <code class="calibre19">options</code> <code class="o">{</code>
<code class="lineno"> 6 </code>    <code class="calibre19">buildDiscarder</code><code class="o">(</code><code class="calibre19">logRotator</code><code class="o">(</code><code class="nl">numToKeepStr:</code> <code class="s">'2'</code><code class="o">))</code>
<code class="lineno"> 7 </code>    <code class="calibre19">disableConcurrentBuilds</code><code class="o">()</code>
<code class="lineno"> 8 </code>  <code class="o">}</code>
<code class="lineno"> 9 </code>  <code class="calibre19">parameters</code> <code class="o">{</code>
<code class="lineno">10 </code>    <code class="calibre19">string</code><code class="o">(</code>
<code class="lineno">11 </code>      <code class="nl">name:</code> <code class="s">"scale"</code><code class="o">,</code>
<code class="lineno">12 </code>      <code class="nl">defaultValue:</code> <code class="s">"1"</code><code class="o">,</code>
<code class="lineno">13 </code>      <code class="nl">description:</code> <code class="s">"The number of worker nodes to add or remove"</code>
<code class="lineno">14 </code>    <code class="o">)</code>
<code class="lineno">15 </code>  <code class="o">}</code>
<code class="lineno">16 </code>  <code class="calibre19">stages</code> <code class="o">{</code>
<code class="lineno">17 </code>    <code class="calibre19">stage</code><code class="o">(</code><code class="s">"scale"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">18 </code>      <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno">19 </code>        <code class="calibre19">git</code> <code class="s">"https://github.com/vfarcic/docker-aws-cli.git"</code>
<code class="lineno">20 </code>        <code class="calibre19">script</code> <code class="o">{</code>
<code class="lineno">21 </code>          <code class="kt">def</code> <code class="calibre19">asgName</code> <code class="o">=</code> <code class="calibre19">sh</code><code class="o">(</code>
<code class="lineno">22 </code>            <code class="nl">script:</code> <code class="s">"source /run/secrets/aws &amp;&amp; docker-compose run --rm asg-name\</code>
<code class="lineno">23 </code><code class="s">"</code><code class="o">,</code>
<code class="lineno">24 </code>            <code class="nl">returnStdout:</code> <code class="k">true</code>
<code class="lineno">25 </code>          <code class="o">).</code><code class="na">trim</code><code class="o">()</code>
<code class="lineno">26 </code>          <code class="k">if</code> <code class="o">(</code><code class="calibre19">asgName</code> <code class="o">==</code> <code class="s">""</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">27 </code>            <code class="calibre19">error</code> <code class="s">"Could not find auto-scaling group"</code>
<code class="lineno">28 </code>          <code class="o">}</code>
<code class="lineno">29 </code>          <code class="kt">def</code> <code class="calibre19">asgDesiredCapacity</code> <code class="o">=</code> <code class="calibre19">sh</code><code class="o">(</code>
<code class="lineno">30 </code>            <code class="nl">script:</code> <code class="s">"source /run/secrets/aws &amp;&amp; ASG_NAME=${asgName} docker-compo\</code>
<code class="lineno">31 </code><code class="s">se run --rm asg-desired-capacity"</code><code class="o">,</code>
<code class="lineno">32 </code>            <code class="nl">returnStdout:</code> <code class="k">true</code>
<code class="lineno">33 </code>          <code class="o">).</code><code class="na">trim</code><code class="o">().</code><code class="na">toInteger</code><code class="o">()</code>
<code class="lineno">34 </code>          <code class="kt">def</code> <code class="calibre19">asgNewCapacity</code> <code class="o">=</code> <code class="calibre19">asgDesiredCapacity</code> <code class="o">+</code> <code class="calibre19">scale</code><code class="o">.</code><code class="na">toInteger</code><code class="o">()</code>
<code class="lineno">35 </code>          <code class="k">if</code> <code class="o">(</code><code class="calibre19">asgNewCapacity</code> <code class="o">&lt;</code> <code class="o">1</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">36 </code>            <code class="calibre19">error</code> <code class="s">"The number of worker nodes is already at the minimum capacity\</code>
<code class="lineno">37 </code><code class="s"> of 1"</code>
<code class="lineno">38 </code>          <code class="o">}</code> <code class="k">else</code> <code class="k">if</code> <code class="o">(</code><code class="calibre19">asgNewCapacity</code> <code class="o">&gt;</code> <code class="o">3</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">39 </code>            <code class="calibre19">error</code> <code class="s">"The number of worker nodes is already at the maximum capacity\</code>
<code class="lineno">40 </code><code class="s"> of 3"</code>
<code class="lineno">41 </code>          <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
<code class="lineno">42 </code>            <code class="calibre19">sh</code> <code class="s">"source /run/secrets/aws &amp;&amp; ASG_NAME=${asgName} ASG_DESIRED_CAPAC\</code>
<code class="lineno">43 </code><code class="s">ITY=${asgNewCapacity} docker-compose run --rm asg-update-desired-capacity"</code>
<code class="lineno">44 </code>            <code class="calibre19">echo</code> <code class="s">"Changed the number of worker nodes from ${asgDesiredCapacity} \</code>
<code class="lineno">45 </code><code class="s">to ${asgNewCapacity}"</code>
<code class="lineno">46 </code>          <code class="o">}</code>
<code class="lineno">47 </code>        <code class="o">}</code>
<code class="lineno">48 </code>      <code class="o">}</code>
<code class="lineno">49 </code>    <code class="o">}</code>
<code class="lineno">50 </code>  <code class="o">}</code>
<code class="lineno">51 </code>  <code class="calibre19">post</code> <code class="o">{</code>
<code class="lineno">52 </code>    <code class="calibre19">success</code> <code class="o">{</code>
<code class="lineno">53 </code>      <code class="calibre19">slackSend</code><code class="o">(</code>
<code class="lineno">54 </code>        <code class="nl">color:</code> <code class="s">"good"</code><code class="o">,</code>
<code class="lineno">55 </code>        <code class="nl">message:</code> <code class="s">"""Worker nodes were scaled.</code>
<code class="lineno">56 </code><code class="s">Please check Jenkins logs for the job ${env.JOB_NAME} #${env.BUILD_NUMBER}</code>
<code class="lineno">57 </code><code class="s">${env.BUILD_URL}console"""</code>
<code class="lineno">58 </code>      <code class="o">)</code>
<code class="lineno">59 </code>    <code class="o">}</code>
<code class="lineno">60 </code>    <code class="calibre19">failure</code> <code class="o">{</code>
<code class="lineno">61 </code>      <code class="calibre19">slackSend</code><code class="o">(</code>
<code class="lineno">62 </code>        <code class="nl">color:</code> <code class="s">"danger"</code><code class="o">,</code>
<code class="lineno">63 </code>        <code class="nl">message:</code> <code class="s">"""Worker nodes could not be scaled.</code>
<code class="lineno">64 </code><code class="s">Please check Jenkins logs for the job ${env.JOB_NAME} #${env.BUILD_NUMBER}</code>
<code class="lineno">65 </code><code class="s">${env.BUILD_URL}console"""</code>
<code class="lineno">66 </code>      <code class="o">)</code>
<code class="lineno">67 </code>    <code class="o">}</code>
<code class="lineno">68 </code>  <code class="o">}</code>
<code class="lineno">69 </code><code class="o">}</code>
</pre></div>

</figure>

<p class="calibre3">You should be able to understand most of the Pipeline without any help so I’ll limit the discussion on the steps of the <code class="calibre19">scale</code> stage.</p>

<p class="calibre3">We start by cloning the <code class="calibre19">vfarcic/docker-aws-cli</code> repository that contains <code class="calibre19">docker-compose.yml</code> file with AWS CLI services we’ll need.</p>

<p class="calibre3">Next, we’re executing Docker Compose service <code class="calibre19">asg-name</code> that retrieves the name of the auto-scaling group associated with worker nodes. The result is stored in the variable <code class="calibre19">asgName</code>. Since all the services defined in that Compose file require environment variables with AWS keys and the region where the cluster is running, we’re executing <code class="calibre19">source /run/secrets/aws</code> before <code class="calibre19">docker-compose</code> commands. The file was injected as the Docker secret <code class="calibre19">aws</code>.</p>

<p class="calibre3">Further on, we’re retrieving the current desired capacity. The new capacity is calculated by adding the value of the <code class="calibre19">scale</code> parameter to the current capacity. Finally, we have a simple <code class="calibre19">if/else</code> statement that throws an error if the future capacity would be lower than <code class="calibre19">1</code> or higher than <code class="calibre19">3</code> nodes. That way we are setting boundaries so that the system cannot expand or contract too much. You should change those limits to better match your current size of the cluster.</p>

<p class="calibre3">Finally, if the new capacity is within the boundaries, we are updating the auto-scaling group.</p>

<p class="calibre3">As you can see, the script is relatively simple and straightforward. Even though this might not be the final version that fits everyone’s purposes, the general gist is there, and I’m confident that you’ll have no problem adapting it to suit your needs.</p>

<p class="calibre3">Do not forget to click the <em class="calibre21">Save</em> button before moving forward.</p>

<p class="calibre3">Let’s give the job a spin.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/blue/organizations/jenkins/aws-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">You should see the <code class="calibre19">aws-scale</code> screen with the <em class="calibre21">Run</em> button in the middle. Please click it.</p>

<p class="calibre3">We already discussed the bug that makes the first build of a Pipeline job with properties fail. All subsequent builds should work properly, so we’ll give it another try.</p>

<p class="calibre3">Please reload the page and click the <em class="calibre21">Run</em> button. You’ll be presented with a screen with a single parameter that allows us to specify how many nodes we’d like to add or remove. Leave the default value of <code class="calibre19">1</code> and click the <em class="calibre21">Run</em> button. A new build will start.</p>

<p class="calibre3">Please click on the row that represents the new build and explore it. The second to last step should state that the number of workers changed from <code class="calibre19">0</code> to <code class="calibre19">1</code>.</p>


<figure class="image1">
  <img src="../images/00083.jpeg" alt="Figure 15-4: Jenkins build results" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-4: Jenkins build results</figcaption>
</figure>


<p class="calibre3">As you saw before, it takes a minute or two until a new node is created and initialized. Fetch a coffee. By the time you come back, the new node will be fully operational within the cluster.</p>

<p class="calibre3">Let’s enter one of the manager nodes and confirm that the new node joined the Swarm cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS
<code class="lineno">2 </code>ip-172-31-31-102.us-east-2.compute.internal Ready  Active       Reachable
<code class="lineno">3 </code>ip-172-31-33-251.us-east-2.compute.internal Ready  Active
<code class="lineno">4 </code>ip-172-31-34-254.us-east-2.compute.internal Ready  Active       Reachable
<code class="lineno">5 </code>ip-172-31-7-121.us-east-2.compute.internal  Ready  Active       Leader
</pre></div>

</figure>

<p class="calibre3">As you can see, the new worker indeed joined the cluster. In your cluster, the new node might not yet be initialized. If that’s the case, please wait for a minute or two and re-execute <code class="calibre19">docker node ls</code>.</p>


<figure class="image">
  <img src="../images/00084.jpeg" alt="Figure 15-5: Infrastructure scaling orchestrated by Jenkins" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-5: Infrastructure scaling orchestrated by Jenkins</figcaption>
</figure>


<p class="calibre3">UI is useful as a learning experience, but our goal is to trigger the job remotely. Let’s check whether we can send a <code class="calibre19">POST</code> request.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl -XPOST -i <code class="se">\</code>
<code class="lineno">4 </code>    <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?token=DevOps2\</code>
<code class="lineno">5 </code><code class="s">2&amp;scale=2"</code>
</pre></div>

</figure>

<p class="calibre3">We exited the cluster and sent a post request with the token and the <code class="calibre19">scale</code> parameter set to <code class="calibre19">2</code>. The response is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HTTP/1.1 201 Created
<code class="lineno">2 </code>Connection: close
<code class="lineno">3 </code>Date: Sat, 23 Sep 2017 20:07:21 GMT
<code class="lineno">4 </code>X-Content-Type-Options: nosniff
<code class="lineno">5 </code>Location: http://devops22-ExternalL-1OG8BA7IMZCT0-900324820.us-east-2.elb.amazon\
<code class="lineno">6 </code>aws.com/jenkins/queue/item/5/
<code class="lineno">7 </code>Server: Jetty(9.4.z-SNAPSHOT)
</pre></div>

</figure>

<p class="calibre3">Let’s confirm that Jenkins build was executed successfully.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/blue/organizations/jenkins/aws-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the last build and observe that the number of nodes scaled. Similarly, you should see a new notification in <em class="calibre21">#df-monitor-tests</em> in the <em class="calibre21">DevOps20</em> Slack channel.</p>


<figure class="image">
  <img src="../images/00085.jpeg" alt="Figure 15-6: Slack notification indicating that nodes scaled" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-6: Slack notification indicating that nodes scaled</figcaption>
</figure>


<p class="calibre3">Finally, we’ll go back to the cluster and confirm not only that a new node was created but also that it joined the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HOSTNAME                                   STATUS AVAILABILITY MANAGER STATUS
<code class="lineno">2 </code>ip-172-31-24-32.us-east-2.compute.internal Ready  Active       Reachable
<code class="lineno">3 </code>ip-172-31-2-29.us-east-2.compute.internal  Ready  Active       Leader
<code class="lineno">4 </code>ip-172-31-42-64.us-east-2.compute.internal Ready  Active       Reachable
<code class="lineno">5 </code>ip-172-31-24-95.us-east-2.compute.internal Ready  Active
<code class="lineno">6 </code>ip-172-31-34-28.us-east-2.compute.internal Ready  Active
<code class="lineno">7 </code>ip-172-31-4-136.us-east-2.compute.internal Ready  Active
</pre></div>

</figure>

<p class="calibre3">The number of worker nodes increased from one to three. If you do not yet see three worker nodes, please wait for a minute or two and re-run the <code class="calibre19">docker node ls</code> command.</p>

<p class="calibre3">Let’s test whether the limits we set are respected. Remember, our Pipeline script should not allow less than one nor more than three worker nodes. Since we are already running three workers, we should be able to test it by attempting to add one more.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl -XPOST -i <code class="se">\</code>
<code class="lineno">4 </code>    <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?token=DevOps2\</code>
<code class="lineno">5 </code><code class="s">2&amp;scale=1"</code>
</pre></div>

</figure>

<p class="calibre3">We exited the cluster and sent a <code class="calibre19">POST</code> request to build the <code class="calibre19">aws-scale</code> job with the <code class="calibre19">scale</code> parameter set to <code class="calibre19">1</code>.</p>

<p class="calibre3">Let’s see the result in Jenkins UI.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/blue/organizations/jenkins/aws-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">You should see that the last build failed. We tried to add more workers than allowed and the build responded with an error. Similarly, we should see an error notification in <em class="calibre21">#df-monitor-tests</em> in the <em class="calibre21">DevOps20</em> Slack channel.</p>


<figure class="image">
  <img src="../images/00086.jpeg" alt="Figure 15-7: Slack notification indicating that node scaling failed" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-7: Slack notification indicating that node scaling failed</figcaption>
</figure>


<p class="calibre3">It won’t hurt to check whether de-scaling nodes works as well.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl -XPOST -i <code class="se">\</code>
<code class="lineno">2 </code>    <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?token=DevOps2\</code>
<code class="lineno">3 </code><code class="s">2&amp;scale=-2"</code>
</pre></div>

</figure>

<p class="calibre3">We sent a similar <code class="calibre19">POST</code> request like a few times before. The only notable difference is that the <code class="calibre19">scale</code> param is now set to <code class="calibre19">-2</code>. As a result, two worker nodes should be removed, leaving us with one.</p>

<p class="calibre3">At this point, there should be no need to check build results in Jenkins or notifications in Slack. The system proved to be working well. So, we’ll skip through those and jump straight into the cluster and output the list of joined nodes.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">We entered the cluster and listed all the nodes.</p>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HOSTNAME                                   STATUS AVAILABILITY MANAGER STATUS
<code class="lineno">2 </code>ip-172-31-24-32.us-east-2.compute.internal Ready  Active       Reachable
<code class="lineno">3 </code>ip-172-31-2-29.us-east-2.compute.internal  Ready  Active       Leader
<code class="lineno">4 </code>ip-172-31-42-64.us-east-2.compute.internal Ready  Active       Reachable
<code class="lineno">5 </code>ip-172-31-24-95.us-east-2.compute.internal Down   Active
<code class="lineno">6 </code>ip-172-31-34-28.us-east-2.compute.internal Ready  Active
<code class="lineno">7 </code>ip-172-31-4-136.us-east-2.compute.internal Down   Active
</pre></div>

</figure>

<p class="calibre3">You’ll notice that status of two of the nodes is set to <code class="calibre19">Down</code>. If, in your case, all the nodes are still <code class="calibre19">Ready</code>, you might need to wait for a minute or two and re-execute the <code class="calibre19">docker node ls</code> command. On the other hand, if you were not fast enough, Swarm might have cleaned its registry, and the two nodes that were <code class="calibre19">down</code> might have been removed altogether.</p>

<p class="calibre3">Finally, the last verification we should do is to check whether the lower limit is respected as well.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl -XPOST -i <code class="se">\</code>
<code class="lineno">4 </code>    <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?token=DevOps2\</code>
<code class="lineno">5 </code><code class="s">2&amp;scale=-1"</code>
</pre></div>

</figure>

<p class="calibre3">You know what to do. Visit the last build in Jenkins, check Slack, list Swarm nodes, or trust me blindly. The number of worker nodes should be left intact since we are running only one and reducing them to zero would violate the lower limit we set in the pipeline job.</p>

<p class="calibre3">Now that we confirmed that triggering the <code class="calibre19">aws-scale</code> job (de)scales our worker nodes, we can turn our attention to Prometheus and Alertmanager and try to tie them all together into a system that will, for example, scale the number of workers depending on memory usage.</p>

<h3 id="leanpub-auto-scaling-cluster-nodes-automatically" class="calibre20">Scaling Cluster Nodes Automatically</h3>

<p class="calibre3">We created the last piece of the chain. Jenkins job will scale nodes of a cluster only if something triggers it. We did that manually by sending <code class="calibre19">POST</code> requests but, as you might have guessed, that is not our ultimate goal. We need to run those builds through alerts based on metrics. Therefore, we’ll move back to the beginning of the chain and explore some of the metrics we can use and try to convert them into meaningful alerts.</p>

<p class="calibre3">Let’s open Prometheus and try to define an alert worthy of our scaling needs.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor"</code>
</pre></div>

</figure>

<p class="calibre3">Please use <em class="calibre21">admin</em> as both the <em class="calibre21">User Name</em> and the <em class="calibre21">Password</em> if you’re asked to authenticate.</p>

<p class="calibre3">We’ll start with an expression we already used in the previous chapters.</p>

<p class="calibre3">Please type the query that follows in the <em class="calibre21">Expression</em> field, click the <em class="calibre21">Execute</em> button, and switch to the <em class="calibre21">Graph</em> tab.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>(sum(node_memory_MemTotal) BY (instance) - sum(node_memory_MemFree + node_memory\
<code class="lineno">2 </code>_Buffers + node_memory_Cached) BY (instance)) / sum(node_memory_MemTotal) BY (in\
<code class="lineno">3 </code>stance)
</pre></div>

</figure>

<p class="calibre3">As a reminder, the expression calculates the percentage of used memory for each instance (node). It does that by taking the total amount of memory and reducing it with free, buffered, and cached memory. Further on, the result is divided with total memory to get a percentage. Each segment of the expression is using <code class="calibre19">BY (instance)</code> to separate the results.</p>

<p class="calibre3">The output should be four graphs representing four nodes currently running in the cluster. Used memory should be somewhere between ten and forty percent for each node.</p>

<p class="calibre3">Don’t get confused if you see more than four lines. We had more than four nodes and, during their lifespan, their metrics were also recorded in Prometheus.</p>


<figure class="image">
  <img src="../images/00087.jpeg" alt="Figure 15-8: Prometheus graph with memory utilization" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-8: Prometheus graph with memory utilization</figcaption>
</figure>


<p class="calibre3">At this point you might be tempted to write an alert that would be fired whenever memory is, let’s say, over 80%. That alert could result in a <code class="calibre19">POST</code> request to Jenkins which, in turn, would scale worker nodes.</p>

<p class="calibre3">What would such a spike in memory usage on one of the nodes tell us? If all nodes are using too much memory, it would not make sense to monitor them individually. On the other hand, if only one node has a spike, that would probably not indicate a problem that should be solved by scaling the number of nodes. The issue would, more likely, lie in incorrect memory reservations and limitations defined for one of our services. Or, maybe one of the services went wild with memory consumption. However, in that case, we probably did not even define its resources. If we did, Swarm would reschedule that service and, before that happens, we’d get a service-level alert. Such an alert might result in scaling of the service, or it might require some other type of actions. There might be other reasons for memory spike in one of the nodes but, in most of the cases, the resolution would have to rely on manual intervention. We’d need to (re)define service resources, fix a bug, or do one of many other actions that should be performed manually.</p>

<aside class="tip">
    <p class="calibre3">We, humans, should deal with unexpected anomalies and let the system correct itself when something predictable happens.</p>

</aside>

<p class="calibre3">All in all, auto-scaling based on memory usage of a single node is, in most cases, not a good strategy. Instead, I believe that it would be better to base our auto-scaling strategy on memory usage of the entire cluster. After all, we already adopted the concept of treating the whole cluster as a single entity.</p>

<p class="calibre3">Let’s try to write an expression that will give us the percentage of used memory across the whole cluster. We’ll break it into two parts. First, we’ll write a query that retrieves the number of used bytes and, later on, we’ll get the total available memory of the whole cluster. If we divide those two, the result should be a percentage of the used memory of a cluster.</p>

<p class="calibre3">Please type the query that follows in the <em class="calibre21">Expression</em> field, and click the <em class="calibre21">Execute</em> button.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(node_memory_MemTotal) - sum(node_memory_MemFree + node_memory_Buffers + node\
<code class="lineno">2 </code>_memory_Cached)
</pre></div>

</figure>

<p class="calibre3">You should see that around 2GB of memory is currently used in the cluster.</p>

<p class="calibre3">The expression we used is very similar to the one that did the calculation for each instance. The only significant difference is that we removed <code class="calibre19">BY (instance)</code> parts.</p>

<p class="calibre3">Next, we need to find out the total amount of memory of the cluster. That part should be easy since the previous expression already starts with the total.</p>

<p class="calibre3">Please type the query that follows in the <em class="calibre21">Expression</em> field, and click the <em class="calibre21">Execute</em> button.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(node_memory_MemTotal)
</pre></div>

</figure>

<p class="calibre3">The output should show that we have 8GB of total memory. You might see that the number was bigger in the past since we had a brief period with five or six nodes when we were experimenting with Jenkins’ job <code class="calibre19">aws-scale</code>. You might still have it set to more than 8GB. In that case, please wait for a few moments until metrics from the removed nodes expire.</p>

<p class="calibre3">If we combine those two expressions, the result is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>(sum(node_memory_MemTotal) - sum(node_memory_MemFree + node_memory_Buffers + nod\
<code class="lineno">2 </code>e_memory_Cached)) / sum(node_memory_MemTotal)
</pre></div>

</figure>

<p class="calibre3">Please type the previous expression and click the <em class="calibre21">Execute</em> button. The result should be current memory usage of approximately 25%.</p>


<figure class="image">
  <img src="../images/00088.jpeg" alt="Figure 15-9: Prometheus graph with total memory utilization of the cluster" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-9: Prometheus graph with total memory utilization of the cluster</figcaption>
</figure>


<p class="calibre3">There’s still one more problem we might need to solve. We should probably not treat manager and worker nodes equally so we might want to split metrics between the two. If we’d distinguish <code class="calibre19">node_exporter</code> services running on manager nodes from those deployed to workers, we could create different types of alerts for each server types.</p>

<p class="calibre3">Let’s go back to the cluster and download an updated version of the <code class="calibre19">exporter</code> stack definition.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl -o exporters.yml <code class="se">\</code>
<code class="lineno">4 </code>    https://raw.githubusercontent.com/vfarcic/docker-flow-monitor/master/stacks/<code class="se">\</code>
<code class="lineno">5 </code>exporters-aws.yml
<code class="lineno">6 </code>
<code class="lineno">7 </code>cat exporters.yml
</pre></div>

</figure>

<p class="calibre3">We entered the cluster, downloaded the <code class="calibre19">exporters-aws.yml</code> stack definition, and displayed its content. The output of the relevant parts of the <code class="calibre19">cat</code> command is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>  node-exporter-manager:
<code class="lineno"> 2 </code>    ...
<code class="lineno"> 3 </code>    deploy:
<code class="lineno"> 4 </code>      labels:
<code class="lineno"> 5 </code>        ...
<code class="lineno"> 6 </code>        - com.df.alertName.2=node_mem_limit_total_above
<code class="lineno"> 7 </code>        - com.df.alertIf.2=@node_mem_limit_total_above:0.8
<code class="lineno"> 8 </code>        - com.df.alertLabels.2=receiver=system,scale=no,service=exporter_node-ex\
<code class="lineno"> 9 </code>porter-manager,type=node
<code class="lineno">10 </code>        - com.df.alertFor.2=30s
<code class="lineno">11 </code>        ...
<code class="lineno">12 </code>      placement:
<code class="lineno">13 </code>        constraints:
<code class="lineno">14 </code>          - node.role == manager
<code class="lineno">15 </code>      ...
<code class="lineno">16 </code>
<code class="lineno">17 </code>  node-exporter-worker:
<code class="lineno">18 </code>    ...
<code class="lineno">19 </code>    deploy:
<code class="lineno">20 </code>      labels:
<code class="lineno">21 </code>        ...
<code class="lineno">22 </code>        - com.df.alertName.2=node_mem_limit_total_above
<code class="lineno">23 </code>        - com.df.alertIf.2=@node_mem_limit_total_above:0.8
<code class="lineno">24 </code>        - com.df.alertFor.2=30s
<code class="lineno">25 </code>        - com.df.alertName.3=node_mem_limit_total_below
<code class="lineno">26 </code>        - com.df.alertIf.3=@node_mem_limit_total_below:0.05
<code class="lineno">27 </code>        - com.df.alertFor.3=30s
<code class="lineno">28 </code>        ...
<code class="lineno">29 </code>      placement:
<code class="lineno">30 </code>        constraints:
<code class="lineno">31 </code>          - node.role == worker
<code class="lineno">32 </code>      ...
</pre></div>

</figure>

<p class="calibre3">We split <code class="calibre19">node-exporter</code> service into two. We added two new label sets besides those we used before. They are <code class="calibre19">@node_mem_limit_total_above</code> and <code class="calibre19">@node_mem_limit_total_below</code> shortcuts that expand to alerts with the expression we wrote earlier. The first one will trigger an alert if the total memory of all the nodes where the exporter is running is above a certain threshold. Similarly, the other will be triggered if total memory is below the threshold. Those shortcuts are accompanied with labels <code class="calibre19">scale</code> and <code class="calibre19">type</code>. Default values of the <code class="calibre19">scale</code> label are <code class="calibre19">up</code> and <code class="calibre19">down</code> depending on the shortcut. The <code class="calibre19">type</code> label is always set to <code class="calibre19">node</code>. That way, we’ll know whether to scale or de-scale and, through the <code class="calibre19">type</code> label, we’ll know that the action should be performed on nodes. For more info, please consult <a href="http://monitor.dockerflow.com/usage/#alertif-parameter-shortcuts">AlertIf Parameter Shortcuts</a> section of the <a href="http://monitor.dockerflow.com/">Docker Flow Monitor</a> documentation.</p>

<p class="calibre3">You’ll notice that the <code class="calibre19">node-exporter-manager</code> service does not have the <code class="calibre19">node_mem_limit_total_below</code> alert. The reason is simple. If memory usage is very low on manager nodes, there’s still nothing we should do. We’re not going to remove one of the managers since that would put cluster at risk. Furthermore, we changed the <code class="calibre19">node_mem_limit_total_above</code> default labels so that <code class="calibre19">scale</code> is set to <code class="calibre19">no</code>. That way, we can instruct Alertmanager not to send a request to Jenkins to scale the nodes but a Slack notification instead. All in all, when memory usage of manager nodes is too low, we will take no action. When it’s too high, we’ll investigate the reason behind that, instead of taking any automated actions.</p>

<p class="calibre3">The <code class="calibre19">node-exporter-worker</code> service will trigger automation in both cases. We’ll configure Alertmanager to send requests to Jenkins to add or remove worker nodes if memory usage goes beyond defined thresholds. We put <code class="calibre19">node_mem_limit_total_below</code> limit to five percent. If this were a production cluster, that value would be too low. The more reasonable lower threshold would be thirty or forty percent. If total memory usage is below it, we have too many nodes in the cluster, and one (or more) of them should be removed. However, since our current cluster already has more capacity than we need, that would trigger an alert right away. Therefore, we decreased the limit to avoid spoiling the surprise.</p>

<p class="calibre3">Finally, both services have <code class="calibre19">placement constraints</code> that will make sure that they are running only on the correct node types.</p>

<p class="calibre3">We are about to deploy the exporters. Before we do that, please note that they will not trigger correct processes in Alertmanager. We are yet to configure it correctly. For now, we’ll limit our scope only to alerts in Prometheus.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack rm exporter
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker stack deploy -c exporters.yml <code class="se">\</code>
<code class="lineno">4 </code>    exporter
<code class="lineno">5 </code>
<code class="lineno">6 </code><code class="nb">exit</code>
<code class="lineno">7 </code>
<code class="lineno">8 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">Since the new stack definition does not have one of the services contained in the old one and <code class="calibre19">docker stack deploy</code> does not delete services (only creates and updates them), we had to remove the whole stack. The alternative would be to remove only that service (<code class="calibre19">exporter_node-exporter</code>) but, since it’s not critical whether we’ll miss a second or two of metrics, removing the whole stack was an easier solution.</p>

<p class="calibre3">Further on, we deployed the stack, exited the cluster, and opened the Prometheus’ alerts screen.</p>

<p class="calibre3">You’ll notice that, this time, we have two sets of <em class="calibre21">nodeexporter</em> alerts. Let’s start with those dedicated to managers.</p>

<p class="calibre3">Please expand the <code class="calibre19">exporter_nodeexportermanager_node_mem_limit_total_above</code> alert.</p>

<p class="calibre3">You’ll see that it contains a similar expression as the one we wrote previously. It is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>(sum(node_memory_MemTotal{job="exporter_node-exporter-manager"}) - sum(node_memo\
<code class="lineno">2 </code>ry_MemFree{job="exporter_node-exporter-manager"} + node_memory_Buffers{job="expo\
<code class="lineno">3 </code>rter_node-exporter-manager"} + node_memory_Cached{job="exporter_node-exporter-ma\
<code class="lineno">4 </code>nager"})) / sum(node_memory_MemTotal{job="exporter_node-exporter-manager"}) &gt; 0.\
<code class="lineno">5 </code>8
</pre></div>

</figure>

<p class="calibre3">The difference is that we are limiting the alert only to metrics coming from the <code class="calibre19">exporter_node-exporter-manager</code> job. That way, we have a clear distinction between node types. The alert will be triggered only if total memory of manager nodes is above eighty percent.</p>

<p class="calibre3">Please click the link next to the <code class="calibre19">IF</code> statement.</p>

<p class="calibre3">You’ll be presented with the graph screen with the alert query pre-populated. Please remove <code class="calibre19">&gt; 0.8</code> and click the <em class="calibre21">Execute</em> button. You’ll see the graph with the memory usage of manager nodes. Whatever the values are, they should be way below eighty percent.</p>

<p class="calibre3">Please explore the <code class="calibre19">exporter_nodeexporterworker_node_mem_limit_total_above</code> and <code class="calibre19">exporter_nodeexporterworker_node_mem_limit_total_below</code>. They use the similar logic as the <code class="calibre19">exporter_nodeexportermanager_node_mem_limit_total_above</code>.</p>


<figure class="image">
  <img src="../images/00089.jpeg" alt="Figure 15-10: Prometheus alert based on total memory utilization of cluster managers" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-10: Prometheus alert based on total memory utilization of cluster managers</figcaption>
</figure>


<p class="calibre3">Now that we created the alerts, we should switch our focus to Alertmanager.</p>

<p class="calibre3">Since Docker secrets are immutable, we’ll have to remove the <code class="calibre19">monitor</code> stack and the <code class="calibre19">alert_manager_config</code> secret before we start working on a new configuration.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker stack rm monitor
<code class="lineno">4 </code>
<code class="lineno">5 </code>docker secret rm alert_manager_config
</pre></div>

</figure>

<p class="calibre3">Now we can create a new Alertmanager configuration and store it as a Docker secret.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">source</code> creds
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code><code class="nb">echo</code> <code class="s">"route:</code>
<code class="lineno"> 4 </code><code class="s">  group_by: [service,scale,type]</code>
<code class="lineno"> 5 </code><code class="s">  repeat_interval: 30m</code>
<code class="lineno"> 6 </code><code class="s">  group_interval: 30m</code>
<code class="lineno"> 7 </code><code class="s">  receiver: 'slack'</code>
<code class="lineno"> 8 </code><code class="s">  routes:</code>
<code class="lineno"> 9 </code><code class="s">  - match:</code>
<code class="lineno">10 </code><code class="s">      type: 'node'</code>
<code class="lineno">11 </code><code class="s">      scale: 'up'</code>
<code class="lineno">12 </code><code class="s">    receiver: 'jenkins-node-up'</code>
<code class="lineno">13 </code><code class="s">  - match:</code>
<code class="lineno">14 </code><code class="s">      type: 'node'</code>
<code class="lineno">15 </code><code class="s">      scale: 'down'</code>
<code class="lineno">16 </code><code class="s">    receiver: 'jenkins-node-down'</code>
<code class="lineno">17 </code><code class="s">  - match:</code>
<code class="lineno">18 </code><code class="s">      service: 'go-demo_main'</code>
<code class="lineno">19 </code><code class="s">      scale: 'up'</code>
<code class="lineno">20 </code><code class="s">    receiver: 'jenkins-go-demo_main-up'</code>
<code class="lineno">21 </code><code class="s">  - match:</code>
<code class="lineno">22 </code><code class="s">      service: 'go-demo_main'</code>
<code class="lineno">23 </code><code class="s">      scale: 'down'</code>
<code class="lineno">24 </code><code class="s">    receiver: 'jenkins-go-demo_main-down'</code>
<code class="lineno">25 </code>
<code class="lineno">26 </code><code class="s">receivers:</code>
<code class="lineno">27 </code><code class="s">  - name: 'slack'</code>
<code class="lineno">28 </code><code class="s">    slack_configs:</code>
<code class="lineno">29 </code><code class="s">      - send_resolved: true</code>
<code class="lineno">30 </code><code class="s">        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is \</code>
<code class="lineno">31 </code><code class="s">in danger!'</code>
<code class="lineno">32 </code><code class="s">        title_link: 'http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts'</code>
<code class="lineno">33 </code><code class="s">        text: '{{ .CommonAnnotations.summary}}'</code>
<code class="lineno">34 </code><code class="s">        api_url: 'https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0KvvyStV\</code>
<code class="lineno">35 </code><code class="s">nIt3ZWpIaLnqLCu'</code>
<code class="lineno">36 </code><code class="s">  - name: 'jenkins-go-demo_main-up'</code>
<code class="lineno">37 </code><code class="s">    webhook_configs:</code>
<code class="lineno">38 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">39 </code><code class="s">        url: 'http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/service-scale/buildWithParameters?\</code>
<code class="lineno">40 </code><code class="s">token=DevOps22&amp;service=go-demo_main&amp;scale=1'</code>
<code class="lineno">41 </code><code class="s">  - name: 'jenkins-go-demo_main-down'</code>
<code class="lineno">42 </code><code class="s">    webhook_configs:</code>
<code class="lineno">43 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">44 </code><code class="s">        url: 'http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/service-scale/buildWithParameters?\</code>
<code class="lineno">45 </code><code class="s">token=DevOps22&amp;service=go-demo_main&amp;scale=-1'</code>
<code class="lineno">46 </code><code class="s">  - name: 'jenkins-node-up'</code>
<code class="lineno">47 </code><code class="s">    webhook_configs:</code>
<code class="lineno">48 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">49 </code><code class="s">        url: 'http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?toke\</code>
<code class="lineno">50 </code><code class="s">n=DevOps22&amp;scale=1'</code>
<code class="lineno">51 </code><code class="s">  - name: 'jenkins-node-down'</code>
<code class="lineno">52 </code><code class="s">    webhook_configs:</code>
<code class="lineno">53 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">54 </code><code class="s">        url: 'http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?toke\</code>
<code class="lineno">55 </code><code class="s">n=DevOps22&amp;scale=-1'</code>
<code class="lineno">56 </code><code class="s">"</code> <code class="calibre19">|</code> docker secret create alert_manager_config -
</pre></div>

</figure>

<p class="calibre3">Feel free to use the <a href="https://gist.github.com/vfarcic/efebfba9d42ba48eedabc118fcac7ed7">15-self-adaptation-infra-alertmanager-config.sh</a> gist if you do not feel like typing the whole config.</p>

<p class="calibre3">Since the config needs environment variable <code class="calibre19">CLUSTER_DNS</code>, we sourced the <code class="calibre19">creds</code> file that already contains it.</p>

<p class="calibre3">We added two new routes. Alerts will be routed to the <code class="calibre19">jenkins-node-up</code> receiver if the <code class="calibre19">type</code> label is set to <code class="calibre19">node</code> and <code class="calibre19">scale</code> is <code class="calibre19">up</code>. Similarly, if the <code class="calibre19">scale</code> is set to <code class="calibre19">down</code>, alerts will be routed to <code class="calibre19">jenkins-node-down</code>. Both receivers are sending <code class="calibre19">POST</code> requests to build the <code class="calibre19">aws-scale</code> job. The only difference is the <code class="calibre19">scale</code> parameter that is either <code class="calibre19">1</code> or <code class="calibre19">-1</code> depending on the outcome we want to accomplish.</p>

<p class="calibre3">Jenkins builds will not be executed with the alert associated with managers. It has the <code class="calibre19">scale</code> label set to <code class="calibre19">no</code>, so none of the routes match it. Instead, we’ll get a notification to Slack (default receiver). On the other hand, worker alerts will trigger Jenkins which, in turn, will scale or de-scale nodes of the cluster. Since <code class="calibre19">repeat_interval</code> and <code class="calibre19">group_interval</code> are both set to thirty minutes, new nodes would spawn every hour if memory usage does not drop.</p>

<p class="calibre3">Now we can deploy the <code class="calibre19">monitor</code> stack again. Alertmanager will, this time, use the new configuration and, if everything goes as planned, act as a bridge between Prometheus alerts and Jenkins.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DOMAIN</code><code class="o">=</code><code class="nv">$CLUSTER_DNS</code> docker stack <code class="se">\</code>
<code class="lineno">2 </code>    deploy -c monitor.yml monitor
</pre></div>

</figure>

<p class="calibre3">Now that Alertmanager is using the new configuration, we can test the system. We’ll start with a simple scenario and verify that increased memory usage of manager nodes results in a notification to Slack. Remember, we’re not trying to scale managers automatically. That is reserved for worker nodes. Instead, we want to notify a human that there is an anomaly.</p>

<p class="calibre3">Since our current memory usage is way below 80%, we need to either increase the number of services we’re running or change the alert threshold. We’ll choose the latter since it is easier to accomplish. All we need to do is change the label of the <code class="calibre19">node-exporter-manager</code> service.</p>

<p class="calibre3">Before we proceed, let’s confirm that all the services in the <code class="calibre19">monitor</code> stack are up and running.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>running monitor
</pre></div>

</figure>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME                     IMAGE                                     NODE         \
<code class="lineno">2 </code>                               DESIRED STATE CURRENT STATE         ERROR PORTS
<code class="lineno">3 </code>monitor_alert-manager.1  prom/alertmanager:latest                  ip-172-31-7-5\
<code class="lineno">4 </code>6.us-east-2.compute.internal   Running       Running 2 minutes ago
<code class="lineno">5 </code>monitor_monitor.1        vfarcic/docker-flow-monitor:latest        ip-172-31-7-5\
<code class="lineno">6 </code>6.us-east-2.compute.internal   Running       Running 2 minutes ago
<code class="lineno">7 </code>monitor_swarm-listener.1 vfarcic/docker-flow-swarm-listener:latest ip-172-31-33-\
<code class="lineno">8 </code>127.us-east-2.compute.internal Running       Running 2 minutes ago
</pre></div>

</figure>

<p class="calibre3">Now we can lower the upper memory threshold for the alert related to the <code class="calibre19">node-exporter-manager</code> and confirm that the alert associated with it works.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add <code class="s">"com.df.alertIf.2=@node_mem_limit_total_above:0.1"</code> <code class="se">\</code>
<code class="lineno">3 </code>    exporter_node-exporter-manager
</pre></div>

</figure>

<p class="calibre3">Since our memory usage is currently between 20% and 30%, setting up the alert to 10% will certainly result in fired event.</p>

<p class="calibre3">Let’s go to Prometheus UI and confirm that the alert is firing.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">exporter_nodeexportermanager_node_mem_limit_total_above</code> should be red. If it isn’t, please wait a few moments and refresh the screen.</p>

<p class="calibre3">Once the alert is fired, we can confirm that a Slack notification was sent by Alertmanager. Please open <em class="calibre21">DevOps20</em> slack channel <em class="calibre21">#df-monitor-tests</em> and observe the note stating that <em class="calibre21">Total memory of the nodes is over 0.1</em>.</p>


<figure class="image">
  <img src="../images/00090.jpeg" alt="Figure 15-11: Prometheus initiated Slack notifications" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-11: Prometheus initiated Slack notifications</figcaption>
</figure>


<p class="calibre3">Before we proceed, we should restore the <code class="calibre19">node-exporter-manager</code> alert definition to its previous threshold. Otherwise, another alert would fire an hour from now. We’ll imagine that someone saw the alert and fixed the imaginary problem.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker service update <code class="se">\</code>
<code class="lineno">4 </code>    --label-add <code class="s">"com.df.alertIf.2=@node_mem_limit_total_above:0.8"</code> <code class="se">\</code>
<code class="lineno">5 </code>    exporter_node-exporter-manager
</pre></div>

</figure>

<p class="calibre3">We entered the cluster and updated the service by adding (overwriting) the alert with 80% threshold.</p>

<p class="calibre3">Now we can test the real deal. We’ll verify that automated scaling of worker nodes works as expected. We’ll repeat a similar simulation by lowering the threshold. The only difference is that, this time, we’ll update the <code class="calibre19">node-exporter-worker</code> service.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add <code class="s">"com.df.alertIf.2=@node_mem_limit_total_above:0.1"</code> <code class="se">\</code>
<code class="lineno">3 </code>    exporter_node-exporter-worker
</pre></div>

</figure>

<p class="calibre3">The alert is now set to fire when 10% of the total memory of worker nodes is reached. We can confirm that by visiting Prometheus one more time.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">You’ll notice that the <code class="calibre19">exporter_nodeexporterworker_node_mem_limit_total_above</code> alert is red. If it isn’t, please wait a few moments and refresh the screen.</p>

<p class="calibre3">Since we configured Alertmanager to send build requests to Jenkins whenever an alert with the label <code class="calibre19">type</code> is set to <code class="calibre19">node</code> and <code class="calibre19">scale</code> is set to <code class="calibre19">up</code>, and those happen to be labels associated with this alert, the result should be a new build of the <code class="calibre19">aws-scale</code> job. Let’s confirm that.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/blue/organizations/jenkins/aws-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">You’ll notice that the new Jenkins build was triggered. As a result, we should see a notification in Slack stating that <em class="calibre21">worker nodes were scaled</em>. More importantly, the number of worker nodes should increase by one.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">We entered the cluster and listed all the nodes.</p>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS
<code class="lineno">2 </code>ip-172-31-9-40.us-east-2.compute.internal   Ready  Active       Leader
<code class="lineno">3 </code>ip-172-31-18-10.us-east-2.compute.internal  Ready  Active
<code class="lineno">4 </code>ip-172-31-25-34.us-east-2.compute.internal  Ready  Active       Reachable
<code class="lineno">5 </code>ip-172-31-35-24.us-east-2.compute.internal  Ready  Active       Reachable
<code class="lineno">6 </code>ip-172-31-35-253.us-east-2.compute.internal Ready  Active
</pre></div>

</figure>

<p class="calibre3">You’ll see that now we have two worker nodes in the cluster (there was one before). If, in your case, there are still no two worker nodes with the <code class="calibre19">Ready</code> status, please wait for a minute or two. We need to give enough time for AWS to detect the change in the auto-scaling group, to create a new VM, and to execute the init script that will join it to the cluster.</p>


<figure class="image">
  <img src="../images/00091.jpeg" alt="Figure 15-12: Prometheus initiated worker nodes scaling" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-12: Prometheus initiated worker nodes scaling</figcaption>
</figure>


<p class="calibre3">Now that we confirmed that scaling up works, we should verify that the system is capable of scaling down as well. But, before we do that, we’ll restore the label to the initial threshold, and thus avoid getting another node an hour later.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add <code class="s">"com.df.alertIf.2=@node_mem_limit_total_above:0.8"</code> <code class="se">\</code>
<code class="lineno">3 </code>    exporter_node-exporter-worker
</pre></div>

</figure>

<p class="calibre3">We’ll follow the same testing pattern. But, since we are now testing the processes triggered when there’s too much unused memory, we’ll have to increase the threshold of the next alert.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add <code class="s">"com.df.alertIf.3=@node_mem_limit_total_below:0.9"</code> <code class="se">\</code>
<code class="lineno">3 </code>    exporter_node-exporter-worker
</pre></div>

</figure>

<p class="calibre3">The rest of validations should be the same as before.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">We opened the Prometheus <em class="calibre21">alerts</em> screen. The <em class="calibre21">exporter_nodeexporterworker_node_mem_limit_total_below</em> alert should be red. You know what to do if it’s not. Have patience and refresh the screen.</p>

<p class="calibre3">Jenkins build was executed, and we got a new notification in Slack. If you don’t believe me, check it yourself. There’s no need for instructions.</p>

<p class="calibre3">Finally, after a few minutes, one of the worker nodes should be removed from the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">If you were patient enough, the output of the <code class="calibre19">node ls</code> command should be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>HOSTNAME                                      STATUS              AVAILABILITY  \
<code class="lineno"> 2 </code>      MANAGER STATUS
<code class="lineno"> 3 </code>ip-172-31-25-180.us-east-2.compute.internal   Ready               Active
<code class="lineno"> 4 </code>ip-172-31-44-104.us-east-2.compute.internal   Down                Active
<code class="lineno"> 5 </code>ip-172-31-24-63.us-east-2.compute.internal    Ready               Active        \
<code class="lineno"> 6 </code>      Leader
<code class="lineno"> 7 </code>ip-172-31-15-200.us-east-2.compute.internal   Ready               Active        \
<code class="lineno"> 8 </code>      Reachable
<code class="lineno"> 9 </code>ip-172-31-32-99.us-east-2.compute.internal    Ready               Active        \
<code class="lineno">10 </code>      Reachable
</pre></div>

</figure>

<p class="calibre3">One of the worker nodes was removed (or will be removed soon).</p>

<p class="calibre3">Before we move on, we’ll restore the alert to its original formula.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add <code class="s">"com.df.alertIf.3=@node_mem_limit_total_below:0.05"</code> <code class="se">\</code>
<code class="lineno">3 </code>    exporter_node-exporter-worker
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">exit</code>
</pre></div>

</figure>

<p class="calibre3">Among many other combinations and actions that we could perform, there is one area that might be very important. We might need to reschedule our services after scaling our cluster.</p>

<h3 id="leanpub-auto-rescheduling-services-after-scaling-nodes" class="calibre20">Rescheduling Services After Scaling Nodes</h3>

<p class="calibre3">We managed to build a system that scales (and de-scales) our worker nodes automatically. Even though we might need to extend alerts to other types of metrics, the system we created is already good as it is. Kind of… The problem is that new nodes are empty. They do not host any services until we deploy new ones of we updated some of those that are already running inside our cluster. That, in itself, is not a problem if we deploy new releases to production often.</p>

<p class="calibre3">Let’s say that, on average, we deploy a new release every hour. That would mean that our newly added nodes will be empty only for a short while. Our deployment pipelines will re-balance the cluster. But, what happens if we do not deploy any new release until the next day? Having empty nodes for a while is not a big problem since our services have memory reservations based on actual memory usage. We observed metrics and decided how much each replica of a service should use. Having nodes with services that are using 80% or even 90% is not a problem. Still, we can do better. We can forcefully update some of our services and, thus, let Swarm reschedule. As a result, new nodes will be filled with replicas.</p>

<p class="calibre3">We could, for example, iterate over all the services in the cluster and update them by adding an environment variable. That would initiate rolling updates and result in better distribution of our services across the cluster. However, that might produce downtime. Some of our services (e.g., <code class="calibre19">go-demo</code>) are scalable and stateless. They can be updated at any time without any downtime. Unfortunately, not all are created using distributed-systems principles. A good example is Jenkins and Prometheus. They cannot be scaled, so we cannot run multiple replicas. Update of a service with a single replica inevitably produces downtime, no matter whether we employ rolling updates or, for example, blue-green deployment. It does not matter whether that downtime is a millisecond or a full minute. Downtime is downtime. We might never be able to avoid downtime with services like those. Still, we should probably not produce it ourselves without a valid reason. Filling newly added nodes with services is not a reason good enough.</p>

<p class="calibre3">Therefore, we need to figure out a way to distinguish which services are safe to update, and from which we should stay away. The solution is probably obvious. We can add one more label to our services. For example, we can use a service label <code class="calibre19">com.df.reschedule</code>. If it’s set to <code class="calibre19">true</code>, it would mean that the service can be rescheduled (updated) without any danger. Services with any other value (including not having that label) should be ignored.</p>

<p class="calibre3">We could use the command that follows to retrieve IDs of all the services with the <code class="calibre19">com.df.reschedule</code> label (do not execute it).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service ls -q <code class="se">\</code>
<code class="lineno">2 </code>    -f <code class="nv">label</code><code class="o">=</code>com.df.reschedule<code class="o">=</code><code class="nb">true</code>
</pre></div>

</figure>

<p class="calibre3">The output would be the list of IDs (<code class="calibre19">-q</code>) of all the services with the label <code class="calibre19">com.df.reschedule</code> set to <code class="calibre19">true</code>.</p>

<p class="calibre3">Further on, we could iterate through that list of IDs and update services. Such an action would result in a redistribution of services across the cluster. We do not have to update anything significant. Anything should do. For example, we can add an environment variable called <code class="calibre19">RESCHEDULE_DATE</code>. Since its value needs to be different every time we update it (otherwise update would not trigger rescheduling) we can put current date and time as the value.</p>

<p class="calibre3">The command that would update a service can be as follows (do not execute it).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update --env-add <code class="s">'RESCHEDULE_DATE=${date}'</code> <code class="si">${</code><code class="nv">service</code><code class="si">}</code>
</pre></div>

</figure>

<p class="calibre3">Finally, we should execute the process only if we are scaling up and skip it when scaling down.</p>

<p class="calibre3">All that, translated to a Jenkins Pipeline script, would produce the snippet that follows (do not paste it to Jenkins).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="k">if</code> <code class="o">(</code><code class="calibre19">scale</code><code class="o">.</code><code class="na">toInteger</code><code class="o">()</code> <code class="o">&gt;</code> <code class="o">0</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno"> 2 </code>  <code class="calibre19">sleep</code> <code class="o">300</code>
<code class="lineno"> 3 </code>  <code class="calibre19">script</code> <code class="o">{</code>
<code class="lineno"> 4 </code>    <code class="kt">def</code> <code class="calibre19">servicesOut</code> <code class="o">=</code> <code class="calibre19">sh</code><code class="o">(</code>
<code class="lineno"> 5 </code>      <code class="nl">script:</code> <code class="s">"docker service ls -q -f label=com.df.reschedule=true"</code><code class="o">,</code>
<code class="lineno"> 6 </code>      <code class="nl">returnStdout:</code> <code class="k">true</code>
<code class="lineno"> 7 </code>    <code class="o">)</code>
<code class="lineno"> 8 </code>    <code class="kt">def</code> <code class="calibre19">services</code> <code class="o">=</code> <code class="calibre19">servicesOut</code><code class="o">.</code><code class="na">split</code><code class="o">(</code><code class="s">'\n'</code><code class="o">)</code>
<code class="lineno"> 9 </code>    <code class="kt">def</code> <code class="calibre19">date</code> <code class="o">=</code> <code class="k">new</code> <code class="calibre19">Date</code><code class="o">()</code>
<code class="lineno">10 </code>    <code class="k">for</code><code class="o">(</code><code class="kt">int</code> <code class="calibre19">i</code> <code class="o">=</code> <code class="o">0</code><code class="o">;</code> <code class="calibre19">i</code> <code class="o">&lt;</code> <code class="calibre19">services</code><code class="o">.</code><code class="na">size</code><code class="o">();</code> <code class="calibre19">i</code><code class="o">++)</code> <code class="o">{</code>
<code class="lineno">11 </code>      <code class="kt">def</code> <code class="calibre19">service</code> <code class="o">=</code> <code class="calibre19">services</code><code class="o">[</code><code class="o">0</code><code class="o">]</code>
<code class="lineno">12 </code>      <code class="calibre19">sh</code> <code class="s">"docker service update --env-add 'RESCHEDULE_DATE=${date}' ${service}"</code>
<code class="lineno">13 </code>    <code class="o">}</code>
<code class="lineno">14 </code>  <code class="o">}</code>
<code class="lineno">15 </code><code class="o">}</code>
</pre></div>

</figure>

<p class="calibre3">We start with a simple <code class="calibre19">if</code> statement that validates whether we want to scale up. Since it takes a bit of time until a new node is created, we’re waiting for 5 minutes (<code class="calibre19">300</code> seconds). We could probably do a more intelligent type of verification with some kind of a loop that would verify whether the node joined the cluster. However, that might be an overkill (for now) so a simple <code class="calibre19">sleep</code> should do.</p>

<p class="calibre3">Further on, we are retrieving the list of all IDs of services that should be rescheduled. The result is split into an array and assigned to the variable <code class="calibre19">services</code>.</p>

<p class="calibre3">Finally, we are iterating over all IDs (<code class="calibre19">services</code>) and executing <code class="calibre19">docker service update</code> which will reschedule the services.</p>

<p class="calibre3">Let’s incorporate the snippet into the <code class="calibre19">aws-scale</code> job we created earlier.</p>

<p class="calibre3">Please open the <code class="calibre19">aws-scale</code> configuration screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/configure"</code>
</pre></div>

</figure>

<p class="calibre3">Click the <em class="calibre21">Pipeline</em> tab and type the script that follows in the <em class="calibre21">Pipeline Script</em> field.</p>

<aside class="tip">
    <p class="calibre3">If you’re not thrilled at the prospect of typing, feel free to copy and paste the script from the <a href="https://gist.github.com/vfarcic/dafb76fe3699e2241e1d6add228bf40e">15-self-adaptation-infra-jenkins-pipeline-02.groovy</a> gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">pipeline</code> <code class="o">{</code>
<code class="lineno"> 2 </code>  <code class="calibre19">agent</code> <code class="o">{</code>
<code class="lineno"> 3 </code>    <code class="calibre19">label</code> <code class="s">"prod"</code>
<code class="lineno"> 4 </code>  <code class="o">}</code>
<code class="lineno"> 5 </code>  <code class="calibre19">options</code> <code class="o">{</code>
<code class="lineno"> 6 </code>    <code class="calibre19">buildDiscarder</code><code class="o">(</code><code class="calibre19">logRotator</code><code class="o">(</code><code class="nl">numToKeepStr:</code> <code class="s">'2'</code><code class="o">))</code>
<code class="lineno"> 7 </code>    <code class="calibre19">disableConcurrentBuilds</code><code class="o">()</code>
<code class="lineno"> 8 </code>  <code class="o">}</code>
<code class="lineno"> 9 </code>  <code class="calibre19">parameters</code> <code class="o">{</code>
<code class="lineno">10 </code>    <code class="calibre19">string</code><code class="o">(</code>
<code class="lineno">11 </code>      <code class="nl">name:</code> <code class="s">"scale"</code><code class="o">,</code>
<code class="lineno">12 </code>      <code class="nl">defaultValue:</code> <code class="s">"1"</code><code class="o">,</code>
<code class="lineno">13 </code>      <code class="nl">description:</code> <code class="s">"The number of worker nodes to add or remove"</code>
<code class="lineno">14 </code>    <code class="o">)</code>
<code class="lineno">15 </code>  <code class="o">}</code>
<code class="lineno">16 </code>  <code class="calibre19">stages</code> <code class="o">{</code>
<code class="lineno">17 </code>    <code class="calibre19">stage</code><code class="o">(</code><code class="s">"scale"</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">18 </code>      <code class="calibre19">steps</code> <code class="o">{</code>
<code class="lineno">19 </code>        <code class="calibre19">git</code> <code class="s">"https://github.com/vfarcic/docker-aws-cli.git"</code>
<code class="lineno">20 </code>        <code class="calibre19">script</code> <code class="o">{</code>
<code class="lineno">21 </code>          <code class="kt">def</code> <code class="calibre19">asgName</code> <code class="o">=</code> <code class="calibre19">sh</code><code class="o">(</code>
<code class="lineno">22 </code>            <code class="nl">script:</code> <code class="s">"source /run/secrets/aws &amp;&amp; docker-compose run --rm asg-name\</code>
<code class="lineno">23 </code><code class="s">"</code><code class="o">,</code>
<code class="lineno">24 </code>            <code class="nl">returnStdout:</code> <code class="k">true</code>
<code class="lineno">25 </code>          <code class="o">).</code><code class="na">trim</code><code class="o">()</code>
<code class="lineno">26 </code>          <code class="k">if</code> <code class="o">(</code><code class="calibre19">asgName</code> <code class="o">==</code> <code class="s">""</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">27 </code>            <code class="calibre19">error</code> <code class="s">"Could not find auto-scaling group"</code>
<code class="lineno">28 </code>          <code class="o">}</code>
<code class="lineno">29 </code>          <code class="kt">def</code> <code class="calibre19">asgDesiredCapacity</code> <code class="o">=</code> <code class="calibre19">sh</code><code class="o">(</code>
<code class="lineno">30 </code>            <code class="nl">script:</code> <code class="s">"source /run/secrets/aws &amp;&amp; ASG_NAME=${asgName} docker-compo\</code>
<code class="lineno">31 </code><code class="s">se run --rm asg-desired-capacity"</code><code class="o">,</code>
<code class="lineno">32 </code>            <code class="nl">returnStdout:</code> <code class="k">true</code>
<code class="lineno">33 </code>          <code class="o">).</code><code class="na">trim</code><code class="o">().</code><code class="na">toInteger</code><code class="o">()</code>
<code class="lineno">34 </code>          <code class="kt">def</code> <code class="calibre19">asgNewCapacity</code> <code class="o">=</code> <code class="calibre19">asgDesiredCapacity</code> <code class="o">+</code> <code class="calibre19">scale</code><code class="o">.</code><code class="na">toInteger</code><code class="o">()</code>
<code class="lineno">35 </code>          <code class="k">if</code> <code class="o">(</code><code class="calibre19">asgNewCapacity</code> <code class="o">&lt;</code> <code class="o">1</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">36 </code>            <code class="calibre19">error</code> <code class="s">"The number of worker nodes is already at the minimum capacity\</code>
<code class="lineno">37 </code><code class="s"> of 1"</code>
<code class="lineno">38 </code>          <code class="o">}</code> <code class="k">else</code> <code class="k">if</code> <code class="o">(</code><code class="calibre19">asgNewCapacity</code> <code class="o">&gt;</code> <code class="o">3</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">39 </code>            <code class="calibre19">error</code> <code class="s">"The number of worker nodes is already at the maximum capacity\</code>
<code class="lineno">40 </code><code class="s"> of 3"</code>
<code class="lineno">41 </code>          <code class="o">}</code> <code class="k">else</code> <code class="o">{</code>
<code class="lineno">42 </code>            <code class="calibre19">sh</code> <code class="s">"source /run/secrets/aws &amp;&amp; ASG_NAME=${asgName} ASG_DESIRED_CAPAC\</code>
<code class="lineno">43 </code><code class="s">ITY=${asgNewCapacity} docker-compose run --rm asg-update-desired-capacity"</code>
<code class="lineno">44 </code>            <code class="k">if</code> <code class="o">(</code><code class="calibre19">scale</code><code class="o">.</code><code class="na">toInteger</code><code class="o">()</code> <code class="o">&gt;</code> <code class="o">0</code><code class="o">)</code> <code class="o">{</code>
<code class="lineno">45 </code>              <code class="calibre19">sleep</code> <code class="o">300</code>
<code class="lineno">46 </code>              <code class="calibre19">script</code> <code class="o">{</code>
<code class="lineno">47 </code>                <code class="kt">def</code> <code class="calibre19">servicesOut</code> <code class="o">=</code> <code class="calibre19">sh</code><code class="o">(</code>
<code class="lineno">48 </code>                  <code class="nl">script:</code> <code class="s">"docker service ls -q -f label=com.df.reschedule=true"</code><code class="o">,</code>
<code class="lineno">49 </code>                  <code class="nl">returnStdout:</code> <code class="k">true</code>
<code class="lineno">50 </code>                <code class="o">)</code>
<code class="lineno">51 </code>                <code class="kt">def</code> <code class="calibre19">services</code> <code class="o">=</code> <code class="calibre19">servicesOut</code><code class="o">.</code><code class="na">split</code><code class="o">(</code><code class="s">'\n'</code><code class="o">)</code>
<code class="lineno">52 </code>                <code class="kt">def</code> <code class="calibre19">date</code> <code class="o">=</code> <code class="k">new</code> <code class="calibre19">Date</code><code class="o">()</code>
<code class="lineno">53 </code>                <code class="k">for</code><code class="o">(</code><code class="kt">int</code> <code class="calibre19">i</code> <code class="o">=</code> <code class="o">0</code><code class="o">;</code> <code class="calibre19">i</code> <code class="o">&lt;</code> <code class="calibre19">services</code><code class="o">.</code><code class="na">size</code><code class="o">();</code> <code class="calibre19">i</code><code class="o">++)</code> <code class="o">{</code>
<code class="lineno">54 </code>                  <code class="kt">def</code> <code class="calibre19">service</code> <code class="o">=</code> <code class="calibre19">services</code><code class="o">[</code><code class="o">0</code><code class="o">]</code>
<code class="lineno">55 </code>                  <code class="calibre19">sh</code> <code class="s">"docker service update --env-add 'RESCHEDULE_DATE=${date}' \</code>
<code class="lineno">56 </code><code class="s">${service}"</code>
<code class="lineno">57 </code>                <code class="o">}</code>
<code class="lineno">58 </code>              <code class="o">}</code>
<code class="lineno">59 </code>            <code class="o">}</code>
<code class="lineno">60 </code>            <code class="calibre19">echo</code> <code class="s">"Changed the number of worker nodes from ${asgDesiredCapacity} \</code>
<code class="lineno">61 </code><code class="s">to ${asgNewCapacity}"</code>
<code class="lineno">62 </code>          <code class="o">}</code>
<code class="lineno">63 </code>        <code class="o">}</code>
<code class="lineno">64 </code>      <code class="o">}</code>
<code class="lineno">65 </code>    <code class="o">}</code>
<code class="lineno">66 </code>  <code class="o">}</code>
<code class="lineno">67 </code>  <code class="calibre19">post</code> <code class="o">{</code>
<code class="lineno">68 </code>    <code class="calibre19">success</code> <code class="o">{</code>
<code class="lineno">69 </code>      <code class="calibre19">slackSend</code><code class="o">(</code>
<code class="lineno">70 </code>        <code class="nl">color:</code> <code class="s">"good"</code><code class="o">,</code>
<code class="lineno">71 </code>        <code class="nl">message:</code> <code class="s">"""Worker nodes were scaled.</code>
<code class="lineno">72 </code><code class="s">Please check Jenkins logs for the job ${env.JOB_NAME} #${env.BUILD_NUMBER}</code>
<code class="lineno">73 </code><code class="s">${env.BUILD_URL}console"""</code>
<code class="lineno">74 </code>      <code class="o">)</code>
<code class="lineno">75 </code>    <code class="o">}</code>
<code class="lineno">76 </code>    <code class="calibre19">failure</code> <code class="o">{</code>
<code class="lineno">77 </code>      <code class="calibre19">slackSend</code><code class="o">(</code>
<code class="lineno">78 </code>        <code class="nl">color:</code> <code class="s">"danger"</code><code class="o">,</code>
<code class="lineno">79 </code>        <code class="nl">message:</code> <code class="s">"""Worker nodes could not be scaled.</code>
<code class="lineno">80 </code><code class="s">Please check Jenkins logs for the job ${env.JOB_NAME} #${env.BUILD_NUMBER}</code>
<code class="lineno">81 </code><code class="s">${env.BUILD_URL}console"""</code>
<code class="lineno">82 </code>      <code class="o">)</code>
<code class="lineno">83 </code>    <code class="o">}</code>
<code class="lineno">84 </code>  <code class="o">}</code>
<code class="lineno">85 </code><code class="o">}</code>
</pre></div>

</figure>

<p class="calibre3">Do not forget to click the <em class="calibre21">Save</em> button.</p>

<p class="calibre3">We should add the <code class="calibre19">com.df.reschedule</code> label to at least one service before we give the <code class="calibre19">aws-scale</code> job a spin.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl -o go-demo.yml <code class="se">\</code>
<code class="lineno">4 </code>    https://raw.githubusercontent.com/vfarcic/docker-flow-monitor/master/stacks/<code class="se">\</code>
<code class="lineno">5 </code>go-demo-aws.yml
<code class="lineno">6 </code>
<code class="lineno">7 </code>cat go-demo.yml
</pre></div>

</figure>

<p class="calibre3">We entered the cluster, downloaded the updated version of the <code class="calibre19">go-demo</code> stack, and displayed its content on the screen. The output of the <code class="calibre19">cat</code> command, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>version: '3'
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>services:
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>  main:
<code class="lineno"> 6 </code>    ...
<code class="lineno"> 7 </code>    deploy:
<code class="lineno"> 8 </code>      ...
<code class="lineno"> 9 </code>      labels:
<code class="lineno">10 </code>        ...
<code class="lineno">11 </code>        - com.df.reschedule=true
<code class="lineno">12 </code>        ...
</pre></div>

</figure>

<p class="calibre3">The only notable change, when compared with the previous version of the stack, is in the addition of the <code class="calibre19">com.df.reschedule</code> label.</p>

<p class="calibre3">Now we can re-deploy the stack and confirm that the updated Jenkins job works as expected.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy -c go-demo.yml <code class="se">\</code>
<code class="lineno">2 </code>    go-demo
<code class="lineno">3 </code>
<code class="lineno">4 </code><code class="nb">exit</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code>curl -XPOST -i <code class="se">\</code>
<code class="lineno">7 </code>    <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?token=DevOps2\</code>
<code class="lineno">8 </code><code class="s">2&amp;scale=1"</code>
</pre></div>

</figure>

<p class="calibre3">We deployed the stack, exited the cluster, and sent a <code class="calibre19">POST</code> request to build the <code class="calibre19">aws-scale</code> job with the <code class="calibre19">scale</code> parameter set to <code class="calibre19">1</code>.</p>

<p class="calibre3">If we go to the <code class="calibre19">aws-scale</code> activity screen, there should be a new build.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/blue/organizations/jenkins/aws-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Let’s go back to the cluster and confirm that the <code class="calibre19">go-demo</code> service was re-scheduled and, since the new node is empty (except for global services), at least one replica should end up there.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">If you were quick, the output of the <code class="calibre19">docker node ls</code> should reveal that the new node did not yet join the cluster. If that’s the case, wait for a while until AWS creates and initializes the new node and repeat the command.</p>

<p class="calibre3">Once the new node is created, please copy its ID. We’ll put it as a value of an environment variable.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">NODE_ID</code><code class="o">=[</code>...<code class="o">]</code>
</pre></div>

</figure>

<p class="calibre3">Please make sure that you replaced <code class="calibre19">[...]</code> with the actual ID of the new node.</p>

<p class="calibre3">If we continued with the fast pace and less than five minutes passed (<code class="calibre19">sleep 300</code>) since the new build started, the new node should be empty except for global services.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker node ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>running <code class="nv">$NODE_ID</code>
</pre></div>

</figure>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME                             IMAGE                      NODE                \
<code class="lineno">2 </code>                     DESIRED STATE CURRENT STATE              ERROR PORTS
<code class="lineno">3 </code>exporter_cadvisor...             google/cadvisor:latest     ip-172-31-4-4.us-eas\
<code class="lineno">4 </code>t-2.compute.internal Running       Running about a minute ago
<code class="lineno">5 </code>exporter_node-exporter-worker... basi/node-exporter:v1.14.0 ip-172-31-4-4.us-eas\
<code class="lineno">6 </code>t-2.compute.internal Running       Running about a minute ago
</pre></div>

</figure>

<p class="calibre3">Once five minutes passed, the update was executed, and the <code class="calibre19">go-demo</code> service (the only one with the <code class="calibre19">com.df.reschedule</code> label) was rescheduled. Let’s take another look at the processes running on the new node.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker node ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>running <code class="nv">$NODE_ID</code>
</pre></div>

</figure>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME                             IMAGE                      NODE                \
<code class="lineno">2 </code>                     DESIRED STATE CURRENT STATE              ERROR PORTS
<code class="lineno">3 </code>exporter_cadvisor...             google/cadvisor:latest     ip-172-31-4-4.us-eas\
<code class="lineno">4 </code>t-2.compute.internal Running       Running about a minute ago
<code class="lineno">5 </code>exporter_node-exporter-worker... basi/node-exporter:v1.14.0 ip-172-31-4-4.us-eas\
<code class="lineno">6 </code>t-2.compute.internal Running       Running about a minute ago
<code class="lineno">7 </code>go-demo_main.2                   vfarcic/go-demo:latest     ip-172-31-4-4.us-eas\
<code class="lineno">8 </code>t-2.compute.internal Running       Running 3 minutes ago
</pre></div>

</figure>

<p class="calibre3">As you can see, rescheduling worked, and one of the replicas of the <code class="calibre19">go-demo_main</code> service was deployed to the new node.</p>


<figure class="image">
  <img src="../images/00092.jpeg" alt="Figure 15-13: Prometheus initiated worker nodes scaling and service rescheduling" class="calibre17"/>
  <figcaption class="calibre18">Figure 15-13: Prometheus initiated worker nodes scaling and service rescheduling</figcaption>
</figure>


<p class="calibre3">If you’d like to test that re-scheduling is not executed when de-scaling nodes, please exit the cluster and send a <code class="calibre19">POST</code> request to Jenkins with the <code class="calibre19">scale</code> parameter set to <code class="calibre19">-1</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>curl -XPOST -i <code class="se">\</code>
<code class="lineno">4 </code>    <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/job/aws-scale/buildWithParameters?token=DevOps2\</code>
<code class="lineno">5 </code><code class="s">2&amp;scale=-1"</code>
</pre></div>

</figure>

<p class="calibre3">I’ll leave to you the tedious steps of checking Jenkins logs and confirming that re-scheduling was not executed.</p>

<p class="calibre3">Even though our goal is within our grasp, we’re not yet finished. There’s still one more critical case left to explore.</p>

<h3 id="leanpub-auto-scaling-nodes-when-replica-state-is-pending" class="calibre20">Scaling Nodes When Replica State Is Pending</h3>

<p class="calibre3">A replica of a service might be in the pending state. There might be quite a few reasons for that, and we won’t go through all of them. Instead, we’ll explore one of the most common causes behind having a replica pending deployment. A service might have memory reservation that cannot be fulfilled with the current cluster.</p>

<p class="calibre3">Let’s say that a service has memory reservation set to 3GB. All the replicas of that service are running but, at one moment, the system scales that service by increasing the number of replicas by one. What happens if none of the nodes have 3GB of unreserved memory? Docker Swarm will set the status of the new replica to pending, hoping that 3GB will be available in the future.</p>

<p class="calibre3">Such a situation might not be discovered with any of the existing alerts. The used memory of each of the nodes might be below the threshold (e.g., 80%). The total used memory of the cluster might be below the threshold as well.</p>

<p class="calibre3">All in all, the system scaled the service, but the new replica cannot be deployed because there are not enough un-reserved resources, and none of the existing alerts noticed an anomaly. To make things even more complicated, if scaling was initiated as, for example, the result of slow response times, the same alert will fire again since the problem was not solved. Without the new replica, response times will continue being slow.</p>

<p class="calibre3">We can fix that problem by evaluating whether the number of containers that belong to a service matches the number of replicas. If, for example, we intend to have five replicas, we should have an alert that confirms that all five replicas are indeed running. Before we try to create such an alert, we should explore a query that will return the number of replicas of a given service.</p>

<p class="calibre3">Let’s go back to Prometheus’ UI.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor"</code>
</pre></div>

</figure>

<p class="calibre3">Since the number of replicas is the same as the number of running containers, the query can be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>count<code class="o">(</code>container_memory_usage_bytes<code class="o">{</code>container_label_com_docker_swarm_service_name<code class="se">\</code>
<code class="lineno">2 </code><code class="o">=</code><code class="s">"go-demo_main"</code><code class="o">})</code>
</pre></div>

</figure>

<p class="calibre3">The query counts the number of metrics with, in the case, the label set to <code class="calibre19">go-demo_main</code>.</p>

<p class="calibre3">Please type the query into the <em class="calibre21">Expression</em> field, press the <em class="calibre21">Execute</em> button, and select the <em class="calibre21">Graph</em> tab. You should see that we are currently running three replicas of the service. If you are a fast reader, the result of the query might have revealed six replicas. When the system updated the <code class="calibre19">go-demo_main</code> service, it created three new containers and removed the old ones. Metrics from the old containers might be included in the <code class="calibre19">count</code>. If that’s the case, wait for a few moments and repeat the query.</p>

<p class="calibre3">The expression we explored, translated to an alert, is as follows (do not try to execute it).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>count(container_memory_usage_bytes{container_label_com_docker_swarm_service_name\
<code class="lineno">2 </code>="go-demo_main"}) != 3
</pre></div>

</figure>

<p class="calibre3">The alert would fire if the number of running containers (replicas) is different than the expected number (in this case <code class="calibre19">3</code>). Since Swarm needs a bit of time to pull images and, in case of a failure, reschedule replicas, we’d have to combine such an <code class="calibre19">IF</code> logic with the <code class="calibre19">FOR</code> statement so that the alert does not produce false positives.</p>

<p class="calibre3">There’s one more thing left to discuss. How do we get the desired number of replicas? We cannot hard-code a value to the alert since it would produce undesirable results when the service is scaled. It needs to be dynamic. The alert needs to change every time the desired number of replicas changes.</p>

<p class="calibre3">Fortunately, <a href="http://swarmlistener.dockerflow.com/">Docker Flow Swarm Listener</a> is, among other parameters, sending the number of replicas of a service to all its notification addresses. <a href="http://monitor.dockerflow.com/">Docker Flow Monitor</a>, on the other hand, already has the shortcut <code class="calibre19">@replicas_running</code> that will expand into the alert we discussed and use the number of replicas from the listener. In other words, all we have to do is define <code class="calibre19">@replicas_running</code> as one more label of the service.</p>

<aside class="tip">
    <p class="calibre3">Please consult <a href="http://monitor.dockerflow.com/usage/#alertif-parameter-shortcuts">AlertIf Parameter Shortcuts</a> for more info about the <code class="calibre19">@replicas_running</code> shortcut.</p>

</aside>

<p class="calibre3">I forgot to mention one more thing. Prometheus is already running that alert. It was defined in the last <code class="calibre19">go-demo</code> stack definition. So, let’s take another look at the YAML file we used previously.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>cat go-demo.yml
</pre></div>

</figure>

<p class="calibre3">We went back into the cluster and listed the contents of the <code class="calibre19">go-demo.yml</code> file. The relevant parts are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">services</code><code class="o">:</code>
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>  <code class="calibre19">main</code><code class="o">:</code>
<code class="lineno"> 4 </code>    <code class="o">...</code>
<code class="lineno"> 5 </code>    <code class="calibre19">deploy</code><code class="o">:</code>
<code class="lineno"> 6 </code>      <code class="o">...</code>
<code class="lineno"> 7 </code>      <code class="calibre19">labels</code><code class="o">:</code>
<code class="lineno"> 8 </code>        <code class="o">...</code>
<code class="lineno"> 9 </code>        <code class="o">-</code> <code class="calibre19">com</code><code class="o">.</code><code class="na">df</code><code class="o">.</code><code class="na">alertName</code><code class="o">.</code><code class="o">4</code><code class="o">=</code><code class="calibre19">replicas_running</code>
<code class="lineno">10 </code>        <code class="o">-</code> <code class="calibre19">com</code><code class="o">.</code><code class="na">df</code><code class="o">.</code><code class="na">alertIf</code><code class="o">.</code><code class="o">4</code><code class="o">=</code><code class="err">@</code><code class="calibre19">replicas_running</code>
<code class="lineno">11 </code>        <code class="o">-</code> <code class="calibre19">com</code><code class="o">.</code><code class="na">df</code><code class="o">.</code><code class="na">alertFor</code><code class="o">.</code><code class="o">4</code><code class="o">=</code><code class="o">10</code><code class="calibre19">m</code>
<code class="lineno">12 </code>      <code class="o">...</code>
</pre></div>

</figure>

<p class="calibre3">There’s no big mystery in those labels. They follow the same pattern as all other Prometheus-related labels we used throughout the book. The shortcut will expand into <code class="calibre19">count(container_memory_usage_bytes{container_label_com_docker_swarm_service_name="go-demo_main"}) != 3</code>. If we change the desired number of replicas, the listener will send a new request to the monitor, and the alert will change accordingly.</p>

<p class="calibre3">You’ll notice that the <code class="calibre19">alertFor</code> label is set to <code class="calibre19">10m</code>. If a Docker image is big, it might take more than ten minutes to deploy a replica, and you might want to increase that time. On top of that, you should keep in mind that the more replicas we have, the longer it might take Swarm to deploy them all. However, since <code class="calibre19">go-demo</code> is very light, and we’re running only a few replicas, ten minutes should be more than enough. If all the replicas are not running within ten minutes, the alert should fire.</p>

<p class="calibre3">Let’s confirm that the alert is indeed registered in Prometheus.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">Please observe the alert <em class="calibre21">godemo_main_replicas_running</em>. If should contain the definition we discussed.</p>

<p class="calibre3">We should test whether the system works so now we need to figure out how force Docker Swarm to create a replica in the <code class="calibre19">pending</code> state. But, before we do that, we need to deal with the intervals we set in <em class="calibre21">Alertmanager</em> configuration.</p>

<p class="calibre3"><em class="calibre21">Alertmanager</em> is grouping alerts by labels <code class="calibre19">service</code>, <code class="calibre19">scale</code>, and <code class="calibre19">type</code>, and has parameters <code class="calibre19">repeat_interval</code> and <code class="calibre19">group_interval</code> both set to <code class="calibre19">30m</code>. That means that an alert will be propagated to one of the receivers only if more than an hour passed since the last one with the same labels. In other words, even though Prometheus is firing the alert, Alertmanager might be discarding it if less then an hour passed since the last time we scaled the nodes.</p>

<p class="calibre3">If you are impatient and do not want to wait for an hour, we can remove <em class="calibre21">Alertmanager</em> and put it back up again.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker service scale <code class="se">\</code>
<code class="lineno">4 </code>    monitor_alert-manager<code class="o">=</code><code class="o">0</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code>docker service scale <code class="se">\</code>
<code class="lineno">7 </code>    monitor_alert-manager<code class="o">=</code><code class="o">1</code>
</pre></div>

</figure>

<p class="calibre3">Scaling to zero and back up to one means that <em class="calibre21">Alertmanager</em> would start over and, as a result, we would not need to wait for an hour to test the new alert.</p>

<p class="calibre3">Now we can go back to the task at hand.</p>

<p class="calibre3">We can, for example, change memory reservation of the service to 1.5GB. Since our nodes have 2GB each, that should result in one of the replicas in the pending state. To be on the safe side, we can also increase the number of replicas to four.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --reserve-memory 1500M <code class="se">\</code>
<code class="lineno">3 </code>    --replicas <code class="o">4</code> <code class="se">\</code>
<code class="lineno">4 </code>    go-demo_main
</pre></div>

</figure>

<p class="calibre3">We entered the cluster and updated the service.</p>

<p class="calibre3">Since Swarm is doing rolling updates, and it takes approximately twenty seconds for each replica, we should wait for a minute or two until all the replicas are updated.</p>

<p class="calibre3">Let’s take a look at the stack processes.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>running go-demo
</pre></div>

</figure>

<p class="calibre3">The output is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>NAME           IMAGE                  NODE                                      \
<code class="lineno"> 2 </code>  DESIRED STATE CURRENT STATE          ERROR PORTS
<code class="lineno"> 3 </code>go-demo_main.1 vfarcic/go-demo:latest ip-172-31-47-61.us-east-2.compute.internal\
<code class="lineno"> 4 </code>  Running       Running 2 minutes ago
<code class="lineno"> 5 </code>go-demo_db.1   mongo:latest           ip-172-31-47-61.us-east-2.compute.internal\
<code class="lineno"> 6 </code>  Running       Running 8 minutes ago
<code class="lineno"> 7 </code>go-demo_main.2 vfarcic/go-demo:latest ip-172-31-36-187.us-east-2.compute.interna\
<code class="lineno"> 8 </code>l Running       Running 53 seconds ago
<code class="lineno"> 9 </code>go-demo_main.3 vfarcic/go-demo:latest ip-172-31-36-187.us-east-2.compute.interna\
<code class="lineno">10 </code>l Running       Running 43 seconds ago
<code class="lineno">11 </code>go-demo_main.4 vfarcic/go-demo:latest                                           \
<code class="lineno">12 </code>  Running       Pending 20 seconds ago
</pre></div>

</figure>

<p class="calibre3">As you can see, Swarm could not deploy one of the replicas. None of the nodes has enough un-reserved memory so the state of one of them is <code class="calibre19">pending</code>.</p>

<p class="calibre3">Let’s see what happens with the alert.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">Since the <code class="calibre19">@replicas_running</code> shortcut creates labels <code class="calibre19">scale=up</code> and <code class="calibre19">type=node</code>, there’s no need to modify <em class="calibre21">Alertmanager</em> config nor the <code class="calibre19">aws-scale</code> Jenkins job.</p>

<p class="calibre3">Given that we set <code class="calibre19">alertFor</code> to <code class="calibre19">10m</code>, the <em class="calibre21">godemo_main_replicas_running</em> alert should be red ten minutes after we executed the <code class="calibre19">docker service update</code> command.</p>

<p class="calibre3">For a short time, you might see “strange” numbers generated by the alert. For example, it might be in the pending state, saying that there are five containers instead four, while we’re expecting to see three. Those “strange” results might be due to caching. The alert might be taking into account the old containers, those that were replaced with the recent update. Fear not. A short while later, the alert will stop counting the old containers and will report that there are three running, while it is expecting four. Ten minutes later it’ll fire the alert.</p>

<p class="calibre3">Finally, let’s confirm that Jenkins executed a new build.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="nv">$CLUSTER_DNS</code><code class="s">/jenkins/blue/organizations/jenkins/aws-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">As you can see, a new build was executed, or, if less than five minutes passed, is about to finish. The auto-scaling group has been modified, and a new worker node joined the cluster.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>ssh -i <code class="nv">$KEY_NAME</code>.pem docker@<code class="nv">$CLUSTER_IP</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker node ls
</pre></div>

</figure>

<p class="calibre3">The output of the <code class="calibre19">node ls</code> command is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>HOSTNAME                                      STATUS              AVAILABILITY  \
<code class="lineno"> 2 </code>      MANAGER STATUS
<code class="lineno"> 3 </code>ip-172-31-15-5.us-east-2.compute.internal     Ready               Active        \
<code class="lineno"> 4 </code>      Reachable
<code class="lineno"> 5 </code>ip-172-31-26-189.us-east-2.compute.internal   Ready               Active        \
<code class="lineno"> 6 </code>      Reachable
<code class="lineno"> 7 </code>ip-172-31-29-239.us-east-2.compute.internal   Ready               Active
<code class="lineno"> 8 </code>ip-172-31-36-140.us-east-2.compute.internal   Ready               Active
<code class="lineno"> 9 </code>ip-172-31-36-153.us-east-2.compute.internal   Ready               Active        \
<code class="lineno">10 </code>      Leader
</pre></div>

</figure>

<p class="calibre3">A new worker node was created. There should be three manager and two worker nodes. If you don’t see the new node, please wait for a while and re-run the <code class="calibre19">docker node ls</code> command.</p>

<p class="calibre3">Let’s see what’s going on with replicas of the <code class="calibre19">go-demo_main</code> service.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>running go-demo
</pre></div>

</figure>

<p class="calibre3">Swarm found of that, with the additional node, there is enough un-reserved memory and deployed the pending replica.</p>

<h3 id="leanpub-auto-what-now-13" class="calibre20">What Now?</h3>

<p class="calibre3">We have a self-sufficient system! It can self-heal and self-adapt. It can work without any humans around. We built <a href="http://www.imdb.com/title/tt0133093/">Matrix</a>! We’ll… We’re not quite there yet. You will have to observe metrics, look for patterns, create new alerts, and so on. You will have to be behind the system we built so far and continue perfecting it. What we have, for now, is a solid base that you will need to expand. You’ll have to use the knowledge you got so far and adapt the examples to suit your own needs.</p>

<p class="calibre3">There are many other combinations and formulas you might want to define as alerts. You might want to perform some actions when CPU usage is too high or when a disk is almost full. I’ll leave that to you with a word of caution. Don’t go crazy. Don’t create too many alerts. Don’t saturate humans with notifications and try to avoid having the system collapse on itself with unreliable alerts. Observe metrics for a while. Try to find patterns. Ask yourself what should be the action when you notice some spike. Define and validate a hypothesis. Wait some more. Repeat the cycle a few more times.</p>

<p class="calibre3">You should extend your alerts only after you’re confident in your observations and actions that should be performed.</p>

<p class="calibre3">Before we move on, please delete the stack we created.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nb">exit</code>
<code class="lineno">2 </code>
<code class="lineno">3 </code>aws cloudformation delete-stack <code class="se">\</code>
<code class="lineno">4 </code>    --stack-name devops22
</pre></div>

</figure>



</div>
</body></html>