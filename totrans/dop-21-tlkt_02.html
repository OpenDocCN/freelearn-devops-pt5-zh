<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Setting Up and Operating a Swarm Cluster</h1>
            </header>

            <article>
                
<div class="packt_quote"><em>Organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations <br/>
                                                                                                                    –M.Conway<br/></em>                                                                                                                   </div>
<p>Many will tell you that they have a <em>scalable system</em>. After all, scaling is easy. Buy a server, install WebLogic (or whichever other monster application server you're using) and deploy your applications. Then wait for a few weeks until you discover that everything is so <em>fast</em> that you can click a button, have some coffee, and, by the time you get back to your desk, the result will be waiting for you. What do you do? You scale. You buy a few more servers, install your monster application servers and deploy your monster applications on top of them. Which part of the system was the bottleneck? Nobody knows. Why did you duplicate everything? Because you must. And then some more time passes, and you continue scaling until you run out of money and, simultaneously, people working for you go crazy. Today we do not approach scaling like that. Today we understand that scaling is about many other things. It's about elasticity. It's about being able to quickly and easily scale and de-scale depending on variations in your traffic and growth of your business, and that you should not go bankrupt during the process. It's about the need of almost every company to scale their business without thinking that IT department is a liability. It's about getting rid of those monsters.<br/></p>
<div class="packt_tip"><strong>A note to The DevOps 2.0 Toolkit readers</strong><br/>
The text that follows is identical to the one published in <em>The DevOps 2.0 Toolkit</em>. If it is still fresh in your mind, feel free to jump to the section <em>Docker Swarm Mode</em> of this chapter. You'll see that a lot has changed. One of those changes is that the old Swarm running as a separate container is deprecated for <em>Swarm Mode</em>. There are many other new things we'll discover along the way.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Scalability</h1>
            </header>

            <article>
                
<p>Let us, for a moment take a step back and discuss why we want to scale applications. The main reason is <em>high availability</em>. Why do we want high availability? We want it because we want our business to be available under any load. The bigger the load, the better (unless you are under DDoS). It means that our business is booming. With high availability our users are happy. We all want speed, and many of us simply leave the site if it takes too long to load. We want to avoid having outages because every minute our business is not operational can be translated into a money loss. What would you do if an online store is not available? Probably go to another. Maybe not the first time, maybe not the second, but, sooner or later, you would get fed up and switch it for another. We are used to everything being fast and responsive, and there are so many alternatives that we do not think twice before trying something else. And if that something else turns up to be better. One man's loss is another man's gain. Do we solve all our problems with scalability? Not even close. Many other factors decide the availability of our applications. However, scalability is an important part of it, and it happens to be the subject of this chapter.</p>
<p>What is scalability? It is a property of a system that indicates its ability to handle increased load in a graceful manner or its potential to be enlarged as demand increases. It is the capacity to accept increased volume or traffic.</p>
<p>The truth is that the way we design our applications dictates the scaling options available. Applications will not scale well if they are not designed to scale. That is not to say that an application not designed for scaling cannot scale. Everything can scale, but not everything can scale well.</p>
<p>Commonly observed scenario is as follows.</p>
<p>We start with a simple architecture, sometimes with load balancer sometimes without, setup a few application servers and one database. Everything is great, complexity is low, and we can develop new features very fast. The cost of operations is low, income is high (considering that we just started), and everyone is happy and motivated.</p>
<p>Business is growing, and the traffic is increasing. Things are beginning to fail, and performance is dropping. Firewalls are added, additional load balancers are set up, the database is scaled, more application servers are added and so on. Things are still relatively straightforward. We are faced with new challenges, but we can overcome obstacles in time. Even though the complexity is increasing, we can still handle it with relative ease. In other words, what we're doing is still, more or less, the same but bigger. Business is doing well, but it is still relatively small.</p>
<p>And then it happens. The big thing you've been waiting for. Maybe one of the marketing campaigns hit the spot. Maybe there was an adverse change in your competition. Maybe that last feature was indeed a killer one. No matter the reasons, business got a big boost. After a short period of happiness due to this change, your pain increases tenfold. Adding more databases does not seem to be enough. Multiplying application servers does not appear to fulfill the needs. You start adding caching and what not. You start getting the feeling that every time you multiply something, benefits are not equally big. Costs increase, and you are still not able to meet the demand. Database replications are too slow. New application servers do not make such a big difference anymore. Operational costs are increasing faster than you expected. The situation hurts the business and the team. You are starting to realize that the architecture you were so proud of cannot fulfill this increase in load. You cannot split it. You cannot scale things that hurt the most. You cannot start over. All you can do is continue multiplying with ever decreasing benefits of such actions.</p>
<p>The situation described above is quite common. What was good at the beginning, is not necessarily right when the demand increases. We need to balance the need for <strong>You ain't gonna need it</strong> (<strong>YAGNI</strong>) principle and the longer term vision. We cannot start with the system optimized for large companies because it is too expensive and does not provide enough benefits when business is small. On the other hand, we cannot lose the focus from one of the primary objectives of any business. We cannot not think about scaling from the very first day. Designing scalable architecture does not mean that we need to start with a cluster of a hundred servers. It does not mean that we have to develop something big and complex from the start. It means that we should start small, but in the way that, when it becomes big, it is easy to scale. While microservices are not the only way to accomplish that goal, they are indeed a good way to approach this problem. The cost is not in development but operations. If operations are automated, that cost can be absorbed quickly and does not need to represent a massive investment. As you already saw (and will continue seeing throughout the rest of the book), there are excellent open source tools at our disposal. The best part of automation is that the investment tends to have lower maintenance cost than when things are done manually.</p>
<p>We already discussed microservices and automation of their deployments on a tiny scale. Now it's time to convert this small scale to something bigger. Before we jump into practical parts, let us explore what are some of the different ways one might approach scaling.</p>
<p>We are often limited by our design and choosing the way applications are constructed limits our choices severely. Although there are many different ways to scale, the most common one is called <em>Axis Scaling</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Axis scaling</h1>
            </header>

            <article>
                
<p>Axis scaling can be best represented through three dimensions of a cube; <em>X-Axis</em>, <em>Y-Axis</em>, and <em>Z-Axis</em>. Each of those dimensions describes a type of scaling:</p>
<ul>
<li>X-Axis: Horizontal duplication</li>
<li>Y-Axis: Functional decomposition</li>
<li>Z-Axis: Data partitioning</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="205" src="assets/scale-cube.png" width="290"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">  <span>Figure 2-1: Scale cube <br/></span></div>
<p>Let's go through the Axes, one at a time.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">X-axis scaling</h1>
            </header>

            <article>
                
<p>In a nutshell, <em>x-axis</em> scaling is accomplished by running multiple instances of an application or a service. In most cases, there is a load balancer on top that makes sure that the traffic is shared among all those instances. The biggest advantage of <em>x-axis</em> scaling is simplicity. All we have to do is deploy the same application on multiple servers. For that reason, this is the most commonly used type of scaling. However, it comes with its set of disadvantages when applied to monolithic applications.</p>
<p>Having a large application usually requires a big cache that demands heavy usage of memory. When such an application is multiplied, everything is multiplied with it, including the cache. Another, often more important, problem is an inappropriate usage of resources. Performance issues are almost never related to the whole application. Not all modules are equally affected, and, yet, we multiply everything. That means that even though we could be better off by scaling only part of the application that requires such an action, we scale everything. Nevertheless, <em>x- axis</em> scaling is important no matter the architecture. The major difference is the effect that such a scaling has.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="318" src="assets/monolith-scaling.png" width="462"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2-2: Monolithic application scaled inside a cluster<span><br/></span></div>
<p>By using microservices, we are not removing the need for <em>x-axis</em> scaling but making sure that due to their architecture such scaling has more effect than with alternative and more traditional approaches to architecture. With microservices, we have the option to fine-tune scaling. We can have many instances of services that suffer a lot under heavy load and only a few instances of those that are used less often or require fewer resources. On top of that, since they are small, we might never reach a limit of a service. A small service in a big server would need to receive a truly massive amount of traffic before the need for scaling arises. Scaling microservices is more often related to fault tolerance than performance problems. We want to have multiple copies running so that, if one of them dies, the others can take over until recovery is performed.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Y-axis scaling</h1>
            </header>

            <article>
                
<p>Y-axis scaling is all about decomposition of an application into smaller services. Even though there are different ways to accomplish this decomposition, microservices are probably the best approach we can take. When they are combined with immutability and self-sufficiency, there is indeed no better alternative (at least from the prism of y-axis scaling). Unlike x-axis scaling, the y-axis is not accomplished by running multiple instances of the same application but by having multiple different services distributed across the cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Z-axis scaling</h1>
            </header>

            <article>
                
<p>Z-axis scaling is rarely applied to applications or services. Its primary and most common usage is among databases. The idea behind this type of scaling is to distribute data among multiple servers thus reducing the amount of work that each of them needs to perform. Data is partitioned and distributed so that each server needs to deal only with a subset of the data. This type of separation is often called <strong>sharding</strong>, and there are many databases specially designed for this purpose. Benefits of z-axis scaling are most noticeable in I/O and cache and memory utilization.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Clustering</h1>
            </header>

            <article>
                
<p>A server cluster consists of a set of connected servers that work together and can be seen as a single system. They are usually connected to a fast <strong>Local Area Network </strong>(<strong>LAN</strong>). The significant difference between a cluster and a group of servers is that the cluster acts as a single system trying to provide high availability, load balancing, and parallel processing.</p>
<p>If we deploy applications, or services, to individually managed servers and treat them as separate units, the utilization of resources is sub-optimum. We cannot know in advance which group of services should be deployed to a server and utilize resources to their maximum. Moreover, resource usage tends to fluctuate. While, in the morning, some service might require a lot of memory, during the afternoon that usage might be lower. Having predefined servers prevents us from having elasticity that would balance that usage in the best possible way. Even if such a high level of dynamism is not required, predefined servers tend to create problems when something goes wrong, resulting in manual actions to redeploy the affected services to a healthy node:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/servers.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2-3: Cluster with containers deployed to predefined servers<span><br/></span></div>
<p>Real clustering is accomplished when we stop thinking in terms of individual servers and start thinking of a cluster; of all servers as one big entity. That can be better explained if we drop to a bit lower level. When we deploy an application, we tend to specify how much memory or CPU it might need. However, we do not decide which memory slots our application will use nor which CPUs it should utilize. For example, we don’t specify that some application should use CPUs 4, 5 and 7. That would be inefficient and potentially dangerous. We only decide that three CPUs are required. The same approach should be taken on a higher level. We should not care where an application or a service will be deployed but what it needs. We should be able to define that the service has certain requirements and tell some tool to deploy it to whichever server in our cluster, as long as it fulfills the needs we have. The best (if not the only) way to accomplish that is to consider the whole cluster as one entity.</p>
<p>We can increase or decrease the capacity of a cluster by adding or removing servers but, no matter what we do, it should still be a single entity. We define a strategy and let our services be deployed somewhere inside the cluster. Those using cloud providers like <strong>Amazon Web Services </strong>(<strong>AWS</strong>), Microsoft Azure and <strong>Google Compute Engine </strong>(<strong>GCE</strong>) are already accustomed to this approach, even though they might not be aware of it.</p>
<p>Throughout the rest of this chapter, we'll explore ways to create our cluster and explore tools that can help us with that objective. The fact that we'll be simulating the cluster locally does not mean that the same strategies cannot be applied to public or private clouds and data centers. Quite the opposite:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/cluster.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2-4: Cluster with containers deployed to servers based on a predefined strategy<span><br/></span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Docker Swarm Mode</h1>
            </header>

            <article>
                
<p><em>Docker Engine v1.12</em> was released in July 2016. It is the most significant version since <em>v1.9</em>. Back then, we got Docker networking that, finally, made containers ready for use in clusters. With <em>v1.12</em>, Docker is reinventing itself with a whole new approach to cluster orchestration. Say goodbye to Swarm as a separate container that depends on an external data registry and welcome the <em>new Docker Swarm</em> or <em>Swarm Mode</em>. Everything you'll need to manage your cluster is now incorporated into Docker Engine. Swarm is there. Service discovery is there. Improved networking is there. That does not mean that we do not need additional tools. We do. The major difference is that Docker Engine now incorporates all the "essential" (not to say minimal) tools we need.</p>
<p>The old Swarm (before <em>Docker v1.12</em>) used <em>fire-and-forget principle</em>. We would send a command to Swarm master, and it would execute that command. For example, if we would send it something like <kbd>docker-compose scale go-demo=5</kbd>, the old Swarm would evaluate the current state of the cluster, discover that, for example, only one instance is currently running, and decide that it should run four more. Once such a decision is made, the old Swarm would send commands to Docker Engines. As a result, we would have five containers running inside the cluster. For all that to work, we were required to set up Swarm agents (as separate containers) on all the nodes that form the cluster and hook them into one of the supported data registries (Consul, etcd, and Zookeeper).</p>
<p>The problem was that Swarm was executing commands we send it. It was not maintaining the desired state. We were, effectively, telling it what we want to happen (example: scale up), not the state we wanted (make sure that five instances are running). Later on, the old Swarm got the feature that would reschedule containers from failed nodes. However, that feature had a few problems that prevented it from being a reliable solution (example: failed containers were not removed from the overlay network).</p>
<p>Now we got a brand new Swarm. It is part of Docker Engine (no need to run it as separate containers), it has incorporated service discovery (no need to set up Consul or whatever is your data registry of choice), it is designed from the ground up to accept and maintain the desired state, and so on. It is a truly major change in how we deal with cluster orchestration.<br/>
<br/>
In the past, I was inclined towards the old Swarm more than Kubernetes. However, that inclination was only slight. There were pros and cons for using either solution. Kubernetes had a few features Swarm was missing (example: the concept of the desired state), the old Swarm shined with its simplicity and low usage of resources. With the new Swarm (the one that comes with <em>v1.12</em>), I have no more doubts which one to use. <em>The new Swarm is often a better choice than Kubernetes</em>. It is part of Docker Engine, so the whole setup is a single command that tells an engine to join the cluster. The new networking works like a charm. The bundle that can be used to define services can be created from Docker Compose files, so there is no need to maintain two sets of configurations (Docker Compose for development and a different one for orchestration). Most importantly, the new Docker Swarm continues being simple to use. From the very beginning, Docker community pledged that they are committed to simplicity and, with this release, they, once again, proved that to be true.</p>
<p>And that's not all. The new release comes with a lot of other features that are not directly related with Swarm. However, this book is dedicated to cluster management. Therefore, I'll focus on Swarm and leave the rest for one of the next books or a blog article.</p>
<p>Since I believe that code explains things better than words, we'll start with a demo of some of the new features introduced in <em>version 1.12</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up a Swarm cluster</h1>
            </header>

            <article>
                
<p>We'll continue using Docker Machine since it provides a very convenient way to simulate a cluster on a laptop. Three servers should be enough to demonstrate some of the key features of a Swarm cluster:</p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>02-docker-swarm.sh</kbd> (<a href="https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896">https://gist.github.com/vfarcic/750fc4117bad9d8619004081af171896</a>) Gist</div>
<pre>
<strong><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>; <span class="hljs-keyword">do</span><br/>    docker-machine create <span class="hljs-operator">-d</span> virtualbox node-<span class="hljs-variable">$i</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>At this moment, we have three nodes. Please note that those servers are not running anything but Docker Engine.</p>
<p>We can see the status of the nodes by executing the following <kbd>ls</kbd> command:</p>
<pre>
<strong>docker-machine ls</strong>
</pre>
<p>The output is as follows (ERROR column removed for brievity):</p>
<pre>
<strong>NAME   ACTIVE DRIVER        STATE  URL                       SWARM DOCKER<br/>node-<span class="hljs-number">1</span> -      virtualbox    Running tcp:<span class="hljs-comment">//192.168.99.100:2376 v1.12.1</span><br/>node-<span class="hljs-number">2</span> -      virtualbox    Running tcp:<span class="hljs-comment">//192.168.99.101:2376 v1.12.1</span><br/>node-<span class="hljs-number">3</span> -      virtualbox    Running tcp:<span class="hljs-comment">//192.168.99.102:2376 v1.12.1</span></strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/nodes.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2-5: Machines running Docker Engines</div>
<p>With the machines up and running we can proceed and set up the Swarm cluster.</p>
<p>The cluster setup consists of two types of commands. We need to start by initializing the first node which will be our manager. Refer to the following illustration:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env node-<span class="hljs-number">1</span>)<br/><br/>docker swarm init \<br/>    --advertise-addr $(docker-machine ip node-<span class="hljs-number">1</span>)</strong>
</pre>
<p>The first command set environment variables so that the local Docker Engine is pointing to the <kbd>node-1</kbd>. The second initialized Swarm on that machine.</p>
<p>We specified only one argument with the <kbd>swarm init</kbd> command. The <kbd>--advertise-addr</kbd> is the address that this node will expose to other nodes for internal communication.</p>
<p>The output of the <kbd>swarm init</kbd> command is as follows:</p>
<pre>
<strong>Swarm initialized: current node (<span class="hljs-number">1</span>o5k7hvcply6g2excjiqqf4ed) is now <span class="hljs-operator">a</span> manager.<br/><br/>To <span class="hljs-built_in">add</span> <span class="hljs-operator">a</span> worker <span class="hljs-built_in">to</span> this swarm, run <span class="hljs-operator">the</span> following <span class="hljs-command"><span class="hljs-keyword">command</span>:</span><br/>    docker swarm join \<br/><span class="hljs-comment">--token SWMTKN-1-3czblm3rypyvrz6wyijsuwtmk1ozd7giqip0m \<br/>6k0b3hllycgmv-3851i2gays638e7unmp2ng3az \</span><br/><span class="hljs-number">192.168</span><span class="hljs-number">.99</span><span class="hljs-number">.100</span>:<span class="hljs-number">2377</span><br/><br/>To <span class="hljs-built_in">add</span> <span class="hljs-operator">a</span> manager <span class="hljs-built_in">to</span> this swarm, run <span class="hljs-operator">the</span> following <span class="hljs-command"><span class="hljs-keyword">command</span>:</span><br/>    docker swarm join \<br/><span class="hljs-comment">--token SWMTKN-1-3czblm3rypyvrz6wyijsuwtmk1ozd7giqi \<br/>p0m6k0b3hllycgmv-6oukeshmw7a295vudzmo9mv6i \</span><br/><span class="hljs-number">192.168</span><span class="hljs-number">.99</span><span class="hljs-number">.100</span>:<span class="hljs-number">2377</span></strong>
</pre>
<p>We can see that the node is now a manager and we've got the commands we can use to join other nodes to the cluster. As a way to increase security, a new node can be added to the cluster only if it contains the token generated when Swarm was initialized. The token was printed as a result of the <kbd>docker swarm init</kbd> command. You can copy and paste the code from the output or use the <kbd>join-token</kbd> command. We'll use the latter.</p>
<p>Right now, our Swarm cluster consists of only one VM. We'll add the other two nodes to the cluster. But, before we do that, let us discuss the difference between a <em>manager</em> and a <em>worker</em>.</p>
<p>A Swarm manager continuously monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run ten replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the ones that failed. Swarm manager assigns new replicas to workers that are running and available. A manager has all the capabilities of a worker.</p>
<p>We can get a token required for adding additional nodes to the cluster by executing the <kbd>swarm join-token</kbd> command.</p>
<p>The command to obtain a token for adding a manager is as follows:</p>
<pre>
<strong>docker swarm join-token -q manager</strong>
</pre>
<p>Similarly, to get a token for adding a worker, we would run the command that follows:</p>
<pre>
<strong>docker swarm join-token -q worker</strong>
</pre>
<p>In both cases, we'd get a long hashed string.</p>
<p>The output of the worker token is as follows:</p>
<pre>
<strong>SWMTKN-1-3czblm3rypyvrz6wyijsuwtmk1ozd7giqip0m6k0b3hll \          ycgmv-3851i2gays638\<br/>e7unmp2ng3az</strong>
</pre>
<p>Please note that this token was generated on my machine and, in your case, it will be different.</p>
<p>Let's put the token into an environment variable and add the other two nodes as workers:</p>
<pre>
<strong>TOKEN=$(docker swarm join-token -q worker)</strong>
</pre>
<p>Now that have the token inside a variable, we can issue the command that follows:</p>
<pre>
<strong><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>; <span class="hljs-keyword">do</span><br/><span class="hljs-built_in">eval</span> $(docker-machine env node-<span class="hljs-variable">$i</span>)<br/><br/>  docker swarm join \<br/>    --token <span class="hljs-variable">$TOKEN</span> \<br/>    --advertise-addr $(docker-machine ip node-<span class="hljs-variable">$i</span>) \<br/>    $(docker-machine ip node-<span class="hljs-number">1</span>):<span class="hljs-number">2377</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>The command we just ran iterates over nodes two and three and executes the <kbd>swarm join command</kbd>. We set the token, the advertise address, and the address of our manager. As a result, the two machines joined the cluster as workers. We can confirm that by sending the <kbd>node ls</kbd> command to the manager node <kbd>node-1</kbd>:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env node-<span class="hljs-number">1</span>)<br/><br/>docker node ls</strong>
</pre>
<p>The output of the <kbd>node ls</kbd> command is as follows:</p>
<pre>
<strong>ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS<br/><span class="hljs-number">3</span>vlq7dsa8g2sqkp6vl911nha8    node-<span class="hljs-number">3</span>    Ready   <span class="hljs-keyword">Active</span><br/><span class="hljs-number">6</span>cbtgzk19rne5mzwkwugiolox    node-<span class="hljs-number">2</span>    Ready   <span class="hljs-keyword">Active</span><br/>b644vkvs6007rpjre2bfb8cro *  node-<span class="hljs-number">1</span>    Ready   <span class="hljs-keyword">Active</span>        Leader</strong>
</pre>
<p>The asterisk tells us which node we are currently using. The <kbd>MANAGER STATUS</kbd> indicates that the <kbd>node-1</kbd> is the <em>leader:</em></p>
<p><img class="image-border" src="assets/swarm-nodes.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2-6: Docker Swarm cluster with three nodes<span><br/></span></div>
<p>In a production environment, we would probably set more than one node to be a manager and, thus, avoid deployment downtime if one of them fails. For the purpose of this demo, having one manager should suffice.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Deploying services to the Swarm cluster</h1>
            </header>

            <article>
                
<p>Before we deploy a demo service, we should create a new network so that all containers that constitute the service can communicate with each other no matter on which nodes they are deployed:</p>
<pre>
<strong>docker network create --driver overlay go-demo</strong>
</pre>
<p>The next chapter will explore networking in more details. Right now, we'll discuss and do only the absolute minimum required for an efficient deployment of services inside a Swarm cluster.</p>
<p>We can check the status of all networks with the command that follows:</p>
<pre>
<strong>docker network ls</strong>
</pre>
<p>The output of the <kbd>network ls</kbd> command is as follows:</p>
<pre>
<strong>NETWORK ID   NAME            DRIVER  SCOPE<br/>e263fb34287a bridge          bridge  <span class="hljs-built_in">local</span><br/>c5b60cff0f83 docker_gwbridge bridge  <span class="hljs-built_in">local</span><br/><span class="hljs-number">8</span>d3gs95h5c5q go<span class="hljs-attribute">-demo</span>         overlay swarm<br/><span class="hljs-number">4</span>d0719f20d24 host            host    <span class="hljs-built_in">local</span><br/>eafx9zd0czuu ingress         overlay swarm<br/><span class="hljs-number">81</span>d392ce8717 <span class="hljs-literal">none</span>            <span class="hljs-built_in">null</span>    <span class="hljs-built_in">local</span></strong>
</pre>
<p>As you can see, we have two networks that have the <kbd>swarm</kbd> scope. The one named <kbd>ingress</kbd> was created by default when we set up the cluster. The second <kbd>go-demo</kbd> was created with the <kbd>network create</kbd> command. We'll assign all containers that constitute the <kbd>go-demo</kbd> service to that network.</p>
<p>The next chapter will go deep into the Swarm networking. For now, it is important to understand that all services that belong to the same network can speak with each other freely.</p>
<p>The <em>go-demo</em> application requires two containers. Data will be stored in MongoDB. The back-end that uses that <kbd>DB</kbd> is defined as <kbd>vfarcic/go-demo</kbd> container.</p>
<p>Let's start by deploying the <kbd>mongo</kbd> container somewhere within the cluster.<br/>
Usually, we'd use constraints to specify the requirements for the container (example: HD type, the amount of memory and CPU, and so on). We'll skip that, for now, and tell Swarm to deploy it anywhere within the cluster:</p>
<pre>
<strong>docker service create --name go-demo-db \<br/>  --network go-demo \<br/>  mongo:<span class="hljs-number">3.2</span>.<span class="hljs-number">10</span></strong>
</pre>
<p>Please note that we haven't specified the port <kbd>Mongo</kbd> listens to <kbd>27017</kbd>. That means that the database will not be accesible to anyone but other services that belong to the same <kbd>go-demo</kbd> network .</p>
<p>As you can see, the way we use service create is similar to the Docker <kbd>run</kbd> command you are, probably, already used to.</p>
<p>We can list all the running services:</p>
<pre>
<strong>docker service ls</strong>
</pre>
<p>Depending on how much time passed between <kbd>service create</kbd> and <kbd>service ls</kbd> commands, you'll see the value of the <kbd>REPLICAS</kbd> column being zero or one. Immediately after creating the service, the value should be <kbd><em>0/1</em></kbd>, meaning that zero replicas are running, and the objective is to have one. Once the <kbd>mongo</kbd> image is pulled, and the container is running, the value should change to <kbd><em>1/1</em></kbd>.</p>
<p>The final output of the <kbd>service ls</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME        MODE        REPLICAS  IMAGE<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span>  replicated  <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span></strong>
</pre>
<p>If we need more information about the <kbd>go-demo-db</kbd> service, we can run the service inspect command:</p>
<pre>
<strong>docker service inspect go-demo-db</strong>
</pre>
<p>Now that the database is running, we can deploy the <kbd>go-demo</kbd> container:</p>
<pre>
<strong>docker service create --name go-demo \<br/><span class="hljs-operator">  -e</span> DB=go-demo-db \<br/>  --network go-demo \<br/>  vfarcic/go-demo:<span class="hljs-number">1.0</span></strong>
</pre>
<p>There's nothing new about that command. The service will be attached to the <kbd>go-demo</kbd> network. The environment variable <kbd>DB</kbd> is an internal requirement of the <kbd>go-demo</kbd> service that tells the code the address of the database.</p>
<p>At this point, we have two containers (<kbd>mongo</kbd> and <kbd>go-demo</kbd>) running inside the cluster and communicating with each other through the <kbd>go-demo</kbd> network. Please note that none of them is <em>yet</em> accessible from outside the network. At this point, your users do not have access to the service API. We'll discuss this in more details soon. Until then, I'll give you only a hint: <em>you need a reverse proxy</em> capable of utilizing the new Swarm networking.</p>
<p>Let's run the <kbd>service ls</kbd> command one more time:</p>
<pre>
<strong>docker service ls</strong>
</pre>
<p>The result, after the <kbd>go-demo</kbd> service is pulled to the destination node, should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME       MODE       REPLICAS IMAGE<br/>go<span class="hljs-attribute">-demo</span>    replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span><br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span></strong>
</pre>
<p>As you can see, both services are running as a single replica:</p>
<p><img class="image-border" src="assets/swarm-nodes-single-container.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2-8: Docker Swarm cluster containers communicating through the go-demo SDN<span><br/></span></div>
<p>What happens if we want to scale one of the containers? How do we scale our services?</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Scaling services</h1>
            </header>

            <article>
                
<p>We should always run at least two instances of any given service. That way they can share the load and, if one of them fails, there will be no downtime. We'll explore Swarm's failover capability soon and leave load balancing for the next chapter.</p>
<p>We can, for example, tell Swarm that we want to run five replicas of then <kbd>go-demo</kbd> service:</p>
<pre>
<strong>docker service scale go-demo=<span class="hljs-number">5</span></strong>
</pre>
<p>With the <kbd>service scale</kbd> command, we scheduled five replicas. Swarm will make sure that five instances of <kbd>go-demo</kbd> are running somewhere inside the cluster.</p>
<p>We can confirm that, indeed, five replicas are running through the, already familiar, <kbd>service ls</kbd> command:</p>
<pre>
<strong>docker service ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME       MODE       REPLICAS IMAGE<br/>go<span class="hljs-attribute">-demo</span>    replicated <span class="hljs-number">5</span>/<span class="hljs-number">5</span>      vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span><br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span></strong>
</pre>
<p>As we can see, five out of five <kbd>REPLICAS</kbd> of the <kbd>go-demo</kbd> service are running.</p>
<p>The <kbd>service ps</kbd> command provides more detailed information about a single service:</p>
<pre>
<strong>docker service ps go-demo</strong>
</pre>
<p>The output is as follows (IDs and ERROR PORTs columns are removed for brevity):</p>
<pre>
<strong>NAME      IMAGE               NODE   DESIRED STATE CURRENT STATE            <br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.1</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> node<span class="hljs-subst">-</span><span class="hljs-number">3</span> Running       Running 1 minute ago<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.2</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> node<span class="hljs-subst">-</span><span class="hljs-number">2</span> Running       Running <span class="hljs-number">51</span> seconds ago<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.3</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> node<span class="hljs-subst">-</span><span class="hljs-number">2</span> Running       Running <span class="hljs-number">51</span> seconds ago<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.4</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> node<span class="hljs-subst">-</span><span class="hljs-number">1</span> Running       Running <span class="hljs-number">53</span> seconds ago<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.5</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> node<span class="hljs-subst">-</span><span class="hljs-number">3</span> Running       Running 1 minute ago</strong>
</pre>
<p>We can see that the <kbd>go-demo</kbd> service is running five instances distributed across the three nodes. Since they all belong to the same <strong>go-demo SDN</strong>, they can communicate with each other no matter where they run inside the cluster. At the same time, none of them is accessible from outside:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="134" src="assets/swarm-nodes-replicas-1.png" width="477"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2-9: Docker Swarm cluster with go-demo service scaled to five replicas<span><br/></span></div>
<p>What happens if one of the containers is stopped or if the entire node fails? After all, processes and nodes do fail sooner or later. Nothing is perfect, and we need to be prepared for such situations.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Failover</h1>
            </header>

            <article>
                
<p>Fortunately, failover strategies are part of Docker Swarm. Remember, when we execute a <kbd>service</kbd> command, we are not telling Swarm what to do but the state we desire. In turn, Swarm will do its best to maintain the specified state no matter what happens.</p>
<p>To test a failure scenario, we'll destroy one of the nodes:</p>
<pre>
<strong>docker-machine rm <span class="hljs-operator">-f</span> node-<span class="hljs-number">3</span></strong>
</pre>
<p>Swarm needs a bit of time until it detects that the node is down. Once it does, it will reschedule containers. We can monitor the situation through <kbd>service ps command</kbd>:</p>
<pre>
<strong>docker service ps go-demo</strong>
</pre>
<p>The output (after rescheduling) is as follows (<kbd>ID</kbd> is removed for brevity):<br/>
<br/></p>
<div class="CDPAlignCenter CDPAlign"><img height="129" src="assets/Capture.png" width="980"/></div>
<p>As you can see, after a short period, Swarm rescheduled containers among healthy nodes (<kbd>node-1</kbd> and <kbd>node-2</kbd>) and changed the state of those that were running on the failed node to <kbd>Shutdown</kbd>. If your output still shows that some instances are running on the <kbd>node-3</kbd>, please wait for a few moments and repeat the <kbd>service ps command</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What now?</h1>
            </header>

            <article>
                
<p>That concludes the exploration of basic concepts of the new Swarm features we got with <em>Docker v1.12+</em>.</p>
<p>Is this everything there is to know to run a Swarm cluster successfully? Not even close! What we explored by now is only the beginning. There are quite a few questions waiting to be answered. How do we expose our services to the public? How do we deploy new releases without downtime? I'll try to give answers to those and quite a few other questions in the chapters that follow. The next one will be dedicated to the exploration of the ways we can expose our services to the public. We'll try to integrate a proxy with a Swarm cluster. To do that, we need to dive deeper into Swarm networking.</p>
<p>Now is the time to take a break before diving into the next chapter. As before, we'll destroy the machines we created and start fresh:</p>
<pre>
<strong>docker-machine rm <span class="hljs-operator">-f</span> node-<span class="hljs-number">1</span> node-<span class="hljs-number">2</span></strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>